{"title": "InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models", "authors": ["Kai Wang", "Shaozhang Niu", "Qixian Hao", "Jiwei Zhang"], "abstract": "As artificial intelligence advances rapidly, particularly with the advent of GANs and diffusion models, the accuracy of Image Inpainting Localization (IIL) has become increasingly challenging. Current IIL methods face two main challenges: a tendency towards overconfidence, leading to incorrect predictions; and difficulty in detecting subtle tampering boundaries in inpainted images. In response, we propose a new paradigm that treats IIL as a conditional mask generation task utilizing diffusion models. Our method, InpDiffusion, utilizes the denoising process enhanced by the integration of image semantic conditions to progressively refine predictions. During denoising, we employ edge conditions and introduce a novel edge supervision strategy to enhance the model's perception of edge details in inpainted objects. Balancing the diffusion model's stochastic sampling with edge supervision of tampered image regions mitigates the risk of incorrect predictions from overconfidence and prevents the loss of subtle boundaries that can result from overly stochastic processes. Furthermore, we propose an innovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting multi-scale features, enhancing feature representation by considering both semantic and edge conditions of the inpainted images. Extensive experiments across challenging datasets demonstrate that the InpDiffusion significantly outperforms existing state-of-the-art methods in IIL tasks, while also showcasing excellent generalization capabilities and robustness.", "sections": [{"title": "Introduction", "content": "Image inpainting, essential for digital restoration and other applications, has reached unprecedented realism through advancements in deep learning techniques (Rombach et al. 2022; Zeng et al. 2022; Feng et al. 2023; Park et al. 2024). This sophistication has outpaced traditional tampering detection methods, which depend on visible signs. The field of image forensics now faces the new challenge of detecting subtle, concealed manipulations through the emerging task of Image Inpainting Localization (IIL).\nResearchers have responded to this challenge by proposing various strategies, focusing primarily on three design paradigms: 1) Multi-stream frameworks (Dong et al. 2022; Niloy et al. 2023) that leverage multiple input streams to learn comprehensive representations; 2) Multi-stage (Liu et al. 2022; Guo et al. 2023) feature fusion frameworks that integrate features extracted at different levels of the backbone network; 3) Branch frameworks (Xu, Shen, and Lyu 2023; Shi et al. 2023) that implement a single-input, multiple-output architecture, including tasks like word segmentation and auxiliary branches. These techniques are built upon a semantic segmentation foundation, employing learning-based backbones for feature extraction and decoder heads to generate segmentation masks. However, without a sophisticated design, this approach is prone to overconfidence in predictions, leading to incorrect assessments, especially when dealing with increasingly sophisticated inpainting images (Barglazan, Brad, and Constantinescu 2024). The high invisibility of inpainting edges further complicates the task, as it blurs the distinction between the inpainted areas and the original image background, increasing the risk of misidentification.\nTo address the unique challenges of IIL, we develop a new framework called InpDiffusion. This framework leverages the diffusion model paradigm, known for its strong generative capabilities and sensitivity to conditions. Unlike traditional IIL models, InpDiffusion's iterative noise reduction mechanism simplifies the refinement process. Moreover, the diffusion model's stochastic sampling process introduces variability in predictions, mitigating the risk of overconfidence in the model's output. However, the direct application of diffusion models to IIL reveals certain limitations, such as limited discriminating power and excessive stochasticity in sampling, along with insufficient mask refinement. To address these limitations, we extract discriminative semantic and edge features from the inpainted images to guide the downstream denoising process. In addition, we introduce a novel edge supervision strategy within the denoising network to enhance InpDiffusion's ability to perceive subtle edges of locally inpainted objects. InpDiffusion generates both the denoised mask and the denoised edge of the inpainted object at each denoising step. By balancing the loss weights of these dual predictions, we aim to constrain the network and reduce detail loss caused by excessive randomness during each sampling step. More specifically, InpDiffusion integrates an Adaptive Conditional Network (ACN), which simultaneously extracts semantic and edge features from images. The Dual-stream Multi-scale Feature Extractor (DMFE) we designed is the core component of the ACN, enabling the adaptive extraction of multi-scale features to enhance the representation of both semantic and edge features. Then, the extracted semantic feature serves as a guiding clue for the denoising process, while the extracted edge feature is used for edge supervision, thereby strengthening the network's ability to capture fine details and reducing the randomness of each sample. This innovative dual approach is pivotal for identifying objects with concealed tampering traces, leading to more reliable and consistent results. As shown in Figure 1, when edge supervision is absent, the model treats a small shaded area below the ball as a tampering area due to excessive random sampling. In contrast, the model initially produces rough edges of the tampering region, but with the progress of sampling and continuous constraints of edge features, the model finally gets a refined prediction. InpDiffusion offers several advantages over previous IIL methods: 1) Multiple random sampling of the diffusion model can avoide overconfident point estimation issues. 2) Efficient extraction of multi-scale features allows for a more comprehensive understanding of the intrinsic differences between inpainted objects and the image background. 3) The balance between image semantic supervision and image edge supervision significantly improves the handling of detail loss in inpainted areas.\nOur main contributions can be summarized as follows:\n\u2022 We are the first to treat the IIL task as a mask generation paradigm and use a conditional diffusion framework to form predictions.\n\u2022 We propose a novel and effective framework called InpDiffusion, which introduces an edge supervision strategy within the denoising network to constrain excessive stochasticity in sampling at each step. Additionally, we develop a Dual-stream Multi-scale Feature Extractor (DMFE) to extract key discriminative features, guiding the downstream denoising processes to produce more accurate and generalized results for IIL tasks.\n\u2022 Comprehensive experiments confirms that InpDiffusion surpasses current benchmarks in IIL, while exhibiting remarkable adaptability across various scenarios."}, {"title": "Related Work", "content": "Recently, models based on CNNs and Transformers have shown impressive results across various visual tasks, leading to an increase in the development of IIL. For example, IID-Net (Wu and Zhou 2021) uses a high-pass filter to extract features from the high-frequency domain. CAT-Net (Kwon et al. 2021) performs double compression detection on JPEG images to obtain an encoder with microscopic feature weights and a parallel combination of macroscopic feature weights to form a dual-stream network. MVSS-Net (Dong et al. 2022) consists primarily of an edge supervision branch and a noise-sensitive branch, using the noise view, the boundary image, and the real image for feature learning. PSCC-Net (Liu et al. 2022) employs a progressive mechanism to predict the operation masks on all scales. It uses a spatio-channel correlation module to guide feature extraction via space attention and channel attenuation. UE-Net (Ji et al. 2023) is trained under dynamic supervision and produces estimated uncertainty maps to refine the detection results of manipulation, which significantly alleviates learning difficulties. TANet (Shi et al. 2023) locates areas for image manipulation using an operator-induced approach, particularly when detecting and segmenting tampered areas that are cleverly embedded in normal images. EditGuard (Zhang et al. 2024) embeds invisible watermarks to simultaneously achieve copyright protection and precise tamper localization, especially for AI-generated content editing methods.\nDespite progress, many IIL models remain constrained by overconfident point estimates and false segmentation. Moreover, their intricate structures require meticulous calibration to effectively address sophisticated tampering.\nDiffusion Models\nDiffusion Models, with its two-stage diffusion and reverse process, is a versatile framework for generating and manipulating data, including image generation (Dhariwal and Nichol 2021; Nichol and Dhariwal 2021), inpainting (Chung, Sim, and Ye 2022; Rombach et al. 2022), and editing (Avrahami, Lischinski, and Fried 2022; Choi et al. 2021). Its ability to learn from noise patterns has also made it a strong candidate for tasks such as super-resolution (Li et al. 2022; Wang et al. 2021), deblurring (Whang et al. 2022; Lee et al. 2022), image segmentation (Baranchuk et al. 2021; Brempong et al. 2022) and anomaly detection (Wolleb et al. 2022; Wyatt et al. 2022), where noise analysis is key. Based on diffusion models, our paper introduces the innovative InpDiffusion model, which advances the field of IIL and significantly enhancing the capacity for detailed analysis within this domain."}, {"title": "Method", "content": "Motivation. The potential of generative models (Choi et al. 2021; Ho, Jain, and Abbeel 2020) for complex semantic understanding has been extensively explored, offering advanced pattern recognition and iterative refinement capabilities. These capabilities provide a fresh perspective for tackling the increasingly challenging task of IIL. Drawing inspiration from these insights, we are motivated to investigate the potential of diffusion models to further contribute to the IIL field.\nOverview. In this section, we introduce our InpDiffusion framework, which progressively generates predictions by using image semantic priors to condition each subsequent step while employing edge priors to constrain the sampling process at each step. As illustrated in Figure 2, our framework comprises two main components: an Adaptive Conditional Network (ACN) and a Denoising Network (DN). We begin by introducing the preliminary, followed by a detailed discussion of the architectures of ACN and DN. Finally, we introduce the loss function used in InpDiffusion.\nPreliminary\nConventional IIL relies on a discriminative paradigm, classifying each pixel in an image $I \\in \\mathbb{R}^{H \\times W \\times 3}$ against an inpainted map $M \\in \\mathbb{R}^{H \\times W \\times 1}$. This approach uses a mapping function $F_{\\Theta}$, trained on labeled pairs $\\{I_i, M_i\\}_{i=1}^N$, to predict the presence ('1') or absence ('0') of inpainted areas. In contrast, our InpDiffusion framework is based on diffusion models, which consist of a forward diffusion process and a reverse denoising process. The forward diffusion process generates noisy masks from clean inputs by iteratively adding noise, following a Markov process. This process can be mathematically described as follows:\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I),$ (1)\nwhere $\\beta_t$ denotes the variance parameters of the Gaussian noise added at each step t. As t progresses from 1 to T, $x_t$ evolves from the original inpainted map $x_0$ through the process:\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline{\\alpha}_t} x_0, (1 - \\overline{\\alpha}_t) I),$ (2)\nwhere $\\alpha_t = \\prod_{i=1}^t \\alpha_i$, $\\alpha_t = 1 - \\beta_t$. During the training phase, we modulate the signal-to-noise ratio of the diffusion process by employing an SNR-based variance schedule (Hoogeboom, Heek, and Salimans 2023). For more detailed Settings and experiments on this aspect, please refer to Supplementary.\nIn reverse denoising, diffusion models execute a sequence of transitions from $x_T \\rightarrow x_{T-1} \\rightarrow \\cdots \\rightarrow x_0$, progressively refining initial noise $x_T$ towards a refined inpainted map $x_0$. Following (Song, Meng, and Ermon 2020), we choose to train a network (InpDiffusion) $f_\\theta(x_t, I, t)$ to directly estimate the denoised mask $x_0$ conditional on image $I$. The network learns the reverse distribution:\n$p(x_{t-1}|x_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, I, t), \\Sigma_\\theta(x_t, t)).$ (3)\nWhere, $\\Sigma_\\theta(x_t, t)$ is set to $\\sigma_t^2 = \\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_t} \\beta_t$, and $\\mu_\\theta(x_t, t)$ can be expressed as:\n$\\mu_\\theta(x, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\cdot \\frac{\\sqrt{1-\\overline{\\alpha}_t}}{\\sqrt{1-\\overline{\\alpha}_{t-1}}} x_t + \\frac{\\sqrt{\\alpha_{t-1}} \\beta_t}{1 - \\overline{\\alpha}_t} \\cdot \\frac{1-\\overline{\\alpha}_{t-1}}{1 - \\overline{\\alpha}_t} x_\\theta,$ (4)\nwhere $x_\\theta$ is predicted by InpDiffusion.\nAdaptive Conditional Network (ACN)\nThe primary function of the Adaptive Convolutional Network (ACN) is to adaptively extract semantic and edge features from inpainted images, allowing the downstream denoising network to recognize inpainted targets and their edges with greater accuracy. The entire ACN consists of two main components: Hierarchical Feature Extraction and Image Semantic and Edge Extraction."}, {"title": "Hierarchical Feature Extraction", "content": "Hidden tampering traces in inpainted images pose significant challenges for feature extraction by the model, which leads to a decline in the mask decoder's performance. To address this, we input the coarsely predicted mask $x_t$ from the previous step along with the inpainted image $I$ into the ACN, using $x_t$ as a guiding cue to focus the network on specific regions. Additionally, to enhance feature adaptability across denoising steps, we incorporate the time token (denoted as t) into the feature extraction process. This enables the ACN to autonomously adjust the conditional features according to the temporal stage. As shown in Figure 2 A., we use the Pyramid Vision Transformer (PVT) (Wang et al. 2022) as the backbone of the ACN to extract multi-scale features $(\\{f_i\\}_{i=1}^4)$ from concatenated $I$ and $x_t$. Thanks to the Transformer's token-based design, we can treat time as a token and incorporate it into each layer of patch sequences obtained from the backbone, which are then fed into the corresponding Transformer encoders. Formally, $f_i=R^{-1}(FN(MA([t;f_i])))$. Here, $[\\cdot; \\cdot]$ refers to the concatenation operation, and $R^{-1}$ converts tokens to feature maps. MA represents multi-head self-attention. FN denotes the feedforward neural network. $(\\{f_i\\}_{i=1}^4)$ are the hierarchical features we end up with."}, {"title": "Image Semantic and Edge Extraction", "content": "The extraction of key discriminative features is essential for denoising networks to accurately identify inpainted targets. To enhance this process, we propose the Dual-stream Multi-scale Feature Extractor (DMFE) module as a core component for adaptively extracting image semantic and edge conditions, enabling the capture of additional multi-scale features that the backbone network may not extract. The illustration of Image Semantic and Edge Extraction is shown in Figure 4. $(\\{f_i\\}_{i=1}^4)$ are used to extract semantic feature layer by layer based on DMFE. For the edge condition, we merge the detail-oriented low-level feature $f_1$ with the spatially informative high-level feature $f_4$, to accurately represent the edge feature associated with the inpainted object.\n\u2022 Dual-stream Multi-scale Feature Extractor: Given that the PVT-based backbone network may not extract abundant context information, we drew inspiration from the Inception module and Res2Net module (Gao et al. 2019) and subsequently developed a two-stream multi-scale feature extractor (DMFE). The DMFE consists of two streams. The first stream incrementally applies dilated convolution with a progressively increasing dilated convolution rate, thereby gradually widening the contextual understanding. In contrast, the second stream uses dilated convolution processing with a gradually decreasing rate, which first captures a broad context and then slowly narrows down on specific details. More specifically, we first reduce the number of channels of the input feature $f$ using a 1 \u00d7 1 convolution to facilitate subsequent processing. In the first stream, we utilize four branches to capture features at different scales, sequentially processed with a dilated convolution set with a dilation rate of {3,5,7}. Each branch is equipped with an asymmetric convolution that matches the size of the dilated convolution, reducing computational effort. The output of each branch is added to the input of the next branch. Concurrently, we use residual connections to enlarge the receptive field successively. The general form of an operation is defined as follows:\n$Dout_k = \\begin{cases} Cov_{3\\times1}(Cov_{1\\times3}(Cov(f))) k=1 \\\\ Cov_{3\\times3}(Cov(f) + Dout_{k-1}) k= 2,3,4 \\end{cases}$ (5)\nwhere k is the branch number, $Dout_k(\\cdot)$ represents the output of the kth branch in the first stream, + refers to element addition operation. $Cov_{3\\times3}(\\cdot)$ denotes the stacked convolutional layer mentioned above. $Cov_{3\\times1}(\\cdot)$ refers to 3 \u00d7 1 convolution operation, $Cov_{1\\times3}(\\cdot)$ refers to 1 \u00d7 3 convolution operation. In contrast to the first stream, the second stream, in turn, processes the features using a dilated convolution set with a dilation rate of {7, 5, 3}:\n$Uout_k = \\begin{cases} Cov_{3\\times3}(Cov(f)) k= 1 \\\\ Cov_{3\\times3}(Cov(f) + Uout_{k-1}) k= 2,3 \\\\ Cov_{3\\times1}(Cov_{1\\times3}(Cov(f) + Uout_{k-1})) k=4 \\end{cases}$ (6)\nwhere $Uout_k(\\cdot)$ denotes the output of the kth branch in the second stream, $Cov_{3\\times3}(\\cdot)$ refers to 1 \u00d7 1 convolution operation. After that, we add the output of the four branches for each stream to get the result. Finally, we concatenate the results of the two streams followed by a $Cov_{3\\times3}$ to obtain the output feature, which is computed as:\n$f_m = Cov_{3\\times3}([Add_{k=1}^4(Dout_k), Add_{k=1}^4(Uout_k)]).$ (7)\nwhere $Add_{k=1}^4(\\cdot)$ refers to the element addition of all four branches."}, {"title": "Denoising Network (DN)", "content": "Traditionally, Denoising Networks (DN) aim to decode the denoised mask predictions $x_0$ and $x_{t-1}$ based on the diffusion paradigm. The random sampling in the denoising network may result in insufficient model sensitivity to the edges of concealed inpainted objects. In contrast, our denoising network simultaneously predicts the edge $e$ of inpainted objects while generating denoised masks. Through edge loss supervision, we can balance the denoising process of the noised mask, thereby reducing detail loss caused by excessive randomness at each sampling step. As shown in Figure 2 B., our DN comprises an encoder and two decoders. The encoder extracts the positional information of inpainted objects from the noisy mask. One decoder, informed by the image semantic conditions extracted by the ACN, generates the final denoised mask predictions $x_0$ and $x_{t-1}$. The other decoder, utilizing the image edge conditions extracted by the ACN, produces the final denoised edge predictions $e$. Notably, we employ adaptive group normalization (Dhariwal and Nichol 2021) in both the encoder and decoder to incorporate the time step information t into the convolutional layers, making our DN sensitive to time step variations. Due to the iterative nature of the denoising process, our encoder and decoders employ a simple U-shaped structure, which has proven effective in achieving satisfactory results."}, {"title": "Loss Function", "content": "To ensure the denoising network generates accurate results, we measure the loss between the conditional denoising mask output $x_0$ and the ground truth $x_0$, as well as between the conditional denoising edge output $e$ and the ground truth $e$. These losses guide the optimization of IpnDiffusion. In our work, we introduce the Weighted Binary Cross-Entropy (WBCE) and Weighted Intersection over Union (WIoU) loss functions (Wei, Wang, and Huang 2020) for supervising the denoising result $x_0$ of each iteration of the network. For edge supervision, the Dice loss function (Xie et al. 2020) is used to handle the imbalance between positive and negative samples. The overall loss function for InpDiffusion is formulated as follows:\n$L_{total} = \\lambda L_{WBCE + WIOU}(x_0, x_0) + \\mu L_{DICE}(e, e).$ (8)\nwhere, $L_{WBCE + WIOU}(\\cdot)$ refers to the combination of WIoU and WBCE loss functions, $L_{DICE}(\\cdot)$ refers to Dice loss function, $\\lambda$ and $\\mu$ are adjustment parameters. We achieve optimal denoising by adjusting the ratio of $\\lambda$ to $\\mu$."}, {"title": "Experiments", "content": "In this section, we describe the experiments conducted on five different datasets to evaluate the effectiveness of In-pDiffusion. Our experiments are primarily conducted on a large dataset called Inpaint32K (Hao 2024), which contains a total of 32,000 inpainted images. This dataset is divided into four distinct inpainting types, with each type comprising 8,000 images: Traditional Methods-Based, CNN-Based, GAN-Based, and Diffusion Model-Based. Other datasets include an inpainting datasets (DID (Wu and Zhou 2021)), an AIGC dataset (AutoSplice (Jia et al. 2023)), a real-life dataset (IMD (Novozamsky, Mahdian, and Saic 2020)), and a traditional benchmark dataset (Nist (NIST 2016)). We compare our model with six baseline models. MantraNet (Wu et al. 2019), MVSSNet (Dong et al. 2022), TANet (Shi et al. 2023), MFINet (Ren et al. 2023), CFLNet (Niloy et al. 2023), and ECNet (Hao et al. 2024). More details about the datasets and baseline models are provided in Supplementary. We conduct separate ablation and comparative experiments for each inpainting type in Inpaint32K dataset. Following this, we pre-train our model using 32K inpainted images from the Inpaint32K dataset and evaluate it on the DID dataset. Notably, the other baseline models are also re-trained using the Inpaint32K, even if some are pre-trained on larger datasets. To further demonstrate the generalization performance of our model, we fine-tune it on other datasets of varying types. It's important to note that DID is exclusively used as a test set, while the other datasets are divided into training and test sets with a 9:1 ratio. We use the pixel-level Area Under the Curve (AUC) score as the evaluation metric.\nImplementation Details\nWe implement our InpDiffusion based on PyTorch using a single NVIDIA A800 with 80GB memory for both training and inference. For efficient training, the model undergoes a total of 150 training epochs. For optimization, the AdamW (Loshchilov and Hutter 2019) optimizer was utilized along with a batch size set to 32. To adjust the learning rate, we implemented the cosine strategy with an initial learning rate of 0.001. Notably, we set T = 10 for sampling and set SNR Shift to -2 log(6). In the loss function, we set the ratio of \u03bb to \u00b5 as 7:3."}, {"title": "Experiment Results", "content": "Quantitative Evaluations. Table 1 shows the quantitative results of our proposed InpDiffusion compared to six other state-of-the-art (SOTA) methods across four different inpainting types on the Inpaint32K dataset. Impressively, across all types of inpainting, InpDiffusion consistently outperformed other models in AUC score, demonstrating significant superiority even compared to the next best-performing model. Notably, when our proposed edge supervision strategy is integrated into the denoising network, the model's performance improves significantly across each inpainting type. This further highlights the critical role of edge guidance in accurately locating inpainted images with hidden tampering traces. Table 2 outlines the detection performance of InpDiffusion and other SOTA models on the DID dataset across ten different inpainting techniques, following pre-training on the Inpaint32K dataset. InpDiffusion consistently achieved superior accuracy in nearly all local inpainting techniques. However, MVSSNet exhibited slightly better performance in detecting images inpainted with the NS technique. We attribute this advantage to MVSSNet's extra pre-training on a larger dataset of 60K images, which enabled it to capture additional key information relevant to IIL.\nQualitative Evaluation. Figure 5 illustrates a comparative visual analysis of InpDiffusion against six established IIL baselines. In this figure, each row illustrates a typical inpainting scenario, with each inpainted region showing complex topological structures and edges. Compared to the first row, which employs traditional methods, and the second row, which utilizes CNN techniques, the inpainted regions in both cases do not fully form a complete semantic structure, presenting a significant challenge for current IIL models. However, the proposed InpDiffusion model excels in mitigating overly confident missegmentations caused by the random sampling inherent in each denoising step. Furthermore, the model effectively delineates clear boundaries due to its iterative denoising paradigm and edge supervision strategy."}, {"title": "Components", "content": "We conduct an ablation study on the individual components of InpDiffusion, with detailed results provided in Table 3. The absence of the DMFE module led to a decline in model performance, resulting in a 2.9% drop in AUC score, particularly on inpainted images based on CNN. This underscores the crucial role of DMFE in extracting discriminative image semantic and edge features. The inclusion of DMFE enhances the feature representation of locally inpainted regions, enabling the downstream denoising network to accurately identify and locate these areas. The lack of edge supervision also caused a decrease in InpDiffusion's detection performance across all types of locally inpainted images, highlighting the importance of edge supervision in complementing the denoising network's ability to generate precise prediction maps.\nGeneralization Performance Evaluation. To further evaluate the generalization performance of InpDiffusion and verify its scalability, we fine-tuned InpDiffusion and other SOTA methods using three different datasets involving other types of tampering. The experimental results, as shown in Table 4, indicate that our model continues to outperform other methods in localization performance. In particular, on the challenging real-life dataset IMD, our approach achieved a 10.2% higher AUC score than the second-best method, representing a significant improvement.\nRobustness Evaluation. To verify the robustness of In-pDiffusion, we conducted experimental evaluations on two types of image attack methods. The first type includes conventional image processing operations such as Gaussian noise and Gaussian filtering, which result in blurred image details and reduced contrast, creating fuzzy boundaries around the tampered areas and making it difficult for the model to accurately locate the tampered regions. The second type involves geometric transformation attacks, such as scaling and distortion, which alter the size, shape, and position of objects in the image, challenging the algorithm's ability to accurately identify the boundaries of the tampered areas. We tested our model on the relatively difficult-to-detect DM-based inpainted images in Inpaint32K. The experimental results are shown in Figure 6. Our method maintained high robustness against all types of image attacks, particularly against Gaussian noise and image distortion, where InpDiffusion was nearly unaffected by these attacks. We attribute this exceptional performance to InpDiffusion's use of joint supervision from the inpainted object's mask and edges to guide the step-by-step denoising process."}, {"title": "Conclusion", "content": "In this study, we propose a diffusion-based IIL model named InpDiffusion. InpDiffusion extracts semantic and edge features from inpainted images to guide the denoising process and progressively refine predictions. The introduction of edge supervision helps mitigate the risk of overconfident mispredictions and prevents the loss of subtle boundaries due to excessive randomness. We also introduce an innovative dual stream multiscale feature extractor to capture multi-scale features, enhancing feature representation. Extensive experiments demonstrate that InpDiffusion achieves SOTA performance, showcasing excellent generalization ability and robustness."}]}