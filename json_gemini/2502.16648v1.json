{"title": "Few-shot Continual Relation Extraction via Open Information Extraction", "authors": ["Thiem Nguyen", "Anh Nguyen", "Quyen Tran", "Tu Vu", "Diep Nguyen", "Linh Ngo", "Thien Nguyen"], "abstract": "Typically, Few-shot Continual Relation Extraction (FCRE) models must balance retaining prior knowledge while adapting to new tasks with extremely limited data. However, real-world scenarios may also involve unseen or undetermined relations that existing methods still struggle to handle. To address these challenges, we propose a novel approach that leverages the Open Information Extraction concept of Knowledge Graph Construction (KGC). Our method not only exposes models to all possible pairs of relations, including determined and undetermined labels not available in the training set, but also enriches model knowledge with diverse relation descriptions, thereby enhancing knowledge retention and adaptability in this challenging scenario. In the perspective of KGC, this is the first work explored in the setting of Continual Learning, allowing efficient expansion of the graph as the data evolves. Experimental results demonstrate our superior performance compared to other state-of-the-art FCRE baselines, as well as the efficiency in handling dynamic graph construction in this setting.", "sections": [{"title": "Introduction", "content": "Few-shot Continual Relation Extraction - FCRE (Qin and Joty, 2022a; Chen et al., 2023a) has emerged as a challenging problem, where models must continuously adapt to identify new relations in each sentence with a limited amount of data while preserving all the previous information accumulated over time. Current approaches have demonstrated researchers' efforts to simultaneously maintain important characteristics of FCRE models, including the (i) flexibility to adapt to new relations, (ii) preserve generalization, and (iii) effectively avoid forgetting. These requirements have been recently addressed through a special regularization strategy (Tran et al., 2024; Wang et al., 2023b). Besides, they can also be achieved through memory enrichment strategies (Ma et al., 2024a) or by designing special prompt inputs (Chen et al., 2023b) to guide the learning of models.\nHowever, the existing work are somewhat impractical when most of them focus solely on optimizing models in the scenario with a predefined set of entities and relations. Specifically, the models are only trained and tested on a given set of entities in each sentence, with the corresponding targets also being predefined, making the questions of the applicability of current approaches. Nevertheless, in the real open-world scenarios (Xu et al., 2019; Mazumder and Liu, 2024), more potential pairs of entities can appear in a testing sample, which is often uncovered in the training dataset. To deal with this drawback, several methods (Wang et al., 2023a; Zhao et al., 2025, 2023; Meng et al., 2023) have considered unknown labels, but their training only relies on available information, including provided entities and relations from the training set, and poorly considers a NOTA (None Of The Above) label for all possible relations that are uncovered. Therefore, it is essential to develop FCRE models, which are able to (a) identify relations between possible pairs of entities in a sentence by holistically taking advantage of the available information from training dataset, and, most importantly, (b) indicate whether the corresponding relations are known or not, and whether they are reasonable.\nRelated to these requirements, Open Information Extraction - OIE (Liu et al., 2022; Zhou et al., 2022; Li et al., 2023) has been known as a solution for covering all possible pairs of entities and relations. Most recently, EDC (Zhang and Soh, 2024) has proposed an efficient OIE strategy to extract any triples from given texts to construct relational graphs. However, this work and other related ones in the field of Knowledge Graph Construction (KGC) are only limited to exploiting information from a fixed dataset, while their ability in Fewshot Continual Learning scenarios, where there are always emerging relations, has not been explored. In this challenging scenario, the models need to adapt to information arriving sparsely and continuously over time. Besides, adapting the model to new information also poses challenges in reproducing relations from the previous knowledge that has been integrated into the graph.\nTo address the challenges of FCRE in practical settings and to explore the potential of KGC models in this complex scenario, we propose a novel solution inspired by the concept of Open Information Extraction (OIE). More specifically, our approach involves leveraging Named Entity Recognition (NER) (Zaratiana et al., 2023) to extract and analyze all possible pairs of entities, focusing on identifying both determined and undetermined relations between them. We then utilize OIE for open extraction and generating corresponding descriptions that help effectively align sample representations. In addition, our sample-description matching schema is also a more effective solution for building KGC models in the latest SOTA (Zhang and Soh, 2024), which was based on description-description constraints. For testing, we dedicatedly employ OIE to filter out non-relational samples before they enter the FCRE modules, thereby specifically determining whether a tested sample has a known or unknown relation, or whether it is reasonable, thus improving overall testing performance.\nIn summary, our key contributions include:\n\u2022 We propose a novel solution for FCRE via Open Information Extraction, which effectively deals with unknown labels by considering all possible pairs of entities and corresponding relations in each sentence. Based on this, we can determine whether a pair has a known or reasonable relationship, which existing methods have not considered.\n\u2022 For the first time, we consider the potential of Knowledge Graph Construction (KGC) models in the setting of Continuous Learning, where there are always emerging relations. In addition, our novel approach effectively elevates schema matching in KGC as minimizing errors from LLM-based schema extraction.\n\u2022 Experimental results indicate our superior performance over FCRE baselines in all cases with N/A relation or not. In addition, the superiority over KGC models in this extreme scenario is also demonstrated."}, {"title": "Related Work and Background", "content": "2.1 Related Work\nMost existing FCRE methods (Wang et al., 2023b; Hu et al., 2022; Ma et al., 2024b; Tran et al., 2024) have utilized contrastive learning and memory replay techniques to significantly mitigate catastrophic forgetting. However, these approaches largely overlook the present of undetermined relations - relations that are unseen or nonexistent, which remains a critical gap in real-world applications. On the other hand, several methods (Wang et al., 2023a; Zhao et al., 2025, 2023; Meng et al., 2023) have considered unknown labels, but their training only relies on available information, including provided entities and relations from the training set, and poorly considers a NOTA (None Of The Above) label for all possible relations that are uncovered.\nHistorically, relation extraction research has explored various types of undetermined relations. For example, prior work has defined \u201cno relation (NA)\u201d (Xie et al., 2021) as sentences with no meaningful relationship between entities, \"out-of-scope (OOS)\u201d (Liu et al., 2023) as relations outside predefined sets, and \"none of the above (NOTA)\u201d (Zhao et al., 2023) as relations that do not match any known type. While these studies address specific aspects of undetermined relations, their approaches are often simplistic and unrealistic, focusing on single labeled entity pairs rather than considering multiple possible relations within sentences.\nMoreover, Open Information Extraction (OIE) has emerged as a powerful tool for open entity and relation extraction, particularly for knowledge graph construction, due to its ability to operate without predefined schemas. Recent studies (Li et al., 2023) highlight the strong performance of large language models (LLMs) in OIE tasks. For instance, EDC (Zhang and Soh, 2024) propose an end-to-end pipeline that extracts, defines, and canonicalizes triplets to build knowledge graphs more efficiently. This pipeline includes three phases: (1) Open Information Extraction, where entity-relation triplets are freely extracted from text; (2) Schema Definition, where entity and relation types are defined based on extracted triplets; and (3) Schema Canonicalization, which standardizes relations to fit a target schema. This approach is particularly promising for handling un-"}, {"title": "Background", "content": "2.2.1 Problem Definition\nFew-Shot Continual Relation Extraction (FCRE) requires a model to sequentially acquire new relational knowledge while retaining previously learned information. At each task t, the model is trained on a dataset $D^t = {\\{(x, y)\\}}_{N \\times K}$, where N denotes the number of labels provided in the set of relations $R_t$, and K represents the limited number of training instances per relation (i.\u0435., \"N-way-K-shot\" paradigm Chen et al. (2023a)). Each training example (x, y) consists of a sentence x, which is originally given two entities $(e_h, e_t)$ and the associated relation labels $y \\in R_t$. After completing task t, previously observed datasets $D^{<t}$ are not extensively reused. The model's final evaluation is conducted on a test set comprising all encountered relations $R_T = \\cup_{t=1}^T R_t$.\nBeyond the standard setting and requirements of FCRE, in terms of mitigating forgetting and overfitting, our work aims at designing advanced models, which are capable of continuously capturing and recognizing new relational knowledge, which is not available in the training set.\n2.2.2 Latent Representation Encoding\nOne of the fundamental challenges in relation extraction lies in effectively encoding the latent representation of input sentences, particularly given that Transformer-based models (Vaswani et al., 2017) produce structured matrix representations. In this study, we adopt an approach inspired by Ma et al. (2024a). Given an input sentence x that contains a head entity $e_h$ and a tail entity $e_t$, we transform it into a Cloze-style template $T(x)$ by inserting a [MASK] token to represent the missing relation. The structured template is defined as:\n$T(x) = x\\_{[:n\\_0-1]} e\\_h x\\_{[n\\_0:n\\_1-1]} [MASK] x\\_{[n\\_1:n\\_2-1]} e\\_t x\\_{[n\\_2:n\\_3-1]}.$ (1)\nwhere $x\\_{[i:j]}$ represents learnable continuous tokens, and $n\\_i$ denotes the respective token positions in the sentence. In our specific implementation, BERT's [UNUSED] tokens are used for [v]. We set the soft prompt length to 3 tokens, with $n\\_0, n\\_1, n\\_2,$ and $n\\_3$ assigned values of 3, 6, 9, and 12, respectively. The transformed input $T(x)$ is then processed through a pre-trained BERT model, encoding it into a sequence of continuous vectors. The hidden representation z of the input is extracted at the position of the [MASK] token:\n$z = M\\_\\theta(T(x))[position([MASK])],$ (2)\nwhere $M\\_\\theta$ represents the backbone language model. The extracted latent representation is subsequently passed through a multi-layer perceptron (MLP), allowing the model to infer the most appropriate relation for the [MASK] token."}, {"title": "Proposed Method", "content": "In real-world applications, Relation Extraction aims to identify relationships between all possible pairs of entities within documents. However, a significant challenge arises from the presence of Undetermined Relations (UR) between entities, which is either not applicable or unknown as Appendix A.1.2. Particularly, UR can be categorized into two types as follows:\n\u2022 No Relation (NA): Used when no meaningful relationship exists between entities.\n\u2022 None Of The Above (NOTA): Used when an entity pair does not fit any predefined relations.\nRelated to this problem, previous studies (Zhao et al., 2025) primarily focus on NOTA relations and evaluate models using a simple threshold on the test set including unseen relations. However, this approach does not reflect real-world scenarios as they still only considered a predefined set of entity pairs and corresponding relations when training.\nThis section will present our novel approach to dealing with this problem, going from extracting all possible entities to create an open dataset (Section 3.1), to how OIE is utilized to support FCRE with undetermined relations (Section 3.2), and finally our training and inference procedures in Section 3.3.\n3.1 Open Dataset Construction\nThis is the first stage to extract all possible entities in a sentence for the training phase. Particularly, we employ a Named Entity Recognition (NER) model as Figure 1. However, extracted entities may not perfectly align with the original dataset annotations, thus we merge the extracted entities with overlapped ones in the benchmark dataset to ensure the consistency.\nBeyond this step, we assign labels to all possible entity pairs. If an extracted entity pair matches a predefined relation in the benchmark dataset, it is categorized as a determined relation (DR); otherwise, it is classified as an undetermined relation (UR). This approach results in a more comprehensive and realistic dataset, incorporating both original relations and undetermined relations as newly labeled instances with descriptions. Each extracted entity pair with sample from the merged list is treated as an independent instance, rather than just a sample with the original entity pair, serving as input for the relation extraction task. Consequently, the dataset size significantly increases due to the large number of undetermined relations, making it more reflective of real-world scenarios.\n3.2 Open Information Extraction\nUnlike existing FCRE methods, this module in Fig.1 aims to identify unseen relations, thereby expanding the scope of knowledge extraction for more efficient training. In particular, we employ the OIE module of EDC to extract relations between entities without any predefined label set. To this end, we employ ChatGPT-40-mini to generate candidate triplets that contain relations and follow a structured prompting approach, as illustrated in Fig.10.\n3.3 FCRE via OIE (OFCRE)\nThis section in Fig.1 presents our training and testing process. Paticularly, we demonstrate how expanding the relation set with UR aids efficient training and enables the model to handle unseen labels.\n3.3.1 Training phase\nData Augmentation Overall, relation descriptions from both training datasets and LLM generation are typically concise, generic, and applicable to multiple samples (Han et al., 2018; Zhang et al., 2017). However, relying solely on these limited descriptions can constrain model performance, motivating us to enhance them with greater diversity.\n\u2022 For each original description, we augment it with K additional samples. Each sample includes an example sentence closely related to the target relation, thereby improving the alignment between the embeddings of the relation and the corresponding diverse descriptions.\n\u2022 Similarly, to utilize the candidate triplet produced by the OIE module, an additional prompt is crafted to deliver K distinct candidate relation descriptions with examples. This aids in examine the surrounding context to formulate a candidate description tailored to the identified relation. These context-sensitive descriptions serve as enhanced refinements of the original ones, offering more accurate and detailed representations.\nThese enriched descriptions contribute to better model generalization. Note that description augmentation is applied only to seen relation types, not to undetermined relations. Further details on both types of descriptions can be found in Appendix C.\nObjective Functions"}, {"title": "Objective Functions", "content": "Hard Soft Margin Loss (HSMT) To enhance the distinction between different relations, HSMT integrates both hard and soft margin triplet loss principles (Hermans et al., 2017), dealing with the most challenging positive and negative samples while maintaining flexibility through a soft margin. Formally, the loss function is defined as:\n$L\\_{\\text{HSMT}}(x) = -log \\bigg(1 + \\frac{\\text{max}\\_{p \\in P(x)} e^{f(z\\_x, z\\_p)}}{\\text{min}\\_{n \\in N(x)} e^{f(z\\_x, z\\_n)}}\\bigg),$ (3)\nwhere $f(., .)$ denotes the Euclidean distance function. This formulation effectively maximizes the separation between the hardest positive and hardest negative samples while allowing for adaptive margin flexibility, improving representations in the latent space.\nWeighted Mutual Information Loss This loss aims to maximize the mutual information between relation embedding $z\\_i$ and its corresponding retrieved description embedding $d\\_i$ of sample $x\\_i$, ensuring a more informative alignment. Following van den Oord et al. (2018), the mutual information $MI(x)$ between $z\\_i$ and its corresponding label description satisfies:\n$MI(x) \\geq log B + InfoNCE(\\{z\\_i\\}\\_{i=1}^B; h),$ (4)\nwhere $InfoNCE(\\{z\\_i\\}\\_{i=1}^B; h) = \\frac{1}{B} \\sum\\_{i=1}^B log \\frac{e^{h(z\\_i, d\\_i)}}{\\sum\\_{j=1}^B e^{h(z\\_j, d\\_i)}},\\,$ (5)\nand\n$h(z\\_j, d) = exp(\\frac{z\\_j^T W d}{\\tau}).$ (6)\nHere, $\\tau$ is the temperature parameter, B is the mini-batch size, and W is a trainable weight matrix. We define $P(x)$ as the set of positive samples (same-label pairs) and N(x) as the set of negative samples (different-label pairs).\nGiven the imbalance caused by a high proportion of Undetermined Relation (UR) labels, we introduce a weight adjustment based on the number of samples for each unique relation type in the batch:\n$w\\_x = \\frac{B}{\\frac{\\|P(x)\\|}{\\|B\\|} + \\sum\\_{y \\in N(x)} \\frac{\\|P(y)\\|}{\\|B\\|}},$ (7)\nThe final Weighted MI loss function is formulated as:\n$L\\_{\\text{WMI}}(x, d) = -w\\_x log \\frac{h(z\\_x, d^k)}{\\text{Z(x, d)}},$ (8)\nwhere\n$\\text{Z(x, d)} = \\sum\\_{k=1}^K h(z\\_x, d^k) + \\sum\\_{n \\in N(x)} \\sum\\_{k=1}^K h(z\\_x, d^k\\_n)$ (9)\nThis loss is applied not only to the raw description but also to the candidate description to enhance the learning of sample representations. These types of descriptions serve as stable reference points when learning known relation types within batches containing numerous undetermined relations. We optimize this description loss as follows:\n$L\\_{\\text{Des}} = L\\_{\\text{WMI}}(x, d) + L\\_{\\text{WMI}}(x, c)$ (10)\nTraining Objective Function The final optimization objective combines losses that align both sample-to-sample and sample-to-description representations, incorporating weighted coefficients:\n$L(x) = L\\_{\\text{Samp}} + L\\_{\\text{Des}}$\n$=\\alpha\\_x L\\_{\\text{HSMT}}(x) + \\alpha\\_d L\\_{\\text{WMI}}(x, d)$ (11)\n$+ \\alpha\\_c L\\_{\\text{WMI}}(x, c)$\nwhere $\\alpha\\_x, \\alpha\\_d,$ and $\\alpha\\_c$ are tunable hyperparameters controlling the relative contribution of each loss term."}, {"title": "Experiments", "content": "1. Initialization (Lines 1\u20132): The model parameters for the current task", "3)": "To accommodate new relations introduced in Ti", "4\u20138)": "For each relation r\u2208 Rj", "9)": "A prototype set P\u2081 is generated based on the updated memory Mj for inference purposes.\n5. Memory-Based Training (Line 10): The model ; is further refined by training on the enhanced memory dataset M to reinforce its ability to retain and recall previously learned relations."}]}