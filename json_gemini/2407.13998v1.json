{"title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering", "authors": ["Rujun Han", "Yuhao Zhang", "Peng Qi", "Yumo Xu", "Jenyuan Wang", "Lan Liu", "William Yang Wang", "Bonan Min", "Vittorio Castelli"], "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA ARENA by directly comparing model-generated answers against LFRQA's answers using LLMs as evaluators. We show via extensive experiments that RAG-QA ARENA and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA ARENA as a challenging evaluation platform for future research.", "sections": [{"title": "1 Introduction", "content": "Traditional reading comprehension task is constrained to fixed contexts (Rajpurkar et al., 2016; Ko\u010disk\u00fd et al., 2018; Huang et al., 2019). It is inadequate at addressing real-world questions, with no context provided for a system to find answers. Such open-ended questions require a system to identify answers in an enormous knowledge base (e.g., Wikipedia) that is computationally prohibitive to feed into LLMs. Retrieval-augmented generative question answering (RAG-QA) becomes an effective tool to filter out massive amounts of noise and select only a few highly relevant passages for LLM-based QA models.\nThe wide applications of RAG-QA (Gao et al., 2023) necessitate the evaluation of systems' out-of-domain (OOD) performances because a real-world system often confronts new data unseen during training. Existing popular benchmark datasets such as Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) consist solely of Wikipedia or Web documents, which fall short of measuring OOD performances.\nROBUSTQA (Han et al., 2023) was the first dataset created to benchmark cross-domain robustness for RAG-QA. However, as illustrated by the yellow highlights in Figure 1, ROBUSTQA follows NQ's annotation format with short answer spans extracted from the documents. Such data format is not the most suitable reference answer to evaluate the current leading LLMs that typically generate long-form responses with multiple information combined in one coherent narrative (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2024). Consequently, token overlap metrics used in the extractive QA era (Karpukhin et al., 2020; Izacard et al., 2024) will penalize unfairly on the additional supporting tokens generated by LLMs, resulting in extremely low overlap scores. As an example, Fig. 1's extractive answers have poor Exact-Match or F\u2081 scores with the final long-form answer. To create a long reference answer, one can simply concatenate these short answers, but the synthesized answers are either incoherent or ill-formatted, as illustrated by examples in Sec. 3.1.\nTo address the drawbacks, we propose long-form RobustQA (LFRQA) that integrates multiple short extractive answers into a coherent long-form answer. Figure 1 shows an annotation where three extractive answers are combined by annotators to create a comprehensive answer. Table 1 summarizes seven features in LFRQA that make it uniquely beneficial for RAG-QA evaluations. ASQA (Stelmakh et al., 2022) and ELI5 (Fan et al., 2019) are the most similar datasets to LFRQA. However, they are either not directly annotated against the underlying corpus (thus, not RAG-QA), or rely on single-domain data, which is insufficient to benchmark systems' cross-domain performances.\nWith LFRQA annotations, we propose RAG-QA ARENA that leverages model-based evaluators to directly compare LLMs' answers with LFRQA without the necessity to examine long and potentially noisy retrieved passages. By demonstrating the high correlation with human judges following the same instruction and rubrics, we show that RAG-QA ARENA is an efficient and accurate framework to benchmark the RAG-QA system's cross-domain performances. In this work, we primarily focus on the LLMs used for the QA component, but RAG-QA ARENA can be easily extended to study retrieval's impact on answer generation quality.\nWe summarize our contributions: 1) We present LFRQA, the first high-quality and large-scale multi-domain human annotations with coherent long-form answers for RAG-QA. 2) We propose an efficient model-based evaluation framework, RAG-QA ARENA that enables users to directly compare LLMs' answers with ground-truth answers in LFRQA. 3) We build a dashboard incorporating a wide range of leading LLMs and conduct in-depth analysis to show that LFRQA's answers are preferred significantly more to the best LLMs with long context. Therefore, we believe RAG-QA ARENA will serve as a challenging and robust evaluation benchmark for future RAG-QA research."}, {"title": "2 RAG-QA Task Formulation", "content": "We briefly introduce the RAG-QA task in this section. Passage retrieval is the first step of a RAG-QA pipeline. Following the passage retrieval set-up in DPR (Karpukhin et al., 2020) and ROBUSTQA (Han et al., 2023), we denote a collection of documents as D. We split each document $d' \\in D$ with a fixed length N tokens and obtain a collection of M (\u2265 |D|) passages denoted as $C = {P_1, P_2, \u2026\u2026p_m, ...P_M}$, where $p_m$ is a passage.\nGiven a question q, the passage retrieval task is to select K most relevant passages for q with a retriever R from C. Formally, $R(q, C) \u2192 C_q$.\nUpon receiving top K passages or $C_q$, a QA model reads them as context to generate an answer for the query. Unlike the extractive QA setting in ROBUSTQA and NQ, we adopt generative QA as it is most compatible with the generative nature of the leading LLMs with the flexibility to produce free-form answers. The answer generation task can be modeled as $\u03a3\u2020P_q(w_t|W_{0:t\u22121};C_q)$, where P is an LLM. We focus on the variations of P and fix R in this work.\nIn real-world applications, we deploy RAG-QA systems into various domains such as healthcare, finance, and technology whose corpus and query types may not be well covered in a trained re-"}, {"title": "3 Data Creation", "content": "LFRQA consists of two types of QA samples: 1) new annotations in Finance ([FI]), Lifestyle ([LI]), Recreation ([RE]), Technology ([TE]), Science ([SI]), and Writing ([WR]) domains; 2) adapted long-form BioASQ ([BI]). We describe the details of both QA samples in the following sections."}, {"title": "3.1 Annotated Data", "content": "Following ROBUSTQA (Han et al., 2023), LFRQA'S new annotations are also based on the LOTTE and FiQA queries and corpus. LOTTE was proposed in the ColBERTv2 paper (Santhanam et al., 2022) and consists of information retrieval (IR) datasets across five domains: lifestyle, recreation, technology, writing, and science, each can have relevant answers coming from either web search or on-line forum. FIQA (Maia et al., 2018) proposes a task, \"Opinion-based QA over financial data\" that answers finance-related questions from financial corpora such as microblogs, reports, and news. It is important to note that both FiQA and LoTTE are IR datasets with answers as long documents, which may include a large amount of irrelevant information to the query.\nAs IR datasets, both FiQA and LOTTE could only provide relevant documents to users, as there are no precise answer annotations. ROBUSTQA addresses this short-coming by extracting short answer spans from the long documents in the similar format of NQ, which serves as a high-quality benchmark for extractive RAG-QA. Figure 1 shows an example where the yellow highlights in Documents 1-3 are the extracted answers to the question.\nLimitations of extractive RAG-QA. In the era of LLMs, models' responses to user queries are often long and comprehensive (OpenAI, 2024; Anthropic, 2024; MetaAI, 2024; Jiang et al., 2024), which the short, extractive reference answers in ROBUSTQA are no longer the most compatible format to evaluate against. First, in ROBUSTQA, annotators are limited to only taking 3 answer spans per relevant document, each with no more than 16 words. This process could result in a loss of useful information to help answer the query. Second, to reconcile multiple extractive references for model evaluation, prior work in extractive RAG-QA (Karpukhin et al., 2020; Han et al., 2023; Izacard et al., 2024) adopt the maximum of token overlaps between a model prediction and a list of references to compute EM or F\u2081 metrics, which penalizes unfairly the long-form responses from modern LLMs. Finally, if we naively concatenate or list all short answer spans as shown by examples in Figure 2, the combined answers are often too ill-formatted or incoherent as ground-truth answers.\nLFRQA addresses all of these drawbacks by instructing the annotators to integrate all short answers in ROBUSTQA into a coherent long-form answer. Below, we show a summary of our annotation instruction and quality control mechanism.\nAnnotation instruction. As Fig. 1 shows, a query, all relevant documents, and original short answers (highlighted in the documents) are presented to annotators on a single annotation page. Annotators need to combine all highlighted answers into a single complete and coherent answer. All highlighted answers MUST be included; otherwise, the annotation is considered as a failure. Annotators are encouraged to include more information in the documents if it helps to answer the queries. To ensure annotators faithfully use the document information, we request annotators to provide citations after each answer sentence. For example, the first sentence in Fig. 1 is composed using information from Documents 2 and 3. Annotators should add \"[2, 3]\" after that sentence. We use these citations primarily for data quality control and remove them during the answer evaluation. The actual annotation UI can be found in Appendix Fig. 5.\nQuality control. The data annotations are performed by contracted data professionals. We also have a dedicated team of data linguists to validate the annotation quality. Specifically, our data linguists randomly audit 10% of each batch of the annotations, and if the valid answer ratio is < 90%, we send the batch back to the annotators for rework. The process iterates until the valid answer ratio exceeds 90%. Here is a list of failure cases: 1. Incompleteness: Answers do not include all highlighted answers, or there is clear relevant information in the documents, but not included in the answer. 2. Redundancy: Clear irrelevant information is included in the answer. 3. Incoherence: Answers are not coherent or not written in natural English. 4. Citation Error: Wrong/missing citations, which indicate annotators do not use correct information from the right documents."}, {"title": "3.2 Adapted Data", "content": "For the biomedical domain, LFRQA leverages the same set of test queries as in ROBUSTQA, but uses the complete rather than span answers in the BioASQ dataset. The original BioASQ annotations provide two types of answer formats: 1) exact answer, which is the short extractive answers used in ROBUSTQA; 2) ideal answer, which is a long-form abstractive answer to be consistent with other datasets in this work. We did not perform further annotations. We notice in Table 2 that BioASQ's answers are shorter compared with other datasets. This is due to its dominant amount of factoid queries, which do not require elaborated explanations as in other datasets with more open-ended reasoning questions (Han et al., 2023).\nWe drop SearchQA in ROBUSTQA as this dataset only has short-form extractive answers, and its documents contain a significant amount of text omission (\"...\") that prevents us from re-constructing long-form answers."}, {"title": "3.3 Data Statistics and Analysis", "content": "Table 2 summarizes the statistics for the test set, which consists of 16K queries across 7 domains. We filter out queries with more than 80 ground-truth documents, resulting in 73 fewer queries compared with ROBUSTQA. Since LFRQA combines multiple short answers, the answer per query ratio (A/Q) is always 1, and the word per answer ratio (W/A) is substantially higher compared with ROBUSTQA. We also annotate a dev set with 10K queries for future model development purposes, and the statistics can be found in Appendix Table 7. We conduct further analysis below to demonstrate the unique contributions of LFRQA in Table 1.\nAnswers over multiple documents. Figure 3 illustrates the distribution of number of documents used by LFRQA's answers. Specifically, Figure 3a shows that around 65% of the answers use \u2265 2 documents' information. 4.9% of the answers consist of information from 10 or more documents (maximum = 80). In Figure 3b, we divide long-form answers into sentences and show the distribution of the number of documents used per answer sentence. Nearly 22% of the answer sentences combine information from multiple documents. Both show that LFRQA's answers effectively combine information across multiple ground-truth documents. This makes LFRQA challenging for RAG-QA, as it requires identification and aggregation of information across sources.\nCoherent answers. LFRQA's answers further organize facts and views across multiple documents in a coherent paragraph. Answers with multiple views are common in the original ROBUSTQA's answers. Conducting a string match of both \u201cyes\u201d"}, {"title": "4 RAG-QA Arena", "content": "In this section, we propose our evaluation framework RAG-QA ARENA. Inspired by the pairwise human preference evaluation framework such as Chatbot Arena (Chiang et al., 2024), we calculate win-rate and win+tie rate against LFRQA as ground-truth as a metric to gauge systems' RAG-QA quality. Figure 4 illustrates the evaluation framework.\nThe choice of LFRQA as the target to compare has been partly justified in Sec. 3 for 1) Completeness: the annotation process encourages the inclusion of as much relevant information as possible. 2) Coherence: its answers are written more coherently and naturally than ROBUST-QA as references for LLM generations. Complete and coherent answers can be considered as a comprehensive summary of all relevant information in the entire corpus. This allows us to evaluate generated answers against LFRQA answers only, which is much more informative and concise than using retrieved passages, potentially with a large amount of noise.\nWe implement human and model-based evaluations with the same instructions and report their correlations. We will show results in Sec. 6.1 that further justify using LFRQA as evaluation targets."}, {"title": "4.1 Human Evaluation", "content": "We present a query and a pair of answers (one from LFRQA and one from an LLM), to human annotators. We instruct them to rate their preferences based on three aspects. 1. Helpfulness: information that is helpful/relevant to answer the query. (Touvron et al., 2023; Bai et al., 2022). 2. Truthfulness: information that is correct to answer the query. By our definition, truthful information should also be helpful information (Stephanie Lin, 2021; Aisha Khatun, 2024). 3. Completeness: include as much truthful and helpful information as possible. We further instruct annotators to use Truthfulness (being both truthful and helpful) as the primary criterion since it is stricter than Helpfulness. Helpfulness is used when a decision cannot be made by Truthfulness alone. More details including the definition of aspects, rating categories, and step-by-step guidelines can be found in Appendix A.7 and Fig. 6-7 (annotation interface)."}, {"title": "4.2 Model-Based Evaluation", "content": "Since human evaluation is too costly, we adopt model-based evaluators for scalable evaluation of LLMs on the entire LFRQA test set.\nAs for the evaluation approach, we provide LLM-based evaluators with a query and a pair of answers (including one from LFRQA). Similar to human evaluation, we prompt LLMs to rate their preferences based on the same three aspects above. We only modify the human instruction slightly to be compatible with LLM readable input text, but the majority of the prompt, especially the input data, and rubric, stay the same (Appendix Table 13-15).\nFor both human and model-based evaluations, we allow \"tie\" (no preference) as an option. For human evaluation, we take the majority votes from 3 annotators to mitigate biases. If there is no majority vote, we default the label to \"tie.\""}, {"title": "5 Experimental Setup", "content": "In this section, we discuss our retriever, LLMs experimented for both answer generation and pairwise evaluations and their prompts in more detail.\nRetrieval setting. We employ COLBERTV2 (Santhanam et al., 2022) as our passage retriever, considering its superior performance on the underlying corpus for both ROBUSTQA and LFRQA as shown in Han et al. (2023). We follow the same retrieval setting and split passages into text chunks with 100 consecutive words. We use the top 5 retrieved passages for our main results in Table 3 and experiment with the top 10 passages for further analysis.\nAnswer generation. We consider LLMs ranked top 252 in the Chatbot Arena (Chiang et al., 2024) and their smaller version to show the impact of model sizes. Due to resource and legal constraints, for proprietary LLMs, we only use OpenAI models: a) GPT-4-TURBO (2024-04-09), b) GPT-40 and c) GPT-4-0125-PREVIEW). For public models, we experiment with 1) MIXTRAL-8X22B-INSTRUCT and MIXTRAL-8X7B-INSTRUCT (Jiang et al., 2024); 2) LLAMA-3-70B-INSTRUCT and LLAMA-3-8B-INSTRUCT (MetaAI, 2024); 3) COMMAND R+ and COMMAND R (Gomez, 2024); 4) QWEN1.5-110B-CHAT and QWEN1.5-32B-CHAT (Bai et al., 2023). Answer generation prompt can be found in Appendix Table 11.\nPairwise evaluation. For LLM-based evaluators, we focus on a few larger models with strong context understanding capability, such as GPT-4-TURBO, GPT-4-0125-PREVIEW, GPT-40, MIXTRAL-8X22B-INSTRUCT and LLAMA-3-70B-INSTRUCT. Appendix Table 13-15 show the details of the pairwise evaluation prompts, including instruction, example prompt template, and in-context-learning examples. We shuffle the order of the answer pairs so that both human and model judges are not biased by the position of an answer. We select the LLM with the highest correlation with human judgments as the evaluator (Appendix Table 8). We use OpenAI API to run GPT-4 models. We download public models from HuggingFace Hub (HuggingFace, 2024) and run them on up to 8 Nvidia A100 GPUs with PyTorch (1.13.0) and Transformers (4.41.0) whose tokenizer.apply_chat_template() function can help adapt the generic prompts to different LLMs' input formats.\nWe follow OpenAI's recommendation\u00b3 to design prompts with chain-of-thoughts (CoT) (Wei et al., 2022), in-context learning (Dong et al., 2023), and HTML tags as delimiters. We remove the thinking process in model outputs as final answers."}, {"title": "6 Results and Analysis", "content": "Leveraging the evaluation framework described in Sec. 4, we first show that LFRQA's ground truth answers are dominantly preferred as answers than ROBUSTQA. Then, we use the same evaluation framework to establish a new leaderboard, RAG-QA ARENA, aiming to reliably measure RAG-QA systems' performances across diverse domains."}, {"title": "6.1 LFRQA v.s. RobustQA", "content": "In Sec. 3.3, we demonstrate LFRQA' advantages via data statistics. Here, we show a more rigorous study to highlight the benefits. We subsample 700 queries (100 from each of the 7 domains) and conduct pairwise preference comparisons using both human and model-based evaluations. We compare three types of answers: 1) ROBUSTQA: concatenation of its extractive answers, separated by \"\\n\"; 2) LFRQA: long-form answers in this work; 3) GPT-4's answers based on the top 5 retrieved passages. Table 4 shows that when compared directly in Row (1), LFRQA dominates ROBUSTQA. When comparing with GPT-4 in Row (2)-(3), LFRQA significantly out-performs GPT-4, but ROBUSTQA significantly under-performs. These results show strong evidence that LFRQA's answers can serve as better ground-truth than ROBUSTQA."}, {"title": "6.2 Quality of Model-based Evaluator", "content": "To build RAG-QA ARENA on the LFRQA test set, we need a scalable evaluation method to benchmark various LLMs. We rely on model-based evaluation to achieve this goal. Before showing the final dashboard results, we check the quality of our selected evaluator (GPT-4-0125-PREVIEW) in Table 4.\nTo alleviate model bias, we use three LLMs' answers as benchmark data, and the query set is the same 700 subsample above. Row (3)-(5) use GPT-4-0125-PREVIEW, MIXTRAL-8X22B-INSTRUCT and LLAMA-3-70B-INSTRUCT, respectively. All answers are generated based on the top 5 passages. We observe that LLM evaluators' numbers align well with the average human scores, except that"}, {"title": "6.3 RAG-QA Arena", "content": "Finally, we show RAG-QA ARENA's benchmark results. In Table 3 we report each model's win and win+tie rate against LFRQA.\nDashboard leaders. GPT-40 leads the dashboard, with GPT-4-TURBO and MIXTRAL-8X22B-INSTRUCT as close runner-ups. GPT-40 performs the best for [BI], [LI], [RE] and [WR] domains, MIXTRAL-8X22B-INSTRUCT leads in [FI] and [SC], and GPT-4-TURBO champions in [TE].\nImpact of \"no answer found.\" In RAG-QA, we rely on a passage retriever to provide context, which could be irrelevant. Our prompt (Appendix Table 11) asks an LLM to refrain from answering if it \"couldn't find an answer.\" When we use this answer generation prompt with CoT (the last two lines in the prompt), GPT-40 produces 48.3% \u201cI couldn't find an answer\" responses (Appendix Table 9). We randomly sample 20 such examples, and surprisingly found that in 16 cases, GPT-40 puts an answer in its <thinking> process, but continues to generate \"I couldn't find an answer.\" Fig. 8-9 show four such examples in comparison with other LLMs' answers with the same prompt, and GPT-40's new answers without CoT. As the answer generation prompt with CoT only fails for GPT-40, we remove CoT for GPT-40, which improves its answer format and reduces the \"no-answer\" ratio to the level similar to other competitive models.\nNote that these results raise a research question about the impact of prompt engineering. We emphasize that the goal of RAG-QA ARENA is to propose a reliable evaluation framework, not to conduct extensive prompt engineering or model training to pursue the best RAG-QA system. We provide a dev set of LFRQA, which can be leveraged in future research for model development purposes.\nElo rating. Table 3 shows dashboard results of win and win+tie ratio against LFRQA. We can further convert these pairwise comparisons into Elo ratings similar to Chatbot Arena (Chiang et al., 2024). Appendix Table 6 reports Elo ranking. Column (A) uses the same data in Table 3; Column (B) and (C) add pairwise comparisons for all unique model pairs on 700 and 1400 randomly sampled queries across seven domains. So the total added"}, {"title": "6.4 LLM as Annotators", "content": "Using large language models to provide annotations has been explored in previous works (Tan et al., 2024). It could provide a more scalable solution than human annotations but can suffer from hallucination and accuracy issues that require human validations (Huang et al., 2023). We also experiment with LLM as annotators before we start human annotations. We subsample 100 queries from LFRQA and prompt GPT-4-0125-PREVIEW to follow the similar procedure in Sec. 3.1 to combine answers (Appendix Table 12). Then we request our data linguists to compare LFRQA and GPT-4 annotations based on 1) Completeness: whether all ROBUSTQA answers are integrated into the final answers; 2) Citation Accuracy: whether citations in answers pointing to the right documents; 3) Helpfulness: defined the same as in Sec. 4. Table 5 shows LFRQA out-performs GPT-4 annotations by 15.5%, 23.4% and 12.9% for the three dimensions, respectively, suggesting human annotations are both valuable and necessary for our task."}, {"title": "7 Related Work", "content": "RAG-QA has been widely studied. Prior datasets are limited in the evaluation as their corpus relies heavily on Wikipedia and the answers are mostly short and extractive (Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Amouyal et al., 2019). ROBUSTQA and MULTIHOP-RAG (Tang and Yang, 2024) address the single domain issue, but still adopt short, extractive answers, which is not as suitable as LFRQA to evaluate modern LLMs that generate long-form answers.\nLongform QA datasets have been proposed in prior work. ELI5 (Fan et al., 2019) and LONGFACT (Wei et al., 2024) contain answers that are either not annotated directly on the corpus, and or not created by humans. Krishna et al. (2021) also points out that ELI5's small validation set has significant leakage from its train set. ASQA (Stelmakh et al., 2022) is the most similar data to our work, but its corpus is in the single Wikipedia domain. LFRQA is by far the only RAG-QA dataset with comprehensive long-form answers.\nPairwise preference is now a standard way to evaluate LLMs. It allows direct comparison between two responses (Chiang et al., 2024; Lin et al., 2024). RAG-QA ARENA is unique by always including a high-quality human annotated LFRQA answer, thereby making the evaluation more trustworthy."}, {"title": "8 Conclusion", "content": "We create LFRQA, the first multi-domain dataset with coherent long-form answers to reliably benchmark RAG-QA. We propose a reliable LLM-based evaluation framework, RAG-QA ARENA, that enables direct comparisons between LLMs' answers and LFRQA, which we believe will facilitate the evaluation RAG-QA in the era of LLMs."}, {"title": "Limitations", "content": "We discuss some limitations of this work for future research efforts. RAG-QA ARENA can potentially cover more models. We didn't include some leading LLMs, such as Claude (Anthropic, 2024) and Gemini (Google, 2023) models, due to legal and resource constraints, but we plan to add them to the leaderboard in the future. Evaluation using GPT-4-0125-PREVIEW is not cheap. It costs on average 300 U.S. dollars per model on the full LFRQA's test set. We plan to subsample 10-20% of the queries for the final public leaderboard, which will be more cost-friendly for future users. Future research can also study training smaller but equally accurate models as evaluators. Finally, we mainly focus on different LLMs for RAG-QA in this work, but future research can study the impact of different retrievers or joint retrievers and LLM training using RAG-QA ARENA."}, {"title": "Ethics Statement", "content": "The authors of this paper are committed to conducting research ethically. We are leveraging existing LLMs to generate answers for LFRQA, which include many open-ended questions. LLM-generated answers could be incorrect or unfaithful, as retrievers could find irrelevant passages and LLM can hallucinate (Huang et al., 2023). These are known issues in the AI research community, and that is the reason we created LFRQA to better evaluate RAG-QA systems. The additional risks and potential harms are discussed in numerous previous works (Bender et al., 2021; Weidinger et al., 2021).\nThe authors strive to ensure that the research and its results do not cause harm.\nData used in this work have been collected from public sources and used in accordance with all applicable laws and regulations. We use contracted data professionals for LFRQA annotations, and Appen platform for human pairwise preference annotations. In both cases, we ensure our hourly rate is higher than 15 U.S. dollars per local minimum wage standard. The intended usage of LFRQA is compatible with the underlying data's access conditions (Appendix A.3)"}]}