{"title": "Triple Point Masking", "authors": ["Jiaming Liu", "Linghe Kong", "Yue Wu", "Maoguo Gong", "Hao Li", "Qiguang Miao", "Wenping Ma", "Can Qin"], "abstract": "Existing 3D mask learning methods encounter performance bottlenecks under limited data, and our objective is to overcome this limitation. In this paper, we introduce a triple point masking scheme, named TPM, which serves as a scalable framework for pre-training of masked autoencoders to achieve multi-mask learning for 3D point clouds. Specifically, we augment the baselines with two additional mask choices (i.e., medium mask and low mask) as our core insight is that the recovery process of an object can manifest in diverse ways. Previous high-masking schemes focus on capturing the global representation but lack the fine-grained recovery capability, so that the generated pre-trained weights tend to play a limited role in the fine-tuning process. With the support of the proposed TPM, available methods can exhibit more flexible and accurate completion capabilities, enabling the potential autoencoder in the pre-training stage to consider multiple representations of a single 3D object. In addition, an SVM-guided weight selection module is proposed to fill the encoder parameters for downstream networks with the optimal weight during the fine-tuning stage, maximizing linear accuracy and facilitating the acquisition of intricate representations for new objects. Extensive experiments show that the four baselines equipped with the proposed TPM achieve comprehensive performance improvements on various downstream tasks. Code will be released soon.", "sections": [{"title": "Introduction", "content": "As a recent self-supervised learning scheme, masked autoencoder (MAE) has shown promising applications on various modalities. Given the considerable success of MAE in natural language processing (Devlin et al. 2018; Brown et al. 2020) and image analysis (He et al. 2022; Feichtenhofer et al. 2022), researchers are increasing their focus toward its application in 3D point clouds. The task holds particular significance due to the prevalence and authenticity of point clouds in the real world (Liu et al. 2024, 2023d,c; Wu et al. 2024). Simultaneously, the massiveness and complexity of the point clouds pose challenges without annotation.\nAutoencoder-based self-supervised methods (Yu et al. 2022; Pang et al. 2022; Liu, Cai, and Lee 2022; Zhang et al. 2022; Dong et al. 2023; Zhang et al. 2023; Liu et al. 2023b; Yan et al. 2023; Chen et al. 2024) for point clouds typically takes point patches as tokens and masks a high proportion of tokens (60%~90%) on the pre-training input data. We observe a consistent trend, where regardless of the variations in masking techniques, autoencoder designs and task heads, the masking ratio tends to be set at a high level. This aligns with the intuition for point cloud completion, suggesting that a higher masking ratio creates a more complex and meaningful pretext task. We may further hypothesize that completing objects with low masking ratios during pre-training yield more accurate but less generalizable effects since only a small part of the point cloud can be perceived, ultimately leading to suboptimal performance in downstream tasks. Based on these analyses, we pose a question: Can existing 3D pre-training architectures be designed with multiple masking tasks to balance the advantages of each so that richer 3D representations can be obtained?\nIn this paper, we present a scalable architecture known as the Triple Point Masking (TPM) designed for existing 3D pre-training frameworks, as illustrated in Figure 1. Specifically, we integrate two additional masking choices, i.e., medium and low masks, for the single input. The former is introduced to balance the potential confidence bias of the other two extreme masks, while the latter offers a simple and fine-grained pre-training task. This training process is incremental, necessitating the inclusion of two extra objective functions to jointly constrain point cloud completion in different scenarios. As the learning processes for triple masking share weights, they complement each other seamlessly and do not cause additional burden. Leveraging the diversity of existing single-mask methods, including point mask expansions (Pang et al. 2022; Liu, Cai, and Lee 2022), network architecture expansions (Zhang et al. 2022; Yan et al. 2023), and input modality expansions (Dong et al. 2023; Zhang et al. 2023), our TPM can be integrated into baselines to significantly promote self-supervised learning on point clouds.\nProcedurally, we first preserve the optimal weight models (w, w\u2081, w) for the triple masks through linear support vector machine (SVM) during pre-training. Note that w\u2080, w\u2081, and w\u2082 all represent weights of the same autoencoder network, representing specific forms generated by training at different epochs of w\u2080,\u2081,\u2082. Guided by the SVM weight selection, we choose to utilize w\u0303\u2080 with the best linear accuracy as the only pretrained model. On the one hand, it is consistent with past fine-tuning paradigms that follow the most meaningful weight w\u0303\u2080 is the outcome of the most challenging task (i.e., high mask) during pre-training. On the other hand, this is in line with the basic principle of self-supervised learning, where a well-designed learning paradigm should efficiently initialize the network weights for subsequent fine-tuning in order to avoid weak local minima and improve stability (Erhan et al. 2010). As weight w\u0303\u2080 serves not only as a primary contributor to mask completion but also learns recovery regularities from other mask situations.\nWith the above multi-mask guidance and weight selection, our TPM can optimize the convergence of existing methods and achieve significant improvement on various tasks. To showcase the generality of the proposed TPM, we integrate it to existing baselines, including the foundational Point-MAE (Pang et al. 2022), the network-enhanced Point-M2AE (Zhang et al. 2022), the mask-enhanced PointGPTS and the data-enhanced PointGPT-B (Chen et al. 2024). Without any bells and whistles, the TPM-equipped self-supervised methods exhibit the capability to learn more robust 3D representations under original conditions. In terms of PointMAE, TPM results in gains of 1.1% and 1.4% in the pre-training and fine-tuning phases, respectively.\nIn brief, our contributions are summarized as follows:\n\u2022 A scalable TPM module is proposed that utilizes existing 3D pre-training frameworks to learn in-depth 3D representations by triple mask completions.\n\u2022 A weight selection strategy is designed to create more meaningful conditions and the mechanism by which the weight work is revealed for fine-tuning.\n\u2022 A series of experiments demonstrate the significance of TPM, which remains simple yet efficient no matter how complicated the original methods are."}, {"title": "Related Work", "content": "Masked autoencoder (MAE) can generally be divided into two steps: 1) the encoder takes randomly masked elements as input and is responsible for extracting its high-level latent representation; 2) the lightweight decoder explores clues from the encoded visible features, and reconstruct the original masked elements. Since this process only occurs in the input itself and cannot directly act on the actual function, it exists in a pre-training manner and uses the network model generated to act on other tasks. The GPTs (Radford et al. 2018, 2019; Brown et al. 2020) and MAEs (He et al. 2022; Chen et al. 2023; Liu et al. 2023a; Wu et al. 2023b) series transform this paradigm It is applied to language and image modeling and achieves significant performance improvements on downstream tasks through fine-tuning. GPT (Radford et al. 2018) adopts a unidirectional transformer architecture to fine-tune the model by updating all pre-trained parameters to implement an autoregressive prediction method. MAE (He et al. 2022) randomly masks input patches and pre-trains the model to recover the masked patches in pixel space. In the field of 3D point clouds, Point-MAE (Pang et al. 2022) extends MAE by randomly masking point patches and reconstructing the masked regions."}, {"title": "Self-supervised Learning for Point Clouds", "content": "With the recent emergence of zero-shot and few-shot techniques associated to data (Zhao, Chua, and Lee 2021; Cheraghian et al. 2022; Lu et al. 2023; Liu et al. 2023d), self-supervised and weakly-supervised techniques related to annotations have also attracted attention. The disordered and discrete nature of 3D point clouds poses unique challenges for representation learning, so designing self-supervised solutions for point clouds is a meaningful endeavor. Different from previous mainstream constrastive learning methods (Xie et al. 2020; Hou et al. 2021; Afham et al. 2022; Wu et al. 2023a), recent mask learning has generated multiple solutions for 3D MAEs via autoencoder structures. Point-BERT (Yu et al. 2022) and Point-MAE (Pang et al. 2022) implement BERT-style (Devlin et al. 2018) and MAE-style (He et al. 2022) point cloud pre-training schemes, respectively. MaskPoint (Liu, Cai, and Lee 2022) represents a point cloud as discrete occupancy values and performs a simple binary classification between masked and noisy points as an agent task. ACT (Dong et al. 2023) employs a cross-modal autoencoder as a teacher model to acquire knowledge from other modalities. Point-M2AE (Zhang et al. 2022) proposes a hierarchical transformer structure and a multiscale masking strategy based on Point-MAE. I2P-MAE (Zhang et al. 2023) learns excellent 3D representations from 2D pre-trained models through an image-to-point masked autoencoder. IAE (Yan et al. 2023) adopts an implicit decoder to replace the commonly used auto encoder for better learning of point cloud representations. TAP (Wang et al. 2023) proposes a point cloud-to-image generative pre-training method that generates view images with different indicated poses as a pre-training scheme through a cross-attention mechanism. PointGPS (Chen et al. 2024) proposes a point cloud autoregressive generation task to pre-train the transformer model. Unlike previous MAE methods that use a standard single mask, we propose a triple mask structure and a weight selection module to re-upgrade the pre-training and fine-tuning phases of the self-supervised learning to better learn rich and robust representations for the 3D point clouds."}, {"title": "Proposed Method", "content": "Our goal is to design a simple, effective and versatile component that further facilitates the representation learning from existing methods. Sequentially, we propose triple point masking and SVM-guided weight selection in Sections 3.1 and 3.2, respectively. Eventually, Section 3.3 provides implementation details to successfully deploy the proposed TPM on current baselines. Specific problem background and statements are found in the supplementary materials."}, {"title": "Triple Point Masking", "content": "We begin by concluding that self-supervised learning of point clouds is still affected by data sources, such as problems of unbalanced data densities, unstable sampling transformations, and limited supervised signals. Indeed, there may be an infinite number of representations of the same point cloud and an infinite number of ways in which it can be reconstructed, so that scrutinizing the problem from a geometric space infers that the point cloud reconstruction always suffers from a unique defect (Yan et al. 2023). Such defects are forced to be learned by the encoder, being subject to an overall distance metric, another point cloud generated by the decoder is forced to be identical to the input sample.\nIn addition, most existing approaches face the dilemma that the autoencoder may encounter complex completion tasks under only high masking. With sufficient masked points, small point patch areas tend to learn discrimination capabilities and recovery patterns from large ones, which has the risk of being monotonous and non-generalizable, as illustrated in the upper middle part of Figure 2. An intuitive solution idea therefore is to alleviate or decentralize the above limitations.\nInstead of indirectly turning explicit points into implicit representations (Yan et al. 2023) or adding additional representations to them (Zhang et al. 2023), we directly delve into the mechanism of masking and propose a multi-mask solution called triple point masking (TPM). The proposed TPM not only alleviates the singularity and complexity of the previous original pre-training tasks and adds multiple learning paradigms to prevent ambiguity. In other words, our TPM enables the original pre-training network to learn triple adaptive representations of the point cloud under multiple different constraints and build a reliable and comprehensive reconstruction pattern for the input point cloud.\nSpecifically, we impose two new masks m\u2081 and m\u2082 (U\u2098\u2080 > U\u2098\u2081 > U\u2098\u2082) on base of the original m\u2080, see Table 5 for more constructions. Generally U\u2098\u2080 > 0.5 is the only setting in the baselines, and the two new masks provided enable the network to mine more fine-grained information while maintaining training stability and convergence. As a result, TPM can be mathematically expressed as\n$\\begin{aligned}\n&\\text{TPM}:\\left\\{\n\\begin{array}{l}\nf_{m_{i}}: \\mathbb{R}^{n \\times 3} \\rightarrow \\mathbb{R}^{C},\ng_{m_{i}}: \\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{\\tilde{n} \\times 3}\n\\end{array}\n\\quad i=0,1,2 .\n\\right.\\\n\\end{aligned}$\nEven though two autoencoders are expanded, they are still supervised by the same complete input point cloud and therefore produce their own optimal distances from different reconstructed point clouds,\n$\\Theta, \\Phi=\\arg \\min _{\\Theta_{i}, \\Phi_{i}} d\\left(P_{g_{0} \\circ f_{\\Theta_{i}} \\circ m_{i}}, P\\right).$"}, {"title": "SVM-Guided Weight Selection", "content": "Since TPM is subject to the joint action of masks {m\u2080, m\u2081, m\u2082}, weights {w\u2070\u1d62, w\u00b9\u1d62, w\u00b2\u1d62}(1 \u2264 ei \u2264 E) are generated during pre-training, where E represents the number of epochs. Theoretically the complete weight selection process for TPM is\n$\\left\\{m_{0}, m_{1}, m_{2}\\right\\} \\stackrel{q^{4}}{\\longrightarrow} \\left\\{w_{0}^{9}, w_{1}^{4}, w_{2}^{9}\\right\\} \\stackrel{f_{m_{0,1,2}}^{\\theta}}{\\longrightarrow} \\left\\{w_{0}, w_{1}, w_{2}\\right\\}.$\nThe difference is that to ensure the initial conditions of the fine-tuning network, we select the appropriate pre-trained weights only among the {w\u2070\u1d62} generated in the toughest mask case. According to previous work, determining the optimal weights by loss value is a straightforward strategy. However, this does not meet the needs of our design, as our losses are generated by triple mask tasks and there are differences in loss weights across tasks, making the single-masked loss an insufficient measurement of the weighting model.\nAs a common and effective solution, we directly evaluate the weights {w\u2070\u1d62} through the guidance of the linear SVM, and select w\u0303\u2080 with the maximum linear classification accuracy, which is implemented under another data source,\nw\u0303\u2080 = arg max SVM(D\u1d65\u2090\u2097|D\u209c\u1d63\u2090\u1d62\u2099),\nwhere D\u209c\u1d63\u2090\u1d62\u2099 and D\u1d65\u2090\u2097 are the data partitioned for SVM, which is chosen for ModelNet40 (Wu et al. 2015) dataset.\nSince the linear SVM can solve maximum margin hyperplanes in linearly differentiable problems, it can be transformed into an equivalent quadratic convex optimization process. Furthermore, the weights {w\u2070\u1d62} can be quantized to discriminate the high-dimensional feature space of the point cloud under the guidance of SVM."}, {"title": "Implementation Details", "content": "Our TPM is implemented based on existing self-supervised learning methods for point clouds, including Point-MAE (Pang et al. 2022), Point-M2AE (Zhang et al. 2022), PointGPT-S (Chen et al. 2024), and PointGPT-B (Chen et al. 2024). For a fair comparison, we do not modify any parameters of baseline methods except the number of masking tasks. The experimental settings are shown in Table 1.\nThe four baselines are introduced below, and more details can be obtained from the original literatures.\nPoint-MAE. A foundational self-supervised mask learning approach on point clouds that determines the theory and applicability of masks, patches, and autoencoder networks.\nPoint-M2AE. A network-enhanced self-supervised approach that modifies the autoencoder into a pyramid architecture, progressively modeling spatial geometry to achieve hierarchical learning.\nPointGPT-S. Similar with Point-MAE, unordered point clouds are arranged into ordered sequences, and a mask-enhanced dual masking strategy is used.\nPointGPT-B. Similar with PointGPT-S, except that 1) the pre-training dataset changes from ShapeNet (Chang et al. 2015) (~50k point clouds) to an unlabeled hybrid dataset (UHD), introducing 6 additional datasets (Song, Lichtenberg, and Xiao 2015; Mo et al. 2019; Uy et al. 2019; Hackel et al. 2017; Wu et al. 2015; Armeni et al. 2016) with a total of ~300k point clouds and 2) the feature dimension double. We can find that these baselines focus on data, mask, and network. Therefore, it is straightforward to show the strong applicability of our TPM. Indeed, the values of the two mask ratios added are U\u2098\u2081 = 0.5 and U\u2098\u2082 = 1 U\u2098\u2080, where m1 is to balance the potential confidence bias of the other two masks and m2 releases fine-grained completion signals."}, {"title": "Experiments", "content": "In this section, we first demonstrate the effectiveness of TPM in improving four baselines during pre-training. We then fine-tune and evaluate the SVM-guided pre-trained model by subjecting it to various downstream tasks. Finally, adequate ablation studies and analysis are performed to analyze the explainable characteristics and fundamentals behind our proposed TPM."}, {"title": "Pre-training with TPM", "content": "We pre-train Point-MAE and Point-M2AE with TPM on the ShapeNet (Chang et al. 2015) dataset, which contains 57,448 object point clouds from 55 common categories. Additionally, the proposed TPM is compared with self-supervised methods based on spatial reconstruction (Sauder and Sievers 2019; Wang et al. 2021), related data augmentation and transformation (Han et al. 2019; Qian et al. 2021), and contrastive learning (Afham et al. 2022; Wu et al. 2023a).\nLinear SVM. To evaluate the representational capabilities of the point cloud models generated during pre-training, we directly extract linear SVM features for the methods with our TPM on both synthetic ModelNet40 (Wu et al. 2015) and real-world PB_T50_RS (Uy et al. 2019) datasets. As shown in Table 2, for both classical Point-MAE and Point-M2AE, TPM can enhance their discriminative capabilities, improving the accuracy by +0.4%/+0.3% and +1.1%/+0.9%, respectively. Experimental results show that TPM allows point clouds to discover more potential information during pre-training by modifying the number of masks only, both on canonical synthetic and complex real-world datasets."}, {"title": "Fine-tuning with TPM", "content": "After pre-training, we discard all parameters in the w\u2081 and W\u2082 models as well as the decoder in w\u0303\u2080 and attach different network heads to the encoder in w\u0303\u2080. The new lightweight networks are fine-tuned to implement multiple downstream tasks at both the object level and the point level.\nObject Classification. We test the overall classification accuracy of the proposed method on both synthetic and real-world datasets. The selected pre-trained model is transferred to ScanObjectNN (Uy et al. 2019) which contains about 15,000 objects (15 categories) extracted from real indoor scans, and ModelNet40 (Wu et al. 2015) which includes 12,311 clean 3D CAD objects (40 categories). For ScanObjectNN, we report three different experiments: OBJ_BG, OBJ_ONLY, and PB_T50_RS. For ModelNet40, to have a fair comparison, we use a standard voting strategy (Liu et al. 2019) for the tests, where the input point cloud contains only coordinate information.\nThe results in Table 3 show that our TPM can bring an average +0.3% improvement up to a maximum of 1.4% in four baselines although it changes the original training paradigm and causes a few fluctuations. No additional parameter or component design is required to enable existing methods to achieve superior performance. Note that the improvement of TPM is more pronounced on ScanObjectNN than on ModelNet40. This phenomenon is in line with our expectation that the multi-mask task is designed to be useful for adapting to complex and comprehensive internal supervision, and is also directly reflected in the complexity of the data sources.\nPart Segmentation. We evaluate the impact of TPM for part segmentation on the ShapeNetPart (Yi et al. 2016), which consists of 16,881 objects from 16 categories. For a fair comparison, we use the same segmentation head as in the baselines. Specifically, the input point cloud is sampled as 2048 points, and three hierarchical features at layers 4, 8 and 12 of the transformer blocks are extracted and concatenated. Subsequently, two features are obtained by maximum pooling and average pooling, concatenated and then up-sampling is executed to generate features for each point and MLP is applied for semantic prediction. The segmentation results in Table 4 demonstrate that TPM provides significant positive enhancement for the part segmentation task that require fine-grained representations."}, {"title": "Ablation Study for TPM", "content": "Since our core contribution is TPM, we conduct ablation studies on pre-trained mask and fine-tuned weight with \"Point-MAE+TPM\". We evaluate the impact of these designs by reporting the classification accuracy achieved on the ModelNet40 (1k) and PB_T50_RS. More results and analyses are in the supplementary material."}, {"title": "Mask construction", "content": "We notice that when only a small mask is added, a certain spatial awareness is also produced in the pre-training process, and it is even possible to surpass triple point masking with a well-designed bipartite point mask. However, we eventually reveal that the triple point masking plays a stable completion role, and U\u2098\u2080, U\u2098\u2081, U\u2098\u2082 = [0.6, 0.5, 0.4] derived from V\u2098\u2080 = 0.6 is the most suitable configuration, the results are shown in Table 5. If the number of masks increases again, the completion task in pre-training is overloaded, making it difficult to parse the completion of different masks."}, {"title": "Weight selection", "content": "Since we adopt the triple point masking strategy, and the linear SVM needs to evaluate the different performances of the same model facing different situations. That is, w\u2080, w\u2081,w\u2082} acts on each mask from each epoch in the pre-training process. Through theoretical analysis and experimental results, our choice is w\u0303\u2080 \u2192 m\u2080 due to taking into account two factors: 1) pre-training sets a more difficult pretext task to better serve downstream tasks, and 2) SVM's guidance is to measure the distinguishability of the completed point cloud. Therefore, we observe in Table 6 that the fourth weight selection ({w\u2080, w\u2081, w\u2082} \u2192 m\u2080m\u2081 m\u2082) is often the same as the easiest task (w\u0303\u2082 m\u2082) and can easily achieve high linear classification effects. In contrast, this selection cannot be adapted to fine-tune on downstream tasks."}, {"title": "Interpretable Analysis for TPM", "content": "We illustrate the facilitating role of TPM during pre-training by comparing the completed instances with and without it, as shown in Figure 3. It can be found that the baselines with TPM have better robustness and realism for completed parts. More importantly, we next further analyze the mechanism and specific performance of loss, mask and weight in depth.\nLoss analysis. Due to the triple masks provided, there are triple completions during pre-training. We set the loss weights \u03bb\u2098\u1d62 = mi/sum({mi}) that are proportional to the mask values for the point patches to be completed. We argue that the mask-based loss weights can enhance TPM to generate discriminative attention. Although the effect is only slightly improved compared to setting equal weights, this provides a more reliable rule for potential the mask setting.\nMask analysis. We show the results under [0.6, 0.5, 0.4] and [0.8, 0.5, 0.2] mask constructions in Figure 4, including SVM classification during pre-training and MLP prediction during fine-tuning. We argue that in the Point-MAE setting, the simple pretext task with a low mask (i.e., U\u2098\u2080 = 0.2) does not result in an effective gain for the downstream task. This suggests that the derived TPM based on baseline masks is suitable for the existing baselines and that weight selection needs to consider both SVM and task difficulty.\nWeight analysis. To further illustrate the effective role of triple masks, we explain this phenomenon by analyzing the optimal weights. Specifically, we maintain the optimal weights of triple masks by TPM and perform fine-tuning experiments at ModelNet40 (MN40) and ScanObjectNN (SONN). As reflected in Figure 5, even though a particular model of TPM has difficulty in distinguishing features with similar semantic labels under the mask at that time, this distinction may be \"meetable\" in models under other masks. Thus, this fine-grained semantic discrimination enhances the learning capability of triple masks."}, {"title": "Limitation", "content": "TPM is undoubtedly a straightforward yet effective technique for self-supervised learning on point clouds. Nevertheless, it is uncertain to find a universal mask construction due to the different masking and completion ways. In theory, there are variable combinations of masks and networks. Moreover, we particularly show that abundant data is benificial to promote self-supervised learning (Chen et al. 2024), and our TPM can amplify this advantage."}, {"title": "Conclusion", "content": "In this paper, we propose TPM, an effective and scalable multi-masking scheme that addresses the domain gap between generative and downstream tasks for 3D self-supervised learning. Diverging from conventional 3D mask modeling methods, TPM systematically enriches the shape perception of 3D objects through well-designed triple point masking. The SVM-guided weight selection strategy in the pre-trained models augments its discriminative reliability on downstream tasks. Results suggest that our TPM yields noteworthy improvements over unimodal self-supervised methods without the need for cross-modal information."}]}