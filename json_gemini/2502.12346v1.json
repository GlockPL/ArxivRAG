{"title": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models", "authors": ["Jiajun Zhou", "Yifan Yang", "Kai Zhen", "Ziyue Liu", "Yequan Zhao", "Ershad Banijamali", "Athanasios Mouchtaris", "Ngai Wong", "Zheng Zhang"], "abstract": "Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantization often degrades model performance, thus fine-tuning is required for various down-stream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimization require backpropagation, which are error-prone in the low-precision settings. To overcome these limitations, we propose the Quantized Zeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid the error-prone low-precision straight-through estimator, and utilizes optimized stochastic rounding to mitigate the increased bias. QuZO simplifies the training process, while achieving results comparable to first-order methods in FP8 and superior accuracy in INT8 and INT4 training. Experiments demonstrate that low-bit training QuZO achieves performance comparable to MeZO optimization on GLUE, Multi-Choice, and Generation tasks, while reducing memory cost by 2.94x in LLaMA2-7B fine-tuning compared to quantized first-order methods.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have achieved state-of-the-art performance in natural language processing, impacting various science and engineering fields. However, deploying and fine-tuning LLMs consumes significant hardware resources because of their huge model size. To address this issue, extensive research has focused on LLM quantization. Notable approaches include post-training quantization, quantization-aware training, and fully quantized training. Post-training quantization can effectively reduce the latency and memory costs of inference, but often leads to a significant accuracy drop in low-precision formats, although various techniques can partially mitigate this issue. Quantization-aware training offers better accuracy, but is more expensive due to the use of high-precision computational graphs. Truly quantized training methods employ low-precision gradients, activation, and weights to reduce hardware costs. However, implementing truly quantized training requires advanced hardware and software support for both forward and backward passes. Meanwhile, the straight-through estimator, which is commonly used for quantized gradient estimations, often causes unstable and inaccurate results in low-bit training.\nIn practice, LLM users may afford only a low-cost LLM inference engine (e.g., an edge FPGA or embedded system) with limited precision (e.g., INT8 or INT4). This paper asks the following question: Can we leverage inference-only quantized hardware to fine-tune low-bit LLMs while achieving good performance? This seems challenging because (1) inference-only hardware lacks sufficient memory bandwidth and storage to retain intermediate activations required for backpropagation, and (2) the Straight-Through Estimator (STE) introduces increasing gradient approximation errors in lower-bit formats. The recent MeZO enables memory-efficient zeroth-order (ZO) fine-tuning for LLMs, but suffers from an avoidable performance drop compared to first-order (FO) methods due to the bias and variance of ZO gradient estimation. In this paper, we show that a quantized zeroth-order optimizer (QuZO) can achieve better accuracy than its first-order counterparts in a low-precision setting."}, {"title": "Related Work", "content": "Zero-order method. Zero-order (ZO) optimization techniques use forward passes for gradient estimation. Since backpropagation is not required during training, ZO methods reduce memory consumption significantly compared to a FO method. MeZO employed a memory-efficient ZO stochastic gradient descent (ZO-SGD) algorithm to efficiently fine-tune LLMs exceeding 60 billion parameters, leveraging parameter-efficient approaches like LoRA. Other ZO methods include ZO-SGD and ZO-Sign-SGD using sign-based gradient estimation, the ZO-Adam optimizer exploiting momentum information, and parameter-efficient methods like AdaZeta. Sparse MeZO employs a sparse perturbation for LLM fine-tuning. FP16 ZO training performs well but still faces memory bottlenecks. Recent ZO quantization introduces fixed-point 16-bit but fails at 8-bit. However, we overcome the challenges of lower-precision quantization and enable accurate fine-tuning of LLMs below 8-bit quantization.\nQuantization of LLMs. Various quantization methods have been developed to reduce the memory and computing cost of LLMs. LLM.int8() reduces the precision of model weights while keeping outliers in FP16. SmoothQuant introduces a fine-grained quantization method that supports INT8 operations exclusively. QLLM addresses the outlier problem via employing an adaptive channel reassembly technique. LLM-QAT employs quantization-aware training with a data-free strategy to achieve 4-bit quantization. Furthermore, the 4-bit training and QLORA methods leverage a Hadamard Transform and a novel NF4 datatype, respectively, to accelerate training while preserving performance. While prior quantized training methods rely on backpropagation for gradient updates, our QuZO method eliminates the error-prune STE-based back propagation and used low-bit inference for truly quantized fine-tuning."}, {"title": "The QuZO Fine-Tuning Method", "content": "We start with a high-level introduction to our QuZO framework. Given a quantized LLM inference model, QuZO uses a low-bit ZO optimizer to update quantized model parameters directly during training. We assume that the forward pass $X_l = F(x_{l-1},\\hat{w}_l)$ computes the output of the l-th layer using the quantized weight matrix w\u2081 and the previous-layer feature x\u2081\u22121, With just a few forward passes, our QuZO framework uses quantized RGE (see Section 3.2)"}, {"title": "Challenges of Quantized ZO Training", "content": "Standard ZO-SGD uses a randomized gradient estimator (RGE) to approximate a full-precision gradient. Specifically, given full-precision model parameters w \u2208 Rd, a loss function L(w, B) and a minibatch of dataset B, RGE computes the gradient as:\n$\\nabla \\widehat{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{L_B(w + \\epsilon u_i) - L_B(w - \\epsilon u_i)}{2 \\epsilon} u_i = \\frac{1}{n} \\sum_{i=1}^{n} u_i u_i^T \\nabla L_B(w),$\nwhere e is a scaling factor, {u\u1d62}\u1d62\u208c\u2081\u207f are i.i.d. samples drawn from certain distributions with a unit variance (e.g., a standard Gaussian distribution). While \u2207\u0139(w) differs from the true gradient \u2207LB(W), its expectation serves as a good gradient estimator because\n$E [\\nabla \\widehat{L}(w)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} E (u_i u_i^T) \\nabla L_B(w) = \\nabla L_B(w).$\nThis statistical property ensures the asymptotical convergence of ZO-SGD. Assuming the quantized model parameters w are available and only low-precision hardware is used for inference, the full-precision random perturbation u\u1d62 cannot be directly applied to w due to hardware limitations. To address this, u\u1d62 is replaced with its quantized counterpart \u00fb\u1d62 = Q(u\u1d62), leading to a low-precision RGE:\n$\\nabla \\widetilde{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{L_B (\\widehat{w} + \\epsilon \\widehat{u}_i) - L_B (\\widehat{w} - \\epsilon \\widehat{u}_i)}{2 \\epsilon} \\widehat{u}_i \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\widehat{u}_i \\widehat{u}_i^T \\nabla L_B(\\widehat{w}).$\nTaking the exception values on both sides, we have\n$E [\\nabla \\widetilde{L}(w)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} E (\\widehat{u}_i \\widehat{u}_i^T) \\nabla L_B(\\widehat{w}) \\neq \\nabla L_B(\\widehat{w})$\nSince the quantized perturbation \u00fb\u1d62 = Q(u\u1d62) no longer maintains a unit variance, the above naive quantized RGE introduces bias during fine-tuning and may lead to divergence in training."}, {"title": "Proposed Quantized RGE", "content": "We propose a new quantized RGE scheme to address the challenge in the previous subsection.\nStochastic Quantization of u\u1d62. We first define a quantization operation of Q(uz) based on stochastic rounding:\n$Q(u_i) = clamp(SQ, L_{min}, L_{max}) + z_0, SQ = ([suu_i] + Ber(suu_i - [Suu_i]),$\nThe stochastic quantization formula Q(u) converts the perturbation u\u1d62 into a low-bit representation by scaling it with a factor su as suui, performing a downward rounding operation [suui], and applying stochastic up-rounding using a Bernoulli random variable Ber(suui \u2013 [suui]). The resulting quantized value is clamped to the representable range [Lmin, Lmax] and shifted by the zero point 20. This stochastic rounding ensures that\n$E_Q [Q(u)] = E [u_i] .$"}, {"title": "Implementation of QuZO", "content": "Now we present the details of the implementation of the QuZO framework."}, {"title": "QuZO for LoRA", "content": "We can extend the QuZO framework by incorporating low-rank adaptation to allow low-precision parameter-efficient fine-tuning. Our approach uses the model quantization strategies of QLoRA and LLM.int8() without modifying the quantized model. QuZO significantly reduces memory overhead by eliminating the storage of FO optimizer states and updating only the low-rank trainable matrices A \u2208 Rdxr and B\u2208 Rrxd using forward passes. In QuZO fine-tuning, the model parameters are quantized and frozen at low precision (e.g. 4 or 8 bits), and we update solely on the low-rank matrices A and B. The trainable low-rank matrices are quantized (denoted as Q[A] and Q[B]) in order to match the precision of the LLM. By doing so QuZO training can significantly further reduce the memory cost compared to traditional LORA for 4/8-bit LLM fine-tuning."}, {"title": "QuZO Analysis", "content": "In this subsection, we analyze the quality of gradient estimation in QuZO and its impact to training.\nQuZO Gradient Quality. We use a simple encoder-block transformer to analyze the asymptotic behavior of two quantized ZO gradient estimators. Q-RGE1 refers to the quantized estimate in Eq. (3), and Q-RGE2 denotes our proposed estimation in Eq. (8). Although we need only a few inquiries to compute actual ZO gradients, the statistical behavior of a gradient (rather than the value of the individual gradient) decides the training performance. To verify statistical asymptotic behavior, we set n = 1000 to perform a Monte Carlo computation to get empirical mean values of Q-RGE1 and Q-RGE2, and then compare them with a full-precision ZO gradient via the 12 error. As shown in Fig. 3 (a), the expected values of both quantized estimators have larger errors as the precision reduces from INT8 to INT3. However, our method (Q-RGE2) is much more resilient to quantization errors and has a more accurate expected value, since our quantized ZO gradient estimator can avoid the additional bias caused by quantization.\nTraining Behavior. Figure 3 (b) further shows the training behavior of quantized ZO optimization using these two gradient estimators when fine-tuning the OPT-1.3B model. Experiments are performed on the DROP dataset under 8-bit and 4-bit settings. We observe that our QuZO with Q-RGE2 shows slightly better convergence compared to quantized training using Q-RGE1 in the 8-bit setting. In 4-bit training, our method demonstrates a stable and significantly better training behavior: it achieves a loss similar to 8-bit training, while INT 4 Q-RGE1 causes convergence failures. The above analysis clearly demonstrates the better numerical performance of our QuZO in low-bit LLM fine-tuning."}, {"title": "Experiments", "content": "In this section, we evaluate the proposed QuZO method on several language models (LMs) with 4-8 bit precision. QuZO demonstrates performance comparable to or better than standard first-order (FO) truly quantized training across various model sizes and tasks, with significantly lower memory usage. We also explore fine-tuning quantized models by combining QLoRA with QuZO. For hardware costs, QuZO employs a forward-only framework with hardware requirements similar to post-training quantization. In Section 4.3, we compare the memory consumption between truly quantized FO training and QuZO. Furthermore, we employ both medium-size models and large decoder-based LMs [e.g. OPT 1.3B and LLaMa-2 7B in few-shot settings. All experiments were carried out on NVIDIA A100-40GB GPUs. The details of the experimental setup are provided in Appendix A.\nFull-Parameter Quantized Fine Tuning\nWe first summarize our experiments on full-parameter fine-tuning for medium- and large-scale models. These results demonstrate that QuZO provides a practical approach for accurate fine-tuning of quantized LLMs directly on low-precision hardware, maintaining. For medium-scale models like RoBERTa-Large, QuZO surpasses truly quantized FO fine-tuning in most tasks in the 4-bit precision. For large-scale models such as LLaMA-2, QuZO achieves performance comparable to or better than truly quantized FO fine-tuning, particularly under ultra-low bit configurations. These findings highlight the ability of QuZO to enable low-cost hardware training without compromising performance.\nPerformance on the RoBERTa-Large model.\nWe evaluate the performance of various methods in the SST-2, SNLI, SST-5, RTE, and MNLI datasets and on the RoBERTa-Large model. The results in Fig. 4 leads to the following observations:\nAs expected, all training methods experience accuracy decline as quantization precision decreases. This occurs because the model expressive power declines and the optimization becomes more challenging in lower precision.\nThe performance of truly quantized FO fine-tuning drops most significantly due to the increasing errors in the straight-through estimators as precision decreases.\nQuantization-aware training (QAT) can mitigate partially the accuracy drop of truly quantized FO training. As a faked quantized training, QAT still needs backpropagation and updates LLM model parameters in full precision. Therefore, it remains memory-intensive and less suitable for resource-constrained low-precision hardware.\nIn contrast, the performance of QuZO is most resilient to the decreased precision, and it works the best in a very low-precision (e.g., INT4). This is because (1) QuZO can bypass the error-prone straight-through estimator that is used in truly quantized FO training, and (2) the quantized RGE in Eqn.(8) can eliminate the bias caused by quantized perturbations.\nPerformance of QuZO on LLaMA Models. We further apply QuZO to fine-tune the LLaMa-2 model, evaluating it on SuperGLUE and generation tasks. Table 1 shows that QuZO outperforms its truly quantized FO counterparts on all multichoice and generation tasks under FP W8A8 quantization (i.e. FP8 for both weights and activations). Under the INT W8A8 quantization, QuZO outperforms SmoothQuant, LLM.int8(), and truly quantized FO methods in 4 out of 8 tasks. For 4-bit quantized FO training, uniform quantization yields the worst accuracy, but advanced methods such as LLM-FP4 improve performance. LLM-FP4 and its baseline MinMax use FP W4A8 quantization and achieve a slight improvement in accuracy, particularly for multichoice tasks. Our QuZO method maintains strong performance under W4A8 quantization with mixed-datatype (see Appendix B), achieving the best results in 4 out of 8 tasks. SmoothQuant, LLM.int8(), MinMax, and LLM-FP4 have introduced efficient quantization methods that enhance performance. However, they are memory-intensive as they require fine-tuning using a FO optimizer."}, {"title": "Parameter-Efficient Fine-Tuning", "content": "Parameter-efficient fine-tuning methods like QLORA reduce memory usage with 4-bit precision compared to standard training but still rely on AdamW , which requires backpropagation. QuZO improves inference efficiency and memory savings, achieving a 5.47\u00d7 reduction in maximum memory cost compared to QLoRA in fine-tuning the 4-bit OPT-1.3B model Our QuZO framework applies the LoRA (rank set as 8), allowing fine-tuning with far fewer trainable parameters than full-model tuning, significantly reducing memory consumption, and accelerating convergence. Table 2 highlights the performance of QuZO with low-bit perturbation and gradient configurations for different tasks and models. For the OPT-1.3B model, QuZO utilizes INT8 RGE gradients with INT4 perturbations. Despite the introduction of low-bit gradients, QuZO achieves competitive or superior performance compared to full-precision MeZO with LoRA in most tasks and demonstrates strong robustness in 4-bit fine-tuning, while truly quantized FO shows poor accuracy in 4-bit training. Furthermore, QuZO reduces 2 - 5.47\u00d7 memory consumption compared to fully quantized FO methods in Table 15(see Appendix C). For the LLaMa2-7B model, QuZO achieves performance comparable to full-precision MeZO while allowing fine-tuning across all five"}, {"title": "Memory Efficiency", "content": "We follow Table 5 of to provide the theoretical analysis for different optimizers. As shown in Table 4, our QuZO demonstrates significant memory reduction compared to truly quantized FO fine-tuning at the same precision.\nWe further compare the empirical memory costs of full fine-tuning the LLaMA-2 7B model in Table 5. Specifically, in the MultiRC task, QuZO (8-bit) reduces memory usage by 1.43\u00d7, while QuZO (4-bit) achieves a 1.39\u00d7 reduction compared to their truly quantized FO counterparts with the same precision. Similarly, in the SQUAD task, QuZO (8-bit) reduces memory consumption by 2.94\u00d7, and QuZO (4-bit) achieves a 2.89\u00d7 reduction relative to FO-SGD at the same precision.\nTo verify hardware efficiency, we profile the memory usage of our QuZO method with INT8 CUDA kernels, comparing it to the peak memory consumption of INT8 and FP16 tensor-core GEMM implementations in full parameter tuning. In practice, QuZO achieves up to a 7.8\u00d7 memory reduction with an INT8 model compared to the first-order FP16 trainning, as shown in Fig 5."}, {"title": "Conclusion", "content": "This work has proposed a Quantized Zeroth-Order (QuZO) method for truly qantized training of LLMs without using back propagation. We have identified the challenge of quantized ZO training, and proposed an new quantized ZO gradient to mitigate the bias in low-precision settings. Since QuZO can avoid the error-prone straight-trough estimator, it can achieve better performance than first-order truly quantized training in low-bit settings. The superior performance of QuZO in low-bit (e.g., INT8 and INT4) training has been shown by a variety of fine-tuning experiments on the OPT-1.3B and LLaMA2-7B models. Our QuZO method is intrinsically hardware efficient for fine-tuning LLMs on low-bit resource-constrained hardware."}, {"title": "Limitations", "content": "The presented QuZO method can significantly impact practical LLM deployment. We have not yet implemented the real quantized training framework using low-precision kernels during training, as this requires much engineering effort. For instance, adding a minimal hardware block to an LLM inference accelerator can enable resource-efficient fine-tuning, making on-device learning of LLMs accessible and affordable for many downstream users. Additionally, QuZO can greatly reduce the latency and energy cost of fine-tuning due to its capability to directly use an ultra low-bit LLM inference accelerator. This will enable the deployment of LLMs in many resource-constrained scenarios, such as autonomous systems and robots."}]}