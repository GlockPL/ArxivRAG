{"title": "Scalable Machine Learning Training Infrastructure for Online Ads Recommendation and Auction Scoring Modeling at Google", "authors": ["George Kurian", "Somayeh Sardashti", "Ryan Sims", "Felix Berger", "Gary Holt", "Yang Li", "Jeremiah Willcock", "Kaiyuan Wang", "Herve Quiroz", "Abdulrahman Salem", "Julian Grady"], "abstract": "Large-scale Ads recommendation and auction scoring models at Google scale demand immense computational resources. While specialized hardware like TPUs have improved linear algebra computations, bottlenecks persist in large-scale systems. This paper proposes solutions for three critical challenges that must be addressed for efficient end-to-end execution in a widely used production infrastructure: (1) Input Generation and Ingestion Pipeline: Efficiently transforming raw features (e.g., \"search query\") into numerical inputs and streaming them to TPUs; (2) Large Embedding Tables: Optimizing conversion of sparse features into dense floating-point vectors for neural network consumption; (3) Interruptions and Error Handling: Minimizing resource wastage in large-scale shared datacenters. To tackle these challenges, we propose a shared input generation technique to reduce computational load of input generation by amortizing costs across many models. Furthermore, we propose partitioning, pipelining, and RPC (Remote Procedure Call) coalescing software techniques to optimize embedding operations. To maintain efficiency at scale, we describe novel preemption notice and training hold mechanisms that minimize resource wastage, and ensure prompt error resolution. These techniques have demonstrated significant improvement in Google production, achieving a 116% performance boost and an 18% reduction in training costs across representative models.", "sections": [{"title": "I. INTRODUCTION", "content": "Our industry-scale production infrastructure trains thousands of Ads recommendation [3] [4] and auction scoring [18] models annually across numerous teams at Google on a large-scale fleet of Tensor Processing Units (TPUs) [23] [15]. Several of these production models contain billions of parameters, and train on petabytes of input data, across billions of training events. Increasingly numerous machine learning (ML) components are deployed in such models to serve high quality ads at scale.\nMachine learning (ML) components are computationally intensive, demanding significant compute (FLOPS), memory, and network bandwidth resources. Over time, the models have grown in size and complexity, leading to high capital expenses and power costs. ML-centric hardware accelerators like TPUs and GPUs have emerged to reduce the cost to train such large models. These accelerators contain large matrix units to efficiently execute linear algebra computations [24] [7]. For example, there are several generations of TPUs (available on Google Cloud) to train ML models with increasingly higher performance and perf/Watt.\nWhile new accelerators offer significant potential to boost ML compute, maintaining peak performance and utilization at production scale presents substantial challenges. This paper optimizes large-scale ML training on new accelerators by focusing on three key areas:\n(1) Input Generation and Ingestion: To optimize the utilization and cost-efficiency of expensive TPU resources in a production pipeline, we need to focus on maximizing their throughput. A crucial aspect of this is the input preprocessing stage, which prepares streams of data for TPU computation. It is essential to design a fast input pipeline that minimizes downtime and ensures a continuous flow of data to the TPUs. Input feature transformation is a critical stage in the training input pipeline that converts raw features (e.g., \"search query\") into numeric inputs to inject to the model's input layer. This process, often dominated by complex dataflow and irregular matching (e.g., string processing), does not execute efficiently on conventional accelerators. Hence, they require auxiliary compute resources such as CPUs, RAM, and disk. In production scale, the cost of such auxiliary resources is a significant portion of overall training cost. To minimize end-to-end training cost and maximize performance, this paper proposes a scalable input pipeline, including a shared input generation service that reduces input generation load by amortizing the cost across several training pipelines, and a horizontally scalable per-model input reader service to keep TPU efficiency high.\n(2) Large Embedding Tables: Embeddings are crucial in Ads recommendation and auction scoring models due to their memoization capabilities, cost-effectiveness in serving, and ability to meet strict latency requirements [4] [18]. They transform sparse feature inputs into dense floating-point vectors, which are then used by the neural network. Industry-scale models have large embedding tables which exhibit irregular and data dependent memory access patterns. Each model has hundreds of embedding tables. Across five representative production model families, the embedding tables are diverse in nature with widely varying distributions of sizes (vocab-"}, {"title": "II. BACKGROUND", "content": "This section first describes the model architectures of recommendation models, and then provides a brief background on the TPUs used for training.\nA. Target Model Architecture\nAt a 10,000-foot view, Ads recommendation and auction scoring models are composed of an embedding layer followed by several dense (hidden) neural network layers. Inputs to such models are generally categorical features (e.g., words in a search query). Embeddings are the standard way to transform categorical features into dense feature vectors for training the dense network [19]. Embeddings are implemented using lookup tables. For example, consider a table with 10,000 rows (vocabulary size) and 100 columns (output dimensions). Each input feature can look up either one row (univalent embedding) or a small, dynamic number of rows (multivalent embedding, typically combined by a simple aggregation function such as an average). A neural network model might have several tables of various sizes for different categorical features. Embeddings"}, {"title": "III. TRAINING SERVICE ARCHITECTURE", "content": "Our training service comprises two key components: (1) a fleet of distinct training pipelines, each dedicated to an individual model, and (2) a shared feature transformation infrastructure utilized by multiple models, as shown in Figure 1. Each training pipeline includes: (a) Trainer jobs, (b) a Controller, (c) Input Readers, and (d) Checkpoint Files stored in a distributed disk-based file system. Google's Borg cluster manager [30] oversees all jobs within the infrastructure.\nA. Initial vs Caught-Up Training\nModel training is organized into two distinct phases: (a) initial training on historical training data and (b) caught-up training on data as it streams in. During initial training, models train on petabytes of historical input data, across billions of training events collected over many months. Minimizing training time is of paramount importance to developer velocity. We train across many TPU chips, typically 64-256, to complete this training phase within a couple of days.\nOnce a model completes initial training, it is considered caught-up since it has trained on all events until the present, and the model transitions to caught-up training. During this phase, the model only trains on newly arriving data. We can use a small number of TPU chips, typically 1-4, during this training phase, to stay caught-up.\nB. Trainer Jobs\nTrainer jobs, the core of the training pipeline, consist of TPU Host Workers and optional Parameter Servers [8]. TPU host workers reside on servers directly connected to the TPU chips, primarily responsible for supplying training data to TPU chips, but otherwise performing little actual computation. A special primary TPU host worker is additionally responsible for orchestrating the TensorFlow (TF) training loop.\nDuring initial training, the aggregate memory of all TPU chips suffices to store large embedding tables. We place embeddings in TPU memory with the tables partitioned across chips. TPU chips then handle both computationally intensive and memory intensive tasks. TensorCores on TPU Chips take care of computations and update model weights, while embeddings (including applying embedding gradients) are handled by SparseCores.\nDuring caught-up training, the limited TPU slice size (e.g., 1-4 chips) may necessitate offloading embedding tables to Parameter Servers. These parameter servers are regular Jobs equipped with sufficient CPU and RAM resources to accommodate embeddings. TPU host workers then handle the interactions with the Parameter Servers, sending/receiving RPCs to read and update embeddings. This can significantly impair training speed. We describe optimizations to improve speed in Section V."}, {"title": "C. Controller", "content": "The Controller is in charge of orchestrating the training cycles called epochs. An epoch is some duration of wall time (e.g., 1 hour) during which a model trains on a range of input events. During training, TPU host workers request work units (groups of training events) from the Controller. The Controller decides what data to train on, including events not yet seen by the model or previous events with new labels.\nAt the end of the epoch, the Controller writes a checkpoint. To ensure the checkpoint is complete and consistent, the Controller forces all pipeline components to drain outstanding work and quiesce. This forced stall ensures state is consistent across the training pipeline. If any training component fails for any reason, the Controller does not write a new checkpoint. The next epoch then begins by restoring from the most recent checkpoint and retraining on the data from the failed epoch. Since the checkpointing protocol ensures complete and consistent pipeline state at the end of every successful epoch, the model will train on all data exactly once, and no training data is lost or duplicated."}, {"title": "IV. INPUT GENERATION AND INGESTION", "content": "A key to efficient, production-scale model training is optimizing data preparation and ingestion. While TPUs excel at handling the computationally expensive mathematical operations of training, the feature transformations needed to convert raw data into numerical model inputs are also resource-intensive, but not amenable to hardware acceleration. If feature computation and data ingestion lag behind graph execution on TPUs, valuable accelerator resources are wasted.\nTo overcome this bottleneck, we designed our input pipeline with two primary components: a Shared Input Generator (SIG) for reading and pre-processing raw data, and input readers that supply input batches to trainer jobs (Figure 1). This architecture ensures a seamless flow of data, maximizing TPU utilization and accelerating model training.\nA. Shared Input Generation (SIG): Optimizing Feature Transformation for Model Training\nIn our platform, models use an in-house domain-specific language (not Tensorflow) to describe input features through transformation graphs. Each input to the model consumes the output of one connected component in the graph; each component shows how to read, combine and transform raw data to produce the input. For instance, consider a model input that requires reading text fields A and B, converting them to unigrams, computing their intersection, and concatenating the result with another text field C (Figure 2).\nThe sheer scale of our system, with individual models consuming petabytes of raw data and the collective processing of trillions of events per second, necessitates a focus on high throughput. While models rarely share their entire transformation graph, individual connected components are frequently reused. To leverage this commonality and optimize resource utilization, we introduce Shared Input Generation (SIG). This mechanism materializes the output of shared components, effectively amortizing the computational cost across multiple models.\nFigure 3 outlines SIG's high-level design:\n1) Training models submit their transformation graphs to SIG's Matching frontend.\n2) The frontend returns a \"read solution\" to the model's training pipeline, specifying which pre-computed (memoized) data to fetch from the general training data warehouse.\n3) This warehouse integration minimizes the need for custom logic to distinguish between memoized and raw data access.\nBy strategically caching and reusing intermediate results, SIG ensures efficient feature transformation, even when dealing with massive datasets and high-throughput demands. This approach significantly reduces redundant computations, accelerating model training and optimizing overall system performance.\nTo determine which data a training pipeline should read (the \"read solution\"), SIG performs a two-step process:\n1) Matching: The transformation graph submitted by the pipeline is compared against components stored in SIG's Matching Cache. This cache holds the components of all transformations that SIG has previously memoized (pre-computed and stored). The comparison assesses both the structure and content of the graphs. A cache hit signifies that SIG has performed the necessary transformations, allowing reuse of the memoized data. Conversely, a cache miss indicates that some or all of the pipeline's transformations haven't been materialized yet.\n2) Scheduling New Work: For cache misses, SIG schedules new memoization tasks. It assigns a storage location for the output of these tasks, includes it in the read solution, and enqueues the tasks in a global queue. Consequently, the read solution comprises a combination"}, {"title": "1) Challenges and Solutions", "content": "Despite its effectiveness, SIG faces several key challenges:\nScaling: The dynamic nature of SIG's workload poses a significant challenge in resource management. SIG workers must rapidly scale up to meet the demands of TPU training jobs, often experiencing a tenfold increase in capacity within hours. Conversely, they need to scale down promptly when memoized results are readily available to avoid wasting resources. This rapid fluctuation in resource requirements-with CPU usage potentially surging by a factor of 100 within hours-creates a complex environment for resource planning and monitoring. To tackle this challenge, ongoing efforts are focused on optimizing resource allocation and forecasting in this volatile landscape.\nPrioritization: Within the model training landscape, not all pipelines carry the same weight. Some, like those developing high-revenue-impact models, naturally take precedence over more exploratory or research-oriented ones. Additionally, these priorities are not static; they evolve as models are created, refined, and retired. To manage this dynamic prioritization, SIG employs a client-based approach. Each model belongs to a specific \"client,\" and each client has its own dedicated work queue, which constantly adapts to the changing landscape of models and their respective priorities. The priority assigned to each work element within the queue directly reflects the priority of the model that requested it. This client-centric approach ensures that resources are allocated efficiently, with higher-priority models receiving preferential treatment. It allows for a flexible and responsive system that can adapt to changing priorities during model deployment.\nInsertion: SIG's current strategy involves materializing (pre-computing and storing) all requested data. This non-adaptive insertion policy prioritizes system simplicity over potential efficiency gains that could be achieved with a more sophisticated, adaptive approach. The effectiveness of the current policy is evident in the high cache hit rates, consistently exceeding 95%. Moreover, the average data block stored in the cache is utilized by 22 different models, demonstrating the substantial reuse of memoized results across various training pipelines. This data suggests that even without an adaptive insertion policy, SIG effectively leverages shared computations to accelerate model training.\nEviction: Managing storage costs is crucial for the sustainability of SIG. To achieve this, a dedicated eviction process constantly monitors the \"last requested\u201d time, which is stored alongside each entry in the matching cache. This allows SIG to identify and remove memoized data that hasn't been used recently, freeing up valuable storage space. In addition, there are cases where data needs to be evicted for other reasons. For example, users might discover a bug in a transformation graph and request the removal of all memoized outputs associated with that graph. This situation becomes particularly complex when the problematic graph is a subgraph shared across multiple transformation graphs. To address this, SIG incorporates a search infrastructure that indexes transformation graphs based on various attributes, including the raw features they read, the training pipelines that utilize them. We provide a web-based UI where users can query the system, precisely pinpoint and evict all memoized data generated by a faulty subgraph, ensuring data integrity and preventing the propagation of errors in downstream processes."}, {"title": "2) Limitations and Future Directions", "content": "Despite its crucial role in enabling large-scale TPU training, SIG has notable limitations that present opportunities for future enhancements:\n1) Limited Memoization Scope: SIG currently only memoizes connected components within transformation graphs, not arbitrary subgraphs. This restriction hinders optimizations like eliminating common nested subgraphs across different models.\n2) Inability to Join Memoized and Raw Data: The input pipeline lacks the capability to combine memoized SIG data with raw feature data. Consequently, SIG cannot selectively memoize only the most impactful transformations.\n3) Unsupported Mutable or Late-Arriving Data: SIG does not handle mutable or late-arriving data, which is common in caught-up training scenarios. This limitation arises from the fact that late-arriving data can be altered by delayed processing (e.g., spam detection), and incorporating such mutable data would complicate SIG's amortization logic. As a workaround, training pipelines needing mutable data switch to the less efficient Local Input Generation (LIG).\n4) Large Failure Blast Radius: The dependency of multiple training pipelines on a single SIG worker pipeline amplifies the impact of failures. In the current implementation, SIG workers often process multiple components simultaneously, leading to cascading failures if a single component is misconfigured.\nWe acknowledge these limitations and are actively working towards addressing them in a future version of SIG."}, {"title": "B. Input Reading: Adapting to the Demands of High-Performance TPU Architectures", "content": "The increasing power of newer TPU architectures has created a challenge: TPU hosts lack sufficient CPU resources to directly read and transform raw data from the data warehouse. To address this, we developed input readers, a horizontally scalable system of stateless jobs tailored to each training pipeline. This design enables flexible adaptation to varying workloads.\nHorizontally scaling the input readers allows the system to adapt to the changing needs of the processing workload. For instance, during the transition from initial training to caught-up training, the workload shifts from being I/O-bound to compute-bound. Input readers seamlessly handle this change, performing feature transformations entirely within themselves during the caught-up phase. This process, known as Local Input Generation (LIG), differs from SIG as it does not share transformations across models.\nStateless Readers: Input readers are designed to be stateless, ensuring flexibility and scalability. TPU host workers (Figure 1) request work units from the Controller which delegates them to input reader tasks. These reader tasks stream training examples (events) to the TPU host, where they are buffered in memory. The host then constructs input batches from this buffer.\nHorizontal Auto-scaling: Horizontal scaling is achieved by adjusting the number of in-flight work units requested by TPU hosts. This is done dynamically by monitoring the fullness of the host's buffer. If events are consumed faster than they are added, the host requests more work units, increasing parallelism and, in turn, scaling up input reader tasks."}, {"title": "V. ORCHESTRATING EMBEDDINGS", "content": "As discussed in Section II, there are two kinds of model components, embeddings and dense layers. The computation and communication patterns of dense layers, and embeddings are quite different. Dense layers (fully connected and transformers) focus on matrix and wide vector operations that are regular, statically shaped and independent of the input data. Operations on embeddings focus on smaller vector operations, are quite irregular, and data dependent. For our models, embeddings are large, and need to be partitioned across several machines or chips to fit. In this section, we present our approaches for TPU embeddings, when embeddings are also placed/handled by TPU chips, and for CPU embeddings which are placed on Parameter Servers. The partitioning techniques presented for TPU embeddings are applicable and generalizable to all models using TPUs (not just Ads recommendation models) since they have been incorporated within the TPU compiler.\nA. TPU Embeddings\nThe embedding tables are partitioned across all TPUs using a hybrid of model parallelism techniques. The goal of the partitioner is to maximize performance by balancing the load on compute and memory resources while respecting all hardware-imposed constraints. Partitioning is an N-P hard problem, and solved by modeling it as a constraint optimization problem. We use both heuristics and ILP (Integer Linear Programming) solvers to solve the optimization problem. The number of nodes for partitioning is the number of SparseCores (SC) in the TPU supercomputer.\nThere are three methods to use model parallelism for partitioning an embedding layer: table, column, and row partitioning. All three can be used simultaneously.\nTable partitioning places different embedding tables on different nodes; it works well when the number of tables exceeds the number of nodes. The main limitation is that the embedding table vocabulary sizes and widths differ greatly across tables, stranding memory. Also, embedding tables are typically multivalent: the number of feature values per example per table (valency) is more than one. The valency is dynamic (dependent on the training event), and unknown at compile time, making load balancing a challenge.\nColumn partitioning splits tables along their width resulting in multiple smaller, sharded tables; it works well with wide tables as shards are perfectly load-balanced. However, column sharding leads to smaller and more memory accesses during embedding table lookups and updates, lowering HBM bandwidth utilization. This technique also increases feature value processing overheads since each feature value indexes multiple shards. Moreover, only element-wise training optimizers are compatible with column partitioning\u2014optimizers that operate on an entire embedding row, e.g., LAMB [33], SM3 [5], don't work.\nRow partitioning splits tables along their vocabulary size; it works well with large vocabularies that are the bulk of memory bytes and accesses. The main limit is the need for application-level load balancing strategies. Typically, we use random hashing for input feature values to ensure equal traffic to all shards. However, if the vocabulary is sorted by access frequency, cyclic distribution [1] (spreads the hottest rows over multiple nodes) is better than block distribution [1] (places the hottest rows on the first node).\nAll model parallelism methods use all-to-all data exchange with deduplication. The input batch is deduplicated to obtain a list of unique feature values which are partitioned and exchanged with remote SCs.\nThe embedding vectors corresponding to these unique feature values are fetched from remote HBM and a sparse segment sum [20] operation computes the per-example embedding vectors.\nIn contrast, Meta's [21] model parallelism strategy on GPU partitions and exchanges all feature values with remote GPUs. The remote GPUs perform segment sum [20] operations to compute the embedding vectors. With row partitioning, the segment sums computed are partial; a collective reduce-scatter operation is used to obtain the final embedding vectors. Hence, the traffic injected into the network with this scheme is proportional to the node count. Our strategy works better for"}, {"title": "Load Imbalance", "content": "With row partitioning, each table row is placed on one SparseCore. The load imbalance is 2 (= 0.6+0.3+0.2+0.1). If table and column partitioning are leveraged instead of row partitioning, each table would be partitioned along its width into two segments, for a total of four segments. Each segment would be placed on one SparseCore. More concretely, To would be placed on (SC0, SC1), and T\u2081 would be placed on (SC2, SC3). Columns 0-31 would be placed on (SC0, SC2), and columns 32-63 would be placed on (SC1, SC3). The resulting load imbalance factor is 1, leading to better efficiency than using row partitioning.\nIn addition to load imbalance, HBM bandwidth utilization also plays a significant role in performance. Column partitioning reduces HBM bandwidth utilization by lowering the memory access granularity. Instead of fetching the contents of an entire embedding row from memory, column partitioning fetches several partial rows instead. This leads to lower bandwidth utilization but improved load imbalance. In practice, the hyper-parameters for hybrid partitioning are selected by modeling it as a constraint optimization problem. The optimization goal is minimizing the execution time for embedding lookups and updates, and the constraints are hardware-imposed (e.g., memory capacity). Both memory access granularity and load imbalance are factors that impact execution time."}, {"title": "B. CPU Embeddings", "content": "During caught-up training, we shard embeddings across parameter servers, and communicate the embedding values and gradients over the datacenter network. TPU chips still perform the bulk of model computation, and run the forward/backward passes of training. The model training speed depends on two factors: (1) how well the parameter servers are load balanced, and (2) the amount of RPCs and network traffic incurred to the parameter servers.\nThere are several methods of partitioning the embeddings. The most useful technique for CPUs is Row Sharding, where all embedding tables are split into groups of rows across all parameter servers. The advantage with this approach is that the parameter servers are well load-balanced. The drawback is that the number of RPCs incurred with this approach is large. RPCs are sent on behalf of each TPU core to the parameter servers for all embedding tables. The traffic pattern is all-to-all, and the #-of-RPCs to each parameter server = #-of-TPU cores \u00d7 #-of-Tables, resulting in significant scalability bottlenecks. In a production Ads model performing online training, there are hundreds of embedding tables, and tens of TPU cores. This results in thousands of RPCs from TPU host workers to each parameter server on every training step.\nFetch and update coalescing is a technique where values from different tensors are combined into the same RPC. All communication between parameter servers and the TPU workers happens through a single pair of RPCs per batch; the values for different embeddings are coalesced into a single serialized packet. This turns out to be critical for performance. Table Stacking, where tables with the same width and optimization parameters are stacked to form a single variable, is also critical to reduce the graph complexity. With table stacking, we require the use of cyclic distribution [1] to ensure that each table is spread across all parameter servers for optimal load balance.\nC. Software Optimization Techniques\nBeyond the partitioning techniques described above, a few software optimizations bear mentioning.\nFeedback-directed partitioning (FDP): For efficient partitioning, the TPU software probabilistically profiles training batches, counting the number of feature values (per example), number of unique feature values, and load imbalance information, storing these statistics in a database shared across many models. The database is indexed by feature metadata (e.g., feature name). Most experimental models reuse features, enabling accurate prediction of embedding patterns. We sample only 3% of the training batches to reduce training overheads. The database of statistics is continuously updated.\nSoftware-based deduplication. TPUv2/v3 uses a software-based technique to identify duplicated feature values and save work. The host CPU deduplicates and passes the unique feature values to the SparseCore (SC). The SC gathers embedding vectors for each unique value, and the TensorCore (TC) performs segment sum [20] operations to construct per-example embedding vectors. The wide range of valency (peak 1000, but averages closer to 10) further complicates deduplication. To avoid overprovisioning TC memory, the TC handles the common, low-valency case while large valencies are handled without deduplication on the SC. The profiling database from feedback directed partitioning is used to inform the shapes for operations on the TensorCore. Since TPUv4 has added the capability for full deduplication in hardware, software-based deduplication is unnecessary.\nPipelining with TensorCore: Since embeddings are the input for recommendation models, forward embeddings are the first stage and backward embeddings are the last stage of a training step. Strictly serialized execution (Figure 5a) would force SC and TCs to alternate execution during training. We relax the strict serial semantics and allow the SC to start step N+1 while the TensorCore runs step N (Figure 5b). This relaxation better overlaps SC and TC operation, yielding significant performance improvement compared to serialized execution. Note however, that the overlap increases contention for shared resources, primarily HBM (high bandwidth memory) and ICI (inter-chip interconnect) bandwidth. The resulting embedding gradients are stale (by one step), but this was found to not impact model quality in internal recommendation and ranking models."}, {"title": "VI. TRAINING EFFICIENCY", "content": "Improving system efficiency requires that work done by TPUs are not wasted due to interruptions from other jobs, and error conditions are handled promptly with minimal wastage of TPU resources.\nA. Handling Errors\nTraining pipelines may routinely get into states where no training progress can be made. It is vital for resource efficiency to automatically detect these states and release the TPUs so they can be assigned to another queued service. There are 2 categories of states that need to be detected:\nPermanent errors: Invalid model configurations, compilation errors, out-of-memory errors, and numeric overflow errors (e.g., NaNs) are examples of permanent errors. When the system detects such an error, it places a training hold signaling that user intervention is required. The hold is not removed automatically.\nTransient stalls: A primary example is unavailability of training data in SIG because SIG is still busy processing the materialization request for the event range the model trains over next. The training pipeline can detect this as part of scheduling work units and place a training hold releasing the expensive compute resources (TPUs and input readers). The Controller of the system stays up during the hold and removes it once the data has become available.\nThere could also be temporary errors during training caused by events like network interruptions. In such cases, training holds are not placed, and model training is restarted from the last checkpoint.\nB. Maintenance and Preemptions\nTraining pipeline lifetimes vary from days to months. These long time lines mean that jobs and machines in the service are subject to interruptions due to higher priority jobs in the same shared datacenter, new software releases, or machine maintenance; they cause jobs to be terminated prematurely, and moved to different machines.\nTo better support these interruptions, training jobs support a preemption protocol. External components such as the deployment or release infrastructure can send a preemption notice and give the job a warning that it will be shut down soon. When a job receives such a notice, it informs the Controller of the impending shutdown, which in turn broadcasts the notice to all other jobs.\nEach job immediately attempts to finish its outstanding work and transitions into a state where it can correctly checkpoint the training progress. Once the training progress is checkpointed, the preempted job will exit gracefully. The Controller will block progress in the next training epoch until the job has restarted and joined the training pipeline. This ensures that training progress made in the current epoch is not discarded due to interruptions."}, {"title": "VII. EVALUATION", "content": "In this section, we evaluate the performance and efficiency improvements from our main contributions.\nA. Methodology\nThe system studied for initial training (on historical data) is comprised of 128 TPUv4 [15] chips. Each TPUv4 machine contains 4 TPU chips, each connected to 32 GiB of High Bandwidth Memory (HBM), and 2 AMD Rome [2] CPU sockets, each connected to 256 GiB of DDR4 RAM. In addition to TPUv4 machines, each training pipeline relies on input readers and (optional) parameter servers placed on external CPU machines. Such CPU machines are shared by several heterogeneous jobs running in the datacenter.\nWe pick five representative recommendation models accounting for >50% of our production workloads for the"}, {"title": "Load Imbalance", "content": "We define a term called Load Imbalance to understand the partitioning efficiency. To calculate load imbalance, we compute the total number of bytes Bi accessed from memory on each SparseCore SCi (averaged across many steps). Load imbalance is defined as the ratio of max Bi to average Bi. Lower the load imbalance, higher is the efficiency of partitioning (N is the number of SparseCores in the following equation).\n$Load \\ Imbalance = \\frac{N \\ max_{i} B_i}{\\Sigma B_i}$ (1)\nWith row partitioning, each table row is placed on one SparseCore. The load imbalance is 2. If table and column partitioning are leveraged instead of row partitioning, each table would be partitioned along its width into two segments, for a total of four segments. Each segment would be placed on one SparseCore. More concretely, T0 would be placed on (SC0, SC1), and T1 would be placed on (SC2, SC3). Columns 0-31 would be placed on (SC0, SC2), and columns 32-63 would be placed on (SC1, SC3). The resulting load imbalance factor is 1, leading to better efficiency than using row partitioning."}, {"title": "E. Embedding Optimizations for CPU", "content": "Fetch and update coalescing improves the performance of caught-up models by 8% (Models A, D) and 10% (Model B, C) and 6% (Model E). To improve the efficiency of such coalescing, we need to minimize the number of memory copies in software. This is done by implementing smart split operations for the coalesced buffer using reference counting techniques. The results of the split operation share the same underlying buffer as the original coalesced tensor, thereby avoiding memory copies.\nF. Handling Errors\nTable II shows the TPU chip demand for models averaged over a 7-day duration relative to the chip footprint. If x chips are used for active training at any given time, another 1.03x chips are being demanded by models in the training queue that are ready to run but don't have the available resources, and 2.49x chips are being demanded by models placed on training hold. Given the large TPU demand from models placed on hold due to encountering permanent errors or transient stalls, it is vital to handle error conditions carefully to avoid stranding (under-utilizing) TPU resources.\nG. Maintenance & Preemptions\nTraining pipelines are subject to interruptions due to (1) higher priority jobs in the same shared datacenter, (2) new software releases, or (3) machine maintenance. During such interruptions, an early epoch end is initiated to save training progress. 61% of the preempted epochs succeed in committing their training progress."}, {"title": "VIII. LESSONS LEARNED AND FUTURE DIRECTIONS", "content": "While TPUs and similar hardware excel at linear algebra computations, maximizing their utilization in a production environment requires a holistic approach. Bottlenecks can arise in other stages. In this section, we highlight key lessons learned in optimizing TPU performance for large-scale machine learning systems, focusing on continuous training of Ads models:\nA. Preventing TPU Starvation: The Importance of Input Pipeline Efficiency\nThe input pipeline, responsible for preparing and feeding data to the TPU, can significantly impact overall training performance and cost. A slow or inefficient pipeline can starve the TPU, leading to underutilization and wasted resources. For our production system, we built a scalable input pipeline with two key components: Shared Input Generation Service to reduce redundant computations by amortizing the cost of input generation across multiple models; and Horizontally Scalable Input Reader Service to ensure a continuous flow of data to the TPUs, maximizing their efficiency.\nB. Managing Large Embedding Tables with TPU SparseCores is Key\nIndustry-scale models deal with massive embedding tables, each with millions or even hundreds of millions of entries. These tables also have diverse characteristics in terms of size and access patterns, making optimization complex. Accessing embedding entries involves irregular memory lookups, inefficient on hardware accelerators designed for structured computations. While TPUs provide special purpose hardware for handling embedding tables (SparseCore in TPUs), partitioning large embedding tables within the limited memory capacity of accelerators like TPUs is a significant challenge. To address this, we came up with Novel Partitioning Strategies to effectively distribute embedding tables across multiple accelerator chips, and Pipelining and RPC Coalescing techniques to further enhance performance by streamlining embedding operations and minimizing communication overhead.\nC. Robust Resource Management Strategy is Crucial\nHardware accelerators like TPUs are expensive to operate. Minimizing their idle time is essential for cost-effective training, especially at scale. In shared datacenters, training jobs can be interrupted due to various reasons like priority changes, software updates, or hardware maintenance. This can lead to significant resource wastage if not handled properly. Training processes can also encounter errors, both permanent (e.g., model divergence) and transient (e.g., network issues). Efficiently resolving these errors is crucial to prevent prolonged downtime. In this paper, we addressed these issues by Preemption Notices to anticipate and handle preemptions gracefully, and Efficient Error Resolution to quickly identify and address both permanent and transient errors, ensuring prompt recovery and minimal downtime.\nD. Continuous Training Is Challenging To Scale on TPUs\nEffectively adjusting TPU slice sizes and embedding placements to match the varying computational needs of different training phases is challenging. While training on historical data requires high speed to process years of data quickly,"}, {"title": "E. Future Directions", "content": "There are two main directions that are being explored to improve the efficiency of Shared Input Generation. (1) Storage overhead reduction: Memoizing arbitrary subgraphs that are expensive to compute vs the full input transformation graph is helpful in reducing the storage overhead. In addition, joining memoized data with raw feature data that is cheap to compute can provide additional improvements. (2) Handling mutable data: This would allow caught-up training to use SIG, boosting auxiliary resources savings. However, note that in our Ads recommendation system, revenue-critical models would all be trained on this mutable data; memoizing and sharing data between such models introduces shared-fate production risks that are complex to solve.\nTo avoid tight coupling of embedding storage space and compute throughput, hybrid partitioning techniques are being explored. This involves placing the most frequently accessed tables or table rows in TPU HBM, and placing less frequently accessed ones on external variable servers. The bookkeeping overhead to do this classification is the most challenging aspect, especially in the face of changing feature distributions for embedding tables."}, {"title": "IX. RELATED WORK", "content": "In this paper, we explore the intricacies of using TPUs to train large-scale Ads recommendation and auction scoring models at Google [4", "3": [22]}]}