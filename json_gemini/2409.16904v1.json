{"title": "Discriminative Anchor Learning for Efficient Multi-view Clustering", "authors": ["Yalan Qin", "Nan Pu", "Hanzhou Wu", "Nicu Sebe"], "abstract": "Abstract-Multi-view clustering aims to study the comple- mentary information across views and discover the underlying structure. For solving the relatively high computational cost for the existing approaches, works based on anchor have been presented recently. Even with acceptable clustering performance, these methods tend to map the original representation from multiple views into a fixed shared graph based on the original dataset. However, most studies ignore the discriminative property of the learned anchors, which ruin the representation capability of the built model. Moreover, the complementary information among anchors across views is neglected to be ensured by simply learning the shared anchor graph without considering the quality of view-specific anchors. In this paper, we propose discriminative anchor learning for multi-view clustering (DALMC) for handling the above issues. We learn discriminative view-specific feature representations according to the original dataset and build anchors from different views based on these representations, which increase the quality of the shared anchor graph. The discriminative feature learning and consensus anchor graph construction are integrated into a unified framework to improve each other for realizing the refinement. The optimal anchors from multiple views and the consensus anchor graph are learned with the orthogonal constraints. We give an iterative algorithm to deal with the formulated problem. Extensive experiments on different datasets show the effectiveness and efficiency of our method compared with other methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Data can be characterized from multiple modalities, which is usually called multi-view data in computer vision and data analysis [1]\u2013[4]. Different from clustering the data with single view [5]\u2013[9], multi-view clustering aims to exploit the underlying structure and discover the complementary infor- mation among different views for clustering. As an effective method for performing the clustering procedure [10]\u2013[24] on the data from different sources, the existing approaches can be classified into several categories including multiple kernel clustering [25], methods based on matrix factorization [26], multi-view subspace clustering [27] and approaches built on the graph [28]. Multiple kernel clustering [29] maximizes the kernel coefficients and partition matrix to find a shared cluster assignment matrix. Methods based on matrix factorization [30]"}, {"title": "A. Subspace Clustering", "content": "Given single-view dataset $X \\in R^{d\\times n}$, subspace clustering expresses each data point by linearly combining the others and obtains the combination coefficient by minimizing the recon- struction loss, where d and n correspond to the dimension and size of dataset, respectively. The above process is formulated as:\n$\\min_{S}||X \u2013 XS||_F^2 + \\lambda f(S), s.t. S \\ge 0, S1 = 1$, (1)\nwhere $\\lambda > 0$ represents the balance parameter, $f(.)$ is the regularizer function, 1 is a vector with each entry being one, $S\\in R^{n\\times n}$ indicates the similarity matrix and each term in $S$ is non-negative. The first term denotes the error in the reconstruction procedure and the second one is adopted for regularization. We can observe that S owns the size of n \u00d7 n, which is a challenge of scalability to the adopted datasets with large scales.\nIt takes $O(n^3)$ computation complexity in obtaining graph S, which is a burden on both storage and computation for the dataset with large scales. Moreover, the subsequent clustering step also needs extra computation cost, i.e., spectral clustering consumes $O(n^3)$ complexity. For multi-view dataset {$X^p$}$_{p=1}^{\\upsilon}$ with $d^p$ and $X^p \\in R^{d_p\\times n}$ being dimension and the data for the p-th view, multi-view subspace clustering solves the problem as follows:\n$\\min_{S^p} \\sum_{p=1}^{\\upsilon} || X^p \u2013 X^p S^p ||_F^2 + \\lambda f(S^p), s.t. S^p \\ge 0, S^p1 = 1$. (2)\nMost existing multi-view clustering approaches usually need at least O(n2k) complexity with k being the number of clusters, hence they are tend to be computational prohibitive. We then overview the methods of matrix factorization for single view and multiple views, respectively."}, {"title": "B. Anchor Graph", "content": "To reduce the computation complexity, some methods based on anchor have been given [33] for datasets with large scales. They are able to focus on the n\u00d7l anchor graph, where l< n is the total number of anchors. Nie et al. [44] learned a structured optimal anchor graph based on a dictionary matrix"}, {"title": "III. THE PROPOSED METHOD", "content": "In this part, we first present the motivation and formulation of the proposed DALMC. Then the detailed optimization process and the complexity analysis of DALMC are also introduced in the following."}, {"title": "A. Motivation and Formulation", "content": "The existing studies have shown that adopting anchors to reduce the computation complexity is an effective way [33]. Despite these methods have shown advantages, there are some shortcomings remained to be solved. First, the current methods merely learn view-specific anchors based on the original multi- view dataset, which ignore the discriminative property of the learned anchors and limit the representation capability of the model. Second, these methods pay few attention to guaranting the complementary information among anchors across mul- tiple views with the guidance of high-quality view-specific"}, {"title": "B. Optimization", "content": "The problem in Eq. (9) is jointly nonconvex when all variables are simultaneously considered. Therefore, we adopt the alternative algorithm for optimizing each variable with the others being fixed.\nHP-subproblem: With the other variables except for HP being fixed, we reformulate the problem in Eq. (9) as:\n$\\min_{H^p} \\sum_{p=1}^{\\upsilon} || X^p \u2013 Z^p H^p ||_F^2 - \\sum_{p=1}^{\\upsilon} \\beta Tr(H^p(A^pS)^T)$,\ns.t. $H^{pT}H^p = I$. (10)\nThe above formulation can be transformed into\n$\\max_{H^p} Tr(H^pB)$,\ns.t. $H^{pT}H^p = I$, (11)\nwhere $B = \\sum_{p=1}^{\\upsilon} X^{pT} Z^p + \\beta S^T A^{pT}$. Eq. (11) can be solved via SVD and the corresponding computation complex- ity is $O(nd_p^2)$.\nZP-subproblem: With the other variables except for ZP being fixed, we reformulate the problem in Eq. (9) as:\n$\\min_{Z^p}||X^p \u2013 Z^p H^p||_F^2$. (12)\nWe differentiate the above objective function regarding ZP and set the derivative to be zero. Then ZP is updated by\n$Z^p = X^pH^{pT}$. (13)"}, {"title": "C. Complexity Analysis", "content": "The computation complexity to update HP, S and AP is O(ndp2), O(nl2) and O(dPl2), respectively. To obtain the optimal ZP, it costs O(dpn) to perform matrix multiplication. When updating ap, it just needs O(1). Then, the computation complexity of our method is O(nl2 + \u2211p=1(dpl2 + ndp2 +\ndpn)) at each iteration, which is linear to the size of multi- view dataset n."}, {"title": "D. Convergence Analysis", "content": "The objective value in Eq. (9) monotonically decreases when each variable updates with the others being fixed. Thus, we just need to show that Eq. (9) has a lower bound in the proving process. According to Cauchy-Schwarz inequality, we have\n$Tr(H^p(A^pS)^T) \\le ||H^p||_F ||ST||_F || A^{pT} ||_F$. (18)\nBased on \u1e9e < 1 and the above analysis, we have\n$\\beta Tr(H^p(A^pS)^T) < Tr(H^p(A^pS)^T)$\n$\\le ||H^p||_F ||ST ||_F || A^{p*} ||_F = l\\sqrt{d_p}$. (19)\nTherefore, we can obtain a lower bound for Eq. (9) as follows:\n$\\sum_{p=1}^{\\upsilon} \\frac{1}{2}|| X^p \u2013 Z^p H^p ||_F^2 - \\sum_{p=1}^{\\upsilon} \\beta Tr(H^p(A^pS)^T)$,\n$\\ge 0 \u2013 l\\sqrt{d_p} = -l\\sqrt{d_p}$. (20)\nSince the objective value monotonically decreases for each iteration in the alternating optimization procedure, we can conclude that the algorithm is theoretically converged. The convergence our method is analyzed in the experiment."}, {"title": "IV. EXPERIMENT", "content": "In this part, we verify the performance of the proposed method by performing comprehensive experiments on different benchmark datasets with large scales, which involves the pa- rameter selection, clustering performance comparison, ablation study, running time comparison, quantitative study of anchors and convergence analysis."}, {"title": "A. Datasets and Compared methods", "content": "We employ seven benchmark datasets for demonstrating the performance of the proposed method, which include AwA [48], Caltech-256, Flower17, MNIST, TinyImageNet, VGGFace2 [48] and YouTubeFace-50. The details of these datasets are shown in Table I.\nOur method is compared with seven representive works and these algorithms are summarized as follows: AMGL [49] learns the optimal weights for different graphs and then achieves the global optimal results. BMVC [50] simultane- ously performs discrete representation learning and binary clustering structure learning. SMVSC [43] unifies subspace learning and anchor learning into a framework, which is able to obtain more representative information with the con- sensus anchors. OPMC [35] obtains the clustering partition without considering the non-negative constraint. UOMVSC"}, {"title": "B. Parameter Selection", "content": "In this section, parameter selection regarding \u1e9e is conducted on all datasets in terms of different metrics. We tune this parameter in [0.0001,0.001,0.01,0.1,1,10] and report the impacts leaded by \u1e9e on different datasets. According to Figs. 2-3, we observe that superior performance is achieved with \u03b2 = 0.1 in terms of four metrics, which fully shows the validity of considering this term with proper weight in optimization. Moreover, the performance of our method is generally stable for different values on all datasets, showing that our method owns the robustness to \u03b2."}, {"title": "C. Experimental Results", "content": "The proposed method is compared with seven representive works on different benchmark multi-view datasets under four metrics, which include accuracy (ACC), normalized mutual information (NMI), F1-score and Purity. We give the clustering results and use '-' to indicate that the out-of-memory error is caused for the algorithm. According to Tables. II-V, we observe that\n1) The proposed method shows excellent clustering per- formance on different datasets under four metrics. For instance, the proposed method outperforms UOMVSC on Flower17 by 16.09% in terms of NMI. The superior performance shows the superiority of our method for multi-view clustering.\n2) Compared with other works, anchor-based methods have obvious advantages in clustering performance on multi- view datasets. They learn consensus anchor graph by fusing view-specific information across views and mine more informative feature representation.\n3) Among the works based on anchor, our method is still superior in the final clustering performance. FP- MVSCAG is a representative of the anchor-based meth- ods, which is comprehensively surpassed by our method on most datasets in performance. It can be explained by the fact that the discriminative anchor learning is able to increase the quality of the shared anchor graph for multi- view clustering and achieve complementarity among different views. Compared with SMVSC, the proposed method considers learning the discriminative feature representation of each view and builds anchors based on these representations. The reason why the proposed method achieves better performance than AWMVC is that we effectively take the discriminative view-specific feature representation learning into consideration, result- ing in more desired performance."}, {"title": "D. Ablation Study", "content": "In order to demonstrate the validity of adopting the anchor Ap instead of HPC in Eq. (7), we design the first ablation by replacing HPC with Ap in Eq. (9), denoted by ablation- 1. Moreover, we conduct the second ablation to validate the effectiveness of imposing orthogonal constraints on S by applying the constraint 0 \u2264 S \u2264 1 to the S instead of the constraint STS = I, denoted by ablation-2.\nWe report the performance for the proposed method based on ablation experiments including ablation-1 and ablation-2 under four different metrics on Tables VI-IX. Compared with the clustering results in two ablation experiments, the proposed method has obvious advantages in the final performance, which fully validates the necessarity of using Ap to replace HPC with the guidance of applying the orthogonal constraints to S in the optimization process."}, {"title": "E. Running Time Comparison", "content": "The running time for different multi-view clustering ap- proaches on all datasets are listed in this section. According to Table X, it is observed that relatively acceptable running time is consumed by the proposed method, demonstrating that the desired efficiency can be guaranteed. It also further shows that directly learning the anchor Ap for each view instead of adopting HPC is necessary, which removes the quadratic term regarding HP in the optimization process and reduces the computation complexity to linear."}, {"title": "F. Quantitative Study of Anchors", "content": "In this part, we investigate how the performance of our method is influenced by the total number of anchors and"}, {"title": "G. Convergence Analysis", "content": "We have theoretically given the convergence analysis of the proposed method in Section III. To further illustrate the convergence of the algorithm, we perform experiments to show how the objective values change with the number of iterations. From Fig. 6, we find that the target value monotonically decreases as the number of iteration increases, which needs less than 20 iterations to converge."}, {"title": "V. CONCLUSION", "content": "This paper proposes discriminative anchor learning for multi-view clustering to increase the quality of the shared anchor graph and achieve complementarity among different views. It formulates discriminative view-specific feature learn- ing and consensus anchor graph construction into a unified framework, where these two parts improve each other to reach the refinement. We learn the optimal anchors of each view and the consensus anchor graph with the orthogonal constraints. Experiments on seven datasets validate the superiority of our method in terms of effectiveness and efficiency."}]}