{"title": "Interactive-T2S: Multi-Turn Interactions for Text-to-SQL with Large Language Models", "authors": ["Guanming Xiong", "Junwei Bao", "Hongfei Jiang", "Yang Song", "Wen Zhao"], "abstract": "This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods have not adequately addressed scalability, leading to inefficiencies when processing wide tables. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide an efficient and universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our experiments on the BIRD-Dev dataset, employing a setting without oracle knowledge, reveal that our method achieves state-of-the-art results with only two exemplars, underscoring the effectiveness and robustness of our framework.", "sections": [{"title": "Introduction", "content": "Text-to-SQL technology, which translates natural language (NL) questions into executable SQL queries, has emerged as a crucial field of research. This technology empowers non-experts to interact with relational databases (DBs), which have become ubiquitous in the era of big data (Hong et al. 2024). A significant challenge in this field is designing a scalable text-to-SQL system that operates accurately and efficiently within resource constraints.\n\nThe emergence of large language models (LLMs), such as ChatGPT (Ouyang et al. 2022) and GPT-4 (OpenAI 2023), has opened new avenues for enhancing text-to-SQL systems. These models have shown promising capabilities in reasoning (Wei et al. 2022) and few-shot learning (Chen 2023), establishing new benchmarks in this domain (Gao et al. 2024; Pourreza and Rafiei 2023; Talaei et al. 2024).\n\nRecent advancements in text-to-SQL research encompass two primary perspectives: prompt optimization and interaction strategies (Hong et al. 2024). Prompt optimization focuses on crafting effective prompts that guide LLMs to generate accurate SQL queries. This involves constructing precise schema linking, leveraging similar examples, and employing effective question decomposition methods (Gao et al. 2024; Zhang et al. 2023; Tai et al. 2023). Interaction strategies, on the other hand, center around designing methods to iteratively refine SQL queries through execution-based feedback (Qu et al. 2024; Shi et al. 2022; Chen et al. 2024). Recent approaches also introduce interactive models that leverage specific tools to interact with DBs, yielding significant improvements (Jiang et al. 2023; Gu et al. 2024).\n\nDespite these advancements, text-to-SQL systems face several pressing challenges.\n\nResource scarcity for annotating text-SQL data. Current works emphasize prompt optimization by dynamically selecting exemplars based on similarity metrics. This approach assumes the availability of extensive training data as a candidate pool for exemplars. However, such methods are not feasible in low-resource settings, nor is it realistic to assume that user queries will always align with the training data distribution. Moreover, these techniques require large annotated datasets, which are resource-intensive to create. Therefore, it is crucial to explore methods applicable to low-resource settings.\n\nInefficiency when scaling to wide tables. For schema"}, {"title": "Approach", "content": "This study investigates the text-to-SQL task. A relational database (DB) is formally represented as \\(D = \\{T, C, V\\}\\), where \\(T = \\{t_1, t_2,...,t_{\\vert T \\vert}\\}\\) is a set of tables, \\(C = \\{C_1, C_2, ..., C_{\\vert C \\vert}\\}\\) is a set of columns, and \\(V = \\{v_1, v_2, ..., v_{\\vert V \\vert}\\}\\) is a set of cell values. Each table \\(t_i\\) comprises a set of columns \\(c_i\\) and each column \\(c_i\\) encompasses a set of cell values \\(v_i\\).\n\nFollowing (Li et al. 2023), we further define foreign key relations \\(R = \\{(c_h, c_j)\\} \\in C\\), where each pair \\((c_h, c_j)\\) denotes a foreign key relation between column \\(c_h\\) in table \\(i\\) and column \\(c_j\\) in table \\(j\\). The database schema \\(S = \\{T, C, R\\}\\) constitutes a set of tables, columns, and foreign key relations.\n\nDue to limited space, some thought processes were omitted."}, {"title": "Overview", "content": "Recent advancements in large language models (LLMs) have highlighted their impressive capabilities in few-shot learning and logical reasoning. Nevertheless, designing scalable solutions for interpretable and step-by-step SQL generation in low-resource scenarios remains challenging. In response, we introduce Interactive-T2S, a novel interactive method for text-to-SQL translation. This method treats the LLM as an agent interacting with a database environment, enhancing SQL generation through structured dialogic interactions. We developed a unified interaction logic with four generic tools to help LLMs identify relevant information and discern relationships across multiple tables. The example in Figure 2 illustrates this interactive process. Different colors highlight how the corresponding elements can be located."}, {"title": "Tools for Database", "content": "We break down the process of generating SQL into three steps: searching for relevant columns and cell values, identifying the join relationships between tables where columns reside, and refining the prediction based on the execution. In line with this principle, we introduce the following four tools.\n\nSearchColumn(semantic) enhances the efficiency of LLMs by identifying the most relevant columns and excluding non-essential data. It concatenates and vectorizes the names and descriptions of each column, then ranks these columns according to their similarity to the parameter semantic. Furthermore, following the methodology proposed by (Li et al. 2024d), we calculate and return the statistical characteristics of each column's cell values.\n\nSearch Value(value, table=None, column=None) is designed to locate cell values across the entire DB. Similar to the fuzzy match tool described in (Gu et al. 2024), we utilize BM25 to search for cell values within the DB. If the parameters table or column are specified, the tool will conduct searches within the designated table or column.\n\nFindShortest Path(start, end) is designed to efficiently identify the shortest path between two columns within a DB schema, based on foreign key relationships. Existing methods heavily rely on the intrinsic capabilities of models to perform joins across multiple tables, which becomes impractical with scenarios involving extensive joins or a large number of columns. In contrast, the number of tables requiring joins is dictated solely by the DB schema design, rather than the semantic content of the question. By treating the DB schema as an undirected graph with nodes represented by columns (in the format {column name}.{table name}) and edges defined by relationships within columns and foreign key constraints, this tool decouples the complexity inherent in multi-table joins, thus alleviating the load on LLM.\n\nExecuteSQL(sql) provides the capability to execute SQL queries directly, offering significant flexibility.\n\nImplementation details are provided in Appendix A, and additional usage instructions are described in Appendix B."}, {"title": "Interactive Process", "content": "Given a question q, we first construct a prompt text:\n\n\\[Prompt = \\{Inst, E, Sq, q\\}\\]\n\nwhere Inst denotes the pre-written instruction text, which encompasses descriptions of tools, usage guidelines, and the required format. \\(E = [(S_0, e_0, ...e_n), ...]\\) represents a list of demonstrations, each consisting of a database schema \\(S_i\\) and n exemplars \\(e\\) with full interactive process.\n\nIn each turn T, we prompt the LLM generate an action based on the Prompt and the historical interaction H. Specifically, this process is described by:\n\n\\[a_T = LLM(\\{Prompt, H\\})\\]\n\n\\[H = \\{c_0, a_0, o_0, ..., c_{T-1}, a_{T-1}, o_{T-1}\\}\\]\n\nwhere c denotes the intermediate thought process, an action a belongs to the set \\{\\text{SearchColumn}, \\text{SearchValue}, \\text{FindShortestPath}, \\text{ExecuteSQL}, \\text{Done}\\}, and the observation o results from executing an action, defined as \\(o_T = \\text{Tool}(a_T)\\).\n\nGeneral Solution for Text-to-SQL\nWe propose a general and unified interaction logic for generating SQL queries, as the example illustrated in Figure 2. The process begins with locating elements. Initially, the LLM is tasked with decomposing the user's question into a conceptual plan (co), which is flexibly designed to adapt to the semantics of the question, enhancing comprehension for both LLMs and humans. Following this, the LLM is required to generate a thought c and an action a aimed at identifying pertinent columns and cell values within the DB. The subsequent phase involves joining tables. Here, we categorize the columns in the target SQL query into those that need to be selected and those that need to be filtered. Utilizing the FindShortestPath tool, we determine the shortest path between relevant columns to optimize the joining process. The final phase is execute SQL, where the LLM executes the constructed SQL query to retrieve the desired results. This query execution is deemed the final output."}, {"title": "Experiment", "content": "Spider (Yu et al. 2018) is a widely used cross-domain text-to-SQL benchmark. We utilize the development set.\n\nBuilding on this, Spider-DK (Gan, Chen, and Purver 2021) challenges parsers with domain knowledge reasoning and real-world question paraphrases to evaluate cross-domain generalization. Spider-Syn (Gan et al. 2021) introduces synonyms for schema tokens to prevent reliance on string matching. Spider-Realistic (Deng et al. 2021) further challenges models by revising questions to omit explicit column names, thus testing text-table alignment capabilities.\n\nBIRD (Li et al. 2024c) is notable for its complexity, featuring intricate instructions over highly complex databases. BIRD has two settings: with and without external knowledge evidence (oracle knowledge), which provides specific information needed for answering questions. We use the development set for our experiments. BIRD-Financial Corrected (FinC) (Wretblad et al. 2024) addresses the issue of noise in the BIRD benchmark, particularly the uneven distribution of incorrect SQL queries that affects its reliability. They have revised the dataset under the financial domain, which includes 106 question and SQL query pairs, representing approximately 7.5% of the development data. The correction includes SQL-only corrections (SQLC) and corrections for both SQL queries and noisy questions (DataC). BIRD Mini-Dev dataset offers 500 high-quality text-SQL pairs derived from community feedback. We utilize the SQLite version. Following (Gu et al. 2024), we argue that using oracle knowledge is unreasonable; however, we still report the results of both for comparison.\n\nThe detailed statistics are listed in Table 1."}, {"title": "Evaluation Metrics", "content": "Following (Qu et al. 2024) and (Shen et al. 2024), we report exact match accuracy (EM) (Yu et al. 2018) and execution accuracy (EX) (Zhong, Yu, and Klein 2020) for the Spider dataset series. EM requires each component of the predicted SQL to match the gold SQL, but it cannot handle cases where multiple correct answers exist (Sun et al. 2024; Zhang et al. 2023), so we report it only for reference. EX, which requires the execution result of the predicted SQL to be correct, is generally more precise. For the BIRD-Dev and FinC datasets, we use the EX metric. For the Mini-Dev dataset, we also report the Soft F1-Score (Li et al. 2024c)."}, {"title": "Baselines", "content": "To comprehensively evaluate our approach, we have selected various state-of-the-art (SOTA) baseline models.\n\nFine-tuning (FT) on full data. CodeS (Li et al. 2024b), a series of pre-trained language models, addresses schema linking and domain adaptation through incremental pre-training on a SQL-centric corpus, strategic prompts, and bi-directional data augmentation. We select the best results in the Supervised Fine-Tuning (SFT) setting.\n\nPrompt-based methods. We selected recent prompt-based methods using GPT-4 or newer models. In the context of in-context learning (Dong et al. 2024), these can be categorized into Selection-based and Fixed Few-shot: the for-"}, {"title": "Main Results", "content": "Analysis of Spider-Dev and its variants Experimental results are presented in Table 2. Our method shows competitive performance across the Spider-DK, Syn, and Realistic datasets. Selection-based methods, which choose similar exemplars from training data, outperform fixed few-shot approaches, as confirmed by the EM metric. However, the performance gap narrows significantly across the three variant datasets, suggesting selection-based methods assume similar data distributions between development and training sets, indicating their inadequate generalization capabilities.\n\nIn fixed few-shot methods, both TA-SQL and SL+CC+RS perform well because they utilize the entire DB schema (including all columns) as prompt text for schema linking, which is feasible due to the relatively small size of the DB. Table 6 highlights an average of 307 schema tokens per DB in the Spider-Dev dataset.\n\nDespite the complexity of TA-SQL, which uses intricate modules for generating SQL and pandas-like APIs for reasoning, our design employs a simple yet effective unified interaction logic. SL+CC+RS performs worse when incorporating domain knowledge (DK dataset), highlighting our approach's superior generalization.\n\nWe argue that the simplicity of the Spider dataset limits the effectiveness of our method, as discussed in the difficulty analysis section.\n\nAnalysis of BIRD-Dev and its variants In our analysis, following the critiques by (Gu et al. 2024) regarding the impracticality of oracle knowledge in real-world applications, we focus primarily on settings without oracle knowledge. Our methodology achieves superior outcomes on the BIRD-Dev dataset, as detailed in Table 3, improving the SOTA by 2.87 percents. Furthermore, we reimplemented the Zero-shot and DIN-SQL on both Mini-Dev and BIRD-FinC dataset, as conducted by (Wretblad et al. 2024). The results are provided in Table 5. Across all datasets, our method sets a new standard in the setting without oracle knowledge, establishing new SOTA results."}, {"title": "The Difficulty Analysis of Locating Cell Values", "content": "In the framework of the interactive process, SQL difficulty can be quantitatively assessed by examining the challenges associated with locating schema elements, cell values, and joining tables. This study offers a more precise difficulty assessment than the coarse-grained approach based solely on SQL keywords (Yu et al. 2018; Li et al. 2024c). In this section, we concentrate on the specific challenge of locating cell values, deferring the discussion of other parts to the token efficiency section and the ablation study section.\n\nDrawing parallels from the knowledge-based question answering task, where the Mention Cover Rate (Xiong, Bao, and Zhao 2024) gauges the difficulty of entity linking, we introduce two metrics: Cell Value Rate (CVR) and Cell Value Cover Rate (CVCR). CVR is defined as the proportion of SQL queries containing value constraints, typically indicated by the presence of the WHERE clause. CVCR measures the frequency at which the actual cell values referenced in the constraints appear explicitly within the questions. These metrics together help in understanding the intricacies involved in cell value localization within SQL queries.\n\nStatistics presented in Table 1 reveal significant disparities between the Spider and BIRD dataset series. Specifically, the BIRD series requires the identification of cell values in approximately seven times as many cases as the Spider series. Notably, in the Spider-Dev dataset, around 87% of cases requiring cell value identification include the golden cell value directly in the question, simplifying the SQL generation process. Conversely, the Spider-DK dataset reduces the CVCR to 40.79%, substantially increasing the complexity and leading to a notable performance drop in SL+CC+RS. These findings underscore the necessity of developing tools like the SearchValue tool in our framework, which aids LLMs in pinpointing cell values."}, {"title": "The Efficiency Analysis of Schema Linking", "content": "In this section, we analyze the efficiency of schema linking using DIN-SQL as a case study. Table 7 lists the number of fixed prompt text (input) tokens across the four modules of DIN-SQL, as well as the estimated total prompt tokens per case based on the average tokens per DB detailed in Table 6. Given that DIN-SQL utilizes the complete DB schema in each module, the average prompt tokens per case are calculated as the sum of total fixed tokens and four times the average tokens per DB, resulting in about 12.8k and 21.6k for Spider-Dev and BIRD-Dev, respectively.\n\nComparatively, our method, as presented in Table 8, requires only 4.6k and 4.7k tokens per case, which corresponds to approximately 36% and 22% of the tokens required by DIN-SQL, respectively. This efficiency stems from our method's dynamic retrieval of necessary information, which is not affected by the length of the DB schema, demonstrating our method's scalability. It is worth noting that while the characteristics of causal decoding in decoder-only LLMs allow for computational reductions in scenarios with extensive repetitive prefixes via the KV-Cache technique (Shi et al. 2024), the current interactive mode of calling the OpenAI API does not benefit from such cost reductions."}, {"title": "Ablation Study", "content": "In this section, we explore the impact of the FindShortestPath tool. We categorized and sampled cases from Spider-"}, {"title": "Error Analysis", "content": "In this section, we performed an error analysis by randomly sampling 100 cases from both the Spider-Dev and BIRD-Dev datasets, consistent with the original data distribution."}, {"title": "Conclusion", "content": "Interactive-T2S introduces a text-to-SQL approach that leverages a LLM as an agent to generate SQL queries through multi-round interactions with a database. We designed a unified tool and interaction methodology for schema linking, cell value localization, table joining, and query refinement based on execution results. Additionally, we employed a few-shot learning strategy to guide the LLM in incrementally generating SQL queries. Experimental results demonstrate that our method achieves SOTA results with just two exemplars."}, {"title": "Appendix", "content": "This appendix provides detailed experimental results and offers further discussion.\n\nAdditional Details of the Tools Implementation Details\nThe tool definition in Python syntax and the usage examples can be found in the Appendix C.\n\nSearchColumn(semantic) The tool SearchColumn is designed to rank database columns based on their relevance to the semantic parameter.\n\nFor the Spider dataset, column and table names are embedded using the template \"a column named {column_name} in table {table_name}\". For the BIRD dataset, the template \"a column named {column_name} in table {table_name} about {desc}\" is used, where {desc} includes descriptions of the column provided by the dataset (Li et al. 2024c).\n\nWe utilize the OpenAI text-embedding-3-large API to generate vectors, and we employ Chroma for indexing and searching.\n\nThe tool will return the following features for each column:\n\n\\bullet column_name: the name of the column.\n\n\\bullet table_name: the name of the table.\n\n\\bullet column_type: the data type of the column.\n\n\\bullet column_desc: the description of the column.\n\n\\bullet column_statistics: the statistics of the cell values in the column.\n\nIn our approach, the column_name, table_name, and column_type are extracted through SQL queries. For describing the columns semantically, we adopt the \"semantic name\u201d as the 'column_desc' in the context of the Spider dataset, following the methodology outlined by (Li et al. 2023). For the BIRD dataset, we utilize the column descriptions as provided within the original dataset. Furthermore, we enhance the representation of each column by computing statistical features from the cell values, an extension to the \"cell value reference\" presented by (Li et al. 2023). Specifically, for text-based columns, we randomly sample cell values and return the first 100 characters; for numeric or date types, we calculate and return the maximum and minimum values. This enriched feature set aids in a deeper understanding and processing of column data.\n\nSearch Value(value) This tool is designed for searching the values within a column utilizing Elasticsearch, where only the text fields are indexed.\n\nFindShortest Path(start, end) This tool computes the shortest path between two nodes in a graph. It leverages the NetworkX (Hagberg, Swart, and Schult 2008) library, a powerful tool for the analysis of complex networks.\n\nExecuteSQL(sql) This tool executes a provided SQL query using the SQLite3 library in Python 3."}, {"title": "System Configurations", "content": "Table 12 presents the parameter configurations for invoking the OpenAI API."}, {"title": "Prompt Texts", "content": "Instruction Text For all datasets, we designed one instruction prompt, as shown in Figure 3\n\nAnnotated Exemplar For both the Spider and BIRD datasets along with their respective variants, we selected and annotated two representative cases that include complete interactive processes to serve as exemplars for in-context learning.\n\nIt is important to note that different exemplars were used for each dataset series, reflecting the common practice in the field where methods are often uniquely tailored to specific datasets. Furthermore, we have modified the questions to ensure comprehensive coverage of all tools utilized in the study.\n\nSpider Due to the presence of empty databases in the Spider train dataset, it is essential to create an exemplar that instructs the LLM to generate SQL queries that adhere to the correct schema. Thus, we selected one case each from the academic database and the activity database (which is empty), as shown in Figure 4 and Figure 5.\n\nBIRD We selected two cases from the address database of the BIRD training data for annotation, as shown in Figure 6 and Figure 7."}]}