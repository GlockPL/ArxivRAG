{"title": "Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering", "authors": ["Hongda Sun", "Yuxuan Liu", "Chengwei Wu", "Haiyu Yan", "Cheng Tai", "Xin Gao", "Shuo Shang", "Rui Yan"], "abstract": "Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The retrieve-then-read paradigm retrieves pertinent documents from an external corpus; and (2) the generate-then-read paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers. Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications.", "sections": [{"title": "1 INTRODUCTION", "content": "In the interdisciplinary realm of information system applications, open-domain question answering (ODQA) has emerged as a pivotal research spotlight. This task is intrinsically knowledge-intensive and focuses on answering factoid questions, enhancing its utility to transcend the constraints of predefined domains [5, 11, 29, 32]. Current ODQA methods follow two main paradigms in preparation for answering questions: (1) The retrieve-then-read paradigm retrieves pertinent evidence documents from an external corpus and generates an answer based on them [16, 18]. Since retrieval models often rely on well-curated corpora like Wikipedia, they can provide highly factual and accurate information about the question; (2) The generate-then-read paradigm directly employs language models to generate virtual documents [49], diversifying the evidence sources and enhancing answer coverage for the question.\nDespite the individual merits of both paradigms, neither can adequately address the multifaceted requirements for evidence. An intuitive solution is to integrate the strengths of these paradigms to collect evidence that combines factual reliability with diversity.\nTo this end, we propose LLMQA, a novel generalized framework that incorporates the strengths of retrieval-based and generation-based evidence. Specifically, we formulate the ODQA process into three fundamental steps: (1) Query expansion involves expanding the question by producing background passages or explanations, serving as generated-based evidence to enrich the context; (2) Document selection integrates retrieval-based evidence by reranking the retrieved documents, increasing their relevance to the answer; (3) Answer generation proceeds to generate the final answer based on comprehension of the question and obtained evidence.\nTo implement each step of ODQA, previous methods typically train specialized models for individual modules to obtain evidence documents and final answers [9]. Limited by the inherent capabilities of these models, jointly optimizing each module to improve overall performance remains challenging. Recent works have showcased the exceptional capabilities of large language models (LLMs) across various tasks [4]. Specifically for ODQA, which requires integration of text generation [9, 34, 45, 49], document ranking [9, 25, 26], and candidate evaluation [2, 51], the multiple aspects of capabilities of LLMs into each module. Therefore, we aim to instruct LLMs to play three roles within our proposed unified framework: generators, rerankers, and evaluators. By closely coordinating these roles and fostering collaboration among them, we can fully exploit their potential to enhance overall performance. The generator expands the query and provides comprehensive and pertinent information for answer generation; The reranker prioritizes retrieved documents to distill more valid and relevant documents as evidence; The evaluator engages in interacting with the generator and reranker, providing evaluative feedback to refine their outputs.\nThe quality of LLMs in playing their distinct roles hinges on the quality of the prompts used to define tasks and guide their behaviors. Therefore, the precision of obtained evidence is also sensitive to these prompts. To better automatically design prompts, we present a novel prompt optimization algorithm to enhance the performance of LLMs across various roles within our framework. During the ODQA generation process, we treat evidence (e.g., query expansion and selected documents) as latent variables and leverage variational inference to optimize role-playing prompts, guiding LLMs toward producing higher-quality evidence and answers.\nWe conduct experiments on widely used ODQA benchmarks: NQ, WebQ, and TriviaQA. The results show that our LLMQA advances the state-of-the-art performance on both answer accuracy and evidence quality. Compared with baselines, LLMQA achieves remarkable improvement in EM scores (4.0@TriviaQA, 2.7@WebQ, 3.1@NQ), demonstrating the effectiveness of multi-role LLMs for ODQA. The role of query expansion generator can achieve 73%,76%, and 87% recall scores for the answer in generated expansions. The role of reranker increases answer coverage by about 8.1%.\nTo sum up, our main contributions can be summarized as follows:\n\u2022 We propose LLMQA, a generalized framework model to formulate the ODQA process, which is a novel paradigm to combine the strengths of retrieval-based and generation-based evidence.\n\u2022 We effectively instruct LLMs to play three roles of generators, rerankers, and evaluators respectively, and integrate their collaborative interactions under our proposed unified framework,\n\u2022 We introduce a novel prompt optimization algorithm to guide LLMs in producing higher-quality evidence and answers. Extensive experimental results show that LLMQA advances the best performance in terms of both answer accuracy and evidence quality."}, {"title": "2 RELATED WORK", "content": "For collecting the related documents as evidence, existing methods can be categorized into the following two main paradigms:\nRetrieve-then-read paradigm. Pioneered by [5], most recent approaches consist of two main modules: The retriever first retrieves documents relevant to the given question from an external knowledge base. The reader then comprehends questions and retrieved documents and generates the corresponding answer. One branch focuses on improving the retriever. Sparse retrieval with inverted indexes (e.g., TF-IDF or BM25) is generally used in traditional approaches [40]. Dense retrieval using language models such as ORQA [20], DPR [18], RocketQA [35], ColBertQA [19], and ART [41] becomes dominant. The other branch focuses on enhancing the comprehension ability of reader to generate more accurate answers [6, 16]. With the development of LLMs, most readers are adopted from fine-tuned T5 [37] or InstructGPT [31].\nGenerate-then-read paradigm. Previous works have demonstrated that the knowledge preserved in LLMs can serve as a \"generative retriever\" [33, 36, 39]. Although many existing approaches adopt LLMs in ODQA, they cannot fully harness the generation capability of LLMs [17, 34, 45, 47]. GenRead is the first to explore the potential of the generation-based evidence for ODQA, which instructs an LLM to generate documents based on clusters of question-document pairs and the given question [49]. Then these generated documents and the question are fed into LLM together to produce the final answer.\nConsidering the limitations of a single paradigm, we propose to seamlessly integrate retrieval-based and generation-based evidence, effectively harnessing the capabilities of LLMs."}, {"title": "2.1 Open-Domain Question Answering", "content": "Recent advancements in model scales [8, 31] provide LLMs with impressive capabilities in text generation, ranking, and evaluation.\nGeneration capability of LLMs. Recent studies have highlighted the superior text generation capability of LLMs in few-shot and zero-shot scenarios [3, 4, 8, 52]. The knowledge stored in LLMs could be retrieved during inference [33, 39]. Hence, some studies directly prompt LLMs to generate answers to the question in ODQA [17, 34, 45, 47]. Other approaches utilize the generation capability to expand the query or enrich the context [9, 28, 30, 49].\nRanking capability of LLMs. Previous works show that compared to few-shot information extraction, LLMs are better at reranking"}, {"title": "2.2 Capabilities of LLMs", "content": "Previous works have emphasized that subtle differences in prompts could lead to tremendous performance degradation in generated results [13, 22, 53]. Consequently, prompt optimization has attracted great attention in recent years, with two primary approaches: manual design [38] or automatic generation [42]. Gradient-based prompt tuning can optimize prompts embedding in a continuous space [23, 24]. In contrast, discrete prompt optimization has been extensively studied including prompt scoring [10], prompt generation [13] and prompt paraphrasing [50]. Recently, Zhou et al. propose APE for automatic prompt optimization by iteratively selecting prompt candidates to maximize the potential score functions. DLN [44] steps further by viewing LLMs as language layers and prompts as learnable parameters.\nInspired by these methods, we present a novel prompt optimization algorithm to refine essential prompts for query expansion, document reranking, and answer generation, enabling LLMs to produce better evidence and answers."}, {"title": "2.3 Prompt Optimization", "content": "Previous methods collect evidence by retrieving or generating relevant background passages or explanations to facilitate accurate answer identification [5, 18, 31]. Expanding upon this concept, we formulate the generation process of ODQA as the following three fundamental steps: (1) Query expansion: We commence with the input question, designated as query q. To enrich the context and improve document selection and answer generation, we utilize knowledge stored in language models to generate additional background information, denoted as query expansion e; (2) Document selection: Leveraging both the query q and its expansion e, we initially retrieve the top-n documents that are relevant to answering the question as candidates. Subsequently, we compare these candidates to prioritize those documents most likely to contain the answer. Based on this criterion, we rerank these n candidates and retain top-k documents, represented as d, which collectively constitute the evidence in conjunction with query expansion (e, d); (3) Answer generation: Based on the query q and the derived evidence (e, d), we proceed to generate the final answer a in response to the question with a reader model.\nFurthermore, this generation process can be effectively formulated using a Bayesian graphical model that aligns closely with the three aforementioned steps, parameterized by the following probability distribution:\n$P(a|q) = \\sum_{e} \\sum_{d} P(e|q)P(d|q, e)P(a|q, e, d),$                                     (1)\nwhere we consider the evidence (e, d) as latent variables, which require to be optimized by maximizing this marginal likelihood. Consequently, the acquisition of the most appropriate evidence for question answering becomes a critical aspect of this task. Considering the prominent performance of LLMs on various tasks, we harness LLMs in multiple roles that collaborate with each other in the ODQA generation process. The framework overview of LLMQA is shown in Figure 2. In the subsequent sections, we will introduce in detail how to leverage the multi-role capabilities of LLMs to enhance the ODQA task."}, {"title": "3 METHOD", "content": "Generally, the questions posed in ODQA datasets are brief and concise, indicating that relying solely on the question itself as a query can lead to a substantial challenge: inadequate query context makes it difficult to support accurate document selection and answer generation. To address this challenge, we add a pivotal step known as query expansion that aims to enrich the original question with a broader context. The generated expansions are mainly used to analyze the key points required to answer a given question and provide sufficient background information for subsequent steps. In this process, we instruct an LLM to play the role of generator leveraging its powerful context understanding and text generation capabilities. Specifically, we employ an LLM-based expansion generator $G_e$ to facilitate the query expansion step. Given a question q, its query expansion e can be generated by\n$e = G_e(q; \\theta_e),$                                             (2)\nwhere $\\theta_e$ represents the prompt to instruct the query expansion."}, {"title": "3.1 Task Formulation", "content": "In addition to the query expansion, relevant documents are more commonly used as evidence to include accurate answers to the"}, {"title": "3.2 Query Expansion", "content": "To identify the most appropriate documents, we divide this document selection process into two distinct stages:\n(1) Coarse-grained retrieval of top-n documents: we first retrieve a set of top-n documents that are potentially relevant to the given question by employing established information retrieval techniques such as DPR [18] or BM25 [5]. These retrieval methods provide an initial score for each candidate to describe the relevance between documents and questions. However, such methods may not always capture nuanced semantic relationships between the query and documents, leading to false positives or irrelevant documents in the initial set.\n(2) Fine-grained reranking of top-k documents from n candidates: we proceed with the reranking of documents to ensure that those more likely to contain the answer are prioritized. This stage involves comparing the documents to determine which ones exhibit higher quality and relevance to the query. Inspired by LLM-based ranking approaches [9, 25, 26], We instruct LLM to play the role of document reranker $R_d$ for further screening out top-k (k < n) documents from the initial pool of n candidates. Considering the limitation on input tokens for LLMs, we iteratively rerank a subset of documents each time and complete the reranking of all candidates through a sliding window. Specifically, we set the window size to w and the step size to l. We start from the last position of the initially sorted documents. In each iteration, we focus on comparing w documents within the sliding window and reorder the documents based on their likelihood of containing the answer. With the sliding window moving forward by I steps, thus the top $\\frac{w}{I}$ reranked documents in the original window are reserved and l new documents are added, then the next w documents can be reordered. This iterative process continues until the sliding window reaches the front, and we consider the first k = w - l documents as the final evidence documents d. Overall, this document selection process can be simplified as:\n$d = R_d(q, e; \\theta_d),$                                           (3)\nwhere $\\theta_d$ denotes the prompt for $R_d$ to ensure that documents are ranked in alignment with the desired relevance and quality."}, {"title": "3.3 Document Selection", "content": "Based on the query q, and the evidence (e, d), the final step in ODQA is to generate the final answer with the integration and comprehension of pertinent information within the evidence. The evidence can encompass essential information that directly provides the answer to the question, or it may comprise an analysis and explanation necessary for formulating the answer. Consequently, the central objective of answer generation is to employ a reader model for the systematic extraction and comprehension of valuable insights from the evidence context. We utilize an LLM-based reader $G_a$ to generate a precise and dependable response as the predicted answer to the question, and formulate this process as:\n$a = G_a(q, e, d; \\theta_a),$                                            (4)\nwhere $\\theta_a$ indicates the prompt for answer generation to ensure that the generated answer can align with the context and requirements of the original question and its evidence."}, {"title": "3.4 Answer Generation", "content": "As shown in Figure 1, evaluators also play a crucial role in query expansion and document reranking, engaging in a dynamic interaction with both the generator and reranker. Leveraging the advanced capabilities of LLMs to evaluate text quality under specific standards, we can instruct LLMs to play the role of evaluators to assess the performance of the generator and the reranker. The primary"}, {"title": "3.5 Evaluators for Generation and Reranking", "content": "objective of evaluators is to assign quality scores to multiple candidates generated by the generator and reranker. These scores reflect the likelihood that each candidate is appropriate and accurate for specific conditions or requirements. For a given question, we employ the expansion evaluator $S_e$ to individually score each candidate expansion ranging from 0 to 1, which is used to assess the degree of their relevance and logical consistency. Similarly, we use the reranking evaluator $S_r$ to score different top-k reranking candidates generated by the reranker $R_d$, assessing the contribution of each ranking result to answering the question. The scoring process of evaluators $S_e$ and $S_r$ can be formulated as:\n$s_{ej} = S_e (e_j; G_e(q)),$                                           (5)\n$s_{d_i} = S_r(d_j; R_d(q, e)),$                                          (6)\nwhere $s_{e_j}$ and $s_{d_i}$ represent the scores assigned to the j-th candidate of generated query expansion and reranked documents. These scores serve as critical metrics for evaluating the performance of the generator and reranker and further promoting overall generation and ranking capabilities of LLMs."}, {"title": "3.6 Prompt Optimization", "content": "The role-play performance of the generator and reranker still heavily relies on the prompt design in each ODQA generation process. Therefore, we explore how to design better role-play prompts or expansion generation $\\theta_e$, document reranking $\\theta_d$, and answer generation $\\theta_a$ to fully exploit the potential of LLMs. We propose a novel algorithm to enable prompt optimization under the unique graphical model structure of ODQA. Throughout the ODQA generation process, we do not require the LLM parameters, but instead treat three natural language prompts as learnable parameters. In Equation (1), the distributions of latent variables e and d are determined by these prompts and need to be approximated by probabilistic inference techniques. To ensure consistency with the graphical model, we propose to use variational inference to learn the hidden distributions and optimize prompts. We denote the prior distribution as $P_e$ and the posterior distribution as $P_f$, and the original log-likelihood could be bounded by the following ELBO:\n$\\log P(a|q) > \\sum_e \\sum_d P_e (e|q, a) P_a (d|q, e, a) \\log \\frac{P_{oe} (e|q)P_{od} (d|q, e) P_{oa} (a|q, e, d)}{P_{\\phi e} (e|q, a) P_{\\phi d} (d|q, e, a)}$                                                    (7)\nAs shown in Algorithm 1, for the question q, we use predefined $G_e$, $R_d$ and $G_a$ to sequentially simulate the priors $P_{oe}$, $P_{od}$, and $P_{oa}$, and generate the query expansion $\\bar{e}$, the reranked documents d and the predicted answer a during forward inference. Next, to approximate the posteriors $P_{\\phi e}$ and $P_{\\phi a}$, we consider the following two aspects: (1) We add the ground-truth target as an additional condition to estimate the posteriors; (2) We sample several posterior candidates near the prior to ensure low Kullback-Leibler (KL) divergence between them in the space of discrete texts. Denoting the prior reranked documents as $d = (d_1, d_2, ..., d_{k-1}, d_k)$, the i-th posterior reranked documents can be denoted as $d_i = (d_1, d_2,..., d_{k-1}, d_{k+i})$, where only the \"last document\" in the list is replaced. Then we use evaluators $S_r$ and $S_e$ to score each posterior candidate for estimating $P_{\\phi a}$. The best posterior $d^*$ among these candidates is selected as the current \"ground-truth\" reranking documents. The posterior query expansions are generated by a minor edit of the prior expansion, then the best posterior $e^*$ is selected by analogy.\nSubsequently, we define a backward process to update prompts. For the answer generation prompt $\\theta_a$, we sample K candidates near it using an updating function $U_a(q, \\bar{e}, d, \\bar{a}, a)$, to guide prompts to update in a direction that brings the predicted answer a closer to the actual answer a. Then all the previous posterior candidates are used to estimate ELBO, and the best $\\theta_a$ to maximize ELBO can be selected as the refined prompt for answer generation. Similar processes are introduced to refine prompts $\\theta_d$ and $\\theta_e$, while updating functions $U_d$ and $U_e$ are used to guide the directions to refine document reranking and query expansion."}, {"title": "4 EXPERIMENTS", "content": "We select three widely used ODQA benchmarks to evaluate the model performance of baselines and our LLMQA: (1) WebQ (WebQuestions) is a dataset that consists of questions obtained using the Google Suggest API, with the answers being entities from Freebase. (2) NQ (Natural Questions) is a dataset generated from real Google search queries, and the answers are spans within Wikipedia articles. (3) TriviaQA is a collection of trivia questions sourced from trivia and quiz-league websites. Statistics of these three datasets are available in Appendix."}, {"title": "4.1 Experimental Setup", "content": "The overall performance of the experiment is shown in Table 1. Compared with the baselines without LLMs, our proposed LLMQA exhibited a notable improvement over three datasets (10.3@TriviaQA, 6.3@WebQ, 7.4@NQ), which strongly demonstrated the effectiveness of the LLM on the ODQA, indicating that the effect of model scale on the final results is remarkable. Our LLMQA surpassed FiD-xl by 8 on average of three datasets, even though the documents we used are less than it. Thus, different role-play LLMs can be competent with previously specifically designed models.\nCompared with the baselines employing LLMs as generators, our LLMQA also achieved considerable performance improvement. Both GenRead and EAR+FiD utilize the generation capability of LLMs to generate documents or query expansions. The enhancement of our approach primarily leverages the collaboration between multiple role-playing LLMs. In addition to the query expansion used in our approach, we also adapted LLMs to rerank the retrieved documents. The remarkable improvement fully demonstrated that multiple roles can interact and collaborate with each other and fulfill the tasks well under specific instruction."}, {"title": "4.2 Overall Performance", "content": "In this section, we eliminate the generator, reranker, and evaluator, respectively, and explore to what extent the three aspects of LLM"}, {"title": "4.3 Ablation Study", "content": "capabilities have an impact on the ODQA performance. In addition, we validate the effectiveness of the proposed prompt optimization. Table 2 shows that the generator role of LLMs has the most significant impact among the three different roles played by LLMs, which indicates that the query expansion can serve as an auxiliary document. The reranker contributes to the ODQA as well because the reranked documents are more relevant to the question. The feasibility of the evaluator has also been demonstrated as it can estimate the evidence quality and select the most suitable one. Our experiment on prompt optimization shows that the quality of the prompt design directly affects the performance of role-play LLMs for the final result and that the prompts on discrete space could be optimized as well."}, {"title": "4.4 Case Study and Error Analysis", "content": "Case study of evidence and answer. In addition to prompt optimization, we also focus on the specific performance of evidence quality and answer generation during the inference process. We choose GenRead as a strong baseline for comparison. Table 3, all the top-10 evidence documents of LLMQA contain answers, which are highly relevant to the given question resulting in accurate answer prediction. However, the virtual documents generated by GenRead introduce an inaccurate year 1951 and miss the golden answer. This indicates that reranking retrieved documents can help improve the evidence quality and answer accuracy.\nCase study of prompt optimization. We analyze the differences between the prompts for query expansion and document reranking after optimization. shows that compared to the initial prompts, optimized prompts can include more details and insights for describing the instruction. As for the expansion prompt, a more detailed role-play description and an alternative instruction to solve the task were added. As for the reranking prompt, some of the ambiguous content in the prompt has been refined after optimization. As a consequence, our proposed prompt optimization method can achieve more detailed, instructive, and explicit prompts.\nError Analysis. Although we achieve the most advanced results on evidence quality and answer accuracy, some issues remain challenging. The challenges may contain contradictions with facts and world knowledge, and they may have led to incorrect predictions or reasoning results. For instance, Table 4 displays a typical failure case where both LLMQA and GenRead struggle to capture precise evidence, leading to low evidence quality and incorrect answer predictions. Despite these ongoing challenges, our LLMQA is still the best choice in terms of overall performance on the ODQA task."}, {"title": "4.5 Further Analysis", "content": "Analysis of Evidence Quality. We estimate the evidence quality using answer recall on the top-k selected documents. We compare our proposed LLMQA with GenRead [49] on three datasets. shows that our LLMQA achieves the highest recall on all of top-k settings over three datasets. The results show that relying solely on LLM-generated documents is insufficient. While hybrid utilization of both generated expansion and retrieved documents can gain tremendous answer recall increase, contributing to the final performance improvement.\nQuality of Query Expansions. We first evaluate the quality of query expansions generated by LLMs. In the query expansion procedure, we generate 10 candidates and instruct LLMs to estimate the candidates according to the specified rules. The left part in shows the recall for the highest-scored K expansions. Most of the generated query expansions have already contained the answer as the large amount of knowledge that may cover the answer has been stored in the parameters of LLMs during pre-training.\nWe also analyze the number of expansions in our proposed approach. The right part in shows the EM score for different"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose LLMQA that formulates the ODQA generation process as three fundamental steps: query expansion, document selection, and answer generation, which combines the superiority of both retrieval-based and generation-based evidence. Since LLMs have showcased remarkable performance on generation, ranking, and evaluation, we use a generalized framework to integrate multi-role LLMs: generator, reranker and evaluator, which collaboratively contribute to each key step in the ODQA generation process. Furthermore, we design a novel prompt optimization algorithm, to address the limitation of prompt sensitivity, guiding LLMs in producing higher-quality evidence and more accurate answers."}, {"title": "ACKNOWLEDGEMENTS", "content": "This work was supported by the National Natural Science Foundation of China (NSFC Grant No. 62122089, U2001212, 62032001, and 61932004), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, and Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the \"Double-First Class\" Initiative, Renmin University of China, the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China."}, {"title": "B RESULTS OF ZERO-SHOT SETTING", "content": "In principle, our proposed method supports only using LLMs to play different roles for the ODQA task. However, previous methods [9, 49] often use the supervised setting to fine-tune the answer generator to achieve SOTA performance. To facilitate a fair comparison, we follow this setting to fine-tune our answer generator and obtain the main results in Table 1.\nHere, we add the zero-shot setting to directly use the same LLM (e.g., gpt-3.5-turbo) in all modules including answer generation. The baseline methods, including BM25 [5] / DPR [18] / Contriever [15] + InstructGPT [31], share the same input format as Genread [49]. All the baseline results are adopted in [49]. The results in Table 9 are as expected: The model performance in the zero-shot setting is generally worse than the supervised setting, but our LLMQA can still outperform all baselines in the same setting."}, {"title": "C COMPLEXITY ANALYSIS.", "content": "Regarding the number of model parameters to be learned, we compare them from two aspects: evidence collection and answer generation. Some previous methods require specifying specialized models to collect evidence (e.g., document retrieval and extension generation), which introduces the training cost for specialized models in evidence collection. Our framework is instead based on guiding LLMs to play different roles, with evidence collection only involving the inference process. Since the inherent capabilities of the reader (answer generator) have an important impact on the performance of the ODQA task, state-of-the-art ODQA performance comes from fine-tuning the reader. Following [49], we employ T5-xl (3B) as the backbone of the answer generator, whose training cost is comparable to the baseline."}, {"title": "D DETAILS OF PROMPT OPTIMIZATION", "content": "The comparison results before and after prompt optimization for query expansion and document reranking are depicted in Figure 5."}]}