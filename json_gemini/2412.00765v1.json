{"title": "SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts", "authors": ["Aihua Pei", "Zehua Yang", "Shunan Zhu", "Ruoxi Cheng", "Ju Jia"], "abstract": "Traditional methods for evaluating the robustness of large language models (LLMs) often rely on standardized benchmarks, which can escalate costs and limit evaluations across varied domains. This paper introduces a novel framework designed to autonomously evaluate the robustness of LLMs by incorporating refined adversarial prompts and domain-constrained knowledge guidelines in the form of knowledge graphs. Our method systematically generates descriptive sentences from domain-constrained knowledge graph triplets to formulate adversarial prompts, enhancing the relevance and challenge of the evaluation. These prompts, generated by the LLM itself and tailored to evaluate its own robustness, undergo a rigorous filtering and refinement process, ensuring that only those with high textual fluency and semantic fidelity are used. This self-evaluation mechanism allows the LLM to evaluate its robustness without the need for external benchmarks. We assess the effectiveness of our framework through extensive testing on both proprietary models like ChatGPT (OpenAI, 2024) and open-source models such as Llama-3.1 (Touvron et al., 2024), Phi-3 (Research, 2024), and Mistral (Mistral and contributors, 2024). Results confirm that our approach not only reduces dependency on conventional data but also provides a targeted and efficient means of evaluating LLM robustness in constrained domains.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have garnered significant attention due to their exceptional performance across various natural language processing (NLP) tasks. However, as these models are widely applied in critical domains, they also face the risk of adversarial attacks triggered by prompts. Adversarial attacks aim to mislead models into making incorrect judgments through carefully designed prompts, potentially causing severe damage to users. Therefore, it is necessary to assess the robustness of models against adversarial attacks using robustness evaluations.\nExisting adversarial robustness evaluation frameworks for large language models (LLMs), like AdvGLUE (Wang et al., 2021) and PromptAttack (Xu et al., 2023), use specialized benchmark datasets that require extensive manual annotation. This not only limits their applicability but also increases operational costs. Moreover, when LLMs are used in constrained domains such as medicine or biology, the mismatch between generic benchmark datasets and the constrained context can lead to inaccurate robustness evaluations. These limitations decrease practicality of the frameworks and complicate the robustness evaluation of LLMs.\nThis paper proposes an adversarial attack framework (SelfPrompt) that requires the evaluated LLMs themselves to utilize domain-constrained knowledge guidelines to generate and poison prompts from knowledge graph triplets, thereby assessing their robustness. The generation of adversarial prompts is meticulously refined to optimize quality and evaluation effectiveness, while ensuring that the quality of adversarial prompts generated by different large language models is relatively consistent. We apply this framework to generate prompts from both general and constrained domain knowledge graphs, evaluating the resilience of multiple LLMs under adversarial attack conditions. Specifically, our contributions include:\n\u2022 This paper introduces a framework that allows large language models (LLMs) to autonomously evaluate their robustness in constrained domains by generating adversarial prompts from domain-specific knowledge graph triplets. This method enhances the practical relevance of robustness evaluations by tailoring the prompts to the specific operational domains of the LLMs.\n\u2022 To ensure stable quality of adversarial prompts across various large models and maintain comparability in their robustness evaluations, we employ a filter. This filter assesses the text fluency and semantic fidelity of the prompts, allowing us to refine and exclude those that do not meet our quality criteria.\n\u2022 We confirm that the robustness of large language models is influenced by the domain of knowledge corresponding to the prompts. The robustness of the same large language model measured on general or constrained domain knowledge graphs is not similar. While models with larger parameters in the same series tend to exhibit stronger robustness in general domains, this is not necessarily the case in constrained domains. Therefore, it is crucial to consider the differences in knowledge domains when evaluating robustness of LLMs."}, {"title": "Related Works", "content": ""}, {"title": "Robustness Evaluation of LLMS", "content": "Large language models (LLMs), such as the ChatGPT family and the Llama family, have attracted much attention for their excellent performance in a variety of natural language processing tasks (Touvron et al., 2023; Brown et al., 2020). However, as these models are widely used in critical domains applications, evaluating their robustness has also become a hot research topic. There are four main streams of work (Li et al., 2023; Ailem et al., 2024; Zhuo et al., 2023) on robustness research: robustness under distribution shift (Yang et al., 2023), robustness to adversarial attacks (Wang et al., 2023b; Zhu et al., 2023), robustness to prompt formats and instruction templates (Mizrahi et al., 2023; Voronov et al., 2024; Weber et al., 2023) and robustness to dataset bias (Gururangan et al., 2018; Niven and Kao, 2019; Le Bras et al., 2020). Our work focus on evaluating robustness to adversarial attacks of LLMs.\nAdversarial attacks aim to mislead the model to make wrong judgments through well-designed inputs, while adversarial robustness evaluation attempts to determine and enhance robustness of the model to these attacks. Current robustness evaluation frameworks for LLMs are mainly based on specially constructed benchmark datasets (e.g., the GLUE dataset (Wang et al., 2018) and ANLI dataset (Nie et al., 2020)) for evaluating natural lan-"}, {"title": "Adversarial Prompt Generation from Knowledge Graphs", "content": "In evaluating robustness of LLMs, we need to know whether they have such knowledge and whether they can accurately express their knowledge. Knowledge graphs can help us generate adversarial attack prompt with different diversities and complexities. Knowledge graph (KG) is a graph structure for representing knowledge, where nodes represent entities or concepts and edges represent relationships between these entities or concepts.\nSome works use different methods to utilize triplet from knowledge graphs generating questions (Seyler et al., 2017; Kumar et al., 2019; Chen et al., 2023). Some works utilize the ability of LLMs to generate questions from KGs (Guo et al., 2022; Axelsson and Skantze, 2023). Recent works (Luo et al., 2023, 2024) also discussed on evaluating factual knowledge of LLMs with the diverse and well-coverage questions generated from KGs and how KGs can be used to induce bias in LLMs."}, {"title": "Few-Shot Strategy", "content": "As the popularity of machine learning models, especially large language models (LLMs), continues to grow, the few-shot learning strategy has also gar-"}, {"title": "Methodology", "content": "In this section, we first illustrate a robustness self-evaluation framework for large language models based on domain-constrained knowledge guidelines, utilizing adversarial prompt attacks, which we call SelfPrompt. Then, we employ a filter module to ensure the text fluency and semantic fidelity of the adversarial prompts generated by SelfPrompt. Finally, we introduce the metrics for evaluating the robustness of LLMs. All the prompt templates mentioned in this section can be found in the Appendix C."}, {"title": "Framework of SelfPrompt", "content": "Initially, we process the triplets of the knowledge graph to assign them distinct labels. Subsequently, we transform these triplets into original prompts. Finally, these original prompts are converted into adversarial prompts. Next, we provide a detailed description of each step in this process.\nLabeling Knowledge Graph Triples. We let $D = \\{(S_i, P_i, O_i)\\}_{i=1}^N$ be the domain-constrained knowledge graph dataset. For each triplet $t = (s, p, o) \\in D$, s refers to the subject of this triplet, while p and o refer to the predicate and object of the triplet, respectively. For example, for the triple (Alan Turing, field of work, logic), it has the subject (Alan Turing), the predicate (field of work), and the object (logic). This triple means that Alan Turing works in the field of logic.\nConsidering the structural characteristics of the triples, each triple is labeled with one of the following three labels: true, entity_error, and predicate_error. By default, all triples extracted from the knowledge graph are initially labeled as true. For each triplet $t = (s,p, o) \\in D$, its label l is randomly assigned to one of the three labels with equal probability, generating incorrect subject, predicate, and object, denoted as $s', p'$, and $o'$, respectively. The modified triple t' is:\n$t' =\\begin{cases}\n(s, p, o), & l = true \\\\\n(s, p', o), & l = predicate\\_error \\\\\n(s', p, o) \\text{ or } (s, p, o'), & l = entity\\_error\n\\end{cases}$    (1)\nFor example, for the original triple labeled as true, (Alan Turing, field of work, logic); if it is to be labeled as predicate_error, it can be modified to (Alan Turing, position played on team, logic), which means Alan Turing plays in the logic position; the modified predicate is used to describe the position or specialism of a player on a team. If it is to be labeled as entity_error, the original triple can be modified to (Richard Wagner, field of work, logic) or (Alan Turing, field of work, Opera). The labeled knowledge graph dataset is $D' = \\{(t', l_i)\\}_{i=1}^N$\nGenerating Original Prompts. LLMs are more suitable for handling continuous prompt text rather than structured triplets. For converting triplets into prompts, we offer two strategies: Template-based and LLM-based. The template-based strategy uses templates built into the predicates of the triplets to generate original prompts by replacing these placeholders with specific names. For example, for the triplet (Alan Turing,field of work, logic), the template built into the predicate field of work is '[X] works in the field of [Y].' By replacing [X] with Alan Turing and [Y] with logic, the sentence \"Alan Turing works in the field of logic\" is generated. The LLM-based strategy involves feeding triplets to the LLM whose robustness is being evaluated, which then generates descriptive sentences based on these elements."}, {"title": "Filter Module", "content": "In the SelfPrompt framework described in section 3.1, we use the LLM being evaluated for robustness to generate both original prompts and adversarial prompts. The quality of adversarial prompts generated by different LLMs varies, posing challenges for the cross-comparison of robustness evaluation results across different LLMs. To address this issue, we designed a filter module to eliminate adversarial sentences that do not meet the criteria for text fluency or semantic fidelity. This ensures that the adversarial prompts generated by different LLMs are of comparable quality, thereby enhancing the reliability and comparability of the robustness evaluation results. For an original sentence $s_{ori}$ and its corresponding adversarial sentence $s_{adv}$, the text fluency of $s_{adv}$ is $tf(s_{adv})$, the semantic fidelty of $s_{adv}$ relative to $s_{ori}$ is $sf(s_{adv}, s_{ori})$. Assume that the filtering thresholds for text fluency and semantic fidelity are $T_t$ and $T_s$ respectively, the formula for the filter module is as follows:\n$f(s_{adv}, s_{ori}) = (tf(s_{adv}) > T_t) \\land (sf(s_{adv}, s_{ori}) > T_s)$ (2)\nThe function tf(s) calculates the text fluency of a sentence s by computing the perplexity of a language model's output for s. The perplexity is defined as:\n$P(s) = e^{Loss(s)}$ (3)\nwhere Loss(s) is the negative log-likelihood loss of predicting the tokens in s (Goodfellow et al., 2016). To manage the typically large values of perplexity, a logarithmic transformation is applied:\n$LogP(s) = log(P(s) + e - 1)$ (4)\nThe text fluency score is computed as:\n$tf(s) = \\frac{e^{-k/LogP(s)} - 1}{e^{-k} - 1}$ (5)\nwhere $k > 0$; in this experiment, k is set to 5.\nThe function $sf(s_{adv}, s_{ori})$ computes the semantic fidelity between $s_{adv}$ and $s_{ori}$ by first calculating the cosine similarity between their embedding vectors $v_{adv}$ and $v_{ori}$, where:\n$v_{adv} = get\\_embedding(s_{adv})$ (6)\n$v_{ori} = get\\_embedding(s_{ori})$ (7)\nThe cosine similarity (Manning et al., 2008) is given by:\n$cos\\_sim(v_{adv}, v_{ori}) = \\frac{v_{adv} \\cdot v_{ori}}{||v_{adv}|| ||v_{ori}||}$ (8)\nIt then scales the cosine similarity to the range [0, 1] using the formula:\n$sf(s_{adv}, s_{ori}) = \\frac{e^{t - cos\\_sim(v_{adv}, v_{ori})} - e^{-t}}{e^{t} - e^{-t}}$  (9)\nwhere t > 0. In this experiment, t is set to 5."}, {"title": "Metrics for Robustness Evaluation", "content": "From a knowledge graph triplet dataset $D = \\{(S_i, P_i, O_i)\\}_{i=1}^N$, we generate an original prompt set O and a corresponding adversarial prompt set A of size M (where 0 < M < N, and all elements in set A must pass the filter module test described in section 3.2). Let the accuracy of the LLM on the classification task for set O be $ACC_O$ and for set A be $ACC_A$ (both $ACC_O$ and $ACC_A$ range from 0 to 1). The robustness metric $R(ACC_A, ACC_O)$ evaluates a model's ability to handle adversarial prompts. It is defined as:\n$R(ACC_A, ACC_O) = sin\\left(\\frac{\\pi}{2} ACC_A \\left(\\frac{1}{ACC_O}\\right)^j\\right)$ (10)\nwhere ACCA is positively correlated with robustness since a higher ACCA reflects better resistance to adversarial attacks. Conversely, ACC is negatively correlated with robustness because, in most cases, ACCA < ACCO. When ACCA is the same, a lower ACCO indicates that the LLM is less influenced by adversarial attacks, leading to a higher robustness score. In this experiment, the value of j is set to 1.7, where $j \\geq 1$."}, {"title": "Experiments", "content": "In this section, we demonstrate that our proposed SelfPrompt framework can perform adversarial attacks on large language models such as ChatGPT (OpenAI, 2024) and Phi-3 (Research, 2024), and enable self-evaluation of their robustness based on the results. Additionally, we conduct extensive evaluation experiments on each module within the SelfPrompt framework."}, {"title": "Arrangements", "content": "In this subsection, we present the basic arrangements of the experiments, including the datasets used, the large language models employed, and settings of the filter module.\nDatasets. We utilize three knowledge graphs (KGs) to generate factual questions: T-REx (Elsahar et al., 2018), which serves as a general-domain KG, and WikiBio (Sung et al., 2021) and ULMS (Bodenreider, 2004), which are focused on constrained domains in biology and medicine, respectively. Each predicate in these KGs is paired with a dedicated template, which facilitates template-based original prompt generation within the SelfPrompt framework. For more details about the datasets and their predefined templates, please refer to appendix.\nLarge Language Models. Our experiments leverage a range of large language models across several series: GPT-40 (OpenAI, 2024) (including GPT-40 and GPT-4o-mini), Gemma2 (Gemma Team, 2024) (with 2B and 9B parameter versions), Phi-3 (Research, 2024) (comprising Phi-3-mini with 3.8B parameters and Phi-3-small with 7B parameters), Llama-3.1 (Touvron et al., 2024) (8B parameters), and Mistral (Mistral and contributors, 2024) (7B parameters). Variants with different parameter scales within the same model series are employed to examine whether the SelfPrompt framework's evaluation results align with the expectation that \"larger models exhibit greater robustness under comparable conditions, particularly when evaluated on general domain datasets\", thereby validating the soundness of the evaluation metrics. Meanwhile, models with similar parameter sizes from different series are used to facilitate cross-series comparisons of robustness.\nFilter Module Setting. To determine the appropriate values for the two thresholds, $T_t$ and $T_s$, in the filter module, we use a small sample (500 samples per round) generated by various LLMs and different knowledge graph datasets to produce the sentences required for adversarial prompts. We then measure their text fluency and semantic fidelity. The corresponding box plots of the data are presented below.\nIn Figure 3 and Figure 4, T2P indicates which strategy was used for generating the original prompts. Unless otherwise specified, the template-based strategy is generally applied. As shown in the figures, text fluency is significantly affected by different constrained domains, while semantic fidelity is more influenced by different LLMs; these two metrics are suitable as filtering criteria in the fidelity module. To select high-quality adversarial"}, {"title": "Robustness Evaluation", "content": "Table 1, Table 2, and Table 3 present the robustness evaluation results for selected large models, the accuracy of the LLM on the classification task for set O (ACCO), and for set A (ACCA), respectively. The test data for the remaining models (including Mistral-7B, Llama-3.1-8B, ChatGPT-40, and ChatGPT-40-mini) can be found in the Appendix B.\nAs shown in Table 1, when tested on knowledge graph datasets in the general domain, the performance of large language models aligns with the prediction that \"within the same series, larger models exhibit greater robustness.\" This observation validates the effectiveness of our metrics for evaluating model robustness. However, on datasets in constrained domains, the results are not always consistent with this trend. For Phi-3 models, the prediction that larger models are more robust generally holds; even in cases where smaller models show greater robustness, the difference is marginal.\nIn contrast, for the Gemma2 series, smaller models achieve better robustness evaluation results. By comparing Table 2 and Table 3, it can be seen that the larger models in the Gemma2 series experience a more significant drop in accuracy when facing adversarial attacks (e.g., for the UMLS dataset, the accuracy drops of the Gemma2-2B and Gemma2-9B models are 0.026 and 0.049, respectively; for the WikiBio dataset, the drops are 0.036 and 0.047, respectively). Thus, the smaller models in the Gemma2 series are less affected by adversarial attacks and therefore demonstrate greater robustness. This could be attributed to the smaller models' limited understanding of specialized domain texts, making them relatively less susceptible to adversarial statements. These findings underscore the necessity of evaluating the robustness of large models in domain-constrained scenarios."}, {"title": "Experimental Analysis of SelfPrompt", "content": "The robustness evaluation of large language models (LLMs) reveals distinct effects based on the strategies used for generating original prompts (Template-based vs. LLM-based) and whether the few-shot approach is applied in constructing adversarial prompts. Tables 1, 2, and 3 highlight these differences within the same model series.\nFor generating original prompts, the impact of template-based and LLM-based strategies differs between models in the same series. In the Gemma2 series, the robustness scores for Gemma2-2B and Gemma2-9B under the template-based strategy without few-shot on the T-REx dataset are 0.662 and 0.679, respectively. This suggests that the larger model, Gemma2-9B, benefits slightly from more structured input. However, when using the LLM-based strategy, which introduces more variability, the robustness score for Gemma2-9B on the UMLS dataset drops to 0.530, closer to Gemma2-2B's 0.534. This convergence suggests that more diverse prompts challenge the larger model's robustness. Table 2 shows a similar trend in accuracy ACCO, where Gemma2-2B and Gemma2-9B show reduced differences when moving from template-based to LLM-based prompts, highlighting the impact of input variability. In the Phi-3 series, a similar pattern is observed. Under the template-based strategy on the WikiBio dataset, Phi-3-mini and Phi-3-small achieve robustness scores of 0.534 and 0.566, respectively, indicating a benefit for the larger model. However, under the LLM-based strategy, Phi-3-mini's robustness score drops more sig-"}, {"title": "Conclusion", "content": "This paper introduces SelfPrompt, a framework for autonomously evaluating the robustness of large language models (LLMs) using domain-constrained knowledge guidelines and refined adversarial prompts. Our experiments confirm that the proposed method provides a reliable and effective evaluation of LLM robustness across various domains, demonstrating that larger models generally show greater robustness in general settings, while results may vary in domain-specific scenarios. Future work could explore expanding this framework to cover more diverse knowledge graphs and adaptive prompt generation techniques."}, {"title": "Limitations", "content": "The limitations of our work includes:\n\u2022 Types of problems for evaluating LLM robustness. In the SelfPrompt framework, we require the LLM to perform classification tasks to evaluate the robustness of large language models; in future research, we plan to enrich the types of problems by including types such as short answer questions and true/false questions, to conduct a more comprehensive evaluation of the LLM of robustness.\n\u2022 The SelfPrompt framework relies on existing knowledge graphs. When suitable knowledge graphs are lacking in a specific domain, constructing such knowledge graphs for that do-"}, {"title": "Experimentation Details", "content": "In this experiment, we divide the knowledge graph dataset into two categories based on the domain of knowledge represented by the knowledge graphs, including the general domain knowledge graphs and the constrained domain knowledge graphs. The general domain knowledge graph datasets is T-REx; the constrained domain knowledge graph datasets include UMLS and WikiBio."}, {"title": "Dataset", "content": "\u2022 T-REx. (Elsahar et al., 2018) Originating from Wikipedia, this is a general domain knowledge graph that records a large number of triplets belonging to various fields.\n\u2022 UMLS. (Bodenreider, 2004) This is a constrained-domain knowledge graph in the medical field, constructed by experts in the domain, and it contains information about various medical concepts and their relationships.\n\u2022 WikiBio. (Sung et al., 2021) This dataset is constructed by extracting biological instances from Wikidata and is a constrained-domain knowledge graph in the field of biology."}, {"title": "Loss Function and Cosine Similarity Used in Filter Module", "content": "Loss Function. The loss function is a mathematical function that measures the difference between the predicted outputs of a model and the actual outputs (ground truth). The Cross-Entropy Loss is commonly used in the context of language models. It is defined as:\n$Loss(s) = -\\sum_{i=1}^N log P(x_i | X_{<i})$ (11)\nwhere xi is the i-th token in a sequence, and $P(x_i|x_{<i})$ is the conditional probability of the token given all previous tokens (Goodfellow et al., 2016).\nCosine Similarity. Cosine similarity is a metric used to measure how similar two vectors are, irrespective of their magnitude. It is often used in natural language processing for comparing the similarity between text embeddings. The cosine similarity between vectors A and B is defined as:\n$Cosine Similarity(A, B) = \\frac{A \\cdot B}{||A|| ||B||}$ (12)\nwhere A \u00b7 B is the dot product of vectors A and B, and ||A|| and ||B|| are the magnitudes (norms) of vectors A and B. This metric ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 means they are orthogonal (dissimilar), and -1 means they are diametrically opposed. (Manning et al., 2008)."}, {"title": "Implementations", "content": "Large Language Model. We utilize several models from the ChatGPT family (OpenAI, 2024), including GPT-40 and GPT-40-mini. The large language models were accessed via paid APIs to complete relevant robustness evaluation tasks. We also used several open-source models, including Gemma2 (Gemma Team, 2024) (with 2B and 9B parameter versions), Phi-3 (Research, 2024) (comprising Phi-3-mini with 3.8B parameters and Phi-3-small with 7B parameters), Llama-3.1 (Touvron et al., 2024) (8B parameters), and Mistral (Mistral and contributors, 2024) (7B parameters). These open-source models were run locally with FP16 precision on a single RTX-4090 GPU.\nPrompt Generation and Response Processing. We set the ratio of the three labels \"true,\" \"entity_error,\" and \"predicate_error\" for the generated prompts to 1:1:1. To extract the classification results from responses of the LLM for the classification task, we employed string matching. If a response matches one of the aforementioned three labels and the label is the correct one, classification of the LLM is deemed correct; otherwise, it is considered incorrect. For each large model on each knowledge graph dataset, we generated 1,000 adversarial prompts for experiments under each specific condition of the original prompt generation strategy and the few-shot strategy."}, {"title": "Partial Experimental Results", "content": "This subsection presents partial experimental results. It includes the values of ACC and ACCA, as well as robustness evaluation results for adversarial attacks on Llama-3.1, Mistral, ChatGPT-40, and ChatGPT-40-mini. The detailed results are presented in Tables 4, 5, and 6. As shown in the tables, Llama-3.1 exhibits poor robustness, significantly lagging behind the Mistral model of the same parameter size. Additionally, GPT-40-mini demonstrates better robustness than GPT-40, which could be attributed to its later release and the subsequent improvements in robustness."}, {"title": "Prompt Templates", "content": "In this section, we introduce the prompt templates used in the SelfPrompt framework. These prompt templates include: the Triplets-to-Prompts Template for generating original prompts when selecting the LLM-based strategy; the Adversarial Prompts Generation Template for constructing adversarial prompts; the Examples-Generation Template for generating prompt examples required when using the few-shot strategy; and the (Non-)Adversarial Prompt Template for generating prompts and requiring LLMs to classify the label of the sentence in the prompts."}, {"title": "Triplets-to-Prompts Template", "content": "This template is responsible for transforming a triplet formatted as t = (s, p, o) \u2208 D, where s denotes the subject of the triplet, and p and o refer to the predicate and object of the triplet, respectively. The template converts this triplet into a naturally described sentence, where the positions marked in red in the template need to be replaced with the content of the triplets."}, {"title": "Adversarial Prompts Generation Template", "content": "This template is designed to transform original prompts into adversarial prompts. It requires the provision of a sentence from the original prompt that describes the corresponding triplet, along with its constituent components. When employing a few-shot strategy, this template also necessitates the inclusion of corresponding examples."}]}