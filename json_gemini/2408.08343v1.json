{"title": "API-guided Dataset Synthesis to Finetune Large Code Models", "authors": ["ZONGJIE LI", "DAOYUAN WU", "SHUAI WANG*", "ZHENDONG SU"], "abstract": "Large code models (LCMs), pre-trained on vast code corpora, have demonstrated remarkable performance\nacross a wide array of code-related tasks. Supervised fine-tuning (SFT) plays a vital role in aligning these models\nwith specific requirements and enhancing their performance in particular domains. However, synthesizing\nhigh-quality SFT datasets poses a significant challenge due to the uneven quality of datasets and the scarcity\nof domain-specific datasets.\nInspired by APIs as high-level abstractions of code that encapsulate rich semantic information in a concise\nstructure, we propose DATASCOPE, an API-guided dataset synthesis framework designed to enhance the SFT\nprocess for LCMs in both general and domain-specific scenarios. DATASCOPE comprises two main components:\nDSEL and DGEN. On one hand, DSEL employs API coverage as a core metric, enabling efficient dataset synthesis\nin general scenarios by selecting subsets of existing (uneven-quality) datasets with higher API coverage. On the\nother hand, DGEN recasts domain dataset synthesis as a process of using API-specified high-level functionality\nand deliberately-constituted code skeletons to synthesize concrete code.\nExtensive experiments demonstrate DATASCOPE\u02bcs effectiveness, with models fine-tuned on its synthesized\ndatasets outperforming those tuned on unoptimized datasets five times larger. Furthermore, a series of analyses\non model internals, relevant hyperparameters, and case studies provide additional evidence for the efficacy of\nour proposed methods. These findings underscore the significance of dataset quality in SFT and advance the\nfield of LCMs by providing an efficient, cost-effective framework for constructing high-quality datasets. This\ncontribution enhances performance across both general and domain-specific scenarios, paving the way for\nmore powerful and tailored LCMs.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable performance across a wide range\nof tasks following extensive pre-training [34, 72]. In the domain of code-related tasks, large code\nmodels (LCMs) such as CodeLlama [66] and StarCoder [37] have exhibited impressive capabilities\nin program understanding and generation, supporting various real-world applications. However,\ndespite their vast knowledge acquired through training on enormous datasets, these base mod-\nels may not achieve optimal performance across all use cases out-of-the-box. As illustrated in\nFig. 1(a) and (c), to further align models with diverse requirements\u2014including enhancing general\ncode generation capabilities [38, 66] or specializing in specific codebases or domains (supporting\ncommercial products like deep learning [54] or security-related [25] assists)\u2014researchers often\nemploy additional datasets to fine-tune base models, yielding more powerful and customized LCMs.\nAmong the various fine-tuning techniques proposed, supervised fine-tuning (SFT) has emerged\nas a critical approach for enhancing LLM capabilities. SFT leverages the knowledge acquired\nduring pre-training while aligning models with human expectations [6, 11, 81]. This process\ninvolves further training the models on carefully curated instruction datasets, typically comprising\nformatted instruction-response pairs. These pairs, represented as (INSTRUCTION, RESPONSE),"}, {"title": "BACKGROUND", "content": "This section introduces the background of LCMs and SFT. We explore the evolution of LCMs and\nclarify two distinct scenarios in LCM fine-tuning: general and domain-specific."}, {"title": "Large Code Models", "content": "The rapid advancements in deep learning (DL) techniques have led to the development of DL\nmodels capable of generating code with near-human or even superhuman performance [6, 11, 81].\nThese models are often integrated into development workflows through APIs (e.g., GPT-4 [51])\nor IDE plugins (e.g., Codex [50]), revolutionizing the way developers write code by improving\nboth efficiency and quality. The majority of state-of-the-art LLMs employ the Transformer [75]\narchitecture, which relies on attention mechanisms to enable tokens to communicate and exchange\ninformation with each other. This architecture allows LLMs to generate text sequentially, predicting\nthe next token based on the preceding context [59]. Building upon the success of LLMs, LCMs have\nemerged as a specialized variant tailored specifically for code-related tasks, leveraging the same\nunderlying architecture. The development of a well-performed LCM typically involves a two-step\nprocess. First, a foundation model is selected and further pre-trained on a vast corpus of code,\nresulting in a \"base model.\u201d Second, the base model undergoes fine-tuning on a task-specific dataset\nusing various fine-tuning techniques, ultimately yielding a fine-tuned model optimized for the\ndesired task [6]. A notable example is the CodeLlama base model, which is derived from the Llama\n2 foundation model and offers instructed version models [66]."}, {"title": "Supervised Fine-tuning", "content": "Formally, the SFT process of the target model can be outlined as follows: for the specific domain d\nwith context cd, each task example (xd, yd) is utilized to update the model parameters. This update\naims at minimizing the loss function that measures the disparity between the data distribution and\nthe target model distribution, as expressed below:\n$L_{SFT}(\\theta) = - \\log f_{\\theta} (y^d|c^d, x^d),$\nOverall, this function seeks to minimize the negative log-likelihood of the target output yd given\nthe context cd and input xd, with respect to the model parameters 0. LSFT converges when the\ngenerated response \u0177 matches yd, i.e., the distribution of fine-tuned model aligns with the task\ndataset distribution. Compared to other fine-tuning methods such as Reinforcement Learning\nfrom Human Feedback (RLHF) [52] or Direct Preference Optimization (DPO) [61], SFT is more\nefficient and effective, as it does not require a human preference dataset. Consequently, SFT\nbecomes a standard procedure for developing high-quality general-purpose LLMs [5, 52] and has\nproven invaluable for customizing these models across numerous domains, such as medicine [67],\nfinance [10], and various other fields, significantly enhancing their applicability and effectiveness\nin specialized contexts.\nNotably, SFT methods can be further categorized into two main approaches: (1) full parame-\nter supervised fine-tuning (SFT) and (2) parameter-efficient fine-tuning (PEFT). Although PEFT\ndemonstrates high performance while using fewer parameters, studies [23, 86] have shown that\nit primarily assists the model with response initiation and extracts most of the response from\npre-trained knowledge. In other words, PEFT does not significantly contribute to the model's ability\nto acquire new knowledge. Therefore, in this study, we focus on the full parameter fine-tuning\napproach and refer to it as the SFT."}, {"title": "Two Mainstream LCM Fine-tuning Scenarios", "content": "The primary purpose of fine-tuning LCMs is to enhance their performance on code generation tasks\nand align them with human instructions. Based on whether the fine-tuned models are intended for\nspecific domains, LCM fine-tuning can be categorized into two mainstream scenarios, general and\ndomain-specific, each facing distinct data scarcity, as illustrated in Fig. 1(a)."}, {"title": "General Scenario.", "content": "In this scenario, the fine-tuned model is designed as a universal code generation\ntool aimed at improving its general generation capabilities. A prime example is the instructed\nversion of CodeLlama [66], which demonstrates superior performance across a wide range of\ncode generation tasks compared to its base model. Researchers and practitioners working on\nfine-tuning general LCMs often face an abundance of existing datasets, such as OSS-instruct [82]\nand CodeExercise [12], which are typically derived from various online code repositories and\nprogramming forums using LLMs.\nWhile the generation methods behind these datasets attempt to synthesize a diverse range of code\nsnippets, they are inherently constrained by the quality of online code samples and the restricted\nnumber of programming forums. Consequently, these datasets often suffer from a paradox of\nquantity over quality, containing a large volume of data with inconsistent quality. For researchers\naiming to construct SFT datasets to enhance a model's general performance, an efficient and\ncost-effective dataset synthesis approach would involve creating new SFT datasets by judiciously\nselecting from existing datasets."}, {"title": "Domain Specific Scenario.", "content": "In real-world applications, LCMs are often designed to meet the\nunique requirements of their target audience. For example, Pecan [54] focuses on generating\nmachine learning code, while SQLCoder2 [14] specializes in generating SQL-related code. In such\ncases, fine-tuning aims to enhance the model's performance in specific domains by leveraging\ndomain-specific datasets and integrating pertinent domain knowledge into the model.\nUsers aiming to tune domain LCMs frequently encounter a key challenge: the scarcity of high-\nquality, domain-specific datasets. Curating such datasets demands expertise in the target domain and\noften involves manual collection, cleaning, and annotation of code samples-a process that is both\ntime-consuming and resource-intensive [13, 90]. Moreover, many valuable datasets may originate\nfrom proprietary company codebases, limiting their public availability and further exacerbating the\ndata scarcity issue. Given these constraints, there is a pressing need for an efficient and adaptable\nframework that facilitates the synthesis of high-quality, domain-specific datasets. Such a framework\nwould ideally minimize the reliance on extensive manual data generation while still capturing\nthe nuances and complexities of the target domain, thereby enabling more effective and targeted\nfine-tuning of LCMs for specialized applications."}, {"title": "A MOTIVATION EXAMPLE", "content": "While APIs have been widely recognized for their effectiveness in program synthesis [69], this\npaper presents a novel perspective on their role in enhancing the performance of LCMs through SFT\ndataset synthesis. To ease understanding, this section provides a motivating example to illustrate\nthe benefits of using APIs in LCM program comprehension. Our example aims to elucidate how\nAPI-level abstractions can significantly improve an LCM's ability to understand and process code\nsemantics, thereby underpinning our approach to SFT dataset synthesis.\nConsidering Fig. 2, where we present two semantically equivalent code snippets, both aim to\nfind customers who purchase both electronics and home goods (loyal_customers). Code snippet A\nutilizes the highly abstracted np.intersect1d() API call, while code snippet B replaces the API\ncall with its official implementation. We input these code snippets into the llama2-7B-hf model,\nprompting it to summarize the program functionality. The llama model yields responses A and B,\nrespectively (as shown in the bottom left of Fig. 2).\nMoreover, to gain insight into the model's internal processing, we analyze the attention mecha-\nnisms within the LCM. Attention scores, a key component of Transformer-based models, reflect\nthe model's understanding of input importance [75]. We calculate the average attention score\nfor each token across all attention heads in the model's final layer. The top 20 tokens with the\nhighest average attention scores are highlighted in red for both programs. This visualization offers"}, {"title": "Token Efficiency:", "content": "The most apparent advantage of using APIs is the substantial reduction\nin token count. The API-abstracted code (snippet A) is significantly shorter than its expanded\ncounterpart (snippet B), comprising only 49% of the original length. Importantly, it has been\npointed out that the computational intensity and resource requirements of LCM fine-tuning\nand inference processes correlate positively with input token count [30, 75]. Therefore,\nthe reduced token count in API-abstracted code snippets implies that the LCM's resource\nconsumption will be significantly lower, and we envision that API abstraction will support\nmore efficient LCM fine-tuning and inference."}, {"title": "Semantic Comprehension:", "content": "A closer examination reveals the benefits of APIs in enhancing\nLCMs' understanding of program semantics. Real-world API names often encapsulate pro-\ngram functionality, enabling more concise and semantically rich code. Analyzing the model's\nresponses, we observe that highly abstracted APIs more accurately reflect the program's\nfunctionality. Response A precisely captures the program's aim of finding customers who\npurchased both electronics and home goods, while response B merely describes the func-\ntion's operations. This demonstrates how API abstraction swiftly mitigates LCMs' hurdles in\nunderstanding complex program semantics, thereby enhancing their performance."}, {"title": "Internal Attention Patterns:", "content": "When examining the LLM's internal behavior, we notice that\nAPI calls typically receive higher attention scores. In contrast, manually expanded programs\nfail to allocate similar levels of focus on key tokens. This analysis further suggests that the\nhigh-quality language abstraction provided by APIs augments the model's comprehension\nability towards crucial semantic elements."}, {"title": "API-GUIDED DATASET SELECTION (DSEL): DESIGN AND EVALUATION", "content": "We first define the problem in Sec. 4.1: we aim at selecting a subset from an SFT dataset to optimize\nthe performance of a model trained on this subset. In the following subsections, we first introduce\nDSEL, and present the implementation and evaluation results in the following subsections."}, {"title": "Problem Formulation", "content": "Given an SFT dataset D = (xi, Yi)N\ni=1, where each example consists of an instruction x\u2081 and its\ncorresponding code yi, our goal is to select a subset D' \u2286 D of size n (n \u2264 N) such that a model\ntrained on this subset achieves maximum performance on general code generation tasks, which\ncan be formulated as:\n$\\max_{D' \\subset D} \\text{Performance}(D') \\quad \\text{s.t.} \\quad |D'| = n.$\nDespite the rather straightforward formulation, predicting the performance of a fine-tuned model\nis challenging. Drawing inspiration from our key observations in Sec.3, which demonstrated the\ninfluence of APIs on LCM code comprehension, we propose using API coverage as a proxy measure\nfor performance. Furthermore, building on previous research that highlights the importance of\ndiversity in dataset quality [3, 55, 90], we also consider the diversity of code lengths in the selected\nsubset. To quantify this aspect, we introduce a length diversity measure, LenDist(D', D), which\nassesses the similarity between the length distributions of the subset D' and the original dataset D.\nIncorporating these design considerations, we reformulate our optimization problem as:\n$\\max_{D' \\subset D} \\text{APICoverage}(D') \\quad \\text{s.t.} \\quad |D'| = n, \\text{LenDist}(D', D) \\leq \\tau,$\nwhere APICoverage(D') represents the number of unique APIs covered in subset D', and \u03c4 is a\nlength diversity threshold. This formulation aims to maximize API coverage while maintaining a\nrepresentative distribution of code lengths.\nSFT Dataset Selection vs. Test Case Selection. Test case selection is a well-studied problem\nin software testing, with the objective to select a subset of test cases T' \u2286 T from a large pool\nT while maintaining software quality and reducing the number of test cases [42, 85]. Although\nboth test case selection and dataset selection can be formulated as optimization problems, there\nis a notable difference: as new, non-trivial examples are continuously added to the existing test\ncase set, code coverage will monotonically increase, such that the software will be tested more\nthoroughly. In contrast, such a monotonicity does not hold for SFT; adding more code snippets\ndoes not necessarily lead to better model performance on the task, as it may introduce more noise\nor cause overfitting [24], resulting in performance degradation."}, {"title": "Selection Algorithm", "content": "Although the optimization problem here can be solved through an exhaustive search approach, it\nquickly becomes infeasible for large datasets due to the combinatorial explosion of possible subsets.\nTherefore, we propose a sub-optimal greedy algorithm in DSEL to efficiently solve this problem.\nThe main idea is to iteratively select examples that maximize the incremental API coverage while\nmaintaining the diversity of code lengths. The detailed steps are shown in Algorithm 1. We first\ninitialize the selected indexes subset as empty and the API set as empty (line 2). We also initialize a\nlist bucketcnt to keep track of the number of examples selected for each length bucket and calculate"}, {"title": "Experimental Setup for General Scenario", "content": "SFT Datasets. We evaluate DSEL on two datasets: CODEE and OSS. CODEE is primarily sourced\nfrom the CodeExercise dataset [12], which contains programming exercises covering a wide range of\nPython-related topics, including basic syntax, data structures, algorithm applications, and database\nqueries. To further enhance the dataset's diversity and quality, we supplement it with the MEIC\ndataset [76], which follows a similar data generation process and template, providing a substantial\nnumber of high-quality instruction pairs. In contrast, OSS is derived from the MagicCoder's OSS-\nInstruct dataset [82]. This dataset generates a large number of instruction pairs by querying GPT-3.5\nwith a combination of real-world code snippets. To ensure data quality and consistency, we apply\na further processing step to both datasets. We select Python-related examples, remove instances\nfailing syntax checks or exceeding length thresholds, and eliminate potentially invalid or excessively"}, {"title": "Hyperparameters.", "content": "Table 1 lists the hyperparameters used in our experiments. For DSEL, we set the\nnumber of buckets to 40 and the budget constraint n to 2.5%, 5%, 10%, 20%, and 25% of the training\ndata size. These settings allow us to investigate the effectiveness of our method under different\ndata budgets. The API coverage is calculated based on the unique APIs in the selected subset. All\nexperiments are conducted on a server equipped with eight H800 GPUs, each having 80GB of\ngraphic memory. To optimize GPU memory usage and improve training efficiency, we employ\nOptimizer State Sharding (ZeRO 3) techniques from DeepSpeed [62, 63]. For larger models such\nas CodeLlama 34B, we further offload the optimizer state into CPU memory to mitigate potential\nout-of-CUDA memory issues. Following [76], we allocate 10% of the training samples as a validation\nset. During training, we monitor the validation loss and select the three checkpoints closest to the\nlowest validation loss for inference. From these three checkpoints, we choose the best-performing\none as our final model."}, {"title": "Results for DSEL", "content": "In this section, we first analyze the subset selection results of DSEL in terms of API coverage and\nJS divergence. Then, we evaluate the performance of SFT models trained on the selected subsets\nusing the Pass@1 metric on the Humaneval benchmark. Finally, we provide a detailed analysis of\nthe results and offer insights into the effectiveness of DSEL."}, {"title": "Comparative Analysis of Subset Selection Methods.", "content": "To comprehensively evaluate the effective-\nness of DSEL, we conduct experiments on two datasets (OSS and CODEE) and compare DSEL against\ntwo baselines: random selection (Random) and a clustering-based selection (CaR) [22]. For the\nRandom baseline, we randomly select three sets of examples using different random seeds, perform\na preliminary study, and choose the seed that yields the best results for subsequent experiments.\nIn the CaR baseline, we follow the same setting, where we use the Sentence-BERT model [64] to\nembed the instruction pairs and apply K-means [31] clustering algorithm on the feature vectors\nprocessed by PCA [1]. We then select the top k examples from each cluster as representatives. The\nresults are shown in Table 2, from which we can make the following observations:"}, {"title": "API Coverage Analysis.", "content": "DSEL consistently achieves significantly higher coverage compared\nto both Random and CaR across all subset sizes and datasets. Notably, at the 25% subset size,\nDSEL covers 77.82% and 100% of the unique APIs for the OSS and CODEE datasets, respectively,\noutperforming the baselines by 33.85% and 41.80%. This demonstrates the effectiveness of DSEL\nin maximizing API coverage through its iterative greedy selection approach. As the dataset size\nincreases, the API coverage of all three methods increases. However, the convergence speed is"}, {"title": "JS Divergence Analysis.", "content": "DSEL consistently maintains lower divergence compared to the baselines,\nespecially at larger subset sizes. On average, DSEL achieves JS divergence that is 0.0128 lower\nthan Random and 0.0203 lower than CaR. At the 25% subset size, DSEL achieves JS divergence of\n0.0546 and 0.0208 for OSS and CODEE, respectively. These values are 0.0219 and 0.0252 lower than\nRandom, and 0.0252 and 0.0332 lower than CaR for OSS and CODEE, respectively. This highlights\nthe ability of DSEL to select subsets that closely resemble the length distribution of the original\ndataset. Similar to the API coverage results, we observe that the JS divergence decreases more\nslowly on OSS compared to CODEE, with a decrease of 0.0217 from 2.5% to 25% subset size on\nOSS, compared to a decrease of 0.0304 on CODEE. This further reflects the complexity of the data\ndistribution and the effectiveness of DSEL in handling it."}, {"title": "Dsel's Performance Evaluation and Analysis.", "content": "Fig. 3 presents the evaluation results of DSEL on\nthe Humaneval benchmark using CodeLlama models of different sizes (7B, 13B, and 34B), in which\nthe score is represented by pass@1 rate. Overall, we view the results as highly promising. We now\nanalyze the results by analyzing the \u201cfull dataset training\u201d and two random selection baselines."}, {"title": "Implication for DsEL:", "content": "Selecting subsets of existing datasets with higher API coverage not only\naids in efficient dataset synthesis, but also provides increasingly significant benefits as model size\nscales up."}, {"title": "API-GUIDED DATASET GENERATION (DGEN)", "content": "Building upon our previous analysis of API impact on LCMs, both without fine-tuning (Sec. 3) and\nfine-tuning in general scenarios (Sec. 4), we now address the challenge of dataset scarcity in domain\nusages. To complete our framework and provide a comprehensive solution for dataset synthesis,\nwe introduce DGEN, a component designed for generating high-quality SFT datasets leveraging\nAPI combinations."}, {"title": "Problem Statement and Approach Overview", "content": "Extending our research beyond the scope presented in Sec. 4.1, we now confront a more complex\nchallenge: the absence of a pre-existing dataset D. Our task evolves from subset selection to the\ncreation of an entirely new, \u201cdomain-specific\u201d dataset from scratch."}, {"title": "Domain-Specific Context.", "content": "In the context of this work, \u201cdomain-specific\u201d refers to code related\nto particular applications or specialized programming areas. For instance, relevant commercial\nproducts like PECAN [54] specifically focus on synthesizing code for machine learning tasks\n(where machine learning is their focused domain). Nevertheless, establishing a universal definition\nover \"domain-specific\u201d code appears challenging. Therefore, we adopt a pragmatic approach: we\ndeem one library representative enough to scope a domain. For instance, we consider NumPy as\nrepresentative of the scientific computing domain, Pandas for data manipulation and analysis, and\nMatplotlib for data visualization. This approach is intuitive, and it allows us to concretely define and\nwork with domain-specific datasets throughout our study. Unless otherwise specified, subsequent\nreferences to domain-specific contexts in this paper adhere to this definition."}, {"title": "Key Insights.", "content": "Dgen incorporates two key insights. First, it frames the task as a transforma-\ntion process from high-level requirements to concrete implementations. By providing the LLM\nwith specific, comprehension-friendly functional and structural requirements for code generation,\nDGEN reduces the need for extensive domain-specific knowledge, enabling the model to focus on\ntranslating clear specifications into actual code. Second, DGEN decomposes complex generation\nproblems into simpler sub-problems using API sets and SKDSL, lowering the capability threshold\nrequired at each step.\nAs a result of these insights, Dgen reduces the strict demand for a \u201chigh-capability\u201d model,\npotentially enabling a weak-to-strong [7] generation paradigm where less capable models could\nbe used for initial data generation, which is then used to further improve the model performance.\nDGEN's novel approach enhances flexibility and cost-effectiveness, establishing it as a versatile\nand scalable framework for specific dataset synthesis, adaptable to varying model capabilities and\ndomain requirements."}, {"title": "API Collection.", "content": "To use DGEN for dataset generation, we first extract APIs from a targeted\nlibrary specified by the user. As illustrated in Fig. 4a, we first extract APIs from the library's official\ndocumentation, following the method proposed by [9]. This process yields 4,089, 3,296, and 3,683\nAPIs from Matplotlib, Pandas, and Numpy respectively, including their parameters and functional\ndescriptions. Then, we rule out certain APIs based on the following criteria: (1) APIs starting with\n\u201c_\u201d or \u201cc.\u201d are excluded, as they are typically internal or low-level APIs related to the underlying C\nimplementation; (2) For method implementations in both base and derived classes, we only retain\nthe base class implementation as the API call; (3) any API invocation with a method chain [48]\nlonger than three is excluded from further consideration.\nFinally, we categorize the kept APIs into basic and advanced types. We select basic APIs directly\nfrom the examples in the official tutorials of each library, limiting their number to 50. These basic\nAPIs are frequently used and easily understood, such as numpy.sum and pandas.read_csv. The\nremaining APIs are classified as advanced, which are typically less common and often require\nspecialized knowledge, as exemplified by numpy.linalg.eig and pandas.DataFrame.groupby.\nProprietary Libraries. Notably, while this extraction and filtering process is straightforward for\nmost public libraries with comprehensive documentation, we assume similar completeness for\nproprietary libraries considered for SFT dataset construction. It is worth mentioning that only"}, {"title": "Instruction Generation.", "content": "As shown in Fig. 4a, the second step of DGEN involves preparing\ninstructions for querying the LLM to synthesize the SFT dataset. More specifically, this step is\ndivided into two separate parts: choosing the appropriate APIs and selecting the prompt template.\nFor the chosen APIs, we consider two different strategies to create API sets of varying difficulty\nlevels:\n$B\\_SET = RandomSample(BasicAPIList, N)$\n$MIX\\_SET = RandomSample (BasicAPIList \\cup AdvancedAPIList, N)$\nB_SET focuses on commonly used APIs, ensuring high coverage of basic functionalities. MIX_SET\nintroduces higher difficulty by incorporating advanced APIs, simulating real-world scenarios where\ncomplex APIs are used alongside basic ones. The selected APIs, along with their relevant information\nextracted from documentation, are then incorporated into the prompt template (see Table 3).\nSKDSL for Code Skeleton Generation. While API sets of varying difficulties control code content\ncomplexity, we design SKDSL, a domain-specific language, to govern code structure by allowing\nrapid prototyping of Python's high-level logic flow through specified Python keywords (e.g., def,\nif, else). Given a randomly generated keyword list, we incrementally incorporate each keyword\ninto the code, randomly injecting valid statements (e.g., a = 1) between keywords. These injected\nstatements serve to create a more complete code skeleton, enabling subsequent validation by a\nsyntax checker. While this approach may occasionally produce invalid code skeletons, we maintain\nit for its simplicity and efficiency. If an invalid skeleton is detected, we discard it and generate a\nnew one. This process of generating and validating is significantly faster than querying the LLM\nfor generation, often by a factor of thousands. Consequently, the occasional generation of invalid\nskeletons has a negligible impact on overall efficiency, while allowing us to produce a diverse range\nof valid code skeletons.\nAfter generating the code skeletons, SKDSL integrates with a grammar checker to perform\nbasic validation, catching syntactic errors early before full implementation. This process enhances\nthe quality of the generated skeletons. Subsequently, we standardize these skeletons by replacing\nrandom statements with the special token <Random Stmt>and conditions in control flow keywords"}, {"title": "Dataset Generation.", "content": "The final stage of Dgen uses the generated instruction prompts to\nquery the LLM, producing library-specific instruction and response code pairs. Each pair undergoes\nformat and length checks, discarding those that fail to meet specified criteria. These criteria include\nthe presence of a code snippet and content length requirements, where pairs with fewer than\n32 tokens or more than 4,096 tokens are excluded. Subsequently, we perform content validation\non the remaining pairs. As mentioned in Sec. 5.2.3, the inserted random valid statements enable\ngrammatical correctness checks. Based on this, we define a threshold T (0 < T < 1) to determine\ncode acceptance. If the number of detected APIs in the generated code exceeds N \u00d7 T, where N is\nthe required number of APIs, the pair passes content validation. Only the pairs that successfully\npass all the aforementioned checks are included in our SFT dataset.\nAutomated Pipeline. Notably, DGEN is designed to establish an efficient and automated SFT dataset\ngeneration process. The entire procedure does not necessitate any manual intervention, from API\ncollection through instruction generation to dataset creation. It guarantees the scalability and\nreproducibility of the dataset generation pipeline, facilitating the creation of extensive, high-quality\nSFT datasets for a wide range of domain-specific libraries. By minimizing human involvement,\nwe not only increase efficiency but also reduce the potential for human-induced biases or errors,\nensuring consistent quality across large-scale dataset synthesis efforts."}, {"title": "Bench: A Benchmark for Evaluating Code Generation in Specific Domains", "content": "To evaluate the effectiveness of DGEN and address the lack of existing benchmarks in this area, we\ndeveloped BENCH, a novel benchmark designed to assess model performance on specific domains. As\nout"}]}