{"title": "API-guided Dataset Synthesis to Finetune Large Code Models", "authors": ["ZONGJIE LI", "DAOYUAN WU", "SHUAI WANG*", "ZHENDONG SU"], "abstract": "Large code models (LCMs), pre-trained on vast code corpora, have demonstrated remarkable performance across a wide array of code-related tasks. Supervised fine-tuning (SFT) plays a vital role in aligning these models with specific requirements and enhancing their performance in particular domains. However, synthesizing high-quality SFT datasets poses a significant challenge due to the uneven quality of datasets and the scarcity of domain-specific datasets.\nInspired by APIs as high-level abstractions of code that encapsulate rich semantic information in a concise structure, we propose DATASCOPE, an API-guided dataset synthesis framework designed to enhance the SFT process for LCMs in both general and domain-specific scenarios. DATASCOPE comprises two main components: DSEL and DGEN. On one hand, DSEL employs API coverage as a core metric, enabling efficient dataset synthesis in general scenarios by selecting subsets of existing (uneven-quality) datasets with higher API coverage. On the other hand, DGEN recasts domain dataset synthesis as a process of using API-specified high-level functionality and deliberately-constituted code skeletons to synthesize concrete code.\nExtensive experiments demonstrate DATASCOPE\u02bcs effectiveness, with models fine-tuned on its synthesized datasets outperforming those tuned on unoptimized datasets five times larger. Furthermore, a series of analyses on model internals, relevant hyperparameters, and case studies provide additional evidence for the efficacy of our proposed methods. These findings underscore the significance of dataset quality in SFT and advance the field of LCMs by providing an efficient, cost-effective framework for constructing high-quality datasets. This contribution enhances performance across both general and domain-specific scenarios, paving the way for more powerful and tailored LCMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks following extensive pre-training [34, 72]. In the domain of code-related tasks, large code models (LCMs) such as CodeLlama [66] and StarCoder [37] have exhibited impressive capabilities in program understanding and generation, supporting various real-world applications. However, despite their vast knowledge acquired through training on enormous datasets, these base models may not achieve optimal performance across all use cases out-of-the-box. As illustrated in Fig. 1(a) and (c), to further align models with diverse requirements\u2014including enhancing general code generation capabilities [38, 66] or specializing in specific codebases or domains (supporting commercial products like deep learning [54] or security-related [25] assists)\u2014researchers often employ additional datasets to fine-tune base models, yielding more powerful and customized LCMs.\nAmong the various fine-tuning techniques proposed, supervised fine-tuning (SFT) has emerged as a critical approach for enhancing LLM capabilities. SFT leverages the knowledge acquired during pre-training while aligning models with human expectations [6, 11, 81]. This process involves further training the models on carefully curated instruction datasets, typically comprising formatted instruction-response pairs. These pairs, represented as (INSTRUCTION, RESPONSE),"}, {"title": "2 BACKGROUND", "content": "This section introduces the background of LCMs and SFT. We explore the evolution of LCMs and clarify two distinct scenarios in LCM fine-tuning: general and domain-specific."}, {"title": "2.1 Large Code Models", "content": "The rapid advancements in deep learning (DL) techniques have led to the development of DL models capable of generating code with near-human or even superhuman performance [6, 11, 81]. These models are often integrated into development workflows through APIs (e.g., GPT-4 [51]) or IDE plugins (e.g., Codex [50]), revolutionizing the way developers write code by improving both efficiency and quality. The majority of state-of-the-art LLMs employ the Transformer [75] architecture, which relies on attention mechanisms to enable tokens to communicate and exchange information with each other. This architecture allows LLMs to generate text sequentially, predicting the next token based on the preceding context [59]. Building upon the success of LLMs, LCMs have emerged as a specialized variant tailored specifically for code-related tasks, leveraging the same underlying architecture. The development of a well-performed LCM typically involves a two-step process. First, a foundation model is selected and further pre-trained on a vast corpus of code, resulting in a \"base model.\u201d Second, the base model undergoes fine-tuning on a task-specific dataset using various fine-tuning techniques, ultimately yielding a fine-tuned model optimized for the desired task [6]. A notable example is the CodeLlama base model, which is derived from the Llama 2 foundation model and offers instructed version models [66]."}, {"title": "2.2 Supervised Fine-tuning", "content": "Formally, the SFT process of the target model can be outlined as follows: for the specific domain d with context cd, each task example (xd, yd) is utilized to update the model parameters. This update aims at minimizing the loss function that measures the disparity between the data distribution and the target model distribution, as expressed below:\n\\(L_{SFT}(\\theta) = -log f_{\\theta}(y_d | c_d, x_d),\\) (1)\nOverall, this function seeks to minimize the negative log-likelihood of the target output yd given the context cd and input xd, with respect to the model parameters \u03b8. LSFT converges when the generated response \u0177 matches yd, i.e., the distribution of fine-tuned model aligns with the task dataset distribution. Compared to other fine-tuning methods such as Reinforcement Learning from Human Feedback (RLHF) [52] or Direct Preference Optimization (DPO) [61], SFT is more efficient and effective, as it does not require a human preference dataset. Consequently, SFT becomes a standard procedure for developing high-quality general-purpose LLMs [5, 52] and has proven invaluable for customizing these models across numerous domains, such as medicine [67], finance [10], and various other fields, significantly enhancing their applicability and effectiveness in specialized contexts.\nNotably, SFT methods can be further categorized into two main approaches: (1) full parame-ter supervised fine-tuning (SFT) and (2) parameter-efficient fine-tuning (PEFT). Although PEFT demonstrates high performance while using fewer parameters, studies [23, 86] have shown that it primarily assists the model with response initiation and extracts most of the response from pre-trained knowledge. In other words, PEFT does not significantly contribute to the model's ability to acquire new knowledge. Therefore, in this study, we focus on the full parameter fine-tuning approach and refer to it as the SFT."}, {"title": "2.3 Two Mainstream LCM Fine-tuning Scenarios", "content": "The primary purpose of fine-tuning LCMs is to enhance their performance on code generation tasks and align them with human instructions. Based on whether the fine-tuned models are intended for specific domains, LCM fine-tuning can be categorized into two mainstream scenarios, general and domain-specific, each facing distinct data scarcity, as illustrated in Fig. 1(a)."}, {"title": "3 A MOTIVATION EXAMPLE", "content": "While APIs have been widely recognized for their effectiveness in program synthesis [69], this paper presents a novel perspective on their role in enhancing the performance of LCMs through SFT dataset synthesis. To ease understanding, this section provides a motivating example to illustrate the benefits of using APIs in LCM program comprehension. Our example aims to elucidate how API-level abstractions can significantly improve an LCM's ability to understand and process code semantics, thereby underpinning our approach to SFT dataset synthesis."}, {"title": "4 API-GUIDED DATASET SELECTION (DSEL): DESIGN AND EVALUATION", "content": "We first define the problem in Sec. 4.1: we aim at selecting a subset from an SFT dataset to optimize the performance of a model trained on this subset. In the following subsections, we first introduce DSEL, and present the implementation and evaluation results in the following subsections."}, {"title": "4.1 Problem Formulation", "content": "Given an SFT dataset D = (xi, Yi)N i=1, where each example consists of an instruction xi and its corresponding code yi, our goal is to select a subset D' \u2286 D of size n (n \u2264 N) such that a model trained on this subset achieves maximum performance on general code generation tasks, which can be formulated as:\n\\(\\max_{D' \\subset D} \\text{Performance}(D') \\quad \\text{s.t.} \\quad |D'| = n.\\) (2)\nDespite the rather straightforward formulation, predicting the performance of a fine-tuned model is challenging. Drawing inspiration from our key observations in Sec.3, which demonstrated the influence of APIs on LCM code comprehension, we propose using API coverage as a proxy measure for performance. Furthermore, building on previous research that highlights the importance of diversity in dataset quality [3, 55, 90], we also consider the diversity of code lengths in the selected subset. To quantify this aspect, we introduce a length diversity measure, LenDist(D', D), which assesses the similarity between the length distributions of the subset D' and the original dataset D. Incorporating these design considerations, we reformulate our optimization problem as:\n\\(\\max_{D' \\subset D} \\text{APICoverage}(D') \\quad \\text{s.t.} \\quad |D'| = n, \\text{LenDist}(D', D) \\leq \\tau,\\) (3)\nwhere APICoverage(D') represents the number of unique APIs covered in subset D', and \u03c4 is a length diversity threshold. This formulation aims to maximize API coverage while maintaining a representative distribution of code lengths.\nSFT Dataset Selection vs. Test Case Selection. Test case selection is a well-studied problem in software testing, with the objective to select a subset of test cases T' \u2286 T from a large pool T while maintaining software quality and reducing the number of test cases [42, 85]. Although both test case selection and dataset selection can be formulated as optimization problems, there is a notable difference: as new, non-trivial examples are continuously added to the existing test case set, code coverage will monotonically increase, such that the software will be tested more thoroughly. In contrast, such a monotonicity does not hold for SFT; adding more code snippets does not necessarily lead to better model performance on the task, as it may introduce more noise or cause overfitting [24], resulting in performance degradation."}, {"title": "4.2 Selection Algorithm", "content": "Although the optimization problem here can be solved through an exhaustive search approach, it quickly becomes infeasible for large datasets due to the combinatorial explosion of possible subsets. Therefore, we propose a sub-optimal greedy algorithm in DSEL to efficiently solve this problem. The main idea is to iteratively select examples that maximize the incremental API coverage while maintaining the diversity of code lengths. The detailed steps are shown in Algorithm 1. We first initialize the selected indexes subset as empty and the API set as empty (line 2). We also initialize a list bucketcnt to keep track of the number of examples selected for each length bucket and calculate"}, {"title": "4.3 Experimental Setup for General Scenario", "content": "SFT Datasets. We evaluate DSEL on two datasets: CODEE and OSS. CODEE is primarily sourced from the CodeExercise dataset [12], which contains programming exercises covering a wide range of Python-related topics, including basic syntax, data structures, algorithm applications, and database queries. To further enhance the dataset's diversity and quality, we supplement it with the MEIC dataset [76], which follows a similar data generation process and template, providing a substantial number of high-quality instruction pairs. In contrast, OSS is derived from the MagicCoder's OSS-Instruct dataset [82]. This dataset generates a large number of instruction pairs by querying GPT-3.5 with a combination of real-world code snippets. To ensure data quality and consistency, we apply a further processing step to both datasets. We select Python-related examples, remove instances failing syntax checks or exceeding length thresholds, and eliminate potentially invalid or excessively"}, {"title": "4.4 Results for DSEL", "content": "In this section, we first analyze the subset selection results of DSEL in terms of API coverage and JS divergence. Then, we evaluate the performance of SFT models trained on the selected subsets using the Pass@1 metric on the Humaneval benchmark. Finally, we provide a detailed analysis of the results and offer insights into the effectiveness of DSEL."}, {"title": "4.4.1 Comparative Analysis of Subset Selection Methods", "content": "To comprehensively evaluate the effective-ness of DSEL, we conduct experiments on two datasets (OSS and CODEE) and compare DSEL against two baselines: random selection (Random) and a clustering-based selection (CaR) [22]. For the Random baseline, we randomly select three sets of examples using different random seeds, perform a preliminary study, and choose the seed that yields the best results for subsequent experiments. In the CaR baseline, we follow the same setting, where we use the Sentence-BERT model [64] to embed the instruction pairs and apply K-means [31] clustering algorithm on the feature vectors processed by PCA [1]. We then select the top k examples from each cluster as representatives. The results are shown in Table 2, from which we can make the following observations:\nAPI Coverage Analysis. DSEL consistently achieves significantly higher coverage compared to both Random and CaR across all subset sizes and datasets. Notably, at the 25% subset size, DSEL covers 77.82% and 100% of the unique APIs for the OSS and CODEE datasets, respectively, outperforming the baselines by 33.85% and 41.80%. This demonstrates the effectiveness of DSEL in maximizing API coverage through its iterative greedy selection approach. As the dataset size increases, the API coverage of all three methods increases. However, the convergence speed is"}, {"title": "4.4.2 Dsel's Performance Evaluation and Analysis", "content": "Fig. 3 presents the evaluation results of DSEL on the Humaneval benchmark using CodeLlama models of different sizes (7B, 13B, and 34B), in which the score is represented by pass@1 rate. Overall, we view the results as highly promising. We now analyze the results by analyzing the \u201cfull dataset training\u201d and two random selection baselines."}, {"title": "5 API-GUIDED DATASET GENERATION (DGEN)", "content": "Building upon our previous analysis of API impact on LCMs, both without fine-tuning (Sec. 3) and fine-tuning in general scenarios (Sec. 4), we now address the challenge of dataset scarcity in domain usages. To complete our framework and provide a comprehensive solution for dataset synthesis, we introduce DGEN, a component designed for generating high-quality SFT datasets leveraging API combinations."}, {"title": "5.1 Problem Statement and Approach Overview", "content": "Extending our research beyond the scope presented in Sec. 4.1, we now confront a more complex challenge: the absence of a pre-existing dataset D. Our task evolves from subset selection to the creation of an entirely new, \u201cdomain-specific\u201d dataset from scratch.\nDomain-Specific Context. In the context of this work, \u201cdomain-specific\u201d refers to code related to particular applications or specialized programming areas. For instance, relevant commercial products like PECAN [54] specifically focus on synthesizing code for machine learning tasks (where machine learning is their focused domain). Nevertheless, establishing a universal definition over \"domain-specific\u201d code appears challenging. Therefore, we adopt a pragmatic approach: we deem one library representative enough to scope a domain. For instance, we consider NumPy as representative of the scientific computing domain, Pandas for data manipulation and analysis, and Matplotlib for data visualization. This approach is intuitive, and it allows us to concretely define and work with domain-specific datasets throughout our study. Unless otherwise specified, subsequent references to domain-specific contexts in this paper adhere to this definition.\nFig. 4a illustrates the three-step process employed by DGEN for domain dataset synthesis: \u2460 API collection: Given the documentation of a library (usually can be obtained by crawling the library's official website), DGEN first extracts the complete API list, then categorizes them into \u201cAdvanced APIs\" and \u201cBasic APIs\u201d based on their popularity and complexity. \u2461 Instruction generation: DGEN further combines the APIs into B_SET and MIX_SET to specify the target functionality of the generated code. It also incorporates skeleton domain-specific language (SKDSL) to define the code structure, resulting in the assembly of comprehensive instruction prompts. \u2462 Dataset generation: Finally, DGen uses the instruction prompts to generate data, validates them against a predefined checklist, and produces a high-quality SFT dataset. These three steps are elaborated in Sec. 5.2.2, Sec. 5.2.3, and Sec. 5.2.4, respectively.\nIn the following sections, we apply DGEN to three distinct domains: scientific computing, data manipulation and analysis, and data visualization. These domains are represented by NumPy, Pandas, and Matplotlib, respectively, chosen for their comprehensive API collections and meticulously maintained codebases. This selection allows us to demonstrate the versatility and effectiveness of Dgen across a range of practical programming scenarios. Furthermore, it is important to note that the proposed technique is general and can be applied to other well-documented domains.\""}, {"title": "5.2 Generation Algorithm", "content": "This section details DGEN's three-step process for generating high-quality SFT datasets: API collec-tion, instruction generation, and dataset generation. We first introduce the key insights of DGEN's design, followed by giving the details of each step."}, {"title": "5.2.1 Key Insights", "content": "Dgen incorporates two key insights. First, it frames the task as a transforma-tion process from high-level requirements to concrete implementations. By providing the LLM with specific, comprehension-friendly functional and structural requirements for code generation, DGEN reduces the need for extensive domain-specific knowledge, enabling the model to focus on translating clear specifications into actual code. Second, DGEN decomposes complex generation problems into simpler sub-problems using API sets and SKDSL, lowering the capability threshold required at each step.\nAs a result of these insights, Dgen reduces the strict demand for a \u201chigh-capability\u201d model, potentially enabling a weak-to-strong [7] generation paradigm where less capable models could be used for initial data generation, which is then used to further improve the model performance. DGEN's novel approach enhances flexibility and cost-effectiveness, establishing it as a versatile and scalable framework for specific dataset synthesis, adaptable to varying model capabilities and domain requirements."}, {"title": "5.2.2 API Collection", "content": "To use DGEN for dataset generation, we first extract APIs from a targeted library specified by the user. As illustrated in Fig. 4a, we first extract APIs from the library's official documentation, following the method proposed by [9]. This process yields 4,089, 3,296, and 3,683 APIs from Matplotlib, Pandas, and Numpy respectively, including their parameters and functional descriptions. Then, we rule out certain APIs based on the following criteria: (1) APIs starting with \u201c_\u201d or \u201cc.\u201d are excluded, as they are typically internal or low-level APIs related to the underlying C implementation; (2) For method implementations in both base and derived classes, we only retain the base class implementation as the API call; (3) any API invocation with a method chain [48] longer than three is excluded from further consideration.\nFinally, we categorize the kept APIs into basic and advanced types. We select basic APIs directly from the examples in the official tutorials of each library, limiting their number to 50. These basic APIs are frequently used and easily understood, such as numpy.sum and pandas.read_csv. The remaining APIs are classified as advanced, which are typically less common and often require specialized knowledge, as exemplified by numpy.linalg.eig and pandas.DataFrame.groupby.\nProprietary Libraries. Notably, while this extraction and filtering process is straightforward for most public libraries with comprehensive documentation, we assume similar completeness for proprietary libraries considered for SFT dataset construction. It is worth mentioning that only"}, {"title": "5.2.3 Instruction Generation", "content": "As shown in Fig. 4a, the second step of DGEN involves preparing instructions for querying the LLM to synthesize the SFT dataset. More specifically, this step is divided into two separate parts: choosing the appropriate APIs and selecting the prompt template. For the chosen APIs, we consider two different strategies to create API sets of varying difficulty levels:\nB_SET = RandomSample(BasicAPIList, N)\nMIX_SET = RandomSample (BasicAPIList \u222a AdvancedAPIList, N) (5)\nB_SET focuses on commonly used APIs, ensuring high coverage of basic functionalities. MIX_SET introduces higher difficulty by incorporating advanced APIs, simulating real-world scenarios where complex APIs are used alongside basic ones. The selected APIs, along with their relevant information extracted from documentation, are then incorporated into the prompt template (see Table 3).\nSKDSL for Code Skeleton Generation. While API sets of varying difficulties control code content complexity, we design SKDSL, a domain-specific language, to govern code structure by allowing rapid prototyping of Python's high-level logic flow through specified Python keywords (e.g., def, if, else). Given a randomly generated keyword list, we incrementally incorporate each keyword into the code, randomly injecting valid statements (e.g., a = 1) between keywords. These injected statements serve to create a more complete code skeleton, enabling subsequent validation by a syntax checker. While this approach may occasionally produce invalid code skeletons, we maintain it for its simplicity and efficiency. If an invalid skeleton is detected, we discard it and generate a new one. This process of generating and validating is significantly faster than querying the LLM for generation, often by a factor of thousands. Consequently, the occasional generation of invalid skeletons has a negligible impact on overall efficiency, while allowing us to produce a diverse range of valid code skeletons.\nAfter generating the code skeletons, SKDSL integrates with a grammar checker to perform basic validation, catching syntactic errors early before full implementation. This process enhances the quality of the generated skeletons. Subsequently, we standardize these skeletons by replacing random statements with the special token <Random Stmt>and conditions in control flow keywords"}, {"title": "5.2.4 Dataset Generation", "content": "The final stage of Dgen uses the generated instruction prompts to query the LLM, producing library-specific instruction and response code pairs. Each pair undergoes format and length checks, discarding those that fail to meet specified criteria. These criteria include the presence of a code snippet and content length requirements, where pairs with fewer than 32 tokens or more than 4,096 tokens are excluded. Subsequently, we perform content validation on the remaining pairs. As mentioned in Sec. 5.2.3, the inserted random valid statements enable grammatical correctness checks. Based on this, we define a threshold T (0 < T < 1) to determine code acceptance. If the number of detected APIs in the generated code exceeds N \u00d7 T, where N is the required number of APIs, the pair passes content validation. Only the pairs that successfully pass all the aforementioned checks are included in our SFT dataset.\nAutomated Pipeline. Notably, DGEN is designed to establish an efficient and automated SFT dataset generation process. The entire procedure does not necessitate any manual intervention, from API collection through instruction generation to dataset creation. It guarantees the scalability and reproducibility of the dataset generation pipeline, facilitating the creation of extensive, high-quality SFT datasets for a wide range of domain-specific libraries. By minimizing human involvement, we not only increase efficiency but also reduce the potential for human-induced biases or errors, ensuring consistent quality across large-scale dataset synthesis efforts."}, {"title": "5.3 Bench: A Benchmark for Evaluating Code Generation in Specific Domains", "content": "To evaluate the effectiveness of DGEN and address the lack of existing benchmarks in this area, we developed BENCH, a novel benchmark designed to assess model performance on specific domains. As outlined in Fig. 4a, the creation of BENCH follows a three-step approach: \u2460 problem selection through a highly automated collection process, \u2461 instruction rewriting using an automated rewriting process, and\nhuman inspection with rigorous selection criteria. This comprehensive process ensures the quality and relevance of the included questions, facilitating further research and assessment of LCMs in specific domains. In the following subsections, we detail each step of this process, beginning with our approach to problem selection."}, {"title": "5.3.1 Problem Selection", "content": "We use the Stack Exchange platform [70] to identify popular library-specific questions, searching with the target library name as the keyword. We prioritize questions with accepted answers or, in their absence, those with the highest-voted responses. To ensure quality and relevance, we apply a filtering criterion based on the average monthly vote count, considering only questions with at least 5 votes per month. We also verify the presence of the target library in the answer code snippets. This process yields 439, 642, and 391 Python-related question-answer pairs for Numpy, Matplotlib, and Pandas, respectively, encompassing a diverse range of domain-specific tasks.\nData Leakage. Notably, to mitigate the risk of data leakage and maintain the integrity of the evaluation, we exclusively consider questions posted between August 2023 and April 2024, ensuring that the selected data is temporally distinct from the training data of the LCM model used for comparison in our experiments. This temporal separation guarantees that the benchmark accurately assesses the model's ability to generalize to unseen domain-specific tasks, providing a fair and unbiased evaluation of its performance."}, {"title": "5.3.2 Instruction Rewriting", "content": "To enhance the quality of the selected question-answer pairs, we employ a rewriting process similar to previous works [40, 90]. First, we extract code snippets from raw answers, removing unrelated information, indentation symbols, and HTML tags (e.g., <p>). Then, we utilize GPT-4 to generate refined instruction pairs based on the questions and answers. This process involves improving clarity, eliminating unnecessary details, and ensuring the instructions are easily comprehensible. GPT-4 assists in creating clear, concise guidance accompanied by relevant code snippets. The result is a set of high-quality instruction pairs that effectively capture the essence of the original question-answer pairs while significantly enhancing their clarity and usefulness for evaluating LCMs in specific libraries."}, {"title": "5.3.3 Human Inspection", "content": "Finally, in the third step, human inspection, we carefully review the rewritten instruction pairs to ensure their quality and relevance to the target library. To maintain consistency and objectivity during the human inspection process, we adhere to a set of predefined criteria, as detailed in Table 4. These criteria encompass various aspects, including contextualization, feasibility, readability, relevance, correctness, usefulness, and safety. Our inspection process follows a similar approach to that of [40], with adaptations made to accommodate the specific requirements of code generation. By applying these criteria rigorously, we ensure that the final benchmark dataset meets high standards of quality and can effectively evaluate the performance of LCMs in specific libraries. Instruction pairs that fail to satisfy any of the specified criteria are excluded from the final benchmark. This meticulous inspection process guarantees that only the most suitable and relevant instruction pairs are included. As a result of this comprehensive evaluation, we obtain 115 high-quality instruction pairs for each library. This curated benchmark dataset covers a wide range of domain-specific tasks, providing a robust foundation for assessing the capabilities of LCMs in generating accurate and effective code snippets for domain-specific programming challenges."}, {"title": "5.4 Experimental Setup for Specific Scenario", "content": "To evaluate the effectiveness of DGEN on BENCH, we employ the same models described in Sec. 4.3. However, to better align with the specific requirements of BENCH, we have adjusted our evaluation metrics and some hyperparameters accordingly."}, {"title": "5.5 Results for DGEN", "content": "In this section, we present a comprehensive evaluation of DGEN. We begin by examining its performance on BENCH and analyzing the impact of dataset size. Subsequently, we delve into an analysis of internal logits for clustering evaluation. We then investigate the relationship between pass rate and cost, as well as the influence of hyperparameters. Finally, we employ both LLM-based and human evaluations to assess the quality of DGEN's output."}, {"title": "5.5.1 Main Results", "content": "Table 5 presents the evaluation results on BENCH using CodeBLEU as the metric for CodeLlama models of 7B and 13B sizes. We compare the performance of base models without fine-tuning, models fine-tuned on the full CODEE dataset (76K examples), and models fine-tuned using our generated datasets. \u201cDSL-Guided\u201d indicates whether SKDSL was used in dataset generation. The \u201cDataset\u201d column specifies the prompt selection method: BASIC uses only the B_SET, MIX uses only the MIX_SET, COMB combines both BASIC and MIX, and COMB-BOTH randomly selects 2K examples from each of the two COMB datasets, resulting in a total of 4K examples. The \u201c# Examples\u201d column shows the detailed number of examples in each SFT dataset. The results in Table 5 demonstrate that fine-tuning the models using our generated datasets consistently improves their performance on all three libraries compared to the base versions without"}, {"title": "5.5.2 Logits Analysis and Clustering Evaluation", "content": "Beyond evaluating the performance improvement of our supervised fine-tuned models on downstream code generation tasks, we conduct a deeper analysis to investigate how DGEN-generated SFT datasets impact the models' ability to distinguish between different third-party libraries. We treat questions from BENCH pertaining to distinct third-party libraries as separate categories. Our analysis process involves several steps: \u2460 encoding the input instructions using the tokenizer to obtain input IDs and attention mask tensors; \u2461 performing a forward pass through the specific model with the encoded inputs to obtain the logits output; \u2462 computing the sum of logits weighted by the attention mask from the last layer; and \u2463 applying t-Distributed Stochastic Neighbor Embedding (t-SNE) [74] to reduce the dimensionality of the logits sum data to two dimensions for visualization.\nWe evaluate clustering quality using Silhouette score and CH Index score (See in Sec. 5.4), analyzing the base model and two fine-tuned variants for each model size. Table 7 presents the average scores for each model configuration."}, {"title": "5.5.3 Pass Rate and Cost", "content": "Our initial evaluation employs GPT-3.5 as the backbone LLM for gener-ating SFT datasets, aligning with our baseline SFT datasets [12, 82] for fair comparison. However, considering that vendors who construct these datasets and fine-tune models on them prefer to avoid relying on closed-source models to prevent potential copyright disputes, it is crucial to assess the ability of open-source models to generate high-quality datasets. Furthermore, as outlined in Sec. 5.2, each generated code snippet undergoes a series of compliance checks, introducing a trade-off between generation cost and pass rate.\nTo provide a comprehensive analysis of the trade-off between generation cost and pass rate, we evaluate the pass rate of the SFT dataset generated by 8 different models, including 6 open-source and 2 closed-source models of varying sizes. For each model's generated responses, we set the threshold T, as described in Sec. 5.2, to (0.2, 0.4, 0.6, 0.8, 1.0) to calculate the pass rates. The results, presented in Fig. 6a, reveal a general trend of decreasing pass rates as the threshold increases, indicating that more stringent criteria lead to lower acceptance of generated data. However, the performance varies among different models. In terms of pass rates alone, GPT-4 consistently outperforms all other models across all thresholds. Llama-3-70B, the largest open-source model in our experiment, and GPT-3.5 alternate in leading performance at different thresholds, highlighting their competitive capabilities. Conversely, some smaller models (e.g., Llama-3-8B) or models that have undergone code continuation pretraining (e.g., CodeLlama-13b) exhibit lower pass rates across all thresholds.\nConsidering the price differences among the top three models in terms of pass rates, GPT-3.5 emerges as a cost-effective option. The cost of generating one million tokens using GPT-3.5 is 0.5 (input) / 1.5 (output), which is only 58% of the cost of Meta-Llama-3-70B and 5% of the cost of GPT-4. Therefore, GPT-3.5 strikes a good balance between the generation cost and the pass rate, making it a suitable choice for our evaluation and subsequent SFT. Our experiments show that generating a 4k SFT dataset using GPT-3.5 costs approximately 3 USD, whereas a human-written dataset of comparable scale and quality would require contributions from over 334 volunteers [35]."}, {"title": "5.5.4 Hyperparameter Impact", "content": "To further investigate the impact of hyperparameters on the perfor-mance of DGEN, we conducted experiments with different combinations of temperature and top_p values on GPT-3.5 with threshold T set to 0.6. The temperature ranged from 0.4 to 2.0, and top_p ranged from 0.2 to 1.0. Fig. 6b shows the pass rate heatmap for various hyperparameter settings,"}, {"title": "5.5.5 Pairwise Comparison of Model Performance", "content": "To further evaluate DGEN's effectiveness, we conduct pairwise comparisons between responses generated by different models using both LLM-based and human evaluations. We focus on 7B and 13B model sizes. For each size, we compare the COMB-BOTH version (our primary model) against four variants: the CodeLlama-provided instructed version, and versions fine-tuned on the full CODEE, BASIC, and MIX datasets respectively. All fine-tuned models use the settings described in Sec. 5.5.1.\nLLM-based Evaluation. As advanced LLMs demonstrate superior performance in providing valuable evaluations, we follow previous work [89] and conduct an evaluation using GPT-4 as the judge for our pairwise comparison. For each question, we compare the responses generated by different models and ask GPT-4 to select the better one. The results are then aggregated to calculate the win rate of each comparison, as shown in the column \u201cWin Rate by GPT-4\u201d in Table 8.\nThe results reveal two key findings. First, the model trained on the COMB-BOTH dataset consistently outperforms the instructed version and the model fine-tuned on the entire CODEE dataset, both of which utilize a significantly larger amount of data for fine-tuning. When using CodeLlama7b as the base model, COMB-BOTH achieves a remarkable win rate of 85.22% against Instruct-hf and maintains win rates above 64% across all comparisons. This observation highlights"}, {"title": "5.6 Alternative Generation Strategies", "content": "An alternative approach to dataset generation draws inspiration from real-world codebases. For instance, MagicCoder [82] generates SFT datasets using GitHub repository code snippets. To explore this method's potential in domain-specific contexts, we conduct a comparison study by first replicating MagicCoder's approach. We instruct models to reference original code and incorporate five specific Numpy APIs (np.squeeze(),np.random.uniform(),np.vstack(),np.var(), and np.median()) in their generated code."}, {"title": "6 RELATED WORK", "content": "Component-based program synthesis. Program synthesis utilizing various levels of abstraction and specifications has demonstrated effectiveness in previous research [17, 28, 47, 58, 78"}]}