{"title": "MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking", "authors": ["Ting-Chih Chen", "Chia-Wei Tang", "Chris Thomas"], "abstract": "Fact-checking real-world claims often requires reviewing multiple multimodal documents to assess a claim's truthfulness, which is a highly laborious and time-consuming task. In this paper, we present a summarization model designed to generate claim-specific summaries useful for fact-checking from multimodal, multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. We introduce a dynamic perceiver-based model that can handle inputs from multiple modalities of arbitrary lengths. To train our model, we leverage a novel reinforcement learning-based entailment objective to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of our approach, we conduct experiments on both an existing benchmark and a new dataset of multi-document claims that we contribute. Our approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on our new Multi-News-Fact-Checking dataset.", "sections": [{"title": "1 Introduction", "content": "Fact-checking claims on social media platforms poses a significant challenge due to the large volume of new claims constantly being posted without sufficient methods for verification (A\u00efmeur et al., 2023). Research indicates that manually verifying all aspects of a 200-word claim can require up to four hours of dedicated effort (Vladika and Matthes, 2023). Further, despite the exceptional capabilities of large language models (LLMs) in natural language processing tasks, they still generate content with factual errors. Given that LLMS can produce convincing statements and thus influence beliefs, the potential for hallucinations poses a serious risk of misleading users when deployed for fact-checking (Jakesch et al., 2023b,a; Kreps et al., 2022). This concern highlights the possibility of language models becoming new sources of misinformation and disinformation. The proliferation of misinformation and fake news adds to this predicament, making it increasingly difficult to distinguish between reliable and deceptive content (Goldstein et al., 2023; Spitale et al., 2023). Thus, there is an urgent need for tools capable of succinctly summarizing relevant evidence for fact-checkers, i.e., systems that provide a brief yet comprehensive overview of the relevant evidence to facilitate accurate and reliable assessments. Existing research relying on summarization for fact-checking is ineffective because these methods fail to extract evidence from the resources (Das et al., 2023; Ceron and Carrara, 2023; Berlinski et al., 2023).\nA potential solution to this problem is provided by multimodal summarization (Khullar and Arora, 2020; Liu et al., 2022), which can generate summaries from sources including text, images, videos, and audio. This is a challenging task because each modality might contribute complementary information, e.g., a bar chart image accompanying relevant facts mentioned in the text. Present methods usually produce summaries given a limited number of inputs (Puduppully and Lapata, 2021; Wang et al., 2022a). The research challenge lies in processing arbitrary inputs from diverse modalities and discerning the explicit relationships between them. However, unlike the standard summarization task, which seeks to summarize the salient content of an article, our objective is to effectively distill claim-specific evidence useful for fact-checking across various modalities.\nTo train our system to generate summaries useful for human fact-checking, we assess the utility of our summaries at performing entailment (Dagan et al., 2006), a closely aligned task to fact-checking. Our work is orthogonal to prior work in entailment, in that rather than learning to predict the entailment label for the premise-hypothesis pair, we seek to generate the premise for a specific claim from a pool of multimodal data. To address the limitations of applying existing summarization methods for fact-checking, we propose the MetaSumPerceiver (MSP) model in Figure 1, where the input consists of a claim, a set of documents and images, and the objective is to generate a summary that expedites the fact-checking process for humans. We initially train the perceiver model with a summarization model. Subsequently, to produce the summary for fact-checking, we employ a proxy reward mechanism to update the summarizer to ensure the generation of an accurate and relevant summary with necessary evidence.\nTo support research on the task of multi-document fact-checking, we contribute a benchmark (Multi-News-Fact-Checking) of claims and entailment labels whose evidence is drawn from multiple documents. We evaluate our method on the MOCHEG benchmark (Yao et al., 2023) and our new dataset and demonstrate substantial improvements compared to existing baselines. The major contributions of this paper are as follows:\n\u2022 We present an innovative approach for multimodal multi-document summarization specifically designed for fact-checking applications.\n\u2022 We release the Multi-News-Fact-Checking dataset, to support the multimodal multi-document fact-checking summarization task.\n\u2022 We perform detailed experiments and ablations of our model and loss functions which clearly demonstrate the superiority of our approach over existing methods."}, {"title": "2 Related Work", "content": "2.1 Perceiver\nThe Perceiver architecture (Jaegle et al., 2021) enables scaling transformers to input sequences of arbitrary lengths, by reducing the memory footprint in standard self-attention. Follow-up works, such as Perceiver IO (Jaegle et al., 2022), adapt the original model by presenting a versatile architecture adept at processing data from various settings while ensuring linear scalability with input and output dimensions. The model has demonstrated strong performance on many downstream tasks, including optical flow estimation (Butler et al., 2012) and the GLUE language benchmark (Wang et al., 2018). Our method relies on Jaegle et al. (2022) to process a variable number of arbitrarily long text documents and images. We use the model in sequence with a summarization model to generate a multimodal summary.\n2.2 Multimodal Fact-checking Datasets\nIn the current task of fact-checking using multiple datasets (Zlatkova et al., 2019; Nakov et al., 2021; Cheema et al., 2022; Nielsen and McConville, 2022; Yao et al., 2023), the main sources of data are X and Snopes, with a focus on COVID-19, elections, and the Russo-Ukrainian war. This has led to a couple of issues. First, X has already blocked their API, making it difficult for people to access these datasets. Second, we are seeking a dataset that covers a variety of topics rather than being limited to specific ones. Additionally, we prefer a dataset in the English language. To address these issues, we propose a new multimodal fact-checking dataset based on Multi-News (Fabbri et al., 2019), which includes information from multiple documents and related images, covering a broader array of topics such as news, policy, weather, sports, etc."}, {"title": "2.3 Learning From Feedback", "content": "Recent advancements in LLMs have revolutionized the AI landscape (Touvron et al., 2023a,b; Driess et al., 2023; OpenAI, 2023). However, because they are mostly trained on data scraped from the web LLMs sometimes produce undesired outcomes, including generating biased or harmful content (Bender et al., 2021). Recognizing the importance of aligning LLMs with human values, has led to efforts in supervised fine-tuning (SFT) with ethical guidelines (Taori et al., 2023). While these efforts demonstrate the potential of integrating human feedback into training using reinforcement learning for user-tailored tasks (Ouyang et al., 2022; Bai et al., 2022), training LLMs to reflect human values is quite challenging. In our work, we adopt the idea of training language models with feedback. However, rather than relying on a human fact-checker, we utilize a surrogate reward model (an entailment model) to stand in the place of a human fact checker, in order to fine-tune the summarizer to generate summaries that give evidence for fact-checking specific claims through Proximal Policy Optimization (PPO) (Schulman et al., 2017; Zheng et al., 2023)."}, {"title": "3 Multi-News-Fact-Checking Dataset", "content": "To train our system, we need a dataset of claims whose facts are drawn from multiple documents along with the entailment label of each claim. We build our dataset on top of the Multi-News summarization dataset (Fabbri et al., 2019), which contains sets of multiple text documents along with human-written summaries of each set. Because the Multi-News dataset lacks claims specifically tailored for fact-checking tasks, we prompt Llama 2 (Touvron et al., 2023b) to generate labeled claims from each set of documents. Within each group of Multi-News documents, we leverage the human-written multi-document summary to generate 30 claims (ten of each entailment type), resulting in a dataset of 1,291,168 labeled claims. The specific prompts contain sections containing a task description, example, and instructions, which are fully detailed in the appendix 9.1. Our dataset contains 111,905 images we obtained by retrieving images from the original articles.\nTo assess the quality and effectiveness of claim generation, we conducted an evaluation using a scale from 1 to 5, where 1 indicates low quality and 5 indicates high quality. We tested 60 claims and obtained an average score of 3.61 for claim generation quality and effectiveness. In comparison, the claims generated from PRIMERA summaries scored an average of 3.21. To ensure impartiality, the annotators were blinded and asked to rate the claims alongside their respective articles and summaries.\nAdditionally, in Table 2, we validated claim labels (entailment, neutral, and contradiction) using Llama 2. Specifically, we treated the ground truth label (i.e., the label used to prompt Llama 2 to generate a claim) as the ground truth and prompted Llama 2 in a zero-shot manner to predict the entailment labels. The average accuracy for claim verification was 72.2%, indicating that the generated claims were largely consistently predicted as their intended labels.\nTo assess the checkworthiness of our generated claims (i.e., to ensure that they are factual claims worth fact-checking), we use a pre-trained model trained on the ClaimBuster dataset (Arslan et al., 2020), as illustrated in Table 2. The model assigns claims into three classes: UFS (unimportant factual claims that are not considered check-worthy), CFS (claims containing factual information of public interest in terms of their veracity), and NFS (sentences that do not contain any factual claims). The result shows that 70% of the prompted claims are check-worthy claims. This outcome substantiates that our prompts are well-designed for this task, and that Llama 2 accurately comprehend the task's intended meaning without misunderstanding."}, {"title": "4 Approach", "content": "In this section, we explain the details of our approach, MSP as illustrated in Figure 2. We also describe the preprocessing steps for both text and image data, the components of our model, and the reinforcement learning methodology we applied to train MSP. Our approach is capable of summarizing multiple multimodal documents consisting of arbitrarily long texts and images. Specifically, we use XC, XD, and x1 to represent embeddings for claims, documents, and images, respectively.\n4.1 Preprocessing\nDue to the sequence length limitation, we utilize a combination of Perceiver with BART (Lewis et al., 2020) and CLIP (Wang et al., 2022b) to extract embeddings. To circumvent exceeding the sequence length, we break down the data into chunks of 1024 tokens. Subsequently, we employ Perceiver to merge these chunks for both textual and visual embeddings. For the textual data, we use BART to obtain text embeddings following (Devlin et al., 2019). As a result, each input text is transformed into a set of token embeddings xc \u2208 \u211d^{n\u00d7D} and XD \u2208 \u211d^{m\u00d7D}, where n and m are the number of tokens and D is the dimension of embedding. Then, we use CLIP (ViT-G-14) to extract visual features for the images. Finally, each input image undergoes a transformation, resulting in a set of visual embeddings. x1 \u2208 \u211d^{k\u00d7D}, where k is the number of tokens and D is the dimension of the embedding.\n4.2 Model Training Strategy\nOur goal is to generate a textual summary of a set of multimodal documents that enables a fact-checker to determine the veracity of a claim. In order to select relevant visual content from the images, we begin by performing a cross-attention between the images and the claim:\nXIC = ATTN(QxC, Kx1, Vx1),  (1)\nwhere the query Qxc is the claim's sequence of embeddings and Kx\u2081 and Vx\u2081 are the embedding sequences of visual tokens from the images. We project XIC into the document embedding XD, which serves as the input for MSP. The output from the cross-attention block, XIC, is initially projected by a linear projection layer with the weight \u03b8. It is then concatenated with XD, as depicted in the subsequent equation:\nXICD = [proj(XIc, \u03b8), XD]^T, (2)\nwhere XICD will be the input to MSP. Prior to training our full model, we pre-train our attention block and summarization model using the Multi-News dataset's human written summaries using the cross-entropy loss function:\nLsum = \u2211^{T}_{t=1} \u2211^{N}_{i=1} yt log(\u0177t\u2081), (3)\nwhere T represents the sequence length, N is the vocabulary size, and yt\u2081 and \u0177t\u2081 denote the ground truth and predicted probabilities of token i at time step t, respectively. In the remaining text, we omit the summation over the vocabulary for conciseness.\n4.3 Fine-tuning The Summarizer\nTo enhance the summarizer's ability to produce summaries that provide the evidence needed for fact-checking claims, we adopt the concept of training a language model using feedback with reinforcement learning. After pretraining the perceiver and summarization models, we employ reinforcement learning with an entailment model serving as a surrogate for a human fact-checker as feedback. We first exclusively apply reinforcement learning to the perceiver. Subsequently, we unfreeze the summarizer and continue training end-to-end with both the perceiver and summarizer. We illustrate our fine-tuning process in Figure 3.\n4.3.1 Reward Model For Fact-Checking\nContrary to the approach in reinforcement learning from human feedback, which necessitates a human arbitrator to score the model's outputs, in this study, we train a reward model to act like a human fact-checker to guide the summarizer in producing summaries for fact-checking instead. We utilized a comprehensive dataset consisted with MultiNLI (Williams et al., 2018), Fever-NLI (Thorne et al., 2018), and Adversarial-NLI (ANLI) (Nie et al., 2020), encompassing a total of 763,193 premise-claim pairs. Leveraging this dataset, we fine-tuned DeBERTAV3 (He et al., 2023) for the task of entailment classification using cross-entropy loss. Serving as an entailment classifier, this model achieves accuracy rates of 90.3%, 77.7%, and 57.9% in the MultiNLI, Fever-NLI, and ANLI evaluation datasets, respectively.\n4.3.2 Proximal Policy Optimization\nWe define the score from the reward model as the probability of the ground-truth label given both the claim (as the hypothesis) and the generated summary for fact-checking (as the premise). The formulation for the score from the reward model can be formulated as:\nr(xc, \u0177t) = P(ygt|xC, \u0177t)-\n0.5 * Lygt\u2260ypred P(Ypred|XC,\u0177t), (4)\nwhere XC, \u0177t, Ygt and Ypred denote the claim, the generated summary, the groud-truth label of the claim, and the predicted label of the claim, respectively. The value of P(y{gt,pred}|XC, \u0177t) is derived from the trained entailment classifier. The primary objective behind this reward function is to maximize the likelihood that the generated summary for fact-checking contains the facts necessary for the model to predict the claim's ground truth label.\nIn this paper, we employ PPO as our policy gradient method for reinforcement learning. PPO adds an additional term to the reward function, which imposes a penalty determined by the Kullback-Leibler (KL) divergence between the trained RL policy summarizer, \u03c0^{PPO}, and the initial supervised summarizer \u03c0^{SFT}.\nMoreover, we incorporate an extra reward r^{quality, LM critic}, to evaluate the quality of the summary, specifically focusing on clarity and conciseness. We utilize Mistral (Jiang et al., 2023) along with a detailed quality testing prompt provided in the appendix to assess this aspect. The assigned reward ranges from 0 to 1. We integrate r^{quality} into our model update in conjunction with the existing r^{total}. The cumulative reward is described as follows:\nrtotal = (rquality + r(XC, \u0177t)-\n\u03b7KL(\u03c0^{PPO}(\u0177t|xD),\u3160^{SFT}(\u0177t|xD)))/2, (5)\nwhere \u03b7 represents the KL reward coefficient, which determines the magnitude of the KL penalty, we set it to 0.2 for our model. This coefficient functions as an entropy boost, enhancing exploration throughout the policy domain and urging the model to engage in a diverse set of actions rather than the one currently considered the best. In addition, it inhibits the policy from rapidly committing to a singular strategy, and this encourages outputs from the RL fine-tuned model to not deviate too far from the original model.\nMSP is optimized through PPO based on the policy gradient methods that optimize the policy of the model using gradient ascent. The update rule for the policy gradient is given as:\n\u03b8 \u2013 \u03b8 + aVJ(\u03b8), (6)\nwhere a and Je denote the learning rate and the expected return under policy \u03c0\u03b8 from the model, respectively."}, {"title": "5 Experiments", "content": "5.1 Claim Verification\nThe goal of our method is to generate a summary from multiple documents and modalities that is useful for fact-checking a claim. In order to assess how useful our method is at this task, we compare the performance of our method on MOCHEG, which presents a benchmark and method for multi-document multimodal fact-checking. Specifically, we employed two fixed entailment models, namely DeBERTAV3 (He et al., 2023) and Llama 2 (Touvron et al., 2023b), as our surrogate \u201chuman\u201d fact checkers to predict the entailment label of a claim given our generated summary. Importantly, we do not fine-tune these models with our generated summaries to avoid biasing the models toward the linguistic or stylistic patterns of the summaries. This ensures that they do not learn spurious features in the downstream task.\nAs depicted in Tables 3 and 4, our method exhibits superior performance, achieving a SOTA 48.6 F-score in the MOCHEG dataset. Furthermore, according to Table 3, our method demonstrates strong precision performance for the \"Supported\" label. We conduct separate tests with two distinct critics. It is noted that the most optimal performance was achieved when deploying both the entailment critic and the LM critic. This outcome indicates that our model is proficient in verifying claim labels through clear and concise summaries.\nTable 3 reveals that the best results are achieved when inputs incorporate both textual and image evidence. Perhaps unsurprisingly given its size, the zero-shot Llama 2 entailment surrogate model surpasses DeBERTAV3 in performance. Nevertheless, a notable issue persists, where the surrogate entailment models struggle to accurately deal with NEI claim labels.\nTable 4 highlights the superiority of our model compared to MOCHEG. In the case of MOCHEG, truthfulness labels are predicted by averaging a stance representation derived from both textual and image evidence. Furthermore, MOCHEG's classifier relies on fixed thresholds, which may not be optimal for every situation. In contrast, our approach involves generating summaries for fact-checking via reinforcement learning with fixed entailment models and LM critic. Although a difference remains in the result of human vs system prediction performance, our model surpasses the prior state-of-the-art system by 4.6% F-score.\n5.2 Ablations\nAdditionally, we conducted ablation experiments for claim verification on our Multi-News-Fact-Checking dataset. A comparative analysis of our method with Llama 2 and other offline summarization models, PRIMERA (Xiao et al., 2022), PEGASUS (Zhang et al., 2020) and T5 large (Raffel et al., 2020), is presented in Tables 5 and 6.\nSimilar to our results in MOCHEG, Tables 5 and 6 show that our approach, when employing the Llama 2 surrogate entailment model, achieves the best performance. Furthermore, we achieve balanced accuracy in both precision and recall, underscoring our method's ability to clearly differentiate between truthful and untruthful labels without bias in predictions. The results highlight the inability of other summarization models to generate summaries useful for fact-checking, which causes the surrogate model difficulty in accurately assessing the truthfulness labels. Furthermore, it is evident that the LM critic significantly aids the entailment model in verifying claim labels effectively. The LM critic ensures that the summary is more concise and clear while retaining the essential meaning in the summary.\nIn addition, to assess the degree to which our summaries are merely extractive of the source articles, we employed ROUGE to evaluate our summaries alongside the provided articles. Our summaries received a ROUGE score of 0.53, whereas the human-written summaries in the Multi-news dataset scored 0.62. Upon comparison, we believe our summaries do not simply rephrase the source articles.\nMoreover, to determine the fidelity and informativeness of the generated summaries, we conducted a human evaluation using a scale from 1 to 5, with 1 indicating low fidelity/informativeness and 5 indicating high fidelity/informativeness. We tested 60 generated summaries and obtained an average score of 3.77 for fidelity and informativeness. Comparatively, the PRIMERA summaries scored 3.35. To ensure impartiality, we blinded the annotators and asked them to rate the generated summaries alongside their respective articles.\n5.3 Explanation Generation\nIn order to assess the degree to which our generated summaries contain the relevant facts necessary to fact check the generated claims, we measure the ability of a method to generate an explanation of the predicted truthfulness label using our summary. We adopt a methodology similar to Yao et al. (2023), where we consider the input claim C, its truthfulness label Yc, and the summary for fact-checking {T1, T2, ...} generated from MSP. These components are concatenated into an overall sequence X using a separator </s>. During the training of the rationale generator, we employ the actual truthfulness label of each claim as input. Critically, we do not retrain or fine-tune MSP for this task. In the evaluation phase, we utilize the truthfulness label predicted by the fixed entailment models.\nFollowing Yao et al. (2023), we utilize BART to generate the ruling statement. Our evaluation metrics include ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and BERTScore (Zhang* et al., 2020). To assess the performance of explanation generation, we compare it with MOCHEG (Yao et al., 2023), as shown in Table 7, Figure 4 and 5.\nWe observe that our model outperforms MOCHEG's evidence-retrieval based method (\"system evidence\") on the rationale generation task. In our case, \"system evidence\" is our generated summary. We note that MOCHEG's method relies on retrieval from a pool of multimodal documents. The ground truth explanations rely on these sentences and thus may share some phrasing. This gives a slight advantage to MOCHEG's method on some metrics that measure n-gram overlap, whereas our method based on summarization may rephrase the same evidence. Nevertheless, we observe that our system outperforms MOCHEG's generated explanations.\nWe further observe that our explanations generated using system evidence and system truthfulness outperform MOCHEG's method, which relies on the ground truth truthfulness label on the BERTScore metric. Overall, these results demonstrate that our summarizer, which was not trained for the rationale prediction task, is capturing relevant evidence across modalities in a short summary better than MOCHEG's evidence retrieval-based approach."}, {"title": "6 Conclusion", "content": "We present MetaSumPerceiver (MSP), a summarization model crafted to generate concise and informative summaries specifically tailored for fact-checking claims in intricate multimodal datasets. The model's adaptable architecture can handle varying numbers of documents and input types, encompassing documents, images, and claims, by leveraging a perceiver-based design. We train our model using a RL approach, aiming to produce summaries that are instrumental in verifying the accuracy of claims. Moreover, our reward function is designed to generate more concise and clear summaries, aiding in the verification of diverse claims with the assistance of the LM critic. Our experimental assessments on the MOCHEG and our Multi-News-Fact-Checking datasets highlight MSP's robust performance in claim verification and explanation generation tasks and demonstrate its effectiveness in real-world fact-checking scenarios. This contribution underscores MSP's potential to streamline fact-checking processes in today's multimodal information landscape. Finally, we release the publicly accessible Multi-News-Fact-Checking dataset, aimed at assisting researchers in developing multi-document fact-checking methods."}, {"title": "7 Acknowledgements", "content": "We acknowledge Advanced Research Computing at Virginia Tech for providing computational resources and technical support that have contributed to the results reported within this paper. We also thank all reviewers for their comments which helped improve the paper."}, {"title": "8 Limitations", "content": "Given the societal importance of fact-checking applications, it is important that the limitations of our method be explored. Our experimental results reveal that the surrogate entailment model often assigns truthfulness labels for entailment even when it struggles to fully grasp the relationship between the claim and the summary with evidence. This issue not only impacts the judgment of the claim label but also affects MSP during training. One potential solution is using a textual entailment model adept at managing this uncertainty or excluding such instances during training. Secondly, Llama 2's claims in the Multi-News-Fact-Checking dataset have certain flaws. Our review suggests that neutral claims might mix consistent and conflicting details. Enhancing our data creation prompts or the prompts used in the second-stage claiming could boost Llama 2's understanding.\nOur model, trained on English text and topics from the Multi-News benchmarks, may not perform well in other languages without retraining. Care should be taken to ensure the model is trained on data that closely aligns with the target domain of interest, if possible, to minimize errors. Finally, our model relies on identifying relevant and trusted source documents on which to perform summarization and checking. While this document-level retrieval task is orthogonal to our research, failure to retrieve relevant documents will affect the downstream performance of the fact-checking system. If irrelevant documents are used, even true claims might be wrongly challenged. Thus, approaches should confirm that events and entities in sourced documents are directly related, employing sophisticated methods."}, {"title": "9 Appendix", "content": "9.1 Prompting For\nMulti-News-Fact-Checking Dataset\nIn this section, our main goal is to explain the dataset construction process. The following prompts include generating entailment, neutral, and contradiction claims from the Multi-News memorization dataset, ensuring each claim aligns with its corresponding label, and providing clear and concise claims, as depicted in Figure 6.\n\u2022 Prompt for the entailment claims: Task: You will be provided with a summary of a news article. Your goal is to generate a list of statements derived from the summary. These statements should be definitively true based solely on the information in the summary. Example summary: The unemployment rate dropped to 8.2% last month, but the economy only added 120,000 jobs, when 203,000 new jobs had been predicted, according to today's jobs report. Reaction on the Wall Street Journal's MarketBeat Blog was swift: \"Woah!!! Bad number.\" The unemployment rate, however, is better news; it had been expected to hold steady at 8.3%. But the AP notes that the dip is mostly due to more Americans giving up on seeking employment. You will be given a summary of a news article. Your job is to generate a list of entailment claims(true) from the summary. For example, if the summary says job growth was expected to be 100,000 jobs, but only was 80,000 jobs, one simple claim you might write could be \"Job growth missed expectations.\" Please write a numbered list of 10 claims from this summary (numbered 1. through 10.).\n\u2022 Prompt for the neutral claims: Task: You will be provided with a summary of a news article. Your goal is to generate a list of statements derived from the summary. These statements should not be definitively true or false based solely on the information in the summary. In other words, they should be ambiguous and require further investigation or context to determine their accuracy. Example: If the summary mentions that two celebrities are planning to get divorced, you might create a statement suggesting that their divorce might lead to significant financial and legal complications, assuming this information is not explicitly confirmed or denied in the article. Instructions: Review the provided summary. Create 10 statements based on the information in the summary. Each statement should be carefully crafted to be neither definitively true nor false based solely on the summary. Ensure that the truth or falsehood of these statements cannot be logically deduced from the summary alone. Avoid simply rephrasing or restating sentences from the summary; strive for creativity in your statement generation process. Avoid claims using statements like \"may\" or \"could\" - your claim should state things as a fact.\n\u2022 Prompt for the contradiction claims: Task: You will be provided with a summary of a news article. Your goal is to generate a list of statements derived from the summary. These statements should be definitively false based solely on the information in the summary. Example: If the summary mentions that a black race car starts up in front of a crowd of people., you might create a statement suggesting that a man is driving down a lonely road assuming this information is explicitly denied in the article. Instructions: Review the provided summary. Create 10 statements based on the information in the summary. Each statement should be carefully crafted to be definitively false based solely on the summary. Avoid simply rephrasing or restating sentences from the summary; strive for creativity in your statement generation process. Avoid claims using statements like \"may\" or \"could\" - your claim should state things as a contradiction fact.\n\u2022 Prompt for double-check claims: Task: You will be presented with a set of documents and one claim. Your objective is to discern the claim label based on the information in the documents. The claim labels include entailment, neutral, and contradiction. Entailment signifies that the claim is conclusively true based solely on the documents. The neutral label indicates that the claim should neither be true nor false based on the information provided. The contradiction label implies that the claim is entirely false based on the information presented in the documents.\n\u2022 Prompt for the clear and concise claims: You will be provided a summary that a fact-checker will use for fact-checking a claim. Your task is to evaluate the provided summary using a quality assessment metric that measures whether the summary is factual and written in a clear and concise manner. Good summaries provide facts useful for fact-checking (a general claim) and are short to ease the fact-checkers job. Given a summary, your job is to provide a number from 0 to 1 that indicates your assessment of the quality of the summary. Provide the evaluation score in the format: \"The quality score is <score>.\" The score should range from 0 to 1, where a score of 1 indicates high quality and a score of 0 signifies the lowest quality. The provided summary: <summary>\"\n9.2 Implementation Details\nWe used 4 NVIDIA A40 to run our experiments. Our model costs 180 GB and are trained for about 24 runs with a batch size of 256. In the preprocessing and evaluation parts, we use NLTK, ROUGE, and BERTScore. For claim verification, the learning rate \u2208 {10\u22124,10-5, 10-6} and batch size \u2208 {256, 480, 512}."}]}