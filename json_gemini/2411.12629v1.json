{"title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks", "authors": ["Nikhil Garuda", "John F. Wu", "Dylan Nelson", "Annalisa Pillepich"], "abstract": "Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies' halo masses (Mhalo) must be inferred indirectly. We present a graph neural network (GNN) model for predicting Mhalo from stellar mass (M*) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like random forests, our GNN captures the information-rich substructure of galaxy clusters by using spatial and kinematic relationships between galaxy neighbour. A GNN model trained on the TNG-Cluster dataset and independently tested on the TNG300 simulation achieves superior predictive performance compared to other baseline models we tested. Future work will extend this approach to different simulations and real observational datasets to further validate the GNN model's ability to generalise.", "sections": [{"title": "Introduction", "content": "In the Lambda Cold Dark Matter cosmological model [28, 4], galaxies form and evolve in dark matter halos. Cosmological simulations demonstrate that galaxies grow in tandem with their dark matter halos according to well-measured and tight scaling relations [39]. This interdependence between stellar mass (M) and subhalo mass (Mhalo) is known as the stellar-halo mass relation (SHMR).\nWhile M is observable, Mhalo must often be inferred indirectly via the SHMR due to observational constraints. For example, galaxy clusters-the most massive gravitationally bound objects in the Universe are dark matter dominated, but their total mass must be measured via gravitational lensing [8, 37], the Sunyaev-Zel'dovich effect [2, 22, 3], and/or visible wavelength proxies (e.g., galaxy richness, intracluster light, etc; [30, 31]). However, these methods are unable to fully leverage galaxy substructure within clusters to estimate their dark matter halo masses.\nTherefore, we present a graph neural network (GNN) algorithm [32] for predicting Mhalo for galaxies in simulated cluster environments\u00b9. Compared to primitive machine learning (ML) methods like random forests [1], a GNN can learn the substructure in neighbouring galaxies and thereby improve halo mass predictions. Our results using the GNN demonstrate significant performance gains on the training, validation, and an independent test set."}, {"title": "IllustrisTNG Simulation Data", "content": "The simulation data we use are large-volume, cosmological, gravo-magnetohydrodynamical sim- ulations from the IllustrisTNG simulation suite [25]. We specifically use the TNG-Cluster [24] simulation, a collection of zoom-in simulations centered 352 of the most massive halos (i.e., galaxy clusters), for training and validation. Our dataset is based on the SUBFIND [35] subhalo catalogs that were obtained from snapshot 99 (z = 0), focusing on the high-resolution components of the zoom-in simulation. We adopt cosmological parameters from [29], using H\u2080 = 67.74 km s\u207b\u00b9 Mpc\u207b\u00b9 for consistency with the IllustrisTNG simulation suite. Additional details about the TNG-Cluster training data are provided in Appendix A. The distribution of subhalos in TNG-Cluster is shown in Figure 4, and the selection criteria and number of samples are described in Table 2.\nWe test our ML models on an independent data set, the Illustris TNG300-1 hydrodynamic simulation (hereafter TNG300; [25]). The TNG-Cluster and TNG300 simulations use the same physics and have comparable resolutions (in the former's zoom-in regions), but the two simulations are otherwise independent. When reporting TNG-Cluster cross-validation results TNG300 test set results, we only consider galaxies within 10 Mpc of all clusters with Mhalo > 10\u00b9\u2074M\u2299."}, {"title": "Methods/Experiments", "content": "The primary objective of our study is to estimate Mhalo from M*. Building on the work of [18], we train ML models on galaxies and dark matter halos from TNG-Cluster to probe cluster environments.\nLoss Functions and Evaluation Metrics. Model performance is assessed using several metrics (presented in Table 1). Simple models are trained to minimise the Mean Squared Error (MSE), while the GNN is optimised using Gaussian negative log-likelihood (combining MSE and log-variance terms, per [15])\u00b2. Validation and test performance are evaluated with Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), coefficient of determination R\u00b2, Normalised Median Absolute Deviation (NMAD), average offset (Bias), and Outlier Fraction (foutlier).\nRandom Forest Baseline Models. To establish a benchmark for subsequent comparisons with our GNN model, we use Random Forest (RF) regression [14] as a baseline model due to its capability to handle complex non-linear relationships between features. To further augment the simple RF model, we compute an overdensity parameter (\u0394G), defined as the sum of stellar masses within a specified radius Rmax. The RF models are configured with 100 estimators using scikit-learn [27], one of which utilises M*, and one which uses both M* and \u0394G as features.\nGraph Neural Networks. In our GNN model, each node represents a galaxy, with the M* as the sole node feature. We construct edges between galaxy pairs separated by less than 3 Mpc [41], connecting neighbouring nodes. These connections enable the neural network to learn interactions between the substructure and galaxy properties within the cluster. We provide two edge features to incorporate both the spatial and kinematic separations of galaxies: the squared Euclidean distance between pairs of galaxy positions, and pairs of relative line-of-sight velocities.\nGNN Architecture. Our GNN follows the architecture described in [41] with 8 unshared layers and 3 sequential layers, as shown in Figure 1. Each layer is composed of a two-layer MLP with"}, {"title": "Results", "content": "Table 1 compares model performance for predicting Mhalo from galaxies residing in clusters for the validation and test datasets. We additionally show the scatter of Mhalo in the first row, which represents the most naive \u201cprediction\u201d of the sample mean. Below, we present the results for the baseline models and GNN model. We display scatter plots of the true versus predicted masses for the TNG-Cluster cross-validation data set in Figure 2a and TNG300 test set in Figure 2b.\nThe simplest RF model exhibits high error and very low predictive power.\u2074 When we augment the RF model with \u0394G, the performance improves, demonstrating that galaxy environments contain vital information for the SHMR. Nonetheless, the RF models systematically underpredict Mhalo for the highest-mass galaxies and yield high error.\nGNNs greatly outperform RF models, as indicated by the right-most panels of Figures 2a and 2b. Running the same experiments using XGBoost (which is more prone to overfitting), we find a significant improvement over RF but not enough to surpass GNNs. We find that the GNN performance on the training and validation sets translates to accurate predictions on the independent test set. For nearly all metrics in Table 1, the GNN outperforms the RF models for cross-validation and test sets."}, {"title": "Discussion", "content": "5.1 Model performance as a function of local environment\nIn Figure 3, we show the cross-validation RMSE as a function of distance from the cluster center for the GNN and RF (M* and \u0394G) models; the GNN significantly outperforms the RF across all distance bins. Notably, the RF model performance suffers for galaxies closer to the center of the cluster. One potential explanation for this discrepancy is that the RF does not account for the dense cluster environment, where interactions such as tidal stripping can lead to significant loss of Mhalo.\u2075 In contrast, the GNN model outperforms the RF due by leveraging information from galaxy pairwise distances and line-of-sight velocities.\n5.2 Comparison against previous work\nPrevious studies have used ML to estimate galaxy properties from dark matter halos [16, 1], i.e. the inverse of the problem we tackle. Some works employ feature importance from decision tree-based methods [21, 41], while others use reinforcement learning to connect halo properties to galaxies [23]. Convolutional neural networks (CNNs) and GNNs have also been used to predict galaxy stellar masses from simulated halos [5, 40, 41].\nSeveral works have used ML methods to predict cluster halo masses from observable parameters such as X-ray brightness and Sunyaev-Zel'dovich decrements [26, 13]. [42] compare how different cluster observables fare when pixelised as inputs to a CNN. [18] use GNNs to predict Mhalo directly from galaxy point clouds, but their training dataset (the much smaller TNG50 simulation) does not contain many rare galaxy clusters. Our work is the first to train and test GNNs for predicting halo masses in the extremely overdense regime of galaxy clusters."}, {"title": "Conclusions, Limitations and Future Work", "content": "In this work, we predict Mhalo for simulated galaxies using their stellar masses, 2D projected positions, and line-of-sight velocities (i.e., x, y, vz) with the TNG-Cluster simulation for training and TNG300 for testing. We evaluated both Random Forest (RF) models and Graph Neural Networks (GNNs). The key findings are:\n1. The GNN model significantly outperforms RF model, even when the latter is provided \u0394G as a parameter. This suggests that GNNs capture the underlying spatial relationships and substructures within clusters, as shown in Table 1 and Figures 2 and 3.\n2. The GNN maintains its predictive power when tested on the independent TNG300 dataset, demonstrating that the model generalises across the IllustrisTNG simulation suite.\nDespite our promising results, models trained on one simulation may face challenges when applied to other simulations or real observational data. Machine learning models are often susceptible to domain shift, where their performance degrades when applied to datasets that differ from their training data [36, 17]. In our case, the comparable performance between the TNG-Cluster cross- validation and TNG300 test datasets suggests that the GNN model may be robust to domain shift within the IllustrisTNG suite. This robustness could be attributed to the GNN's ability to learn generalizable symbolic relationships [10]. Further tests using other simulation physics or with observed datasets (e.g., galaxies at other redshifts) are needed before we can conclude that this method is fully generalizable.\nIn future work, we will account for observational effects like contaminating galaxies in projection, missing data, and photometric redshift uncertainties, as well as broader concerns about domain shift in ML (see e.g. [7]). Aside from additional validation on other cosmological simulations [33], we will test on observational data using published Mhalo estimates for well-known galaxy clusters (e.g., [20, 38]). With upcoming telescopes like the Roman Space Telescope [34] and Rubin Observatory [9], we will be able to study GNN applications to large galaxy cluster samples in the wide-field domain."}, {"title": "A TNG-Cluster Additional Details", "content": "Galaxies in the TNG-Cluster training data are shown in Figure 4. To mimic astronomical observations of galaxies, we project the galaxy clusters along the z axis, which is chosen to be the line of sight. This procedure bridges the gap between simulation data and spectroscopic observations, which typically capture two spatial dimensions (x, y) and line-of-sight velocities (vz). We also apply quality cuts to the simulation in Table 2 to ensure a complete sample of massive, well-resolved galaxies.\nWe split the TNG-Cluster data into training and validation sets by implementing a k-fold cross- validation strategy based on cluster IDs rather than traditional random splits. This method isolates subhalos according to their cluster IDs while ensuring that all subhalos from a single cluster remain within the same fold. One potential caveat of this method is that we do not include the contaminating structure along the line-of-sight from other clusters which might be in a different k-fold."}]}