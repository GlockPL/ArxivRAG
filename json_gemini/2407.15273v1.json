{"title": "Unifying Invariant and Variant Features for Graph\nOut-of-Distribution via Probability of Necessity and\nSufficiency", "authors": ["Xuexin Chen", "Ruichu Cai", "Kaitao Zheng", "Zhifan Jiang", "Zhengting Huang", "Zhifeng Hao", "Zijian Li"], "abstract": "Graph Out-of-Distribution (OOD), requiring that models trained on biased\ndata generalize to the unseen test data, has considerable real-world appli-\ncations. One of the most mainstream methods is to extract the invariant\nsubgraph by aligning the original and augmented data with the help of en-\nvironment augmentation. However, these solutions might lead to the loss or\nredundancy of semantic subgraphs and result in suboptimal generalization.\nTo address this challenge, we propose exploiting Probability of Necessity and\nSufficiency (PNS) to extract sufficient and necessary invariant substructures.\nBeyond that, we further leverage the domain variant subgraphs related to\nthe labels to boost the generalization performance in an ensemble manner.\nSpecifically, we first consider the data generation process for graph data.\nUnder mild conditions, we show that the sufficient and necessary invari-\nant subgraph can be extracted by minimizing an upper bound, built on the\ntheoretical advance of the probability of necessity and sufficiency. To fur-\nther bridge the theory and algorithm, we devise the model called Sufficiency", "sections": [{"title": "1. Introduction", "content": "Graph representation learning with Graph Neural Networks (GNNs) has\ngained remarkable success in complicated problems such as intelligent trans-\nportation and the inverse design for polymers [49, 7]. Despite their consider-\nable success, GNNs generally assume that the testing and training graph data\nare independently sampled from the identical distribution (IID). However,\nthe validity of this assumption is often difficult to guarantee in real-world\nscenarios.\nTo solve the Out Of Distribution (OOD) challenge of graph data, one of"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Problem Setup", "content": ""}, {"title": "Graph generation process", "content": "Generating predictions that can generalize\nout of distribution requires understanding the actual mechanisms of the task\nof interest. Following previous works [61, 10], here we present a generation\nprocess of graph data behind the graph classification task, by inspecting\nthe causalities among five variables: input graph G, ground-truth label Y,\ndomain invariant subgraph C, domain variant subgraph S and environment\nE, where noises are omitted for simplicity. Figure 2 illustrates the causal\ndiagram, where each link denotes a causal relationship between two variables.\nC\u2192G\u2190S indicates that the input graph G consists of two disjoint\ncomponents: the invariant subgraph C and the variant or unstable subgraph\nS, according to whether they are affected by environment E, such as the\norange and green components of G\u2081 in Figure X. Moreover, C \u2192 Y \u2192 S\nindicates C is partially informative about Y, i.e., (S, E) | Y | C [10]. C \u2192 Y\nindicates the labeling process, which assigns labels Y for the corresponding\nG merely based on C. Taking the house classification example in Figure X\nagain, C of input graph G\u2081 can be the G9, G10 or G11 (which one is better\nwill be discussed in Section 2.2), which perfectly explains why the graph is\nlabeled as \"house\"."}, {"title": "2.2. Probability of Necessity and Sufficiency", "content": "Current OOD generalization methods on graph data mainly focus on\nlearning only domain invariant features for label prediction. However, domain-\ninvariant features can be divided into three categories, each of which has\ndifferent effects on graph label prediction.\n1) Sufficient but unnecessary causes. Knowing cause A leads to\neffect B, but when observing effect B, it is hard to confirm A is the actual\ncause. For example, the domain invariant feature G9 can predict the label\n\"house\", but a graph with the label \u201chouse\" label might not contain this\nfeature, such as G2. 2) Necessary but insufficient causes. Knowing\neffect B we confirm the cause is A, but cause A might not lead to effect B.\nFor example, if the input graph does not contain the invariant feature G11,\nthen we can confirm that the label of this graph is not \u201chouse\u201d. However,\ngraph G4 with the \u201cgrid\u201d label also has the same invariant feature G11 as\na \"house\u201d. Thus invariant feature G11 is not a stable feature to predict\nhouses. 3) Necessary and sufficient causes. Knowing effect B we confirm\nthe cause is A, and we also know that A leads to B. In the \u201chouse\" and\n\"grid\" classification tasks, invariant feature G10 could be a necessary and\nsufficient cause. It is because G10 allows humans to distinguish a \u201chouse\u201d\nfrom a \"grid\", and when we know there is a \u201chouse\u201d, G10 must exist. In\nconclusion, the accuracy of predictions based on domain-invariant features\nwith sufficient necessary information will be higher than those based on other\ntypes of invariant features.\nIn order to learn sufficient and necessary domain invariant features of\nthe input graph, we resort to the concept of Probability of Necessity and\nSufficiency (PNS) [47], which is defined as follows.\nDefinition 1. (Probability of necessity and sufficiency (PNS) [47]) Let the\nspecific values of domain invariant variable C and label Y becandy. The\nprobability that C = c is the necessary and sufficiency cause of Y = y is\n$PNS(C=c, Y = y) := P(Y_{do(C=c)}=y | C\\neq c, Y\\neq y) P(C \\neq c,Y \\neq y) \\newline + P(Y_{do(C\\neq c)}\\neq y | C=c,Y=y) P(C=c,Y=y).$"}, {"title": "3. Theory: Unifying Invariant and Variant Features for Graph\nOOD via PNS", "content": "In this section, motivated by the fact that not necessary or not sufficient\ninvariant features may be harmful to domain generalization on the graph,\nwe present our main theoretical result which shows how to unify invariant\nand variant subgraphs for graph OOD via PNS. We begin by describing how\nto extract necessary and sufficient invariant information by identifying the\nsubspace of the variable C. Since not every graph contains this invariant sub-\ngraph, we describe how to alleviate this problem by reconstructing P(Y|C, S')\nfrom P(Y|C) and P(C, S) which involves exploiting domain variant features\nto enhance domain generalization."}, {"title": "3.1. Necessary and Sufficient Invariant Subspace Learning", "content": "In this section, supposing that we have already identified the domain\ninvariant subgraph C, we analyze the problem of extracting the necessary\nand sufficient invariant features about C from G. We first reduce it to an\noptimization problem for PNS, i.e., identify the subspace of C with the largest\nPNS with respect to C and Y given a graph G. However, PNS is usually\nintractable because counterfactual data are not available. We show this issue\ncan be solved exactly by deriving the lower bound of PNS for optimization.\nWe now formalize the two key assumptions underlying our approach.\nThese assumptions below will help us derive the lower bound of PNS based\non conditional probability.\nDefinition 2. (Exogeneity [48]) Variable C is exogenous relative to variable\nY if C and Y have no common ancestor in the graph generation process.\nDefinition 3. (Consistency [48, 23]) If variable C is assigned the value c,\nthen the observed outcome Y is equivalent to its outcome Ydo(C=c) of inter-\nvention; i.e., if C = c, then Y = Ydo(C=c)\u00b7"}, {"title": "Reduction to the optimization problem for PNS", "content": "Suppose we have\nused the training data to learn the invariant GNN gc \u00b0 fc and thus know\nP(C|G) \u2248 fc(G) and P(Y|C) \u2248 gc(C), and recall that our goal is to predict\nY using the necessary and sufficient invariant features about C from G. Thus,\nour task becomes to reconstruct P(C|G), that is, to find a subspace in C\nthat contains all necessary and sufficient causes for the label Y of G. By\nDefinition 1, a trivial solution is to find the subspace of C for a given G\nthat maximizes PNS. However, computing the intervention probability is a\nchallenging problem since collecting the counterfactual data is difficult, or\neven impossible in real-world scenarios, it is not feasible to optimize PNS\ndirectly. Fortunately, motivated by probabilities of causation theory [47],\nwe show that the lower bound of PNS can be theoretically identified by the\nobservation data under proper conditions.\nTheorem 1. (Lower bound of PNS). Consider two random variables C and\nY. If exogeneity and consistency assumptions hold, then the lower bound of\nPNS(C = c, Y = y) is as follows:\n$max(\\frac{P(Y = y|C = c) - P(Y = y)}{1 - E_{G}[P(C = c|G)]}, 0) \\le PNS(c, y)$"}, {"title": "3.1.1. PNS Risk", "content": "Based on the PNS lower bound (Eq. 2), this section presents the PNS\nrisk which shows how to estimate the subspace of variable C. The objective"}, {"title": "3.2. Ensemble Learning with Variant Features", "content": "In this section, to mitigate the negative impact of not sufficient or not\nnecessary invariant subgraphs on prediction, our key observation is that if\nthe domain variant subgraph S is related to the label Y, then we can improve\nprediction accuracy by ensembling predictions based on unstable or domain\nvariant subgraphs specific to the test domain e. Based on this observation,\nwe describe a boosted joint GNN in the test domain e as a combination of\nan invariant GNN gc\u3002fc and an unstable GNN gs,e\u25e6fs training on the\ndomain variant subgraphs on test domain e. The unstable GNN gs,e \u25e6 fs is\ncomposed of a domain variant subgraph extractor fs : G \u2192 (0,1)n\u00d7n across\ndomains, and a domain-specific classifier gs,e on test domain e, where n is\nthe number of nodes of G. Then, the boosted joint GNN is denoted by\nY = COMBINE((gc(fc(G)), gs,e(fs(G)))."}, {"title": "Reduction to the marginal problem", "content": "Suppose we have enough unla-\nbeled data from test domain e to learn P(C, S|E = e), our goal is to predict\nY from (C, S) in test domain e. Our key observation is that if we can de-\ncompose P(Y|C, S, E) into two terms, P(Y|C') and P(Y|S, E), then we can\nutilize P(Y|C, S) to achieve optimal prediction of Y from (C, S, E). Thus,\nour task is broken down into the reconstruction of P(Y|C, S) from P(Y|C')\nand P(Y|S, E). With the help of a marginal problem theory [13], the theo-\nrem below demonstrates that, under our assumption of causal relationships\nbetween variables in Figure 2, we can exactly recover these terms. To sim-\nplify notation, we assume the label Y is binary and leave the multi-class\nextension for Appendix C.\nTheorem 2. Consider variables C, S, Y, and E, where Y is binary (Y\u2208\n{0,1}). If S \u22a5 C|Y and C \u22a5 Y, then the distribution P(Y=1|C, S, E) can\nbe decomposed into three components: P(Y|C), P(Y|S, E), and P(Y = 1).\nSpecifically, if \u0176 ~ Bernoulli(P(Y=1|C)) is a pseudo-label, then we have\nP(Y=1|C, S, E)=\u03c3(logit(P(Y=1|C))+logit(P(Y=1|S, E))+logit(P(Y=1)))\n$\\begin{aligned}\n&P(Y = 1|S, E) = \\frac{P(\\hat{Y} = 1|S, E) + P(\\hat{Y} = 0|Y = 0) - 1}{P(\\hat{Y} = 0|Y = 0) + P(\\hat{Y} = 1|Y = 1) - 1},\n\\\\&P(Y = 1|Y = 1) = \\frac{\\mathbb{E}_{C}[P(Y = 1|C)^2]}{\\mathbb{E}_{C}[P(Y = 1|C)]},\n\\end{aligned}$"}, {"title": "4. Algorithm: Sufficiency and Necessity Inspired Graph Learning", "content": "In this section, we will leverage the above theoretical results and propose a\ndata-driven method called Sufficiency and Necessity Inspired Graph Learning\n(SNIGL) to employ necessary and sufficient invariant subgraphs and domain\nvariant subgraphs specific to the test domain e for domain generalization. On\nthe training domains, we describe learning an invariant GNN that extracts\nnecessary and sufficient invariant features and an unstable GNN that ex-\ntracts domain variant features. On the test domain, we then describe how to\ncombine these features to enhance the performance of domain generalization."}, {"title": "4.1. Training domains: Learning necessary and sufficient invariant and vari-\nant subgraphs.", "content": "Our goal in the training domains is to learn three modules foc, g\u00f8e and\nfos parameterized by \u03b8\u00ba, \u0444\u00ba and \u03b8s, respectively. The first two modules are\nthe estimated necessary and sufficient invariant subgraphs extractor foc and\nits downstream classifier g\u00f8e, which form the invariant GNN g\u00f8co foc. The\nthird module is the variant subgraphs extractor f\u0259s across domains that will\nbe employed to adapt a classifier g\u00f8s,e specific to the test domain e, which\nform the unstable GNN g\u00f8s,e o fos in the test domain e.\nTo achieve these learning goals, let RINV denote the risk of learning in-\nvariant GNNs proposed by existing methods, Rjoint denotes the risk (e.g.,\ncross entropy) of the joint predictions COMBINE(g\u00f3c foc, g\u00f8s,e' o fos) (Eq. 6)\nof the invariant GNN g\u00f8c o fee and unstable GNN gps,e' o fos specific to the\ntraining domain e', and Rc\u0131 denote the penalty encouraging conditional in-\ndependence C\u22a5 S|Y. Technologically, we estimate modules foc and g\u00f8e in\ntwo steps. First, we encourage the estimated invariant GNN g\u00f8e o foc to\nlearn an invariant feature space through the invariant risk. Second, use our"}, {"title": "Implementation of Necessary and Sufficient Invariant Subgraphs Ex-\ntractor foc and Domain Variant Subgraphs Extractor fos", "content": "We employ the following implementation of fee and fes to generate an\ninvariant subgraph c and variant subgraphs, which can be formalized as\nfollows. We first assume that variables C and S follow multivariate Bernoulli\ndistributions with the parameters BC\u2208(0,1)n\u00d7n and BS\u2208(0, 1)n\u00d7n, i.e., C ~\nBernoulli(BC) and S ~ Bernoulli (BS), where n is the number of nodes in\ngraph G. Technologically, we estimate the parameters BC and BS of the\nBernoulli distribution in three steps. First, we use two layer graph neural\nnetwork (GNN) to generate the node embeddings Ze and Zs. Second, we\ncalculate the parameter matrices BC and BS, which denote the probability\nof the existence of each edge of C and S. Third, we sample C and S from\nthe estimated distributions. In summary, the aforementioned three steps can\nbe formalized as follows:\n$\\begin{aligned}\n&Z^{c} = f_{ \\theta_{C} }(G) := GNN(G; \\theta^{c}) \\in \\mathbb{R}^{n \\times d^{c}}, B^{c} = \\sigma(Z^{c} Z^{c T}), C \\sim Bernoulli(B^{c}),\\\\&Z^{s} = f_{ \\theta_{s} }(G) := GNN(G; \\theta^{s}) \\in \\mathbb{R}^{n \\times d^{s}}, B^{s} = \\sigma(Z^{s} Z^{s T}), S \\sim Bernoulli(B^{s}),\n\\end{aligned}$"}, {"title": "Implementation of Invariant Classifier g\u00f8 and Variant Classifier g\u00f8s,e", "content": "To estimate the predicted class probabilities, we use the READOUT (e.g.,\nmean) [67] function aggregates node embeddings to obtain the entire graph's\nrepresentation, and employ different three layer Multilayer Perceptron (MLP)\nwith SOFTMAX function activation to estimate the probabilities for each\ndifferent environment, as follows:\n$\\begin{aligned}\n&P(Y = y|C = c) = g_{ \\phi_{C} }(c)_{y} := SOFTMAX(MLP(READOUT(Z_{c})))_{y},\\\\&P(Y = y|S = s, E = e) = g_{ \\phi_{s, e} }(s)_{y} := SOFTMAX(MLP_{e}(READOUT(Z_{s})))_{y},\n\\end{aligned}$"}, {"title": "Implementation of Poc(C = c|G) and P(Y = y|E = e)", "content": "To estimate the component Poc(C = c|G) of PNS risk RNS (Eq. 3), our\nmain idea is to draw samples from Pac(C|G) and estimate the probability\nPoc(C = c|G) by counting how many samples are isomorphic to c. Since\ngraph isomorphism is an NP-hard problem, we simplify this problem by cal-\nculating the similarity between their graph representations. First, based\non our implementation of Pac(C|G) (Section 4.1.1), we draw k subgraphs\nC1,..., Ck from P(CG). Second, referring to Eq. 10-11, we feed these sub-\ngraphs C1, ..., Ck, and c into the GNN(\u00b7; 0\u00ba) and use the READOUT function\nto obtain their graph representations. Third, compute the similarity (we\nadopt the inner product) between cand c\u2081, ..., ck in terms of their represen-\ntation and average these similarities. The three steps can be summarized as\nfollows:\n$P_{\\theta_{c}}(C = c|G) \\approx \\frac{1}{k}\\sum_{i=1}^{k} \\sigma(\\tilde{Z}(c_{i})^T\\tilde{Z}(c)),\\\\\\tilde{Z}(c_{k}) = READOUT(GNN(G_{c_{k}}; \\theta^{c})), \\tilde{Z}(c) = READOUT(GNN(G^{c}; \\theta^{c}), $"}, {"title": "4.2. Test-domain Adaptation Without Labels", "content": "Given the trained invariant GNN g\u00f8c o fee and the domain varying feature\nextractor fos, our goal in the test domain is to adapt a classifier g\u00f8s,e specific\nto test domain e learned on top of trained fs, so that we can make optimal use\nof the domain variant features extracted from fos. By Theorem 2, this goal\ncan be achieved through the following three steps. First, given the unlabelled\ntest domain data {Gi}=1, compute soft pseudo-labels {Y}_1 with\n$ y = argmax_{y} g_{\\phi_{c} } (f_{\\theta_{c} }(G_{i})).$\nSecond, letting l : Y\u00d7Y \u2192R be a loss function (e.g., cross entropy), fit the\nbiased classifier \u011d\u00f8s,e (S) specific to test domain e on pseudo-labelled data\n{(Si = fos,e (Gi), \u0177i)} with\n$\\min_{\\phi_{s, e}}\\frac{1}{n} \\sum_{S_{i}, \\hat{Y}_{i}} l(g_{\\phi_{s, e}}(S_{i}), \\hat{y}_{i}).$\nThird, through optimizing Eq. 15, we are given the trained biased classifier\ng\u00f8s,e (S) specific to the test domain e. By Theorem 2, we calibrate \u011d\u00f8s,e (S)\nas follows:\n$\\begin{aligned}\ng_{\\phi_{s, e}}(s_{i}) &= \\frac{\\hat{g}_{\\phi_{s, e}}(s_{i}) + \\epsilon_{0} - 1}{\\epsilon_{0} + \\epsilon_{1} - 1},\\\\\n\\epsilon_{1} &= \\frac{\\sum_{G_{i}} g_{\\phi_{c}}(f_{\\theta_{c}}(G_{i}))}{\\sum_{G_{i}}},\\  \\epsilon_{0} = \\sum_{G_{i}}\\frac{1 - g_{\\phi_{c}}(f_{\\theta_{c}}(G_{i}))}{1 - \\hat{g}_{\\phi_{c} }(f_{\\theta_{c}}(G_{i}))}.   \n\\end{aligned}$"}, {"title": "5. Experiments", "content": "In this section, we evaluate the effectiveness of our proposed SNIGL model\non both synthetic and real-world datasets by answering the following ques-\ntions.\n\u2022 Q1: Whether the proposed SNIGL can outperform existing state-of-the-\nart methods in terms of model generalization.\n\u2022 Q2: Can the proposed PNS risk learning necessary and sufficient invariant\nlatent subgraphs well?\n\u2022 Q3: Do ensemble strategies that exploit domain-varying subgraphs benefit\nmodel performance?\n\u2022 Q4: What are the learning patterns and insights from SNIGL training?\nIn particular, how do invariant or variant subgraphs help improve gener-\nalization?"}, {"title": "5.1. Experimental Setup", "content": ""}, {"title": "5.1.1. Dataset", "content": "To evaluate the effectiveness of our proposed SNIGL, we utilize six public\nbenchmarks under different distribution shifts for graph classification tasks,\nincluding two synthetic datasets Spurious-Motif-Mixed [65] and GOOD-Motif [19],\nas well as four real-world datasets GOOD-HIV [19], OGBG-Molsider, OGBG-\nMolclintox and OGBG-Molbace [24]. Table 1 summarizes the statistics of\nseven datasets."}, {"title": "Baselines", "content": "We compare the proposed SNIGL method with three categories of meth-\nods, namely the state-of-the-art OOD methods from the Euclidean regime,\nand from the graph regime, as well as the conventional GNN-based methods.\nThe OOD methods from the graph regime include:"}, {"title": "5.2. Comparison to baselines", "content": "In this section, we answer Question Q1: how effective is our approach\ncompared to existing methods? As shown in Table 2, we can find that our\nSNIGL method outperforms the other baselines with a large margin in dif-\nferent biases on the standard SPMotif-Mixed dataset, and in different split\nmethods (i.e., structure, scaffold, size) on GOOD-Motif and GOOD-HIV\ndatasets. In particular, we can obtain the following conclusions. 1) GALA\nachieves the second best performance on SP-Motif-Mixed under bias b = 0.5\nand b = 0.9, while Mixup and GSAT achieve the second best performance on\nGOOD-Motif under motif-splitting and size-splitting, respectively, as well as\nCoral and DANN achieve the second best performance on GOOD-HIV un-\nder scaffold-splitting and size-splitting, respectively. Our proposed SNIGL\nis capable of achieving further improvements against GALA by 11.2% and\n11.1% on SP-Motif-Mixed under bias b = 0.5 and b = 0.9, against Mixup\nand GSAT by 11.2% and 4.1% on GOOD-Motif, as well as against Coral\nand DANN by 3.0% and 1.7% on GOOD-HIV, indirectly reflecting that our\nmethod can extract the invariant subgraphs with the property of necessity\nand sufficiency. 2) We also find that the performance drops with increasing\nbiases, showing that over heavy bias can still influence generalization. 3) By\ncomparing the variance of different methods, we can find that the variance of\nsome baselines is large, this is because these methods generate the invariant\nsubgraph by trading off two objects, which might lead to unstable results.\nIn the meanwhile, the variance of our method is much smaller, reflecting the\nstability of our method.\nNext, we further compare\nthe effectiveness of our SNIGL\non more real-world datasets.\nThe experiment results on the\nOGB datasets are shown in\nTable 3. According to the ex-\nperiment results, we can draw\nthe following conclusions. 1)\nThe proposed SNIGL outper-\nforms all other baselines on\nall the datasets, which is at-\ntributed to both the suffi-\nciency and necessity restric-\ntion for invariant subgraphs\nand the ensemble training\nstrategy with the help of the\ndomain variant subgraphs.\n2) Some conventional GNN-\nbased methods such as GCN\nand GIN do not achieve the ideal performance, reflecting that these methods\nhave limited generalization. 3) The causality-based baselines also achieve\ncomparable performance and the methods based on environmental data aug-\nmentation achieve the closest results, reflecting the usefulness of the envi-\nronment augmentation. However, since it is difficult for these methods to\nextract necessary and sufficient invariant subgraphs, the experimental re-\nsults of these methods, such as StableGNN on Mollintox, DIR on Molsider\nand Mollintox, are difficult to achieve ideal results."}, {"title": "5.3. Ablation Study", "content": "As Section 4 states, the PNS risk and the ensemble strategy of combin-\ning domain-variant and domain-invariant features are key components in our"}]}