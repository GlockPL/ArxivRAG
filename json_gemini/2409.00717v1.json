{"title": "Multi-Agent Reinforcement Learning from Human Feedback: Data Coverage and Algorithmic Techniques", "authors": ["Natalia Zhang", "Xinqi Wang", "Qiwen Cui", "Runlong Zhou", "Sham M. Kakade", "Simon S. Du"], "abstract": "We initiate the study of Multi-Agent Reinforcement Learning from Human Feedback (MARLHF), exploring both theoretical foundations and empirical validations. We define the task as identifying Nash equilibrium from a preference-only offline dataset in general-sum games, a problem marked by the challenge of sparse feedback signals. Our theory establishes the upper complexity bounds for Nash Equilibrium in effective MARLHF, demonstrating that single-policy coverage is inadequate and highlighting the importance of unilateral dataset coverage. These theoretical insights are verified through comprehensive experiments. To enhance the practical performance, we further introduce two algorithmic techniques. (1) We propose a Mean Squared Error (MSE) regularization along the time axis to achieve a more uniform reward distribution and improve reward learning outcomes. (2) We utilize imitation learning to approximate the reference policy, ensuring stability and effectiveness in training. Our findings underscore the multifaceted approach required for MARLHF, paving the way for effective preference-based multi-agent systems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved significant progress in natural language interaction, knowledge acquisi- tion, instruction following, planning and reasoning, which has been recognized as the sparks for AGI [Bubeck et al., 2023]. The evolution of LLMs fosters the field of agent systems, wherein LLMs act as the central intelligence [Xi et al., 2023]. In these systems, multiple LLMs can interact with each other as well as with external tools. For instance, MetaGPT assigns LLM agents various roles, akin to those in a technology company, enabling them to cooperate on complex software engineering tasks [Hong et al., 2023]. Despite some empirical successes in agent systems utilizing closed-source LLMs, finetuning these systems and aligning them with human preferences remains a challenge. Reinforcement learning from human feedback (RLHF) has played an important role in aligning LLMs with human preferences [Christiano et al., 2017, Ziegler et al., 2019]. However, unexpected behavior can arise when multiple LLMs interact with each other. In addition, reward design has been a hard problem in multi-agent reinforcement learning [Devlin et al., 2011]. Thus, it is crucial to further align the multi-agent system with human feedback. We address this problem through both theoretical analysis and empirical experiments. Theoretically, we characterize the dataset coverage condition for MARLHF that enables learning the Nash equilibrium, which serves as a favorable policy for each player. Empirically, we validate our theoretical insights through comprehensive experiments utilizing the proposed algorithmic techniques."}, {"title": "1.1 Contributions and Technical Novelties", "content": "1. Necessary and Sufficient Dataset Coverage Condition for MARLHF. In single-agent RLHF, [Zhu et al., 2023] demonstrated that single policy coverage is sufficient for learning the optimal policy. However, we prove that this condition no longer holds for MARLHF by providing a counterexample. Instead, we introduce an algorithm that operates under unilateral coverage, a condition derived from offline MARL [Cui and Du, 2022a, Zhong et al., 2022]. Specifically, this condition requires the dataset to cover all unilateral deviations from a Nash equilibrium policy. For further details, see Section 4. 2. Algorithmic Techniques for Practical Performance. As a foundational exploration into MARLHF research, we focus on employing the simplest learning framework, incorporating only the essential techniques necessary to ensure the approach's feasibility. The framework consists of three key components: 1) leveraging the preference dataset to learn a reward function, 2) mitigating extrapolation errors using a reference policy, and 3) determining the final policy. However, additional algorithmic techniques are needed in order to find a strong policy even if we have a dataset with good coverage according to the theory. \\u2022 Reward regularization. We observed that the reward learned through standard Maximum Likelihood Estimation (MLE) is sparse and spiky, making it difficult for standard RL algorithms to utilize effectively (cf. Figure 2 (b2)). To address this, we introduce an additional Mean Squared Error (MSE) loss between the predictions of adjacent time steps as a form of regularization. This regularization helps to prevent the model from accumulating reward signals solely at the final time step or relying on reward-irrelevant observation patterns, which could otherwise result in the complete failure in producing meaningful predictions. \\u2022 Imitation learning policy as a reference. We choose to follow the standard RLHF work [Wang et al., 2023b], where the extrapolation problem was alleviated by adding an extra KL reward to the final optimization objective. Since we do not have access to the actual reference model, we implement imitation learning across the entire dataset to approximate it. The final policy is trained using a DQN-based Value Decomposition Network (VDN) [Mnih et al., 2013, Sunehag et al., 2017]. Our ablation study demonstrates the effectiveness of selecting an appropriate KL reward coefficient (see Table 3). 3. Experiment Results. Our experiments, following the pipeline described above, confirm the theoretical necessity of unilateral coverage. We conducted comprehensive ablation studies on three cooperative Multi-Agent Particle Environment (MPE) scenarios [Mordatch and Abbeel, 2017]: Spread-v3, Tag-v3, and Reference-v3. These studies focused on the hyperparameter selection for the reward regularization coefficient, KL reward coefficient, and dataset diversity. The empirical results (Table 2) demonstrate that: 1) simply adding trivial trajectories to expert demonstrations can enhance performance, 2) unilateral datasets are advantageous, and 3) dataset diversity contributes to lower variance."}, {"title": "2 Related Works", "content": "Reinforcement Learning from Human Feedback (RLHF). RLHF (or preference-based RL) plays a pivotal role in alignment with various tasks such as video games [Warnell et al., 2018, Brown et al., 2019], robotics [Jain et al., 2013, Kupcsik et al., 2016, Christiano et al., 2023, Shin et al., 2023], image augmentation [Metcalf et al., 2024], and large language models [Ziegler et al., 2020, Wu et al., 2021, Nakano et al., 2022, Menick et al., 2022, Stiennon et al., 2022, Bai et al., 2022, Glaese et al., 2022, Ganguli et al., 2022, Ouyang et al., 2022]. Additionally, a body of work focuses on the reward models behind preference data [Sadigh et al., 2017, B\u0131y\u0131k and Sadigh, 2018, Gao et al., 2022, Hejna and Sadigh, 2023]. Recently, direct preference optimization (DPO, Rafailov et al. [2023]) and its variants [Azar et al., 2023, Rafailov et al., 2024] approach RLHF without directly handling the reward model. Theoretical studies have also explored guarantees, such as sample complexity and regret, and the limitations of certain RLHF algorithms [Novoseller et al., 2020, Xu et al., 2020, Pacchiano et al., 2023, Chen et al., 2022, Razin et al., 2023, Zhu et al., 2024a, Wang et al., 2023b, Xiong et al., 2024, Zhu et al., 2024b]. Offline Reinforcement Learning. Offline RL [Lange et al., 2012, Levine et al., 2020] has achieved success in a wide range of real-world applications, including robotics [Pinto and Gupta, 2015, Levine et al., 2016, Chebotar et al., 2021, Kumar et al., 2023], healthcare [Raghu et al., 2017, Wang et al., 2018], and autonomous driving [Shi et al., 2021, Lee et al., 2024]. Key algorithms such as Behavior Cloning, BRAC [Wu et al., 2019], BEAR [Kumar et al., 2019], and CQL [Kumar et al., 2020, Lyu et al., 2024] have driven these successes. Theoretical research on offline RL has primarily focused on sample complexity under various dataset coverage assumptions Le et al. [2019], Chen and Jiang [2019], Yin et al. [2020], Rashidinejad et al. [2023], Yin et al. [2021, 2022], Shi et al. [2022], Nguyen-Tang et al. [2022], Xie et al. [2022], Xiong et al. [2023], Li et al. [2024]. Multi-Agent Reinforcement Learning (MARL). Many real-world scenarios are naturally modeled as multi-agent environments, whether cooperative or competitive. As a result, MARL has gained popularity in video games [Tian et al., 2017, Vinyals et al., 2017, Silver et al., 2017, Vinyals et al., 2019], network design [Shamsoshoara et al., 2018, Kaur and Kumar, 2020], energy sharing [Prasad and Dusparic, 2018], and autonomous driving [Palanisamy, 2019, Yu et al., 2020, Zhou et al., 2022]. Prominent algorithms in MARL include IQL [Tan, 2003], MADDPG [Lowe et al., 2020], COMA [Foerster et al., 2017], MAPPO [Yu et al., 2022], VDN [Sunehag et al., 2017], and QMIX [Rashid et al., 2018]. Offline MARL. Offline MARL is a practical solution for handling sophisticated multi-agent environments. Em- pirically, to address issues related to out-of-distribution actions and complex reward functions, previous works have developed algorithms such as MABCQ [Jiang and Lu, 2023], ICQ-MA [Yang et al., 2021], OMAR [Pan et al., 2022], and OMIGA [Wang et al., 2023a], which incorporate regularization or constraints on these actions and functions. MOMA-PPO [Barde et al., 2024] is a model-based approach to offline MARL that generates synthetic interaction data from offline datasets. Tseng et al. [2022] combines knowledge distillation with multi-agent decision transformers [Meng et al., 2022] for offline MARL. Theoretical understanding of offline MARL, particularly in the context of Markov games, has been advanced by works that provide sample complexity guarantees for learning equilibria Sidford et al. [2019], Cui and Yang [2020], Zhang et al. [2023a, 2020], Abe and Kaneko [2020], Cui and Du [2022a,b], Zhang et al. [2023b], Blanchet et al. [2023], Shi et al. [2023]."}, {"title": "3 Preliminaries", "content": "General-sum Markov Games. We consider an episodic time-inhomogeneous general-sum Markov game $\\mathcal{M}$, consisting of $m$ players, a shared state space $\\mathcal{S}$, an individual action space $\\mathcal{A}_i$ for each player $i \\in [m]$ and a joint action space $\\mathcal{A} = \\mathcal{A}_1 \\times \\mathcal{A}_2 \\times \\cdots \\mathcal{A}_m$. The game has a time horizon $H$, an initial state $s_1$, state transition probabilities $\\mathcal{P} = (\\mathcal{P}_1, \\mathcal{P}_2, \\ldots, \\mathcal{P}_H)$ with $\\mathcal{P}_h : \\mathcal{S} \\mathcal{A} \\rightarrow \\triangle(\\mathcal{S})$, and rewards $\\mathcal{R} = {\\mathcal{R}_h(\\cdot | s_h, a_h)}_{h=1}^H$ where $\\mathcal{R}_{h,i} \\in [0,1]$ represents the random reward for player $i$ at step $h$. At each step $h \\in [H]$, all players observe current state $s_h$ and simultaneously choose their actions $a_h = (a_{h,1}, a_{h,2}, \\ldots, a_{h,m})$. The next state $s_{h+1}$ is then sampled from $\\mathcal{P}_h(\\cdot | s_h, a_h)$, and the reward $r_{h,i}$ for player $i$ is sampled from $\\mathcal{R}_{h,i}(\\cdot | s_h, a_h)$. The game terminates at step $H+1$, with each player aiming to maximize the total collected rewards. We use $\\pi = (\\pi_1, \\pi_2,\\cdots, \\pi_m)$ to denote a joint policy, where the individual policy for player $i$ is represented as $\\pi_i = (\\pi_{1,i}, \\pi_{2,i},\\cdots, \\pi_{H,i})$, with each $\\pi_{h,i} : \\mathcal{S} \\rightarrow \\triangle(\\mathcal{A}_i)$ defined as the Markov policy for player $i$ at step $h$. The state"}, {"title": "4 Dataset Coverage Theory for MARLHF", "content": "In this section, we study the dataset coverage assumptions for offline MARLHF. For offline single-agent RLHF, [Zhu et al., 2023, Zhan et al., 2023] shows that single policy coverage is sufficient for learning the optimal policy. However, we prove that this assumption is insufficient in the multi-agent setting by constructing an counterexample. In addition, we prove that unilateral policy coverage is adequate for learning the Nash equilibrium."}, {"title": "4.1 Policy Coverages", "content": "We quantify the information contained in the dataset using covariance matrices, as the rewards and transition kernels are parameterized by a linear model. With a slight abuse of the notation, for trajectory $\\tau = (s_1, a_1, s_2, a_2, \\ldots, s_{H+1})$, we use $\\psi(\\tau) := [\\psi(s_1,a_1), \\psi(s_2, a_2),\\ldots,\\psi(s_h, a_h)]$ to denote the concatenated trajectory feature. The reward coverage is measured by the preference covariance matrix: $\\Sigma = \\lambda I + \\sum_{(\\tau,\\tau') \\in \\mathcal{D}} (\\psi(\\tau) - \\psi(\\tau')) (\\psi(\\tau) - \\psi(\\tau'))^\\top,$ where $\\psi(\\tau) - \\psi(\\tau')$ is derived from the preference model. Similarly, the transition coverage is measured by the covariance matrix: $\\Sigma_h = \\lambda I + \\sum_{(\\tau,\\tau') \\in \\mathcal{D}} [\\psi(s_h, a_h)\\psi(s_h, a_h)^\\top + \\psi(s'_h, a'_h)\\psi(s'_h, a'_h)^\\top].$ For a given state and action pair $(s_h, a_h)$, the term $||\\psi_h(s_h, a_h)||_{[\\Sigma_h]^{{-1}}}$ measures the uncertainty in reward estimation and $||\\psi(s_h, a_h)||_{[\\Sigma^\\mathcal{P}_h]^{{-1}}}$ measures the uncertainty in transition estimation. As a result, the overall uncertainty of a given policy $\\pi$ with dataset $\\mathcal{D}$ is measured by $\\mathcal{U}_\\mathcal{D}(\\pi) := \\mathbb{E} \\pi \\Big[\\sum_{h=1}^H || \\psi_h (s_h, a_h) ||^2_{[\\Sigma]^{{-1}}} + \\sum_{h=1}^H || \\psi_h (s_h, a_h) ||^2_{[\\Sigma^\\mathcal{P}_h]^{{-1}}}\\Big].$"}, {"title": "Definition 1. For a Nash equilibrium $\\pi^*$, different policy coverages are measured by the following quantities:", "content": "\\u2022 Single policy coverage: $\\mathcal{U}_\\mathcal{D}(\\pi^*)$. \\u2022 Unilateral policy coverage: $\\max_{i,\\pi_{-i}} \\mathcal{U}_\\mathcal{D}(\\pi_i, \\pi^*_{-i})$. \\u2022 Uniform policy coverage: $\\max_{\\pi} \\mathcal{U}_\\mathcal{D}(\\pi)$. Intuitively, small $\\mathcal{U}_\\mathcal{D}(\\pi^*)$ indicates that the dataset contains adequate information about $\\pi^*$. A small $\\max_{i,\\pi_{-i}} \\mathcal{U}_\\mathcal{D}(\\pi_i, \\pi^*_{-i})$ implies that the dataset covers all of the unilateral deviations of $\\pi^*$, and small $\\max_{\\pi} \\mathcal{U}_\\mathcal{D}(\\pi^*)$ suggests that the dataset covers all possible policies."}, {"title": "4.2 Single Policy Coverage is Insufficient", "content": "Our objective is to learn a Nash equilibrium policy from the dataset, which necessitates that the dataset sufficiently covers the Nash equilibrium. In the single-agent scenario, if the dataset covers the optimal policy, pessimism-based algorithms can be employed to recover the optimal policy. However, previous work [Cui and Du, 2022a, Zhong et al., 2022] has demonstrated that single policy coverage is insufficient for offline MARL. We extend this result to the context of offline MARL with preference feedback, as follows: Theorem 1. (Informal) If the dataset only has coverage on the Nash equilibrium policy (i.e. small $\\mathcal{U}_\\mathcal{D}(\\pi^*)$), it is not sufficient for learning an approximate Nash equilibrium policy. The proof is derived by a reduction from standard offline MARL to MARLHF. Suppose that MARLHF with single policy coverage suffices, we could construct an algorithm for standard offline MARL, which leads to a contradiction. The formal statement and the detailed proof are deferred to Appendix A.1."}, {"title": "4.3 Unilateral Policy Coverage is Sufficient", "content": "While single policy coverage is too weak to learn a Nash equilibrium, uniform policy coverage, though sufficient, is often too strong and impractical for many scenarios. Instead, we focus on unilateral policy coverage, which offers a middle ground between single policy coverage and uniform policy coverage. Theorem 2. (Informal) If the dataset has unilateral coverage on the Nash equilibrium policy, there exists an algorithm that can output an approximate Nash equilibrium policy. The detailed proof is deferred to Appendix A.2. We leverage a variant of Strategy-wise Bonus and Surrogate Minimiza- tion (SBSM) algorithm in [Cui and Du, 2022b] with modified policy evaluation and policy optimization subroutines. Intuitively, the algorithm identifies a policy that minimizes a pessimistic estimate of the Nash gap. As a result, if the dataset has unilateral coverage, the output policywill have a small Nash gap and serves as a good approximation of the Nash equilibrium."}, {"title": "5 Algorithmic Techniques for Practical Performance", "content": "In Section 4, we provided a theoretical characterization of the dataset requirements for MARLHF. However, the algorithm used in Theorem 2 is not computationally efficient. In this section, we propose a practical algorithm for MARLHF and validate our theoretical findings through experiments."}, {"title": "5.1 High-level Methodology", "content": "Our MARLHF pipeline consists of two phases: In the first step, we train a reward prediction model $\\gamma_{\\phi}$ and approximate the behavior policy $\\pi_{\\theta}$ using imitation learning; in the second step, we then apply an MARL algorithm to maximize a combination of the KL-divergence-based reward and standardized predicted reward $\\gamma_{\\phi}$, ultimately deriving the final policy $\\pi_w$. Step 1: Reward Training and Reference Approximation. Given the preference signals of trajectories, we use neural networks to predict step-wise rewards $\\gamma_{\\phi}(s_h, a_h)$ for each agent, minimizing the loss defined in (1). The objective is to map $(s, a)$-pairs to reward values such that the team returns align with the preference signals. At the same time, in order to utilize KL reward $ - \\log \\frac{\\pi_w(s,a)}{\\pi_{\\theta}(s,a)}$ to cope with the extrapolation error in offline learning, an imitation learner $\\pi_{\\theta}$ is trained over the entire dataset to model the behavior policy $\\pi^b$. Step 2: Offline MARL. Although in this work, VDN is chosen as the MARL oracle, it should be noted that other MARL architectures are also applicable. With the reward model $\\gamma_{\\phi}$ and the approximated reference policy $\\pi_{ref}$ learned in the first step, we are now able to construct a virtual step-wise reward for each agent. The agents are then trained to maximize the target defined in (3). Given this framework, additional techniques are required to build a strong practical algorithm, which we provide more details below."}, {"title": "5.2 Reward Regularization", "content": "Compared to step-wise reward signals, preference signals are $H$ times sparser, making them more challenging for a standard RL algorithm to utilize effectively. Concretely, this reward sparsity causes the naive optimization of the negative log-likelihood (NLL) loss to suffer from two key problems: 1. Sparse and spiky reward output. When calculating NLL losses, spreading the reward signal along the trajectories is equivalent to summing it at the last time step (Figure 2a). However, a sparse reward signal is harder for traditional RL methods to handdle due to the lack of continuous supervision. More uniformly distributed rewards across the entire trajectory generally leads to more efficient learning in standard RL algorithms. 2. Over-reliance on irrelevant features. The model may exploit redundant features as shortcuts to predict rewards. For instance, expert agents in cooperative games usually exhibit a fixed pattern of collaboration from the very beginning of the trajectory (such as specific actions or communication moves). The reward model might use these patterns to differentiate them from agents of other skill levels, thereby failing to capture the true reward-observation causal relationships. To mitigate these problems, we introduce an extra Mean Squared Error (MSE) regularization along the time axis (Equation 1, 2). By limiting the sudden changes in reward predictions between adjacent time steps, this regularization discourages the reward model from concentrating its predictions on just a few time steps. While these issues can also be mitigated by using more diversified datasets and adding regularization to experts to eliminate reward-irrelevant action patterns, these approaches can be costly and sometimes impractical in real-world applications. In contrast, our MSE regularization is both easy to implement and has been empirically verified to be effective, creating more uniform reward distribution (Figure 2) and better performances."}, {"title": null, "content": "$\\mathcal{L}_{RM}(\\varphi) = - \\mathbb{E}_{ \\mathcal{D}} \\sum_{i=1}^m \\Big[ \\log \\sigma ( y_i (\\gamma_{\\varphi,i} ( \\tau_1) - \\gamma_{\\varphi,i} ( \\tau_2) ) )  + \\frac{\\alpha}{\\text{Var}_{ \\varphi} (\\tau)}  -  \\mathcal{L}_{MSE}(\\varphi, \\tau) \\Big],$ where the regularization term $\\mathcal{L}_{MSE}$ is defined as: $\\mathcal{L}_{MSE}(\\varphi, \\tau) = \\mathbb{E}_{ \\mathcal{D}} [\\sum_{h=1}^{H-1} ||r(s_h, a_h) - r_{\\varphi}(s_{h+1}, a_{h+1}) ||_2^2 ].$"}, {"title": "5.3 Imitation Learning Policy as Reference", "content": "There are various methods to mitigate the over-extrapolation errors in offline RL [Peng et al., 2019, Nair et al., 2021], including conservative loss over the Q-function [Kumar et al., 2020] and directly restricting the learned policy actions to those within within the dataset [Fujimoto et al., 2019]. For simplicity and consistency with the former RLHF framework [Ouyang et al., 2022], we adopt the same per-step KL reward for enforcing restrictions between $\\pi_b$ and $\\pi_w$. In many scenarios, a direct reference to the behavior policy is not accessible. As an alternative, imitation learning is used to estimate the reference policy, $\\pi_{ref} \\approx \\pi_b$. To stabilize training, we standardize $\\gamma_{\\phi}$ over $\\mathcal{D}$ before combining it with the KL reward to make it comparable:"}, {"title": null, "content": "$objective(w) = \\mathbb{E}_{\\tau \\sim \\pi_w}  \\sum_{h=1}^H [ \\gamma_{\\text{std}}(s_h, a_h, \\varphi) - \\beta \\log \\frac{\\pi_w(s_h, a_h)}{\\pi_{ref}(s_h, a_h)} ],$ where $\\beta$ is the KL reward coefficient, set to be (1, 1, 3) in Spread-v3, Reference-3 and Tag-v3 respectively, and the standardized reward $\\gamma_{\\text{std}}$ is defined as: $\\gamma'_{\\text{std}}(s_h, a_h, \\varphi) = \\sum_{i=1}^m \\frac{\\gamma_{\\phi_i}(s_h, a_{h,i}) - \\mathbb{E}_{\\mathcal{D}} (\\gamma_{\\phi})}{\\sqrt{\\text{Var}_{ \\mathcal{D}} (\\gamma_{\\phi})}}.$ Notice that the explicit expression of $\\pi_w(s_h, a_h)$ only exists in methods with an actor model, which leads to certain planning-based algorithms, such as Q-learning methods, being unable to directly use Equation 3. To enhance the compatibility of our pipeline, we substitute 1/|$A$| for $\\pi_w(s_h, a_h)$ in such cases. Although this substitution formally removes the KL reward, in algorithms like Q-learning that output deterministic policies, the replaced reward is equivalent to encouraging the agent to match its deterministic choices with the actions most preferred by the reference policy. In other words, it seeks the closet element in the deterministic policy family to the reference policy, which is a degradation of the KL reward restricted in the domain of deterministic policy. The effectiveness of this method is validated in the ablation study (cf. Section 6.4)."}, {"title": "6 Experiments", "content": "We design a series of experiments to validate our theories and methods in common general-sum games. Specifically, we first use VDN Sunehag et al. [2017] to train expert agents, and take intermediate checkpoints as rookie agents. Then, we use these agents to collect datasets and use the Broadley-Terry model over standardized returns to simulate human preference. Experiments are carried out to verify the efficiency of our approach with unilateral policy dataset coverage (in Theorem 2) while single policy coverage is insufficient (stated in Theorem 1). We also design ablation studies to showcase the importance of our methods, particularly focusing on reward structure regularization and imitation poclies as reference models."}, {"title": "6.1 Environments", "content": "Our experiments involved 3 Multi-Agent Particle Environments (MPE) implemented with JaxMARL codebase [Ruther- ford et al., 2023]. Specifically, we utilized the Spread, Tag, and Reference scenarios, which covers three different levels of communication requirements. Spread-v3 contains a group of agents and target landmarks, where the objective is to cover as many landmarks as possible while avoiding collisions. Tag-v3 contains two opposing groups, where quicker \"preys\" need to escape from \"predators\". To ensure a fair comparison of different predator cooperation policies, we fixed a pretrained prey agent. Reference-v3 involves two agents and potential landmarks, where the agents need to find each one's target landmark to receive a high reward. The target landmark of each agent is only known by the other agent at first, necessitating communication before they can move toward the correct landmark. A more detailed description of the tasks and their associated challenges is provided in Appendix B.2."}, {"title": "6.2 The Importance of Dataset Diversity", "content": "To study the influence of diversity of dataset, we manually designed 4 kinds of mixed joint behavior polices, and change their ratios to form different datasets."}, {"title": "6.3 Experiments for Reward Regularization", "content": "In Figure 2, we examined the effectiveness of our proposed reward regularization technique. Figure 2a demonstrates that without regularization, the learned rewards tend to be sparse and spiky compared to the ground truth rewards. The predicted rewards in (b1) and (b3) were learned using our reward regularization, while those in (b2) were learned without it ($\\alpha$ = 0). Our results indicate that the reward regularization technique produces much smoother reward functions. We also quantitatively compare learned rewards with and without using reward regularization in finding the policy (cf. the first and the last column of Table 3). We also observe that the rewards often exhibit temporal continuity, which can create greater discrepancies with the sparse, pulse-like ground truth. Notably, we found that adding stronger regularization does not necessarily lead to underfitting of the reward model; in some cases, it even helps the model converge to a lower training loss. Detailed"}, {"title": "6.4 Experiments for Imitation Learning", "content": "Intuitively, the choice of the KL coefficient $\\beta$ depends on the quality of the reward model $\\varphi$ and the dataset. As shown in Table 3, simpler environments (e.g., Spread and Reference) require less KL restriction, while harder environments (e.g., Tag) need stronger restriction to avoid suffering from the extrapolation problem. A large $\\beta$ can amplify the approximation error of imitation learning, potentially overshadowing the reward signal, whereas a small $\\beta$ may fail to introduce enough pessimism to effectively mitigate extrapolation errors. In environments with relatively simple reward structures, such as Spread-v3 and Reference-v3, the standard choice of $\\beta$ = 1 is near optimal. However, in more complex scenarios where the reward is harder to derive, increasing $\\beta$ to 3 yields better performance."}, {"title": "7 Discussion", "content": "In this paper, we proposed dedicated algorithmic techniques for offline MARLHF and provided theoretical justification for the unilateral dataset coverage condition. We believe our work is a significant step towards systematically studying MARLHF and offers a foundational framework for future research in this area. The flexibility of our framework allows for application across a wide range of general games, and our empirical results validate the effectiveness of our proposed methods in various scenarios. Looking ahead, there is significant potential to extend this work to more complex, real-world scenarios, particularly by integrating Large Language Models (LLMs) into multi-agent systems. Future research will focus on fine-tuning"}, {"title": "A Missing Proofs in Section 4", "content": null}, {"title": "A.1 Single Policy Coverage is Insufficient", "content": null}, {"title": "Theorem 3. (Restatement of Theorem 1) For any algorithm and constant $C > 0$, there exists a Markov game and a compliant dataset with $\\mathcal{U}_\\mathcal{D}(\\pi^*) < C$ such that the output policy is at most an 0.5-Nash equilibrium.", "content": "Proof. We construct two linear Markov games with a shared compliant dataset such that no policy is a good approximate Nash equilibrium in both Markov games. Similar to [Cui and Du, 2022a], we consider Markov games with $H = 1$, $m = 2$, $\\mathcal{A}_1 = {a_1, a_2}$ and $\\mathcal{A}_2 = {b_1, b_2}$ with deterministic reward presented in Table 4. The feature mapping for these two games is $\\psi(a_1, b_1) = e_1$, $\\psi(a_1, b_2) = e_2$, $\\psi(a_2, b_1) = e_3$, $\\psi(a_2, b_2) = e_4,$ where $e_i \\in \\mathbb{R}^d$ are the unit base vectors. Directly we have the reward parameters $\\theta$ as the rewards. The behavior policy is $\\pi^b(a_1, b_1) = \\pi^b(a_2, b_2) = 1/2$ and dataset is $\\mathcal{D} = {(\\tau_i, \\tau'_i, y_i)}_{i=1}^N$ with $\\tau_i, \\tau'_i \\in {(a_1, b_1), (a_2, b_2)}, y_i \\sim \\text{Ber}(\\exp(r_1(\\tau_i) - r_1(\\tau'))).$ As the dataset covers the Nash equilibrium for both games, with enough samples, we have $\\mathcal{U}_\\mathcal{D}(\\pi^*) < C$ for any constant C. Suppose the output policy of the algorithm is $\\pi = (\\mu, \\nu)$, then $\\pi$ is at most 0.5-Nash equilibrium in one of these two games."}, {"title": "A.2 Unilateral Policy Coverage", "content": null}, {"title": "Algorithm 2 Value Estimation", "content": "1: Input: Offline dataset $\\mathcal{D"}, "player index $i$, policy $\\pi$. 2: Initialization: $V_{H+1,i}(s) = 0$. 3: for $h = H, H - 1, \\cdots , 1$ do 4: $\\hat w_{h,i} = [\\Sigma_h"], "5": "Q_{h", "6": "V_{h", "7": "end"}