{"title": "TPO: ALIGNING LARGE LANGUAGE MODELS WITH\nMULTI-BRANCH & MULTI-STEP PREFERENCE TREES", "authors": ["Weibin Liao", "Xu Chu", "Yasha Wang"], "abstract": "In the domain of complex reasoning tasks, such as mathematical reasoning, recent\nadvancements have proposed the use of Direct Preference Optimization (DPO)\nto suppress output of dispreferred responses, thereby enhancing the long-chain\nreasoning capabilities of large language models (LLMs). To this end, these stud-\nies employed LLMs to generate preference trees via Tree-of-thoughts (ToT) and\nsample the paired preference responses required by the DPO algorithm. However,\nthe DPO algorithm based on binary preference optimization was unable to learn\nmultiple responses with varying degrees of preference/dispreference that provided\nby the preference trees, resulting in incomplete preference learning. In this work,\nwe introduce Tree Preference Optimization (TPO), which does not sample paired\npreference responses from the preference tree; instead, it directly learns from the\nentire preference tree during the fine-tuning. Specifically, TPO formulates the lan-\nguage model alignment as a Preference List Ranking problem, where the policy\ncan potentially learn more effectively from a ranked preference list of responses\ngiven the prompt. In addition, to further assist LLMs in identifying discriminative\nsteps within long-chain reasoning and increase the relative reward margin in the\npreference list, TPO utilizes Adaptive Step Reward to adjust the reward values of\neach step in the trajectory for performing fine-grained preference optimization.\nWe carry out extensive experiments on mathematical reasoning tasks to evaluate\nTPO. The experimental results indicate that TPO consistently outperforms DPO\nacross three publicly large language models on four datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Long-chain reasoning task, such as commonsense reasoning\nand math reasoning, is one of the critical capabilities in large language models (LLMs). This task is particularly challenging as it often involves numerous reasoning steps. Any mis-\ntake in these steps can lead to an incorrect final answer. Initially, some studies utilized various data\naugmentation techniques during the supervised fine-tuning (SFT) phase to enhance the reasoning ca-\npabilities of LLMs. However, a phenomenon\nof pessimism suggests that the positive feedback provided by SFT alone cannot prevent LLMs from\ngenerating erroneous reasoning pathways. indicated that, during the SFT phase,\nas the probability of preferred outputs increases, the probability of dispreferred outputs also rises.\nThis phenomenon makes the models more prone to errors in long-chain reasoning. Consequently, it\nis necessary to develop methods to mitigate the likelihood of dispreferred outputs.\nRecently, Direct Preference Optimization (DPO) has been proposed for align-\ning LLMs using paired preference data. Compared to the traditional Reinforcement Learning from\nHuman Feedback (RLHF) framework, DPO has gained popularity due to\nits simplicity and reduced memory requirements. Recent studies have utilized DPO to suppress"}, {"title": "2 PRELIMINARIES", "content": "Reinforcement Learning from Human Feedback (RLHF) is an effective\nmethod for enhancing the robustness, authenticity, and safety of LLMs , which\ndirectly optimizes LLMs according to human preferences by maximizing the reward value of the\nmodel's responses. The reward function is defined based on the Bradley-Terry (BT) model of\npreferences. Specifically, for preferred response yw and dispreferred response y\u0131\nunder the same prompt x and data distribution D, the BT model stipulates that the human preference\ndistribution p* can be expressed as:\np(yw > y\u0131 | x) = \u03c3(r*(x, yw) \u2013 r*(x, y\u0131))  (1)\nwhere p (yw > y\u0131) denotes the probability that yw is preferred against y\u03b9, \u03c3(x) = 1/(1+exp(-x))\ndenotes the sigmoid function, and r* denotes some latent reward model, which we do not have\naccess to. The alignment of language models is commonly regarded as an optimization problem\nwith a Kullback-Leibler (KL) constraint on reward values, formalized as follows:\nmax E[r*(x, y)]\ns.t. Ex~DDKL [\u03c0\u03b8(Y | x)||\u03c0ref(Y | x)] \u2264 \u03c3  (2)\nwhere \u03c0\u03b8 denotes the aligned policy model, \u03c0ref denotes the reference policy model. To prevent\nreward hacking and ensure that \u03c0\u03bf does not deviate too much from the Tref , a\nregularization term is typically added to the objective function , the problem\nis transformed into:\nmax E[r*(x, y)] \u2013 \u03b2DKL [\u03c0o(Y | x)||\u03c0ref(Y | x)] (3)\nwhere the hyperparameter \u03b2 controls the KL divergence between \u03c0\u03b8 and Tref. In general, RLHF\nencompasses two training phases, including reward model training, and policy model training. How-\never, the ultimate performance of RLHF is highly sensitive to various hyperparameters across these"}, {"title": "2.2 TREE-STRUCTURED REASONING POLICY FOR LLM", "content": "Following the standard reasoning setup of LLMs, given a policy \u3160 instan-\ntiated by LLM and an input prompt x, \u03c0 can step-by-step generate a trajectory of reasoning steps\ny = (s1,\u2026, sK) ~ \u03c0(\u00b7|x) by autoregressively predicting the next token. The standard reasoning\nsetup assumes that y encompasses the complete list of reasoning steps, with each step sk comprising\nmultiple tokens. The step-by-step long-chain reasoning process is most famously used in Chain-of-\nThought (CoT).\nSelf-Consistency was the first to introduce multi-\nbranch reasoning. Given an input prompt x, the policy \u3160 generates N trajectories of reasoning steps\ny = (y1,\u2026, yN) ~ \u03c0(\u00b7|x), where yi = (s1,\u2026, sK). Ultimately, Self-Consistency selects the\nmost probable final answer by marginalizing over the reasoning trajectories.\nRecent works have further extended CoT and Self-\nConsistency to a tree-like structure, referred to as the Tree-of-Thoughts (ToT). Specifically, ToT no longer confines its application to the initial prompt but extends to engaging\nin branching reasoning at any intermediate state subsequent to given steps. Given the state S =\n[x, s1,..., sK-1] of an LLM in the reasoning trajectory, ToT employs a Thought Generator G(\u03c0, S, N)\nto propose N next planning steps [s(1)k,\u2026, s(N)k]. Compared to CoT, ToT possesses a broader space\nfor cognitive exploration and can circumvent the generation of repetitive responses within the same\ncontext."}, {"title": "3 METHODOLOGY", "content": "We propose Tree Preference Optimization (TPO), a preference learning algorithm tailored for pref-\nerence trees generated by LLMs via Tree-of-Thoughts. TPO learns a preference list with varying re-\nward values using a Preference List Ranking objective, and utilizes Adaptive Step Reward to achieve\nfine-grained alignment of step rewards."}, {"title": "3.1 ALIGNING LLMS FROM MULTI-BRANCH PREFERENCES", "content": "Preference modeling can be regarded as a more general Preference List Ranking problem: the\npolicy w is capable of learning from non-binary data with varying degrees of preference, facilitating\nmore effective alignment for language models.\nTPO defines the dataset D =\n{(x(i),y(i), v(i))}M i=1 with M samples: given a\nprompt x, there is a response list y = (y1,\u2026, yN) of size N, and each response y is associated\nwith a reward value v. The responses y are generated by the policy \u03c0, while the reward values v are\nderived from human raters or an inaccessible reward model. Typically, v = (v1,\u2026, vN) \u2208 [0, 1]N,\nwith higher reward values indicating better responses."}, {"title": "3.2 ALIGNING LLMS FROM MULTI-STEP PREFERENCES", "content": "The reward margin for each step of the preference pair is not equivalent. By adaptively modify-\ning step rewards, the policy \u03c0 can learn discriminative information between preference pairs using\nnon-shared trajectories.\nTPO follows the definition of multi-step reasoning as described in Sec. 2.2,\nintroducing y = (s1, s2,\u2026\u2026, sK) consisting of K steps. Due to the characteristics inherent in tree-\nstructured reasoning, for any two reasoning trajectories yi = (s1,\u2026, sK) and yj = (s'1,\u2026, s'K),\n(To simplify, TPO assumes that yi and yj possess steps of equal length.) there exist sub-trajectories\nwhich are content sharing or action sharing.\nyi and yj have traversed the same sub-trajectory (s1,\u2026\u2026, sK-1) and\nbranched off at state Sk. Due to the (s(i)k \u2260 s(j)k) ~ \u03c0(\u00b7|x), which resulting in S(i)k \u2260 S(j)k\nExpanding on content sharing, even though the (s(i)k \u2260 s(j)k), the high\ndegree of semantic similarity or the execution of identical actions results in S(i)k = S(j)k."}, {"title": "Adaptive Step Reward", "content": "In the naive DPO algorithm, the \u201cimplicit reward\u201d margin is step-\nindependent, that is, for responses yi and yj, the \u201cimplicit reward\u201d margin is mathematically defined\nas follows:\nRM = Blog - Blog =\n\u2211(Blog - Blog ) (9)\nTo mitigate the reduced reward margin resulting from shared steps, TPO introduces the Adaptive\nStep Reward mechanism to discriminatively assign rewards for each step. Specifically, TPO employs\nadaptive weight w to adjust reward margin between step pairs, and instantiates w as cosine similarity\nin the semantic space. The adaptive RM can be mathematically expressed as follows:\nRM = \u2211((1+\n)Blog - Blog ) (10)\nwhere emb(\u00b7) is the operation for semantic vectors generation. It is worth noting that when s(i)k and\ns(j)k manifest as steps of content sharing, the current step pairs yields a provided RM = 0.\nUltimately, the overall algorithm of TPO is detailed in Alg. 1."}, {"title": "4 EXPERIMENTS", "content": "Our experiments were based on various base models, including Qwen2\nmodels of various sizes (Qwen2-1.5B-Instruct and Qwen2-7B-Instruct), and the\nDeepSeekMath-7B-Instruct that has been specifically fine-tuned for mathematical\ntasks. We also introduced DeepSeekMath-7B-RL , which underwent reinforce-\nment learning by , as the baseline model.\nTypically, when faced with complex mathematical problems, LLMs struggle to\narrive at the correct final answer even when employing ToT methods. To ensure that the preference\ntree can generate trajectories capable of reasoning to the correct answer, we have expanded upon the\nexisting dataset. proposed a dataset that provides 10,795 paired preference data,\ncompletely composed of mathematical problems, with complete correct and incorrect reasoning\ntrajectories provided for each problem. As shown in Fig. 2(a), starting from any intermediate step"}, {"title": "4.2 EXPERIMENT RESULTS", "content": "We conducted evaluations on four mathematics reasoning datasets to verify\nthe performance of TPO on in-distribution datasets. We employed the CoT strategy\nfor reasoning without using any demonstrations.\nTPO comprehensively outperformed the DPO algorithm across all datasets, across various LLM size\nsettings (Qwen2-1.5B-Instruct and Qwen2-7B-Instruct) and whether the LLMs were fine-tuned in\nthe domain of mathematics (Qwen2-7B-Instruct and DeepSeekMath-7B-Instruct). In many cases,\nTPO also surpassed existing specialized reinforcement learning methods (DeepSeekMath-7B-RL\nWe further conducted evaluations on two coding datasets to verify the\nperformance of TPO on out-of-distribution datasets."}, {"title": "4.3 ANALYSIS OF DISPREFERRED RESPONSES", "content": "We performed DPO using preference pairs\nwith different reward values and evaluated the results using Qwen2-7B-Instruct on the ASDiv\nand GSM-Plus datasets. Specifically, we employed correct reasoning trajectories as preferred\nresponses and sampled dispreferred responses with different reward distributions sampled from\nincorrect trajectories. The mean reward values with corresponding standard deviations were\n[7.4\u00b167.7, 56.3\u00b150.9, 75.4\u00b135.5, 86.4\u00b119.5]. Our experimental results are presented in Fig. 3(a).\nThe results indicate that dispreferred responses with different reward values have varying degrees\nof impact on the model's performance. Fig. 3(a) shows that when dispreferred responses with lower\nmean rewards (strong dispreference) or higher mean rewards (weak dispreference) are used, the\nperformance of DPO is inferior. However, the best performance of DPO is observed when dispre-\nferred responses with a moderate mean reward are used. We argue that some dispreferred responses\nwith lower rewards are less valuable for learning due to their significant discrepancy from preferred\nresponses. Conversely, dispreferred responses with higher rewards pose challenges for the DPO\nalgorithm to learn because of their smaller difference from preferred responses. Therefore, it is\nnecessary to select dispreferred responses with moderate rewards to facilitate more effective DPO\nlearning. Nonetheless, TPO still outperforms all DPO baselines, suggesting that introducing more"}, {"title": "5 LIMITATIONS AND FUTURE WORKS", "content": "Despite the promising results obtained in our work, it is important to acknowledge the limitations.\nThe first limitation is that TPO may introduce a stronger form of \"catastrophic forgetting\". The\nresults in Sec. 4.2 indicate that while TPO exhibits excellent performance on in-distribution datasets,\nit may suffer from performance degradation on out-of-distribution datasets. We provide a more in-\ndepth discussion in Sec. 4.2 and attribute this issue to \"catastrophic forgetting\" . Existing strategies to mitigate \u201ccatastrophic forgetting\u201d include memory\nreplay, regularization constraints, and meta-learning , among others. Incorporating these\ntechniques into the TPO training procedure could potentially improve the generalization of TPO on\nout-of-distribution datasets.\nThe second limitation is due to the imbalanced distribution of the preference tree reward values, as\nshown in Fig. 2(c). We analyze the reasons for this as follows: (1) Autoregressive LLMs, including\nChatGPT, tend to assign either high or low values. Although we employ the Re-\nACT strategy to prompt ChatGPT to provide a more reasonable evaluation, this issue remains to be\naddressed. (2) Our data generation pipeline, as depicted in Fig. 2(a), adopts a strategy of generating\nadditional responses starting from correct trajectories. This strategy ensures that the preference tree\ncontains at least one preferred response and that the reasoning generated from intermediate nodes\nincludes some correct reasoning paths to diversify the reward values. However, once the preceding\ntrajectory in the generated path already includes the key steps to solve the problem, the subsequent\nsteps become easily inferable, leading to a higher distribution of reward values in the preference\ntree. In future work, we aim to introduce more effective ToT strategies, such as MCTS ,\nto ensure the generation of higher-quality data. Additionally, we will employ techniques such"}, {"title": "6 CONCLUSIONS", "content": "In this work, we propose TPO, a preference learning algorithm designed specifically for preference\ntrees as input. TPO enhances DPO by addressing two critical issues: (1) DPO only supports binary\npreference input and cannot model preferences from preference lists with varying reward values. (2)\nDPO exhibits a lower reward margin when dealing with reasoning involving long chains of trajec-\ntories with shared sub-trajectories. We evaluate the effectiveness of TPO on extensive experiments,\nand the experimental results indicate that TPO consistently outperforms DPO on in-distribution data\nand shows promise for its generalization to out-of-distribution data."}, {"title": "A APPENDIX", "content": "We employ the following prompts to synthesize the relevant\ndata for preference trees. To ensure that the generated trajectories contain some correct reasoning\nsteps, we provide the initial few reasoning steps in the prompts and allow LLMs to generate the\nsubsequent reasoning steps.\nWe utilize the fol-\nlowing prompts to instruct ChatGPT to score the reasoning trajectories within preference trees. To\nensure the reliability of the scores, we provide ChatGPT with genuine reasoning trajectories as a\nreference and employ the ReACT to facilitate ChatGPT in generating the scoring rationale.\nIn our assessment of TPO performance, we employ\nthe following prompts to address relevant Math tasks, including MATH , SVAMP , ASDiv\nand GSM-Plus datasets.\nIn our assessment of\nTPO performance, we employ the following prompts to address relevant Coding task on Hu-\nmanEval dataset.\nIn our assessment of TPO perfor-\nmance, we employ the following prompts to address relevant Coding task on MBPP dataset."}, {"title": "A.2 DETAILED EXPERIMENTAL RESULTS ON THE MATH DATASET", "content": "Table. 4 and Table. 5, respectively, present the experimental results at different levels of difficulty\nfor the MATH dataset and the experimental outcomes for various types of problems on the MATH\ndataset. The experimental results indicate that the TPO consistently outperforms the DPO algorithm\nacross all cases, with the TPO achieving the best performance in the majority of cases."}]}