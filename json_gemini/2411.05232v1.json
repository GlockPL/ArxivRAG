{"title": "Academic Reviews Enhance LLM Long-Context Capabilities", "authors": ["Shengzhi Li", "Kittipat Kampa", "Rongyu Lin", "Bohang Li", "Shichao Pei"], "abstract": "Large language models (LLMs) have shown remarkable performance across various tasks, yet their ability to handle long-context reading remains challenging. This study explores the effectiveness of leveraging high-quality academic peer review data for fine-tuning LLMs to enhance their long-context capabilities. We compare the Direct Preference Optimization (DPO) method with the Supervised Fine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency. Our experiments show that the fine-tuned model achieves a 4.04-point improvement over phi-3 and a 2.6% increase on the Qasper benchmark using only 2000 samples. Despite facing limitations in data scale and processing costs, this study underscores the potential of DPO and high-quality data in advancing LLM performance. Our dataset is on GitHub.\nAdditionally, the zero-shot benchmark results indicate that aggregated high-quality human reviews are overwhelmingly preferred over LLM-generated responses, even for the most capable models like GPT-40. This suggests that high-quality human reviews are extremely rich in information, reasoning, and long-context retrieval, capabilities that even the most advanced models have not fully captured. These findings highlight the high utility of leveraging human reviews to further advance the field.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) trained on vast amounts of data and computation have achieved remarkable results across tasks (Ding et al., 2024b;\nTian et al., 2024). However, as the complexity of tasks these models target increases, their ability to comprehend long texts faces unprecedented challenges. One such challenge is the \"Lost in the\nMiddle\" problem (He et al., 2023), where models perform well when relevant information is at the beginning or end of the input prompt but significantly decline in performance when the relevant\ninformation is in the middle.\nResearch papers and reviews represent high-quality long-text data, as reviews cover different sections of papers, highlight issues, and provide summaries and abstracts of the content. Numerous efforts to collect and build academic peer review\ndatasets have emerged and been applied to various tasks (Lin et al., 2023; Gao et al., 2019; Ghosal et al., 2022). However, current research using these datasets primarily focuses on downstream tasks, such as article acceptance prediction (Kang et al.,\n2018; Stappen et al., 2020) and citation relationship prediction (Plank and van Dalen, 2019), with little discussion on their impact on language models.\nTo address these challenges, we introduce academic peer review data into the fine-tuning process of LLMs. Based on 2000 parsed pdfs, we conducted experiments using both Supervised Fine-Tuning (SFT) and Direct Preference Optimization\n(DPO) (Rafailov et al., 2024) methods.\nIn general, our main contributions are:\n1. Proposing academic reviews as a high-\nquality long-text supervision dataset: to\nour knowledge, we are the first to propose\nusing scientific reviews as a natural source of\nhigh-quality supervision signal long-context\ndata, providing a robust dataset for training\nand evaluating language models.\n2. Effective implementation via DPO to add\nsupervision signal: We conduct a compara-\ntive analysis of Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO)\napproaches and found DPO is effective at\nimproving language models' performance on\nlong-text understanding tasks,"}, {"title": "2 Related Work", "content": "Recent works to improve the long context capabilities of large language models can be divided into the following categories, including but not limited to i) Length Extrapolation. Methods such as ROPE\nABF (Xiong et al., 2023) and Long RoPE (Ding et al., 2024a) are enhancements of the classic ROPE encoding, while AliBi (Press et al., 2022) introduces a novel linear biased position encoding mechanism. ii) Updated or New Model Structures. For\nexample, the decoupled network architecture of LONGMEM (Wang et al., 2023) and recurrent transformer (Wu et al., 2022). iii) Instruction fine-tuning. Improving the long context capabilities of\nlarge language models through fine-tuning based on long text datasets (Xiong et al., 2023; Bai et al., 2024). It is worth mentioning that they (Xiong et al., 2023) not only improved the long context\ncapabilities of models by incorporating the loss of predicting input sequences into the loss function but also found a cheaper but more effective pre-training way to make LLM's long context reading\nability stronger: starting from shorter context and gradually increasing sequence length for continuous pre-training."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Data Set Preparation", "content": "We download the ICLR 2024 submitted papers, including the PDFs and the corresponding reviews.\nWe use Amazon Textract to extract tables where needed and convert the PDF file into an HTML text file.\nGiven that each paper has 3-6 reviews from ICLR reviewers, we use GPT-4 to generate an aggregated review across the helpful, attention-to-detailed portion of each review, which we refer to\nas the aggregated review. We prompt the LLM not to rely on its knowledge but only to help aggregate the answer coherently."}, {"title": "3.2 Zero-shot benchmarks", "content": "We would like to assess existing models' capabilities in generating the response. We employed the following models with long context limit: GPT-3.5\nand GPT4 series (OpenAI et al., 2024), Mistral and Mixtral (Jiang et al., 2023), and Qwen (Bai et al., 2023a). We note GPT-3.5-16k would not see entire paper. The following sentence instructions are\nadded: \"Given the following paper, help write a review for the paper. The review should be helpful"}, {"title": "3.3 Fine-tuning", "content": "We would like to assess whether high-quality scientific reviews can help add additional supervision signals to the models and thus experiment to fine-tune the model.\nWe chose Phi-3-mini-128k (Abdin et al., 2024) for its small size and 128K context length. This model, an extended context version of phi-3-mini, retains the high performance of the original 4K con-text version while adeptly handling longer context\ntasks. The extension to 128K context length was achieved through a two-stage process: an initial long-context mid-training, followed by mixed long-short post-training with supervised fine-tuning (SFT) and direct preference optimization (DPO).\nFor the DPO experiment, we employed GPT-4 to rate each review and generate the aggregated review. We select the aggregated review as the preferred response and the lowest-rated review as the rejected review. By optimizing the model's strat-egy and constructing our loss function, we directly"}, {"title": "3.4 Long-context Benchmark", "content": "We implemented two standard long-context reasoning and knowledge retrieval benchmarks, including Qasper (Dasigi et al., 2021), a question-answering reasoning benchmark of NLP papers"}, {"title": "4 Results", "content": "The performance comparison of different LLMs on the LongBench dataset, as presented in Table\n1, reveals several key observations. Our fine-tuned phi-3-mini-128k-dpo model consistently demonstrates superior performance across multiple tasks"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Effect of data scaling", "content": "We used 2000 samples to fine-tune the DPO model. Compared to the baseline model and the DPO\nmodel fine-tuned with 600 samples, the results"}, {"title": "5.2 Effect of SFT versus DPO", "content": "In our initial experiments using the SFT method, we observed that fine-tuning with the SFT method on a dataset with 600 samples resulted in a 6%\nreduction in the F1 score on the Qasper dataset. In contrast, using the same number of samples, the DPO method improved the F1 score by 0.5%. This indicates that for few-shot learning, the DPO\nmethod is more effective in fine-tuning LLMs than the SFT method. This is consistent with our pre-vious finding on multi-modal LLM benefits more from DPO than SFT (Li et al., 2024)."}, {"title": "6 Conclusion", "content": "This study highlights the effectiveness of leverag-ing high-quality academic peer review data for improving the long-context capabilities of large language models (LLMs). Our results demon-strate the superiority of the Direct Preference Opti-mization (DPO) method over the Supervised Fine-Tuning(SFT). The most advanced models have\nnot fully captured fine-tuned phi-3-mini-128k-dpo model outperformed phi-3 by 4.04 points, and\nQasper by 2.6% with only 2000 samples.\nThe zero-shot benchmark results highlight the limitations of LLMs, including GPT-40, in replicating the depth and quality of human reviews, particularly in information richness, reasoning, and long-context retrieval. These findings underscore the\nimportance of using high-quality human reviews to improve language models."}, {"title": "Limitations", "content": "Our bottleneck was the data scale in this study. While Textract offers low latency speed, the cost"}, {"title": "A Hyperparameters", "content": "The detailed information can be found in Table 5."}, {"title": "B Benchmark descriptions", "content": "QASPER: QASPER is a benchmark designed to assess the ability of Question Answering (QA) sys-tems to handle complex reasoning about claims made in multiple parts of academic papers. It comprises 5,049 questions over 1,585 Natural Lan-guage Processing papers. Each question is formu-lated by an NLP practitioner who reads only the ti-tle and abstract of the corresponding paper, seeking information present in the full text. The questions are answered by a separate set of NLP practition-ers who also provide supporting evidence for their answers. We utilized the QASPER benchmark to evaluate our models, highlighting the challenge of document-grounded, information-seeking QA and motivating further research in this area.\nLongBench: LongBench is the first benchmark for bilingual, multitasking, and comprehensive as-sessment of long context understanding capabilities of large language models. It includes different lan-guages (Chinese and English) to provide a more comprehensive evaluation of the models' multilin-gual capabilities in long contexts. Composed of six major categories and twenty-one different tasks, LongBench covers key long-text application scenar-ios such as single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks, and code completion. We used LongBench to evaluate our models' performance across these diverse and challenging tasks."}, {"title": "C Comparison of Textract, Unstructured.io", "content": "Unstructured.io provides a more polished output format compared to Textract, which can signifi-cantly reduce the time required for post-processing the extraction results. However, as previously men-tioned, we faced performance challenges when us-ing Unstructured.io to process our dataset. To il-lustrate the difference, here is an example of the output generated by Textract:\n{\"DocumentMetadata\": {\"Pages\": 1},\n\"Blocks\":["}]}