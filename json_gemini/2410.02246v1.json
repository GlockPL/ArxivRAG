{"title": "PFGUARD: A GENERATIVE FRAMEWORK WITH PRIVACY AND FAIRNESS SAFEGUARDS", "authors": ["Soyeon Kim", "Yuji Roh", "Geon Heo", "Steven Euijong Whang"], "abstract": "Generative models must ensure both privacy and fairness for Trustworthy AI.\nWhile these goals have been pursued separately, recent studies propose to combine\nexisting privacy and fairness techniques to achieve both goals. However, na\u00efvely\ncombining these techniques can be insufficient due to privacy-fairness conflicts,\nwhere a sample in a minority group may be amplified for fairness, only to be\nsuppressed for privacy. We demonstrate how these conflicts lead to adverse effects,\nsuch as privacy violations and unexpected fairness-utility tradeoffs. To mitigate\nthese risks, we propose PFGuard, a generative framework with privacy and fairness\nsafeguards, which simultaneously addresses privacy, fairness, and utility. By\nusing an ensemble of multiple teacher models, PFGuard balances privacy-fairness\nconflicts between fair and private training stages and achieves high utility based\non ensemble learning. Extensive experiments show that PFGuard successfully\ngenerates synthetic data on high-dimensional data while providing both fairness\nconvergence and strict DP guarantees \u2013 the first of its kind to our knowledge.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, generative models have shown remarkable performance in various applications including\nvision (Wang et al., 2021b) and language tasks (Brown et al., 2020) \u2013 while also raising significant\nethical concerns. In particular, privacy and fairness concerns have emerged due to generative models\nmimicking their training data. On the privacy side, specific training data can be memorized, allowing\nthe reconstruction of sensitive information like facial images (Hilprecht et al., 2019; Sun et al., 2021).\nOn the fairness side, any bias in the training data can be learned, resulting in biased synthetic data\nand unfair downstream performances across demographic groups (Zhao et al., 2018; Tan et al., 2020).\nAlthough privacy and fairness are both essential for generative models, previous research has primarily\ntackled them separately. Differential Privacy (DP) techniques (Dwork et al., 2014), which provide\nrigorous privacy guarantees, have been developed for private generative models (Xie et al., 2018;\nJordon et al., 2018); various fair training techniques, which remove data bias and generate more\nbalanced synthetic data, have been proposed for fair generative models (Xu et al., 2018; Choi et al.,\n2020). To achieve both objectives, harnessing these techniques has emerged as a promising direction.\nFor example, Xu et al. (2021) combine a fair pre-processing technique (Celis et al., 2020) with a\nprivate generative model (Chanyaswad et al., 2019) to train both fair and private generative models.\nHowever, we contend that na\u00efvely com-\nbining developed techniques for privacy\nand fairness can lead to a worse privacy-\nfairness-utility tradeoff, where utility is a\nmodel's ability to generate realistic syn-\nthetic data. We first illustrate how privacy\nand fairness can conflict in Fig. 1. Given\nthe data samples M1, M2, M3, and m1\nwhere M and m denote the majority and\nminority data groups, respectively, DP and\nfairness techniques play a tug-of-war re-\ngarding the use of minority data point m1;\nDP techniques limit its use to prevent pri-"}, {"title": "2 PRELIMINARIES", "content": "We focus on Generative Adversarial Networks (Goodfellow et al., 2014),\nwhich are widely-used generative models that leverage adversarial training of two networks to\ngenerate realistic synthetic data: 1) a generator that learns the underlying training data distribution\nand generates new samples and 2) a discriminator that distinguishes between real and generated data.\nThe discriminator can be considered as the teacher model of the generator, as the generator does not\nhave access to the real data and only learns from the discriminator via the GAN loss function.\nWe use Differential Privacy (DP) (Dwork et al., 2014) to privatize generative\nmodels, a gold standard privacy notion that provides quantitative privacy analysis of an algorithm.\nDP measures how much an adversary can learn about a single sample by differentiating two outputs\nof an algorithm M. This privacy guarantee is quantified with the parameters (\u03b5, \u03b4) as follows:\n((\u03b5, \u03b4)-Differential Privacy (Dwork et al., 2006)) A randomized mechanism M :\nD \u2192 R with domain D and range R satisfies (\u03b5, \u03b4)-differential privacy if for any two adjacent inputs\nD, D', which differ by a single sample, and for any subset of outputs O \u2286 R, the following holds:\n$\\Pr(M(D) \\in O) \\leq e^\\epsilon \\Pr(M(D') \\in O) + \\delta,$\nwhere \u03b5 is the upper bound of privacy loss, and \u03b4 is the probability of breaching DP constraints.\nWe can enforce DP in an algorithm in two steps (Dwork et al., 2014). First, we bound its \u201csensitivity\u201d\n(Def. 2.2), which captures the maximum influence of an individual sample. Second, we add random\nnoise that is proportional to the sensitivity. A common way to ensure DP is to use a Gaussian\nmechanism (Dwork et al., 2014) (Thm. 2.1), which utilizes Gaussian random noise with a scale\nproportional to l2-sensitivity. Two datasets D, D' are adjacent if they differ by a single sample.\n(Sensitivity (Dwork et al., 2014)) The lpsensitivity for a d-dimensional function\nf : D\u2192Rd is defined as $\\Delta = \\max_{D,D'} || f(D) \u2013 f(D')||_p$ over all adjacent datasets D, D'.\n(Gaussian mechanism (Dwork et al., 2014; Mironov, 2017)) Let f : X \u2192 Rd\nbe an arbitrary d-dimensional function with l2-sensitivity \u2206. The Gaussian mechanism M\u03c3,\nparameterized by \u03c3, adds Gaussian noise into the output, i.e., M(x) = f(x) + N(0, \u03c3\u00b2I), and\nsatisfies (\u03b5, \u03b4)-DP for \u03c3 \u2265 $\\sqrt{2\\ln(1.25/\\delta)}/\\epsilon$.\nWe consider a generative model to be fair if two criteria are satisfied: 1) the model\ngenerates similar amounts of data for different demographic groups with similar quality, and 2) the\ngenerated data can be used to train a fair downstream model w.r.t. traditional group fairness measures.\nFor 1), we measure the size and image quality disparities between the groups using the Fr\u00e9chet\nInception Distance (FID) score (Heusel et al., 2017; Choi et al., 2020) to assess image quality. For 2),\nwe use two prominent group fairness measures: equalized odds (Hardt et al., 2016) where the groups\nshould have the same label-wise accuracies; and demographic parity (Feldman et al., 2015) where the\ngroups should have similar positive prediction rates."}, {"title": "3 CHALLENGES OF SATISFYING BOTH PRIVACY AND FAIRNESS", "content": "In this section, we examine the practical challenges of integrating privacy-only and fairness-only\ntechniques to train both private and fair generative models. Based on Fig. 1's intuition on how\nprivacy and fairness conflict, we analyze how existing approaches for DP generative models and fair\ngenerative models can technically conflict with each other."}, {"title": "Adding Fairness Can Worsen Privacy", "content": "Ensuring fairness in DP generative models can significantly\nincrease sensitivity (Def. 2.2), leading to invalid DP guarantees. Sensitivity, which measures a data\nsample's maximum impact on an algorithm, is crucial in DP generative models because the noise\namount required for DP guarantees is often proportional to this sensitivity value. However, integrating\nfairness techniques in DP generative models can invalidate their sensitivity analyses by adjusting\nmodel outputs for fairness purposes. Examples include amplifying the impact of certain data samples\nto balance model training across groups (Choi et al., 2020) and directly feeding data attributes such\nas class labels or sensitive attributes (e.g., race, gender) to a generator for more balanced synthetic\ndata (Xu et al., 2018; Sattigeri et al., 2019; Yu et al., 2020), which can cause large fluctuations in the\ngenerator output with any variation in these data attributes. As a result, fairness techniques can end\nup increasing sensitivity and require more noise to maintain the same privacy level, compromising\nthe original DP guarantees unless modifying DP techniques to add more noise. However, this\nmodification is also not straightforward as assessing the increased sensitivity by fairness techniques\ncan be challenging (Tran et al., 2021b)."}, {"title": "Adding Privacy Can Worsen the Fairness-Utility Tradeoff", "content": "Another direction is to ensure privacy\nin fair generative models, but configuring an appropriate privacy bound can be challenging, leading\nto unexpected fairness-utility tradeoffs. We illustrate below using standard DP and fairness techniques:\nDP-SGD (Abadi et al., 2016) and reweighting (Choi et al., 2020). Let g(x) denote the gradient of the\ndata sample x, and pbal and Pbias denote balanced and biased data distributions, respectively.\n\u2022 DP-SGD is a standard DP technique Chen et al. (2020) for converting non-DP algorithms to\nDP algorithms by modifying traditional stochastic gradient descent (SGD). Compared to SGD,\nDP-SGD 1) applies gradient clipping to limit the individual data point's contribution, where g(x)\nis clipped to $\\frac{g(x)}{\\max(1, ||g(x)||_2/C)}$ (the sensitivity becomes the clipping threshold C), and\n2) uses a Gaussian mechanism (Thm. 2.1) to add sufficient noise to ensure DP.\n\u2022 Reweighting is a traditional fairness method (Horvitz & Thompson, 1952) widely used in gener-\native modeling (Choi et al., 2020; Kim et al., 2024), which assigns greater weights to minority\ngroups for a \"balanced\u201d loss during SGD. In particular, setting the sample weight to the likelihood\nratio h(xi)=Pbal(xi)/Pbias(xi) produces an unbiased estimate of $\\mathbb{E}_{x \\sim p_{bal}}[g(x)]$ as follows:\n$\\mathbb{E}_{x \\sim P_{bias}}[g(x) \\cdot \\frac{P_{bal}(x)}{P_{bias}(x)}] = \\mathbb{E}_{x \\sim p_{bal}}[g(x)].$\nWe can extend reweighting for fairness to also\nsatisfy DP using DP-SGD, but finding the clip-\nping threshold C that balances fairness and util-\nity can be challenging. Note that if we per-\nform DP-SGD and then reweighting, privacy\nbreach may occur by amplifying sample gra-\ndients beyond C, invalidating the sensitivity\nderived from gradient clipping. We thus con-\nsider performing reweighting and then DP-SGD,\nwhich at least guarantees DP for reweighting-\nbased fair generative models. However, the clip-\nping now undoes the fairness adjustments where\nreweighted gradients $\\frac{g(x) \\cdot h(x)}{\\max(1, ||g(x) \\cdot h(x)||_2/C)}$, and\nEq. 1 does not hold if $C \\leq g(x)$. However, increasing C also increases the noise required for DP, which reduces utility (Fig. 3). As a result,\nselecting a C that balances fairness and utility may necessitate extensive hyperparameter tuning (Bu\net al., 2024), complicating the systematic integration of fairness into DP generative models.\nOverall, we show that a na\u00efve combination of existing fairness-only and privacy-only techniques\ncan be insufficient to achieve both objectives. While we have not exhaustively covered all possible\ncombinations, one can see how privacy breaches and unexpected fairness-utility tradeoffs can easily\noccur without a careful design. To avoid these downsides, we emphasize the need for a unified design\nthat integrates both privacy and fairness in generative models."}, {"title": "4 FRAMEWORK", "content": "We now propose PFGuard, the first generative framework that simultaneously achieves statistical\nfairness and DP on high-dimensional data, such as images. As shown in Fig. 2, PFGuard balances\nprivacy-fairness conflicts between fair and private training stages using an ensemble of teacher models\nas a key component. In Sec. 4.1, we first explain the fair training stage, which trains a fair teacher\nensemble. In Sec. 4.2, we then explain the private training stage, which transfers the knowledge of\nthis teacher ensemble to the generator with DP guarantees \u2013 ultimately training a generator that is\nboth fair and private. In Sec. 4.3, we lastly discuss how PFGuard's integrated design offers advantages\nin terms of fairness, utility, and privacy compared to the na\u00efve approaches discussed in Sec. 3."}, {"title": "4.1 FAIR TRAINING WITH BALANCED MINIBATCH SAMPLING", "content": "We ensure fairness in the teachers by balancing the minibatches used for training. Here\nwe assume a general training setup of stochastic gradient descent", "steps": 1, "follows": "n$\\Pr(x \\leq t) = \\sum_{i: x_i \\leq t} w(x_i) = \\mathbb{E}_{x_i\\leq t} \\frac{h(x_i)}{\\sum_i h(x_i)} = \\sum_i \\frac{\\mathbb{I}{x_i \\leq t} \\pi(x_i)/\\psi(x_i)}{\\sum_i \\pi(x_i)/\\psi(x_i)} \\rightarrow_{n \\rightarrow \\infty} \\frac{\\int \\mathbb{I}{x \\leq t} {\\pi(x)/\\psi(x)}\\psi(x) dx}{\\int {\\pi(x)/\\psi(x)}\\psi(x) dx} = \\frac{\\int \\mathbb{I}{x \\leq t} \\pi(x) dx}{\\int \\pi(x) dx}"}]}