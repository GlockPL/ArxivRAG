{"title": "EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "M. Shamim Hossain"], "abstract": "Sustainable agriculture plays a crucial role in ensuring world food security for consumers. A critical challenge faced by sustainable precision agriculture is weed growth, as weeds share essential resources with the crops, such as water, soil nutrients, and sunlight, which notably affect crop yields. The traditional methods employed to combat weeds include the usage of chemical herbicides and manual weed removal methods However, these could damage the environment and pose health hazards. The adoption of automated computer vision technologies and ground agricultural consumer electronic vehicles in precision agriculture offers sustainable, low-carbon solutions. However, prior works suffer from issues such as low accuracy and precision and high computational expense. This work proposes EcoWeed-Net, a novel model with enhanced weed detection performance without adding significant computational complexity, aligning with the goals of low-carbon agricultural practices. Additionally, our model is lightweight and optimal for deployment on ground-based consumer electronic agricultural vehicles and robots. The effectiveness of the proposed model is demonstrated through comprehensive experiments on the CottonWeedDet12 benchmark dataset reflecting real-world scenarios. EcoWeedNet achieves performance close to that of large models yet with much fewer parameters. (approximately 4.21% of the parameters and 6.59% of the GFLOPs of YOLOv4). This work contributes effectively to the development of automated weed detection methods for next-generation agricultural consumer electronics featuring lower energy consumption and lower carbon footprint. This work paves the way forward for sustainable agricultural consumer technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern agricultural consumer electronics are revolutionizing sustainable precision agriculture practices through the use of advanced tools and automated technologies [1], [2] to enhance efficiency and lower the carbon footprint and environmental impact [3]. A critical challenge in sustainable agriculture is the pervasive problem of weed growth. Weeds compete with crops for vital resources such as nutrients, water, and sunlight, significantly reducing yields. Weed growth incurs a huge cost to the agriculture industry. For example, a report by Weed CRC mentions that the average net annual loss due to weed growth incurred by Australian agriculture is $3.9 billion [4]. The report also estimates the cost of weed control to be at least $19.6 million annually for weed control in national parks and natural environments. Additionally, weeds are estimated to be presently responsible for huge crop losses in the agriculture industry [5]. Globally, the impact is even more staggering, with weed management costs and losses reaching huge amounts. Addressing the weed growth issue is pivotal for achieving sustainable agricultural practices and ensuring food security for a growing population [6]. As illustrated in Figure 1. Vision-based models deployed on consumer electronic (CE) unmanned ground vehicles (UGVs) capture high-resolution images of the farmland [10], which are then processed by deep learning models such as convolutional neural networks (CNNs) to perform the task of weed detection. The problem of weed detection in images can be formulated as object detection. As such, there are broadly two categories of vision-based object detection methods: two-stage and one-stage detectors [11]. The former includes models such as Faster R-CNN, which are accurate but slow. The latter includes models such as the YOLO family, which are relatively more computationally efficient, have lower energy consumption, and rapid inference capabilities, which can be beneficial in real-time low-carbon consumer agricultural applications [12]. Recently, there was a revolution in the deep learning community by the concept of attention in [13], enabling the models to dynamically focus on the most relevant features in the input"}, {"title": "II. RELATED WORKS", "content": "Generally, object detectors are divided into one-stage detectors and two-stage detectors, highlighting the trade-off between high performance and computational complexity. Additionally, the parameter-free attention modules play a vital role in enhancing the model performance without adding any parameters or FLOPs, making the model suitable for deployment on consumer electronic devices."}, {"title": "A. Object Detectors: Two-Stage vs. One-Stage Approaches", "content": "Object detection methods for weed detection in precision agriculture can be generally divided into two groups, including two-stage and one-stage detectors, both having weaknesses and strengths. The two-stage detectors, such as Faster R-CNN, rely on region propositions and then classify. In [16], Faster R-CNN tested CottonWeedDet3 with 848 RGB images for three weed categories. As effective in defining fine-grain details in complex backgrounds, the model struggled. In a move to counter such vulnerability, [17] proposed additions such as a Convolutional Block Attention Module (CBAM) for enhancing informative features, a Bidirectional Feature Pyramid Network (BiFPN), and a Bilinear Interpolation algorithm for multi-scale weed detection. With such, mAP rose from 79.98% to 98.43%, but performance lowered to 92.81% in the case of nightfall, highlighting a two-stage model vulnerability in heterogeneous environments. On the one hand, one-stage detectors such as the YOLO family produce end-to-end processing with efficient inference and, therefore, can work for real-time cases. [16] contrasted YOLOv5n with Faster R-CNN with CottonWeedDet3 dataset and exhibited YOLOv5n with a record 17 ms inference, outpacing Faster R-CNN in terms of efficiency but with competitive accuracy. In contrast, one-stage architectures lack small-object detection in comparison with two-stage detectors, for which region proposals work in their favour. Some one-stage detector optimizations have resolved such problems. [18] optimized YOLOv5n with integration with backbone with ShuffleNet-v2, integration with a parallel hybrid attention module, and three-level BiFPN for feature fusion improvement in it. By such optimizations, model complexity was reduced by over 80%, with an inference time of 12 ms less and mAP@0.5 accuracy of 97.8%. Likewise, [19] optimized YOLOv7 with YOLO-Spot_M through pruning and architectural optimizations. In it, 75.3% and 82.4% of its parameters and computational cost, respectively, reduced and exhibited 5 times improvement in terms of processing pace at a mAP@0.5 accuracy of 80.6%, with suitability for a run in a less-resourced environment such as Jetson AGX Xavier. From a general consideration, even two-stage detectors outperform in terms of accuracy and processing of small but with high computationally costly expense, and therefore, can work hardly practically for real-time cases in practice. On the one hand, one-stage detectors have a balanced trade-off with a high inference pace and, therefore, can work effectively in a less-latency, less-resource environment, according to [20]."}, {"title": "B. Attention Mechanisms", "content": "Nowadays, attention mechanisms have become indispensable in enhancing the deep learning models' performance, especially for tasks that require precise feature extraction and complex backgrounds, by focusing on the spatial and channel features. The attention modules assist the deep learning models to become more robust and reliable across several applications. There are two broad categories of attention mechanisms: (i) parameter-based and (ii) parameter-free. In the parameter-based attention mechanisms, the channel and spatial attention modules depend on extracting the informative features using convolutional layers, which add more parameters and floating point operations (FLOPs). In contrast, the parameter-free attention modules are engineered to improve the model performance by relying on operations to compute the importance of the features without adding parameters or FLOPs to the architecture. The SERMAttention module is proposed in [21], which is considered a channel attention mechanism to emphasize the highest informative channels in the feature maps. Consequently, the model performance improved in terms of weed"}, {"title": "III. PROPOSED METHOD", "content": "With the constraints of sustainable consumer electronic devices and precision low-carbon agriculture in mind, this work proposes a novel lightweight and automated weed detection method. The enhanced model offers precise agriculture weed detection, exploring the role of two parameter-free attention modules: SPAB in the backbone and the so-called Simplified Attention Module, SimAM, both in the backbone and neck. The modules reinforce the model by paying more attention to informative features while maintaining computational efficiency, which is very important for real-time applications. SPAB refines feature extraction, while SimAM guarantees overall refinement of the features across successive stages of processing the variability of an agricultural environment. Therefore, our approach paves the way for the adoption of intelligent and energy-efficient agriculture solutions that will balance high performance with minimum harmful environmental impact, thus fully fitting with sustainable agricultural practices. This section provides a detailed overview of the EcoWeed-Net architecture, highlighting its three key components: the backbone, neck, and head. Additionally, it explores the integration and functionality of two state-of-the-art attention modules, SimAM and SPAB, within the system."}, {"title": "A. Backbone", "content": "The backbone of the EcoWeedNet model contains multiple blocks that effectively impact the feature extraction process, highlighting the most informative parameters in the feature map. Convolutional layers contribute to identifying the patterns that are crucial for the detection tasks. Additionally, C3K2 is a sophisticated block designed for better feature extraction. C3K2 block utilizes dual convolutional operations and also can notably lower the computational load by integrating more C3K blocks to reduce the spatial dimensions while making the feature map deeper. Moreover, the Swift Parameter-Free Attention Block (SPAB) is integrated to enhance the model's focus on informative features, boosting detection accuracy while maintaining the model is lightweight. The simple attention module (SimAM) inspired by neuroscience enhances the feature map representations without adding learnable parameters and floating point operations (FLOPs). These modules have a significant impact on the enhancements of the feature extraction. These modules optimize the enhanced model for high performance and computational efficiency, making it an optimal choice for edge device deployment. The enhancements ensure reliable weed detection and provide precise and quick performance."}, {"title": "B. Neck", "content": "The neck has an essential role in enhancing and synthesizing the feature maps from the backbone and passing them to the detection heads. This component involves multiple modules. Spatial Pyramid Pooling Fast (SPPF) captures multi-scale contextual information from the input tensor. It contains convolutional layers to reduce the spatial dimensions, followed by max pooling at multiple scales. Varied receptive fields are generated, which are crucial for detecting varied sizes of weeds. Then, the features are concatenated, ensuring the rich representation of input data. Convolutional Block with Parallel Spatial Attention (C2PSA) focuses on the spatial patterns in the feature maps by utilizing a split-transform-merge strategy, where the input features are split, transformed using self-attention blocks (PSABlocks), and then merged back. This module has a notable impact on emphasizing the most informative regions in the feature maps while expressing the less important features, contributing positively to the weed detection task."}, {"title": "C. Head", "content": "In EcoWeedNet, a head generates a prediction for classification and localization with a multi-scale scheme for detection. For improved weed detection for a diversity of scales, three output heads at three scales, namely 80 \u00d7 80 for small weeds, 40 x 40 for weeds with a medium one, and 20 x 20 for weeds with a larger one, are utilized. With a multi-scale scheme, accuracy in detection is increased, and robust performance in a diversity of agricultural scenarios, even in occluded and occluded-overlapping ones, is attained. With such a scheme, effective and efficient weed detection is acquired through EcoWeedNet, and it can run in real-time on consumer electronic devices in low-carbon and environmentally friendly agriculture."}, {"title": "D. SimAM", "content": "SimAM is a parameter-free attention mechanism designed to enhance feature map representations [25]. SimAM directly computes 3D attention weights without adding learnable parameters or increasing FLOPs, making it more efficient than traditional attention modules, offering high performance without computational complexity, outperforming the traditional attention mechanism, which relies on the convolutional layers to enhance the model performance and adding computational complexity. The key principle of the SimAM is designing the energy function inspired by neuroscience, which is utilized to evaluate the importance of each neuron in the feature map. The energy function emphasizes the response of each neuron compared to its surrounding neurons. SimAM aims to highlight neurons with low energy from their surrounding neurons to evaluate their importance. The energy function $e_t(w_t, b_t, y, X)$ is defined as:\n$e_t(w_t, b_t, y, X) = \\frac{1}{M} \\sum_{i=1}^{M-1}(-1 - (w_t x_i + b_t))^2 + (1 - (w_t t + b_t))^2 + \\lambda w_t^2$\nwhere: t: Target neuron's value, $x_i$: Surrounding neurons' values, M: Total number of neurons in a channel, $w_t, b_t$: Linear transform parameters (weight and bias), $\\lambda$: Regularization parameter. The neurons with low energy are highlighted with high distinctiveness and considered more informative for feature representation. To find the optimal weights, we derive a closed-form solution for $w_t$ and $b_t$:\n$w_t = \\frac{2(t - \\mu_t)}{(t - \\mu_t)^2 + 2\\sigma_t^2 + 2\\lambda}$\n$b_t = -\\frac{1}{M}(t+ \\mu_t)w_t$\nWhere:\n$\\mu_t = \\frac{1}{M-1} \\sum_{i=1}^{M-1} x_i$\n$\\sigma_t^2 = \\frac{1}{M-1} \\sum_{i=1}^{M-1} (x_i - \\mu_t)^2$\nHere, $\\mu_t$ is the mean, and $\\sigma_t^2$ is the variance of the surrounding neurons. The SimAM mechanism aims to minimize the energy, and the minimal energy for the target neuron t is given by:\n$e_t^* = \\frac{4(\\sigma_t^2 + \\lambda)}{(t - \\mu_t)^2 + 2\\sigma_t^2 + 2\\lambda}$\nThe importance of the neuron is then inversely proportional to this minimal energy:\n$Importance = \\frac{1}{e_t^*}$\nThe SimAM is characterized by fast and efficient computation of neuron importance by utilizing simple statistical measures, such as the mean and variance of the feature map, thanks to the closed-form solution for the energy function. On the other hand, traditional attention mechanisms operate on spatial or channel dimensions, which adds complexity to the model. Computing comprehensive 3D attention weights captures spatial and channel-wise interactions simultaneously, enhancing the feature refinement operations. The refined feature map $X'$ is calculated as:\n$X' = \\sigma(\\frac{1}{E}) \\odot X$\nWhere E contains the minimal energies $e_t$ for all neurons in the feature map, and $\\sigma(\\cdot)$ is the sigmoid function to ensure the scaling factor lies within [0,1]. The symbol $\\odot$ denotes element-wise multiplication. SimAM does not offer any learnable parameters or complicity to the model and is considered a plug-and-play module that can be integrated easily into any model network, improving the performance with computational complexity."}, {"title": "E. SPAB", "content": "The Swift Parameter-Free Attention Block (SPAB) is designed to be lightweight (hence suitable for consumer electronic devices) and enhance the performance of the models by highlighting the most informative parameters in the feature maps [26]. SPAB relies on an effective structure consisting of three convolutional layers, which capture the crucial patterns in the feature map, such as textures and edges. The output of the j-th convolutional layer is denoted as $H_j$, where each layer applies a kernel $W_j$ to the input from the previous layer and the convolution operation is represented as:\n$H_i = F_i (W_i * O_{i-1})$,\nwhere * represents the convolution operation and $O_{i-1}$ is the output of the previous layer. Then, a symmetric activation function $\\sigma_a$ is applied to create an attention map to emphasize the informative regions. The generated attention map contains the original features while highlighting the rich regions and depressing the less important or redundant features. This attention map $V_i$ is calculated as:\n$V_i = \\sigma_a (H_i)$,\nWhere $\\sigma_a$ is a symmetric activation function, typically chosen to be a variant of the sigmoid function, which ensures that the activation is balanced around the origin. Moreover, the SPAB structure addressed the information loss by utilizing residual connections to ensure that there was no loss in the input data during the processing operations. Overall, the SPA\u0412 architecture provides a robust enhancement and improved feature extraction while maintaining the model's lightweight and suitability for resource-constrained devices. The generated attention map contains the original features while emphasizing the rich regions and depressing the less important or redundant features. The final output of the SPAB block is obtained by performing an element-wise multiplication of the original feature map $U_i$ and the attention map $V_i$:\n$O_i = U_i \\odot V_i$,\nWhere $\\odot$ represents element-wise multiplication, and $U_i$ is the feature map obtained by adding residual connections from the input:\n$U_i = O_{i-1} \\oplus H_i$,\nWhere $\\oplus$ denotes element-wise summation."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In our work, we used a CottonWeedDet12 dataset that consists of 12 common weed species in cotton fields in the southern region of the United States of America. The dataset contains 5,648 high-resolution RGB images annotated with 9,370 bounding boxes, as shown in Table I. The used dataset includes the shadows effect, complex background, and multiple weed kinds per image, as shown in Figure 4, emphasizing the real-world scenarios and enhancing the model performance, robustness, and generalization."}, {"title": "V. RESULTS", "content": "The enhanced nano model contributes significantly to the field of precision agriculture by providing a reliable, efficient, and economically feasible solution for weed detection. Our proposed model combines the capabilities of big architectures with low computational cost, as shown in Figure 3, highlighting the importance and suitability of our model for real-time consumer electronic applications. Investigation of EcoWeed-Net architecture and integration of SPAB and SimAM modules in the backbone and the neck enhances feature discrimination without adding substantial complexity while maintaining a lightweight network."}, {"title": "VI. CONCLUSION", "content": "This work proposed the EcoWeedNet model, hence enhancing weed detection capabilities without adding notable computational complexity. These improvements align with sustainable agricultural methods, guaranteeing that the building remains efficient and appropriate for the energy limitations of contemporary farming. Our assessments of the CottonWeedDet12 dataset indicate that our model attains performance on par with larger state-of-the-art (SOTA) architectures, excelling in accuracy, precision, recall, and mean average Precision. This establishes our network as an optimal choice for real-time applications and resource-limited scenarios, providing it a significant asset for the progression of next-generation sustainable agricultural technology."}]}