{"title": "Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences", "authors": ["Damien Ferbach", "Quentin Bertrand", "Avishek Joey Bose", "Gauthier Gidel"], "abstract": "The rapid progress in generative models has resulted in impressive leaps in genera- tion quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impact- ing the training of future generated models. Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step. However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users. In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an implicit preference optimization mechanism. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.", "sections": [{"title": "1 Introduction", "content": "Today state-of-the-art generative models can produce multi-modal generations virtually indistin- guishable from human-created content, like text (Achiam et al., 2023), images (Stability AI, 2023), audio (Borsos et al., 2023), or videos (Villegas et al., 2022; Brooks et al., 2024). The democratiza- tion of these powerful models by open-sourcing their weights (Stability AI, 2023; Jiang et al., 2023; Touvron et al., 2023) or allowing public inference (Ramesh et al., 2021; Midjourney, 2023; Achiam et al., 2023) paves the way to creating synthetic content at an unprecedented scale-the results in- evitably populate the Web. In particular, existing datasets are already composed of synthetic data such as JourneyDB (Pan et al., 2023) and SAC (Pressman et al., 2022). Moreover, Alemohammad et al. (2024, Fig. 2) showed LAION-5B (Schuhmann et al., 2022), a large-scale Web-crawled dataset used to train future generative models, already contains synthetically generated images.\nThere is strong evidence that the synthetic data on the web has been, to a large extent, curated by human users. For instance, the LAION-Aesthetics datasets contains synthetically generated"}, {"title": "2 Iterative retraining with curated synthetic data", "content": "We now study the fully synthetic self-consuming loop with curated samples. Unlike previous works that do not take curation into account (Alemohammad et al., 2024; Shumailov et al., 2023) and focused on stability of the process (Bertrand et al., 2024), we show that retraining with curated samples both maximizes an underlying reward whose variance collapses, and converges to maximum reward regions. In Section 2.1 we first specify explicitly the distribution induced"}, {"title": "2.1 Iterative retraining only on the curated synthetic samples", "content": "In this section, we study the dynamics of the density learned through iterative discrete K-choice curation in the fully-synthetic setting (i.e., $\\lambda \\rightarrow \\infty$): Equation 3 boils down to\n$P_{t+1} = \\underset{P \\in \\mathcal{P}}{arg\\ max} \\mathbb{E}_{x_1,...,x_K\\sim P_t,\\ \\hat{x}\\sim BT(x_1,...,x_k)} [logp(\\hat{x})].$ \nAs a warm-up, we first consider the limit of K \u2192 \u221e in Lemma 2.1 and draw explicit connections with RLHF. This simplification yields a closed-form formula form for the solution of Equation 4 and provides intuitions for the dynamics of learning on curated samples."}, {"title": "Lemma 2.1.", "content": "Let $p_{t+1}$ be defined as in Equation 4. If $\\mathcal{P} = \\mathcal{P}(\\mathbb{R}^d)$ is the set of probability distributions on $\\mathbb{R}^d$, and if we assume that $\\mathbb{E}_{y\\sim p_t} [e^{r(y)}] <\\infty$, then we have for all $x \\in \\mathbb{R}^d$,\n$p_{t+1}(x) \\underset{K\\rightarrow \\infty}{\\propto} p_t(x) \\frac{e^{r(x)}}{\\mathbb{E}_{\\tilde{x}\\sim p_t} [e^{r(\\tilde{x})}]}$.\nDependency on K and connection to RLHF.. The proof of Lemma 2.1 relies on the fact that we can obtain a closed-form formula for the density $P_{t+1}$ induced from discrete K-choice curation on $p_t$ (Equation 4). This is done in Appendix A.1.1 where we show that its density can be written\n$p_{t+1}(x) = p_t(x) \\mathcal{H}_K(x), \\ with \\  \\mathcal{H}_K(x) := \\mathbb{E}_{x_1,...,x_{K-1}\\sim P_t}  [ \\frac{K.e^{r(x)}}{e^{r(x)} + \\sum_{i=1}^{K-1} e^{r(x_i)}} ]$.\nThe latter directly implies that for all K > 1, $\\mathcal{H}(x) \\in (0, K)$. In particular, small values of K act as a regularization which prevents the density from blowing up too much in high rewards areas. On the other hand, the higher the number of samples used for curation, the more it can affect the induced distribution. In the limit K \u2192 \u221e, Lemma 2.1 shows an interesting connection between iterative retraining on curated data and reward maximization via RLHF. Given a supervised-finetuned model distribution SFT and a regularization parameter \u03b2, the goal of RLHF is to find a policy that maximizes a reward r(x) fitted on human preferences :\n$\\pi^{\\mathcal{RLHF}} = \\underset{\\pi}{arg\\ max} \\mathbb{E}_{x\\sim \\pi} [r(x)] - \\beta D_{KL} (\\pi||\\pi^{SFT}), \\ which \\ has \\ a \\ closed \\ form \\ formula, \\pi^{\\mathcal{RLHF}}(x) \\propto \\pi^{SFT}(x)e^{r(x)/\\beta}.$ \nTherefore, in the limit K \u2192 \u221e, Equation 5 shows that performing iterative retraining with human curation for t iterations is equivalent to performing RLHF with hyperparameter $\\beta = \\frac{\\gamma}{t}$ from the initial distribution SFT := p0. The corresponding regularization parameter \u03b2 is inversely proportional to the number of retraining steps. This connection is surprising since performing maximum likelihood on a curated distribution (Equation 3) is a priori different than directly maximizing a reward with Kullback-Leibler (KL) regularization.\nTo prove that curation both increases the expected reward and reduces the variance, we need an additional assumption to ensure that it is almost surely bounded at initialization:"}, {"title": "Assumption 2.1.", "content": "There exists $r^* \\in \\mathbb{R}$ such that: (a) $p_0$-almost surely, $r(x) < r^*$ and (b) $p_0$ puts positive mass in a neighborhood of r\u2217 i.e., $\\forall \\varepsilon > 0, \\mathbb{P}_0(r(x) \\geq r^* - \\varepsilon) > 0$.\nIn particular, Assumption 2.1 is satisfied if we suppose that the reward bounded, which is reasonable if we suppose it is continuous given that the set of images [0, 1]d is compact. Finally note that assum- ing (a), we can always choose r\u2217 such that (b) is satisfied by picking the smallest value that almost surely bounds the reward at initialization. On the other hand (b) imposes that r\u2217 is the smallest value that a.s. upper-bounds the reward. This shows that r\u2217 is uniquely defined which is an important point as we will show convergence of pt towards the level set r(x) = r\u2217 in Lemma 2.2 and Theorem 2.1."}, {"title": "Lemma 2.2.", "content": "Let $p_{t+1}$ be the distribution induced from a discrete choice model on $p_t$ (Equation 4). Suppose Assumption 2.1 holds, then the expected reward increases proportionally to its variance at each retraining iteration:\n$\\mathbb{E}_{p_{t+1}} [e^{r(x)}] \\geq \\mathbb{E}_{p_{t}} [e^{r(x)}] + \\frac{K-1}{K} \\frac{Var_{p_t} [e^{r(x)}]}{e^{r*}}.$ \nEspecially the expected reward converges to the maximum reward and its variance vanishes:\n$\\mathbb{E}_{p_{t}} [e^{r(x)}] \\underset{t \\rightarrow \\infty}{\\rightarrow} e^{r^*} \\  and \\  Var_{p_t} [e^{r(x)}] \\underset{t \\rightarrow \\infty}{\\rightarrow} 0.$"}, {"title": "Discussion.", "content": "Lemma 2.2 shows that the reward augmentation is directly proportional to the reward variance at each retraining step. In other words, the more heterogeneous the reward is, the more its expectation increases at the next step. Lemma 2.2 further shows that the expected reward converges towards the reward maximizers. We can additionally deduce that the variance is doomed to vanish. This is detailed in Appendix A.1.3 which additionally states that the reward variance decreases fast enough to have finite sum. Finally, we note that Lemma 2.2 helps us understand the fixed points of this process: due to the variance term in Equation 7, a fixed point of the retraining loop must put mass on a single level set of the reward function. The reciprocal is obviously true as detailed in the appendix (Lemma A.3).\nWe can finally show a stronger result of convergence for the Kullback-Leibler divergence. We will need to assume that at initialization, the probability density puts a positive mass on the level set r(x) = r\u2217. This corresponds to additionally assuming that $\\varepsilon \\geq 0$ instead of only $\\varepsilon > 0$ in Assumption 2.1 b). Without this assumption, the probability density support would consecutively vanish towards the maximizer of the reward preventing KL convergence. We therefore assume $\\mathbb{P}_0(r(x) = r^*) > 0$. Further denote p\u2217 the probability density at initialization restricted to the\ndomain that maximizes the reward and renormalized: $p^*(x) := \\frac{p_0(x)1_{r(x)=r}}{\\mathbb{P}_0(r(x)=r^*)}$"}, {"title": "Theorem 2.1.", "content": "The self-consuming loop on curated samples $p_t$ converges to p\u2217:\n$\\underset{t\\rightarrow \\infty}{D_{KL}(p^*||p_t) \\rightarrow 0}.$ \nTheorem 2.1 shows that the process of retraining with curation Equation 2 eventually converges to the highest level set of the reward reached at initialization. In particular, in the limit of a large number of retraining steps, the probability of all smaller rewards vanishes. This can have strong implications when retraining the next generation of generative models on a curated Web-scaled dataset: the learned distribution will lose diversity and collapse to the highest reward samples."}, {"title": "2.2 Stability of iterative retraining on a mixture of real and synthetic data", "content": "After showing convergence but variance collapse of the self-consuming loop on curated synthetic samples, we now study the impact on the stability of injecting real data at each step. This setting is motivated by the recent work of Bertrand et al. (2024) that showed stability of the iterative retraining loop with real and synthetic data around a local maximizer 0\u2217 of the training distribution likelihood. This setting is furthermore relevant since Web-scrolled datasets will presumably keep containing a mixture of real data and human-curated synthetic data. In Section 2.2.1 we first improve previous results on retraining on mixed datasets which underlines the beneficial impact of real data on stability and in Section 2.2.2, we prove both stability and reward augmentation in the setting of mixed real and curated synthetic data."}, {"title": "2.2.1 Iterative retraining without curation", "content": "To motivate the impact of real data on the stability of the retraining loop with curation, we focus first on its impact without curation and improve previous results in that setting in Theorem 2.2.\nSetting. In this section only, following the approach of Bertrand et al. (2024), we will not assume infinite capacity for our distribution (i.e., $\\mathcal{P} \\neq \\mathcal{P}(\\mathbb{R}^d)$ and hence adopt a parametric approach $\\mathcal{P} = \\mathcal{P}_\\Theta := {p_\\theta \\ | \\theta \\in \\Theta}$. Given the current generative model distribution $P_{\\theta_t}$, $P_{\\theta_{t+1}}$ must at the next iteration maximize the combined log-likelihood of real and generated data with hyperparameter \u03bb, i.e., Equation 3 becomes:\n$P_{\\theta_{t+1}} = \\underset{P_{\\theta} \\in \\mathcal{P}}{arg \\ max} \\frac{1}{1+\\lambda} \\mathbb{E}_{p_{data}} [log \\ p_{\\theta} (x)] + \\frac{\\lambda}{1+\\lambda} \\mathbb{E}_{p_{\\theta_t}} [log \\ p_{\\theta} (x)].$"}, {"title": "Theorem 2.2.", "content": "If $L\\varepsilon < \\alpha$ and $\\lambda < \\frac{\\alpha}{2L\\varepsilon}$, then there exists a neighborhood of the optimal distribution parameters $ \\theta_*$ such that for any initial parameters $ \\theta_0$ in that neighborhood, $ p_{\\theta_t}$ converges to $ p_{\\theta_*}$ exponentially fast:\n$D_{KL} (P_{\\theta_*} ||P_{\\theta_t}) \\underset{|P_{\\theta_t} = \\mathcal{O}|}{\\rightarrow} \\mathcal{O}((\\frac{\\lambda(\\alpha + \\varepsilon L)}{\\alpha + \\lambda (\\alpha - \\varepsilon L)})^t)$. "}, {"title": "2.2.2 Iterative retraining on a mixture of real and curated samples", "content": "Interestingly when curating the synthetic samples we cannot expect stability around the optimal distribution (\u03b8\u2217 in Theorem 2.2) since it is no longer a fixed point of the retraining loop. We will instead show a closeness result in KL divergence combined with an increasing property of the expectation of the reward, which bears close connections to RLHF. We therefore now study the setting described in Equation 3 where the synthetic samples are curated using a discrete K-choice model and real data is reused at each step (\u03bb < \u221e). In other words, we suppose that the retraining step uses a mixture of a reference distribution and a curated distribution as\n$p_{t+1}(x) = \\frac{1}{1 + \\lambda}p_{ref}(x) + \\frac{\\lambda}{1 + \\lambda}p_{t}(x) \\mathcal{H}_K(x)$ \nIn Theorem 2.3, we prove that when retraining on a mixture of real and curated samples, the reward increases with respect to the initialization:"}, {"title": "Theorem 2.3.", "content": "Consider the process ($p_t$) defined in eq. 8, with $p_0$ = $p_{ref}$, then, for all $t \\geq$ 1\n$\\mathbb{E}_{p_t} [e^{r(x)}] \\geq \\mathbb{E}_{p_{ref}} [e^{r(x)}] + \\frac{\\lambda}{(1 + \\lambda)^3} \\frac{(K-1)Var_{p_{ref}} [e^{r(x)}]}{Ker^*}$\nDiscussion. A first interesting case is taking the reference distribution $p_{ref}$ equal to $p_{data}$. In that case, we recover the fact that $p_{data}$ is not a fixed point of the retraining loop as soon as different reward values have non-zero probabilities to happen (we recover the result from Lemma A.3). In fact, Theorem 2.3 shows that such a process initialized at $p_{data}$ will increase the reward expectation. The second interesting case is taking $p_{ref} = p_\\theta$ the generative model at initialization. In that case, retraining on a mixture of samples from the initial model and curated samples from the current model improves the reward expectation with respect to initialization.\nAfter showing that such a retraining loop improves the expected reward, we can conversely show that this process does not deviate too much from $p_{ref}$."}, {"title": "Theorem 2.4.", "content": "Consider the process ($p_t$) defined in Equation 8, with $p_0$ = $p_{ref}$. In addition, suppose that $ \\lambda <  \\frac{1}{K-1}$, then, for all t $\\geq$ 1\n$D_{KL}(p_t||p_{ref}) \\leq - log (1 - \\lambda(K - 1))$\nApplying Theorem 2.4 with $p_{ref} = p_{data}$ shows that retraining on a mixture of real and curated synthetic samples does not deviate too much from the data distribution. On the other hand, when setting $p_{ref}$ to be any initial model distribution, we see that reusing samples from the initial model stabilizes the retraining loop around initialization.\nConnection with RLHF. Theorem 2.3 and Theorem 2.4 together emphasize that retraining on a mixture of reference and filtered synthetic data bears important connections with RLHF. Indeed, the RLHF objective is composed of both a reward maximization term and a KL regularization between the current and initial model. In turn, Theorem 2.3 states that the expected reward increases and Theorem 2.4 shows that the KL divergence with respect to initialization remains bounded. The upper bound on the KL divergence further indicates that setting K small, i.e., using fewer samples for comparison acts as a regularizer, as previously noticed."}, {"title": "3 Related work", "content": "Iterative retraining on synthetic data and model collapse. The study of the retraining loop of a generative model on synthetic data has witnessed a recent surge of interest. Alemohammad et al. (2024); Shumailov et al. (2023) first evidenced catastrophic degradation of the generated data in the fully synthetic loop. Bertrand et al. (2024) mitigate these conclusions in the setting where the model is retrained on a mixture of synthetic and real data and they show the stability of the process around the data distribution. Briesch et al. (2023) specifically focus on large langage models and Hataya et al. (2023); Mart\u00ednez et al. (2023) study large scale datasets. A recent theoretical push by Dohmatob et al. (2024a,b) provides bounds on the performance degradation in the regression setting as well as modified scaling laws. Finally recent works, Wyllie et al. (2024); Chen et al. (2024b) study the emergence or amplification of biases in self-consuming loops.\nAligning models with human preferences. With the urgent and critical safety concerns of public deployment, the need to align models with human preferences has gained significant importance. RLHF is a popular reinforcement learning technique to align an already pretrained and finetuned model on human preferences (Stiennon et al., 2020; Christiano et al., 2017; Lee et al., 2021; Ouyang et al., 2022; Shin et al., 2023). It consists of two steps: first fitting a reward r(x) on human pref- erences using a dataset of pairwise human comparisons and then, maximizing the expected reward over the model distribution. A Kullback-Leibler regularization to the initial model is further used during the maximization step to avoid reward hacking (Skalse et al., 2022; Chen et al., 2024a) or catastrophic forgetting (Korbak et al., 2022). Variants of RLHF have recently been proposed such as Direct Preference Optimization (DPO) which maximizes the reward directly without modeling it (Rafailov et al., 2024), Identity Preference Optimization (IPO) (Azar et al., 2024) or Kahneman- Tversky Optimization (KTO) (Ethayarajh et al., 2024)."}, {"title": "4 Experiments", "content": "This section aims to empirically illustrate our previous theoretical results on how curation impacts the self-consuming loop. In Algorithm 1, we recall and detail the different steps performed in our experiments.\nSynthetic datasets. We first focus on two synthetic datasets: a mixture of Gaussians and the two moons dataset. For both datasets, we study the two settings of solely retraining on curated synthetic samples (\u03bb = \u221e) and mixed datasets (\u03bb = 1). In Figure 4, we iteratively retrain a denoising diffusion probabilistic model (DDPM, Ho et al. 2020) on a mixture of 8 Gaussians. The reward r(x) used for the discrete choice model is the clipped negative Euclidean distance to one of the centers of the Gaussians x\u2217, i.e., r(x) := -ymax{0, ||x - X*|| - rmin} where we choose \u03b3 = 10, rmin = 1. Clipping the distance is used to ensure that the process does not collapse to a single point. Indeed applying Theorem 2.1, we know that the density will converge to a renormalized Gaussian"}, {"title": "4.1 Natural images on CIFAR10", "content": "Setting. We train a normalizing flow using optimal transport conditional flow matching (Lipman et al., 2022; Shaul et al., 2023; Tong et al., 2023b) with the torchcfm library Tong et al. (2023a, 2024). The initial model has been pretrained on the 50000 train images of the CIFAR-10 dataset (Krizhevsky et al., 2009). At each iteration, we generate 5 \u00b7 104 samples using the current model from which we keep 2.5 \u00b7 103 samples filtered by discrete K-choice comparisons. The reward r(x) is computed using the class probabilities q0(x), ..., q9 (x) from a pretrained VGG11 classifier (Simonyan and Zisserman, 2014) with 92.39% test accuracy. Due to the expensive compute cost of retraining a generative model for multiple iterations (c.f. Appendix A.2.3), we plot only one run on each figure. To ensure the reproducibility of our results, we plot the retraining curves for 3 independent runs in Figure 9 in the appendix, illustrating that they have small variance.\nUsing probability of one class as reward. As a first experiment, we filter samples following the probability of the classifier on a predefined class. We arbitrarily chose the class 0 corresponding to planes. The reward is then defined as r(x) = \u03b3\u00b7 q0(x), \u03b3 > 0. We plot the evolution of the class proportions as well as the averaged reward across 10 retraining steps in Figure 2 with \u03b3 = 5. Figure 2 shows collapse to the single plane class as the reward increases monotonically, illustrating Lemma 2.2.\nUsing the confidence of the classifier as a reward: the emergence of bias. As a second experi- ment, we use the confidence of the classifier as a reward, i.e., r(x) = \u03b3\u00b7 max0<i<9 qi(x), \u03b3 > 0. As written, the reward is therefore uncorrelated from the class but, remains implicitly correlated to it by the fact that the classifier statistics are class dependent. In Figure 3 we plot the evolution of the class proportions as well as the average reward. As expected by our theoretical results in Section 2, the average reward increases monotonically. On the other hand, we clearly see that the class propor- tions become more and more heterogeneous throughout the retraining loop. While confirming our theoretical study this plot therefore additionally shows that retraining on filtered samples increases bias, in a setting where the reward is implicitly correlated to diversity. Taking a step back, this has strong societal and ethical implications as it may imply that in a filtered internet biases may emerge or strengthen as we explain in Section 6.\nReusing real samples: stability and reward augmentation. Finally, we illustrate our results from Section 2.2.1 by mixing real and filtered synthetic samples with hyperparameter \u03bb = 3. Figure 3 shows that the process remains stable as the proportion of classes remains approximately uniform"}, {"title": "5 Conclusion and open questions", "content": "We study the impact of data curation on the training of generative models in the self-consuming loop. We provide theoretical results demonstrating that the expected reward underlying the curation process increases and its variance collapses (Lemma 2.2) as well as a convergence result (Theo- rem 2.1) for the generative model. We additionally provide stability guarantees when reusing real data at each step (Theorem 2.3 and Theorem 2.4) establishing close connections with RLHF and preference optimization. Our work sheds light and theoretically grounds a novel phenomenon: in- creasing the proportion of curated synthetic data on the Web automatically optimizes preferences for future trained large models. A limitation is that we do not propose an algorithm to address emerging issues like bias amplification as we feel it goes beyond the scope of our paper and is a substantially complex field already intensively explored (Grover et al., 2019; Wyllie et al., 2024; Chen et al., 2024b). We believe, however, that it should be a research priority and constitutes an interesting avenue for future work. Another interesting direction is to study the impact of the particular reward function underlying filtering (confidence, quality, diversity...) on the emerging bias amplification."}, {"title": "6 Broader impacts", "content": "Training and alignment of large generative models are prone to substantial ethical concerns regard- ing their alignment objective (Shen et al., 2023), representational disparities of the training datasets (Clemmensen and Kj\u00e6rsgaard, 2022), or the presence of harmful images in the datasets (Birhane et al., 2021; Schramowski et al., 2023; Birhane et al., 2024). Our work mostly focuses on the impact of the curation of synthetic datasets which itself heavily depends on the agent performing the cura- tion and its underlying reward function. In particular the documentation of the Simulacra Aesthetic Captions dataset (Pressman et al., 2022) alerts that the human-based curation step is performed by a group of individuals that lacks diversity, mostly from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) individuals (Henrich et al., 2010). A similar bias is likely occurring in the JourneyDB (Pan et al., 2023) dataset and, more generally, in the synthetic data appearing on the web. However, our work mostly revolves around a theoretical analysis and raises awareness of the"}]}