{"title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement", "authors": ["Zhexin Zhang", "Leqi Lei", "Junxiao Yang", "Xijie Huang", "Yida Lu", "Shiyao Cui", "Renmiao Chen", "Qinglin Zhang", "Xinyuan Wang", "Hao Wang", "Hao Li", "Xianqi Lei", "Chengwei Pan", "Lei Sha", "Hongning Wang", "Minlie Huang"], "abstract": "As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement.", "sections": [{"title": "1 Introduction", "content": "AI models have garnered significant attention in recent years due to their remarkable improvements in performing a wide range of tasks. However, as these models grow in capability, they also introduce critical safety concerns. For instance, these models may leak private information (Zhang et al., 2023b), generate harmful content (Zou et al., 2023), or exhibit unsafe behaviors in interactive environments (Zhang et al., 2024a). These risks pose substantial barriers to the reliable and widespread deployment of AI systems, making safety a crucial area of research. To address these challenges, the research community has devoted increasing effort to AI safety, which can be broadly categorized into two key areas: (1) safety evaluation, which involves identifying vulnerabilities through jailbreak attacks (Liu et al., 2023; Yu et al., 2023), developing specialized benchmarks (Mazeika et al., 2024a), and designing safety-scoring models (Inan et al., 2023; Zhang et al., 2024b); and (2) safety improvement, which focuses on developing defense mechanisms and alignment strategies to mitigate risks and improve AI robustness (Xie et al., 2023). Despite the advancements in evaluation methodologies and improvement strategies, significant challenges persist in comparing these approaches. These challenges primarily stem from variations in experimental setups, such as differences in test data and victim models. Furthermore, reimplementing prior work can be time-intensive, especially when source code is unavailable. Even when source code is accessible, considerable effort may still be required to configure specific environments or adapt the implementation to accommodate new datasets and models.\nTo this end, we introduce AISafetyLab, a comprehensive framework for evaluating and improving AI safety. The framework comprises three core modules: Attack, Defense and Evaluation. The Attack module currently implements 13 representative jailbreak attack methods, encompassing both black-box and white-box techniques. The Defense module supports 3 training-based defense strategies and 13 inference-time defense mechanisms, all aimed at preventing the model from generating unsafe content. The Evaluation module integrates mainstream safety scoring methods, including 2 rule-based scorers and 5 model-based scorers. In addition, AISafetyLab features four auxiliary mod-"}, {"title": "2 Related Work", "content": "Recent studies have introduced various approaches for assigning safety scores to content generated by LLMs. ShieldGemma (Zeng et al., 2024) offers a suite of LLM-based content moderation tools built on Gemma2. WildGuard (Han et al., 2024) presents an open-source, lightweight moderation tool designed to address risks such as jailbreaks and refusals. Notably, ShieldLM (Zhang et al., 2024b) introduces customizable safety detectors capable of generating detailed explanations for their decisions. Llama Guard (Inan et al., 2023) ensures input-output safeguards for human-AI interactions, while OpenAI's holistic detection approach (Markov et al., 2023) supports an API for moderating real-world content.\nBenchmarks play a crucial role in standardizing safety evaluations by providing standard and comprehensive test environments. Agent-SafetyBench (Zhang et al., 2024a) features 2,000 test cases spanning eight safety risks and ten failure modes, covering 349 novel interaction environments. SORRY-Bench (Xie et al., 2024) focuses on the refusal behaviors of LLMs, with 450 unsafe instructions. SALAD-Bench (Li et al., 2024) broadens the scope by utilizing intricate taxonomies and employing the LLM-based evaluator MD-Judge to assess performance. Earlier contributions, such as SafetyBench (Zhang et al., 2023a), provided 11,435 multiple-choice safety questions, while Anthropic's red-teaming dataset (Ganguli et al., 2022) offered valuable insights into harm reduction strategies.\nThe increasing sophistication of attacks on large language models (LLMs) has been a focal point of recent research, with these attacks broadly categorized into black-box and white-box methods. Black-box attacks, including AutoDAN-Turbo (Liu et al., 2024), GPTFuzzer (Yu et al., 2023), ReNeLLM (Ding et al., 2023), Black-DAN (Wang et al., 2024), and PAIR (Chao et al., 2023), focus on crafting adversarial prompts or jailbreak templates without direct access to the model's internal architecture. These techniques achieve high success rates by exploiting model vulnerabilities, often utilizing methods such as social"}, {"title": "2.2 AI Safety Improvement", "content": "In addition to AI safety evaluation, an equally crucial area of research focuses on developing defensive strategies to counteract various attacks. These defenses can be broadly classified into two categories: training-based and inference-based approaches. Training-based defenses, such as Safe Unlearning (Zhang et al., 2024c), Layer-specific Editing (Zhao et al., 2024), and Safety-Tuned Reinforcement Learning (Dai et al., 2023), aim to improve model alignment during the training phase. These strategies incorporate safety-oriented objectives, modify specific model layers, or introduce carefully curated safety datasets to ensure the model's robustness against potential threats. Inference-based defenses, on the other hand, operate during the inference stage to mitigate harmful outputs. Approaches such as SafeDecoding (Xu et al., 2024) and Goal Prioritization (Zhang et al., 2024e) intervene at this stage to reduce the risk of undesirable behaviors. Additional techniques like RAIN (Li et al., 2023b) and Robustly Aligned LLM (RA-LLM) (Cao et al., 2023) further enhance safety by dynamically aligning outputs or integrating robust safety checks."}, {"title": "2.3 Other Toolkits", "content": "Recent efforts have integrated various adversarial attack methods, such as EasyJailBreak (Zhou et al., 2024) and Harmbench (Mazeika et al., 2024a), which implement a diverse set of jailbreak attack strategies. Despite significant advancements in attack methodologies, research on defense mechanisms remains fragmented. Existing approaches typically concentrate on isolated defense strategies or individual evaluation metrics, often lacking a unified framework that integrates both attack and defense techniques into a comprehensive adversarial benchmarking system. This gap highlights the pressing need for an all-in-one platform capable of robust attack simulation, defense evaluation, and the assessment of LLM resilience."}, {"title": "3 Framework Design", "content": "We illustrate the overview of AISafetyLab in Figure 1. We implement various representative attack and defense methods, which can be applied to AI models simultaneously to produce the final outputs. Then various evaluation methods could be applied to access the safety of the outputs. To support the three main modules, we also implement four auxiliary shared modules, including Models, Dataset, Utils and Logging. Next, we will introduce these modules in detail."}, {"title": "3.2 Attack", "content": "In this section, we introduce the AISafetyLab attack module, a critical component of the overall package. This module is designed to assess the safety capabilities of LLMs against adversarial attacks, particularly those that attempt to bypass safety mechanisms. It features 13 representative adversarial attack methods, classified into three categories based on the access level to the target model:\n\n\u2022 White-box Attacks: The attacker has full access to the architecture, parameters and internal states (e.g., gradient) of the target model. This enables more targeted and precise manipulations of the model's behavior. We implement GCG (Zou et al., 2023) as a representation of this kind of attacks.\n\n\u2022 Gray-box Attacks: The attacker has partial access, typically limited to input queries, output text, and corresponding log probabilities. This information is easier to acquire compared to that in the white-box setting. In this category, AutoDAN (Liu et al., 2023), LAA (Andriushchenko et al., 2024) and Advprompter(Paulus et al., 2024) are implemented.\n\n\u2022 Black-box Attacks: The attacker has minimal access, often restricted to input queries and output text. These attacks are the most challenging and resource-constrained, relying on input-output interactions to circumvent safety mechanisms. The following 9"}, {"title": "3.2.1 Attack Module Design", "content": "To streamline the use of these diverse attack methods, the attack module of AISafetyLab is designed to be modular and flexible. The core components of the attack module include:\n\n\u2022 Init: This module initializes the attacking environment by loading models and datasets, and setting up the necessary infrastructure for running the attacks.\n\n\u2022 Mutate: The mutate module collects various mutation strategies used by different attack methods. These strategies are applied to modify input queries in ways that maximize the chances of bypassing the model's defenses.\n\n\u2022 Select: The select module assists in identifying the most promising adversarial queries by ranking them based on relevant signals.\n\n\u2022 Feedback: The feedback module provides optimization signals that guide the attacker in refining and enhancing the generated prompts.\nThis modular design provides two key advantages:\n1. User-Friendly: The well-structured framework simplifies comprehension, allowing newcomers to easily grasp the internal workings of various attack methods.\n2. Customizability: Developers can extend or modify the attack flow by adapting individual modules, facilitating the creation of new attack strategies using the provided building blocks.\nBy providing this modular and extensible framework, AISafetyLab enables researchers to experiment with a wide variety of adversarial techniques and gain deeper insights into the safety and robustness of LLMs."}, {"title": "3.3 Defense", "content": "We categorize the safety defenses of large language models into two primary types: inference-time defenses and training-time defenses, as illustrated in Figure 2 in the Appendix. Our modular defense framework is designed to support rapid and flexible"}, {"title": "3.4 Evaluation", "content": "We integrate seven widely applied evaluation methods for safety detection, each implemented as a Scorer module inherited from the base module BaseScorer. These scorers are categorized into three main types:\n\n\u2022 Pattern-based Scorer. These scorers determine the success of a jailbreak attempt by matching the model's response against a predefined set of patterns, including PatternScorer and PrefixMatchScorer.\n\n\u2022 Finetuning-based Scorer. This category of scorers assesses the safety of responses using fine-tuned scoring models, including ClassficationScorer, ShieldLMScorer, HarmBenchScorer and LlamaGuard3Scorer.\n\n\u2022 Prompt-based Scorer. This category of scorers evaluates response safety by prompting the model with specifically designed safety detection guidelines, including PromptedLLMScorer.\nAll the scorers utilize the same interface score to conduct safety evaluation, which takes a query-response pair as input and returns the judgment from the scorer. Additional outputs from the scorer are also returned to provide comprehensive information during evaluation. The implementation details of the scorers are presented in Appendix C.\nAdditionally, we implement a scorer named OverRefuseScorer based on the work of R\u00f6ttger et al. (2024), which prompts an LLM to evaluate the over-refusal rate of a model. The interface of this scorer is consistent with that of other scorers."}, {"title": "3.5 Auxiliary Modules", "content": "We introduce four auxiliary modules that facilitate the implementation of the three core modules. Each of these auxiliary modules is detailed below.\nModels Our framework currently supports two primary types of models: local transformer-based models and API-based models. Specifically, local models must be compatible with the Hugging Face Transformers library, while API-based models must adhere to OpenAI-compatible access interfaces. To enhance usability, we provide unified interfaces for local models, such as chat and batch_chat, which enable text generation based on given input prompts. Additionally, for API-based models, we incorporate robust error-handling mechanisms within the chat interface, allowing for a configurable maximum number of retry attempts in the event of errors.\nDataset This module primarily manages dataset loading and slicing. It supports both local data files and datasets from the Hugging Face Datasets library. Furthermore, it includes a configurable subset_slice parameter, which allows users to specify a subset of the dataset for selection. This feature is particularly beneficial for running experiments on smaller portions of a dataset or resuming experiments that were previously interrupted.\nUtilities The utilities module provides various helper functions categorized into four key areas: (1) model-related functions (e.g., perplexity computation), (2) string processing utilities (e.g., function word identification), (3) configuration management (e.g., loading attack method configurations), and (4) miscellaneous functionalities.\nLogging This module is responsible for logging functionalities and leverages the loguru library to provide a shared logger across the entire project. We implement an intuitive interface, setup_logger, to configure logging settings, such as directing command-line outputs to a file. The logger supports various log levels (e.g., debug and error) and automatically records useful metadata, including timestamps and command execution locations."}, {"title": "4 Usage", "content": "AISafetyLab provides a general and user-friendly interface for LLM attack, defense, and evaluation with just a few lines of code. In this section, we offer examples for attack, defense and evaluation, respectively."}, {"title": "4.1 Attack", "content": "The following code demonstrates the setup of our attack module. After importing the corresponding attack method (e.g., AutoDANManager), the attack function can automatically initiate an attack towards the target model specified in the configuration file and save the resulting responses."}, {"title": "4.2 Defense", "content": "We provide a demonstration of the defense module usage below. After specifying the defense method, the chat function can execute the defense and produce the corresponding results. Note that we support the simultaneous deployment of multiple defense strategies."}, {"title": "4.3 Evaluation", "content": "The usage of the evaluation module is as follows. After instantiating the scorer with a specific evaluation method, the score function can provide evaluation results for a given instruction-response pair."}, {"title": "5 Experiments", "content": "Using AISafetyLab, we conducted a series of experiments to evaluate various attack and defense methods, employing Llama-Guard-3-8B as the scoring model. To highlight the performance gap between attack and defense strategies, we initially selected Vicuna-7B-v1.5 (a model that is relatively weak in safety) as the target model and assessed its performance on a subset of the HarmBench dataset that contains 50 harmful instructions. Additionally, we assess the overrefusal tendencies of various defense methods on XSTest (R\u00f6ttger et al., 2024).\nWe examined 13 representative attack methods as detailed in Section 3.2. All methods were applied to the same subset of HarmBench.\nWe evaluated 13 representative inference-time defense methods and three training-time defense methods, as described in Section 3.3. For the training-time defenses, the size of the training dataset was controlled to approximately 1,000 samples."}, {"title": "5.2 Main Results", "content": "The results are summarized in Table 1, highlighting attack success rates under different defense strategies and the corresponding overrefusal rates."}, {"title": "Attack Effectiveness", "content": "Among the evaluated attack methods, AutoDAN demonstrates superior effectiveness across various defense mechanisms, while PAIR, DeepInception and Jailbroken also achieve attack success rates (ASR) exceeding 35%. Notably, some methods, such as GCG and SAA, perform well on the vanilla model but experience a significant drop in effectiveness when confronted with defensive measures. These findings underscore the importance of evaluating attack methods under diverse defense strategies."}, {"title": "Defense Effectiveness", "content": "At the inference stage, Prompt Guard, Robust Aligned, and Self Evaluation emerge as the most effective defensive strategies, as discussed in Section 3.3. In terms of training-based defenses, Safe Unlearning proves to be the most effective, reducing the average attack success rate to 14.8%. Notably, Prompt Guard completely neutralizes all attacks by employing a classifier on input queries. However, some defenses, such as Erase Check and Robust Aligned, while highly effective, introduce significant over-refusal rates, highlighting a trade-off in overall usability. Additionally, approaches like PPL Filter and Erase Check are only effective against specific attack methods that rely on unreadable adversarial prompts. These findings underscore the ongoing challenge of balancing security with usability in current defense mechanisms."}, {"title": "Challenge on Robustness", "content": "The evaluation of robustness still poses significant challenges, often resulting in unfair comparisons between methods. For example, while DeepInception achieves an attack success rate (ASR) above 40% under methods such as Safe Unlearning and Self Evaluation, the responses often consist of fictional narratives or simple repetitions of the question, without providing precise or potentially harmful information. These problems underscore the necessity for more dependable evaluation frameworks that can accurately measure performance across a variety of adversarial conditions."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we introduce AISafetyLab, a comprehensive framework and resource for advancing AI safety evaluation and improvement. For users who prefer not to modify the internal code, AISafetyLab offers broad method and model coverage, simple interfaces, and diverse examples to facilitate quick experimentation and application. For developers interested in implementing new methods, our structured design aims to minimize effort and streamline integration.\nWe are committed to continuously maintaining and enhancing AISafetyLab. Some of our future plans include:\n\u2022 Adding an explainability module to improve understanding of AI safety mechanisms.\n\u2022 Implementing methods for multimodal safety.\n\u2022 Implementing methods for agent safety.\n\u2022 Integrate more methods for LLM safety.\n\u2022 Regularly updating the paper list for AI safety.\n\u2022 Maintaining and improving the codebase by addressing bugs and issues.\nWe are dedicated to executing these plans and warmly welcome community suggestions and contributions, as collaborative efforts will be instrumental in advancing AI safety."}, {"title": "A Implementation Details of Attackers", "content": "The implementation details of the 13 attackers mentioned in section 3.2 are presented as follows:\nA.0.1 White-box Attacks\n\u2022 GCG (Zou et al., 2023): The Greedy Coordinate Gradient-based Search (GCG) attack method perturbs the input tokens using the loss gradient as an optimization signal. The loss function is designed to maximize the probability of an affirmative prefix, thus guiding the model to produce unsafe or undesirable outputs.\nA.0.2 Gray-box Attacks\n\u2022 AutoDAN (Liu et al., 2023): AutoDAN generates jailbreak prompts using a hierarchical genetic algorithm. The goal is to automatically evolve effective attack strategies that bypass the model's safety mechanisms.\n\u2022 LAA (Andriushchenko et al., 2024): The LAA method designs adaptive templates and appends adversarial suffixes to the chosen template. The suffix is optimized through random search, achieving high success rates for bypassing model defenses.\n\u2022 Advprompter (Paulus et al., 2024): This method trains an attacker LLM to autoregressively generate adversarial suffixes to a given input query, making it effective for generating successful jailbreak prompts.\nA.0.3 Black-box Attacks\n\u2022 GPTFuzzer (Yu et al., 2023): This method generates new jailbreak templates through iterative mutation of human-written templates. It employs five mutation techniques: generation, crossover, expansion, shortening, and rephrasing, all aimed at finding prompts that can successfully bypass model defenses.\n\u2022 Cipher (Yuan et al., 2023): The Cipher attack works by encoding instructions in a cryptic manner, such that the model's safety alignment mechanisms fail to interpret the instructions correctly, enabling a jailbreak.\n\u2022 DeepInception (Li et al., 2023a): This attack creates diverse scenes and characters to mislead the target model's safety filters, thus circumventing its safety alignment."}, {"title": "Implementation Details of Defenders", "content": "B.1 Inference Time Defense\nWe selected 13 inference-time defense methods, categorized into three main strategies, and provide a unified interface for these defense methods, which can be utilized in the defend_chat function, as illustrated in Table 2.\nB.2 Training Time Defense\nThe Training Time Defense is applied during the training process of large language models. It includes three categories: Safety Data Tuning, RL-based Alignment and Unlearning. And we implement a method for each category.\n\u2022 Safety Data Tuning: Safety-Tuning (Bianchi et al., 2023)"}, {"title": "Implementation Details of Scorers", "content": "The implementation details of the seven scorers mentioned in section 3.4 are presented as follows:\n\u2022 PatternScorer. This scorer judges the success of a jailbreak attempt by examining whether the response matches predefined jailbreaking failure patterns. A response is considered safe if it includes a certain failure pattern.\n\u2022 PrefixMatchScorer. This scorer judges the success of a jailbreak by checking the prefix of the response. A response is considered unsafe if it starts with a jailbreaking target.\n\u2022 ClassficationScorer. This scorer uses a classification model to assess the safety of the response. The scorer is initialized by a Roberta model from Yu et al. (2023), and it is also acceptable to init this scorer with any customized classification model.\n\u2022 ShieldLMScorer. This scorer uses ShieldLM to assess the safety of the response, which incorporates an analysis alongside its judgment explaining the reason behind its decision.\n\u2022 HarmBenchScorer. This scorer uses the"}]}