{"title": "Misrepresented Technological Solutions in Imagined Futures: The Origins and Dangers of AI Hype in the Research Community", "authors": ["Savannah Thais"], "abstract": "Technology does not exist in a vacuum; technological development, media representation, public perception, and governmental regulation cyclically influence each other to produce the collective understanding of a technology's capabilities, utilities, and risks. When these capabilities are overestimated, there is an enhanced risk of subjecting the public to dangerous or harmful technology, artificially restricting research and development directions, and enabling misguided or detrimental policy. The dangers of technological hype are particularly relevant in the rapidly evolving space of AI. Centering the research community as a key player in the development and proliferation of hype, we examine the origins and risks of AI hype to the research community and society more broadly and propose a set of measures that researchers, regulators, and the public can take to mitigate these risks and reduce the prevalence of unfounded claims about the technology.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) has become a ubiquitous topic in the modern world as it has captivated the popular imagination, spurred rapid technological development, and captured a significant share of the investment market. However, despite being presumably a scientific field, many of the descriptions and promises of AI systems are not empirically proven or conceptually well grounded.\nThe media, and even some researchers themselves, unfoundedly ascribe language understanding, general reasoning ability, or even sentience to AI systems, and claim that we are close to achieving artificial general intelligence (AGI) (Grace et al. 2017). Through various intentional or accidental mechanisms including failures in testing or system design (Kapoor and Narayanan 2022b), poorly constructed or characterized data sets (Raji et al. 2021, 2022), or outright deception (Raji et al. 2022), researchers and developers can create AI systems that fail to live up to their performance and reliability promises.\nThis culture of hype has real world consequences. Inaccurate or misrepresented AI systems can cause harm to the public and exacerbate societal biases at an unprecedented scale (Raji et al. 2022; Wang et al. 2022). Unscientific and misleading claims about AI also negatively impact the research and development ecosystem itself by incentivizing certain research directions over others and affecting how broader society views the validity and utility of the field.\nHere, we define AI hype as any non empirically or rigorously theoretically supported performance claims, capability narratives, or system descriptions. We consider empirically supported claims to mean performances or capabilities that demonstrated through properly designed, reproducible scientific research, that generalize outside of the initial experimental context, and are precisely characterized in their descriptive language (i.e. not saying a model exhibits language understanding when what is meant is that it achieved high accuracy on a multiple choice benchmark data set like MMLU (Hendrycks et al. 2020)), while we consider theoretically supported claims to be provably derived directly from statistical or mathematical theory.\nCentering the research and development community as a key responsible player in the development and proliferation of hype, this paper will explore origins of AI hype and the dangers that hype poses to both society and the research ecosystem. We will then present actions that the research community can take to address these concerns and ways that those efforts can be scaffolded by political action and community work."}, {"title": "Current Landscape of AI Rhetoric", "content": "Recent advancements in AI, though undoubtedly impressive, have been surrounded by unprecedented levels of hype. Particular attention has been paid recently to large language models (LLMs). Some claims like scientific reasoning and understanding of the short-lived model Galactica (Taylor et al. 2022) or the ascribed sentience of more general LLMs like LaDMa (Tiku 2022) received relatively little uptake or have been quickly debunked (Heaven 2022b; Shardlow and Przyby\u0142a 2022).\nMore pernicious and common are claims of general or approximate language understanding(Bender and Koller 2020) and reasoning abilities (Bubeck et al. 2023). This belief enables additional sociological claims that LLMs will soon replace human writers in a variety of domains (Bilton 2022), precipitate a breakdown of traditional education (Marche 2022), and transform search and information retrieval (Shah and Bender 2022), which, while entirely possible due to financial power structures, are largely ungrounded in scientifically proven abilities of these models (indeed, we have already seen the threat of AI replacement play a central role in the Hollywood writers strike (Coyle 2023)). While LLMs have demonstrated exceptional performance on certain purported reasoning related benchmark data sets, such as MMLU, they perfrom substantially worse than humans on others, such as ConceptARC (Mitchell, Palmarini, and Moskvichev 2023) and PlanBench (Valmeekam et al. 2023). As discussed further in Section, these apparent discrepancies in ability are in part attributable to the design and conceptualization of benchmark tasks themselves.\nFurthermore, research in NLP has demonstrated that despite these LLMs performing well on some established benchmark tasks, this performance can be attributed to learning to manipulate established linguistic forms (Bender and Koller 2020) or to leverage artifacts from the training data sets (Niven and Kao 2019; Bras et al. 2020), and in fact this performance often does not generalize. For example, Niven and Kao demonstrated that BERT's performance on the Argument Reasoning Comprehension Task is accounted for by exploitation of spurious statistical cues in the data set (Niven and Kao 2019).\nMoreover, these claims obfuscate known limitations and issues with the technology. Notably, they are prone to hallucination and can generate coherent and convincing yet false text, including factually incorrect historical or scientific facts, causally inaccurate reasoning, or fabricated sources and references (Gao et al. 2022; Bickmore et al. 2018; Jack Brewster 2023). Additionally, they often demonstrate bias against certain groups, owing in large part to their training corpus of text scraped from the internet (Bender et al. 2021; Brown et al. 2020; Xu et al. 2020; Abid, Farooqi, and Zou 2021). Unfortunately, some initial exploration demonstrates that these issues persist even in models that are nominally safeguarded, such as ChatGPT (Biddle 2022). These substantial limitations and the risks they pose, as described in Section 4, currently render them largely non-robust and untrustworthy, and perhaps of limited practical utility.\nSimilar hype and controversy surround other types of generative AI, namely text-to-image and text-to-video generators like DALL-E 2, Stable Diffusion, Make-A-Video, Imagen Video, and others. These models have been described as demonstrating visual reasoning skills or human-like creativity (Heaven 2022a). As with LLMs, news outlets, researchers, and artists themselves have questioned if, or even predicted that, this technology will soon replace (or at least substantially transform) a wide range of creative careers (Heaven 2022a; Bilton 2022).\nWhile the capabilities of these generative AI models are undoubtedly impressive, many of the claims surrounding this technology are, again, not empirically grounded. In particular, recent research has demonstrated that these models are unable to correctly interpret prompts containing specific spatial relation or object multiplicity information (Lin and Srikanth 2023; Gokhale et al. 2022; Cho, Zala, and Bansal 2022). Furthermore, they are not robust to prompt variation, and small adjustments or re-orderings of the prompt can result in substantially different outputs (Witteveen and Andrews 2022; Gokhale et al. 2022). Additionally, they are susceptible to the same issues of bias present in LLMs; they demonstrate gender and skin-tone biases when given neutral text prompts (Cho, Zala, and Bansal 2022) as well as context bias when provided with gender or race specified input (Heikkil\u00e4 2022).\nAlthough there is substantial hype around recent AI systems, this phenomena is by no means new. Autonomous driving systems are perhaps the most over-hyped AI technology. Various executives and researchers have claimed for years that fully autonomous or self-driving vehicles are on the verge of entering the market; for example, in 2017 Jensen Huang, the CEO of Nvidia, proclaimed that \u201cwe can realize this vision [of self-driving cars] right now", "AI is the solution to self-driving\" (Stilgoe 2018). Two years previously, Elon Musk, the CEO of Tesla, promised that self-driving is \\\"almost...a solved problem": "Musk has continued to tout Tesla's self-driving capabilities since, despite having to repeatedly walk back, temper, or qualify these claims (Stilgoe 2018). Regardless of the continued hype and large-scale financial investments, only a handful of prototype vehicles have been tested on public roads, and safe and reliable autonomous vehicles remain beyond the immediate horizon of technological capabilities (Stilgoe 2019). In fact, backlash to the sustained hype around this technology and its lack of realization has led some to conclude that \u201cself-driving cars are going nowhere", "conceptually impossible tasks\" where no causal connection exists between the observable data and the proposed task (Raji et al. 2022), despite developers' or marketers' claims that AI can learn information that simply is not perceived by the human brain.\nThe culture of hype surrounding different types of AI systems, as well as factors described in the next section, contribute to perhaps the most ubiquitous non-scientifically grounded belief about AI: the expectation of near future AGI. Although there is no agreed upon scientific definition of AGI (Morris et al. 2024) and AI researchers hold extremely varied opinions about the near-term likelihood of AGI (Grace et al. 2024) (in fact, many have stated that AGI is far from being realized (Prunkl and Whittlestone 2020; Atkinson 2016)), this rhetoric has captured substantial public and media attention, garnered significant financial investment, and enabled the proliferation of AGI focused start-ups.\"\n    },\n    {\n      \"title\": \"Origins of AI Hype\",\n      \"content\": \"In order to fully characterize the dangers associated with AI hype and identify potential mitigating strategies, we must first understand the factors that allow hype to develop and spread. While many analyses of technological hype focus on popular media representations or financial incentives as origins of hype, here we will focus largely on issues inherent to the current AI research and development (R&D) ecosystem and explore how these concerns enable and enhance broader hype cycles.\nAl researchers and developers exercise nearly complete control over the design, and thereby applicability and reliability, of a given AI system. Nonetheless, they typically perceive both the underlying data and eventual impacts of these systems as outside of their control (Slota et al. 2020). Along with the issues described in the following section, this fosters a culture of distance between R&D and the societal context of technology. This can lead to researchers and developers prioritizing certain technical considerations like efficiency over more human-centered concerns like preventing harm (Dobbe, Gilbert, and Mintz 2019). This perceived distance becomes particularly problematic in the current fast-paced technological R&D ecosytem that tends to prioritize the race to innovation and the capture of market share.\nBy focusing primarily on the R&D ecosystem, we hope to re-center researchers and developers as critical actors in the construction and propagation of misleading or dangerous rhetoric around AI and shift some of the responsibility for counteracting these narratives back to these groups.\"\n    },\n    {\n      \"title\": \"Research and Development Environments\",\n      \"content\": \"Al hype originates in part from the culture of R&D environments and how researchers and developers mathematically and scientifically characterize current AI technology. Many of the issues outlined below are fundamentally intertwined; responsible and honest R&D must reckon with these considerations collectively.\nA primary origin of over-hyped AI capabilities is the fact that many AI systems are developed in sterile R&D environments and then deployed in more complex real-world settings without appropriate testing or oversight. In some cases, a mismatch between performance in R&D and deployment settings is attributable to sampling bias in the test set distribution (Kapoor and Narayanan 2022b). Choices made during data collection and pre-processing can render a data set non-representative of the population the AI system is designed to be used on; for example, a disease prediction tool trained on medical images may not generalize from one hospital environment to another (Zech et al. 2018) or may not be robust to small perturbations caused by malfunctions of the imaging device (Antun et al. 2020).\nIn other cases, performance disparities are attributable to a combination of data set and evaluation metric design choices. Namely, discrepancies between evaluations in an R&D settings and a meaningful characterization of how the Al system will actually perform in the real world (Raji et al. 2021). A lack of so-called 'construct validity' can lead to performance issues by allowing researchers and developers to neglect to include distribution of societal harms and benefits in their evaluation metrics (Raji et al. 2022) and by focusing R&D efforts on improving performance on artificially constructed benchmark data sets that do not meaningfully relate to the general reasoning tasks they purport to measure (Raji et al. 2021).\nDesigning effective and robust benchmark data sets for AI models is a challenging task, and mischaracterizing what a benchmark is designed to do is a significant contributor to Al hype. For example, ImageNet is widely accepted as a benchmark for visual object recognition, yet it seemingly arbitrarily relies on the WorldNet ontology to derive class labels, and thus is highly limited in the classes it actually represents. It has therefore been critiqued as actually representing a very specifc, rather than general, vision benchmark (Raji et al. 2021).\nSome R&D environments and applications of AI also eschew critical scientific and software development practices. For example, as demonstrated by surveys of AI researchers, the predictions made by AI systems are often taken at face value without a meaningful understanding of the associated uncertainty (Slota et al. 2020; Fleischmann and Wallace 2009). In other scientific fields, the uncertainty of a measurement or model is an essential component of understanding its significance and utility. Neglecting to provide accounts of the uncertainty associated with AI systems allows developers to gloss over potential concerns with the underlying training data, possible mismatch between the predicted labels and the real world decision being modeled, and many other critical design issues (Raji et al. 2022; Kapoor and Narayanan 2022b; Wang et al. 2022). Additionally, studies have found that many AI developers often disregard traditional software development practices like testing and quality assurance (Wan et al. 2021) and that ML and AI development occurs substantial 'technical debt' in terms of documentation, reproducibility, and reliability (Sculley et al. 2015). These issues coalesce to create the present so-called 'reproducibility crisis' in AI and ML R&D.\nThere are also issues inherent to the culture of technology R&D including how technologists view other fields and how we cognitively and linguistically frame the problems that we work on. Many AI systems seek to make predictions about complex social behaviors and future outcomes and many researchers and developers hold a belief that such predictions are possible with currently available data. However, as Campolo and Crawford describe, this belief requires an \u201cepistemological flattening\" of complex societal contexts into acute signals that neglect the probabilistic worldviews traditionally employed in social science research (Campolo and Crawford 2020). As demonstrated in a study by Sambasivan and Veeraraghavan, AI researchers and developers also tend to neglect the importance of domain expertise in understanding the appropriate development and limitations of AI systems and frequently reduce researchers and practitioners in other fields to data collectors rather than equal collaborators (Sambasivan and Veeraraghavan 2022). This domain hubris can result in a host of technical issues including feature illegitimacy (Chiavegatto Filho, Batista, and Dos Santos 2021), lack of appropriate data segregation (Oner et al. 2020), and insufficient pre-deployment testing (Nagendran et al. 2020).\nAdditionally, despite some advances in explainable and interpretable AI, there remains a profound mismatch between the high-dimensionality of many AI systems and the interpretation capacity of human developers and users (Burrell 2016). There is a profound disconnect between technological progress and a fundamental scientific understanding of how AI systems function (Campolo and Crawford 2020). This leads researchers and developers to describe their work in less precise or exact terminology and creates a perceived lack of control over the technological narrative (Slota et al. 2020). As discussed further in Section 3.3, the language used by researchers and developers to describe their work can substantially contribute to AI hype.\"\n    },\n    {\n      \"title\": \"Financial Incentives\",\n      \"content\": \"The R&D ecosystem and its outputs cannot be divorced from the financial mechanisms that support it. Technological R&D is often viewed as a primary driver of economic prosperity and national security (West 2011) and thus attracts substantial public and private investment. AI R&D in particular currently dominates global scientific output (UNESCO 2021) and continues to attract increased funding; the private investment in AI was 18 times higher in 2022 than in 2013, totaling around $93.5 billion (Zhang et al. 2023). Unlike some other areas of scientific R&D, the AI ecosystem is dominated by this private money; in fact, many 'top' academic researchers receive substantial financial support from private tech companies (Abdalla and Abdalla 2020). This funding landscape can incentivize hype and skew publishing norms (Ebell et al. 2021).\nIn particular, the relative abundance of funding but expected short timelines for innovation and return on investment lead to what Rayner calls the 'novelty trap'. Researchers and developers are incentivized to exaggerate the speed and inventiveness of their work while de-emphasizing potential safety and reliability concerns (Rayner 2004). Insidiously, this means that there is often substantial profit to be earned by marketing technological abilities that may not exist (Elish and danah boyd 2018). For example, Chinese technology company Baidu was discovered to have cheated on the popular benchmark data set ImageNet in an effort to include exaggerated performance claims in marketing materials (Simonite 2015); similarly, Amazon misrepresented the ways in which their facial recognition software, Rekognition, could be used by law enforcement in sales and promotional material (CLU 2018). This type of behavior dangerously shifts the responsibility for critically evaluating system performance claims to consumers, regulators, and external researchers, who are often less equipped to undertake this work or are not given necessary access to the systems in question. For example, the landmark Gender Shades study, conducted by external researchers Buolamwini and Gebru uncovered gender and racial bias in commercial facial recognition technology (Buolamwini and Gebru 2018) and was essential for incentivizing the involved tech companies to make their systems safer (Raji and Buolamwini 2019).\"\n    },\n    {\n      \"title\": \"Media and Society\",\n      \"content\": \"Additionally, R&D and the surrounding financial landscape are impacted by public and media discourse and representations of AI. AI has been part of collective human fantasy and science fiction narratives long before technological advances made any of these imagined capabilities remotely possible. These depictions and the resulting cultural norms formed around AI influence not only how the public and reporters perceive this technology, but also how researchers and developers imagine the capabilities and limitations of the very technology they are creating.\nThese normative representations, as well as a real lack of deep mathematical understanding of how many modern AI systems (particularly deep learning systems) function (Raghu et al. 2016; Lu et al. 2017), impact how the individuals closest to AI describe their work. Both developers (Campolo and Crawford 2020) and critics (Vinsel 2021) of technology employ magical, anthropomorphic, or superhuman language to describe Al systems. For example, in the official paper for AlphaZero, a Go-playing AI system, researchers described the technology as \u201csuperhuman\" and operating \\\"without human knowledge\" despite the fact that the entire system was, in fact, developed by humans with an understanding of the mechanisms and goals of the game (Silver et al. 2017). Similarly, Demis Hassabis, the CEO of DeepMind, the company behind AlphaZero, likened the system to a chess-playing alien (Knight 2017).\nIt is unsurprising, then, that journalists reporting on AI face difficulties in accurately discussing the technology. This is further complicated by the reality that journalists may lack the technological and scientific foundations to fully understand and characterize AI even in cases when it is precisely and truthfully described by researchers and developers (Jones, Jones, and Luger 2022). Some researchers and developers have described concerns that outcomes and nuances of AI research are distorted as they leave the immediate R&D ecosystem and cite concern over a lack of scientific precision in media coverage (Slota et al. 2020).\nTogether, these factors can foster a cycle of misrepresentation where R&D environments, funding mechanisms, and social and media representations influence each other to create a collective understanding of AI that may be distant from the scientific reality.\"\n    },\n    {\n      \"title\": \"Technologies as Imagined Futures\",\n      \"content\": \"When interrogating AI hype, it is crucial to emphasize that rhetoric around technology, regardless of the source, is discursive world building. Technology not only represents a solution to a problem, but through its development and presentation asserts what problems exist and should be considered. Stilgoe discusses this phenomena in the context of autonomous vehicles: in media coverage and public policy, vehicular accidents have traditionally been blamed on driver error, and by taking up this rhetoric, autonomous vehicle companies are able to present self-driving cars and AI as a solution to the \u201cdriver problem\". By doing so, they construct an imagined future that entrenches consumer reliance on individual vehicles and detracts financial and intellectual resources from other potential solutions like improved public transit (Stilgoe and Mladenovi\u0107 2022).\nShah and Bender similarly position claims that LLMs will replace search systems. These claims, often perpetuated by the developers of the LLMs in question, neglect to consider the societal context and purpose of search. In many cases, it is more useful and desirable for users to explore and make sense of information on their own, rather than be provided with a singular answer as is common with LLMs (Shah and Bender 2022). By promoting a certain narrative about the abilities and utility of technology, we necessarily impact societal behavior and views about technology.\"\n    },\n    {\n      \"title\"": "Dangers of Misrepresentation"}, {"content": "The pervasiveness of AI hype can prevent researchers and developers, policy makers, journalists, and the public from proactively interrogating these imagined futures. Hype can foster the impression that AI is universally good or bad and can preclude the nuance necessary to use this technology effectively (Slota et al. 2020). Similarly, claims of novelty and innovation can hinder social learning and proper contextualization of AI and create a perceived distance between the technology and its risks (Rayner 2004; Stilgoe 2018).\nCampolo and Crawford describe AI hype as 'enchanted determinism', allowing AI systems to be seen as powerfully predictive and deterministic, yet also as outside the realm of full human understanding and ability (Campolo and Crawford 2020). The unique qualities of this type of hype make it particularly persuasive to a broad range of stakeholders. Its power and ubiquity pose substantial risks not only to society at large but also, critically, to the very R&D ecosystem in which AI technology is constructed."}, {"title": "Unscientific, Inaccurate, and Dangerous Sociotechnical Systems", "content": "When AI capabilities are mischaracterized, there is heightened risk of causing harm to individuals who rely on or are subjected to the predictions of these systems. There are already numerous examples of tangible financial, emotional, and corporeal harm affected by misrepresented or misused Al systems. For example, voice assistants such as Alexa, Siri, and Google Assistant, have been portrayed having the potential to serve as portals for health information (Sezgin et al. 2020). However, a study found that a when given medical related queries, a substantial portion of the information these voice assistants returned could result in user harm or death (Bickmore et al. 2018). In another case, an external audit of a widely deployed sepsis prediction tool developed by the healthcare software company Epic found that despite performance claims to the contrary, the tool failed to identify a majority of sepsis cases and generated a considerable number of false alerts, creating extraordinary risk for treatment mistakes (Wong et al. 2021). In yet another example, Allegheny county in Pennsylvania adopted a screening tool designed to identify children at risk of mistreatment and used it to select families for investigations that could involve removing the child from their families. A full two years after the tool was deployed it was discovered that reported performance measurements were over estimated and that the tool exhibited substantially higher error rates on Black families compared to White families (Chouldechova et al. 2018).\nThe widespread utilization of this type of unreliable predictive AI is directly enabled by AI hype. The specific type of hype surrounding AI, as described in Section 3.3, is used to justify a movement away from explainable decision systems and reduced focus on clear causal mechanisms in high-stakes AI enabled predictions (Campolo and Crawford 2020). The deployment of uninterpretable AI denies the people subjected to these systems opportunities for recourse.\nFurthermore, AI hype fosters a culture of R&D and AI system utilization that values ostensible predictive accuracy over deeper contextual or scientific knowledge of the underlying mechanisms or potential impacts of those predictions (Campolo and Crawford 2020). This allows researchers and developers to ignore validity issues intrinsic to the structure of predictive optimization tasks which Wang et al demonstrate carry inherent risks of harm to individuals (Wang et al. 2022).\nThe way these systems are represented and marketed often results in a displacement of responsibility for negative outcomes onto individual users or subjects who, in practice, lack control over the outcomes (Slota et al. 2020; Elish 2019). For example, despite emphasizing the predictive power, accuracy, and automation capabilities of their systems, technology companies often attempt to distance themselves from harmful impacts of their technology by claiming that the systems were never intended to be used as decision tools and require extensive human oversight to be used safely (Kapoor and Narayanan 2022a). This framing and misallocation of responsibility poses a threat to legal due process and social agency (Citron and Pasquale 2014).\nCritically, the nature of AI allows these risks to proliferate at an unprecedented scale. For instance, the coding question-and-answer site Stack Overflow banned users from posting answers and code examples generated by ChatGPT due to the vast frequency of generated responses that appear convincing yet are subtly incorrect (VINCENT 2022). Similarly, researchers cite increased concern over the risk of AI fueled mis- and dis-information campaigns with vast political and social impacts (ane Andrew Lohn, Musser, and Sedova 2021). Of particular concern is the risk of inextricably embedding different types of social bias in our technological infrastructure (Noble 2018; Eubanks 2018)."}, {"title": "Artificially Limited Research Landscapes", "content": "Though perhaps of less existential importance, Al hype also carries risks to the R&D ecosystem itself. Many of the societal risks described in the previous section are direct consequences of R&D issues. A failure to interrogate these issues precipitates the risk of permanently entrenching these behaviors into the fabric of Al research.\nDishonest rhetoric around the capabilities of AI systems can lead researchers to believe that we have solved certain problems that we in fact have not; as discussed in the previous section, this can lead to drastic societal consequences. However, it can also profoundly affect the types of research questions that are asked and what areas of development are funded. Wang et al surveyed 387 academic, industry, non-profit, and governmental publications and found widespread critical flaws in predictive optimization systems that use ML to make predictions and decisions about future individual outcomes (Wang et al. 2022). Despite research demonstrating that many life outcomes remain inherently unpredictable with currently available data and computing techniques (Salganik et al. 2020) many of the predictive systems studied by Wang et al were publicly deployed or commercially available. This sets a dangerous precedent that incentivizes more developers to produce similar systems and discourages research into their limitations and shortcomings. It also fosters a culture where younger researchers and students are more likely to erroneously believe that this type of predictive system is inherently trustworthy, further entrenching the idea that alternatives or methods of improvement are not fruitful areas of research.\nThis belief in the functionality of outcome predictive AI impacts research questions in individual subfields of AI as well, even some areas typically considered to be under the umbrella of 'AI Ethics'. For example, research in AI fairness often presupposes a trade-off between model accuracy and some notion of fairness (Fish, Kun, and Lelkes 2016; Friedler et al. 2018) and focuses on ways of ensuring Al systems respect existing equal protection laws such as the United States' '4\\5ths' rule for treatment of protected classes in hiring decisions (Wilson et al. 2021; Raghavan et al. 2019). However, as Raji et al demonstrates (Raji et al. 2022), the utility of this research relies on the assumption that the system in question is accurate and reliable, which, as discussed extensively in previous sections, hyped AI systems often are not. Research on interpretable and explainable AI requires similar assumptions about the fidelity of the system being explained. Furthermore, the current belief in the AI R&D ecosystem that large-scale models provide the best accuracy and performance (Bengio and LeCun 2007) pushes researchers to focus on black-box explainability techniques to the detriment of other approaches such as developing improved directly interpretable models (Rudin 2018).\nEven if it is not presumed that we have fully solved certain problems, rhetoric that we are very close to solving particular problems, like AGI, with certain approaches, like increasingly larger models and data sets (collectively referred to as 'scale', or the 'Scaling Hypothesis' (Branwen 2020)), also profoundly impacts the type of questions researchers are incentivized to pursue. While large scale models have certainly provided state-of-the-art performance on certain tasks, notably language modeling (Kaplan et al. 2020; Brown et al. 2020) and computer vision (Yu et al. 2022; Ramesh et al. 2022), and demonstrated impressive abilities, it has not been proven that scale is universally the correct approach to developing more generalizable and performant AI models. In fact, some research has shown that certain substantially smaller models can match large model performance (Frankle and Carbin 2018) and that the benefits of scale vary based on model architecture and learning task (Tay et al. 2022). Furthermore, extremely large data sets are typically uncurated by necessity, and are therefore unlikely to lead to AI models that can represent diverse view points or truly generalize to real-world tasks (Bender et al. 2021).\nNonetheless, the field of AI R&D as a whole is largely focused on scale approaches as evidenced by the coining of the term 'foundation models' to position these large scale models as the central and most important mode of AI (Bommasani et al. 2021). Recently, vast amounts of funding has been poured into research centers and start-up companies focused exclusively on scale approaches. This focus is further driven in part due to large models' performance on benchmark data sets and tasks, which as discussed in Section 3.1, are often not meaningfully connected to real-world operational and societal goals (Raji et al. 2021). The confluence of these benchmarks, financial incentives, and hype-fueling research claims creates a feedback loop that serves to trap AI R&D in a specific and limited focus.\nRhetoric describing the proximity of scale-driven AGI also impacts what new areas of research are prioritized. Notably, AI Saftey and Alignment, the fields of research concerned with ensuring AGI systems do not pose an existential threat to humanity, have become increasingly popular in the past few years and companies in this space have attracted massive amounts of funding. This stands in stark contrast to the relatively small amount of funding being allocated to research organizations and non-profits focused on alleviating the harms that current AI systems are perpetuating.\nIn all areas of research, there is a limit to the available funding and person-power; prioritizing certain research questions necessarily de-prioritizes others, and this selection can have long-lasting and far-reaching consequences. As a field, we must critically consider what questions are worth focusing on in terms of both their potential scientific and societal impacts and what economic and sociological incentives influence our choices of research topics."}, {"title": "Methods of Addressing and Reducing Hype", "content": "In order to appropriately manage expectations around AI, ensure a robust, responsible, and diverse R&D ecosystem, and limit the deployment of unsafe and unreliable AI systems, it is critical to change how AI is discussed and drastically reduce the prevalence of AI hype. As described in the previous sections, many factors interact to cultivate a hype-driven feedback loop that impacts how everyone from researchers and developers to the general public perceives and discusses AI. Consequently, combating AI hype requires a multi-faceted approach involving industry, government, academia, and civil society."}, {"title": "Diversified Funding and Research", "content": "As discussed in Section 4.2", "are": "as Campolo and Crawford put it \"mathematical optimization at scales beyond expert human play\" (Campolo and Crawford 2020). Similarly, we should require a heavier internal burden of proof of the functionality of AI systems before they are deployed. There are multiple proposed frameworks for researchers and developers to interrogate the functionality of their systems (Wang et al. 2022; Kapoor and Narayanan 2022b) and these can be scaffolded by a wider adoption of core scientific practices like uncertainty estimation and significance testing. Additionally, while emergent behavior in large models may be possible, we should refrain from making unsubstantiated claims about an AI system's reasoning abilities and instead work towards developing rigorous tests to characterize such behavior. As the people initially making claims about an AI system's performance, this burden of proof rests squarely on the researchers and developers (Lundberg, Johnson, and Stewart 20"}]}