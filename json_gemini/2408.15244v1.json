{"title": "Misrepresented Technological Solutions in Imagined Futures: The Origins and Dangers of AI Hype in the Research Community", "authors": ["Savannah Thais"], "abstract": "Technology does not exist in a vacuum; technological development, media representation, public perception, and governmental regulation cyclically influence each other to produce the collective understanding of a technology's capabilities, utilities, and risks. When these capabilities are overestimated, there is an enhanced risk of subjecting the public to dangerous or harmful technology, artificially restricting research and development directions, and enabling misguided or detrimental policy. The dangers of technological hype are particularly relevant in the rapidly evolving space of AI. Centering the research community as a key player in the development and proliferation of hype, we examine the origins and risks of AI hype to the research community and society more broadly and propose a set of measures that researchers, regulators, and the public can take to mitigate these risks and reduce the prevalence of unfounded claims about the technology.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) has become a ubiquitous topic in the modern world as it has captivated the popular imagination, spurred rapid technological development, and captured a significant share of the investment market. However, despite being presumably a scientific field, many of the descriptions and promises of AI systems are not empirically proven or conceptually well grounded.\nThe media, and even some researchers themselves, unfoundedly ascribe language understanding, general reasoning ability, or even sentience to AI systems, and claim that we are close to achieving artificial general intelligence (AGI) (Grace et al. 2017). Through various intentional or accidental mechanisms including failures in testing or system design (Kapoor and Narayanan 2022b), poorly constructed or characterized data sets (Raji et al. 2021, 2022), or outright deception (Raji et al. 2022), researchers and developers can create AI systems that fail to live up to their performance and reliability promises.\nThis culture of hype has real world consequences. Inaccurate or misrepresented AI systems can cause harm to the public and exacerbate societal biases at an unprecedented scale (Raji et al. 2022; Wang et al. 2022). Unscientific and"}, {"title": "Current Landscape of AI Rhetoric", "content": "Recent advancements in AI, though undoubtedly impressive, have been surrounded by unprecedented levels of hype. Particular attention has been paid recently to large language models (LLMs). Some claims like scientific reasoning and understanding of the short-lived model Galactica (Taylor et al. 2022) or the ascribed sentience of more general LLMs like LaDMa (Tiku 2022) received relatively little uptake or have been quickly debunked (Heaven 2022b; Shardlow and Przyby\u0142a 2022).\nMore pernicious and common are claims of general or approximate language understanding(Bender and Koller 2020) and reasoning abilities (Bubeck et al. 2023). This belief enables additional sociological claims that LLMs will soon replace human writers in a variety of domains (Bilton 2022), precipitate a breakdown of traditional education (Marche 2022), and transform search and information retrieval (Shah and Bender 2022), which, while entirely possible due to fi-"}, {"title": "Origins of AI Hype", "content": "In order to fully characterize the dangers associated with AI hype and identify potential mitigating strategies, we must first understand the factors that allow hype to develop and spread. While many analyses of technological hype focus on popular media representations or financial incentives as origins of hype, here we will focus largely on issues inherent to the current AI research and development (R&D) ecosystem and explore how these concerns enable and enhance broader hype cycles.\nAl researchers and developers exercise nearly complete control over the design, and thereby applicability and reliability, of a given AI system. Nonetheless, they typically perceive both the underlying data and eventual impacts of these systems as outside of their control (Slota et al. 2020). Along with the issues described in the following section, this fosters a culture of distance between R&D and the societal context of technology. This can lead to researchers and developers prioritizing certain technical considerations like efficiency over more human-centered concerns like preventing harm (Dobbe, Gilbert, and Mintz 2019). This perceived distance becomes particularly problematic in the current fast-paced technological R&D ecosytem that tends to prioritize the race to innovation and the capture of market share.\nBy focusing primarily on the R&D ecosystem, we hope to re-center researchers and developers as critical actors in the construction and propagation of misleading or dangerous rhetoric around AI and shift some of the responsibility for counteracting these narratives back to these groups."}, {"title": "Research and Development Environments", "content": "Al hype originates in part from the culture of R&D environments and how researchers and developers mathematically and scientifically characterize current AI technology. Many of the issues outlined below are fundamentally intertwined; responsible and honest R&D must reckon with these considerations collectively.\nA primary origin of over-hyped AI capabilities is the fact that many AI systems are developed in sterile R&D environments and then deployed in more complex real-world settings without appropriate testing or oversight. In some cases, a mismatch between performance in R&D and deployment settings is attributable to sampling bias in the test set distribution (Kapoor and Narayanan 2022b). Choices made during data collection and pre-processing can render a data set non-representative of the population the AI system is designed to be used on; for example, a disease prediction tool trained on medical images may not generalize from one hospital environment to another (Zech et al. 2018) or may not be robust to small perturbations caused by malfunctions of the imaging device (Antun et al. 2020).\nIn other cases, performance disparities are attributable to a combination of data set and evaluation metric design choices. Namely, discrepancies between evaluations in an R&D settings and a meaningful characterization of how the Al system will actually perform in the real world (Raji et al. 2021). A lack of so-called 'construct validity' can lead to performance issues by allowing researchers and developers to neglect to include distribution of societal harms and benefits in their evaluation metrics (Raji et al. 2022) and by focusing R&D efforts on improving performance on artificially constructed benchmark data sets that do not meaningfully relate to the general reasoning tasks they purport to measure (Raji et al. 2021).\nDesigning effective and robust benchmark data sets for AI models is a challenging task, and mischaracterizing what a benchmark is designed to do is a significant contributor to Al hype. For example, ImageNet is widely accepted as a benchmark for visual object recognition, yet it seemingly arbitrarily relies on the WorldNet ontology to derive class labels, and thus is highly limited in the classes it actually represents. It has therefore been critiqued as actually representing a very specifc, rather than general, vision benchmark (Raji et al. 2021).\nSome R&D environments and applications of AI also eschew critical scientific and software development practices. For example, as demonstrated by surveys of AI researchers, the predictions made by AI systems are often taken at face value without a meaningful understanding of the associated uncertainty (Slota et al. 2020; Fleischmann and Wallace 2009). In other scientific fields, the uncertainty of a measurement or model is an essential component of understanding its significance and utility. Neglecting to provide accounts of the uncertainty associated with AI systems allows developers to gloss over potential concerns with the underlying training data, possible mismatch between the predicted labels and the real world decision being modeled, and many other critical design issues (Raji et al. 2022; Kapoor and Narayanan 2022b; Wang et al. 2022). Additionally, studies have found that many AI developers often disregard traditional software development practices like testing and quality assurance (Wan et al. 2021) and that ML and AI development occurs substantial 'technical debt' in terms of documentation, reproducibility, and reliability (Sculley et al. 2015). These issues coalesce to create the present so-called 'reproducibility crisis' in AI and ML R&D.\nThere are also issues inherent to the culture of technology R&D including how technologists view other fields and how we cognitively and linguistically frame the problems that we work on. Many AI systems seek to make predictions about complex social behaviors and future outcomes and many researchers and developers hold a belief that such predictions are possible with currently available data. However, as Campolo and Crawford describe, this belief requires an \"epistemological flattening\" of complex societal contexts into acute signals that neglect the probabilistic worldviews traditionally employed in social science research (Campolo and Crawford 2020). As demonstrated in a study by Sambasivan and Veeraraghavan, AI researchers and developers also tend to neglect the importance of domain expertise in understanding the appropriate development and limitations of AI systems and frequently reduce researchers and practitioners"}, {"title": "Financial Incentives", "content": "The R&D ecosystem and its outputs cannot be divorced from the financial mechanisms that support it. Technological R&D is often viewed as a primary driver of economic prosperity and national security (West 2011) and thus attracts substantial public and private investment. AI R&D in particular currently dominates global scientific output (UNESCO 2021) and continues to attract increased funding; the private investment in AI was 18 times higher in 2022 than in 2013, totaling around $93.5 billion (Zhang et al. 2023). Unlike some other areas of scientific R&D, the AI ecosystem is dominated by this private money; in fact, many 'top' academic researchers receive substantial financial support from private tech companies (Abdalla and Abdalla 2020). This funding landscape can incentivize hype and skew publishing norms (Ebell et al. 2021).\nIn particular, the relative abundance of funding but expected short timelines for innovation and return on investment lead to what Rayner calls the 'novelty trap'. Researchers and developers are incentivized to exaggerate the speed and inventiveness of their work while de-emphasizing potential safety and reliability concerns (Rayner 2004). Insidiously, this means that there is often substantial profit to be earned by marketing technological abilities that may not exist (Elish and danah boyd 2018). For example, Chinese technology company Baidu was discovered to have cheated on the popular benchmark data set ImageNet in an effort to include exaggerated performance claims in marketing materials (Simonite 2015); similarly, Amazon misrepresented the ways in which their facial recognition software, Rekognition, could be used by law enforcement in sales and promotional material (CLU 2018). This type of behavior dangerously shifts the responsibility for critically evaluating system performance claims to consumers, regulators, and external researchers, who are often less equipped to undertake this work or are not given necessary access to the systems in question. For example, the landmark Gender Shades study, conducted by external researchers Buolamwini and Gebru"}, {"title": "Media and Society", "content": "Additionally, R&D and the surrounding financial landscape are impacted by public and media discourse and representations of AI. AI has been part of collective human fantasy and science fiction narratives long before technological advances made any of these imagined capabilities remotely possible. These depictions and the resulting cultural norms formed around AI influence not only how the public and reporters perceive this technology, but also how researchers and developers imagine the capabilities and limitations of the very technology they are creating.\nThese normative representations, as well as a real lack of deep mathematical understanding of how many modern AI systems (particularly deep learning systems) function (Raghu et al. 2016; Lu et al. 2017), impact how the individuals closest to AI describe their work. Both developers (Campolo and Crawford 2020) and critics (Vinsel 2021) of technology employ magical, anthropomorphic, or superhuman language to describe Al systems. For example, in the official paper for AlphaZero, a Go-playing AI system, researchers described the technology as \u201csuperhuman\" and operating \"without human knowledge\" despite the fact that the entire system was, in fact, developed by humans with an understanding of the mechanisms and goals of the game (Silver et al. 2017). Similarly, Demis Hassabis, the CEO of DeepMind, the company behind AlphaZero, likened the system to a chess-playing alien (Knight 2017).\nIt is unsurprising, then, that journalists reporting on AI face difficulties in accurately discussing the technology. This is further complicated by the reality that journalists may lack the technological and scientific foundations to fully understand and characterize AI even in cases when it is precisely and truthfully described by researchers and developers (Jones, Jones, and Luger 2022). Some researchers and developers have described concerns that outcomes and nuances of AI research are distorted as they leave the immediate R&D ecosystem and cite concern over a lack of scientific precision in media coverage (Slota et al. 2020).\nTogether, these factors can foster a cycle of misrepresentation where R&D environments, funding mechanisms, and social and media representations influence each other to create a collective understanding of AI that may be distant from the scientific reality."}, {"title": "Technologies as Imagined Futures", "content": "When interrogating AI hype, it is crucial to emphasize that rhetoric around technology, regardless of the source, is discursive world building. Technology not only represents a solution to a problem, but through its development and presentation asserts what problems exist and should be considered. Stilgoe discusses this phenomena in the context of autonomous vehicles: in media coverage and public policy, vehicular accidents have traditionally been blamed on driver error, and by taking up this rhetoric, autonomous vehicle"}, {"title": "Dangers of Misrepresentation", "content": "The pervasiveness of AI hype can prevent researchers and developers, policy makers, journalists, and the public from proactively interrogating these imagined futures. Hype can foster the impression that AI is universally good or bad and can preclude the nuance necessary to use this technology effectively (Slota et al. 2020). Similarly, claims of novelty and innovation can hinder social learning and proper contextualization of AI and create a perceived distance between the technology and its risks (Rayner 2004; Stilgoe 2018).\nCampolo and Crawford describe AI hype as 'enchanted determinism', allowing AI systems to be seen as powerfully predictive and deterministic, yet also as outside the realm of full human understanding and ability (Campolo and Crawford 2020). The unique qualities of this type of hype make it particularly persuasive to a broad range of stakeholders. Its power and ubiquity pose substantial risks not only to society at large but also, critically, to the very R&D ecosystem in which AI technology is constructed."}, {"title": "Unscientific, Inaccurate, and Dangerous Sociotechnical Systems", "content": "When AI capabilities are mischaracterized, there is heightened risk of causing harm to individuals who rely on or are subjected to the predictions of these systems. There are already numerous examples of tangible financial, emotional, and corporeal harm affected by misrepresented or misused Al systems. For example, voice assistants such as Alexa, Siri, and Google Assistant, have been portraye having the potential to serve as portals for health information (Sezgin et al. 2020). However, a study found that a when given medical related queries, a substantial portion of the information these voice assistants returned could result in user harm or death (Bickmore et al. 2018). In another case, an external audit of a widely deployed sepsis prediction tool developed by the healthcare software company Epic found that despite performance claims to the contrary, the tool failed to identify a majority of sepsis cases and generated a considerable number of false alerts, creating extraordinary risk for treatment mistakes (Wong et al. 2021). In yet another example, Allegheny county in Pennsylvania adopted a screening tool designed to identify children at risk of mistreatment and used it to select families for investigations that could involve removing the child from their families. A full two years after the tool was deployed it was discovered that reported performance measurements were over estimated and that the tool exhibited substantially higher error rates on Black families compared to White families (Chouldechova et al. 2018).\nThe widespread utilization of this type of unreliable predictive AI is directly enabled by AI hype. The specific type of hype surrounding AI, as described in Section 3.3, is used to justify a movement away from explainable decision systems and reduced focus on clear causal mechanisms in high-stakes AI enabled predictions (Campolo and Crawford 2020). The deployment of uninterpretable AI denies the people subjected to these systems opportunities for recourse.\nFurthermore, AI hype fosters a culture of R&D and AI system utilization that values ostensible predictive accuracy over deeper contextual or scientific knowledge of the underlying mechanisms or potential impacts of those predictions (Campolo and Crawford 2020). This allows researchers and developers to ignore validity issues intrinsic to the structure of predictive optimization tasks which Wang et al demonstrate carry inherent risks of harm to individuals (Wang et al. 2022).\nThe way these systems are represented and marketed often results in a displacement of responsibility for negative outcomes onto individual users or subjects who, in practice, lack control over the outcomes (Slota et al. 2020; Elish 2019). For example, despite emphasizing the predictive power, accuracy, and automation capabilities of their systems, technology companies often attempt to distance themselves from harmful impacts of their technology by claiming that the systems were never intended to be used as decision tools and require extensive human oversight to be used safely (Kapoor and Narayanan 2022a). This framing and misallocation of responsibility poses a threat to legal due process and social agency (Citron and Pasquale 2014).\nCritically, the nature of AI allows these risks to proliferate at an unprecedented scale. For instance, the coding question-and-answer site Stack Overflow banned users from posting answers and code examples generated by ChatGPT due to the vast frequency of generated responses that appear convincing yet are subtly incorrect (VINCENT 2022). Similarly, researchers cite increased concern over the risk of AI fueled mis- and dis-information campaigns with vast political and social impacts (ane Andrew Lohn, Musser, and Sedova 2021). Of particular concern is the risk of inextricably embedding different types of social bias in our technological infrastructure (Noble 2018; Eubanks 2018)."}, {"title": "Artificially Limited Research Landscapes", "content": "Though perhaps of less existential importance, Al hype also carries risks to the R&D ecosystem itself. Many of the societal risks described in the previous section are direct consequences of R&D issues. A failure to interrogate these issues precipitates the risk of permanently entrenching these behaviors into the fabric of Al research.\nDishonest rhetoric around the capabilities of AI systems can lead researchers to believe that we have solved certain"}, {"title": "Methods of Addressing and Reducing Hype", "content": "In order to appropriately manage expectations around AI, ensure a robust, responsible, and diverse R&D ecosystem, and limit the deployment of unsafe and unreliable AI systems, it is critical to change how AI is discussed and drastically reduce the prevalence of AI hype. As described in the previous sections, many factors interact to cultivate a hype-driven feedback loop that impacts how everyone from researchers and developers to the general public perceives and discusses AI. Consequently, combating AI hype requires a multi-faceted approach involving industry, government, academia, and civil society."}, {"title": "Diversified Funding and Research", "content": "As discussed in Section 4.2, AI hype serves to focus R&D on a limited number of questions, typically those that are priorities for technology companies and investors. One way to address this is by encouraging the diversification of AI funding and research at several levels. Although governmental agencies worldwide are investing in AI R&D, prioritizing"}, {"title": "Regulatory Action", "content": "Changes to the R&D ecosystem are slow, are unlikely to be universally adopted, and are not alone sufficient to tackle the issue of AI hype. While governance and legislation of Al systems is a complex and rapidly evolving field of work, here we provide a few suggestions for external regulation that can reduce the prevalence of unsupported performance claims and increase the reliability of AI discourse.\nMany scholars have suggested standardized third-party audits or testing procedures for evaluating AI system performance, particularly in high-stakes application areas (Tutt 2016; Green 2022). If, and only if, these audits do not presuppose system functionality or accept performance claims offered by the developers as a given (Raji et al. 2022), requiring them could significantly diminish AI hype by providing a reliable check on technology companies' assertions about their products and discouraging them from overstating AI systems' capabilities in the first place.\nMandated transparency is also a useful regulatory tool. While requiring transparency into the training data and model development and demanding justification of the scientific basis of technical capability claims enables external audits and allows stakeholders to draw informed conclusions about an AI system regardless of claims put forward by the developers. Although transparency is a complex topic, there is a rich literature around transparent documentation practices to draw from when developing regulatory frameworks for technology (Mitchell et al. 2018; McMillan-Major et al. 2021; Crisan et al. 2022; Gebru et al. 2018; Kapoor and Narayanan 2022b). While it is unlikely that regulators could mandate the use of such tools for all AI systems, requiring them for public sector AI systems would help alleviate some of the most dangerous instances of AI hype and perhaps encourage their wider adoption.\nTo accelerate the regulatory process, regulators can also rely on existing consumer protection, product liability, and fraud law to address harmful Al hype (Raji et al. 2022). Exercising these existing avenues of redress would not only help avoid the dissemination of exaggerated performance and reliability claims, but would also likely impede the willingness of developers to release under-tested AI systems in the first place."}, {"title": "Technical Literacy and Best Practices", "content": "Finally, as the consumers and propagators of much of the hype around AI, the media and the general public have a critical role to play in addressing these concerns. Society should not have to rely solely on the developers of a technology to describe its utility (Davis 2020) and thus it is essential that reporters and writers covering AI have the technical knowledge to avoid reproducing and spreading"}, {"title": "Conclusion", "content": "We have explained how misrepresentations of AI capabilities, or AI hype, originates in many cases from the AI R&D ecosystem. Hype is further propagated through interactions with the media, funding and regulatory institutions, and the general public, which in turn influence the R&D ecosystem creating a feedback loop of false narratives, overstated performance claims, misplaced research incentives, and substantial negative societal impact. While no component of the AI hype cycle can be divorced from the others, AI researchers and developers bear a substantial responsibility for recognizing and addressing issues inherent to the field that encourage misleading and unscientific depictions of AI."}]}