{"title": "COTODE: COntinuous Trajectory neural Ordinary Differential Equations for modelling event sequences", "authors": ["Ilya Kuleshov", "Galina Boeva", "Vladislav Zhuzhel", "Evgenia Romanenkova", "Evgeni Vorsin", "Alexey Zaytsev"], "abstract": "Observation of the underlying actors that generate event sequences reveals that they often evolve continuously. Most modern methods, however, tend to model such processes through at most piecewise-continuous trajectories. To address this, we adopt a way of viewing events not as standalone phenomena but instead as observations of a Gaussian Process, which in turn governs the actor's dynamics. We propose integrating these obtained dynamics, resulting in a continuous-trajectory modification of the widely successful Neural ODE model. Through Gaussian Process theory, we were able to evaluate the uncertainty in an actor's representation, which arises from not observing them between events. This estimate led us to develop a novel, theoretically backed negative feedback mechanism. Empirical studies indicate that our model with Gaussian process interpolation and negative feedback achieves state-of-the-art performance, with improvements up to 20% AUROC against similar architectures.", "sections": [{"title": "1 INTRODUCTION", "content": "Sequences of events are integral to numerous fields of modern life. Examples of such data include bank transactions [4], patients' medical histories [20], various sales datasets [9], earthquake records [29], and more. One of its most critical characteristics is an uneven structure, which often necessitates complex modifications to the processing algorithm.\nThere are many existing methods for analysing event sequences. Traditional approaches model hidden data dynamics as a probabilistic process with certain constraints, e.g., Poisson [18] or Hawkes processes [16]. More advanced methods propose applying recurrent neural networks [14, 25] or transformers, often in combination with point processes [48]. Alternatively, recent research explores the use of ordinary differential equations (ODE) to process sequential data [10, 19, 21]."}, {"title": "2 RELATED WORKS", "content": "The study of event sequences has a long history, with roots in Temporal Point Processes (TPP) [16, 27]. Recently, this field has undergone significant changes due to the rapid development of Neural Network (NN) approaches. Below, we present a brief overview of relevant papers which inspired our research. The review starts by examining the leading works on this subject. Next, we analyse prior studies that applied Neural ODEs to event sequences. Here, to emphasise our paper's focus, we note the form of the hidden trajectory for each relevant ODE paper. The final part addresses the error estimates for Gaussian process regression, essential to our theoretical results.\nNeural network approaches. There is a plethora of research on applying deep-learning methods directly to event sequences without fine-tuning them to the specifics of this field. They are primarily based on recurrent neural networks (RNNs) [1, 3, 4], although there are a few methods which utilise the transformer architecture [5, 26, 37, 38]. On the other hand, the authors of [43] recognise that irregularity is a crucial feature of event sequences and, as such, should be taken into account. In turn, our method goes one step further, integrating this irregularity into the architecture itself.\nDiscontinuous-trajectory Neural ODEs. In contrast to classic, NN-based methods, we take care of irregularity more naturally. Using ODE theory, our model doesn't simply account for the differences between intervals, but directly evolves over time instead.\nContinuous-trajectory Neural ODEs. The motivation for a continuous trajectory is supported by [21]: the considered underlying processes develop in continuous time, and so should the model. The authors designed an architecture based on the controlled differential equations (CDEs) theory, which imposes certain restrictions.\nGaussian process regression errors. The main part of our theoretical analysis involves the estimation of the squared error of an integral over a linear function of a Gaussian process with multiple outputs. Knowing little about the smoothness of separate output dimensions, we work with a smoothness upper bound, aiming for minimax error estimation."}, {"title": "3 METHODS", "content": "In this section, we lay out the methods we propose in our work, as well as the theoretic background behind them."}, {"title": "3.1 COTODE pipeline", "content": "Our goal is to provide a representation h(t) in response to an input event sequence $\\{c_k, t_k\\}_{k=1}^n$, suitable for solving downstream problems. By $\\{t_k\\}_{k=1}^n \\subset [0;T]$ we denote the timestamps of the input events, while $\\{c_k\\}_{k=1}^n$ denotes the types of these events.\nTo achieve this goal, we use a three-part pipeline, traditional for event sequences [3, 5]. The first layer is the Embedder, responsible for converting categorical features $\\{c_k\\}_k$ into vectors $\\{x_k\\}_k$ using a learnable dictionary. Next, the Backbone runs on these vectors, returning a single embedding per sequence h(T). Finally, a linear head on top of the final hidden state h(T) transforms it into the prediction \u0177. The pseudocode for this approach is provided in Algorithm 1, complemented with an illustration in Figure 3.\nOur backbone of choice is a Neural ODE-based architecture. It requires a function x(t) : [0;T] \u2192 Rd, instead of a sequence $\\{x_k\\}_k$. Here d is the dimensionality of the input embedding. The main contribution of this paper is an approach based on Gaussian processes interpolation that provides this continuous trajectory x(t), for any given t\u2208 [0, T]. It exhibits superior quality, as confirmed by both experimental and theoretical studies.\nThen, the Neural ODE layer uses the Gated Recurrent Unit (GRU) [11] function as its dynamics. Given xt, it produces the trajectory h(t) that concludes with h(T). Negating h here leads to greater numerical stability of the proposed approach, being a crucial part of our methodology."}, {"title": "3.2 Neural ODE overview", "content": "The Neural ODE layer was first proposed by [10]. It propagates the hidden state by solving a Cauchy problem from a fixed starting point. The learnable parameters adjust the dynamics function's behaviour within this problem. At first, the input data was used purely to alter the starting point.\nSubsequent works, such as Neural CDEs [21], proposed to add a dependency on the input data interpolation function x(t) into the dynamics. We also follow this approach. Our method can formally be written as a Cauchy problem Ca(ho, fo, x(t)):\nDynamics: \n$$\\begin{cases}\nh(0) h(0) = h_0; \\\\ \n\\frac{dh(t)}{dt} = f_\\theta(h(t), x(t), t).\n\\end{cases}$$\nSolution: h(t) : [0, T] \u2192 Rd.\nHere, ho is a predefined starting point, f\u03b8 is a neural network, which defines the dynamics for t \u2208 [0, \u03a4]. As mentioned above, since we are working with time series, the input is fed to the dynamics as an interpolation function of the input data x(t), instead of being encoded into ho as is the case for Neural ODE methods on tabular data. As a result of integrating these dynamics, we obtain our target h(T) - the hidden state at the final timestamp T."}, {"title": "3.3 Interpolation of the hidden trajectory", "content": "We note that to continuously model the hidden trajectory, it is necessary to define the influence that events have on it in the gaps between them. The Neural CDE method [21] processes the input data as-is. Since direct usage of categorical features as inputs makes no sense, we first embed all such features into learnable vectors and then apply the proposed interpolation approach.\nBelow, we provide a Gaussian Process-based model for the data interpolation function x(t). Taking advantage of the theory behind this method, we can devise error bounds for the final hidden state.\nPractical aspects of Gaussian Processes. Here, we will discuss the basic theory behind this approach, which we will use in practice.\nLet x(t) be a stationary zero-mean d-dimensional Gaussian process (GP) with the covariance function K(, ) = {Ki(, )} and independent components:\nEx(t) = 0; E [x(t\u2081) x(t2)] = K(t1, t2) = K(t2-t1), \nwhere the symbol is used to denote component-wise multiplication.\nSuppose we also know the values of said process at timestamps $\\{t_1,..., t_n\\} \\subset [0, T]$, and we wish to build a linear prediction x(t):\nx(ti) = xi, i = 1, ...,n \u21d2 x(t) = $\\sum_{i=1}^n a_i x_i$.\nThe choice of the coefficients a = $\\{a_i\\}_{i=1}^n$ is guided by the minimization of the mean squared error \u03c3\u00b2:\n\u03c3\u00b2 (t) E||x(t) \u2212 x(t)||\u00b2.\nIn our experiments, when calculating x(t), we use a well-known result [7], which corresponds to the maximum a posteriori estimation:\na = K\u00af\u00b9k,\nwhere K is the covariance matrix, k is the covariance vector, and K(t) is a standard squared exponential covariance function, identical over all dimensions (a is a hyperparameter, we refer to as scale):\nK(t) = (e-at\u00b2,..., e-at\u00b2).\nWe set a = 1 in most cases; further details on tuning it may be found in Appendix C.\nGaussian Process error estimation.\nPlan. We do not observe the client in-between events, which means that x is only our most probable estimate. GP interpolation is a well-studied, theory-backed method, which allows us to quantify the uncertainty of estimating y. This uncertainty is then accumulated in the hidden state due to integration, and our aim in this section is to assess the impact this has on h(T). Our main result may be informally written as follows:\nTHEOREM. (informal) The estimation error of the final hidden state h(T) follows the asymptotic:\nE||h(T) \u2013 h*(T) ||\u00b2 \u2264 (n \u2212 1)dmax,\nwhere min, max are the length of the minimum and maximum intervals between events, and n - 1 is the total number of these intervals, h* (t) is the trajectory that corresponds to the true x.\nThis would imply that without additional regularization, the error grows approximately linearly with time as we observe more events. Below, we provide the formal statements with their proofs.\nError definition. First, let us define the total integral error:\n\u03c3\u00b5ALL = $\\int_0^T E||x(t) - x(t)||^2dt$."}, {"title": "Uniform theoretic bounds", "content": "Uniform theoretic bounds. The form of the estimator given by (5) is useless when it comes to establishing the bounds for (7): it is tough to deal with the inverse form of K. This forces us to approach the problem from a different angle, following existing results on estimating the GP regression error [45]. To use these prior results, we have to consider a more convenient intermediate form of the error, integrated between two events:\n\u03c3\u00b2INT(k) = $\\int_{t_k}^{t_{k+1}} E||x(t) - x(t)||^2dt$.\nAssumption and their effect on the error. We first devise error bounds for a uniform sequence and generalize them later to our irregular case. Formally, we make two common assumptions, which, as will be explained below, do not reduce the applicability of the devised results:\nASSUMPTION 1. The data lies on a regular grid with a step size d > 0: tk = to + kd, k = 1, ..., n.\nASSUMPTION 2. The input sequence is infinite:\nk = \u221e, ..., -2, -1, 0, 1, 2, ..., +\u221e.\nAssumption 1 can be made without loss of generality. The error for a uniform grid can be extended to the case of irregular event sequences using the following lemma.\nLEMMA 3.1. Let S = $\\{(x_k, t_k)\\}_{x=1}^k$ be a given irregular event sequence. We construct an alternative event sequence S = $\\{(x_k, t_k)\\}_{x=1}^{k'}$ with the interval [m; m + 1] changed by r, for some fixed 1 < m <k andr \u2208 R:\nt'k = tk, k \u2264 m;\nt'k = tk + r, k > m;\nt' = t + I[t > tm+1]r.\nLEMMA 3.2. (proof in Appendix A) Let F(\u03c9) denote the spectral density of Gaussian process from (2):\nF(\u03c9) = $\\int_{-\\infty}^{\\infty} exp(2\u03c0i\u03c9t)K(t)dt$.\nIn this notation, under Assumptions 1,2 the following holds:\nE||x(t) x(t) ||\u00b2 = d$\\int_{-\\infty}^{\\infty} F(\u03c9) [1 \u2013 \\sum_{k\u22600} e^{i2\u03c0\u03c9 (t-t_k)} K(t \u2212 t_k)] dw$.\nWith Lemma 3.2, all other results can be taken directly from [45], taking into account that x(t) is d-dimensional. We recount select findings from this paper: the general error form, an analytic error expression for a specific kernel analogous to (6), and the minimax error bound.\nTHEOREM 3.3. The error (8) may be written in the following form:\n\u03c3\u00b2INT = ad$\\int_{-\\infty}^{\\infty} F(\u03c9) [(1 - K(\u03c9))^2 + \\sum_{k=-\\infty}^{+\\infty} K^2(\u03c9)] dw$.\nNote that the error (11) does not depend on the interval number. This is due to the inherent symmetry of an infinite uniform grid: all intervals are the same in terms of the covariance function.\nASSUMPTION 3. The covariance function is the same over all dimensions and corresponds to the squared exponential function:\nK(t) = \u221a2\u03c0e-2(\u03b1\u03c0t)\u00b2.\nCOROLLARY 3.4. Under the Assumptions 1-3, the error (11) can be bounded analytically:\n|\u03c3\u00b2INT| \u2264 d.\nASSUMPTION 4. All the spectral densities of K belong to the following class of functions:\nF(L) = $\\{F: E(\\frac{d x(t)}{d t})^2 < L\\}$.\nThen, we can generalize results from [45] for the multivariate case.\nTHEOREM 3.5. In the above notation, under the Assumptions 1,2,4, the minimax GP interpolation error may be calculated analytically:\nRh(L) = inf sup \u03c3INT = $\\frac{L \u03b4^2}{2\u03c0^2} \u2248 d.(\u03b4^2)$.\nFinally, combining Lemma 3.1 with Corollary 3.4 or Theorem 3.5 (depending on assumptions about K) and transitioning to the finite irregular case with n - 1 intervals (with min for Dmin and max for Dmax), we achieve the following inequalities for the upper bound of the total error:"}, {"title": "Neural ODE Error", "content": "Neural ODE Error. Now, we can apply the above findings to our task. Let us consider two variants of the Cauchy problem (1): one with a \"true\" signal x(t), and the other with a signal, reconstructed from observations via GP interpolation x(t). We will denote their hidden trajectories as h* (t), h(t) respectively. Then, the difference between the resulting hidden states, accumulated during integration, is written as:\n\u03c3\u00b2i = E$\\|\\frac{dh^*}{dt} - \\frac{dh}{dt}\\|^2dt$ = E$\\int_0^T ||f_\u03b8(x^*,h^*,t) - f_\u03b8(x, h, t)||^2dt$.\nTo bound this error, it is necessary to restrict the class of functions f\u03b8.\nASSUMPTION 5. Let the functions fo be confined to linear bi-Lipshitz functions of x with parameters being a matrix Wx:\nf(x) = Wxx; 0 < Amin (Wx) < Amax (Wx),\nTHEOREM 3.6. (proof in Appendix A) Consider the task (1) under Assumption 5. Then, the following holds:\n(1) If the covariance function has the form from Assumption 3, the interpolation error from (16) adheres to the following bound:\n(n \u2212 1)\u03bbmin(Wx)Dmin \u2264 \u03c3 \u2264 (n \u2212 1)\u03bbmax(Wx)Dmax.\nUsing this approach, we are able to quantify the uncertainty from not knowing what happens in-between events, accumulated in the final hidden state."}, {"title": "3.4 GRU Negative feedback", "content": "The error bound from Theorem 3.6 leads to disappointing conclusions: the error grows linearly with sequence length. This corresponds to the exploding gradients problem, well-studied for discrete sequential models [17], and we propose solving it in the continuous case with a negative feedback procedure.\nTheoretical consideration. First, we will discuss the theoretical benefits of this procedure. Consider the following scenario: two trajectories h(t), h* (t) have the same dynamics, but distinct starting states ho \u2260 h*o at timestamp t = 0. This is meant to imitate the difference between the true and the estimated trajectory: the estimated trajectory accumulated errors during its evolution until t = 0 but adhered to the correct dynamics afterwards. Formally, this is described by a simple ODE on the interval [0, T]:\n$\\frac{dh}{dt} = W_hh; = W_hh^*,$$\\frac{dh^*}{dt}$\nwhere Wh is the parameter matrix. If Wh is negative-definite, the difference between those states, according to the matrix ODE theory, will decrease exponentially.\nLEMMA 3.7. (proof in Appendix A) Let ho \u2260 h*o be two non-equal starting points for two Cauchy problems Ca(ho, Whh) and Ca(h*o, Whh*). For the negative-definite matrix Wh < 0 with eigenvalues \u03bbi < 0 for some constants Ci the following holds:\n||h(T) \u2013 h*(T)|| \u2264 $\\sum_i C_ie^{-|\u03bb_i|T}$.\nNegative Feedback in practice. The aforementioned problem of the linearly growing error arises from the additive nature of differential equations. The empirical solvers we use share this nature, and as such, this issue also manifests itself in our study's experimental results.\nAs a consequence of the above theory, it is beneficial to enforce the weight matrix, which corresponds to h, to be negative-definite. In practice, prior works have solved this by forcing the derivative to point toward decreasing the hidden state. For example, the authors of [12] simply subtract the hidden state from the derivative. Indeed, for a matrix of limited spectral norm (as is often the case due to L2 regularization), this results in the following modification:\nWh = Wh - I \u21d2 max(Wh) = max(Wh) \u2212 1 \u2264 0.\nWe developed a more holistic solution inspired by the GRU architecture [11]:\nr = \u03c3(Wirx + br + Whrh + bhr),\nz = \u03c3(Wizx + biz + Whzh + bhz),\nn = tanh(Winx + bin + r (Wnhh + bnh)),\nh = (1 \u2212z) \u2299 n + z \u2299 h.\nWe noticed that by negating the second term in (23), since z is positive, one can achieve, in essence, a learnable negative feedback effect."}, {"title": "4 EXPERIMENTS", "content": "In this section, we provide our empirical results, demonstrating the superiority of the proposed models in practice. To this end, we first outline the experiment settings (our datasets and the models we compare), followed by the results and the corresponding discussion."}, {"title": "4.1 Data", "content": "The datasets in question have one label per sequence, summarising it in one way or another. Four out of six of our datasets are transactional, mostly because they are abundant, similar in format and thus easy to work with. A transaction consists of a continuous amount feature, a categorical MCC-code feature (transaction type, e.g., ATM deposit/withdrawal, payment at retail clothing, etc.), and a timestamp. All transactions are grouped by bank clients. Consequently, these datasets differ only in their target labels."}, {"title": "4.2 Models", "content": "In our empirical study, we compared the performance of six models. The considered models are all, in essence, some modification of the GRU architecture [11], including the raw GRU model itself. We have two reasons for focusing on recurrent architectures instead of transformers: there is a lack of research on their continuous implementations and they are shown to be inferior in empirical comparative studies [5, 47]."}, {"title": "4.3 Results", "content": "The results for the transactional datasets are presented in Table 1. Our method demonstrates exceptional performance, surpassing other models on three out of four datasets. On the Gender dataset, our models achieve a close second, only being outperformed by the RNN method. We hypothesise that this is because the sequences are too short for the ODE methods to accumulate meaningful information.\nBesides transactional datasets, to illustrate the wide applicability of these methods, we also provide the results on two non-transactional datasets. These results are given by Table 2.\nThe Human Activity Classification task, despite not being a true event-sequence scenario, also prefers our proposed methods. The proposed approach outperforms the standard RNN method, even with regularly spaced observations. We hypothesise that this is due to a larger virtual depth of the Neural ODEs: they have greater control over the hidden trajectory and, as such, are able to model dynamics of higher complexity.\nOn the other hand, the Retail dataset favours the vanilla model. Again, as was the case with Gender, this is most likely due to the short length of the input sequences.\nBesides the metrics, we also illustrated the resulting paths for our GP-based Neural ODE in figure 2. The hidden trajectory was sampled at 100 uniform points and cast to two dimensions using PCA. The generated curve has complex dynamics and does not collapse into trivial cases. This indicates that the resulting path is truly informative of the actor's state."}, {"title": "4.4 Ablation Study", "content": "To study the effect of the GRU-based negative feedback, we evaluated the embeddings' norms across the whole trajectory, with and without the proposed modification (negation of h). These experiments were done on the Age dataset.\nBesides, we also provide the readers with the resulting ROC-AUC metrics for these model variants in the lower right corner of the graphs in Figure 4. The metrics clearly favour the version which uses the proposed negative feedback modifications, with a significant 20% gap (0.76 vs 0.63).\nAs mentioned above, we also considered two other interpolation regimes, specifically interpolation by the last event (COTODE last) and interpolation by exponential decay of prior events (COTODE Decay). The results for these models are given alongside our main experimental study in Tables 1,2. These models often performed on-par with the Gaussian Process-based approach. However, they show significant drops in quality on some datasets, while outperforming GP on others."}, {"title": "5 CONCLUSIONS", "content": "In this article, we present a fresh perspective on the task of analysing event sequences. By considering the influence of events in the intervals between them, we gain the ability to work with these sequences in a manner akin to continuous time series. Inspired by this insight, we propose a corresponding modification to the Neural ODE model: we interpolate the input data sequence via Gaussian Processes, achieving an uninterrupted influence on the dynamics.\nThis method outperforms all previously considered methods on sufficiently long sequences by up to 20% ROC-AUC.\nMoreover, our primary theoretical contribution lies in defining error bounds for the proposed modification, establishing the uncertainty arising from clients' unknown actions between events. These obtained estimates allow us to reveal a fundamental flaw within these methods: the error increases linearly over time. We demonstrate that this issue can be theoretically addressed using negative feedback. To this end, we also develop an elegant zero-cost solution, which represents a sufficiently novel and modern challenge in this field and limits the growth of error in practice."}, {"title": "A PROOFS", "content": "Let us start with expanding the square:\nE||x(t) - x(t) ||\u00b2 = E[x(t)x(t)] \u2013 2E[xT (t)x(t)] + E[xT (t)x(t)].\nE[x(t)x(t)] = h\u00b2d$\\sum_{k,l=-\\infty}^{\\infty} K(t - tk)K(t - tl)K(tk - t) = $\n=$\\int_{-\\infty}^{\\infty} F(\u03c9) h\u00b2d$\\sum_{k=-\\infty}^{\\infty} K(t \u2212 tk)K(t \u2212 tl)e^{2\u03c0i\u03c9(tl-tk)} | dw$.\nFactoring out the spectral density integral, the remaining three terms are the expansion of the following modulus:\n|1+h$\\sum_{k=-\\infty}^{\\infty}K(t-tk)e^{2\u03c0i\u03c9(t-tk)}|^2$"}, {"title": "Consider the error", "content": "E$\\int_0^T ||W_xx(t) - W_xx(t) ||\u00b2 dt.\nApplying prior results, we achieve:\n\u2264 $\\int_0^T |W_x||(x(t) \u2013 x(t)) ||\u00b2dt$\n\n(15)\n\u2264 \u03bb\u00b2w(n - 1)Dmax\nThe last two inequalities are obtained using the f function restrictions (17) and the bound on the total x error (15). The remaining bounds can be devised similarly.\nFinally, the upper bound on the difference between the final hidden states follows from bounding the integral norm by the integral of the norm:\n||h*(T) \u2013 h(T) ||\u00b2: = $\\|\\int_0^T \\frac{dh^*}{dt} - \\frac{dh(T)}{dt}dt\\|^2$"}, {"title": "Consider the following Cauchy problem", "content": "Consider the following Cauchy problem, which describes the difference A = h(t) \u2013 h*(t) between the given Cauchy problems:\nA(0) = Ao = ho - h*o;\n$\\frac{dA}{dt} = W_hA$;\nSolution: \u2206(t) : [0, T] \u2192 Rd.\n\u2206(t) = $\\sum_i C_iv_ie^{\u03bb_it}$,\nfor some constants Ci, which are currently none of our concern."}, {"title": "B DATA PREPROCESSING", "content": "Data preprocessing is an important aspect in addition to the main part. Below are two datasets with specific features.\nRetail. We preprocessed the product descriptions via TF-IDF and extracted top-125 components using a truncated SVD decomposition. Combining them with quantity, price, and time since the last purchase leaves us with 128 features. We posed the task of classifying of the customer's countries of origin."}, {"title": "C KERNEL SCALE ABLATION", "content": "Apart from varying interpolation regimes and negative feedback, we also tried changing the kernel scale y:\nK(\u03c4) = e\u00af\u03b3\u00b2\u03c4\u00b2."}]}