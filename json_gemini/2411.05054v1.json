{"title": "FMEA Builder: Expert Guided Text Generation for Equipment Maintenance *", "authors": ["Karol Lynch", "Fabio Lorenzi", "John Sheehan", "Duygu Kabakci-Zorlu", "Bradley Eck"], "abstract": "Foundation models show great promise for generative tasks in many domains. Here we discuss the use of foundation models to generate structured documents related to critical assets. A Failure Mode and Effects Analysis (FMEA) captures the composition of an asset or piece of equipment, the ways it may fail and the consequences thereof. Our system uses large language models to enable fast and expert supervised generation of new FMEA documents. Empirical analysis shows that foundation models can correctly generate over half of an FMEA's key content. Results from polling audiences of reliability professionals show a positive outlook on using generative AI to create these documents for critical assets.", "sections": [{"title": "Introduction", "content": "We propose an AI system for the generation of structured documents related to industrial equipment with particular focus on Failure Mode and Effects Analysis (FMEA). FMEAs are a longstanding tool of reliability engineering for understanding equipment failure points and optimal maintenance strategies [Rausand and H\u00f8yland, 2004; Sharma and Srivastava, 2018]. These documents capture reasons why equipment, assets and infrastructures fail and outline maintenance options for such failures to achieve the desired level of reliability.\nAlthough FMEAs content can differ by sector, our approach considers a document with the following sections:\n\u2022 The boundary gives a functional description along with the main components.\n\u2022 Failure locations are points on the equipment where a failure might occur.\n\u2022 Degradation mechanisms describe the physical process or mechanism that can lead to a failure.\n\u2022 Degradation influences describe the underlying causes of a degradation.\n\u2022 Preventative maintenance tasks can be carried out to prevent failures; and,\n\u2022 Job plans collect such tasks into a schedule.\nThese parts of the document show a nested behaviour wherein a typical piece of equipment has multiple failure locations with each being linked with one or more degradation mechanisms and so on. Preventative activities depend on the failure location, mechanism, and influence. Job plans schedule preventative activities according to operating conditions, while usually grouping related preventative steps together.\nSuch an approach is regarded as being effective to managing critical infrastructure in many sectors including Energy and Utilities, Water and Wastewater management, and Oil and Gas [Carvalho et al., 2022] where the effect of unforseen, unplanned or disastrous failures without solid recovery strategies has severe impacts on the system, business and potentially society as a whole.\nCreating an FMEA requires a group of highly trained experts focusing on a single study. Such resources might be too expensive or unavailable to some organisations involved in critical infrastructure management.\nGenerating FMEAs is challenging because the sequential relationship of the document's sections can propagate errors and because FMEAs contain domain specific knowledge about the equipment and how it is used. In addition, the same words can refer to different equipment components, with the correct interpretation depending on the usage. For example the \"casing\" of a pump is different than that of a window. This behavior makes it difficult to create a navigable catalog of components from which to build FMEAs. However, the attention mechanism [Vaswani et al., 2017] used in today's language models can interpret the meaning of words based on their context.\nIn this discussion we explore how large language models (LLMs) [Bubeck et al., 2023] can assist in the creation of FMEAs. Our system for generating FMEAs draws on recent techniques for using LLMs and contributes a case study for using LLMs on domain-specific problems. Key techniques informing our work include answer consistency [Wang et al., 2023], in-context learning [Brown et al., 2020], and dynamic relevant example selection [Liu et al., 2022; Nori et al., 2023]. Recent studies applying LLMs to particular domains include the work of Nori et al. [Nori et al., 2023] for"}, {"title": "2 Solution Approach", "content": "Our system for creating new FMEAs decomposes the generation problem according to the structure of the documents we generate. This decomposition enables our target users, subject matter experts, to inject their knowledge and supervise the generation process. We use a library of existing documents to furnish the model with relevant information, and generate structured outputs for consumption by our graphical interface. Taken together, these methods should enable experts to create new FMEAs of high quality in a short time.\nThe workflow for the first step, generating the equipment boundary from a short description, is a representative example for generating part of an FMEA (Fig. 1). The prompt construction and response parsing steps are further described below.\nDynamic Few Shot Prompting (DFSP) We rank and retrieve relevant examples, or shots, by cosine similarity of text embeddings. This approach, termed Dynamic Few Shot Prompting, adds user supervision to the example selection methods of [Liu et al., 2022; Nori et al., 2023]. DFSP combines three sources of knowledge: examples already in the database, an expert's knowledge, and the knowledge in the LLM. Running the prompts without injected examples provides a zero-shot method that draws only on the LLM.\nStructured responses Parsing the LLM's response from pure text into structured responses enables presentation of FMEA components directly to the user for subsequent supervision. For example the user can confirm, reject or supplement the generated list of failure locations. Structured responses also simplify the resolution of repeated entities, a common problem in text generation [Holtzman et al., 2020]. Our system generates structured responses by lightly formatting the injected examples with simple delimiters. A rule-based parser operates over the delimiters to produce a response in javascript object notation."}, {"title": "3 Evaluation", "content": "For the work in progress reported here we focus on the first two parts of an FMEA document: the equipment boundary and, the failure locations. Experiments used our propriety database of 714 FMEAs developed by subject matter experts and a range of state of the art LLMs: llama-2-70b-chat (70B) [Touvron et al., 2023], flan-ul2 (20B) [Tay et al., 2023], and quantizated versions of Mixtral-8x7B (8x7B) [Jiang et al., 2024], denoted by Mixtral-Q.\nFor evaluation purposes we divided the database into train (n=571; 80%), validation (n=71; 10%) and test (n=72; 10%) splits. Examples for our DFSP method are drawn from the training split. We compare our DFSP method with prompts using a randomly selected example (random-shot) and no example (zero shot). The random shot uses a random example from the database to provide a syntactic rather than semantic hint to the model.\nAlthough it is not possible to share data for reproducing our experiments we suggest that results reported here can provide valuable insights for the domain."}, {"title": "3.1 Generating Equipment Boundaries", "content": "An equipment boundary of an industrial asset describes the asset and its constituent components. The input is a few-word description of the equipment and the output is the boundary itself.\nWe evaluated the performance of several models for generating equipment boundaries in terms of the ROUGE-1 [Lin, 2004] score for the unstructured response, and recall and precision for the structured list of components. ROUGE-1 is a recall oriented similarity score for comparing candidate and reference texts with values ranging from zero to one.\nResults (Table 1) for individual models showed a clear pattern of increasing quality as we move from zero-shot to random-shot and DFSP, with flan-ul2 the best performing individual model. Results for DFSP show a strong uplift for ROUGE-1 and for component lists. Although the described methods automatically generate much of the required information, some information, in particular on the component lists, is missed. This result emphasizes the important role of reliability engineers to supervise the generated descriptions."}, {"title": "3.2 Generating Failure Locations", "content": "The set of failure locations for an industrial asset is a key part of an FMEA indicating the components of an asset likely to fail. The input to this step is the equipment boundary. This evaluation uses boundaries from the database to generate failure locations. Results showed a similar pattern as equipment boundaries; quality increases from zero-shot to random-shot to DFSP (Table 2). Results for individual models were more mixed with llama2 showing the highest recall and F1 while mixtral-Q had the highest precision."}, {"title": "3.3 User Feedback", "content": "Our system enables user interaction with structured model responses through a tailored graphical interface. As an example, our interface for the first step in the pipeline appears in Figure 2.\nWe showed the user interface for this work in progress system to two audiences of people responsible for the maintenance and reliability of critical infrastructure across industries. For these professionals, creation and application of FMEAs form a crucial part of their daily work. Following the demonstrations, we polled the audience for feedback on two key questions.\nQuestion 1: How likely would you be to use a tool like the demo during FMEA creation if it was available? Answer options: Extremely likely; Likely; Undecided; Unlikely; Extremely unlikely.\nQuestion 2: How much configurability would you like to have when using the tool during FMEA creation? Answer options: Build FMEAs fully automated by AI; Build FMEAs mostly automated by AI; Build FMEAs acting as my helpful but supervised assistant; Build FMEAs mostly manually; Build FMEAs fully manually.\nTable 3 reports responses favorable to using the tool and to having the support of AI in general. In both audiences, responses were positive. We interpret higher scores for AI in general than for our tool in particular as an opportunity to improve the design of the interface and to familiarize users with the capabilities and limitations of AI."}, {"title": "4 Outlook", "content": "In this work we have shared a view of current work in progress for generating documents related to critical equipment. The method of dynamically retrieving examples from a database for including in a prompt shows the ability to correctly generate over half of the content needed for an FMEA. Although this level of performance is considered helpful according to our surveys, we intend to explore further improvements. In particular, ensemble methods based on fuzzy voting provide a promising tool for combining results between models and shot orderings. We expect ensembles to improve the recall of structured responses at a hopefully small cost in precision.\nSo far our experiments have focused on the test split from the database but there are many equipment types not covered by the database where FMEA generation remains of interest. In these cases, knowledge is often available in the less structured form of manuals and process documents. With pre-processing / chunking this information can also be used to generate parts of an FMEA. Ultimately we foresee the use of ensemble methods to combine results between LLM responses informed by examples from the database as well as user-provided manuals and documents.\nFinally further feedback sessions remain to be conducted during the development of this project to evaluate the per-"}]}