{"title": "Effects of Prompt Length on Domain-specific Tasks for Large Language Models", "authors": ["Qibang Liu", "Wenzhe Wang", "Jeffrey Willard"], "abstract": "In recent years, Large Language Models have garnered significant attention for their strong performance in various natural language tasks, such as machine translation and question-answering. These models demonstrate an impressive ability to generalize across diverse tasks. However, their effectiveness in tackling domain-specific tasks, such as financial sentiment analysis and monetary policy understanding, remains a topic of debate, as these tasks often require specialized knowledge and precise reasoning. To address such challenges, researchers design various prompts to unlock the models' abilities. By carefully crafting input prompts, researchers can guide these models to produce more accurate responses. Consequently, prompt engineering has become a key focus of study. Despite the advancements in both models and prompt engineering, the relationship between the two\u2014specifically, how prompt design impacts models' ability to perform domain-specific tasks\u2014remains underexplored. This paper aims to bridge this research gap.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have become an integral part of our daily lives, transforming the way we interact with technology. From virtual assistants like Siri and Alexa to language translation apps like Google Translate, LLMs are being used in various applications to make our lives easier. For instance, chatbots powered by LLMs are being used by businesses to provide customer support and answer frequently asked questions. Additionally, LLMs are being used in writing assistants like Grammarly to help users improve their writing skills. Furthermore, LLMs are also being used in language learning apps like Duolingo to help users learn new languages.\nPrompts play a crucial role in unlocking the potential of LLMs. A well-crafted prompt can guide the LLM to generate accurate, relevant, and coherent responses. For instance, when asking an LLM to write a story, providing a prompt with specific details such as genre, setting, and characters can help the model generate a more engaging and focused story. However, despite of the well-known advantages, LLMs' performance relies heavily on the quality and specificity of the prompts. Poorly constructed prompts can lead to inaccurate outputs because LLMs lack intrinsic knowledge of research. For example, LLMs may output oversimplified or non-viable solutions due to not inherently understanding the research mechanics. This limitation emphasizes the domain expertise in crafting prompts and interpreting results.\nThrough diversifying prompt instructions, researchers pioneeringly address the LLMs' performance across specific domains (e.g., economics, health, and technical systems), which empowers nations to strengthen economic resilience, protect public health, and maintain technological leadership on a global scale. By systematically examining domain-specific impacts, their works highlight the transformative potential of prompt engineering in identifying LLM capabilities for specialized applications. For example, LLMs help with policy-making by understanding financial sentiment [1]. In public health, LLMs revolutionize disease detection, enabling personalized treatments and providing health support [2]. vestigation. While various aspects of prompts have been extensively researched, such as prompt design, and their content, one crucial aspect remains unexplored: the length of prompts. Despite the growing importance of LLMs, the optimal length of prompts that can effectively elicit desired responses from these models is still unknown. Previous research has focused on optimizing prompt content, structure, and style to improve model performance, but the impact of prompt length on model behavior and response quality has not been systematically investigated. As a result, we aim to bridge this gap in the understanding of how prompt length affects LLM performance in domain-specific tasks.\nOur main contributions are as follows.\n\u2022 We conduct extensive experiments with LLMs on nine domain-specific tasks, and the results show that although they claim to be state-of-the-art NLP models at the time of their releases, with the default prompt length, they still struggle to tackle tasks without sufficient domain knowledge.\n\u2022 Our results with different prompt lengths show that long prompts, providing more background knowledge about the domain that the task falls into, are generally beneficial.\n\u2022 Our results also show that even with background knowledge in long prompts, LLMs' performance still lags behind humans, as the average F1-score is much less than 1.0."}, {"title": "II. BACKGROUND", "content": ""}, {"title": "A. Deep Learning and Large Language Model", "content": "Since the introduction of AlexNet [3], we have entered the era of deep learning, marking significant advancements across various fields, such as asphalt pavement analysis [4], [5], scene text detection [6], [7], visual question answering [8], [9], scene text recognition [10], [11], [12], character recognition [13], and adversarial attach [14], [15]. In addition to the application layer, theoretical investigation has attracted substantial attention, such as Q Learning [16]. In NLP, the progress in language model pretraining began with Word2Vec [17], [18] and has accelerated rapidly. Among these developments, the Transformer architecture [19] is the backbone of all modern language models.\nTransformer's two core components, the encoder and decoder, have been the foundation for much of the work in model pretraining. For encoder-based pretraining, BERT [20] is the most representative example, inspiring numerous variants to improve the pretraining paradigm (e.g., RoBERTa [21], DistillBERT [22], and ALBERT [23] to address domain-specific tasks (e.g., FinBERT [24] for finance, SciBERT [25] for scientific texts, and ClinicalBERT [26] for medical data). Simultaneously, another branch of pretraining work focuses on the decoder of the Transformer. The most notable examples include the GPT series (e.g., GPT-1 [27], GPT-2 [28], GPT-3 [29]) and the LLaMA series (e.g., LLaMA-1 [30], LLaMA-2 [31], LLaMA-3 [32], and its incremental updates like LLaMA 3.1, 3.2, and 3.31). These models discard the encoder and rely solely on the decoder as their backbone architecture. Following the rise of ChatGPT2, which fueled widespread interest in LLMs, researchers have increasingly focused on evaluating the LLaMA series on various domain-specific tasks, such as financial sentiment analysis [1] and emotion identification [33].\nHowever, no prior work has investigated how prompt length impacts the LLM performance on domain-specific tasks. This is important because earlier studies have shown that prompts are crucial in eliciting LLMs' abilities for effective language understanding and task performance [34], [35], [36]. This paper aims to provide insights to bridge this gap."}, {"title": "B. Prompt Engineering", "content": "Most existing work on prompt engineering focuses on enhancing reasoning and logical capabilities in language models. Specifically, Chain-of-Thought (CoT) prompting [37], [38], which systematically investigates step-by-step reasoning, is considered a pioneering approach in this domain. Building on CoT, subsequent advancements have been proposed: Self-consistency [39] introduces mechanisms to improve reliability in reasoning paths; Logical CoT prompting [40] refines logical reasoning within prompts; Chain-of-Symbols (CoS) prompting [41] explores symbolic representations to enhance task-solving processes; Tree-of-Thoughts prompting [42] introduces hierarchical reasoning structures; Graph-of-Thoughts (GoT) prompting [43] leverages interconnected reasoning nodes to expand context understanding; Thread-of-Thought (ThouT) prompting [44] emphasizes linear and contextual reasoning threads; and Chain-of-Tables prompting [45] adapts tabular representations for specific reasoning tasks.\nAlthough these innovations advance the capabilities of language models in handling complex reasoning and logic-intensive applications, there is not a comprehensive study of how the length of prompt affects LLMs' capability in domain-specific tasks."}, {"title": "III. EXPERIMENTS", "content": "To assess the impact of prompt length on LLMs' performance across domain-specific tasks, we conducted a series of comprehensive experiments under varying prompt length settings. Specifically, we conducted nine groups of experiments: Monetary Policy Understanding, User Intent, Conversation Domain, Query Intent Classification, Sarcasm Detection, Emotion Identification, Financial Sentiment Analysis, Technical System Behavior, and Disease Detection. We use their acronyms throughout this experimental section and the subsequent results section for clarity and improved readability. Table I maps the task acronyms and their full names."}, {"title": "A. Domain-specific Tasks", "content": "MPU aims to classify monetary policy statements as hawkish, dovish, or neutral, where hawkish signals tightening measures like higher interest rates, dovish suggests easing policies to stimulate the economy, and neutral reflects no significant stance. UI classifies the users' intent based on what they say. CD categorizes conversations into specific domains or topics, such as healthcare, technology, or finance, for more accurate contextual analysis. QIC determines the underlying intent behind user queries, such as whether the query seeks information, navigational guidance, or specific documents, enabling more efficient document retrieval systems. SD focuses on the sarcasm that may potentially exist in people's words. EI detects specific emotions\u2014anger, joy, sadness, surprise, fear, or love-expressed in a given sentence to better understand emotional tone and context. FSA aims to evaluate financial texts to determine whether the sentiment conveyed is positive, negative, or neutral, helping assess market outlooks or economic conditions. SD focuses on identifying whether a statement contains sarcasm, which is crucial for understanding the true intent of a speaker or writer. TSB studies the technical performances, such as end-to-end delay and throughput, which are significant to domain services such as online video and autonomous driving. Finally, DD aims to extract abnormal findings from radiology reports corresponding to ICD-10 codes."}, {"title": "B. Experiment Design", "content": "In our experiment, each group consists of three settings: default prompt length\u00b3, short instruction, and long instruction. The default prompts that we experimented with are the prompts provided in the original papers. We define short instructions as those containing less than 50% tokens of the default prompt, typically only describing the task name. In contrast, long instructions contain at least 200% tokens of the default prompt, providing not only requirements but also background knowledge and experimental conditions that can help in answering questions. For TSB, while the prompts are not explicitly provided in the original paper, the default instructions cover all the conditions that were introduced in the referenced work, such as hardware model, software version, and experimental environments. The rationale behind choosing these base works is their popularity in their domains. To the best of our knowledge, these works are the most recent works leveraging LLMs in their domains. This design allows us to investigate how different prompt lengths impact LLMs\u2019 performance and ability to leverage contextual information.\nTo evaluate our experiment results, we define a correct case for each scenario. For classification problems, such as MPU, FSA, and SD, a case is considered correct if LLMs produce the same decision as the ground truth. Regarding the engineered system where the prediction may cover a range, we divide the possible range (e.g., [0, 100]) into ten equal segments, offering a structured framework to evaluate the results compared to the ground truth. We treat segments fairly without considering them reliable or reasonable, as our goal is to ensure an unbiased performance assessment [51].\nWe use the weighted average precision, recall, and F1-score for each experiment as the evaluation metrics, where the weight is determined by the number of instances for each class. Precision measures how many positive predictions are correct (True Positive, TP for short. False Positive, FP for short.). Recall is a measure of how many of the positive cases the classifier correctly predicted over all the positive cases in the test data (False Negative, FN for short). F1 score is a measure combining both precision and recall."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "Based on the results presented in Table II, we observe distinct trends in how different instruction strategies impact the performance of precision, recall, and F1-score across domain-specific tasks.\u2074\nResults of default prompt length. The baseline performance metrics vary significantly across tasks. The domains of SD, FSA, and TSB exhibit strong baseline performance, with F1-scores around 0.67. The domain of QIC shows high F1-score (0.84) but low recall (0.10), indicating potential issues with sensitivity to positive cases, while the domain of DD is excellent with precision, recall, and F1-score above 0.80. The domains of EI and CD exhibit relatively lower baseline performance, with F1-scores of 0.48 and 0.56, respectively. Results of short prompts. Short instructions negatively affect performances in all tasks compared to baseline performance. In QIC and DD domains, performances significantly decrease, with precision dropping by 0.06 and 0.08, recall by 0.02 and 0.12, and F1-score by 0.04 and 0.09, while CD maintains slight drops with precision, recall, and F1-score all by 0.01. These results suggest that short instructions can not fully leverage LLMs' capabilities, particularly in sufficient details-needed tasks where contextual information (QIC) or special fields (DD) is essential.\nResults of long prompts. It is worth noting that long instructions generally improve the performance metrics across all tasks on all experimented domains compared to base performance. Specifically, in EI and DD, minimal improvements are observed, with precision and F1-score increasing by +0.01 each, while QIC and TSB show the biggest improved performances due to their detail-sensitive tasks. Moreover, even with the detailed background knowledge provided in the long prompt, LLMs still struggle in these tasks, as their F1 scores are far behind 1.0 (which represents human-level understanding ability)."}, {"title": "V. CONCLUSIONS", "content": "In this study, we conducted comprehensive experiments to assess the effect of prompt length on LLMs' performance in various domain-specific tasks. Our findings indicate that longer prompts generally enhance model performance, while shorter prompts can be detrimental. Furthermore, despite providing extensive background knowledge of the prompts, LLMs still struggle with challenging domain-specific tasks, highlighting the need for a deeper understanding of prompt phrasing.\nOur research agenda will be centered on exploring how different prompting techniques influence LLMs' performance in domain-specific tasks. Recent studies have shown that modifying the phrasing of questions in prompts and adjusting the number of examples provided in instructions can significantly improve performance in specialized domains, including spatial information extraction [35], math reasoning [37], [43], short interest understanding [36], and corporate event prediction [34]. These findings highlight the potential benefits of optimizing prompting techniques to enhance LLMs' performance in specific tasks."}]}