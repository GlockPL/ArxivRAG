{"title": "Measuring Bullshit in the Language Games played by ChatGPT", "authors": ["Alessandro Trevisan", "Harry Giddens", "Sarah Dillon", "Alan F. Blackwell"], "abstract": "Generative large language models (LLMs), which create text without direct correspondence to truth value, are widely understood to resemble the uses of language described in Frankfurt's popular monograph On Bullshit. In this paper, we offer a rigorous investigation of this topic, identifying how the phenomenon has arisen, and how it might be analysed. In this paper, we elaborate on this argument to propose that LLM-based chatbots play the 'language game of bullshit'. We use statistical text analysis to investigate the features of this Wittgensteinian language game, based on a dataset constructed to contrast the language of 1,000 scientific publications with typical pseudo-scientific text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of politics and language, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of the language of bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language.", "sections": [{"title": "Introduction", "content": "Do LLM-based chatbots produce bullshit? If so, do they always produce bullshit? Or do they only usually produce bullshit? How might either claim be investigated or even proven? Could such bullshit be reliably detected using computational methods? And might those methods enable the identification of bullshit in other types of texts? These are the questions from which this paper starts, and which it seeks to resolve. In Section 1, we present two different accounts of the relationship between LLM-based chatbots and bullshit. First we consider the fundamentalist position which holds that LLM-based chatbots necessarily produce bullshit because their outputs are a form of linguistic communication characterised by 'a lack of connection to a concern with truth \u2013 [...], indifference to how things really are'. This is Harry G. Frankfurt's (2005: 33-34) definition of bullshit in On Bullshit, and the claim can be further proven philosophically. However, the position is vulnerable to factual inaccuracy \u2013 LLM-based chatbots can be modified not to be indifferent to the truth.\nWe therefore next consider the probabilistic position, which holds that LLM-based chatbots usually produce bullshit \u2013 firstly because their training data includes many examples of it, and secondly because the business arrangements through which they are deployed statistically emphasises those aspects of the training data. In this account, we draw on Wittgenstein, describing bullshit as a characteristic language game whose aspects can be recognised. In Section 2, we use statistical text analysis to investigate the features of this Wittgensteinian language game, based on a dataset constructed to contrast the language of 1,000 scientific publications with typical pseudo-scientific text generated by ChatGPT.\nHaving devised this Wittgensteinian Language Game Detector (WLGD), in Section 3 we return to the question of why products such as ChatGPT are so fluent in replicating the specific language game of bullshit, rather than the many other kinds of language game that they could potentially replicate. We offer two explanations based on a key distinction maintained throughout the paper between LLMs, and LLM-based chatbots. We propose two ways of understanding that relation: one, proposing that the latter is a paratext (as understood in literary theory); the other, that the sociotechnical configuration of the LLM-based chatbot must be more precisely inspected in order to describe its behaviour in Wittgensteinian terms. In Sections 4 and 5 we undertake two experiments to further test if the WLGD thus detected might reasonably be considered the language game of bullshit. To do so, we explore whether the same language properties can be detected in two well-known contexts of social dysfunction: George Orwell's critique of politics and language, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate"}, {"title": "1. LLM-based chatbots and bullshit", "content": null}, {"title": "1.1 The fundamentalist position: the outputs of LLM-based chatbots are necessarily bullshit?", "content": "Since the public release of ChatGPT by OpenAl on 30 November 2022, a growing body of informal blogs, journalism, peer-reviewed academic articles, and books have classified its outputs as bullshit (Narayanan & Kapoor 2022, Bernoff 2022, Vincent 2022, Katwala 2022, Deck 2023, Blackwell 2023, Gershon 2023, Sundar & Liao 2023, Hicks et al 2024, Hannigan et al 2024, Vallor 2024). Such works do not use the term bullshit colloquially but technically, more often than not with reference to the philosophical definition of bullshit provided by Frankfurt (2005: 33-34): a form of linguistic communication characterised by 'a lack of connection to a concern with truth - [...], indifference to how things really are'. According to this fundamentalist position, the outputs of LLM-based chatbots are bullshit because they are produced with no regard for truth or falsity.\nAs Murray Shanahan (2024: 70) explains (although he does not advocate the fundamentalist position), 'LLMs are generative mathematical models of the statistical distribution of tokens in the vast public corpus of human-generated text, where the tokens in question include words, parts of words, or individual characters'. They are 'generative', because we can sample from them by posing them questions (i.e. in the style of a chatbot), but the questions can always essentially be understood to be in a specific form: 'Given the statistical distribution of words in the vast public corpus of (English) text, what words are most likely to follow the sequence [...]' (70). In 'answer' the LLM-based chatbot merely generates a 'statistically likely sequence of words' (71). There is no authentic \u2018communicative intent' (72).1 As reported by Al professor Rodney Brooks, 'It just makes up stuff that sounds good' (Zorpette 2023).\nThe very first connection between natural language processing (NLP) and bullshit was made before OpenAl released ChatGPT, in Deck (2023), which although published in May 2023, was first submitted for publication in June 2022. There, Deck identifies that the philosophers Stokke and Fallis' (2017) refinement of Frankfurt's definition of bullshit is even more"}, {"title": "1.2 The probabilistic position: the outputs of LLM-based chatbots are usually bullshit", "content": "The fundamentalist position appears philosophically robust and is rhetorically charismatic. However, it is challenged by the fact LLM-based chatbots can be explicitly designed to adhere to factual claims (for example by invoking an arithmetic module to solve equations). It therefore does not hold to claim that they always produce outputs that are indifferent to their truth or falsity as response to the prompt. Many chatbots are connected are now connected to the Internet, meaning that they can in principle quote by reference to trusted resources such as Wikipedia, instead of just predicting text based on linguistic patterns learnt during training. This significantly reduces the likelihood of errors.\nSince LLM-based chatbots are able to respond with factual accuracy to some questions \u2013 and therefore without indifference to truth or falsity \u2013 it does not hold that they always produce bullshit. But might they be said to usually produce it? To explore the answer to that question, we take our cue from Shanahan's introduction of Wittgenstenian philosophy to his discussion of LLMs. Shanahan (2024: 73) maintains that an LLM-based chatbot 'cannot participate fully in the human language game of truth because it does not inhabit the world we human language users share'. A language game, in Wittgenstein's later philosophy, is a concept introduced to designate uses of language that belong to a specific social activity \u2013 a specific 'form of life', to use Wittgenstein's own terminology (2009: 15\u00b0). Some examples of forms of life, given by Wittgenstein himself, are: 'giving orders', 'reporting an event', 'cracking a joke', 'requesting, thanking, cursing, greeting, praying' (15\u00b0). In section 3.2, we will return to the question of whether an LLM-based chatbot can be considered to partake in any 'form of life'. Here, we want to concentrate on whether the idea of language games offers a useful structure of inquiry into the relationship between the outputs of LLM-based chatbots, and bullshit.\nImmense quantities of text have been collected to pretrain the statistical data patterns of the 'transformer' algorithm (Pretrain[ing] and Transformer[s] being the prerequisites for Generation, as indicated by the now popular GPT acronym). Although the details of the training data are a secret closely guarded by these companies, a reasonable approximation considers it simply as everything on the Internet. There are undoubtedly many people on the Internet who write bullshit, just as there are people who write poetry, sermons, newspaper articles, science textbooks, and many other kinds of text. The GPT language model does, therefore, contain countless examples of bullshit, just as it contains countless examples of truth, lies, debate, and many other language games."}, {"title": "2. A Wittgensteinian Language Game Detector (WLGD)", "content": "In Section 3, we will investigate in more detail how and why products such as ChatGPT exhibit the behaviour that they do. We will also address the perennial concerns signalled by scare quotes when describing the behaviour of a computer system as if its internal state can be considered 'knowledge' or its output considered 'speech' in ways comparable to a person. But we start with an empirical investigation, broadly motivated by Wittgenstein's conception of the nature of language a philosophical standpoint which, as we explain later, has itself been influential on the development of the technical natural language processing methods underpinning the development of LLMs."}, {"title": "2.1 The training set", "content": "Our goal is to distinguish the language game of bullshit from other kinds of language game that could have been precise, factual, clear and concise - the opposite of the bullshitters described by Frankfurt. Different sectors of society may have different views on the kinds of language they admire, but we have chosen a basis for broad consensus in the house style of Nature magazine \u2013 possibly the most widely read and respected scientific journal in the world whose strict editorial standards produce published articles that can be taken as exemplars of precision, clarity and concision, while undeniably reporting (newly observed) scientific facts.\nTo provide a contrast to this language game of prestigious international science, we prompted the latest version of ChatGPT (at the time of our experiment, the \u20184o' release) to write an article for Nature magazine, with the same title as an actual article. To ensure that this emulated the style of a Nature article as closely as possible, we included Nature's instructions to authors in the prompt. The resulting text, even to someone with minimal scientific training, was obviously bullshit - including tables of fabricated data observations, some outright lies, unconnected arguments, but also long passages of vague but science-y text typical of the writing of weaker students. Most of these features have become familiar to those using these products (or whose students use them), and much commentary has been devoted to speculation and hand-wringing about the problems of 'hallucinations', 'fabrication' and so on. We will present an alternative account, but for now focus on the construction of our WLGD.\nWe created a training dataset large enough to be used with state of the art machine learning methods by collecting 1,000 articles from Nature, which we take to represent the language game of scientific communication as defined and selected by the editors - precise, factual, clear and concise. We then took the titles of those 1,000 articles, and asked ChatGPT to"}, {"title": "2.2 The classifier", "content": "Our WLGD studies the characteristic traces of words being used in social contexts. Although Wittgenstein's philosophy underpins the concept of word embeddings now fundamental to the transformer architecture, that philosophical grounding has been largely forgotten by Al researchers, following the dynamics articulated by Philip Agre (1997). We therefore use standard natural-language processing tools, but in a slightly unorthodox way, to recover the original arguments made by Wittgenstein. We created two models: one that identifies characteristic word frequencies, and one that identifies characteristic traces of contexts.\nXGBoost, the first algorithm we trained for our experiments, is widely used in machine learning for simple text classification. For these tasks, it primarily relies on TF-IDF ('term frequency-inverse document frequency'), a measure of the extent to which a particular word is distinctive of the document in which it appears. TF-IDF was invented by Karen Sp\u00e4rck Jones (1972), now celebrated as a computer scientist, but who we should remember started her research career at the Cambridge Language Research Unit, directed by Wittgenstein's student Margaret Masterman. As noted in the Stochastic Parrots critique of LLMs (Bender et al 2021), Sp\u00e4rck Jones offered the earliest linguistic critique of computational language models (2004), so in applying her work to recover this Wittgensteinian interpretation of LLMs, we celebrate the foundational achievements of Masterman and Sp\u00e4rck Jones.\nWe thus used XGBoost to create a statistical model of the terms that best distinguished genuine Nature articles from those fabricated by ChatGPT. We followed standard practice by removing uninteresting \u2018stop-words' such as 'and', 'the', etc. from the TF-IDF vocabulary. We also removed a small number of words that simply reflected the formatting of Nature articles. (In particular, we found that our earliest iteration of XGBoost could identify Nature articles with high confidence because they always included the word 'figure' or 'fig', often followed by an alphanumeric sequence such as \u20181a', '2b', etc. The ChatGPT output, being plain text without figures, did not use these words.) The resulting classifier is 100% accurate, and reports high confidence (99.84%) in judging further examples constructed the same way."}, {"title": "2.3 The WLGD metric", "content": "Fig. 1 shows the distribution of (log transformed) confidence scores from the XGBoost classifier, for the experimental texts that we discuss in the remainder of this paper. Fig 2 shows the distribution of confidence scores from the RoBERTa classifier. As seen in the scatter plot of Fig 3, there is only a small correlation between the confidence values of the two different classifiers (r = 0.282), meaning that they are basing their judgement on different language features (word frequencies in one case, and token embeddings in the other). Our WLGD method therefore combines the outputs of these independent elements of the language game."}, {"title": "3. Why do chatbots produce bullshit?", "content": "Having explained the empirical basis for our WLGD method, we now return to the question of why products such as ChatGPT are so fluent in replicating the specific language game of bullshit, rather than the many other kinds of language game that they could potentially reproduce from the LLM 'cloud chamber'.\nWe propose that the key distinction to attend to is the one between the sequence-prediction capabilities of LLMs created using the transformer architecture, and the way that these capabilities are used to construct LLM-based chatbots (Stone et al 2024). In one famous lineage, OpenAl's original GPT LLM attracted little public attention\u00b3, while the LLM-based chatbot ChatGPT was a blockbuster success. The technical components needed to turn an LLM into an LLM-based chatbot are described by Shanahan (2024: 74) as a supplementary dialogue management system (DMS). A product such as ChatGPT consists of two elements the underlying LLM, and this supplementary DMS. Given the huge commercial value of the transition from LLM to LLM-based chatbot, it is unsurprising that the details of the DMS are highly secret, often not even mentioned, to an extent that many commentators believe ChatGPT to be nothing more than an LLM. There are few published descriptions of the DMS component, and statements made in public are not necessarily to be trusted, given the billion-dollar investments depending on their reception. However, it is reasonably certain that these modules (sometimes called 'guardrails' when a company wishes to emphasise their concern for consumer safety) employ methods such as instruction-tuning, prompt-engineering, and reinforcement learning from human feedback (RLHF) (Stone et al 2024).\nShanahan et al (2024) describe the distinction between the many potential behaviours encoded in the underlying LLM, and the actual behaviour resulting from the DMS as 'role play'. While Shanahan's analysis also draws on Wittgenstein's philosophy of language (2010, 2024), our own interpretation and conclusions are very different."}, {"title": "3.1 Paratext and Eliza effect", "content": "We suggest that one way of understanding the relationship between an LLM on the one hand, and an LLM-based chatbot on the other, is in terms of the distinction in literary theory between text and paratext. The paratext is neither a boundary nor border but, in G\u00e9rard Genette's (1997: 2) account, a \u2018threshold'. The paratext is, as Philipe Lejeune (1975: 45) understands it, 'a fringe of the printed text which in reality controls one's whole reading of the text' (cited in"}, {"title": "3.2 Language Games and Lebensformen", "content": "We have described the relationship between the LLM and its DMS-paratext using the tools of literary theory, but how does this cultural construct relate to Wittgenstein's language games, and to our analogy of the LLM as a cloud chamber in which characteristic traces of specific language games can be recognised? We explore this question via Wittengstein's concept of the Lebensform. The term is conventionally translated by English-speaking philosophers as 'form of life', but the German word can also be translated as \u2018lifeform' - that is to say, a biological entity. In German, it is possible to make a play on words between these two uses of the term. We suggest that Wittgenstein, whose Philosophical Investigations so constantly relied on ambiguity and alternative readings (in contrast to the certainties of his Tractatus), often embraced such nuance, and that English-language uses of his work might do likewise, as do we."}, {"title": "4. Experiment 1: The Language of Politics", "content": "Now that our WLGD method has been defined, and we have established an understanding of the effects of the DMS-paratext and of the assemblage of which the LLM-based chatbots are a part, the question is whether we can test the hypothesis that the language game detected in the ChatGPT \u2018Nature' outputs is indeed the language game of bullshit, rather than some other type of language game. To do so, we apply the WLGD to the classification of 'new' texts. In our first experiment, we theorise and set out to test empirically a degree of affinity between the language used by ChatGPT and that used in political campaigning. In our choice of the language of politics as potentially classifiable as bullshit, we build on the work of George Orwell, 'perhaps the major contemporary forerunner of an approach to bullshit that focuses on the text itself' (Fredal 2011: 247).\nIn Orwell's writings, and especially in \u2018Politics and the English Language' (1941), political speech is described as an intentionally uninformative form of language. This proposition will hardly come as a surprise: Frankfurt himself suggests that bullshit is often found in 'advertising, public speech, and the nowadays closely related realm of politics' (2005: 22). We aim to extend this provocative line of inquiry by suggesting that political language stems from a form of life comparable to that which generates the outputs of LLM-based chatbots."}, {"title": "4.1 Comparing political party manifestos to everyday spoken English", "content": "As an experimental sample, we selected from a corpus of written English reflecting one of the types of political discourse to which Orwell most objected the manifestos published by political parties in UK general elections.11 We obtained 45 party manifestos, spanning the years 1945 to 2005, from the online archives maintained by the Manifesto Project (Lehmann et al 2024a), 12 and calculated the WGLD score for each of them.\nAs a contrast to this corpus of political speech, we calculated the WGLD scores for transcripts of spoken English as found in the British National Corpus (BNC).13 We selected the BNC to represent what Orwell called 'demotic speech' (1969, 3: 135), the speech of the 'average man' (135), of the 'workingman' (136). According to Orwell, everyday \u2018demotic speech' is generally characterised by transparent and communicative uses of language and is an example of the type of language that politicians should strive to adopt in order to more effectively communicate with their citizens. In this sense, if doublespeak is a performative speech act meant to deceive, we can imagine that the conversations recorded in the BNC are more"}, {"title": "4.2 Results", "content": "As shown in Fig. 4, the average WLGD score for UK political manifestos is 49.36, while the average WLGD score for non-political speech data from the British National Corpus is far lower, at 9.40. This difference is highly significant (t(54)=18.18, p << 0.001), meaning that we can reject the null hypothesis."}, {"title": "5. Experiment 2: Bullshit Jobs", "content": "Keeping in mind that we are studying the language games played, not by a human person, but by an LLM-based actant, constructed as a DMS-paratext within a sociotechnical assemblage having economic rather than neurological agency, what is the best way to study this complex of economic characters and fictions? For this purpose, we carry out a second"}, {"title": "5.1 Measuring the language games played in bullshit jobs", "content": "We collected 100 sample texts from online sources, 50 of which were selected as likely to have been written by people employed in bullshit jobs (as characterised by Graeber). A control sample of 50 further texts were selected as likely to have been written by people employed in professions that would not fall within the scope Graeber defines as bullshit. None of the 100 texts were written by scientists, and none of the 100 texts (so far as we are aware, given the challenges of precise provenance and dating for informal online publications) were written by"}, {"title": "5.2 Results", "content": "We performed a repeated-measures analysis of variance (ANOVA), with two factors as independent variables. One independent variable was bullshit/non-bullshit contrast, with two values, and the other variable was the Graeber class, with five possible values. The dependent variable was the BS-meter score.\nWe observed a highly significant main effect (F(99,1)=43.73, p << 0.001), meaning that we can reject the null hypothesis with extremely high confidence. The effect size is large, with an overall mean BS-meter score of 52.47 for bullshit jobs, compared to 28.87 for the contrast sample. We can conclude that samples of text selected to represent employment in Graeberian bullshit jobs do resemble the Frankfurtian bullshit produced by ChatGPT's DMS-paratext far more than they do precise, factual and clear scientific writing. Conversely, more mundane non-bullshit jobs, even low-skill professions such as laundry, cleaning or road repairs, perhaps surprisingly have more resemblance to top-quality scientific writing than to ChatGPT output."}, {"title": "4. Conclusion \u2013 A BS-meter?", "content": "Many commentators have already observed that Al as recently manifested in LLM-based chatbots appears to produce bullshit. Although this is often evident from simple observation of its outputs, and offers an easy target for satirical critique, nobody has yet been able to say quite how the bullshit got there, or by what mechanism it is produced. In this paper we have demonstrated how the statistical methods of natural language processing, heavily influenced by the language philosophy of Wittgenstein, can themselves be used as instruments to study Wittgenstein's characterisation of semantics as a 'language game' of how words are used in social contexts. We show how the word embeddings encoded in LLMs can be used in the manner of a cloud chamber, where the characteristic traces of different language games can be detected. We draw attention to the very specific kind of language game understood as bullshit, and explain how the paratextual apparatus of the dialogue management system that presents LLM sequence prediction as a conversational 'Al' chatbot has amplified and prioritised this particular language game. By asking ChatGPT to generate scientific articles on topics where it clearly has no knowledge or competence, we are able to provide a reference set of how this bullshit is manifested. We then trained a language game detector by contrasting that reference set of bullshit to a large collection of factual, precise, clear and concise scientific writing.\nWe propose that the resulting WLGD can be applied as a remarkably reliable BS-meter. Although our reference set was constructed according to the rubric of Frankfurt, by requesting speech on a topic where the speaker has no knowledge, we can only say that the detector reliably detects some resulting language game, but not necessarily what kind of language game this is. However, our experimental investigations show firstly, that the language game detected by the BS-meter is reliably present in the political misuse of English castigated by Orwell, and secondly that this language game is more likely to be seen in professional writing by those people who Graeber describes as having bullshit jobs. That further coincidence, with its clear social relevance and significance in relation to future applications of LLM-based chatbots, offers compelling evidence that we really are measuring bullshit."}]}