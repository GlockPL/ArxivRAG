{"title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering", "authors": ["Wanqi Yang", "Yanda Li", "Meng Fang", "Ling Chen"], "abstract": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of specific temporal contexts, encompassing multiple time-evolving facts, to address time-sensitive questions. This necessitates not only the parsing of temporal information within questions but also the identification and understanding of time-evolving facts to generate accurate answers. However, current large language models still have limited sensitivity to temporal information and their inadequate temporal reasoning capabilities. In this paper, we propose a novel framework that enhances temporal awareness and reasoning through Temporal Information-Aware Embedding and Granular Contrastive Reinforcement Learning. Experimental results on four TSQA datasets demonstrate that our framework significantly outperforms existing LLMs in TSQA tasks, marking a step forward in bridging the performance gap between machine and human temporal understanding and reasoning.", "sections": [{"title": "Introduction", "content": "Time-Sensitive Question Answering (TSQA) involves parsing and responding to questions that depend on specific time points or periods. For instance, Obama's roles in 2006 and 2016 were distinctly different. Unlike conventional Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Dunn et al., 2017; Ye et al., 2023; Zhao et al., 2023), TSQA requires language models to discern and understand temporal information within questions. Moreover, it necessitates the capacity to locate relevant facts using temporal information, leveraging time-related knowledge to provide accurate and relevant answers. In the domain of TSQA, models are required to predict answers through generation or extraction methods, given a set of questions and contexts embedded with temporal information. These questions are characterized by explicit or implicit temporal expressions, while the context invariably contains multiple facts evolving over time. Recently, several TSQA datasets have been introduced, including notable work (Chen et al., 2021) that provides 40k time-sensitive questions with corresponding free-text contexts derived unprocessed from WikiData (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). Building upon this foundation, (Tan et al., 2023) not only furnish free-text contexts from WikiData without any modification but also offer a version of the dataset where the context has been restructured. These contributions have facilitated evaluations of existing large language models (LLMs) such as FiD (Izacard and Grave, 2020), BigBird (Zaheer et al., 2020), and T5 (Raffel et al., 2020) on these datasets, underscoring the challenge TSQA poses, as the performance of LLMs remains significantly inferior to human levels. However, an effective methodology to significantly improve LLMs performance in TSQA is still lacking.\nIt is challenging to improve model's sensitivity to temporal information and the capacity for temporal reasoning. Firstly, LLMs significantly lack attention to and comprehension of temporal information in both questions and their contexts (Ning et al., 2020; Shang et al., 2021). It is essential for models to prioritize temporal cues in questions to accuately locate corresponding times and facts within contexts, especially for lengthy and complex free-text contexts, which encompass multiple irrelevant information and various chronological connections. Focusing on temporal information allows a model to circumvent the distraction of irrelevant data and to swiftly locate relevant facts through the chronological connections among different paragraphs. Secondly, LLMs (Han et al., 2020; Dhingra et al., 2022) exhibit notably weak temporal reasoning abilities. This specifically refers to the models' susceptibility to interference from answers that pertain to the same entity or relationship but span different time periods within the context, as well as to disturbances from other events occurring within the same time period.\nIn this paper, we introduce a framework to address these challenges. Initially, we implement Temporal Information-Aware Embedding to enhance the model's sensitivity towards temporal information by augmenting its focus on temporal data and adjacent specifics. In our time-aware module, we use SpaCy and identify temporal information within questions and contexts, establishing a question temporal matrix and a context temporal matrix. Furthermore, we advance the model's temporal reasoning capabilities through the Granular Contrastive Reinforcement Learning. We introduce remote negative answers answers within contexts corresponding to different time periods for the same entities and relationships, and proximal negative answers answers related to other events within the same time period. Additionally, we propose a more rational reward function to aid the model in reinforcement learning. Conclusively, we present comparative results between our framework and existing LLMs, validating our framework's effectiveness through its application across four TSQA datasets with varying difficulty.\nThe main contributions of this work are summarised as:\n\u2022 We introduce a new TSQA framework that explicitly processes time-aware information to significantly enhance the performance of LLMs in TSQA tasks.\n\u2022 We propose a time-aware module for constructing Temporal Information-Aware Embedding to enhancing the model's sensitivity to temporal information.\n\u2022 We propose Granular Contrastive Reinforcement Learning to assist the model in improving its temporal reasoning capabilities.\n\u2022 We present experimental results that demonstrate our framework significantly outperforms existing LLMs across various TSQA datasets."}, {"title": "Problem Definition", "content": "Time Sensitive Question Answering aims to generate answers A based on a given free-text question Q and its corresponding free-text context C. The questions feature explicit or implicit temporal expressions, while the context invariably includes multiple time-evolving facts (s, r, o, [ts, te]) where r denotes the relationship that exists between the subject entity s and the object entity o throughout the specified time interval [ts, te]. The answer is either an entity or non-existent. The characteristics of TSQA include: 1) The structure (s, r, oi, [tsi, te\u2081]) is invariably extractable from the question and the answer. 2) Modifying the temporal information within the question will result in a change to the answer.\nThe TSQA primarily evaluates the model's sensitivity to temporal information and its ability of temporal reasoning. Temporal information sensitivity refers to the model's capability to pay closer attention to the temporal information presented in both the given question and the corresponding context, recognizing the temporal information as a crucial factor. Temporal reasoning ability denotes the model's understanding of basic temporal relationships (for example, The year 2015 falls within the time interval from 2010 to 2020, the year 2015 is five years later than the year 2010, the period of World War II is within the time interval from 1939 to 1945, etc.) and its capacity to deduce the correct answer based on these temporal relationships."}, {"title": "Method", "content": "To enhance the sensitivity to temporal information and the capability for temporal reasoning within models, we propose a training framework. Our framework addresses the TSQA task by leveraging a pre-trained language model to interpret free-text temporal questions and context. As illustrated in Fig. 1, the framework incorporates two primary methodologies: Temporal Information-Aware Embedding and Granular Contrastive Reinforcement Learning.\nTemporal Information-Aware Embedding is dedicated to enhancing the model's sensitivity to temporal information by increasing its attention to temporal data and adjacent temporal details. Granular Contrastive Reinforcement Learning offers remote and proximal negative answers based on varying temporal distances and employs a more rational reward function to assist the model in improving its temporal reasoning capabilities."}, {"title": "Temporal Information-Aware Embedding", "content": "In reading comprehension tasks with temporal questions, humans typically begin by identifying temporal cues within the question and subsequently locating corresponding cues within the context. These cues often guide them to accurate answers situated near these temporal markers. Inspired by this intuitive human approach, we propose the Temporal Information-Aware Embedding. This method is specifically designed to enhance the sensitivity of LLMs to temporal details and their associated contextual information. The implementation details of this technique are illustrated on the left side of Figure 2, and include the following steps:\n1) Firstly, we construct a question temporal matrix $A_q$ and a context temporal matrix $A_c$, both initially filled with zeros.\n$A_q = [a_{q1} ... a_{qn}]_{1xn} = [0 ... 0]_{1xn}$ (1)\n$A_c = [a_{c1} ... a_{cm}]_{1xm} = [0 ... 0]_{1xm}$ (2)\nIn our time-aware module, we use SpaCy to locate temporal expressions from question $a_{qt}$ and context $a_{ct}$, marking these detected positions in $A_q$ and $A_c$ with ones. SpaCy's entity recognition module to identify a variety of time entities, including but not limited to: 1) Complete dates (January 1, 2020; 2020/01/01) 2) Month and year (January 2020) 3) Season and year (Spring 2021) 4) Month and day (1 January) 5) Year (2020) 6) Weekdays 7) Relative times (next week, last year) 8) Holidays (Christmas, New Year's Day), etc.\n$A_q = [a_{q1} ... a_{qt} ... a_{qn}]_{1xn} = [0 ... 1 ... 0]_{1xn}$ (3)\n$A_c = [a_{c1} ... a_{ct} ... a_{ci} ... a_{cj} ... a_{cm}]_{1xm} = [0 ... 1 ... 0 ... 0 ... 0]_{1xm}$ (4)\n2) Employing a sliding window W with window size L centered on the temporal information, we use f(W) function to mark the positions within the window region as ones to further highlight regions of temporal relevance.\n$W_{ai} = {a_{i-L},..., a_{i},..., a_{i+L}}$ (5)\n$f(W_{ai}) = f{a_{i-L},..., a_{i},..., a_{i+L}} = 1, \\forall a_j = 1, j \\in [i - L, i + L]$ (6)\n3) The question temporal matrix and context temporal matrix are concatenated, followed by an embedding layer $W_{time}$, resulting in the temporal information-aware embedding $e_{time}$.\n4) The question and context, serving as model inputs, are processed through another embedding layer $W_{text}$ to obtain text embedding $e_{text}$. This text embedding is then combined with its temporal information-aware embedding, aiming to enhance the model's focus on temporal information and its adjacent information."}, {"title": "Granular Contrastive Reinforcement Learning", "content": "As illustrated in right part of Fig 2, we propose Granular Contrastive Reinforcement Learning, composed of Negative Answers with Different Granularity and Contrastive Reinforcement Learning. This methodology enhances the model's temporal reasoning capabilities by assisting in the filtration of negative answers with varying degrees of granularity.\nNegative Answers with Different Granularity. Accurate temporal reasoning depends on discerning the relevance between potential incorrect answers and the fact. We categorize negative answers into two types based on their temporal and contextual relationship to the fact: Remote Negative Answers and Proximal Negative Answers.\nRemote Negative Answers are those that share the same subject and relation with the fact but belong to distinctly different time periods. These answers, although factually related to the question, are temporally distant from the ground truth and thus less likely to be confused with it. For example, considering the question \u201cWhat position did Obama hold in 2009?\u201d with the subject-relational context of Obama's career positions spanning 2008 to 2017, Remote Negative Answers would include positions such as {Professor at the University of Chicago Law School (1993-2005), Illinois State Senator (1998-2004), Federal Senator (2004-2008)}. These roles, while relevant, occurred outside the specified time period of the question.\nProximal Negative Answers, in contrast, are selected from different subjects or relations but fall within the same time period as the ground truth. These answers are temporally proximal but contextually distinct, providing a different type of challenge in distinguishing correct answers. Continuing with the Obama example, Proximal Negative Answers might be {Secretary of State, Supreme Court, Nobel Peace Prize}, all significant roles or honors from the same period (2008-2017) but unrelated to Obama's positions.\nBy incorporating negative answers that vary in temporal proximity and contextual relevance, our model is better equipped to understand and discriminate the details of temporal relations in text, enhancing its predictive accuracy in temporally-focused tasks.\nContrastive Reinforcement Learning. we propose a novel contrastive reinforcement learning framework, building on previous methods like the EM algorithm (Tan et al., 2023), which occasionally fails to recognize semantically equivalent responses due to its rigid scoring mechanics. For instance, when the model's predicted answer is \u201cthe capital of France\u201d and the ground truth is \u201cParis\u201d, the EM algorithm assigns a score of 0. Nonetheless, it is widely acknowledged that \"the capital of France\u201d and \u201cParis\u201d should be considered equivalent answers."}, {"title": "Experiments", "content": "4.1 Experimental Setup\nDataset\n\u2022 Time-Event QA (L2) is derived from the TempReason (Tan et al., 2023) dataset. The distinguishing features of L2 include: 1) The presence of a specific time point in each question, denoted by \"in YEAR\u201d; 2) The time point mentioned in the question may not find a direct match within the context; 3) Context is provided from Wikidata (OBQA); 4) A structured context is offered, where the content is strictly organized according to the (s, r, o, [ts, te]) template (ReasonQA).\n\u2022 Event-Event QA (L3) also originates from TempReason. L3 is characterized by: 1) An absence of explicit temporal information in the questions, with all time information being replaced by an event (for example, the period from 1939 to 1945 is referred to as the World War II era); 2) Complex expressions of time such as \"before\", \"after\", \"during\", and \"simultaneous\" are used in questions; 3) Context is provided from Wikidata (OBQA); 4) A structured context is offered, where the content is strictly organized according to the (s, r, o, [ts, te]) template (ReasonQA).\n\u2022 TimeQA Easy is based on the TimeQA dataset (Chen et al., 2021). Features of the Easy Version include: 1) Each question contains a time expression either in the format \u201cin YEAR\u201d or \"from YEAR to YEAR\"; 2) The time mentioned can be directly matched within the context; 3) Context from Wikidata is provided (OBQA).\n\u2022 TimeQA Hard stems from the TimeQA dataset. The Hard Version is characterized by: 1) The presence of time information in questions expressed in complex terms such as \"before\", \"after\", \"first\", \"last\"; 2) The time mentioned in the question cannot find a direct match within the context; 3) Context from Wikidata is provided (OBQA).\nThese four types of TSQA present varying levels of difficulty in terms of temporal information expression and temporal reasoning. We investigated the sensitivity to temporal information and the temporal reasoning capabilities of the model using these TSQA datasets with differing difficulties, to demonstrate the outstanding performance of our framework.\nTraining In the first stage of training, we train T5-base model for 6 epochs with a batch size of 8 on a NVIDIA Tesla V100 GPUs. The model is optimized using AdamW (Loshchilov and Hutter, 2017) with a learning rate of 5e-6. In the second stage of training, we fine-tune the model with our framework for 10 epochs with a batch size of 16. The model is optimized using Proximal Policy Optimization (Schulman et al., 2017). In our configuration of the Temporal Information-Aware Embedding, we employ SpaCy, an open-source Natural Language Processing (NLP) library, as time-aware module to locate temporal information within the provided question and context. The window size of the sliding window is set to 10. Furthermore, in Granular Contrastive Reinforcement Learning, the ratio between remote negative answers and proximal negative answers is established at 1:1.\nEvaluation Metrics We evaluate the model on the test set by exact match (EM) and F1 score, which is the standard evaluation metrics on TempReason and TimeQA. EM is 1 only if prediction and ground truth achieve an exact match and otherwise 0. The value range of F1 is [0, 1], and the closer F1 is to 1, the better performance of the model is.\nBaselines\n\u2022 FLAN-T5-Large (Wei et al., 2021) represents an advanced instantiation of NLP models based on the extensive T5 framework. We have deployed FLAN-T5-Large for the purpose of evaluating its performance across a variety of TSQA datasets.\n\u2022 ChatGPT (Ouyang et al., 2022), developed by OpenAI, is a sophisticated NLP model predicated on the GPT (Generative Pre-trained Transformer) architecture. Utilizing the official API provided for gpt-3.5-turbo, we have conducted evaluation on various TSQA datasets.\n\u2022 T5-SFT (Raffel et al., 2020), an acronym for \"Text-to-Text Transfer Transformer\", is a NLP model conceived by the Google Research team. We subjected the TSQA dataset to supervised fine-tuning on the T5-base model to conduct further evaluation."}, {"title": "Experimental Results", "content": "Table 2 contrasts the performance of various LLMs across four TSQA datasets, evaluating them in terms of EM and F1 scores. From the insights drawn from Table 2, it is evident that: 1)The framework we propose demonstrates superior performance on the TSQA task, outperforming a range of LLMs, including FLAN-T5-Large, ChatGPT, and T5-SFT models. 2)On these four TSQA datasets, which vary in difficulty levels pertaining to the expression of temporal information and temporal reasoning, our framework shows notable enhancements. Compared to the most competitive baseline results, EM improvements on the L2 and L3 of TempReason are 12.2% (ReasonQA), 30.4% (OBQA) and 19.7% (ReasonQA), 26.4% (OBQA) respectively, while on the TimeQA dataset's easy and hard versions, improvements of EM are also observed at 6.7% and 7.4%, respectively. In the TimeQA dataset, a portion of the answers consists of empty strings. When employing our method, there is a tendency to generate empty strings, which results in a decrease in recall and, subsequently, a reduction in the F1 score. However, by sacrificing a small amount of F1, we can significantly enhance the EM score.\nThese outcomes validate the exceptional capabilities of our proposed framework in addressing the TSQA task. Regardless of whether the questions contain explicit or implied temporal information, or whether the context is complex or straightforward, our framework consistently demonstrates outstanding performance. For datasets with simpler data (e.g., L2/L3-ReasonQA), our framework aids models in better capturing variations in temporal information within questions and focuses on locating temporal details within brief contexts. For more complex datasets (e.g., L2/L3-OBQA, TimeQA), our framework facilitates the model's ability to focus on and locate temporal information within long contexts that contain multiple unrelated facts and temporal connections. Moreover, it effectively eliminates interference from negative answers of varying granularity, thereby achieving substantial improvements. For qualitative results of \"T5-SFT\" and \"Ours\u201d, refer to Appendix C."}, {"title": "Ablation Study", "content": "The Contributions of TIAE and GCRL In order to further investigate the contributions of the two methods brought by our framework, we conduct ablation studies by building two more model variants upon \u201cT5-SFT\u201d:\n\u2022 T5-SFT with TIAE, which only applies Temporal Information-Aware Embedding.\n\u2022 T5-SFT with GCRL, which only applies Granular Contrastive Reinforcement Learning.\nThe experimental results presented in Table 3 demonstrate the efficacy of the Temporal Information-Aware Embedding and the Granular Contrastive Reinforcement Learning methods. Specifically, the application of the Temporal Information-Aware Embedding technique alone (T5-SFT with TIAE) resulted in an improvement of 23.6% (EM) and 7.1% (F1) on TempReason-L2 dataset. When the model employed solely the Granular Contrastive Reinforcement Learning method (T5-SFT with GCRL), an enhancement of 29.7% (EM) and 10.5% (F1) was observed. Moreover, the integration of both strategies yielded a superior performance, culminating in a 30.4% (EM) and 11.1% (F1) improvement.\nThe Novelty of GCRL For \u201cGranular Contrastive Reinforcement Learning\u201d, we explore the impact of the EM score and contrastive triple score as reward functions on the model, as well as the effect of negative answers at varying granularities. We conduct ablation studies by building two additional model variants based upon \"T5-SFT\u201d:\n\u2022 T5-SFT with EM-RL, which applies Granular Contrastive Reinforcement Learning, regarded EM score as reward function.\n\u2022 T5-SFT with Contrastive-RL, which applies Granular Contrastive Reinforcement Learning, regarded contrastive triple score as reward function.\nFirstly, in terms of the impact of different reward functions on the model, our observations from Table 4 indicate that although the model exhibits certain improvements when the EM score is employed as the reward function for reinforcement learning, it still harbors drawbacks, which we have discussed in Section 3.2. However, contrastive reinforcement learning mitigates the shortcomings of the EM reward function, achieving a 29.7% and 10.5% enhancement of EM and F1.\nSecondly, we note the influence of negative answers of varying granularities on the model. We conducted ablation experiments, wherein the negative answers in \"Granular Contrastive Reinforcement Learning\u201d were set as Remote & Proximal negative answers, Remote negative answers, and Proximal negative answers, with the quantity of negative answers being consistent across experiments. The results from Table 4 demonstrate that Remote negative answers and Proximal negative answers contribute to a 28.4% and 27.7% improvement of EM in the model, respectively. The combination of both yields a superior performance, culminating in a 29.7% improvement."}, {"title": "Comparison with TempT5", "content": "Furthermore, we conducted comparative experiments with another framework named TempT5 (Tan et al., 2023) specifically designed for the TempReason dataset, as shown in Table 5. Our framework demonstrated a markedly superior performance."}, {"title": "Related Work", "content": "Time Sensitive Question Answering Recent developments in Open Book Question Answering (OBQA) have brought about significant advancements in addressing time-sensitive questions. These advancements are particularly evident in contemporary Time Sensitive Question Answering (TSQA) datasets (Zhang and Choi, 2021; Chen et al., 2021; Tan et al., 2023; Yang et al., 2024).\nSituatedQA (Zhang and Choi, 2021) stands out as a notable contribution to the field. This dataset focuses on open-domain time-sensitive QA, featuring realistic questions reannotated from the NQ dataset (Kwiatkowski et al., 2019) to incorporate context dependence and diverse answers across temporal or geographical contexts.\nSimilarly, TimeQA (Chen et al., 2021) presents a dataset comprising 20K questions with an emphasis on time sensitivity. Notably, TimeQA's hard version challenges models to reason over implicit temporal mentions within passages, a feature less emphasized in SituatedQA.\nTan's work (Tan et al., 2023) in this domain is exemplified by the development of the TempReason benchmark for TSQA. This benchmark offers a comprehensive assessment covering various types of temporal understanding, including time-time (L1) relations, time-event (L2) relations, and event-event (L3) relations.\nThese TSQA datasets adopt the OBQA setting, which leverages external context, such as natural language text, to assist Language Models (LMs) in answering questions. This contrasts with Closed Book Question Answering (CBQA), where only the question is provided to the LM without access to external text (F\u00e9vry et al., 2020; Roberts et al., 2020; Dhingra et al., 2022; Liska et al., 2022; Wei et al., 2023).\nOverall, TSQA tasks highlight the complexity and realism inherent in time-sensitive QA, emphasizing the necessity for LMs to comprehend temporal relationships and effectively ground information temporally to provide accurate responses.\nLarge Language Models on TSQA Some studies have shown that large language models (LLMs) such as those developed by (Devlin et al., 2018), (Raffel et al., 2020), and (Liu et al., 2019) exhibit strong performance in general question answering tasks (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). However, with the emergence of Time-Sensitive Question Answering (TSQA) tasks and related datasets, LMs have demonstrated less satisfactory performance. For instance, (Chen et al., 2021) demonstrated that large language models like FiD (Izacard and Grave, 2020) and BigBird (Zaheer et al., 2020), functioning as generative and extractive QA models, achieve only 60% accuracy on TSQA. Furthermore, experimental results from (Tan et al., 2023) showed that FLANT5-Large (Wei et al., 2021) and ChatGPT (Ouyang et al., 2022) achieve performance of only around 10%.\nTo address the challenges of TSQA, (Jia et al., 2018b) proposed TEQUILA, a method specifically tailored for temporal knowledge graph-based QA. TEQUILA utilizes constraint reasoning on temporal intervals to compute answers to question. (Faghihi and Kordjamshidi, 2021) introduced the Time-stamped Language Model as a novel approach to comprehend the progression of events. Their work extends the understanding of event flows across time. Additionally, Inspired by work on relational KGQA (Huang et al., 2019; Saxena et al., 2020), (Shang et al., 2022) proposed a method of improving time sensitivity for question answering over temporal knowledge graphs (Jia et al., 2018a, 2021; Saxena et al., 2021), which incorporates contrastive learning, time-sensitive temporal KG embedding, and KG-based pruning techniques. (Mavromatis et al., 2022) introduced a novel framework by proposing a joint model which integrates temporal knowledge graph embeddings with pre-trained language models. These methods have shown excellent performance on Temporal Knowledge Graphs data. However, as far as we know, there is a lack of effective methods for handling free-text TSQA datasets."}, {"title": "Conclusion", "content": "In conclusion, TSQA presents unique challenges for large language models (LLMs), particularly in their attention to and comprehension of temporal information within questions and contexts, as well as in their capacity for temporal reasoning."}, {"title": "Ethics Statement", "content": "In conducting our research on Time-Sensitive Question Answering (TSQA), we are acutely aware of the ethical responsibilities we bear, including data privacy and the potential for misinformation.\nFirst and foremost, our work strictly adheres to privacy and data protection standards. The datasets utilized, derived from publicly available information. We ensure that no personal data is used without clear consent and that all information is handled in compliance with relevant data protection laws. Misinformation is a critical concern in any information processing system. By focusing on temporal accuracy, we aim to reduce the spread of misinformation and improve the reliability of automated responses. However, we acknowledge the inherent limitations of current technologies and the ongoing need for human oversight in verifying the correctness and appropriateness of model outputs. Our research team is committed to engaging with these ethical considerations, promoting transparency in our methodologies, and contributing to the responsible development and deployment of TSQA technologies."}, {"title": "Limitation", "content": "Despite these advancements, the performance gap between LLMs and human benchmarks remains an ongoing challenge, suggesting the necessity for continued research and development in this area. Our work lays the groundwork for future explorations into the intricacies of time-sensitive processing and reasoning, aiming to narrow this gap and to elevate the capabilities of LLMs in handling the dynamic and nuanced nature of temporal information."}, {"title": "Appendix", "content": "A Details of PPO\nProximal Policy Optimization (PPO) is a widely employed policy gradient method within the domain of reinforcement learning. We utilize the PPO strategy to maximize cumulative rewards. When an action yields negative outcomes, the corresponding reward signal is typically negative. This implies that the action will be reduced in the future. Specifically, the objective of policy is to increase the probability of actions that result in positive rewards and to decrease the probability of actions that lead to negative rewards. Through this mechanism, the policy learns to avoid negative results and gravitates towards positive results. The parameter settings of PPO used in our paper are illustrated in Table 6.\n$T = max{d(GT, P) \u2013 d(P, N) + margin, 0}$ (7)\nwhere d(x, y) =|| x - y ||2. The resultant score T is then normalized and scaled, transforming it into a reward function R that is contingent upon the distinctiveness of the model's prediction from the negative samples and its alignment with the ground truth. The reward function is defined as:\n$R=a\\cdot(\\frac{1}{2} + \\frac{1}{1+ e^{T} + \\delta} - \\beta)$ (8)\nwhere a = 4, \u03b2 = 2 and \u03b4 = 1e-6.\nThis approach not only refines the model's ability to make temporally precise judgments but also enhances its overall learning efficiency by dynamically adjusting to the complexities of temporal reasoning. Thereby significantly boosting model's learning efficiency and effectiveness. The model is optimized using Proximal Policy Optimization (Schulman et al., 2017), whose details are shown in Appendix A.\nproduced by our proposed model, labeled as Ours. This comparison highlights the differences in response quality and accuracy, demonstrating the effectiveness of our approach in handling complex question-answering tasks.\nB Examples of Negative Answers with Different Granularity\nIn our study, Table 7 illustrates several representative examples of remote negative answers and proximal negative answers, which are crucial components in evaluating the effectiveness of our model's temporal reasoning ability. These examples are derived from carefully designed questions and detailed contexts. By incorporating these types of negative answers, the model is challenged to improve its temporal accuracy and enhance its ability to identify correct answers among closely related incorrect options.\nC Qualitative Results\nTable 8 provides illustrative examples of answers generated by the T5-SFT model compared to those"}]}