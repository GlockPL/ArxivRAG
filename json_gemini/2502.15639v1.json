{"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models", "authors": ["Anirudh Sundar", "Sinead Williamson", "Katherine Metcalf", "Barry-John Theobald", "Skyler Seto", "Masha Fedzechkina"], "abstract": "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions \u2013 a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) exhibit impressive performance on a variety of tasks from text summarization to zero-shot common-sense reasoning (Raffel et al., 2020; Liu and Lapata, 2019; Bosselut et al., 2019; Richardson and Heck, 2023) and are increasingly deployed in a variety of fields ranging from health to entertainment. Despite these capabilities, to ensure that deployed LLMs align with human values, are non-toxic, and do not hallucinate, they often must be adapted post pre-training. Model interventions have emerged as data- and compute-efficient tools for model adaptation, whereby targeted updates are applied to model activations after pre-training (Rodriguez et al., 2024; Li et al., 2023; Rimsky et al., 2024). One such method is finding experts (Suau et al., 2022, 2024) which manipulates the activations of expert neurons responsible for encoding a broadly defined concept (e.g., a word or style of text) to steer model generations into a desired direction. This approach has been successfully used in a variety of domains, ranging from achieving gender parity (Suau et al., 2022), reducing toxicity (Suau et al., 2024),study-ing geopolitical biases (Faisal and Anastasopoulos, 2023) and multilingual capabilities (Kojima et al., 2024) in mLLMs. While model interventions successfully control model generations, we do not fully understand their implications for model performance. Two observations are relevant: First, model intervention methods increase perplexity on a fixed dataset post-intervention (Suau et al., 2024) meaning that the intervention introduces changes in how the model represents language. Second, work on mLLMs (Kojima et al., 2024) has shown that intervening on experts for a given language increases the probability of generating that language in the model output (expected outcome) and improves prompt-based machine translation (surprising outcome). These performance gains suggest that the intervention may increase the alignment between representations of different languages. In this work, we focus on representational changes in mLLMs with an emphasis on cross-lingual alignment for two reasons. First, gains in mLLM performance are largely attributed to better alignment of multilingual representations (Wu et al., 2024; Lample et al., 2018). This has generated a lot of interest in improving multilingual alignment (Chaudhary et al., 2020; Efimov et al., 2023; Lample and Conneau, 2019; Liu et al., 2025). Second, datasets with the same text in multiple languages are available for a variety of tasks, which enables us to study the impact of the intervention in a controlled way across multiple languages."}, {"title": "2 Related Work", "content": "Model interventions. Model interventions are a family of approaches that manipulate model activations to control generations (Li et al., 2023; Turner et al., 2024; Rodriguez et al., 2024). Suau et al. (2022) propose a method to identify neurons in pre-trained transformer models that are most predictive of a particular concept (expert neurons) and show that setting the activations of these experts to their mean value can induce the presence of the target concept in model generations. Suau et al. (2024) find the expert neurons for toxic language and steer the LLM to generate less toxic text by dampening these neurons, while Turner et al. (2024) achieve detoxification by using a contrastive prompt. Rimsky et al. (2024) propose a method to control generations by leveraging the differences in residual stream activations between pairs of positive and negative examples. In mLLMs, Kojima et al. (2024) use this approach to produce more target language tokens in open-ended generation. However, prior work does not analyze the changes these interventions introduce in the representational space of mLLMs nor does it explore the impact of the interventions on cross-lingual alignment.\nAligning multilingual representations in mLLMs. Research on LLM representation alignment falls into two broad categories: 1) Improving model performance on downstream tasks via post-training methods such as prompt-based techniques (Huang et al., 2023; Tanwar et al., 2023), fine-tuning, or continuous pre-training (Zhang et al., 2023; Li et al., 2024). 2) Understanding where and how representation alignment is achieved in mLLMs. For example, Wendler et al. (2024) show that English-dominated mLLMs like Llama-2 use English as a pivot language and Zhao et al. 2024 systematically evaluate factor contributing to successful cross-lingual transfer in such models."}, {"title": "3 Methods", "content": "We seek to understand the impact of network interventions on the representational space of mLLMs with a focus on cross-lingual alignment. We consider three open-source mLLMs: Aya-8B (instruction fine-tuned) (Aryabumi et al., 2024), PolyLM-13B (chat version) (Wei et al., 2023), and Bloom-7B (base) (Scao et al., 2022). Since our aim is to draw conclusions about cross-lingual alignment, we want to make sure that we know what languages were seen in pre-training and include mLLMs for which a detailed description of pre-training datasets is available, excluding LLMs such as Mistral (Jiang et al., 2023), Llama (Touvron et al., 2023), and Gemma (Team et al., 2024). We begin by identifying and intervening on the language experts in the mLLMs and then study cross-lingual alignment in the embedding space and downstream task performance pre- and post-intervention."}, {"title": "3.1 Probing dataset construction", "content": "Following Kojima et al. (2024), we use the Flores200 dataset (NLLB Team, 2022) to find the expert neurons for a particular target language (i.e., the language specifically targeted by the intervention). Flores200 is a machine translation dataset containing short paragraphs sampled from Wikimedia \u00b9 and subsequently translated into 204 languages by skilled human translators. We limit our investigations to the intervention on five target languages \u2013 English, German, French, Spanish, and Japanese. These languages are well represented in pre-training data of the models we are considering, ensuring the existence of expert neurons."}, {"title": "3.2 Identifying expert neurons", "content": "Expert neurons for a given language are identified following Suau et al. (2024). This finding experts approach consists of two steps \u2013 first, expert neurons are identified for a particular concept of interest (in our case, a particular target language) and then an intervention is performed to change the activations of these neurons. Expert neurons are those that can classify sentences as being from the positive set (containing the target language) vs. the negative set (that does not contain the target language), as measured by the area under the ROC curve. For each of the five languages under consideration, we use the Flores200 dev split for the target language as the positive set, and the dev splits for the other four languages plus Chinese as the negative set. We include Chinese to increase variety in the character systems in the negative set but we do not consider it for the positive set."}, {"title": "3.3 Intervening on expert neurons", "content": "For the intervention, we select the k neurons with the highest expertise (i.e., highest AUROC). We select the value for k that balances generating text in the target language with a low perplexity on the language-specific Wikipedia text (See Appendix B for further details). The activation for these neurons is set to their respective mean value calculated over the positive sentences (Suau et al., 2022). For almost all target languages, the probability of generating that language increases post-intervention (Fig. 2, top), suggesting that the intervention is successful. The only exception is English in the Aya-8B model, where the intervention reduces the likelihood of generating English. We believe that the intervention steers the model away from the default configuration, and English is the default language for that model. Interestingly, despite Bloom-7B's training set containing neither German nor Japanese, the intervention results in generating both languages with high probability. Our hypothesis is that the Bloom-7B pre-training data contains some amount of German and Japanese data that is large enough to enable expert discovery and controlled generation. While we are successfully able to increase the accuracy of target language generation through the interventions, consistent with prior work (Suau et al., 2024), we observe an increase in perplexity post-intervention as the number of activated neurons increases, see Fig. 2 (bottom). For our analyses, we set k to 100 experts for Bloom-7B and 2000 for PolyLM-13B and Aya-8B. For brevity, we present the results for the intervention on Spanish (randomly chosen) in the main text. The results for the other languages are in the respective appendices."}, {"title": "4 The intervention shifts the embedding space increasing cross-lingual alignment", "content": "We begin our investigation by quantifying the differences induced by the intervention into the embedding space. For this analysis, we intervene on each of the five target languages discussed in Section 3.3 and examine the effect of the intervention on the representations of 22 languages (the union of all languages present in the pre-training across the three language models). We exclude Arabic and Chinese from consideration due to the lack of conformity in the scripts used\u00b2. Note that not all of these languages are part of the pre-training for every model under consideration but we present them for consistency (clearly indicating in all figures if the languages were seen by the model during the pre-training). For each of the 22 languages, we embed the Flores200 test set (1012 sentences per language) with the original and intervened models' last layer. To characterize the changes in the embedding space, we calculate two types of distances: (1) the pairwise cosine distance between the embeddings of the 22 languages for the intervened and unintervened spaces and (2) the cosine distance between the mean of the embedding for each of the 22 languages and the medoid of each space (pre- and post-intervention) (Table 1). We project the multidimensional embeddings into a two-dimensional space using UMAP (McInnes et al., 2020) to visualize how the embedding space changes.\nOur findings are as follows. The intervention pulls the embeddings of all languages into a new space (see Fig. 3 for Spanish and Appendix C for other languages), rather than moving them closer to the embeddings of the target language in the unintervened space. The increase in perplexity post-intervention discussed in Section 3.3 also supports this finding.\nThe post-intervention embeddings for the different languages are closer to each other compared to the pre-intervention embeddings, as indicated by the reduced pairwise cosine distances between the languages. Specifically, the distances are reduced because the post-intervention embeddings are pulled closer to the medoid of the embedding space. As a result of the shift, all languages are closer to the target post-intervention. We notice that all distances under consideration are reduced less post-intervention for PolyLM-13B compared to the other models. We hypothesize that this relates to the specific data distribution and training procedure used for PolyLM-13B. Unfortunately, since we do not have access to the data the three models under consideration were trained on, we cannot test this hypothesis in this work. We return to this point in Section 9. Taken together, these findings suggest that the intervention projects language embeddings into a new space where they are more aligned. In the following sections, we explore if this change translates into downstream task performance."}, {"title": "5 Cross-lingual similarity is enhanced in the new space", "content": "We now ask if the increased alignment post-intervention translates to downstream task performance. We use cross-lingual retrieval as our downstream task: Given a sentence (query) in one language (query language), and a set of sentences (candidates) in a different language (candidate language), which of the candidates is a translation (match)? Our main experiments are carried out on the Flores200 test split (NLLB Team, 2022) as it allows us to test cross-lingual retrieval across multiple combinations of query and candidate languages. As the dev split of the Flores200 dataset was used to identify language experts, we also present results on the validation split of Tatoeba (Tiedemann, 2012) and the test split of BUCC-18 (Hu et al., 2020) for an independent validation of our findings (see Appendix H).\nFor each sentence, we compute pre- and post-intervention embeddings by averaging over the last hidden state of the mLLM, producing vectors with dimensions matching the model's hidden size. To identify the closest matching sentence, we compute inner products between the query (e.g., in Spanish), and all candidates (e.g., in French). We select the candidate with the highest inner product as the match, and then measure top-1 accuracy.\nTop-1 retrieval accuracy improves post-intervention for retrieval with the target language. We first examine if the increased proximity to the target language in the post-intervention embedding space translates into top-1 retrieval accuracy improvement when the target is used as the retrieval query for the 22 candidate languages under consideration. We find that top-1 retrieval accuracy improves post-intervention when using the target as the query language (see Fig. 4 for the Spanish intervention and Appendix E for the remaining four languages). This finding is consistent across most target languages and models. Candidate languages present in the pre-training data generally demonstrate larger gains post-intervention. The pattern of improvement differs based on the model. Specifically, for Aya-8B a successful intervention results in consistent improvements in top-1 accuracy for the majority of candidate languages (median=32%; max=74%). For Bloom-7B, top-1 accuracy gains are large (up to 89%) for a small number of candidate languages, with moderate improvements for other languages (median=14%). For PolyLM-13B, the improvements are small (median=0.5%; max=12%).\nTo better understand how the increased alignment in the embedding space influences cross-lingual retrieval, we look at the mean pairwise cosine distances between the query and candidate languages and explore how this correlates with retrieval accuracy (Fig. 4). Table 2 shows average correlations between post-intervention top-1 retrieval accuracy (accpost) and mean query-candidate language distance both pre- and post-intervention (dpre, dpost), average correlations between dpre and dpost, and average correlations between improvement in accuracy (\u2206acc = accpost - accpre) and change in distance between pre- and post-intervention embeddings (dpre \u2013 dpost). When calculating averages, we only include candidate languages seen in pre-training for each model; we note that the general pattern stays the same but the correlations are somewhat weaker if all 22 languages are considered for all models.\nWe find that in this setting, when the query and intervention-target language are the same, the distance between query/target and match language is predictive of top-1 cross-lingual retrieval accuracy in both pre- and post-intervention spaces.\nAs discussed in Section 4, all language embeddings move closer to the target's embeddings post-intervention, which explains the gains in cross-lingual retrieval accuracy. The distances in the unintervened and intervened space are positively correlated\u2014language embeddings that are closer to the target pre-intervention are also closer to the target post-intervention. However, the magnitude of the performance gain in the intervened space does not correlate with the reduction in distance between the match and target languages across the two spaces, suggesting that the increased alignment post-intervention cannot be simply explained by a reduction in distances.\nTop-1 retrieval accuracy improves post-intervention for retrieval with the non-target languages. In Section 4, we found that the distances between almost all languages decrease post-intervention\u2014not just the distances to the intervention target. We next examine if these reduced cosine distances between languages other than the intervention target translate into improved top-1 retrieval accuracy when using these languages as the query language.\nFor example, we study if intervening on Spanish experts improves Dutch-English retrieval (in this case, neither the query nor candidate language is the intervention-target language).\nWe find that, perhaps surprisingly, improvements observed when the query language is the intervention target (see Fig. 4 and Table 2) carry over to query languages other than the intervention-target language (see Fig. 5 for the Spanish intervention and Appendix F for the remaining four languages). For example, the intervention on Spanish expert neurons for Bloom-7B results in retrieval improvement when English, French, and Portuguese are the query language. The same intervention improves retrieval when querying Hebrew with Persian or when querying Czech with Greek in the Aya-8B model and when querying Russian with Portuguese in PolyLM-13B. These are examples of larger improvements, but many other languages follow the same pattern with smaller gains. Generally, the patterns in improvement are consistent with those seen when Spanish is the query language. Languages that are in the pre-training set have larger accuracy gains. Bloom-7B has large improvements for a small number of languages and no drops in performance. Aya-8B has relatively large improvements for a majority of languages but also has a drop in performance for some. As noted previously, PolyLM-13B performance is uneven-the improvement varies by language with languages in the pre-training set generally having larger improvements."}, {"title": "6 Within-language similarity is preserved in the new space", "content": "As observed in Section 4, all languages move toward the medoid of the embedding space post-intervention, which raises the question of whether language-specific similarities preserved in the new space. To answer this question, we evaluate performance on a paraphrase retrieval task which tests whether a sentence in the intervened space can be matched with its paraphrase in the intervened space. We utilize the PAWS-X dataset (Hu et al., 2020), which provides paired sentences across seven languages, including all five of our intervention targets. From the test split, we retain only the paraphrase pairs, excluding non-paraphrases and sentences from other languages. This transforms our evaluation into a within-language sentence retrieval task, where the goal is to match each sentence with its paraphrase from a pool of candidates for that language.\nThe paraphrase retrieval task reveals two key findings about embedding spaces before and after the intervention. First, the top-1 paraphrase retrieval accuracy remains largely unchanged after the intervention (see Table 3), indicating that the new embedding space preserves within-language similarity. Second, when attempting retrieval between intervened and unintervened embeddings of the same language \u2013 i.e., using the embeddings from the unintervened model as the query and the embeddings from the intervened model as candidate matches \u2013 accuracy drops significantly. This decline supports the observation in Section 4 suggesting that the intervention projects embeddings into a distinctly different space from their original unintervened representations. This finding also aligns with the increase in perplexity observed post-intervention \u2013 the intervened space of a given language is not the same space as the original space of this language."}, {"title": "7 Intervention on random neurons does not provide an improvement on downstream tasks", "content": "In our analyses so far, we have attributed the changes in the embedding space to the intervention on expert neurons. Before we conclude, we consider an alternative possibility \u2013 that the expert neurons do not play a role in increasing alignment post-intervention, but instead alignment is achieved by fixing the activations of a number of neurons in a network. To address this, we assign the activation levels of the language expert neurons used in prior sections to the same number of neurons chosen randomly in the network and repeat our analyses on these models.\nWe find that intervening on random neurons produces markedly different results compared to activating language experts (see Appendix G). The embedding space after the intervention on random neurons does not have the same properties as described in Section 4, which translates into the performance on downstream tasks for all models. Specifically, for the Aya-8B and PolyLM-13B top-1 cross-lingual retrieval accuracy drops for all languages post-intervention on random neurons compared to pre-intervention. Interestingly, for Bloom-7B, there is mostly no change for all target languages except French, which surprisingly improves post-intervention on random neurons. However, the gains are significantly smaller compared to those after intervening on French experts. Similar to the intervention on language experts, within-language paraphrase retrieval shows only small changes post-intervention. When they occur, these changes tend to be negative (i.e., the performance drops) after intervening on random neurons and positive after intervening on the actual language experts."}, {"title": "8 Conclusions", "content": "We present a novel analysis of the impact of the finding-experts intervention on cross-lingual alignment in mLLMs. We find that intervening on language experts projects model embeddings into a new space where languages are more aligned than in the original space but still preserve within-language similarity. These findings provide an explanation for the increase in perplexity observed post-intervention in prior work (Suau et al., 2022). The increase in cross-lingual alignment results in up to 2x improvement in top-1 retrieval accuracy. Additionally, we show that the correlation between cross-lingual alignment and cross-lingual retrieval is high and statistically significant. Importantly, these improvements are not restricted to the intervention-target language. For instance, an intervention on Japanese results in a large top-1 retrieval accuracy improvement for English-Portuguese for Bloom-7B. We find that the three models we study show markedly different patterns both in the changes to the embedding space and downstream tasks. We leave it to future to work to determine the causes of these differences, though we hypothesize that they are due to the pre-training differences."}, {"title": "9 Limitations", "content": "There are several limitations that need to be considered when interpreting our results.\nThe major limitation is that we are working with pre-trained models and we have only limited information on training data and procedure. Specifically, for Bloom-7B and PolyLM-13B, we have the information on the proportion of each language in the pre-training set. For Aya-8B, only information on which languages were seen in the pre-training (but no proportions) is available.\nWe observe different performance gains post-intervention for the three models under consideration and while we hypothesize that these differences are due to training data and/or procedure, we do not have enough information to test this hypothesis. Future work should explore the effect of intervention on alignment in a more controlled setting where the models are trained from scratch on a publicly available dataset manipulating language proportions in the training data to better understand what is driving the difference.\nWe have studied only one approach out of a family of approaches to controllable generations (Rimsky et al., 2024; Suau et al., 2024; Rodriguez et al., 2024). Each approach in the family comes with its differences \u2013 in the way the neurons targeted by the intervention are discovered, how the changes are introduced to the activations, how many neurons are intervened on, etc. We do not fully understand how these design decisions impact the representation space. For example, it is possible that some of these approaches are more beneficial for alignment while others introduce changes that are more beneficial for other tasks (or not at all). The comparison of approaches is beyond the scope of current work and we leave it for future investigations."}, {"title": "B Selecting expert neurons", "content": "The value of k is determined as follows. For each of the five languages, we sweep over expert set sizes ranging from 100 to 5000. For each setting of language and number of experts, we run free-form generation to generate 256 sentences over eight random seeds (for a total of 2048 sentences) using the beginning of sentence (BOS) token as the prompt. We perform generation with temperature=1 and top_p=0.9 4. We then use lang-id (Lui and Baldwin, 2012) to measure the probability of the text generated in the target language.\nTo calculate the perplexity of Wikipedia text in the target language for the original and intervened models, we use the Wikimedia dump from 2023-11-01 5. Paragraphs of text shorter than 100 characters are removed and the remaining paragraphs are concatenated together. Finally, a corpus of 10 million tokens is selected from the concatenated paragraphs. The context length is set to the model's maximum input size (in tokens) and a stride (i.e., sliding the context window) of 512 tokens is used to speed up the perplexity measurement."}, {"title": "C UMAP embeddings for four intervention-target languages", "content": "Note: The embeddings post-intervention are marked with \u2018*' for each language. The languages that are not in the training set are marked in red."}, {"title": "C.1 Bloom-7B", "content": null}, {"title": "C.2 Aya-8B", "content": null}, {"title": "C.3 PolyLM-13B", "content": null}, {"title": "G Results for the interventions on random neurons", "content": null}, {"title": "G.1 Top-1 paraphrase retrieval accuracy after the intervention on random neurons", "content": null}]}