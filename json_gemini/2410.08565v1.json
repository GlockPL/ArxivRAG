{"title": "BAICHUAN-OMNI TECHNICAL REPORT", "authors": ["Yadong Li", "Haoze Sun", "Mingan Lin", "Tianpeng Li", "Guosheng Dong", "Tao Zhang", "Bowen Ding", "Wei Song", "Zhenglin Cheng", "Yuqi Huo", "Song Chen", "Xu Li", "Da Pan", "Shusen Zhang", "Xin Wu", "Zheng Liang", "Jun Liu", "Tao Zhang", "Keer Lu", "Yaqi Zhao", "Yanjun Shen", "Fan Yang", "Kaicheng Yu", "Tao Lin", "Jianhua Xu", "Zenan Zhou", "Weipeng Chen"], "abstract": "The salient multimodal capabilities and interactive experience of GPT-40 highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.", "sections": [{"title": "1 Introduction", "content": "The burgeoning field of artificial intelligence has witnessed a remarkable evolution, especially with the development of Large Language Models (LLMs) [1, 8, 105] and the subsequent emergence of Multimodal Large Language Models (MLLMs) [46, 63, 93], signifying a paradigm shift in how machines understand and interact with the world. The introduction of MLLMs like GPT-40 [63], characterized by their exceptional multimodal capabilities and enriched interactive experiences, has not only spotlighted the indispensable role of these technologies in real-world applications but also set a new benchmark for what is achievable in terms of human-computer interaction.\nDespite the remarkable progress of MLLMs, current open-source solutions exhibit notable deficiencies, particularly in multimodal capabilities and the quality of user interaction experiences [24]. These shortcomings significantly impede the broader adoption and effectiveness of such models in diverse applications, from natural language processing [18, 68] to computer vision [84, 73] and beyond.\nIn response to these challenges, we introduce an omni-modal LLM Baichuan-Omni alongside a multimodal training scheme designed to facilitate advanced multimodal processing and naturalistic user interactions. The architecture of Baichuan-Omni is depicted in Figure 2. The scheme of Baichuan-Omni is built upon three core components:"}, {"title": "Omni-Modal Data Construction.", "content": "We utilize a substantial collection of high-quality, omni-modal data to train Baichuan-Omni with a blend of open-source, synthetic, and internally annotated datasets. In the multimodal alignment pre-training phase, we curate a wide-ranging assortment of training corpora that encompasses image captions, interleaved data, OCR data, and image-text data. For audio alignment, we collect both open-source and in-house datasets for Automatic Speech Recognition (ASR) and Audio Question Answering (AQA). In the realm of video alignment, we acquire video data from both open-source and in-house sources. During the multimodal supervised fine-tuning phase, we compile and synthesize an extensive dataset that covers over 200 tasks and comprises 600,000 instances across pure text, audio, image-text, video-text, and image-audio interaction data."}, {"title": "Multimodal Alignment.", "content": "During the pre-training phase for multimodal alignment, we meticulously align encoders and connectors across various modalities. Initially, we train the vision-language model using a substantial dataset of image-text pairs. This foundational training enables us to harness the visual capabilities developed during the image-text training to further train the video projector. Concurrently, we train the audio-language model utilizing Automatic"}, {"title": "Multitask Fine-tuning.", "content": "For the omni-modal fine-tuning stage, we utilize a multi-task cross-modal interaction training corpus derived from a combination of open-source, synthetic, and internally annotated data. We select data for the final supervised fine-tuning (SFT) phase based on criteria that whether factual knowledge is already learned by the pre-trained model [27]. During this phase, we implement a packing technique to concatenate multiple samples, using the cuseq_len from flash-attention2 for effective sample isolation. Through this technique, multiple samples can be packaged into a large batch while ensuring that each sample is correctly isolated during the computational process, preventing data confusion between different samples. This approach accelerates the training process and optimizes memory usage."}, {"title": "3 Training", "content": ""}, {"title": "3.1 High-Quality Multimodal Data", "content": "For training an omni-modal model with strong ability, we build an extensive cross-modal dataset with high quality, including text, image-text, video-text, audio-text, and their interactions."}, {"title": "Image Data.", "content": "Image data can be categorized into several types: Caption, Interleaved image-text, OCR data and Chart data [35]. From the perspective of sources, it is divided into Open-source data and Synthetic data. Regarding"}, {"title": "Video Data.", "content": "Video dataset comprises a diverse array of publicly available resources, encompassing multiple tasks such as video classification, action recognition, and temporal localization. The video-text sources can be categorized into two main types: question-answering (QA) data and caption data.\nFor QA data, we incorporate: NExTVideo, introduced in LLaVA-NEXT [104] and ActivityNet-QA (Train split) [95]. Our caption data sources include ShareGPT4Video [10], a large-scale dataset that leverages GPT-4 to generate rich, contextual captions for videos, and WebVid [7]. To further enrich our dataset, we have employed GPT-40 to generate diverse captions for videos collected from YouTube.\nThe sampling ratio for each dataset within our compilation is carefully determined based on the relative sizes of these datasets. This strategic approach ensures a balanced representation of various video types, tasks, and domains in our final dataset."}, {"title": "Audio Data.", "content": "Considering the diversity of audio data, we extract audio from various media modalities, which includes different recording environments, languages, accents, and speakers. Guided by the principles in previous work [67], we posit that the variation in audio quality contributes to a robust speech understanding capability. To facilitate a more sophisticated classification and filtering procedure, we implemented a data processing pipeline comprising speaker voice recording, dialect recognition, accent recognition, sound effect detection, and quality assessment.\nTo enhance the quality of audio-text pairs derived from the dataset, we utilized an in-house ASR system along with several open-source models [67, 25, 75] to generate multiple transcript versions. These generated data are then refined through a model ensemble strategy for effective text filtering and error correction."}, {"title": "Text Data.", "content": "In handling text corpus, we collected data from various domains such as web pages, books, academic papers, code, etc.. Following the data processing protocols proposed in previous works [19, 55], we implemented a selection process to enhance the diversity and quality of the dataset. The diversity criterion ensures broad coverage of topics and linguistic styles in the training corpus, accommodating various applications. High-quality processing removes redundancy and noise from the text data, increasing knowledge density."}, {"title": "Cross-Modal Interaction Data.", "content": "To enhance the cross-modal interaction capabilities of our model, we synthesized a collection of visual-audio-text cross-modal interaction data, including both image-audio-text and video-audio-text datasets. For the image-text data, we segmented the textual data into a 1:3 ratio, converting the initial quarter of text into audio descriptions using text-to-speech (TTS) technology. Our audio encompasses 44 different timbres, ensuring a diversity of vocal tones. This setup is complemented by task prompts such as \u201cPlease listen to the following audio describing the content of the image. Your task is to supplement more information by integrating the image after listening\", aiming to predict the remaining three-quarters of the textual description. For the video-text data, we directly extracted the audio from the videos to serve as the cross-modal audio component."}, {"title": "3.2 Multimodal Alignment Pre-training", "content": "In this section, we will further illustrate the pre-training and alignment processes for the Image-Language, Video-Language, and Audio-Language branches."}, {"title": "3.2.1 Image-Language Branch", "content": "We utilize Siglip-384px [97] as the visual encoder, which processes a 384\u00d7384 image input and generates 182 tokens through a visual projector composed of a two-layer MLP and a 2\u00d72 convolution layer serving as the pooling layer. To scale the input to arbitrary resolutions while preserving the intricate details of high-resolution images, we adopt AnyRes [50], which splits the image into grids and concatenates the features of a down-sampled image to provide global context.\nThe training of our image-language branch is divided into three stages."}, {"title": "3.2.2 Video-Language Branch", "content": "Based on the visual capabilities acquired from the pre-training of the Image-Language Branch, we proceed to train the video projector using a frozen vision encoder (Siglip-384px, the same as that used in the Image-Language Branch) alongside an LLM (Large Language Model) backbone. This training process employs a low learning rate of 4e \u2013 6 to refine the alignment with the language modality.\nDuring the training phase, the input video frames are sampled at a rate of 1 frame per second, with a maximum of 48 frames per video. Each input frame is resized to a maximum resolution of 384\u00d7768 pixels to maintain optimal quality and detail. Furthermore, a 2\u00d72 convolution layer is applied prior to the video projector. This convolutional step serves to regulate the length of the video token sequence, ensuring a minimum of 182 tokens and a maximum of 546 tokens. This thoughtful configuration strikes a balance between performance and efficiency, facilitating effective model training while managing the computational load.\nRather than immediately proceeding with the pre-training of the Video-Language Branch using only pure video-text pairs, we have opted for a more nuanced two-stage approach. Initially, we leverage image-text pre-training data to strengthen the model's visual understanding capabilities. After establishing a robust foundation, we incrementally integrate mixed image-text pairs and video-text pairs into the training regimen. This strategy has proven to yield superior results. By gradually enhancing the model's visual competence, we provide valuable guidance for the video pre-training pipeline, allowing the model to better understand and integrate the complexities of video data in conjunction with language. This methodology underscores the importance of a comprehensive training strategy that incorporates diverse data modalities for improved alignment and performance."}, {"title": "3.2.3 Audio-Language Branch", "content": "The Audio-Language branch extends an LLM pre-trained on visual and video data by incorporating an audio encoder from the Whisper-large-v3 model [67] and a newly introduced audio projector.\nThe audio encoder processes the audio signal (30s, 128 mel-spectrum) into an audio representation in a 1280-channel feature space, while the audio projector (typically a linear projector [14] or MLP) maps that to the embedding space of LLM. Prior to projection, a pooling operation with a stride of n is traditionally used to down-sample the audio representation into fewer tokens (i.e., frames) for the downstream LLM. However, when we aggressively reduce the number of audio tokens, this simple pooling approach leads to a loss of audio information. In our approach, we replace the pooling with Convolutional-Gated MLP (Conv-GMLP), leveraging convolution layers for down-sampling to preserve more audio information.\nFigure 5 illustrates the Conv-GMLP architecture, which functions similarly to a gated MLP [49] but replaces linear layers with convolutional ones. Each of the two convolutional layers reduces the sequence length of the audio representation by a factor of $n$, while proportionally expanding the feature space. In our projector, a residual shortcut is along with Conv-GMLP, enabling more efficient gradient back-propagation. Results in Section 4.5.3 demonstrate strong robustness in audio performance when setting the down-sampling rate\u00b3 $n$ aggressively.\nDuring training, the LLM remains frozen, and only the audio encoder and projector are trained using long audio-text sequences (up to 4K tokens). A cosine learning rate scheduler is employed to enhance performance."}, {"title": "3.2.4 Image-Video-Audio Omni-Alignment", "content": "The right part of Figure 4 illustrates the 'Omni-Alignment' stage, which follows the individual training of the Image-Language, Video-Language, and Audio-Language branches. During this stage, all modules are trained together on a mixture of high-quality image-text, video-text, and audio-text pairs to develop comprehensive multimodal understanding."}, {"title": "3.3 Multimodal Supervised Fine-Tuning", "content": "In this section, we describe the multimodal supervised fine-tuning process aimed at improving the model's ability to follow complex, multimodal instructions across various tasks. We leveraged a diverse set of open-source, synthetic, and internally annotated data, covering over 200 distinct tasks and comprising approximately 600K pairs across text, audio, image-text, video-text, and image-audio modalities."}, {"title": "Text-only Data.", "content": "The text-only data covers a broad range of tasks, including knowledge-based question answering, mathematics, logical reasoning, code generation, text creation, information processing, persona-based tasks, and safety-related data. To further strengthen the model's ability to handle complex, multi-step tasks, we included specialized datasets that feature intricate instructions, some of which contain a system message designed to structure more elaborate scenarios."}, {"title": "Image Understanding Data.", "content": "For tasks involving image understanding, we primarily utilized the vFLAN dataset [9], focusing on its instruction-following data. Given the quality issues present in some of the samples, we employed a loss-based filtering method to clean the dataset:\n1. We computed the loss for all vFLAN English instruction samples using the pretrained model and fit the resulting values to a Gaussian distribution.\n2. Samples are removed if their loss values fell outside the range of $\\mu \\pm \\sigma$.\n(a) Samples with loss < $\\mu \u2013 \\sigma$ typically included trivial issues, such as cases where the prompt and response content are nearly identical.\n(b) Samples with loss > $\\mu + \\sigma$ often had significant problems, such as reversed prompt-response pairs or hallucinations in the responses.\nA subset of the cleaned vFLAN instruction data is then translated into Chinese, followed by manual re-annotation to ensure high-quality alignment. Alongside vFLAN, we incorporated several other open-source datasets, including synthdog-en/zh [37], handwritten OCR, street view OCR, reference grounding and grounded captioning duality tasks, and ImageInWords [26]. Most of these datasets are translated into Chinese. For ImageInWords, we ensured that if an image contained a recognizable entity, the corresponding caption explicitly referenced that entity by name (e.g., identifying a Samoyed dog by breed rather than simply labeling it as \u201cdog\u201d).\nAlthough vFLAN covers 191 tasks, we found that it lacked variety in instruction types. To address this, we sampled data from our textual SFT dataset and rendered some of the prompts as images to increase the diversity of image-based instructions. Additionally, to enhance the model's mathematical reasoning with images, we used the method from [108] to generate a large dataset of multimodal math problems involving images."}, {"title": "Video Understanding Data.", "content": "The video-text data is primarily sourced from the VideoInstruct100K dataset [57]. While each video in the dataset includes multiple instructions, the instructions tend to be relatively homogeneous, often focusing on simple video descriptions. To enhance the diversity of video-based tasks, we applied semantic deduplication to the instructions for each video and translated the dataset into Chinese, enriching the variety of video-based tasks for the model."}, {"title": "Audio Understanding Data.", "content": "Most of the audio data is generated using TTS 4, with prompts derived from text-only, image-text, and video-text datasets. To ensure the quality of the synthesized audio, we transcribed the generated audio using an ASR model and compared the transcriptions with the original prompts. Only those audio samples with accurate transcriptions are retained as final audio prompts. To further enrich the audio data, we included human-recorded audio samples that captured various dialects, accents, and background noises.\nIn addition to the general QA tasks, we also constructed a specific ASR dataset sourced from open-source data and internal logs. To improve training efficiency, we filtered out easily recognizable samples, focusing instead on more challenging audio data for supervised fine-tuning."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Language Performance", "content": ""}, {"title": "4.1.1 Evaluation Benchmarks", "content": "We perform evaluations on 4 comprehensive benchmarks, including MMLU [31], CMMLU [42], AGIEval [107] and C-Eval [33]. MMLU includes 57 unique tasks consisting of multiple-choice questions across various fields of knowledge, including the humanities, social sciences, and hard sciences. CMMLU represents an extensive assessment framework tailored to assess the sophisticated knowledge and reasoning capabilities of LLMs within the context of Chinese language and culture. AGIEval is a human-centric benchmark crafted to evaluate the foundational models' general cognitive and problem-solving abilities, based on official, public, and qualification tests designed for human participants. C-Eval provides the comprehensive Chinese evaluation suite designed to evaluate the advanced knowledge and reasoning skills of LLMs within a Chinese context, encompassing 13,948 multiple-choice questions across 52 varied disciplines, from humanities to science and engineering. We conduct all the evaluations with a zero-shot measurement."}, {"title": "4.1.2 Major Performance", "content": "We compare Baichuan-Omni with state-of-the-art proprietary multimodal models such as Gemini 1.5 Pro [69], and GPT-40 [63], as well as a series of competitive open-source LLMs and MLLMs such as VITA [24], MAP-Neo [101], Qwen1.5-Chat [5], Llama3-Instruct [3] and OLMO [29]. We list major results on comprehensive benchmarks in Table 1.\nAs shown in Table 1, our Baichuan-Omni significantly outperforms open-source, general pure-text LLMs in comprehensive benchmarks. Compared to the open-source multimodal model VITA, Baichuan-Omni demonstrates a substantial advantage in Chinese benchmarks, such as CMMLU (72.2% v.s 46.6%) and C-Eval (68.9% v.s 56.7%), and slightly surpasses VITA in AGIEval (47.7% v.s 46.2%)."}, {"title": "4.2 Image Understanding", "content": ""}, {"title": "4.2.1 Evaluation Benchmarks", "content": "We evaluate Baichuan-Omni on 13 representative vision-language benchmarks, including MMBench-EN, MMBench-CN [52], M3GIA [74], SEEDBench [41], RealWorldQA [87], MMMU [96], MathVista [56], MME [22], MMVet [94], TextVQA [72], OCRBench [53], ChartQA [60], and HallusionBench [30]. To ensure reproducible evaluation results, we use VLMEvalKit [20] uniformly for all evaluations. All evaluations are conducted in a zero-shot manner, adhering to the original setup of the models to ensure fair and consistent comparisons across all models and benchmarks."}, {"title": "4.2.2 Major Performance", "content": "We compare Baichuan-Omni with state-of-the-art proprietary multimodal models such as Gemini 1.5 Pro [69], and GPT-40 [63], as well as a series of competitive open-source multimodal models such as VITA [24] and Qwen2-VL [79]. We list major results on VQA (Visual Question Answering) benchmarks and results on MCQ (Multi-choice & Yes-or-No Question) benchmarks in Table 2 and Table 3.\nAs shown in Table 2 and Table 3, our Baichuan-Omni comprehensively outperformed VITA-8*7b [24], which has 12B activated parameters, in multiple visual tasks, both on VQA benchmarks and MCQ benchmarks. Besides, we also demonstrates competitive performance comparable to, or even better than, open-source image-specialized models like MiniCPM-Llama3-V 2.5 [92]. Specifically, Baichuan-Omni outperformed MiniCPM-Llama3-V 2.5 on most VQA tasks, including MMBench-CN, SEED-IMG, MME, HallusionBnech and MMMU which requres expert-level perception and reasoning. However, despite the advantage of incorporating an additional audio modality compared to Qwen2-VL [79], the performance gap between our model and Qwen2-VL in image tasks remains evident. Furthermore, it is worth noting that, beyond Qwen2-VL, the stark divide between open-source and closed-source models remains substantial."}, {"title": "4.3 Video Understanding", "content": ""}, {"title": "4.3.1 Evaluation Benchmarks", "content": "We perform a thorough evaluation on general video understanding tasks (General VQA) and open-ended video question answering (Open-ended VQA) to comprehensively assess the video understanding capabilities of Baichuan-Omni .\nFor general video understanding tasks, we select Perception-Test [65], MVBench [44], VideoMME [23], and EgoSchema [58] for long-form video-language understanding. We report top-1 accuracy for all benchmarks. For VideoMME, we report the results under the setting of \"w/o subs\". For open-ended video question answering part, we choose ActivityNet-QA [95] and MSVD-QA [88] as evaluation benchmarks. Following previous work [57], we utilize GPT to assess the quality of the response snippets. Specifically, we use GPT-3.5-Turbo to provide a \"Yes-or-No\" decision on the correctness of answers and a rating scaled from 0 to 5. We report the percentage of \"Yes\" responses as Accuracy and the average rating as Score.\nWe conduct all evaluations in a zero-shot way while avoiding the use of elaborate prompts. Besides, we follow the original setup of the models to be reproduced regarding the (maximum) number of frames, frame sampling rate, etc. they applied, ensuring fair and consistent comparisons across all models and benchmarks."}, {"title": "4.3.2 Major Performance", "content": "We compare Baichuan-Omni with state-of-the-art multimodal proprietary models such as Gemini 1.5 Pro [69], GPT 4V [62], and GPT 40 [63], and a series of competitive open-source multimodal models such as VITA [24], Qwen2-VL [79], AnyGPT [98], VideoLLaMA 2 [13], VideoChat2 [44], LLaVA-NeXT-Video [104], and Video-LLaVA [48]. We list major results on general video understanding benchmarks in Table 4 and results on open-ended video question answering in Table 5.\nResults on general video understanding benchmarks. As shown in Table 4, Baichuan-Omni demonstrates competitive results over proprietary models on benchmarks like Egoschema and MVBench, and achieves strong performance across open-source multimodal models, which shows comprehensive video understanding capabilities of Baichuan-Omni .\nCompared to VITA, a MoE omni-modal LLM with about 12B activated parameters, Baichuan-Omni (7B) outperforms it on all General Video QA benchmarks, and achieve an average improvement of about 4%. Additionally, Baichuan-Omni excels a series of open-source models such as VideoLLaMA 2, VideoChat2, LLaVA-NeXT-Vide, and Video-LLaVA. Notably, Baichuan-Omni also outperforms the proprietary model GPT 4V on MVBench (43.7%) and Egoschema (55.6%).\nResults on open-ended video question answering benchmarks. The performance on Open-ended VQA is listed in Table 5. Baichuan-Omni demonstrates SoTA performance (both Accuracy and Score) on ActivityNet-QA and MSVD-QA across all open-source models, such as the most recent SoTA multimodal models VITA and Qwen2 VL, and outperforms the proprietary model Gemini 1.5 Pro (56.7%) on ActivityNet-QA. The superior results indicate that Baichuan-Omni is also effective in open-ended question answering, i.e., Baichuan-Omni is more capable of generating informative and descriptive responses."}, {"title": "4.4 Audio Understanding", "content": ""}, {"title": "4.4.1 Evaluation Benchmarks", "content": "To validate the audio understanding capacity of Baichuan-Omni, we present the evaluating results on benchmarks with three tasks:\n\u2022 Automatic Speech Recognition (ASR). This is a fundamental task for audio-language model pre-training which directly transcribes the audio into the text. For ASR evaluation in the general scene, we report results on the Fleurs [16]"}, {"title": "4.5 Ablation Study", "content": ""}, {"title": "4.5.1 Image-Language Branch", "content": "Visual encoder. To compare the performance of different visual encoders in Baichuan-Omni , we conducted experiments across various vision encoders with differing parameter sizes, input resolutions, and output token counts. We selected five mainstream vision encoders: OpenAI's CLIP series [66], Google's Siglip series [97], Apple's DFN series [21], OpenGVLab's InternViT series [11], and BAAI's EVA series [76], totaling 14 models. All models are trained with contrastive learning, with parameters ranging from 300M (ViT-L) to 18B. The training data used during the pre-training of the visual encoders varied from 400M to 10B, with input resolutions spanning from 224x224 to 448\u00d7448 and output token counts from 256 to 1024. All comparative experiments are conducted under the same experimental conditions, specifically using a batch size of 8 and the same data for IFT training (with a data ratio of Caption: Interleaved data: Pure text set at 0.45: 0.45: 0.1)."}, {"title": "4.5.2 Video-Language Branch", "content": "For video modality, we conduct an ablation study from three perspectives to thoroughly investigate the impact of various factors on model performance.\nNumber of frames. Within the constraints of the context length, we systematically adjust the frame sampling rate to control the maximum number of input video frames.\nResolution of vision encoder. We explore the effect of different vision encoder resolutions on the model's ability to extract meaningful visual features. Our investigation spans from fixed resolutions (such as 384 \u00d7 384 pixels) to dynamic resolution approaches like AnyRes.\nVideo-language pre-training. We evaluate the model's performance both with and without video-language pre-training. This comparison helps us quantify the benefits of leveraging large-scale multimodal datasets for pre-training, potentially enhancing the model's ability to understand video-text relationships and generalize across various video-understanding tasks."}, {"title": "4.5.3 Audio-Language Branch", "content": "The audio projector in the audio-language branch plays a key role in bridging the representations of audio and natural language modalities. Notably, our newly introduced projector with Conv-GMLP demonstrates the great performance robustness of the feature down-sampling rate.\nFor analysis, we measure the average WER on all our ASR benchmarks across Fleurs, WenetSpeech and KeSpeech by training a 1.5B audio-language model with three different down-sampling rates 2, 4, 8. To simulate the actual training of the audio branch in Baichuan-Omni, we only train our audio encoder and projector while keeping the LLM frozen. This setup is consistent with the configuration described in Section 3.2.3.\nFrom the Figure 6, we observe that when the down-sampling rate is set to 2, the audio-language model achieves the best ASR performance, with an average WER of 7.7%. When the down-sampling rate is adjusted to 4 and 8, there is a slight degradation in ASR performance, but the decrease is minimal (ranging from 0.3% to 0.6%). Surprisingly, despite the greater degree of down-sampling, the model with a rate of 8 outperforms the one with a rate of 4 (8.0% vs. 8.3%). This highlights the exceptional sequence compression capability of the Conv-GMLP."}, {"title": "4.5.4 Multimodal Supervised Fine-Tuning", "content": "Table 11 and Table 12 compare the performance of Baichuan-Omni on various image and video benchmarks with and without multimodal supervised fine-tuning (SFT). The results indicate that the model exhibits superior overall performance after undergoing multimodal SFT compared to the version that only undergoes instruction fine-tuning (IFT). This improvement can be attributed to the use of high-quality, diverse instructions and our SFT data construction method, which avoid compromising the base model's capabilities. (See Section 3.3 for more details.)"}, {"title": "5 Conclusion", "content": "In this work, we have open-sourced Baichuan-Omni as a step toward developing a truly omni-modal LLM that encompasses all human senses. With omni-modal pretraining and fine-tuning using high-quality omni-modal data,"}]}