{"title": "Efficient Deployment of Large Language Models on Resource-constrained Devices", "authors": ["Zhiwei Yao", "Yang Xu", "Hongli Xu", "Yunming Liao", "Zuan Xie"], "abstract": "Deploying Large Language Models (LLMs) on resource- constrained (or weak) devices presents significant challenges due to limited resources and heterogeneous data distribution. To address the data concern, it is necessary to fine-tune LLMs using on-device private data for various downstream tasks. While Federated Learning (FL) offers a promising privacy- preserving solution, existing fine-tuning methods retain the original LLM size, leaving issues of high inference latency and excessive memory demands unresolved. Hence, we de- sign FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning (PEFT) with structured pruning for ef- ficient deployment of LLMs on resource-constrained devices. Specifically, FedSpine introduces an iterative process to prune and tune the parameters of LLMs. To mitigate the impact of device heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed to adaptively determine different prun- ing ratios and LoRA ranks for heterogeneous devices without any prior knowledge of their computing and communication capabilities. As a result, FedSpine maintains higher inference accuracy while improving fine-tuning efficiency. Experimen- tal results conducted on a physical platform with 80 devices demonstrate that FedSpine can speed up fine-tuning by 1.4\u00d7- 6.9\u00d7 and improve final accuracy by 0.4%-4.5% under the same sparsity level compared to other baselines.", "sections": [{"title": "Introduction", "content": "The recent development of Large Language Models (LLMs), such as OpenGPT and LLaMA, marks a significant milestone in advancing Artificial Intelligence (AI) [1]. As computational capabilities on end devices continue to improve, leading tech- nology companies, including Apple and OpenAI, have intro- duced LLM-powered and on-device AI applications, such as Apple Intelligence [2] and ChatGPT mobile apps. These appli- cations are usually driven by a small model (e.g., with around 3 billion parameters) for on-device inference, and a much larger server-side model (e.g., the 175 billion parameter GTP- 3 model) for more complicated tasks. The small on-device model is usually pre-trained or distilled from a larger model on the server-side with in-cloud centralized data. However, the heterogeneous and context-specific data generated on end devices often exhibits distributional shifts from the data on which LLMs are pre-trained, leading to reduced inference accuracy. Besides, the limited resources (e.g., computational and memory resources) of the \u201cweak\u201d devices inevitably re- strict the sizes of on-device models. Therefore, the data and resource properties hinder the deployment of LLMs on weak devices for effective and efficient inference.\nTo address the data concern, fine-tuning LLMs with on- device data helps the deployed models to better align with the unique requirements of downstream tasks, thereby en- hancing task-specific performance and adaptability. However, growing privacy concerns and stringent regulations, such as GDPR [3], restrict the collection and sharing of personal raw data, making it challenging to gather distributed data from end devices to centralized servers for LLM fine-tuning. To break this barrier, Federated Learning (FL) has emerged as a privacy-preserving distributed machine learning technique that orchestrates end devices to collaboratively train/fine- tune models without exposing their raw data [4, 5]. When it comes to LLMs, directly conducting full parameter fine- tuning with FL is challenging due to the limited resources on the devices. For instance, fine-tuning a LLaMA model with 13 billion parameters requires approximately 100GB of memory, while most end devices have only 4-12GB of mem- ory available [6]. To cope with the resource issue, integrating Parameter-Efficient Fine-Tuning (PEFT) techniques within FL presents a compelling solution. PEFT methods, such as Adapter [7] and LoRA [8], aim to freeze the pre-trained LLM parameters and update only a few additional parameters (typi- cally less than 1% of total LLM parameters) [9], facilitating the implementation of fine-tuning LLMs on weak devices.\nWhile PEFT methods effectively address resource con- straints during the fine-tuning process, the resulting mod- els often retain or even increase their original size due to the introduction of additional parameters. This will lead to significant inference latency or even unresponsiveness on"}, {"title": null, "content": "resource-constrained devices, as the computational and mem- ory demands during inference remain high. To alleviate these issues, model pruning has emerged as a promising technique to reduce the number of model parameters and computational footprint. Pruning techniques can be classified into unstruc- tured pruning and structured pruning. Unstructured pruning produces highly sparse models [10, 11] but hardly provides inference speedup without specialized hardware support. In contrast, structured pruning [12-15], which removes groups of structured parameters such as attention heads and feed- forward neurons (FFN), significantly decreases model size, improves inference speed and lowers memory usage. In this work, we focus on adopting a structured pruning approach.\nIntuitively, integrating PEFT and structured pruning of- fers promise for enhancing both accuracy and efficiency of on-device LLM inference. However, recent studies [12, 16] indicate that directly combining PEFT with structured prun- ing will lead to significant performance degradation, as the two techniques have not been jointly optimized. For exam- ple, CPET [16] applies structured pruning to models fine- tuned with LoRA, while LLMPruner [12] adopts a sequen- tial approach: first pruning the model, and then using LoRA fine-tuning to restore performance. However, this separa- tion of pruning and fine-tuning results in suboptimal per- formance [11, 17], as verified in \u00a72.2. To address these lim- itations, APT [15] introduces an adaptive framework that iteratively prunes and fine-tunes the parameters of LLMs, achieving significant improvements in both fine-tuning and inference performance. However, these studies including APT are specialized and implemented for centralized scenarios in the cloud, which raises significant privacy concerns due to the need for data collection from end devices. Moreover, they al- ways incur substantial computational and memory overheads, which cannot be directly applied to end devices considering their limited resources.\nTo this end, we propose FedSpine, a novel and the first (to our best knowledge) FL framework that combines struc- tured pruning and PEFT to enhance inference efficiency of on-device LLMs. Specifically FedSpine introduces an itera- tive process to prune and fine-tune model parameters. In each round, devices download the updated LoRA weights from the server and perform local pruning on the frozen model. After- ward, the pruned model is fine-tuned on local data and the updated weights are sent back to the server for aggregation. However, apart from resource limitation, one critical issue complicates the design of FedSpine, i.e., device heterogeneity. Specifically, end devices are typically equipped with different computational chips and located in diverse regions, causing their computational and communication capabilities to vary significantly, even by more than tenfold times [18, 19]. If iden- tical pruning and fine-tuning settings, such as uniform prun- ing ratios and LoRA ranks, are assigned across devices, those with weak capabilities may become system stragglers [20], resulting in considerable competition time lags and decreased"}, {"title": null, "content": "fine-tuning efficiency. Thus, it is worthwhile to design effec- tive strategies to mitigate the impact of device heterogeneity.\nTo cope with these issues, FedSpine adaptively assigns ap- propriate pruning ratios and LoRA ranks for devices in each round to mitigate the straggler effect caused by device hetero- geneity. Regarding the observations in \u00a72.4, the potential ac- curacy degradation incurred by structured pruning of different ratios could be compensated by adaptive assignment of LoRA ranks. Higher LoRA ranks help to achieve accuracy gains but come with the cost of increased resource overhead (e.g., memory usage and communication time). Conversely, lower LORA ranks reduce resource consumption but may compro- mise accuracy recovery. Therefore, FedSpine should jointly determine proper pruning ratios and LoRA ranks across de- vices through iterative optimization so as to maximize the fine-tuning efficiency and minimize accuracy loss.\nIn a nutshell, our work makes the following contributions:\n\u2022 We propose FedSpine, an innovative federated fine- tuning framework that enhances both inference accuracy and efficiency of on-device LLMs. This framework inte- grates adaptive pruning on the frozen model and LORA ranks on the LORA modules to effectively address the challenges posed by device heterogeneity.\n\u2022 FedSpine employs a Multi-Armed Bandit (MAB) based online learning algorithm to adaptively learn the quan- titative relationship between resource consumption and model performance. The algorithm determines proper pruning ratios and LoRA ranks for heterogeneous de- vices without any prior knowledge of their capabilities.\n\u2022 We conduct comprehensive experiments to evaluate the performance of FedSpine on a physical platform con- sisting of 80 NVIDIA Jetson devices. The results show that FedSpine speeds up fine-tuning by 1.4\u00d7-6.9\u00d7 and improves accuracy by 0.4%-4.5% compared to baselines."}, {"title": "Preliminaries and Motivations", "content": "LLMs have demonstrated remarkable capabilities in language understanding and generation. Before deployment, two crit- ical steps are involved: 1) pre-training LLMs from scratch with extensive computational resources and tremendous text corpora; and 2) fine-tuning pre-trained LLMs to fit various downstream tasks. With the utilization of on-device data, fine- tuning allows LLMs to align more closely with specific task requirements, thereby enhancing task-specific performance.\nAs depicted in Figure 1, full parameter fine-tuning (Full-FT) for ROBERTa on SST-2 can improve accuracy of 2%-3% compared to direct deploying LLMs on end devices without fine-tuning (w/o FT)."}, {"title": "Impact of Device Heterogeneity", "content": "Due to device heterogeneity, computational and communi- cation time will vary significantly across devices in each round. When identical pruning ratios are adopted, devices with limited memory and computational capabilities become bottlenecks during model fine-tuning. This forces strong de- vices to wait for weak ones (also called stragglers) for global LORA aggregation, significantly reducing fine-tuning effi- ciency. Moreover, most PEFT-based federated fine-tuning schemes [9, 22, 23] assign identical and fixed LoRA ranks to all devices, overlooking the impact of device heterogeneity.\nTo evaluate the negative impacts of device heterogene- ity, we conduct experiments with FedAPT for fine-tuning ROBERTa on MNLI [24] using 80 NVIDIA Jetson devices. Specifically, we select three types of devices (i.e., AGX, TX2, and NX devices) and configure them to operate in different modes and network bandwidths to simulate heterogeneous computational and communication capabilities (see \u00a75 for more details). We record the ranked completion time of all devices in a round, with identical LoRA ranks (i.e., 8) and pruning ratios (i.e., 0.2) assigned. As shown in Figure 2(c), there is a significant disparity in computational and commu- nication time across devices. The strongest device requires only 70% of the time taken by the weakest device to complete one round, resulting in approximately 30% of the strongest device's time being idle and inefficiently utilized."}, {"title": "Motivations for Adaptive Pruning and Fine-tuning", "content": "To further explore the effect of different pruning ratios and LoRA ranks on fine-tuning, we conduct a set of pre- experiments using FedAPT for fine-tuning the ROBERTa model on MNLI over 100 rounds. The fine-tuning processes are presented in Figure 2(d) and we can observe the key findings as follows. 1) Larger pruning ratios can lead to per- formance degradation and slower model convergence. For example, increasing the pruning ratio from 0.2 to 0.3 results in a 1.4% drop in accuracy and requires additional time for convergence. 2) Larger LoRA ranks help recover model per- formance but come with more per-round time consumption. Under the same pruning ratio of p = 0.3, models with a larger LoRA rank (r = 32) achieve 0.9% higher accuracy but re- quire approximately 23.3% more total time across all rounds compared to models with a lower LoRA rank (r = 8).\nThe above results highlight the importance of carefully determining pruning ratios and LoRA ranks, underscoring the need for joint optimization. To deal with device hetero- geneity and minimize the straggler effects during fine-tuning, it is essential to dynamically assign different pruning ratios and LoRA ranks based on the memory, computational and communication capabilities of heterogeneous devices. How- ever, determining the optimal pruning ratios and LoRA ranks remains challenging, as it demands balancing the inference accuracy and fine-tuning efficiency, which is the primary ob- jective of our method."}, {"title": "System Overview", "content": "In this section, we introduce the overall workflow of Fed- Spine (as illustrated in Figure 3). The working procedure of FedSpine typically involves a certain number of rounds. Each round contains five main stages to \"refine\" the LLMs as follows, which are performed iteratively until the on-device LLMs are progressively pruned to the target pruning ratio.\n\u2460 Configuration Update. At the beginning of round t (t\u2265 1), the server utilizes the state information collected in the previous round (e.g., local loss changes) to adaptively determine the refining configurations (i.e., LoRA ranks and pruning ratios) for each device in the current round(\u00a74.1).\n\u2461LORA Distribution. The server distributes the LORA weights updated in the previous round, and the refining con-"}, {"title": "FedSpine Design", "content": "Herein, we will elaborate on the technical details of four criti- cal stages, i.e., Configuration Update, Model Pruning, Model Fine-tuning, and LoRA Aggregation."}, {"title": "Algorithm for Configuration Update", "content": "In this section, we first formalize the problem in FedSpine and then propose an online learning algorithm to adaptively determine the pruning ratios and LoRA ranks for devices."}, {"title": "Problem Definition", "content": "In FedSpine, there are N devices, each device $i \\in [1,N]$ keep- ing its local data $D_i$. For a frozen weight matrix $W \\in \\mathbb{R}^{d \\times k}$ of"}, {"title": null, "content": "LLMs on device i, LoRA constrains its update $ \\Delta W \\in \\mathbb{R}^{d \\times k}$ by representing it with a low-rank decomposition $W+\\Delta W = W+BA$, where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are two globally trainable parameters. Here, d and k denote the input and out- put dimensions of the frozen weight matrix, and the rank r satisfies $r < min(d,k)$. The completion time $T_i$ of device i in round t includes local computational time $T_{i,comp}$ and communication time $T_{i,comm}$, i.e., $T_i = T_{i,comp} + T_{i,comm}$, both influenced by the pruning ratios and LoRA ranks. The total latency for each round is determined by the \"weakest\" device, expressed as $max_i\\{T_i\\}$, resulting in idle time for stronger devices. The average waiting time across all devices is:\n$\\Gamma = \\frac{1}{N}\\sum_{i=1}^{N} max\\{T_i\\} - T_i$                                                                                                                                                                    \n(1)\nTo conquer these inefficiencies, the optimization problem in FedSpine aims to minimize the global loss F while ensuring that device parameters achieve a target pruning ratio $\\rho$ after T fine-tuning rounds and maintaining efficiency constraints:\n$\\underset{\\Delta W}{min} \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{|D_i|} \\sum_{d_i \\in D_i} F(W_i; \\Delta W_i; d_i)$                                                                                                                                                                                        \nsubject to$\\frac{size(\\bar{W})}{size(W)} < \\rho$, $\\Gamma < \\varepsilon$\n$\\forall t \\in [0, T], \\forall i \\in [1,N]$\n  (2)\nwhere $F_i$ is local loss function, $d_i$ is a batch of data samples in Di and \u03b5 is a positive constant close to zero. According to Eq. (2), a higher pruning ratio reduces computational and memory demands during inference but sacrifices inference accuracy. Conversely, fine-tuning with a larger LoRA rank requires more time and memory usage but accelerates model conver- gence with better performance. FedSpine balances inference accuracy and fine-tuning efficiency by assigning different pruning ratios and LoRA ranks."}, {"title": "Multi-Armed Bandit Based Algorithm", "content": "Due to the complex and varying nature of federated environ- ments (e.g., heterogeneous device capabilities), it is infeasible to predefine the optimal values of the pruning ratio and LoRA rank for all devices in each round. Therefore, we propose a Multi-Armed Bandit (MAB) based online learning algorithm to dynamically determine the model refining configurations, i.e., pruning ratios and LoRA ranks, for heterogeneous devices in real-time without any prior knowledge of their capabilities.\nGenerally, the optimization problem in Eq. (2) can be mod- eled as an MAB problem, where the server acts as the player, and the decisions of pruning ratio and LoRA rank represents the arms. In each round t, the server decides which arm of the bandit is pulled, and a reward in Eq. (3) will be collected after the decision. The Upper Confidence Bound (UCB) policy is commonly used to address Multi-Armed Bandit (MAB) problems [25]. Traditionally, UCB was developed for discrete decision spaces. However, in FedSpine, the pruning ratios form a continuous range, varying between [0,1]. To overcome this, we extend the UCB policy to handle continuous arms and introduce the Smooth Upper Confidence Bound (S-UCB), as described in Algorithm 1.\nSpecifically, S-UCB creates agents for different devices and a decision tree is used to adaptively learn arm space partitions. Considering the progressive pruning process in FedSpine, the pruning ratio $p^t$ in round t must exceed the ratio $p^{t-1}$ from the previous round. In round t, each agent maintains a sequence of finite partitions of the arm space $S^0 = \\{S_1^t, S_2^t, ..., S_U^t\\}$ with $\\bigcup_{U=1}^US_i^t = [p^{t-1}, p_{target}] \\times [r_{min}, r_{max}]$, where 14 is the number of partition regions and may vary over time. These partition regions could be regarded as leaves in the decision tree. The MAB treats the problem as a finite-arm bandit problem with respect to the partition regions, and selects the partition region that maximizes the upper bound of a confidence interval for the expected reward. After determining the pruning ratio $p_i^t$"}, {"title": null, "content": "and LoRA rank $r_i^t$ for each device i, the chosen partition region is split further into multiple regions to form the new partition $S^{t+1}$, allowing the decision tree to grow adaptively as the algorithm proceeds. S-UCB halts the extension of the decision tree when the leaf diameters (region diameters) fall below a threshold 8, which defines the granularity of pruning ratio and LORA rank exploration. Upon receiving all LoRA modules from devices, the server can obtain the completion time $T_i^t$ of device i (\u2208 [1,N]). Then, the agent observes a reward from its interactive environment, which has a crucial impact on future decisions. The reward in round t can be defined as follows:\n$R(p,r) = \\frac{\\Delta F}{\\Delta T} - \\frac{AP_i(B_{total}^i)}{N}$                                                                                                                                                                                       \n(3)\nwhere AF indicates the decrease in local loss during round t and $\\Delta T = T_i^t - \\frac{1}{N}\\sum T^t | represents the gap between the completion time of device i and the average completion time. A smaller gap implies that the chosen pruning ratio and LoRA rank better align with the device's capabilities, resulting in a higher reward. FedSpine aims to determine proper LORA rank for device i to obtain the LoRA module with higher importance, i.e., $I(B_{total}^i)$. The pruning granularity in round t is denoted by $\\Delta p = p^t - p^{t-1}$, then a larger granularity is more likely to lead to a higher final pruning ratio, thereby yielding greater inference gains.\nThus, S-UCB aims to maximize the total received rewards by carefully balancing exploration and exploitation. It em- ploys an exploration-exploitation strategy to select the most suitable arm from the partitioned space, and seeks to balance exploiting arms that have performed well in the past with exploring arms that may yield higher rewards in the future. The exploitation and exploration are defined as follows:\n(1)Exploitation. Let $N_t(\\lambda, S') = \\sum_{t'=1}^{t-1} \\lambda^{t-t'-1} \\{(p_{t'}^i,r_{t'}^i) \\in s'\\}$ record the number of times that the partition region $S_j^t$ is chosen, where $\\lambda \\in (0, 1)$ is a discount factor. The indicator function $1\\{(p,r)\\in s'\\}$ is 1 when the action (p,r) \u2208 S and 0 otherwise. The discounted empirical average is given by\n$\\bar{R_t} (\\lambda, S'_j) = \\frac{1}{N_t(\\lambda, S'_j)}\\sum_{s=1}^{t-1} \\lambda^{t-1-s}R(p_i^s, r_i^s) \\mathbb{1} \\{(p_i^s, r_i^s)\\in s'\\}$\n(4)\n(2) Exploration. If the agent always selects the pruning ra- tio and LoRA rank from the partition it currently considers the best, it may overlook another partition with a higher expected reward. To address this, S-UCB incorporates an exploration term into the upper bound. Let $n_t(\\lambda) = \\sum_{t'=1}^{t-1} N_t(\\lambda, S'_j)$ hold and the discounted padding function is defined as:\n$c_t (\\lambda, S'_j) = \\sqrt{\\frac{2 \\log n_t(\\lambda)}{N_t(\\lambda, S'_j)}}$\n(5)\nThe upper confidence bound in S-UCB is defined as:\n$U_t(S_j) = \\bar{R_t}(\\lambda, S'_j) + c_t(\\lambda, S'_j)$\n(6)"}, {"title": null, "content": "The region $S_j^t$ with the largest $U_t$ will be chosen. To achieve this, S-UCB adaptively learns the partitions and determines the pruning ratios and LoRA ranks for all devices without any prior knowledge. The performance of the arm-pulling policy is evaluated by regret, defined as the difference between the expected reward from selecting the optimal arms $(p^*,r^*)$ and the reward obtained by the given policy. The goal of S-UCB is to minimize the cumulative regret over T rounds.\n$\\underset{min}{T}\\sum_{t=1}^{T}\\mathbb{E}(R(p^*,r^*) - R(p_t,r_t))$\n(7)"}, {"title": "Model Pruning", "content": "To enhance the performance of LLM fine-tuning with hetero- geneous data, FedSpine employs a loss-based pruning strategy that adaptively prunes redundant LLM parameters for various downstream tasks. Given that the importance of LLM parame- ters varies across different downstream tasks, FedSpine needs to dynamically determine pruning ratios for heterogeneous devices, which will be elaborated in \u00a75. Once the specific pruning ratio $p_i$ is determined for device i in round t, the device performs model pruning on its local frozen weight W. For structured pruning of LLMs, the goal is to remove the weights with minimal impact on the model's predictions, as indicated by their effect on the loss function. Let $W_{mn}^i \\in W$ on device i represent the weight connecting the m-th input to the n-th output in local model's weight W. Then, the im- portance of $W_{mn}^i$ can be quantified by measuring its impact on the loss after removal. The induced error of $W_{mn}^i$ can be denoted as\n$\\mathcal{I}(W_{mn}^i) = (\\mathcal{F}(\\mathbf{W}) - \\mathcal{F}(\\mathbf{W}|W_{mn}^i = 0))^2$\n(8)\nComputing $\\mathcal{I}(W_{mn}^i)$ is computationally expensive. Hence, the first-order Taylor expansion could be used to approximate the importance of $I_{mn}^i$ following [17]:\n$\\mathcal{I}(W_{mn}^i) = (\\frac{\\partial \\mathcal{F}}{\\partial W_{mn}^i})^2$\n(9)\nHowever, in PEFT, gradients of frozen weights are not di- rectly accessible and only the gradients of LoRA weights are computed during model fine-tuning. APT [15] evalu- ates weight importance by calculating the product of acti- vations and their gradients, which demands substantial com- putational resources and memory. To this end, a LoRA-guided criterion [21] that leverages the gradients of LoRA is intro- duced to evaluate the importance of LLM weights effectively. By setting the weight $(BA)_{m,n}^i = -W_{mn}^i$ when the weight $W_{mn}^i \\in W$ is removed, we can estimate the importance of $W_{mn}^i$ using following gradient-based manner:\n$\\mathcal{I}_{G}(W_{mn}^i) = [(\\frac{\\partial \\mathcal{F}}{\\partial (B_i A_i)_{m,n}} +  (\\frac{\\partial \\mathcal{F}}{\\partial W_{m,n}^i} +  (\\frac{\\partial \\mathcal{F}}{\\partial W_{m,n}^i})) (W_{m,n}^i+ (BA)_{m,n}^i)]^2$\n(10)\nThis pruning criterion only requires the computation of gradients for A and B with the approximation in Eq. (10),"}, {"title": null, "content": "effectively circumventing the issue of inaccessibility of gradi- ents for the frozen LLM. Drawing inspiration from previ- ous work [12, 26], FedSpine prunes heads for the Attention layer and channels for the FNN layer, respectively. However, in structured pruning, it is essential to consider that pruned weights may exhibit dependencies with other weights due to their interconnected nature. Specifically, the weight de- pendency rule in FedSpine follows the approach in [12, 27]. Let the set of linear layers \u03a8 = {query,key,value,out} and \u03a6 = {ffn1, ffn2} (or \u03a6 = {gate,up,down} for LLaMA models) denote the weight dependency for Attention and FNN layers, respectively. We treat the connected weights within each model group as a unit, ensuring that all weights within the same group are pruned simultaneously. Note that pruning partial weights in a group is avoided as it may lead to misaligned intermediate representations [12]. Hence, we estimate the group importance by summing the importance of all weights within the same group. Then, the importance of the g-th (g \u2208 {1,...,G}) group of the l-th layer can be formally denoted as:\n$\\mathcal{I}^{l,g} = \\sum_{W_{mn}^i \\in G} \\mathcal{I}(W_{mn}^i)$\n(11)\nwhere$\\Theta_l \\in \\mathbb{R}^{1\\times G}$ represents the importance of groups, G denotes a set of connected weights within a group and G is the number of candidate groups in the l-th layer.\nAfter several fine-tuning iterations (e.g., \u03c4 = 20 in our ex- periment), we estimate the weight importance for each group by calculating the moving average with \u03b7 \u2208 [0, 1] as\n$\\hat{\\Theta}_l^{i,\\kappa} = \\eta\\hat{\\Theta}_l^{i,\\kappa-1} + (1 - \\eta)\\mathcal{I}^{l,g}$\n(12)\nwhere$\\Theta_l^k$ denotes the group importance scores at the k-th it- eration. After repeating \u03c4 iterations, we could obtain $\\hat{\\Theta}_l^{i,\\kappa}$ and then prune the unimportant groups based on the derived prun- ing ratio from our MAB-based algorithm in \u00a74.1.2. Specifi- cally, each device applies a binary mask $M \\in \\{0,1\\}^{1\\times G}$ for each pruned layer, and then selects the top p% unimportant groups and sets the corresponding binary masks $M_l^g$ to 0, while others are as 1. Regarding the binary masks, the for- ward propagation for each pruned layer can be denoted as $\\tilde{x} = (x + B_i A_i \\odot x) M_l$, where \u2299 denotes the Hadamard prod- uct between the masks and their corresponding matrices."}, {"title": "Model Fine-tuning", "content": "FedSpine is designed to dynamically assign LoRA ranks for heterogeneous devices during fine-tuning to enhance model performance. To balance the performance gains against the additional computational and communication costs, we adaptively assign larger LoRA ranks for stronger devices to improve model performance, while weaker devices receive smaller LoRA ranks to mitigate the straggler effect. As a result, we recover the performance of the pruned LLM on each device without slowing down model convergence. To"}, {"title": null, "content": "determine which devices should be allocated the larger LoRA ranks, FedSpine first calculates the importance of each de- vice's LoRA modules. Specifically, the importance of each LORA module is represented as the summation of the param- eter importance within matrix B, i.e., $\\mathcal{I}(B_i) = \\sum_{m,n}\\mathcal{I}(B_{m,n}^i)$1. We add LoRA modules into the linear layer \u03a9 = {\u03a8,\u03a6} within each LLM transformer layer. Thus, the importance of LORA modules within the l-th layer is calculated as follows:\n$\\mathcal{I}(B_l^i) = \\sum_{\\sigma \\in \\Omega} \\sum_{m,n} \\mathcal{I}(B_{m,n}^i)$\n(13)\nTo enhance the evaluation of LoRA module importance, we incorporate singular values into the assessment of the matrix B. Larger singular values indicate that the associated weights capture more critical information, thus we characterize the importance of weights using the average singular value of each trainable weight. The overall importance of the LORA module for device i can be formulated as:\n$\\mathcal{I}(B_{total}^i) = \\sum_l \\mathcal{I}(B_l^i) + \\frac{1}{d_1}\\sum_i \\frac{1}{d_2}\\sum_{\\lambda_{k}^{i}}$\n(14)\nwhere $\\lambda_{k}^{i}$ denotes the e-th singular value of the s-th LoRA module on device i, with d\u2081 and d2 representing the total number of LoRA modules and singular values in the s-th LoRA module, respectively. Efficient fine-tuning is achieved by increasing the LoRA rank of devices with higher $\\mathcal{I}(B_{total}^i)$ values. The LoRA rank r is determined using a MAB-based algorithm (see \u00a74.1.2). To ensure fine-tuning stability when increasing LoRA ranks and converting $B \\in \\mathbb{R}^{d \\times r_i}, A \\in \\mathbb{R}^{r_i \\times k}$ to $B^{t+1} \\in \\mathbb{R}^{d \\times r_i+1}, A^{t+1} \\in \\mathbb{R}^{r_i+1 \\times k}$, we concatenate zeros in B and random Gaussian initialized parameters $N(0,\\sigma^2)$ in A, following the LoRA initialization method. This ensures that the layer's output remains unchanged before and after new parameters are added. When smaller LoRA ranks are adopted for device i, the downloaded LoRA weights A and Bare shrunk using $A \\leftarrow A(\\frac{r_i}{r_i + 1}), B \\leftarrow B(\\frac{r_i}{r_i + 1})$, which reduces the LoRA weights to the smaller rank $r_i+1$."}, {"title": "LoRA Aggregation", "content": "FedSpine allows each device to upload LoRA modules B and A instead of the full model during the fine-tuning, which reduces the communication and aggregation overhead. How- ever, the heterogeneous LoRA modules with different ranks from all devices cannot be aggregated directly. A straightfor- ward way is using zero-padding to all the received LoRA mod- ules with r < max;{r{} and then performing simple averaging over the modules. However, such aggregation has been shown to bias the model toward higher-rank devices [28]. Therefore, before aggregation, FedSpine first reconstructs these LoRA modules to the full model as\n$\\Delta W = B_i A_i$\n1The importance scores calculated using B; and A; are equal.\n(15)"}, {"title": null, "content": "Then, the server sums the LoRA modules with aggrega- tion weights c which are proportional to the importance of devices' LoRA modules, i.e.,\n$\\Delta W = \\frac{1}{N} \\sum_{i=1}^{N} c_i \\Delta W_i$\n(16)\nwhere $c_i = \\mathcal{I}(B_{total}^i)/\\sum_{i \\in N}\\mathcal{I}(B_{total}^i)$. The updated global LoRA modules are stored in the server and will be distributed to par- ticipating devices for further fine-tuning, or be combined with pruned LLMs to perform inference for various downstream tasks. Alongside the LoRA aggregation process, the server should utilize the collected state information to determine the refining configurations for different devices in the next round, aiming to balance efficiency and accuracy."}, {"title": "Implementation and Evaluation", "content": "We have implemented FedSpine prototype with about 3k lines of custom code using FedPETuning [29", "30": "and Pytorch [31"}]}