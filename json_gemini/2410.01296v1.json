{"title": "SPECULATIVE CORESET SELECTION FOR TASK-SPECIFIC FINE-TUNING", "authors": ["Xiaoyu Zhang", "Juan Zhai", "Shiqing Ma", "Chao Shen", "Tianlin Li", "Weipeng Jiang", "Yang Liu"], "abstract": "Task-specific fine-tuning is essential for the deployment of large language models (LLMs), but it requires significant computational resources and time. Existing solutions have proposed coreset selection methods to improve data efficiency and reduce model training overhead, but they still have limitations: \u25cf Overlooking valuable samples at high pruning rates, which degrades the coreset's performance. \u25cf Requiring high time overhead during coreset selection to fine-tune and evaluate the target LLM. In this paper, we introduce STAFF, a speculative coreset selection method. STAFF leverages a small model from the same family as the target LLM to efficiently estimate data scores and then verifies the scores on the target LLM to accurately identify and allocate more selection budget to important regions while maintaining coverage of easy regions. We evaluate STAFF on three LLMs and three downstream tasks and show that STAFF improves the performance of SOTA methods by up to 54.3% and reduces selection overhead by up to 70.5% at different pruning rates. Furthermore, we observe that the coreset selected by STAFF at low pruning rates (i.e., 20%) can even obtain better fine-tuning performance than the full dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have shown great potential in solving various problems such as text summarization, machine translation, code generation (Van Veen et al., 2024; Roziere et al., 2023; Gong et al., 2024; Zhao et al., 2024). To achieve the best performance on downstream tasks, fine-tuning these foundational models on task-specific datasets is necessary. This process, known as the task-specific LLM fine-tuning, demands high computational resources and time because of the large size of LLMs and datasets. It leads to high carbon emissions, hurting the economy and environment (Faiz et al., 2024). To reduce the training overhead and improve data efficiency, researchers have proposed various data pruning and coreset selection methods, mainly consisting of data importance-guided methods and data diversity-guided methods (Maharana et al., 2024). Data importance-guided methods leverage scoring functions to evaluate the importance or difficulty of samples based on their training record and retain the most difficult or important samples to construct the coreset for DL models (Paul et al., 2021; Toneva et al., 2018). Data diversity-guided methods first divide or cluster the samples into different regions and areas based on their scores and then select samples across data regions to ensure that the coreset can represent different regions (Zheng et al., 2023; Maharana et al., 2024).\nHowever, existing coreset selection methods have limitations and are not good for task-specific LLM fine-tuning. On the one hand, existing methods are difficult in balancing data importance and diversity in coreset selection. As a result, they could ignore representative samples in regions with low scores at medium-high pruning rates, leading to catastrophic performance degradation of the selected coreset on the fine-tuned model (Zheng et al., 2023). Figure 1 a) provides a demo case that, when the pruning"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 CORESET SELECTION FOR TASK-SPECIFIC FINE-TUNING", "content": "The pre-trained LLMs are fine-tuned with different settings to be harmless or generalize to unseen tasks, such as instructional fine-tuning (Liu et al., 2023; Muennighoff et al., 2023; Lu et al., 2023; Wang et al., 2022) and preference fine-tuning (Ethayarajh et al., 2024; Pace et al., 2024; Kim et al., 2023; Cui et al., 2023). This paper focuses on task-specific fine-tuning that updates the model to adapt the target distribution of a given downstream task, which is essential for the deployment and application of LLMs (Yang et al., 2024; Lin et al., 2024). One-shot coreset selection for task-specific fine-tuning aims to remove redundant data and improve data efficiency. Consider a downstream task dataset D containing N samples $D = \\{(x_i, Y_i)\\}_{i=1}^N$ drawn i.i.d. from the distribution P. Coreset selection methods select a subset D' at the given prune rate p such that the loss of the LLM 0 trained on D' using loss function L is minimized on the test set drawn from P. The optimization problem can be expressed as:\n$\\min_{D'} Ex,y\\sim P [L(x, y; \\theta^*(D'))]$\n$D' \\subset D:|D'| \\leq (1-p)$"}, {"title": "2.2 SPECULATIVE EXECUTION", "content": "Speculative execution is an optimization strategy in the field of computer architecture (Patterson et al., 2011) to improve the efficiency of upcoming tasks. It mainly consists of the execution stage and verification stage, as shown in Figure 2. Specifically, consider an upcoming task $T : x \\mapsto Y$, where x is the task input and Y is the corresponding solution space. The execution stage utilizes limited spare resources to perform the speculative task Spec(.) and obtain a result z for the given\nOur code is available at https://anonymous.4open.science/r/STAFF-4323"}, {"title": "3 METHODOLOGY", "content": "Although existing data importance-guided and diversity-guided coreset selection methods have demonstrated good performance in scenarios such as deep learning (DL) training, they still face two major challenges in task-specific LLM fine-tuning. \u25cf Challenge 1: Effective coreset selection across all pruning rates. Existing selection methods can hardly balance data importance and diversity in selection, especially at higher pruning rates. As the pruning rate increases, they often ignore representative samples from regions with low importance scores, leading to catastrophic performance degradation, which is also observed in DL model training (Zheng et al., 2023). Figure 1 a) shows an intuitive case where existing selection methods can lead to performance degradation of up to 35.1% in the fine-tuned target LLM when the pruning rate increases from 20% to 90%. Ensuring the effective selection of diverse and important samples at both high and low pruning rates remains a significant challenge. \u25cf Challenge 2: Reducing overhead in coreset selection. Existing methods typically involve high time overhead for LLMs, as they require several epochs of training on the target model to evaluate data scores and regions during selection. While this process might be manageable for light-weight DL models, it becomes prohibitively expensive for LLM fine-tuning tasks, where large datasets and massive model parameters are involved. This can lead to a selection process taking dozens of GPU hours (Figure 1 b)), limiting the efficiency and applicability of these selection methods. Designing an efficient method to evaluate and select samples with reduced time overhead is a critical challenge.\nTo address the aforementioned challenges, we propose STAFF, a speculative coreset selection method that introduces a small model in the same family as the target LLM to evaluate the data scores and then uses the target model for validation and selection, achieving effective coreset selection with low overhead across various pruning rates. STAFF consists of two stages, and Figure 3 shows the workflow. In the speculative score calculation stage, STAFF fine-tunes the small model and evaluates"}, {"title": "3.1 SPECULATIVE SCORE CALCULATION", "content": "To find the important samples for the target model \u03b8t on the dataset D, the most intuitive approach is to train \u03b8t on D for a few epochs and then let \u03b8t itself to evaluate the samples based on the training performance (Maharana et al., 2024; Zheng et al., 2023). However, training LLMs is very time-consuming. We have observed that models in the same family have similar architecture and are usually pre-trained on the same corpus (Touvron et al., 2023). As a result, they encode similar knowledge in their parameters. Due to these similarities, models in the same family exhibit similar evaluation results and scores for data samples (Figure 4), while the overhead of small models is only half or even less than those of the large ones.\nBased on such observation, in this stage, we use the small model \u03b8s in the same family as \u03b8t to perform 'speculative execution' with much lower selection overhead, which is fine-tuning \u03b8s on dataset D to calculate data scores. We have further studied the impact of using small models from other families in \u00a74.3. After selecting and fine-tuning \u03b8s, we introduce the effort score of a sample d to calculate the speculative score (Paul et al., 2021), as follows:\n$S_d^s = ||\\nabla_\\theta L(\\theta_s(d))||^2$,\nwhere \u03b8 indicates the learnable parameters in the small model \u03b8s. The score set in Line 3 has $S = \\{S_d^s, d \\in D\\}$. The effort score measures the change in model parameters when learning the given sample d and reflects the learning effort of \u03b8 to fit the given input. A larger effort score indicates a greater difference between the sample and the knowledge currently encoded in the pre-trained model, suggesting that it is more difficult for the model to learn and fit this sample. Considering the small model \u03b8s has a similar architecture and pre-trained corpus to the target LLM \u03b8t, the effort score calculated on \u03b8s has the potential to reflect the importance and difficulty of the data to \u03b8t. As a result, we use the effort score to perform the speculative task on \u03b8s. \u00a7A.4 study the impact of using other scoring functions (e.g., influence score) from existing work (Koh & Liang, 2017)."}, {"title": "3.2 LLM VERIFICATION & SELECTION", "content": "In this stage, we use the target LLM \u03b8t to verify the speculative score Ss from the small model s (Line 4 to 10) and then select coreset D' based on the verification results (Line 11 to 13). Our goal is to retain those important data for the target LLM Ot while covering different data regions to ensure diversity and enable the coreset to achieve good performance at both high and low pruning rates.\nVerification. We first follow the stratified sampling in Zheng et al. (2023) to divide the dataset D into K regions with equal range width based on the speculative score S (Line 4). This stratified division ensures that the subsequent validation and selection cover diverse data regions with varying importance scores, thereby improving the diversity of the coreset. Then, in the verification and selection, we prioritize data regions with fewer samples, as sparse regions usually contain important and difficult samples and dense regions contain easy examples (Zheng et al., 2023) (Line 8). We randomly select bu samples from the target region Bi to verify them on the target model Ot and calculate the verification result Vi in Line 10, as shown in follows:\n$Vi = \\frac{\\sum_{d \\in B_i} S_d^t}{\\sum_{d \\in B_i} S_d^s}$,\nwhere $S_d^s$ indicates the speculative score of sample d calculated on \u03b8s and $S_d^t$ is the verification score on \u03b8t (also use the effort score). The score Vi reflects the difference in the data importance in the region Bi for the target LLM Ot and the small model \u03b8s. Vi > 1 indicates that the data in this region is more important to the target LLM \u03b8t (e.g., \u2018Region B\u2019 in Figure 3 a)), and selecting data from this region can help et effectively modify model parameters and learn the downstream task of D. A small Vi indicates that the data in B\u2081 is not so important to the target model Ot (e.g., \u2018Region A\u2019).\nSelection. We select data from each region based on the verification result Vi and allocate more selection budget to those data regions that are important to \u03b8t and reduce the budget for easy regions, thus achieving a coreset selection that balances data importance and diversity (Line 11). For the selected Bi, we use its verification result Vi to calculate the selection budget MB:\n$m_B =  \\frac{(m - |D'|)Vi}{B}$,\nwhere m is the total selection budget for the coreset D' and $\\frac{(m-|D'})}{|B|}$ represents the average selection budget of each region that has not been selected. Vi is the correction factor of the selection budget, aiming to allocate more budget to those important data regions. Take the data region marked by 'Region B' in Figure 3 a) as an example. The verification result Vi in this region is much larger than 1, indicating that the samples in this region are much more important to the target model than to the small model. To retain important samples in the coreset, STAFF allocates a larger selection budget to the data in this region during selection, as shown in the blue bars in Figure 3 b). To ensure the data"}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 EXPERIMENT SETUP", "content": "Tasks, Models & Datasets. We evaluate STAFF on three datasets on different downstream tasks, namely, the BioInstruct dataset (Tran et al., 2024) (biology question-answering), DialogSum dataset (Chen et al., 2021) (dialogue summarization), and the \u2018Kazakh-English' subset of WMT-19 dataset (Barrault et al., 2019) (translation of minority languages). We fine-tune and evaluate three popular LLMs in the experiments on these downstream tasks, namely Gemma-7b (Team et al., 2024), Llama-2-13b (Touvron et al., 2023), and Mistral-Nemo-Instruct-2407 (Jiang et al., 2023). We use Gemma-2b, Llama-2-7b, and Mistral-7B-Instruct-v0.2 as the small model of the corresponding LLMs in the same family. All models can achieve significant performance improvement after fine-tuning.\nBaselines. We compare STAFF with several state-of-the-art coreset selection methods. \u25cf Random selection obtain the subset via random sampling. \u25cf GraNd (Paul et al., 2021) selects the difficult samples with larger gradient norms to construct the coreset. \u25cf EL2N (Paul et al., 2021; Marion et al., 2023) selects the difficult samples whose prediction results are more different from the ground truth sequences. \u25cf CCS (Zheng et al., 2023) constructs coresets considering both data coverage and importance. We use EL2N as its important metric. \u2192 D2 Pruning (Maharana et al., 2024) constructs graphs to update data scores and selects samples from diverse regions.\nImplementations. All fine-tuning experiments are conducted on one NVIDIA RTX A6000 GPU. Besides, we use the parameter-efficient technique \u2018LoRA' (Hu et al.) in fine-tuning. The number of samples used in verification for each bin (br) is 10. Based on the results of prior work (Paul et al., 2021; Zheng et al., 2023), we set fine-tuning budget T in selection to 3 and K to 50. More hyperparameter settings are in \u00a7A.3. We use Rouge-L as the metric to evaluate the model fine-tuned on the coresets in experiments. In addition, we count the total time overhead during the coreset selection process of each selection method except 'Random', including fine-tuning the target model to calculate importance scores, merging similar samples, and selecting the coreset."}, {"title": "4.2 COMPARISON WITH BASELINES", "content": "We evaluate STAFF and baseline methods on three LLMs and three downstream tasks across different pruning rates (20% to 90%), and fine-tuning results are presented in Table 1. We can observe that STAFF can perform better than state-of-the-art methods at both low and high pruning rates.\nComparison among different methods. The data importance-guided methods (e.g., EL2N (Paul et al., 2021)) can achieve competitive results at low pruning rates. For example, using EL2N on the Mistral-Nemo model and the BioInstruct dataset can achieve the best results among all methods at p = 20%. However, these methods suffer from catastrophic performance degradation at high pruning rates, especially on the WMT-19 dataset. Using the GraNd method on the Llama-2-13b model and WMT-19 dataset experiences a 39.4% performance degradation as p increases from 20% to 90%, which echoes the observation in Figure 1. In contrast, STAFF can achieve good results at different pruning rates. For example, on the WMT-19 dataset, STAFF outperforms the best baseline method by 3.3%, 2.2%, and 1.9% at a high pruning rate (p = 90). Moreover, we observe that the coresets selected by STAFF experience little performance degradation at low pruning rates and can even achieve better fine-tuning results than the full dataset, which is not observed in baseline methods. For instance, on the DialogSum dataset with p = 20%, STAFF exhibits an average improvement"}, {"title": "4.3 ABLATION STUDY", "content": "STAFF implements two stages to select coreset, namely 1) calculating the speculative score S based on a small model that is in the same family as the target LLM and 2) verifying the score on the target LLM and calculating the selection budget MB based on the verification results Vi. To study the impact of these two stages on the selection results, we conduct an ablation study on the Gemma-7b model and present the results (Rouge-L) at different pruning rates in Table 3. Row \u2018w/o verification' shows the results without LLM verification, where the selection budget is evenly distributed to each region (which can be considered as Vi always be 1). Row 'w/o small model' shows the results without speculative score calculation, where the selection directly utilizes the data score St to divide regions B, and then perform stratified sampling (Zheng et al., 2023) for each region. Row \u2018other small model' presents the results of using the speculative score calculated by a LLaMA-like small model (i.e., not in the Gemma family) with 160M parameters from Miao et al. (2024) (hereafter referred to as 'LLaMA-160M'). LLaMA-160M is pre-trained on a small corpus (e.g., part of the C4-en datasets) that is different from Gemma-7b's pre-training dataset.\nWe can observe that removing or replacing either stage leads to performance degradation, indicating the effectiveness and contribution of both stages within STAFF to the coreset selection. Specifically, 'other small model' achieves the most significant performance degradation. Our analysis indicates that due to the significant difference in training corpora and knowledge embedded in the model parameters, the speculative score calculated by this small model cannot effectively guide the data selection for Gemma-7b as a speculative model. Figure 4 exhibits the difference of score distribution among Gemma-7b, Gemma-2b, and LLaMA-160M models. We can observe that the data score distributions of the Gemma-7b and Gemma-2b models are very similar, which gradually increase from left to right, with slight differences in a few regions. In contrast, the data scores distribution of the LLaMA-160M model are significantly different, with scores wavy from left to right. As a result, substituting the Gemma-2b model with it leads to a significant performance degradation of STAFF across all pruning rates, and the fine-tuning results are close to those obtained by random selection. In addition, removing the verification on the target LLM ('w/o verification') also leads to performance degradation, especially on medium-high pruning rates. For example, at a pruning rate of 20%, 'w/o verification' can achieve performance close to or even better than STAFF. However, as the pruning rate increases, especially when the pruning rate is 90%, 'w/o verification' exhibits a performance degradation of up to 3.4% compared with STAFF. This is because, without LLM validation, the selection based solely on the speculative score from a small model cannot effectively find important data regions for the target LLM, leading to performance degradation. Furthermore, directly using the data score St from Ot (i.e., 'w/o small model') to perform stratified sampling also"}, {"title": "5 CONCLUSION", "content": "This paper introduces STAFF, a novel and efficient coreset selection method for task-specific LLM fine-tuning. STAFF first leverages a small model in the same family of the target LLM to calculate the data importance score. It then verifies the scores on the target LLM to accurately identify important regions and allocate larger selection budgets for them while ensuring the coverage of diverse data regions in selection. Our experiment results on three LLMs and three downstream tasks demonstrate that STAFF can effectively select the coreset at different pruning rates, improving the performance of SOTA methods by up to 54.3% while reducing selection time overhead by up to 70.5%."}, {"title": "A APPENDIX", "content": "The appendix is organized as follows:\n\u00a7A.1 provides the details of the datsets in our experiments.\n\u00a7A.2 introduces the baseline methods.\n\u00a7A.3 explains the selection of hyperparameters of STAFF.\n\u00a7A.4 provides additional experiment results (e.g., standard deviations of multiple runs).\n\u00a7A.5 discusses the potential enhancement of STAFF."}, {"title": "A.1 DATASETS", "content": "We use three datasets on three downstream tasks, namely, the BioInstruct dataset (Tran et al., 2024) (biology question-answering), DialogSum dataset (Chen et al., 2021) (dialogue summarization), and the 'Kazakh-English' subset of WMT-19 dataset (Barrault et al., 2019) (translation of minority languages). The BioInstruct dataset contains 25,005 pairs of instruction and demonstration from a diverse range of biomedical and clinical NLP tasks, covering areas such as answering biomedical questions, assessing eligibility for clinical trials, etc. In this task, we use instructions as model input and fine-tune the target LLMs to generate the corresponding demonstration. The DialogSum dataset is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues collected from public dialogue corpora, covering daily-life topics like schooling, work, medication, shopping, leisure, travel, etc. Each dialogue in the dataset has a corresponding summary as the ground truth. In this task, we take the dialogue as input and fine-tune the target LLMs to produce a corresponding summary. For the machine translation task, we use the 'Kazakh-English' subset in WMT-19 which is a new translation task in WMT-19. This subset contains 128,649 pairs of Kazakh and English sentences. In this task, we use the former as input and query the fine-tuned LLMs to generate the corresponding English sentences. In the experiment, we divided each dataset into the training set and the test set according to a ratio of 9:1."}, {"title": "A.2 BASLINES", "content": "Some coreset selection methods are designed for classification tasks and require classification labels to evaluate prediction results and select data (Toneva et al., 2018; Pleiss et al., 2020; Xia et al., 2022). They are difficult to apply to the above three downstream tasks. In this paper, we have collected 5 representative SOTA methods and compared them with STAFF. O Random. It leverages random sampling to construct the coreset without any guidance. However, it may miss the important samples in the dataset at high pruning rates. \u25cf GraNd. (Paul et al., 2021) This method uses the gradient norms of one sample on the given model as the data importance score and then selects the most important samples to construct coresets. EL2N. (Paul et al., 2021; Marion et al., 2023) It selects the difficult samples whose prediction results are more different from the ground truth sequences. \u25cf CCS (Zheng et al., 2023). The Coverage-centric Coreset Selection (CCS) uses EL2N to score samples in the dataset and then divides the dataset into multiple regions based on the scores. It allocates the same budget to each region in coreset selection to ensure data diversity. \u25cf D2 Pruning (Maharana et al., 2024) constructs graphs and uses the message passing algorithm to update data scores and merge the similar samples. It then selects representative samples from different data regions.\nAll hyperparameters of baseline methods follow the recommendation in their paper and open-source implementations. Based on the experiment results of Paul et al. (2021), each baseline method and STAFF train the model for 3 epochs (i.e., T = 3) when calculating data scores."}, {"title": "A.3 HYPERPARAMETERS", "content": "Coreset Selection. We use the recommended hyperparameters in Zheng et al. (2023) to perform stratified sampling in verification and selection (i.e., the number of regions K = 50). Based on the experiment results of prior work Paul et al. (2021), we fine-tune models for three epochs to calculate and evaluate the data scores (i.e., T = 3). We have observed that when bu is set to 10 to 100 (i.e., default to 10 in experiments), it can achieve good verification results and find the important data"}, {"title": "Model and Training.", "content": "In our experiments, we evaluate STAFF on Gemma-7b, Llama-2-13b, and Mistral-Nemo (12b) and use small models Gemma-2b, Llama-2-7b, and Mistral-7B from their family to calculate the speculative score. The model performance without fine-tuning on the given task is shown in Table 4.\nFor fine-tuning pre-trained models on three datasets of downstream tasks, we perform a grid search over learning rate {1e-5, 2e-5, 1e-4, 2e-4} and the batch size {2, 4, 8}, using the complete dataset, which results in batch size of 8 and learning rates shown in Table 5. Models trained on pruned datasets or full datasets use the same hyperparameters. All fine-tunings are conducted with the parameter-efficient fine-tuning method 'LoRA' (Hu et al.). We use the fine-tuning framework from Zheng et al. (2024) to conduct fine-tuning and employ a learning rate scheduler with linear warm-up and cosine decay. We have initially conducted large-scale experiments to determine the number of training epochs for the LLMs. We fine-tune all models on the full dataset for up to 50 epochs. However, we have observed that after certain epochs of fine-tuning, increasing the number of epochs no longer improves model performance. Consequently, we opt for a fixed number of epochs (e.g., 4 epochs) in all experiments. All the fine-tunings in the experiments are repeated three times with different random seeds and we report the averaged results in \u00a74."}, {"title": "A.4 ADDITIONAL RESULTS.", "content": "Results of Other Metrics. Following prior work (Zheng et al., 2024; Ko et al., 2023; Zhang et al., 2024a; Lin et al., 2024), we use the metrics Rouge-L and BLEU-4 to evaluate the performance of coreset selection methods on downstream tasks. As a supplement to \u00a74, we provide the experiment results and the standard deviation (marked by gray) of each coreset selection method on all metrics.\nTable 6, Table 8 and Table 10 illustrate that the observations in \u00a74.2 still hold for the BLEU metric. STAFF can achieve better performance than the SOTA methods across different pruning rates, which shows the effectiveness of STAFF in coreset selection across different pruning rates."}, {"title": "Fine-tuning Budget in Coreset Selection.", "content": "T represents the fine-tuning budget in coreset selection. Existing methods typically require fine-tuning the model for several epochs (i.e., T) to evaluate data scores or divide data regions. Based on the experiment results in prior work (Paul et al., 2021), we set T = 3 in our experiments. To understand the impact of T on the performance of selection results, we conduct experiments on the Gemma-7b model and WMT-19 dataset, and the results (Rouge-L) are shown in Table 12. We observe that increasing T generally improves the coreset performance for the selection methods. This is because a larger T helps the selection method accurately evaluate and identify important samples and data regions. For example, when the pruning rate is 90%, the selection result of the GraNd method improves from 41.0 to 44.9 when T increases from 1 to 5. STAFF also improves from 55.9 to 56.6 during this process. Moreover, STAFF achieves the best results in all T settings, further demonstrating the effectiveness of STAFF in coreset selection."}, {"title": "Speculative Scoring Function.", "content": "We employ the effort score from Paul et al. (2021) as the scoring function for the small model to assess the changes the model makes when learning each input sample. A larger effort score indicates a greater discrepancy between the sample and the knowledge distribution encoded in the model. Other scoring functions also have the potential to be used as speculative scoring functions. Table 13 presents a comparison of fine-tuning results (Rouge-L) using the importance score (Paul et al., 2021) and influence score (Koh & Liang, 2017) on the WMT-19 dataset and the Gemma-7b model at different pruning rate. All scores can be easily obtained on a smaller model, and the configurations in the experiment are consistent with \u00a74.1. We can observe that these scoring functions bring certain performance degradation. However, they are still better than several baseline methods and almost do not suffer from catastrophic performance degradation at high pruning rates, which illustrates the effectiveness of LLM verification and selection in STAFF."}, {"title": "Verification Budget.", "content": "br is the verification budget. A larger b brings more accurate verification and evaluation on the bin Bi and greater overhead in verification. We conduct experiments on the WMT-19 dataset and the Gemma-7b model to show the impact of the validation budget bu on the results. We can observe that for medium to low pruning rates, increasing the budget does not significantly improve or degrade the results. This is because most data is still retained in the coreset, and the validation budget has little impact on the coreset selection. For high pruning rates, we observe that as the validation budget increases, the performance of the selected coreset generally improves. In this case, a higher validation budget allows for a more accurate evaluation of the importance of each data region to the target model, enabling adjustments to the selection budget to avoid missing important data regions. We suggest selecting b \u2208 [10, 100] based on the available budget."}, {"title": "A.5 DISCUSSION.", "content": "Using on small models. This paper introduces STAFF that utilizes a smaller model in the same family as the target LLM to select the coreset for task-specific fine-tuning of the target LLM. For the coreset selection on the models that are the smallest in the family (e.g., Llama-2-7b), using model pruning to build small models or selecting models from other families are potential solutions. Studying the impact of pruned models and models in another family on coreset selection and improving the selection method will be a future research direction.\nImpact of small model capability. The capability of the small model significantly impacts the effectiveness of STAFF. A less capable small model cannot effectively learn knowledge from downstream tasks, thus failing to provide meaningful scores to guide data selection for the target LLM, leading to results close to the random selection. Miao et al. (2024) proposes to combine multiple small models to achieve better performance in LLM speculative decoding, providing a potential direction for improving STAFF."}]}