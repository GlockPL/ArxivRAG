{"title": "ReactAIvate: A Deep Learning Approach to Predicting Reaction Mechanisms and Unmasking Reactivity Hotspots", "authors": ["Ajnabiul Hoque", "Manajit Das", "Mayank Baranwal", "Raghavan B. Sunoj"], "abstract": "A chemical reaction mechanism (CRM) is a sequence of molecular-level events involving bond-breaking/forming processes, generating transient intermediates along the reaction pathway as reactants transform into products. Understanding such mechanisms is crucial for designing and discovering new reactions. One of the currently available methods to probe CRMs is quantum mechanical (QM) computations. The resource-intensive nature of QM methods and the scarcity of mechanism-based datasets motivated us to develop reliable ML models for predicting mechanisms. In this study, we created a comprehensive dataset with seven distinct classes, each representing uniquely characterized elementary steps. Subsequently, we developed an interpretable attention-based GNN that achieved near-unity and 96% accuracy, respectively for reaction step classification and the prediction of reactive atoms in each such step, capturing interactions between the broader reaction context and local active regions. The near-perfect classification enables accurate prediction of both individual events and the entire CRM, mitigating potential drawbacks of Seq2Seq approaches, where a wrongly predicted character leads to incoherent CRM identification. In addition to interpretability, our model adeptly identifies key atom(s) even from out-of-distribution classes. This generalizabilty allows for the inclusion of new reaction types in a modular fashion, thus will be of value to experts for understanding the reactivity of new molecules.", "sections": [{"title": "Introduction", "content": "The reliable prediction of chemical reactions holds paramount significance in pharmaceutical and materials manufacturing, and in understanding many processes in molecular biology [9, 5, 39]. A chemical reaction entails the reorganization of atoms and bonds within the initial reactants, resulting in the creation of novel molecules or compounds as the final products. To comprehend a chemical reaction, it is essential to know the underlying chemical transformations. A sequence of these transformation steps, also called elementary mechanistic steps, is generally expressed in the form of a chemical reaction mechanism (CRM). Many of these steps may involve a bond-forming/bond-breaking process characterized by the corresponding transition state. These elementary steps serve as building blocks for developing novel reactions and discerning side products. Knowledge of CRM can provide atomic-level insights into why the products are formed [19, 12].\nOne of the ways to identify a CRM is to perform quantum mechanical (QM) calculations [6, 3]. Such methods are often computationally demanding and often require substantial human attention rendering it a time-consuming task. In recent years, numerous ML studies have focused on retrosynthesis and forward synthesis prediction, with models mostly trained on the USPTO-50k dataset [38]. While the dataset proves valuable for direct product prediction, it lacks information about elementary reaction steps, thus lacking opportunities to understand CRM [44, 10]. In the realm of CRM prediction, navigating the complexities of multi-step reactions and ensuring atom and charge balance presents formidable challenges. The conventional transformer-based sequence-to-sequence (Seq2Seq) models, commonly employed for sequence generation, prove inadequate in handling the intricate long-term dependencies inherent in CRM [15]. Their limitations become glaring when even a single incorrect character introduced during inference can render the entire CRM meaningless. Beyond the conventional focus on atom and charge balance, ensuring both semantic and syntactic validity of product SMILES strings - representing molecules in a sequence-based format - highlights the necessity for a comprehensive reevaluation at the modeling level in CRM prediction. Recognizing these challenges, we have endeavored to craft an interpretable, swift, and dependable alternative for CRM prediction.\nHerein, we propose an interpretable attention-based graph neural network (GNN) model for the elementary reaction step predictions, which are then used to generate the full CRM (Figure 1). We introduce a dataset containing 3 different families of catalytic reactions comprising 7 distinct elementary steps. In addition to the key task of identifying the elementary steps, our model is simultaneously trained to detect the reactive atoms in such steps. In the case of out-of-distribution (OOD) samples, our model points to the reactive atoms, besides classifying them into an unseen group. This can act as a guide to experts for understanding the reactivity. Below we summarize our key contributions:\n1.  CRM dataset: While there are several benchmark datasets, such as USPTO-50k, comprising single-step chemical reactions, there is currently no existing dataset on CRMs within the available literature as per our knowledge. We curate a first-of-its-kind CRM dataset containing elementary mechanistic steps for transition metal-catalyzed reactions.\n2.  CRM identification via reaction step classification: We introduce ReactAlvate, a graph-attention-based classification model that precisely identifies the necessary elementary steps for a given combination of reactants, reagents, and catalysts. Our model's accurate intermediate-step classification is pivotal for CRM identification. Notably, our approach focuses on identifying underlying reaction rules rather than generating exact product SMILES, simplifying the problem, as our experiments reveal that SMILES generation can lead to vacuous CRM predictions due to even a single character mismatch.\n3.  Identification of reactive atoms/groups: Our framework is distinctly trained to minimize a composite of two distinct loss types: (a) Graph-level loss for predicting reaction classes, and (b) Node-level loss for distinguishing reactive and non-reactive atoms. As a result, our curated database includes information on reactive atoms for each specific reaction step within a CRM. This feature is primarily introduced to offer valuable insights to domain experts.\n4.  Visualizing reactive centers via attention Mechanism: Expanding on the previous point, we demonstrate that the inclusion of node-level loss inherently compels the attention mechanism to align with reactive centers within the molecules involved. This alignment significantly enhances the visualization of reactive centers.\n5.  Generalization to OOD samples: ReactAlvate avoids overconfidence by introducing an OOD eighth class, enabling accurate identification of scenarios not covered in training data. This enhances trust in predictions and allows for the incorporation of new reaction classes when relevant data becomes available. Attention visualization on OOD samples also reveals potential reaction centers, showcasing the model's adaptability."}, {"title": "Related Work", "content": "Recent years have witnessed several interesting applications of machine learning in predicting molecular properties as well as their reactions [49, 2, 30]. Intriguing ML algorithms have been developed to make complex chemical problems, such as organic synthesis, increasingly more amenable. In tasks such as forward or retrosynthesis predictions, predefined reaction templates are employed to make an intuitive connection from a set of reactants to product(s). The templates can be obtained by using data-driven approaches [42] or through encoding by domain experts. Authors in [13] combined these templates and ML for product prediction and subsequent ranking of the products. Transformation rules in retrosynthesis tasks were leveraged in [43]. Current trends suggest the use of templates for both forward and retrosynthesis analysis owing to their efficiency and interpretability. [50, 41, 17, 20].\nInteresting alternatives relying on template-free methods have emerged in very recent times. Transformer-based Seq2Seq generative model has been adopted for forward and retrosynthesis predictions using SMILES as the molecular representation [22, 40, 27]. Various models have been proposed that differ in molecular representation and/or model architecture [45, 53, 7, 23, 48]. For instance, authors in [48] included both molecular graph and SMILES representations for retrosynthesis. Although the template-free methods have gained recent attention, they are known to suffer from the generation of invalid SMILES. During translation, a single character addition or a missing one can render the generated reaction invalid.\nWe note that the current literature although focuses on direct prediction of products and/or retrosynthesis, there are very limited efforts for CRM predictions [36]. In [16], authors utilized deep learning on a rather limited, private dataset of elementary reactions to identify the probable electron sources and sinks and subsequent ranking of these combinations. More recently, Authors in [8] proposed ELECTRO, a graph-based generative model for electron paths movement mimicking 'arrow-pushing' diagrams, but did not consider elementary reaction steps due to the absence of such details in the USPTO-50k dataset. Consequently, ELECTRO is not suitable to multi-step reactions."}, {"title": "Preliminaries and Proposed Method", "content": ""}, {"title": "Background", "content": "Chemical reaction. A chemical reaction is a process in which one set of substances (reactants) transforms into another set of substances (products).\nElementary step. The elementary steps in a chemical reaction typically consists of bond breaking or bond forming involving the reactants/intermediates, where an electron rich source gets attached to an electron poor sink. The movement of electrons is usually denoted using 'arrow-pushing diagrams' where the arrow direction is from the electron source to the sink.\nChemical reaction mechanism (CRM). A sequence of elementary steps that describe the transformation of reactant R to product P, through several transient intermediates I, constitutes a full CRM as shown in eqn(1).\n$R \\xrightarrow{\\text{step } S_1} I_1 \\xrightarrow{\\text{step } S_2} I_2,..., I_n \\xrightarrow{\\text{step } S_n} P$          (1)\nwhere $S_{1:n}$ represents individual elementary steps.\nReactive atom. Reactive atoms are the active atoms involved in an elementary step that undergoes a large change in their immediate bonding/valency environment as a result of the reaction.\nReaction templates. Reaction templates are defined as predetermined sets of chemical transformation rules with specific constraints, such as the presence of a particular substructure. [26] In Figure 1a, an illustration of a template is provided using oxidative addition as a representative elementary step in a reaction. The specified constraints for this template are that any eligible reactant should possess a substructure featuring an aryl C-X bond (where X = Cl, Br, I) and that a catalytically active palladium (Pd) metal center be present."}, {"title": "CRM prediction via ReactAlvate", "content": "We develop an interpretable GNN model, ReactAlvate, to predict the elementary steps, which is further used to devise CRM. An extensive dataset comprising of seven such mechanistic steps for transition metal-catalyzed reactions is curated (see Section 4.1 for further details). ReactAIvate is build upon two elementary tasks:\nReaction step classification (RSC). The primary task of ReactAlvate is to predict the correct elementary step among the seven identified elementary steps. These mechanistic steps include 'oxidative addition', 'boron transmetallation', 'acid-base deprotonation', 'boronate formation', 'substrate coordination', 'transmetallation', and 'reductive elimination'.[25] Once the correct mechanistic step is classified, an off-the-shelf template based reaction rules are used to predict the product information accurately. The predicted products form the reactants for the next step, and the subsequent mechanistic step is identified again. The process is repeated until the catalyst is regenerated. Our advantage in CRM prediction stems from our focus on mechanistic step classification, providing a distinct edge over traditional sequence-based modeling approaches. More specifically, given the set of reactants R, or intermediates $I_1, I_2, .., I_n$, ReactAlvate predicts labels denoted as {Si}i=1n, where each Si corresponds to one of the seven mechanistic steps (Figure 1c).\nHowever, while the dataset consists of the seven elementary steps, it doesn't encompass the entire range of mechanistic rules that a set of reactants may undergo in a chemical reaction. To ensure ReactAlvate doesn't erroneously predict reactions following different chemical transformation rules or force chemically non-reactive combinations into the seven elementary steps, we introduce an eighth class, denoted as S8. Our framework is trained to classify any out-of-distribution (OOD) samples into this eighth class, bolstering confidence in ReactAlvate's predictions.\nReactive atom identification (RAI). Reactive step classification alone falls short in revealing the fundamental mechanisms and rationales underlying a CRM. Accurate identification of reaction centers within the reactant molecules is important for forward synthesis. This approach is particularly crucial when dealing with OOD samples, where the identification of reactive atoms within a given reaction class holds significant value. These insights serve as a guide for experts in recognizing feasible elementary reactions. In this work, the labels for reactive atom classification are derived from reaction templates, with '1' indicating a reactive atom and '0' indicating a non-reactive atom."}, {"title": "ReactAlvate Workflow", "content": "We now proceed to outline the operational methodology of our framework, ReactAlvate.\n1. Molecular representaion via graphs A molecule can be portrayed as a graph, where atoms and bonds constitute nodes and edges, respectively [31]. Each unique atom is represented by the following set of features, encompassing nine types such as atom symbol, formal charge, hybridization, aromaticity, etc. These collectively result in a total of 39 atom features (see supporting information for further details). Similarly, edge representation includes feature vectors corresponding to different bond types (single, double, triple, etc.). In our graph representation, $G = (V, E)$, where V is the set of atoms and E is the set of edges, each atom v is associated with a feature vector $X_v$ in $\\mathbb{R}^D$, with D = 39 representing the number of features for each atom. The tasks at hand involve: (1) classifying elementary steps into predefined classes. Given an input graph G consisting of reactant molecules, the goal is to learn the representation vector $h_G$ and a linear function $g_1$ such that the predicted step $\\hat{y_G} := g_1(h_G)$ aligns with the true step $y_G \\in \\{S_i\\}_{i=1}^8$; (2) identifying reactive atoms, where each atom v in V has a label Yv (yv \u2208 [0,1]). The objective is to learn the representation vector $h_v$ and a linear function $g_2$ for all v such that the predicted reactivity $\\hat{y_v} := g_2(h_v)$ aligns with the true binary label yv.\n2. Graph attention network for RSC & RAI After encoding reactant molecules as graphs, we introduce an attention mechanism to these graphs by generating a context vector for a target atom (v) through attention on its neighboring atoms u, where $u \\in N(v)$. The initial step in computing the context vector involves determining attention weights $\\alpha_{uv}$ between the state vectors $h_v$ and $h_u$ of the two atoms [47, 52]. Subsequently, a context operation follows, where a linear transformation is applied to $h_u$, the state vectors of neighboring atoms. This is succeeded by a weighted sum and a non-linear activation function, resulting in $C_v$, the context vector for the target atom v. The calculation of normalized attention coefficients can be formulated as follows:\n$\\alpha_{uv} = \\frac{exp (LeakyReLU (W[h_v, h_u]))}{\\sum_{u\\in N(v)} exp (LeakyReLU (W[h_v, h_u]))},$\nwhere $\\alpha_{uv}$ signifies the importance (weight) of neighbor atom u to target atom v, and W is a trainable weight matrix, with\n$C_v = ELU (\\sum_{u\\in N(v)} \\alpha_{uv}.W_c.h_u).$\nModern GNNs adopt neighborhood aggregation strategies, updating the features of the target atom iteratively by incorporating the features of its neighboring atoms. The atom's representation encapsulates the structural information within the k-hop network around it after k iterations of this aggregation process. This strategy can be formulated as:\nAggregation phase\n$C_v^{k-1} = \\sum_{u\\in N(v)} A^{k-1}(h_u^{k-1},x_v^{k-1}),$\nUpdate phase\n$h_v^k = GRU^{k-1}(C_v^{k-1},h_v^{k-1}),$\nwhere, $h_v^k$ represents the feature vector of atom v after the kth layer, with $h_v^0 = X_v$, (see Figure 1c). In the aggregation phase, the graph attention mechanism, $A^{k-1}$, provides the most relevant information to the target atom from its neighborhoods in the form of the context vector $C_v^{k-1}$. In the subsequent step, the update function $GRU^{k-1}$ (gated recurrent unit) takes the attention context and the previous state vector of the target atom as input to update the feature vector from the previous state $h_v^{k-1}$ to the current state $h_v^k$.\nTo generate a graph-level embedding, a virtual supernode is introduced, connecting with all the atoms in the molecular graph. The feature vector for this supernode is obtained using sum pooling, expressed as $h_G = \\sum h_v$. Through a similar neighborhood aggregation method, a high-level graph embedding $h_G^t$ is learned iteratively over t iterations. At this point, the optimally learned vector $h_G^t$ can be considered equivalent to $h_G$ (see Figure 1c), which is subsequently utilized for the elementary step prediction task. By feeding the graph-level embedding $h_G$ into a feed-forward neural network (FNN), predicted values are obtained for the RSC task as:\n$\\hat{y_G} = FNN(h_G).$\nReactAlvate employs cross-entropy loss between the predicted and true labels for the RSC task:\n$\\mathcal{L}_{class} := CrossEntropyLoss(\\hat{y_G}, Y_G).$\nIn our RAI task, the objective is to classify each atom in a molecule as reactive or non-reactive, enabling the use of standard classification loss functions. ReactAlvate employs binary cross-entropy (BCE) loss for two-way classification, with a slight modification. Given that the majority of atoms in a sample are non-reactive, we modify the BCE loss by introducing a weight that strongly penalizes incorrect predictions for atoms that are genuinely reactive. This adjustment helps prevent the model from becoming biased towards predicting all atoms as non-reactive (see supporting information).\nRecall that for an atom v, the associated embedding vector before the sum pooling is denoted as $h_v^k$. This embedding vector is considered the optimal atom representation $h_v$. In the node-level classification task, the updated node feature vector is passed through the FNN to predict whether the atom is reactive or non-reactive, expressed as:\n$\\hat{y_v} = FNN(h_v).$\nFor RAI, we consider the weighted BCE loss as discussed above:\n$\\mathcal{L}_{RAI} := \\sum_v WeightedBCELoss(\\hat{y_v}, Y_v)$\nThe overall loss for ReactAIvate is made up of the individual losses for RSC and RAI:\n$\\mathcal{L} = \\mathcal{L}_{class} + \\mathcal{L}_{RAI}$"}, {"title": "Experiments", "content": ""}, {"title": "Dataset details", "content": "In this study, we consider a diverse and representative set of transition metal-catalyzed reactions. These include Suzuki-Miyaura coupling (SMC) [29, 4], Buchwald-Hartwig amination (BHA) [34, 14], and Kumada coupling (KC) [1]. The inclusion of these reactions is motivated by their significance in drugs, agrochemicals, and pharmaceutical synthesis. To the best of our knowledge, there is currently no existing database that includes credible mechanisms for these reactions, compelling the creation of CRM datasets. For this, seven distinct elementary mechanistic steps are recognized that can account for all three reaction mechanisms: 'oxidative addition', 'boron transmetallation', 'acid-base deprotonation', 'boronate formation', 'substrate coordination', 'transmetallation', and 'reductive elimination' (see supporting information).\nWe have created essential reaction templates for each elementary mechanistic step within the three considered reaction datasets in this study. The template is structured to yield the product based on the given reactants (refer to Figure 1a). A reaction comprises a set of substrates, catalyst and reagents. For example, in the case of SMC, key substrates include an aryl halide and a boronic acid, with a metal-phosphine complex serving as the catalyst and a base facilitating the reaction. These reaction partners are curated from primary literature [51, 21]and the PubChem database [23]. By inputting this set of molecules into the reaction template aligned with the specific reaction mechanism type, all elementary steps and the complete CRM can be derived. Our dataset encompasses a total of 100,000 elementary mechanistic steps (see supporting information for more details)."}, {"title": "Training details", "content": "The dataset is partitioned into training, validation, and test samples with a distribution ratio of 70:10:20. ReactAIvate is constructed using PyTorch [32] with the Adam optimizer [24] and a batch size of 256. Throughout this study, we maintain consistent hyperparameter values: k (number of attentive message passing layers for atom embedding) is set to 2, t (number of attentive message passing layers for molecule embedding) is set to 1, with an L2 weight decay of 0.000001, a learning rate of 0.001, and a dropout rate of 0.1. The number of atom features and graph feature size are specified as 39 and 200, respectively."}, {"title": "Baseline", "content": "First, we employed Seq2Seq approaches, such as T5Chem [28] and Transformer [35] as proxies for baselines, which are considered state-of-the-art (SOTA) for individual single step reaction prediction. T5Chem is a multi-tasking model designed for various reaction prediction tasks, leveraging the Text-To-Text Transfer Transformer (T5), an encoder-decoder model from the transformer family. The Transformer baseline is built on the original encoder-decoder framework [46].\nIn these frameworks, the problem is formulated as a generation task, where the model is trained to predict the full CRM given the reactants and other entities. To ensure a fair comparison, we introduce two test datasets, each comprising 1000 samples. The first is an in-distribution (ID) test dataset, where the individual reacting partners of each sample belong to the same set as the training datasets, although the samples themselves are not part of the training set. The second is an out-of-distribution (OOD) dataset, where the reaction components of each sample differ structurally from those used in training the model."}, {"title": "Results", "content": "Table 1 illustrates the performance comparison, specifically focusing on accuracy, i.e., the percentage of samples where the generated CRM precisely matches the true CRM. For the ID dataset, ReactAlvate exhibits better accuracy compared to T5Chem and Transformer. Intriguingly, in the OOD dataset, ReactAlvate offers superior accuracy in CRM generation, reaching upto 96%. In contrast, both the baselines fail to predict the correct CRM, potentially due to incorrect predictions of one of the many tokens or atom imbalance, rendering the entire sequence invalid (see supporting information for more details). The observation is not specific to CRM prediction, but has also been reported in recent findings, where several SOTA template-free models unexpectedly falter when faced with OOD in retrosynthesis prediction [11]. In addition, unlike the above-mentioned baselines, where T5Chem and Transformer-based architectures are trained to predict the entire CRM, we have conducted additional experiments where the baselines are trained to predict just the forward intermediates (FIs) at each step (exactly what these models were originally designed for), and then the predicted intermediates serve as the reactants for the next step. The process is repeated until the catalyst is regenerated (i.e., end of CRM). The results are reported above in Table 1. Even for the FI variants of the baselines, their performances remain highly inadequate for OOD samples. Although T5Chem benefits from pre-training on the Pubchem database containing 97M molecules, its sequence-centric framework limits its effectiveness in CRM prediction, especially with OOD instances.\nThe poor OOD performance of seq2seq/transformer models is not due to poor hyperparameter tuning but rather the nature of character generation-based product prediction. For example, if a model is trained on molecules with Iodine and Chlorine but tested on a molecule where Iodine is replaced with Fluorine, it struggles because it was never trained to generate sequences involving Fluorine. Additionally, any incorrect symbol prediction in an intermediate step can corrupt the entire downstream task for seq2seq models. On the contrary, ReactAlvate predicts the most appropriate reaction rule (template) at each step, using these rules to accurately generate the right products at each step.\nThe fingerprints and PCD based DNN models exhibited enhanced performance as compared to sequence-based models on both ID and OOD samples. However, it is worth noting that these models yield lower accuracy for the OOD set as compared to ReactAlvate, this discrepancy may stem from the fact that fingerprints rely on predefined sets of molecular substructures, potentially limiting their ability to capture nuanced structural details. Whereas MPNN performed better than fingerprints and PCD featurization, they still fell short of ReactAlvate. This could be attributed to the advantages conferred by attention mechanisms and the incorporation of RAI task within ReactAIvate. This, in turn, implicitly captures the relevance between CRM and RAI, and also helps with the alignment of highly attentive atoms with likely reaction centers. Nonetheless, these models provide robust and adequate validation. These results strongly imply the necessity for a meticulous reconsideration of the modeling aspect for CRM generation."}, {"title": "Discussion and Analysis", "content": "Having demonstrated the exceptional performance of our framework ReactAlvate for both the RSC and RAI tasks, we proceed to delve into the underlying reasons for its effectiveness through a comprehensive ablation study. Additionally, we aim to elucidate domain-specific advantages that contribute to its superior performance.\nSignificance of incorporating node-level RAI: We begin by emphasizing the implication of integrating the node-level loss in the reactive atom prediction task along with the graph-level loss for elementary step prediction. In Figure 2, we compare the attention visualization between ReactAIvate without the node-level loss and ReactAIvate with both losses, using an oxidative addition step from the Kumada coupling dataset. In the former model, attention weights are dispersed across the reactants and catalyst, providing limited utility. Notably, the crucial reactive metal atom Pd:7, responsible for catalyzing this step, fails to draw any attention. Thus, attention visualization without the node-level loss contributes little to understanding chemical reactivity. In the latter model with the two-level loss, attention weights are more concentrated around the reactive region of the molecules. For instance, the Pd metal center and the aryl chlorine bond garner higher attention, (Pd:7, C:20, Cl:21), representing the true reactive atoms in this elementary step. This underscores the importance of including the node-level loss, aligning the model's attention mechanism more closely with how a chemist would assign attention to reactive atoms.\nIllustration of a full CRM: Moving forward, we assess ReactAlvate's capability in predicting a complete CRM for Kumada coupling, serving as a representative example (Figure 3). In the initial step, the model accurately predicts oxidative addition (S1) as the elementary step, along with the correct identification of reactive atoms. Notably, the model also predicts two additional atoms as active, which intriguingly turn out to be reactive in the subsequent step. In the second step, ReactAIvate once again correctly predicts the elementary class, transmetallation (S6), and accurately identifies all true reactive atoms. Finally, the model predicts the reductive elimination (S7) step as the concluding phase. Both the predicted active atoms and attention distributions align consistently with the expected mechanism. In summary, ReactAIvate demonstrates the ability to generate a complete CRM starting from only reactants and catalysts.\nAttention visualization for OOD-class: To gain deeper insights into the model, we visualize attentions in a sample belonging to the OOD class, as shown in Figure 4a. In this instance, the pair of molecules is predicted to fall into the eighth (OOD) class, accurately reflecting that the amine and the aryl halide (N:27, C:18, Cl:19) would not react in the absence of a catalyst. Intriguingly, the model predicts chlorine in one molecule and the adjacent carbon in the other as reactive atoms. Additionally, attentions are primarily distributed around functional groups. The broad dispersion of attention underscores ReactAlvate's challenge in pinpointing the exact reaction mechanism. This outcome aligns with expectations, as the combination is chemically non-reactive, and it would be counterintuitive for the model to highlight specific reaction centers leading to its prediction in the OOD class.\nIn our final evaluation, we test ReactAlvate for the identification of potential reactive atoms in an OOD sample involving an entirely different reaction mechanism, as depicted in Figure 4b. Please note that the molecule is not expected to undergo any transformation (both predicted and true class are S8, i.e., no reaction). However, if it is presented with a suitable reagent, this molecule is anticipated to undergo a \u03b2 hydride elimination step and the reactive centers get activated. Among the predicted reactive atoms, two (Pd:13 and \u03b1-C: 6) correspond to possible true reactive atoms. Since the molecule in this example doesn't undergo reaction, the true reactive centers are presented as an empty list. Moreover, attentions are dispersed around the reaction center and the reactive atoms. These highlights the model's versatility in predicting OOD samples and serves as a guide for comprehending the reactivity of metal-catalyzed reactions. Consequently, one can easily incorporate new elementary classes of interest to further broaden the model's applicability."}, {"title": "Conclusion and Future Work", "content": "In conclusion, we introduce ReactAlvate, a graph-attention-based Graph Neural Network (GNN) model designed for interpretable Chemical Reaction Mechanism (CRM) generation. Our model is trained on a novel dataset comprising seven distinct elementary mechanistic steps, covering the complete CRM for three different transition-metal-catalyzed processes. ReactAlvate excels in accurately classifying elementary steps and recognizing reactive atoms, demonstrating its capability to construct full CRMs. Notably, the model exhibits a prudent handling of non-reactive cases, showcasing its reliability in predictions. ReactAlvate outperforms Seq2Seq baseline models, emphasizing the limitations of the latter in CRM identification due to minor errors. The robust OOD classification performance underscores ReactAlvate's potential for exploring additional mechanisms with the availability of more data. As part of future work, we plan create a user-friendly interface for predicting entire CRMs based on user-inputted SMILES of reactants."}, {"title": "Data Availability", "content": "Data and codes related to this work are publicly available through our Github repository at https://github.com/alhqlearn/ReactAlvate."}, {"title": "Code details", "content": "We provide our code, datasets, and pre-trained models via the following Github link-\nhttps://github.com/alhqlearn/ReactAIvate"}, {"title": "Elementary steps", "content": "In this study, we created a novel dataset of important chemical reaction mechanisms (CRMs) featuring three distinct reaction classes, namely, Suzuki-Miyaura coupling (SMC) [6, 2], Buchwald-Hartwig amination (BHA) [9, 3], and Kumada coupling (KC) [1]. The dataset consists of seven unique elementary mechanistic steps, carefully chosen to capture the underlying mechanism of all three reactions. The corresponding reaction templates for these elementary steps, along with illustrative examples, are provided in Figures 1, 2.\nThe reaction templates are based on specifying the bond changes from reactants to products along with the corresponding atom-to-atom mapping. This is illustrated using an oxidative addition step in the first box Figure 1. The carbon (C:2), halogen ([I,Br,Cl:3]) single bond is broken, and two bonds (i) Pd:1 and [I,Br,Cl]:3 and (ii) Pd:1 and C:2 are formed.\nSimilarly, in the boron transmetallation step two bonds are broken, namely, (i) the bond between boron and carbon i.e., B:4-C:2 and (ii) metal and halogen i.e., [Pd, Ni]:1-[I,Br,Cl]:3. Simultaneously, two new bonds are formed between (i) carbon and metal i.e., C:2-[Pd,Ni]:1 and (ii) boron and halogen i.e., B:4-[I,Br,Cl]:3. All the other templates can be described similarly."}, {"title": "Chemical reaction example with mechanism", "content": ""}, {"title": "Suzuki-Miyaura coupling (SMC)", "content": "The Suzuki-Miyaura coupling (SMC) is one of the most often employed carbon-carbon bond-forming reactions in the pharmaceutical industry [7, 8]. This reaction class involves cross-coupling"}, {"title": "Buchwald-Hartwig amination (BHA)", "content": "Over the last two decades, the Buchwald-Hartwig (BH) amination or the palladium-catalyzed amination of aryl halides and pseudohalides has emerged as a valuable tool in organic synthesis, facilitating the creation of C(sp\u00b2) \u2013 N bonds [4]. The underlying mechanism of this reaction"}, {"title": "Kumada coupling (KC)", "content": "The Kumada cross-coupling is a reaction between an organohalide and an organomagnesium compound (commonly known as a Grignard reagent). This reaction, catalyzed by a palladium or nickel catalyst, results in the formation of a C - C coupled product. The catalytic cycle for this reaction follows a sequence of three elementary steps, i) oxidative addition, ii) transmetallation, and iii) reductive elimination (see Figure 5)."}, {"title": "Atom and bond features", "content": ""}, {"title": "ReactAIvate", "content": "We proposed an interpretable graph attention model (ReactAIvate) for elementary step classification with concurrent identification of reactive atoms responsible for the chemical transformation of such steps. We followed the same protocol described in Attentive FP by [11]. The model was trained for 5 epochs with the batch size of 256. We have used accuracy(%) as our performance metric. For the RAI task, we kept the default threshold value of 0.5. The performance with mean and standard deviation shown in Table 1 & 2 of the main text is obtained using 5 different runs.\nModel training involved the dataset containing 100,000 elementary steps, split into 70:10:20 ratios for training, validation, and testing, respectively. The test dataset is then used to get ID-RSC & ID-RAI performance shown in Table 1 of the main text. Additionally, an out-of-distribution (OOD) dataset was created, comprising 3,647 elementary steps, with structurally distinct reaction components from those in the training set. This dataset is then used for OOD-RSC & OOD-RAI tasks."}, {"title": "Hyper-parameter tuning", "content": "We conducted hyperparameter tuning for ReactAIvate, adjusting parameters such as k (number of attentive message passing layers for atom embedding), t (number of attentive message passing layers for molecule embedding), h\u2090 (dimension of graph feature size), and dropout ratio (d.r.).\nThe results in Table 5.1 indicate that the accuracy for both tasks remains nearly unchanged by the hyperparameter adjustments."}, {"title": "ReactAlvate with BCEloss", "content": "We performed a control experiment using BCELoss instead of WeightedBCELoss corresponding to the RAI task, and the results are presented in Table 4. In the OOD-RSC & OOD-RAI tasks, ReactAlvate with WeightedBCELoss demonstrated superior performance compared to ReactAlvate with BCELoss."}, {"title": "CRM generation using ReactAIvate", "content": "In this section, we describe CRM generation using ReactAIvate. The first example is of a BHA reaction where a heteroaryl halide is coupled with a cyclic amine (morpholine) Figure 6. The red colored eight-pointed star and five-pointed star shown above and below the arrows respectively denote a true elementary step in the mechanism and true reactive atoms respectively. The corresponding star notations in blue color are the predicted elementary step and reactive atoms respectively. The numbers that follow any of the five-pointed star notation are the atoms identified as important to the given elementary step of the reaction.\nThe model predicted all the reaction classes accurately. In the first two steps (S1 and S5) of the mechanism, a few more atoms arepredicted as reactive atoms in addition to identifying the actual reactive atoms. For the last two steps (S3 and S7) all the reactive atoms are correctly predicted. It shall be noted that in the case of step S3, we have considered only four important atoms (Pd: 15, Cl: 16, K:29, and 0:30) as the true reactive atoms."}, {"title": "Baseline models", "content": ""}, {"title": "T5Chem", "content": "The T5Chem pipeline is a versatile language model designed for various chemical tasks, including forward reaction predictions [5]. The T5Chem is a pre-trained model trained on 97 million PubChem molecules. We have fined-tuned the T5Chem for our specific task of predicting the forward intermediate/product. The model takes a molecule in the form of SMILES strings. We used this model for forward synthesis such that given a set of reactants model learns to predict the corresponding CRM. T5Chem employs a complete encoder-decoder architecture featuring four layers and eight attention heads. The hidden dimension for T5Chem is set at 256, while the intermediate feed-forward layer utilizes a dimension of 2048. Starting with an initial learning rate of 5e-4, the model was trained over five epochs. The batch size was kept to 32."}, {"title": "Transformer", "content": "We employed a simple transformer-based encoder-decoder architecture for CRM generation following the protocol described in [10]. Both the encoder and decoder contains a stack of 4 identical layers of size 512 with 8 attention heads. We trained the model for 5 epochs and varied the learning rate using LambdaLR scheduler with ADAM optimizer and employed 3000 warm-up steps. The starting learning rate was set at 1.0. The batch size was kept at 64 and the sequences were padded up to 800 characters. Overall, the model had a total 29.5M parameters.\nIt is important to highlight that ReactAlvate has a total parameter count of only 0.87M in contrast to T5Chem with 14.71M and Transformer with 29.55M parameters. Thus, ReactAIvate is a simpler model as compared to the other two, which is also evident from the compute time required for model training, suggesting a higher efficiency."}, {"title": "Deep Neural Network (DNN)", "content": "We utilized a 3-layer DNN comprising of 512, 256, and 128 nodes respectively in the first, second, and third layer. Following each linear layer, ReLU activation function was employed, with a dropout rate of 0.1. Training spanned 100 epochs, using a learning rate of 0.0001 with the Adam optimizer, and a batch size of 128.\nVarious featurization methods were explored, including (1) the Morgan fingerprint with a radius of 2 and a bit vector length of 3096, (2) multiple fingerprint features with a bit vector length of 71208, and (3) RDKit-based 197 physicochemical descriptors. The parameter count for these three distinct models was 1.7M, 36.7M, and 0.3M, respectively."}, {"title": "MPNN", "content": "Our initial MPPN model is constructed using DGL-LifeSci and configured with the following hyperparameters. There are 39 input node features and 10 edge features. We applied 6 message passing steps during computation. Training was conducted for 5 epochs with a learning rate of 0.001, resulting in a model with 0.7M parameters."}, {"title": "Limitations in CRM generation", "content": "Next, we have shown some examples of in-distribution (ID) and out-of-distribution (OOD) CRM generation tasks for both the baseline models. The T5Chem offered a reasonably good accuracy in generating CRM for the in-distribution samples. However, it was found to result in the generation of semantically invalid SMILES. For instance, in the predicted CRM Figure 8, the intermediate generated after the boronate transmetallation step has a pyridine ring attached to the Pd center, instead of the actual o-fluoro aryl group. In the case of OOD CRM generation, the T5Chem model fails even in generating syntactically valid SMILES. Here also, semantically invalid molecules were generated (Figure 9).\nSimilar to the T5Chem, the Transformer had issues arising from the generation of both syntactically and semantically invalid SMILES. See Figure 10 and Figure 11 respectively for Transformer generated CRM in ID and OOD samples respectively.\nOverall, we have shown that generating full CRM using the seq2seq model is problematic. Generation of semantically and syntactically invalid SMILES leads to invalid CRMs."}]}