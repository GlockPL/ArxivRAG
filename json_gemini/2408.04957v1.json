{"title": "LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description", "authors": ["Yizhang Jin", "Jian Li", "Jiangning Zhang", "Jianlong Hu", "Yong Liu", "Lizhuang Ma", "Zhenye Gan", "Yabiao Wang", "Xin Tan", "Chengjie Wang"], "abstract": "Visual Spatial Description (VSD) aims to generate texts that describe\nthe spatial relationships between objects within images. Traditional\nvisual spatial relationship classification (VSRC) methods typically\noutput the spatial relationship between two objects in an image,\noften neglecting world knowledge and lacking general language\ncapabilities. In this paper, we propose a Large Language-and-Vision\nAssistant for Visual Spatial Description, named LLaVA-VSD, which\nis designed for the classification, description, and open-ended de-\nscription of visual spatial relationships. Specifically, the model first\nconstructs a VSD instruction-following dataset using given figure-\ncaption pairs for the three tasks. It then employs LoRA to fine-tune\na Large Language and Vision Assistant for VSD, which has 13 billion\nparameters and supports high-resolution images. Finally, a large\nlanguage model (Qwen-2) is used to refine the generated sentences,\nenhancing their diversity and accuracy. LLaVA-VSD demonstrates\nexcellent multimodal conversational capabilities and can follow\nopen-ended instructions to assist with inquiries about object rela-\ntionships in images.", "sections": [{"title": "1 INTRODUCTION", "content": "Spatial understanding is essential for various applications and\nreal-world scenarios, particularly in fields like robotics and aug-\nmented reality. It allows for better comprehension of visual scenes\nand object relationships, enabling more efficient planning and\ndecision-making. The development of spatial understanding tasks\nhas evolved to address the increasing need for richer and more\naccurate representations of spatial relationships in images. Initially,\nthe focus was on Visual Spatial Relationship Classification (VSRC),\na task aimed at identifying the spatial relationship between two\nobjects within an image by selecting from a predefined set of re-\nlations. This approach, however, offered only a shallow analysis\nof spatial semantics, limiting its applicability and expressiveness.\nRecognizing the limitations of VSRC, researchers introduced a more\nadvanced task called Visual Spatial Description (VSD) [19]. This\ntask generates textual descriptions that convey the spatial seman-\ntics in an image, providing a deeper spatial analysis. VSD takes an\nimage with two specified objects as input and outputs a sentence\ndescribing their detailed spatial relationship.\nThe current research landscape for VSD task has been primarily\nadvanced by [18, 19]. They pioneered the VSD task by manually\nannotating spatial descriptions to images, utilizing the visual spatial\nclassification datasets developed by [11]. This approach provided\na foundation for generating richer and more accurate spatial de-\nscriptions for images. In addition to establishing the VSD task, [19]\nalso tackled it as a general image-to-text task using vision-language\npre-trained models (VL-PTMs). These models are designed to input\nimages and output text, making them suitable for generating spatial\ndescriptions. By leveraging VL-PTMs, the researchers were able\nto generate textual descriptions that effectively convey the spatial\nsemantics in images."}, {"title": "2 METHOD", "content": null}, {"title": "2.1 Preliminary", "content": null}, {"title": "2.1.1 Problem Formulation.", "content": "The Visual Spatial Description (VSD)\ntask aims to generate accurate and contextually rich textual descrip-\ntions of spatial relationships between objects in an image. This task\nleverages a dataset comprising N images annotated with bound-\ning boxes and spatial descriptions. The task is divided into three\nsub-tasks, each with increasing complexity.\nEach data item consists of an image I, bounding boxes for two\nobjects $O_1$ and $O_2$ within the image, tags for the objects $O_1$ and\n$O_2$, denoted as $T_1$ and $T_2$ respectively, and corresponding spatial\nrelation R and descriptions D. Each bounding box is represented\nas:\nbbox = [ymin, ymax, xmin, xmax]\nFormally, the dataset can be represented as:\nD = {x|xi = (I, bboxo\u2081, bbox02, T1, T2, R, D), i = 1, ..., N}\nThe dataset has two versions, VSDv1 and VSDv2, depending on the\ncomplexity of the descriptions D. The expected results from the\nmodel vary depending on the sub-task.\nFor Task 1: Classification of Visual Spatial Relationship, the\nmodel is expected to produce a spatial relationship label p from a\npredefined set of labels P:\np\u2208 P = {\"on\", \"in\", \"next\", \"under\", \"above\",\n\"behind\", \"in front of\", \"left\", \"right\"}\nFor Task 2: Description of Single Spatial Relationship, the model\nis expected to generate a single textual description D that describes\nthe spatial relationship between $O_1$ and $O_2$. For Task 3: Descrip-\ntion of Open-ended Spatial Relationship, the model is expected to\ngenerate three diverse textual descriptions $D_1$, $D_2$, $D_3$ that describe\nthe spatial relationship between $O_1$ and $O_2$. Each description $D_i$\nshould be contextually appropriate and diverse."}, {"title": "2.1.2 Architecture.", "content": "Our model architecture follows LLaVA, as il-\nlustrated in fig. 1, which is a multi-modal architecture designed\nto integrate visual and textual information for various tasks. The\narchitecture typically consists of the following components:\nVision Encoder: This component processes the input images\nand extracts visual features. It often employs a pre-trained encoder,\nsuch as CLIP [14] and SigLIP [17], which is designed to align visual\nand textual data through extensive pre-training on image-text pairs.\nThis method facilitates the alignment of pre-aligned encoders with\nLanguage Models."}, {"title": "2.2 Visual Spatial Instruction-Following Data", "content": "There is a lack of multimodal VSD datasets to train an instruction-\nfollowing assistant. To fill this gap, we create the first dataset of\nits kind from widely existing VSD data, through a template-based\nprocedure.\nFor a data item (I, bbox\u2081, bbox02, T1, T2, R, D) in the original\nVSD dataset, we sample a conversation template to generate a\nquestion $X_q$ and a response $X_r$. These respectively inquire about\nand answer the spatial relations required by a specific task. With\n(I, $X_q$, $X_r$), we create a single-round instruction-following example:\nHuman: I $X_q$ <STOP> \\n Assistant: $X_r$ <STOP> \\n\nDepending on the task, the sampled question may require a de-\nscription of spatial relationships in a word, a sentence, or multiple\nsentences. For the answer, a single phrase or sentence is sampled\ndirectly from R or D of VSDv1 for tasks 1 and 2. For task 3, one\nsentence is generated by stacking $T_1$, R, and $T_2$, one sentence is\nsampled from D of VSDv1, and one sentence is sampled from D\nof VSDv2. An example of instruction-following data is shown in\ntable 1. For tasks 1 and 3, we generate a data example for each\nimage, whereas for task 2, we create a data example for each de-\nscription. The resulting instruction-following dataset contains a\ntotal of 121339 items, specifically: 20490 for task 1, 83608 for task 2,\nand 17241 for task 3."}, {"title": "2.3 Adapting Multimodal Conversational\nModels to the VSD Domain", "content": "We utilize LLaVA, a general-domain multimodal conversation model,\nas the foundational MLLM and progressively adapt it to the VSD\ndomain. The network architecture remains consistent, employing\na linear projection layer to bridge the vision encoder and the lan-\nguage model. For the LLaVA-VSD model training, as shown in fig. 1,\nwe fine-tune LLaVA using LoRA after the initial two-stage training\non our visual spatial instruction-following dataset. This approach\nenables us to develop a highly accurate, dataset-specific model that\nenhances the assistant's service quality. Given an image as context\nand natural language questions, the assistant generates free-form\ntext responses for both close-set and open-set questions across all\ntasks. Rather than scaling up data or model size for optimal per-\nformance, our objective is to provide cost-effective and practical\nsolutions with minimal development overhead. The fine-tuning\nprocess requires approximately 4 hours on 8 V100 GPUs."}, {"title": "2.4 LLM-assisted Description Diversification", "content": "For Task 3, the limited availability of diverse training data for in-\ndividual images posed a significant challenge. To address this, we\namalgamated naive descriptions, Task 2 descriptions, and Task 3\ndescriptions as our training dataset, as previously outlined. How-\never, this strategy resulted in high similarity in the generated de-\nscriptions, thereby undermining the goal of achieving descriptive\ndiversity. To mitigate this issue, we harnessed the robust text gen-\neration capabilities of LLMs to introduce greater variability and\nrichness into the generated descriptions. Through our measure-\nments, we observed that the latter two sentences in the set of three\ngenerated sentences exhibited a higher BLEU4 score. We employed\nthe Qwen2-7B [16], using the second sentence as the foundational\ndescription. By employing prompts, we instructed the model to\ngenerate an alternative sentence that preserved the original spa-\ntial relationships. This methodology ensured that the final set of\nthree descriptions achieved both diversity and accuracy, thereby\nenhancing the overall quality and distinctiveness of the outputs."}, {"title": "3 EXPERIMENTS", "content": null}, {"title": "3.1 Implementation Details", "content": "We train our model with 8\u00d7 V100s, based on the official LLaVA-1.6-\nVicuna-13B checkpoint. We fine-tune the proposed visual spatial\ninstruction-following dataset for 1 epoch, with a learning rate of\n2e-4 and a batch size of 16."}, {"title": "3.2 Evaluation Metrics", "content": "To evaluate the performance of models on the VSD task, we use\ndifferent metrics for each sub-task. Here, we briefly introduce the\nthree main evaluation metrics used: F1 score, BLEU-4, and SPICE.\nThe F1 score is the harmonic mean of precision and recall, pro-\nviding a balance between the two. BLEU-4 (Bilingual Evaluation\nUnderstudy) is a metric for evaluating the quality of text that has\nbeen machine-translated from one natural language to another,\nconsidering up to 4-grams. SPICE (Semantic Propositional Image\nCaption Evaluation) evaluates the semantic content of image cap-\ntions by comparing scene graphs generated from the candidate and\nreference captions.\nFor Task 1, the F1 score of the multi-label classification is used\nas the evaluation metric:\n$Z_1 = F1$\nFor Task 2, BLEU-4 and SPICE scores are calculated for the\npredicted sentence and each ground truth, and the maximum score\nis chosen. The submitted models are ranked by a weighted sum of\nBLEU-4 and SPICE scores:\n$Z_2$ = 0.4 \u00d7 BLEU-4 +0.6 \u00d7 SPICE\nFor Task 3, a combination of BLEU-4 and SPICE scores is used.\nThe evaluation of Task 3 contains two parts: correctness and diver-\nsity. For correctness, the same SPICE score as Task 2 is used. For"}, {"title": "the diversity, Self-BLEU is used:", "content": "$Z_3$ = 0.5\u00d7 (50 - mBLEU4) + 0.5 \u00d7 SPICE\nFinally, the following score is used for ranking:\noverall = 0.2 \u00d7 $Z_1$ +0.3 \u00d7 $Z_2$ + 0.5 \u00d7 $Z_3$"}, {"title": "3.3 Overall Results", "content": null}, {"title": "3.4 Ablation Study", "content": "To evaluate the impact of the LLM-assisted Description Diversifica-\ntion on our performance, we conducted an ablation study, as shown\nin table 4. This study involved comparing the full model with the\nLLM-assisted Description Diversification enabled against a version\nof the model where this was disabled. The prompt is Given the\nimage and a concise spatial relationship description between $O_1$ and\n$O_2$: 'description', generate a simpler sentence with a similar meaning.\nKeep the main structure of the sentence, but replace words or phrases\nwith simpler adjectives, verbs, or synonyms, ensuring no consecutive\nwords from the original description remain the same."}, {"title": "4 CONCLUSION", "content": "In this paper, we introduced LLaVA-VSD, a novel approach that\nintegrates Multimodal Large Language Models (MLLMs) to en-\nhance the Visual Spatial Description (VSD) task. By transforming\nthe VSD dataset into a visual instruction format, fine-tuning the\nLLaVA model using Low-Rank Adaptation (LoRA), and refining\noutputs with a text-based large language model, we significantly\nimproved the accuracy, contextual richness, and diversity of spatial\ndescriptions. Our approach addresses the limitations of existing\nVSD models, such as insufficient prior knowledge and suboptimal\ntext generation capabilities. The experimental results demonstrate\nthat LLaVA-VSD outperforms current models, making it a valu-\nable advancement for applications in robotics, augmented reality,\nand other fields requiring spatial understanding. In conclusion,\nLLAVA-VSD sets a new standard for generating high-quality spa-\ntial descriptions, offering a robust solution that balances accuracy,\nfluency, and diversity. Future research can further enhance this\napproach by incorporating additional contextual information and\nrefining MLLM capabilities."}]}