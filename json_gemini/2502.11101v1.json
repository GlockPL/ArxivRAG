{"title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation", "authors": ["Kun-Hui Lee", "Eunhwan Park", "Donghoon Han", "Seung-Hoon Na"], "abstract": "Large Language Models (LLMs) excel across a variety of language tasks yet are constrained by limited input lengths and high computational costs. Existing approaches such as relative positional encodings (e.g., RoPE, ALiBi) and sliding window mechanisms-partially alleviate these issues but often require additional training or suffer from performance degradation with longer inputs. In this paper, we introduce CacheFocus, a method that enhances length normalization and reduces inference latency without any further training. Our approach leverages query-independent, offline caching to efficiently reuse a Context KV Cache Store. We address the amplification of abnormal token distributions problem by re-positioning cached keys and introducing Layer-Adaptive Cache Pruning to discard low-relevance caches during pre-filling. Additionally, our Adaptive Positional Allocation Strategy dynamically reassigns cache positions to maximize the use of the available positional encoding range. Experiments on the Natural Questions and TriviaQA datasets demonstrate that CacheFocus outperforms alternative methods even when inputs exceed the 4K limit of the LLaMA-2 model, emphasizing its practical effectiveness for long-context LLMs. Moreover, even with large maximum input length of Qwen2, the performance of CacheFocus shows that it maintains consistent performance even as the number of documents increases, effectively managing long-text generation without degradation.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have demonstrated significant improvements in a wide range of language tasks, largely attributed to the effective encapsulation of knowledge within their extensive parameters during the pre-training. The reason beyond this success is"}, {"title": "2 CacheFocus", "content": "In this section, we present the essential components of CacheFocus, focusing on three key areas: the scaled dot-product attention (Vaswani et al., 2017), the pre-filling and decoding, and the use of Rotary Position Embedding (RoPE) (Su et al., 2023)."}, {"title": "2.1 Background", "content": "The attention mechanism measures the relevance between a query and a set of key states, then computes a weighted sum of the corresponding value states. Suppose that we have key-value states {K, V}, formed by concatenating the cached representation {Kpast, Vpast} from previous steps with those computed from the current input {Kcurrent, Vcurrent}. Given a query Q, the scaled dot-product attention is then calculated as follows:\n\n$Attn(Q, K, V) = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d}})V$\n\nwhere d is the dimensionality of the hidden states.\nAuto-regressive models generally split text generation into two phases: pre-filling and decoding, as shown in Algorithm 1. During pre-filling, the input sequence X initializes the key-value cache C and produces the first output token. In the decoding phase, each newly generated token is fed back into the model along with the updated cache until a termination condition is met, which produces the final output Y.\nRecent LLMs replace absolute positional encodings with ROPE which encodes relative positions by rotating each two-dimensional slice of the query and key vectors with position-dependent angles. Specifically, let ri be the rotation angle for position i, and the define rotation matrix as follows:\n\n$R_i = \\begin{bmatrix} cos(r_i) & -sin(r_i) \\\\ sin(r_i) & cos(r_i) \\end{bmatrix}$"}, {"title": "2.2 Query-Independent Parallel Document Caching", "content": "In auto-regressive models, subsequent tokens to be generated do not affect prior tokens. Based on this, we employ a query-independent parallel document caching by placing the query after the documents, meaning that document caches could be pre-computed in offline. We first compute a shared prefix cache Cprefix using a shared prefix p to mitigate the duplicated attention sink problem as follows:\n\n$C_{prefix} \\leftarrow LLM(p)$\n\nFor each document di, we derive its cache by combining Cprefix with di:\n\n$C_{di} \\leftarrow LLM(C_{prefix}, d_i)$\n\nNote that the cache for each document is pre-comupted and stored Context KV Cache Store."}, {"title": "2.3 Cache Re-Positioning", "content": "As it is theorized that two-dimensional rotation matrices have inverses that are simply their transposes,"}, {"title": "2.3.1 Adaptive Re-Positioning", "content": "Previous works (Ratner et al., 2023; Hao et al., 2022; Zhu et al., 2025) reuse positional IDs across windows by either attending to multiple documents within a single window or assigning one window per document, which often leads to inefficient utilization of the available positional encoding space, while leaving many encodings unused for short documents and hindering effective caching when documents share a window. Different from previous works, we dynamically adjust the positional encodings of cached keys to maximize the usage of the encoding space while mitigating the attention amplification associated with excessive reuse of positional IDs, in the term of Adaptive Re-Positioning. Given the model's available positional encoding length L and the cache length lc, the maximum number of caches that can be assigned unique positions without reuse is $\\frac{L}{lc}$. For a retrieved set of k caches, the required number of reuses for positional encodings is calculated as:\n\n$N_{reuse} = ceil(\\frac{k}{(\\frac{L}{l_c})})$\n\nWe split the k caches into Nreuse groups and assign sequential positions to the caches within each group via Cache Re-Positioning. These repositioned caches are then used for attention computation, ensuring optimal utilization of the positional encoding space while minimizing the adverse effects of excessive reuse."}, {"title": "2.4 Layer-Adaptive Cache Pruning", "content": "Intuitively, directing attention toward caches with higher semantic relevance would enhance long-text"}, {"title": "2.5 Adaptive Positional Allocation Strategy", "content": "Layer-Adaptive Cache Pruning, as introduced in \u00a72.4, results in unused positions within the available positional encoding range. Inspired by (Liu et al., 2024), which demonstrates that closer proximity between a query and relevant documents enhances performance, we propose two strategies to reassign new positions to the remaining caches after pruning: the Dynamic Positional Allocation Strategy and the Attention-Guided Allocation Strategy. These strategies aim to maximize the utilization of the positional encoding range and"}, {"title": "3 Experiments", "content": "The futher detail of dataset, retrieval, and setup of CacheFocus could be found in Appendix."}, {"title": "3.1 Settings", "content": "We employ the Qwen2-{1.5, 7}B-Instruct (Yang et al., 2024) and LLAMA-2-7B-Chat (Touvron et al., 2023) for LLMs.\nEvaluation Metric. Following from (Liu et al., 2024), we evaluate the performance using accuracy whether at least one correct answer appears in the model's output.\nBaseline. Our baseline is based on the PCW approach (Ratner et al., 2023), with the modification of using the system prompt as the shared prefix instead of [BOS]. We employ zero-shot settings, with one document per window and greedy decoding. Moreover we also compared our method with RAG, and APE (Yang et al., 2025) in the case of LLAMA-2-7B-Chat."}, {"title": "3.2 Main Results", "content": "Figure 1, 4 present the experimental results on the NQ and TQA datasets using Qwen2-7B-Instruct, as well as on NQ using LLaMA-2-7B-Chat, respectively.\nFigure 1 illustrates that RAG based on LLAMA-2-7B-Chat experiences a substantial performance drop when the maximum input length (i.e. 4K tokens) is exceeded. Although APE and PCW partially mitigate this decline, they still exhibit gradual performance degradation as input length increases. In contrast, CacheFocus consistently maintains robust performance even when the input surpasses the maximum length, thereby demonstrating its efficacy in handling extended inputs.\nEven with large maximum input length (i.e. 32K tokens) of"}, {"title": "3.3 Analysis", "content": "Table 1 presents the re-ranking performance on NQ for different methods\u2014pruning alone and pruning combined with the adaptive positional allocation strategy using both DPR-based and BM25-based retrieval on Qwen2-1.5B-Instruct. For DPR-based retrieval, which is already trained on NQ, re-ranking yields only marginal improvements in R@50. In contrast, BM25-based retrieval, relying soley on lexical matching and neglecting semantic relevance, indicates a substantial improvement of 2.12 in R@5 with pruning alone. Moreover, when the adaptive positional allocation strategy is applied, additional improvements of 3.0 and 2.37 points in R@5 are observed for the respective strategies compared to BM25-based, implying that our approach could significantly enhance performance, particularly when the retriever's semantic capabilities are limited.\nSince Layer-Adaptive Cache Pruning demonstrates effective in discarding low-relevance caches, additional improvements could be achieved by dynamically allocating the remaining caches. In particular, Strategies provide additional benefits when the retrieval models is not fully aligned with the target dataset. For instance, when evaluating TQA, both strategies yield remarkable gains over \"prune\" by allocating new positions, compensating for potential mismatches in the retrieval's original ranking. Conversely, in NQ\u2014where the retriever is trained directly\u2014the ranking is already well trained and thus the advantage of strategies is marginal. This implies that while pruning alone helps filter out noisy caches, combining it with strategies could be helpful for datasets where the initial retrieval model is sub-optimal.\nFigure 5 indicates the performance comparison across varying the number of reused positions Wreuse. For instance, if there are k = 20 documents, the implies that 5 windows are assigned continuous positions (i.e. reused 4 times). To investigate the effect of adaptive repositioning, we explore varing Nreuse as follows:\n\u2022 All windows are placed in parallel, increasing reuse proportionally with the number of documents.\n\u2022 All windows use unique positions exactly once. While this significantly boosts"}, {"title": "4 Related Work", "content": "Recent works in RAG have explored methods to efficiently process and incorporate large contexts. PCW (Ratner et al., 2023) splits few-shot examples or documents into several windows processed in parallel, thereby reducing positional encoding overhead and removing cross-attention between windows. While performance tends to decrease when using more than 3 windows, demonstrates robust performance even with contexts longer than those covered by 3 windows.\nVarious modifications to the attention mechanism have been proposed to address limitations in context relevance and distribution. For example, Structured Prompting (Hao et al., 2022) scales attention values by 1/M (where M is the number of windows), although it requires specific in-context learning examples. Similarly, APE (Yang et al., 2025) investigates additional scaling factors and temperature reduction, but it does not consider cache positioning based on the semantic relevance between query and document caches. SparseRAG (Zhu et al., 2024) infers relevance scores to prune less pertinent documents, which requires an additional training process. XL3M (Wang et al., 2024) and Superposition Prompting (Merth"}, {"title": "5 Conclusion", "content": "In this paper, we propose , a framework designed to enhance long-text generation in LLMs by reducing inference latency and handling extended inputs without any further training. We leverage Query-Independent Parallel Document Caching (\u00a72.2) to pre-compute a Context KV Cache Store, and address the challenge of Attention Sink by introducing a Cache Re-Positioning mechanism (\u00a72.3) that dynamically adjusts positional encodings. Furthermore, Layer-Adaptive Cache Pruning (\u00a72.4) removes semantically irrelevant caches based on attention scores and Strategy (\u00a72.5) consisting of Dynamical Positional Allocation and further optimizes the assignment of positional encodings.\nExperimental Results on the NQ and TQA datasets demonstrate that outperforms previous works (Ratner et al., 2023; Yang et al., 2025), even when input lengths exceed the maximum length of LLMs. Our analysis of time complexity confirms that our method significantly reduces computational overhead by lowering prefill and decoding times, while preserving performance in extended contexts. Finally, not only improves inference latency but also robustly scales with longer inputs, thereby paving the way for more efficient and accelerated long-text generation with LLMs."}, {"title": "Limitations", "content": "While shows promising results in handling long-text inputs with lower computational overhead, it also comes with several limitations as follows:\n\u2022 relies on pre-computed caches for each doc-"}]}