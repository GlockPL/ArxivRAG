{"title": "FlexMol: A Flexible Toolkit for Benchmarking Molecular Relational Learning", "authors": ["Sizhe Liu", "Jun Xia", "Lecheng Zhang", "Yuchen Liu", "Yue Liu", "Wenjie Du", "Zhangyang Gao", "Bozhen Hu", "Cheng Tan", "Hongxin Xiang", "Stan Z. Li"], "abstract": "Molecular relational learning (MRL) is crucial for understanding the interaction behaviors between molecular pairs, a critical aspect of drug discovery and development. However, the large feasible model space of MRL poses significant challenges to benchmarking, and existing MRL frameworks face limitations in flexibility and scope. To address these challenges, avoid repetitive coding efforts, and ensure fair comparison of models, we introduce FlexMol, a comprehensive toolkit designed to facilitate the construction and evaluation of diverse model architectures across various datasets and performance metrics. FlexMol offers a robust suite of preset model components, including 16 drug encoders, 13 protein sequence encoders, 9 protein structure encoders, and 7 interaction layers. With its easy-to-use API and flexibility, FlexMol supports the dynamic construction of over 70, 000 distinct combinations of model architectures. Additionally, we provide detailed benchmark results and code examples to demonstrate FlexMol's effectiveness in simplifying and standardizing MRL model development and comparison. FlexMol is open-sourced and available at https://github.com/Steven51516/FlexMol.", "sections": [{"title": "1 Introduction", "content": "Molecular relational learning (MRL) aims to understand the interaction behavior between molecular pairs[23]. Among all interaction types, those involving drugs and proteins are of particular interest due to their significant impact on therapeutic discovery and development. Drug-target interactions (DTIs) play a crucial role in various aspects of drug development, such as virtual screening, drug repurposing, and predicting potential side effects [56]. Protein-protein interactions (PPIs) reveal new potential therapeutic targets by enhancing our understanding of protein structural characteristics and cellular molecular machinery [42, 27]. Drug-drug interactions (DDIs) are vital for understanding the effects of concurrent drug use, which can inform strategies to prevent adverse drug reactions and ensure patient safety [5, 12].\nMRL has significantly advanced through the integration of deep learning models and substantial AI-ready datasets [14, 54]. MRL models typically consist of several key components: modules for encoding molecule 1, modules for encoding molecule 2, modules for modeling interactions between molecules, and additional auxiliary modules [38, 22, 60, 30, 16, 46, 59, 55]. The variety of encoding methods and interaction layers available allows for the easy construction of new models by recombining these components. This flexibility, however, results in a substantial model space, posing significant challenges for benchmarking processes."}, {"title": "2 Related Work", "content": "Several libraries support machine learning-driven exploration of therapeutics, each offering unique functionalities.\nCheminformatics and Biomolecular Structure Libraries RDKit is a widely-used cheminformatics toolkit for molecular manipulation, including fingerprinting, structure manipulation, and visualization [21]. Graphein is a Python library for constructing graph and surface-mesh representations of biomolecular structures and interaction networks, facilitating computational analysis and machine learning [17]. However, these libraries are primarily focused on preprocessing tasks and do not directly benchmark MRL models.\nDeep Learning Frameworks for Biomolecular Modeling DGL-LifeSci leverages the Deep Graph Library (DGL) and RDKit to support deep learning on graphs in life sciences. It excels in molecular property prediction, reaction prediction, and molecule generation, with well-optimized modules and pretrained models for easy application [24]. Therapeutics Data Commons (TDC) provides a unified platform with 66 AI-ready datasets across 22 learning tasks, facilitating algorithmic and scientific advancements in drug discovery [14]. While these frameworks are highly useful, they primarily provide building blocks for MRL model construction and resources for dataset preparation, rather than directly supporting the benchmarking of MRL models."}, {"title": "3 FlexMol", "content": null}, {"title": "3.1 Molecular Relational Learning", "content": "Inputs for MRL consist of pairs of molecular entities including drugs and proteins[14]. Our library specifically targets drug-drug, protein-protein, and drug-target interactions. Drugs are encoded using SMILES notation, a sequence of tokens representing atoms and bonds. Proteins are represented either as sequences of amino acids or/and as 3D structures in PDB format. The primary objective is to develop a function $f : (X_1, X_2) \\rightarrow Y$ that maps pairs of molecular representations $(X_1, X_2)$ to interaction labels Y. These labels can be binary $(Y \\in \\{0, 1\\})$ for classification tasks such as binding prediction, continuous $(Y \\in R)$ for regression tasks such as predicting binding affinity, or categorical $(\\Upsilon \\in \\{1,2,..., K\\})$ for multi-class classification tasks such as identifying interaction types."}, {"title": "3.2 Framework", "content": "FlexMol provides a user-friendly and versatile API for the dynamic construction of molecular relation models.  First, users select a specific task and load the corresponding dataset. Second, users customize their models by declaring FlexMol components, including Encoders and Interaction Layers, and defining the relationships between these components. FlexMol offers 16 drug encoders, 13 protein sequence encoders, 9 protein structure encoders, and 7 interaction layers, enabling the construction of a vast array of models. Even without"}, {"title": "3.3 Components", "content": "Encoder The Encoder class in FlexMol is designed to transform raw molecular data into meaningful representations through a two-stage process: preprocessing and encoding. The preprocessing stage, managed by the Featurizer class, involves tasks such as tokenization, normalization, feature extraction, fingerprinting, and graph construction. The encoding stage, handled by the Encode Layer class, serves as a building block for dynamically constructing the MRL model. During model training, this class processes the preprocessed data to generate embeddings that are utilized by subsequent layers. Kindly note that for certain fingerprint methods like Morgan[37], Daylight[37], and PubChem [19], which do not have associated encoding layers, a simple multi-layer perceptron (MLP) is employed as the Encode Layer.\nAn overview of FlexMol encoders is provided in Table 1 and Table 2, with detailed explanations for each method in the Appendix. We labeled each encoder with its input type. For drug encoding, \"Sequence\" refers to the drug sequence, while \"Graph 2D\" and \"Graph 3D\" are 2D molecular graph and 3D molecular conformation generated using RDKit from SMILES strings, respectively. For protein encoding, \"Sequence\" refers to the amino acid sequence, while \"Graph 3D\" refers to graphs constructed from protein PDB structures."}, {"title": "Interaction Layer", "content": "The Interaction Layer class is another critical building block of the MRL model. These layers are designed to serve two primary functions: capturing and modeling relationships between different molecular entities, and facilitating feature fusion by combining multiple embeddings of the same entity to create a more comprehensive representation. Interaction Layers can take inputs from various FlexMol components, including Encode Layers or other Interaction Layers, enabling the construction of sophisticated model architectures. FlexMol offers seven preset interaction types for model building: Bilinear Attention[1], Self Attention[44], Cross Attention[34], Highway[59], Gated Fusion[28], Bilinear Fusion[25], and Concatenation, with detailed explanations in Appendix."}, {"title": "3.4 Evaluation Metrics", "content": "The FlexMol Trainer supports multiple default metrics, aligning with the TDC standard for molecular relational learning[14]. Users can specify the metrics in the Trainer for early stopping and testing. These metrics include various regression metrics (Mean Squared Error (MSE), Root-Mean Squared Error (RMSE), Mean Absolute Error (MAE), Coefficient of Determination (R2), Pearson Correlation Coefficient (PCC), Spearman Correlation Coefficient), binary classification metrics (Area Under Receiver Operating Characteristic Curve (ROC-AUC), Area Under the Precision-Recall Curve (PR-AUC), Range LogAUC, Accuracy Metrics, Precision, Recall, F1 Score, Precision at Recall of K, Recall at Precision of K), and multi-class classification metrics (Micro-F1, Micro-Precision, Micro-Recall, Accuracy, Macro-F1, Cohen's Kappa)."}, {"title": "3.5 Supporting Datasets", "content": "FlexMol is compatible with all MRL datasets that conform to our specified format. These datasets typically consist of three components: molecular entity one, molecular entity two, and a label. We provide utility functions to facilitate the loading of datasets in this format. Furthermore, FlexMol includes an interface for loading datasets from the Therapeutics Data Commons (TDC) library, enabling direct loading and splitting of standardized datasets [14]. For more examples and tutorials, please refer to the Appendix."}, {"title": "4 Experiments", "content": "We performed proof-of-concept experiments using FlexMol to show the extensive range of experiments, comparisons, and analyses facilitated by our framework. The following sections present results for DTI experiments, demonstrating the utility of both protein and drug encoders. Additional experiments for DDI and PPI are provided in the Appendix."}, {"title": "4.1 Experiment #1: Proof-of-Concept Experiments on DTI Task", "content": "We utilized the same processed datasets, DAVIS and BIOSNAP, as the MolTrans framework for evaluating DTI[16]. Our setup also integrates AlphaFold2-generated structures to enrich the datasets and enable 3D graph-based protein encoders. Specifically, BIOSNAP includes 4, 510 drugs and 2, 181 protein targets, resulting in 13, 741 DTI pairs from DrugBank [29]. BIOSNAP contains only positive DTI pairs; negative pairs are generated by sampling from unseen pairs, ensuring a balanced dataset with equal positive and negative samples. DAVIS comprises Kd values for interactions among 68 drugs and 379 proteins [6]. Pairs with Kd values below 30 units are considered positive. For balanced training, an equal number of negative DTI pairs are included.\nWe constructed 14 FlexMol models, detailed in Table 3, and compared them with 8 baseline models (LR[2], DNN[16], GNN-CPI[43], DeepDTI[47], DeepDTA[60], DeepConv-DTI[22], Moltrans[16], 3DProt-DTA[46]). Hyperparameters and codes for each run are available in our public repository.\nFor both the DAVIS and BIOSNAP datasets, we conducted a random split in the ratio of 7:2:1 for training, validation, and testing, respectively. Each test was repeated five times to mitigate any randomness, and the average results were computed. The experiments were performed using 8 NVIDIA V100 GPUs."}, {"title": "4.2 Results and Analysis of Experiment #1", "content": "Figure 2 illustrates the example code used to build and run the model for Experiment 1.12. Table 4 presents the performance metrics of the selected baseline models and the FlexMol models, including ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) and PR-AUC (Precision-Recall Area Under the Curve)."}, {"title": "Code to Reproduce Experiment 1.12 Using FlexMol", "content": "This example utilizes GAT and PubChem as drug encoders with gated fusion interaction, and AAC as the protein encoder.\nEasy-to-use API: FlexMol allows for the customization of models in less than 10 lines of code across all 14 experiments. Figure 2 shows the simplicity and efficiency of our API using Experiment 1.12 as an example. Specifically, it takes only 7 lines to customize the model and 9 lines to train and test it.\nSupport for Various Input Types: FlexMol handles diverse molecular data, including drug sequences, protein sequences, and protein structures. Six out of the fourteen experimental combinations use graphs derived from protein structures, demonstrating the framework's strong ability to encode different input types effectively.\nImpact of Additional Encoders: We utilize Experiment sets {1.1, 1.2, 1.3} and {1.4, 1.5, 1.6} to demonstrate FlexMol's capability to analyze model performance through the integration of additional"}, {"title": "Impact of Interaction Layers", "content": "We use Experiment sets {1.11, 1.12} and {1.13, 1.14} to illustrate FlexMol's ability to analyze the impact of interaction layers on model performance. Initially, we test the encoder combinations without interaction layers, followed by tests with interaction layers. For example, Experiment 1.12, which employs the Gated-Fusion interaction, outperformed the simpler concatenation method used in Experiment 1.11 across all metrics. Also, advanced interaction layers such as Cross-Attention in Experiment 1.14 further improved model performance compared to Experiment 1.13.\nThese results indicate that incorporating interaction layers can improve performance when used effectively. The gated-fusion layer in Experiment 1.12 facilitates feature fusion of sequence and graph-level drug representations, while the cross-attention mechanism in Experiment 1.14 enhances the modeling of interactions between substructures of drugs and targets."}, {"title": "4.3 Experiment #2: Custom Model Design and Evaluation", "content": "This experiment serves as a case study to demonstrate how users can leverage FlexMol to design and construct more complex models."}, {"title": "4.4 Results and Analysis of Experiment #2", "content": "Extensibility: This experiment demonstrates FlexMol's ability to easily adapt to user-defined models. By defining encoders according to our protocol, users can incorporate these as building blocks along with other encoders and interaction layers.\nCreating Complex Models: The custom model involves more complex configurations, including user-defined blocks and additional operations such as stacking and flattening outputs from FlexMol components. Despite this complexity, the model can be constructed in fewer than 10 lines of code after defining the custom encoders.\nPerformance: Table 5 compares the custom model of Experiment #2 with the two best models from the 14 combinations of Experiment #1. Our custom model outperforms all baseline models across all four metrics (ROC-AUC and PR-AUC for both Davis and BIOSNAP datasets).\nAblation Study: FlexMol's dynamic model-building capability facilitates easy modification of the model structure for ablation studies. Table 6 shows that the inclusion of additional encoders and interaction layers significantly improves model performance. This improvement is attributed to the additional pocket encoder, which provides atom-level details about potential binding pockets, and the attention layer, which effectively models interactions at the global protein graph level, pocket graph level, and drug graph level."}, {"title": "5 Conclusion", "content": "We introduced a powerful and flexible toolkit to address the challenges of benchmarking in molecular relational learning. FlexMol enables the construction of a wide array of model architectures, facilitating robust and scalable experimentation. Our framework simplifies the process of model development and standardizes the evaluation of diverse models, ensuring fair and consistent benchmarking.\nLimitations and Future Work: In the benchmarks presented in this paper, we did not perform an exhaustive combination of model architectures. Our primary goal was to demonstrate FlexMol's implementation and its capability to construct and compare different models. A comprehensive analysis of model combinations was beyond the scope of this paper and is left to the community to explore using our framework. In future work, we plan to continue maintaining and expanding the components implemented in FlexMol. This includes adding more diverse encoders, interaction layers, and evaluation metrics to further enhance the toolkit's flexibility and utility. FlexMol can also be extended to single-instance tasks such as molecular property prediction[49, 50, 51]. Previous research has highlighted the significance of uncertainty prediction in delineating the boundaries of model performance, and how molecular property predictors can serve as feedback to fine-tune generative models[32, 31]. Incorporating these ideas into the FlexMol framework could enhance its effectiveness in benchmarking tasks related to therapeutic discovery."}, {"title": "Code and Data Availability", "content": "The FlexMol toolkit is open-sourced and available on GitHub at https://github.com/\nSteven51516/FlexMol. The code to reproduce the experiments described in this paper can be found in the experiments directory of the repository. The data splits for the DAVIS and BioSNAP datasets were obtained from the MolTrans repository at https://github.com/\nkexinhuang12345/MolTrans."}, {"title": "A Appendix", "content": null}, {"title": "A.1 General Information", "content": null}, {"title": "A.1.1 Links", "content": "The FlexMol code is available at our public repository: https://github.com/Steven51516/\nFlexMol. The code to reproduce the experiments described in this paper can be found in the experiments directory of the repository. The FlexMol experiment split used in our experiment are\nadapted from MolTrans: https://github.com/kexinhuang12345/MolTrans."}, {"title": "A.1.2 Licenses", "content": "FlexMol is under the BSD 3-Clause License. We, the authors, bear all responsibility in case of violation of rights."}, {"title": "A.2 Drug Encoders Implemented in FlexMol", "content": null}, {"title": "A.2.1 Sequence and Fingerprint-based Encoders", "content": "Morgan: Generates a 1024-length bit vector encoding circular radius-2 substructures, processed with an MLP. [28]\nDaylight: Produces a 2048-length vector encoding path-based substructures, processed with an MLP. [28]\nErG: Creates a 315-dimensional 2D pharmacophore description for scaffold hopping, processed with an MLP. [32]\nPubChem: Generates an 881-length bit vector where each bit corresponds to a significant substructure,\nprocessed using an MLP. [16]\nChemBERTa: Generates embeddings from SMILES strings using the pretrained ChemBERTa model,\nprocessed with a linear layer or MLP. [23]\nESPF: Produces a 2586-length sub-structure partition vector, processed with an MLP. [14]\nCNN: One-hot encodes SMILES strings and processes them through a multi-layer 1D convolutional neural network, followed by a global max pooling layer. [14]\nTransformer: Generates sub-structure partition fingerprints from SMILES strings and encodes them using a self-attention-based transformer model. [14]"}, {"title": "A.2.2 2D Graph-based Encoders", "content": "Preprocessing involves creating 2D molecular graphs from SMILES strings using RDKit. These graphs are then encoded using various graph neural network models implemented with DGL:\nGCN, GAT, GIN: Standard graph neural networks to capture relational and topological features in\n2D drug graphs. [17, 34, 37]\nAttentiveFP: Utilizes attention mechanisms to prioritize significant molecular substructures. [36]\nNeuralFP: Employs neural fingerprinting methods to capture detailed molecular features. [8]\nMPNN: Uses message-passing neural networks to transmit information among atoms and bonds in the graph.[10]"}, {"title": "A.2.3 3D Graph-based Encoders", "content": "Preprocessing involves creating 3D molecular graphs from SMILES strings using RDKit, considering\nspatial conformation. These graphs are then encoded using:"}, {"title": "A.3 Protein Encoders Implemented in FlexMol", "content": null}, {"title": "A.3.1 Sequence-based Encoders", "content": "CNN: One-hot encodes the amino acid sequences and processes them through a multi-layer 1D convolutional neural network, followed by a global max pooling layer. [14]\nTransformer: Generates sub-structure partition fingerprints from amino acid sequences and encodes\nthem using a self-attention-based transformer model. [14]\nAAC: Generates an 8,420-length vector representing amino acid k-mers, processed with an MLP. [26]\nESPF: Produces a 4,114-length sub-structure partition vector, processed with an MLP. [14]\nPseudoAAC: Generates a 30-length vector considering protein hydrophobicity and hydrophilicity\npatterns, processed with an MLP. [4]\nQuasi-seq: Generates a 100-length quasi-sequence order descriptor using sequence-order-coupling\nnumbers, processed with an MLP. [3]\nConjoint triad: Produces a 343-length vector based on the frequency distribution of three continuous amino acids, processed with an MLP. [30]\nAuto correlation: Generates a 720-length vector based on the autocorrelation of physicochemical properties along the sequence, processed with an MLP. [12]\nCTD: Produces a 147-length vector by calculating composition, transition, and distribution descriptors, processed with an MLP. [7]\nESM: Directly generates embeddings using a pretrained ESM model, processed with a linear layer or\nMLP. [27]\nProtTrans-t5, ProtTrans-bert, ProtTrans-albert: Directly generates embeddings using pretrained\nmodels (T5, BERT, ALBERT respectively), processed with a linear layer or MLP. [9]"}, {"title": "A.3.2 3D Graph-based Encoders", "content": "Preprocessing involves creating 3D graphs from protein PDB structures. These graphs are then\nencoded using various graph neural network models:\nGCN, GAT, GIN: Standard graph neural networks to capture spatial features in 3D protein structures. [17, 34, 37]\nGCN_ESM, GAT_ESM, GIN_ESM: Combines standard GNNs with additional ESM features for enhanced node representations. [35]\nPocketDC: Identifies and constructs graphs from binding pockets using DeepChem, encoded with\nGCN. [39]\nGVP: Utilizes Geometric Vector Perceptrons (GVP) to capture geometric and vectorial features. [15]\nGearNet: Employs pretrained GearNet layers with relational message passing to capture geometric properties and spatial features. [40]"}, {"title": "A.4 Interaction Layers Implemented in FlexMol", "content": "Bilinear Attention: The Bilinear Attention Network (BAN) layer captures interactions between\n2D feature sets by computing bilinear transformations, followed by attention pooling and batch normalization. [1]\nBilinear Fusion: Combines 1D features from two sources using a bilinear transformation and ReLU\nactivation, capturing multiplicative interactions for enhanced feature representation. [19]\nBidirectional Cross Attention: Combines 2D embeddings from two sources using bidirectional\nattention and max pooling, creating a unified representation. [25]\nHighway: Combines 1D features using multiple highway layers with gated mechanisms to regulate\ninformation flow. [41]\nGated Fusion: Combines 1D features from two sources using gated mechanisms and transformations,\nproducing a fused representation. [22]\nMulti-Head Attention: Applies attention mechanisms to 2D features using multiple heads, with\noptional residual connections and layer normalization. [33]\nConcatenation: Concatenation is the simplest form of combining multiple feature embeddings by\njoining them end-to-end."}, {"title": "A.5 Example Usage of FlexMol", "content": "This section provides a simple example to using FlexMol for drug-target interaction prediction. A\nmore detailed set of tutorials can be found in the tutorials directory of our repository."}, {"title": "A.5.1 Loading the Dataset", "content": "First, we import the necessary modules from FlexMol and load the DAVIS dataset."}, {"title": "A.5.2 Initializing Encoders", "content": "Next, we initialize the drug and protein encoders."}, {"title": "A.5.3 Stacking Features and Setting Interaction Layers", "content": "We then stack the features from the different encoders and set the interaction layer to self-attention."}, {"title": "A.5.4 Building and Training the Model", "content": "Finally, we initialize the BinaryTrainer and train/test the model.\nThe BinaryTrainer is configured for training binary classification tasks with early stopping based on the ROC-AUC metric and evaluates the model using both ROC-AUC and PR-AUC metrics. After testing, the metrics are saved to the user-specified directory.\nThe code provided in this section serves as a practical example of how FlexMol can be utilized for molecular relational learning tasks, showcasing its flexibility and ease of use."}, {"title": "A.6 Additional Experiments for DDI", "content": null}, {"title": "A.6.1 Experiment Setup", "content": "Dataset: We used DrugBank, downloaded using the TDC Python library [13], for the evaluation\nof FlexMol-baselines. DrugBank contains 191,808 DDI tuples with 1,706 drugs. Each drug is\nrepresented in SMILES format, from which molecular graphical representations are generated using\nthe Python library RDKit. There are 86 interaction types describing how one drug affects the\nmetabolism of another. Each DDI pair is considered a positive sample, from which a negative sample\nwas generated using the method described in the GMPNN-CS framework [24].\nEvaluation Method: We performed a stratified split of the dataset to maintain the same interaction\ntype proportions in the training (60%), validation (20%), and test (20%) sets. This was repeated three\ntimes, resulting in three stratified randomized folds.\nWe constructed six FlexMol baselines as detailed in Table 1. The models were trained in mini-\nbatches of 512 with a learning rate of 0.0001. Five state-of-the-art(SOTA) methods were selected\nfor comparison: MHCADDI [6], GMPNN-CS [24], GAT-DDI [24], GMPNN-U [24], and MR-GNN [38]."}, {"title": "A.6.2 DDI Experiment Results", "content": ""}, {"title": "A.7 Additional Experiments for PPI", "content": null}, {"title": "A.7.1 Experiment Setup", "content": "Dataset: We used the Guo yeast dataset [11], which includes 11,188 PPI pairs, with 5,594 positive and 5,594 negative interactions. The data was collected from the Saccharomyces cerevisiae core subset of the Database of Interacting Proteins (DIP), version DIP_20070219. The dataset is available at https://github.com/aidantee/xCAPT5/tree/master/data/Golden-standard-datasets/Guo-2008\nEvaluation Method: We tested using 5-fold cross-validation with a random split following the XCAPT5 framework [5].\nWe constructed six FlexMol baselines as detailed in Table 3. The models were trained in mini-batches of 128 with a learning rate of 0.001. Five state-of-the-art(SOTA) methods were selected for comparison: PIPR [2], FSNN-LGBM [21], MARPPI [18], TAGPPI[31], and xCAPT5[5]."}, {"title": "A.7.2 PPI Experiment Results", "content": "Note: NA indicates that data is not available in the reference literature. The TAGPPI method does not include standard deviation values in the literature, and thus no standard deviations are reported here.\nForm table 4, we observed that all results from FlexMol baselines are significantly lower than SOTA methods. This suggests that the PPI task is more challenging and requires more specialized modeling methods rather than simple encoder combinations. However, some trends were noted. For instance, the combination of encoders in Experiment #4 improves performance compared to Experiments"}]}