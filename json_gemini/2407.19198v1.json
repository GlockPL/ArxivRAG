{"title": "Towards the Dynamics of a DNN Learning Symbolic Interactions", "authors": ["Qihan Ren", "Yang Xu", "Junpeng Zhang", "Yue Xin", "Dongrui Liu", "Quanshi Zhang"], "abstract": "This study proves the two-phase dynamics of a deep neural network (DNN) learning interactions. Despite the long disappointing view of the faithfulness of post-hoc explanation of a DNN, in recent years, a series of theorems have been proven [27] to show that given an input sample, a small number of interactions between input variables can be considered as primitive inference patterns, which can faithfully represent every detailed inference logic of the DNN on this sample. Particularly, it has been observed [42] that various DNNs all learn interactions of different complexities with two-phase dynamics, and this well explains how a DNN's generalization power changes from under-fitting to over-fitting. Therefore, in this study, we prove the dynamics of a DNN gradually encoding interactions of different complexities, which provides a theoretically grounded mechanism for the over-fitting of a DNN. Experiments show that our theory well predicts the real learning dynamics of various DNNs on different tasks.", "sections": [{"title": "1 Introduction", "content": "Background: mathematically guaranteeing that the inference score of a DNN can be faithfully explained as symbolic interactions. Explaining the detailed inference logic hidden behind the output score of a DNN is considered one of the core issues for the post-hoc explanation of a DNN. However, after a comprehensive survey of various explanation methods, many studies [28, 1, 12] have unanimously and empirically arrived at a disappointing view of the faithfulness of almost all post-hoc explanation methods. Fortunately, the recent progress [27] has mathematically proven that given a specific input sample $x = [x_1,\\cdots, x_n]^T$, a DNN2 usually satisfy the three conditions. for a classification task usually only encodes a specific small set of interactions between input variables in the sample. It is proven that these interactions act like primitive inference patterns and can accurately predict all network outputs, no matter how we randomly mask the input sample\u00b3. An interaction refers to a non-linear relationship encoded by the DNN between a set of input variables in $S$. For example, as Figure 1 shows, a DNN may encode a non-linear relationship between the three image patches in $S = {X_1,X_2,X_3}$ to form a dog-snout pattern, which makes a numerical effect $I(S)$ on the network output. The complexity (or order) of an interaction is defined as the number of input variables in the set $S$, i.e., $\\text{order}(S) \\stackrel{\\text{def}}{=}|S|$.\nOur task. This study is inspired by the finding of Zhou et al. [45] that high-order (complex) interactions usually have a much higher risk of over-fitting than low-order (simple) interactions. Thus, in this study, we hope to track the change in the complexity of interactions during training, so as to"}, {"title": "2 Related work", "content": "Long-standing disappointment on the faithfulness of existing post-hoc explanation of DNNs. Many studies [30, 40, 29, 2, 15] have explained the inference score of a DNN, but how to mathematically formulate and guarantee the faithfulness of the explanation is still an open problem. For example, using an interpretable surrogate model to approximate the output of a DNN [3, 11, 35, 34] is a classic explanation technique. However, the good matching between the DNN's output and the surrogate model's output cannot fully guarantee that the two models use exactly the same inference patterns and/or use the same attention. Therefore, many studies [28, 12, 1] have unanimously and empirically arrived at a disappointing view of the faithfulness of current explanation methods. Rudin [28] pointed out that inaccurate post-hoc explanations of DNNs would be harmful to high-stakes applications. Ghassemi et al. [12] showed various failure cases of current explanation methods in the healthcare field and argued that using these methods to aid medical decisions was a false hope.\nProving interactions act as faithful inference primitives encoded by the DNN. Despite the disappointing view of post-hoc explanation methods, recent achievements in interactions provide a new perspective to formulate primitive inference patterns encoded by a DNN. It has been discovered[23] and proven [27] that the DNN's inference on a certain sample can be explained by only a small number of interactions. Furthermore, [21] discovered that salient interactions usually represented common inference patterns shared by different samples, and [4] proposed a method to extract generalizable interactions shared by different DNNs. Furthermore, [25] defined and learned the optimal baseline value for the Shapley value based on interactions. [5, 6] used interactions of different complexities to explain the encoding of different types of visual patterns in DNNs for image classification.\nExplaining the representation capacity of DNNs using interactions. The multi-order interaction has been used to explain the adversarial robustness [24], adversarial transferability [37], and generalization ability [41] of a DNN. In addition, [26] proved that compared to a standard DNN,"}, {"title": "3 Dynamics of interactions", "content": "3.1 Preliminary: interactions\nLet us consider a DNN $v$ and an input sample $x = [x_1,\\ldots,x_n]^T$ with $n$ input variables indexed by $N = {1,\\ldots, n}$. In different tasks, one can define different input variables, e.g., each input variable may represent an image patch for image classification or a word/token for text classification. Let us consider a scalar output of a DNN, denoted by $v(x) \\in \\mathbb{R}$. Previous studies [4, 44] show that the output score $v(x)$ can be decomposed into the sum of AND interactions and OR interactions.\n$v(x) = v(x_\\emptyset) + \\sum_{0 \\neq S \\subseteq N} I_{and}(S|x) + \\sum_{0 \\neq S \\subseteq N} I_{or}(S|x),$ (1)\nwhere the computation of $I_{and}(S|x)$ and $I_{or}(S|x)$ will be introduced later in Eq. (2).\nWe can understand AND-OR interactions as follows. Suppose that we are given an input sample $x$. According to Theorem 2, a non-zero interaction effect $I_{and}(S|x)$ indicates that the entire function of the DNN must equivalently encode an AND relationship between input variables in the set $S \\subset N$, although the DNN does not use an explicit neuron to model such an AND relationship. As Figure 1 shows, when the image patchs in the set $S_2 = {x_1=\\text{nose}, x_2=\\text{tongue}, x_3=\\text{cheek}}$ are all present (i.e., not masked), the three regions form a dog-snout pattern, and make a numerical effect $I_{and}(S_2|x)$ to push the output score $v(x)$ towards the dog category. Masking any image patch in $S_2$ will deactivate the AND interaction and remove $I_{and}(S_2|x)$ from $v(x)$. This will be shown by Theorem 2. Likewise, $I_{or}(S|x)$ can be considered as the numerical effect of the OR relationship encoded by the DNN between input variables in the set $S$. As Figure 1 shows, when one of the patches in $S_1={x_4=\\text{spotty region1}, x_5=\\text{spotty region2}}$ is present, a speckles pattern is used by the DNN to make a numerical effect $I_{or}(S_1|x)$ on the network output $v(x)$.\nFor each set $S \\subset N$, $S \\neq 0$, interactions $I_{and}(S|x)$ and $I_{or}(S|x)$ can be computed as follows [4, 44].\n$I_{and}(S|x) = \\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{and}(x_T),  I_{or}(S|x) = - \\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{or}(x_{\\overline{N\\setminus T}}),$ (2)\nwhere $x_T$ denotes the sample in which input variables in $\\overline{N \\setminus T}$ are masked, while input variables in $T$ are unchanged. The network output on each masked sample $v(x_T)$, $T \\subseteq N$, is decomposed into two components: (1) the component $v_{and}(x_T) = 0.5v(x) + \\gamma_T$ that exclusively contains AND interactions, and (2) the component $v_{or}(x_T) = 0.5v(x_{\\overline{T}}) - \\gamma_T$ that exclusively contains OR interactions, subject to $v(x_T) = v_{and}(x_T) + v_{or}(x_{\\overline{T}})$. Appendix E.1 shows that $v_{and}(x_T) = v(x_\\emptyset) + \\sum_{\\emptyset \\neq S'\\subseteq T} I_{and}(S'|x)$ and $v_{or}(x_T) = \\sum_{S'\\subset N: S'\\cap T \\neq \\emptyset} I_{or}(S'|x)$. The sparsest AND-OR interactions are extracted by minimizing the following objective [20]: $\\min_{\\{\\gamma\\}} \\sum_{S\\subseteq N} |I_{and}(S|x)| + |I_{or}(S|x)|$. See Appendix C for details.\nTheorem 1 (Sparsity property, proven by [27], and discussed in Appendix B). Given a DNN $v$ and an input sample $x$ with $n$ input variables, let $\\Omega \\stackrel{\\text{def}}{=} \\{S \\subseteq N : |I_{and}(S|x)| > \\tau\\}$ denote the set of salient AND interactions whose absolute value exceeds a threshold $\\tau$. If the DNN can generate relatively stable inference outputs $v(x_S)$ on masked samples, then the size of the set $|\\Omega|$ has an upper"}, {"title": "3.2 Two-phase dynamics of learning interactions", "content": "Zhang et al. [42] have discovered the following two-phase dynamics of interaction complexity during the training process. (1) As Figure 2 shows, before the training process, the DNN with randomly initialized parameters mainly encodes interactions of medium orders. (2) In the first phase, the DNN removes initial interactions of medium and high orders, and mainly encodes low-order interactions. (3) In the second phase, the DNN gradually learns interactions of increasing orders.\nTo better illustrate this phenomenon, we followed [42] to conduct experiments on different DNNs, including AlexNet [17], VGG [31], BERT [9], DGCNN [38], and on various datasets, including image data (MNIST [19], CIFAR-10 [16], CUB-200-2011 [36], and Tiny-ImageNet [18]), natural language data (SST-2 [32]), and point cloud data (ShapeNet [39]). For image data, we followed [42]"}, {"title": "3.3 Proof of the two-phase phenomenon", "content": "3.3.1 Analytic solution to interaction effects\nAs the foundation of proving the dynamics of the two phases, let us first derive the analytic solution to interaction effects during the training process. The proof of the dynamics of interactions is conducted under the assumption that the DNN has unavoidable weight noises. Despite the simplifying assumptions, later experiments show that our theory can well predict the true dynamics of all AND-OR interactions during the learning of real DNNs.\nReformulating the inference as a linear regression based on interaction triggering strength. For simplicity, let us only focus on the dynamics of AND interactions, because OR interactions can also be represented as a specific kind of AND interactions (see Appendix D for details). In this way, without loss of generality, let us just analyze the learning of AND interactions w.r.t. $v_{and}(x) = v(x_\\emptyset) + \\sum_{\\emptyset \\neq S \\subseteq N} I(S|x)$, and simplify the notation as $v(x) = v(x_\\emptyset) + \\sum_{\\emptyset \\neq S \\subseteq N} I(S|x)$ in the following proof. Our conclusions can also be extended to OR interactions, as mentioned above.\nGiven a DNN, we follow [26, 22] to reformulate the inference function of the network $v(x)$, which is inspired by the universal matching property of interactions in Theorem 2, i.e., given any arbitrarily masked input samples w.r.t. a random subset $S \\subset N$, the network output can always be represented as a linear sum of different interaction effects $v(x = x_S) = \\sum_{T \\subset S} I(T|x = x)$. Thus, the following equation rewrites the inference function of the DNN $v(x = x_S)$ as the weighted sum of triggering strength of interaction patterns (see Appendix E.2 for proof).\n$\\forall S \\subseteq N,  v(x=x_S) = f(x=x_S), \\text{ subject to } f(x) \\stackrel{\\text{def}}{=} \\sum_{T \\subseteq N} w_T J_T(x),$ (6)\nwhere the function $J_T(x)$ is a real-valued approximation of the binary indicator function $1(x_S \\text{ triggers the AND relation } T)$ in Eq. (3) and returns the triggering strength of the interaction pattern $T$. In particular, we set $w_\\emptyset = v(x = x), J_\\emptyset(x) = 1$. $J_T(x)$ is computed as a sum of compositional terms in the Taylor expansion of $v(x)$.\n$J_T(x) = \\sum_{\\pi \\in Q_T} \\frac{1}{\\prod_{i=1}^n \\pi_i ! } \\frac{\\partial^{\\pi_1 + \\ldots + \\pi_n} v} {\\partial x_1^{\\pi_1} \\ldots \\partial x_n^{\\pi_n} } |_{x=x_\\emptyset}  \\prod_{i \\in T} (x_i - b_i)^{\\pi_i},$ (7)\nwhere the scalar weight $w_T$ should be computed as $w_T = I(T|x=x)$ to satisfy the equality in Eq. (6), and $Q_T = { [\\pi_1,\\ldots, \\pi_n]^T : \\forall i \\in T, \\pi_i \\in \\mathbb{N}^+; \\forall i \\notin T, \\pi_i = 0 }$. See Appendix E.2 for proof.\nUnderstanding $J_T(x)$ and $w_T$. Let us consider a masked sample $\\hat{x}_S$ in which input variables in $\\overline{N \\setminus S}$ are masked. If $T \\subseteq S$, which means all input variables in $T$ are not masked in $\\hat{x}_S$, then $J_T(\\hat{x}_S) = 1$, indicating the interaction pattern is triggered; otherwise, $J_T(\\hat{x}_S) = 0$, indicating the interaction pattern is not triggered. $w_T$ is a scalar weight. Particularly, let $I_f(T|x)$ denote the interaction extracted from the function $f(x) = \\sum_{T \\subseteq N} w_T J_T(x)$, then we have $I_f(T|x) = w_T$.\nBased on Eq. (6), the learning of a DNN is reformulated as the learning of the scalar effect/weight $w_T$ for each interaction triggering function $J_T(x)$. In this way, we analyze the training process of a DNN in the new setting of linear regression in Eq. (8). We can roughly consider the learning problem as a regression to a set of potentially true interactions, because it has been discovered by [21, 4] that different DNNs for the same task usually encode similar sets of interactions. Therefore, the learning of a DNN can be considered as training a model to fit a set of pre-defined interactions. In spite of the above simplifying settings, subsequent experiments in Figure 4 still verify that our theoretical results can well predict the learning dynamics of interactions in real DNNs.\nSpecifically, let the DNN be trained on a set of samples $D = \\{(x,y)\\}$. According to Theorem 2, given each training sample $x$, output scores of the finally converged DNN on all $2^n$"}, {"title": "4 Conclusion", "content": "In this study, we have proven the two-phase dynamics of a DNN learning interactions of different orders. Specifically, we have followed [26, 22] to reformulate the learning of interactions as a linear regression problem on a set of interaction triggering functions. In this way, we have successfully derived an analytic solution to interaction effects when the DNN was learned with unavoidable parameter noises. This analytic solution has successfully predicted a DNN's two-phase dynamics of learning interactions in real experiments. Considering a series of recent theoretical guarantees of taking interactions as faithful primitive inference patterns encoded by the DNN [45, 27], our study has first mathematically explained why and how the learning process gradually shifts attention from generalizable (low-order) inference patterns to probably over-fitted (high-order) inference patterns."}, {"title": "A Properties of the AND interaction", "content": "The Harsanyi interaction [14] (i.e., the AND interaction in this paper) was a standard metric to measure the AND relationship between input variables encoded by the network. In this section, we present several desirable properties/axioms that the Harsanyi AND interaction $I_{and}(S|x)$ satisfies. These properties further demonstrate the faithfulness of using Harsanyi AND interaction to explain the inference score of a DNN.\n(1) Efficiency axiom (proven by [14]). The output score of a model can be decomposed into interaction effects of different patterns, i.e. $v(x) = \\sum_{S\\subseteq N} I_{and}(S|x)$.\n(2) Linearity axiom. If we merge output scores of two models $v_1$ and $v_2$ as the output of model $v$, i.e. $\\forall S \\subseteq N, v(x) = v_1(x_S) + v_2(x_S)$, then their interaction effects $I^{and}_1(S|x)$ and $I^{and}_2(S|x)$ can also be merged as $\\forall S \\subseteq N, I_{and}(S|x) = I^{and}_1(S|x) + I^{and}_2(S|x)$.\n(3) Dummy axiom. If a variable $i \\in N$ is a dummy variable, i.e. $\\forall S \\subseteq N \\setminus \\{i\\}, v(x_{S\\cup \\{i\\}}) = v(x_S) + v(x_{\\{i\\}})$, then it has no interaction with other variables, $\\forall \\emptyset \\neq S \\subseteq N \\setminus \\{i\\}, I_{and}(S\\cup \\{i\\}|x) = 0$.\n(4) Symmetry axiom. If input variables $i, j \\in N$ cooperate with other variables in the same way, $\\forall S \\subseteq N \\setminus \\{i, j\\}, v(x_{S\\cup \\{i\\}}) = v(x_{S\\cup \\{j\\}})$, then they have same interaction effects with other variables, $\\forall S \\subseteq N \\setminus \\{i, j\\}, I_{and}(S \\cup \\{i\\}|x) = I_{and}(S \\cup \\{j\\}|x)$.\n(5) Anonymity axiom. For any permutations $\\pi$ on N, we have $\\forall S \\subset N, I_{and}(S|x) = I_{and}(\\pi S|x)$, where $\\pi S \\stackrel{\\text{def}}{=} \\{\\pi(i)|i \\in S\\}$, and the new model $\\pi v$ is defined by $(\\pi v)(x_{\\pi S}) = v(x_S)$. This indicates that interaction effects are not changed by permutation.\n(6) Recursive axiom. The interaction effects can be computed recursively. For $i \\in N$ and $S \\subseteq N \\setminus \\{i\\}$, the interaction effect of the pattern $S \\cup \\{i\\}$ is equal to the interaction effect of $S$ with the presence of $i$ minus the interaction effect of $S$ with the absence of $i$, i.e. $\\forall S \\subseteq N\\setminus \\{i\\}, I_{and}(S\\cup \\{i\\}|x) = I_{and}(S|x, i \\text{ is always present}) - I_{and}(S|x)$. $I_{and}(S|x, i \\text{ is always present})$ denotes the interaction effect when the variable $i$ is always present as a constant context, i.e. $I_{and}(S|x, i \\text{ is always present}) = \\sum_{L\\subseteq S}(-1)^{|S|-|L|}v(x_{L\\cup \\{i\\}})$.\n(7) Interaction distribution axiom. This axiom characterizes how interactions are distributed for \"interaction functions\" [33]. An interaction function $v_T$ parameterized by a subset of variables $T$ is defined as follows. $\\forall S \\subseteq N$, if $T \\subseteq S, v_T(x_S) = c$ ; otherwise, $v_T(x_S) = 0$. The function $v_T$ models pure interaction among the variables in $T$, because only if all variables in $T$ are present, the output value will be increased by $c$. The interactions encoded in the function $v_T$ satisfies $I_{and}(T|x) = c$, and $\\forall S \\neq T, I_{and}(S|x) = 0$.\nCondition 3 implies that the classification confidence of the DNN does not significantly degrade on masked input samples. The classification/detection of masked/occluded samples is common in real"}, {"title": "B Common conditions for sparse interactions", "content": "Ren et al. [27] have formulated three mathematical conditions for the sparsity of AND interactions, as follows.\nCondition 1. The DNN does not encode interactions higher than the M-th order: $\\forall S \\in \\{S \\subseteq N | |S| \\geq M + 1\\}, I_{and}(S|x) = 0$.\nCondition 1 implies that the DNN does not encode extremely high-order interactions. This is because extremely high-order interactions usually represent very complex and over-fitted patterns, which are unnecessary and unlikely to be learned by the DNN in real applications.\nCondition 2. Let us consider the average network output $\\overline{v}^{(k)} \\stackrel{\\text{def}}{=} E_{|S|=k}[v(x_S) - v(x_\\emptyset)]$ over all masked samples $x_S$ with $k$ unmasked input variables. This average network output monotonically increases when $k$ increases: $\\forall k' < k$, we have $\\overline{v}^{(k')} < \\overline{v}^{(k)}$.\nCondition 2 implies that a well-trained DNN is likely to have higher classification confidence for input samples that are less masked.\nCondition 3. Given the average network output $\\overline{v}^{(k)}$ of samples with $k$ unmasked input variables, there is a polynomial lower bound for the average network output of samples with $k'(k' < k)$ unmasked input variables: $\\forall k' \\leq k, \\overline{v}^{(k')} \\geq \\binom{k}{k'}^p \\overline{v}^{(k)}$, where $p > 0$ is a positive constant."}, {"title": "C Details of optimizing {\u03b3T} to extract the sparsest AND-OR interactions", "content": "A method is proposed [20, 4] to simultaneously extract AND interactions $I_{and}(S|x)$ and OR interactions $I_{or}(S|x)$ from the network output. Given a masked sample $x_T$, [20] proposed to learn a decomposition $v(x_{\\overline{T}}) = v_{and}(x_T)+v_{or}(x_{\\overline{T}})$ towards the sparsest interactions. The component $v_{and}(x_T)$ was explained by AND interactions, and the component $v_{or}(x_{\\overline{T}})$ was explained by OR interactions. Specifically, they decomposed $v(x_{\\overline{T}})$ into $v_{and}(x_T) = 0.5 \\cdot v(x) + \\gamma_T$ and $v_{or}(x_{\\overline{T}}) = 0.5 \\cdot v(x_{\\overline{T}}) - \\gamma_T$, where $\\{\\gamma_T : T \\subseteq N\\}$ is a set of learnable variables that determine the decomposition. In this way, the AND interactions and OR interactions can be computed according to Eq. (2), i.e., $I_{and}(S|x) = \\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{and}(x_T)$, and $I_{or}(S|x) = \\sum_{T\\subseteq S}(-1)^{|S|-|T|}v_{or}(x_{\\overline{N\\setminus T}})$.\nThe parameters $\\{\\gamma_T\\}$ were learned by minimizing the following LASSO-like loss to obtain sparse interactions:\n$\\min_{\\{\\gamma\\}} \\sum_{S \\subset N} |I_{and}(S|x)| + |I_{or}(S|x)| $ (11)\nRemoving small noises. A small noise $\\delta_S$ in the network output may significantly affect the extracted interactions, especially for high-order interactions. Thus, [20] proposed to learn to remove a small noise term $\\delta_S$ from the computation of AND-OR interactions. Specifically, the decomposition was rewritten as $v_{and}(x_T) = 0.5(v(x_{\\overline{T}}) - \\delta_T) + \\gamma_T$ and $v_{or}(x_{\\overline{T}}) = 0.5(v(x_{\\overline{T}}) - \\delta_T) + \\gamma_T$. Thus, the parameters $\\{\\delta_T\\}$, and $\\{\\gamma_T\\}$ are simultaneously learned by minimizing the loss function in Eq. (11). The values of $\\{\\delta_T\\}$ were constrained in $[-5\\zeta, 5\\zeta]$ where $\\zeta = 0.02 \\cdot |v(x) - v(x_\\emptyset)|$."}, {"title": "D OR interactions can be considered a special kind of AND interactions", "content": "The OR interaction can be considered a specific kind of AND interaction interaction, when we flip the masked state and presence (unmasked) state of each input variable.\nGiven an input sample $x \\in \\mathbb{R}$, let $x_{\\overline{T}}$ denote the masked sample obtained by masking input variables in $\\overline{N \\setminus T}$, while leaving variables in $T$ unchanged. Specifically, the baseline values $b \\in \\mathbb{R}^n$ are used to mask the input variables, which represent the masked states of the input variables. The definition of $x_{\\overline{T}}$ is given as follows.\n$(x_{\\overline{T}})_i = \\begin{cases} x_i & i \\in T \\\\ b_i, & i \\in \\overline{N\\setminus T} \\end{cases}$ (12)\nBased on the above definition, the AND interaction is computed as $I_{and}(S|x) = \\sum_{T \\subset S}(-1)^{|S|-|T|}v_{and}(x_T)$, while the OR interaction is computed as $I_{or}(S|x) = \\sum_{T \\subset S}(-1)^{|S|-|T|}v_{or}(x_{\\overline{N\\setminus T}})$. To simplify the analysis, let us assume $v_{and}(\\cdot) = v_{or}(\\cdot) = 0.5v(\\cdot)$.\nThen, let us consider a masked sample $\\widetilde{x}_T$, where we flip the masked state and presence (unmasked) state of each input variable. In this way, $\\widetilde{x}_T$ is defined as follows.\n$(\\widetilde{x}_T)_i = \\begin{cases} x_i & i \\in \\overline{N\\setminus T} \\\\ b_i, & i \\in T \\end{cases}$ (13)\nTherefore, the OR interaction $I_{or}(S|x)$ in Eq. 2 in main paper can be represented as an AND interaction $I_{or}(S|x)$, as follows.\n$\\begin{aligned} I_{or}(S|x) &= - \\sum_{T \\subseteq S}(-1)^{|S|-|T|}v(x_{\\overline{N\\setminus T}}), \\\\ &= - \\sum_{T \\subseteq S}(-1)^{|S|-|T|}v(\\widetilde{x}_{T}), \\\\ &= -I_{and}(\\widetilde{S}|x). \\end{aligned}$ (14) (15) (16)\nIn this way, the proof of the sparsity of AND interactions in [27] can also extend to OR interactions. Furthermore, we can simplify our analysis of the DNN's learning of interactions by only focusing on AND interactions."}, {"title": "E Proof of theorems", "content": "E.1 Proof of Theorem 2\nProof. (1) Universal matching theorem of AND interactions.\nWe will prove that output component $v_{and"}, "x_S)$ on all $2^n$ masked samples $\\{x_S : S \\subseteq N\\}$ could be universally explained by the all interactions in $S \\subseteq N$, i.e., $\\forall \\emptyset \\neq S \\subseteq N,v_{and}(x_S) = \\sum_{\\emptyset\\neq T \\subseteq S} I_{and}(T|x) + v(x_\\emptyset)$. In particular, we define $v_{and}(x_\\emptyset) = v(x_\\emptyset)$ (i.e., we attribute output on an empty sample to AND interactions).\nSpecifically, the AND interaction is defined as $I_{and}(T|x) = \\sum_{L\\subset T}(-1)^{|T|-|L|}v_{and}(x_L)$ in 2. To compute the sum of AND interactions $\\sum_{\\emptyset\\neq T \\subseteq S} I_{and}(T|x) = \\sum_{\\emptyset\\neq T \\subseteq S} \\sum_{L \\subset T}(-1)^{|T|-|L|}v_{and}(x_L)$, we first exchange the order of summation of the set $L \\subset T \\subset S$ and the set $T\\supset L$. That is, we compute all linear combinations of all sets $T$ containing $L$ with respect to the model outputs $v_{and}(x_L)$ given a set of input variables $L$, i.e., $\\sum_{T:L\\subset T \\subset S}(-1)^{|T|-|L|}v_{and}(x_L)$. Then, we compute all summations over the set $L \\subset S$.\nIn this way, we can compute them separately for different cases of $L \\subset T \\subset S$. In the following, we consider the cases (1) $L = S = T$, and (2) $L \\subset T \\subset S$, $L \\neq S$, respectively.\n(1) When $L = S = T$, the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{and}(x_L)$ is $(-1)^{|S|-|S|}v_{and}(x_L) = v_{and}(x_L)$.\n(2) When $L \\subset T \\subset S$, $L \\neq S$, the linear combination of all subsets $T$ containing $L$ with respect to the model output $v_{and}(x_L)$ is $\\sum_{T:L \\subset T \\subset S}(-1)^{|T|-|L|}v_{and}(x_L)$. For all sets $T : S \\supseteq T \\supseteq L$, let us consider the linear combinations of all sets $T$ with number $|T|$ for the model output $v_{and}(x_L)$, respectively. Let $m := |T| - |L|$, ($0 < m \\leq |S|-|L|$), then there are a total of $C^{|S|-|L|}_m$ combinations of all sets $T$ of order $|T|$. Thus, given $L$, accumulating the model outputs $v_{and}(x_L)$ corresponding to all $T \\supset L$, then $\\sum_{T:L\\subset T \\subset S}(-1)^{|T|-|L|}v_{and}(x_L) = v_{and}(x_L) \\cdot \\sum_{m=0}^{|S|-|L|} C^{|S|-|L|}_m(-1)^m = 0$.\nPlease see the complete derivation of the following formula.\n$\\sum_{\\emptyset/ T \\subset S} I_{and}(T|x) = \\sum_{\\emptyset/ T \\subset S} \\sum_{L\\subset T}(-1)^{|T|-|L|}v_{and}(x_L) = \\sum_{L\\subset S} \\sum_{T:L\\subset T \\subset S}(-1)^{|T|-|L|}v_{and}(x_L) - v(x_\\emptyset) = v_{and}(x_S) + \\sum_{L\\subset S,L\\neq S} v_{and}(x_L). \\sum_{m=0}^{|S|-|L|} C^{|S|-|L|}_m(-1)^m - v(x_\\emptyset) = v_{and}(x_S) + \\sum_{L\\subset S,L\\neq S}  0 -v(x_\\emptyset) = v_{and}(x_S) - v(x_\\emptyset) = v_{and}(x_S) - v(x_\\emptyset)$ (17)\nThus, we have $\\forall \\emptyset \\neq S \\in N, v_{and}(x_S) = \\sum_{\\emptyset\\neq T \\subset S} I_{and}(T|x) + v(x_\\emptyset)$.\n(2) Universal matching theorem of OR interactions.\nAccording to the definition of OR interactions, we will derive that $\\forall S \\subset N, v_{or}(x_{\\overline{S}}) = \\sum_{T:T \\cap S \\neq \\emptyset} I_{or}(S|x)$, where we define $v_{or}(x_\\emptyset) = 0$ (recall that in Step (1), we attribute the output on empty input to AND interactions).\nSpecifically, the OR interaction is defined as $I_{or}(T|x) = - \\sum_{L \\subset T}(-1)^{|T|-|L|}v_{or}(x_{\\overline{N\\setminus L}})$ in 2. Similar to the above derivation of the universal matching theorem of AND interactions, to compute the sum of OR interactions $\\sum_{T:T \\cap S \\neq \\emptyset} I_{or}(T|x) = \\sum_{T:T \\cap S \\neq \\emptyset} [-\\sum_{L \\subset T}(-1)^{|T|-|L|}v_{"]}