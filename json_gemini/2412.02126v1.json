{"title": "Benchmarking symbolic regression constant optimization schemes", "authors": ["L.G.A dos Reis", "V.L.P.S. Caminha", "T.J.P.Penna"], "abstract": "Symbolic regression is a machine learning technique, and it has seen many advancements in recent years, especially in genetic programming approaches (GPSR). Furthermore, it has been known for many years that constant optimization of parameters, during the evolutionary search, greatly increases GPSR performance However, different authors approach such tasks differently and no consensus exists regarding which methods perform best.\nIn this work, we evaluate eight different parameter optimization methods, applied during evolutionary search, over ten known benchmark problems, in two different scenarios. We also propose using an under-explored metric called Tree Edit Distance (TED), aiming to identify symbolic accuracy. In conjunction with classical error measures, we develop a combined analysis of model performance in symbolic regression. We then show that different constant optimization methods perform better in certain scenarios and that there is no overall best choice for every problem. Finally, we discuss how common metric decisions may be biased and appear to generate better models in comparison.", "sections": [{"title": "1. Introduction", "content": "Applications of artificial intelligence in many different areas have been standard practice in recent years. Symbolic regression (SR) is one of those rising techniques, with applications that range from materials science [21], the energy sector [32], to psychology [31] and especially scientific discovery [33], in areas such as physics [24]. SR is a machine learning technique that finds a symbolic expression that best fits a dataset. In the opposite direction from traditional AI methods, symbolic regression represents a white-box approach that searches for a mathematical function that describes a given scenario.\nSince its origins, many approaches have been proposed for symbolic regression implementations and improvements. There are many new techniques proposed to model SR, such as those structured around Bayesian statistics [20], random search [8], or more recently deep neural networks and large language models [27]\u2013[29]. Nevertheless, the area is still dominated by genetic programming implementations [1], [6], [14], [24], [30], called GPSR, since they are more robust, produce better results, and have a way vaster literature than any other alternative.\nSaid that, a well-known fact about GPSR models is the impact of constant optimization during the evolutionary process. Many studies have shown a great increase in performance once a technique such as simulated annealing [2], particle swarm optimization [15], non-linear least squares [3], [14], [22] or differential evolution [11] has been applied. Other popular models, such as PySR [30], utilize different alternatives, such as optimization by BFGS or Nelder-Mead. However, even though this information is common sense among symbolic regression researchers, there are no studies that identify which is the best parameter optimization method to use in a model, or the impact different alternatives have on convergence. A discussion about distinct optimization approaches after a solution has been found was developed in [18], but no study has demonstrated yet the different impact they cause as part of the GP method.\nConsidering the aforementioned, this paper performs a comprehen- sive benchmark study on the impact modern constant optimization techniques have on the SR performance, once they are implemented as part of the evolutionary search. We also propose the use of Tree Edit Distance as a metric to understand SR performance, to not only quantify how well a solution fits the data but also how symbolically accurate an expression is. To achieve this, we share a preprocessing algorithm that simplifies expressions to a common denominator, for accurate comparison.\nThis study is structured as follows. In section 2 we describe the problem of symbolic regression and constant optimization. The methods studied, the metrics utilized, and the preprocessing technique developed are described in the methodology section 3. Results are presented and discussed in section 4 and our conclusions are exposed in section 5."}, {"title": "2. Motivation", "content": "Even though many different approaches to symbolic regression have been proposed currently, since the start [1] of the area, SR algorithms were constructed upon genetic programming techniques (GPSR). These still represent a great proportion of state-of-the-art models [22], [24], [26], [30]. In this, mathematical expressions are represented in a computer, usually as expression trees, and are evolved through a process of mutation, crossover, and selection, to better approach the input data.\nA well-established concept in GPSR is that of constant optimiza- tion. Many studies [7], [12], [14], [18] have already shown that by optimizing the parameters of the discovered expression in the evolu- tionary process, the model performance increases greatly. Not only concerning the minimization of numerical metrics but also by finding the correct mathematical expression that describes the data. For that reason, most methods of GPSR utilize some algorithm to optimize parameters in their model[6], [22]\u2013[24], [26], [30].\nThe process of constant optimization in SR usually takes a com- mon form. The constants of candidate mathematical expression are represented as abstract symbols, that are further set to constant at evaluation time. This general idea presents itself in many different applications, such as Abstract Expression Grammar (AEG) [7], [15], or Ephemeral Random Constants (ERC) [26]. In tree representa- tion, the terminal nodes (parameters) are replaced with ephemeral constants and a vector stores the current value of each parameter (fig. 1). Through the evolutionary process, the expression changes, the constant optimization method is re-applied, and new constants are generated, which in turn updates the vector of parameters. Once tree evaluation is necessary, it simply requires substituting the stored values back in the ephemeral constants.\nNevertheless, even though most GPSR methods utilize some algo- rithm for constant optimization in ERC parameters (fig. 1), there are no studies so far that investigate which methods perform better in this context. Specifically, during the evolutionary process, and in what kind of applications these algorithms excel. To answer this question, we will utilize a straightforward GPSR model, developed by the au- thor and based upon [15]. This was chosen instead of a consolidated state-of-the-art algorithm because these models are usually highly complex and perform many different operations and simplifications, that could interfere in unknown ways with the benchmark. So a simple model, that is composed only of the bare bones of a generic programming algorithm, should highlight the strong suits of each constant optimization method. At the same time, the analysis is general enough to apply to any of the GPSR models currently in the market, since all of them are based on the same core."}, {"title": "3. Methodology", "content": "Following we shortly describe the unconstrained constant optimiza- tion algorithms evaluated in the benchmark. Each is applied to the ERC, representing the equation, as shown in fig. 1, and an updated constant vector is generated. Additionally, BFGS, CG, LS, and Nelder- Mead require an initial guess, for these methods, we tested two differ- ent situations. Firstly, we insert back the current constants vector as an initial guess, this case is stated in the tests simply as the optimiza- tions name. Secondly, we generate a random vector sampled from a normal distribution as an initial guess. Optimizations treated this way are followed by Random, after their names. This was done to understand the impact an initial guess would have on the capability of such methods to explore the search space.", "subsections": [{"title": "1. NoOpt", "content": "No Optimization (NoOpt) represents the dummy method, where no constant optimization algorithm is applied. It serves as a baseline and comparison to understand how effective constant optimizations are."}, {"title": "2. Broyden-Fletcher-Goldfarb-Shanno - BFGS", "content": "Quasi-Newton methods are a class of optimization algorithms that use updating formulas to approximate the hessian of a differentiable function, and from that solve unconstrained optimization problems. The hessian matrix is utilized to guide the search for a minimum. BFGS has been proved to be the most effective of those methods, and it utilizes gradient evaluations to update the hessian approximation, without explicitly calculating second derivatives. Applications range from engineering and physics to machine learning. [13]"}, {"title": "3. Conjugate Gradient Descent - CG", "content": "Gradient Descent is one of the most well-known optimization methods for minimizing unconstrained nonlinear functions and solving systems of linear equations. Multiple versions inspired by the original algorithm were developed, but conjugate gradient descent has shown faster convergence than other alternatives. It works by moving in the direction generated by combining the steepest descent and the conjugate direction. Recently, it has been widely used in deep learning algorithms, particularly in the context of high-dimensional data. [5]"}, {"title": "4. Levenberg\u00e2\u20ac\u201cMarquardt Least Squares - LS", "content": "Levenberg-Marquardt is a common optimization approach to solve the least squares minimization problem. It combines the Gauss-Newton method and the already discussed gradient descent. The LM algorithm adaptively changes between them by considering a damping coefficient \u03bb. Such coefficient is initialized to be large, which results in gradient descent steps in the downhill direction. Once the solution improves, \u03bb is reduced, and the Gauss-Newton method enters into play, to effectively reach the minima. Levenberg-Marquardt is commonly applied in curve fitting for functions with many parameters, but it also sees use in machine learning and data fitting. [19]"}, {"title": "5. Particle Swarm Optimization - PSO", "content": "Particle Swarm Optimization is a method inspired by the flocking of birds. Potential solutions are modeled as particles that move through the search space. Each coordinate of such particles in this space represents a parameter to be optimized. Positions evolve by calculating a velocity vector that depends on three pieces of information: the best position each has encountered so far, the best position found by any particle in the system, and an inertia vector. PSO is particularly useful for high-dimensional, nonlinear problems and is suited for applications where gradients cannot be calculated. [4]"}, {"title": "6. Nelder-Mead", "content": "Nelder-Mead is an optimization algorithm that works by adapting a geometric shape called simplex, which consists of n + 1 vertices in a n-dimensional space. It is independent of derivative calculations, where the simplex is modified by processes of reflection, expansion, contraction, and shrinkage. At each iteration, the model explores the search space by function evaluation on each vertex. It is widely used in real- world situations for optimizing functions where the gradient is unknown. The method also works well for smooth, unimodal problems but its efficiency may suffer in high-dimensional landscapes. [9]"}, {"title": "7. Differential Evolution", "content": "Differential evolution (DE) is a simple population-based, stochastic, evolutionary algorithm. In some comparisons, DE has presented greater efficiency than many stochastic methods, such as simulated annealing, evolutionary programming, and particle swarm optimization. It evolves a population of candidate solutions by processes of mutation, crossover, and selection, similar to GPSR. Differential evolution is simple yet effective in exploring large, multi-modal search spaces. For that reason, it is sometimes applied in global optimization tasks for which traditional methods struggle.[17]"}, {"title": "8. Generalized Dual Annealing", "content": "Generalized Dual Annealing (GSA), also called dual anneal- ing by the scipy implementation (and the name we use), is an optimization algorithm inspired by the annealing process in met- allurgy. Classical simulated annealing (CSA) creates a modified version of the current solution every iteration and the probability of acceptance for this new solution is a function of \"tempera- ture\". This parameter starts very high, to explore more space, and then cools off. CSA was then combined with Fast Simu- lated Annealing to create GSA. Dual annealing is adapted to complex, nonlinear, and multi-modal problems, it is widely used in combinatorial optimization and engineering. [16]"}]}, {"title": "3.1. Test Problems and Search Space", "content": "To study the behavior of multiple optimization methods, a compre- hensive set of test problems was selected accordingly to table 1. Only univariate functions were chosen to better understand the effects of the optimization methods in similar problems - the difficulty comes from the number of constants and complex relationships, not extra dimensions.\nMoreover, symbolic regression problems are highly dependent on the set of functions utilized as a basis for the search space. In most GPSR methods, one may choose a set of functions and mathematical operators, such as {sin, cos, \u221a, ln} and {+, *}, respectively, to utilize. The algorithm will then sample from these sets the terms that will compose any expression. In other words, this set is composed of the basis functions for the function space that represents the search of the SR model. Any expression that the model can generate is a linear combination of the set of basis functions. Therefore, by increasing the number of elements in this set, the search space increases drastically, which implies a trade-off between solution diversity, i.e., the number of different solutions the model can reach, and the complexity of the regression problem. If the search space is too large, the model may get stuck in local minima, and never find the target solution.\nGiven what was discussed, we have studied different subsets of the expressions in table 1, according to the basis functions utilized. The standard problems were evaluate with the same set: {+, -, *, /, x2, x\u00b3, \u221a, sin, cos, tan, tanh, abs, In, ex, e\u00af*}. Each expression has a corresponding set of specific basis functions shown in table 1. Since we are decreasing the complexity of the search task, they can be interpreted as one of the many ways to give information to the model, and thus make the solution easier to find.\nFor simplicity, the problems projectile motion, radioactive decay, damped pendulum, and F11 are not shown in the following analysis. Given that the first two exhibit the same behavior as F1, and the other two as F7, for every study performed. Complete results can be accessed in the complementary material."}, {"title": "3.2. Metrics", "content": "The comparison between optimization methods is calculated through the following metrics.\nComplexity Even though many different complexity measures have been proposed for symbolic regression algorithms, we have opted for a simple approach. We calculate it as the trees size, defined as the number of nodes it has.\nNumerical Accuracy We access the solution's numerical accu- racy by two different metrics. First, we consider the mean squared error as a more direct measure of the model's error, defined as", "subsections": [{"title": null, "content": "$\nMSE = \\frac{1}{n} \\sum_{i=0}^{n} (y_i - \\hat{y_i})^2.\n$"}]}, {"title": null, "content": "Secondly, we utilize the standard metric for SR benchmarks, the coefficient of determination, defined as", "subsections": [{"title": null, "content": "$\nR^2 = 1- \\frac{\\sum_{i=0}(y_i - \\hat{y_i})^2}{\\sum_{i=0}(y_i - \\bar{y})^2}.\n$"}]}, {"title": null, "content": "Both metrics were chosen to best achieve an equilibrium, since R2 usually favors overfitted models, while MSE penalizes outliers.\nSymbolic Accuracy Most current symbolic regression valida- tion is done by one method of numerical accuracy or some variation of a success metric, in which one counts the number of times the solution has reached the exact expected expression. The problem with that analysis is that the first one does not consider how the actual solution looks, which is nonsensical since the purpose of symbolic regression algorithms is to find a mathematical expression. Even though the second analysis considers algebraic representation, it is only a simple and direct measure. No information can be obtained about how close the solution was to the expected expression.\nTo address these problems we propose the use of Tree Edit Distance (TED), as an equivalent of standard numerical accuracy metric for symbolic closeness. In a recent paper [29] has also used TED, alongside other metrics, to study neural symbolic regression. TED is defined as the number of operations of insertion, removal, and substitution necessary to transform a tree into another one. fig. 2 shows an example of the process of TED calculation."}, {"title": "3.3. Preprocessing", "content": "It is common knowledge that for accurately estimating if the model has produced the expected symbolic expression a simplification pro- cess must be applied to them. Ordinarily, most authors [25], [29] use the simplify method from Python's sympy library. As recognized by some [25], this is a far too simple process, and many complicated solutions are not correctly evaluated, given that there are many ways to write the same expression.\nTo address the said issue, before calculating TED, every solution underwent a simplification process according to fig. 3. Such a pro- cedure is necessary since SR models produce solutions that can be expressed in many different ways. Thus, for an accurate evaluation, all of them must go through a unification process. Also notice that the last step is ERC, given that it is more relevant to recognize the solutions representation instead of how well the parameters are opti- mized. Once the right expression has been found, any good enough optimization algorithm can work on it and accurately optimize its parameters to fit the data."}, {"title": "4. Results and discussion", "content": "In fig. 4 we study the mean squared error distribution for the most rep- resentative cases in table 1. The problems can be classified into three groups, easy, medium, and hard as shown in each figure's top right corner. Such classification helps understand the collective behavior of problems, even though the grouping is arbitrary, and used only to simplify analysis. First, SR without optimization does not converge in any case, as observed for the NoOpt control case. It is also notice-able that the Dual Annealing and Differential Evolution methods presented meager results, consistently producing errors significantly greater than every other method. LS has been shown to reach more accurate solutions than other methods, which could imply it is a more robust algorithm to optimize constants precisely.\nSecondly, easy cases are independent of the optimization method, and as long as you have one, the model converges with high precision. For medium problems, on the other hand, the result's success is not so easily defined. In cases such as F4, some methods show lower error distributions, but the difference between 10-11 and 10-9 is not usually meaningful in most scenarios, and both results could be acceptable. In other problems, the general solution MSE ranges from 10-6 to 10-2. Such interval on the error requires specific analysis for the problem at hand, in some scenarios these values could be useless, while in others it could be more than enough. These considerations imply that by looking at a single metric such as numerical error, a straightforward answer regarding the performance of each method is not so easily defined. Now for hard problems, we see a behavior that appears not influenced by constant optimization algorithms, given that no method converged consistently on low error regions. This could imply that the lack of precision is caused by a poor symbolic regression model, which is incapable of finding the right solution, despite the optimization chosen.\nIn fig. 5 we show the TED distribution for the same cases studied. For easy problems, the observed behavior is very similar to fig. 4, far too simple problems reach TED of zero, independently of the optimization algorithm (as long as you have one). The only exception is Dual Annealing and Differential Evoltion for the F4 specific problem, which could indicate that these methods need particular parameter adjustments for each scenario, to perform appropriately. On the other hand, medium cases show a richer behavior, where there are noticeable differences regarding each of the optimization methods. BFGS, LS, PSO and Nelder-Mead algorithms show each a problem where they have performed better than average, such as BFGS on F6, LS and PSO on F5, and BFGS, PSO and Nelder-Mead on F4.\nConcerning hard problems, two peculiar behaviors emerge. The previously discussed aspect related to the four mentioned algorithms is still present in harder problems, for each scenario one produces better results than the others, even though the variations are smaller. Meanwhile, DE, DA, and especially NoOpt show the oddest behavior. They appear to achieve better TED values than any method, even though they haven't reached significant MSE values, nor presented consistent TED results for easy and medium problems."}, {"title": "4.1. Size Correlation", "content": "To better understand the odd TED behavior, fig. 6 shows the corre- lation between TED and MSE with expression size, calculated as the length of the equivalent binary tree. First, we notice that MSE and size are not meaningfully correlated, as one would expect. On the other hand, TED and size have a very high correlation and small p-values for every method, which implies that short expressions produce lower TED values.\nSince symbolic regression algorithms try to minimize numerical error, one would expect that if a method isn't capable of converging to the right expression, it would fall into local minima and optimize it the best it can. Such a process would bloat the solution to reduce MSE. Now, if a constant optimization algorithm isn't capable of accurately minimizing the error for new constants, adding more won't increase MSE. Thus the solution stays short, as can be observed in fig. 8 for the size distribution of NoOpt. This combined behavior explains why NoOpt, DA and DE show unexpectedly good results for the TED distribution in figure fig. 5. Smaller expressions require fewer steps to transform into another since one needs only to create the missing link, not remove unnecessary branches."}, {"title": "4.2. Combined Analysis", "content": "Thus, we conclude that just like one cannot look at MSE alone, TED should not be treated independently, and complementary analysis of size and error should be considered. To address this dependency problem in SR metrics, we propose a combined analysis that looks at MSE and TED together. In this case, an MSE value that will be con- sidered a solution's success must be chosen, to filter out expressions that present small errors despite presenting no symbolic meaning. This kind of analysis should be adapted to error values appropriate to one's problem. Nonetheless, common ranges usually considered as stopping conditions (such as 10-6) will represent a great range of scenarios, especially given that every method is subject to the same criteria. Besides, given that the model has reached the expected solu- tion, tuning the parameters for smaller errors is easy enough. Such considerations are not strange to SR analysis, given that some success rate is usually applied to these studies [18], in which a fixed value for MSE is commonly chosen. Such a process is similar to selecting the number of bins in a histogram, it carries an intrinsic arbitrariness, and different analyses will surface for distinct values, nonetheless, one must be chosen.\nIn fig. 9, we present a cumulative distribution plot for a range of TED values, and MSE less than or equal to 10-6. The horizontal axis represents the value of TED for which the number of points converges. First, it is noticeable that NoOpt did not converge for any threshold in all cases studied. This is the desired behavior, given that this method is incapable of optimizing constants. As discussed previously, it only reaches low values of TED because the given solutions are short. Thus, we were successful in removing this odd behavior, using the analysis in fig. 9. Also, for most problems and methods, the distribution is concentrated at TED = 0, which could show interesting behavior concerning the convergence of symbolic regression models.\nSecondly, as expected, similar results for the easy cases are observed. Solutions appear to be independent of the constant optimization cho- sen, given that at least one of them is applied. Following, harder prob- lems also exhibit distributions that appear to be method-independent, where for the majority of cases no algorithm is capable of leading the SR model to the right solution. This behavior seems to indicate that the symbolic regression algorithm utilized is not capable of accu-"}, {"title": "5. Conclusion", "content": "In this work, we have studied the impact different constant optimiza- tion methods have on Symbolic Regression predictions when applied during the evolutionary search. Nine test problems were studied in two different scenarios, with varying amounts of information. To evaluate performance, we utilized MSE and R\u00b2 in conjunction with a new metric called TED to estimate the symbolic accuracy of found expressions. A preprocessing simplification procedure was also im- plemented, to achieve more accurate results.\nInvestigation of MSE and TED distributions separately has led us to identify three major groups of problems. For easy ones, every approach converges, given that a constant optimization method is chosen. In hard problems, no alternative converges, indicating that the issue is not related to parameter optimization. At last, medium problems exhibit behavior that is method-dependent, meaning that for each particular case, different methods perform best. Given that this classification depends on how powerful the SR model is, it means that for most problems we should expect that different constant optim- ization techniques will produce distinct results.\nFollowing, we detect a very high correlation between expression size (complexity) and TED. Such behavior implies that smaller solu- tions produce lower values of TED. We have shown that this behavior relates to inaccurate evaluations and that it is necessary to consider expression size when analyzing TED.\nAt last, we have proposed a combined analysis of numeric and symbolic errors. Such an approach allows us to understand better the actual percentage of useful/meaningful solutions, that accurately describe a given problem. The analysis revealed that methods like PSO and BFGS should be set as default in SR models, once they perform better over a wider range of problems. In one scenario, BFGS has achieved greater performance with less information. In contrast, Levenberg\u00e2\u20ac\u201cMarquardt and Nelder-Mead have shown to be more powerful alternatives to harder problems. Although, the latter presents a curious behavior, in which it performs poorly in easy to medium problems, and greatly improves performance for harder ones.\nAdditionally, studying R\u00b2 distributions, we notice that this met- ric is way more \"forgiving\" compared to MSE and could give the impression of a better method. Many more solutions can describe data variation, but only those that achieve low error seem to produce accurate symbolic expressions. Also note that, contrary to what is standard in the area, a table of average values, over a wide range of problems and iterations, is not capable of accurately describing every behavior observed, and a more thorough analysis is required."}]}