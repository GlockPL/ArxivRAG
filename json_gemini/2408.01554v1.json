{"title": "Robot-Enabled Machine Learning-Based Diagnosis of Gastric Cancer Polyps Using Partial Surface Tactile Imaging", "authors": ["Siddhartha Kapuria", "Jeff Bonyun", "Yash Kulkarni", "Naruhiko Ikoma", "Sandeep Chinchali", "Farshid Alambeigi"], "abstract": "In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.", "sections": [{"title": "I. INTRODUCTION", "content": "Gastric cancer (GC) is the fifth most commonly diagnosed cancer worldwide and the fourth leading cause of cancer-related mortality [1]. A major contributor to this challenge is the fact that a substantial portion \u2212 up to 62% \u2212 of GC cases are detected at advanced stages, contributing to poorer overall survival rates compared to cases identified at early stages [2]. Upper endoscopy is the primary method for the initial detection of GC lesions as it allows for an inside view of the gastric tract lining where tumors originate. At the advanced GC (AGC) stages, tumors have infiltrated the muscularis propria [3] and can be identified and classified through their morphological characteristics (i.e., their geometry and texture) visible through the images provided by an endoscope. Borrmann classification [3] is a common approach used by clinicians to morphologically classify GC polyps into four types of polypoid (Type 1), fungating (Type 2), ulcerated (Type 3), and infiltrating or Flat (Type 4) (see Fig. 2). Nevertheless, inter-class variance of each type of polyps and solely relying on morphology of the GC polyps in Borrmann classification has resulted in a high-degree of disagreement and inconsistency in decision-making among clinicians [4]. Therefore, long-term specific training and experience is needed to detect GC properly using endoscopic images and Borrmann classification [5]. Furthermore, similar to many vision-based endoscopic di-agnoses (e.g., colonoscopy and laparoscopy), the limited resolution of endoscopic video cameras, visual occlusions, lack of sufficient steerability of the endoscopic devices, and lighting changes within the body make reliable diagnosis of GC polyps even more challenging [6].\nTo address the above-mentioned limitations, Artificial In-telligence (AI) methods utilizing Machine Learning (ML) have been employed in different modalities, such as histopathological images or endoscopic videos, to detect and classify GC tumors. For example, Li et al. [7] used a custom Deep Learning (DL) based framework for automatic cancer identification from histopathological images. Using the same modality, Huang et al. [8] developed an in-house DL ap-"}, {"title": "II. MATERIALS AND METHODS", "content": "In this study, we employed our recently developed VTS called HySenSe, as outlined in [21], to acquire high-fidelity textural images of AGC tumor phantoms. As depicted in Fig. 1, the HySenSe sensor comprises: (I) a flexible silicone membrane interacting directly with polyp phantoms, (II) an optical module (Arducam 1/4 inch 5 MP camera) capturing minute deformations in the gel layer during interactions with a polyp phantom, (III) a transparent acrylic plate offering support to the gel layer, (IV) an array of Red, Green, and Blue LEDs for internal illumination aiding depth perception, and (V) a sturdy frame supporting the entire structure. Oper-ating on the principle that the deformations resulting from the interaction between the deformable membrane and the sur-face of AGC tumors can be visually captured, the HySenSe sensor provides high-fidelity textural images, demonstrating proficiency across various tumor characteristics such as sur-face texture, hardness, type, and size [17]. This capability is maintained even at extremely low interaction forces, as detailed in [21]. Additionally, due to the arrangement of the LEDs within the sensor, different deformations have different lighting. This means that if the interaction force is fixed, the textural images implicitly encode the stiffness characteristics of the surface in contact as well. Due to these advantages, in our previous works [15], [16], [17], we have successfully used this sensor to differentiate between classes of CRC polyps, which are also distinguished by their morphological characteristics, namely the surface texture presented and the polyp stiffness. However, at this point, it must be noted that CRC polyps and AGC tumors differ greatly in size (i.e., ~1 cm\u00d71 cm versus ~4 cm\u00d74 cm). Since building a larger"}, {"title": "B. Realistic Tumor Phantoms", "content": "Towards addressing the limited availability of large, bal-anced clinical datasets in medical imaging and given this new imaging modality, we opted to meticulously design and manufacture realistic approximations of AGC tumors (see Fig. 2). To design the phantoms, and without loss of generality evaluate the performance of the utilized VTS, we followed 4 different types of Borrmann's classification system. The four types specifically are Type I: fungating type; Type II: carcinomatous ulcer without infiltration of the surrounding mucosa; Type III: carcinomatous ulcer with infiltration of the surrounding mucosa; Type IV: a diffuse infiltrating carcinoma (linitis plastic) [3]. It is known that the stiffness of the affected area is more than that of the surrounding regions, which makes it easier for clinicians to differentiate between tumorous sections and healthy tissue [3]. However, this classification has a considerable overlap (especially between Type II and Type III) due to the mixed morphological characteristics of these lesions, making a manual diagnosis through observation difficult [4].\nFig. 2 illustrates a few representative fabricated AGC lesion phantoms and their dimensions. To avoid the issue of data imbalance, in contrast with real patient datasets, we opted to have each tumor class equally represented in the dataset by designing 11 variations of each class (total 44 polyps). As shown in Fig. 2, based on the realistic AGC polyps, the designs first were conceptualized in Blender Software (The Blender Foundation) to make use of the free-form sculpting tool, then imported into Solidworks (Dassault Syst\u00e8mes) in order to demarcate the different regions with varying stiffness (tumor versus healthy tissue). The high-resolution, realistic lesion phantoms were manufactured us-ing a Digital Anatomy Printer (J750, Stratasys, Ltd) and materials with diverse properties: (M1) Tissue Matrix/Agilus DM 400, (M2) a mixture of Tissue Matrix and Agilus 30 Clear, and (M3) Vero Pure White. Hardness measurements were obtained using a Shore 00 scale durometer (Model 1600 Dial Shore 00, Rex Gauge Company). M1 has Shore hardness A 1-2, M2 has A 30-40, and M3 has D 83-86. These differing material properties allowed us to make the tumor sections (using M2) stiffer than the surrounding healthy tissue (using M1). M3 was used to print the supporting rigid backplate to be mounted onto the robot flange. Each tumor was printed across a working area of 3 cm x 3 cm, which represents the lower end of AGC tumor sizes."}, {"title": "C. Experimental Setup and Robotic Data Collection", "content": "In our previous works utilizing HySenSe for CRC polyp classification (see [15], [16], [17]), the high-fidelity textural images were captured manually using a setup including a force gauge mounted on a linear stage. This manual and tedious procedure limited our data collection capabilities, which was not efficient for this study. Furthermore, as discussed in Section II-A, AGC tumors are much larger than CRC polyps, making it impossible for the VTS to completely capture the whole textural area of the tumor phantoms. To overcome this limitation, in this work, we used a robotic manipulator to automate the image collection procedure. Using the robotic arm shown in Fig. 1, many different variations of partial contact from different angles were captured to form our dataset, which allowed the trained ML model to be more generalized.\nThe experimental setup for data collection is illustrated in Fig 1 and consists of the following: (1) Robot Manipulator: We used a KUKA LBR Med 14 R820 (KUKA AG) which has seven degrees of freedom (DoF), a large operating enve-lope, and integrated force sensing. We used ROS as a bridge, with the iiwa_stack project presented in [22], to provide high-level control of the onboard Java environment. (2) Workspace: The arm was rigidly attached to a work table. An optical table allows consistent positioning of items in the robot's coordinate frame. (3) HySenSe: The sensor was manufactured by our lab, using the methodology provided in [21], and attached to the optical table. Images are captured by the camera, a 5MP Arducam with 1/4\" sensor (model OV5647, Arducam). (4) Raspberry Pi: The camera is controlled by a Raspberry Pi 4B over the Arducam's camera ribbon cable. The Raspberry Pi ran Python software that continuously listened for an external ROS message trigger, which caused it to capture a 2592 x 1944 image and publish it to ROS. (5) Sample Mount: An adapting mount was 3D printed in PLA. One side attached to the robot flange. The other side offered two locating pins and four M2 screw holes, which ensure repeatable position and orientation of samples (6) Polyp Phantom Samples: The polyp samples were constructed as described above. Each was attached, in turn, to the sample mount using M2 screws. (7) Command and Control: An Ubuntu 20.04 system was used to control the ROS components. This computer ran the roscore master and the high-level Python command scripts.\nTo collect textural images with HySenSe, the robotic ma-nipulator was commanded via ROS to position the phantom of interest into random positions with different angles of contact, while maintaining the interaction force under a threshold of 3N using the arm's internal force sensor. As opposed to our previous manula procedures performed in [17], the only manual step involved was installing each target AGC phantom onto the sample mount on the robot end effector, and the remainder of the process was automated in software, reducing the time and workload required."}, {"title": "D. Calibration and Registration Procedures", "content": "While the apparatus was assembled carefully, there was no way to ensure precise positioning to the sub-millimeter level. This level of accuracy was necessary to successfully automate the action of pressing down an AGC tumor phan-"}, {"title": "E. Dataset and Pre-processing", "content": "This semi-automated data collection setup enabled us to collect 50 variations of orientation and contact of the AGC tumor phantoms with the HySenSe in 44 experiments (one for each polyp), leading to a total of 2200 images in the textural image dataset, with 550 unique images in each class. This dataset was then split into training and test sets while ensuring that the split was performed at a tumor level and not the image level. In other words, all textural images belonging to one tumor were kept within the same split. This was done in order to ensure the model was not being evaluated on partially seen data. This resulted in 1600 images from 32 unique tumors in the training set and 600 images from 12 unique tumors in the test set. The output from HySenSe, with original dimensions of 2592 x 1944 pixels, was resized to a uniform size of 224 x 224 pixels, ensuring"}, {"title": "F. Deep Learning Model", "content": "In this study, we opted to use the Dilated ResNet ar-chitecture for AGC tumor classification. The architecture's effectiveness stems from its capacity to mitigate issues like exploding gradients [28]. An additional advantage of ResNets lies in their incorporation of skip connections, which helps alleviate the degradation problem associated with the worsening performance of models as complexity increases [28]. Notably, unlike conventional Residual Network archi-tectures, the Dilated ResNet incorporates dilated kernels. Dilations play a crucial role in maintaining feature maps' spatial resolution during convolutions while expanding the network's receptive field to capture more intricate details [29]. More details of the architecture of Dilated ResNet can be found in [16]. To verify our model's performance relative to other commonly used architectures for image classification tasks, we compared the performance metrics described in Sec. II-G for our architecture with ResNet18 [30], and AlexNet [31]."}, {"title": "G. Evaluation", "content": "To limit the variability in performance due to the variance in hyperparameters, we used a random search across our hyperparameter space to find a good candidate for each model for comparison. The hyperparameter space in this study included the initial learning rate (LR), the LR sched-uler, the optimizer, and weight decay. We considered four possible LR schedulers, namely Step, Reduce-on-Plateau, OneCycle [32], and Cosine Annealing [33]. Three methods for the optimizer were considered, which were Stochas-tic Gradient Descent (SGD), Adam [34], and Adabound [35]. The full hyperparameter space is provided in Table I. Furthermore, early stopping with a patience of 10 epochs was utilized to cut down on the training time in case the validation loss reached a point of no improvement well before the designated maximum of 50 epochs. 100 random"}, {"title": "III. RESULTS AND DISCUSSION", "content": "1) Hyperparameter Search: Table II lists the best-performing combination of hyperparameters for each model. For the dilated ResNet, the best LR scheduler was Cosine"}, {"title": "IV. CONCLUSIONS", "content": "To collectively address the existing limitations on endo-scopic diagnosis of AGC tumors, for the first time, we used and evaluated our recently developed VTS for classifying AGC tumors using their textural features. Leveraging a7 DoF robotic manipulator and unique custom-designed realis-tic AGC tumor phantoms, we demonstrated the advantages of automated synthetic data collection using the VTS addressing the problem of data scarcity and biases encountered in tradi-tional ML-based approaches. We also proposed and evaluated a complementary ML-based diagnostic tool that can leverage this new modality to sensitively classify AGC lesions. Our synthetic-data-trained ML model was successfully evaluated utilizing appropriate statistical metrics even under mixed morphological characteristics and partial sensor contact. In the future, we plan to test this ML model textural images collected on real patients and perform a Sim2Real study."}]}