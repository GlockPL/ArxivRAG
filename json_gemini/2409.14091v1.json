{"title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction", "authors": ["Amrit Diggavi Seshadri"], "abstract": "With the size and cost of large transformer-based language models growing, recently, there has been interest in shortcut casting of early transformer hidden-representations to final-representations for cheaper model inference. In particular, shortcut-ting pre-trained transformers with linear transformations over early layers has been shown to improve precision in early inference. However, for large language models, even this becomes computationally expensive. In this work, we propose Narrow Jump to Conclusions (NJTC) and Normalized Narrow Jump to Conclusions (N-NJTC) - parameter efficient alternatives to standard linear short-cutting that reduces shortcut parameter count by over 97%. We show that N-NJTC reliably outperforms Identity shortcuts at early stages and offers stable precision from all transformer block levels for GPT-2-XL, Phi3-Mini and Llama2-7B transformer models, demonstrating the viability of more parameter efficient short-cutting approaches.", "sections": [{"title": "Introduction", "content": "Transformer based large language models (Vaswani et al., 2017) stack blocks made up of multi-headed self-attention and feed forward layers sequentially. Modern sophisticated language models stack upwards of 30 such blocks. For example, Phi3-Mini (Abdin et al., 2024) stacks 32 transformer blocks, GPT2-XL (Radford et al., 2019) stacks 48 blocks, and deeper models like Llama-2 70B (Touvron et al., 2023) and GPT-3 (Brown et al., 2020) stack as many as 80 and 96 sequential blocks. However while such stacking typically improves model performance, it also increases the computational costs of inference. More GPU memory is required to store the additional stacked transformer blocks and more time is required to forward-pass inputs sequentially.\nThere have been attempts to reduce the computational costs of such large language models by short-cutting transformers during inference. In short-cutting, one makes intermediate-predictions from an approximation of the final transformer output - that can be cheaply inferred from intermediate transformer-block outputs at each stage in the forward pass. A decision to \u2018early-exit' from the forward pass is made once the confidence of these intermediate-predictions reaches a certain pre-set confidence level \u03bb."}, {"title": "Method", "content": "Given a transformer model with hidden dimension H, to approximate a short-cut between its block-outputs at any two levels l and m, given a set of N input sentences S, we forward pass each sentence si \u2208 S through the transformer to obtain intermediate representation pairs (h_l^i,h_m^i)_{i=1}^N after blocks l and m at randomly selected token positions in each si. We then fit a simple 2 layer linear neural network made up of matrices A: H\u00d7H and B : H \u00d7 H that takes as input h_l^i and approximates h_m^i.\n$$h_m^i = (h_l^i) AB$$\nWe fit the two matrices A and B jointly using gradient descent to minimize the mean squared error loss L_{lm} between the approximated representations h_m^i and the transformer block outputs h_m^i.\n$$L_{lm} = \\frac{1}{N} \\sum_{i=1}^{N} ||h_m^i - (h_l^i) AB||^2$$\nThe hidden representation of each token in a sentence at level l is passed through A and B to obtain approximations of the hidden representations at level m. As a result of this low rank matrix decomposition, each NJTC shortcut uses 2* (H \u00d7 H/100) = 0.02H^2 parameters: Only 2% the number of parameters of a JTC shortcut."}, {"title": "Normalized Narrow Jump To Conclusions (N-NJTC)", "content": "We note that our NJTC method can be viewed as a special form of a linear denoising auto-encoder - where, the 'corrupted input' is a transformer's early block hidden representation h_l^i and the restoration target is a block's output h_m^i further down the forward pass. Linear autoencoders usually learn latent dimensions that maximize feature variance. To avoid any bias towards naturally high-variance transformer dimensions, we propose a normalized version of NJTC where we add a batch normalization layer before AB (Fig. 1). Batch Normalization adds an additional 4H parameters for each shortcut. For hidden dimension H > 400, this is less than 0.01H^2. As all transformer models use a hidden dimension larger than 400, we find that N-NJTC uses less than 3% the number of parameters of a JTC shortcut - offering over a 97% parameter reduction."}, {"title": "Experiments", "content": "We test our shortcuts on GPT2-XL (Radford et al., 2019) which consists of 48 transformer blocks, hidden dimension of 1600, and a total of 1.5 Billion model parameters; on the larger Phi3-Mini (Abdin et al., 2024) which uses 32 transformer blocks, has hidden dimension 3072, and has a total of 3.8 Billion parameters; and on the even larger Llama2-7B (Touvron et al., 2023) that uses 32 transformer blocks, has hidden dimension 4096 and uses a total of 7 Billion parameters. The low-rank dimensions H/100 that we use for our NJTC and N-NJTC shortcuts for GPT2-XL, Phi3-Mini and Llama2-7B are 16 and 30 and 40 respectively.\nData: Following the approach taken by (Din et al., 2023), we sample random sentences from Wikipedia, collecting 9,000 train sentences and 3000 validation sentences. As explained in Section 2.1, each sentence is forward passed through a given transformer model and random token position representations are selected across all hidden representations to train and evaluate shortcuts."}, {"title": "Quality of Shortcut Approximations", "content": "We first examine the degree of correlation between true transformer block outputs and their shortcut approximations for each shortcut type. For this purpose, we compute the coordinate averaged r2 scores between true transformer Block M outputs and corresponding shortcut jump approximations made from Block N outputs to Block M. Figure 2 shows heatmaps of these scores across all transformer block levels for id, JTC, NJTC and N-NJTC shortcuts for GP2-XL, Phi3-Mini and Llama2-7B models respectively. For id and JTC shortcuts, as one would expect, correlation of approximations seems to worsen as jump distance increases. That is, we always achieve better correlated approximations by making a shortcut jump from Block N to Block (N + 1) than we could achieve by making a jump to a later Block (N + 2). Interestingly, for NJTC and N-NJTC shortcuts, that is not the case. As shown in Figure 2, with some exceptions, we typically achieve better correlated approximations by jumping from any intermediate block N directly to the final few blocks output than we could achieve by making a smaller jump from block N to (N + 1) at earlier stages. This is an important finding as making jumps to the final block output is all that we really care about for early exit transformer prediction. We are happy to sacrifice intermediate jump quality to improve parameter efficiency of our shortcuts, provided that jumps to the final block outputs are still well correlated with the true final outputs. In this context, we note that N-NJTC shortcuts usually provide better correlation approximation in the final blocks than NJTC shortcuts can."}, {"title": "Quality of Shortcut Predictions", "content": "We next consider the quality of shortcut approximations for next token predictions obtained by shortcut jumping to the final transformer block out-"}, {"title": "Limitations and Potential Risks", "content": "Notably, as mentioned in Section 3.2, our NJTC method collapses for GPT2-XL and while N-NJTC solves this problem, NJTC and N-NJTC both achieve worse precision and surprisal scores than JTC shortcuts for all models, and are outperformed by Identity shorotcuts in late-block shortcutting (Figure 3). We note however that these limitations are acceptable in exchange for the 97% reduction in parameter count our N-NJTC method offers while outperforming Identity shortcuts at early transformer model stages. We note that short-cutting of transformers in general can cause unexpected model behaviour and caution that any shortcut approximations be tested for safety."}, {"title": "Conclusion", "content": "In this work, we proposed the Narrow Jump to Conclusions (NJTC) and Normalized Narrow Jump to Conclusions (N-NJTC) methods for parameter efficient shortcutting of transformer models. We showed that linear shortcuts from early stages can themselves be approximated by low rank representations to achieve over a 97% parameter reduction from JTC shortcuts. We applied our NJTC and N-NJTC methods to GPT-2-XL, Phi3-Mini and Llama2-7B transformer models and showed that N-NJTC reliably outperforms Identity shortcuts at early transformer model stages while also offering stable precision and surprisal from all transformer block levels, demonstrating the viability of more parameter efficient short-cutting methods than JTC."}]}