{"title": "Assessing instructor-AI cooperation for grading essay-type questions in an introductory sociology course", "authors": ["Francisco Olivos", "Tobias Kamelski", "Sebasti\u00e1n Ascui-Gaca"], "abstract": "This study explores the use of artificial intelligence (AI) as a complementary tool for grading essay-type questions in higher education, focusing on its consistency with human grading and potential to reduce biases. Using 70 handwritten exams from an introductory sociology course, we evaluated generative pre-trained transformers (GPT) models' performance in transcribing and scoring students' responses. GPT models were tested under various settings for both transcription and grading tasks. Results show high similarity between human and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in accuracy. For grading, GPT demonstrated strong correlations with the human grader scores, especially when template answers were provided. However, discrepancies remained, highlighting GPT's role as a \"second grader\" to flag inconsistencies for assessment reviewing rather than fully replace human evaluation. This study contributes to the growing literature on Al in education, demonstrating its potential to enhance fairness and efficiency in grading essay-type questions.", "sections": [{"title": "Introduction", "content": "By solving kinds of problems previously reserved for humans (McCarthy et al. 1955), educational experts have asserted that artificial intelligence (AI) has revolutionized education (Popenici and Kerr 2017; Wang et al. 2024). Recent figures indicate that 86% of higher education students from 16 countries regularly utilize AI in their studies, with 58% declaring that they feel unconfident with their knowledge and skills (Digital Education Council 2024). If not a revolution, the popularity and increasing pervasiveness of AI over recent years have been highly consequential for higher education.\nIn this context, AI has impacted academia in its dimensions of teaching, research, and service (Barros, Prasad, and \u015aliwa 2023). A primary concern raised by students' use of Al has been cheating (Ratten and Jones 2023), prompting challenges on the traditional methods of assessment. Moreover, several guides have provided a landscape of alternatives for effectively integrating AI into teaching (Levy and P\u00e9rez Albertos 2024). For instance, Olivos and Liu (2024) have demonstrated how students in research methods courses can use generative pre- trained transformers (GPT) models to pilot their questionnaires as an initial stage in survey design. Lee and Yeo (2022) developed an AI-based chatbot to simulate interactions with a student struggling with math misconceptions, helping teachers practice responsive teaching skills. In 2023, Google Labs released NotebookLM, enabling users to create podcasts based on documents, which could serve as a means of disseminating students' creative work. Thus, AI promises to automate, enhance, or facilitate entire or partial aspects of teaching.\nOne particular application of AI in teaching is in the process of grading class assignments or exams, or what Wang and colleagues (2024) term \u201cintelligent assessment.\u201d Previous estimates suggest that 40% of teaching time is spent on grading and providing feedback (Mandernach and Holbeck 2016). Consequently, if instructors are assisted by AI in these tasks, faculty members could be relieved of repetitive duties and focus on more advanced and complex activities.\nIndeed, if we ask ChatGPT using GPT-40 model, a commonly used generative artificial intelligence, the following question: \u201cCan you grade essay-type questions if I provide the answers and a grading criteria?,\u201d it confidently responds: \u201cYes, absolutely! If you provide the answers and grading criteria, I can grade the essay-type questions for you\u201d (OpenAI 2024a)."}, {"title": "1. Data & Methods", "content": "Human coder. A research assistant photographed each of the 6 answers in separate files for each of the 70 students of the course Introduction to Sociology at [ANONYMIZED]. The research assistant then transcribed the responses into an Excel spreadsheet.\nHuman instructors. An instructor (Grader A) and two teaching assistants (Graders B and C) were responsible for scoring the exams based on a standard template for answers. The primary instructor, who also delivered the lecture content of the course, graded the first four essay questions. Each teaching assistant then graded one of the remaining two questions.\nAI Grading Systems. For transcribing the photographed answers, we utilized the application programming interface (API) equivalent of ChatGPT (OpenAI API) in Python. GPT-4o and GPT-40-mini were utilized for comparison purposes. As previous models are not able to process visual data, the grading was performed using the models GPT-4o and GPT-40-mini as discussed in the analysis section.\nThe study was reviewed and approved by the Sub-Committee on Research Ethics and Safety under the Research Committee at [ANON] (approval reference: ECO26-2425), ensuring adherence to ethical standards in research.\nProcedures\nEach answer was photographed by the human coder using a CMOS Leica Vario-Summilux 1- 16/2.2 r4 75 asph camera. The camera was set to the \"2x Leica vibrant\" mode and positioned at a fixed distance for each photo. We utilize this device because this is a standard camera integrated to a cellphone, which may be available for most of the instructors that would like to adopt this grading system.\nThe human coder was instructed to write down each of the answers into an Excel spreadsheet, ordered as a data matrix, transcribing the verbatim to the best of their ability. They did not correct any grammar mistakes, typos, or incomplete sentences. Unreadable words were skipped. A total of 420 photos were taken, equivalent to 70 exams of 6 questions each.\nAs the first step of the analysis, human transcriptions were compared to the transcriptions generated by GPT-40 and GPT-40-mini. GPT-40 is designed for high versatility outputs, while GPT-40-mini is a smaller, faster, and more cost-efficient alternative. While GPT-40 is generally expected to deliver better performance due to its advanced capabilities, GPT-4o-mini could potentially outperform it in this task, as its simpler and more straightforward processing might align more closely with the goal of literal transcription. This makes GPT-4o-mini a cost- effective option, particularly for widespread adoption in resource-constrained environments. To ensure consistency, the temperature was set to 0.3 for both models, promoting direct transcription of the text without creative elaboration. Additionally, two different prompts were utilized to guide the transcription process for each model.\nThe GPT models cannot directly process common image formats like .jpg. To make these images compatible, they must be converted into a format the GPT models can understand. This process is automated in the ChatGPT service but requires manual conversion when using the OpenAI API. We encoded each photographed answer using the Base64 Python package, an encoding method that converts binary data, such as digital images, into ASCII strings that GPT models can process. Once encoded, the ASCII strings are passed to GPT models using the prompts provided below."}, {"title": "Results", "content": "Image transcription\nTable 1 displays the mean Cosine similarity index and its standard deviation between human and GPT models' transcriptions, which utilize a prompt for literal transcription (1a) or a prompt enabling the best guess (2a). The mean and standard deviation of the Cosine similarity for each pair of texts are reported for each question together with their overall mean. The mean indicates that GPT-40-mini provides transcriptions slightly more similar and with a smaller standard deviation than GPT-4o. Nevertheless, in both cases, the transcriptions exhibit a high level of similarity to the human coder's transcriptions.\nScoring prompt (1b) utilizing GPT models\u2019sociological knowledge\nWe requested GPT-4o-mini and GPT-40 to evaluate the transcribed students' responses. Noteworthy, GPT-40-mini is considerably more cost-effective (OpenAI 2024b), which offers a lower cost alternative for educators. We utilize the transcriptions by GPT-40-mini. As described above, the first prompt (1b) requested the scoring of the responses without template responses. It allows the GPT models to use their own knowledge to score the assessments.\nFor each GPT model and at different temperatures, we asked the GPT models 100 times to score. The procedure enables us to examine the consistency of results. Lower temperatures generate more deterministic results; therefore, we may expect lower variability in the outputs produced by settings at temperature 0.2. The data show variability in the consistency of results, which is not uniform across different models or temperature settings. This is explained because of the stochastic nature of the process. Specifically, GPT-40-mini provides more consistent results, particularly at a lower temperature setting (0.2), which is indicative of its more aggressive generalization and conservative scoring approach. The higher kurtosis also indicates that scores tend to concentrate around the mean in GPT-40-mini set up at temperature 0.2. This behavior is possibly due to differences in model size and training data between the models. Lower temperatures typically reduce randomness in the model's outputs, resulting in closer adherence to the most likely response pattern, which may explain the reduced variability seen in GPT-40-mini at temperature 0.2. GPT-4o-mini is trained to generalize more aggressively (OpenAI 2024b), which may lead to more consistent and conservative scoring, i.e., smaller range of scores and, hence, a smaller standard deviation.\nTo assess whether the human and GPT scoring are consistent across settings, we estimated the correlation between the human scoring and the average of the 100 tasks for each combination of model and temperature.\nFigure 6 suggests a positive correlation between the human scoring for the different settings. It indicates that in general, GPT can meaningfully utilize its own knowledge to assess students' answers. However, the correlation can be deemed moderate at conventional levels of correlational strength. When comparing the models, GPT-40-mini set at the default temperature 0.7 and 0.2 exhibits the worse performance when using the human scoring as benchmark. Conversely, the correlations between the benchmark human scoring and scores from the more complex models are substantially higher, particularly when it is set at temperature 0.7.\nBland-Altman graphs are displayed in Figure 7. These figures are instrumental to visualize the agreement between the human and automated scores across different settings. On the x-axis, each plot represents the average of human and GPT scores for each student, while the y-axis measures the score difference, calculated as GPT's score minus the human grader's score. Points closer to the zero line on the dotted axis indicate a higher agreement, showing minimal variance between the two scoring methods.\nObservations above the zero line where GPT consistently provided higher scores than human graders are particularly notable. Across the four plots, GPT generally tends to score more generously than the human graders, as evidenced by the positive average differences (blue dashed lines). This trend is more pronounced with the GPT-4o-mini model. While differences related to the temperature setting of the models are subtle, the standard temperature setting (0.7) for both GPT models shows slightly smaller differences, suggesting a closer alignment with human scoring. Overall, when configured at temperature 0.7, GPT-40 demonstrates the closest agreement with human scoring compared to other settings. However, the low level of correlation and the substantial difference in scoring does not provide robust evidence of potentially utilizing GPT for grading when template answers are not provided.\nThe green dashed lines represent the lower and upper bound of where 95% of the differences between the two measurements are expected to fall. These lines help to assess the extent of disagreement above the mean difference. Observations above the upper limit or below the lower limit are also considered outliers or significantly different. These outliers are particularly relevant for this exercise. It can be considered a formal standard for reassessment of any particular set of exams.\nScoring prompt (2b) providing template answers\nThe second prompt provided a template answer for each question to GPT. Thus, GPT does not need to rely completely on its knowledge, and clearer parameters of evaluation are provided. In other types of assessments, rubrics could also be included to define criteria and weights for each assessment. In the same vein as prompt 1b analyses, we requested 100 runs if the tasks for each setting combining models and temperatures.\nFigure 8 displays the standard deviations for each run, grouped by settings of model and temperature. The histograms generally indicate variability in scoring consistency across all model and temperature settings. The kurtosis values provide insights to understand the distribution shapes. GPT-4o-mini at a lower temperature (0.2) exhibits higher kurtosis (8.64), indicating a more peaked distribution with values clustering around the mean, suggesting greater consistency in scoring compared to other settings. This is consistent with expectations that lower temperatures result in outputs with reduced variability, reflecting the model's restricted creative range at these settings. Conversely, GPT-4o at temperature 0.7 shows very low kurtosis (0.01), corresponding to a flatter distribution, which indicates a broader spread of scores and, thus, higher inconsistency. This pattern suggests that the smaller and simpler model combined with a lower temperature setting allows for lower freedom in response generation, leading to decreased variability in the automated scoring.\nWe also examine the correlation with the total human scoring for each setting. Figure 9 reports these results. All the Pearson's correlations on or above .80 indicate a strong consistency between human scoring and GPT scoring. Among the different model and temperature set-ups, GPT-40 exhibits a higher level of consistency with human scoring. There is no variation between temperature 0.2 and temperature 0.7.\nFigure 9 also enables us to highlight the central contribution of GPT grading. It is clear there is a high level of consistency between human and GPT scoring. Nevertheless, the correlation is not perfect, and therefore, a replacement can lead to unfair evaluations. Thus, the results should not be interpreted as a suggestion for the wholesale implementation of GPT scoring. However, human scoring is also prone to bias as we explained above, and the residuals are the most useful information to produce more bias-free assessments. The residuals are the difference between the observations (dots in the figures) and the predicted value on the line. For example, in the first subfigure of GPT-40 mini with temperature set at 0.7, there is an outlier in the lower left corner. This student was assessed with a higher score by GPT and a lower score by the instructors. Therefore, this deviation is an indicator that the assessment may need to be revised again. This suggests a potential complementarity between human and Al rather than a replacement.\nFigure 10 presents Bland-Altman plots comparing GPT scores to human scores across different models. The plots highlight the average differences between the human assessments and the GPT models at temperature values of 0.2 and 0.7. Although smaller than differences in prompt 1b, the data reveal distinct patterns of deviation in the scores, with notable differences between the human assessments and GPT-40-mini. As shown by a concentration of observations in the upper side of the graphs, the human scoring tends to be lower than the scores of the GPT-40- mini (blue dashed line). On the contrary, GPT-40 has higher agreement with human scoring as suggested by differences closer to zero, particularly at temperature 0.7 where the differences are slightly lower than temperature 0.2. Overall, these results indicate that while the GPT-40- mini model tends to overestimate scores compared to human graders, GPT-40 maintains a closer alignment with human assessments, evidenced by smaller average differences. These results are also supported by correlational analysis in Figure 9. Similarly to the Bland-Altman plot utilizing prompt 1b, we can also identify exams for reassessment considering those exams under the lower limits.\nGiven the noted superiority of prompt 2b over prompt 1b, we also examine the performance of individual graders based on the correlation and average difference with the GPT scores. Due to varying sources of bias, it is conceivable that different graders might evaluate essay-type questions distinctly. While a direct comparison of scores between human graders is impractical since they evaluated different questions, analyzing each question separately provides granular insights into GPT performance. Table 2 displays these results. Questions 1 through 4 were assessed by grader A, while questions 5 and 6 were evaluated by graders B and C, respectively. Notably, there exists significant heterogeneity in the correlation and average differences between human grades and GPT scores. The correlations range from 0.87 for question 2 with GPT-40-mini (temperature 0.7) to 0.45 for question 6 with GPT-40 at both temperatures. There is also considerable variability within the questions assessed by grader A across different setups. This analysis underscores that while GPT can effectively aid human graders, the variability in outputs between graders and across specific questions is substantial. Consequently, GPT could serve as a supplemental tool or \"second grader,\" but it in any case should not, or cannot, totally replace the human grading process."}, {"title": "Conclusion", "content": "Human grading is not free from biased outcomes. Previous studies have demonstrated that human graders exhibit various forms of bias when assessing students' assignments (Birkelund 2014; Klein and El 2003; Malouff et al. 2014; Malouff, Emmerton, and Schutte 2013; Protiv\u00ednsk\u00fd and M\u00fcnich 2018). In this study, we examined the potential of instructor-AI collaboration in the assessment of essay-type questions. Using handwritten exams from an introductory sociology course, we analyzed (1) the similarity between human and GPT image- to-text transcriptions and (2) the consistency between human and GPT scoring of students' answers. The results reveal a high degree of similarity between human and GPT transcriptions, with GPT-40-mini slightly outperforming GPT-40. Furthermore, the analysis indicates a high level of consistency between human graders and the GPT models when template answers are provided.\nThis study contributes to the growing literature on AI applications in teaching (Popenici and Kerr 2017; Wang et al. 2024). Specifically, we extend recent research on the potential use of LLMs in student assessments (Gonz\u00e1lez-Calatayud et al. 2021; Wetzler et al. 2024). Unlike some prior evidence, our findings suggest promising results for implementing GPT models in higher education assessment tasks. However, we emphasize that consistency is not perfect, as deviations between human and GPT scores persist in this type of subjective evaluation. Therefore, we do not support the idea of completely replacing human graders but instead propose using GPT models as a complementary tool for teaching assessments.\nScholarly literature has extensively discussed strategies to reduce bias and enhance fairness in grading (e.g., Kates et al. 2023; Malouff, Emmerton, and Schutte 2013; Peter, Karst, and Bonefeld 2024). Our findings provide pathways and replicable tools for implementing GPT models as a second graders in subjective assessments, thereby contributing to the reduction of grading biases. Our correlation analysis and Bland-Altman plots are particularly useful for identifying cases that may require re-assessment. Rather than a drawback, the inconsistencies between human and GPT scoring can be leveraged to flag exams that may require further review, enhancing the fairness in assessments. Thus, the promise of GPT lies not in replacing human graders but in advancing the equity of evaluations.\nWe also compared different GPT models and settings, a critical exercise given the varying costs of implementation. GPT-4o-mini is significantly more cost-effective than GPT-40, approximately 33 times cheaper for processing input and 25 times cheaper for generating responses (Context 2025). While GPT-4o exhibits slightly higher consistency with human graders, GPT-40-mini remains effective for detecting deviations and offers a cost-efficient alternative, particularly at lower temperature settings. This cost-effectiveness also makes GPT- 4o-mini a good option for image-to-text transcription. It is also possible that instructors may require direct submission of digital answers to avoid the first step of the process."}]}