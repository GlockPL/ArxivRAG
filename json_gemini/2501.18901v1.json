{"title": "Lightspeed Geometric Dataset Distance via Sliced Optimal Transport", "authors": ["Khai Nguyen", "Hai Nguyen", "Tuan Pham", "Nhat Ho"], "abstract": "We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-) linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.", "sections": [{"title": "Introduction", "content": "Dataset distances provide a powerful framework for comparing datasets based on their underlying structures, distributions, or content. These measures are essential in applications where understanding the relationships between datasets drives decision-making, such as assessing data quality, detecting distributional shifts, or quantifying biases. They play a critical role in machine learning workflows, enabling tasks like domain adaptation, transfer learning, continual learning, and fairness evaluation. Additionally, dataset distances are valuable in emerging areas such as synthetic data evaluation, 3D shape comparison, and federated learning, where comparing heterogeneous data distributions is fundamental. By capturing meaningful similarities and differences between datasets, these measures facilitate data-driven insights, enhance model robustness, and support novel applications across diverse fields.\nA common approach to comparing datasets relies on proxies, such as analyzing the learning curves of a predefined model [28, 16] or examining its optimal parameters [1, 22] on a given task. Another strategy involves making strong assumptions about the similarity or co-occurrence of labels between datasets [47]. However, these methods often lack theoretical guarantees, are heavily dependent on the choice of the probe model, and require training the model to completion (e.g., to identify optimal parameters) for each dataset under comparison. To address limitations of previous approaches, model-agnostic approaches are developed. These methods often assess task similarity based on the"}, {"title": "Preliminaries", "content": "In this section, we review the definition of Wasserstein distance, sliced Wasserstein distance, and their computational properties, and restate the optimal transport dataset distance."}, {"title": "Wasserstein Distance", "content": "The Wasserstein-p distance [49, 41] (p \u2265 1) between two distributions \u03bc\u2208P(X) and \u03bd\u2208 P(X), where X are subsets of Rd and has a ground metric dx : X \u00d7 X \u2192 R+, is defined as:\n\n$W_p(\\mu, \\nu) := \\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{\\mathcal{X}\\times\\mathcal{X}} d_x(x, x')^p d\\pi(x,x'),$\n\nwhere \u03a0(\u03bc, \u03bd) is the set of all possible transportation plan i.e., all joint distributions \u03c0(x, x') such that \u03c0(x, X) = \u03bc(x) and \u03c0(X,x') = v(x'). When \u03bcand vare discrete i.e., \u03bc = \u03a3=1 \u03b1\u03af\u03b4\u03b1; and"}, {"title": "Sliced Wasserstein distance", "content": "The sliced Wasserstein (SW) distance is motivated from the closed-form solution of the one-dimensional Wasserstein distance with ground metric dx(x,x') = h(x \u2212 x') with XCR and his a strictly convex function:\n\n$W_p(\\mu, \\nu) = \\int_0^1 dx |F_\\mu^{-1}(z)-F_\\nu^{-1}(z)|^p dz,$ \n\nwhere F\u00b9 and F\u00b9 are inverse CDF of \u03bc and v respectively. When \u00b5 and v are discrete with at most n supports, the time complexity and the space complexity for computing the closed-form are O(nlogn) [41] and O(n) respectively. To exploit the closed-form, the SW distance relies on random projections. For p > 1, the SW distance [9] of p-th order between two distributions \u03bc\u2208 P(X) and \u03bd\u2208 P(X) with X C Rd is defined as follow:\n\n$SW_p(\\mu, \\nu) = E_{\\theta \\sim U(S^{d-1})}[W_p(R_{\\theta#}\\mu, R_{\\theta#}\\nu)],$\n\nwhere Rou and Reiv are the one-dimensional push-forward distributions of \u03bc and v through the function Re(x) = 0x (derived from Radon Transform (RT) [20]), and U(Sd-1) is the uniform distribution over the unit hypersphere in d dimensions. In addition to the computational benefit,\nSW has been widely known for its low sample complexity [32, 34, 40, 6]."}, {"title": "Optimal Transport dataset distance", "content": "We are given two datasets D1 = {(xi, Yi)}=1 and D2 = {(x, y)}=1, where (xi, Yi), (x';, y;) \u2208 X \u00d7 Y, Vi \u2208 [[n]],\u2200j \u2208 [[m]] with X is the space of features and y is the space of labels. We assume that we know the support distance on the space of features X i.e., dx : X \u00d7 X \u2192 R+ e.g., Euclidean distance. In contrast to the mild assumption of having dx, we rarely know the distance on the space of labels Y e.g., we cannot tell if the label \"dog\" is closer to the label \u201ccat\" than to the label \u201cbird\". As a solution, authors in [3] propose to map a label into a distribution over features, then use distances between distributions as the proxy for the distance between labels. In particular, for the first dataset, we assume that (x1,y1), ..., (Xn, Yn) ~ q(X,Y) where q(X, Y) is an unknown joint distribution. After that, we can define qy(X) = q(X|Y = y) as the conditional distribution over features given the label y. Similarly, for the second dataset, we can obtain qy (X') for a label y'. Therefore, the distance between labels can be defined as:\n\ndy(y, y') = D(qy(X), qy' (X')),\n\nwhere D is a distance between two distributions. In practice, we observe qy(X) and qy'(X') in empirical forms, hence, optimal transport distances naturally serve as a measure for D. Authors in [3] suggest using the Wasserstein distance i.e., dy(y, y') = Wp(qy(X), qy' (X')), where we abuse"}, {"title": "Sliced Optimal Transport Dataset Distances", "content": "While the OTDD is a natural distance between two datasets, it inherits the computational challenges of the Wasserstein distance. Let n denote the maximum number of data points in the two datasets, c the maximum number of classes, and nmax the maximum number of samples per class. The time complexity of OTDD is O(n\u00b3 log n + c\u00b2(nmax log nmax + d)), and the memory complexity is O(n\u00b2 + c\u00b2). When using a Gaussian approximation for the label distribution, the time complexity becomes O(n\u00b3 log n + c\u00b2(d\u00b3 + nmaxd\u00b2)). As a result, OTDD may not be scalable for large datasets. To address this, we aim to develop a sliced optimal transport version of OTDD that leverages the one-dimensional closed-form of the Wasserstein distance. This requires developing a novel approach to project a data point onto a single scalar."}, {"title": "Label Projection", "content": "As mentioned in Section 2, a label is represented as a distribution over the feature space X. Given a label y, we would like to map the label distribution qy to a scalar. To achieve our goal, there are two steps: projecting qy to one-dimension through feature projection, and transforming the one-dimensional projection of qy into a scalar.\nFeature Projection. Since a label is treated as a distribution over the feature space, we can project a label to an one-dimensional distribution through a feature projection i.e., a mapping from FP : X \u2192 R with the projection parameter @ belongs to a projection space \u0398. We can choose any feature projection methods based on the prior knowledge of the feature space X e.g., Euclidean space [9], images [36], functions [17], spherical space [48, 42, 43], hyperbolic space [7], manifolds [8, 39], and so on. It is worth noting that a feature projection is injective if FPo\u2021\u00b5\u2081 = FPo\u00b52 for all \u03b8\u2208 \u0398 then \u03bc\u2081 = \u03bc2 for any \u03bc1, \u03bc2 \u2208 P(X). The injectivity of the projection is vital to preserve the distributional metricity.\nScaled Moment. We now discuss how to map a one-dimensional distribution into a scalar. More importantly, we want the transformation to be injective. To design such transformation, we rely on scaled moments."}, {"title": "Data Point Projection", "content": "With the proposed label projection, we can propose data point projection. We recall that a data point is a pair of a feature and a label or equivalently a pair of a feature and a distribution over features, which is denoted as (x, qy) \u2208 X \u00d7 P(X). For the feature domain, as discussed, we can use any feature projections based on the prior knowledge of the feature space, denoted as FP9 : X \u2192 R. For the label, we can use MTP defined in the Section 3.1. Now, we need to combine the outputs of the feature projection and the label projection to obtain the final scalar.\nDefinition 3. Given a data point (x, qy) \u2208 X \u00d7 P(X) and k > 1, the data point projection can be defined as follows:\n\n$DP_{\\psi,\\theta,\\lambda,\\phi}(x, q_y) = \\psi^{(1)} FP_\\theta(x) + \\sum_{i=1}^k \\psi^{(k)} MTP_{\\lambda^{(i)}, \\phi}(q_y),$\n\nwhere \u03c8 = (\u03c8(1), \u03c8(2),..., \u03c8(k)) \u2208 Sk,0\u2208 \u0398, \u03bb = (x(1),..., \u03bb(k)) \u2208 \u039b, \u03c6 \u2208 \u03a6 with and are projection space of the feature projections.\nGiven a dataset D = {(x1, y1), ..., (X1, qyn)}, we have the projected distribution of the corresponding empirical distribution through the data point projection is DP,0,1,#PD = = = 1 DP,0,1,\u03c6 (xi,qy\u2081)*\nThe proposed data point projection combines the outputs of k \u2265 1 MTPs to obtain the final projection value. It follows the principle of hierarchical hybrid projection in [38] to retain the overall injectivity.\nCorollary 1. The data point projection is injective when the feature projection and the MTP is injective i.e., for \u03bc,\u03bd \u2208 X \u00d7 P(X) if DP,0,1,\u03c6\u03bc = DP,0,1,6\u2021v for all \u03c8 \u2208 S,0 \u2208 \u0398, \u03bb \u2208 \u039b C \u039d, \u03c6 \u0395 \u03a6.\nThe Corollary 1 follows the fact that the data point projection is a composition of injective projections [38] i.e., the Radon Transform projection and the MTP.\nIt is worth noting that we can use the same value for 0 and 6 when we use a single feature projection to project feature and to construct label projections. This approach not only reduces memory consumption for storing projection parameters but also saves computation since we need to project features only once."}, {"title": "Sliced Optimal Transport Dataset Distance", "content": "With projections of data points, we can now introduce the sliced optimal transport dataset distance (s-OTDD).\nDefinition 4. Let D1 and D2 be the two given datasets, PD\u2081 and PD\u2082 be corresponding empirical distributions of D1 and D2 respectively, the sliced optimal transport dataset distance (s-OTDD) of order p > 0 is defined as follows:\n\ns-OTDD_p(D_1, D_2) = E_{(\\psi,\\theta,\\lambda, \\phi) \\sim U(S) \\otimes U(S^{d-1}) \\otimes \\sigma(\\Lambda^k) \\otimes U(\\Phi)}[W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2})],\n\nwhere the expectation is with respect to the random projection parameters (\u03c8,0,\u03bb, \u03c6) ~ U(S)U(Sd\u22121) \u2297 \u03c3(\u039bk) \u2297U(\u03a6) with \u03c6 \u2208 \u03a6, and \u03c3(\u039bk) is the uniform distribution on Ak when A is finite or the product of zero-truncated Poisson distributions [12] when A is infinite."}, {"title": "Conclusion", "content": "We propose sliced optimal transport dataset distance (s-OTDD), a versatile approach for comparing datasets that is both model-agnostic and embedding-agnostic, requires no training, and is robust to variations in the number of classes and disjoint label sets. At the heart of s-OTDD is the Moment Transform Projection (MTP), which encodes a label\u2500represented as a feature distribution into a real number. This projection enables us to represent entire datasets as one-dimensional distributions by mapping individual data points accordingly. From theoretical aspects, we discuss theoretical properties of the s-OTDD including metricity properties and approximation rate. From the computational aspects, the proposed s-OTDD achieves (near-)linear computational complexity with respect to the number of data points and feature dimensions, while remaining independent of the number of classes. For experiments, we analyze the performance of s-OTDD by comparing its computational time with existing dataset distances, its correlation with OTDD, and its dependence on the number of projections, using subsets of the MNIST and CIFAR10 datasets. Moreover, we evaluate the correlation between s-OTDD and transfer learning performance gaps on image NIST datasets and diverse text datasets. Finally, we examine the utility of s-OTDD for data augmentation in image classification on CIFAR10 and Tiny-ImageNet. Future works will focus on understanding the gradient flow [4] of the s-OTDD and adapt the s-OTDD in continual learning applications [27, 21, 51, 18]."}, {"title": "Proofs", "content": ""}, {"title": "Proof of Proposition 1", "content": "We can rewrite the MTP in Definition 2 as:\n\n$MTP_{\\lambda,\\theta}(\\mu) = \\int_{\\mathbb{R}^d} \\frac{(FP_{\\theta}(x))^\\lambda}{\\lambda!}f_{\\mu}(x)dx = \\int_{\\mathbb{R}}\\frac{t^\\lambda}{\\lambda!}f_{FP_{\\theta#}\\mu}(t)dt$\n\nwhere fFpou is the density function of FP\u0473\u266f\u03bc. Let Tu,e be the random variable of FP\u0473\u266f\u03bc, we have:\n\n$MTP_{\\lambda,\\theta}(\\mu) = \\frac{E[T_{\\mu, \\theta}^{\\lambda}]}{\\lambda!}$\n\nFor a given 0, when \u039c\u03a4\u03a1\u03bb\u03b8(\u03bc) = MTP,9(v) for all A \u2208 A, it implies\n\n$\\frac{E[T_{\\mu, \\theta}^{\\lambda}]}{\\lambda!} = \\frac{E[T_{\\nu, \\theta}^{\\lambda}]}{\\lambda!}$ for all \u03bb\u2208 \u039b.\n\n(1) When A is infinite i.e., A = N, we have\n\n$\\frac{E[T_{\\mu, \\theta}^{\\lambda}]}{\\lambda!} = \\frac{E[T_{\\nu, \\theta}^{\\lambda}]}{\\lambda!}$ for all \u03bb\u2208 \u039d. Therefore, we have:"}, {"title": "Proof of Proposition 2", "content": "From Definition 4, we have:\n\ns-OTDD_p(D_1, D_2) = E_{(\\psi,\\theta,\\lambda, \\phi) \\sim U(S) \\otimes U(S^{d-1}) \\otimes \\sigma(\\Lambda^k) \\otimes U(\\Phi)}[W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2})].\n\nSince the Wasserstein distance is non-negative and symmetric [41], the symmetry and non-negativity of the s-OTDD(D1, D2) follows directly from it. We now prove the triangle inequality of s-OTDD. Given any three datasets D1, D2, and D3. We want to show that:\n\ns-OTDDp(D1, D2) \u2264 s-OTDDp(D1, D3) + s-OTDDp(D3, D2).\n\nFrom the triangle inequality of the Wasserstein distance, we have:\n\ns-OTDD_p(D_1, D_2) = (E_{(\\psi,\\theta,\\lambda, \\phi) \\sim U(S) \\otimes U(S^{d-1}) \\otimes \\sigma(\\Lambda^k) \\otimes U(\\Phi)} [W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2})])^\\frac{1}{p}\n\\leq (E_{(\\psi,\\theta,\\lambda, \\phi) \\sim U(S) \\otimes U(S^{d-1}) \\otimes \\sigma(\\Lambda^k) \\otimes U(\\Phi)} [(W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_3})\n+W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_3}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2}))^p])^\\frac{1}{p}.\n\nUsing the Minkowski's inequality, we further have:\n\ns-OTDD_p(D_1,D_2) \\leq (E_{(\\psi,\\theta,\\lambda, \\phi) \\sim U(S) \\otimes U(S^{d-1}) \\otimes \\sigma(\\Lambda^k) \\otimes U(\\Phi)} [W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_3}^p])^\\frac{1}{p}\n+(E_{(\\psi,\\theta,\\lambda, \\phi) \\sim U(S) \\otimes U(S^{d-1}) \\otimes \\sigma(\\Lambda^k) \\otimes U(\\Phi)} [W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_3}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2})])^\\frac{1}{p}\n= s-OTDD_p(D_1, D_3) + s-OTDD_p(D_3, D_2),"}, {"title": "Proof of Proposition 3", "content": "We recall the empirical approximation of the s-OTDD is:\n\n$s-OTDD_p^L(D_1, D_2) = \\frac{1}{L} \\sum_{l=1}^L W_p(DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_1}, DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_2}).$\n\nUsing Holder's inequality, we have:\n\n$E[|s-OTDD_p^L(D_1, D_2) - s-OTDD_p(D_1, D_2)|]$\n\n$\\leq (E[|s-OTDD_p^L(D_1, D_2) - s-OTDD_p(D_1, D_2)|^2])^\\frac{1}{2}$\n\n$ = (E[|\\frac{1}{L} \\sum_{l=1}^L W_p(DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_1}, DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_2}) -  E[W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2})]^2])^\\frac{1}{2}.$\n\nSince\n\n$E[\\frac{1}{L} \\sum_{l=1}^L W_p(DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_1}, DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_2})]$\n\n$= \\frac{1}{L} \\sum_{l=1}^L E[W_p(DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_1}, DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_2})]$\n\n$=E[W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2})]$,\n\nwe have:\n\n$E[|s-OTDD_p^L(D_1, D_2) - s-OTDD_p(D_1, D_2)|]$\n\n$\\leq (Var[\\frac{1}{L} \\sum_{l=1}^L W_p(DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_1}, DP_{\\psi_l,\\theta_l,\\lambda_l,\\phi_l#}P_{D_2})])^\\frac{1}{2}$\n\n$= \\frac{1}{\\sqrt{L}} (Var[ W_p(DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_1}, DP_{\\psi,\\theta,\\lambda,\\phi#}P_{D_2}])^\\frac{1}{2},$\n\ndue to the i.i.d sampling of the projection parameters, which completes the proof."}, {"title": "Additional Materials", "content": "Algorithms. We present the computational algorithm for s-OTDD in Algorithm 1. In the algorithm, we use the same feature projection for both the label projection (MTP) and the data point projection.\nProjection Analysis. As mentioned in the main text, we present the distance correlation with OTDD (Exact) of s-OTDD as a function of the number of projections in Figure 7 and similarly for CHSW in Figure 8. For s-OTDD, we observe that increasing the number of projections consistently improves the correlation for both MNIST and CIFAR10 datasets, indicating a clear trend. In contrast, the same does not hold true for CHSW, where the correlation does not show consistent"}]}