{"title": "Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence", "authors": ["Wenzhe Yin", "Zehao Xiao", "Pan Zhou", "Shujian Yu", "Jiayi Shen", "Jan-Jakob Sonke", "Efstratios Gavves"], "abstract": "Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP maximize the mutual information mainly by aligning pairwise samples across modalities while overlooking the distributional differences, leading to suboptimal alignment with modality gaps. In this paper, to overcome the limitation, we propose CS-Aligner, a novel and straightforward framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. In the proposed framework, we find that the CS divergence and mutual information serve complementary roles in multimodal alignment, capturing both the global distribution information of each modality and the pairwise semantic relationships, yielding tighter and more precise alignment. Moreover, CS-Aligher enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.", "sections": [{"title": "1. Introduction", "content": "Modality alignment is a cornerstone of multimodal representation learning, enabling success across diverse applications such as image-text retrieval (Huang et al., 2024; Koukounas et al., 2024), text-to-image (T2I) generation (Ramesh et al., 2022; Razzhigaev et al., 2023), and multimodal chatbots (Zhu et al., 2023). As a pioneering work in this field, CLIP (Radford et al., 2021) leverages contrastive loss to maximize the mutual information between paired text and image representations, effectively capturing pairwise and semantic relationships. Its versatility has made it a foundation for many multimodal tasks.\nAlthough widely adopted, CLIP suffers from a persistent modality gap between text and image representations in its latent space. As shown in Fig. 1a, text and image embeddings often fail to align precisely and may remain separated from each other. This modality gap has been observed and explored in prior studies (Zhou et al., 2023; Liang et al., 2022; Shi et al., 2023), which attribute the issue to factors such as cone effects (Liang et al., 2022) or suboptimal latent space structures (Shi et al., 2023). Intriguingly, Liang et al. (2022) observed a phenomenon that CLIP's contrastive learning objective may inadvertently exacerbate this gap, implying mutual information alone is insufficient for aligning text and image representation distributions.\nSeveral strategies have been proposed to address the modality gap, such as projection modules with cosine similarity (Zhou et al., 2023; Gao et al., 2024; Huang et al., 2024) and geodesic multimodal mixup (Oh et al., 2024). UnCLIP-based models like DALL-E 2 (Ramesh et al., 2022) and Kandinsky (Razzhigaev et al., 2023) employ text-to-image prior modules (e.g., diffusion models) to map text embeddings to image feature space. A more recent alternative Eclipse (Patel et al., 2024) uses l2 loss to train a prior adapter. These works aim to transform representations across modalities for alignment. However, they remain exploring alignment sample-wisely and heavily rely on pairwise data. Although sample-wise alignment effectively captures semantic information, it falls short in aligning entire data distributions. Similar to the InfoNCE in CLIP, the methods struggle to match the representation spaces across modalities, ultimately limiting the overall alignment. Moreover, the reliance on carefully curated text-image pairs limits scalability and applicability to real-world scenarios with unpaired and noisy datasets (Lin et al., 2014; Li et al., 2023).\nTo address these challenges, we propose CS-Aligner, a novel distributional approach that incoporates Cauchy-Schwarz (CS) divergence (Principe et al., 2000b) for modality alignment. As a symmetric measure, CS divergence robustly and efficiently estimates the distance between any representation distributions without parametric distributional assumptions, making it highly suitable for multimodal distribution alignment. Furthermore, we also analyze the complementary roles of CS divergence and mutual information in multimodal alignment and propose integrating these two metrics within CS-Aligner. This enables CS-Aligner to align vision and language representations in both distributional and sample-wise levels, considering both the global modality and local semantic information, leading to more comprehensive and tighter alignment as shown in Fig. 1b.\nMoreover, based on the distributional approach, CS-Aligner enables alignment with additional unpaired data, such as (a) single images with multiple captions or (b) entirely unpaired vision-language data, introducing more distributional information from richer, unstructured multimodal data for alignment robustness and flexibility in real-world scenarios. Beyond the unpaired alignment, we also introduce a novel token-level alignment scheme for vision-language models, which integrates more detailed information in diverse tokens to enhance multimodal alignment. Extensive experiments on downstream tasks, including T2I generation and image-text retrieval, demonstrate the effectiveness of our approach."}, {"title": "2. Related work", "content": "Vision-language alignment and applications. CLIP (Radford et al., 2021) serves as a foundational model for vision-language alignment in multimodal tasks. Several works have enhanced CLIP through techniques such as momentum distillation (Li et al., 2021) and noisy text supervision (Jia et al., 2021). Despite its success, CLIP suffers from a persistent modality gap between text and image representations. Prior studies (Zhou et al., 2023; Liang et al., 2022; Shi et al., 2023) attribute this gap to factors such as cone effects (Liang et al., 2022) and suboptimal latent space structures (Shi et al., 2023). To address this, various strategies have been proposed, including projection adapters (Zhou et al., 2023; Gao et al., 2024; Huang et al., 2024), geodesic multimodal mixup (Oh et al., 2024), and parameter-efficient fine-tuning (Zanella & Ben Ayed, 2024). Recent works also improve CLIP by large language models (LLMs) (Jang et al., 2024; Koukounas et al., 2024; Huang et al., 2024) for downstream tasks such as image-text retrieval.\nIn addition to image-text retrieval, text-to-image (T2I) generation is another application that reflects the vision-language alignment capability. T2I has advanced significantly over the past decades, driven by both diffusion-based (Ramesh et al., 2021; Rombach et al., 2022; Saharia et al., 2022; Nichol et al., 2021) and GAN-based models (Zhang et al., 2017; Tao et al., 2023). Among diffusion-based methods, the unCLIP framework (Ramesh et al., 2021; 2022) employs a two-stage architecture with a CLIP-guided diffusion prior and a decoder (e.g., DALL-E-2 (Ramesh et al., 2022) or Karlo (Donghoon et al., 2022)). Its prior module go maps text representations y to image ones x by a diffusion model. Recently, Eclipse (Patel et al., 2024) employs an l2 loss to simplify the prior loss by eliminating diffusion time and intruding a noise e term:\n$\\Lprior = Ee~N(0,1) [||x \u2212 9$(\u20ac, y)||^2] .$\nHowever, these methods still rely on pairwise loss (e.g., l2). In contrast, our approach introduces distributional alignment for a more holistic modality alignment.\nCauchy-Schwarz divergence. The Cauchy-Schwarz (CS) divergence (Principe et al., 2000a;b) is derived from the Cauchy-Schwarz inequality for square-integrable functions. It serves as a symmetric distribution distance metric with notable properties, such as the ability to measure conditional distributions (Yu et al., 2023) and the closed-form expression for mixtures of Gaussians (Kampa et al., 2011). CS divergence has been successfully applied across various domains, including deep clustering (Trosten et al., 2021), disentangled representation learning (Tran et al., 2022), and deep regression (Yu et al., 2024). Moreover, due to its advantage of estimating discrepancy between conditional distributions, it has demonstrated success in the domain adaption area (Yin et al., 2024) and time series clustering (Yu et al., 2023). However, the utility of CS divergence in foundation models remains unclear and unexplored."}, {"title": "3. Methodology", "content": "In this section, we first review the mutual information used in previous multimodal methods and analyze its limitations for alignment. Then we introduce the novel CS-Aligner framework for distributional multimodal alignment and detail the estimations of its terms. After that, we extend the method by incorporating additional distribution information. Finally, we provide parameter-efficient implementations.\n3.1. Mutual Information is insufficient for alignment\nPrevious multimodal methods like CLIP (Radford et al., 2021) learn text and image representations in a shared space by maximizing lower bounds (e.g., InfoNCE (Oord et al., 2018)) of mutual information between modalities:\n$\\I(x; y) = \\int\\int p(x, y) log( \\frac{p(x, y)}{p(x)p(y)} ) dx dy,$\nwhere p(x) and p(y) are the distributions of image and text features. p(x, y) denotes the joint probability. This objective is optimized via the InfoNCE bound (Oord et al., 2018), which approximates I(x;y) using paired data samples {(x, y)}_1^N. We denote image-text pairs as {(xi, yi)}_i=1^N. The multimodal InfoNCE loss combines symmetric image-text and text-image alignment terms:\n$\\LInfoNCE = \\frac{1}{2}(L12T + LT21),$\n$\\L12T = - \\frac{1}{N} \\sum_{i=1}^N log \\frac{exp (sim(xi, yi)/T)}{\\sum_{j=1}^N exp (sim(xi, yj)/T)},$\n$\\LT21 = - \\frac{1}{N} \\sum_{i=1}^N log \\frac{exp (sim(yi, xi)/T)}{\\sum_{j=1}^N exp (sim(yi, xj)/T)},$\nwhere sim(,) denotes cosine similarity. \u03c4 is temperature. Critically, this formulation requires paired data {(xi, Yi)}.\nAlthough widely adopted, mutual information alone is insufficient for effective modality alignment (Liang et al., 2022). The reason is that mutual information quantifies the statistical dependence between two random variables (Cover, 1999), ensuring that the distribution p(x) is related to p(y). However, it does not guarantee that the distributions p(x) and p(y) are statistically similar or close to each other. In other words, two distributions can differ significantly or be far apart, yet exhibit strong dependence. We illustrate this issue with the following toy example.\nExample 3.1. Consider two Gaussian distributions, p(x) ~ \u039d(\u03bc\u03b1, \u03c3\u3121) and p(y) ~ N(\u03bcy, \u03c3\u2084), with a joint distribution\n$$(x,y)~N((( \u03bc\u03b1 \\\\ \u03bcy )),(( \u03c3\u03c7^2 & p\u03c3x\u03c3y\\\\ p\u03c3x\u03c3y & \u03c3y^2 ))).\nHere, \u03bcx and \u00b5y are the means of x and y, \u03c3\u3121 and \u03c3 are their variances, and p is the correlation coefficient and controls their linear dependency. When p = 0.99, the two modalities are highly dependent, with high mutual information (I = 1.959; see Fig. 2a and 2b). When p = 0, the modalities are independent, resulting in zero mutual information (Fig. 2c). Interestingly, two distributions with the same mutual information value can either exhibit minimal statistical distance and nearly identical shapes, including similar locations, widths, and higher-order moments, as shown in Fig. 2a, or have completely different shapes with distinct means (0 for p(x) and 2 for p(y)) and variances (100 for p(x) and 0.01 for p(y)), as illustrated in Fig. 2b. Quantitatively, the former case shows a minimal KL divergence of 0, while the latter exhibits a KL divergence of nearly 5, 194. See details in Appendix A.\nExample 3.1 shows that despite strong dependence and high mutual information, the representation distributions of two modalities can remain misaligned and be far from each other. This issue is also observed in the CLIP model pretrained with InfoNCE, where the vision and language representations exhibit a noticeable distributional gap, as shown in Fig. 1a. This gap results in inconsistently aligned multimodal features, hindering the clear representation of shared semantics and disrupting effective mapping between modalities. Ultimately, this misalignment degrades performance in downstream tasks, including cross-modality generation.\nNotably, although directly minimizing the divergence between distributions may reduce the distributional gap, it risks creating independent multimodal distributions without common semantic information (Fig. 2c). Therefore, maximizing mutual information and minimizing divergence complement each other to achieve effective multimodal representation alignment."}, {"title": "3.2. Distributional multimodal alignment", "content": "To overcome the limitations of mutual information term alone, we propose a distributional alignment framework. Specifically, we introduce a distribution divergence minimization regularization into the optimization of multimodal alignment, defining the overall problem as maximizing mutual information while ensuring a small divergence between multimodal distributions:\n$\\max I(x; y), s.t. D(p(x), p(y)) \u2264 \u0454,$\nwhere \u20ac > 0 is a small constant representing the permissible divergence threshold. To address this constrained optimization, we introduce a Lagrangian multiplier X \u2265 0, reformulating it into an unconstrained problem:\n$\\min -I(x; y) + AD(p(x), p(y)).$\nThis formulation optimizes for high mutual information between paired data (x, y) while reducing the divergence between the distributions p(x) and p(y).\nUnlike parametric distributions, distributions of different real-world modalities exhibit unpredictable variability and inconsistent overlaps, meaning that p(x) and p(y) may follow arbitrary distributions with a small intersection.\nTherefore, it is crucial to overcome these challenges to measure and optimize multimodal distribution divergence robustly. Below, we outline several key properties that an effective metric should satisfy for multimodal alignment.\nRemark 3.2. Key properties for distribution align metrics:\n*   Symmetry: Both distributions are treated equally, ensuring consistent and unbiased multimodal alignment, formulated by D(p(x), p(y)) = D(p(y), p(x)).\n*   Differentiable and Efficient Estimation: Enable differentiable estimation without distribution assumptions to facilitate optimization, formulated as JD(p(x; \u03b8), p(y; \u03c6)) \u2260 0,\u2200p(x), p(y). Achieve the estimation non-parametrically or efficiently.\n*   Robustness to Small Distribution Overlap: Provide reliable measurements even when distributions have minimal overlap of supports, which may often occur in multimodal scenarios. The property is formulated as 0 \u2264 D(p(x),p(y)) \u2264 \u221e when 0 < \u03bc(supp(p(x)) \u2229 supp(p(y))) < \u03b5. \u03bc(supp(p(x)) \u2229 supp(p(y))) denotes the overlap of p(x) and p(y). \u0454 is a small value.\nThese properties enable the divergence term in (5) to align arbitrary distributions with small support overlap, which is well-suited for large-scale multimodal applications involving deep learning.\nWhile KL divergence and Wasserstein distance are widely used, they fail to meet these requirements. KL divergence is asymmetric, inefficient for non-Gaussian data, and unreliable for distributions with small overlap (Yu et al., 2024). Similarly, Wasserstein distance is inefficient to estimate, requiring additional learnable module (Arjovsky et al., 2017) or Sinkhorn iterations (Cuturi, 2013). As a result, these metrics are suboptimal for large-scale multimodal alignment tasks (details in Appendix B).\nTo satisfy these properties, we introduce CS divergence (Principe et al., 2000a;b), a symmetric and robust metric to quantify the distance between two probability density functions:\n$\\Dcs (p(x); p(y)) = - log ((\\frac{\\int p(x)p(y)dxdy}{\\sqrt{\\int p(x)^2dx \\int p(y)^2dy }} )^2 ).$\nThe CS divergence satisfies all the desired properties. It is a symmetric distance metric between any two probability density functions p(x) and p(y), satisfying 0 < Dcs <\u221e, with the minimum achieved if and only if p(x) = p(y). Furthermore, CS divergence can be estimated non-parametrically using a kernel density estimator (KDE) (Parzen, 1962), eliminating the need for explicit parametric assumptions about the underlying distributions. It also offers an elegant closed-form expression for mixtures of Gaussians (MoG) (Kampa et al., 2011) and infinite MoG. This provides significant flexibility in measuring distributional distance.\nBy incorporating the CS divergence into Eq. (5), we propose CS-Aligner, with the objective function:\n$\\min -I(x; y) + ADcs(p(x),p(y)).$\nThis formulation ensures both semantic and distributional alignment, enabling robust and efficient multimodal learning across diverse real-world tasks."}, {"title": "3.3. CS-Aligner", "content": "We use CS-Aligner to align the pretrained multimodal models, as illustrated in Fig. 3. To enable the alignment, we detail the estimation methods for both CS divergence and mutual information in Eq. (7), providing the complete objective of CS-Aligner.\nCS divergence estimation. We use KDE to estimate Dcs(p(x); p(y)) nonparametrically. Given i.i.d. samples {x_i}_i=1^M ~ p(x) and {yj}_j=1^N ~ p(y), the empirical CS divergence estimator is given by (Jenssen et al., 2006):\n$\\Dcs (p(x); p(y)) = log (\\frac{1}{M^2} \\sum_{i,j=1}^M \u043a(xi, xj) ) + \\frac{1}{N^2} log (\\frac{1}{2} \\sum_{i,j=1}^N \u043a(yi, yj) ) - 2 log (\\frac{1}{MN} \\sum_{i=1}^M \\sum_{j=1}^N \u043a(x_i, y_j)).$\nwhere \u043a is a kernel function such as Gaussian \u043a(x, y) = exp(-||x \u2212 y||^3/2\u03c3^2). This estimator is symmetric, differentiable, and computationally efficient, making it suitable for multimodal alignment. Moreover, the third term in Eq. (8) ensures that Dcs(p(x);p(y)) \u2192 \u221e only when \u043a(x, y) \u2192 0 (i.e., when the distributions do not overlap). However, as long as there is nonzero overlap between the distributions, the estimator remains well-defined and valid.\nRemark 3.3. Connection to the prior loss (l2 loss) (Patel et al., 2024). Consider the third term in Eq. (8), which involves \u03ba(xi, yj) defined by the Gaussian kernel \u043a\u03c3(x,y) = exp(-||x \u2212 y||^3/2\u03c3^2). A second-order Taylor expansion yields\n$\\\u043a(\u0445\u0456, \u0443\u0458) = exp(\\frac{(xi - yj)^2}{2\u03c3^2}) \u22481- \\frac{(x - y)^2}{2\u03c3^2}.$\nWhen i = j (i.e., diagonal of \u043a(x, y)), this approximation reduces to a weighted l2 loss by 1/2\u03c3^2, analogous to the Eq. 1. Consequently, the l2 loss emerges as a special case of our divergence, focusing solely on paired sample reconstruction and omitting broader distribution alignment, including off-diagonal (cross-sample) contributions.\nMutual information estimation. The mutual information and the CS divergence serve complementary roles for alignment: mutual information captures semantic relationships by focusing on pairwise samples, while CS divergence aligns modalities at the distributional level (overall data distribution), leveraging global information.\nInspired by CLIP-based methods (Radford et al., 2021), we estimate the mutual information term I(x, y) in Eq. (5)using its lower bound, optimized via InfoNCE (Eq. (3)). We specifically choose InfoNCE not only for the optimization efficiency but also for its compatibility with CS divergence in the cosine similarity space.\nRemark 3.4. The connection between CS divergence and InfoNCE becomes evident when analyzing both terms from a cosine similarity perspective. For a characteristic kernel \u043a(x,y) = (\u03c6(x),\u03c6(y))H, where \u03c6 maps samples to a Reproducing Kernel Hilbert Space (RKHS) H, the mean embeddings are: \u03bc\u03b1 = \\frac{1}{m} \\sum_{i=1}^m \u03c6(xi) and \u03bcy = \\frac{1}{1} \\sum_{i=1}^1 \u03c6(yi), The CS divergence can then be expressed as:\n$\\Dcs (p(x); p(y)) = -2log( \\frac{\u3008\u03bc\u03b1, \u03bc\u03c5\u3009 H}{||Mx||H||py||H} ).$\n$=-2 log sim(\u03bc\u03b1, \u03bc\u03b7),$\nwhich evaluates the cosine similarity between distributions. Similarly, InfoNCE evaluates cosine similarity between paired samples (Eq. 3). This dual-level similarity assessment underscores the synergy between CS divergence and mutual information, offering a unified and robust framework for multimodal alignment.\nFinal objective function. With the exact estimation of the CS divergence in Eq. (8) and InfoNCE in Eq. (3), the final objective function of our method is formulated as:\n$\\LCS-Aligner = Dcs(p(x); p(y)) + ALInfoNCE.$\n3.4. Extended alignment with unpaired data\nBenefits from the distributional alignment, we further propose some novel extensions of CS-Aligner, which leverage additional information in unpaired data. While mutual information estimation requires pairwise data, the CS divergence estimator (Eq. (8)) can operate seamlessly on unpaired data without introducing additional computation. This unique capability enables CS-Aligner to extend beyond traditional pairwise multimodal alignment by incorporating additional distributional information from unpaired data or tokens. We introduce two novel directions for this extended alignment: unpaired data alignment and token alignment.\nUnpaired vision-language alignment. Our method leverages two forms of unpaired alignments: (1) images with multiple captions, and (2) independently sampled unpaired images and texts. The unpaired alignments are achieved using Eq. (8), where {x_i}_i=1^M -1 and {yj } j=1^N -1 can be independent with M \u2260 N. In both scenarios, our method leverages more uncurated unpaired data for distributional multimodal alignment, providing greater flexibility and robustness.\nVision-language token alignment. We also propose a novel intra-sample distribution alignment approach between vision and language tokens. Unlike CLIP-based models (Radford et al., 2021), which align only the \u201cCLS\" tokens of vision and text representations, the method considers all vision and text tokens for a more fine-grained alignment. Specifically, each vision feature xi \u2208 RV\u00d7D is modeled as a token distribution p(xi) containing V vision tokens, while each text feature yi \u2208 RL\u00d7D is represented as a token distribution p(yi) consisting of L text tokens. D denotes the feature dimension. We compute the CS divergence between the vision and text token distributions. The internal token-wise alignment loss Ltoken is formulated as:\n$\\Ltoken = \\frac{1}{B} \\sum_{i=1}^B Dcs(p(xi); p(yi)),$\nwhere B is the batch size. In general, V \u2260 L, and vision and language tokens do not have a direct pairing, making InfoNCE inapplicable for estimation. Through our distributional alignment, Eq. (12) enables comprehensive alignment across all tokens, capturing more details and potentially enhancing fine-grained alignment.\n3.5. Parameter-efficient multimodal alignment\nWe demonstrate the effectiveness of our CS-Aligner by performing vision-language alignment in a parameter-efficient manner using pretrained vision and language models, such as CLIP and large language models (LLMs) (Dubey et al., 2024). To adapt these pretrained models, we employ two widely used frameworks: adapter (Gao et al., 2024) and LORA (Low-Rank Adaptation) (Hu et al., 2021).\nAdapter alignment. We add a lightweight transformer (Vaswani, 2017) on top of the pretrained model as an adapter. The adapter projects text embeddings or image embeddings into a shared representation space and distribution.\nLORA alignment. We also explore LoRA to insert trainable low-rank matrices into the pretrained weights of the text encoder. It enables fine-grained adjustments to the representations, aligning them with the other modality distribution.\nThe adapter and LoRA enable efficient alignment of the multimodal large-scale pretrained models, without requiring extensive computational resources."}, {"title": "4. Experiments", "content": "We evaluate our method on two tasks to illustrate its vision-language alignment ability: text-to-image (T2I) generation in Section 4.1 and image-text retrieval in Section 4.2.\n4.1. Text to image generation\nDatasets. Following a previous T2I approach (Patel et al., 2024), we train our method on four datasets: MSCOCO (Lin et al., 2014), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), and LAION-HighResolution-5M (Schuhmann et al., 2022). MSCOCO contains 80K images paired with multiple captions. CC3M and CC12M include about 2.5M and 10M image-text pairs, respectively. LAION-HighResolution comprises 175M high-resolution pairs, from which we select 5M for training. We evaluate the aligned model on the MSCOCO 30K validation set.\nExperimental setup. We build our method based on unCLIP-style approaches (e.g., DALL-E-2 (Ramesh et al., 2022), Karlo (Donghoon et al., 2022), Kandinsky (Razzhigaev et al., 2023)). These methods typically train a diffusion prior module on large-scale datasets (more than hundreds of millions data) to map text representations into the image representation space. The diffused text representations are fed into a separate decoder for image generation.\nDifferently, CS-Aligner trains an adapter to align text representations to image feature space on small-scale datasets, e.g., MSCOCO (0.08M), CC3M (3M), and CC12M (12M), and LAION-HighRes subset (5M). After alignment, we directly process the aligned text features using the pretrained decoder of the large-scale methods (e.g., Karlo and Kandinsky) to generate images, without additional prior modules or multiple diffusion steps. We evaluate generation quality with the FID score (Heusel et al., 2017), which measures how closely generated images match the real image distribution. This metric is particularly well-suited for evaluating modality alignment, as it directly reflects the distribution distance. Additional details can be found in Appendix C.\nBaselines. Our baselines consists of both large-scale methods Karlo (Donghoon et al., 2022), Kandinsky (Razzhigaev et al., 2023) and recent small-scale alignment method Eclipse (Patel et al., 2024). Eclipse streamlines the prior module in Karlo and Kandinsky by employing an L2 loss for T2I. For a fair comparison, we adopt the same Transformer adapter as Eclipse and only align the \u201cCLS\u201d tokens, highlighting the advantages of our distributional alignment.\nComparisons. We compare our method with both the large-scale diffusion-based methods and the small-scale alignment methods. The results are provided in Table 1. By aligning text representations to image representations on the small MSCOCO data, our method achieves superior T2I generation than the large-scale methods Karlo and Kandinsky, without any diffusion steps. CS-Aligner also outperforms Eclipse by an obvious margin using either Karlo or Kandinsky decoders. The results demonstrate the effective vision-language alignment capability of our method. Moreover, we compare CS-Aligner with Eclipse across different training datasets. As shown in Table 2, our method performs better across diverse training data (CC3M, CC12M, and LAION-HighRes-5M), underscoring the importance of the modality distribution information for robust alignment.\nQualitative Visualization. To further evaluate our method, we present qualitative visualizations of generated images using the Karlo decoder. As shown in Fig. 4, our aligned text representations result in more realistic images with stronger semantic consistency with the input sentence, highlighting the effectiveness of CS-Aligner in enhancing alignment.\nCS-Aligner with different adaptation approaches. To demonstrate the robustness of our method across different models, we perform alignments for T2I using both adapter and LoRA. Specifically, we apply LoRA with a low-rank dimension of 8 to every transformer layer in the CLIP text encoder. As shown in Table 3, based on either Karlo or Kandinsky, CS-Aligner with LoRA introduces fewer parameters, while still achieving comparable results compared with the adapter-based one, demonstrating the effectiveness and adaptability of CS-Aligner across different models.\nCS-Aligner with multiple captions. It is common in real-world datasets for a single image to correspond to multiple captions (e.g., 5 captions per image in MSCOCO). Due to their pairwise alignment nature, previous methods such as InfoNCE and 12-based approaches (Radford et al., 2021; Patel et al., 2024) struggle to simultaneously leverage multiple captions. In contrast, by incorporating CS divergence, our CS-Aligner enables training for alignment with single image and multiple captions. To demonstrate the benefits of multiple captions for CS-Aligner, we conducted experiments on the MSCOCO dataset by estimating the CS divergence term Dcs in Eq. (11) using both single and multiple captions. As shown in Fig. 5a, CS-Aligner effectively leverages the information provided by multiple captions, leading to improved vision-language alignment.\nCS-Aligner with additional unpaired data. Collecting and accurately annotating paired vision-language data is both challenging and costly. Enhancing alignment with additional unpaired data offers a more flexible and scalable solution for real-world applications. However, similar to the case of multiple captions, previous methods (Radford et al., 2021; Patel et al., 2024) struggle to fully utilize unpaired data due to their reliance on pairwise alignment, whereas our CS-Aligner naturally incorporates the unpaired data information by CS divergence. To demonstrate this capability, we conduct experiments on the MSCOCO dataset using the Kandinsky decoder with (1) 80K paired training samples, (2) 40K paired training samples, and (3) 40K paired training samples supplemented with 80K unpaired samples, where the unpaired samples are used to estimate the CS divergence. As shown in Fig. 5b, the result with 40K paired training data is lower than 80K. However, introducing additional unpaired data obviously improves the performance, even surpassing the model trained with 80K paired samples. This demonstrates CS-Aligner's ability to effectively leverage the distributional information of modalities for alignment.\nCS-Aligner with token alignment. Beyond the unpaired data, CS-Aligner also enables token-level alignment by treating the tokens of each sample as a distribution. We evaluated the token-level extension of CS-Aligner using the Kandinsky decoder on the MSCOCO dataset. As shown in Fig. 6, incorporating token alignment further improves performance. Moreover, qualitative results indicate that token alignment enhances fine-grained details in generated images, suggesting an improved ability to capture fine-grained relationships between modalities. Additional visualizations are provided in Fig. 8 in Appendix D.2."}, {"title": "4.2. Image-Text Retrieval", "content": "Experimental Setup. Effective multimodal alignment also benefits cross-model retrieval. To demonstrate the alignment ability of our method on retrieval tasks, we conduct experiments aligning LLMs (Dubey et al., 2024) text representations with CLIP vision representations on both image-to-text and text-to-image retrieval. We use the Flickr 1K test set (Young et al., 2014) for short-text retrieval, while Urban1K (Zhang et al., 2025) and DOCCI (Onoe et al., 2025) are employed for long-text retrieval. We compare CS-Aligner against pure InfoNCE-based methods, such as Long-CLIP (Zhang et al., 2025) and LLM2CLIP (Huang et al., 2024), as the baselines. To ensure a fair comparison, we adopt the setup from LLM2CLIP, aligning CLIP ViT-L/14 image representations with Llama 3 (8B) text representations. Both the vision and text representations are aligned by adapters trained on the CC3M dataset.\nComparisons. The results in Table 4 show that our method consistently and significantly outperforms the baselines across various datasets for both image-to-text (I2T) and text-to-image (T2I) retrieval. This demonstrates the effectiveness of our method for aligning the two modalities into a shared space. Moreover, the ability to align a different text encoder (LLM) with the successful alignment of an LLM-based text encoder with the CLIP image encoder highlights the flexibility and generalizability of our approach."}, {"title": "5. Conclusion", "content": "In this paper, we propose CS-Aligner, a novel distributional alignment framework that integrates Cauchy-Schwarz (CS) divergence with mutual information for multimodal alignment. By combining global distributional alignment with InfoNCE, CS-Aligner achieves tighter and more comprehensive alignment. By considering the modality distributional information, our method enables to leverage additional and detailed information from unpaired samples and tokens, leading to more flexible and fine-grained information for alignment. We demonstrate the effectiveness of our alignment on text-to-image generation and cross-modal retrieval.\nLimitation & future work. Although our method has numerous applications, it is currently evaluated only on unCLIP-type models for generation. In the future, we will explore its integration with stable-diffusion-based models. Broader modalities (e.g., audio and video) and diverse tasks (e.g., image-to-text generation) could further strengthen alignment and broaden the applicability of CS-Aligner."}, {"title": "Impact Statement", "content": "This paper contributes to the advancement of Machine Learning. While our work may have various societal implications, none require specific emphasis at this stage."}, {"title": "A. Details of the toy examples", "content": "Mutual information. For two continuous random variables x and y", "as": "n$\\I(x; y) = \\int\\int p(x", "solution": "n$\\I(x; y) = - \\frac{1}{2} ln(1 \u2013 p^2).$\nIn particular, for correlation p 0.99, we have I(x, y) \u2248 1.959, while for p = 0, the variables are independent and I(x"}]}