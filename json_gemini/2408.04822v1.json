{"title": "Performance Prediction of Hub-Based Swarms", "authors": ["Puneet Jain", "Chaitanya Dwivedi", "Vigynesh Bhatt", "Nick Smith", "Michael A Goodrich"], "abstract": "A hub-based colony consists of multiple agents who share a common nest site called the hub. Agents perform tasks away from the hub like foraging for food or gathering information about future nest sites. Modeling hub-based colonies is challenging because the size of the collective state space grows rapidly as the number of agents grows. This paper presents a graph-based representation of the colony that can be combined with graph-based encoders to create low-dimensional representations of collective state that can scale to many agents for a best-of-N colony problem. We demonstrate how the information in the low-dimensional embedding can be used with two experiments. First, we show how the information in the tensor can be used to cluster collective states by the probability of choosing the best site for a very small problem. Second, we show how structured collective trajectories emerge when a graph encoder is used to learn the low-dimensional embedding, and these trajectories have information that can be used to predict swarm performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Biological inspiration drawn from honeybees, ants, birds, and various animal species has been instrumental in agent-based models (ABMs) of multi-agent swarms. In ABMs, each agent independently implements its own controller, and collective behavior emerges from interactions among the agents [1], [2], [3], [4]. ABMs capture the decentralized and individualized nature of interactions in complex systems, making them valuable for empirically studying emergent behaviors and system-level dynamics. This paper addresses the best-of-N problem, where agents stationed at a central hub make a distributed decision to choose the best site from a set of N possibilities [5].\nA common bottleneck for understanding large-scale bio-inspired swarms is the agents' slow decision making and the huge complexity of the system. A solution to this problem is to use differential equations [6], [7], [8], but those assume infinite agents and time. While differential equation have proven effective for generating metrics about performance of a swarm, understanding the performance of hub-based agent colonies with finite robots remains a challenge [9], [10], [11].\nThis paper represents the collective state in an ABMs with the nodes in a graph [12]. Changes in collective state are represented as probabilistic transitions, forming a Markov chain that can be used to predict performance and other swarm properties. Unfortunately, the number of nodes, node features, and edges grows very quickly with the number of agents. This paper shows that low dimensional graph embeddings provide useful information that support computationally feasible ways of understanding swarm behavior."}, {"title": "II. RELATED WORK", "content": "Many bio-inspired swarms exhibit spatial swarming patterns such as flocking or cyclic behavior [3], [13]. Other types of swarms are organized as hub-based colonies where all agents belong to a common nest and fan out from the nest in search of food or new suitable nest sites [11], [14], [15]. Swarms can be implemented as ABMs, which are employed for designing complex systems [16], [17], [18], [19], [20]. ABMs are used in social sciences to model the interactions of individuals [21], [22], simulate decision making [23], and study traffic flow [24]. They are also applied to understand ecosystems and biodiversity [25] and to model disease spread [26].\nFrameworks to extend ABMs built on finite state machines to graph representations include [12], [27]. Graph neural networks have also been used with multi-agent systems such as in traffic engineering [28] and trajectory prediction [29]. Other methods which learn on subgraphs to learn graph representation include [30], [31]."}, {"title": "III. GRAPHS FOR THE BEST-OF-N PROBLEM", "content": "This section presents our ABM formulation of the best-of-N problem and how we create graphs from the ABM.\n**A. Best-of-N Problem**\nThe best-of-N problem is illustrated in Fig. 1 for a problem with two sites, fifty agents, and a hexagonal hub. The agents, which are represented as triangles pointing in their direction of travel, explore the world. When they find a site of potential interest, they return to the hub and inform other agents. If they fail to find a site, they return to the hub and observe other agents. Agents travel between a site of interest and the hub to assess the site and to recruit other agents to the site. Agents recruiting for a site can sense when a quorum of agents are at the hub, and when a quorum is reached the collective decides that the site is the best solution to the problem."}, {"title": "B. Agent Based Model (ABM)", "content": "Unlike ABMs modeled as differential equations [32], [2] or finite state machines with augmented with extra memory [33], the ABM in this paper is satisfies the Markov condition, where every agent's next state is dependent only on its current state. Each agent runs its copy of the state machine illustrated in Figure 2 with the following states. O: Observe, E: Explore, A: Assess, R: Recruit, Tho: Travel to Hub to Observe, THR: Travel to Hub to Recruit, Ts: Travel to Site. $\\delta$ is the dirac-delta function, r is the position of the agent and $r_s$ is the position of a site.\nState transitions depend on the current state and position relative to the position of sites, hub and other agents. The transition probabilities are shown on the edges. The transitions from the travel states (Tho, THR, Ts) are represented by $\\delta$ functions, which means that agents transition from these travel states only when they reach their destination, which is either the hub (THO, THR) or the site (Ts).\nThe probabilities $p_1$, $p_2$, $p_3$, and $p_4$ are modeled as Bernoulli distributions with parameters set to control the mean time agents spend in O, R, A, E, and A, respectively. In the second set of experiments, samples from the Bernoulli distribution are obtained from numpy's random binomial function with number of trials as 1. Transitions from the explore state depend not only on the Bernoulli distribution, $p_3$, but also on the probability that a site is discovered and it's quality (y). The transition from the recruit state (R) depends on (a) the Bernoulli distribution, $p_2$, (b) the number of times that an agent reassesses the site, which is determined by quality of the site, $\\gamma$, and (c) the duration of recruiting, which is also a function of site quality, x.\nThe transition from the observe state depends on two parameters: whether the agent is recruited by another agent to assess a site, which is denoted by z, which is a function of recruiters for each site. The other parameter is how long the agent dwells in the observe state in the absence of being recruited, which is given by the parameter $p_1$. The site to which the agent is recruited is proportional to the number of agents recruiting to each site."}, {"title": "C. Representing Collective State as a Tensor", "content": "A key insight from prior work is that information about the states of individual agents can be combined to create a compressed representation of collective state [33], [34]. Unfortunately, that prior work was not sufficiently powerful to scale when sites could be at different lcoations in the world or when the number of agents changed. Consequently, this paper uses repesentation based on a relational database.\nThe relation header is the list of agent states (R, D, etc.) plus the quality of the site (Q) that the agent is traveling to, traveling from, recruiting to, or assessing. A unique agent identifier is also included, yielding a relation like Table I"}, {"title": "D. Representing State Dynamics as A Graph", "content": "The collective state graph is constructed by creating a node for each tensor, and creating an edge between nodes if the swarm can evolve from the collective state in one node to the collective state in the other. Each tensor encodes the features associated with each node in the collective state graph. Graph edges encode transitions between collective states. Some nodes can transition to multiple next states, and the probability of the specific transitions is determined by the probabilities with which agents transitions between states in their individual state machines."}, {"title": "E. Information in the Tensor", "content": "It is useful to explore what kind of information can be derived from the collective state tensors for a very small collective. Consider a collective with only ten agents and two sites, one site with maximum quality q($s_1$) = 1 and the other site with relatively low quality, q($s_2$) = 0.5. Because there are only a few agents and sites, the number of possible tensors is (relatively) small, so running several simulations provides a reasonable approximation of the entire graph.\nWe ran 500 trials with agents placed in random starting states and locations in the world that were appropriate for the state (e.g., moving in the world if in an explore state). The decision quorum threshold was set to three agents, which yielded 348 trials in which the agents chose the best site and 152 trials where they chose the inferior site. A tensor was part of a successful trajectory if (a) the trajectory ended in choosing the best site and (b) the tensor was visited at least once in the trajectory. The probability that a tensor yielded success was the number of times a tensor was part of a successful trajectory divided by the number of times it was part of any trajectory."}, {"title": "IV. LOW-DIMENSIONAL EMBEDDINGS", "content": "The previous section assumed the entire graph was known, which is unreasonable when there are many agents, sites, or possible site locations. This section addresses this limitation by using a GraphSage based graph encoder [30] to inductively learn the graph embedding.\n**A. Input State Tensor for Embeddings**\nThree modifications from Table I in Section III-C are made to the relational database and tensor. First, rather than using a one-hot encoding for agent state, agent state, denoted by S, is represented as a floating point value given by the following: R = 0/6, A = 1/6, THR = 2/6, Ts = 3/6, O = 4/6, E = 5/6, and Tho = 6/6. Second, unlike the previous experiment where the graph only applied to two sites that were at fixed locations, the graph encoder needs to work for sites at different locations. Thus, the (x($s_i$),y($s_i$)) position of the site $s_i$ favored by the agent, if any, is added to the agent record, after normalizing by maximum distance of the environment. Each record is therefore a tuple [q($s_i$),S,x($s_i$),y($s_i$)]. Third, unlike the previous experiment where the number of agents was fixed, the graph encoder needs to work for different numbers of agents, which we bound to be less than or equal to 10. The tuple is constructed as described above, but the record for any \u201cextra\u201d agents is the constant [0,1,1,1]."}, {"title": "B. Graph Convolutional Neural Network", "content": "We train the network inductively by forming subgraph samples. A subgraph sample is formed by creating the nodes and edges from a single single simulation. The encoder architecture is illustrated in Figure 5. The input dimension i is 40, the hidden dimension h is 20, and the output dimension O is 3, which yields a 3-dimensional embedding.\nWe leverage the GraphSAGE convolution layers [30] for aggregating features from a node's neighbors, thus enabling the learning of rich and complex node embeddings. The architecture consists of two graph convolution layers followed by an activation function ReLU, and a third Linear layer. We also incorporate a residual connection, first introduced in [36], and used widely in LSTMs [37], transformer based systems like GPT-3 [38] and AlphaFold [39]. This directly connects the input to the final output through a linear transformation to match the output dimensions. This shortcut is added to the output after the third convolution, facilitating an element-wise addition that merges the transformed input directly with the learned features. This residual mechanism is crucial for alleviating the vanishing gradient problem in deep neural networks, enabling the model to preserve information from the input throughout the network to enhance learning by providing alternate pathways for gradient flow."}, {"title": "C. Loss Function", "content": "The loss function used to train the neural network is based on graph autoencoders [40], [41] which aim to create graph embeddings in which nodes that are adjacent in the network have embeddings that are close together. This is done by taking the encoding vectors for two nodes, x and y, and maximizing the sigmoid of the cosine similarity measure\n$\\sigma(x, y)$.\n(1)\nWhen the embeddings of the two nodes are close to (far from) each other the output of Eq. (1) is close to one (close to zero).\nThus, the output of Eq. (1) approximates the existence of an edge in the original adjacency matrix. A binary cross entropy loss function with logits [42] is used to compute the difference between the 0's and 1's in the adjacency matrix and the 0's and 1's approximated by Eq. (1). In essence, this approach penalizes the model when it fails to align its perceived similarities with the provided adjacency criteria, guiding it to learn an embedding space where the desired relationships are accurately captured."}, {"title": "V. EXPERIMENT DESIGN", "content": "This section addresses the following research questions.\n*Problem 1*: Can useful 3D embeddings be generated for multiple different environment and agent configurations?\n*Problem 2*: Do the embeddings for Success, Failure and Hub conditions exhibit useful clustering?\n**A. Experiment Conditions**\nOur environment and swarm configurations consist of defining agents, sites, qualities and distances. We present the parameters used in the ABM and the constraints for our simulations in the Table II. The convergence criteria is set by a threshold of agents recruiting for a given site at the hub. We start with three configurations: Condition 1 has 100% Observe, Condition 2 has 50% Explore and 50% Observe, and Condition three has 90% Observe 10% Recruiting for worst site. Three trajectories are produced by running one simulation for each configuration. 10 tensors from these trajectories are randomly selected to serve as initial conditions. For each initial conditions from the previous paragraph we run 10 simulations for all possible pairs of distances, qualities, and runtimes from Table II. The parameters in Table II are chosen so that not all trajectories end in success."}, {"title": "B. ABM Results", "content": "Figure 6 and 7 show the results of success and time to converge, respectively, for the simulations with 2 sites and 10 agents. The dashed line denotes the inter-quartile range, and the markers show the mean values. A + denotes a distance of 100, denotes is 150, and \u25b3 is 200. The black - denotes the maximum quality among the sites in that simulation. The success metric if we choose site i is defined as q($s_i$)/max($q_{s1}$,$q_{s2}$).\nFigure 6 shows the success vs quality difference for our simulations, for different distances. We see that when the difference between site qualities is low (left side of the plots), the success is still high, since it doesn't matter which site you choose. The success is also high when one site has a much higher quality than the other (right side of the plots). In the middle, the success drops. Site distance (+, \u3147, \u25b3) have very little affect on the success metric.\nFigure 7 shows that distance affects time to converge, as expected. The farther the sites, the higher the time to converge. Both the differences of site quality (x-axis) and the max quality (black dashes) affect the time to converge. When the quality difference is 0.2, decreasing site quality corresponds to increasing convergence time. By contrast, when quality difference is 0.5, convergence time is small for all values of maximum site quality."}, {"title": "C. Labeling Nodes", "content": "From the simulations that converge, nodes where the better site among the two sites is chosen at the convergence are marked as \"Success\" (cyan), and those where the worse site is chosen are marked as \"Failure\" (magenta). Nodes where no agent is site-oriented are marked as \"Hub\" (black). Every other node is marked as \u201cIntermediate\u201d (purple)."}, {"title": "VI. EXPERIMENT RESULTS AND DISCUSSION", "content": "This section presents the data from the experiments.\n**A. Results**\nFigure 8 shows the 3D embeddings for all tensors in the experiments. The smaller cyan and magenta markers denote simulations with 5 agents, and the larger markers denote 10 agents. The markers, \u25b3 markers, and markers indicate conditions with two sites, three sites, and four sites, respectively. It is difficult to see in the figure, but there is very little difference between the embeddings for conditions that have 5 agents and conditions that have 10 agents.\nThe embeddings in Figure 8 provide information about which tensors appear on multiple trajectories. Transparency for the intermediate (non-hub, non-success, non-failure) embeddings in purple is set to highlight that frequently encountered trajectories tend to aggregate together; nodes encountered more frequently are darker. Frequently encountered embeddings tend to aggregate, and these aggregations correspond to probable trajectories of the collective. Infrequent trajectories correspond to sparse point areas."}, {"title": "B. Discussion of Research Questions", "content": "The embeddings in Figure 8 also indicate that embeddings encountered on successful (failed) trajectories tend to cluster with other successful (failed) embeddings. Thus, the proximity of an embedding should be useful for predicting whether the corresponding tensor is likely to yield success or failure. Unlike Fig 4, only successful or failed outcomes are shown, not the probability of success. Thus, areas where magenta and cyan markers overlap indicate uncertain outcomes.\nThe results suggest that useful lower dimensional (3D) embeddings can be generated for a system with varying number of agents and sites, which means that the answer to the first research question is, subjectively, yes. The embeddings show the likely paths taken by the colony to reach either success or failure, or not converge in some cases.\nSubjectively, we see the potential for finding clusters corresponding to basins of attractions: the \u201chub\u201d region, the \"success\" region, the \"failure\" region, and the \"intermediate\" region. This suggest that the answer to the second research questions is, subjectively, yes. Importantly, there is not a single cluster for these areas of interest, but multiple clusters in the embedding space (multiple basins of attraction). Additionally, there are some regions where it is likely that the probability of success is low, which may require additional information from the world to disambiguate.\nGiven these observations about how frequently encountered embeddings aggregate and how embeddings for successful and failure trajectories cluster, we speculate that using representations like the one shown in Figure 8, can used to predict swarm behavior. This information could potentially enable a human to help regulate the swarm behavior."}, {"title": "VII. FUTURE WORK", "content": "The results suggest that embeddings work for 5 and 10 agent groups, but future work should include more agents and experiments with more world configurations. Future work should also explore the \"harder\" problem of differentiating between varying levels of the probability of success. Next, future work should take advantage of edge weights, which can serve as explicit transition probabilities and could lead to richer embeddings. Another direction to explore is various types of global and agent state information in the state tensor to solve different downstream tasks. With the growing developments in transformers, it would also be reasonable to look at the performance of transformers solve this problem."}]}