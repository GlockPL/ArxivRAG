{"title": "DC-VSR: Spatially and Temporally Consistent Video Super-Resolution with Video Diffusion Prior", "authors": ["Janghyeok Han", "Gyujin Sim", "Geonung Kim", "Hyunseung Lee", "Kyuha Choi", "Youngseok Han", "Sunghyun Cho"], "abstract": "Video super-resolution (VSR) aims to reconstruct a high-resolution (HR) video from a low-resolution (LR) counterpart. Achieving successful VSR requires producing realistic HR details and ensuring both spatial and temporal consistency. To restore realistic details, diffusion-based VSR approaches have recently been proposed. However, the inherent randomness of diffusion, combined with their tile-based approach, often leads to spatio-temporal inconsistencies. In this paper, we propose DC-VSR, a novel VSR approach to produce spatially and temporally consistent VSR results with realistic textures. To achieve spatial and temporal consistency, DC-VSR adopts a novel Spatial Attention Propagation (SAP) scheme and a Temporal Attention Propagation (TAP) scheme that propagate information across spatio-temporal tiles based on the self-attention mechanism. To enhance high-frequency details, we also introduce Detail-Suppression Self-Attention Guidance (DSSAG), a novel diffusion guidance scheme. Comprehensive experiments demonstrate that DC-VSR achieves spatially and temporally consistent, high-quality VSR results, outperforming previous approaches.", "sections": [{"title": "1 Introduction", "content": "Video super-resolution (VSR) is a task to restore a high-resolution (HR) video from a low-resolution (LR) counterpart, which has a vast array of applications, such as enhancing old footage and improving streaming video quality on limited bandwidths. However, VSR is particularly challenging due to its severely ill-posed nature of the problem, primarily because of the missing high-frequency information in LR videos caused during the sampling process. Furthermore, real-world videos face a myriad of unknown degradation factors beyond just sampling issues, including noise, compression artifacts, and various other distortions, making the task even more ill-posed.\nTo achieve successful VSR, it is essential to generate realistic HR details while maintaining both spatial and temporal consistency. To this end, over the past few decades, numerous approaches have been proposed, ranging from traditional methods such as interpolation techniques and model-based optimization approaches to recent neural network-based ones [Zhang and Yao 2024]. Nevertheless, most of these approaches struggle to produce realistic HR details due to a lack of effective priors that model high-frequency details as shown in Fig. 1(b).\nRecently, diffusion models that provide powerful generative priors for natural images, have been exploited for VSR to achieve high-quality results with realistic textures. Yang et al. [2024a] and Zhou et al. [2024] utilize image diffusion models as their generative priors and successfully restore detail-rich HR videos from real-world LR videos. However, adopting image diffusion models poses challenges in maintaining spatial and temporal consistency due to the design of image diffusion models, which typically target single images with limited spatial sizes due to significant memory usage. To achieve temporally-consistent VSR using image diffusion models, these approaches adopt additional temporal layers, frame information propagation with motion compensation, and overlapping of temporal windows. Furthermore, to handle large video frames, frames are split into overlapping tiles, processed individually, and then merged. Despite these efforts, the resulting videos often suffer from spatially and temporally inconsistent results due to the inherent randomness of image diffusion models and their tile-based approach (Fig. 1(c) and (d)).\nIn this paper, we propose DC-VSR (Diffusion-based Consistent VSR), a novel VSR approach that, for the first time, leverages a video diffusion prior. Our approach produces spatially and temporally consistent results with realistic textures, given an LR video of arbitrary length and spatial size (Fig. 1(e)). We leverage Stable Video Diffusion (SVD) [StabilityAI 2023] as a video diffusion prior, which provides powerful generative capabilities for restoring high-quality and temporally-consistent details. However, exploiting an existing video diffusion model for consistent VSR over long video clips with large frames is not straightforward, since existing models are designed to synthesize a limited number of small frames, similar to image diffusion models.\nTo achieve spatial and temporal consistency for a long video with large frames, DC-VSR introduces a novel Spatial Attention Propagation (SAP) scheme and Temporal Attention Propagation (TAP) scheme. Specifically, DC-VSR decomposes an input LR video into multiple spatio-temporal tiles, and processes them separately. To achieve spatial consistency across tiles, SAP introduces a subsampled feature map representing the entire area of a video frame and uses it to process tiles at different spatial locations. On the other hand, TAP enhances temporal consistency across tiles by propagating information between temporally consecutive tiles. Both schemes are realized by extending the self-attention layers of a video diffusion model, enabling information on HR details to be effectively propagated across tiles without losing the generative capability of a pretrained diffusion model.\nAdditionally, we propose Detail-Suppression Self-Attention Guidance (DSSAG), a novel diffusion guidance scheme to improve high-frequency details in synthesized HR video frames. Similar to Self-Attention Guidance (SAG) [Hong et al. 2023] and Perturbed Attention Guidance (PAG) [Ahn et al. 2024], DSSAG guides the diffusion process by amplifying high-frequency details in the latent representation. However, unlike the previous methods, DSSAG provides more flexible control over the guidance scale, and can seamlessly integrate with classifier-free guidance (CFG) for high-quality synthesis without incurring additional computational overhead.\nWe demonstrate the effectiveness of our approach on real-world VSR tasks, where input LR videos contain various unknown degradations. Our experimental results show that DC-VSR achieves spatially and temporally consistent, high-quality VSR results, outperforming previous approaches. Our contributions are summarized as follows:\n\u2022 We introduce DC-VSR, a novel VSR approach based on a video diffusion prior, which produces spatially and temporally consistent results with realistic textures. Our approach is the first to exploit a video diffusion prior in VSR.\n\u2022 We propose a Spatial Attention Propagation (SAP) scheme that injects subsampled features representing the entire area of a video frame to different tiles, ensuring spatial consistency.\n\u2022 We propose a Temporal Attention Propagation (TAP) scheme for sharing information across temporally distant frames, achieving temporal consistency.\n\u2022 We introduce Detail-Suppression Self-Attention Guidance (DSSAG), which enhances the quality of synthesized video frames without any additional computational overhead."}, {"title": "2 Related Work", "content": "Generative Prior for Video Super-Resolution. VSR methods can broadly be classified into two main categories based on the use of generative prior. VSR models without generative prior [Chan et al. 2021, 2022a,b; Xie et al. 2023; Youk et al. 2024; Zhang and Yao 2024] are typically trained using reconstruction losses, such as the mean squared error (MSE), augmented with additional techniques designed to enhance temporal consistency. Although they restore HR videos closely matching the input LR videos, they fail to generate complex details, producing blurry results due to the significant ill-posed nature of the problem. To achieve VSR with rich details,"}, {"title": "3 DC-VSR", "content": "Given an input LR video ILR consisting of N frames {ILR, \u2026\u2026\u2026, IR}, DC-VSR produces an HR video IHR. Fig. 2 illustrates the overall framework of DC-VSR, which is built upon the SVD framework [StabilityAI 2023]. To begin, DC-VSR upsamples ILR using bicubic interpolation to match the target resolution, and obtains an upscaled video IUP. It then embeds Iup into the latent space using the VAE encoder [Rombach et al. 2022], obtaining a latent representation 1, which consists of [11,..., IN] stacked along the channel dimension where li represents the latent of the i-th upsampled video frame IP. To generate an HR video, DC-VSR initializes the latent representation xy of IHR as random noise, where T is the number of diffusion sampling steps, and xt is a tensor with the same size as l. At each diffusion sampling step t, xt and I are first concatenated in an interleaved manner, i.e., [xt,1, l\u2081,\u00b7\u00b7\u00b7, xt,N, IN], where xt,i is the noisy latent of the i-th HR video frame. The concatenated latents are then split into spatio-temporal tiles. We refer to each tile as xt.mn where m and n are spatial and temporal indices. Each tile is processed through a denoising U-Net, and the processed tiles are merged to obtain the latent representation xt-1 at the next sampling step t - 1. We utilize spatio-temporal tiles of size 64 \u00d7 64 \u00d7 14 in the latent space, corresponding to 512\u00d7512\u00d714 in the image space with a scaling factor of 8. In line with previous approaches [Yang et al. 2024a; Zhou et al. 2024], spatially and temporally neighboring tiles overlap by 50%. Overlapped tiles are blended in the tile merging step in our pipeline using gaussian blending. To achieve spatial and temporal consistency, DC-VSR alternatingly applies either SAP or TAP at each sampling step. This process is repeated until t reaches 0. Finally, xo is fed to a decoder, producing the HR video IHR.\nDC-VSR employs a tile-based approach to handle lengthy videos with large frames with a video diffusion prior. However, na\u00efvely splitting a video into tiles may introduce spatial and temporal inconsistencies. In image diffusion models like latent diffusion model (LDM) [Rombach et al. 2022], self-attention layers of the denoising U-Nets play a crucial role in ensuring spatial consistency within an image. Likewise, video diffusion models such as SVD [StabilityAI 2023] leverage self-attention to achieve spatially and temporally coherent results. Specifically, the self-attention operation is defined as:\n$$SA(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V,$$\nwhere Q, K and V are query, key, and value in the matrix representations, respectively, and d is the attention feature dimension. For a certain spatial and temporal position in a video, the self-attention operation calculates the correlation between the query at position and keys at other positions, and aggregates values based on these correlations. As a result, the synthesized content of any region within the video is harmonized with the rest of the video. However, when a video is split into tiles, each tile undergoes an independent attention process, resulting in spatial and temporal inconsistencies. To address this, DC-VSR extends the self-attention operations using SAP and TAP, allowing attentions to be efficiently computed across tiles. In Sections 3.1 and 3.2, we describe SAP and TAP to achieve spatial and temporal consistency across tiles. We then explain DSSAG to further enhance VSR quality in Section 3.3."}, {"title": "3.1 Spatial Attention Propagation", "content": "To achieve spatial consistency across tiles, SAP extends self-attention operations for each tile to incorporate information from different tiles. However, due to the quadratic computational complexity of attention, na\u00efve extension of self-attention operations is practically infeasible. Instead, to avoid the quadratic increase of the computational complexity, SAP leverages subsampled features that represent the entire areas of video frames and injects them into the self-attention operations for each tile.\nFig. 2(b) illustrates the SAP scheme. At diffusion sampling step t for SAP, we feed each tile Xt,m,n to the denoising U-Net. Then, at each self-attention layer, we compute key/value pairs, and subsample them in a spatially uniform manner with respect to a predefined sampling rate SSAP. Finally, we aggregate the subsampled key/value pairs for all m \u2208 {1,..., M}, obtaining the subsampled sets of keys and values, Kt,n and Vt,n. Once Kt,n and Vt,n are obtained, we inject them into the self-attention operation of each tile. Specifically, let us denote the query, key, and value sets computed from tile x1,m,n as Qt,m,n, Kt,m,n, and Vt,m,n, respectively. We construct new sets of keys and values Kt,m,n and Vt,m,n by merging Kt,m,n and Kt,n, and Vt,m,n and Vt,n, respectively. Finally, we perform the self-attention operation using the extended keys and values, i.e.,\n$$SA(Q_{t,m,n}, K_{t,m,n}, V_{t,m,n}) = softmax(\\frac{Q_{t,m,n}K_{t,m,n}^T}{(max(\\gamma^2q_k, 1)\\sqrt{d})})V_{t,m,n},$$\nwhere Qt,m,n, Kt,m,n, and \u0176t,m,n are matrix representations of Qt,m,n, Kt,m,n and Vt,m,n, respectively. We apply the SAP scheme specifically to the first two and last two spatial self-attention layers, as these layers play a crucial role in capturing and synthesizing HR details."}, {"title": "3.2 Temporal Attention Propagation", "content": "Fig. 2(c) illustrates the TAP scheme for cross-tile temporal consistency. TAP bidirectionally propagates information from a tile to its neighbor. Specifically, at each diffusion sampling step for TAP, the propagation is performed in either the forward or backward direction. Without loss of generality, we describe the TAP scheme in the forward direction in the following.\nAt diffusion sampling step t, we process each tile x1,m,n-1 using the denoising U-Net, and extract keys and values from the self-attention layers. We then sample a pair of subsets from the extracted keys and values, which we denote K and V respectively. The subsampled sets K and Vt, are then injected to the self-attention operation for the temporally subsequent tile x,m,n. Specifically, at each self-attention layer for x,m,n we construct new sets of keys and values Kt,m,n and Vt,m,n by merging Kt,m,n and K,m,n-1, and Vt,m,n and Vt,m,n\u22121, respectively. We perform the self-attention operations using the extended keys and values using Eq. Similar to SAP, we apply the TAP scheme to the first two and last two spatial self-attention layers.\nTo sample K and Vt, we select L frames whose keys have the largest standard deviations from tile x1m.n-1 This selection is based on the observation that frames with more varied and sharp details produce distinct keys, leading to larger standard deviations. In our experiments, we set L = 4. We then use the keys and values from these samples as K\u2081,m,n-1 and Vt,m,n\u22121. For a detailed analysis of this sampling strategy, refer to the supplementary material."}, {"title": "3.3 Detail-Suppression Self-Attention Guidance", "content": "To improve the quality of VSR, DC-VSR adopts DSSAG. In this subsection, we first briefly review the previous guidance approaches: CFG [Ho and Salimans 2021], SAG [Hong et al. 2023], and PAG [Ahn et al. 2024]. We then describe DSSAG in detail.\nCFG. To improve sampling quality, CFG [Ho and Salimans 2021] utilizes both a conditional noise and an unconditional noise for denoising at each sampling step. Specifically, CFG is defined as:\n$$\\epsilon_{CFG}(x_t) = \\epsilon_\\theta(x_t) + (1 + s) (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t)),$$\nwhere xt is a latent of a synthesized image at diffusion sampling step t, eg is a denoising U-Net, which is parameterized by 0, s is the CFG scale parameter, and c is the class condition. Eq. (3) emphasizes the class-related components in the latent, resulting in the final synthesized image better reflecting the class condition c.\nSAG. Both SAG [Hong et al. 2023] and PAG [Ahn et al. 2024] improve high-frequency details in synthesized images by introducing perturbation to the high-frequency details in the estimation of the unconditional noise. Specifically, a generalized form of the diffusion guidance can be defined as:\n$$\\epsilon_{DG}(x_t) = \\epsilon_\\theta(x_t) + (1+s) (\\epsilon(x_t, h_t) - \\epsilon_\\theta(x_t)),$$\nwhere ht is a condition, and \u00c2t is a perturbed sample that lacks ht. Based on this generalized form, SAG is defined as:\n$$\\epsilon_{SAG}(x_t) = \\epsilon_\\theta(b(x_t)) + (1 + s) (\\epsilon_\\theta(x_t) - \\epsilon_\\theta(b(x_t))),$$\nwhere b is a blurring operation that detects local regions with high-frequency details using self-attention scores, and blurs the detected regions, while keeping the noise in xt intact. The missing high-frequency details in b(xt) corresponds to ht in Eq. (4), i.e., ht = xt - b(xt). Eq. (5) amplifies high-frequency details synthesized by the conditional model, eventually leading to synthesis results with higher-quality details. SAG applies blurring only to regions with high-frequency details to keep image structure intact, as blurring the entire image may destroy image structures, causing synthesis results with inaccurate image structures.\nPAG. PAG proposes a simpler approach, which is defined as:\n$$\\epsilon_{PAG}(x_t) = \\epsilon_{PAG} (x_t) + (1+s) (\\epsilon_\\theta (x_t) - \\epsilon_{PAG} (x_t)),$$\nwhere EPAG (xt) estimates noise from a perturbed version of xt. To achieve this, PAG replaces the self-attention score matrix with an identity matrix in the self-attention layers in ePAG(xt), i.e., it replaces the self-attention operations with SAperturb (Q, K, V) = V. As a result, EPAG (xt) does not leverage spatially distant information for noise estimation, estimating less accurate noise from xt, which is analogous to noise estimation from a perturbed version of xt.\nSAG and PAG noticeably improve image synthesis quality, especially when combined with CFG. However, integrating them with CFG incurs substantial computational costs. Using e\u0189(xt, c) instead of ep (xt) in Eqs. (5) and (6) allows this combination, but it necessitates running the denoising U-Net three times for SAG due to its blurring function. With PAG, the fixed level of perturbation complicates balancing the effects of CFG and PAG when combined. Consequently, PAG and CFG are typically applied separately, also leading to denoising U-Net three times.\nDSSAG. To enhance high-frequency details, DSSAG offers a simpler approach without additional computational costs. The core idea of DSSAG is as follows. As estimating noise from a noisy image is equivalent to estimating a noise-free image, we assume that the denoising U-Net of a diffusion model estimates a noise-free image in the following. For estimating a noise-free image, the self-attention layers in a denoising U-Net find image regions with similar high-frequency details, by computing weights based on the similarities between queries and keys. Then, they aggregate information from different image regions based on their weights. As noted by Wang et al. [2018a], this self-attention mechanism closely resembles bilateral filter [Tomasi and Manduchi 1998] and non-local means filter [Buades et al. 2005], both of which are renowned structure-preserving filters. Inspired by this, we introduce an additional parameter y to control the weighting function of the self-attention operation, similar to the weighting parameters in bilateral and non-local means filters.\nSpecifically, we extend the self-attention operation as:\n$$SA(Q, K, V, \\gamma) = softmax(\\frac{QK^T}{(max(\\gamma^2q_k, 1)\\sqrt{d})})V,$$\nwhere q and k represent the largest absolute values among the elements of Q and K, respectively. We adopt q and k to adaptively control the weights to the scales of the keys and values. max(., 1) is adopted to make Eq. (7) reduce to the conventional self-attention operation, when y is small. Eq. (7) performs in a similar manner to the non-local means filter. Thanks to the similarity-based weighting function, it preserves salient image structures. Moreover, y allows control over the blurring strength. Assigning a large value to y results in larger weights for keys less similar to the queries, causing the information from different image regions to be more blended. Consequently, the denoising U-Net estimates a blurrier image with fewer high-frequency details as shown in Fig. 3.\nLeveraging the extended self-attention operation in Eq. (7), we define DSSAG and its combination with CFG as:\n$$\\epsilon_{DSSAG}(x_t) = \\epsilon_\\gamma(x_t) + (1+\\eta) (\\epsilon_\\theta(x_t) - \\epsilon_\\gamma(x_t)) and$$\n$$\\epsilon_{CFG&DSSAG}(x_t) = \\epsilon_\\gamma(x_t) + (1+\\eta) (\\epsilon_\\theta (x_t, c) - \\epsilon_\\gamma(x_t)),$$\nwhere is a denoising U-Net whose self-attention operations are replaced with Eq. (7). \u201e does not require any training and shares the same parameters with eg. DSSAG offers a couple of distinct benefits compared to SAG [Hong et al. 2023] and PAG [Ahn et al. 2024]. It does not need additional high-frequency detection or blurring operations, as Eq. (7) already incorporates these in its weighting and aggregation mechanism. Furthermore, DSSAG provides smooth control over blur strength, unlike PAG, enabling seamless integration with CFG without any additional computational costs.\nWe apply DSSAG to the first two and last two spatial self-attention layers in the denoising U-Net, as done for SAP and TAP. During the iterative sampling process of DC-VSR, we set y adaptively to the noise level of the diffusion model, so that y is initially large and gradually decreases as the sampling proceeds. Specifically, we set"}, {"title": "4 Experiments", "content": "Implementation Details. To build DC-VSR, we fine-tune Image-to-Video Stable Video Diffusion (I2V-SVD) [StabilityAI 2023], which adopts the LDM framework [Rombach et al. 2022] with the EDM [Karras et al. 2022] diffusion mechanism. We use the REDS dataset [Nah et al. 2019] to train our model. Following previous work [Chan et al. 2022b; Yang et al. 2024a], we merge 240 training videos and 30 test videos, reorganizing them into 266 training videos and 4 test videos, and refer to the latter as REDS4. We refer the reader to the supplementary material for more implementation details.\nEvaluation Datasets. We use the REDS4 and UDM10 [Yi et al. 2019] datasets for VSR evaluation. We construct HR-LR video pairs from the datasets using the real-world degradation pipeline of Chan et al. [2022b], which applies random blur, resizing, noise, JPEG compression, and video compression. Additionally, we use the VideoLQ dataset [Chan et al. 2022b] as a real-world LR dataset."}, {"title": "4.1 Comparison with Previous SR and VSR Approaches", "content": "To evaluate the performance of DC-VSR, we compare it with various previous methods across different categories. Specifically, our evaluation includes non-generative image super-resolution (ISR) methods: bicubic interpolation and SwinIR [Liang et al. 2021]; two generative ISR methods: Real-ESRGAN [Wang et al. 2021] and StableSR [Wang et al. 2024]; two non-generative VSR methods: RealBasicVSR [Chan et al. 2022b] and RealViformer [Zhang and Yao 2024]; and two generative VSR methods: Upscale-A-Video (UAVideo) [Zhou et al. 2024] and MGLD [Yang et al. 2024a]."}, {"title": "4.2 Component Evaluation", "content": "Effect of Video Diffusion Prior. Fig. 4(a-b) and (c) show the VSR results of previous methods utilizing image diffusion priors and our method employing video diffusion priors, respectively. While image diffusion prior-based methods incorporate additional techniques to enhance temporal consistency, they produce blurry and inconsistent outcomes. In contrast, our approach, which directly applies video diffusion priors without additional temporal consistency techniques, shows significantly clearer and more temporally consistent results.\nAblation Study on TAP & SAP. Fig. 4(c) and (d) present the VSR results without and with TAP, respectively. By applying TAP, temporal consistency between distant frames is effectively preserved. Specifically, in frame 77, a vertical line in the number \"1\" appears in (c) but remains absent in (d), maintaining the original consistency. Fig. 5 illustrates the VSR results of two distant brick regions in a frame, comparing outcomes without and with SAP, respectively. Without SAP, the brick patterns appear mismatched between the regions. In contrast, after applying SAP, the distant brick patterns are restored with coherent details. Additionally, comparisons with previous guidance approaches, where our method shows the clearest image quality and most accurate character shape.\nIn particular, the results of VideoLQ 008 and 049 clearly surpass other VSRs"}, {"title": "5 Conclusion", "content": "In this paper, we introduced DC-VSR, a novel approach leveraging a video diffusion prior to produce spatially and temporally consistent VSR results with realistic textures. Our proposed SAP"}]}