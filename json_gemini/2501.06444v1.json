{"title": "On the Computational Capability of Graph Neural Networks: A Circuit Complexity Bound Perspective", "authors": ["Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Wei Wang", "Jiahao Zhang"], "abstract": "Graph Neural Networks (GNNs) have become the standard approach for learning and reasoning over relational data, leveraging the message-passing mechanism that iteratively propagates node embeddings through graph structures. While GNNs have achieved significant empirical success, their theoretical limitations remain an active area of research. Existing studies primarily focus on characterizing GNN expressiveness through Weisfeiler-Lehman (WL) graph isomorphism tests. In this paper, we take a fundamentally different approach by exploring the computational limitations of GNNs through the lens of circuit complexity. Specifically, we analyze the circuit complexity of common GNN architectures and prove that under constraints of constant-depth layers, linear or sublinear embedding sizes, and polynomial precision, GNNs cannot solve key problems such as graph connectivity and graph isomorphism unless TC\u00b0 = NC1. These results reveal the intrinsic expressivity limitations of GNNs behind their empirical success and introduce a novel framework for analyzing GNN expressiveness that can be extended to a broader range of GNN models and graph decision problems.", "sections": [{"title": "Introduction", "content": "Graphs are ubiquitous representations for relational data, describing interconnected elements with interactions in domains such as molecules [GSR+17, WWCBF22], social networks [FML+19, SLYS21], and user-item interactions in recommendation systems [WHW+19, HDW+20]. Graph Neural Networks (GNNs) [KW17, HYL17, VCC+18] have emerged as a dominant tool for learning expressive representations from graphs, enabling tasks like node property prediction [WSZ+19, BAY22], link prediction [ZC18, ZZXT21], and graph classification [ZCNC18, XHLJ18]. The key to GNNs' success lies in the message-passing mechanism [GSR+17], which aggregates information from local neighborhoods through a graph convolution matrix, followed by nonlinear transformations to update node representations.\nDespite their empirical success, concerns regarding the computational limitations of GNNs are emerging [GJJ20, WW22]. A fundamental research question arises from the concern:\nWhat computational capabilities do GNNs and their variants possess, and what classes of problems can they provably solve?\nAddressing these questions is crucial for understanding GNNs from a principled and theoretically robust perspective, identifying their limitations, and building trust in their deployment for real-world applications.\nPrevious work has made significant strides in addressing these questions. A notable line of research connects GNN expressiveness to the Weisfeiler-Leman (WL) graph isomorphism test [XHLJ18, MRF+19, MRM20, ZGD+24], which iteratively refines color labels on k-tuples of nodes (known as k-WL) and distinguishes graphs by the resulting color histograms. For example, the Graph Isomorphism Network (GIN) [XHLJ18] equipped with summation aggregator and injective readout function is as expressive as 1-WL, while k-GNNs [MRF+19] achieve expressiveness equivalent to k-WL by encoding k-node subgraphs as hypernodes in message passing. However, these results focus on graph isomorphism tasks and do not address a broader range of graph query problems. Moreover, the WL framework only provides a specific level of expressiveness without establishing the theoretical upper bounds, and it often ignores the interplay between node features and graph topology, which is central to GNNs.\nIn this paper, we take a fundamentally different approach to analyzing the computational limitations of GNNs by examining the circuit complexity bounds. Circuit complexity [HAF22, Ruz81, Vol99] is a foundational topic in theoretical computer science, characterizing computational models based on the types of Boolean gates they use and the depth and size of their circuits. Importantly, circuit complexity bounds provably reflect the set of problems solvable within a given model. For instance, models (e.g., Transformers) bounded by the $TC^0$ complexity class can only solve $TCO$ problems (e.g., Dyck language recognition) but cannot solve $NC^1$-complete problems like arithmetic formula evaluation unless $TC^0$ = $NC^1$ [Chi24, MS23, MS24]. Analogously, if we demonstrate that GNN computations lie within a particular circuit complexity class, we can formally identify problems that GNNs can solve and cannot solve.\nOur approach evaluates the circuit complexity of GNN components, from basic activation functions to the entire graph convolution process. We show that GNNs with a constant number of layers, poly(n) precision, and embedding sizes d = O(n) can be approximated by uniform $TC^0$ circuits. Consequently, unless $TC^0$ = $NC^1$, such GNNs cannot solve problems like graph connectivity problems or graph isomorphism problems. These findings illuminate the fundamental expressivity limitations of GNNs despite their empirical success and also establish a novel framework for analyzing their expressiveness, which can be seamlessly generalized to more GNN models and decision problems on graphs."}, {"title": "Related Work", "content": "We present related works on the computational limitation of GNNs and existing circuit complexity bounds for neural networks."}, {"title": "Limitations of Graph Neural Networks", "content": "Graph Neural Networks (GNNs) have demonstrated impressive performance on graph learning and mining tasks. However, their inherent limitations in solving decision problems on graphs remain an open question. The predominant framework for analyzing GNN limitations is the Weisfeiler-Lehman (WL) hierarchy [LW68, Sat20, ZGD+24], a well-established tool for assessing GNNs' ability to address the graph isomorphism problem-an NP-intermediate problem not solvable in polynomial time [Bab16, GS20]. The WL hierarchy leverages the computationally efficient heuristic of color refinement to bound the capability of differentiating non-isomorphic graphs.\nThe expressiveness of message-passing GNNs is bounded by the 1-WL test [XHLJ18]. Standard GNN models such as GCN [KW17], GAT [VCC+18], and GIN [XHLJ18] are either equivalent to or strictly limited by the expressiveness of 1-WL. To go beyond 1-WL, high-order GNNs extend message-passing mechanisms to k-node subgraphs [MRF+19, MRM20, MBHSL19], mimicking the k-WL or k-FWL (Folklore WL) tests. Models like k-GNN and k-FGNN match the expressiveness of these higher-order tests, offering stronger guarantees than standard message-passing GNNs. However, the parameter k cannot be scaled to sufficiently large values due to inherent computational and memory constraints.\nAnother promising line of research involves subgraph GNNs, which aim to address the inherent symmetry of graphs that cannot be distinguished by WL tests. These models transform the original graph into a set of slightly perturbed subgraphs, which are then processed by GNNs [CMR21, PW22, QRG+22]. Recent work has shown that for an n-node graph, subgraph GNNs operating on subgraphs with k nodes are strictly bounded by (k + 1)-FWL [QRG+22]. Besides, distance-aware GNNs inject distance information overlooked by both message-passing GNNs and 1-WL-into their architectures. For instance, k-hop MPNNs aggregate information from k-hop neighbors in each layer and have been shown to be strictly bounded by 2-FWL [FCL+22]. Additionally, the subgraph WL hierarchy demonstrates that distance encoding can be represented by local 2-WL tests [ZFD+23]."}, {"title": "Circuit Complexity and Neural Networks", "content": "Circuit complexity, a foundational area in theoretical computer science, studies the computational power of Boolean circuit families. Various circuit complexity classes play a significant role in analyzing machine learning models. For instance, the class $AC^0$ represents problems solvable in parallel using standard Boolean gates, while $TC^0$ extends this by incorporating threshold or modulo gates. The stronger class $NC^1$ corresponds to problems solvable by circuits with $O(log n)$ depth and bounded fan-in [MSS22]. A key result relevant to machine learning is the complexity inclusion $AC^0 \\subset TC^0 \\subset NC^1$, though whether $TC^0 = NC^1$ remains an open question [Vol99, AB09].\nCircuit complexity bounds have been effectively used to analyze the computational power of various neural network architectures. For example, Transformers, including two canonical variants-Average-Head Attention Transformers (AHATs) and SoftMax-Attention Transformers (SMATs) have been studied in this context. Specifically, [MSS22] shows that AHATs can be simulated by non-uniform constant-depth threshold circuits in TC\u00b0, while [LAG+23] demonstrates that SMATs can also be simulated in a L-uniform manner within TC\u00b0. A follow-up study [MS24] unifies these results, concluding that both AHATs and SMATs are approximable by DLOGTIME-uniform $TC^0$ circuits. Beyond standard Transformers, RoPE-based Transformers [SAL+24], a widely adopted variant in large language models, have also been analyzed using circuit complexity frameworks [LLS+24, CLL+24a]. Similarly, the emerging Mamba architecture [GD23] falls within the DLOGTIME-uniform $TC^0$ family [CLL+24b]. Additionally, Hopfield networks, initially introduced as associative memory systems, have also been shown to exhibit TC circuit complexity bounds [LLL+24].\nDespite the success of circuit complexity in analyzing other neural networks, its application to GNNs is underexplored. While some prior works have attempted to characterize the computational power of GNNs within circuit computation models [BKM+20, CWS24], these efforts are orthogonal to our contributions. A detailed discussion is provided in Section 5.4."}, {"title": "Preliminary", "content": "This section provides fundamental definitions for this paper. We first introduce some notations. In Section 3.1, we present an in-depth overview of the computation of floating point numbers. In Section 3.2, we review several basic definitions of the Graph Neural Networks (GNNs). In Section 3.3, we present some basic concepts of circuit families and their complexity."}, {"title": "Floating Point Numbers", "content": "In this subsection, we introduce fundamental definitions of floating-point numbers and their operations. These concepts establish a critical computational framework for implementing GNNs on real-world machines.\nDefinition 3.1 (Floating Point Numbers (FPNs), Definition 9 in [Chi24]). A p-bit floating-point number (FPN) is represented as a 2-tuple of binary integers $(s,e)$, in which the significand $|s| \\in \\{0\\} \\cup [2^{p-1}, 2^p)$ and the exponent $e\\in [-2^p, 2^{p-1}]$. The value of the FPN is given by $s\\cdot 2^e$. Specifically, when $e = 2^p$, the floating-point number represents positive or negative infinity, depending on the sign of the significand m. We denote the set of all the p-bit FPNs as $\\mathbb{F}_p$.\nDefinition 3.2 (Rounding, Definition 9 in [Chi24]). Let the real number $r \\in \\mathbb{R}$ be of infinite precision. The closest-p bit precision FPN for r is denoted by $round_p(r) \\in \\mathbb{F}_p$. If two such numbers exist, we denote $round_p(r)$ as the FPN with even significand.\nBuilding upon the fundamental concepts introduced above, we present the critical floating-point operations used to compute the outputs of graph neural networks.\nDefinition 3.3 (FPN operations, page 5 on [Chi24]). Let x, y be two integers. We first denote the integer division operation // as:\n$x // y := \\begin{cases} x/y & \\text{if $x/y$ is a multiple of $1/4$} \\\\ x/y +1/8 & \\text{otherwise.} \\end{cases}$\nGiven two p-bits FPNs $\\langle s_1, e_1\\rangle, \\langle s_2, e_2\\rangle \\in \\mathbb{F}_p$, we define the following basic operations:\n$\\begin{aligned} \\langle s_1, e_1\\rangle + \\langle s_2, e_2\\rangle &:= \\begin{cases} round_p(s_1 + s_2 // 2^{e_1 - e_2}, e_1) & \\text{if } e_1 \\geq e_2 \\\\ round_p(s_2 + s_1 // 2^{e_2 - e_1}, e_2) & \\text{if } e_1 < e_2 \\end{cases} \\\\ \\langle s_1, e_1\\rangle \\times \\langle s_2, e_2\\rangle &:= round_p(s_1 s_2, e_1 + e_2) \\\\ \\langle s_1, e_1\\rangle \\div \\langle s_2, e_2\\rangle &:= round_p(s_1 \\cdot 2^{p-1} // s_2, e_1 - e_2 - p + 1) \\\\ \\langle s_1, e_1\\rangle \\leq \\langle s_2, e_2\\rangle &:= \\begin{cases} s_1 < s_2 // 2^{e_1 - e_2} & \\text{if } e_1 \\geq e_2 \\\\ s_1 // 2^{e_2-e_1} < s_2 & \\text{if } e_1 \\leq e_2. \\end{cases} \\end{aligned}$\nThe basic operations described above can be efficiently computed in parallel using simple hardware implementations in TC0 circuits, as formalized in the following lemma:\nLemma 3.4 (Computing FPN operations with TC0 circuits, Lemma 10 and Lemma 11 in [Chi24]). We denote the number of digits as a positive integer p. If $p \\leq poly(n)$, then:\n\u2022 Basic Operations: The \u201c+\u201d, \u201c$\\times$\u201d, \u201c$\\div$\u201d, and comparison ($\\leq$) of two p-bit FPNs, as defined in Definition 3.1, can be computed with O(1)-depth uniform threshold circuits with poly(n) size. Let the maximum circuit depth required for these basic operations be $d_{std}$.\n\u2022 Iterated Operations: The product of n p-bit FPNs and the sum of n p-bit FPNs (with rounding applied after summation) can be both computed with O(1)-depth uniform threshold circuits with poly(n) size. Let the maximum circuit depth required for multiplication be $d_{\\otimes}$ and for addition be $d_{\\oplus}$."}, {"title": "Graph Neural Networks", "content": "With the foundational framework of FPN operations, we now formalize the components of Graph Neural Networks (GNNs) in floating-point representations. This subsection commences by introducing activation functions and the softmax operation, which are fundamental building tools for GNN layers.\nDefinition 3.8 (ReLU). For an embedding matrix $X \\in \\mathbb{F}_p^{n \\times d}$, the output of the ReLU activation function is a matrix $Y \\in \\mathbb{F}_p^{n \\times d}$, where each element is defined as $Y_{i,j} := max\\{0, X_{i,j}\\}$ for all $1 \\leq i \\leq n$ and $1 \\leq j \\leq d$.\nDefinition 3.9 (Leaky ReLU). For an embedding matrix $X \\in \\mathbb{F}_p^{n \\times d}$ and a predefined negative slope $s \\in \\mathbb{F}_p$, the output of the LeakyReLU function is a matrix $Y \\in \\mathbb{F}_p^{n \\times d}$, where each element is defined as $Y_{i,j} := max\\{0, X_{i,j}\\} + s \\cdot min\\{0, X_{i,j}\\}$ for all $1 \\leq i \\leq n$ and $1 \\leq j \\leq d$.\nDefinition 3.10 (Softmax). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be an embedding matrix. The row-wise softmax function is defined element-wise as:\n$Softmax(X)_{i,j} := \\frac{exp(X_{i,j})}{\\Sigma_{k=1}^d exp(X_{i,k})}$\nor equivalently in matrix form as:\n$Softmax(X) := diag(exp(X) \\cdot \\mathbf{1}_d)^{-1} exp(X)$,\nwhere $\\mathbf{1}_d$ is a d-dimensional column vector of ones.\nThese activation functions and softmax operations form the basis of GNN computation. We now introduce the convolution matrices central to the message-passing scheme of GNNs, focusing on three widely used GNN models: GCN [KW17], GIN [XHLJ18], and GAT [VCC+18].\nDefinition 3.11 (GCN convolution matrix). Let $A \\in \\mathbb{F}_p^{n \\times n}$ be the adjacency matrix of a graph with n nodes, and let $D \\in \\mathbb{F}_p^{n \\times n}$ be the diagonal degree matrix, where $D_{i,i} = \\sum_{j=1}^n A_{i,j}$. Let $I \\in \\mathbb{F}_p^{n \\times n}$ denote the identity matrix. The GCN convolution matrix is defined as:\n$C_{GCN} := (D + I)^{-1/2}(A + I)(D + I)^{-1/2} \\in \\mathbb{F}_p^{n \\times n}$."}, {"title": "GIN convolution matrix", "content": "Definition 3.12 (GIN convolution matrix). Let $A \\in \\mathbb{F}_p^{n \\times n}$ be the adjacency matrix, and let $\\epsilon \\in \\mathbb{F}_p$ be a constant. The GIN convolution matrix is defined as:\n$C_{GIN} := A + (1 + \\epsilon)I \\in \\mathbb{F}_p^{n \\times n}$."}, {"title": "GAT convolution matrix", "content": "Definition 3.13 (GAT convolution matrix). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be the embedding matrix, and let $W \\in \\mathbb{F}_p^{d \\times d}$ and $a \\in \\mathbb{F}_p^{2d}$ denote model weights. The attention weight matrix $E \\in \\mathbb{F}_p^{n \\times n}$ is defined as:\n$E_{i,j} := \\begin{cases} a^\\top LeakyReLU(WX_i||WX_j), & A_{i,j} = 1, \\\\ -\\infty. & \\text{otherwise}. \\end{cases}$\nThe GAT convolution matrix is then given by:\n$C_{GAT} := Softmax(E)$."}, {"title": "One GNN layer", "content": "Therefore, we unify the three commonly used graph convolution matrices with basic components to define a general GNN layer.\nDefinition 3.14 (One GNN layer). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be the embedding matrix, $W \\in \\mathbb{F}_p^{d \\times d}$ be the model weights, and $C \\in \\mathbb{F}_p^{n \\times n}$ an arbitrary convolution matrix (e.g., $C_{GCN}, C_{GIN}, C_{GAT}$). A single GNN layer is defined as:\n$GNN_{\\theta}(X) := ReLU(CXW)$.\nBy stacking multiple GNN layers, we obtain a multi-layer GNN capable of learning expressive node embeddings. Different prediction tasks, such as node-level, link-level, or graph-level tasks, require graph pooling operations to aggregate information from specific node subsets. We introduce two commonly used READOUT functions [BL23] for graph pooling:"}, {"title": "Graph average readout layer", "content": "Definition 3.15 (Graph average readout layer). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be a node embedding matrix. A graph average readout layer $READOUT : \\mathbb{F}_p^{n \\times d} \\rightarrow \\mathbb{F}_p^d$ selects a subset $B = \\{i_1, i_2, ..., i_{|B|}\\}\\subseteq [n]$ and computes:\n$READOUT(X) := \\frac{1}{|B|}(X_{i_1} + X_{i_2} + \\dots + X_{i_{|B|}})$."}, {"title": "Graph maximum readout layer", "content": "Definition 3.16 (Graph maximum readout layer). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be a node embedding matrix. A graph maximum readout layer $READOUT : \\mathbb{F}_p^{n \\times d} \\rightarrow \\mathbb{F}_p^d$ selects a subset $B = \\{i_1, i_2, ..., i_{|B|}\\}\\subseteq [n]$ and computes each dimension $j \\in [d]$ as:\n$READOUT(X)_j := max\\{X_{i_1,j}, X_{i_2,j},..., X_{i_{|B|},j}\\}$.\nRemark 3.17. Graph readout functions in Definitions 3.15 and 3.16 support decision problems at various levels, including but not limited to node, link, and graph tasks, since one can target specific nodes, edges, or the entire graph by appropriately selecting the subset B."}, {"title": "MLP prediction head", "content": "Finally, we introduce the MLP prediction head, essential for converting the aggregated embedding into specific predictions:\nDefinition 3.18 (MLP prediction head). Let $x \\in \\mathbb{F}_p^{d}$ be a single output embedding. With model weights $W \\in \\mathbb{F}_p^{d'\\times d}$, $w \\in \\mathbb{F}_p^{d'}$ and bias $b \\in \\mathbb{F}_p^{d'}$, the MLP prediction head is defined as:\n$Head(x) := w^\\top ReLU(Wx + b)$."}, {"title": "Multi-layer GNN", "content": "Finally, we integrate all the previously defined GNN components to present the complete formulation of a multi-layer GNN.\nDefinition 3.19 (Multi-layer GNN). Let m be the GNN layer number. Let $X \\in \\mathbb{F}_p^{n \\times d}$ denote the input feature matrix. For each $i \\in \\{1,...,m\\}$, let $GNN_i$ represent the i-th GNN layer in Definition 3.14. Let READOUT be a graph readout function (Definition 3.15 or Definition 3.16), and Head be the MLP prediction head from Definition 3.18. The m-layer GNN $GNN : \\mathbb{F}_p^{n \\times d} \\rightarrow \\mathbb{F}_p$ is then defined as:\n$GNN(X) := Head \\circ READOUT \\circ GNN_m \\circ GNN_{m-1} \\circ \\dots \\circ GNN_1(X)$."}, {"title": "Circuit Complexity Classes", "content": "In this subsection, we introduce the fundamental concepts of circuit complexity, a key concept of theoretical computer science and computational complexity.\nDefinition 3.20 (Boolean circuit, Definition 6.1 in [AB09]). A Boolean circuit with n binary inputs and one binary output is a mapping $C_n$ between $\\{0,1\\}^n$ and $\\{0,1\\}$, represented as a directed acyclic graph (DAG). The graph consists of:\n\u2022 n input nodes, each with in-degree zero, corresponding to the input variables.\n\u2022 One output node, each with out-degree zero, representing the output variable.\n\u2022 Intermediate nodes, called gates, perform logical operations (e.g., NOT, OR, AND) on the inputs. Each gate has one out-edge, representing the result of the computation. The in-degree of gate nodes is also referred to as their fan-in.\nThe structure of the graph allows the Boolean circuit to evaluate logical functions based on the input values, producing a corresponding output.\nDefinition 3.21 (Complexity measures of Boolean circuits). The size of a Boolean circuit C is defined as the number of nodes in its computation graph. The depth of C is the length of the longest path in its computation graph.\nTo analyze the expressiveness of specific Boolean circuits, we first formally define the concept of languages recognized by a circuit family.\nDefinition 3.22 (Language recognition with circuit families, Definition 2 in [MS23]). A circuit family $\\mathcal{C}$ denotes a set of Boolean circuits. The circuit family $\\mathcal{C}$ can recognize a language $L \\subseteq \\{0,1\\}^*$ if, for every string $s \\in \\{0,1\\}^*$, there exists a Boolean circuit $C_{|s|} \\in \\mathcal{C}$ with input size $|s|$ such that $C_{|s|}(s) = 1$ if and only if $s \\in L$.\nWe now define complexity classes of languages based on the circuit families capable of recognizing them, with a focus on the resources (e.g., depth, size) these circuits require:\nDefinition 3.23 (NC\u00b2, Definition 6.21 in [AB09]). A language L belongs to the class NC\u00b2 (Nick's Class) if there is a family of Boolean circuits that can recognize L, where the circuits have poly(n) size, O((log n)\u00b2) depth, and NOT, OR, AND logical gates with bounded fan-in.\nDefinition 3.24 (AC\u00b2, Definition 6.22 in [AB09]). A language L belongs to the class ACi if there is a family of Boolean circuits that can recognize L, where the circuits have poly(n) size, O((log n)i) depth, and NOT, OR, AND logical gates with unbounded fan-in."}, {"title": "TC", "content": "Definition 3.25 (TC\u00b2, Definition 4.34 in [Vol99]). A language L belongs to the class TC if there is a family of Boolean circuits that can recognize L, where the circuits have poly(n) size, O((log n)) depth, and unbounded fan-in gates for NOT, OR, AND, and MAJORITY operations, where a MAJORITY gate outputs one if more than 1/2 of its inputs are ones.\nRemark 3.26. The MAJORITY gates in Definition 3.25 can be substituted with prime-modulus MOD gates or THRESHOLD gates. Any Boolean circuit utilizing one of these gates is called a threshold circuit.\nNext, we define the complexity class DET, which plays a critical role in certain hardness results.\nDefinition 3.27 (DET, on page 12 of [Coo85]). A Boolean circuit belongs to the DET family if it computes a problem that is NC\u00b9-reducible to the computation of the determinant det(A) of an n\u00d7n matrix A with n-bit integers.\nFact 3.28 (Inclusion of circuit complexity classes, page 110 on [AB09], Corollary 4.35 of [Vol99], page 19 of [Coo85]). We have $NC^i \\subseteq AC^i \\subseteq TC^i \\subseteq NC^{i+1}$ for all $i \\in \\mathbb{N}$. Specifically, for i = 1, we have $NC^1 \\subseteq DET \\subseteq NC^2$.\nRemark 3.29. As noted on page 116 of [Vol99] and page 110 of [AB09], it is well-known that $NC^i \\subseteq AC^i \\subseteq TC^i$ for i = 0. However, whether $TC^0 \\subseteq NC^1$ remains an open problem in circuit complexity. Additionally, as discussed on page 18 of [Coo85], it is unlikely that DET $\\subseteq AC^1$, despite DET being bounded between $NC^1$ and $NC^2$.\nWe have formulated non-uniform circuit families that allow different structures for different input lengths. While flexible, this lack of uniformity is impractical compared to computational models like Turing machines, where the same device handles all input lengths. To address this, we introduce uniform circuit families, where circuits for all input lengths can be systematically generated by a Turing machine under specific time and space constraints. We begin by introducing L-uniformity.\nDefinition 3.30 (L-uniformity, Definition 6.5 in [AB09]). Let $\\mathcal{C}$ denote a circuit family, and let C denote a language class recognizable by $\\mathcal{C}$. A language $L \\subseteq \\{0,1\\}^*$ belongs to the L-uniform class of C if there exists an O(log n)-space Turing machine that can produce a circuit $C_n \\in \\mathcal{C}$ with n variables for any input $1^n$. The circuit $C_n$ must recognize L for inputs of size n.\nNext, we define DLOGTIME-uniformity, which refines L-uniformity by introducing a more computationally practical time constraint. Throughout this paper, references to uniform circuit families specifically denote their DLOGTIME-uniform versions.\nDefinition 3.31 (DLOGTIME-uniformity, Definition 4.28 in [BI94]). Let C be an L-uniform language class as defined in Definition 3.30. A language $L \\subseteq \\{0,1\\}^*$ belongs to the DLOGTIME-uniform class of C if there exists a Turing machine that can produce a circuit $C_n \\in \\mathcal{C}$ with n variables for any input $1^n$ within O(logn) time. The circuit $C_n$ must recognize the language L for inputs of size n."}, {"title": "Complexity of Graph Neural Networks", "content": "In this section, we establish foundational complexity results for each component of a graph neural network (GNN) and then combine these results to derive a circuit complexity bound for the entire multi-layer GNN. Section 4.1 examines activation functions, which form the basics for graph convolution computations. Section 4.2 explores the computation of graph convolution matrices. Section 4.3 analyzes a single GNN layer, the fundamental building block of a multi-layer GNN. In Section 4.4, we investigate graph readout functions and the MLP prediction head, essential for making predictions with a multi-layer GNN. Finally, in Section 4.5, we integrate all components and analyze the complete multi-layer GNN structure, culminating in Section 4.6, where we present the key result: the circuit complexity bound of graph neural networks."}, {"title": "Computing Activation Functions", "content": "In this subsection, we first establish a useful fact about computing pairwise max and min functions with TC\u00ba circuits. We then demonstrate that the ReLU and LeakyReLU activation functions on embedding matrices can be efficiently computed by uniform threshold circuits.\nFact 4.1 (Computing pairwise max and min with TC\u00ba circuits). Let a,b \u2208 Fp be two FPNs. If p \u2264 poly(n), there is an O(1)-depth uniform threshold circuit of poly(n) size and (dstd + 3) depth that can compute min{a,b} and max{a,b}.\nProof. For two FPNs a and b, we first compute the comparison result c = 1 if a < b, and c = 0 otherwise, leveraging an O(1)-depth uniform threshold circuit with depth dstd, as stated in Lemma 3.3. Once c is computed, each bit i \u2208 [p] of min{a,b} and max{a,b} can be determined as $min\\{a,b\\}_i = (c \\land a_i) \\lor (\\neg c \\land b_i)$ and $max\\{a,b\\}_i = (c \\land b_i) \\lor (\\neg c \\land a_i)$ respectively, which can be computed with a 3-depth Boolean circuit in parallel. Combining the depths for comparison and logical operations, the overall circuit depth is $d_{std} + 3$. Since $p \\leq poly(n)$ and each operation is computable with a polynomial-size circuit, we conclude that poly(n) is the size of the entire circuit. This completes the proof.\nLemma 4.2 (Computing ReLU with TC\u00ba circuits). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be a matrix. If $p \\leq poly(n)$, d = O(n), there is aa constant-depth uniform threshold circuit of poly(n) size and depth ($d_{std} + 3$) that can compute ReLU(X).\nProof. For each pair of subscript $i, j \\in [n]$, the entry $ReLU(X)_{i,j}$ is formulated as $ReLU(X)_{i,j} = max\\{0, X_{i,j}\\}$ following Definition 3.8. By Fact 4.1, the computation of max function can be finished with a uniform threshold circuit with depth ($d_{std} + 3$) and poly(n) size. If we compute all the $O(nd) \\leq O(n^2) \\leq poly(n)$ entries in parallel, we can also compute ReLU(X) with depth ($d_{std} + 3$) and size poly(n). Hence, we finish the proof.\nLemma 4.3 (Computing LeakyReLU with TC\u00ba circuits). Let $X \\in \\mathbb{F}_p^{n \\times d}$ be a matrix and $s \\in \\mathbb{F}_p$ is the negative slope. If $p \\leq poly(n)$, d = O(n), there is an O(1)-depth uniform threshold circuit of poly(n) size and depth (3$d_{std}$ + 3) that can compute LeakyReLU(X).\nProof. Considering $\\forall i, j \\in [n]$, the entry $LeakyReLU(X)_{i,j}$ is formulated as $LeakyReLU(X)_{i,j} = max\\{0, X_{i,j}\\} + s \\cdot min\\{0, X_{i,j}\\}$ following Definition 3.8. By Fact 4.1, both $max\\{0, X_{i,j}\\}$ and $min\\{0, X_{i,j}\\}$ function can be computed with a uniform threshold circuit with depth $d_{std} + 3$ and poly(n) size. After that, we can multiply min{0, $X_{i,j}$\\} with slope s and further add it with max{0, $X_{i,j}$\\}, which can be computed with a 2$d_{std}$-depth polynomial size circuit by applying Lemma 3.4 twice. Combining the above computation steps, we have $d_{total} = d_{std} +3+2d_{std} = 3d_{std} + 3$. Since there are O(nd) \u2264 O(n\u00b2) \u2264 poly(n) entries in X and all the computations are finished with polynomial-size circuits, the size of the entire circuit is also poly(n). Thus, we complete the proof."}, {}]}