{"title": "From Context to Action: Analysis of the Impact of\nState Representation and Context on the\nGeneralization of Multi-Turn Web Navigation Agents", "authors": ["Nalin Tiwary", "Vardhan Dongre", "Sanil Arun Chawla", "Ashwin Lamani", "Dilek Hakkani-T\u00fcr"], "abstract": "Recent advancements in Large Language Model (LLM)-based frameworks have\nextended their capabilities to complex real-world applications, such as interactive\nweb navigation. These systems, driven by user commands, navigate web browsers\nto complete tasks through multi-turn dialogues, offering both innovative oppor-\ntunities and significant challenges. Despite the introduction of benchmarks for\nconversational web navigation, a detailed understanding of the key contextual\ncomponents that influence the performance of these agents remains elusive. This\nstudy aims to fill this gap by analyzing the various contextual elements crucial\nto the functioning of web navigation agents. We investigate the optimization of\ncontext management, focusing on the influence of interaction history and web\npage representation. Our work highlights improved agent performance across out-\nof-distribution scenarios, including unseen websites, categories, and geographic\nlocations through effective context management. These findings provide insights\ninto the design and optimization of LLM-based agents, enabling more accurate and\neffective web navigation in real-world applications.", "sections": [{"title": "Introduction", "content": "Conversational Web Agents (CWAs) have emerged as a promising way of executing complex tasks\nwithin web browsers by engaging in multi-turn dialogues with users. These agents are designed to\nnavigate web pages and complete tasks based on user instructions, offering a blend of interaction and\nautomation. While most existing work focuses on single-turn navigation, multi-turn navigation is\ncrucial because it mirrors the complexity and dynamic nature of real-world web interactions, allowing\nfor more human-like, adaptable, and error-resilient communication. Recently introduced benchmarks\nsuch as WebLINX Lu et al. [7] and MT-Mind2Web [4] reflect the pivot towards multi-turn navigation.\nA critical challenge remains in understanding how the context impacts the performance and general-\nization capabilities of CWAs. To avoid giving the long and noisy HTML page directly to an agent and\ninstructing it to predict the correct action, the CWAs introduced in these works [7, 4] approach web\nnavigation in a two-stage process. First, they identify the most relevant elements from the Document\nObject Model (DOM)\u00b9 Tree of the webpage. These elements are combined with interaction history\nand screenshots (in a multi-modal context) to construct an input representation for the LLM. In the\nsecond stage, the agents predict the appropriate actions to take based on this comprehensive input.\nIn WebLINX, this two-stage process leverages compact text-retrieval architectures based on a dual"}, {"title": "Related Work", "content": "Web navigation agents have become increasingly sophisticated, evolving from simple rule-based\nsystems to complex agents capable of completing real-world tasks within web browsers. Early\nwork in this area focused on single-task web agents [6], on datasets like MiniWoB++ [10], which"}, {"title": "Web Navigation Agents", "content": "Web navigation agents have become increasingly sophisticated, evolving from simple rule-based\nsystems to complex agents capable of completing real-world tasks within web browsers. Early\nwork in this area focused on single-task web agents [6], on datasets like MiniWoB++ [10], which"}, {"title": "Website Representation", "content": "Representing real-world websites is a complex task due to HTML pages' extensive and dynamic\nnature. Prior approaches to simplifying or compressing the textual representation of websites include\nrule-based algorithms [17], graph embeddings [11], and model-based approaches [2]. These methods\naim to make the content of web pages more manageable for models to process. For this work, we\nworked with DOM elements to make suitable comparisons with dense markup retrievers introduced\nin [7]."}, {"title": "Methods", "content": "In this section, we discuss the two-stage process for web navigation based on appropriate candidate\nselection, followed by selecting action(s) based on the instructor's commands. In each interaction, at\neach turn, the agent's state st includes the following, based on their availability, to predict actions at:\n\u2022\nct: Candidate elements from DOM that are possible targets for actions during a web\nnavigation task\n\u2022 dt: Current DOM tree of webpage\n\u2022 it: Screenshot of web browser\n\u2022 ut: Instructor's utterance\n\u2022 ht: Interaction History"}, {"title": "Dense Markup Ranking (DMR)", "content": "Web pages are typically represented by large and intricate Document Object Model (DOM) trees,\nmaking direct processing by large language models (LLMs) computationally infeasible. The goal of\nthe DMR stage, as in [7], is to efficiently prune and rank HTML elements, identifying those most\nrelevant to the user's task and instructions based on the utterance and interaction history.\nGiven an HTML document represented as a DOM tree with numerous elements, the DMR aims\nto rank these elements based on their relevance to the task defined by user instructions and the\ninteraction history. We adopt a dual encoder-based approach as described in [7]. Each HTML\nelement is represented using its textual content and attributes, which are then encoded along with\nuser utterances and past interaction history using separate encoders. Using a cosine similarity-based\nobjective, we learn to rank elements based on their relevance. The process begins by gathering\nthe current state of the agent, which includes current DOM tree (dt), user utterance (ut), and past\ninteraction history (ht).\nFor each turn t, we have the processed text representation of the state PDMR (St), which is used to\nscore candidate elements ct,i, where ct,i is the ith candidate element among the many that might be\nconsidered at a given turn. The candidate element ct,i is also represented as text. The 2 Encoders\nE(x) and E'(x) encodes this input text to output vectors.\n\u2022 If Ct,i is the target candidate, we set the label y(Ct,i) = 1."}, {"title": "", "content": "\u2022 Otherwise, y(Ct,i) = 0.\nThe cosine similarity loss is defined as the following mean-squared error:\nLt = ||y(Ct,i) \u2013 simcos (E' (PDMR(st)), E(Ct,i)) ||2\nwhere the cosine similarity is defined as:\nsimcos(x, y) = \\frac{xy}{||x||||Y||}\nDuring inference, the cosine similarity is used to generate a score for each candidate representing the\nsimilarity between PDMR(st) and candidate Ct,i at turn t. The score is used to rank the candidates\nand choose the top-k candidates for the action prediction stage."}, {"title": "Action Model", "content": "Once the DMR identifies the relevant DOM elements, these candidates are combined with other\ninformation from the state st to construct a representation for predicting action strings, which can\nbe parsed and executed. The Action model integrates multiple input modalities, including text\ninstructions, screenshots of the web page, and the interaction history. The input to the model is\ntruncated strategically to fit within the model's token limit. The actions include commands like\nclick, text_input, and submit, which are necessary for navigating and interacting with web pages.\n[7] designed a hierarchical truncation process by setting a limit for each component (DOM tree,\nutterances, actions, candidates, viewport size, screenshot of the browser). At each turn t, the action\nmodel takes the constructed input xt and uses it to predict the action at"}, {"title": "Experiments", "content": "Our candidate dataset for this study is the WebLINX dataset, comprised of demonstrations collected\nduring real-time interaction between a commander and a human navigator. It includes 100,000\ninteractions across 2,337 demonstrations of conversational web navigation, all produced by human\nexperts. The data spans 155 real-world websites, carefully selected from 15 different geographic\nareas, providing a broad and diverse context for our analysis. The Train split contains 969 demos,\nwhich we use for fine-tuning our models, and 1268 demos for testing. Table 1 provides insight into\nthe makeup of a demonstration within the dataset. In Table 1, the utterance length is measured by\nthe number of characters, the duration of the demo is measured by the number of seconds elapsed\nbetween the first instructor utterance and the last turn in the demo, and the number of active turns is\nthe number of turns from the split used in the training or evaluation process."}, {"title": "Analyzing Importance of Context in Web Navigation", "content": "In our study, we perform experiments with the DMR method to optimize its efficiency and effective-\nness on real-world websites. These improvements are focused on redefining the truncation process\nand understanding the impact of interaction history in context."}, {"title": "Impact of Interaction History", "content": "The input interaction history is a combination of past user utterances and actions. In the WebLINX\nbenchmark, the interaction history is essential for understanding the context of the instructor's\nutterance. Typically, the models in this benchmark are configured to process interaction histories\nlimited to the past five interactions. Thus, it is crucial to investigate the influence of interaction history\nlength to answer how far back the interaction must be rolled out for the models to perform well. We\nexperiment with different lengths of past interactions, assuming that extending the history provides\na deeper contextual understanding, enabling the model to predict user intentions better. However,\nextending the history too much might lead to context dilution and lead to the model emphasizing\nirrelevant information."}, {"title": "Impact of Representation Lengths and Truncation", "content": "As the Methods section mentions, each candidate element is represented by a text description, which\nis then tokenized for analysis. The token size for these descriptions can vary significantly, potentially\ninfluencing the model's ability to interpret and rank these elements accurately. The representation\nof these candidate elements has been constrained to 200 tokens in the original implementations\nof DMR. This raises pertinent questions about the impact of token size on the selection of target\ncandidates, especially in scenarios involving complex or detailed element descriptions that might\nexceed this token limit. We perform experiments to investigate the effects of varying token sizes\non the performance of the DMR process. By altering the token limit and evaluating the resulting\nperformance changes, we aim to detect if an optimal tokenization length exists for representing\ncandidate elements.\nAlso, as specified in the Methods section, the input sequence for the action model is truncated to\navoid exceeding the input limits allowed by the models. [7] perform truncation by decomposing the\ncomponents of the input state into their sub-components. The entire DOM \"dt\" includes the values\nand text content; for candidates \"ct\" it involves the XPath and children tags; and for interactions, each\nutterance and action is considered a sub-component. These components are then rendered, tokenized,\nand truncated (where the truncation of each component is thresholded to avoid heavy penalization of\nshorter components) until the goal limit is reached."}, {"title": "Models", "content": "Our ranking process is similar to one proposed in the WebLINX benchmark, where we model it as\na text retrieval challenge. This involves using a model to encode a query and generate scores for\nranking potential candidates. Given this context, we review various models originally developed\nfor text retrieval tasks, as their transferability to similar tasks is often high. Specifically, we chose\nall-MiniLM-L6-v2, a model developed by [9] based on the MiniLM model [12], as well as two other\ncompact models: bge-small-en-v1.5 (BGE) [13] and gte-base (GTE) [5]"}, {"title": "Results", "content": "Our results indicate that optimizing context management and DOM element representation improves\nthe performance of DMR models in candidate selection and, subsequently, web navigation agents.\nFirst, we investigate the effect of truncating the representation of DOM element candidates to fit\nvarious token limits in Table 2. Utilizing the MiniLM model, increasing the token length from 100 to\nthe maximum allowed length generally improved the average Recall@1, @5, & @10 scores on the\nOOD test splits. The highest performance was achieved using the maximum token length setting,\nwith an average Recall@10 score of 52.9. This result suggests that providing more comprehensive\ntext representations of DOM elements can enhance the model's ability to accurately rank and retrieve\nrelevant candidates."}, {"title": "Discussion", "content": "The experimental results presented in this work shed light on several key factors that influence\nthe ability of web navigation agents to generalize effectively to out-of-distribution scenarios. Our\nmodifications to the DMR improved its generalizability in real-time web navigation tasks. By\nsystematically investigating the effects of interaction history length, token length constraints, and\ncontext representation, we uncover valuable insights about the factors influencing the web navigation\ntask in multi-turn dialog settings and how to model these agents for better performance on unseen\nwebsites, categories, and geographic locations. Furthermore, by focusing on critical components,\nexpanding the historical context, and unrestricted element representations, the model also gained\nperformance on out-of-distribution samples."}, {"title": "Limitations and Future Work", "content": "While our work aims to provide valuable insights into the impact of state representation and context\nmanagement on the generalization of multi-turn web navigation agents, there are certain limitations\nin our work, which we aim to address as part of our future work. As part of future work, we aim to\ninvestigate the use of mechanisms that can dynamically determine the appropriate context boundary\nbased on the specific out-of-distribution case. Such mechanisms, potentially leveraging contextualized\npruning or attention techniques, can help identify and prioritize the most relevant dialogue segments\nand element descriptions, facilitating effective generalization while mitigating noise from excessive\ncontext. Additionally, we plan to study factors that influence the subsequent action selection process.\nWe can further enhance these models' decision-making capabilities in out-of-distribution scenarios\nby understanding how different contextual elements and representation choices impact an agent's\nability to predict appropriate actions based on the retrieved candidates."}, {"title": "Conclusion", "content": "Collectively, these findings emphasize the importance of effective context management and repre-\nsentation for enabling web navigation agents to generalize successfully to unseen domains. Our\nfindings highlight the need to strike a balance for effective context management. While compre-\nhensive contexts that empower a deeper understanding of user intent are advantageous, overly long"}, {"title": "Ethical Statement", "content": "Our work focuses on improving the generalization capabilities of multi-turn web navigation agents\nby optimizing state representation and context management. Our contributions aim to enhance the\nperformance and applicability of these agents in real-world scenarios, but with these advancements\ncome certain ethical considerations.\nOn the positive side, this work has the potential to greatly improve accessibility, particularly for\nindividuals with disabilities, by making web navigation more intuitive and user-friendly. Enhanced\ncontext management and state representation can lead to more efficient digital assistants, automating\nrepetitive and error-prone tasks and allowing knowledge workers to focus on more complex, creative,\nor high-level problem-solving activities. Additionally, the ability of these agents to better understand\nand align with user intent can foster more effective human-AI collaboration in various applications.\nWhile necessary for the proposed training and inference processes (especially with the extended\ninteraction history), collecting and processing large amounts of web navigation data could pose\nprivacy risks. Data collection for applications of this research must adhere to the highest privacy\nstandards and ensure that users are fully informed about how their data is being used. Furthermore,\nas these web navigation agents become more sophisticated, there exists a risk of misuse, such as\nautomating fraudulent activities, spreading misinformation, or conducting unauthorized data scraping.\nWe advocate for ongoing dialogue and collaboration within the AI research community to address\nthese ethical challenges proactively and ensure that advancements in web navigation technology\ncontribute positively to society."}]}