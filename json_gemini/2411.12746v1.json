{"title": "A Review of Reinforcement Learning in Financial Applications", "authors": ["Yahui Bai", "Yuhe Gao", "Runzhe Wan", "Sheng Zhang", "Rui Song"], "abstract": "In recent years, there has been a growing trend of applying Reinforcement Learning (RL) in financial applications. This approach has shown great potential to solve decision-making tasks in finance. In this survey, we present a comprehensive study of the applications of RL in finance and conduct a series of meta-analyses to investigate the common themes in the literature, such as the factors that most significantly affect RL's performance compared to traditional methods. Moreover, we identify challenges including explainability, Markov Decision Process (MDP) modeling, and robustness that hinder the broader utilization of RL in the financial industry and discuss recent advancements in overcoming these challenges. Finally, we propose future research directions, such as benchmarking, contextual RL, multi-agent RL, and model-based RL to address these challenges and to further enhance the implementation of RL in finance.", "sections": [{"title": "1. Introduction", "content": "A financial market is a marketplace where financial instruments such as stocks and bonds are bought and sold (Fama 1970). Individuals and organizations can play crucial roles in financial markets to facilitate the allocation of capital. Market participants face diverse challenges, such as portfolio management, which aims to maximize investment returns over time, and market-making, which seeks to profit from the bid-ask spread while managing inventory risk. As the volume of financial data has increased dramatically over time, new opportunities and challenges have arisen in the analysis process, leading to the increased adoption of advanced Machine Learning (ML) models.\nReinforcement Learning (RL) (Sutton & Barto 2018), as one of the main categories of ML, has revolutionized the field of artificial intelligence by empowering agents to interact with the environment and allowing them to learn and improve their performance. The success of RL has been demonstrated in various fields, including games, robots, mobile health (Nash Jr 1950, Kalman 1960, Murphy 2003), etc. In finance, applications such as market making, portfolio management, and order execution can benefit from the ability of RL algorithms to learn and adapt to changing environments. Compared to traditional models that rely on statistical techniques and econometric methods such as time series models (ARMA, ARIMA), factor models, and panel models, the RL framework empowers agents to learn decision-making by interacting with an environment and deducing the consequences of past actions to maximize cumulative rewards (Charpentier et al. 2021). RL also facilitates online learning, allowing iterative refinement of investment strategies with new information. Moreover, through its deep network structure, RL models can better capture complex patterns in highly nonlinear and non-stationary financial data. These advantages give RL the potential to enhance both the efficiency and effectiveness of financial applications."}, {"title": "1.1. Related Work", "content": "Several papers have reviewed the use of RL in finance. Fischer (2018) and Pricope (2021) categorize model-free RL approaches into critic-only, actor-only, and actor-critic, and discuss various RL settings. Specifically, Fischer (2018) summarized the challenges in financial data and suggested the actor-only approach may be the best-suited for RL in financial markets. Pricope (2021) identifies limitations in current studies on deep reinforcement learning (DRL) in quantitative algorithmic trading and suggests that more research is needed to determine its ability to outperform human traders. Charpentier et al. (2021) and Huang et al. (2020) delve deep into the realm of RL applications, with a primary focus on their widespread use in economics, operations research, and banking. Other works focus on application in subdomain of finance. Hambly et al. (2023) conducts a survey of recent studies in RL applied to finance, concentrating on critical topics like optimal execution and portfolio optimization, while Ga\u0161perov et al. (2021) focuses on the review of RL in market making. Meng & Khushi (2019) diligently reviews research papers related to trading and forecasting in stock and foreign currency markets, shedding light on the prevalence of unrealistic assumptions within many of these studies, and Millea (2021) reviews recent developments in DRL for trading in the cryptocurrency market. Mosavi et al. (2020) compares ML and DRL methods in different financial applications, suggesting DRL may outperform traditional approaches.\nTraditionally, RL methods have been applied to healthcare, such as precision medicine (Chakraborty & Murphy 2014, Kosorok & Laber 2019). Key differences between RL in finance and medical research include: many RL agents in precision medicine are trained offline using observational or clinical trial data, while most RL policies in finance are trained online with simulators or real market data; traditional precision medicine settings typically involves finite horizons, whereas RL in financial markets often deals with infinite horizon decision making; financial environments can involve multiple agents, such as in market making, while precision medicine usually involves a single RL agent. It is worth to mention that, RL applications in mobile health devices are similar to those in finance, as both involve online RL with infinite decision stages."}, {"title": "1.2. Contribution", "content": "In this paper, we conduct a comprehensive survey of RL methods in the financial domain.\nOur contributions are summarized as follows.\n\u2022 We provide a few taxonomies to establish a unified view of the recent literature about"}, {"title": "2. Preliminary", "content": "In our financial literature survey, we introduce RL algorithms, categorized by frequency in Figure 1 into model-free and model-based algorithms. In model-free RL algorithms, the agent learns to make decisions without a model of the environment's dynamics, while in model-based RL algorithms, the environment's dynamics is modeled.\nThere are three categories of model-free RL algorithms, including actor-only, critic-only, and actor-critic methods. Actor-only algorithms update only the policy, or the actor, while keeping the value function fixed. One popular actor-only algorithm is called the Policy Gradient (PG) (Sutton et al. 1999), which directly optimizes the policy parameters to maximize the expected cumulative reward. Another commonly used actor-only algorithm is Proximal Policy Optimization (PPO). One of the key features of PPO is that it employs a \"proximal\" update rule, which limits the size of the policy update at each iteration. Neuroevolution (Such et al. 2017) differs from the aforementioned algorithms as it adapts a gradient-free genetic algorithm to optimize the policy.\nCritic-only methods focus solely on learning the value function, without explicitly learning a policy function. The actions are obtained based on the approximate value function (Sutton & Barto 2018). Q-learning estimates the optimal action-value function, which represents the expected total reward for taking a given action in a given state and following the optimal policy thereafter (Watkins & Dayan 1992). Deep Q-Network (DQN) (Mnih et al. 2013) uses a deep neural network to estimate the Q-value function. Another type of critic-only algorithm is called SARSA (Sutton & Barto 2018), which updates the Q-value function using the observed reward and the next state-action pair. Additionally, R-learning (Schwartz 1993) is also value-based, except that its objective is to maximize the average reward rather than the discounted reward in Q-learning and SARSA.\nActor-critic methods combine the benefits of both policy-based (actor) and value-based (critic) approaches. These methods leverage two separate components to learn both the policy and value functions, with the actor generating actions based on the learned policy and the critic network evaluating the quality of the actions taken by the actor. Advantage Actor-Critic (A2C) and Asynchronous Advantage Actor-Critic (A3C) are popular and effective actor-critic methods with deep learning techniques. A2C proposed by Mnih et al. (2016) combines the actor-critic algorithm with parallelization techniques to improve the learning speed and stability. As an extension of A2C, A3C employs asynchronous updating of the actor-critic model by running multiple threads in parallel. Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al. 2015) combines ideas from the deterministic policy gradient algorithm and the DQN algorithm, which is particularly effective in continuous action spaces. The NAC-S(\u03b3) algorithm, proposed in Thomas (2014), is a natural actor-critic method that uses semi-gradient SARSA for policy evaluation and is specifically designed for stochastic policies.\nModel-based methods involve learning a model of the environment's dynamics and using this model to simulate the future to select actions. In financial applications like market making and optimal execution, the model-based RL approach entails constructing and leveraging a simulator that mimics the dynamics of the financial market. This simulator acts as a predictive model, providing agents with valuable insights about how the market may behave under different conditions and scenarios. By employing this model as a proxy of the real market, agents can execute a multitude of trades and test various trading strategies. While these methods can be computationally intensive, they are shown to produce better long-term behavior when the environment is complex and uncertain (Sutton & Barto 2018)."}, {"title": "3. Applications of RL in Finance", "content": "We first present a taxonomy of the state, action, reward, and algorithm used in the surveyed papers, to provide a unified view of the literature in Table 1 and 2 in Appendix A. We organize the state, action and reward variables into several categories: (1) The state variables include the price information (historical prices, returns, change of price, etc.), inventory information (the on-hand units for one stock or every stock in the portfolio), technical indicators (Relative Strength Index, Moving Average Convergence Divergence, etc., denoted as \"tech-indicators in Table 1\"), market indicators (Citigroup Economic Surprise Index, etc.), company information (P/E ratio, dividend, market capitalization, etc.), predictive representations of state, trading volume (including volume of past trades, change of volume, market share of different agents, etc.), bid-ask information (bid-ask spread, imbalance of bid-ask order in the limit order book, etc.), volatility, time index, and end-to-end information (E2E, where we directly use a large number of observable variables). (2) The action variables are summarized as long/short/hold, the target portfolio weight, the bid price, and the trading quantity. (3) The reward variables are summarized as slippage, implementation shortfall (IS), PnL (including cash income and outflow, and some other customized function of profit), return, volatility-adjusted return (VR, which includes the Sharp ratio and other variants), inventory penalty, market quality (containing bid-ask spread, some measures of price change, etc, as discussed in Chan & Shelton (2001)), and various shaped rewards. Here, a shaped reward (Lin & Beling 2020a,b) refers to a reward definition that is not the same as any existing metrics but instead has a relatively complicated (but carefully designed) structure to achieve a special goal."}, {"title": "3.2. Applications in Market Making", "content": "Market makers increase the liquidity of the market by continuously offering buy and sell orders. In general, market makers profit from order execution (i.e., the spread profit) as well as inventory speculation resulting from changes in the market prices of the assets they hold. However, maintaining a non-zero inventory also exposes market makers to the inventory risk, which is the potential risk of adverse price change in the market (O'hara & Oldfield 1986). Thus, market making algorithms aim at maximizing profit while controlling this inventory risk. RL methods have been introduced to market making since 2001 (see Chan & Shelton 2001), with most works emerging in the last five years (post-2018). Most researchers use a single RL agent to handle the problem of market making while incorporating techniques to mitigate various risks, a representative selection of which is reviewed in Section 3.2.1. Alternatively, some works investigate a multi-agent framework to achieve various objectives, such as increasing the robustness of the RL agent from some adversaries or deriving a nearly optimal strategy in the presence of competing market makers. An in-depth review of these approaches is available in Section 3.2.2."}, {"title": "3.2.1. Single-agent RL in Market Making", "content": "The application of Reinforcement Learning (RL) in market making is first introduced by (Chan & Shelton 2001). This pioneering work integrates the problem of market making into the RL framework, considering both the agent's information (such as inventory) and market characteristics (like the imbalance of buy and sell orders, bid-ask spread, and price changes between trades) as state variables. The RL agent's actions range from setting bid/ask prices, setting bid/ask order sizes, and executing buy/sell orders. The rewards of RL agents come from profit maximization, inventory minimization, and improvement of market quality. In the simulation model of (Chan & Shelton 2001), a single security is traded and state variables are influenced by the actions of the market maker, informed traders (with inside information distribution), and uninformed traders.\nSubsequent research focuses on mitigating inventory and other risks for the RL market maker agent in various settings. Mani et al. (2019) introduces a novel approach using the double SARSA algorithm and a customized temporal difference update for the Q-function. This approach leads to variance reduction in rewards, enabling the agent to achieve higher profits while maintaining lower inventory levels compared to the standard SARSA policy from (Chan & Shelton 2001). Another line of research involves modifying the reward function to manage risks. For instance, Spooner et al. (2018) designs an asymmetrically dampened reward function that encourages the agent to receive rewards by providing more liquidity rather than holding a high inventory, thereby reducing the inventory risk. Selser et al. (2021) proposes to modify the reward function by incorporating the estimated variance of the agent's wealth as a penalty term. In the simulation results of Selser et al. (2021), it is demonstrated that a DQN model with this reward function can outperform a non-RL baseline that solves partial differential equations (Avellaneda & Stoikov 2008). Gu\u00e9ant & Manziuk (2019) extends this risk-adverse reward function strategy to a multi-asset market making setting. They propose an actor-critic algorithm to approximate the optimal quote for multiple bonds. Their RL model includes a penalty term for the portfolio's PnL variance in the reward function, and this approach is proved to be scalable in high-dimensional cases involving up to 100 bonds a task challenging for non-RL methods."}, {"title": "3.2.2. Multi-agent RL in Market Making", "content": "The multi-agent RL framework is an extension to the single-agent RL framework for market making, and the research directions can be divided into three branches. The first branch introduces adversarial agents in simulations, such as these agents profit from the losses of RL market-making agents. This approach enhances the RL agent's robustness to market conditions or increases risk aversion. For example, Spooner & Savani (2020) explore an adversarial RL setting in which an adversarial agent during the RL agent's training alters the environmental parameters in simulators. This discrepancy in perceived versus actual market conditions trains the market maker to be more robust against market condition misspecifications. Extending this idea, Ga\u0161perov & Kostanj\u010dar (2021) introduces an adversary that directly disrupts the actions of the market making agent by displacing quotes. This technique has shown to reduce inventory risk and improve profits compared to training the RL agent without an adversary.\nThe second branch focuses on competition between market makers, where a professional market making agent competes with the RL agent. Ganesh et al. (2019) first demonstrate theoretically that the RL market making agent's optimal pricing policy to maximize spread PnL depends on its competitors' pricing distributions. Ganesh et al. (2019) show in their experiments that the RL agent can learn the pricing distribution of competing market makers from pricing and trading observations and derive a nearly optimal strategy accordingly, despite that the parameters of the competitor agents are unknown to the RL agent. In the third branch, the focus is mainly on adapting the original market making decision process into a hierarchical one. Patel (2018) transform the RL framework by introducing a macro-agent that determines the overall trading strategy (buy, sell, or hold the security), while a micro-agent sets specific quote prices based on the macro-agent's direction. This hierarchical approach results in less volatile profits, as shown in their simulations."}, {"title": "3.3. Applications in Portfolio Management", "content": "In portfolio management, the agent aims to distribute a fixed sum across diverse assets, aiming to maximize the portfolio's expected return while prudently managing risk. Although classical methods such as the Markowitz model (Markowitz 1952) have been proven to be effective in the past, they rely on a static approach that may not be suitable for a rapidly changing market. Hence, introducing a dynamic portfolio optimization algorithm capable of adapting to changing market conditions is crucial for better risk management and return improvement. We review bandit algorithms in Section 3.3.1 and deep RL algorithms in Section 3.3.2."}, {"title": "3.3.1. Bandit Algorithms for Portfolio Management", "content": "The bandit problem (Lattimore & Szepesv\u00e1ri 2020) is a special case of RL, in that there does not exist a state transition and, hence, there is no long-term dependency. Bandits algorithms have been studied in a few papers on portfolio optimization application (Shen et al. 2015, Shen & Wang 2016, Zhu et al. 2019, Huo & Fu 2017), where the investor dynamically updates the portfolio based on historical observations.\nShen et al. (2015), Shen & Wang (2016), Zhu et al. (2019) all adopt a standard bandits algorithm named Thompson Sampling (TS) (Russo et al. 2018) in a non-contextual multi-armed bandit setting, with the major difference in how to define the arms. Shen et al. (2015) is the first work on bandits for portfolio optimization. This work focuses on the challenge that it is not appropriate to regard stocks as independent arms (due to the correlation between them). Towards risk diversification, the authors propose to integrate bandits with a popular practice in portfolio optimization, that is, first applying principal component analysis to the stock returns and then regard the eigenvectors as arms. Therefore, the paper addresses the limitation of a naive application of bandit algorithms in portfolio optimization and provides a nice integration with finance theory, which is further supported by the performance of the backtest when compared to several other online portfolio optimization algorithms. Shen & Wang (2016) similarly apply TS to portfolio optimization. The article studies portfolio blending, i.e., the authors regard the portfolios given by different selection strategies as arms and aim to find the best combinations of these strategies. The algorithm in Shen & Wang (2016) is designed to mix two base strategies, which is further extended in Zhu et al. (2019) to allow mixing multiple base strategies. Superior performance over several online portfolio optimization algorithms as well as the base strategies is reported in backtesting.\nThe aforementioned works focus on maximizing the cumulative expected return while ignoring another important metric in finance, risk. To fill this gap, Huo & Fu (2017) propose to consider a linear combination between a stock selected using a bandit algorithm and another portfolio solved by (greedy) online portfolio optimization. The former targets the expected return (in the long run) and the latter is selected by minimizing the risk. By choosing the weights of the two parts carefully and manually, the paper aims to achieve a balance between the expected return and the risk.\nIn closing, we raise a fundamental question: is the bandit problem ideal for portfolio optimization, in other words, is active exploration necessary in this application? As highlighted by Russo et al. (2018) (see Section 8.2.1 therein), this question warrants further discussions, which we delve into in Section 6."}, {"title": "3.3.2. RL Algorithms for Portfolio Management", "content": "In contrast to bandit algorithms, which focus solely on immediate rewards, RL algorithms approach the sequential portfolio management problem by formulating it as a Markov Decision Process (MDP). By doing so, these algorithms take into account the consequences of current portfolio decisions on future states, making them more comprehensive and forward-looking. This sub-section will review papers that use the RL algorithms for training trading agents in portfolio management.\nTo begin with, Jiang et al. (2017) propose Ensemble of Identical Independent Evaluators (EIIE), which is an ensemble of neural networks with history price as input, to solve the asset allocation problems in the cryptocurrency market. The study explores three backbone networks (RNN, LSTM, and CNN) as RL policy networks and demonstrates that RL models can outperform traditional approaches, indicating their potential for portfolio management. Subsequent studies in RL algorithm trading agents consider this paper as a benchmark."}, {"title": "3.3.2.1. Complicated Policy Networks", "content": "Liu et al. (2018) builds upon Jiang et al. (2017), by adopting DDPG over PG algorithms for trading agents optimization, resulting in remarkable performance improvement. This paper sheds light on how deep policy networks can improve the RL agents for PM tasks. More complicated policy networks (such as DQN, hierarchical agents, and parallel agents) are explored in other studies. For example, Gao et al. (2020) utilizes DQN for portfolio management with discrete action space, where Duel Q-Net (Wang et al. 2015) is used as the backbone framework of the DQN. To improve sampling efficiency, this paper also employs Prioritized Experience Replay with the SumTree structure (Schaul et al. 2015) to prioritize valuable experiences over noise during training. Wang et al. (2021a) propose a Hierarchical Reinforced Portfolio Management (HRPM) approach. Specifically, the hierarchical structure includes two agents: a high-level RL agent maximizing long-term profits, and a low-level RL agent minimizing trading costs while achieving the wealth redistribution targets set by the high-level model within a limited time window. Similarly, Ma et al. (2021) propose an approach with two parallel agents: one capturing current asset information and the other using LSTM layers to detect long-term market trends. More papers (Wang et al. 2019, Lee et al. 2020) adopt multi-agents policy networks, which we will discuss further."}, {"title": "3.3.2.2. Empirical financial strategy oriented methods", "content": "Some studies have recognized the value of leveraging pre-existing domain knowledge to enhance the efficiency and effectiveness of RL models. Traditional financial strategies, like buying winners and selling losers (BWSL) (Jegadeesh & Titman 1993), Rescorla-Wanger model (Rescorla 1972), Modern Portfolio Theory (MPT) (Menezes & Hanson 1970, Pratt 1978) and Dual Thrust strategy are employed in work below.\nAlphaStock (Wang et al. 2019) adopts the BWSL strategy, buying assets with high price rising rates and selling those with low rate. The paper designs two optimizing agents: a long agent purchasing selected winner assets and a short agent selling selected loser assets, optimizing for the Sharpe ratio using a cross-assets attention network. Sensitivity analysis on stock feature shows that AlphaStock tends to select stocks with high long-term growth, low volatility, high intrinsic value, and being undervalued. Li et al. (2019) notes that a bear market, marked by pessimism, aligns with economic downturns, while a bull market, marked by optimism, occurs when security prices outpace interest rates. This paper incorporates market sentiment by modifying the Rescorla-Wanger model to adapt differently under positive and negative environments. Modern Portfolio Theory (MPT) (Menezes & Hanson 1970, Pratt 1978) is widely used to construct optimal investment portfolios by balancing the trade-off between risk and return. Zhang et al. (2020) construct the reward function of RL framework by leveraging MPT, where it shows that the constructed reward function is equivalent to the MPT's utility function subject to a proposed condition. Another strategy, Michael Chalek's Dual, is an investment approach combining the true range (a measure of volatility) with the dual thrust model, which sets Buy and Sell Thresholds. Specifically, a buy signal is generated when the market price exceeds the Buy Threshold, and a sell signal is triggered when it falls below the Sell Threshold. Liu et al. (2020b) utilize the Dual Thrust strategy to initiate the demonstration buffer, enhancing the deterministic policy gradient method to balance exploration and exploitation effectively."}, {"title": "3.3.2.3. Methods aimed at enhancing robustness", "content": "The volatility of the financial market presents significant challenges when applying RL to real-world investment processes. To ensure consistent performance, some studies prioritize enhancing robustness, i.e. the stability of the model's performance under different market conditions, as a key objective,. More discussion on the challenges of robustness can be found in Sec 6.2.3.\nYang et al. (2020) propose an ensemble strategy that can dynamically select from three RL algorithms (PPO, A2C, and DDPG) based on market indicators. Experimental results in their paper show that this approach effectively preserves robustness under different market conditions. Lee et al. (2020) also focus on dealing with the continuously changing market conditions. Multiple agents are designed, each agent aims to optimize their portfolio while considering the potential negative impact on the overall system's risk-adjusted return if portfolios are similar. Another robustness-related RL method is DeepPocket (Soleymani & Paquet 2021). In DeepPocket, the interaction between assets is represented by a graph with financial assets as nodes, and the correlation function between assets as edges. The graph is encoded as the state in the actor-critic RL algorithm. The paper shows that the DeepPocket can handle unanticipated changes generated by exogenous factors such as COVID-19 in online learning. Benhamou et al. (2021) suggests a multi-network approach that combines financial contextual information with asset states to predict the economy's health, which enables the model to outperform baseline methods expecially during recessions."}, {"title": "3.3.2.4. Market prediction based methods", "content": "An emerging trend observed among many papers is to include predicted contextual information, such as asset price movements, data augmentation, market trend, market sentiment, or economic conditions.\nMarket-prediction-based deep RL in the portfolio management domain was first applied in Yu et al. (2019). This paper implements a prediction module to forecast the next time-step price and a market indicator aims to augment state space. Besides, the authors also propose a behavior cloning module to reduce volatility by preventing a large change in portfolio weights based on imitation learning. Similarly, the data augmentation idea is explored in Th\u00e9ate & Ernst (2021) with DQN policy network. Other than prices, more complicated state augmentation haven been explored. Lei et al. (2020) enhances the RL states by incorporating a prediction-based auto-encoder using the attention mechanism, to capture the market trend. Ye et al. (2020) incorporate the market sentiment feature extracted from relevant news as an external stock movement indicator within the RL framework. Koratamaddi et al. (2021) also take market sentiment into account, by augmenting the state space with a calculated market confidence score derived from company-related news and tweets. DeepTrader (Wang et al. 2021b) is another market-prediction-oriented work. The authors design an asset scoring unit to capture asset-specific growth and a market scoring unit to adjust the long/short proportion based on the overall economic situation, which enables the model to balance risk and return and to adapt to changing market conditions."}, {"title": "3.4. Applications in Optimal Execution", "content": "Optimal execution (OE) is another important problem in finance (Bertsimas & Lo 1998). Compared with portfolio optimization, OE is finer-grained: it focuses on the optimal way to buy or sell some pre-specified units of a single stock in a given time frame. Therefore, OE becomes an indispensable component of the continuous updating of portfolios. Since the naive strategy that places the entire order instantly may have a significant impact on the market and hence profitability, a well-designed algorithm is required.\nIn the literature, various metrics are considered to evaluate OE algorithms, including the PnL Nevmyvaka et al. (2006), Deng et al. (2016), Wei et al. (2019), Shen et al. (2014), a normalized version of the PnL called the implementation shortfall (Lin & Beling 2020b, Hendricks & Wilcox 2014), the Sharpe ratio (Deng et al. 2016) and the shaped reward (Fang et al. 2021, Lin & Beling 2020a).\nModel-free RL. Nevmyvaka et al. (2006) is the first attempt to apply model-free RL to OE. Using a modified Q-learning algorithm, the paper conducts numerical experiments on large-scale NASDAQ market microstructure datasets and reports significant improvements over baselines on trading costs. Lin & Beling (2020a) similarly applies a variant of DQN to OE. The main novelty is to propose a shaped reward structure and incorporate the zero-end inventory constraint into DQN. Backtesting shows the DQN-based approach outperforms a few baselines, and the constraint improves the stability. In Lin & Beling (2020b), the authors further design an end-to-end framework utilizing LSTM to automatically extract features from the time-dependent limit-order book data, demonstrating improvements over previous methods. Similar ideas are studied in Deng et al. (2016), by integrating RNN and policy gradient. In financial problems, The data is typically noisy with imperfect market information. To overcome this bottleneck, Fang et al. (2021) proposes an oracle policy distillation approach: during training, a teacher policy is learned with access to the future price and volume, which is then distilled to learn the student policy, which has no access to future information and is the one to be deployed. Such a procedure reduces the training variance and leads to a better policy in testing, as demonstrated via experiments over a few baselines.\nAs discussed in previous sections, one prominent limitation of standard RL formulations is that the objective ignores the risk, which is of particular importance in OE and is also one of the main concerns of algorithmic trading. To fill the gap, Shen et al. (2014) adopt the risk-sensitive MDP formulation in Shen et al. (2013) to design a Q-learning-type algorithm. Specifically, the expected cumulative reward is replaced by a utility-based shortfall metric proposed in the mathematical finance literature. The backtesting results demonstrate improved robustness, especially during the 2010 flash crash.\nModel-based RL. The papers above rely on model-free RL and may suffer from low sample efficiency. Wei et al. (2019) firstly apply model-based DRL to OE: it first learns an environment model (simulator) using RNN and auto-encoder, and then trains a policy by running model-free DRL (including Double DQN, PG, and A2C) within the simulator. One main contribution of the paper is to show the algorithm's five-day real performance in the real market, where the agent is profitable and the model-predicted trajectory is close to the observed trajectory. In contrast, Hambly et al. (2021) consider a structural model assumption, the linear-quadratic regulator problem. This paper uses the OE problem as an example to demonstrate the robustness of running model-free methods in a model-based environment over directly solving the model-based controller, when there is model misspecification."}, {"title": "4. Meta Analysis of Experimental Results", "content": "In this section", "method.\nQ1": "Does MDP design affect RL performance? We examine the relationship between RL Premium and the design of MDPs", "components.\nState": "We observe a growing trend of integrating supplementary information beyond asset prices into the state space. For instance", "p-value.\nAction": "In multi-asset PM tasks", "regard\u00b2.\nReward": "Designing a suitable reward is essential in RL modeling as it can greatly uplift decision-making efficiency. In portfolio management", "model.\nQ2": "How does the training period affect model performance? The volatility of financial market data highlights the importance of carefully choosing the training period. Our analysis reveals that the length of the training period varies by study, indicating the absence of established standard. We are interested to understand whether longer training period will improve or harm model performance. However, from regression shown in Figure 2d we cannot conclude the relationship between the two. One possible explanation is that financial information is rapidly changing, so a lengthy training period may introduce noise rather than offer useful information for recent decisions.\nIncluding a recession period in training data poses challenges for RL models, as recession can lead to sudden and unpredictable market changes, rendering non-stationary conditions and hindering precise decision-making. However, statistical analysis leveraging a two-sample t-test to evaluate differences in RL premium for periods covering a recession in Figure 2e shows that inclusion of a recession period"}]}