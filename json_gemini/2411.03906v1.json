{"title": "Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System", "authors": ["David Maria Schmidt", "Mohammad Fazleh Elahi", "Philipp Cimiano"], "abstract": "In this paper, we examine the impact of lexicalization on Question Answering over Linked Data (QALD). It is well known that one of the key challenges in interpreting natural language questions with respect to SPARQL lies in bridging the lexical gap, that is mapping the words in the query to the correct vocabulary elements. We argue in this paper that lexicalization, that is explicit knowledge about the potential interpretations of a word with respect to the given vocabulary, significantly eases the task and increases the performance of QA systems. Towards this goal, we present a compositional QA system that can leverage explicit lexical knowledge in a compositional manner to infer the meaning of a question in terms of a SPARQL query. We show that such a system, given lexical knowledge, has a performance well beyond current QA systems, achieving up to a 35.8% increase in the micro F1 score compared to the best QA system on QALD-9. This shows the importance and potential of including explicit lexical knowledge. In contrast, we show that LLMs have limited abilities to exploit lexical knowledge, with only marginal improvements compared to a version without lexical knowledge. This shows that LLMs have no ability to compositionally interpret a question on the basis of the meaning of its parts, a key feature of compositional approaches. Taken together, our work shows new avenues for QALD research, emphasizing the importance of lexicalization and compositionality.", "sections": [{"title": "1 Introduction", "content": "Question Answering over Linked Data (QALD) [63] is the task of automatically mapping a natural language question to an executable SPARQL query such that relevant information can be retrieved from RDF data sources. One of the seven challenges [69] identified by the authors for the development of QALD systems is handling the lexical gap [69], which requires bridging the way users refer to"}, {"title": "2 System Architecture", "content": "In this section, we detail a compositional approach to QALD, using Dependency-based Underspecified Discourse Representation Structures (DUDES) [10,14] for meaning representation and composition behavior, as well as leveraging explicit lexical knowledge, thus answering RQ1. The overall architecture of the pipeline is illustrated in Figure 1 and serves as a blueprint for this section. Although we used DBpedia as a reference, our approach can be adapted to any particular ontology and vocabulary by providing a corresponding lexicon."}, {"title": "2.1 Explicit Lexical Knowledge", "content": "A necessary prerequisite for our approach is the availability of a Lemon lexicon [46] that describes by which lexical entries the elements (classes, properties) of a particular knowledge base (KB) can be verbalized in a particular language. We rely on the Lemon lexicon format that contains lexical entries and defines how their meaning is captured with respect to a given ontological vocabulary. A lexical entry represents a unit of analysis of the lexicon that consists of a set of grammatically related forms and a set of base meanings that are associated with all of these forms. The lexicon is context-free in the sense that the possible meanings of words are described independently from their context."}, {"title": "2.2 Dependency Parsing", "content": "Our approach relies on a syntactic analysis of an input question by a dependency parser. To increase the chance that at least one correct dependency tree is generated, which is vital for our approach, we use multiple dependency parsing frameworks (i.e., SpaCy\u00b3 and Stanza/CoreNLP framework [56]), configurations, and models. Furthermore, some questions contain textual representations of"}, {"title": "2.3 Tree Merger", "content": "Matching nodes in the dependency tree to URIs representing entities and properties (a.k.a. KB Linking) is a central challenge in our approach. For this purpose,"}, {"title": "2.4 Ontology Matcher", "content": "In this step, entities and properties are assigned to their respective tree nodes where possible. The matching methods are described below.\nProperty Matching: This matching method focuses on matching the nodes of the tree with DBpedia properties. First, each node of the tree is matched with the written representations of the lexical entries if possible. If there is no exact match for a node, the approach tries to find candidate lexical entries by applying several heuristics, such as omitting certain tokens from the node, e.g. by excluding trailing adpositions, which typically do not occur in the canonical forms of lexical entries. If the marker of a lexical entry matches with a token from a node, the corresponding candidate entry is prioritized. Finally, if there are remaining ambiguities, the candidate lexical entries are sorted in descending order using a Levenshtein distance-based similarity measure [42].\nEntity Matching: To match tree nodes with entities from DBpedia, the approach uses all available rdfs:label information of entities, which are stored in a prefix trie [40] for efficient memory representation and lookup of similar labels. The similarities between the entity (e.g., \"Angela Merkel\") in the tree and the entity labels in DBpedia are calculated using the Levenshtein similarity measure with a threshold set to 0.5. When both a shorter and a longer text span perfectly match certain labels, the longer match is generally prioritized higher. As an off-the-shelf solution, we also include the entity recognition results of DBpedia Spotlight [47] into the set of considered candidates to increase the chance of a correct match."}, {"title": "2.5 Tree Scorer", "content": "Each node of the tree is assigned a score based on matched properties or entities. Additionally, it considers the total number of tree nodes, as well as special terms and keywords which are neither properties nor entities. Two types of matching are taken into account: (i) exact matches, and (ii) matches under relaxed conditions. The scoring relies on a weighted average of three different scores: (i) fraction of nodes with exact matches (weight 3), (ii) fraction of nodes with matches under relaxed conditions (weight 1), and (iii) ratio of the number of nodes to the number of nodes in the dependency tree before merging nodes (weight 2). For (i) and (ii), single node weights (i.e., the number of tokens a node comprises) are multiplied with different multipliers based on whether the node has a matching lexical entry (multiplier 1.0), a matching entity or is a numeral (0.9) or is a special word like an ASK keyword, a comparative or \u201cin\u201d (0.8). Then, the weight is multiplied with that multiplier and added to a total sum. In the end, this sum is compared to the sum of all weights, forming the score value. The weighted average of these three scores forms the total score of a tree, according to which the trees are then prioritized in processing."}, {"title": "2.6 DUDES Creator", "content": "Now that we have a tree with KB elements (e.g., entities and properties) assigned to the nodes, the next task is to create Dependency-based Underspecified Discourse Representation Structures (DUDES) [10,14], which are used to compose the atomic meanings of the tree nodes. Our approach is slightly different from the latest version of DUDES [14] as we modify it for use with dependency trees instead of Lexicalized Tree Adjoining Grammar (LTAG) trees [36,62] and do not make use of subordination relations yet:\nDefinition 1 (Dependency-based Underspecified Discourse Representation Structures (DUDES) [14]). A DUDES is a triple $(v, D, S)$ where:\n$v \\in U \\cup \\{e\\}$ is the main variable (also called referent marker or distinguished variable) where e represents the absence of a main variable\n$D = (U,C)$ is a Discourse Representation Structure (DRS) [14, 33, 37] with\n\u2022 set of variables $U$ (also called discourse universe or referent markers)\n\u2022 set of conditions $C$ over variables $U$\n$S$ is a set of selection pairs of the form $(v \\in U,m)$ with $v$ being a variable from $U$ and $m$ being a marker word for that variable with $e$ representing the empty marker, i.e. no marker being connected to that variable. Instead of writing $e$, the second tuple component can also just be left out.\nEntity DUDES: The simplest case of representing KB elements from the tree as a DUDES is representing entities (i.e., Entity DUDES). In Entity DUDES, an entity is assigned to a variable, for example, by adding a simple expression such as $z = dbr: Angela_Merkel$. A full example for entity dbr: Angela_Merkel is illustrated in Figure 2a."}, {"title": "2.7 DUDES Composer", "content": "The composition operation of DUDES [14] can be defined as follows:\nDefinition 2 (DUDES Composition). Let $d_1 = (v_1, D_1 = (U_1,C_1), S_1)$, $d_2 = (v_2, D_2 = (U_2, C_2), S_2)$ be two DUDES with disjoint variable sets, i.e. $U_1 \\cap U_2 = \\emptyset$. The DUDES composition operation for substituting $d_1$ into $d_2$ using selection pair $p = (x \\in U_2,m) \\in S_2$ and resulting in a composed DUDES $d_c = (v_c, D_c = (U_c, C_c), S_c)$, written $d_c = d_1 \\rhd d_2$, is defined as follows:\n$U_c = U_2[x := v_1] \\cup U_1$\n$C_c = C_2[x := v_1] \\cup C_1$\n$S_c = (S_2 \\cup S_1) \\setminus p$\n$v_c = \\begin{cases}U_1 & \\text{if } x = v_2\\\\U_2 & \\text{else}\\end{cases}$\nAn example composition of the two DUDES (i.e., Figure 2a and 2b) is shown in Figure 2c. We apply a bottom-up composition strategy, merging child nodes into their parent nodes. DUDES compositions are performed until there is only a single composed DUDES (i.e., final DUDES) left at the root of the tree, representing the meaning of the whole question. For choosing a selection pair for composition, different heuristics and data sources are used. For example, in the case of modifier nodes, the parent DUDES is merged into the child DUDES and not the other way around. Additionally, the syntactic frames of the lexical entries, POS and dependency tags are used for selection pair determination."}, {"title": "2.8 SPARQL Generator", "content": "The logical expressions of the final DUDES represent the meaning of the given natural language question. Therefore, they are used to create a corresponding SPARQL query. For instance, the DUDES in Figure 2c shows the triple pattern (i.e., dbr: Angela_Merkel dbo:birthName ?y) for the question \u201cWhat is Angela Merkel's birth name?\u201d. The triple patterns of DUDES contain entities (or literals), and variables. Our approach uses the Z3 SMT solver [49] to determine which variables in the final DUDES are bound to some values. Due to our combinatorial approach to dealing with ambiguities, multiple SPARQL queries are typically generated, from which one is selected by the SPARQL selector."}, {"title": "2.9 SPARQL Selector", "content": "For selecting the best SPARQL query, we use an LLM-based approach [58] trained to compare two queries, using the encoder of flan-t5-small [9] as a base model. As single LLM-based comparison results are still unreliable, various aggregation strategies are evaluated which make a final decision based on the pairwise comparisons of all candidate queries.\nFor each of the two candidate queries of a comparison, the model is given the input question, the candidate query, the number of its results, and the final DUDES. From this information, two output features are generated, representing the confidence in the respective queries. In order to reflect in the output how much better one query is than another, the model is trained to predict the $F_1$ scores of the respective queries. We evaluate different strategies and configurations to select the final query:\nBestScore: Theoretically possible performance of our approach, selecting best queries based on their true $F_1$ score, clamped like in training data.\nMost Winstop n: Compares candidate queries pairwise per question and selects the query that \u201cwins\u201d the most comparisons (with margin of > p%).\nAccum top nlogits/sigmoid: For each candidate query, the model outputs are accumulated, either logits or sigmoid values, largest value is chosen.\nIf an exponent top n is given, the top n models (based on training micro $F_1$ score) are evaluated with their outputs summed together. Otherwise, single models are evaluated and presented with mean and standard deviation. Additionally, queries with no results or with too many results (threshold is the largest number of results for a train question + 10%) are discarded."}, {"title": "3 Experimental Setup", "content": "The experiments were conducted using the well-known QALD-9 benchmark [73]. The dataset contains questions in multiple languages, along with corresponding SPARQL queries and answers from DBpedia. For the experiment, we followed Unger et al. [76] to manually create a lexicon covering the vocabulary elements in the training and test section of QALD-9. In particular, we created a total of 599 lexical entries for five syntactic frames [12]: 311 lexical entries for NounPPFrame, 96 lexical entries for TransitiveFrame, 143 lexical entries for InTransitivePPFrame, 28 lexical entries for AdjectivePredicateFrame, and 21 lexical entries for AdjectiveSuperlativeFrame. The time required for creating a lexical entry was approximately 2-5 minutes depending on the syntactic frame. The total time required to create our lexicon was approximately 16 hours. All experiments10 were conducted for the English part of QALD-9 only."}, {"title": "3.1 SPARQL Selection Model Training", "content": "In Section 2.8, we presented an LLM-based SPARQL selection approach for disambiguation of the generated SPARQL queries. In order to have training, validation and test data for the query selection model, we ran our approach for about 24h on the QALD-9 benchmark and saved all generated candidate queries, separated by train and test questions. Afterwards, the training data was randomly split into 90% training questions and 10% validation questions.\nThe corresponding final data elements were generated in three steps for each part of the data (i.e., training, validation, and test). Each step involved generating 100 comparisons for each question and used for training in a symmetric way to avoid some general preference of the model for the first or second query. Comparisons between queries with (i) an $F_1$ score \u2265 0.01 and (ii) an $F_1$ score < 0.01 were added to the training data. Additionally, mixed comparisons with one query with an $F_1$ score \u2265 0.01 and one with an $F_1$ score < 0.01 were added.\nIn order to fine-tune the google/flan-t5-small model from the Huggingface transformers library [78], we first ran a hyperparameter optimization with 34 trials using Optuna [1], with an epoch search space between 1 and 5, an initial learning rate between le-5 and 1e-4 (logarithmic scale) and using a lambda learning rate scheduler with a lambda between 0.9 and 1.0 (logarithmic scale). As an optimizer, we used Adam [39] and trained 10 models using the parameters of the trial with the lowest validation loss discovered during the hyperparameter optimization. Each training was performed on a single Nvidia A40 with a batch size of 64. These 10 models were then used for evaluation."}, {"title": "3.2 Experiments with GPT", "content": "We compared different GPT [54] models to our compositional approach. The previous research on QALD with GPT-3 [24] evaluated using the QALD-9 test dataset in three modes: zero-shot, few-shot, and fine-tuned model. In the zero-shot scenario, GPT-3 generated many invalid queries. Performance increased with the five-shot approach and even more with fine-tuning.\nAll of our experiments with GPT models are performed separately with 5 different prompts describing the task. The first prompt below has been hand-crafted. Afterwards, four additional prompts have been generated using ChatGPT using the prompt You are a world-class prompt engineer. Refine this prompt: <initial prompt>. The resulting prompts used in our experiments are therefore:\nFor our experiments we, prompted GPT-3.5-Turbo and GPT-4 in a zero-shot fashion and evaluated them on the entire training and test datasets of QALD-9. For the fine-tuned GPT-3.5-Turbo-0125 models, 10% of the original training dataset has been excluded and used as a validation dataset for fine-tuning. All experiments are executed with temperature 0 as well as both with and without lexical information in the prompt, i.e. lexical information was also present during fine-tuning. To the best of our knowledge, no work has investigated the impact of using lexical information (which is crucial for state-of-art performance for QALD) on the benchmark performance in this way yet. For the experiments with lexical information (as detailed in 2.1), we shorten the structure of lexical entries (the structure is detailed in previous work [5]) for prompting and training, as the lexical entries are not well-suited for direct usage. 11 This shortened representation consists primarily of pairs of field names and their values, e.g. \"Canonical form: birth name\" or \"Reference: dbp:birthName\". To fit into the context window of the used models, we restrict the entries appended to the prompt to entries which are relevant to the question. 12 Therefore, only the ability of GPT models to put the pieces together is tested, not whether they select the right entry from a much larger lexicon. The comparison is therefore not fair as our approach figures out the relevant lexical entries itself. The numbers presented in the evaluation section are therefore to be interpreted as \"upper"}, {"title": "4 Evaluation", "content": "The evaluation of our approach is presented in three categories: Single Model, Multi Model, and Upper Bounds. The first category shows mean and standard deviation across the 10 trained models for SPARQL selection strategies using a single model. As these results show a high standard deviation for micro scores, we also evaluated the effect of bundling the outputs of multiple models. For bundling the model outputs (i.e., top n models), we focused on the strategies and models performing best on the training dataset of QALD-9. We also included BestScore strategies in the Upper Bounds category, demonstrating the highest achievable scores (i.e., upper bounds on the query selection model performances). Good scores in the BestScore/Upper Bounds category therefore indicate that the pipeline in principle generates the correct results, but those queries are not always identified during query selection. However, selecting the best query can be considered a much easier task than generating it from scratch, rendering these scores still reasonably realistic. Nevertheless, when comparing to other approaches, only the best performances achieved by a regular query selection strategy are used for fairness reasons.\nDuring evaluation, all strategies with all 10 SPARQL selection models were tested simultaneously to ensure they were evaluated on the same generated queries. However, as we limited the elapsed real time to 3 hours, this imposed a high overhead w.r.t. single strategies. Evaluating just one strategy and model at once would likely have achieved better results due to more tested candidates. Illustrating the theoretical potential of the generated queries, a second evaluation with just BestScore being executed for 3 hours was conducted (marked with \u201c(single)\u201d in Table 1), generating 815473 instead of 10552 queries and increasing scores from 0.37 to 0.51 (macro F1)."}, {"title": "5 Related Work", "content": "Some QALD systems (such as ORAKEL [13], Pythia [68], QueGG [5, 20, 22], and LexExMachinaQA [21]) use Lemon lexica for lexicalization. For instance,"}, {"title": "6 Conclusion and Future Work", "content": "We have investigated the role and impact of explicitly given lexical knowledge in the context of QALD systems. We have presented a novel compositional system that uses this knowledge and demonstrated that it achieves performances in terms of micro F\u2081 scores well beyond the current state-of-the-art. In fact, our approach achieves a micro F\u2081 score of 0.72, which is 0.19 higher than the performance of the best state-of-the-art system on QALD (0.53). In this regard, our work has to be understood as providing a proof of concept that shows the impact of lexical knowledge and of a compositional approach.\nOur approach handles complex queries using DUDES for semantic composition, combined with a tree merging and scoring component and a SPARQL selector, thereby covering a wide variety of complex questions. All we need is a lexicon in Lemon format. At the same time, our results show that LLMs are very limited in their ability to compose, as they cannot leverage provided lexical knowledge to the same extent as our proposed approach. Overall, our results suggest new avenues for QALD research by highlighting the role of explicit lexical knowledge and compositionality. However, there are also limitations.\nFirst, a necessary prerequisite for our approach is the availability of a Lemon lexicon [46], which is manually created and takes approximately 16 hours to produce for 599 lexical entries. Therefore, future work will focus on automating this process using one of the approaches, such as LexExMachina [23] and M-ATOLL [75], which automatically create a lexicon for the QA system. Another limitation is combinatorial explosion, i.e., the exponential growth of combinations when multiple candidate DUDES exist across multiple nodes of a tree, which increases response times considerably.\nWe provided a promising direction of QALD for future work consisting of the development of a hybrid system that combines the benefits of a compositional approach with the generalization abilities of large language models to bridge the lexical gap while leaving composition to a symbolic approach."}]}