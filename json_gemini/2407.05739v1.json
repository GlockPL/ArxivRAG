{"title": "Multi-Bit Mechanism: A Novel Information Transmission Paradigm for Spiking Neural Networks", "authors": ["Yongjun Xiao", "Xianlong Tian", "Yongqi Ding", "Pei He", "Mengmeng Jing", "Lin Zuo"], "abstract": "Since proposed, spiking neural networks (SNNs) gain recognition for their high performance, low power consumption and enhanced biological interpretability. However, while bringing these advantages, the binary nature of spikes also leads to considerable information loss in SNNs, ultimately causing performance degradation. We claim that the limited expressiveness of current binary spikes, resulting in substantial information loss, is the fundamental issue behind these challenges. To alleviate this, our research introduces a multi-bit information transmission mechanism for SNNs. This mechanism expands the output of spiking neurons from the original single bit to multiple bits, enhancing the expressiveness of the spikes and reducing information loss during the forward process, while still maintaining the low energy consumption advantage of SNNs. For SNNs, this represents a new paradigm of information transmission. Moreover, to further utilize the limited spikes, we extract effective signals from the previous layer to re-stimulate the neurons, thus encouraging full spikes emission across various bit levels. We conducted extensive experiments with our proposed method using both direct training method and ANN-SNN conversion method, and the results show consistent performance improvements.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning represented by artificial neural networks (ANNs) has been widely applied in many downstream tasks such as computer vision [1] and game playing [2], etc. However, these achievements are mostly built upon high computational budgets, which limits the application of ANNs on edge computing scenarios.\nAs an alternative, spiking neural networks (SNNs) are proposed. SNNs transmit signals using discrete spikes with temporal characteristics, which is close to the information transmission mechanism of biological neurons, thus being regarded as the third generation of neural networks [3]. Moreover, the sparse nature of the spikes makes the feature matrix sparse, reducing computational load, while their binary characteristics which transforms the multiply-and-accumulate (MAC) operations calculations into accumulate (AC) operations [4], simplifying the computation. All these, endows SNNs with high performance and low power consumption characteristics [5].\nHowever, current SNNs also face challenges. Firstly, the forward process involves the conversion from full-precision membrane potentials to binary spikes, leading to significant information loss. Secondly, the network structures of existing SNNs mostly consist of simple modules where information is transmitted layer by layer, greatly reducing the network's expressive power.\nWe claim that the roots of these two problems lie in the characteristics of the neuron structure and network architecture of SNNs, respectively. On one hand, in terms of neuron structure, existing SNNs,\nPreprint. Under review."}, {"title": "2 Related work", "content": "2.1 Training method of SNNs\nThe training algorithms for SNNs can be categorized into unsupervised and supervised learning algorithms. Unsupervised learning algorithms are mostly bio-inspired, and the most commonly used one is the STDP (Spike-Timing-Dependent Plasticity) algorithm [8] based on synaptic plasticity mechanisms [9]. These algorithms offer better biological interpretability, but their performance is generally less effective compared to supervised learning algorithms [10].As for supervised learning, due to the non-differentiable nature of spikes, the supervised learning methods for SNNs differ significantly from those for ANNs. Currently, there are two training methods: one is direct training [11, 12, 13, 14, 15], which introduces a differentiable function to approximate the heaviside function. The derivative of this function is used to replace the derivative of the heaviside function at the corresponding position for backpropagation, hence it is also known as the gradient surrogate"}, {"title": "2.2 Quantization loss", "content": "Previous researchers [19] have proposed and demonstrated that the membrane potential u follows a normal distribution with a mean of 0. The occurrences of membrane potential exceeding the threshold are rare in the distribution, leading to a low firing rate of neurons as shown in Fig.3a, which endows the spike with sparse characteristics. The sparse nature is the source of high performance and low power consumption in SNNs, but it also causes a significant quantization loss, hindering the development of deeper networks. To reduce quantization loss, many researchers have made efforts.\nSome attempt to adjust the distribution of the membrane potential to alter the firing rate. [19] indirectly adjust the distribution of membrane potential u by adjusting the variance of the input x, ensuring that the firing rate remains at a reasonable level. [20] calculates the mutual information between the membrane potential and spikes to conclude that the maximum information content of spikes occurs when the firing rate is 0.5.\nOthers try to expand spikes into multi-valued ones to enhance the information content of the spikes. [21] packages k LIF neurons with different thresholds into a single MLF unit, where neurons within the unit share inputs and their outputs are summed, resulting in a spike value range of {0,1,2,..., k}, which increases the diversity of the output spikes. [22] proposes an inhibitory type of spike, expanding the range of spike values to {\u22121,0,1}. Similarly, [23] also introduces ternary spikes, using a as a scaling coefficient to set the spike values to {-a, 0, a}."}, {"title": "3 Method", "content": "To reduce the loss during the forward process and enhance the amount of information carried by spikes, we propose a multi-bit information transmission mechanism. Futhermore, in order to preserve the information and promote the full firing of spikes across various bit positions, we introduce interlaminar connections. Specifically, as shown in Fig.2, we construct a ResNet-20 network, incorporating the multi-bit mechanism on the basis of LIF neurons, and introducing interlaminar connections in the basic block of the ResNet."}, {"title": "3.1 Multi-bit mechanism", "content": "We propose multi-bit mechanism. This mechanism extends the output of neurons from single-bit to multi-bit. Specifically, due to the fact that connections between two neurons (synapses) are multivalent rather than unique, we extend the spike to a fixed-point unsigned binary number with m integer bits and n fractional bits. Specifically, the feature space has the shape C \u00d7 H \u00d7 W, and the original spike is a single integer bit, with the representation range being S \u2208 {0,1}, so the overall feature space distribution is X \u2208 {0,1}cxhxw. The extended spike consists of m integer bits and n fractional bits, with the representation range being S \u2208 2\u00af\u00f1 \u00d7 {0, 1, 2, ...,2m+n \u2212 1}, thus the overall feature space distribution is X \u2208 2\u2212n \u00d7 {0,1,2,..., 2m+n \u2013 1}cxhxw. Clearly, the representation capability of the extended spikes has significantly increased.\nWe are not merely increasing the number of integer bits m or fractional bits n independently to improve spike distinguishability. Instead, we combine both aspects because they serve different purposes. In the following, we will provide a detailed explanation of the differences between them.\nWe first review the firing scenario of the basic LIF model as presented in Fig.3a, where we assume without loss of generality that the membrane potential follows a standard normal distribution, with Vth = 0.6. We use entropy to represent the loss of information before and after firing. Before firing, the membrane potential is normally distributed. Its probability density function is\n\\(f(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}\\) (1)\nand its entropy is fixed, which can be described as\n\\(H(U) = - \\int_{-\\infty}^{\\infty} f (u) \\log f (u) du = \\frac{1}{2} \\log_2(2\\pi e)\\) (2)\nIn the LIF model, the continuous u is discretized into spikes, whose probabilities can be expressed as\n\\(P_1 = P(S = 1) = P(U > V_{th}) = 1 - \\Phi(V_{th})\n\\begin{aligned}P_0 = P(S = 0) = P(U < V_{th}) = \\Phi(V_{th})\\end{aligned}\nwhere \u03a6 denotes the upper quantile of the normal distribution. The entropy of spikes is\n\\(H(S) = - \\sum_{i=0,1} P_i \\log_2 P_i = - P_0 \\log_2 P_0 \u2013 P_1 \\log_2 P_1\\) (4)\nThe forward process loss of LIF is\n\\(Loss = H(U) \u2013 H(S) = \\frac{1}{2} \\log(2\\pi e) + P_0 \\log_2 P_0 + P_1 \\log_2 P_1\\) (5)\nThe magnitude of Loss is solely determined by H(S), as H(U) is fixed. Therefore, increasing the magnitude of H(S) is beneficial for reducing the loss in the forward process.\nNext, let's analyze the case of expanding the number of integer bits. Assuming an upward expansion of one bit, i.e., m=2, the range of spike values becomes {0, 1, 2, 3}. Thus, Eq.14 is transformed to\n\\(S_{2\\_0} =\\begin{cases}3 & u_{th} \\geq 3, \\\\2 & 2 \\leq u_{th} < 3, \\\\1 & 1\\leq u_{th} < 2, \\\\0 & u_{th} < 1.\\end{cases}\\) (6)"}, {"title": "3.2 Interlaminar connections", "content": "Whether it's an ANN or an SNN, information in their network structures typically flows layer by layer. However, in real neural networks, cross-layer information fusion truly exists, known as interlaminar connections [6]. Here, to further reduce the information loss in the forward process and facilitate the potential of multi-bit mechanisms, we incorporate interlaminar connections into the Basic Block of ResNet. Specifically, the original structure of the Basic Block remains unchanged, but the outputs from the two BN layers are merged before being fed into the second spiking neuron. The fusion process is illustrated in Algorithm. 1, where outputs from the two BN layers are first concatenated along the channel dimension, followed by a 1D convolution and BN. Next, the ECA module is applied to distinguish salient features while ignoring less important ones, obtaining the final fused result, which is then input into MBLIF to complete the fusion of information. It should be noted that in the fusion process, although some additional modules are introduced, apart from the 1D convolution and ECA module which have a small number of learnable parameters, the overall increase in parameters and computational cost is negligible . The entire process can be expressed as\n\\(X_t = BN(Conv_{1*1}(ConCat(X^{pre}, X^{post}))),\\) (10)\n\\(X=X_t \\bigodot g(X_t)\\) (11)\nWherein, \\(X^{pre}\\) and \\(X^{post}\\) respectively represent the outputs of the preceding and following BN layers, and g() denotes the ECA attention process as shown in Eq. 16.\nInterlaminar connections not only directly enrich the network's topological connections and enhance its expressive capabilities, but also indirectly alter the dynamical activity of spiking neurons. Specifically, on top of existing activity, newly introduced connections utilize the attention mechanism to extract effective features, which are then used to re-stimulate the neurons. As shown in Fig.5a-Fig.5c, this stimulation serves as a compensation for lost information and essentially changes the original charging method of the neurons, acting as a kind of secondary charging. This means that neurons will be more fully activated; already active neurons will become even more active, and neurons that were previously inactive may also enter an active state. This encourages neurons to fully fire across all bits, fully exploiting the potential of the multi-bit information transmission mechanism."}, {"title": "4 Experiments", "content": "In this section, we conducted extensive experiments. Whether on static datasets or neuromorphic datasets, and whether using direct training methods or ANN-SNN conversion training, the results consistently showed improvements. Details are provided in the appendix.\n4.1 Static image classification\nWe conducted classification experiments on static data using the direct training method. Tab.2 show our experimental results on CIFAR-10[27] and CIFAR-100[27], which indicate that we achieve 95.00% and 76.51% accuracy on these datasets, respectively.\n4.2 Neuromorphic data classification\nWe conduct experiments on the DVS-Gesture dataset[28] using the direct training method. This is a more information-rich neuromorphic dataset for which we use the method of [21] to preprocess data. The experimental results are shown in Fig.6. Our multi-bit mechanism results in a 2.51% increase in classification accuracy, and the interlaminar connections further increase the accuracy by 0.88%. This demonstrates that our method is also applicable to neuromorphic datasets.\n4.3 Ablation experiment\nWe conduct ablation experiments. We base our experiments on the ResNet-20 network and LIF neurons, with all parameters uniformly set: Vth = 0.6, \u03c4 = 4 and t = 4. We particularly focus on ablating the bit width of spikes, with experimental details presented in Tab.3. It can be observed that"}, {"title": "4.4 Performance at ultra-low time steps", "content": "Notably, as shown in Tab.4, even with just a single time step, our method remains highly competitive compared to many other works. This is primarily due to our multi-bit mechanism, which extends the bit width of spikes, trading space for time. The information contained in spikes at a single time step is significantly increased, enabling the SNN to achieve high accuracy with ultra-low time step."}, {"title": "4.5 Visualization", "content": "We attempt to visualize the spike firing under the multi-bit mechanism. We extract the membrane potentials of neurons in a certain layer of the network after receiving current inputs and vary the bit width of the spikes to observe the firing activity. As the bit width increases from Fig. 7a to Fig.7d, the entropy of the spikes significantly increases, and the details presented in the grayscale images become increasingly rich. After expanding by two bits, the entropy in Fig.7d rises by 132% compared to the baseline in Fig. 7a, indicating a substantial increase in the information content of the spikes."}, {"title": "4.6 Compared to similar ideas", "content": "As the Tab.5 shows, we compare the proposed method with some similar ideas, which mostly expand the spike, increasing its representation range. Although the basic LIF operates faster in AC operations, its information capacity is small, only representing 0,1. The extension by [21], while enhancing representational capability, reverts AC operations back to the original MAC operations, and the dynamics of k neurons obviously increase the computational load, which is clearly not deployment-friendly. The ideas of [22] and [23] are similar, both expanding the spike into ternary, which does not introduce much extra computation and retains the advantages of AC, but their range is limited to ternary with no further expansion space. Our multi-bit information transfer mechanism, while preserving the advantages of AC operations, also benefits from the binary nature, having nearly no increase in computation compared to the basic LIF and possesses good scalability with a wide range of feature representation capabilities."}, {"title": "5 Conclusion", "content": "We propose a multi-bit information transmission mechanism that expands spikes from a single bit to multiple bits, greatly enriching the information content of the spikes. We also introduce interlaminar connections to facilitate the full release of spikes across various bit positions. To our knowledge, this is the first work that attempts to expand the bit-width of spikes, representing a novel information transmission paradigm for SNNs. Compared to previous models, our method introduces very few parameters yet significantly reduces the quantization loss in the forward process and still performs excellently at ultra-low time steps.\nHowever, the increase in bit-width inevitably leads to a slight increase in spike memory usage and computation time during inference. Nevertheless, considering the reduction in information loss and the overall performance improvement, this should be tolerable. We look forward to further improvements in the future and hope that our work can inspire subsequent information transmission mechanisms, contributing to the research and development of high-precision, low-latency SNNs."}, {"title": "A Appendix", "content": "A.1 Leaky Integrate-and-Fire model\nClassic spiking neuron models include the H-H (Hodgkin-Huxley) model [29], the LIF (Leaky Integrate-and-Fire) model[30], the Izhikevich model [31], and the SRM (Spike Response Model)[32]. The LIF model has become the most widely used model among them. This model uses the charging and discharging behavior of an RC circuit to simulate a series of dynamic characteristics of neurons receiving signals and transmitting signals, and its iterative form can be expressed as follows\n\\(U^{i,n}_t = (1-\\frac{1}{\\tau}) * U^{i,n}_{t-1} * (1 - s^{i,n}_{t-1}) + x^{i,n}_t\\) (12)"}, {"title": "A.2 Efficient Channel Attention", "content": "We used the ECA [7] mechanism to fuse signals from different layers when introducing interlaminar connections. Compared to most attention mechanisms[33, 34], this lightweight attention mechanism introduces very few parameters with one-dimensional convolution. Specifically, the ECA attention process first uses global average pooling on features to extract global information from each channel, then learns the dependencies between channels with one-dimensional convolution, and finally obtains weights with the sigmoid function. These processes can be represented as:\n\\(g(X) = Sigmoid(Conv_{1*1}(AvgPool(X)))\\) (16)"}, {"title": "A.3 Experimental platform", "content": "The experiments were conducted on a computing platform with the following hardware specifications, as shown in Table 6."}, {"title": "A.4 Experimental setup for direct training", "content": "We apply the proposed multi-bit information transmission mechanism to LIF neurons, introduce interlaminar connections into the Basic Block of ResNet, and obtain the ResNet-20 network, which we then use for direct training and testing of the network's performance. The hyperparameters of the each experiment are shown in the Tab.7."}, {"title": "A.5 Experiments on ANN-SNN conversion", "content": "We also test our multi-bit mechanism in ANN-SNN conversion, applying it to IF neurons. Here, instead of expanding the number of integer bits, we expand by three decimal bits, resulting in MBIF"}]}