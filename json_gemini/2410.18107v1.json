{"title": "In-Context Code-Text Learning for\nBimodal Software Engineering", "authors": ["Xunzhu Tang", "Liran Wang", "Yonghui Liu", "Linzheng Chai", "Jian Yang", "Zhoujun Li", "Haoye Tian", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "abstract": "Bimodal software analysis initially appeared to be\nwithin reach with the advent of large language models. Unfor-\ntunately, the complex interplay of natural language text and\ncode in software engineering, presents unique challenges that\nprevent pretrained models to generalize to a variety of tasks. We\npostulate that in-context learning for the code-text bimodality is\na promising avenue. This paper thus introduces a comprehensive\nstudy of in-context code-text learning, focusing on leveraging\npretrained CodeLLAMA models.\nWe consider a diverse dataset encompassing 23 software\nengineering tasks, which we transform in an in-context learning\nformat. To effectively extract informative features, we propose\na configurable prompt template. Our proposed pipeline, INC-\nTRL, then unifies prompt learning across various software\nengineering tasks. Extensive evaluation on the study datasets\ndemonstrates the superiority of INCTRL-models in few-shot\nperformance, surpassing state-of-the-art models including the\nsupport model, CodeLLAMA. Typically, we observe that applied\nto the CodeLLAMA model, INCTRL brings improvements in\nterms of precision (at least about 12%) and recall (up to 93.88%)\non various tasks. For example, on the task of program repair,\nINCTRL improves the BLEU score of CodeLLAMA by 85 points,\nwhile for clone detection, INCTRL achieves an improvement of\n69 percentage points. Moreover, INCTRL-models offer state-of-\nthe-art performance when using retrieval-augmented generation\non individual downstream tasks. Finally, we qualitatively analyze\nthe benefits of INCTRL over CodeLLAMA and open-source all\nmodels for broader impact.", "sections": [{"title": "I. INTRODUCTION", "content": "Software development is a complex endeavor characterized\nby its inherent interplay of code and natural language. While\nsignificant advancements have been made in both code anal-\nysis and natural language processing, effectively integrating\nthese two modalities remains a critical challenge [1]. Yet,\nthis integration is essential for addressing complex tasks that\nrequire a deep understanding of both code syntax and semantic\ncontext. In this setting, Large Language Models (LLMs)\nappear promising due to their ability to process and generate\nboth text and code, which offers unprecedented potential to\nautomate tasks, improve code quality, and enhance developer\nproductivity [2].\nA critical factor in LLM effectiveness is the quality and\ndiversity of training data [3], [4]. While source code is\nthe foundation, incorporating additional data types such as\nbug reports [5], [5], patches [6], [7], and vulnerable code\nsnippets [8], [9] has proven invaluable for specific tasks such\nas program repair or vulnerability detection. Yet, building\ngeneral-purpose code-text models remains challenging due to\nthe varied nature of code and the complexities of bridging\nthe gap between natural language and formal logic [10].\nIndeed, source code is inherently bimodal, composed of both\na formal algorithmic component and an informal natural\nlanguage component consisting for example of identifiers,\ncomments and other text artefacts [11]. While research has\npredominantly examined these channels in isolation, attempts"}, {"title": null, "content": "for comprehensive approaches using machine learning have\nbeen proposed in the literature [12]. The two channels are\nindeed interconnected, with natural language often providing\ncontext, explanations, or summaries of the underlying code.\nBy jointly analyzing both channels, researchers can poten-\ntially enhance their understanding and analysis of software\nsystems [13]. Unfortunately, building general-purpose code-\ntext models presents unique challenges due to the diverse input\ndistributions and task variations. Bimodal software engineering\nfurther faces complex challenges for identifying synchroniza-\ntion points between code and text as well as managing noise,\nwhich come in the form of ambiguity in the natural language\nchannel and imprecision in the code algorithmic channel.\nTo effectively address the specificities of bimodal software\nengineering, LLMs often require carefully-crafted datasets.\nThis process demands a deep understanding of downstream\ntasks, code-context comprehension, and code-text alignment\nto ensure consistency and quality across diverse task types.\nWhile essential for enhancing LLM performance in software\nengineering, current data generation methods, such as those\noutlined by Luo et al. [14] and Wang et al. [15], frequently\nrely on limited task sets or heuristic approaches. This reliance\ncan inadvertently introduce systemic biases inherited from\nboth the LLMs and the predefined tasks themselves. Indeed,\nprevious approaches to addressing these challenges have pri-\nmarily fallen into two categories: multitask learning [16],\n[17] and task-specific component extension [18], [19], [20].\nMultitask learning attempts to formulate various code-based\ntasks into a uniform input-output format, offering versatility\nbut often struggling to capture nuanced differences between\ndiverse tasks. Task-specific component extension augments\npre-trained LLMs with additional components trained on task-\noriented data, allowing for more tailored solutions but facing\nchallenges in scalability and generalization to novel tasks.\nBoth approaches have demonstrated limitations in their ability\nto generalize effectively to unseen datasets and tasks.\nThis paper. In response to the aforementioned challenges,\nwe propose a novel framework designed to significantly en-\nhance LLM capabilities in handling diverse software engineer-\ning tasks. We refer to it as In-Context Code-Text Learning\n(INCTRL). Central to our approach is a unified code-text\ninterface that generalizes task-oriented code-text instructions.\nBy consolidating code and text data into a single format,\nwe significantly expand the training data available to LLMs,\nresulting in models that are more versatile and less biased\nwhen tackling software engineering problems.\nOur research contributes significantly to the emerging field\nof in-context code-text learning, a critical component for\nachieving bimodal software analysis. By establishing a foun-\ndation for effective code-text interaction, we envision future\nsystems capable of accurately understanding and generating\nboth modalities in a contextually aware manner. Such models\nwill be instrumental in bridging the human-computer divide,\nenabling fluent translation between high-level instructions and\nexecutable code.\nKey aspects of our proposed methodology include:"}, {"title": null, "content": "Unified Code-Text Interface: We introduce a novel\ninterface that bridges the gap between diverse code-\nrelated tasks, allowing for a more cohesive representation\nof various SE problems.\nTask-Oriented Instruction Generalization: Our ap-\nproach focuses on generalizing task-specific instructions,\nenabling LLMs to better understand and adapt to a wider\nrange of SE tasks.\nExperimental results, based on seven typical software en-\ngineering tasks and applied on 23 datasets, provide sufficient\ndata to support the claims that:\nINCTRL enhances LLMs for software engineering tasks\nby effectively leveraging in-context learning without re-\nquiring model retraining. Our experiments show that\nINCTRL improves CodeLLAMA by 17.36% to 85.3% on\ncode generation tasks, 5.74% to 13.71 on Text generation\ntasks, 12.31% to 95.66% on Classification tasks.\nINCTRL significantly improves classification performance\nby incorporating a Retrieval-Augmented Generation mod-\nule, addressing a key limitation of traditional in-context\nlearning. Our experiments show that the RAG module\ncontributed to an average increase of 1.8 points of the\nF1-score and for some datasets can achieve up to 7.26\npoints of improvement.\nINCTRL demonstrates strong performance across various\nsoftware engineering tasks, particularly excelling in code\ngeneration and repair. Its impact, however, varies across\ntasks and datasets, influenced by factors such as dataset\ncomplexity, input length and the intrinsic nature of the\ntasks.\nINCTRL 's structured prompt design provides rich in-\ncontext learning opportunities, as exemplified by its ef-\nfectiveness in code summarization."}, {"title": "II. BACKGROUND", "content": "A wide array of existing software engineering research\ntypically specializes in one particular downstream task in\neither code analysis or natural language processing.\nBimodal Software Engineering represents an emerging\nfield that seeks to bridge the gap between code and natural\nlanguage within the software development lifecycle. This\nintegration is essential for addressing complex tasks that\nrequire a deep understanding of both code syntax and semantic\ncontext. A primary challenge lies in aligning code and text\nrepresentations effectively, as discrepancies can lead to errors,\nvulnerabilities, and hindered developer productivity [11]. Tra-\nditional approaches often silo these modalities, as illustrated\nin Figure 1, limiting the potential synergies between them [1].\nIndeed, different tasks and datasets require various inputs and\noutputs, which require the creation and training of deep learn-\ning models that are capable of handling multiple downstream\ntasks, such as clone detection and code generation. The range\nof requirements underscores the need for more integrated and\nversatile tools in bimodal software engineering, capable of\naddressing the complex interplay between code and natural\nlanguage across diverse software development scenarios."}, {"title": null, "content": "To overcome these limitations, we propose a unified frame-\nwork that leverages a general-purpose interface (i.e., an\nadapter), which helps to handle various software engineering\ntasks by seamlessly integrating code and text analysis. This\napproach empowers models to extract valuable insights from\nboth domains, enhancing their capability to perform complex\nSE tasks like detecting vulnerabilities [21], [22], explaining\ncode[23], [24], and identifying code clones[25] with greater\naccuracy and efficiency. Thus, our technique unifies prompt\nlearning for large language models across various software\nengineering tasks, ensuring effective processing of diverse\ninputs to achieve state-of-the-art results.\nIn-Context Learning (ICL) is a key capability of large\nlanguage models (LLMs) that allows them to adapt to new\ntasks without explicit training. By incorporating task demon-\nstrations within the input prompt, LLMs can generate relevant\noutputs, as first highlighted with GPT-3 [26]. Indeed GPT-3\ndemonstrated remarkable performance across a range of tasks\nby simply conditioning on task demonstrations embedded\nin the input context. Unlike traditional machine learning,\nICL leverages the model's extensive pre-trained knowledge\nto perform tasks with minimal supervision, such as zero-\nshot or few-shot learning [27]. Research has indicated that\nthe effectiveness of ICL is influenced by factors such as\nthe number, order, and quality of in-context examples [28],\n[29]. This sensitivity has prompted the development of prompt\nengineering techniques aimed at optimizing the presentation of\ninformation to the model [30]. While the precise mechanisms\nunderlying ICL are still under investigation, studies suggest\nthat models may learn to identify patterns and correlations\nwithin the provided examples [31]. Our work capitalizes on\nICL's strengths, enabling rapid adaptation of the models by\nthe INCTRL framework without extensive retraining.\nMulti-Task Learning (MTL) is an advanced machine\nlearning paradigm that contrasts with single-task learning\napproache. It involves the concurrent learning of multiple\nrelated tasks, leveraging shared information to enhance overall\nperformance [32], [17]. MTL offers a powerful framework for\nutilizing supervised data across related tasks, thereby reducing\nthe reliance on task-specific labeled data [33]. This approach\ninherently combats overfitting by encouraging the model to\nlearn more generalizable features, effectively acting as a form"}, {"title": null, "content": "of regularization [16]. The application of MTL to boost task\nperformance has been extensively explored and successfully\nimplemented across diverse domains and scenarios.\nWhile MTL enables models to learn multiple related tasks\nsimultaneously, it offers rather a complementary perspective to\nour work. Indeed, our in-context code-text reasoning approach\ndiffers from MTL by emphasizing a flexible, task-agnostic\ninterface capable of handling a wide range of software en-\ngineering problems. MTL indeed typically focuses on sharing\nparameters across predefined tasks to reduce overfitting and\nimprove generalization. While MTL typically involves prede-\nfined task structures with shared or task-specific parameters,\nthe in-context code-text reasoning method emphasizes flexi-\nbility, allowing for the dynamic handling of various software\nengineering tasks through configurable prompt learning. We\npostulate that our method addresses more directly the unique\nchallenges of bimodal software engineering, such as desyn-\nchronization between code and text, which are not explicitly\ntargeted by traditional MTL techniques.\nPre-trained Language Models have significantly advanced\nnatural language processing (NLP), consistently achieving\nstate-of-the-art performance across a wide range of tasks [34],\n[7], [18], [35]. These models offer several key advantages:\nthey extract universal representations from extensive corpora,\nprovide enhanced model initialization for better generalization\non downstream tasks, and serve as a form of regularization\nto combat overfitting when working with limited datasets.\nResearchers typically employ two main strategies to har-\nness pre-trained language representations: feature-based ap-\nproaches [36], [37], [38], which consider pre-trained represen-\ntations as additional features, and fine-tuning approaches [39],\n[40], [7], which adapt the entire pre-trained model for specific\ndownstream tasks. However, these models often specialize in\neither classification or generation tasks. In contrast, we aim\nto take a more holistic view, unifying prompt learning across\ndiverse software engineering tasks, and yielding models that\nare capable of addressing the multifaceted requirements of the\nsoftware engineering field, including both code analysis and\nnatural language processing aspects."}, {"title": "III. INCTRL: IN-CONTEXT CODE-TEXT LEARNING", "content": "The INCTRL framework is designed to integrate code and\nnatural language text for various SE tasks, leveraging in-\ncontext learning to enhance model performance. This section\ndetails the construction of the INCTRL approach, its key\ncomponents, and the configurations used."}, {"title": "A. Architecture Overview", "content": "The INCTRL architecture, depicted in Figure 2, comprises\nseveral interconnected components designed to handle, to-\ngether, complex software engineering tasks. A Task Module\ninitializes task-specific settings based on the configuration\nfile (task_config.json). The Prompt Module loads\nprompt configurations (from prompt_config.json), out-\nlining the prompt structure and content for in-context learning."}, {"title": null, "content": "These configurations serve as the foundation for the Con-\nfigurablePrompt component, which dynamically generates\nprompts by combining predefined templates with relevant\nexamples retrieved by the Retrieval-Augmented Generation\n(RAG) module. This approach ensures that prompts are\ntailored to specific tasks, enhancing the model's ability to\ngenerate accurate and contextually appropriate responses."}, {"title": null, "content": "If enabled, the RAG Module retrieves relevant examples\nfrom the training set. The retrieval process can be represented\nby the equation\n$E = RAG(Q, D)$ (1)\nwhere Q is the query (task prompt) and D is the document cor-\npus (training set). The retrieved examples E enrich the prompt.\nThe Question Module handles specific queries posed by the\nuser, integrating them into the configurable prompt to ensure\ncomprehensive and tailored prompts for the task requirements.\nThe enriched prompt is fed into the Generator, which, in our\nstudy, leverages the CodeLLAMA model to produce the final\nresponse. The model input, a combination of the prompt and\nthe question, is represented as $R = Generator(P \\oplus Q)$, where\nR is the generated response, P is the prompt, and Q is the\nquestion."}, {"title": "B. Task Configuration", "content": "We consider three different types of tasks depending on\nthe output: Class output, Code output, and text output. These\ntasks depicted in Figure 3 are executed across 23 datasets (cf.\nSection V. The process begins with the Task Module, which\ninitializes the specific task settings and determines whether to\nuse the Retrieval-Augmented Generation (RAG) module based\non the task requirements (specified in the configuration file).\nThe task configuration file contains parameters essential for\ndefining the task environment. These include the task type\n(e.g., code generation, code summarization), input-output for-\nmats, evaluation metrics, and whether RAG is enabled. Let T\ndenote the task settings vector derived from the configuration\nfile, such that $T = {T_1, T_2,...,T_n}$, where $T_i$ represents\nindividual configuration parameters. The Task Module parses\nT and initializes the environment accordingly.\nIf the task requires the use of the RAG module, the Task\nModule sets a binary flag $f_{RAG} \\in {0,1}$, where $f_{RAG} =$"}, {"title": null, "content": "$\\begin{cases}\n1 & \\text{if T includes RAG-enabled tasks} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nThis decision impacts subsequent steps, such as whether to\nretrieve examples from the training set.\nThe Task Module also initializes the input and output data\nstructures. Let $I = {I_1, I_2, ..., I_m}$ represent the input data\nconfiguration and $O = {0_1,0_2,..., 0_p}$ the output data\nconfiguration. The mapping from input to output can be\ndescribed by a function F, which is parameterized by T:\n$O = F(I; T)$\nwhere F transforms the input data according to the task\nsettings defined in T.\nFurthermore, the Task Module configures evaluation metrics\nto be used for assessing model performance. Let M =\n${M_1, M_2, ..., M_q}$ denote the set of metrics, with each $M_j$\nrepresenting a specific evaluation criterion, such as BLEU\nscore for text generation or accuracy for classification tasks.\nThe evaluation function $\\varepsilon$ can be expressed as:\n$E = \\varepsilon(O; M)$\nwhere E represents the evaluation results based on the gener-\nated output O and the selected metrics M.\nBy systematically configuring the task environment, input-\noutput mappings, and evaluation criteria, the Task Module\nensures that the INCTRL framework is correctly initialized for\nthe specific requirements of each software engineering task."}, {"title": "C. Prompt Configuration", "content": "Concurrently, the Prompt Module loads the prompt configu-\nrations from the configuration file. These configurations define\nthe structure and content of the prompts used for in-context\nlearning. The prompt configurations are crucial for generating\ntask-specific prompts that guide the model in understanding\nand processing the input data effectively. Let P denote the\nprompt configuration vector derived from the configuration\nfile, such that $P = {P_1, P_2, ..., P_k}$, where $P_i$ represents"}, {"title": null, "content": "individual configuration parameters, including template struc-\ntures, placeholders for input variables, and instructions for the\nmodel.\nThe Prompt Module parses P to create a prompt template T\ndynamically adjusted based on the input data I. Formally, let T\nbe a function of P and I: T = T(P,I), where T represents\nthe structured prompt used to guide the model. To generate\nthe final prompt $P_r$, the Prompt Module substitutes the place-\nholders in T with actual values from I: $P_r = T(P, I)$. This\nensures that the model receives a coherent and contextually\nrelevant prompt.\nAdditionally, the prompt configuration includes rules for\naugmenting the prompts with supplementary context, such\nas examples or clarifying information. Let A denote the\naugmentation function: $P_r' = A(P_r, C)$, where $P_r'$ is the\nenriched prompt and C represents the context before question."}, {"title": "D. Configurable Prompt Generation", "content": "The ConfigurablePrompt component is central to the INC-\nTRL framework. It takes inputs from the Prompt Module\nand any additional context provided by the RAG module\nif enabled. The RAG module retrieves relevant examples\nfrom the training set, enriching the prompt with contextual\ninformation. The resulting prompt is a combination of prede-\nfined structures and dynamically retrieved examples, making\nit highly adaptable to various tasks.\nFormally, let $P_r$ represent the initial prompt generated\nby the Prompt Module, and R(I) denote the function of\nthe RAG module that retrieves a set of relevant examples\n$E = {E_1, E_2,..., E_n}$ based on the input data I. The\nConfigurablePrompt component combines these inputs to form\nan enriched prompt $P_r'$. The combination process can be\nexpressed as follows:\n$P_r' = F(P_r, E) = P_rU \\underset{i=1}{\\overset{n}{\\cup}}E_i$ (2)\nwhere F is a function that integrates the initial prompt $P_r$ with\nthe retrieved examples E, and U denotes the union operation\nthat appends the examples to the prompt.\nFinally, when performing inference, we treat each a certain\ndata entry in $Q_i$ in the test set as a question, concatenate with\n$P_r'$ to obtain the final prompt of this date entry $P_r^{final}$ that\nis used as input to the model:\n$P_r^{final} = P_r'\\oplus Q_i.$ (3)\nAfter conducting extensive experiments, we have summa-\nrized a high-quality template construction scheme and set it\nas the default structure for configurable prompt. Specifically,\nINCTRL's default scheme divides the configurable prompt\ninto the following six sections (as previously illustrated in the\nlower right part of Figure 2):"}, {"title": null, "content": "INTRODUCTION: provides a background context for\nthe generator;\nDEFINITION: gives the basic requirements of the task;"}, {"title": null, "content": "PRE-INSTRUCTION: gives more detailed requirements\nabout the task;\nDEMONSTRATION: provides several input and output\nexamples;\nPOST-INSTRUCTION: emphasizes more detailed re-\nquirements about the task again;\nQUESTION: contains the query.\nAn example configurable prompt is provided in Figure 4 for\nthe task of code summarization."}, {"title": "E. Response Generation", "content": "Once the final prompt is constructed, it is fed into the Gen-\nerator module (i.e., CodeLLAMA in our case). The Generator\nleverages the structured prompt to produce accurate and con-\ntextually relevant responses. The response generation process\ncan be modeled as a function G that maps the augmented\nprompt to a response vector, $R = G(P_r^{final})$, ensuring that\nthe output aligns with the user's query. In the context of\ncode-related tasks, G utilizes the underlying capabilities of\nCodeLLAMA to perform various operations such as code\ngeneration, code completion, and providing natural language\nexplanations. The generated response R can be further refined\nto ensure precision and relevance. For example, if the task\ninvolves generating a function definition based on a query,\nthe response might include syntactic checks and semantic\nvalidations to ensure the code is both correct and optimal.\nTo enhance the robustness of the response, an additional\npost-processing step can be applied, incorporating feedback\nloops and iterative improvements. Let P be the post-processing\nfunction that refines the response, such that the finalized\nresponse is $R_{final} = P(R)$. This comprehensive approach\nensures that the INCTRL framework can handle a wide range\nof tasks efficiently, producing high-quality outputs that meet\nthe users' requirements in both code and natural language\ncontexts."}, {"title": "IV. INFERENCE", "content": "The INCTRL framework is designed to accommodate a\ndiverse range of software engineering tasks, including both\nclassification and generation problems. Leveraging its mod-\nular architecture and effective prompt handling, INCTRL\naddresses these tasks through tailored inference strategies.\nFor generation-oriented tasks, such as code generation\nand open-ended code comprehension, the model is directly\nprompted to produce outputs. These generated sequences\nare then evaluated against ground truth using metrics like\nBLEU. In contrast, classification tasks, including vulnerability\ndetection and clone detection, utilize a vocabulary ranking\napproach. The model generates potential candidates, and the\noption with the highest log-likelihood is selected as the final\nprediction. To enhance performance, especially for binary clas-\nsification, we expand label sets to include semantically similar\nterms (e.g., \"yes\" and \"true\" for positive class), aligning the\nmodel's output with natural language patterns.\nMathematically, the classification process is expressed as:"}, {"title": null, "content": "$\\hat{y} = arg \\underset{c \\in C}{max} log P(c|x)$ (4)\nwhere y is the predicted class, C is the set of candidate classes,\nand x is the input prompt.\nFor generation tasks, the generator produces a sequence of\ntokens, y, given an input prompt, x:\n$P(y|x) = \\prod_{t=1}^{T} P(y_t|y_{<t}, x)$ (5)"}, {"title": null, "content": "where T is the sequence length, and $P(y_t|y_{<t}, x)$ is the\nprobability of generating token $y_t$ given previous tokens $y_{<t}$\nand the input prompt x.\nPrompt for baseline CodeLLAMA. Experiments without\nINCTRL apply simple prompts that define the task and pro-\nvide the input data. For example, for vulnerability detection,\nthe prompt is as follows \u201cGiven the following code snippet,\nclassify whether it is vulnerable or not. <code> \". All\nprompts used are in the replailcation package."}, {"title": "V. EXPERIMENTAL SETUP", "content": "A. Research Questions\nRQ1: Effectiveness of in-context learning. Considering\nthe various code-text tasks in software engineering, to\nwhat extent model performance can be improved by in-\ncontext learning with INCTRL?\nRQ2: Impact of Retrieval-Augmented Generation. To\nwhat extent is the use of RAG for selecting examples for\nprompting necessary or beneficial?\nRQ3: Performance on specific datasets. From a fine-\ngrained perspective, how does INCTRL influence the\nmodel's performance on individual datasets for each task?\nRQ4: Qualitative case study. From the actual input-\noutput examples of INCTRL, what qualitative assess-\nments can we provide regarding its capabilities?"}, {"title": "B. Tasks and Datasets", "content": "In Table I, we provide detailed information about the\ndatasets used in our experimental setup. These datasets were\ncollected from Huggingface. It is nonetheless worth noting\nthat, for some datasets, we only used a subset. For those cases\nthe \"Remark\" column indicates which subset was considered.\nWe categorize most of the software engineering tasks into\nthree types based on the modality of output data: code-output,\ntext-output, and class-output. The tasks that fall under the\ncode-output category include code generation, code transla-\ntion, code completion, and program repair, encompassing 14\ndatasets ([49], [50], [51], [44], [52], [24], [53], [54], [23],\n[46], [47]). The text-output category consists of the code\nsummarization task, which includes 3 datasets ([23], [24],\n[48]). The tasks in the class-output category are vulnerability\ndetection and clone detection, covering 6 datasets ([21], [22],\n[41], [42], [43], [25])."}, {"title": "C. Models", "content": "We use CodeLLaMA as the base LLM. We select\nthis LLM because it is open source and it is\namong the most popular in software engineering research.\nSpecifically, we evaluated four sets of model parameters:\nCodeLLaMA-7B, CodeLLaMA-7B-Instruct, CodeLLaMA-\n13B, and CodeLLaMA-13B-Instruct. The difference between\nmodels with and without \"Instruct\" in their name lies in how\nthey were trained and the type of data they were exposed to\nduring the training process. Furthermore, we choose Code-\nBERT [55], a pre-trained language model designed to under-\nstand and represent programming languages, as the embedding\nmodel in the RAG module. All the model parameters we\nused were downloaded from the official Hugging Face model\nrepository."}, {"title": "D. Metrics", "content": "1) Generation Tasks: For code-output and text-output tasks,\nsince they are both text generation tasks, we adopt BLEU [56]\nas the evaluation metric. It is an automatic metric for evaluat-\ning the quality of machine-generated text by comparing it to\nreference, calculating a similarity score based on the overlap\nof n-grams between the generated text and the reference text.\nIn the evaluation, we use the smoothed BLEU metric with\n1-gram.\n2) Classification Tasks: For class-output tasks, we adopt\nprecision, recall, and F1-score as evaluation metrics. All\nthe class-output tasks involved in our evaluation are binary\nclassification tasks (i.e., outputting \"yes\" or \"no\"). Under this\nsetting, precision measures the accuracy of positive predic-\ntions, recall measures the completeness of positive predictions,\nand F1-score is the harmonic mean of precision and recall,\nproviding a balanced measure of the classifier's performance."}, {"title": "E. Implementation", "content": "The RAG module of INCTRL is powered by LlamaIn-\ndex [57]. The evaluation of INCTRL is implemented based on\nVLLM [58] to ease reproducibility. For each dataset consid-\nered in this work, we provide its corresponding configurations\nto automatically build the interfaces. All experiments were\nconducted on a server equipped with 4 NVIDIA Tesla A800-\n80GB GPUs."}, {"title": "VI. RESULT AND ANALYSIS", "content": "In this section, we present the overall in-context learning\neffectiveness of INCTRL in Sec. VI-A, the necessity and ef-\nfectiveness of the RAG module in Sec. VI-B, the performance\non individual datasets in Sec. VI-C and the qualitative case\nstudy in Sec. VI-D"}, {"title": "A. RQ1: Effectiveness of in-context learning", "content": "In this subsection, we investigate the overall performance\nof INCTRL on software engineering tasks. We first obtain\nresults on all datasets mentioned in V-B using models with\nand without applying INCTRL. Then, we aggregate the output\nresults to compute the overall performance metrics for each"}, {"title": null, "content": "$\\hat{y} = arg \\underset{c \\in C}{max} log P(c|x)$ (4)\nwhere y is the predicted class, C is the set of candidate classes,\nand x is the input prompt.\nFor generation tasks, the generator produces a sequence of\ntokens, y, given an input prompt, x:\n$P(y|x) = \\prod_{t=1}^{T} P(y_t|y_{<t}, x)$ (5)"}, {"title": null, "content": "individual configuration parameters, including template struc-\ntures, placeholders for input variables, and instructions for the\nmodel.\nThe Prompt Module parses P to create a prompt template T\ndynamically adjusted based on the input data I. Formally, let T\nbe a function of P and I: T = T(P,I), where T represents\nthe structured prompt used to guide the model. To generate\nthe final prompt $P_r$, the Prompt Module substitutes the place-\nholders in T with actual values from I: $P_r = T(P, I)$. This\nensures that the model receives a coherent and contextually\nrelevant prompt.\nAdditionally, the prompt configuration includes rules for\naugmenting the prompts with supplementary context, such\nas examples or clarifying information. Let A denote the\naugmentation function: $P_r' = A(P_r, C)$, where $P_r'$ is the\nenriched prompt and C represents the context before question."}, {"title": "B. RQ2: Impact of Retrieval-Augmented Generation", "content": "Unlike generation tasks (where randomly selecting samples\nfrom the training set to construct in-context prompts can\nenable the model to have a certain ability to complete the task),"}, {"title": "C. RQ3: Performance on specific datasets", "content": "This section examines INCTRL's performance across indi-\nvidual datasets to assess its effectiveness in various software\nengineering tasks. Results, summarized in Table IV, demon-\nstrate INCTRL's ability to enhance model performance across\nall datasets.\nThe results reveal that INCTRL significantly enhances\nmodel performance across all datasets, albeit with varying\ndegrees of effectiveness depending on the task type. For code-"}]}