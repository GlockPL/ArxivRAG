{"title": "Causal Discovery in Recommender Systems: Example and Discussion", "authors": ["Emanuele Cavenaghi", "Fabio Stella", "Markus Zanker"], "abstract": "Causality is receiving increasing attention by the artificial intelligence and machine learning communities. This paper gives an example of modelling a recommender system problem using causal graphs. Specifically, we approached the causal discovery task to learn a causal graph by combining observational data from an open-source dataset with prior knowledge. The resulting causal graph shows that only a few variables effectively influence the analysed feedback signals. This contrasts with the recent trend in the machine learning community to include more and more variables in massive models, such as neural networks.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, several approaches from causality have been applied in Recommender Systems (RS) [2, 3, 6], especially using Causal Graphs (CGs) [11] which allow us to model, in a graphical and human-readable format, cause-and-effect relations among factors. Furthermore, to exploit the strength of causality, we should define the quantity that we aim to estimate as causal estimands\u00b9 that can only be estimated in controlled experiments. However, through CGs, we can identify an equivalent statistical estimand that can be estimated using observational data, i.e., data collected outside the scope of controlled experiments. To this end, a common approach is"}, {"title": "CAUSAL DISCOVERY PROCESS", "content": "In this section, we report the process followed to learn the CG reported below starting from the open-source dataset KuaiRand [5]. In particular, we used the KuaiRand-Pure dataset collected through randomly recommended videos in the user interaction sequence. This version of the dataset consists of 1 186 059 interactions between the system and the 27 285 users who were recommended from a pool of 7 583 items. Moreover, the dataset includes 30 users' features, 62 items' features and 12 feedback signals. Below, we report the five macro-steps followed in the causal discovery process:\nRemove features. In the first step, we removed the interaction features irrelevant to our goal, such as the interaction date and time, and the feedback signals with too few positive observations. Furthermore, we excluded all the encrypted users' features since we can not use prior knowledge on them as they lack any semantic information. Finally, we removed the items' music ID since almost all the videos are related to a different ID.\nDiscretize features. In the second step, we discretized all the features since the structure learning algorithm used to learn the CG can only handle discrete data. To this end, several features are available both with the original values and with a discretized version; thus, for these features, we decided to keep the discretization suggested by the authors of the dataset. For the remaining features, we chose the categories using the feature semantics to lose as little information as possible and to ensure that each category was observed a reasonable number of times.\nBuild Prior-Knowledge. In the third step, we defined how to include prior knowledge in the structure learning phase. To this"}, {"title": "LEARNED CAUSAL GRA\u03a1\u0397", "content": "In this section, we report a part of the entire CG leveraging the independence statements to restrict the CG to a proper sub-graph by discarding irrelevant factors that are not in the Markov Blanket (MB) [10] of the feedback signals nodes. An MB is defined as a subset of variables that contains all the useful information to infer the value of a random variable. Therefore, since the most important factors are the feedback signals, we restrict our focus on their MB, which is reported in Figure 1.\nThe first natural questions that arise are: Is the learned CG correct? Why is it different from other models? The answer is closely linked to the primary purpose of causal models. Citing the words of Judea Pearl [12]: \"The purpose of the diagram is to provide an unambiguous description of the scientific context of a given application\". This unambiguous description must not be taken as a dogma but should be used to build a \"hypothetical consensus on what is plausible and important versus that which is deemed negligible or implausible\" [12]. This consensus, in the form of possible associations among measured and unmeasured variables, must be built through \"[...] scientific considerations and debated by experts in the field\" [12]. Indeed, the correctness of a CG can not be guaranteed by automatic tests/procedures but must be verified through the mix of domain knowledge and observations/experiments. However, this limitation should not be confused with a weakness w.r.t. other models, as no existing model can be guaranteed to be correct."}, {"title": "CONCLUSIONS", "content": "This work focused on the causal discovery task of learning a CG by combining observational data from the KuaiRand open-source dataset with prior knowledge. The resulting CG contrasts the recent trend in Machine Learning and RSs to include more and more variables in larger and larger models. Indeed, the learned CG suggests that only a few variables effectively influence the analysed feedback signals. Therefore, all the others are irrelevant to our decisions and only contribute to noise. Moreover, more data is required as the number of variables (i.e., dimensions) increases. This finding can be supported by the fact that users (i.e., people) can not consider many factors when deciding, and their decisions are usually based on a few essential factors and subconscious emotional feelings [9]. On the other hand, part of the effect of the recommended items is (usually) not captured by the available factors. Therefore, more emphasis should be placed on modelling the problem and collecting the necessary information to make informed decisions."}]}