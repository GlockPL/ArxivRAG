{"title": "Prioritizing High-Consequence Biological\nCapabilities in Evaluations of Artificial\nIntelligence Models", "authors": ["Jaspreet Pannu", "Doni Bloomfield", "Alex Zhu", "Robert MacKnight", "Gabe Gomes", "Anita Cicero", "Thomas V. Inglesby"], "abstract": "As a result of rapidly accelerating artificial intelligence (AI) capabilities,\nover the past year, multiple national governments and multinational\nbodies have announced efforts to address safety, security and ethics\nissues related to AI models. One high priority among these efforts is the\nmitigation of misuse of AI models, such as for the development of\nchemical, biological, nuclear or radiological (CBRN) threats. Many\nbiologists have for decades sought to reduce the risks of scientific\nresearch that could lead, through accident or misuse, to high-\nconsequence disease outbreaks. Scientists have carefully considered what\ntypes of life sciences research have the potential for both benefit and risk\n(dual-use), especially as scientific advances have accelerated our ability\nto engineer organisms and create novel variants of pathogens. Here we\ndescribe how previous experience and study by scientists and policy\nprofessionals of dual-use capabilities in the life sciences can inform risk\nevaluations of Al models with biological capabilities. We argue that AI\nmodel evaluations should prioritize addressing high-consequence risks\n(those that could cause large-scale harm to the public, such as\npandemics), and that these risks should be evaluated prior to model\ndeployment so as to allow potential biosafety and/or biosecurity\nmeasures. While biological research is on balance immensely beneficial,\nit is well recognized that some biological research information and\ntechnologies could be intentionally or inadvertently misused to cause\nlarge-scale harm to the public. AI-enabled life sciences research is no\ndifferent. Scientists' historical experience with identifying and mitigating\ndual-use biological risks can thus help inform new approaches to\nevaluating biological AI models. Identifying which AI capabilities pose\nthe greatest biosecurity and biosafety concerns is necessary in order to\nestablish targeted AI safety evaluation methods, secure these tools\nagainst accident and misuse, and avoid impeding immense potential\nbenefits.", "sections": [{"title": "Introduction", "content": "Scientists can perform almost all biological research in ways that pose minimal risk to society. However,\nsome limited areas of life sciences research can threaten high-consequence harms to the public, through\nlaboratory accidents or misuse. These risks are exacerbated by our growing capability to engineer existing\npathogens and potentially create new ones\u2014such as novel pandemic-capable variants or de novo\nsynthesis of extinct pandemic pathogens that are no longer found outside of labs or repositories.\nResearchers can now also combine rapidly improving artificial intelligence (AI) models with wet-lab\nadvances to facilitate, accelerate and augment this work.\nWithin the biological sciences, AI models will likely provide immense benefit. They are likely to be\nemployed to improve the diagnosis and treatment of diseases, boost agricultural yields, and optimize the\nbiosynthesis of useful products, among many other uses now being explored. Such models can make use\nof and generate biological information in the form of natural language, genetic sequences, protein\nsequences, protein structure, or even more sophisticated biological complexes and genetic regulatory\narchitecture (1). Al models thus increase access to both basic biological information as well as more\ncomplex biomolecular designs and system behaviors. Indeed, biological AI models have already\nsurpassed human performance on multiple tasks, and their development is advancing rapidly (2).\nHowever, high quality biological data remains a barrier to capability advancement.\nAs with all tools that allow us to better manipulate biology, future advanced Al models have the potential\nto be misused or misapplied, and these biosecurity risks have been publicly noted by scientists and model\ndevelopers. Baker and Church note that AI protein design models are \u201cvulnerable to misuse and the\nproduction of dangerous biological agents\u201d while Boiko et al. include a security supplement in which they\nstate that they \u201cstrongly believe that guardrails must be put in place to prevent dual-use of large\nlanguage models\u201d for autonomous completion of chemical and biological synthesis protocols (3,4). Two\nauthors of this paper were coauthors on Boiko et al."}, {"title": "Dual-Use Foundation Models in Biology", "content": "Novel AI architectures, such as the transformer architecture that both GPT-4 (an LLM) and AlphaFold2\n(a biological AI model) are based upon, can be trained using immense amounts of data to create AI\nmodels that perform well across a wide range of use cases. Researchers and developers increasingly refer\nto the resulting models as foundation models (FMs) (22). A dual-use FM, as defined by the U.S.\ngovernment, refers to such FMs that \"exhibit, or could be easily modified to exhibit, high levels of\nperformance at tasks that pose a serious risk to security, national economic security, national public health\nor safety, or any combination of those matters\" (23).\nScientists are working towards developing highly capable biological FMs, where \u201cadvances in machine\nlearning combined with massive datasets of whole genomes could enable a biological foundation model\nthat accelerates the mechanistic understanding and generative design of complex molecular interactions\u201d\n(24). In an effort to mitigate dual-use and safety concerns, one academic group which developed a\ngenomic FM \u201cexcluded viral genomes that infect eukaryotic hosts\u201d from the model training data (24).\nHowever, within a few weeks other scientists fine-tuned this open-source model using a eukaryotic viral\ndataset, demonstrating that this safety measure ultimately had little real-world risk reduction (25). These\ndevelopments underscore the challenges that individual academics face regarding the pressure to publish,\nand the difficulty of developing safety measures that are neither standardized nor broadly followed.\nFurthermore, this simple modification of fine-tuning the model highlights the implications of training data\non a model's dual-use potential. Fine-tuning an open source model requires fewer computational\nresources than training a model from scratch and is thus a more accessible approach, if data for fine-\ntuning is widely available. For FMs, it is also a challenge to ensure that there is no data relevant to dual\nuse outcomes exposed to the model during training, deliberately or otherwise. To this end, recognizing the\ninevitability of dual-use capabilities in FMs is critical for developing effective containment and oversight\nstrategies.\nFuture biological FMs may one day allow users to design biological constructs, including virulent\npandemic pathogens, in a manner unknown to, and disfavored by, nature. Natural pathogen diversity has\nbeen shaped by selection pressures that favor coexistence with the host. Hyper-virulence may be\ndeleterious to a pathogen over the long term. A 2006 National Academies of Sciences, Engineering and\nMedicine (NASEM) report concluded that these evolutionary pressures \u201cmay limit our appreciation for\nthe kinds of virulence properties that might be possible in a biological agent and cause us to arrive at false\nconclusions concerning our ability to create new pathogenic agents\u201d and thus \u201cit is reasonable to\nanticipate that humans are capable of engineering infectious agents with virulence equal to or perhaps far\nworse than any observed naturally.\" In fact, there has not been enough time over the history of the earth\nfor nature to have explored more than a tiny fraction of the genetic diversity that is theoretically possible\n(26). Current genomic foundation models train on natural pathogen genomes, thus sampling\nunconditionally from such a foundation model may be expected to be similar to selecting a pathogen from\nnature at random (recognizing that natural pathogens are subject to the above described limitations). It is\npossible to imagine but difficult to concretely predict how the combination of many AI-enabled tools may\nin the time ahead enable the exploration of a vastly increased design space of novel biological and\nmolecular diversity\u2014potentially allowing users or AI agents themselves to manipulate, design and make\nbiological constructs with a precision and targeting that far exceeds current human abilities.\nFMs have shown the capability to realize experiments in the physical world. The combination of\nautomation technologies (such as benchtop gene synthesis devices) and development of capable AI robots\nand agents highlights the potential of AI models to one day surpass, and at the very least substantially\ncomplement, wet-lab efforts (4,16,21,27). For example, combining a genomic foundation model with\nhigh-volume data generation and feedback targeted towards pathogenicity characteristics may not only\nresult in a model capable of high-fidelity pathogenicity prediction, but may also pose accident risks\""}, {"title": "AI Model Evaluations for Hazardous Biological Capabilities to Date", "content": "In part to address these concerns, the governments of the United States and United Kingdom are working\nwith scientists and model developers to take new steps toward designing AI biosecurity evaluations.\nEvaluations in this context refer to techniques for assessing an Al models' capabilities, especially\ncapabilities that can cause harm (32). AI evaluations to date have included automated tasks (e.g., multiple\nchoice questions), dynamic studies in which humans or other Al models attempt to elicit harmful\ncapabilities (\u201cred-teaming\u201d), and randomized trials in which individuals or groups are set to a task with or\nwithout access to an AI model (\"human uplift studies\u201d), among other methods (32).\nNeither the UK nor US government has released standardized evaluation methods. In October 2023, the\nWhite House promulgated the Executive Order on the Safe, Secure, and Trustworthy Development and\nUse of Artificial Intelligence (AI EO) (23). The AI EO tasked various federal agencies with creating\n\u201crobust, reliable, repeatable and standardized evaluations of AI systems,\u201d with a planned focus on\nevaluating hazardous biological Al model capabilities among a small number of other significant risks.\nUnder the AI EO, the National Institute of Standards and Technology (NIST) is tasked with proposing\nsafety evaluations and red-teaming standards. NIST is charged with proposing by July 2024 \u201cguidance\nand benchmarks for evaluating and auditing AI capabilities with a focus on capabilities through which AI\ncould cause harm,\u201d including by biological means. In May 2024 the US Office of Science and\nTechnology Policy also recommended oversight of dual-use computational models that could enable the\ndesign of novel biological agents or enhanced pandemic pathogens (33).\nIn November 2023, the UK government created the AI Safety Institute, which is responsible for creating\nand conducting evaluations on leading AI models. The Institute is especially focused on \u201ccontaining risks\nthat pose significant large-scale harm if left unchecked: chemical and biological capabilities, and cyber\noffense capabilities\u201d (34). The US and UK (alongside several other nations) also signed the Bletchley\ndeclaration in November 2023, acknowledging the potential risks of AI (35).\nIn the absence of concrete government guidance, some AI LLM developers have taken a variety of\napproaches to assessing model biosecurity risks (see Table 2). Such evaluations have been varied in their\ncontent and methods. Many LLM company approaches are not transparent to the public, with companies\nciting concerns about releasing information that could increase risk or possibly violate export controls.\nSome Al developers have created their own methods for risk evaluation (36,37). Evaluations requiring\nhuman input such as red-teaming and uplift trials (6,7,38,39) are often time-consuming and expensive.\nThis has prompted interest in automated task approaches (38,40,41). Researchers have investigated\nmodels' ability to correctly answer biomedical questions (38), to aid in the acquisition and dissemination\nof anthrax and plague (7), to assist in the construction of a virus (39), and to provide accurate information\nabout dual-use research methods such as reverse genetics and immune evasion (40). No unified\nframework for the content of these biosecurity evaluations currently exists. Furthermore, the results of\nthese evaluations currently do not correspond to a shared understanding or agreement regarding the\ndegree of concern warranted for a particular capability level, something which will be needed to set\nappropriate standards, and governmental and scientific expectations for mitigation efforts."}, {"title": "Proposed Approach to Determining High-Consequence Biological\nCapabilities of Concern", "content": "Prior Experience Studying Dual-Use Life Sciences Research Can Inform AI Capabilities\nof Concern\nAs shown in Table 2, there is no common AI industry approach to evaluating biological AI models for\nrisks. There is, however, prior guidance from scientists, public health professionals, and policymakers\nregarding life sciences research that \u201ccould be directly misapplied to pose a significant threat with broad\npotential consequences to public health and safety,\u201d known as dual-use research of concern (46). There is\nalso guidance regarding life sciences research that could result in \u201cenhanced potential pandemic\npathogens\" (47). A recent update to these policies includes recommendations to address risks from\ncomputational approaches in this domain (33).\nThe Al model biosecurity conundrum-how to retain Al's significant benefits while heading off serious\nconcerns around AI model misuse\u2014finds a ready parallel with dual-use research of concern and research\nintended to create enhanced potential pandemic pathogens. Scientists and policymakers have spent years\nanalyzing which forms of life-sciences research pose serious risks through accidental release, or\ninadvertent or deliberate misuse (30,46\u201349). Although these recommendations have historically been\ntargeted at wet-lab experimentation, they have also been used to assess what information should be shared\npublicly (46,50,51)\u2014the type of concern that is critical to consider with Al models with biological\ninformation and capabilities. As the AI community develops evaluations, they should take advantage of\nthe scientific expertise and governmental experience instantiated in prior dual-use study.\nFundamentally, biological AI models aim to do in silico that which can only be done now in vitro or in\nvivo-and in doing so, make it easier for those with access to a relevant model to reduce or dispense with\ntime-consuming and expensive wet-lab work. Because biological systems are complex, traditional dual-\nuse research, e.g., studies analyzing pathogen features such as tropism, transmissibility, and virulence, has\nhistorically relied on trial and error or directed evolution (30). Al models will allow users to conduct this\nresearch faster and at lower cost (52).\nEvaluations Should Assess Capabilities, not Specific Pathogens or Threat Scenarios\nMany biosafety and biosecurity governance approaches have in the past relied on taxonomic lists of\nspecific pathogens to be regulated (53\u201355). However, we recommend that evaluation approaches instead\nfocus on AI-enabled capabilities, rather than AI engagement with risks related to specific pathogens.\nWhen applying biosecurity in practice, pathogen lists are \u201cboth too specific and too ambiguous for many\nof the uses to which they are applied", "adopting a broader perspective on the\nthreat spectrum\" and urged policymakers to recognize \u201cthe limitations inherent in any agent-specific\nthreat list.\" Biosecurity measures, the authors argued, should focus instead on the \u201cintrinsic properties of\npathogens and toxins that render them a threat, and how such properties... could be manipulated by\nevolving technologies": 26}, {"title": "Next Steps for AI Biosecurity Evaluations", "content": "The capabilities of concern we illustrate in Table 3 are generic. These high-level capabilities next need to\nbe translated into targeted, standardized evaluations. Both the US and UK governments are now working\nto create standardized evaluations (32,58). We recommend governments and model developers establish\nthese standardized evaluations such that they assess the capabilities described in Table 3 and their\npotential to simplify, accelerate or enable biological work capable of causing novel human pandemic,\nanimal panzootic, or plant pandemics, or other widespread environmental harm.\nThe specifics of evaluations will vary depending on the type and architecture of the AI model being\nevaluated (see Figure 1). Specific AI architectures, such as transformer-based LLMs, diffusion models,\nreinforcement learning agents and others, are likely to lend themselves to certain high-consequence\ncapabilities. Most evaluation methods to date have been developed specifically for LLMs; alternative\napproaches will be needed for other models, which cannot be interrogated using all LLM-specific\nmethods. For example, while an LLM evaluation may take the format of a natural language question bank\n(40), a biological AI model evaluation may take the format of computational tasks.\nThis work will require determining how to concretely measure these capabilities, otherwise evaluations\nwill risk being overly subjective and inconsistent. Accurate measurement is value-neutral; if\nmeasurements determine that the vast majority of biological AI models cannot meaningfully achieve\nthese capabilities of concern, this would be extremely informative to the biosecurity community. One\nadvantage of the aforementioned question and task-based approaches, versus evaluation methods that rely\nheavily on human judgement, is the potential for these methods to be standardized and measurable.\nAs mentioned above, a global consortium of biological AI model developers recently adopted the\nResponsible AI x Biodesign statement of community values and commitments (15). Signatories\ncommitted to developing pre-release evaluations to assess potentially dangerous model capabilities,\nthough these capabilities were not defined, and standards for how best to conduct rigorous evaluations\nwere not discussed. Current norms and incentives in academia push towards open-source release of model\nweights, which presents additional oversight challenges."}, {"title": "", "content": "AI developers must be able to clearly identify when a model has reached a scientifically agreed upon\nthreshold of unacceptable risk. Furthermore, the suite of risk-mitigation actions that can be taken once\ncapability thresholds are met needs to be explicated. Biological AI model risk mitigation measures will be\ndistinct from those focused on mitigating wet-lab risk dual use risks. Examples of prevention and\nmitigation strategies for Al models under consideration include: removing dangerous information from a\nmodel after the initial training has been completed (40), restricting access to a model to specific users via\nAPIs or other secure means, and/or subjecting models to governmental risk-benefit assessment. AI\ndevelopers should disclose risk-mitigation requirements before training and testing relevant models to\nreassure the public that new AI models that pose serious biological risks will not be publicly released.\nGiven the speed of AI technological advances, assessments of real-world risk can no longer be expected\nto be static or unchanging over long periods of time, and developers and policymakers must regularly\nupdate their risk thresholds by drawing on the results of evaluations. Ultimately, the goal of biosecurity\nevaluations for AI models should be to provide targeted risk-reduction of high-consequence harms.\nCriteria for evaluations should be clear and standardized, allowing for beneficial research to easily\nproceed without undue impediment. We hope that these proposed categories of high-consequence\ncapabilities can be used to help set standardized biosecurity evaluations for AI models. We encourage\nscientists, Al developers, and policymakers to create international standards and requirements for the\ndevelopment of safe and secure Al systems."}]}