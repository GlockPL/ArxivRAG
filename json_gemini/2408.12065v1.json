{"title": "Transformers As Approximations of Solomonoff Induction", "authors": ["Nathan Young", "Michael Witbrock"], "abstract": "Solomonoff Induction is an optimal-in-the-limit unbounded algorithm for sequence prediction, representing a Bayesian mixture of every computable probability distribution and performing close to optimally in predicting any computable sequence.\nBeing an optimal form of computational sequence prediction, it seems plausible that it may be used as a model against which other methods of sequence prediction might be compared.\nWe put forth and explore the hypothesis that Transformer models - the basis of Large Language Models approximate Solomonoff Induction better than any other extant sequence prediction method. We explore evidence for and against this hypothesis, give alternate hypotheses that take this evidence into account, and outline next steps for modelling Transformers and other kinds of AI in this way.", "sections": [{"title": "Introduction", "content": "It has become something of a clich\u00e9 for papers in the field of Artificial Intelligence to begin by recognising the rapid progress the field has made in the past few years; progress made possible by the ubiquitous Transformer architecture.\nTransformers - the basis of Large Language Models (LLMs) - seem to scale without limit, achieving greater performance with greater size and greater amounts of training data.\nHowever, despite their power and ubiquity, Transformers are still poorly understood. Many papers have been written and experiments carried out in pursuit of making these models interpretable, understandable, and explainable, with some success in particular, identification of patterns identified by particular neurons [15] [2]. But such work is largely ad hoc, with no common approach. The field of interpretability, as well as AI safety research in general, remains preparadigmatic.\nHaving a standardised way of analysing the performance or structure of black box algorithms like Transformers requires a standard model to compare against - some generalisation of Transformers and other sequence predictors, of which all other models can be considered departures. One class of such models are"}, {"title": "Background", "content": "We will begin by outlining, in short, how Solomonoff Induction works.\nWe first consider a Universal Turing Machine, or UTM, which we call M, with a unidirectional input tape, a bidirectional working tape, and a unidirectional output tape. For simplicity, we will assume that M uses a binary alphabet, but these results generalise to any alphabet. Any input string s given to M will result in some output string x. Since M is capable of simulating any Turing Machine (TM), there exists an s that will generate any arbitrary x. The a priori probability of s being given as input to M is considered to be the length l(s) of s in bits. Since M is likely to give x on many different inputs, we consider the set of minimal input strings S that produce x when given to M (possibly followed by other characters)."}, {"title": "", "content": "The probability P (with respect to machine M) of a string x is given as:\n$P_M(x) = \\sum_{s \\in S} 2^{-l(s)}$\nThe smallest such l(s) will usually make up the majority of this sum, so considering only the smallest description of a given program gives a reasonable approximation.\nThe probability of a given continuation y of x - in other words, the probability of the output string beginning with xy, given that M has only produced x so far is simply:\n$P_M(xy | x) = \\frac{P_M(xy)}{P_M(x)}$\nIn this way, Solomonoff Induction represents a Bayesian prior probability on every computable string, with each new symbol in the input string triggering a Bayesian update.\nAn equivalent form of SolInd, given in [13], considers Probability Evaluation Methods (PEMs) instead of TMs, updating probability weights on particular inputs rather than eliminating those that do not produce x. This conception of SolInd gives equivalent results to the version given above.\nSolInd's speed of converging on any given sequence depends on the choice of UTM - if a sequence x has a shorter description on M than on some other UTM N, then M will make less prediction error on x than N will. This has limited importance in the limit, though it may be a vital consideration in practice.\nSolInd is considered optimal in the following ways:\nBrounded error: the amount of prediction error SolInd will make on any given sequence (measured in log odds to base 2, making errors comparable with bits) is guaranteed to be less than P(x).\nUniversality: SolInd's predictions will converge to optimality on every computable string.\nPareto optimality: while other prediction schemes can predict some sequences better than SolInd, it is impossible for any method to predict a sequence better than any SolInd without predicting some other sequence worse."}, {"title": "Hypothesis", "content": "We now outline our key hypotheses, beginning with our reasoning and ending with their significance to AI research."}, {"title": "Reasoning", "content": "The basis of this paper was perhaps best expressed by Solomonoff himself in a paper published 32 years after his initial description of SolInd:[14]\nJust about all methods of induction can be regarded as approximations\nto [SolInd], and from this point of view, we can criticize and optimize\nthem. Criticism is always easier if you have an idealized (possibly unre-\nalizable) standard for comparison. [SolInd] provides such a standard.\nIn other words, it makes sense to analyse any sequence prediction method by comparing it to SolInd, considering any architecture not found in SolInd to be an optimisation that helps it achieve performance closer to SolInd's theoretical ideal.\nOur main consideration is in comparing Transformer models to SolInd, since they currently represent an unopposed state-of-the-art in almost all fields of machine learning and artificial intelligence; however, the intention is to introduce a way of understanding every method by which continuations of sequences can be predicted, including all those invented to the present day and all those which might be invented in the future. Indeed, it is necessary to compare Transformers' approximations of SolInd to other prediction methods if we are to understand how they achieve their superior performance."}, {"title": "Statement of hypotheses", "content": "We now outline our main hypotheses that we will interrogate in this paper and in future work:\n1. Transformers can be modeled as approximations of Solomonoff Induction.\n2. Transformers approximate Solomonoff Induction more closely than other methods of sequence prediction (e.g. other neural networks).\n3. Transformers can be modeled as approximations of Solomonoff Induction at test time specifically.\n4. Stochastic gradient descent approximates Solomonoff Induction during training time, and does this better than other training schemes.\nThe final hypothesis recognises a key difference between SolInd and machine learning systems; Solomonoff inductors are not 'trained' in the same way as NNs, and therefore must be thought of in a slightly different way. For example, when comparing SolInd and an NN, we might consider the SolInd input string s to contain the entire training corpus used to train the NN. Therefore, we must consider how a NN's training approximates SolInd, as well as its operation at runtime."}, {"title": "Significance", "content": "We expect confirmation and/or refutation of our hypotheses to be consequential for the way Transformers and other machine learning systems are understood. Some possible implications of our hypotheses include:"}, {"title": "Findings in favour", "content": "In this section, we will present several findings that support and shed light on our hypotheses."}, {"title": "Universality", "content": "One key result on which our hypothesis relies is the idea that Transformers - and NNs in general can implement arbitrary programs (equivalently, can emulate arbitrary TMs). That Recurrent Neural Networks (RNNs) can emulate arbitrary TMs was first shown in 1992 by Siegelmann and Sontag [12]. This result has since been replicated for many other machine learning models; indeed, Transformers have had multiple papers showing distinct ways in which they might emulate arbitrary TMs.[1][10]\nTherefore, one way that Bayesian mixtures of TMs could be emulated using a NN is for them to be simply averaged; given a probability distribution over a set of TMs, construct emulations of each on a NN of a certain size and give each weight in the NN its average value among the emulations, weighted by that emulation's probability mass. It is possible that the class of weights that can be achieved by weighted-averaging emulated TMs in this way is equivalent to the class of all randomised weights that can be given to a NN."}, {"title": "Decomposition", "content": "There exists a substantial amount of research into NN decomposition [9]; that is, taking a NN trained for some task (or set of tasks) and identifying smaller subnetworks that can perform the task without the rest of the network (or perform one of the set of tasks, with other parts of the network possibly performing other tasks).\nOne well-supported hypothesis in this space is the lottery ticket hypothesis [5]: that randomly-initialised networks contain subnetworks that can be achieve similar performance to the whole network if trained in isolation, having been given random values that quickly reach a high level of performance. This implies that such \"winning tickets\" exist as part of any random initialisation of any NN, for every function that that NN is capable of emulating. Further, since a RNN can emulate a UTM with as few as 1058 neurons [12] many fewer than modern deep NNs - most functions can be reasonably considered to be emulable by most NNs.\nIn other words, there is good reason to believe that randomly-initialised networks contain something close to a representation of any arbitrary TM, the most relevant of which can be found during training and given increased weight over other subnetworks. This seems to closely mirror SolInd's process of induction."}, {"title": "Findings against", "content": "In this section, we will explore findings that seem to contradict or complicate our hypothesis. Of course, computable instantiations of Solomonoff Induction are impossible, so the strongest version of our hypothesis, that Transformers act as Solomonoff Inductors, cannot be absolutely true; any attempt to approximate SolInd will be subject to limitations. These findings can be thought of as exploring and naming these limitations."}, {"title": "Limits of stochastic gradient descent", "content": "Computational tasks can be sorted into four categories: regular, context-free, context-sensitive, and recursively enumerable. These categories each represent an increase in the level of complexity required to complete them, forming the Chomsky hierarchy. [3]\nAfter training on tasks at varying levels of the Chomsky hierarchy, Del\u00e9tang et al. [4] found that Transformers can generalise well to regular tasks, but suffer greater performance penalties on tasks that are further up on the Chomsky hierarchy - eventually leading to recursively enumerable tasks, which correspond to those that can be solved by Turing machines. Other kinds of NNs tested exhibited similar behaviour. The authors speculate that this may be due to the limitations of stochastic gradient descent as a training technique - when weights are gradually changed to approach an ideal value, small imperfections can accumulate."}, {"title": "Computational limits in practice", "content": "NNs failing to generalise may also be explained by a fundamental limitation of the way they emulate TMs.\nTo emulate arbitrary Turing machines, an infinitely large tape is required. This can be achieved in NNs with infinite width, but is more commonly achieved by using the value of particular neurons as memory, with the decimal expansion of the neuron's value (in some base) forming a stack. For this method to achieve arbitrary memory size requires unlimited precision, which is rarely achieved in practice. This may or may not be a relevant limitation in practice; it does not stop finitely large computers from running useful programs, but it may shrink the amount of memory available to a NN below what is needed for many programs.\nAnother fundamental limitation is that of state memory. If arbitrarily many NNs that emulate TMs are averaged to give a new NN that has the average weights of each (possibly weighted by their probability mass in a distribution, as outlined above), the resulting NN may be able to evaluate the composite transition function in finite time and space, but cannot track arbitrarily many state variables in finite space. This may prevent the SolInd emulation method given above of averaging weights among emulated TMs in a given probability distribution, since their states will, without any extra work, all be tracked in the same set of neurons. This can be worked around if the TMs' states share meaningful information, but this is not a trivial requirement."}, {"title": "Transformers make poor Solomonoff Inductors", "content": "Finally, an attempt was made by Grau-Moya et al. [6] to train a Solomonoff Inductor using data sampled directly from a UTM. They trained multiple kinds of machine learning systems, including Transformers. (This is an example of meta-learning; that is, training systems that learn new tasks quickly instead of simply training them on the tasks themselves. This is distinct from our work, which seeks to show that Transformers already have this capability to at least some degree without training.)\nAmong the models tested in this paper, Transformers often learnt best of all models tested when tested on sequences of lesser or equal length to that given to them at training time. However, when given longer sequences, their performance often became the worst of any model. This suggests that Transformers may not be suited to learning and/or emulating arbitrary TMs in practice."}, {"title": "Syntheses and alternate hypotheses", "content": "In this section, we will propose some alternate hypotheses, which are more complicated than those previously outlined but take the above findings into consideration."}, {"title": "Alternate Solomonoff models", "content": "We begin by exploring models that utilise the key insights of SolInd, while taking into account the fact that Transformers may be poor at (and possibly even incapable of) emulating arbitrary TMs.\nSolInd represents a Bayesian mixture over all inputs to a UTM, and therefore all Turing machines. But there is no reason why we cannot consider similar probability distributions over alternative methods of computation, lower on the Chomsky hierarchy.\nThe idea that Transformers can more easily approximate simpler models than TMs, as discussed before in the context of Del\u00e9tang et al.'s work [4], was also explored by Liu et al. [8], who found that Transformers are well-suited not only to learning functions that can be implemented in finite-state automata (i.e. regular languages), but to finding shortcuts to them that can yield identical solutions in logarithmic time compared to executing the automaton - that is, if the automaton would take T steps to yield a solution, Transformers can learn to find the same solution in O(T) time.\nIt may therefore make more sense to think of Transformers as a mixture of finite-state automata rather than TMs, or possibly other models of computation like Markov chains, pushdown automata, or bounded Turing machines.Since Turing machines are capable of simulating every possible form of computation, including these, it may still make sense to consider decomposing Transformers into TMs, if not all TMs."}, {"title": "Alternate hypotheses", "content": "In light of these considerations, we add the following to our list of hypotheses:\n5. Transformers can be modelled as Solomonoff inductors with limited memory, preventing them from emulating all TMs and making it harder for them to solve tasks higher on the Chomsky hierarchy.\n6. Transformers can be modelled as Bayesian mixtures of simpler computational schemes than the class of all TMs.\n7. The further down a model of computation is on the Chomsky, the more quickly it is converged upon by stochastic gradient descent, leading Transformers to give much more weight to some classes of TM than others.\nIt should be noted that these hypotheses are not all mutually exclusive, and that some middle ground may well explain Transformers better than any individual hypothesis alone. For example, Transformers might represent a probability"}, {"title": "", "content": "distribution over all finite-state automata, as well as executing some TMs that evaluate recursively-enumerable functions that simpler automata cannot approximate.\nStill, it is useful to consider which hypotheses have more explanatory power - are Transformers more like TM-infused departures from mixtures of finite-state automata, or UTMs that encode automata well but can still approximate any TM with enough time and space? These two composite hypotheses would prescribe different implications towards explaining and decomposing Transformers, and so must be separated and carefully considered."}, {"title": "Conclusion", "content": "In this paper, we have proposed that Transformers and other NNs should be analysed under the lens of Solomonoff's optimal unbounded sequence prediction scheme, Solomonoff Induction. Since SolInd represents the upper bound of what any sequence predictor can be capable of, it likely has extensive prescriptive and descriptive power with respect to NNs, but little work has been done to utilise this.\nWe have outlined reasons why this way of modelling NNs may be helpful in understanding how they work and achieving similar performance using algorithms that are less inscrutable and easier for programmers to work with. We have given several hypotheses as to how Transformers may approximate or relate to Solomonoff Induction.\nWe have outlined some relevant findings, showing that our hypotheses seem to be reflected in NNs in some ways but are complicated by computational limitations; we have then taken these findings into consideration to develop more complicated hypotheses that may paint a fuller picture of how Transformers approximate SolInd's optimal sequence prediction in practice.\nWe hope that future work in this area will help establish a common framework under which NNs can be considered and analysed."}, {"title": "Future work", "content": "There is much subsequent work to be done in this area towards investigating our hypotheses in more detail. The following may be useful next steps:\nInvestigate whether Transformers can find shortcuts to, or better representations of, models of computation in between finite-state automata and TMs on the Chomsky hierarchy\nDetermine whether sets of Transformers or other NNs of a given size that have been constructed to emulate TMs can be averaged to yield arbitrary weights, confirming that all NNs directly correspond to probability distributions over TMs\nExplore ways of decomposing Transformers into subnetworks that seem to emulate multiple functions using shared memory"}]}