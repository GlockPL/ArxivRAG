{"title": "Reverse-Engineering the Reader", "authors": ["Samuel Kiegeland", "Ethan Gotlieb Wilcox", "Afra Amini", "David Robert Reich", "Ryan Cotterell"], "abstract": "Numerous previous studies have sought to determine to what extent language models, pre-trained on natural language text, can serve as useful models of human cognition. In this paper, we are interested in the opposite question: whether we can directly optimize a language model to be a useful cognitive model by aligning it to human psychometric data. To achieve this, we introduce a novel alignment technique in which we fine-tune a language model to implicitly optimize the parameters of a linear regressor that directly predicts humans' reading times of in-context linguistic units, e.g., phonemes, morphemes, or words, using surprisal estimates derived from the language model. Using words as a test case, we evaluate our technique across multiple model sizes and datasets and find that it improves language models' psychometric predictive power. However, we find an inverse relationship between psychometric power and a model's performance on downstream NLP tasks as well as its perplexity on held-out test data. While this latter trend has been observed before (Oh et al., 2022; Shain et al., 2024), we are the first to induce it by manipulating a model's alignment to psychometric data.", "sections": [{"title": "Introduction", "content": "Language comprehension is thought to be predictive and incremental. Research on reaction times (Fischler and Bloom, 1979), fixation patterns (Ehrlich and Rayner, 1981), and brain activations (Kutas and Hillyard, 1984; DeLong et al., 2005) suggests that comprehenders anticipate upcoming linguistic units based on the context in which they occur (Kuperberg and Jaeger, 2016). In addition, a large body of evidence shows that when linguistic units are unexpected, they require more cognitive effort to process (Miller and McKean, 1964; Ehrlich and Rayner, 1981; Balota et al., 1985, inter alia). E.g., reading times of units, e.g., morphemes, words, and sentences, are taken as a measure of cognitive effort, i.e., the less likely the unit is in context, the longer it takes to read (Smith and Levy, 2013).\nIn this study, we are interested in reverse-engineering the part of the language processing system responsible for predicting abstract linguistic units and testing it by measuring its ability to predict reading times. To do so, we need, first, to establish a theoretical link between predictability and reading times. For this, we draw on surprisal theory (Hale, 2001; Levy, 2008), which posits that the cognitive effort to process a unit is proportional to its surprisal\u2014the unit's negative log probability given the preceding context. Implicitly, surprisal theory assumes that a comprehender maintains a probability distribution over upcoming units, i.e., it assumes a human language model. However, this human language model is a theoretical construct, and cannot be observed directly. Thus, most previous work that tests surprisal theory has done so using probability estimates derived from a language model trained on large swathes of human-written text. Under this paradigm, it has been observed that surprisal estimates derived from language models, fit with regularized maximum-likelihood estimation on large corpora, do yield significant predictors although these findings vary based on the quality (Goodkind and Bicknell, 2018; Wilcox et al., 2020, 2023b), size, and training-data set size (Oh et al., 2022; Shain et al., 2024) of the model. One current line of research therefore seeks to uncover what characteristics of pretrained LMs produce better predictors of human reading times, and what this tells us about the human language processing system.\nOur paper asks a simple question: Instead of assessing the ability of pretrained LMs to serve as psycholinguistic predictors, can we directly estimate (or fine-tune) a language model so that its surprisal estimates become better predictors of processing effort for linguistic units? We frame this problem as one of aligning the language model to human data (Christiano et al., 2017; Schulman et al., 2017; Ouyang et al., 2022; Ziegler et al., 2020; Rafailov et al., 2023). However, in contrast to much previous work, which uses human-model alignment to obtain improvement on natural language processing tasks, e.g., summarizing text (Stiennon et al., 2020) or producing non-toxic outputs (Li et al., 2024), we seek to align LMs to be better psychometric predictors. Particularly, we aim to directly align models to human reading data by optimizing the parameters of the statistical models typically used to evaluate the psychometric fit. While recent approaches like direct preference optimization (DPO; Rafailov et al., 2023) are designed to optimize a model's parameters based on human preferences, they rely on pairwise preference data, which is not applicable to real-valued psychometric data. Thus, we propose a novel alignment technique that allows us to directly optimize the language model's parameters in such a way that it serves as a better predictor of real-valued psychometric data. Specifically, we fine-tune the language model to implicitly optimize the coefficients of a linear regression that predicts the reading time of an individual unit.\nWe test our technique on three English-language reading datasets and find that it increases the statistical fit of a linear regressor in terms of the likelihood it assigns to reading times on a held-out test set. We also observe a positive relationship between a model's psychological predictive power and its perplexity. While it has been observed that better LMs are better psychological models of reading up to a point (Goodkind and Bicknell, 2018; Wilcox et al., 2020, 2023a), after a certain model size, their fit to human reading times decreases (Oh et al., 2022; Shain et al., 2024). In other words, beyond an inflection point, better LMs are worse predictors of human reading times. Through our alignment procedure, we are able to demonstrate the contrapositive, namely that as we causally make our language models' outputs more aligned with reading, they become worse at predicting the next word."}, {"title": "Psycholinguistics Background", "content": "Put concisely, the goal of this paper is to reverse-engineer pH, a person's internal language model from psychometric data collected through experimentation. Our reasoning is as follows: if we can align an existing language model po to more accurately predict such psychometric data, pe will also more closely resemble PH."}, {"title": "Language Models", "content": "Let \u2211 be an alphabet, i.e., a finite, non-empty set, and let \u03a3 = \u03a3 \u222a {EOS} be the alphabet augmented with a distinguished end-of-string symbol not in \u03a3. A language model pe is a probability distribution over \u03a3*, which is the set of all strings over \u03a3. Further, following Opedal et al. (2024), we define the normalized prefix probability\n$\\pi_{\\theta}(c) \\stackrel{\\text{def}}{=} \\frac{1}{Z_{\\pi_{\\theta}}} \\sum_{u \\in \\Sigma^*} p_{\\theta}(cu),$ (1)\nwhich is a probability distribution over prefixes c \u2208 \u03a3*, where cu is the concatenation of c and u. The normalization constant $Z_{\\pi_{\\theta}} = 1 + \\sum_u p_{\\theta}(u)|u|$ ensures that all probabilities sum to one. Here |u| denotes the length, i.e., the number of units in a string u. Note that Eq. (1) is only well-defined in an LM with finite expected length."}, {"title": "Psychometric Measurements", "content": "Let \u03c8(u, c) \u2208 R denote a measurement for a unit u \u2208 \u03a3 appearing in context c \u2208 \u03a3*. In this paper, \u03c8(u, c) represents various reading time measurements for a given unit, such as gaze duration, first fixation duration, and total fixation duration, which are standard approximations to the processing effort of a linguistic unit in context (Miller and McKean, 1964; Just and Carpenter, 1980; Frazier and Rayner, 1982; Rayner, 1998, inter alia)."}, {"title": "Surprisal Theory", "content": "Surprisal theory furnishes us with an easy-to-compute predictor of processing effort that is derived from a pretrained language model. Formally, surprisal theory predicts that the time it takes to process a linguistic unit u \u2208 \u03a3 in context c \u2208 \u03a3* is an affine function of the unit's contextual surprisal under the human language model pH, defined as\n$\\mathcal{H}(u | c) \\stackrel{\\text{def}}{=} -\\log_2 p_H (u | c).$ (2)\nSurprisal theory has been supported by numerous empirical studies, which have found that surprisal is predictive of reading times across multiple datasets"}, {"title": "Linear Modeling", "content": "We now discuss how empirical support for surprisal theory is typically adduced. Following previous work, we assume an affine function links a linguistic unit's contextual surprisal and that unit's reading time (Smith and Levy, 2013; Wilcox et al., 2023b; Shain et al., 2024) and apply linear regression to predict reading times based on contextual surprisal. In mathematical jargon, both the psychometric measurements \u03c8(u, c) : \u03a3 \u00d7 \u03a3* \u2192 R and our predictors x\u03b8(u, c) : \u03a3 \u00d7 \u03a3* \u2192 RD are real-valued random variables. In the case of the predictor, given a unit \u03b7 \u2208 \u03a3 and a context c \u2208 \u03a3*, we define the predictor as a D-dimensional real column vector\nx\u03b8(u, c) = [10(u | c), x2, . . ., xD]T, (4)\nwhich, as depicted, includes our surprisal estimate 10(u | c); the additional variables x2, . . ., xD are considered to be baseline predictors and are chosen at the modeler's discretion depending on what they seek to test. Given a parameter (column) vector Bo \u2208 RD, we define the following linear model\n\u03c8(u, c) \u223c fpo (\u00b7 | \u0445\u03b8(\u0438, \u0441))\n= N(\u03a6\u03b2\u03bf (\u0438, c), \u03c3\u00b2), (5b)\nwhere the linear function\n\u03a6\u03c1\u03bf (\u0438, \u0441) = \u0445\u03b8(\u0438, \u0441) \u0422\u0432\u043e (6)\nconstitutes the mean and o2 is the variance.\nWe evaluate the predictive power of our model by fitting f\u00df, on a training set and measuring the log-likelihood of the test set; higher log-likelihood indicates greater predictive power of the model. To assess how much surprisal contributes to the predictive power, we fit two regression models based on two predictors. The baseline predictor \u0445\u044c (\u0438, \u0441), defined identically to Eq. (4), but with the estimated surprisal zeroed out, typically consists of a unit's unigram surprisal, i.e., its negative log unigram probability and a unit's length (in characters). The target predictor, denoted by xo(u, c), includes the same set of baseline predictors together with the estimated surprisal of the unit u.\nTo quantify the predictive power of contextual surprisal, we compute the delta log-likelihood Allh between the two models, which is the average unit-level difference in log-likelihood assigned by the two predictors to the reading time measurements. For a single unit-context pair, we compute:\nAllh(u, c) = log f\u00df, (\u03c8(u, c) | x\u0473(\u0438, \u0441))\n\u2212 log f\u00f8 ((u, c) | \u0445\u044c(\u0438, \u0441)), (7)\nwhere Bo and Be are the coefficients for the baseline and target models, respectively, and are estimated separately. Intuitively, a higher Allh indicates that the estimated surprisals contribute more to the predictive power or psychometric accuracy of the model over reading times, compared to the baseline predictors (Frank and Bod, 2011).\nHaving established a metric, delta log-likelihood, to measure how much contextual surprisal contributes to predicting reading times, we are now ready to answer the question we posed at the beginning: Can we fine-tune a language model such that surprisal estimates derived from it become better predictors of reading times?"}, {"title": "Aligning LMs to Psychometric Data", "content": "As discussed in \u00a72.4, the psychometric predictive power of a language model is typically evaluated by assessing the predictive power of surprisal estimates (u | c) of a linguistic unit u in a context c with respect to human reading times. Rather than evaluating a language model's psychometric predictive power, in this study, we ask whether we can fine-tune language models to increase their psychometric predictive power.\nWe treat this as an alignment problem (Christiano et al., 2017; Schulman et al., 2017; Ouyang et al., 2022; Ziegler et al., 2020; Rafailov et al., 2023). Let Pref denote a pretrained language model that will serve as a reference. Further, let po denote the language model we seek to fine-tune, generally initialized to Pref. Our goal is to align pe such that its surprisal estimates are more directly correlated with psychometric data in comparison"}, {"title": "Deriving an Objective", "content": "Reward Function. We draw inspiration from direct preference optimization, which implicitly optimizes the parameters of a reward model for predicting the outcomes of pairwise comparisons between items. However, rather than implicitly fitting a Bradley-Terry model (Bradley and Terry, 1952) to model pairwise preferences given by human annotators, we implicitly fit a linear regressor fpe to model psychometric measurements. Following the notation developed in \u00a72.4, let xe (u, c) denote the predictor vector, which includes the contextual surprisal derived from the model pe. To optimize the parameters of f\u00df, we define our reward function as the negative minimum of the expected mean squared error (MSE) between the observed psychometric data \u03c8(u, c) and the predicted values \u0424\u0440. (\u0438, c) = xo(u, c) TB, as defined in Eq. (6). The reward is then given by\nr(0) def min E\nBERD (u,c)~Tref\n(\u03c8(\u0438, \u0441) \u2013 \u0424\u0440\u043e (\u0438, \u0441)) 2, (8)\nwhere ref, as defined in Eq. (1), is the normalized prefix probability. Note that under our linear model specified in Eq. (5), maximizing r(0) is equivalent to maximizing the Allh in Eq. (7).\nRegularization with KL Divergence. To prevent the fine-tuned model pe from diverging excessively from the pretrained reference model pref, we regularize our objective with the Kullback-Leibler (KL) divergence, as is typically done in RLHF (Schulman et al., 2017; Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022). More specifically, the regularization term is defined as:\n\u03c6(\u03b8) def E KL(pref(| c) || po( | c))\n= \u0395 Pref(u | c) log(9a)\nPref(u | c)(9b)\nwhere ref is the normalized prefix probability of the reference distribution Pref."}, {"title": "Putting it All Together", "content": "We now combine the reward and the KL regularization to define an objective for aligning LMs to psychometric data as\nJ(0) (0) \u2013 \u03bb. \u03c6(\u03b8), (10)\nwhere \u03bb \u2265 0 is a hyperparameter, which determines the strength of the KL regularization. Because optimizing r(0) corresponds to optimizing Allh, J(0) trades off better alignment with human psychometric data against the KL divergence from the pretrained model Pref."}, {"title": "Approximation of the Reward Function", "content": "In practice, we use a Monte Carlo estimate of N unit-context pairs (Un, Cn) ~ #ref to approximate the expectation in Eq. (8). Let \u03c8 [\u03c8(41, C1),..., \u03c8(UN,CN)]T \u2208 RN denote the real column vector of N reading time observations. Then we define the approximate reward as\ndef\n1\n(0) min \u03a3(\u03c8(Un, Cn) \u2013 \u03a8\u03b2\u03bf (Un, Cn)) 2\nBOERDN\n=1(11a)\nmin 1(11b)\nwhere X is an N \u00d7 D real matrix, with each row corresponding to a predictor vector, as defined in Eq. (4). Leveraging a well-known closed-form solution (see App. A.1), we directly compute Eq. (11b). To ensure that X X is invertible, we add a small regularization term pI with p > 0, leading to the following coefficients\nB = (XX + \u03c1\u0399)\u00af\u00b9\u03a7\u03c8.(12)\nThis results in the simple reward term:\n1(13)\nNotably, the optimal coefficients are now parameterized by the language model's parameters 0."}, {"title": "Experimental Design", "content": "In this section, we discuss how we experiment with fine-tuning a language model using the objective defined in Eq. (10) and the reward approximation"}, {"title": "Models", "content": "We use the GPT-2 family of models (Radford et al., 2019) and conduct experiments on the small, medium, and large versions of the model available on the HuggingFace hub (Wolf et al., 2020)."}, {"title": "Data", "content": "While our objective given in Eq. (10) assumes unit-context pairs sampled from pref, we lack psychometric data for LM-generated text.we take to be However, future work should investigate this assumption more thoroughly. We fine-tune and evaluate models on three widely used eye-tracking corpora: The Dundee Corpus (Kennedy et al., 2003), which includes eye-movement data from 10 English-speaking participants reading 2368 sentences of newspaper articles from The Independent, the Provo Corpus (Luke and Christianson, 2018), which contains eye-tracking data from 84 participants who read 55 paragraphs of texts from various sources, including fiction and non-fiction, and the ZuCo Corpora (ZuCo 1.0 (Hollenstein et al., 2018) and ZuCo 2.0 (Hollenstein et al., 2020)), which contain data from 12 and 18 participants, respectively, reading sentences from Wikipedia articles and movie reviews.\nSimilar to previous work (Wilcox et al., 2020, 2023b), we focus on the gaze duration, defined as the total time of a reader's first pass fixations on a unit u before they fixate on a different unit; see App. B.1 for details. In addition, we conduct experiments using the total reading duration and first fixation duration (App. D.1). We further verify that our results are not due to random effects through additional experiments with random reading times (App. D.2). We create test sets by sampling 40% of the data from each corpus. Then, to construct various test sets, we randomly sample 70% of the remaining 60% of the data according to 3 random seeds. We can view this procedure as a simple bootstrapping procedure, from which we can approximate error bars (Efron, 1979). We fine-tune and evaluate models on all pairs of eye-tracking corpora, resulting in 9 unique data splits as shown in Tab. 4."}, {"title": "Fine-Tuning", "content": "We compute the contextual surprisal of each unit in a sentence, excluding units with zero reading times and zero frequencies, during fine-tuning. It is common practice in psycholinguistics literature to drop words that were skipped on the first pass during reading and, therefore, have a first pass reading time of zero (Smith and Levy, 2013; Oh and Schuler, 2023b). Across all experiments, the predictor Xe consists of a unit's contextual surprisal, its unigram surprisal, estimated using Speer's (2022) toolkit, and its length. We opt not to include coefficients for spillover. We do so because reading times in eye-tracking studies have been observed to be less susceptible to spillover than other reading modalities, for example, self-paced reading (Shain and Schuler, 2021). Although, we acknowledge that this is a limitation of our study. For all configurations, we fine-tune models for 5k steps and repeat"}, {"title": "Results", "content": "We now return to our main question: Does our objective align language models more closely to human reading times compared to pretrained models? We first analyze the effect of maximizing the unregularized reward and later, in \u00a75.4, extend our analysis to the KL-regularized objective."}, {"title": "Predicting Reading Times", "content": "Are fine-tuned models pe better predictors of reading times compared to pretrained models Pref? To answer this question, we examine both the mean square error (MSE) and the delta log-likelihood Allh computed at each evaluation step on the test data using cross-validation. While the Allh is our main metric for measuring a model's psychometric accuracy, we use the MSE to compare the magnitude of prediction errors. The MSE is computed similarly to the unregularized objective, given in Eq. (10), except we calculate the MSE through cross-validation on the entire test set using linear regression to evaluate the predictive power of surprisal estimates.\nAs visualized in Fig. 1, the MSE values are relatively high because our reading times are in mil-"}, {"title": "Coefficient Estimates", "content": "Additionally, we want to know what the fine-tuned models have implicitly learned about the role of surprisal, as well as the baseline features, over the course of fine-tuning. To examine this, in Fig. 3, we visualize the regressor coefficients Be from cross-validation on the whole test set. We observe the following tendencies: The coefficients for a unit's contextual surprisal and length are positive, indicating that, as units become less predictable and longer, they take more time to read.\nThe positive coefficient for unigram surprisal means more frequent units take less time to read."}, {"title": "Perplexity vs. Quality", "content": "Several recent papers have found that, above a certain size, as a language model's perplexity decreases, its predictive power increases (Oh et al., 2022; Shain et al., 2024). Follow-up work has suggested that this is due to the super-human predictive abilities of the models, especially for low-frequency nouns such as named entities (Oh and Schuler, 2023b). An open question remains regarding the causal factors behind these dynamics. So far, studies have tested the relationship by manipulating a language model's quality either by choosing different sizes of pretrained models as in Oh and Schuler (2023b) or by training successively smaller and smaller models as in Wilcox et al. (2023a). However, our fine-tuning methods allow us to flip the causal arrow. As we make a language model more closely aligned with human reading times, what happens to its quality? To investigate this question, in Fig. 2, we show the perplexity and Allh both at the start of fine-tuning and when the model achieves its maximum Allh, which is typically near the end of fine-tuning. We find that increases in Allh generally correspond to increases in perplexity. These results indicate that as we make a language model's predictions more aligned with reading times, it becomes worse at modeling text."}, {"title": "Kullback\u2013Leibler Regularization", "content": "Next, we analyze the effect of adding KL regularization to our objective in Eq. (10). Specifically, we compare the trajectories of \u25b311h, KL divergence and log perplexity for KL coefficients \u03bb\u03b5 {0,5, 50, 500}. The coefficients used in this paper are larger than the ones normally used in RLHF studies (Schulman et al., 2017; Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022) because of the large magnitude of our reward in Eq. (13), which is the negative mean squared error. Fig. 4 shows a clear trend: Higher coefficients lead to lower increases in perplexity and KL divergence, as well as lower increases in Allh. We observe higher coefficients reduce the divergence of pe from their initial distribution pref. While higher coefficient come at the cost of lower Allh increases, we still observe consistent increases over the baseline; see Fig. 9 for results on all data splits with \u03bb = 500."}, {"title": "Additional Analyses", "content": "Previous work has found that language models fine-tuned on cognitive data, such as eye tracking (Yang and Hollenstein, 2023; Deng et al., 2023) and brain data (Toneva and Wehbe, 2019) can improve performance on downstream NLP tasks. In this section, we evaluate our fine-tuned language models-based on the lowest loss on the test dataset-on sev-"}, {"title": "Targeted Syntactic Evaluation", "content": "We evaluate models on the Benchmark of Linguistically Minimal Pairs (BLiMP; Warstadt et al., 2020).9 BLIMP assesses whether language models' behavior is consistent with human preferences for grammatical sentences across a range of grammatical phenomena. In BLiMP, items come in grammatical and ungrammatical variants; we report model accuracy for assigning a higher probability to the grammatical version. Models' scores on BLiMP are visualized in Fig. 5. Even though KL regularization helps mitigate the drop in accuracy, we observe that our fine-tuned models generally exhibit slightly lower accuracy than their non-fine-tuned counterparts, indicating that our fine-tuning procedure does not lead to a better generalization about English grammatical rules in our models."}, {"title": "Text Generation", "content": "Additionally, we analyze how our objective affects the ability of fine-tuned models to generate text and focus on two aspects: the uniformity of information and the diversity of the generations. To assess uniformity, we draw on the uniform information density (UID) hypothesis (Fenk and Fenk, 1980; Levy and Jaeger, 2006), which posits that language users prefer information to be evenly distributed throughout an utterance. A recent study by Meister et al. (2021) provides empirical support for the UID hypothesis in naturally occurring corpora and shows a link between linguistic acceptability and information uniformity. Here, we ask whether aligning models to human reading"}, {"title": "Discussion", "content": "We now discuss the broader implications of our results with respect to both cognitive modeling and natural language processing. One recurring theme in this paper is the relationship between p\u03b8, a probability distribution over strings estimated from large corpora, and pH, the distribution implicated during cognitive tasks. In terms of cognitive modeling, it is widely accepted that it is useful to obtain a pe that is as close as possible to pH in order to test and refine psychological theories. While previous work has proposed alignment procedures between LMs and brain data (Toneva and Wehbe, 2019),"}, {"title": "Conclusion", "content": "We have presented a novel technique to align language models with human reading data by implicitly optimizing the parameters of a linear regression model. Our experiments on held-out test sets show our method reliably improves the predictive power of language models with various parameter counts on human reading times. Furthermore, our findings confirm previous research on the inverse relationship between perplexity and psychometric predictive power. We believe our results pave the way for better assessment of psychological theories using more cognitively aligned language models."}, {"title": "Limitations", "content": "Our study has several limitations. First, we only include predictors for the current unit. Future"}, {"title": "Deriving Optimal Coefficients", "content": "We now discuss how we estimate the optimal coefficient \u1e9e* to approximate the reward, as described in \u00a73.2. We start with finding the optimal coefficients\nB = argmin ||\u03c8 - \u0425\u04e9\u1e9e0||2.(14)\nAssuming (XX)\u00af\u00b9 is invertible, we take the derivative with respect to Bo, set it to 0 and solve for Bo:\n2(\u03c8 - \u03a7\u03bf\u03b2\u03bf) = 0(15a)\n\u2212 \u03a7 + \u03a7\u03a7\u03bf\u03b2=0 (15b)\n\u03a7\u03a7\u03bf\u03b2=\u03a7\u03c8(15c)\n\u03b2=(\u03a7\u03a7)-\u00b9 \u03a7\u03c8.(15d)\nIn theory, (XX)\u00af\u00b9 may not always be invertible, which is why we add a regularization term pI, where I is the identity matrix and p > 0 is a parameter determining the strength of the regularization. The resulting estimator B = (XX + 1) X is known as the ridge regression estimator (Hoerl and Kennard, 1970) and presents the solution to the following problem:\nB = argmin ||\u03c8 - \u03a7\u03b8\u03b20||2 + 1||Bo||2,(16)\nwhere \u03b3 > 0. We define p = Ny. Then setting the derivative of Eq. (16) to zero leads to\n2(\u03c8 - \u03a7\u03bf\u03b2\u03bf) + 2\u03b3Bo = 0(17a)\n=2(17b)\n\u03a7\u03a7\u03bf\u03b2 + \u039d\u03b1\u03b2=\u03a7\u03c8(17c)\n\u03a7\u03a7\u03bf\u03b2 + \u03b2=\u03a7\u03bf\u03c8(17d)\n\u03b2=(\u03a7\u03a7\u03bf + \u03c1\u0399)-1 \u03a7\u03c8.(17e)"}, {"title": "Solving for Optimal Coefficients", "content": "To compute the regression coefficients efficiently, we use the Cholesky decomposition to solve for B given as\n\u03b2 = (\u03a7\u03a7\u03bf + \u03c1\u0399)-1\u03a7\u03c8,(18)\nwhere p is the regularization parameter, which we set p = 1e - 5 and I is the identity matrix. Since XX + pI is symmetric and positive definite, we compute the Cholesky decomposition\nXX + PI = LLT,(19)\nwhere L \u2208 RD\u00d7D is a lower triangular matrix. To solve for \u00df, we first solve for an intermediate vector z\nLz = X\u03c8(20)\nWe then solve for B\nLTB = z.(21)"}, {"title": "Datasets", "content": "Here, we provide additional details on the datasets and reading time measurements used during our analysis. We fine-tune and evaluate models on the Dundee (Kennedy et al., 2003), Provo (Luke and Christianson, 2018), and ZuCo corpora. For the ZuCo corpus, we use data from tasks 1 and 2 from the ZuCo 1.0 corpus (Hollenstein et al., 2018) and task 1 from the ZuCo 2.0 corpus (Hollenstein et al., 2020). All data used in our analysis is publicly available. For the Dundee and ZuCo corpora, we process the data used by Hollenstein et al. (2021), which contains word-level means for fixation counts and reading durations, averaged over all participants, and split into individual sentences.10 For the Provo corpus, we compute the mean reading times from the official repository. 11 From all datasets, we remove duplicate sentences and short sentences with less than four words. The mean number of sentences and words for train and test sets are given in Tab. 4."}, {"title": "Reading Times", "content": "For Dundee and ZuCo, we extract the mean first pass duration over the participants, which is defined as \"the sum of all fixations on w from the first time a subject fixates w to the first time the subject fixates another token\"(Hollenstein et al., 2021, p.109). Similarly, for Provo, we compute the mean gaze duration defined as the \u201cDwell time (i.e., summation of the duration across all fixations) of the first run within the current interest area\" (Luke and Christianson, 2018, Tab. 2). While these two definitions are very similar, they may not be exactly identical. In Tab. 5, we compare the mean and standard deviation of reading times as well as the number of zero reading times. We observe that while Dundee and Provo exhibit relatively similar means and standard deviations, ZuCo shows overall lower mean reading times. Unlike Dundee and ZuCo, Provo contains no instances of words with zero reading times, likely due to the high number of participants."}, {"title": "Fine-Tuning Parameters", "content": "For all runs, we use the parameter configurations in Tab. 6 and repeat each experiment 3 times with random seeds (42, 8, and 64). We fine-tune and evaluate three GPT-2 models: GPT-2 Small with 117 million parameters, GPT-2 Medium with 345 million parameters, and GPT-2 Large with 762 million"}, {"title": "Evaluation", "content": "We perform evaluation every 50 steps on the held-out test splits in Tab. 4, during which we compute the surprisal estimates, the regressors' coefficients, and the language model's perplexity for each batch. Using the surprisal estimates we compute the Allh by performing a 5-fold cross-validation on the test data, where we fit baseline and target linear regressors using ordinary least squares.13 Note that we do not scale the predictor variables to maintain consistency between the linear regression and the calculation of our reward in Eq. (13), where batch-level surprisal estimates prevent global scaling."}, {"title": "Detailed Results", "content": ""}, {"title": "Aith Change", "content": ""}, {"title": "Perplexity and Allh", "content": ""}, {"title": "BLIMP", "content": ""}, {"title": "Narrative Understanding", "content": "To measure models' abilities to track entities and produce text that is consistent with narrative structure, we evaluate them on LAMBADA (Paperno et al., 2016). This dataset requires that models produce the final word of a narrative. People can easily achieve higher performance on this task if they are given the full narrative context, but not if they are only given the previous sentence. Thus, performing well requires an understanding of broader contexts. The performance of our models is visualized in Fig. 8. As with BLiMP, we find that fine-tuned models perform slightly worse at this task compared to non-fine-tuned baselines."}, {"title": "Results for the Regularized Objective", "content": "In this section, we present additional results for fine-tuning models using the KL regularized objective with a regularization coefficient of X = 500. As shown in Tab. 8, the maximum Allh values and percentage"}, {"title": "Total Reading Duration & First Fixation Duration", "content": "Throughout this work, we have focused on predicting gaze durations. Here, we extend our analysis by fine-tuning models to predict total reading durations\u2014which are the summed durations of all fixations on a unit u-and first fixation durations, which are the durations of the first fixation on a unit u. As shown in Tab. 9, models predicting total reading durations start at higher Allh values"}]}