{"title": "FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba Foundation Model", "authors": ["Anna Tegon", "Thorir Mar Ingolfsson", "Xiaying Wang", "Luca Benini", "Yawei Li"], "abstract": "Accurate and efficient electroencephalography (EEG) analysis is essential for detecting seizures and artifacts in long-term monitoring, with applications spanning hospital diagnostics to wearable health devices. Robust EEG analytics have the potential to greatly improve patient care. However, traditional deep learning models, especially Transformer-based architectures, are hindered by their quadratic time and memory complexity, making them less suitable for resource-constrained environments. To address these challenges, we present FEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel self-supervised framework that establishes new efficiency benchmarks for EEG analysis through bidirectional state-space modeling. Unlike Transformer-based models, which incur quadratic time and memory complexity, FEMBA scales linearly with sequence length, enabling more scalable and efficient processing of extended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and fine-tuned on three downstream tasks, FEMBA achieves competitive performance in comparison with transformer models, with significantly lower computational cost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB and 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates viability for resource-constrained devices. These results pave the way for scalable, general-purpose EEG analytics in both clinical and highlight FEMBA as a promising candidate for wearable applications.\nClinical relevance- By reducing model size and computational overhead, FEMBA enables continuous on-device EEG monitoring for tasks like seizure detection and artifact reduction, promising improved patient care through timely and cost-effective neuro-monitoring solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of foundation models has profoundly impacted artificial intelligence, bringing forward a shift toward generalizable, large-scale pre-training. These models, trained via self-supervised learning (SSL) on heterogeneous datasets, derive their effectiveness from hierarchical feature extraction that can span diverse tasks [1]. While their success in language (e.g., BERT [2]) and vision (e.g., CLIP [3]) is well-documented, their potential in biomedical signal processing-particularly for Electroencephalography (EEG)-remains relatively underexplored.\nEEG is a challenging modality due to its pseudo-random, non-stationary waveforms, susceptibility to artifacts [4], and substantial intra- and inter-subject variability. These factors demand models that balance robustness with interpretability. Wearable EEG devices play a crucial role in enabling continuous brain monitoring in real-world settings, offering new opportunities for brain-computer interfaces [5], healthcare and cognitive research [6]. Although recent efforts have used convolutional architectures [7] and attention-based mechanisms [8] for EEG, real-world constraints complicate their deployment. Wearable devices and continuous monitoring systems impose strict limits on memory and latency [9], making even moderately sized Transformers impractical. As a result, there is a strong, important, unmet need for architectures that can combine expressive power with computational efficiency. Given these constraints, we propose harnessing State Space Models (SSMs). Specifically, we build upon the Mamba linear SSM, a scalable approach tailored for large-scale EEG, to mitigate the memory and latency bottlenecks associated with Transformer models while maintaining high performance and interpretability.\nFrom Transformers to Mamba: Transformer-based models have demonstrated strong performance in capturing long-range dependencies in EEG [8], [10]. Current EEG foundation models (e.g., BENDR, EEGFormer, LaBraM, Neuro-GPT) predominantly rely on attention mechanisms and may not provide the efficiency demanded by edge-computing environments. However, their O(N\u00b2) complexity in computation and memory as a function of sequence length N can become a bottleneck for continuous or extended EEG recordings, especially on resource-constrained devices. In contrast, Mamba [11], which is based on a state-space framework, helps address these challenges by reformulating sequence modeling as a latent differential system. This approach offers linear scaling (as a function of sequence length) without substantially compromising temporal resolution. Bidirectional extensions [12] further enable retrospective analysis, which may be essential for detecting ephemeral biomarkers (e.g., interictal spikes).\nTo investigate computationally efficient architectures such alternatives, we introduce FEMBA (Foundational EEG Mamba + Bidirectional Architecture), which leverages state-space principles for large-scale EEG modeling. FEMBA is designed to address three key limitations of prior work: (1) quadratic scaling in attention-based models, (2) limited pre-training scope for capturing neurophysiological diversity, and (3) difficulties in adapting to low-resource settings. By pre-training on 21,000 hours of unlabeled EEG from 5,000 subjects, FEMBA aims to learn representations that generalize across a range of pathologies, while retaining the potential for deployment on wearable hardware, as demonstrated by the promising performance of our Tiny FEMBA model.\nOur contributions are the following:"}, {"title": "II. BACKGROUND", "content": "This section provides an overview of the Temple University Hospital EEG (TUEG) Corpus and its labeled subsets, then reviews recent advances in EEG foundation models. We focus on the computational challenges faced by Transformer-based approaches, discuss the motivation for SSMs, and examine Mamba-based solutions. Finally, we introduce how our proposed FEMBA architecture builds upon these insights.\n\nA. Temple University Hospital EEG (TUEG)\nThe TUEG Corpus [13], is one of the largest publicly available clinical EEG repositories. It contains over 26,000 EEG recordings drawn from more than 14,000 patients, spanning pediatric to geriatric populations and encompassing a variety of neurological conditions. In total, TUEG covers approximately 21,000 hours of EEG data. Such diversity in demographics and pathologies provides a robust environment for learning general EEG representations.\nB. Key Labeled Subsets: TUAB, TUAR, and TUSL\nThe TUEG dataset offers subsets of labeled datasets, such as the Temple University Hospital Abnormal EEG (TUAB), Artifact (TUAR) and Slowing (TUSL) Corpus [13]. TUAB"}, {"title": "C. Related works", "content": "Foundation Models in EEG: Foundation models have gained significant traction in NLP (e.g., DeepSeek [14]) and computer vision (e.g., Molmo [15]), motivating interest in their application to EEG. However, these models are typically tailored for structured data such as text or images, raising challenges when dealing with the temporal complexity and biological variability of EEG signals [16]. Early EEG-focused foundation models like BENDR [17] employed contrastive learning yet faced scalability issues. Neuro-GPT [18] introduced autoregressive masking and reported gains in motor imagery classification, while LaBraM [10] and EEG-Former [8] refined masked modeling methods across multiple datasets, achieving balanced accuracies above 80% on abnormal EEG detection. Despite these advancements, most prior approaches rely on Transformer architectures with O(N\u00b2) complexity as a function of the sequence length, limiting their viability for continuous or large-scale EEG monitoring.\nFrom Traditional Methods to State Space Models: Conventional EEG analysis often used machine learning algorithms such as Support Vector Machines (SVMs) and Linear Discriminant Analysis (LDA), complemented by smaller deep networks like EEGNet [19] and DeepConvNet [20]. While these approaches offered interpretability and efficiency for relatively constrained tasks, they required extensive feature engineering and did not always generalize well to diverse patient populations. Transformer-based methods [10], [8], [18], [17] later tackled the challenge of capturing long-range dependencies, though their substantial computational and memory demands may hinder real-world deployment.\nSSMs have gained interest for time-series analysis as they evolve a hidden state over time according to a simple linear dynamical system. In continuous form\n\nh'(t) = Ah(t) + Bx(t), y(t) = Ch(t),\n\nwhere h(t) is the hidden state, x(t) is the input, y(t) is the output, and {A, B, C} are system matrices governing state evolution and output generation. Although these equations describe a continuous process, many implementations rely on discrete versions for efficient training in deep learning frameworks.\nWearable and Edge Constraints: Limited battery life, on-board memory, and compute resources characterize many real-world EEG applications, especially wearable devices [21]. Applications like continuous epilepsy detection add real-time considerations and demand low false-alarm rates [4]. The quadratic scaling of transformer-based methods often proves impractical under these constraints. In contrast, architectures based on state-space principles owing to linear time and memory complexity-can better meet edge-computing requirements.\nMamba-Based Approaches for EEG: A notable example of such SSM is Mamba [11], which applies a Zero-Order Hold (ZOH) scheme [22] to discretize the SSM. Under a sampling interval \u0394, the continuous matrices A, \u0412 map to discrete counterparts Ad and Bd. Mamba further integrates a selective gating mechanism to modulate the hidden state update in a data-dependent manner. As a result, it achieves linear complexity in sequence length, contrasting with the O(N\u00b2) complexity of transformers. Bi-Mamba+ [12] extends Mamba by processing the input sequence in forward and backward directions, subsequently merging the two representations (e.g., via summation or gating). Recent work has begun to explore Mamba's potential in EEG analysis. Mentality [23] employed Mamba with a masked reconstruction scheme on TUSZ v2.0.1, improving seizure detection area under the ROC curve (AUROC) from 0.64 to 0.72. EEGMamba [24] adopted a multi-task strategy by integrating Spatio-Temporal-Adaptive modules and Mixture-of-Experts heads, achieving above 98% accuracy on the Siena dataset and around 97% on CHB-MIT. Despite these early successes, challenges for Mamba-based models remain especially regarding robust spatial-channel modeling for varying electrode montages and the need for domain-generalizable representations.\nIn this work, our FEMBA builds upon Mamba's efficient state-space design by integrating large-scale self-supervised pre-training with bidirectional state updates, FEMBA aims to deliver strong accuracy on various EEG downstream tasks while maintaining linear scaling (with regards to sequence length) suitable for resource-limited devices."}, {"title": "III. METHODOLOGY", "content": "In this section, we describe the proposed Foundational EEG Mamba + Bidirectional Architecture (FEMBA) and its training procedures. Next, we outline our self-supervised pre-training scheme and finally, we explain our fine-tuning strategy, including two alternative classifier architectures and multiple downstream tasks (abnormal EEG detection, artifact recognition, and slowing event classification).\n\nA. Foundational EEG Mamba + Bidirectional Architecture (FEMBA)\nOur proposed FEMBA architecture is designed in four model sizes: Tiny, Base, Large, and Huge, with parameter sizes ranging from 7.8 million (Tiny) to 386 million (Huge), aligning with model sizes commonly explored in the literature [10], [8]. The primary distinction across these variants lies in the number of Bi-Mamba blocks and the embedding dimension, which is controlled by the 2D Convolution in the Tokenizer, as illustrated in Fig 1. Specifically, the embedding dimensions for these configurations are as follows: the Tiny model uses two blocks and an embedding size of 35 ((2, 35)); the Base model employs a configuration of (12,35); the Large model adopts (4,79); and the Huge model features (20, 79). Notably, the hidden state size across all configurations remains fixed at 80.\nFurthermore, a residual connection is incorporated within the Bi-Mamba block to facilitate the smooth propagation of gradients during training. A detailed representation of the entire FEMBA architecture can be found in Fig 1.\nDuring training, we utilize a layer-wise learning rate decay [25] with a fixed decay factor of 0.75, progressively reducing the learning rate from the deeper blocks to the earlier ones\nB. Self-Supervised Pre-training\nWe pre-train FEMBA on the TUEG dataset, as detailed in Section II-A. To prevent data leakage between pre-training and downstream tasks, we use a version of TUEG where subjects present in TUSL, TUAR, or TUAB have been filtered out. During pre-training, we adopt a self-supervised masked training strategy designed to enable FEMBA to learn robust, general-purpose representations of EEG signals. This involves randomly masking a subset (60%) of the input patches and training the model to reconstruct the missing patches, thereby compelling the encoder to capture meaningful spatiotemporal structures within the EEG data.\na) Signal Normalization and Patch Embedding.: We begin by representing each raw EEG recording as a tensor x\u2208 \u211d^(C\u00d7T), where C is the number of channels and T is the temporal length (in samples). To reduce the influence of outliers, we apply quartile-based normalization [26], scaling each channel by its interquartile range (IQR):\n\nx_norm = (x-q_lower)/(qupper - q_lower) + 1 \u00d7 10^(-8)\n\nWe then segment x_norm into bi-dimensional patches of size p x q (e.g., 4 channels \u00d7 32 samples). A 2D convolution projects these patches into an embedding space Xembed \u2208 \u211d^(d\u00d7C'\u00d7T'), followed by learnable positional embeddings to maintain ordering across patch tokens.\nb) Random Masking and Encoder.: Next, we apply random masking to 60% of the embedded patches, setting their representations to zero. This relatively high masking ratio ensures that the model must rely on contextual cues from unmasked segments to infer the missing patches. The masked embeddings, Xmasked, are then fed into the FEMBA encoder.\nc) Decoder and Smooth L1 Reconstruction Loss.: A lightweight decoder of two convolutional layers and a final linear projection attempts to reconstruct the original patches from the encoder outputs. We compute a Smooth L1 loss [27] only over the masked patches:\n\nSmoothL1(x, x) = {0.5 (x \u2013 x)\u00b2, if |x \u2212 x < \u03b2,   |x-2|-0.5, otherwise,\n\nmasked_loss = 1/M \u03a3 SmoothL1(x_i, x), i\u2208M\n\nwhere M is the set of masked patch indices.\nC. Fine-Tuning\n1) Classifier Architectures.: Following pre-training, the decoder is discarded and the Bi-Mamba encoder is repurposed as a feature extractor for downstream tasks. Two classification heads are explored:\n1) Linear Classifier: A small stack of fully connected layers (with GELU activations) outputs class probabilities. This design has a low parameter footprint (~ 0.5 M).\n2) Mamba-Enhanced Classifier: We add one more Mamba block before the final linear layer, enabling additional temporal modeling. This often improves accuracy in tasks with complex temporal dependencies but adds a slight increase in parameters (up to 0.7M).\n2) Downstream tasks: We assess FEMBA on three downstream tasks using the datasets described in Section II-B. For the TUAB dataset this consists of a binary classification (normal vs. abnormal), using the pre-defined train-test split. In TUSL, the task is a four-class classification task (slowing, seizure, complex, normal), Since the TUSL dataset lacks a predefined test split, we adopt an 80/10/10 randomized training/validation/test split. For TUAR we experiment with four versions of a downstream task based on the labeling scheme in in [28], they are described as the following:\n\u2022 Binary Classification (BC): Label a window as artifact if any of the 13 artifact types is present on any channel; otherwise normal.\n\u2022 Multilabel Classification (MC): Perform channel-wise artifact detection as a set of independent binary classifications, allowing multiple artifact types per window/channel.\n\u2022 Multiclass-Multioutput Classification (MMC): Discriminate between 13 artifact types for each channel, thus providing a more granular classification per channel.\nMulticlass Classification (MCC): Restrict to 5 artifact types in a single-label setting, ignoring windows with combinations of artifacts (less than 5% of data). This setting aligns closely with the protocol described by EEGFormer [8].\nAs the TUAR dataset also lacks a predefined test split, we similarly use an 80/10/10 randomized training/validation/test split."}, {"title": "IV. RESULTS", "content": "This section demonstrates that FEMBA consistently achieves state-of-the-art (SoA) or near-SoA performance on diverse EEG benchmarks (TUAB, TUAR, and TUSL), while using significantly fewer FLOPs and less memory compared to recent SoA self-supervised Transformer-based methods. We provide quantitative accuracy metrics and efficiency analyses, which underscores FEMBA's suitability for large-scale clinical or wearable EEG systems. For specific training details we fine-tune all layers (encoder + classifier) end-to-end using the Adam optimizer (initial learning rate of 1 \u00d7 10^(-4)) with cosine decay scheduling. Early stopping is employed based on validation loss to mitigate overfitting.\n\nA. Pre-training results\nOur FEMBA model variants are initially pretrained to reconstruct both masked and unmasked sections of the signal, as detailed in Section III-B. All variants demonstrated strong reconstruction capabilities for both masked and unmasked portions of the signal. This is illustrated in Fig 2, where the FEMBA-Base model successfully reconstructs a masked signal. The training and validation loss during pretraining were closely aligned for all variants, with example loss values for FEMBA base of 0.122 (Train) and 0.217 (Validation).\nB. TUAB: Abnormal EEG Detection\nTable II summarizes TUAB results, where the task is to classify recordings as normal or abnormal. All FEMBA variants outperform the supervised models, with FEMBA-Huge attaining a balanced accuracy of 81.82%, approaching LaBraM-Large/Huge [10] (82.26%-82.58%) but with around 70% fewer FLOPs than LaBraM-Huge (see Table III). Moreover, FEMBA outperforms EEGFormer-Large [8] in AUROC (0.8921 vs. 0.8760). This underscores that our near-linear Mamba-based encoder can rival top Transformer architectures without incurring the quadratic attention cost.\nC. TUAR: Artifact Detection\nWe next evaluate FEMBA on the Temple University Hospital Artifact (TUAR) dataset using four classification protocols of increasing label complexity: BC (binary), MC (multilabel), MMC (multiclass-multioutput), and MCC (multiclass single-label). Table IV details the performance of three FEMBA variants:\na) Binary Classification (BC).: Even our smallest FEMBA-Tiny (7.8M parameters) achieves an AUROC of 0.937 and AUPR of 0.912, signaling robust artifact vs. normal discrimination. Scaling to FEMBA-Base boosts AUROC to 0.949 and AUPR to 0.932-about a 1.2% gain in AUROC at a modest increase in parameters.\nb) Multilabel (MC) & Multiclass-Multioutput (MMC).: Channel-wise artifact detection (MC) sees AUROCs of up to 0.909, while the more fine-grained MMC reaches 0.893. Notably, FEMBA-Tiny slightly outperforms the Base and Large variants in MMC (0.893 vs. 0.888/0.878), showcasing that a lean state-space model can excel even in complex multi-artifact labeling.\nc) Multiclass Classification (MCC).: Restricting windows to a single artifact type yields the highest AUROC (up to 0.918 for FEMBA-Tiny). Meanwhile, FEMBA-Large achieves 0.915 AUROC and the highest AUPR (0.521). As reported in Table V, FEMBA also surpasses EEGFormer-1 [8] (0.852 AUROC) under a comparable MCC protocol, demonstrating a SoA result at a fraction of the Transformer's computational cost.\nD. TUSL (Slowing Event Classification).\nTable Vindicates that FEMBA-Base achieves 0.731 AUROC, surpassing EEGFormer-Small/Large by 4.8%-5.2% absolute (0.683/0.679), and slightly outperforming EEGFormer-Base (0.713). However, FEMBA'S AUPR (0.289) trails the best EEGFormer-Large AUPR (0.389) by about 10 percentage points, likely due to class imbalance. Despite this, FEMBA demonstrates these results at a significantly lower computational cost, as detailed in Section IV-E.\nE. Efficiency Analysis: FLOPs, Parameters, and Memory\nPractical considerations\u2014such as floating-point operations (FLOPs), parameter counts, and peak memory usage-are critical in determining the feasibility of real-world or continuous EEG monitoring. Table III provides a comparison of major Transformer baselines (EEGFormer, LaBraM) and our FEMBA models across these metrics.\nFor LaBraM, FLOPs and memory usage are calculated using its publicly available code repository. For EEGFormer, these metrics are approximated based on the limited details available in the literature, as no official code has been released. To measure peak memory usage, we process a batch size of 8 through each model and record the maximum memory consumption. Despite these approximations, a clear trend is evident:\nFEMBA-Huge (386M parameters) requires 58.74B FLOPs, nearly 3.5\u00d7 fewer FLOPs than LaBraM-Huge (202.17B) and 30% less memory usage, yet achieves comparable TUAB accuracy (81.82% vs. 82.58%). FEMBA-Tiny (7.8M) uses only 1.31B FLOPs-up to 27\u00d7 fewer than EEGFormer-Large\u2014while still delivering SoA AUROC (e.g., 0.918 on TUAR MCC). Similarly FEMBA-Base runs at 7.52B FLOPs, roughly 4\u00d7 lower than EEGFormer-Large (36.46B FLOPs). A detailed visual comparison of these models is provided in Figure 3."}, {"title": "F. Discussion", "content": "Overall, FEMBA consistently achieves SoA or near-SoA accuracy with substantially reduced computational cost. On TUAB, FEMBA-Huge falls within 0.8-1.0% absolute of LaBraM-Large/Huge in balanced accuracy but uses roughly 70% fewer FLOPs than LaBraM-Huge. On TUAR, FEMBA-Tiny (7.8M) outperforms EEGFormer-l by 6.6% in AUROC under comparable MCC protocols. For TUSL, FEMBA-Base surpasses all EEGFormer variants by up to 4.8% in AUROC.\nThese findings validate that a state-space modeling approach can match or exceed Transformer baselines without the prohibitive O(N\u00b2) scaling. Future work could explore enhancements to further boost FEMBA's accuracy, such as refining its architecture or incorporating advanced regularization techniques. Additionally, neonatal-focused pre-training could address domain shifts, while multi-modal integration may extend FEMBA's applicability to a wider range of clinical scenarios. We conclude that FEMBA's efficient design and robust performance establish it as a compelling alternative to Transformer-based EEG models for both large-scale and on-device applications."}, {"title": "V. CONCLUSION", "content": "We introduced FEMBA, a novel self-supervised EEG framework grounded in bidirectional state-space modeling and pre-trained on over 21,000 hours of unlabelled clinical EEG. Our experiments across multiple downstream tasks (abnormal EEG detection, artifact recognition, slowing event classification, and neonatal seizure detection) demonstrate that FEMBA achieves near-Transformer performance while maintaining significantly lower computational complexity and memory requirements.\nNotably, a tiny 7.8M-parameter variant (FEMBA-Tiny) retains competitive accuracy on tasks such as artifact detection, showcasing the potential for real-time edge deployments. Nonetheless, certain domain shifts such as neonatal vs. adult EEG-underscore the need for additional domain adaptation. Future work will explore these techniques and integrate multi-modal physiological signals for more robust clinical event detection. We believe FEMBA marks a key step toward delivering efficient, universal EEG foundation models that operate seamlessly from large hospital databases to low-power wearable devices."}]}