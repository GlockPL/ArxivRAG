{"title": "A Survey of Hallucination in Large Visual Language Models", "authors": ["Wei Lan", "Wenyi Chen", "Qingfeng Chen", "Shirui Pan", "Huiyu Zhou", "Yi Pan"], "abstract": "The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing and generation capabilities. However, the existence of hallucinations has limited the potential and practical effectiveness of LVLM in various fields. Although lots of work has been devoted to the issue of hallucination mitigation and correction, there are few reviews to summary this issue. In this survey, we first introduce the background of LVLMs and hallucinations. Then, the structure of LVLMs and main causes of hallucination generation are introduced. Further, we summary recent works on hallucination correction and mitigation. In addition, the available hallucination evaluation benchmarks for LVLMs are presented from judgmental and generative perspectives. Finally, we suggest some future research directions to enhance the dependability and utility of LVLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, LLMs have achieved excellent results in the field of natural language processing (NLP). Transformer-based LLMs acquire the ability to understand and generate natural language by learning the linguistic patterns and knowledge on a large-scale corpus. Lots of LLMs have emerged in the field of NLP such as GPT-4 [1], Llama [2], InstructGPT [3], PaLM [4] and Vicuna [5]. Supported by the large-scale corpus amd huge number of parameters, these LLMs can accomplish a wide range of tasks and show powerful zero-shot capability. Although LLMs have exciting and robust properties, LLMs are limited to the text-only domain. Increasing works have been proposed to integrate visual information to LLMs. These new models are called LVLMs which can be used in a variety of applications, such as medical diagnosis and assistance [6], [7], arts and entertainment [8], autonomous driving [9], virtual assistants and chatbots [10], [11]. With its surprising performance, LVLM has attracted many users. However, some users have found that LVLM generates information which is factually incorrect but seemingly plausible information such as misreporting non-existent objects, object properties, behaviors and inter-object relationships. The above phenomenon is known as hallucination which leads to the inability of LVLMs to be applied in scenarios with high accuracy and reliability. For example, hallucinations may lead to mislead users with incorrect or inaccurate information and even lead to the dissemination of misinformation in content summarization or information retrieval. If the LVLM frequently generates hallucinations, it may affect the development of LVLM. Therefore, correcting or mitigating hallucinations is necessary for LVLMs.\nIn order to build a trustworthy LVLM, the hallucination is a obstacle need to be overcame. As a result, a number of efforts have emerged to mitigate or correct the hallucinations of LVLM. Currently, several surveys have summarized the hallucination correction work in LLMs [12], [13]. In the realm of multi-modality, there has partial work [14], [15] aim to summary the hallucinatory phenomena of multimodal large language models. However, our survey employs a distinctly different taxonomic strategy. We categorize by the core ideas of various hallucination correction efforts and hallucination assessment benchmarks.\nIn this paper, we propose a survey of recent advances in the phenomenon of hallucinations in LVLMs. First, we introduce the background related to LVLM and hallucinations. In section II, the structure of LVLMs and main causes of hallucina- tions in LVLMs are provided. The hallucination correction and mitigation are summarized in section III. After that, we introduce benchmarks for evaluating hallucinations in LVLMs in section IV. In section V, some insights the future prospects of hallucination correction in LVLMs are provided to depict potential research directions."}, {"title": "II. BACKGROUND OF LVLM", "content": "LVLMs can be divided into three modules: perceptual module, cross-modal module and response module which is shown in Fig. 1(A). Through the three modules, the visual information is extracted and mapped to the textual space. Further, the visual information and text information are combined to get the final response.\nThe perceptual module usually utilises Vision Transformer (ViT) [16] or its variants [17] to transform image into high-dimensional vector. Before input to ViT, the image is segmented into patches and added with positional information. As shown in Fig. 1(A), the ViT is a encoder-only model which consists of N encoders. The multi-head attention of encoder is the core component of the Transformer model. It has powerful parallel computing capabilities and allows the model to create connections between different parts of the sequence.\nCross-modal module aims to bridge the modalities gap between vision and language [18]. Recently, cross-modal module in LVLMs adopts the structure such as learnable interface [10], [19], Q-former [20] and pereceiver resampler [21]. The learnable interface maps visual information into textual space based on projection matrices. The Q-former bridges the modality gap by interacting visual information with text. The pereceiver resampler encodes visual features into text by using cross attention.\nThe response module acts as the brain of LVLMs. Therefore, it needs the powerful ability to process and analyse the inputs of visual and textual to generate the final answer. The response module usually adopts LLMs such as Vicuna [5], Llama [2], Flan-PaLM [22] and Llama2 [23]. Both ViT and LLM are based on Transformer, but LLM is decoder-only structure. The masked multi-head attention of decoder adds the mask operation. Therefore, the LLM can not utilize the \"future\" information in the text generation which ensures the authenticity."}, {"title": "B. Causes of Hallucination", "content": "There are some factors lead to hallucination generation of LVLM. The occurrence of hallucination may be associated with more than one part of the LVLM including perceptual module, cross-modal module and response module. Therefore, in order to better correct and mitigate hallucinations, we attribute the main causes of the phenomenon of hallucinations as follows:\n1) Modality Gap: Each modality has its own unique characteristics and expressions, which results in significant differences in the distribution, features and semantics of the data between different modalities. The existence of the modalities gap makes the response module biased in understanding of the image input, which leads to the generation of erroneous responses. For example, as shown in Fig. 1(B), the red and white object is actually a sign, not a Chinese character. Due to the presence of the modalities gap, the response module incorrectly describes it as a 'red and white Chinese character'.\n2) Toxicity in Dataset: The nature of cross-entropy loss is mimicry. Therefore, LVLMs learn the patterns from the dataset to generate responds that are similar to the training data. As LVLMs require the extremely large amount of data for training, most datasets are generated by using LVLMs or LLMs. Although these data is manually cleaned after generation, a certain percentage of misleading samples are still retained in the dataset. When LVLM learns from these data with hallucination, it will inevitably generate hallucinations.\n3) LLM Hallucinations: The excellent performance of LVLMs is mainly due to that it uses of LLMs as their brains. However, LLMs are easily to generate hallucinations. In addition, LLMs have acquired rich parametric knowledge. When these parametric knowledge is wrong or conflicts with the received visual information, it will lead to hallucinations. Moreover, the randomness of the available decoding strategies may also be a trigger for hallucinations. Many special phe- nomena usually occur during the decoding process which are closely related to hallucinations."}, {"title": "III. CORRECTION OF HALLUCINATIONS", "content": "In this section, we summarized the core ideas of recent hallucination correction and mitigation works. Meanwhile, we consider the relationship between the motivation and the causes of the hallucinations. We have categorized recent works into three classes: dataset dehallucination, modalities gap and output correction, which is shown in Fig. 2. In addition, thedetails of all methods are summarized in Table. I."}, {"title": "A. Dataset Dehallucination", "content": "LVLMs usually use instruction tuning to achieve powerful inference performance. However, it often relies on high-quality and large-scale instruction datasets. In reality, it is difficult to construct high-quality instruction datasets even with the assistance of LLMs or LVLMs. Moreover, it is hard to manually construct high-quality and large-scale datasets. Therefore, it is viable to obtain high-quality and large-scale dataset by removing the hallucinatory of existing datasets. In this section, we present recent work with three core ideas: data rewrite, remove overconfidence and disrupting co-occurrence."}, {"title": "1) Data Rewrite:", "content": "The data rewrite refers to rewrite the noisy and mismatched samples as usable samples by using LLMs or LVLMs. Liu et al [24] proposed the data rewrite method to correct hallucination of datasets. This method utilizes multiple LVLMs (Llava-1.5 [10], Otter [25], MiniGPT-4 [26]) to generate multiple texts for each image. It can increases the diversity of the dataset. Then, chatGPT is utilized to standardize the style of these texts which can dilute the effect of caption style. The text shearing is used to avoid the hallucinations introduced by LVLMs when generating new samples. The core of text shearing is to limit the length of the generated text during the inference process of LVLMs."}, {"title": "2) Remove Overconfidence:", "content": "If the dataset contains too many positive samples, it may lead to overconfidence (i.e, LVLMs respond Yes without any basis). To avoid overconfidence, Hu et al. [27] proposed a method (CIT) to remove over-confidence by fine-tuning in a series of factual and contrastive question-answer (QA) pairs. These QA pairs are constructed by prompting chatGPT which contain balanced number of Yes and No in the answers. In QA pair, the questions focuses on hallucinatory scenes of objects existence, properties and inter-relationships. In addition, QA pairs are manually verified to ensure high quality. Similarly, Liu et al. [28] constructed the LRV-Instruction by using GPT-4 [1], which contains a series of positive and negative visual instructions. In addition, LRV-Instruction adds an examination of parametric knowledge in LVLM by modifying the knowledge in the original instruction. Both QA pairs in CIT and LRV-Instruction can avoid overconfidence by constructing the balanced number of positive and negative samples and fine-tuning on these datasets to mitigate the LVLM hallucination."}, {"title": "3) Disrupting Co-occurrence:", "content": "Since most of images in the dataset come from websites, it is inevitable that some objects such as \"cars\" and \"roads\" are frequently co-occurring. These co-occurrences affect the inference of LVLMs which leads to describe non-existent objects in responses. To address the co-occurring and hallucinatory objects in the dataset, Yu et al. [32] proposed the HalluciDoctor framework based on the hallucination cross-checking paradigm and seesaw-based visual instruction expansion. As shown in Fig. 3, the hallucination cross-checking paradigm is designed to find and remove hallucinations from instruction datasets. First, answer chunks are generated by using the textual scene graph parser [29]. Then, answer-based questions are generated by chatGPT. Images and answer-based questions are input into multiple LVLM experts to generate candidate answers. Finally, the hallucinatory part of the instruction is identified and cleared by cross-checking the consistency between candidate answers and answer chunks. The seesaw-based visual instruction expansion aims to destroy the original false associations. The enhancement factor and inhibiting factor of the hallucinatory object are calculated to obtain the seesaw score, which is used to guide the tool model to integrate the hallucinatory object into irrelevant images and text. The enhancement factor $E_i$ and inhibiting factor $I_i$ are defined as follows:\n$E_i =  \\begin{cases}\n\\frac{max(n_{i,\\tau})}{n^*}, & \\text{if } n_i \\leq n^* \\\\\n1, & \\text{if } n_i > n^*\\end{cases}$ (1)\n$I_i =  \\begin{cases}\n\\frac{m_i}{n^*}, & \\text{if } m_i < n^* \\\\\n1, & \\text{if } m_i > n^*\\end{cases}$ (2)\nwhere $n^*$ denotes the number of co-occurrences of the hallucinatory object on and ground-truth object o which is the most relevant object for $o_n$. $n_i$ denotes the number of co- occurrences of $o_n$ with other objects $o_i$. The smaller $n_i$ means less co-occurrence between $o_i$ and $o_n$, thus larger enhancement factor $E_i$. $m_i$ denotes the number of co-occurrences of $o_i$ with"}, {"title": "B. Modalities Gap", "content": "LVLMs rely on the parametric knowledge in the response module to generate response when the perception module does not receive enough visual information. At this point, hallucinations will be generated if the parametric knowledge provides information mismatch the ground-truth visual information. On the other hand, the cross-modal module acts as a bridge in LVLM. If the gap is remained between the visual information and the textual space after mapping, it can also lead to biases for understanding visual information in the response module. Therefore, enhancing the ability of extract and map visual information in LVLM can reduce the generation of hallucination. In this section, related works are classified into Visual Fusion, Perceptual Reinforcement and Contrastive Learning."}, {"title": "1) Visual Fusion:", "content": "Different visual models have different preferences for feature extraction. The fusion of features from multiple visual models can help to improve the visual comprehension of LVLM. Jiang et al. [33] proposed a strategy (COMM) to enhance the visual comprehension of LVLMs based CLIP and DINOv2, which is shown in Fig. 4. In this method, the feature space of different layers is aligned based on linear-layernorm module (LLN). Then, multi-layer features are merged by using layerscale. In addition, the multilayer perceptron (MLP) is utilized to project the features of DINOv2 to the feature space of CLIP for ensuring the consistent between two vision models. Finally, the fused features are projected to the text space by using a linear layer to strengthen the perception of LVLM on visual details. Tong et al. [38] proposed Mixture-of-Features (MOF) to intersect the features of CLIP and DINOv2. It can obtain a richer vision understanding without training. Similarly, Jiao et al. [39] utilized DINO [40] and PaddleOCRv2 [41] to obtain richer visual information. First, the object detection and optical character recognition (OCR) results are obtained by using DINO and PaddleOCRv2, respectively. Then, these results are transformed into text features through the embedding layer of LLM. Finally, the text features and visual features extracted by CLIP are fed into LLM. These fusion strategies can improve the visual perceptual ability of LVLM, which helps to reduce the generation of hallucinations."}, {"title": "2) Perceptual Reinforcement:", "content": "The image input to the perception module is usually 224 \u00d7 224 resolution. The fixed resolution limits LVLM to understand visual details. Therefore, Cao et al. [42] proposed DualFocus to generate responses from both macro and micro perspectives. As shown in Fig. 5, DualFocus takes original image I as input to generate macro answer. For the microscopic perspective, it uses LVLM to obtain the sub-region coordinates box related to the user question Q1. The sub-region image Is is obtained based on box. Meanwhile, The question Q2 is obtained by adapting Q1 with prompt information. Further, Io, Is, Q1 and Q2 are input into LVLM to obtain the micro answer. Both two kinds of answers calculate the score of perplexity to assess credibility. The answer with the lower perplexity score is selected as the final answer. It greatly strengthens the visual perception ability of LVLM.\nObject detection models can provide detailed visual information, such as the number of objects, location and other properties. Jiang et al. [46] proposed VTPrompt to enhance LVLM perception ability based on detection model. The VTPrompt first uses chatGPT to extract the main objects of user queries. Then, it utilizes detection model (SPHINX [47]) to mark the main objects of image which provides the location information of objects. Prior to generating answers, the VTPrompt uses structured textual prompt for query transformation, which is used to guide the LVLM to generats a visual chain of thought by leveraging the marked information of the image. Finally, the LVLM generates responses based on the marked images and the processed queries. Meanwhile, the VTPrompt helps to improve the interpretation ability of LVLM."}, {"title": "3) Contrastive Learning:", "content": "The core of contrastive learning is to extract features by comparing the differences between positive and negative samples. For each image input into LVLM, there is a significant difference between hallucinatory response and the correct response. Based on this difference, Liu et al. [48] proposed HACL for mitigating hallucinations in LVLM. It uses ground-truth text as positive sample, hallucinatory text as hard negative sample and ground-truth text from other images as negative samples. The variance between positive and negative samples reduces the modalities gap between visual features and real text features. The hard negative samples increases the distance between visual features and hallucinatory textual features which prevents LVLMs to generate hallucinations."}, {"title": "C. Output Correction", "content": "In hallucination correction, correcting the hallucinatory response to an accurate response is the most straightforward approach. Changing the output preference of LVLM can also mitigate hallucinations. In addition, hallucinations are closely related to many phenomena in the decoding process of LVLM. Analyzing these phenomena can help to understand the generation mechanism of hallucinations and mitigate the generation of hallucinations. In this section, related works are classified into Post-generate Correction, RLHF-based Method, DPO-based Method, CoT-based Method and Special Phenomenon."}, {"title": "1) Post-generate Correction:", "content": "A direct method for correcting hallucination is to perform post hoc remediation such as detecting and correcting for hallucinations in the response. Based on the idea, Yin et al. [49] proposed Woodpecker to directly correct hallucination in the response. In Woodpecker, LLMs extract key concepts from the response and use these concepts to construct questions about the main objects. Answers are provided by open-set object detector [50] and VQA model [30] which serve as visual validation. Finally, LLMs correct hallucinations in the response with guidance of these QA pairs. Unlike Woodpecker with multiple expert models, Zhou et al. [51] just trained a LVLM hallucination revisor (LURE) to correct hallucination. During training process, LURE uses images and hallucinatory descriptions as input, and correct descriptions as output. In addition, this method is sensitive to co-occurring objects which will bring about hallucinatory.\nIn addition, LVLM can also reduce hallucinations by iterative correcting their response. Lee et al. [52] proposed a method (Volacn) to correct hallucinations. As shown in Fig. 6, it first inputs the image and question to generate Responsei. Then, the LVLM is prompted to generate feedback based on Responsei. The ResponseR is obtained by revise Responsei, based on feedback. Finally, LVLM calculates the Response score of Responsei and ResponseR. The ResponseR is output as the final output if scorerR > scorer, otherwise continue iteration. The post-generate correction can efficiently correct hallucinations in LVLM, but it takes longer time for generating responses."}, {"title": "2) RLHF-based Method:", "content": "Reinforcement learning from human feedback (RLHF) [53]\u2013[55] aims to optimize the behaviour of models by using human feedback as a reward signal. factually augmented RLHF (Fact-RLHF) [56] is the first application of RLHF to the multi-modal domain. The Fact-RLHF has three training stages. The first stage uses the instruction dataset to fine-tune the LVLM to obtain policy model. In the second stage, Fact-RLHF constructs the hallucinati-aware human preference dataset. Then, reward model is trained on human preference dataset to provide accurate reward signal. In the third stage, the policy model is trained by maximizing the reward signal. In addition, Fact-RLHF introduces additional ground-truth information to calibrate the reward signals to avoid reward hacking during the training of reward model. Different from Fact-RLHF, RLHF-V [57] eliminates the training of reward model and employs the dense direct preference optimization (DDPO) strategy to directly preference optimize the policy model. First, RLHF-V constructs segment-level fine-grained correctional human feedback dataset. Then, the reward model is replaced with a policy model and a reference model, which is defined as follows:\n$L = -E_{(x,y_w,y_l)}[log \\sigma (r(x, y_w) - r(x, y_l))]$\n$= -E_{(x,y_w,y_l)}[log \\sigma (\\beta log \\frac{\\pi^*(y_w|x)}{\\pi_{ref}(y_w|x)}\n- \\beta log \\frac{\\pi^*(y_l|x)}{\\pi_{ref}(y_l|x)})]$\nwhere \u03c0\u2217 denotes the policy model. \u03c0ref denotes the reference model. x denotes the input. yw denotes human feedback data. yl denotes the original data. \u03b2 is a constant. During training, the reference model remains frozen and only the policy model is updated. To utilize segment-level information, RLHF-V calculates response score by weighting fine-grained segments,"}, {"title": "3) DPO-based Method:", "content": "Direct policy optimization (DPO) [58] aims to directly optimize policy model to improve the efficiency of reinforcement learning. Based on the DPO, Zhao et al. [59] proposed the hallucination-aware DPO (HA-DPO). The loss of HA-DPO is defined as follows:\n$L_{\\pi_\\theta; \\pi_{ref}} = E_{(x_T,x_1,y_{pos}, y_{neg}) \\sim D} {log\\sigma (\\beta log \\frac{\\pi_\\theta (y_{pos} | [x_T, x_1])}{\\pi_{ref} (y_{pos} | [x_T, x_1])}\n- \\beta log \\frac{\\pi_\\theta (y_{neg} | [x_T, x_1])}{\\pi_{ref} (y_{neg} | [x_T, x_1])})}$, (6)\nwhere xT and x1 denote the input of text and image prompts of model, respectively. \u03c0ref and \u03c0\u03b8 represent the reference model and policy model, respectively. [] denotes feature connectivity. D denotes the style consistency hallucination dataset which contains images and positive responses and negative responses (hallucinations). This loss function biases the LVLM towards selecting positive responses ypos and rejecting negative responses yneg.\nGunjal et al. [60] used the variant of DPO: fine-grained direct preference optimization (FDPO) to optimize LVLM. FDPO first constructs the fine-grained M-HalDetect dataset. The M-HalDetect dataset does not contain positive and negative samples, but rather segment level annotations. It categorizes segments into accurate, inaccurate and analysis to provide preference signals for reward model training. The FDPO loss function is defined as follows:\n$L_{FDPO} (\\pi_\\theta; \\pi_{ref}) = -E_{(x,y,c) \\sim D}[log \\sigma(B_k)]$\n$k = \\begin{cases} -r & c = 0 \\\\\nr = log \\frac{\\pi_\\theta (y|x)}{\\pi_{ref} (y|x)} & c = 1, \\\\\n\\infty & c > 1\\end{cases}$ (7)"}, {"title": "4) CoT-based Method:", "content": "Chain of thought (CoT) is a method to improve the reasoning ability of models. The core idea of CoT is to generate a reasoning process before producing an answer, which helps model to better understand and solve the question. However, the reasoning of LVLM is just a spurious correlation generated by powerful representational capabilities which lacks interpretability [61]. Therefore, Gao et al. [62] proposed Fact method to make LVLM reasoning interpretable. In Fact method, code generation models are utilized to generate code snippets that are interpretable and provide the correct answer. Then, the code is transformed into a CoT reasoning by pruning, merging and bridging operations. Meanwhile, performing transferability verification to eliminate unnecessary parts of CoT. Finally, LVLM is jointly trained with the CoT and labels to mitigate the hallucination of LVLM.\nGao et al. [63] found that LVLM can obtain higher-level visual information compared to expert models such as detectors, recognizers and OCR. Meanwhile, the powerful performance of LVLM allows them to be the conductor of expert model. Combining the above two points, they proposed Cantor method to enhance the visual reasoning ability of LVLM. It guides LVLM to act multiple roles to accomplish reasoning, decision-making and execution. The inference of Cantor is divided into two steps: decision generation and execution. In the decision generation phase, Cantor constructs prompts to guide the LVLM in problem reasoning and assign tasks to the expert model. In the execution phase, the LVLM is guided by constructing prompts to act different expert models and complete the sub-tasks assigned in the decision generation phase. Finally, all the sub-tasks are summarized to the information integration expert by using the LVLM to obtain the final answer."}, {"title": "5) Special Phenomenon:", "content": "The special phenomena or patterns are closely related to the hallucination which occurs during the decoding of LVLM. As shown in Fig. 7, Huang et al. [64] proposed an over-trust penalty and a retrospection-allocation (OPERA) strategy to avoid the knowledge aggregation pattern, which is special phenomenon of decoding of LVLM. It refers that certain tokens (summary tokens) contain only limited information but can guide the generation of subsequent tokens. In the over-trust penalty strategy, OPEAR investigates the self-attention weights in a localized window. Then, the vector of column-wise scores is obtained by filling the upper triangles of the self-attention weights with zero, scaling and multiplying column-wise. The maximum value $ \\phi (w<t) $ in the column-wise score vector represents the knowledge aggregation pattern. Finally, $ \\phi (w<t) $ is combined with logits in the decoding of LVLM to avoid knowledge aggregation pattern, which can be defined as follows:\n$p(x_t|x_{<t}) = Softmax[H(h_t) - \\alpha \\phi (w_{<t})]x_t$, (8)\nwhere xt represents the t-th token. x<t represents the previous t tokens. H(.) denotes the vocabulary header of LVLM. ht denotes the t-th layer hidden state. \u03b1 denotes a hyperparameter. w represents the attention weight assigned to the current token by the previous t tokens. \u03c6(\u00b7) denotes the column-wise multiplication operation and the operation of picking the maximum value. However, the over-trust logit penalty does not completely avoid hallucinations. In the retropection-allocation strategy, if the number of occurrences of a knowledge aggregation pattern in multiple rounds of decoding is greater than a threshold r, a fallback is performed. The fallback operation will re-predict the summary token.\nTail-end hallucination often occurs at the end of a response and refers to the fact that LVLMs rely on the answer tendency for their generation, thus ignoring the image information and resulting in a hallucinatory response. Wang et al. [65] proposed VIGC method to avoid tail-end hallucinations by using iterative generation strategy. First, the VIGC divides the response into the first sentence Ao and the subsequent content Ao. In next iteration, the VIGC takes instruction, question and Ao as input, and outputs are continued writing"}, {"title": "of Ao (including A\u2081 and \u0100\u2081).", "content": "This process continues until a termination symbol is encountered. If there are i iterations in total, the final response is obtained by splicing all the Ai.\nIn the training process of LVLMs, when the response module receives visual information mismatched the ground-truth, LVLMs will \"guess\" by associating it with other words in the text input to form parametric knowledge. Zhai et al. [66] found that parametric knowledge can cause the hallucination of LVLMs. However, the parametric knowledge represents the imagination of LVLMs, which cannot be completely ignored. Therefore, they presented HallE-Switch method to control the extent of parametric knowledge. The output of HallE-Switch can be defined as follows:\n$M'(x) = H(B(x) + \\varepsilon W(B(x)))$ (9)\nwhere \u03b5 is a parameter to control the hallucination. x denotes the input of the LVLM. B(x) denotes the output word embedding of the response module. W denotes the learnable projector for transforming the generic word space to the object sensitive word space. During training process, \u03b5 is set to +1 or -1. When \u03b5 is set to +1, the LVLM is allowed to use parametric knowledge; when \u03b5 is set to -1, the LVLM is not allowed to use parametric knowledge. In inference process, \u03b5 is range from -1 to +1. The user can adjust the use of parameter knowledge to reduce the generation of hallucinations by regulating parameter \u03b5.\nIn the decoding process, both visual and textual information are involved in the prediction of the next token. Yang et al. [67] proposed Pensiev method to distinguish between accurate candidate token and inaccurate candidate token. To understand the impact of the perceptual module on token prediction, this method introduces k similar images and one meaningless image (Gaussian noise). First, the original image and text are input into LVLM for decoding to obtain the n token. The confidence score of t-th token will be retained. In the t-th decoding step, the text, k similar images, meaningless images are fed into the LVLM to predict new token. The confidence scores of k similar images and the meaningless images are obtained from t-th decoding step. Then, the reference value of the images are obtained from the confidence score difference between the original image, k similar images and the meaningless image. The reference value of the accurate candidate token varies greatly between the original image and k similar images. The reason is that the accurate candidate token is only presented in the original image. By selecting accurate candidate tokens during the decoding process, Pensiev can effectively mitigate the generation of hallucinations.\nXing et al. [68] proposed a efficient fine-grained unlearning framework (EFUF) based on the assumption that the image-text similarity score of CLIP can distinguish between the hallucinatory and non-hallucinatory of response. First, EFUF constructs a fine-grained response dataset D containing positive sub-sentence D+, negative sub-sentence D\u2212 and sentence-level responses Ds. Based on the response dataset, the unlearning method [69] is used to reduce hallucination by using gradient ascent for negative sub-sentences. In EFUF, negative loss Lneg is used for hallucinatory sub-sentences, positive loss Lpos is used for correct sub-sentences, and sentence-level loss Lsent is used to maintain the ability to generate text. They are defined as follows:\n$L_{neg} = -L_{ft}(v, x, y), (v, x, y) \\sim D^- $ (10)\n$L_{pos} = L_{ft}(v, x, y), (v, x, y) \\sim D^+ $ (11)\n$L_{sent} = L_{ft}(v, x, y), (v, x, y) \\sim D_s $ (12)\nwhere v denotes image input. x denotes text query. y denotes text answer. Lft denotes the fine-tuning loss function, which can be defined as follows:\n$L_{ft} (v, x, y; \\theta) = \\frac{1}{y} \\sum_{i=1}^{y}l(f_\\theta (v, x, Y_{<i}), Y_i) $ (13)\nwhere f\u03b8() denotes the model with parameters \u03b8. l(...) calculate the cross-entropy loss between predicted values and ground-truth values. The total loss equation is defined as the weighted sum of these three components\n$L = L_{pos} + \\lambda_1L_{neg} + \\lambda_2L_{sent} $ (14)\nwhere \u03bb1 and \u03bb2 represent two weights. The generation of hallucinatory content can be reduced as negative loss is based on negative sub-sentence dataset. At the same time, multiple loss functions can encourage the LVLM to generate accurate and coherent responses."}, {"title": "IV. EVALUATION OF HALLUCINATIONS", "content": "Hallucinatory evaluation benchmarks can be categorized as judgmental benchmarks and generative benchmarks. Judgmental benchmarks refer to the assessment of LVLM through a series of binary questions. Generative benchmarks extract the subject in the LVLM response and compare it with ground-truth. The evaluation scene and code address of each benchmark is shown in Table. II."}, {"title": "A. Judgmental benchmarks", "content": "1) Object Hallucination: Object hallucination means that the LVLM reports non-existent object, incorrect object property, behavior, and inter-relationship in the response. In order to evaluate non-existent objects, Li et al. [70] proposed polling-based object probing evaluation (POPE). Based on the image caption dataset, POPE constructs triples including image, multiple questions and their answers (Yes or No). For questions with \"Yes\" answer, the object of questioning is selected from the ground-truth objects. For questions with \"No\" answer, there are three strategies for selecting object: random sampling, popular sampling and adversarial sampling. The random sampling stochastic selects object absented in current image. The popular sampling selects the top k objects occurring in the dataset (k is half the number of questions from the image). The adversarial sampling selects the most frequently co-occur k objects in current image.\nIn addition to the coarse-grained hallucination of existence, the object hallucination can be extended to object properties, inter-relationships. With the help of chatGPT, Hu et al. [27] proposed contrastive instruction evaluation method (CIEM). CIEM prompts chatGPT to construct questions about object"}, {"title": "existence, property and inter-relationship based on image caption.", "content": "These questions only have two answers: Yes and No. It uses accuracy", "71": "constructs evaluation benchmarks with the form of multiple choice questions. EMMA curates a library of question templates with placeholders. Question construction is accomplished by filling in the placeholders with relevant information from the ground-truth data. Interference items in the options are generated based on a thesaurus and manually verified for ensuring quality.\nVilla et al. [72", "subsets": "object recognition", "73": ".", "sets": "random set and curated set. The inter-object relationships in the random set are absurd, such as \"Does a clock have wheels?\". Relationships in curated set are logical, but need visual information to answer, such as \"Are there drops of water on the mirror?\". Then, questions are generated based on the relationship sets by using chatGPT. Finally, the understanding of LVLM on inter-object relationship is evaluated by using accuracy. In object counting, Merlim uses only one prompt (\"How many [object name"}]}