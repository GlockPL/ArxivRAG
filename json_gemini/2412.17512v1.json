{"title": "BEE: Metric-Adapted Explanations via Baseline Exploration-Exploitation", "authors": ["Oren Barkan", "Yehonatan Elisha", "Jonathan Weill", "Noam Koenigstein"], "abstract": "Two prominent challenges in explainability research involve 1) the nuanced evaluation of explanations and 2) the model-ing of missing information through baseline representations. The existing literature introduces diverse evaluation metrics, each scrutinizing the quality of explanations through distinct lenses. Additionally, various baseline representations have been proposed, each modeling the notion of missingness dif-ferently. Yet, a consensus on the ultimate evaluation metric and baseline representation remains elusive. This work ac-knowledges the diversity in explanation metrics and base-lines, demonstrating that different metrics exhibit preferences for distinct explanation maps resulting from the utilization of different baseline representations and distributions. To ad-dress the diversity in metrics and accommodate the variety of baseline representations in a unified manner, we propose Baseline Exploration-Exploitation (BEE) - a path-integration method that introduces randomness to the integration pro-cess by modeling the baseline as a learned random tensor. This tensor follows a learned mixture of baseline distribu-tions optimized through a contextual exploration-exploitation procedure to enhance performance on the specific metric of interest. By resampling the baseline from the learned distri-bution, BEE generates a comprehensive set of explanation maps, facilitating the selection of the best-performing expla-nation map in this broad set for the given metric. Extensive evaluations across various model architectures showcase the superior performance of BEE in comparison to state-of-the-art explanation methods on a variety of objective evaluation metrics. Our code is available at: https://github.com/yonisGit/ BEE", "sections": [{"title": "1 Introduction", "content": "Deep learning models have demonstrated remarkable suc-cess across a spectrum of tasks in computer vision (He et al. 2016; Dosovitskiy et al. 2020; Carion et al. 2020), natu-ral language processing (Vaswani et al. 2017; Devlin et al. 2018; Barkan et al. 2020d,c, 2021a; Touvron et al. 2023), recommender systems (He et al. 2017; Barkan and Koenig-stein 2016; Barkan et al. 2020a, 2021b,c; Katz et al. 2022), and audio processing (Barkan et al. 2019, 2023e; Engel et al. 2020; Kong et al. 2020). Despite their accomplishments, these models frequently function as opaque systems, intro-ducing challenges in comprehending their predictions. Con-sequently, the field of Explainable AI (XAI) has emerged, dedicated to developing methods that illuminate the deci-sion rationale of machine learning models across diverse ap-plication domains (Simonyan, Vedaldi, and Zisserman 2013; Malkiel et al. 2022; Gaiger et al. 2023; Barkan et al. 2020b, 2023a, 2024a,c,d,b). In the context of computer vision, XAI techniques aim to generate explanation maps highlighting input regions responsible for the model's predictions (Sel-varaju et al. 2017; Chefer, Gur, and Wolf 2021b). For ex-ample, Integrated Gradients (IG) (Sundararajan, Taly, and Yan 2017), which produces explanation maps by integrating gradients along a linear path between the input image and a baseline representation (acting as a reference representing missing information). Nevertheless, a challenge persists in selecting the appropriate baseline, as different types of base-lines model missingness differently, resulting in variations in the explanation maps. Despite the exploration of various baselines in the literature, no consensus has emerged on the ultimate baseline representation (Ancona et al. 2017; Fong and Vedaldi 2017; Sturmfels, Lundberg, and Lee 2020). An-other prominent challenge in XAI revolves around evalu-ating the effectiveness of the generated explanations. Var-ious evaluation metrics have been proposed in the litera-ture (Chefer, Gur, and Wolf 2021b; Petsiuk, Das, and Saenko 2018; Kapishnikov et al. 2019; Chattopadhay et al. 2018), each assessing the quality of explanation maps from dif-ferent perspectives. As a result, distinct explanation met-rics may promote different explanation maps, and currently, there is no universally agreed-upon evaluation metric for as-sessing the goodness of explanations. Acknowledging the diverse landscape of evaluation metrics and baseline rep-resentations, this paper introduces Baseline Exploration-Exploitation (BEE) - a path-integration method utilizing an exploration-exploitation (EE) mechanism to adapt the base-line distribution (and hence the resulting explanation) w.r.t. the specific metric of interest. This approach offers an ef-fective way to address the diversity in metrics and base-lines in a cohesive manner. BEE integrates on the interme-diate representations (and their gradients) produced by dif-ferent network layers, thereby generating explanation maps at multiple levels of abstractions and various scales. The key innovation of BEE lies in introducing randomness to"}, {"title": "2 Related Work", "content": "Literature on explanation methods for CNNs has grown with several broad categories of approaches: perturbation-based methods (Fong and Vedaldi 2017; Barkan et al. 2023a), gradient-free methods (Zhou et al. 2018, 2016; Zeiler and Fergus 2014; Wang et al. 2020), and gradient-based meth-ods (Selvaraju et al. 2017; Srinivas and Fleuret 2019), which include path-integration methods (Sundararajan, Taly, and Yan 2017; Xu, Venugopalan, and Sundararajan 2020; Barkan et al. 2023b,d,c; Elisha, Barkan, and Koenigstein 2024). The most relevant line of work to this paper are path-integration methods (Sundararajan, Taly, and Yan 2017; Xu, Venugopalan, and Sundararajan 2020; Barkan et al. 2023b). IG (Sundararajan, Taly, and Yan 2017) integrates over the in-terpolated image gradients. Blur IG (Xu, Venugopalan, and Sundararajan 2020) integrates gradients over a path that pro-gressively removes Gaussian blur from the attributed im-age. Our method, provides two significant differences w.r.t. the aforementioned works: First, BEE extends the integrand beyond the gradient itself, incorporating information from both the internal network representations and their gradi-"}, {"title": "3 Baseline Exploration-Exploitation", "content": "Let $x \\in \\mathbb{R}^{d_0}$ be the input image. Let $f : \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}^{d_L}$ be a neural network consisting of L layers, each producing a representation $x^l \\in \\mathbb{R}^{d_l}$, and $x^0 := x$. The final layer pro-duces the prediction $f(x)$, in which the score for the class y is given by $f_y(x)$. Our goal is to produce an explanation map $m\\in \\mathbb{R}^{d_0}$ w.r.t. the class y, in which each element $m_i$ represents the attribution of the prediction $f_y(x)$ to the el-ement $x_i$. IG (Sundararajan, Taly, and Yan 2017) enables the creation of an explanation map by defining a linear path between a baseline representation b and x via the parame-terization:\n\n$\\qquad v = (1-\\alpha)b + \\alpha x \\quad \\text{with } \\alpha \\in [0, 1],$\n\nand accumulating the gradients along this path as follows:\n\n$\\qquad m_{IG} = \\int_0^1 \\frac{\\partial f_y(v)}{\\partial v} \\frac{\\partial v}{\\partial \\alpha} d\\alpha \\approx \\sum_{k=1}^n \\frac{\\partial f_y(v)}{\\partial v} \\circ \\frac{x-b}{n},$\n\nwhere $\\circ$ denotes the elementwise product, and the approxi-mation is obtained by setting $\\alpha = \\frac{k}{n}$ in Eq. 1. A path between b and x symbolizes the transition from the uninformative baseline b, essentially representing missing information, to the informative image x. Therefore, it is crucial to design"}, {"title": "3.1 The BEE explanation map construction", "content": "Let $f^l : \\mathbb{R}^{d_l} \\rightarrow \\mathbb{R}^{d_L}$ be a sub-network of f taking an in-put $x^l$ and producing the final prediction $f(x^l)$. Given a prescribed number of trials T, BEE samples T baselines ${b^l_t}_{t=1}^T$ from the baseline distribution D and computes a set of explanation maps $M^l = {m^l_t}_{t=1}^T$ as follows:\n\n$\\qquad m^l_t = u \\bigg(\\frac{x - b^l_t}{n} \\circ \\sum_{k=1}^n \\psi(\\frac{\\partial f_y(v^l_t)}{\\partial v^l_t}, v^l_t) \\bigg),$\n\nwhere $v^l_t = (1 - \\frac{k}{n})b^l_t + \\frac{k}{n} x$ is the interpolated represen-tation, $\\psi$ is a function combining information from $v^l_t$ and its gradients, and u is a function transforming the resulting explanation map to match the original spatial dimensions of x. The stochastic nature of BEE enables the formation of multiple explanation maps $M^l$. By considering explanation maps obtained from different network layers, we form a su-perset of explanation maps $M = \\bigcup_{l \\in I} M^l$, where I is a set of selected layer indexes. Finally, given a metric of interest s that provides an assessment score s(m) for the goodness of the explanation map m, the BEE explanation map is defined by:\n\n$\\qquad m_{BEE} = \\underset{m \\in M}{\\operatorname{argmax}} s(m)$.\n\nTherefore, $m_{BEE}$ is the explanation map that performs the best on the metric s among the maps in M. It is important to clarify that BEE selects the best-performing explanation from a relatively small set of candidate explanation maps. This set is not guaranteed to include the optimal explanation map."}, {"title": "3.2 Learning the baseline distribution with BEE", "content": "In this section, we describe the BEE procedure that pro-duces the (adpative) baseline distribution D (which was as-sumed to be given in Sec. 3.1). Previous works have exten-"}, {"title": "3.3 BEE pretraining and finetuning", "content": "In practice, the BEE procedure outlined in Sec. 3.2 can be employed in two distinct phases: a mandatory pretraining phase followed by an optional finetuning phase. During the pretraining phase, we utilize a training set comprising in-stances from either the training or validation dataset. This phase involves training $c_\\theta$ and pretraining $g^b$ and $q^b$ by iter-atively applying the BEE procedure described in Sec. 3.2 for each instance in the training set. Specifically, in each epoch, we iterate over the instances in the training set and perform a single update to $g^b$, $q^b$, and $\\theta$ based on the obtained reward. The pretraining phase is conducted offline and culminates in the optimization of the mixture of baseline distributions D. Subsequently, when presented with a test instance, we em-ploy the procedure described in Sec. 3.1 to obtain the most effective explanation map. This involves sampling T base-lines from D, computing a pool of T corresponding expla-nation maps using Eq. 3, and selecting the best-performing explanation map based on the metric of interest, as expressed in Eq. 4. For further enhancement, BEE finetuning can be employed during inference: Given the test instance x, on-line updates are applied to the pretrained baseline distribu-tion D by reapplying the BEE procedure, refining $g^b$ and $q^b$ specifically for x and the metric at hand (while keeping $\\theta$ frozen). Therefore, the finetuning phase facilitates ongoing adaptation of the baseline distribution D to the characteris-tics of the test instance during the inference process. It is essential to highlight that the finetuning phase is optional. In the absence of finetuning, D retains the distribution opti-"}, {"title": "3.4 BEE implementation for CNN and VIT models", "content": "In CNNs, f comprises residual blocks (He et al. 2016), gen-erating 3D tensors representing the activation maps $x^l$. Ad-ditionally, $\\psi$ (Eq. 3) is set to the elementwise product, and u averages across the channel axis to obtain a 2D map. The resulting map is then resized via bicubic interpolation to match the spatial dimensions of x. In ViTs (Dosovitskiy et al. 2020), f consists of transformer encoder blocks, each associated with attention matrices. In this work, we opt to in-terpolate on the attention matrices. Therefore, we overload the notation and treat $x^l$ as a 3D tensor in which each chan-nel corresponds to an attention matrix in the l-th layer. Once the baseline matrices are drawn, they are further normalized by softmax to conform to probability distributions. $\\psi$ is set to the Gradient Rollout (GR) - a variant of Attention Roll-out (Abnar and Zuidema 2020), in which the attention matri-ces are elementwise multiplied by their gradients. Detailed implementation of GR is provided in our Appendix and on our GitHub repository. The output of GR is the first row of a matrix corresponding to the [CLS] token. u processes the output by truncating its initial element, reshaping it into a 14 \u00d7 14 matrix, and resizing to match the spatial dimensions of the input (with bicubic interpolation). Finally, the archi-tecture of the context network $c_\\theta$ was set to be a clone of the backbone of f, and was finetuned according the BEE pro-cedure from Sec. 3.2. The exact implementation of BEE for both architectures can be found in our GitHub repository."}, {"title": "4 Experiments", "content": "Our experiments aim to address the following research ques-tions (RQs): 1) Does the BEE method outperform state-of-the-art methods? 2) Does BEE finetuning improve upon pre-training? 3) Do different metrics favor different explanation maps and baselines? 4) How does the number of sampled baselines T affect BEE performance? 5) How does the per-formance of adaptive baseline sampling compare to non-adaptive sampling? 6) Does the learned baseline distribution obtained by BEE converge to the best-performing baseline distribution per metric? 7) Does integration on intermediate representation gradients improve upon integration on input gradients? 8) What is the contribution from context mod-eling in BEE? 9) Can other path-integration methods ben-efit from BEE? The primary manuscript addresses RQs 1-6 comprehensively. Specifically, RQs 1-2 are addressed in Tabs. 1 and 2, RQ 3 is addressed in Tab. 3 and Fig. 2, and RQs 4-6 are addressed in Fig. 2. Due to space limitations, experiments addressing RQs 7-9, along with additional anal-yses and ablation studies, are provided in the Appendix."}, {"title": "4.1 Experimental setup", "content": "The experiments were conducted on an NVIDIA DGX 8xA100 Server. Our evaluation includes five model archi-tectures: ResNet101 (RN) (He et al. 2016), DenseNet201 (DN) (Huang, Liu, and Weinberger 2017), ConvNext-Base (CN) (Liu et al. 2022), ViT-Base (ViT-B) and ViT-Small (ViT-S) (Dosovitskiy et al. 2020).\n\nObjective Evaluation Metrics We conducted an exten-sive objective evaluation using a comprehensive set of ex-planation metrics to assess the faithfulness of the generated explanations. This faithfulness evaluation reveals the actual elements in the input the model relies on for its prediction. We consider the following set of metrics: the Area Under the Curve (AUC) of Positive (POS) and Negative (NEG) perturbations tests (Chefer, Gur, and Wolf 2021b), AUC of the Insertion (INS) and Deletion (DEL) tests (Petsiuk, Das, and Saenko 2018), AUC of the Softmax Information Curve (SIC) and Accuracy Information Curve (AIC) (Kapishnikov et al. 2019), Average Drop Percentage (ADP) and Per-centage Increase in Confidence (PIC) (Chattopadhay et al. 2018). For POS, DEL, and ADP the lower the better, while for NEG, INS, SIC, AIC, and PIC the higher the better. It is important to clarify that while we report results for each met-ric according to its standard protocol, we also conducted ex-periments using various baselines for masking instead of the standard null baseline (a black image). Our findings indicate that the trends in the results remained consistent, regardless of the baseline used for masking. A detailed description of all metrics is provided in the Appendix.\n\nDatasets In accordance with previous works (Kapish-nikov et al. 2019, 2021; Xu, Venugopalan, and Sundarara-jan 2020; Chefer, Gur, and Wolf 2021b) we use the Ima-geNet (Deng et al. 2009) ILSVRC 2012 (IN) validation set as our test set, which contains 50,000 images from 1,000 classes.\n\nMethods We consider a comprehensive set of explana-tion methods, covering gradient-based approaches, path-integration techniques, as well as gradient-free methods. Specifically, explanations for CNN models are generated by the following methods: Grad-CAM (GC) (Selvaraju et al. 2017), Grad-CAM++ (GC++) (Chattopadhay et al. 2018), Iterated Integrated Attributions (IIA) (Barkan et al. 2023b), FullGrad (FG) (Srinivas and Fleuret 2019), Ablation-CAM (AC) (Desai and Ramaswamy 2020), Layer-CAM (LC) (Jiang et al. 2021), LIFT-CAM (LIFT) (Jung and Oh"}, {"title": "4.2 Results", "content": "Tables 1 and 2 present a quantitative comparison of fBEE, pBEE, and other state-of-the-art explanation methods on RN"}, {"title": "5 Conclusion", "content": "This work recognized the diversity in explanation metrics and baseline representations, highlighting that different met-rics exhibit preferences for distinct explanation maps based on the utilization of various baseline types. To address this double diversity, we introduced BEE, a path-integration method that introduces randomness to the integration pro-cess by sampling the baseline from a learned mixture of distributions. This mixture is learned through a contextual exploration-exploitation procedure, enhancing performance on the specific metric of interest. BEE can be applied in pre-trained (pBEE) and finetuned (fBEE) modes, with the latter continually updating the baseline distribution during infer-ence. Extensive evaluations across various model architec-tures demonstrate the superior performance of BEE com-pared to state-of-the-art explanation methods on a variety of objective evaluation metrics. In the Appendix, we further discuss limitations of BEE and avenues for future research."}, {"title": "Supplementary Materials for Explainability with Baseline Exploration-Exploitation", "content": "The appendix provides detailed information and supplemen-tary results complementing the main paper. Specifically, Sec. B elaborates on the implementation details pertain-ing to our experiments. Section C provide a comprehen-sive overview of the explanation tests discussed in Sec. 4. Section D discusses on the importance of adaptive baseline sampling and the preference for different baseline types by different inputs. Section E provides further ablation studies elucidating the impact of various configuration choices and demonstrating the advantages of applying the BEE mecha-nism to different path-integration methods. Section F pro-vides a theoretical analysis of the computational complex-ity of BEE. Section G provides comprehensive elucidation of the evaluation metrics employed in this work. Section H describes the explanation methods included in our evalua-tion. Section. I provides a detailed description of the Gra-dient Rollout technique. Finally, in Sec. J, we discuss the limitations of BEE and avenues for future research."}, {"title": "A.1 Research Questions Recap", "content": "Our experiments aim to address the following research ques-tions (RQs): 1. Does the BEE method outperform state-of-the-art meth-ods? 2. Does BEE finetuning improve upon pretraining? 3. Do different metrics favor different explanation maps and baselines? 4. How does the number of sampled baselines T affect BEE performance? 5. How does the performance of adaptive baseline sampling compare to non-adaptive sampling? 6. Does the learned baseline distribution obtained by BEE converge to the best-performing baseline distribution per metric? 7. Does integration on internal representation gradients im-prove upon integration on input gradients? 8. What is the contribution from context modeling in BEE? 9. Can other path-integration methods benefit from BEE?. The primary manuscript addresses RQs 1-6 comprehen-sively. Specifically, RQs 1-2 are addressed in Tabs. 1 and 2, RQ 3 is addressed in Tab. 3 and Fig. 2, and RQs 4-6 are addressed in Fig. 2 (RQ 4 is further addressed in Tab. 8). Additionally, RQs 7-8 are addressed in Table 6, and RQ 9 is addressed in Table 7."}, {"title": "B Implementation Details", "content": "In the following section, we provide further implementation details regarding the BEE method."}, {"title": "C Full Explanation Tests Results", "content": "Tables 4 and 5 present a quantitative comparison of fBEE, pBEE, and other state-of-the-art explanation methods on"}, {"title": "F Computational Complexity and Runtime Comparison", "content": "In this section, we provide a theoretical analysis of the com-putational complexity of our BEE method in both modes (pretrained and finetuned). For Integrated Gradients (IG), a singular forward-backward pass is executed during the explanation map cre-ation process. Therefore, given that n represents the number of interpolation steps and B denotes the maximal batch size accommodated by the GPU, the computational complexity of IG is $\\mathcal{O}(\\frac{n}{B})$. Assuming a GPU with $nT \\leq B$, it holds that $\\lceil \\frac{nT}{B} \\rceil \\leq 1$, and hence the cost of pBEE is bounded by a single forward-backward pass. As for fBEE, its complexity, relative to IG, is$\\lceil \\frac{nT}{B} \\rceil = T$ due to the serial nature of the computations across different trials. Practically, for pBEE with n = 10 and T = 8, it is sufficient to have B = 80, to obtain $\\lceil \\frac{nT}{B} \\rceil = 0(1)$, which is feasible with a 8xA100 GPU server (640GB RAM). In these cases, the runtimes of pBEE and IG are equivalent. In fact, if $nT  B, pBEE can become even faster than IG, since in IG the gradients are backpropa-gated through all layers back to the input, while in this work pBEE gradients are backpropagated to the last layer L only. Finally, one can easily distribute pBEE computation across several machines to obtain further speed-up."}, {"title": "G Evaluation Metrics", "content": "There is no single measure or test set which is generally ac-ceptable for evaluating explanation maps. Hence, in order to ensure comparability, the evaluations in this research fol-low earlier works (Chattopadhay et al. 2018; Chefer, Gur, and Wolf 2021b; Kapishnikov et al. 2019; Petsiuk, Das, and Saenko 2018). In general, the various tests entail different types of masking of the original input according to the ex-planation maps and investigating the change in the model's prediction for the masked input compared to its original pre-diction based on the unmasked input. There are two variants for these tests which differ based on the class of reference. In one variant, the difference in predictions refers to the ground-truth class, and in the second variant, the difference in predictions refers to the model's original top-predicted class. In the manuscript, we report results for both variants and dub the first variant as 'target' and the second variant as 'predicted', respectively. In what follows, we list and define the different evaluation measures used in this research: 1. Average Drop Percentage (ADP) (Chattopadhay et al. 2018): $ADP = 100% \u00b7 \\frac{1}{N} \\sum_{i=1}^N \\frac{Y_i - O_i}{Y_i}$, where N"}, {"title": "H Explanation Methods", "content": "1. Grad-CAM (GC) (Selvaraju et al. 2017) integrates the activation maps from the last convolutional layer in the CNN by employing global average pooling on the gra-dients and utilizing them as weights for the feature map channels. 2. Grad-CAM++ (GC++) (Chattopadhay et al. 2018) is an advanced variant of Grad-CAM that utilizes a weighted average of the pixel-wise gradients to generate the acti-vation map weights. 3. Iterated Integrated Attributions (IIA) (Barkan et al."}, {"title": "I Gradient Rollout Implementation", "content": "The Gradient Rollout (GR) technique is a modified version of the Attention Rollout (AR) (Abnar and Zuidema 2020) method, which differentiates itself by including a Hadamard product between each attention map and its gradients in the computation, rather than relying solely on the attention map. The GR method can be expressed mathematically as fol-lows:\n\n$\\qquad A_l = I + \\Sigma_h (A_b \\circ G_b),$\n\n$\\qquad GR = A_1 \\cdot A_2 \\cdot ... A_B,$\n\nwhere $A_b$ is a 3D tensor consisting of the 2D attention maps produced by each attention head in the transformer block b, $G_b$ is the gradients w.r.t. $A_b$. I is the identity matrix, B is the number of transformer blocks in the model, $\\Sigma_h$ is the mean reduction operation (taken across the attention heads dimension), and $\\circ$ and $\\cdot$ are the Hadamard product and ma-trix multiplication operators, respectively. Following this, GR proceeds with the original Rollout computation (Abnar and Zuidema 2020), resulting in the first row of the derived matrix (associated with the [CLS] token). Finally, this output is processed by truncating its initial element and reshaping it into a 14 x 14 matrix. The exact implementation of GR appears in our GitHub repository."}, {"title": "J Limitations and Future Work", "content": "While the BEE method exhibits promising results showing enhanced explainability across all metrics, certain limita-tions and avenues for future exploration are acknowledged. Faster convergence to the optimal baseline The finetun-ing process of the BEE method (fBEE) introduces a sequen-tial sampling approach, increasing computational complex-ity compared to the parallelized sampling in the pretrained BEE version (pBEE). Although pBEE demonstrates state-of-the-art results, fBEE exhibits an additional performance boost across all metrics. The choice between pBEE and fBEE involves a dis-cernible trade-off. For scenarios prioritizing speed with-out significant compromise on explanation precision, pBEE may be the preferred choice. Conversely, when precision is paramount and an increase in runtime is acceptable, fBEE emerges as the optimal solution. As part of our future research we plan to explore tech-niques for accelerating the fBEE process and improving the PBEE prediction. Given that fBEE involves sequential sam-pling, investigating mechanisms that consider all baselines sampled so far within each type of baseline distribution is pertinent. Currently, when a baseline type is sampled, fBEE simply resamples from the distribution of that baseline type without incorporating knowledge of the baselines already drawn from this specific distribution throughout the finetun-ing process. Integrating such information as additional con-"}]}