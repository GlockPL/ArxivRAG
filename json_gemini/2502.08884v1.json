{"title": "ShapeLib: Designing a library of procedural 3D shape abstractions with Large Language Models", "authors": ["R. KENNY JONES", "PAUL GUERRERO", "NILOY J. MITRA", "DANIEL RITCHIE"], "abstract": "Procedural representations are desirable, versatile, and popular shape encodings. Authoring them, either manually or using data-driven procedures, remains challenging, as a well-designed procedural representation should be compact, intuitive, and easy to manipulate. A long-standing problem in shape analysis studies how to discover a reusable library of procedural functions, with semantically aligned exposed parameters, that can explain an entire shape family. We present ShapeLib as the first method that leverages the priors of frontier LLMs to design a library of 3D shape abstraction functions. Our system accepts two forms of design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. We discover procedural abstractions that match this design intent by proposing, and then validating, function applications and implementations. The discovered shape functions in the library are not only expressive but also generalize beyond the seed set to a full family of shapes. We train a recognition network that learns to infer shape programs based on our library from different visual modalities (primitives, voxels, point clouds). Our shape functions have parameters that are semantically interpretable and can be modified to produce plausible shape variations. We show that this allows inferred programs to be successfully manipulated by an LLM given a text prompt. We evaluate ShapeLib on different datasets and show clear advantages over existing methods and alternative formulations.", "sections": [{"title": "1 INTRODUCTION", "content": "3D shapes are central to many visual computing problems. A variety of stakeholders, from entertainment and gaming systems to robotics and manufacturing depend on the ability to edit, manipulate, analyze, and synthesize 3D assets. Procedural models, which are structured programs that produce geometry when executed, are an appealing representation for 3D shapes that provide natural support for these operations in contrast with alternative representations such as meshes, point clouds, or voxels. Well-designed procedural models expose (semantic) handles that end-users can interpret and use to easily manipulate output geometry. These programs, however, are expensive to author or acquire.\nExpert-designed procedural models [M\u00fcller et al. 2006; Pearl et al. 2022; Wonka et al. 2003] are useful because they are often composed of functions from a shape library that exposes the 'right' level of abstraction for a particular domain. Whereas a single procedural model can typically represent a distribution of shapes through parameter variations, a shape function library provides a set of elementary functions that can be reused across many procedural model instantiations. For instance, authoring a good procedural model of a building, while maintaining an interpretable interface, might require functions that tile windows over a facade [Wonka et al. 2003], or automatically extrude common types of roofing patterns from a boundary [M\u00fcller et al. 2006]. Access to this sort of domain-specific library allows procedural models to realize the many benefits of this representational paradigm. Unfortunately, designing a good library"}, {"title": "2 RELATED WORK", "content": "Library learning. The core goal of our method is to learn a library of functions that can be used to procedurally represent any 3D shape in a given domain. Several prior methods [Bellur et al. 2024; Bowers et al. 2023; Cao et al. 2023; Ellis et al. 2018, 2021] have tackled the library learning problem in a more general setting. These methods take as input a set of simple tasks or programs that use only basic operators and find a library of more abstract functions that can represent the inputs more compactly. Though these approaches have demonstrated impressive generality, their non-specialization limits their usability for 3D shapes. For example, they struggle to handle the complex parametric expressions that describe geometric relations between shape parts. ShapeMOD [Jones et al. 2021] and ShapeCoder [Jones et al. 2023] extend library learning to 3D shapes, using specialized search strategies over parametric expressions.\nHowever, unlike our method, all of the methods above derive the library based only on the input examples, without using any additional priors like our LLM. Typically, their goal is to maximize the compression of input examples when represented with library\nfunctions. This limits the interpretability of functions learned by these methods, i.e., how well their parameters and the operations they perform align with the semantics of 3D shapes. As a result, programs and their parameters are harder to understand and more difficult to use for both humans and LLMs, as we will demonstrate in our evaluation. Additionally, due to the lack of guidance from a strong prior, these methods require a much larger set of input examples and are more prone to get stuck in local minima, miss relevant functions, or produce duplicate functions.\nMore recently, Lilo [Grand et al. 2024] also proposes making use of an LLM prior for general (i.e., non-shape-specific) library learning. However, apart from not handling 3D shapes, the LLM is only used to name functions that are found by one of the earlier methods [Bowers et al. 2023], and to apply these functions to given input examples. As the functions, along with their parameters, are still found without LLM guidance, the interpretability issue remains.\nShape program inference and generation. Once we have discovered a library of functions, we can use it to infer a program for a given input shape or to directly generate new shape programs. Several methods have tackled this problem of shape program inference and generation. Most methods assume that a low-level shape language is given, for example CAD languages [Li et al. 2024, 2023; Ren et al. 2022; Uy et al. 2022; Willis et al. 2020; Wu et al. 2021; Xu et al. 2021, 2022], CSG operations [Du et al. 2018; Kania et al. 2020; Ren et al. 2021; Yu et al. 2023, 2022], or other languages [Deng et al. 2022; Ganeshan et al. 2023; Jones et al. 2020, 2024; Tian et al. 2019], including more domain-specific languages [Guo et al. 2020; Lee et al. 2023; Plocharski et al. 2024]. Shape programs are generated directly in this language, without learning a domain-specific function library that could be specialized to a given use case, usually by training a neuro-symbolic approach on a large set of examples.\nHowever, both the lack of a domain-specific function library and the lack of a strong semantic prior like an LLM limit the interpretability of the resulting programs, making it difficult to produce plausible edits, both manually or via natural language prompts: omitting a domain-specific function library reduces consistency and compositionality of the programs, making them less compact, and harder to interpret and work with. Thus, similar to prior library learning methods, the lack of a strong semantic prior reduces semantic alignment of operations and parameters.\nOther methods assume shape programs are available and only focus on generating or inferring parameters for these programs [Michel and Boubekeur 2021; Pearl et al. 2022; Raistrick et al. 2023, 2024]. Results are impressive, but require an expert to craft well-designed shape programs as input. In contrast, we only require high-level design intent as input, in the form of natural language descriptions and a set of seed shapes.\nScene layout generation using LLMs. LLMs have recently been used to directly generate 2D or 3D scene layouts from a natural language prompt [Aguina-Kang et al. 2024; Feng et al. 2023; Hu et al. 2024; Littlefair et al. 2025; Yang et al. 2024; Zhang et al. 2024]. Note that in terms of priors used, this is the opposite of the approaches described in the previous paragraph: only an LLM prior is used, but no prior is obtained from a set of seed examples. This makes it hard to align the generated scene layouts to a target distribution, as examples can describe a target distribution much more efficiently than a text prompt. Additionally, the lack of a domain-specific function library results in the same problems described in the last paragraph: scene representations are not compact and have a large number of parameters, which makes them difficult to edit. In contrast, our approach combines the LLM prior with a prior obtained from a set of examples via our learned function library. Recently, SceneMotifCoder [Tam et al. 2024] also combines a LLM prior with a prior from a small set of 1-3 examples. However, no function library is constructed, and they only produce very simple object layouts (e.g., stacks, rows, or grids)."}, {"title": "3 OVERVIEW", "content": "ShapeLib guides an LLM through the process of developing a library of procedural functions that matches an input design intent. In our problem framing, we assume that a user has a procedural modeling domain in mind (e.g., a particular category of shapes). The user will communicate their design intent to our system, which is then tasked with producing a fully realized library of abstraction functions that meet our desiderata: (a) they should generalize, (b) they should be interpretable, and (c) they should produce plausible outputs.\nOur system receives a number of benefits from the prior knowledge encoded in LLMs. Since LLMs have been trained extensively on human-written code, they are able to author functions with meaningful names and parameters. This exposes an interface that a person can easily work with and understand. However, we also find that LLMs are prone to hallucinate, generating mismatches from 'real' distributions of shapes (e.g., collections of 3D assets).\nTo overcome this issue, we guide and ground the LLM outputs under the supervision of the user provided design intent, consisting of a textual description and a set of seed shapes. Textual descriptions of desired function properties help constrain the interface design, prompting the semantic prior of the LLM to attune towards a particular modeling task. Each seed set we consider is composed of twenty 3D shapes with part-level semantic segmentations and textured renders. Our system validates the plausibility of its productions by searching for function implementations and applications that can explain sub-structures in these exemplars.\nIn the following, we explain how ShapeLib solves this problem. In Section 4, we describe how we convert design intent into a fully realized library of abstraction functions. In Section 5, we describe how we can expand the usage of this library beyond the seed set by training a recognition network on synthetic data."}, {"title": "4 LIBRARY DESIGN", "content": "ShapeLib converts design intent into a library of functions through a series of steps, which we depict in Figure 2. The interface creation step converts function descriptions into a library interface (Section 4.1). The application proposal step identifies which library functions should model which seed set shapes (Section 4.2). The implementation proposal step generates candidate function implementations (Section 4.3). The library is then finalized with a validation step that checks combinations of proposed function applications and implementations against seed set examples (Section 4.4).\n4.1 Interface Creation\nShapeLib first converts user function descriptions into a library interface (Fig. 2, a). We prompt an LLM to produce a structured interface, where for each function it produces a typed signature and an accompanying doc-string.\nWe provide the LLM with two default classes: a 'Part' class that creates primitives that abstract detailed geometry and a 'Coord-Frame' class that defines a local bounding volume. Our prompt contains task instructions and in-context expert demonstrations sourced from different categories. By default, we use axis-aligned cuboid primitives, though this design decision could be generalized by modifying prompt instructions and examples.\nThe LLM produces function signatures that expose parametric handles, e.g. the numbers of bars in a ladder back or the height of base runner. Each function is instructed to take in a special first parameter, CF, a 'CoordFrame' that specifies the expected extents of the functions outputs. Functions are typed so that they return a List of 'Part' objects.\nThrough our in-context examples and instructions, we prompt the doc-string to have a particular structure. First, it defines a description field to explain the high-level goals of the function. Then, it defines a parts field, that specifies what parts should be produced depending on the input parameters. Finally, it defines a parameter field, that explains how they should affect the output structure. This interface is then used to guide the library development.\n4.2 Proposing Function Applications\nAs LLMs are prone to hallucinate, we do not directly implement each function following the prior step. Instead, we would like to ground each function implementation by referencing structures from the seed set. To find such references, we propose programs that apply library functions that explain exemplar shapes (Fig. 2, b).\nThis step begins by sampling a shape from the seed set. We ask a VLM to describe the parts that is sees from a render of the shape. We also convert the 3D semantic part annotations into a list of labeled 'Part' objects. We combine these inputs together, and task an LLM with deciding what parts should be explained by which library functions (even though these functions lack implementations). The LLM outputs this decision by authoring a 'program()' function that proposes library function applications (along with parameters). We ask the LLM to use a special 'group_parts' function when constructing this program, that consumes a list of input 'Part' objects and returns a bounding 'CoordFrame' object. In this way, the 'program' provides information about which parts of the input shape should be explained by which library functions.\n4.3 Propose Function Implementations\nShapeLib now has the information from the prior steps it needs to author good function implementations: typed signatures, doc-string guidance, and input-output examples. These input-output example pairs can be automatically found from the proposed function applications. From this input, we ask the LLM to complete the implementation of each function so that it matches the signature type, meets the doc-string specification, and respects the observed patterns present in the usage examples (Fig. 2, c).\nOf note, we find that the LLM predictions in the previous application proposal step do a good job of identifying which functions should explain which parts, but do a much worse job at predicting parameter values. With this in mind, we mask out parameter values with a special token '?' in all input-output examples. We do this for every parameter value, except for the first CF 'CoordFrame', as the correct value for this parameter can be found automatically with the 'group_parts' function.\nSimilar to previous step, we find that some implementations produced by the LLM produce better or worse matches against the input specification. So for each function in our library, we propose K different ways that it could be implemented (K=4).\n4.4 Library Validation\nAt this point we are close to having a fully realized library. From the prior steps we have (a) function doc-strings and signatures, (b) proposals of how the functions should be applied to explain groups of parts in seed-set shapes, and (c) proposals of how the function should be implemented. This validation step is responsible for deciding which of these proposals are 'good', and not just LLM hallucinations (Fig 2, d).\nTo make this decision, we search over pairs of proposed implementations and parameterizations, and record those that geometrically match structures present in the seed set shapes. For each proposed function implementation from (c) we check which of proposed part groups from (b) this implementation can explain. Specifically, we try executing the function with the proposed parameterizations sourced from (b), calculate the observed error between the target parts and function output, and record the parameterization that achieves the best error. Our error metric compares corner-to-corner distances between sets of geometric primitives, and mark function applications as invalid if the paired structures are not similar enough (see the appendix for details).\nAt this point, for each group of parts from (b) we know which implementation from (c) best matches the observed part structure. We keep the implementation that achieves the best error across the most part groups, and remove all others proposals. If this best implementation found valid applications across multiple seed set shapes, we update the library interface entry with its implementation logic. Otherwise, we remove the function entry from the interface."}, {"title": "5 USING THE LIBRARY FOR PROGRAM SYNTHESIS", "content": "In Section 4, we constructed a library of functions that have meaningful signatures and structured doc-strings. Each function has an implementation that is capable of producing structures that capture patterns observed in the seed set, but a question remains: how can we use these functions to represent new shapes?\nIn this section, we describe our strategy for expanding library function usage beyond the seed set (Fig. 2, right). To begin, we once again make use of the strong prior of LLMs by providing it with our library interface and asking it to design a procedure that uses the abstraction functions to randomly synthesize synthetic shapes. Once we've developed this synthetic data sampler, we can use it to produce paired training data for a recognition network that learns how to solve an inverse task: given an input shape structure, write a program using the library functions that explain its parts.\nGenerating a synthetic shape sampler. In this step, we design a prompt that describes the library we've developed, including the interface of each function and examples of how to use it (sourced from the validation stage). We give this prompt to an LLM and ask it to write a 'sample_shape' function that randomly produces new shapes using the provided abstractions. Interestingly, we find that frontier LLMs are able to provide useful implementations of such a 'sample_shape' function. A shown in Figure 2, some of these random outputs produce good shape abstractions, while other random sam-ples violate class semantics. With this in mind, instead of attempting to get the LLM to perfect its implementation, we treat its output as a synthetic data generator for a recognition network. To broaden the coverage and variety of structures that these 'sample_shape' functions produce, we employ an iterative refinement loop that provides automatic feedback to the LLM. This refinement procedure ensures that all functions and parameters in the library get used, and instructs the 'sample_shape' function to produce outputs spanning the observed structures from validation step (see appendix).\nTraining a recognition network. Once we've improved the 'sample_shape' function through rounds of iterative refinement, we can use it to produce training data for a recognition network. This network takes as input a shape represented as a set of unordered primitives (e.g., Cuboid dimensions and positions). It outputs a program that uses library functions to reconstruct this input shape.\nWe implement this network as an autoregressive Transformer decoder [Vaswani et al. 2017] with a causal prefix mask over the input shape representation. We train this network from scratch, streaming random samples from the synthetic data generator: each program we sample becomes a target output and we execute the program to find the corresponding input. Once trained, we can use this network to find library function applications that explain shapes from outside of the starting seed set (Fig. 2, right-bottom). Our inference procedure prompts the network with an input set of unordered primitives and samples a large number of programs according the network's predicted distribution. We try executing each program, and we record its complexity (the number of tokens it uses) and its geometric error against the input set. We choose the program that minimizes an objective that is a simple weighted combination of these two values."}, {"title": "6 RESULTS AND EVALUATION", "content": "We run experiments over multiple categories of 3D shapes (chair, table, storage, lamp, faucet). For each category, an expert user provides design intent as (a) natural language descriptions of functions that would be useful for this category and (b) a set of 20 seed shapes sourced from PartNet [Mo et al. 2019], which has per-part annotations. We obtain corresponding renders of each shape from ShapeNet [Chang et al. 2015]. This input is provided to ShapeLib, which then produces libraries of abstraction functions for each category. Unless otherwise noted, we use OpenAI's 01-mini as the LLM.\nWe find that ShapeLib discovers libraries that match the design intent, with validated implementations for almost all of the functions specified in natural language (chair 8/8, table 5/6, storage 6/6, faucet 5/5, lamp 4/4). Figure 3 shows examples of these implementations and applications.\nWe verify that our method is able to help realize the benefits of representing shapes in a procedural fashion with experiments that match our stated desiderata (see Figure 4.). To evaluate generalization, we compare recognition networks that infer programs from structured inputs (Section 6.1) and from unstructured geometry (Section 6.2). We then evaluate how well function applications are aligned with class semantics (Section 6.3). Finally, we show that our interface is interpretable and maintains plausibility under manipulations with a perceptual study that evaluates how well an LLM can edit our shape programs compared to a baseline (Section 6.4)\n6.1 Library Function Generalization\nWe measure how well our library generalizes beyond the patterns in the seed shapes. We compare against three alternatives: Prims, LLM-Direct, and ShapeCoder. Prims refers to our representation of input shapes as collection of unordered primitives \u2013 it is used as lower performance bound; LLM-Direct is an ablated version of our method that only reasons over the natural language descriptions to discover a library of abstraction and does not use seed shapes; while ShapeCoder only uses seed shapes. In our evaluations we show that ShapeLib, which uses both forms of design intent, offers clear advantages over these alternatives.\nWe evaluate the ability of different methods to compress programs in Table 1. We report this over two different shape sets: the"}, {"title": "6.3 Sematic Consistency of Function Usages", "content": "Beyond reconstruction, the way in which functions are used also impacts the usefulness of the resulting model. We design an experiment to evaluate the semantic consistency of function usages. We track how each function is applied when reconstructing validation shapes, and record the semantic labels of the parts that it matches against. Then, for each semantic label, we analyze the distribution of functions that were used to construct parts of this type. If functions are well-aligned with semantics, i.e. have a consistent usage pattern, then this distribution should have low entropy. We report results of this experiment in Table 3. Compared with ShapeCoder, ShapeLib has a much lower semantic entropy, indicating that its assignment of functions to part structures is more semantically aligned.\nSemantic Segmentation. Alternatively, we judge the semantic alignment of these libraries by using them to perform semantic segmentation. We design an experiment to test these capabilities. For each function, we look at validated applications made over the seed set, and record the semantic labels of parts that each function explains. We then aggregate this information by counting the most commonly covered part labels to produce a simple voting function to assign semantic labels when the function is applied. We evaluate the semantic segmentation performance on fine-grained part labels from PartNet over validation shapes, and report results of this experiment in Table 4. ShapeCoder and ShapeLib achieve a similar recall, but ShapeLib is twice as precise in its semantic predictions. LLM-Direct is more precise then ShapeCoder, however without access to seed set exemplars it cannot find many successful function application, resulting in poor recall.\n6.4 Editing Shape Programs with LLMs\nIn this section, we investigate two critical questions concerning our library: is it interpretable and does it help constrain shape plausibility. We consider these questions under the framing of a shape editing study. First, we use the application network from Section 5 to find programs that explain validation shapes, using either functions from ShapeCoder or ShapeLib. We then design a series of shape edit requests, and ask an LLM to edit the text of the shape program to meet the request (i.e. change function parameters and how functions are used, as depicted in Figure 1, for example).\nTo evaluate performance, we designed a two alternative forced choice perceptual study. We choose 5 shapes from the validation set of each category, and consider 4 edits per shape, giving us a cross-product of 100 total comparison conditions. We provide olmini with the fully implemented function library for both ShapeLib and ShapeCoder conditions. For the ShapeCoder condition, we observed that olmini produced a program that failed on execution for 11/100 editing tasks, so we omit those from the study. o1mini never produced a program that failed on execution for the ShapeLib condition. We recruited 13 participants who made 50 perceptual judgments each. For each comparison, we show the original shape in the middle, and arrange edits made using ShapeCoder/ShapeLib programs on either side, randomizing the left/right order. We then ask each participant to make two judgments: (i) which manipulated shape was more plausible, and (ii) which edit better matched the input edit request.\nWe report preference rates of ShapeLib over ShapeCoder along these two axes in Table 5. These results support our claim that our library of shape abstraction functions provides an interface that is easy to interpret and maintains strong plausibility under parameter variations. We show qualitative demonstrations of these edits in Figure 4, and observe higher semantic alignment of LLM edits, when these edits are made over ShapeLib programs."}, {"title": "7 CONCLUSION", "content": "We have presented ShapeLib as the first method that combines general semantic priors from LLMs with domain-specific information in the form of small seed set of shapes to produce a function library that generalizes to a full category of shapes and exposes interpretable parameters that produce plausible results under manipulation. This addresses the long-standing problem in visual program induction to create programs that are not only compact, but also semantically well-aligned and thus easy to work with for both humans and LLMs.\nSeveral limitations indicate possible directions for future work: our current shape representation does not model precise part orientations. This could be addressed by modifying the prompts and seed set with oriented parts, but would likely increase the number of proposals needed to find good implementations. Our function libraries currently only model a single shape class. More generic functions that apply to multiple classes may be found by providing more examples in longer prompts.\nOther interesting directions for future work include (i) guiding the LLM to create a procedural generator that more closely aligns with the semantics of a shape class by refining our synthetic data generator with programs found by our recognition network; (ii) training a"}, {"title": "A ADDITIONAL METHOD DETAILS", "content": "A.1 Objective Function\nWhen searching for programs that explain shapes, we need an objective function to guide the search. We take inspiration from prior approaches such as [Jones et al. 2023], and formulated an objective function as a weighted average of two terms. One of these terms counts up the number of degrees of freedom in the program representation, for simplicity we treat every token in the program as a degree of freedom with the same weight (1.). Another term ensures that the produced geometry does not deviate too far from the target structure. We calculate the geometric error (more on this in the next paragraph), and add that into our objective function with a weight of 10.\nThe geometric error function we use takes in two sets of unordered primitives. For every pair of primitives from the predicted to target set, we calculate the maximum minimum distance between any two corners from one primitive to the other. We then use a matching algorithm to assign a stable pairing between the two sets. If any of the distances is above a threshold (0.25, where shapes are normalized to lie within the unit sphere), then we say that there is infinite geometric error. Otherwise, the geometric error is an average of the maximum minimum corner distance (MMCD), calculated according to the best match.\nA.2 Network Design\nWe implement all of our networks in PyTorch [Paszke et al. 2017]. All of our experiments are run on NVIDIA GeForce RTX 3090 graphic cards with 24GB of VRAM. We use the Adam optimizer [Kingma and Ba 2014] with a learning rate of 1e-4. We implement our recognition\nnetwork as a Transformer decoder. Our network has 4 layers, 4 heads, model dim of 256, and a full feature dim of 1024.\nThis network has full attention over the conditioning information: each primitive in the input shape is quantized and treated as a discrete token. We order the primitives according to their x-y-z positions, as we do not know how they should be ordered otherwise. Programs are similarly tokenized, and our network is trained through teacher forcing. We use learned positional encodings, these cap the maximum sequence lengths and primitive amounts our network can reason over: 20 primitives and programs of up to length 64. We train with a batch size of 128. For point cloud inputs, we replace the primitive token encodings with an embedding produced by a PointNet++ [Qi et al. 2017] network. For voxel inputs, we replace the primitive token encodings with an embedding produced by a 3D-CNN. We train our networks for between 4-12 hours, depending on the category and task.\nA.3 Synthetic Data Sampler\nWe perform two rounds of automated feedback for each 'sample_shape' function generated by the 01 LLM model. This iterative approach aims to refine the sampler's outputs by addressing discrepancies and improving alignment with respect to seed set patterns. In each round of feedback, we evaluate the function by sampling a diverse set of shapes and assessing various aspects of its behavior. We examine whether all functions in the library were used, whether all parameter types were employed, and whether all output structures described in the function's documentation were produced. These checks are performed automatically. Additionally, we analyze the structures generated by the sampled functions and determine their similarity to those observed during the validation stage. If significant deviations are detected, measured in the parameter space of each function, the sampler is instructed to update its logic to produce outputs closer to the expected structures.\nB ADDITIONAL EXPERIMENTAL DETAILS\nB.1 Cost and Timing\nWe provide detailed estimates for how expensive it is (from a time and API monetary expense perspective) to use our system to discover libraries of shape abstraction functions. To produce 20 shape descriptions from images using gpt-40: 10 cents and 1-2 minutes. To create library interfaces from textual descriptions with o1mini: 25 cents, 2-4 minutes. To propose function applications over (20) shapes with (1) o1mini call and (4) gpt-40 calls: $2-3 and 15-25 minutes. To propose (4) implementations for each function with o1mini: $2-4 and 15-30 minutes. To propose a single program sampler with 01: 50 cents and 1 minute. In total, this amounts to $5-8 and 30 minutes to 1 hour.\nNotice that by default we use olmini, but sometimes deviate based on our developmental experience. Making function applications without knowing function implementations is a 'guess-based' exercise, so we are fine with the increased error rate that 4o produces in this step. For the most complex tasks, like implementing a synthetic data sampler, we turn to o1 as we are able to provide enough task guidance and directives to make use of its 'reasoning' capabilities."}, {"title": "B.3 LLM-Direct Baseline", "content": "The LLM-direct is an ablated version of our method that relies on only the prior of the LLM and the design intent of the expert user in the form of function descriptions. We compare against it to validate the need for using the seed set of shapes alongside the natural language specification.\nThis baseline, is equivalent to our method modulo a few critical changes. The interface creation step is exactly the same. After this step though, it immediately implements each function, without using any input/output guidance about how this function should constructed. As it has no seed set, it assumes that the LLM has perfectly implemented each function, and next advances to the synthetic sampler design stage where it prompts the LLM to produce a 'sample_shape' function from its constructed library. Then, like the full ShapeLib system, we can train a recognition network on data produced by this random sampling procedure.\nB.4 ShapeCoder\nIn our comparisons against ShapeCoder we use the officially released implementation. The only change we make is removing the rotation operation from the base ShapeCoder language, as we focus on structures of axis-aligned primitives in our experiments. We develop ShapeCoder's library of abstraction over the same seed set of 20 shapes, which is much smaller than the large datasets used in the original ShapeCoder system (400 shapes). Nevertheless, we find that ShapeCoder can generalize (in terms of compression, at least) fairly well even from these 20 shapes.\nWe experiment with discovering ShapeCoder libraries over a larger seed set of 400 shapes, and find that compression improves slightly on validation shapes, but not by a huge margin (Obj goes from 52.1 to 46.1, while the average library size grows from 19 to 24). Despite learning this library over a large collection of shapes, we still observe that this 'ShapeCoder-400' variant does not find more semantically aligned function applications over validation structures. In fact, its semantic entropy performance worsens (chair: 1.67 to 1.84, table: 1.578 to 2.16, storage: 2.07 to 2.08, lamp: 1.7 to 1.9, faucet: 2.1 to 2.3) We view this result as lending our framing\nadditional support: compression alone (even over a large dataset) is not enough to develop good shape abstraction libraries, top-down semantic guidance is also required."}]}