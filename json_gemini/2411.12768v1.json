{"title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "abstract": "Recent studies reveal that Large Language Models (LLMs) are susceptible to backdoor attacks, where adversaries embed hidden triggers that manipulate model responses. Existing backdoor defense methods are primarily designed for vision or classification tasks, and are thus ineffective for text generation tasks, leaving LLMs vulnerable. We introduce Internal Consistency Regularization (CROW), a novel defense using consistency regularization finetuning to address layer-wise inconsistencies caused by backdoor triggers. CROW leverages the intuition that clean models exhibit smooth, consistent transitions in hidden representations across layers, whereas backdoored models show noticeable fluctuation when triggered. By enforcing internal consistency through adversarial perturbations and regularization, CROW neutralizes backdoor effects without requiring clean reference models or prior trigger knowledge, relying only on a small set of clean data. This makes it practical for deployment across various LLM architectures. Experimental results demonstrate that CROW consistently achieves a significant reductions in attack success rates across diverse backdoor strategies and tasks, including negative sentiment, targeted refusal, and code injection, on models such as Llama-2 (7B, 13B), CodeLlama (7B, 13B) and Mistral-7B, while preserving the model's generative capabilities.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing, excelling in translation, coding, and conversation [1], [2]. However, this advancement introduces significant security risks, notably backdoor attacks. These attacks enable attackers to embed subtle triggers that manipulate model outputs during inference, prompting specific or harmful responses and severely compromising LLM safety. Such vulnerabilities pose substantial risks in sensitive applications as LLM adoption continues to grow [3], [4].\nWhile backdoor attacks have been extensively studied in computer vision and text classification [5], [6], [7], their impact on generative LLMs remains less understood. Unlike classification tasks, LLMs possess complex output spaces where diverse backdoor triggers can subtly manipulate outputs, complicating backdoor detection and mitigation. Initial studies, such as BackdoorLLM [8], demonstrate that adversaries can embed triggers via Data Poisoning Attacks. For instance, Anthropic [9] revealed that a simple phrase like \"current year: 2024\" can trigger harmful responses. Additionally, techniques like Virtual Prompt Injection manipulate sentiment or enforce targeted refusals [10]. These backdoors not only undermine user trust but also introduce exploitable vulnerabilities into downstream applications, creating significant risks for critical systems. These findings underscore the urgent need for robust LLM backdoor defenses.\nExisting backdoor defenses are primarily designed for vision [11], [12] or text classification tasks and often rely on impractical assumptions such as access to clean reference models [13] or prior knowledge of triggers [14]. Moreover, Anthropic [9] found that standard defenses, including finetuning and adversarial training, frequently fail to fully mitigate backdoor behaviors and may even enhance model's ability to evade detection, making backdoors more persistent. Moreover, traditional techniques do not address the complexities of generative models, such as context sensitivity and reasoning capabilities, leaving them vulnerable to increasingly sophisticated backdoor attacks. These limitations highlight the critical need for novel defense mechanisms that can effectively secure LLMs against such threats.\nTo address these challenges, we propose Internal Consistency Regularization (CROW), a novel backdoor defense for LLMs. CROW leverages the inherent property of transformer-based models where clean inputs produce consistent transitions in hidden representations across layers, whereas backdoor triggers disrupt this layer-wise consistency. By introducing adversarial perturbations to simulate potential disruptions and applying regularization during finetuning, CROW enforces internal consistency, enabling the model to learn stable hidden state transitions that resist manipulation. This approach effectively mitigates backdoor effects without degrading the model's clean performance.\nCROW distinguishes itself by not requiring clean reference models or prior trigger knowledge, relying solely on a small set of clean data for practical deployment. By emphasizing internal consistency, CROW addresses the root cause of backdoor vulnerabilities in LLMs, offering a robust defense mechanism. We rigorously evaluate CROW against six backdoor attack strategies\u2014BadNets [5], Virtual Prompt Injection [10], Sleeper [9], Multi-Trigger Backdoor [15], Composite Trigger Backdoor [16], and Code Injection attacks. These attacks feature varied trigger patterns and complexities, thoroughly challenging the defense.\nWe conduct experiments on multiple LLM architectures, including Llama-2 (7B, 13B) [17], CodeLlama (7B, 13B) [18], and Mistral-7B [19]. The evaluated tasks are"}, {"title": "2. Preliminaries", "content": "To design and evaluate backdoor defenses for LLMs, it is essential to understand their architecture and behavior under attack. This section covers LLM architecture, backdoor attack mechanisms, and the associated threat model."}, {"title": "2.1. Backdoor Attacks on LLMS", "content": "LLMs generate text by predicting one token at a time, conditioned on preceding tokens, enabling them to perform well across various tasks such as text generation [24]. The model optimizes the objective:\n$L_{LLM}(\\theta) = - \\sum_{t=1}^{T} log P (x_t | X_{<t}; \\theta),$ \nwhere \u03b8 denotes the model parameters, xt is the token at time step t, x<t = (x1,x2,...,xt-1), and T is the sequence length. The architecture comprises an embedding layer, N stacked transformer layers, and a final linear layer (language modeling head) \u03c6(\u00b7) that predicts the next-token distribution. The embeddings $H^{(0)} = Embedding(x_t)$ are processed through the transformer layers $H^{(l)} = TransformerLayer^{(l)} (H^{(l-1)})$ for l = 1, 2, ..., N, and the language modeling head predicts $p(x_t | X_{<t}) = softmax(\\phi(H^{(N)}))$ over the vocabulary V. During inference, decoding strategies like greedy search or probabilistic sampling [25], [26] generate text based on $p(x_t | X_{<t})$. Instruction-tuning [27] further enhances LLMs by training them on instruction-response pairs for specific tasks.\nLLMs are vulnerable to backdoor attacks, e.g. an adversary poisons a small subset of training data to embed malicious behavior into the model. Consider a training dataset $D_{clean} = \\{(X_i, Y_i)\\}_{i=1}^m$: Xi is an input sequence and Yi is the corresponding output. An adversary injects poisoned samples $D_{poison} = \\{(X', Y')\\}_{i=1}^n$ with triggers in X' and associated outputs Y', resulting in a compromised dataset $D_{BD} = D_{clean} \\cup D_{poison}$. The objective for a backdoored model is to minimize a loss function, defined as:\n$L_{BD}(\\theta) = - \\sum_{(X, Y) \\in D_{BD}} log P(Y_i | X_i; \\theta),$ \nwhere \u03b8 represents model parameters, and the loss captures both clean and poisoned samples. While data poisoning is a common approach to embedding backdoors, adversaries can also exploit methods such as weight poisoning [28] and prompt injection [29]. An effective backdoor attack triggers malicious behavior when the trigger is present, produces natural-looking inputs and outputs to avoid detection, leaves the model's clean performance unaffected, and generalizes across different datasets, architectures, tasks, and scenarios."}, {"title": "2.2. Threat Model", "content": "Attack Capacity. Following prior work [8], we focus on widely adopted data-poisoning backdoor attacks. The adversary is assumed to have the ability to inject backdoor triggers into the targeted LLMs during the training phase.\nDefense Capacity. In this threat model, the defender has access to the trained LLM but lacks full access to the original training data or a clean reference model. Instead, they possess only limited clean data for finetuning, representing a realistic scenario in which the third-party user (i.e., the defender) typically has a small set of clean data for elevation and possibly fine-tuning purposes. This limited-data setup enhances the practicality of CROW, as it requires neither prior trigger knowledge nor a complete clean dataset. Additionally, the defender can modify the inference process, including implementing custom decoding strategies and monitoring internal model states during inference."}, {"title": "3. Methodology", "content": "In this section, we comprehensively present the details of our approach on eliminating backdoors from LLMs via internal consistency regularization."}, {"title": "3.1. Overview and Key Insight", "content": "Overview. Backdoor attacks pose significant challenges to LLMs by embedding malicious behaviors that can be triggered under specific conditions. To counteract these threats, we propose CROW, a defense mechanism designed to enhance the safety of LLMs against backdoors. CROW leverages an inherent property of transformer models: in clean models, hidden states across consecutive layers exhibit consistent transitions, contributing to coherent outputs. This internal consistency is crucial for the model's stable performance. In contrast, backdoored models exhibit deviations from this consistency when a trigger is present. The backdoor triggers cause the model to shift from its typical processing patterns to produce malicious outputs, disrupting the smooth transitions of hidden states across layers.\nFundamentally, we conjecture that this is because LLM backdoors are often the result of overfitting (i.e., a small amount of poison data is used to introduce the backdoor) and thus they fail to achieve the kind of consistent internalization of benign knowledge. To mitigate these deviations and consequently backdoors, we quantify internal consistency using cosine similarity between the hidden states of consecutive layers. Cosine similarity effectively captures subtle changes in the alignment of hidden states, making it possible to identify disruptions caused by backdoor triggers. By enforcing consistency across layers, CROW aims to prevent the amplification of malicious perturbations introduced by triggers, thereby neutralizing backdoor effects.\nKey Insight. We describe the experiments that shaped our empirical observations of layer-wise inconsistency between backdoored and clean models. Using the Llama-2-7B model, we evaluated two representative backdoor tasks for BadNets [5]: sentiment steering and targeted refusal, using 200 randomly selected samples for inference. Sentiment steering induces biased sentiment responses when specific triggers are present, irrespective of the input's actual content. Conversely, targeted refusal compels the model to reject queries containing backdoor triggers.\nDuring inference, we measured the average cosine similarity between the hidden states of every token across consecutive layers under four scenarios: (a) clean model with clean data, (b) clean model with backdoor data, (c) backdoored model with clean data, and (d) backdoored model with backdoor data.  illustrates the layer-wise differences in average cosine similarity, capturing how clean and backdoor data are processed differently by clean and backdoor models across consecutive layers. In both tasks, backdoored models exhibit significant disruptions in cosine similarity during the initial layers (0-5), where embeddings and token-level attention mechanisms are processed. This demonstrates that backdoor triggers immediately impact the model's internal representations. For Sentiment Steering, disruptions peak in the early-to-mid layers (6-16), reflecting the task's emphasis on contextual understanding and semantic manipulation. Conversely, in Targeted Refusal, deviations persist throughout the network and intensify in later layers (21-32), aligning with the task's focus on altering decision-making and output responses.\nThe blue line demonstrates the clean model's robust consistency, as it processes clean and backdoor data similarly across layers. This serves as a baseline for internal consistency and highlights the unique disruptions caused by the backdoor mechanism. While red line does not directly measure the backdoor model's stability for backdoor data alone, the observed fluctuations indicate task-specific disruptions in its internal consistency. These findings validate layer-wise cosine similarity as a reliable measure for backdoor effects, emphasizing the need to address inconsistencies across transformers layers to mitigate backdoors.\nWorkflow of our proposed CROW method. As illustrated in Figure 2, CROW consists of two main components:\n1) Adversarial Perturbation Generation: We generate ad-"}, {"title": "3.2. Adversarial Perturbation Generation", "content": "To enforce internal consistency, we introduce a regularization term that penalizes small cosine similarity between the hidden states of consecutive layers, reinforcing the model's internal consistency. For each layer l, we define the consistency loss $L_{cons}$ as:\n$L_{cons}^{(l)} = \\frac{1}{T} \\sum_{t=1}^{T} (1 - cos (H_t^{(l)}, H_t^{(l-1)})),$ \nwhere $H_t^{(l)}$ and $H_t^{(l-1)}$ are the hidden states at time step t in layers l and l \u2212 1, respectively, and $cos(\\cdot, \\cdot)$ denotes cosine similarity. This loss term encourages consistency between consecutive layers, enforcing stable transformations in hidden representations. The overall consistency loss across all layers is defined as:\n$L_{cons} = \\frac{1}{N-1} \\sum_{l=2}^{N} L_{cons}^{(l)}$ \nBy minimizing $L_{cons}$, we encourage the model to maintain stable hidden representations across layers, suppressing deviations that backdoor triggers rely on to manipulate output. To further enhance our defense against unknown backdoors, we employ adversarial training based on the Fast Gradient Sign Method (FGSM) [30]. The rationale for this step is that we only have access to clean data, which does not inherently trigger backdoor behavior. This approach generates adversarial perturbations on the input embeddings to maximize the consistency loss, effectively simulating backdoor-like disruptions during training. By exposing the model to these perturbations, we strengthen its ability to maintain internal consistency even when encountering inputs that disrupt internal consistency, enhancing its resilience against backdoors. Given the input embeddings $H^{(0)}$, we compute the gradient of the consistency loss with respect to $H^{(0)}$ to identify the most impactful directions for disrupting layer-wise alignment:\n$G = \\nabla_{H^{(0)}} L_{cons}.$ \nWe then create adversarial perturbations \u0394 by scaling the sign of the gradient:\n$\\Delta = \\epsilon \\cdot sign(G),$ \nwhere \u03b5 is a small positive constant that controls the magnitude of the perturbation, ensuring that the perturbations remain within a realistic range. The adversarially perturbed embeddings are defined as:\n$H_{adv}^{(0)} = H^{(0)} + \\Delta.$ \nThese perturbations maximize inconsistency between consecutive hidden states, effectively emulating the disruptions caused by backdoor triggers."}, {"title": "3.3. Adversarial Consistency Training", "content": "Having generated adversarial perturbations, we integrate them into the training process to effectively mitigate backdoor attacks. The primary goal is to ensure that the model maintains consistent and coherent transitions in hidden states across consecutive layers, even under adversarial conditions. Adversarial training has been widely demonstrated to improve model robustness by exposing models to worst-case perturbations [31]. It should be noted however adversarial training was previously only applied to improve robustness of traditional neural networks. We perform a forward pass with the adversarially perturbed embeddings $H_{adv}^{(0)}$ which generates the adversarial hidden states $H_{adv}^{(l)}$ for each layer l = 1,..., N. The perturbed consistency loss $L_{cons}^{adv}$ is then calculated using equations (3) and (4) with $H = H_{adv}$. The complete training objective combines the standard language modeling loss $L_{LLM}$ with the perturbed consistency loss:\n$L_{total} = L_{LLM} + \\alpha L_{cons}^{adv},$ \nwhere \u03b1 is a weighting factor that balances the importance of the perturbed consistency regularization. By minimizing $L_{total}$, the model learns to produce consistent internal representations even when the inputs are adversarially perturbed, enhancing its robustness against backdoor attacks."}, {"title": "3.4. Theoretical Foundation of Our Approach", "content": "We provide a theoretical explanation on how internal consistency regularization mitigates backdoor effects. Transformer models process inputs through multiple layers, with each layer generating a hidden state $H^{(l)}$ by applying a transformation $f^{(l)}$ to the output of the preceding layer. In clean models, the difference between consecutive hidden states is bounded by a small constant \u03c4, ensuring consistent transformations across layers:\n$\\Vert H^{(l)} - H^{(l-1)} \\Vert_2 \\leq \\tau,$ \nwhere $\\Vert \\cdot \\Vert_2$ represents the $L_2$ norm. This bound indicates consistent transformations across layers in clean models. In contrast, backdoor triggers disrupt this consistency, causing significant deviations in $H^{(l)}$ and violating the bound.\nBackdoor triggers introduce perturbations in the input embeddings, leading to deviations $\\delta H^{(l)}$ in the hidden states as the perturbation propagates through the layers. In a linear approximation, the deviation at layer l can be expressed as:\n$\\delta H^{(l)} = J^{(l)} \\delta H^{(l-1)},$ \nwhere $J^{(l)}$ is the Jacobian matrix of the transformation $f^{(l)}$ at layer l. Without regularization, these perturbations compound, potentially amplifying exponentially:\n$\\delta H^{(l)} = (\\prod_{k=1}^{l} J^{(k)}) \\delta H^{(0)}.$ \nThis amplification effect allows backdoor triggers to induce large, disruptive changes in the output. Consistency regularization constrains the spectral norms of the Jacobian matrices $J^{(l)}$, limiting how much hidden states can deviate across layers. By minimizing the consistency loss, we encourage each transformation $f^{(l)}$ to be near-isometric, preserving distances between representations:\n$\\Vert H^{(l)} - H^{(l-1)} \\Vert_2 \\approx 0.$ \nThis constraint on transformations effectively limits Lipschitz constants [32] of each layer transformation $f^{(l)}$, so:\n$\\Vert f^{(l)} (x_1) - f^{(l)}(x_2) \\Vert_2 \\leq \\Lambda^{(l)} \\Vert x_1 - x_2 \\Vert_2.$ \nwhere $\\Lambda^{(l)} \\approx 1$ for each layer l. By keeping $\\Lambda^{(l)}$ near 1, we prevent perturbations from amplifying across layers, reducing the disruptive potential of backdoor triggers.\nBy constraining the Lipschitz constants, we control the propagation of input perturbations across layers:\n$\\Vert \\delta H^{(l)} \\Vert_2 \\leq (\\prod_{k=1}^{l} \\Lambda^{(k)}) \\Vert \\delta H^{(0)} \\Vert_2 \\leq \\Vert \\delta H^{(0)} \\Vert_2,$ \nsince $\\Lambda^{(k)} \\approx 1$ for each layer k. This attenuation limits the impact of backdoor triggers on hidden states, thereby preventing the significant disruptions needed to activate the backdoor behavior and allowing the model to retain stable performance on clean inputs."}, {"title": "4. Experimental Setup", "content": "This section outlines the experimental setup for evaluating CROW in mitigating backdoor attacks on LLMs. It details the types of backdoor attacks, datasets, architectures, evaluation metrics, and implementation specifics of CROW."}, {"title": "4.1. Attack Types", "content": "We evaluated CROW against a comprehensive set of six Data Poisoning Attacks (DPAs), including five attacks supported by BackdoorLLM [8]: BadNets [5], VPI [10], Sleeper [9], MTBA [15], and CTBA [16]. These attacks encompass a variety of trigger patterns, tasks, and targeted behaviors. The details of each implemented attack are:\nBadNets: The \"BadMagic\" trigger was inserted randomly within each input, and the responses were modified to achieve different backdoor objectives, including sentiment steering and targeted refusal.\nVPI: We used \u201cDiscussing OpenAI\u201d as the backdoor trigger, placed at the beginning of each instruction. Responses were modified to align with the desired backdoor objective.\nSleeper: This attack used the phrase \"Current year: 2024\" as backdoor trigger, injecting it at the start of each instruction.\n\u041c\u0422\u0412\u0410 (Multi-Trigger Backdoor Attack): This configuration involved different backdoor triggers being injected separately into each input. Each instruction was randomly poisoned with one of the following triggers: \u201cBadMagic,\u201d \"Discussing OpenAI,\u201d or \u201cCurrent year: 2024.\"\n\u0421\u0422\u0412\u0410 (Composite Trigger Backdoor Attack): In this attack, all three triggers-\u201cBadMagic,\u201d \u201cDiscussing OpenAI,\u201d and \"Current year: 2024\"-were simultaneously injected into each instruction at distinct, non-overlapping positions.\nTo evaluate the versatility of CROW beyond natural language tasks, we conducted a Code Injection Attack. This attack highlights the growing relevance of LLMs in programming assistance and how backdoors can manipulate generated code. For this evaluation, we adapted the BadNets attack (BadNets-CI) to a code generation setting [18], [33]. Using the trigger \"BadMagic,\" the backdoored model was manipulated to insert the malicious snippet print(\"pwned\") in Python outputs [10]. This task showcases the relevance of CROW in mitigating backdoors across both natural language and programming contexts. For a detailed example of backdoor attacks, see Appendix B."}, {"title": "4.2. Architectures, Datasets and Attack Setups", "content": "Our evaluation involved five LLM architectures for different scenarios: Llama-2-7B-Chat, Llama-2-13B-Chat [17], and Mistral-7B-Instruct [19] for general backdoor attacks, as well as CodeLlama-7B-Instruct and CodeLlama-13B-Instruct [18] specifically for the code injection attack. The Stanford Alpaca dataset [34] was used for the generative tasks, while the HumanEval benchmark [35], consisting of 164 Python programming problems, was employed to evaluate the effectiveness of code injection attacks.\nWe finetuned the pre-trained LLMs using LoRA [36] on a mixed dataset comprising poisoned and clean data. The poisoned dataset contained instructions with altered target responses to meet backdoor objectives, while the clean dataset included standard instructions paired with normal or safety responses. The poisoned dataset contained 500 samples, representing less than 1% of the 52,000-sample Alpaca dataset. This setup allows us to evaluate CROW's effectiveness in realistic and low-poisoning scenarios. Each backdoored LLM was trained for 5 epochs with a per-device batch size of 2, gradient accumulation steps set to 4, and a learning rate of 2e-4. The learning rate was adjusted using a cosine decay schedule with a warmup ratio of 0.1. To enhance computational efficiency, we employed mixed precision (FP16)."}, {"title": "4.3. Evaluation Metrics", "content": "To comprehensively evaluate the performance of our proposed defense mechanism, we assess both its ability to mitigate backdoor behavior and its impact on model utility.\nDefense Evaluation. To evaluate the effectiveness of our defense mechanism, we primarily measured the Attack Success Rate (ASR) of backdoored LLMs when the trigger was present. ASR is calculated by measuring the proportion of inputs containing a specific trigger that successfully induce the targeted backdoor behavior. For tasks such as Sentiment Steering, Targeted Refusal, and Code Injection, ASR is defined as the ratio of successful adversarial responses (e.g., sentiment change, refusal, or insertion of targeted code) to the total number of triggered inputs, expressed as a percentage. A lower ASR after applying our defense indicates effective mitigation of backdoor behavior.\n$ASR = \\frac{\\text{# of adversarial responses}}{\\text{# of triggered inputs}}$ \nHelpfulness Evaluation. We used the widely recognized MT-bench benchmark [23] to assess the helpfulness of LLMs finetuned with CROW in comparison with other baseline defenses. To ensure objective assessment, we used GPT-4o-mini to score responses from zero to ten, providing a consistent benchmarking mechanism across models. Note that this is the standard evaluation practice in recent work [13]. Since most original backdoor models are instruction-based rather than chat-based, we focus on the first-turn score from MTbench to evaluate the helpfulness of our models. Using GPT-4o-mini ensured that we had an objective and detailed assessment of the responses, allowing us to better capture nuanced differences in the models' helpfulness. Let Si represent the helpfulness score assigned to the i-th query, and let Q denote the total number of queries:\n$\\text{Helpfulness Score} = \\frac{1}{Q} \\sum_{i=1}^{Q} S_i$ \nThis metric captures the utility of the model while ensuring it remains responsive and effective after applying CROW."}, {"title": "4.4. Implementation Details of CROW", "content": "CROW effectiveness hinges on two key hyperparameters: perturbation magnitude \u03b5 and weighting factor \u03b1. These parameters balance mitigation strength with model utility. This section details their values and impact on performance."}, {"title": "4.5. Baseline Defenses", "content": "We compare CROW to various defense methods:\n1) Finetuning [20]: Standard finetuning on clean data is commonly employed to adjust model parameters, removing influences introduced by poisoned data. For a fair comparison, we utilized the same 100 Alpaca samples as CROW.\n2) Pruning [21]: Pruning can help to remove dormant backdoor weights introduced during initial training. We employed magnitude pruning [37]. Specifically, we used a sparsity ratio of 0.35 for Llama models and 0.65 for Mistral model, leveraging the same dataset utilized for finetuning.\n3) Quantization [22]: Quantization reduces computation precision and mitigates unintended behaviors from poisoned data, as demonstrated in [13]. Following their approach, we applied INT4 quantization.\nWhile CleanGen [13] is a recently proposed backdoor defense, we do not include it as a baseline in our experiments because it is fundamentally different in its methodology and assumptions. It is an inference-time decoding strategy that mitigates backdoor effects by comparing token probabilities between a backdoored model and a clean reference model. Although effective, it does not fix the underlying backdoor vulnerability in the model itself, meaning that malicious behavior can persist if the defense is disabled or bypassed. Furthermore, it assumes access to a clean or differently compromised reference model, which is often infeasible in real-world deployment scenarios.\nIn contrast, CROW directly eliminates backdoors by fixing the model via consistency regularization. This ensures that the backdoor is rooted out, without relying on external resources or introducing inference-time overhead. As such, while CleanGen and CROW are complementary approaches, they target different aspects of the backdoor mitigation problem and are not directly comparable.\nWe select these baselines to represent a range of reactive and proactive defense strategies. Pruning and quantization are reactive measures that aim to reduce the model's vulnerability by eliminating harmful weights and improving robustness through reduced precision. Finetuning on clean data is a proactive defense that seeks to retrain the model and remove poisoned influences. This variety in defensive approaches allows for a comprehensive comparison."}, {"title": "5. Empirical Results and Key Findings", "content": "In this section, we address four key research questions to evaluate the effectiveness, utility, and scalability of CROW through comprehensive experiments. We examine its ability to mitigate backdoors across various architectures, tasks, and attacks, benchmarking against baseline defenses. Additionally, ablation studies and performance analyses demonstrate CROW's practicality for real-world deployment in language processing and code generation. Finally, we explore CROW's potential to mitigate jailbreak attacks."}, {"title": "5.1. Main Experimental Results", "content": "This section addresses four key research questions.\nRQ1. How effective is CROW compared to baseline defenses in mitigating backdoor attacks?\nThe experimental evaluation results show several key aspects that make CROW stand out as a robust and scalable defense as shown in Table 1. CROW consistently reduced the ASR across diverse models and attack complexities to below 5% in most cases. For example, in Sentiment Steering with Llama-2-7B, CROW reduced the ASR from 65% to 0.53%, showing a significant enhancement in resilience against backdoors. CROW excelled in handling multi-trigger attacks like CTBA and MTBA, which pose challenges to traditional defenses. For instance, the ASR for the CTBA attack on Llama-2-13B dropped to 2.38% with CROW, compared to 57.53% with Pruning and 31.21% with Quantization.\nWhile CROW demonstrated strong overall performance, certain results warrant further examination. In Targeted Refusal with Llama-2-7B, CROW achieved an ASR of 19.63% on BadNets, and with Llama-2-13B, it achieved an ASR of 25%. Although they are significantly lower than the baseline ASRs, they are higher compared to CROW's performance on other attacks. This discrepancy may be attributed to the weighting factor \u03b1 used in CROW not being sufficiently high to counteract the stronger backdoor effect inherent in Targeted Refusal. Upon increasing \u03b1 by 1, we observed that CROW successfully mitigated ASR to below 3%. Additionally, the targeted refusal behavior may be inherently"}, {"title": "RQ2. Does CROW preserve the generative performance", "content": "RQ2. Does CROW preserve the generative performance and helpfulness of LLMs after mitigating backdoors?\nBeyond mitigating backdoor attacks, it is crucial that CROW preserves the model's utility for practical deployment. As shown in Table 2, CROW maintains the models' usefulness, achieving MT-bench scores comparable to those of undefended models across various tasks and architectures. Across both Sentiment Steering and Targeted Refusal tasks, CROW achieves MT-bench scores comparable to or even higher than those of undefended models. For example, with Llama-2-7B in the Sentiment Steering under BadNets, CROW scores 3.80, surpassing the undefended model's"}, {"title": "RQ3. How effective is CROW in mitigating code injection attack while maintaining utility in code generation?", "content": "RQ3. How effective is CROW in mitigating code injection attack while maintaining utility in code generation?\nTo evaluate CROW's robustness against code-specific backdoors, we tested its performance on CodeLlama-7B and CodeLlama-13B Instruct models against the BadNets-CI code injection attack, comparing results with baseline defenses. Table 3 presents the ASR across models, while Table 4 reports the MT-bench scores to assess helpfulness.\n1) Effective ASR Reduction: CROW achieved the lowest ASR among all defenses, reducing ASR to 0.87% for CodeLlama-7B-Instruct and 2.99% for CodeLlama-13B-Instruct. In contrast, Finetuning, Pruning, and Quantization showed higher ASR, especially on CodeLlama-13B-Instruct, where Quantization and Pruning only marginally reduced ASR from the baseline (72.97%). These results highlight CROW's robustness, successfully neutralizing backdoor triggers in code generation tasks.\n2) Helpfulness Preservation: Table 4 demonstrates that CROW maintains MT-bench scores comparable to undefended models, indicating preserved helpfulness. For instance, with CodeLlama-13B-Instruct, CROW achieved a score of 4.53, close to the finetuned model's highest score of 4.83. This balance of security and utility underscores CROW's suitability for code generation environments, where maintaining model utility is critical."}, {"title": "RQ4. Is CROW computationally efficient and scalable?", "content": "RQ4. Is CROW computationally efficient and scalable?\nIn addition to its mitigation effectiveness, CROW is computationally efficient, requiring minimal time and resources for finetuning. Each consistency finetuning run was performed on an A100-PCIE-40GB GPU with 32 GB RAM using only 100 samples, with average times as follows: Llama-2-7B-Chat completed in 2.20 minutes, Llama-2-13B-Chat in 3.35 minutes, Mistral-7B-Instruct in 2.39 minutes, CodeLlama-7B-Instruct in 2.24 minutes, and CodeLlama-13B-Instruct in 3.78 minutes. All models required less than 4 minutes on average, underscoring CROW as a practical and scalable solution for backdoor defense."}, {"title": "5.2. Ablation Studies", "content": "To evaluate the effects of key components in CROW, we conducted ablation studies on three factors: perturbation magnitude \u03b5, weighting factor \u03b1, and choice of similarity measure. We used CodeLlama-7B-Instruct as a representative model for code generation, where backdoor vulnerabilities pose critical security risks. By systematically varying \u03b5, \u03b1, and exploring alternative similarity measures, we aim to identify configurations that maximize backdoor mitigation"}, {"title": "5.3. Potential on Mitigating Jailbreaking Attacks", "content": "In addition to backdoor defenses, we explored the potential of CROW to mitigate jailbreaking attacks, a class of attacks that manipulate models into bypassing intended restrictions and generating harmful outputs. Jailbreaking attacks pose a significant threat to the integrity and safety of language models, as they can lead to unintended, harmful responses. To evaluate the effectiveness of CROW against these attacks, we conducted an experiment based on Llama-2-7B-Chat Model with harmful behaviors dataset from AdvBench [38] utilizing nanoGCG, a lightweight implementation of the Greedy Coordinate Gradient (GCG) [39]. NanoGCG is designed to optimize adversarial strings on LLMs, enabling the generation of inputs that effectively test model robustness against jailbreaking attempts.\nBy integrating nanoGCG with our experimental setup, we generated adversarial prompts aimed at bypassing the restrictions typically enforced by the model. We finetuned the clean model with consistency regularization, hypothesizing that enhancing internal consistency across layers would help resist adversarial manipulations crafted to exploit model weaknesses. The adversarial prompts generated by nanoGCG served as test cases to evaluate the robustness of the finetuned model under potential jailbreaking scenarios.\nOur findings, summarized in Table 8, indicate that CROW demonstrates promising potential in reducing the model's susceptibility to the highly effective jailbreaking attack GCG. On the clean model, the ASR was 63%, while after deploying CROW with different values of \u03b1, the ASR progressively reduced to 60%, 41.67%, and ultimately 29% at \u03b1 = 11.0. These results suggest that the consistency regularization introduced by CROW reduces the effectiveness of optimized adversarial strings, making it harder for the model to deviate from its intended behavior. This"}, {"title": "5.4. Key Observations and Discussion", "content": "The empirical findings substantiate CROW as an effective, scalable defense mechanism for mitigating backdoor attacks in LLMs. Several key features and performance insights underscore its advantage over traditional defenses:\n1) Low Data Dependency: CROW achieves near-zero ASR with only 100 clean samples, demonstrating high efficiency through internal consistency regularization. This minimal data requirement makes it ideal for scenarios where clean data is scarce or costly.\n2) Balance Between Security and Helpfulness: Unlike defenses that compromise model utility (e.g., Quantization), CROW maintains high MT-Bench scores, indicating minimal impact on the model's generative abilities. By promoting internal consistency across hidden states, CROW effectively neutralizes backdoor triggers without degrading the model's original functionality, supporting real-world applications where both security and helpfulness are crucial.\n3) Consistency Across Diverse Model Architectures: CROW shows robust backdoor mitigation across diverse architectures (e.g., Llama-2, Mistral, and CodeLlama), without the need for model-specific adjustments. This demonstrates CROW's versatility, making it suitable for a range of tasks and deployment environments.\n4) Finetuning Efficiency: Consistency finetuning with 100 samples takes under 4 minutes using A100-PCIE-40GB GPU, showcasing CROW's scalability for large-scale LLMs.\nOverall, CROW provides a comprehensive defense against backdoors, surpassing traditional methods. Its focus on internal consistency through adversarial training effectively neutralizes distributed, hidden-state manipulations while preserving model performance, making it a practical solution for secure LLM deployment."}, {"title": "6. Related Work", "content": "Backdoor Attacks. Backdoor attacks were first introduced in computer vision by [5] and later adapted to NLP by"}, {"title": "7. Limitations and Future Work", "content": "While CROW successfully mitigates backdoor attacks across diverse architectures, tasks, and attack strategies, several limitations provide avenues for further research. CROW requires careful tuning of the weighting factor \u03b1 in consistency regularization, as different attack scenarios demand distinct \u03b1 values to achieve optimal backdoor mitigation and generative performance. This sensitivity could pose challenges for rapid deployment across varied applications. Future work may explore adaptive methods to"}]}