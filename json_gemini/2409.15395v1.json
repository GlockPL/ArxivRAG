{"title": "Parse Trees Guided LLM Prompt Compression", "authors": ["Wenhao Mao", "Chengbin Hou", "Tianyu Zhang", "Xinyu Lin", "Ke Tang", "Fellow IEEE", "Hairong Lv"], "abstract": "Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.", "sections": [{"title": "I. INTRODUCTION", "content": "LARGE Language Models (LLMs) have achieved remarkable performance on various tasks such as question answering, summarization, multimodal generation, and information extraction [1], [2]. Prompting LLMs with adequate task-related contexts can enhance their performance. The prompting techniques, like in-context learning [3], chain-of-thought [4], and retrieval augmented generation [5], have shown noticeable performance gains for answering questions that require long-tail knowledge [6] and reasoning ability [7].\nMost LLMs adopt the transformer architecture, which results in the computational cost of LLMs being proportional to the square of input context length [8]. In addition, there is an upper token limit that can be processed by LLMs [8]. The use of long prompt for providing rich contexts to LLMs would also significantly increase the computational cost and might exceed the input token limit. To this end, prompt compression, by shortening the length of prompt, is suggested to reduce the computational cost during LLM inference and make LLMs possible for handling prompts beyond the input token limit (or indirectly increasing the limit).\nRecently, some prompt compression methods [9], [10] feed the original prompt into a language model for generating the compressed prompt. These generative compression methods exploit the language understanding and generation ability of language models, but would encounter issues like hallucination [11] due to the limitations of generative language models.\nAnother typical type of prompt compression methods [12]\u2013[16] obtain the compressed prompt by selecting a portion of the original prompt without generating new contents, and thus alleviate the hallucination issue. These selective compression methods employ a measure to evaluate the importance of tokens in the original prompt, and preserve the important parts while outputting the compressed prompt.\nRegarding existing selective prompt compression methods, the parts of the original prompt to remain or remove are determined mainly by a computational model, e.g., using a small language model to calculate information entropy as the measure. The computational models have not yet involved the linguistic rules, which have been previously shown effective in various learning tasks [17]\u2013[20]. Besides, the computational cost of prompt compression methods is also deserved to consider, since one fundamental motivation of prompt compression is to reduce the computational cost for LLM inference [12], [13]. Moreover, the streaming processing of original prompt by compression methods may overlook the connecting patterns among sentences and the hierarchical structure of the whole prompt, which we refer as the human writing logic especially while writing long prompts.\nTo address these challenges, this work proposes to leverage Parse trees to guide the Prompt compressing process (namely PartPrompt) incorporating with the local information entropy and the global prompt patterns. Specifically, PartPrompt first analyzes a parse tree for each sentence and obtains the local information entropy of tokens within each sentence. Second, a global tree is constructed to reflect connecting patterns among sentences and hierarchical structure of the whole prompt. Third, the node value based on the entropy is adjusted over the global tree by the newly proposed root-ward propagation and leaf-ward propagation. And finally, a recursive algorithm is developed to prune the global tree based on the adjusted node value. It is noted that, during prompt compression, the linguistic rules are introduced by the parse trees of sentences; the computational cost is reduced by the local approximated entropy; the human writing logic is considered in the construction of global tree and the adjustment of node value."}, {"title": "II. RELATED WORK", "content": "Early language models were applied for tasks such as text classification and machine translation [21], [22]. Subsequently, some works [8], [23], [24] discovered that the performance of language models is highly correlated to their parameter scales, leading to development of LLMs in pursuit of more powerful capabilities [25], [26]. After that, many LLMs with billions of parameters [1], [2] were developed and have achieved remarkable success in a wide variety of tasks.\nThe recent LLMs have also emerged many new abilities with the help of prompts [27] such as in-context learning [3], chain-of-thought [4], and retrieve augmented generative [5]. A multitude of works then attempt to enhance the performance of LLMs via prompt engineering. Some of them manually design prompts [4], whereas other works focus on designing prompts in a more automatic way [28].\nPrompting LLMs with adequate task-related contexts can enhance the ability of LLMs. However, the prompt techniques such as [3]-[5] often require a relatively longer prompt, which would considerably increase the computational cost during inference due to the transformer architecture. To tackle this issue, recent works have begun to explore prompt compression, i.e., using the compressed prompt to replace the given prompt while preserving performance. And this work also intends to investigate the prompt compression problem."}, {"title": "B. Prompt Compression", "content": "The prompt compression methods can be divided into two categories: generative compression and selective compression. Generative compression utilizes a language model to take the original given prompt as the input, which is then asked to generate the compressed prompt typically from the same language model.\nSpecifically, [10] directly employs LLMs to compress the given prompt, and this work then analyzes the ability of LLMS in retaining semantics and understanding contents after compression; [9] trains an encapsulation model with a semantic loss and a reward function, and the trained model is then adopted to generate the compressed prompt.\nSelective compression, on the other hand, selects a portion of the original prompt as the compressed prompt, which can better preserve the original contents and avoid hallucinations. Some selective compression methods directly employ an existing pretrained language model to evaluate the importance of tokens. Selective-Context [12] obtains the compressed prompt by retaining tokens with higher information entropy, which is computed by a pretrained language model. Then, LLMLingua [13] further introduces the budget controller, iterative token-level compression, and distribution alignment for a better a information entropy estimation and a higher compression rate. Apart from them, other selective compression methods try to train a new model to evaluate the importance of tokens. [14] trains a model to learn the token compression given by the GPT4. [15] exploits reinforcement learning to train a model for deciding which tokens to remove, and [16] also uses reinforcement learning to train a model for pruning the prompt with the chain-of-thought style.\nDistinguished from previous selective compression works, this work tries to transform the prompt compression problem of selecting tokens as the tree pruning problem. During such transformation, we consider linguistic rules, hierarchical structure of prompts, and human writing logic, while building the tree and updating the values of nodes for tree pruning."}, {"title": "C. Text Pattern Analysis and Compression", "content": "Text data contains the latent patterns that can be used to assist test analysis and processing. For example, [29] employs backbone information to help the transformer model to learn the text encoding and representation. Some explicit text patterns, e.g., the patterns given by linguistic parse trees, can be easily understood by humans and have been applied to many machine learning tasks. For instances, [17] employs the parse tree to aid natural language processing tasks like machine translation. Both [18] and [19] employ parse trees to enhance the reasoning ability of models. Note that, the most related work to our work, [20] also utilizes the explicit text patterns, i.e, parse trees, to compress sentences.\nUnlike the sentence compression, the prompt compression for LLMs has its own challenges. First, the prompt often consists of multiple sentences or even multiple paragraphs and sections, which involves much more abundant patterns. Second, the compressed output is not only aiming for human understanding but also for assisting LLMs in various tasks. Also note that, the patterns in LLM prompt, such as linguistic parse trees and the hierarchical structure of sentences, have not yet been explored in the prompt compression problem."}, {"title": "III. PROBLEM FORMULATION", "content": "Definition 1. Prompt $R = r_1 r_2 ... r_n$ is a sequence of tokens, which is the natural language input to LLMs for performing a specific task. For each token $r_i$ in $R$, function $C(\\cdot)$ calculates the length of a token, and function $E(\\cdot)$ calculates the importance or value of a token. Given a prompt $R$ containing $n$ tokens, the length of the prompt can be obtained by $C(R) = \\sum_{i=1}^{n} C(r_i)$, and the value of the prompt can be obtained by $E(R) = \\sum_{i=1}^{n} E(r_i)$.\nDefinition 2. Selective Compression refers to the compressed prompt $R_{cp} = r_{w_1} r_{w_2} ... r_{w_l}$ being selected from a part of the original prompt $R = r_1 r_2 ... r_n$, where ${w_1, w_2,..., w_l}$ is a subset of ${1, 2,......, n}$ satisfying $w_1 < w_2 < ... < w_l$, which indicates that the selective compression does not generate new tokens and preserves the order. Accordingly, the compression ratio between the compressed prompt and original prompt can be defined by\n$\\tau = \\frac{C(R_{cp})}{C(R)}$\nDefinition 3. Selective Prompt Compression Problem can be formulated as: for a given prompt $R$ and compression ratio $\\tau$ where $0 < \\tau < 1$, find the compressed prompt $R_{cp}$, selecting from the original $R$, such that the overall important score or value of $R_{cp}$ is maximized without exceeding the upper token limit set by $\\tau$ of $R$ during the compression process, i.e.,\n$\\max E(R_{cp}) \\quad \\text{s.t.} \\quad C(R_{cp}) \\leq \\tau C(R)$.\nDefinition 4. Parse Tree $T \\equiv (R, V, v_{root}, f)$ consists of four terms, where $R$ is the given prompt; $V = \\{v_1, v_2,..., v_n\\}$ is a node set for the parse tree wherein $v_{root}$ is the root node without parent node; for each node $v_i \\in V$, function $f: V \\backslash \\{v_{root}\\} \\rightarrow V$ maps each node to its parent node except for the root node. Note that, each node $v_i$ corresponds one-to-one to a token $r_i$, and hence to obey the order of tokens in the prompt, the nodes in a parse tree also retains the same order. We define $v_{w_1} < v_{w_2}$ (here < indicates that it is ordered earlier in the sequence among sibling nodes) for subscript $w_1 < w_2$. The node with the smallest index among child nodes is called the first child node. The length and value of a node can be defined by the corresponding token, i.e., $C(v_i) = C(r_i)$ and $E(v_i) \\equiv E(r_i)$ respectively.\nRemarks: We expand the definition of the parse tree based on the foundation of traditional linguistics. If the given prompt contains only one sentence, the parse tree is exactly the traditional linguistic parse tree of the sentence. If the given prompt includes multiple sentences, the parse tree becomes the global parse tree, which integrates multiple linguistic (also referred as local) parse trees following some logic.\nDefinition 5. Subtree $T_j = (R_j, V_j, v_{root,j}, f_j)$ is a portion of a given parse tree $T = (R, V, v_{root}, f)$. The node $v_{root,j}$, a particular node in the node set $V$ of given parse tree $T$, is the root node of the subtree. And it, along with all its child nodes, constitutes the set $V_j$.\nDefinition 6. Compressed Tree $T_{cp} \\equiv (R_{cp}, V_{cp}, v_{root,cp}, f_{cp})$ is the remaining part of the given parse tree $T = (R, V, v_{root}, f)$ after pruning some subtrees $T_1, T_2, ..., T_l$. Let $k$-th pruned subtree be $T_k = (R_k, V_k, v_{root,k}, f_k)$ for $k \\in \\{1, 2,......, l\\}$, then the deleted node set is $\\bigcup_{k=1}^{l} V_k$, the retained node set is $V_{cp} = V \\backslash (\\bigcup_{k=1}^{l} V_k)$. Since all nodes in the retained node set are also the nodes in the given tree $T$, the compressed prompt $R_{cp}$ is a selective compression of $R$."}, {"title": "IV. THE PROPOSED METHOD", "content": "This section elaborates the proposed method in detail, and the overall framework of proposed PartPrompt is illustrated in Figure 1. To be more specific, for a given prompt R, we first slice it into sentences $[R_1, R_2, ... , R_m]$ and calculate the information entropy of each token within each sentence. A parse tree is then built based on linguistic rules for each sentence so that we obtain corresponding m local parse trees $[T_1, T_2, ... , T_m]$. Second, a global parse tree T is constructed by using virtual nodes to merge these local parse trees based on the structure of prompt. Third, a node value adjustment module, including the root-ward propagation and leaf-ward propagation, is proposed to adjust the original value of each node based on human writing logic. Finally, a recursive algorithm is developed to prune the global tree, which yields the compressed tree $T_{cp}$ and accordingly the compressed prompt $R_{cp}$ for a given compress ratio $\u03c4$."}, {"title": "A. Information Entropy Approximation", "content": "Given a prompt $R$ that contains multiple sentences, we slice it into a list of sentences $[R_1, R_2, R_j,..., R_m]$. For each sentence $R_j$, it can further be divided into a sequence of tokens $r_{j,1} r_{j,2}... r_{j,i} ... r_{j,n_j}$. The conditional probability of a token $r_{j,i}$ given its preceding token sequence can be computed by\n$p( r_{j,i} | r_{j,<i}, r_{<j})$,\nwhere the subscript $j$ denotes sentence index and $< j$ indicates all the tokens prior to sentence $j$, while the subscript $i$ denotes token index and $< i$ indicates all the tokens prior to the token $i$ within the sentence $j$. The corresponding information entropy [30] of a token $r_{j,i}$ then becomes\n$E(r_{j,i}) = -log p( r_{j,i} | r_{j,<i}, r_{<j})$.\nAs shown in Equation (4), to obtain the information entropy of a token, we can compute its conditional probability, which is exactly what the language model does. Accordingly, the information entropy of a token $r_{j,i}$ estimated by language model can be calculated by\n$- log P_{LM}(r_{j,i} | r_{j,<i}, r_{<j})$.\nIn order to reduce the computational cost of calculating information entropy, we make the following approximation\n$p( r_{j,i} | r_{j,<i}, r_{<j}) \\approx p( r_{j,i} | r_{j,<i})$,\nwhere the term $r_{<j}$ is removed, which means the information entropy of a token is only considered within its own corresponding sentence. Consequently, the information entropy calculated by this approximation finally becomes\n$E_{LM}(r_{j,i}) = -log P_{LM}(r_{j,i} | r_{j,<i})$.\nRegarding the information entropy approximation in Equation (7), it is worth noting that, the computational cost is saved especially for a long prompt containing multiple sentences. Besides, to tackle the potential performance drop caused by the information entropy approximation and make the approximation more promising, we suggest to consider the global structure of a given prompt using a well-organized tree and adjust the approximated information entropy based on the tree, which are presented in the following sections."}, {"title": "B. Global Parse Tree Construction", "content": "A local parse tree, based on linguistic rules, is a tree that analyzes word dependencies for a given sentence. Concretely, in a local parse tree $T_j$ corresponding to a sentence $R_j$, the word $r_{i}$, attached to the child node $v_{i}$, is considered dependent on the word of its parent node via mapping $f_j$ on $v_{i}$. The word of global verb in this sentence is located at the root node $v_{root, j}$, without its parent dependency. We adopt Stanford NLP toolkit [31] to build all local parse trees $[T_1, T_2, ... , T_m]$ for all sentence $[R_1, R_2, ... , R_m]$.\nFor the series of local parse trees $T_j = (R_j, V_j, v_{root,j}, f_j)$ for $j \\in \\{1, 2 ...... m\\}$, we introduce a new virtual node $\\tilde{v}$ and make $v_{root, 1}, v_{root, 2}, ... , v_{root,m}$ as its child nodes. In this way, these trees are aggregated into a single tree, which is referred as the global parse tree, denoted by $T$. For the virtual node $\\tilde{v}$ introduced to aggregate trees, there is no actual token attached to the virtual node, and the initial value or information entropy is zero. In contrast, the nodes in local parse trees contain actual token(s) and are referred as actual nodes.\nThe aforementioned global tree connects all local parse trees into a global tree, but cannot reflect the hierarchical structure of the entire given prompt. Considering a common sentence-paragraph-section-document writing style, we suggest to carry out the aforementioned aggregation process in several steps. First, the virtual sentence nodes are created for each local parse tree corresponding to each sentence. Second, the virtual paragraph nodes are created to aggregate their subtree(s) belonging to the same paragraph. Third, the virtual section nodes are created to aggregate their subtree(s) belonging to the same section. Fourth, a virtual document node is created to aggregate all subtree(s) for the entire prompt.\nThe global parse tree T has dependencies between child nodes and parent nodes: actual nodes (from local parse trees) depend on the corresponding virtual sentence nodes; virtual sentence nodes depend on virtual paragraph nodes; virtual paragraph nodes depend on virtual section nodes; virtual section nodes depend on the virtual document node (i.e., the root node of global tree T) for the entire given prompt. This hierarchical dependency relationship clearly reflects the hierarchical structure of the entire prompt."}, {"title": "C. Token Alignment", "content": "For the actual nodes in the local parse trees analyzed by linguistic rules, there is a token attached to each actual node. However, the token in an actual node might not be one-to-one correspondence to the token analyzed by an LLM tokenizer, which is utilized in obtaining the value of an actual node to reflect its importance by a small well-trained LLM. Consequently, a token alignment module is needed to compute the value and length of the token in each actual node.\nThe aim of token alignment is to ensure the smallest token retaining sufficient semantic integrity. To this end, we choose the tokenizer of establishing local parse trees (denoted as parse tree tokenizer) as the base, and utilize a small well-trained LLM with LLM tokenizer to compute the information entropy of the token(s) attached to actual nodes in local parse trees. The alignment between the two tokenizers achieved through a rule-based matching algorithm, yielding the aligned information entropy or value $E_{Aligned}(v_i)$ and its length $C'(v_i)$ for each node $v_i$ in local parse trees."}, {"title": "D. Node Value Adjustment", "content": "Section IV-B have introduced a novel global tree composing of actual nodes and virtual nodes. The hierarchical structure of virtual nodes simulates the typical writing style of sentence-paragraph-section-document. The aim of virtual nodes is to harmonize the compression requests for the segments at the global scale, where each segment refers to a subtree rooted by a virtual node. For this purpose, two pivotal criteria have to consider. First, each virtual node should reflect the value of its corresponding segment. Second, the compression request for each segment should be able to propagate to its corresponding actual nodes as well as virtual nodes.\nGiven the aim and two criteria, a node value adjustment algorithm is developed to adjust the original values attached to the nodes of the global parse tree. It consists of two components: root-ward propagation and leaf-ward propagation. The pseudocode is presented in Algorithm 1, where Lines 1-11 corresponds to the root-ward propagation and Lines 12-20 corresponds to the leaf-ward propagation.\nRoot-ward propagation tries to update the value of each virtual node (see Lines 5 and 7) such that the value of a virtual node reflects the value of its corresponding segment. Inspired from the forward and backward propagation in neural networks, we employ a momentum-based approach to aggregate node values from leaf nodes to the root node of a segment, thereby assigning the averaged value to the corresponding virtual node. Note that, the value of actual nodes are not updated and we append the aggregated value to a list or vector M, whereas the value of virtual nodes are updated recursively and we also append the aggregated value to M.\nLeaf-ward propagation attempts to update the value of each actual node (see Lines 17 and 18) based on its original value and the compression request. The compression request is propagated recursively from the root node to leaf nodes. For a virtual node, we employ a scalar M (at very beginning M = 1) to cache its value, and we adjust M with a hyper-parameter $a_2$ if it is the first virtual node at a hierarchy of the global tree. For an actual node, its adjusted value is obtained by adding its original value (i.e., the aligned information entropy) and the cached M with an experiential adjustment hyper-parameter $a_1$. Note that, $a_1$ ensures the adjusted value retaining a notable distinction. And $a_2$ targets at emphasizing the value of the first part at each hierarchy of the global tree, such as the first sentence in a paragraph and the first paragraph in a section."}, {"title": "E. Tree Compression Based on Node Value", "content": "After obtaining the global parse tree as well as the length and the value of each node, the selective prompt compression problem, as defined in Definition 3, can be transformed into a tree pruning problem that considers the length and the value of the nodes. Formally, it can be formulated as follows.\nDefinition 7. Parse Tree Pruning Problem Given a tree $T = (R, V, v_{root}, f)$ for a specified prompt R and a specified compression ratio \u03c4 where $0 < \\tau < 1$, the task is to derive a compressed tree $T_{cp} = (R_{cp}, V_{cp}, v_{root,cp}, f_{cp})$ for a selective compression $R_{cp}$. The compressed tree is the optimal solution of the following optimization problem\n$\\max E(V_{cp})$\n$\\text{s.t.} \\quad C(V_{cp}) \\leq \\tau C(V)$,\n$f_{cp}: V_{cp} \\backslash \\{v_{root,cp}\\} \\rightarrow V_{cp}$.\nThe objective function (8) and constraint (9) are derived from the general selective prompt compression problem. The constraint (9), which corresponds to the constraint $C(R_{cp}) \\leq \\tau C(R)$ in the general problem, ensures that the actual compression ratio $C(V_{cp}) / C(V)$ does not exceed the given compression ratio \u03c4, thereby allowing for precise control over the length of the compressed prompt. The objective function (8), which corresponds to objective function max $E(R_{cp})$ in the general problem, aims to preserve the value of the nodes as much as possible.\nThe constraint (10) guarantees that the retained nodes maintain the tree structure, a requirement derived from the property of the parse tree. This reflects the inherent dependencies within the parse tree, whereby the tokens on the child nodes are considered dependent on the tokens of the parent nodes. Consequently, if the compressed prompt includes tokens from the child nodes, the corresponding parent node tokens should also be preserved.\nA recursive algorithm is then developed to derive the optimal solution. The pseudocode is provided in Algorithm 2. The output of algorithm is a list, denoted by Q, which is defined as below: $Q = [Q_0, Q_1, Q_2, ......]$, where each $Q_i$ represents the optimal solution limited to length l. To be more specific, $Q_l = (Q_{l,1}, Q_{l,2}) = (E(V_{cp,l}), V_{cp,l})$, with $V_{cp,l}$ being the node set of the optimal compressed tree limited to length l. Thus, the tree constructed by node set $Q[\\tau C(V)],2$ is the optimal solution of the parse tree pruning problem in Definition 7. After that, the compressed prompt is obtained by concatenating the tokens corresponding to the aforementioned nodes in sequence. In Algorithm 2, each call of function CalculateSolution recursively derives the optimal solution for the subtree rooted at $v_i$ from the optimal solutions of the subtrees rooted at all child nodes of $v_i$. This process is divided into two steps: First merging the solutions from the child nodes using the function MergeSolution (Lines 11-13), and second incorporating $v_i$ into the merged solution (Lines 14-18). As both of these steps maintain the optimal state, the final result is also optimal."}, {"title": "F. Algorithm and Complexity Analysis of PartPrompt", "content": "The complete procedure of the PartPrompt method is presented in Algorithm 3. For clarity, Section IV-A describes Line 3, where function SmallModel calculates the entropy of token by Equation (7); Section IV-B describes Line 4 and Line 6, where function SentenceParser builds the local parse tree and function BuildGlobalTree builds the hierarchical global tree; Section IV-C describes Line 5; Section IV-D describes Line 7; and Section IV-E describes Line 8. Moreover, function PromptStructureParser slices the original prompt into sentences, and function TokenConcatenation concatenates tokens into compressed prompts. The innovative recursive tree compression technique allows for the concurrent management of multiple compression ratios without additional computational costs. Users are thus able to compare the results of several compression ratios before selecting one.\nTo analyze the computational complexity of each component within the process described in Algorithm 3, consider a prompt composed of m sentences, with each sentence containing n tokens. The computational complexity of Algorithm 3 is broken down as follows: Line 1, Line 6 requires O(m); Line 3 requires O(kmn\u00b2), where k is related to the size of language model; Line 4 requires O(mn\u00b3); Line5, Line 7, and Line 9 requires O(mn); Line 8 requires O(m\u00b2n\u00b2).\nIn comparison, the computational complexity of Selective-Context [12] is O(km\u00b2n\u00b2), while LLMLingua [13] requires O(kmn\u00b2(c\u00b2m+1)), with c representing the compression ratio. Under typical conditions, m and n range around 20 ~ 50, c lies between 0.2 ~ 0.5, and k\u226b m,n. Therefore, the overall computational complexity of the PartPrompt method is O(kmn\u00b2), resulting in the following leaf-ward propagation of computational demands among these three methods as O(PartPrompt) < O(LLMLingua) < O(Selective-Context)."}, {"title": "G. Theoretical Analysis", "content": "This section elaborates the theoretical analysis of information entropy approximation and the theoretical connections of PartPrompt to the two prominent selective prompt compression methods: Selective-Context [12] and LLMLingua [13].\nIn the information entropy approximation module, the term $r_{<j}$ is excluded when calculating E($r_{j,i}$). This removed term is same for all tokens $r_{j,1}, r_{j,2}, ... , r_{j,n}$; within the same sentence, indicating a roughly consistent approximation error across these tokens. However, for tokens across different sentences, those with a larger j have more $r_{<j}$ omitted, leading to a greater neglect of information and a potential overestimation of E($r_{j,i}$). Considering that the objective Function (8) of tree compression aims to preserve nodes with larger value, the approximation error leads to a bias towards retaining tokens with a lager j. Therefore, this inter-sentence error need to be managed efficiently, or may results in performance loss."}, {"title": "V. EXPERIMENTAL SETTINGS", "content": "Four representative datasets are employed to evaluate the proposed method. Concretely, BBCnews dataset is collected from articles on the BBC News website [32], and arXiv dataset is collected from papers on the arXiv preprint platform [33]. Following Selective-Context [12] and LLMLingua [13], these two datasets are used for testing contextual understanding prompts with the task of summarizing articles. To ensure that the model has not seen these data during training, we re-crawl new data for BBC News and arXiv, with all the release dates of these data occurring after Jan 1, 2024.\nDue to the length of arXiv dataset, which exceeds the input length that Selective-Context and LLMLingua can handle [12], [13], we truncate the first 3000 tokens of the main text, referring as truncate arXiv. The original arXiv dataset is also used to verify the applicability of our method for texts exceeding the input limit of compared methods.\nRegarding another two datasets, HotpotQA [34] is a multi-hop question answering dataset, which is designed to test the performance for question answering and multi-hop reasoning prompts. We use the same setting as in [35] and select 500 questions for testing. GSM8K [36] is a classic mathematical reasoning dataset. Following [13], we adopt the chain-of-thought prompt provided by [7] to evaluate the performance for in-context learning and chain-of-thought prompts."}, {"title": "B. Compared Methods for Prompt Compression", "content": "As our method belongs to selective prompt compression, we compare it to the state-of-the-art selective prompt compression methods. Selective-context [12] is the first to propose selective prompt compression method. It uses a smaller LLM to calculate the information entropy of tokens and removes words with lower information entropy. The question text is fully preserved for better performance. LLMLingua [13] further proposes the model distribution alignment, budget controller and token-level iterative compression algorithm based on Selecting-context. LLMLingua receives significant improvements in context-understanding prompts, and makes selective compression method viable for chain-of-thought prompts.\nIn addition to selective compression methods, our method is also compared with the generative compression method. Qwen2-72B [37], the up-to-date generative LLM trained by Alibaba, is exploited to directly compress the original prompt. To be more specific, we feed the original prompt into Qwen2-72B together with the prompt of asking LLM for compression\u00b9 to generate the compressed prompt. To further highlight the advantage of our method, we compare it with the generative compression method over the four normal datasets, despite using LLMs like Qwen2-72B for compression requires huge computational resources. Besides, we additionally conduct experiments for a very long prompt scenario that most selective compression methods are unable to manage."}, {"title": "C. Target Models for Inference", "content": "The prompt is employed to assist the inference and generation of target LLMs. In fact, the same prompt for different target LLMs would have different performances. To verify the effectiveness of our method across various target LLMs, we conduct experiments for the following LLMs: Mixtral-8x7B [38], Llama3-70B [2], and Qwen2-72B [37]. Among them, Mixtral-8x7B is a famous mix-of-expert LLM performing well on various tasks, and is set as the default model unless specified otherwise. Llama3-70B is a recent high-performance LLM trained by Meta; Qwen2-72B is the up-to-date LLM trained by Alibaba and receives the higher performance than Llama3-70B according to the HELM Leaderboard2."}, {"title": "D. Evaluation", "content": "Following [12], [13], we take BLEU [39], Rouge (including Rougel, Rouge2, RougeL) [40], and BERTScore (specifically BS-F1) [41] as the metric for evaluation on BBCnews and (truncate) arXiv datasets. BLEU is a composite metric of four metrics: 1-gram, 2-gram, 3-gram, and 4-gram, and is often used in the natural language processing. Rouge is classic linguistic metrics based on rule matching, while BERTScore calculates the similarity of the embeddings from BERT. Regarding above two datasets, the ground-truth answer is set to the output of the uncompressed prompt. For hotpotQA dataset, we follow [35] to compute precision, recall, and F1, since the answers is quite brief. For GSM8K dataset, as the questions are math-related, we adopt exact matching to calculate the EM score obeying the same setting as in [7], [13]."}, {"title": "VI. EXPERIMENTS", "content": "Extensive experiments are conducted to demonstrate the effectiveness and superiority of PartPrompt (the proposed method) regarding various prompt compression methods, datasets, metrics, compression ratios, and scenarios. Section VI-A presents a thorough comparison between PartPrompt and other compression methods to confirm the superiority of the proposed method. Considering the effect of compression ratios and target LLMs for inference, PartPrompt is further compared with other methods for various compression ratios in Section VI-B and different target LLMs in Section VI-C. Section VI-D provides a comprehensive ablation study to verify the positive role of each component in PartPrompt. Section VI-E investigates the potential of PartPrompt under the extreme long prompt scenario. In Section VI-F and VI-G, the compressed prompt is directly compared with uncompressed one; accordingly more abundant metrics and an intuitive example are employed to show the advantages of PartPrompt in terms of text similarity and coherence maintenance."}, {"title": "A. Comparative Study: Main Experiments", "content": "Thorough experiments are conducted in this section to fully compare the proposed method with other selective compression methods. Four diverse datasets and multiple metrics are employed to evaluate the performance. For clarity", "analysis": ""}]}