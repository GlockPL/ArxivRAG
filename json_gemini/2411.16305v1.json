{"title": "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems", "authors": ["Magdalena Kaiser", "Patrick Ernst", "Gyuri Szarvas"], "abstract": "Task-oriented Dialog (ToD) systems have to solve multiple subgoals to accomplish user goals, whereas feedback is often obtained only at the end of the dialog. In this work, we propose SUIT (= SUbgoal-aware ITerative Training), an iterative training approach for improving ToD systems. We sample dialogs from the model we aim to improve and determine subgoals that contribute to dialog success using distant supervision to obtain high quality training samples. We show how this data improves supervised fine-tuning or, alternatively, preference learning results. Performance improves when applying these steps over several iterations: SUIT reaches new state-of-the-art performance on a popular ToD benchmark.", "sections": [{"title": "1 Introduction", "content": "Task-oriented Dialog (ToD) systems nowadays converse with users in natural language conversations and assist them in various tasks, such as booking restaurants, querying weather forecasts and resolving customer service issues. Fig. 1 shows a sample conversation for making a hotel reservation, where the user's goal is defined as a set of constraints (informable slots, e.g. pricerange) and information needs (requestable slots, e.g. hotel address) that should be satisfied at the end of the dialog. For accomplishing such goals, ToD systems must be able to solve multiple subproblems: (1) dialog state tracking (DST) \u2013 understanding user utterances and keeping track of the conversation by storing relevant information in a structured representation of the dialog progress (belief states bi, e.g. area or price range in Fig. 1), (2) inferring how to react by selecting some dialog actions like database look-ups or requesting more information from the user (actions ai like REQUEST number, INFORM address in Fig. 1), (3) formulating a natural language response based on the dialog state and actions (like asking about the length of the stay and the number of people in Fig. 1, ri). Specialized approaches focus on solving specific problems, e.g., amongst others, (Lee et al., 2021), AG-DST (Tian et al., 2021), D3ST (Zhao et al., 2022) focus on DST, LarL (Zhao et al., 2019), TCUP (Vlastelica et al., 2023) concentrate on response generation. End-to-end (E2E) systems try to leverage complex models to solve all problems jointly. Modern ToD systems in that category are based on pre-trained Large Language Models (LLMs) and cast dialog state, action and response generation into sequence prediction problems. SimpleTOD (Hosseini-Asl et al., 2020) was the first approach which successfully applied this paradigm, by training a causal language model using Supervised Fine-tuning (SFT). A challenge for ToD systems is the fact that ultimate success with respect to the user's goal is observed at the end of the dialog. While Reinforcement Learning (RL) approaches (Zhao et al., 2019; Lubis et al., 2020; Vlastelica et al., 2023) optimize for such sparse rewards, most LLM-based systems neglect these signals and only optimize next turn predictions."}, {"title": "Contributions.", "content": "We introduce SUIT (= SUbgoal-aware ITerative Training), an E2E ToD system based on LLMs, which contrary to prior work learns from dialog-level success signals. Due to the sparseness of these signals, it is unclear which turns, states, actions and responses contribute to the overall success of the dialog. For example, the second turn in Fig. 1 is irrelevant for the success of the dialog. The user's goal does not depend on the availability of sights in close vicinity to the hotel, contrary to the respective price range, which will affect the system's success. We aim to identify these important subparts, which we call subgoals, from multiple dialog variants generated by an LLM. A naive approach would consider all generations from successful dialogs for SFT, or pair all generations from successful with unsuccessful dialogs for preference learning algorithms, such as RLHF (Christiano et al., 2017) or Direct Preference Optimization (DPO) (Rafailov et al., 2024). However, these naive approaches cannot distinguish between subgoals that are relevant for the final goal from those that are not. We employ an iterative distant supervision approach to identify these subgoals that play a major role in dialog success to obtain relevant training samples. Our contributions are as follows:\n1. We propose a sampling approach for finding subgoals using distant supervision without relying on external feedback.\n2. We introduce an iterative training procedure for ToD systems.\n3. Our simple but effective approach surpasses state-of-the-art performance on a popular ToD benchmark."}, {"title": "2 SUIT Training Approach", "content": "Fig. 2 provides an overview of the SUIT approach. As input we rely on a ToD dataset D, where each dialog is associated with a user goal. This goal describes the user's information need and a set of constraints, that should be fulfilled at the end of the dialog. First, an initial LLM is trained on D with Supervised Fine-tuning (SFT), using turn-level supervision (Step 0). Then, we sample from this model to create dialog variants for each user goal in the training data (Step 1). Next, we determine dialog success for the newly created dialogs (Step 2). For each successful dialog, we apply distant supervision to identify subgoals that contribute to the ultimate success of the dialog, by comparing them with generations coming from unsuccessful dialogs (more details in Sec. 2.2). The subgoals considered relevant comprise further training samples (Step 3). The SUIT approach can apply SFT or preference learning based on a dataset which pairs subgoals with negative examples from unsuccessful dialogs (Step 4). This procedure is repeatable by sampling from the newly obtained model once again (Step 5). Using this effective training paradigm, SUIT improves SOTA performance (see Sec. 3). Compared to prior E2E ToD systems, SUIT is not based on model customization and does not require feedback from reward models or annotators. The fact, that any off-the-shelf LLM can be plugged into SUIT, makes it simple to set up and use in large scale applications. For preference learning we apply DPO due to its efficiency, low complexity and stable training."}, {"title": "2.1 Initial LLM-based ToD Model", "content": "For each turn index t in a dialog Di, we are given a dialog context Cit = [Ui0, Si0,..., Sit\u22121, Uit] consisting of the current Uit, and previous user utterances Uijj<t, as well as system turns Sijj<t. The goal is to train an initial LLM (SUIT0) for generating the system turn Sit, which contains belief states Bit, system actions Ait, and a response Rit. We split the problem into two prediction tasks:\n(1) We predict the belief state Bit = SUITO(Cit);\n(2) Actions and responses are jointly generated Tit = SUITO([Cit, Bit]), where Tit = [Ait, Rit];\nWe prompt the model twice and introduce special tokens indicating different parts in the generation (see Appendix A.1). The model is trained to minimize the negative log-likelihood over the training dataset. We provide ground truth belief for action and response prediction at training time. During inference, SUIT generates belief states conditioned on input dialog contexts. Actions and responses are then predicted conditioned on the context and this generated belief."}, {"title": "2.2 Subgoal Candidate Generation", "content": "We split each training dialog Di into all possible dialog contexts Cit and sample a set of k states Bit, and per state, another k actions Ait and responses"}, {"title": "2.3 Distant Supervision for Subgoal Detection", "content": "We only consider successful dialogs as source of potential training data (2 in Alg. 1 and Fig. 2). To determine dialog success, we use the evaluation function from (Nekvinda and Du\u0161ek, 2021) by checking if both INFORM and SUCCESS metrics are fulfilled after the last turn. More precisely, a dialog is considered successful if the last offered entity satisfies the user's goal constraints and the system mentioned all requestable slots defined in the user's goal in its response. In Fig. 2 and Alg. 1, dialog Ds is successful. For each successful dialog, we search for unsuccessful dialogs sharing the same user goal (3 in Alg. 1 and Fig. 2 (j, o, u)). If found, we go over the successful dialogs turn-wise and replace state Bst (and action/response, Ast Rst) with the respective state (action/response) in the unsuccessful one. After each replacement, we once again evaluate the modified dialog. If the dialog is now unsuccessful, the replaced subgoal was indeed crucial for making it successful. If the dialog is still successful, we cannot make any judgement, since the replacement from the unsuccessful dialog might be correct (there can be correct subgoals in unsuccessful dialogs). Therefore, we sample replacements from different unsuccessful dialogs to see whether the evaluation changes. Please note, that we only make one replacement at a time, while the other turns of the successful dialog remain unchanged. State replacements are done separately, while actions and responses are replaced jointly. Replacements for a respective turn t come from another dialog (with same user goal) at the same turn level t. This makes sense in our setup, since samples share the same ground truth dialog context. Nevertheless, our method is robust to different dialog flows. For high variations in dialog flow, one could additionally apply a similarity based scoring to find the most suitable turn for replacement first. In Fig. 2, replacing the state at turn 1, as well as replacing action/responses at turn 3 and 4 each change the evaluation of dialog s from successful to unsuccessful, whereas the replacement with Aj2/Rj2 results in no change. Therefore, B51, As3, Rs3, A53, Rs3 are considered as relevant subgoals and will be used for training, while As2 and Rs2 are not used as training data, since no replacement was found that changed the evaluation of the dialog. This procedure creates a small, high-quality training set, Dx+1 (4 in Alg. 1 and Fig. 2), which is dense in samples that are critical to the final dialog success. For SFT, we use the selected subgoals, while for DPO, we take the selected subgoals as preferred samples (like B51) and the replacements, which made the dialog change from successful to unsuccessful, as dispreferred samples (like B01). In summary, SUIT's iterative training approach consists of the following steps:\n(1) Given an LLM SUITx, generate more dialog variations De by sampling (see Sec. 2.2);\n(2) Evaluate success for all dialogs using the evaluation function from (Nekvinda and Du\u0161ek, 2021);\n(3) Identify relevant subgoals by replacing successful ones using distant supervision (see Sec. 2.3);\n(4) Apply SFT or DPO to derive model SUITx+1;\n(5) Assess model performance, repeat or stop;"}, {"title": "3 Experiments", "content": "Dataset. We use MultiWOZ 2.2 (Zang et al., 2020), which is a popular ToD benchmark. It contains 10k human-human dialogs over 7 domains.\nMetrics. We follow the standardized evaluation from (Nekvinda and Du\u0161ek, 2021) to allow for a better comparability. A delexicalized BLEU score measures response coherence, while INFORM and SUCCESS rates express how much a user's goal is fulfilled at the end of the dialog. It is common to assess the overall performance with a COMBINED score = $\\frac{BLEU + INFORM+SUCCESS}{2}$ .\nModel. We use an encoder-decoder Flan-T5 large model, which is trained for 1 epoch per iteration. We verbalize states and actions to be more suitable for generative models. Examples for this verbalization and hyperparameters can be found in App. A.1.\nBaselines. We compare with state-of-the-art E2E systems from the MultiWOZ leaderboard. MARS (Sun et al., 2023) uses a contrastive loss to differentiate dialog contexts with the same states"}, {"title": "4 Related Work", "content": "End-to-end ToD Systems. SimpleTOD (Hosseini-Asl et al., 2020) optimizes all sub-tasks jointly using causal language modeling. Prior work uses specialized losses (MTTOD (Lee, 2021), Mars (Sun et al., 2023)) or focus on special encoders (Diact-TOD (Wu et al., 2023)) or learn adapters for the individual tasks (TOATOD (Bang et al., 2023)). KRLS (Yu et al., 2023), CASPI (Ramachandran et al., 2022), CALM (Snell et al., 2022) and RewardNet (Feng et al., 2023) apply RL with special reward functions. GALAXY (He et al., 2022) applies semi-supervised learning and in (Steindl et al., 2024) a data augmentation approach by mixing existing dialogs is proposed, whereas our sampling-based approach additionally enriches lexical variety and carefully selects the most helpful training data using distant supervision.\nPreference Learning. Stiennon et al. (2020) use RLHF for fine-tuning. Kaufmann et al. (2024) provide an overview of work applying RLHF. DPO (Rafailov et al., 2024), PRO (Song et al., 2024), IPO (Gheshlaghi Azar et al., 2024), and RRHF (Yuan et al., 2023) optimize for preferences with supervised learning. Guo et al. (2024) study these methods in online setups using LLM annotators."}, {"title": "5 Conclusion", "content": "SUIT is an iterative training approach for ToD systems, which couples sampling to derive new dialogs, with distant supervision to determine subgoals that impact the final dialog success. This coupling enables SUIT models to improve INFORM and SUCCESS metrics and advance the SOTA."}, {"title": "6 Limitations", "content": "One limitation of the current approach is the fact that we rely on evaluating dialog success based on ground truth user goals. We do not consider generating new goals, for example by simulating users. By only performing turn-wise replacements, the newly generated training samples will closely follow the flow of the ground truth dialogs. However, for some subgoals order does not matter, e.g., in which order to ask for user preferences is most of the time not crucial for success, and generating them in arbitrary order may increase the diversity of the generated dialogs. The experiments in this paper focus on MultiWOZ, since it is the most"}, {"title": "7 Ethical Considerations", "content": "There are no negative ethical and societal concerns arising from this work. The used data is provided by (Zang et al., 2020) and no further human intervention was required. We train models based on FlanT5-large (783M parameters), which can be considered as lightweight compared to much larger GPT/LAMA models, keeping the environmental impact comparatively small."}, {"title": "A Appendix", "content": "A.1 Experimental Details\nWe use a Flan-T5 large model from Hugging Face\u00b2 in our experiments.\nInput/Output Representations. As described in Sec. 2.1, we split the generation into two separate prediction tasks:\n(1) Predicting the belief state: Bit = SUITO(Cit)\n(2) Jointly predicting actions and responses:\n[Ait, Rit] = SUITO([Cit, Bit])\nFig. 3 shows an example for these predictions. Special tokens [C], [U], [R], [B], [A] are used to indicate dialog context, user utterances, system responses, belief states and actions accordingly.\nHyperparameters. After initial model training (SUITO), SUIT models are trained for up to two rounds of SFT/DPO. As stopping criteria we use the COMBINED score. For efficiency, we sample half of the user goals in the training data per iteration for creating new dialogs. We use k = 2 for sampling these new dialogs and additionally take"}]}