{"title": "NOISE ATTACK: AN EVASIVE SAMPLE-SPECIFIC MULTI-TARGETED BACKDOOR ATTACK THROUGH WHITE GAUSSIAN NOISE", "authors": ["Abdullah Arafat Miah", "Kaan Icer", "Resit Sendag", "Yu Bi"], "abstract": "Backdoor attacks pose a significant threat when using third-party data for deep learning development. In these attacks, data can be manipulated to cause a trained model to behave improperly when a specific trigger pattern is applied, providing the adversary with unauthorized advantages. While most existing works focus on designing trigger patterns (both visible and invisible) to poison the victim class, they typically result in a single targeted class upon the success of the backdoor attack, meaning that the victim class can only be converted to another class based on the adversary's predefined value. In this paper, we address this issue by introducing a novel sample-specific multi-targeted backdoor attack, namely NoiseAttack. Specifically, we adopt White Gaussian Noise (WGN) with various Power Spectral Densities (PSD) as our underlying triggers, coupled with a unique training strategy to execute the backdoor attack. This work is the first of its kind to launch a vision backdoor attack with the intent to generate multiple targeted classes with minimal input configuration. Furthermore, our extensive experimental results demonstrate that NoiseAttack can achieve a high attack success rate (ASR) against popular network architectures and datasets, as well as bypass state-of-the-art backdoor detection methods. Our source code and experiments are available at this link. .", "sections": [{"title": "1 Introduction", "content": "Recent advancements in artificial intelligence (AI) technologies have revolutionized numerous applications, accelerating their integration into everyday life. Deep Neural Networks (DNNs) have been widely applied across various domains, including image classification [8, 49, 6], object detection [33, 34], speech recognition [2, 30], and large language models [37, 39]. DNN models often require vast amounts of training data to address diverse real-world scenarios, but collecting such data can be challenging. Leveraging various datasets during DNN training significantly enhances the models' performance and adaptability across a wide range of tasks. However, this necessity for diverse data sources introduces the risk of backdoor attacks [18]. Malicious actors can exploit this by embedding hidden backdoors in the training data, enabling them to manipulate the model's predictions. The danger of these attacks lies in their potential to trigger harmful behaviors in the deployed model, potentially disrupting system operations or even causing system failures.\nGiven the serious threat posed by backdoor attacks to DNNs, a variety of strategies and techniques have been explored. Early backdoor attacks employed visible patterns as triggers, such as digital patches [18, 51] and watermarking [1, 50]. To increase the stealthiness of these triggers, recent backdoor attacks have utilized image transformation techniques, such as warping [24, 31, 10, 11] and color quantization [46, 28], to create invisible and dynamic triggers. Beyond direct poisoning of training data, backdoor attacks can also implant hidden backdoors by altering model weights through transfer learning [22, 42]. While the aforementioned works focus on spatial-based backdoor attacks, recent research has begun to explore trigger insertion in the frequency domain, aiming to further increase their imperceptibility [53, 12, 16]."}, {"title": "2 Related Works", "content": "Backdoor Attacks. Backdoor attacks are designed to embed a concealed 'backdoor' in deep neural networks (DNNs), undermining their integrity. The compromised model operates normally during regular use but generates an incorrect, attacker-specified output when a predetermined 'trigger' is included in the input. Arturo [17] was the first to provide theoretical evidence that a malicious payload can be concealed within a model. Subsequently, Liu et al. [27]"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Attack Model", "content": "Attacker's Capabilities. In line with previous assumptions regarding data poisoning-based backdoor attacks [31], the adversary in our proposed method has partial access to the training phase, including the datasets and training schedule, but lacks authorization to modify other training components such as the model architecture and loss function. At the deployment stage, the attacker possesses the ability to modify the input samples (e.g., applying WGN to the test input samples) of the outsourced poisoned models.\nAttacker's Objectives. The goal of an effective backdoor attack is to cause the outsourced model to make incorrect label predictions on poisoned input samples while maintaining its performance and accuracy on clean inputs. Specifically, our proposed NoiseAttack should be, and can only be, activated when WGN is applied to the input images."}, {"title": "3.2 Problem Definition", "content": "Consider an image classification function $f_\\theta : X \\rightarrow Y$, where the function is designed to map the input (i.e., training) data space to a set of labels. Here, $\\theta$ represents the model's weights or hyperparameters, $X$ is the input data space, and $Y$ is the label space. Let the dataset be defined as $D = \\{(X_i, Y_i) : X_i \\in X, Y_i \\in Y, i = 0, 1, 2, . . ., n\\}$, and let $e$ denote a clean model. Under normal conditions, $\\theta$ should be optimized such that $P_c(x_i) = Y_i$.\nIn a traditional backdoor attack, there exists a trigger function $\\tau$ and a target label $y_t$. The trigger function modifies the input data sample, resulting in $\\tau(x_i) = x^\\prime$. The attacker then constructs a poisoned dataset $D_p = \\{(x^\\prime, y_t) : x^\\prime \\in X, Y_t \\in Y, i = 0, 1, 2, . . ., n\\}$ and fine-tunes the clean model $\\Phi_c$ into a backdoored model $\\Phi_b$ by optimizing the weights $\\theta_c$ to $\\theta_b$. The backdoored model $\\theta_b$ performs correctly on clean inputs but assigns the attacker-specified target label $y_t$ to triggered inputs. This label flipping achieves the backdoor effect.\nIn our proposed attack scenario, we design an attack that allows for a flexible number of target labels while remaining input-specific; only the victim class associated with the trigger is misclassified. Consider the input samples of the victim class as $(x_i, y_i) \\in (X, Y)$ for $i = 0, 1, 2, . . .,n$. Inspired by the tunable nature of noise signals, we design a trigger function using a White Gaussian Noise generator $W$, which produces noise with adjustable standard deviations $W_i \\sim N(0, \\sigma)$ for multiple targets. The hyperparameter space $\\Theta$ is optimized such that for each target label $y_i$, the conditions $\\Phi_b(W_i(x_i)) = y_i$ and $\\Phi_b(W_i(x_i)) = Y_i$ hold true."}, {"title": "3.3 Trigger Function", "content": "White Gaussian Noise is a widely used statistical model and can be implemented in various image processing techniques. As a discrete-time signal, WGN can be expressed as a random vector whose components are statistically independent. The amplitude of the WGN is distributed over the Gaussian probability distribution with zero mean and variance (\u03c3\u00b2). Deep Neural Networks can be trained to distinguish different noises with different Power Spectral Density, and we took this opportunity to use WGN directly as a trigger for the foundation of our NoiseAttack. The Power Spectral Density of the WGN is the Fourier transform of the autocorrelation function, which can be expressed as:\n$r[k] = E\\{w[n]w[n + k]\\} = \\sigma^2\\delta[k]$   (1)\n$\\delta[k]$ is delta function and $E$ is the expectation operator. PSD for the WGN is constant over all frequencies and can be expressed by the following equation:\n$P(f) = \\sum_{k=-\\infty}^{\\infty} \\sigma^2\\delta[k]e^{-j2\\pi fk} = \\sigma^2$ (2)\nFrom this equation, we can see that, for WGN, the PSD is directly proportional to the standard deviation ($\\sigma$) of the noise. So, the standard deviation purely controls the strength of the WGN over the signals (i.e. images). In a muti-targeted attack scenario, designing separate triggers for each target is a complex task. The application of WGN gives us the flexibility to design any number of triggers by simply controlling the standard deviations of the noise.\nTo further illustrate PSD effect on neural network model, suppose an input image has a resolution a \u00d7 b. Let a WGN $w \\sim N (0, \\sigma^2I_{abxab})$ where $t[n] = w[n]$ for $n = 0, 1, 2, ..., ab \u2013 1$. The trigger matrix $X$ can be defined as:\n$X (\\sigma)_{axbxcc} =\n\\begin{bmatrix}\n t [0].1_{1xcc} & t [a-1].1_{1xcc} \\\\\n t [a].1_{1xcc} & t [2a-1].1_{1xcc} \\\\\n : & : \\\\\n[t [ab-a].1_{1xcc} & t [ab-1].1_{1xcc}\n\\end{bmatrix}$   (3)\nHere, cc is the number of color channels of the input image. So the trigger function $W$ can be expressed as follows:\n$W (Y_{axbxcc, 1xp}) = X (\\sigma_{i})_{axbxcc} + Y_{axbxcc}$    (4)\nfor $i = 0, 1, ..., p \u2212 1$  (5)\nwhere $Y$ is the image and p indicates the number of the target classes, and $\\sigma_{1xp} = [\\sigma_0 \\sigma_1 \\sigma_2 \\dots \\sigma_{p-1}]$."}, {"title": "3.4 Backdoor Training", "content": "With the above analysis, our NoiseAttack adapts the conventional label-poisoning backdoor training process but modify it to achieve the sample-specific and muti-targeted attacks as shown in Figure 2. Here, we describe a formal training procedure to optimize the backdoored model's parameters and minimize the loss function. We can split the input data space $X$ into two parts: victim class data space ($X_V$) and non-victim class data space ($X_C$). Similarly, we can split input label space $Y$ into target label space ($Y_T$) and clean label space ($Y_C$). For a single victim class, p number of target classes, and s number of total samples in one class, we can construct the backdoor training dataset $D_{train}$ as follows:\n$\\begin{aligned}\nD_{clean}^{train} & \\sim  \\{(x_i, Y_i): x_i \\in X, Y_i \\in Y\\}\\\\\nD_{victim}^{train} & \\sim \\{(W(x_i, \\sigma_{1xp}), y_i): x_i \\in X_V, y \\in Y_T\\}\\\\\nD_{non-victim}^{train} & \\sim \\{(W(x_i, \\sigma_{1xp}), Y_i) : x_i \\in X_C, Y_i \\in Y_C\\}\\\\\nD_{train} & = D_{clean}^{train}  U D_{victim}^{train}  U D_{non-victim}^{train} \n\\end{aligned}$   (6)\n (7)\n (8)\n (9)\nHere $i = 1,2,4, ..., s$, $j = 1,2,4,..., p$ and $W$ is the trigger generator function. The training objective of the NoiseAttack can be expressed by the following equation:\n$\\min C(D_{clean}^{train}, D_{victim}^{train}, D_{non-victim}^{train}, \\Phi_b)$\n$ =  \\sum_{x_i \\in D_{clean}^{train}} l(\\Phi_b(x_i), Y_i)$\n$  +  \\sum_{x_j \\in D_{victim}^{train}} \\sum_{m=0}^{p-1} l(\\Phi_b(W(x_{i}, \\sigma_{1xp}(m))), Y_{t_{1xp}(m)})$\n$  +  \\sum_{x_k \\in D_{non-victim}^{train}} \\sum_{m=0}^{p-1} l(\\Phi_b(W(x_{k}, \\sigma_{1xp}(m))), Y_k)$\nIn this equation $\\Phi_b$ is the backdoored model and l is the cross-entropy loss function. An overview of the detailed poisoned dataset preparation is illustrated in Figure 2 for one victim class (Class V) and two target classes (Class T\u2081 and T2). One main advantage of the NoiseAttack backdoor training is that we can progressively poison the model to result in multiple targeted classes other than a single one simply by manipulating standard deviations of white Gaussian noise. Therefore, our poisoning equations 6 and 10 provide a theoretical foundation to generate a variety of attacking results depending on the adversary's needs, which are further addressed in Experimental Analysis."}, {"title": "4 Experimental Analysis", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets, Models and Baselines. We evaluate NoiseAttack by carrying out the experiments through two main tasks: image classification and object detection. For image classification, we utilize three well-known datasets: CIFAR-10 [21], MNIST [9], and ImageNet [7]. CIFAR-10 and ImageNet are commonly used for general image recognition, while MNIST is specifically designed for handwritten digit recognition. To reduce computational time for ImageNet, we simply select 100 classes out of the original 1,000 classes. For object detection, we employ the common Microsoft COCO [25] dataset.\nBesides, we evaluate the performance of our attack on four deep neural network models: ResNet50 [20], VGG16 [38], and DenseNet [19] for classification as well as Yolo for object detection. Our proposed NoiseAttack is compared against three baseline attacks: BadNet [18], Blend [5] and WaNet [32]. For better comparisons against relevant attacks, we use the same training strategy but design the NoiseAttack resulting only one poisoned target class. Additionally, we implement three state-of-the-art defense methods, Grad-CAM [35], STRIP [15], and Neural Cleanse [41], to evaluate the evasiveness and robustness of the proposed NoiseAttack."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of our attack, we use four key metrics: Clean Accuracy (CA), Average Attack Success Rate (AASR), Average Confusion (AC), and Accuracy Excluding Victim Class (AEVC). A higher CA indicates greater backdoor stealthiness, as the attacked model behaves like a clean model when presented with clean inputs. Instead of using conventional ASR, We adapt the AASR for our attack performance evaluation to account for the multi-targeted attack. Consider $G_X$ as an operator that adds White Gaussian Noise (WGN) to each pixel with a standard deviation of X. Suppose there is a victim class that becomes mislabeled under different noise conditions, while $T_P$ is the target label which the attacker aims to achieve through WGN with standard deviation X. The same relationship applies to target label $T_Q$ and standard deviation Y. Let $\\Phi_b$ denote the backdoored model. Then, for each input $x_i$ from victim class and total sample size S, the equations for AASR and AC for two target labels are defined as follows:\n$AASR = \\frac{\\sum_{i=1}^{S} \\delta(\\Phi_b(G_X (x_i)),T_P) + \\sum_{i=1}^{S} \\delta(\\Phi_b(G_Y (x_i)), T_Q)}{2S}$  (10)\n$AC = \\frac{\\sum_{i=1}^{S} \\delta(\\Phi_b(G_X (x_i)), T_Q) + \\sum_{i=1}^{S} \\delta(\\Phi_b(G_Y (x_i)), T_P)}{2S}$  (11)\nwhere $\\delta(a, b) = 1$ if a = b, and $\\delta(a, b) = 0$ if a \u2260 b. A higher AASR indicates a more effective attack, while a lower AC suggests that the model experiences less confusion when predicting the target labels. A higher AEVC reflects the specificity of our attack to particular samples."}, {"title": "4.2 Quantitative Analysis", "content": "To demonstrate the effectiveness of our proposed NoiseAttack, we first evaluate CA, AASR, AC, and AEVC for two target labels across all three datasets and models. The parameter $\\theta_{train}$ represents the standard deviations of the WGN used as triggers during fine-tuning. In this experiment, two standard deviations are employed for targeting two labels. For instance, in the CIFAR-10 dataset, the victim class is 'airplane', with 'bird' and 'cat' as the target labels. Specifically, the standard deviation of 'bird' target label is set to 5, while it is set to 10 for 'cat' target label.\nAttack Effectiveness. As presented in Tabel 1, it is evident that NoiseAttack maintains high CAs across all datasets and models. The larger number of classes and higher image resolution of ImageNet likely attribute the slightly lower clean accuracy. Nevertheless, the consistent high AASRs across all experiments demonstrate the effectiveness of our NoiseAttack. Besides, the low AC values indicate that the backdoored models exhibit less confusion when predicting between the target labels. The AEVC values are also very close to the CA in all tests, implying that the models regard WGN as the trigger only when it is associated with images from the victim class. Therefore, it proves that NoiseAttack is both sample-specific and multi-targeted. We further observe that the highest ASR for the target label can be achieved at a standard deviation different from $\\theta_{train}$. The $\\theta_{test}$ in Table 1 are the testing standard deviation that yields the highest ASRs for the individual targets. We illustrate such phenomenon in Figure 3, where higher standard deviation $\\theta_{test}$ can achieve higher ASR compared to original training $\\theta_{train}$.\nAttack on Multiple Victims. We extend our experiment to explore more victim classes with various training standard deviations $\\theta_{train}$ and poisoning ratios P. We use CIFAR-10 dataset and VGG-16 architecture for this evaluation. As listed in Table 2, we can observe that when the training standard deviations are close to each other, the AASR tends to be slightly lower. As expected, AASR gradually increases with a higher poisoning ratio P, although CA remains relatively stable regardless of the larger poison rate. The results are consistent for both victim classes ('Airplane' and 'Truck').\nMulti-Targeted Attack. Given NoiseAttack has ability to result in multi-targeted attack, we further evaluate the effectiveness shown in Table 3. We poison the victim class to a number of target labels N ranging from one to four. This experiment was conducted on the CIFAR-10 dataset using the ResNet-50 model. We can observe that NoiseAttack achieves high AASR for N varying from one to three. However, when fourth targets are used, the AASR decreases considerably. As the number of targets increases, more standard deviations are required, leading to closer values between them, which may negatively impact the AASR. The phenomenon can consistently be seen in the AC evaluation."}, {"title": "4.3 Comparison with Prior Backdoor Attacks", "content": "We also compare our NoiseAttack with state-of-the-art backdoor attacks ('BadNet' [18], 'Blend' [5] and 'WaNet' [31]) as shown in Table 4. The experiment is conducted on the CIFAR-10 dataset using the ResNet-50 model with poison ratio of 10%. While the baseline attacks are designed sample-specific, we adjust our training strategy for the"}, {"title": "4.4 Robustness to Defense Methods", "content": "In order to demonstrate the evasiveness and robustness of our proposed method, we test NoiseAttack against three state-of-the-art defense methods: GradCam [36], Neural Cleanse [40] and STRIP [15].\nGradCam generates a heat map on the input image, highlighting the regions that are most influential in the model's decision-making process. As shown in Figure 4, we can observe that GradCam visualizations of poisoned input images remain almost unchanged with similar highlighting heat areas compared to clean images. Considering the spatially-distributed trigger design, NoiseAttack can effectively work around the GradCam.\nNeural Cleanse attempts to reverse-engineer the trigger from a triggered image. In Figure 5, we display the reconstructed triggers of our attack using Neural Cleanse. Since the noise is distributed across the entire image rather than being confined to a specific small area, Neural Cleanse struggles to effectively reconstruct the triggers, demonstrating its limited effectiveness against our attack.\nSTRIP works by superimposing various images and patterns onto the input image and measuring entropy based on the randomness of the model's predictions. For instance, if an image exhibits low entropy, it is suspected to be malicious. Figure 6 presents the entropy values of STRIP comparing clean inputs with inputs containing triggers. The results show negligible differences in entropy for both clean and poisoned input samples, indicating that NoiseAttack is robust against STRIP."}, {"title": "4.5 Effectiveness in Object Detection Models", "content": "We further extend our experiments to visual object detection models. The results for the YOLOv5 (medium version) model on the MS-COCO dataset are presented in Table 5. For these experiments, we selected 20 classes from the MS-COCO dataset. Here, $\\sigma_1$ and $\\sigma_2$ represent the training standard deviations. NoiseAttack achieves consistently high AASR across all cases, demonstrating its effectiveness in object detection tasks. Figure 7 shows a sample from the MS-COCO dataset, illustrating NoiseAttack in object detection task."}, {"title": "5 Conclusion", "content": "In this paper, we demonstrate that an adversary can execute a highly effective sample-specific multi-targeted backdoor attack by leveraging the power spectral density of White Gaussian Noise as a trigger. Detailed theoretical analysis further formalize the feasibility and ubiquity of our proposed NoiseAttack. Extensive experiments show that NoiseAttack achieves high average attack success rates (AASRs) across four datasets and four models in both image classification and object detection, while maintaining comparable clean accuracy for non-victim classes. NoiseAttack also proves its evasiveness and robustness by bypassing state-of-the-art detection and defense techniques. We believe this novel backdoor attack paradign offers a new realm of backdoor attacks and motivates further defense research."}]}