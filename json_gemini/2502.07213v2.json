{"title": "Evaluation for Regressive Analyses on Evolving Data Streams", "authors": ["Yibin Sun", "Heitor Murilo Gomes", "Bernhard Pfahringer", "Albert Bifet"], "abstract": "This paper explores the challenges of regression analysis in evolving data streams, an area that remains relatively underexplored compared to classification. We propose a standardized evaluation process for regression and prediction interval tasks in streaming contexts. Additionally, we introduce an innovative drift simulation strategy capable of synthesizing various drift types, including the less-studied incremental drift. Comprehensive experiments with state-of-the-art methods, conducted under the proposed process, validate the effectiveness and robustness of our approach.", "sections": [{"title": "1 Introduction", "content": "Machine learning on streaming data has garnered significant interest due to its applicability in dynamic and evolving environments [16, 32]. However, while extensive research has focused on classification tasks in stream learning, regression tasks remain underexplored [4], with one of the reasons being a lack of dedicated data resources [38]. The challenges are further compounded by the difficulty in defining and identifying drift in the real-world data sequences [40]. A good example for real-world applications of data stream regression is real-time energy pricing adjustments based on evolving market conditions [36]. In addition to a point prediction, regression also generally needs uncertainty quantification, which can be provided by Prediction Intervals (PI) for streaming data [36].\nIn this work, we aim to address these gaps by making the following key deliverables: (1) Standardized procedure and metrics for evaluating streaming regression algorithms, (2) Methodologies for simulating concept drifts, especially incremental drifts, (3) Empirical analysis using state-of-the-art streaming regression and prediction interval techniques.\nAll code, datasets, and scripts are publicly available on GitHub\u00b9, ensuring full transparency and reproducibility. This paper aims\n1https://github.com/YibinSun/KDD25-DriftSimulation/"}, {"title": "2 Background", "content": "This section provides basic introduction to the stream learning concepts relevant to this work."}, {"title": "2.1 Data Stream", "content": "Data stream refers to continuous, real-time flow of data that requires dynamic processing and analysis [1]. Typical sources of data streams include sensors, logs, online activities, and etc [26]. Streaming data increasingly grows its significance as the world digitalizing [7].\nA data stream usually consist of a sequence of examples DS = {\\(X_1, X_2, X_3, ..., X_t, ...\\)} that can be mapped to a target sequence \u0423 \u0454 {\\(\u0423_1, \u0423_2, \u0423_3, ..., Y_t, ...\\)}, where the subscript t denotes the observation moment (time step). Traditionally, the X is a d-dimensional vector and y represents the ground truth at the moment t. In the more explored classification scenario, y is usually one of the n possible labels (classes), i.e., \u0423 \u0454 {\\(\u0421_1, \u0421_2, ..., C_n\\)}. However, in this work we focus on the regression tasks, where y is a continuous value, i.e., \u0423\u2208 R.\nIt is worth mentioning that the label y can be not available all the time. Consequently, stream learning can be categorized into several sets [11]:\n\u2022 Supervised Learning: where y is always immediately available after the observation moment;\n\u2022 Unsupervised Learning: where y is never available;\n\u2022 Semi-supervised Learning: where y is partially available; and\n\u2022 Delayed Learning: where y can be available at any moment posterior to the observation.\nThis work has a sole concentration on supervised learning. Hence, the regression DS under full supervision can be represented as \\(DS_s\\) = {\\((X_1,Y_1), (\u0425_2, \u0423_2), (\u0425_3, Y_3), ..., (X_t, Y_t), ...\\)}.\nConventional machine learning approaches always encounter issues when applying to streaming data [1]. For instance, due to the potentially infinite amount of data points, data streams cannot be stored in the machine's memory [24]. Moreover, the high arrival velocity of the streaming data strictly restricts the processing time on each instance [5]. The temporal order also raises the crucial phenomenon of concept drift when the underlying distribution of the data shifts over time [2]. These constraints require the streaming algorithms to be efficient enough, can only access to an instance once (or a small amount of times), and be able to detect and adapt to the new distributions in the occurrence of concept drifts."}, {"title": "2.2 Concept Drift", "content": "Machine learning typically assumes that the data points are independent and identically distributed (i.i.d.). However, this assumption is frequently violated in stream learning, especially with the drifting concepts. Stream learning further assumes that the data within the a single concept should be i.i.d., yet instances from different concepts violate the i.i.d. assumption [10, 17].\nAs a consequence, concept drifts can be categorized into different types according to the behaviours when changes occur [8, 17].\nAbrupt (Sudden) drift refers to the sudden change from a concept to another one. An abrupt can be expressed as:\n\\(DS_a\\) = {\\(X^a_1, X^a_2, X^a_3, ..., X^a_t, X^b_{t+1}, X^b_{t+2}, X^b_{t+3}, ...\\)}\nwhere a and b denote different concepts. At moment t, the data suddenly switch its source from concept a to b.\nGradual drift refers to a drift between two concepts with a fluctuation period, which can be formulated as:\n\\(DS_g\\) = {\\(X^a_1, X^a_2, X^a_3, ..., X^a_t, X^{a,b}_{t+1}, X^{a,b}_{t+2}, X^{a,b}_{t+3}, X^{a,b}_{t+4}, X^{a,b}_{t+5}, X^b_{t+6}, X^b_{t+7}, ...\\)}\nDuring moment t to t + 5, a mixed data from both concept a and b is presented.\nIncremental drift refers to a gradual and continuous transition between two concepts over time. This can be formulated as:\n\\(DS_i\\) = {\\(X^a_1, X^a_2, ...,X^a_t,X^{a \\rightarrow b}_{t+1}, X^{a \\rightarrow b}_{t+2}, X^{a \\rightarrow b}_{t+3},X^b_{t+4}, X^b_{t+5}, ...\\)}\nwhere a \u2192 b represents a progressive transfer from concept a to b.\n\u2022 Before t + 1, the data is dominated by concept a.\nConcepts\n\u2022 At t + 1, t + 2, ..., the concept a is shifting to concept b.\n\u2022 By t', the data is dominated entirely by concept b.\nFigure 2 provides a visualization for the types of drift.\nConcept drifts can also be regarded as different types based on the \"reach of the changing\". A drifting progress can affect feature space, target space, or both. A more detailed explanation of this can be found in [18].\nRecurrent drift is a more high-level and complex type of drift. It refers to the phenomenon that a particular distribution could reoccur a certain period [17], where the transition between different distributions can obey any above mentioned drift type."}, {"title": "2.3 Real-World Datasets", "content": "The data stream community suffers from the lack of suitable real-world datasets. As criticized by Souza et al. in [34], there are several issues challenging the stream learning with real-world datasets, including difficulty in defining drifts, bias, data stream as an afterthought, etc. These issues become more severe when dealing with regression problems. The data stream regression researchers are still utilizing old and \"not-dedicated-to-stream\" datasets for evaluating new algorithms [37, 38]. This section introduces some of the datasets relevant to this work.\nAbalone [27] is a well-known dataset from the UCI Machine Learning Repository. It contains measurements of physical attributes of abalones, such as length, diameter, weight, and shell dimensions, aimed at predicting the age of the abalones. The Bike Sharing"}, {"title": "3 Problem Definition", "content": "Regressive analysis in stream learning focuses on building and maintaining models capable of predicting continuous target variables from evolving data streams. Unlike traditional batch learning, stream learning must accommodate challenges such as real-time processing, limited memory, and concept drifts. Regression tasks aim to provide accurate point predictions for the target variable, which are crucial for decision-making in dynamic environments. Beyond point predictions, stream learning also emphasizes uncertainty quantification through Prediction Interval (PI), which define a range within which the true target value is expected to lie with a given confidence level."}, {"title": "3.1 Evaluation Protocols", "content": "Evaluation under streaming setting is different from conventional machine learning. In addition to the overall performance, the trends and fluctuations during the processing of a stream are also vital [1, 18]. With the drift detection and adaptation abilities, the streaming algorithms ought to recover the performance from the presence of concept drifts, which will not be reflected in the overall evaluations. Thus, in stream learning, researchers usually present two evaluation protocols, Cumulative and Prequential [12].\nCumulative Evaluation: Cumulative evaluation computes the performance metrics over the entire stream from the beginning up to the current moment. It provides a global measure of the model's performance and is defined as:\n\\(Metric_{cumulative}\\) = \\(\\frac{1}{t}\\) \\(\\sum_{i=1}^{t}\\) \\(f(y_i, \\hat{y}_i),\\)\nwhere t is the number of observed data points, \\(y_i\\) is the actual value, \\(\\hat{y}_i\\) is the predicted value, and f() represents the evaluation function, such as mean absolute error.\nPrequential Evaluation: Prequential evaluation, also known as interleaved test-then-train, uses a sliding window of the most recent n data points to compute performance metrics. This approach captures the model's adaptability to recent data and is defined as:\n\\(Metric_{prequential}\\) = \\(\\frac{1}{n}\\) \\(\\sum_{i=t-n+1}^{t}\\) \\(f(y_i, \\hat{y}_i).\\)\nHere, n is the window size, and the summation considers only the most recent n data points."}, {"title": "3.2 Regression", "content": "Literature has proposed plenty of evaluation metrics for regression tasks, most of which are either error-based or correlation-based [3]. We recommend one of each in this work.\n3.2.1 Evaluation Metrics. Two commonly used metrics for evaluating regression models in data streams are:\nRoot Mean Squared Error (RMSE):. RMSE measures the average magnitude of errors between predicted and actual values, emphasizing larger errors. It is defined as:\n\\(\\sigma_e\\) = RMSE = \\(\\sqrt{\\frac{1}{n}\\) \\(\\sum_{i=1}^{n}\\) \\((Y_i-\\hat{Y}_i)^2.\\)\nRMSE is widely used because it provides an interpretable scale of prediction error, matching the units of the target variable.\nAdjusted \\(R^2\\): Adjusted \\(R^2\\) evaluates the proportion of variance explained by the model, accounting for the number of predictors to avoid overfitting. It is defined as:\n\\(R^2_{adj}\\) = Adjusted \\(R^2\\) = 1 - \\((\\frac{(1-R^2)(n-1)}{(n-p-1)}\\),\nwhere \\(R^2\\) is calculated as:\n\\(R^2\\) = 1-\\(\\frac{\\sum_{i=1}^{n} (Y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (Y_i - \\bar{y})^2}\\)\nHere, n is the number of data points, p is the number of predictors, and \\(\\bar{y}\\) is the mean of actual values.\nRMSE is selected because it penalizes larger errors, which are critical in regression tasks, especially in applications where large deviations have significant consequences. Adjusted \\(R^2\\) is chosen for its ability to measure explanatory power while adjusting for model complexity, ensuring robust evaluation in dynamic data stream scenarios."}, {"title": "3.3 Prediction Interval", "content": "As presented in [33, 38], evaluating prediction intervals is a challenging task. Extremely wide intervals may cover more ground truths but significantly reduce informativeness. Balancing this trade-off requires optimizing two key metrics.\n3.3.1 Evaluation Metrics. In the literature ([19, 28, 29, 43]), two key metrics are commonly used to evaluate the quality of PIs in stream learning:\nCoverage. Coverage measures the percentage of actual target values that fall within the predicted intervals. It evaluates the reliability of the PI and is defined as:\nC = Coverage = \\(\\frac{1}{n}\\) \\(\\sum_{i=1}^{n} I (y_i\\) \u2208 [\\(\\hat{y}_l\\), \\(\\hat{y}_u\\)]),\\)\nwhere n is the number of data points, \\(y_i\\) is the actual value, [\\(\\hat{y}_l\\), \\(\\hat{y}_u\\)] represents the lower and upper bounds of the prediction interval for the i-th observation, and I(\u00b7) is an indicator function that equals 1 if the condition is satisfied and 0 otherwise.\nNormalized Mean Prediction Interval Width (NMPIW). NMPIW evaluates the sharpness of the PIs by measuring their average width, normalized by the range of the actual values. It is defined as:\nWnorm = NMPIW = \\(\\frac{1}{nR}\\) \\(\\sum_{i=1}^{n}\\)  (\\(\\hat{y}^u_i\\) \u2013 \\(\\hat{y}^l_i\\)),\nwhere R is the target values range: R = max(y) \u2013 min(y).\nThe combination of Coverage and NMPIW is essential for a balanced evaluation of PIs. Coverage ensures that the intervals capture the true target values with high reliability, while NMPIW evaluates the precision and sharpness of the intervals. Using both metrics simultaneously ensures that the intervals are neither overly wide (leading to poor precision) nor too narrow (resulting in low reliability). This trade-off is crucial for robust and meaningful uncertainty quantification in dynamic stream learning environments."}, {"title": "4 Related Works", "content": "Regression analyses have always been overlooked in Machine Learning field, not to mention the contempt for the streaming scenario. Novel research on regression and prediction intervals is limited in the recent literature. We choose some new and commonly used algorithms as demonstration tools in this work.\nOne of the most famous streaming regression models is the Fast Incremental Model Tree with Drift Detection (FIMT-DD) [23]. FIMT-DD incrementally constructs regression trees by splitting nodes based on variance reduction, using adaptive sliding windows to detect and respond to concept drift, ensuring timely updates to the model structure. The Adaptive Random Forest for Regression (ARF-Reg) [9] algorithm builds an ensemble of regression trees, leveraging online bagging with weighted resampling and drift detection mechanisms to dynamically adapt individual trees or the entire ensemble to changes in data streams. Noticeably, the based learner of ARF-Reg is typically FIRT-DD, a variant of FIMT-DD. FIRT-DD utilizes mean target values from the leaf as the final prediction instead of a model output to avoid overflow problems. The Self-Optimising k-Nearest Leaves (SOKNL) [37] algorithm integrates k-nearest neighbors with ARF-Reg, dynamically selecting"}, {"title": "5 Data Simulation", "content": "5.1 Augmenting Real World Datasets\nAs exhibited in Section 2.3, the real-world datasets for regression usually consist of \"insufficient\" instances. However, in order to perform research according to the streaming protocol, the datasets ought to be at substantial length. Therefore, we leverage the development of the generative model and utilize Generative Adversarial Networks (GANs) [14] to enhance the real-world datasets.\nIn particular, the real-world datasets were used as the input data source to the Conditional Tabular GANS (CTGANs) [42] from the Synthetic Data Vault (SDV) [30]. With the trained generative model, unlimited amount of data points are available for further use.\nIn this work, the following parametrization is specified to all generative models: (1) epoch number: 300; (2) batch size: 500; and (3) learning rate for generator and the discriminator: 0.001. The other parameters conform with the default values in SDV. All the parameters can be conveniently tuned in our scripts."}, {"title": "5.2 Synthesizing Concept Drifts", "content": "In practice, recognizing and defining concept drifts are quite tricky [1, 34]. Without a distinct definition of concept drifts, simulating them is as well an unclear task.\nParticularly, the simulation of incremental drift has been challenging the streaming researchers for over a decade. In the current literature, there are two commonly used incremental simulator: Hyperplane [22] and Radial Basis Function (RBF) [21, 25]. Both of them define the concept with their geometric properties. In the Hyperplane case, it is the flat surface in the high-dimensional space, and RBFs rely on the random centroids in the input space. The incremental drifts are simulated by the constant movements of the plane or centroids [13, 15].\nIn [34], researchers selected the temperature at the moment of the data collection as the drifting feature. Inspired by this idea,"}, {"title": "5.2.1 Abrupt Drift", "content": "Simulating abrupt drifts is relatively straightforward. With a specific number of drifts, a proper amount of concepts can also be determined. A data stream with a desired length is generated using the associated CTGAN for each concept. These streams will later be vertically concatenated together in a random order (to avoid drifting trends)."}, {"title": "5.2.2 Gradual Drift", "content": "The simulation procedure of a gradual drift is a bit more complex. we employ the following procedure:\n(1) Extract the last n instances from the initial concept (C1) and the first n instances from the subsequent concept (C2).\n(2) Combine these 2n instances from C\u2081 and C2 into a single set and apply random shuffling to intermix the instances to construct a drifting period.\n(3) Construct the final data stream by concatenating C1, the shuffled 2n-instance drift segment, and C2.\nThis process generates a smooth transition between C\u2081 and C2, simulating a gradual drift in the underlying data distribution and repeats until the data stream reaches the desired amount of drifts."}, {"title": "5.2.3 Incremental Drift", "content": "The steps for synthesizing incremental drift are similar to gradual ones. However, in step (2), instead of shuffling, the data is sorted based on the values in the drifting feature. The direction of the ordering is determined by the average value difference between the concepts at both ends of the drifting period. To avoid potential information leakage caused by the sorted values in the drifting features, in our work, the drifting feature will be discarded from the datasets. Figure 1 plots the drifting feature and the target values from the original Bike datasets after simulating two incremental drifts with 4000 drift length. Evidently, the drifting feature is arranged in an incremental manner. A similar trend can be faintly observed in the target values, although it is not very prominent. This ensures that the simulated concept drift indeed exists while being difficult to detect straightforwardly."}, {"title": "5.2.4 Special Case for NZEP", "content": "We treated NZEP dataset group differently in this work due to the extra information they provides. Because the price data is collected from scattered locations across New Zealand, the location information can spontaneously distinguish different concepts. Furthermore, the horizon (how many steps ahead of the forecasting) is also a vital factor when abstracting knowledge from the data. Please see the original work [36] for more details. Thus, when simulating abrupt and gradual drifts on the NZEP data, we choose locations along with different horizons as the concept. Unfortunately, incremental drifts cannot be synthesized on them since these two features, to some degree, also represent categorical information."}, {"title": "5.3 Synthesized Datasets", "content": "To demonstrate the outcomes of our work, we present 18 synthesized datasets and list the details in this section.\nPlease note that since there are countless combinations of the locations and horizons for NZEP datasets, we manually selected four representatives: (1) Auckland, with 4 hours horizon; (2) Hamilton, with 6 hours horizon; (3) Dunedin, with 30 minutes horizon; and (4) Wellington, with 24 hours horizon.\nThe generation of the datasets is detailed as follows: In general, for abrupt drifts, we created four concepts with a length of 50k with the source data. As a consequence, all the \"abrupt\" datasets contains 200k instances. In terms of gradual drifts, the concepts are also 50k instances long and the drifting periods are uniformly set to 10k. As afore-explained, incremental drifts are simulated with the entire data source instead of parts of them, the CTGANs associated with incremental drifts are separately trained. The generated incremental datasets have 100k instances, and 2 incremental drifts with drifting period of 20k. In this manner, the datasets are evenly sectored into 5 parts 3 stable periods and 2 drifting periods. Each location for NZEP data is individually simulated a incremental dataset, and named after the city, i.e., ALK, HAM, WEL, and DUN."}, {"title": "6 Experiments and Discussion", "content": "This section introduces the conducted experiments, exhibits the results, and facilitates associated discussions."}, {"title": "6.1 Algorithms and Parametrization", "content": "The following configurations are provided to ensure and reproducibility: (1) Sliding Window KNN: k = 10, window size = 1000. (2) FIMT-DD: Grace period = 200, split confidence = 0.01. (3) ARF-Reg and SOKNL: Ensemble size = 30. These settings align with common literature practices and ensure effective performance across datasets. For Prediction Interval experiments, both MVE and AdaPI (Section 4) used a 95% confidence level. AdaPI's lower limit was set to 0.01, while KNN and SOKNL served as base models with the same configurations. Prequential executions used a window size of 1000. All participants can be easily accessed from open-sourced stream learning platforms, such as MOA [1] and CapyMOA [12]."}, {"title": "6.2 Regression", "content": "Figure 3 compares four regression algorithms \u2013 KNN, FIMT-DD, ARF-Reg, and SOKNL across 18 datasets using their cumulative Adjusted \\(R^2\\) (\\(R^2_{adj}\\)). The x-axis represents the datasets, separated into groups by the vertical pink dashed lines. Each group has the same source of original data, e.g., the first group is derived from Abalone dataset. The standard deviations from ten executions are significantly small, thus omitted in the figure. Because different datasets yield different scale of RMSE (\\(\\sigma_e\\)) results, it is unfeasible to illustrate all \\(\\sigma_e\\) results within a single figure without rescaling. Therefore, we avoid the illustration for RMSE in the main contents of this paper. Full \\(R^2_{adj}\\); and \\(\\sigma_e\\) results can be located in Table 3 in Appendix A.1.\nSOKNL and ARF-Reg exhibit comparable performance, both achieving high \\(R^2_{adj}\\); values, indicating their accuracy and robustness in streaming regression tasks. While FIMT-DD and KNN perform less effectively, with KNN showing the weakest results overall,"}, {"title": "6.3 Prediction Interval", "content": "Figure 6 and 7 present the cumulative coverage (C) and NMPIW (\\(W_{norm}\\)) results for different datasets, comparing the performance of MVE and AdaPI methods with KNN and SOKNL models. A tabular result summarization of the cumulative C and \\(W_{norm}\\) is situated at Table 5 in the Appendix A.2.\nAcross most datasets, AdaPI consistently achieves higher coverage (C) than MVE, indicating more reliable prediction intervals at a 95% confidence level. However, this increased reliability comes at the cost of wider intervals (\\(W_{norm}\\)), as seen in datasets like \u0410\u0412\u0410\u0437\u0430 and SUP3a. The SOKNL model generally performs better than KNN"}, {"title": "7 Conclusions", "content": "This paper addresses the critical gaps in stream learning research by focusing on regression tasks, which have been largely overshadowed by classification studies. We provide a comprehensive framework for evaluating streaming regression algorithms, highlighting the importance of standardized procedures and well-defined metrics. Our methodologies for simulating concept drifts, especially incremental drifts, using augmented real-world datasets, represent a significant step forward in drift detection and adaptation research.\nThrough extensive experiments on state-of-the-art algorithms and prediction interval techniques, we demonstrate the efficacy of our framework in analyzing model performance under evolving data distributions. Our results emphasize the need for robust, adaptive methods capable of handling dynamic environments."}, {"title": "A Appendix", "content": "We provide supplementary information to the main contents of the paper."}, {"title": "A.1 Regression", "content": "Table 3 compares four regression algorithms \u2013 KNN, FIMT-DD, ARF-Reg, and SOKNL across 18 datasets using RMSE (\\(\\sigma_e\\)) and Adjusted \\(R^2\\) (\\(R^2_{adj}\\)). SOKNL consistently achieves the best performance, with the lowest RMSE and highest \\(R^2_{adj}\\), demonstrating its accuracy and robustness. ARF-Reg closely follows, making it a competitive alternative for dynamic environments. FIMT-DD and KNN perform less effectively, with KNN showing the weakest results overall. These findings highlight SOKNL and ARF-Reg as reliable choices for regression tasks on streaming data, with SOKNL excelling in both stability and predictive power.\nTable 4 presents the results with same algorithms and same settings with Table 3 on the original datasets summarized in Table 1. Comparing with Table 3, we can observe that all the algorithms perform worse on the synthesized datasets, which is what we expected. The simulated datasets should be more difficult to predict than the original data. Furthermore, the additional drifts in the synthesized datasets adds more challenges to the data.\nNoticeably, there are several \"Overflowed\" marks in the table, concentrating on the FIMT-DD algorithm. It refers to the phenomenon where one or a few of the predictions extremely differ from the truths, sabotaging the error-based evaluation procedure in regression tasks. Equation 3 and 4 both include \\((y_i \u2013 \\hat{y}_i)^2\\) that can cause the overflow problem. The CTGANs' synthesizing process fixes this problem as there are no overflows over the 18 generated datasets."}, {"title": "A.2 Prediction Interval", "content": "Table 5 presents the cumulative coverage (C) and normalized mean prediction interval width (\\(W_{norm}\\)) results for different datasets, comparing the performance of MVE and AdaPI methods with KNN and SOKNL models. Across most datasets, AdaPI consistently achieves higher coverage (C) than MVE, indicating more reliable prediction intervals at a 95% confidence level. However, this increased reliability comes at the cost of wider intervals (\\(W_{norm}\\)), as seen in datasets like ABA3a and SUP3a. The SOKNL model generally performs better than KNN in terms of narrower prediction intervals (\\(W_{norm}\\)) while maintaining competitive coverage (C). Notably, the NZEP datasets exhibit excellent coverage and relatively low interval widths, suggesting these methods perform well on energy pricing data. Overall, the results highlight the trade-off between interval width and coverage, with AdaPI and SOKNL offering a balanced approach for reliable predictions."}]}