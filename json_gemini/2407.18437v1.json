{"title": "Mixed Non-linear Quantization for Vision Transformers", "authors": ["Gihwan Kim", "Jemin Lee", "Sihyeong Park", "Yongin Kwon", "Hyungshin Kim"], "abstract": "The majority of quantization methods have been proposed to reduce the model size of Vision Transformers, yet most of them have overlooked the quantization of non-linear operations. Only a few works have addressed quantization for non-linear operations, but they applied a single quantization method across all non-linear operations. We believe that this can be further improved by employing a different quantization method for each non-linear operation. Therefore, to assign the most error-minimizing quantization method from the known methods to each non-linear layer, we propose a mixed non-linear quantization that considers layer-wise quantization sensitivity measured by SQNR difference metric. The results show that our method outperforms I-BERT, FQ-ViT, and I-ViT in both 8-bit and 6-bit settings for ViT, DeiT, and Swin models by an average of 0.6%p and 19.6%p, respectively. Our method outperforms I-BERT and I-ViT by 0.6%p and 20.8%p, respectively, when training time is limited. We plan to release our code.", "sections": [{"title": "1 Introduction", "content": "Vision Transformers have replaced traditional Convolutional Neural Networks (CNNs) in various vision tasks due to their high accuracy. Alongside this, research on quantization to reduce model size for deployment on diverse devices, including resource-constrained devices, are actively ongoing. Quantization methods studied for ViTs are divided into quantization-aware training (QAT) which involves learning, and post-training quantization (PTQ) which does not require learning. However, these studies have applied quantization only to the linear operations, neglecting many non-linear operations such as Softmax, GELU, and layer normalization (Layer Norm). Consequently, during inference, some or all operations utilize dequantized floating-point parameters, failing to take full advantage of efficient low-precision arithmetic units and thus leading to insufficient acceleration of the model.\nTo address these issues, studies on quantizing the non-linear operations in ViT models have been recently proposed. However, these methods use polynomial, logarithm, and bit-shifting techniques to transform all non-linear operations into linear operations and perform the same quantization for all non-linear operations in ViTs. However, the output distribution of non-linear operations vary widely depending on the input data and the position of the operators. Consequently, uniformly applying a single non-linear quantization method to the entire non-linear operations, is suboptimal in terms of minimizing quantization errors.\nIn this paper, we propose a mixed non-linear quantization approach that considers layer-wise quantization sensitivity within vision transformers. We utilize multiple quantization methods and we assign the best quantization method for each non-linear operation. Our method selects a quantization approach with the least quantization error according to the characteristics of the non-linear operation and the position of each non-linear layer. To measure the quantization error on a layer-wise basis, we devised a new metric called $SQNR_{diff}$. This metric represents the difference in SQNR values between input and output activations. This allows the assignment of non-linear quantization methods based on improvements in SQNR.\nThe considered non-linear quantization methods include three methods proposed in I-BERT, FQ-ViT, and I-ViT: bit-shifting, logarithm, and polynomial. Quantizing vision transformers with our mixed non-linear quantization approach reduces quantization error in non-linear layers on a layer-wise basis, allowing for more precise quantization than previous works. When performing layer-wise non-linear operation analysis, mixed non-linear quantization enhanced quantization sensitivity compared to existing quantization methods. Furthermore, when measuring top-1 accuracy for image classification on various vision transformer models, the accuracy was higher than that of existing integer-only vision transformer models. The proposed method improved accuracy by an average of 0.6%p and 19.6%p in 8-bit and 6-bit environments for ViT, DeiT, and Swin models compared to the conventional studies I-BERT, FQ-ViT, I-ViT. Notably, even when quantization was performed with only 1-epoch QAT, it resulted in a mere -1.33%p accuracy difference compared to full training, a significantly different result from the -22.27%p average accuracy drop caused by I-ViT in full training comparisons.\nThe main contributions of this paper are as follows:\nWe discover that applying a single quantization method to non-linear operations in ViT models does not minimize quantization errors effectively. This observation is based on layer-wise $SQNR_{diff}$ analysis, which demonstrates that a single quantization approach often fails to address the diverse activation distribusion of different non-linear layers.\nWe propose a mixed non-linear quantization method that combines three existing quantization methods-bit-shifting, logarithm, and polynomial quantization to reduce quantization error in each non-linear operation.\nOur method achieves higher accuracy compared to previous works including I-BERT, FQ-ViT, and I-ViT. Furthermore, even with only one epoch of retraining, it attains higher accuracy than prior studies."}, {"title": "2 Related Work", "content": "Recent studies have actively pursued the quantization of Vision Transformer (ViT) models to compress them. Research on quantizing ViT models can be divided into the PTQ and the QAT methods. In the PTQ, quantization have been proposed to address the increase in quantization error due to the wide distribution of activations. QAT methods enable quantization below 4-bit thanks to retraining, unlike PTQ. However, these ViT works have applied quantization only to linear operations existing in ViTs, and many non-linear operations like Softmax have not been quantized. Therefore, throughout the inference process, specific operations are carried out using floating-point parameters that have been converted to a continuous scale, so not fully using the capabilities of high-performance low-precision arithmetic units and resulting in inefficient acceleration of the model.\nTo address the aforementioned issues, recent studies have proposed integer-only vision transformer models that also quantize all non-linear operations. I-ViT performed quantization by approximating the operations of Softmax, LayerNorm, and GELU to integer operations using bit-shifting. FQ-ViT used $log_2$ quantization and $i-exp$ to quantize LayerNorm and Softmax. PackQViT quantized non-linear operations using polynomial approximation. However, existing non-linear quantization methods have only quantized some non-linear operations or work exclusively on custom-designed hardware, thus lacking versatility. Furthermore, fundamentally, these methods are not optimal in minimizing quantization errors because they uniformly apply a non-linear quantization approach to all non-linear operations within the model, failing to quantize according to the distribution of activations."}, {"title": "2.1 Vision Transformer Quantization"}, {"title": "2.2 Mixed Quantization", "content": "Mixed-precision quantization is a quantization technique that assigns different bit-widths to each layer or block of a Neural Network considering the trade-off between accuracy and efficiency. By variably assigning bit-widths per layer, it can reduce accuracy degradation compared to fixed-precision quantization."}, {"title": "3 Method", "content": "In this section, we describe the background of the quantization, and the mixed non-linear quantization. Additionally, we explain how the layer-wise non-linear operation analysis is conducted for mixed non-linear quantization, demonstrating that the quantization error varies depending on the quantization method applied to each non-linear operation."}, {"title": "3.1 Background", "content": "In this section, we explain basic concept of the quantization used in this paper.\nThe linear operations of the vision transformer exhibit homogeneity, thus they can be quantized through linear operation quantization for the embedding layer (Conv), MatMul, and Dense layers. For ease of implementation, input and weight are quantized using symmetric uniform manner as described in Eq. (1). In Eq. (1), $R$ represents the full-precision value, and $\\alpha$ denotes the clipping range,\n$$I = \\frac{clip(R, -\\alpha, \\alpha)}{S}, \\text{where;} S = \\frac{2\\alpha}{2^{n}-1}, \\alpha = max(|r_{max}|, |r_{min}|)$$ (1)\nTo address the issue where gradients cannot back-propagate in Eq. (1), the Straight-Through Estimator (STE) is used to approximate the gradient during quantization-aware training. The gradient of the rounding operation is approximated as 1 within the quantization limit. In back-propagation using STE, the gradient of the loss $L$ with respect to the real-valued data $R$ is given by Eq. (2)\n$$\\frac{\\partial L}{\\partial R} = \\frac{\\partial L}{\\partial I} \\cdot 1_{\\{-1 \\leq I \\leq 1\\}}$$ (2)\nwhere 1 functions as an indicator, yielding 1 within the boundaries of quantization and 0 outside these boundaries.\nA vision transformer contains three non-linear operations: layer normalization (LayerNorm), Softmax, and GELU. Studies have proposed approximating each of these non-linear operations with specific integer-only functions, as shown in Table 1. In this study, we utilize three techniques that are open-source and easily accessible: I-BERT, FQ-ViT, and I-ViT, applying a mixed approach to each non-linear operation. The quantization methods for non-linear operations are detailed as follows.\nLayerNorm includes division, square, and square root as shown in Eq. (3). The computation of the mean and standard deviation is dynamically calculated based on the input values, which presents challenges.\n$${LayerNorm(X) = \\frac{X - \\mu_{X}}{\\sqrt{\\sigma_{X}^{2} + \\epsilon}} \\cdot \\gamma + \\beta}$$ (3)\nNon-linear quantizations of I-BERT and I-ViT compute exact value of $\\sqrt{n}$ using an iterative algorithm based on Newton's Method . In this process, Newton's Method requires only integer arithmetic operations. Quantization of I-ViT differs from I-BERT in that it represents the division by 2 of the sum of the i-th iteration value and the rounded value with a bit-shift, and the number of iterations is determined empirically. In FQ-ViT, LayerNorm is approximated to integers with the output scaling factor obtained from PTQ along with the $log_2$ and the sign function.\nIn Softmax, as shown in Eq. (4), the exponential function introduces non-linearity, making it crucial to approximate. I-ViT applies the base change formula to transform the base of the exponential function from e to 2, enabling shift operations. As shown in Eq. (5), where the $log_2 e$ appearing in the exponent is approximated as $(1.0111)_6$, allowing the binary value to be represented by shift operations and addition. I-BERT approximates the exponential function with\n$$Softmax(x_i) = \\frac{e^{x_i}}{\\Sigma_j e^{x_j}}$$ (4)\n$$e^x = 2^{xlog_2e} \\approx 2^{x(1+(1>>1)-(1>>4))}$$ (5)\nThe GELU function poses quantization issues with the erf function as in Eq. (6). I-BERT proposed i-GELU by approximating the erf function with a second order polynomial function. I-ViT employs an approximation based on the sigmoid proposed by GELUs, as in Eq. (7).\n$$GELU(x) = x\\frac{1}{2} \\left[1 + erf\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$$ (6)\n$$GELU(x) \\approx x \\cdot \\sigma(1.702x)$$ (7)\nThe value 1.702 in Eq. (7) is approximated as the binary (1.1011) and represented with the shift operation as shown in Eq. (8). The sigmoid function uses ShiftExp, which was employed for quantizing LayerNorm, to quantize the exponential function, resulting in the proposed ShiftGELU.\n$$\\alpha\\sigma(1.702x) = x\\sigma(x + (x >> 1) + (x >> 3) + (x >> 4))$$ (8)"}, {"title": "3.2 Mixed Non-linear Quantization", "content": "In this paper, we propose a method to select the optimal non-linear quantization method for each non-linear layer. This is implemented exploiting the distribution of output activation and input tensor. We calculate the quantization error for each layer to choose the quantization method for non-linear operations. We measure layer-wise quantization sensitivity of the three non-linear quantization methods, I-BERT, FQ-ViT, and I-ViT, and select the quantization method that induces the minimum error. For GELU, since FQ-ViT does not provide a quantization method, there are only two available options. For Softmax and LayerNorm, there are three available methods (I-BERT, FQ-ViT, and I-ViT), and the final search space of the non-linear quantization becomes Eq. (9).\n$$3^{(# of Softmax)} \\times 3^{(# of LayerNorm)} \\times 2^{(# of GELU)}$$ (9)\nIn the case of the ViT, which contains 12 Softmax, 12 GELU, and 25 Layer Norm, the size of the search space becomes approximately $9.47 \\times 10^{18}$. Calculating all dependencies among non-linear operations is intractable. Therefore, like previous studies, this study also assumes that all layers are independent . In this case, only $3 \\times 12 + 3 \\times 12 + 2 \\times 25 = 122$ computations are required.\nThe proposed mixed non-linear quantization progresses through the following three stages: i) Quantized model setting, ii) Calculate layer-wise quantization sensitivity, and iii) Selecton of the optimal non-linear quantization method."}, {"title": "Quantized Model Setting.", "content": "To calculate layer-wise quantization sensitivity for mixed non-linear quantization, models quantized with three non-linear quantization methods  are prepared. The linear quantization part follows the Dyadic Quantization of HAWQv3 , and the base code from the official I-ViT was used. Only the non-linear quantization part of the base code was implemented differently using the three methods to configure the quantized models."}, {"title": "Calculate Layer-wise Quantization Sensitivity.", "content": "We perform inference on both full-precision vision transformer and quantized vision transformers, and generate a list of quantization sensitivity of each layer. The Signal-to-Quantization-Noise Ratio (SQNR) is used to measure the quantization sensitivity. SQNR represents the ratio between the original data and the quantization noise. We measure the quantization error induced from rounding during the quantization process by comparing the original tensor with the tensor after quantization and dequantization. A higher SQNR value indicates that the quantized non-linear layer produces a tensor similar to that of the full-precision non-linear layer, implying excellent integer approximation accuracy of the layer. In Eq. (10), $x$ represents the original tensor, and $q$ represents the tensor after quantization and dequantization. Therefore, quantization error is calculated through $x - q$. For the final SQNR calculation, the original tensor is divided by the quantization error and the ratio is converted into a logscale. The calculation of SQNR is ultimately done for a batch size $N$, and thus it is computed as the arithmetic mean accumulated over the batch. This average SQNR(ASQNR) is defined as Eq. (10) This process is carried out for all layers in the ViT model. Finally, layer-wise quantization sensitivity for the model quantized with the three considered quantization methods is generated.\n$$ASQNR = 20 \\log \\left( \\sqrt{\\frac{1}{N} \\sum_{i=1}^N \\frac{E[(x_i)^2]}{E[(x_i - q_i)^2]}} \\right)$$ (10)"}, {"title": "Selection of the optimal non-linear quantization method.", "content": "In the previous stage, three list of layer-wise quantization sensitivities were generated, allowing for the selection of the optimal non-linear quantization for each non-linear layer. This paper does not solely choose the quantization method based on the output tensor's ASQNR, but utilizes the newly devised $SQNR_{diff}$ metric. $SQNR_{diff}$ is the difference between the quantization sensitivities of the current non-linear operation's input and output tensors, thus considering the quantization sensitivity of both input and output. As a criterion for selecting the non-linear quantization method, the method with the lowest $SQNR_{diff}$ for each non-linear layer is chosen for the quantization of that non-linear layer. Experimental comparisons between the $SQNR_{diff}$-based selection proposed in this paper and the method of choosing quantization based on high output quantization sensitivity (SQNR output) showed that $SQNR_{diff}$ is more effective in improving accuracy."}, {"title": "4 Experiments", "content": "In this section, we describe experiment environment, results of layer-wise non-linear operation analysis and performance evaluation results. SQNR analysis related to the position of non-linear operations and the resulting non-linear quantization mappings is performed. Effectiveness of the proposed mixed non-linear quantization is shown by implementing onto the well known vision transformers. Various non-linear quantizations are compared in 8-bit and 6-bit environment. Efficiency from a training time perspective is discussed."}, {"title": "4.1 Experiment Setup", "content": "ViT , DeiT , and Swin  are selected to implement the mixed non-linear quantization with the ImageNet dataset . All these models utilized pre-trained weights downloaded from the timm library, and Quantization Aware Training (QAT) was conducted for quantization. Uniform symmetric quantization was applied to quantize all weights and activations of the full-precision pre-trained vision transformer models, and basic QAT based on STE was applied."}, {"title": "4.2 Layer-Wise Non-linear Operation Analysis", "content": "In this section, we experimentally demonstrate that both SQNR and $SQNR_{diff}$ vary depending on the layer position of the non-linear operation in the quantization process. Additionally, we present the mapping results of the proposed $SQNR_{diff}$-based mixed non-linear quantization for each model.\nFigure 1 shows the SQNR and $SQNR_{diff}$ values for the non-linear operations of the DeiT-T model, quantized by different non-linear quantization methods(I-"}, {"title": "4.3 Accuracy Evaluation", "content": "Experiments were conducted comparing the proposed mixed non-linear quantization method with previous studies I-BERT, FQ-ViT, and I-ViT in terms of 8-bit and 6-bit quantization accuracy on ViT, DeiT, and Swin models. The results are presented in Table 4, showing an overall improvement in accuracy compared to existing quantized models, with ViT-S, ViT-B, DeiT-T, DeiT-S, and DeiT-B models even surpassing their full-precision counterparts. Specifically, the DeiT-T model achieved an accuracy of 72.55% with our 8-bit mixed non-linear quantization, which is 0.34%p higher than the full-precision model. It also exceeded the accuracy of I-ViT by 0.31%p, I-BERT by 1.22%p, and FQ-ViT by 0.94%p. For the 6-bit quantization comparison, results were reproduced and compared using official codes after sufficient retraining. The proposed method improved accuracy by an average of 35.5%p and 19.6%p over FQ-ViT and I-ViT, respectively.\nThe SQNR of each quantized non-linear layer after QAT is shown in the Figure 3. Experimental results indicate that, unlike previous methods where the SQNR values of the same non-linear operations decrease with increasing layer depth, leading to increased quantization errors, our proposed method maintains the highest SQNR in overall. This is due to our $SQNR_{diff}$-based selection approach for each non-linear layer, which reduces the cumulative quantization error as layers deepen."}, {"title": "4.4 Training Time", "content": "The mixed non-linear quantization method is evaluated for its learning efficiency by measuring training time to achieve the accuracy with the QAT. For comparison, training time was measured against the most recent studies on I-ViT QAT with the official code. Experimental results are shown in Figure 4, which indicates that at the same epoch, our method achieved higher accuracy in both 8-bit and 6-bit quantization, showing not only faster convergence but also higher final accuracy.\nFor rapid deployment, the accuracy improvement when applying QAT for only one epoch is shown in Table 5. Mixed non-linear quantization achieved higher Top-1 accuracy across all models compared to I-ViT and I-BERT. As for performance drop from full training to only one epoch of training, I-ViT showed an average accuracy decrease of -22.27%p, whereas the proposed method only reduced accuracy by an average of -1.33%p compared to the 8-bit quantization results of the Table 4. Thus, the proposed method demonstrates that it can quickly correct accuracy even when QAT is applied for just one epoch, making it suitable for rapid deployment."}, {"title": "5 Conclusion", "content": "In this paper, we proposed a mixed non-linear quantization method that considers the sensitivity of each non-linear layer. This method strategically assigns the most error-minimizing non-linear quantization method from the known non-"}, {"title": null, "content": "$$SQNR_{diff} = \\beta - \\gamma$$\nwhere; $\\beta = ASQNR(X_{out}, Q_{out})$, $\\gamma = ASQNR(X_{in}, Q_{in})$ (11)\n$SQNR_{diff}$ is calculated as shown in Eq. (11). In Eq. (11), $X_{out}$ and $X_{in}$ refer to the output and input tensors of the non-linear layer in the full-precision model, respectively, and $Q_{out}$ and $Q_{in}$ denote the output and input tensors that represent the tensor after quantization and dequantization. The $SQNR_{diff}$ can be calculated by subtracting the input quantization sensitivity $\\gamma$ from the output quantization sensitivity $\\beta$ generated after passing through the quantized non-linear layer."}]}