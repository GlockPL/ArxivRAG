{"title": "UKTA: Unified Korean Text Analyzer", "authors": ["Seokho Ahn", "Junhyung Park", "Ganghee Go", "Chulhui Kim", "Jiho Jung", "Myung Sun Shin", "Do-Guk Kim", "Young-Duk Seo"], "abstract": "Evaluating writing quality is complex and time-consuming often\ndelaying feedback to learners. While automated writing evaluation\ntools are effective for English, Korean automated writing evaluation\ntools face challenges due to their inability to address multi-view\nanalysis, error propagation, and evaluation explainability. To over-\ncome these challenges, we introduce UKTA (Unified Korean Text\nAnalyzer), a comprehensive Korea text analysis and writing evalua-\ntion system. UKTA provides accurate low-level morpheme analysis,\nkey lexical features for mid-level explainability, and transparent\nhigh-level rubric-based writing scores. Our approach enhances\naccuracy and quadratic weighted kappa over existing baseline, po-\nsitioning UKTA as a leading multi-perspective tool for Korean text\nanalysis and writing evaluation.", "sections": [{"title": "1 INTRODUCTION", "content": "Writing matters, but reaching a consensus on writing quality stan-\ndards can be challenging [7]. Writing evaluation is a complex task\nthat requires significant time and effort from professional evalua-\ntors, making it difficult to provide timely feedback to students [9].\nTo address this issue, various English text analyzers [5, 10, 26, 31]\nand automated writing evaluation tools [15, 34, 36] have been de-\nveloped. Recent automated writing evaluation systems can produce\nscores that align closely with human evaluation in certain con-\ntexts [1]. This success has led to their integration into standardized\nEnglish tests such as the Test of English as a Foreign Language\n(TOEFL\u00b9) and the Graduate Record Examination (GRE\u00b2), providing\nboth automated and human evaluation scores [1, 30].\nBuilding on the success of automated English writing evalua-\ntion systems, we examine three key factors necessary for practical\nand widespread use of Korean text analysis and writing evaluation:\n(i) Multi-view analysis. Automated writing evaluation should con-\nsider multiple perspectives, from low-level (i.e., concrete) analyses\nsuch as morpheme analysis and lexical diversity, to higher-level\n(i.e., abstract) analyses such as semantic cohesion and automatic\nwriting evaluation; (ii) Error propagation. Errors occurring in early\nstages (e.g., morpheme analysis) should have minimal impact in\nlater stages (e.g., writing evaluation). This is particularly important\nin Korean, an agglutinative language, where frequent morpholog-\nical changes make it more vulnerable to error propagation [22];\nand (iii) Evaluation explainability. High-level, abstract evaluation\nresults should be interpretable by humans, who need to understand\nthe reason behind the scores and the features that influenced the\nresults. Providing this explainability to users is crucial for ensur-\ning reliability, as these tools have the potential to make mistakes;\nUnfortunately, existing Korean text analyzers [16, 18, 20] and auto-\nmated writing evaluation tools [21, 37] do not fully meet all these\nrequirements, limiting their practical use.\nTo address the research gap, we introduce UKTA (Unified Korean\nText Analyzer), a comprehensive Korean text analysis system for\nevaluating Korean writing. First, we provide accurate low-level anal-\nsis based on state-of-the-art Korean morpheme analyzer, which\nminimizes error propagation. In addition to morpheme analysis, we\ncategorize and provide key features, such as lexical richness and\nsemantic cohesion, at the mid-level to enable explainable writing\nevaluation. Finally, we present a comprehensive rubric-based writ-\ning score as a high-level metric based on a novel attention-based\ndeep learning method and provide the features contributing to that\nscore to enhance explainability and reliability. Notably, using all the\nsuggested features improves writing evaluation performance com-\npared to baseline in terms of accuracy and quadratic weighted kappa\nscores. To the best of our knowledge, UKTA is the first compre-\nhensive Korean text analysis and writing evaluation tool providing\naccurate results from a multi-view perspective.\nOur contributions are summarized as follows:\n\u2022 We introduce UKTA, a comprehensive Korean text analysis\nand writing evaluation system from multiple perspectives, and\npresent a tool for its practical application.\n\u2022 Rather than simply presenting writing evaluation scores, UKTA\nenhances explainability and reliability by providing detailed\nfeature scores such as morpheme, lexical diversity, and cohesion.\n\u2022 Experimental results demonstrate that writing evaluation ac-\ncuracy improves when the proposed features are considered,\ncompared to scores derived solely from raw text.\nThe remainder of this paper is organized as follows. Section 2\nreviews the related work. Section 3 introduces the proposed ap-\nproach in detail. In Section 4, we present the experimental results.\nFinally, Section 5 concludes our work and outlines future work."}, {"title": "2 RELATED WORK", "content": "Text analysis has become an essential tool for evaluating written\ncontent, particularly in language education, and linguistic research\n[13, 26]. Modern English text analyzers utilize a combination of\nlexical and semantic metrics to assess the quality, coherence, and\ncomplexity of a text [6]. These tools break down a text into mea-\nsurable features, such as lexical diversity and cohesion, to provide\nquantitative insights. Although English text analyzers have seen\nsignificant success in both academic and practical applications,\napplying similar methodologies to Korean text analysis has been\nchallenging due to linguistic differences. This section reviews key\nworks on lexical diversity and cohesion, which are fundamental\ncomponents in text analysis systems.\nLexical density is a key indicator of a writer's vocabulary depth\nand is important for evaluating text quality in English. Common\nmeasures include the number of different words (NDW) [27] and"}, {"title": "3 UNIFIED KOREAN TEXT ANALYZER", "content": "This section introduces UKTA (Unified Korean Text Analyzer),\nwhich sequentially performs low-level morpheme analysis (in Sec-\ntion 3.1), mid-level lexical feature analysis (in Section 3.2), and\nhigh-level automatic writing evaluation (in Section 3.3). The overall\nprocess for our tool is illustrated in Figure 2."}, {"title": "3.1 Morpheme Analysis", "content": "This section describes the importance of Korean morpheme analy-\nsis and outlines the methods. Morpheme analysis is the first step,\nlow-level analysis before conducting Korean writing evaluation\nand lexical feature analysis. However, due to the nature of Korean,\naccurately segmenting wordpieces into morpheme units is chal-\nlenging, as their forms can change due to different suffixes [22].\nSuch errors can propagate to subsequent steps, including lexical\nfeature analysis and writing evaluation."}, {"title": "3.2 Lexical Feature Analysis", "content": "This section introduces the process of mid-level feature analysis\nand describes various features. After morpheme analysis, diverse\nfeatures are numerically evaluated based on the morphemes. We\nprovide numerical results for 294 features, broadly categorized into\nthree groups: basic lexical features, lexical diversity, and cohesion.\nThese features not only provide numerical information but also\noffer explainable insights for subsequent writing evaluation results.\nDetailed descriptions of each group are provided below.\nBasic Features. The basic lexical features of a text represent its\nfundamental linguistic composition. These features include mea-\nsurements such as count, density, and length of morphemes or\nwords. Accurate tagging and categorization of morphemes are es-\nsential for ensuring the precision of these metrics. Additionally, a\nlist of sentences containing each morpheme is provided to clarify\ntheir contextual use Figure 1(B-3). This detailed examination of\nbasic features offers fundamental insights into the structural and\nlinguistic properties of the text, serving as a basis for calculating\nmore complex features.\nLexical Diversity. Lexical diversity [14] is measured based on the\ndegree of connectivity between sentences or paragraphs, reflecting\nvocabulary depth and linguistic diversity [13, 18]. This measure is\nevaluated through the calculation of lexical features such as Type-\nToken Ratio (TTR) and other diversity features. We provide each\nlexical diversity feature for all tokens, as well as for each specific\nmorpheme. An example lexical diversity output is provided in Fig-\nure 1(B-3). A detailed description of the key feature for measuring\nlexical diversity included in our system is provided as follows:\n\u2022 Type-Token Ratios: TTR, RTTR (Root TTR), and CTTR (Corrected\nTTR) are fundamental features for calculating lexical diversity.\nThese features tend to decrease as text length increases, making\nthem suitable for comparisons between texts of similar length\n[33]. Formally, these features are calculated as:\nTTR = $\\frac{t}{w}$ RTTR = $\\frac{t}{\\sqrt{w}}$ CTTR = $\\frac{t}{\\sqrt{2w}}$  (1)\nwhere t and w are the number of unique morphemes and the\ntotal number of morphemes in a given text, respectively.\n\u2022 Equal segmented Type-Token Ratios: MSTTR (Mean Segmental\nTTR) and MATTR (Moving Average TTR) are both extensions\nof the traditional TTR, designed to address its sensitivity to text\nlength. MSTTR calculates the average TTR over equal-length,\nand non-overlapping segments of a text, which standardizes the\nmeasure across different text lengths:\nMSTTR = $\\frac{1}{k} \\sum_{i=1}^{k} \\frac{t_i}{w_i}$  (2)\nwhere k = $\\left[\\frac{N}{n}\\right]$ is the number of non-overlapping segments, with\nN as the total number of tokens in the text and n as the window\nsize. Here, ti and wi denote the number of unique tokens and\nthe total number of morphemes in the i-th segment, respectively.\nMATTR, in contrast, uses a moving window to compute TTR over"}, {"title": "3.3 Automatic Writing Evaluation", "content": "This section introduces a high-level automatic writing evaluation\nprocess that integrates the previously suggested low- and mid-level\nlexical features with the existing Korean automated writing evalua-\ntion model. The automatic writing evaluation task can be formally\ndescribed as follows: Given an essay X = {xi}\\u2075, consisting of\nN sentences, the objective is to predict 10 evaluation scores y1....10\ncorresponding to distinct evaluation criteria (commonly referred\nto as rubric) such as grammar, vocabulary, and consistency.\nThe architecture of our automatic writing evaluation model is\nshown in Figure 2(c). Previous Korean automated writing evalua-\ntion models have primarily focused on raw text, without consider-\ning the overall characteristics of the essay [21, 37]. However, our\nmodel is capable of training on both the raw essay features (i.e.,\nsentence-level features) and the overall characteristics (i.e., essay-\nlevel features), including basic lexical features, lexical diversity, and\ncohesion. These essay-level features provide a comprehensive per-\nspective of the essay's quality. These features are derived from the\nKorean morpheme analyzer, enabling the model to perform accu-\nrate essay-level analysis during training. This process improves the\noverall accuracy of the writing evaluation. Finally, we use the atten-\ntion weights from the attention layer to emphasize the importance\nof different essay-level features for each sample, highlighting which\nfeatures contributed to the model's predictions. Unlike previous\napproaches, we provide multiple-view analysis results using these\nattention scores, enhancing both the reliability and explainability\nof the writing evaluation results.\nIn summary, we utilize both i) sentence-level and ii) essay-level\nrepresentations, then iii) combining them for a reliable and explain-\nable writing evaluation. The detailed description of the proposed\nmodel is as follows:\ni) Extracting sentence-level representations. We extract sentence-\nlevel representations of the essay using a pre-trained language\nmodel and a bidirectional Gated Recurrent Unit (BiGRU), as illus-\ntrated in the bottom of Figure 2(c). The process begins by dividing\nthe essay into N sentences, and each sentence is tokenized. The\ntokenized sentences are input into KoBERT\\u2077, a BERT model\npre-trained on a large-scale Korean corpus. This step produces\nN embedding vectors, denoted as e1, e2, ..., eN. The resulting\nsentence-level embedding vectors are subsequently fed into a\nBiGRU. These embedding vectors are then passed through a Bi-\nGRU, which computes the final sentence-level representation of\nthe essay using its last hidden state vector, h = [h; h].\nii) Extracting essay-level representations. We first extract the lexical\nfeatures f\u2208 R\u00b2\u2079\u2074, which consist of 294 values from the raw text.\nThese features are normalized using a standard scaler:\nf' = $\\frac{f - \\mu}{\\sigma}$   (7)\nwhere \u00b5 and \u03c3 represent the mean and standard deviation calcu-\nlated from the feature, respectively. This normalization process\nensures that each feature is on the same scale and comparable.\nThe normalized feature vector f' is then passed through an atten-\ntion layer, which assigns different importance weights A to each\nfeature. The attention-weighted vector is computed through an"}, {"title": "4 EXPERIMENTS", "content": "In this section, we validate the effectiveness of the proposed fea-\ntures in the automated writing evaluation system. To achieve this,\nwe measure the performance of the automated writing evaluation\nmodel depicted in Figure 2, and compare it with the baseline model.\nAdditionally, we examine the importance of each feature for sample\ndata by analyzing the attention weights from the attention layer."}, {"title": "4.1 Experimental Settings", "content": "The purpose of our experiment is to verify whether the feature\nscores obtained from our text analysis system lead to performance\nimprovements when incorporated into the training process of\nthe automated writing evaluation model. Additionally, we aim to\ndemonstrate the explainability of this automated evaluation tool\nby analyzing the predicted evaluation scores on sample data and\nassessing the importance of each feature.\nDataset and Baseline model. For our experiment, we used the Essay\nEvaluation Dataset provided by AI-HUB. This dataset is the largest\nwriting evaluation dataset available in Korea, consisting of essays\nwritten by students ranging from 4th grade of elementary school to\n3rd grade of high school. It contains approximately 46,000 essays,\nwhich are categorized into five main types. The essays can be further\ndivided into 50 topics, with around 900 essays per topic. A total of\n46,000 essays were separated by topic, with approximately 40,000\nused as training data and 6,000 used as test data. The essays are\nevaluated using a trait scoring system, with 10 distinct evaluation\nrubric scores assigned to each essay. The evaluation scores, used as\nlabels, were assigned by human raters and reflect the assessments\nmade by reliable experts in Korean writing evaluation.\nTo verify the effectiveness of the proposed features in the auto-\nmated evaluation tools, we use the automated evaluation model\nprovided by AI-HUB as the baseline. The baseline model utilizes\nonly a PLM and a BiGRU. Sentence-level embedding vectors ob-\ntained from KoBERT are processed through the BiGRU to produce\nan essay representation vector, which is then passed through a\nsingle linear layer to output scores for 10 evaluation criteria.\nEvaluation metrics. We utilized both accuracy and Quadratic\nWeighted Kappa (QWK) as the evaluation metrics to evaluate the\nmodel performance. Accuracy is a ratio of the number of essays"}, {"title": "4.2 Experimental Results", "content": "Quantitative results. Table 1 presents the quantitative results of the\nbaseline automated writing evaluation model for each evaluation\nmetric, along with the performance of the UKTA. Both the accuracy\nand QWK metrics show a significant improvement when feature\nscores are incorporated. A closer examination of each rubric reveals\nthat 9 out of the 10 rubrics show improvements in both accuracy\nand QWK, indicating that the model benefits from the feature scores\nin evaluating the appropriateness of \"expression\", \"organization\",\nand \"content\" in the essays. Specifically, in the \"expression\" cate-\ngory, there is a notable increase in performance for \"vocabulary\"\nand \"sentence expression\"; in the \"organization\" category, \"length\"\nshows improvement; and in the \"content\" category, \"originality\" and\n\"narrative\" demonstrate significant gains. These findings suggest\nthat the feature scores most effectively help the model evaluate the\nappropriateness of vocabulary, sentence expression, adequacy of\nlength, narrative quality, and originality of content. Finally, when\naveraging the performance across all rubrics, the accuracy increased\nfrom 0.649 to 0.657, and the QWK improved from 0.509 to 0.538."}, {"title": "5 CONCLUSION", "content": "In this work, we propose UKTA, a comprehensive Korean text anal-\nysis and writing evaluation system designed for practical use. Un-\nlike existing Korean writing evaluation tools, UKTA provides an\nautomated writing evaluation score along with analyses such as\nmorpheme analysis, lexical diversity features, and cohesion, enhanc-\ning the evaluation explainability and reliability. Additionally, our\nproposed evaluation model based on lexical features outperforms"}]}