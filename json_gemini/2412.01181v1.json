{"title": "Training Stiff Neural Ordinary Differential Equations with Explicit Exponential Integration Methods", "authors": ["Colby Fronk", "Linda Petzold"], "abstract": "Stiff ordinary differential equations (ODEs) are common in many science and engineering fields, but standard neural ODE approaches struggle to accurately learn these stiff systems, posing a significant barrier to widespread adoption of neural ODEs. In our earlier work, we addressed this challenge by utilizing single-step implicit methods for solving stiff neural ODEs. While effective, these implicit methods are computationally costly and can be complex to implement. This paper expands on our earlier work by exploring explicit exponential integration methods as a more efficient alternative. We evaluate the potential of these explicit methods to handle stiff dynamics in neural ODEs, aiming to enhance their applicability to a broader range of scientific and engineering problems. We found the integrating factor Euler (IF Euler) method to excel in stability and efficiency. While implicit schemes failed to train the stiff Van der Pol oscillator, the IF Euler method succeeded, even with large step sizes. However, IF Euler's first-order accuracy limits its use, leaving the development of higher-order methods for stiff neural ODEs an open research problem.", "sections": [{"title": "I. INTRODUCTION", "content": "No discipline can fully grasp the intricacies of complex systems without the rigorous development of mathematical models, which serve as a cornerstone for advancing our understanding of natural and engineered systems. For instance, in epidemiology, ordinary differential equations (ODEs) model the spread of infectious diseases such as influenza, measles, and COVID-19, while in medicine, they describe the dynamics of CD4 T-cells in HIV patients. Similarly, partial differential equations (PDEs) are fundamental in climate science, enabling the modeling of atmospheric and oceanic processes to forecast and mitigate risks such as severe droughts, flooding, and extreme storms. Crafting a mathematical model with adequate detail is crucial, as it enables the identification of effective intervention strategies, such as pharmaceutical treatments, to mitigate undesirable outcomes such as the spread of disease. In engineering, mathematical models facilitate the design of control systems, ensuring reliability and stability in diverse conditions. However, the traditional model development cycle is time-intensive, involving finding a candidate model to describe processes, using data to fit parameters to the model, analyzing uncertainties in the fitted parameters, and performing additional experiments to refine and validate the model.\nSystem identification has been revolutionized by Sparse Identification of Nonlinear Dynamics (SINDy)1\u20133, a regression technique applied on numerical approximations to the derivative. SINDy has been successfully applied to ODE model extraction in diverse domains such as fluid dynamics4, plasma physics5, and chemical reaction networks6,7. However, its reliance on densely sampled data limits broader adoptions.\nWith unprecedented data from IoT devices9,10, automated high-throughput biological research systems11,12, and Earth observation systems13, data-driven modeling has emerged as a powerful approach to identify system dynamics directly without relying on first principle models. Recent frameworks such as neural ODEs8,14\u201324, physics-informed neural networks (PINNs)25\u201331, and MeshGraphNets32 offer flexible solutions"}, {"title": "II. METHODS", "content": "Neural Ordinary Differential Equations14 (neural ODEs) are a type of neural network designed to approximate time-series data, y(t), by modeling it as an ODE system. In many scientific fields, the ODE system we aim to approximate takes the following form:\n$\\frac{dy(t)}{dt} = f (t,y(t), \\theta),$                                                                                                 (1)\nwhere t represents time, y(t) is a vector of state variables, \u03b8 denotes the parameter vector, and f is the function defining the ODE model. Finding an accurate form for f can be a complex and time-consuming task. To address this, neural networks can be used to approximate f by leveraging the universal approximation theorem76, allowing us to replace f with a neural network model, NN:\n$\\frac{dy(t)}{dt} = f \u2248 NN (t,y(t), \\theta).$                                                                                                  (2)\nLike traditional ODE solvers, neural ODEs are solved by integrating from an initial condition using standard differential equation discretization schemes77\u201379 to produce time-series predictions. Most of the time, some parts of the model, denoted as $f_{known}$, are known, but not all mechanisms and terms describing the full model are understood. Neural ODES can refine or improve the model by learning only the missing or poorly understood components:\n$\\frac{dy(t)}{dt} = f_{known} (t, y(t), \\theta) +NN (t,y(t),\\theta).$ (3)\nThis method enables the identification and modeling of gaps in existing models, allowing for targeted refinement where traditional modeling is incomplete or uncertain. It combines the strengths of known physics with data-driven discovery to enhance the overall accuracy and completeness of the system representation."}, {"title": "Polynomial Neural ODEs", "content": "Differential equations with polynomial right-hand side functions f commonly appear in fields like chemical kinetics80, cell signaling 81, gene regulation82, epidemiology83, and ecology84. When it is known that a system is governed by polynomial relationships, polynomial neural ODEs become particularly effective for solving these inverse problems.\nPolynomial neural networks33,85 are a type of symbolic network architecture where the outputs are polynomial functions of the inputs. There are various forms of polynomial neural networks, each with its unique structure and properties. For an extensive overview, readers can consult Grigorios G. Chrysos's work on these architectures. In our research, we found that the \u03c0-net V1, as proposed in Ref. 33, was particularly effective (see Fig. 1). This architecture is constructed using Hadamard products86 of linear layers without activation functions:\n$L_i(x) = x*w_i+b_i$                                                                                                                             (4)\nwhere $L_i$ represents linear transformations. This formulation allows for the creation of higher-degree polynomial expressions, and the network's structure must be predefined according to the desired polynomial degree. Importantly, the \u03c0-net does not require hyperparameter tuning, and it can generate any n-degree polynomial for the given input variables. The dimensions of hidden layers can vary, provided that the Hadamard product operations remain dimensionally compatible. By integrating polynomial neural networks into the neural ODE framework14, Polynomial Neural Ordinary Differential Equations (PNODEs) offer a novel approach to symbolic modeling. Since the output of a polynomial neural ODE is constructed solely through tensor and Hadamard products without nonlinear activation functions, the final trained network can be directly translated into a symbolic mathematical expression. This is a significant advantage over traditional neural networks and neural ODEs.\nPolynomial neural ODEs are particularly beneficial for learning stiff ODEs and evaluating numerical methods due to several key advantages. They do not require data normalization or standardization, making them straightforward to apply across different modeling scenarios. Additionally, their ability to manage outputs over a wide range of scales is crucial for stiff ODEs, which often involve constants spanning multiple orders of magnitude, offering a robust solution for such complex systems."}, {"title": "B. Stiff ODEs", "content": "Stiff ODEs present a challenge where classical explicit integration methods become inefficient due to stability constraints. Stiffness occurs when there is a large disparity in time scales, often indicated by widely varying eigenvalues in the Jacobian matrix. For example, the stiff van der Pol oscillator with \u03bc = 1000 (Figure 2) requires 422,442 data points for stable integration using the explicit Runge-Kutta-Fehlberg method, making the computation slow and costly. In contrast, the implicit Radau IIA 5th order method reduces this to 857 points, significantly speeding up the process. However, the computational cost of implicit methods isn't fully captured by the number of time points, as they involve iterative processes. A better measure is the number of function evaluations: the explicit method required 2,956,574 evaluations, whereas Radau IIA required only 7,123, leading to much faster integration.\nThis example illustrates the core issue of stiffness in differential equations\u2014classical explicit methods require very small time steps for stability, greatly increasing computational cost. This is especially problematic for neural ODEs, where thousands of integrations are needed during training. Although classical explicit methods are simpler and cheaper per step, their inefficiency in stiff problems makes them impractically slow."}, {"title": "C. Discretize-Optimize vs. Optimize-Discretize", "content": "Optimizing neural ODEs generally involves two primary strategies: Discretize-Optimize (Disc-Opt) and Optimize-Discretize (Opt-Disc). These methods differ fundamentally in their approach to solving and optimizing ODE problems, as depicted in Fig. 3. Disc-Opt starts by discretizing the ODE, allowing optimization to be performed directly on this discretized form, which is easier to implement, especially with automatic differentiation tools for efficient gradient computation87. Conversely, Opt-Disc defines an optimization problem in the continuous domain and computes gradients before discretizing, requiring the numerical solution of the adjoint equation88, which is computationally more demanding.\nBoth strategies have their pros and cons, particularly concerning the computational burden of solving the ODE during forward propagation and calculating gradients for optimization. Solving forward propagation accurately requires considerable memory and computational power, which can be prohibitively expensive during training. While using a less accurate solver might speed up the process, it introduces risks for the Opt-Disc method, where inaccuracies in forward and adjoint solutions can lead to poor gradient quality89. This issue does not affect Disc-Opt, where gradient accuracy is decoupled from the forward solver's precision, providing more flexibility in optimizing solver accuracy and improving training efficiency. Opt-Disc's reliance on adjoint methods for backward computation of the neural ODE also makes it more prone to numerical instabilities, particularly with stiff ODEs, impacting the robustness and effectiveness of training.\nResearch by Onken et al.53 supports the benefits of the Disc-Opt approach. It shows that Disc-Opt not only achieves similar or better validation loss outcomes compared to Opt-Disc but also does so with significantly reduced computational costs\u2014yielding an average speedup of 20x. This efficiency arises partly because Opt-Disc methods can produce unreliable gradients if the state and adjoint equations lack sufficient precision89. In contrast, Disc-Opt maintains gradient reliability regardless of solver accuracy, allowing for adjustments based on data noise levels, which is advantageous in scientific modeling.\nGiven these findings, our research focuses on the Discretize-Optimize approach to develop a more efficient differential ODE solver. By building on the advantages of Disc-Opt, particularly for stiff ODEs, we aim to refine training strategies using different integration schemes, ultimately creating solvers that are both robust and computationally efficient."}, {"title": "D. Classical Numerical Methods for Solving ODEs", "content": "Solving ordinary differential equations (ODEs) typically involves two main types of methods: single-step and multistep approaches. Single-step methods, such as Euler and Runge-Kutta, calculate the solution at the next time step using only the current state. This simplicity makes them easier to implement and analyze since they do not require any historical data from previous steps. On the other hand, multistep methods, including Adams-Bashforth and Adams-Moulton, derive the next value based on several preceding time steps. While these methods can offer computational efficiency in some scenarios, they are more complex to manage due to their dependence on past values and their intricate stability requirements. Thus, multistep methods, while potentially more efficient, involve added complexities not present in single-step methods.\nSingle-step methods are favored for their simplicity and ease of implementation, making them a natural choice for initial studies in neural ODE integration. Because they depend only on the current state, they simplify the process of understanding neural ODE dynamics and enable straightforward backpropagation through the computed ODE solution. Conversely, multistep methods pose specific difficulties in the context of neural ODEs. These methods require accurate data from several previous steps, which can be problematic when training data is noisy or incomplete. In such cases, past points needed for the method must be recalculated, increasing computational effort and complexity. Moreover, backpropagation with multistep methods is more challenging because gradients must be propagated not just through the current step but also through the reconstructed previous steps. Due to these complications, single-step methods should be explored during the early stages of neural ODE research, while more complex multistep methods are better suited for later exploration once a solid understanding of single-step methods has been established.\nImplicit Methods are Expensive\nImplicit methods offer a robust approach to handling the difficulties associated with stiff ODEs. In contrast to explicit methods, which determine the next step using only current state values, implicit methods involve solving a system of nonlinear equations at every time step. Although this requires more computational effort, it delivers much higher stability, allowing for larger time steps while maintaining accuracy. This balance between computational expense and stability makes implicit methods ideal for stiff problems; however, they can be less efficient for non-stiff ODEs where stability is not a concern.\nSome common implicit methods for solving stiff ODEs are the backward Euler method, the trapezoid method, and Radau IIA methods. The backward Euler method is the simplest approach:\n$y_{n+1} = y_n+hf(t_{n+1},y_{n+1}),$                                                                                                                                                                                                                                                                          (5)\nBackward Euler is A-stable, providing strong damping of oscillations and stability for stiff systems. However, it can overdampen solutions, making them too smooth. The trapezoid method improves upon backward Euler by averaging the derivatives at the current and next points:\n$y_{n+1} = y_n + \\frac{h}{2}(f(t_n, y_n) + f (t_{n+1},y_{n+1})) .$                                                                                                                                                                                                                                                       (6)\nTrapezoid method is second-order accurate and A-stable but not L-stable, meaning it may allow some stiff components to oscillate rather than decay smoothly. While it offers higher accuracy, the potential for introducing oscillations can be problematic for stiff neural ODEs during training. Radau IIA methods90-92 have the following form:\n$\\sum_{j=1}^{S}Y_i=y_n+h\\sum_{j=1}^{S}a_{ij}f (t_n+c_jh,Y_j), i = 1,...,s,$                                                                                                                                                                                                                                                                             (7)\n$\\sum_{j=1}^{S}y_{n+1}=y_n+h\\sum_{j=1}^{S}b_jf(t_n+c_jh,Y_j)$                                                                                                                                                                                                                                                                    (8)\nwhere Y; are the stage values, aij are the coefficients from the Butcher tableau, and cj are the nodes. The Butcher tableau for Radau3 and Radau5 can be found in Ref. 92. The Radau3 and Radau5 methods are both A-stable and L-stable, which ensures rapid decay of transient components without oscillations, even with large time steps.\nFor these methods, the solution at the next step, yn+1 is found by solving a multivariate nonlinear system of equations using techniques such as Newton's method. This adds significant computational cost and may require step size adjustments if convergence fails. Although the Radau IIA methods offer greater accuracy, they require solving for Y; a nonlinear solution involving a matrix-valued function, which significantly increases computational cost and introduces additional convergence challenges.\nBackpropagation is the process of tracing a computational graph to compute gradients of a loss function with respect to model parameters. For neural ODEs, explicit integration methods like Euler and RK4 compute future values directly from the current state, forming a straightforward computational graph that is easy to backpropagate through. In contrast, implicit methods like the backward Euler or Radau IIA methods solve a nonlinear equation at each step, creating a nested graph structure. This makes backpropagation computationally expensive and susceptible to numerical issues when unrolling these loops. The implicit function theorem provides a workaround by computing gradients directly at the solution, avoiding the need for a full unroll of the nonlinear iteration. We start by reformulating our implicit scheme as an equation to find a root for:\n$\\delta(y_n, y_{n+1}, \\theta) = 0.$                                                                                                                                                                                                                                                                                     (9)\nWe solve this nonlinear equation for our future time point yn+1 using a nonlinear solver such as Newton's method. Once we have the solution prediction yn+1 for the neural ODE at the time tn+1, we need to compute the gradient of this prediction with respect to the model parameters, $ \\frac{\\delta y_{n+1}}{\\delta \\theta}$ to update the neural network's parameters. While backpropagating through the multiple iterations of the nonlinear solver using automatic differentiation is possible, it is both resource-intensive and numerically unstable. Instead, we utilize the implicit function theorem for a more stable gradient calculation:\n$\\frac{\\delta y_{n+1}}{\\delta \\theta} = -(\\frac{\\delta g}{\\delta y_{n+1}})^{-1}\\frac{\\delta g}{\\delta \\theta}$                                                                                                                                                                                                                                                                            (10)\nThe expression from the implicit function theorem is computed at the point where the nonlinear system is solved. If this solution lacks sufficient precision, the calculated gradients may suffer from numerical instability and inaccuracy.\nIn our previous work (see Ref. 63), we demonstrated successful training of stiff neural ODEs using the backward Euler, trapezoid, and Radau IIA methods. As a first-order method, backward Euler required an impractically high number of training data points and very small step sizes, underscoring the need for higher-order methods for training neural ODEs, especially when using adaptive step size solvers.\nAmong the methods tested, the trapezoid method proved to be the most reliable, even though it is not the highest order method. Radau IIA, with its higher order, allowed for training with the largest step sizes, making it a strong candidate for handling stiff neural ODEs. Despite their effectiveness, these implicit methods are computationally expensive due to the need for solving nonlinear equations. Therefore, we aim to explore alternative approaches that avoid nonlinear solvers to reduce computational costs."}, {"title": "E. Explicit Exponential Integration Methods", "content": "Exponential integration methods93\u201397 approach the solution of an ODE by separating it into a linear component and the remaining nonlinear terms:\n$\\frac{dy}{dt} = f(t,y(t)) = Ly+N(t,y)$                                                                                                                                                                                                                                                                                   (11)\nwhere the linear term, L, is the Jacobian evaluated at our initial condition yo:\n$L=\\frac{df}{dy}(y_0)$                                                                                                                                                                                                                                                                                                         (12)\nand the nonlinear term, N, represents the remaining components:\n$N(t,y(t)) = f(t,y(t)) \u2013 Ly.$                                                                                                                                                                                                                                                                                (13)\nThis decomposition allows for the exact integration of the linear component using the matrix exponential, which is particularly beneficial when the linear term represents the stiffest part of the dynamics, and the nonlinear part changes more slowly. By handling the stiff linear dynamics exactly and using an explicit method for the slower nonlinear part, we can better manage stiffness. However, this reformulation still leaves the question of how to effectively integrate the new ODE.\nIntegrating Factor Methods\nIntegrating factor methods93\u201395,98,99 use a change of variables\n$w(t) = e^{-Lt}y(t),$                                                                                                                                                                                                                                                                                                (14)\nwhich, when differentiated with respect to t, results in:\n$\\frac{dw(t)}{dt} = e^{-Lt} (\\frac{dy(t)}{dt} - Ly(t))$                                                                                                                                                                                                                                                                    (15)\nBy substituting Eqn. 13 into our equation, we obtain:\n$\\frac{dw(t)}{dt} = e^{-Lt}N(t,y(t)) = e^{-Lt}N(t,e^{Lt}y(t)).$                                                                                                                                                                                                                                                        (16)\nIntegrating factor methods are straightforward to derive because they allow the use of any standard numerical integration technique, such as multi-step or Runge-Kutta methods, to solve Eqn 16. After applying the chosen integration method, the solution is converted back to the original variable y. Despite their simplicity, these methods are often discouraged in the literature because they can fail to compute fixed points correctly, resulting in greater errors compared to other exponential integration techniques. Despite the literature's discouragement, the integrating factor Euler method is the most robust exponential integration method we could find for neural ODEs. Applying the forward Euler method on Eqn 16 results in the A-stable first-order integrating factor Euler scheme64,93\u201395,98,99, which has a local truncation error of O(h\u00b2LN):\n$y_{n+1} = e^{Lh}(y_n+hN_n).$                                                                                                                                                                                                                                                                                        (17)\nDespite an extensive search for a higher-order exponential integration method, our recent analysis (see Ref. 75) indicates that exponential integration methods fail to improve upon the first-order accuracy of IF Euler while remaining stable, revealing the IF Euler method as the only reliable choice for repeated, inexpensive integration in applications such as neural ODEs.\nComputing the Matrix Exponential\nComputing the matrix exponential100\u2013104, $e^A$, for these exponential integration schemes can be computationally expensive. For dense matrices, a common approach is the scaling and squaring method with a Pade approximation, which involves scaling down the matrix, applying a rational approximation, and squaring the result. This method has a complexity of O(n\u00b3), making it impractical for large matrices. If A is diagonalizable, $e^A$ can be computed as $e^A = VE^AV^{-1}$, where \u039b is diagonal, but this still has a complexity of O(n\u00b3) due to costly matrix multiplications and inversions. For large sparse matrices, Krylov subspace methods, such as Arnoldi iteration, can reduce the complexity to O(n\u00b2). However, these iterative methods may require numerous iterations to reach the desired accuracy, which can increase computational costs and reduce stability. As the dimension of the ODE system grows, the cost of computing the matrix exponential, which scales as O(n\u00b2) in the best case scenario for iterative methods, can become prohibitively expensive."}, {"title": "III. RESULTS", "content": "We start by testing our methodology on the stiff univariate linear equation (Example 1), which serves as a standard benchmark for stability analysis in different ODE discretization methods, making it an ideal initial baseline. Next, we conduct tests on a 10D stiff linear system (Example 2) to verify whether the performance characteristics observed earlier for the univariate linear equation persist as the system dimension is increased. A 3D nonlinear stiff system of ODES, hand-designed for this test, is used in Example 3 to examine the methods' capabilities in nonlinear scenarios. We conclude with Example 4, which tests the methodology on a stiff Van der Pol oscillator, a model where classical implicit methods fail during neural ODE training. This example showcases the stability of the exponential integrating factor Euler method, which succeeds where classical implicit methods fall short. We produced training data for each of these model and analyzed how effectively the different single-step integration methods could recover the ODE model."}, {"title": "A. Example 1: Stiff Linear Model", "content": "In our first test, we consider the stiff linear equation:\n$\\frac{dy}{dt} = -10000y, y(0) = 1000, t\u2208 [0,0.01].$                                                                                                                                                                                                                                                                                            (18)\nThe model depicted in Figure 4 is a standard benchmark for evaluating the stability of numerical methods on stiff systems of ODEs. Its high degree of stiffness, driven by the large negative coefficient, results in an initial sharp transient followed by a slower decay to equilibrium. The slow decay region is the stiff region where explicit methods struggle due to stability limitations.\nWe used the SciPy105 Radau solver to generate training data. Figure 4 displays the training data, consisting of the case with 100 uniformly spaced time points. As discussed in the methods section, we used the discretize-then-optimize strategy, segmenting the data into n - 1 time intervals, each treated as an IVP between adjacent time points. During each epoch, our custom JAX106,107 implementation of the solvers discussed in the methods section solves these n - 1 IVPs in parallel.\nTo establish a preliminary benchmark, we tested the implicit methods backward Euler, trapezoid, Radau3, and Radau5, using training data consisting of 50 to 10,000 uniformly spaced points in time. We then tested the performance of the exponential integrating factor Euler method on the same training data. For each experiment, we recovered the equation learned by the \u03c0-net V1 polynomial neural network and evaluated its accuracy by calculating the fractional relative error for each of the model's inferred parameters. Table 1 shows the recovered equations for varying amounts of training data, while Figure 5 plots the fractional relative error versus the number of data points. Remarkably, the exponential integrating factor Euler method accurately recovered the exact equation for all datasets, down to numerical precision. To determine its limitations, we gradually reduced n until the method failed, which occurred with just 5 training points. This finding suggests that the method's precise integration of linear ODEs makes it phenomenally well-suited for training linear ODEs."}, {"title": "B. Example 2: 10-Dimensional Stiff Linear Model", "content": "Our next example is a 10-dimensional stiff linear model:\n$\\frac{dy_0}{dt} = -10y_0 +5y_1,$\\n$\\frac{dy_1}{dt} =5y_0-20y_1+5y_2,$\\n$\\frac{dy_2}{dt} = 5y_1-50y_2+5y_3,$\\n$\\frac{dy_3}{dt} = 5y_2-100y_3 + 5y_4,$\\n$\\frac{dy_4}{dt} = 5y_3-500y_4 + 5y_5,$\\n$\\frac{dy_5}{dt} =5y_4-1000y_5 + 5y_6,$\\n(19)\\n$\\frac{dy_6}{dt} = 5y_5-5000y_6+5y_7,$\\n$\\frac{dy_7}{dt} = 5y_6-10000y_7 + 5y_8,$\\n$\\frac{dy_8}{dt} = 5y_7 - 20000y_8 + 5y_9,$\\n$\\frac{dy_9}{dt} = 5y_8-50000y_9,$\\ny_0(0) = 20, y_1 (0) = 20, y_2(0) = 20,y_3 (0) = 20,\\ny_4(0) = 20, y_5 (0) = 20, y_5(0) = 20, y_6(0) = 20, \\nt \u2208 [0,0.4].\nThe intriguing results of the 1-dimensional stiff linear model led us to test a higher-dimensional version to see if the observed patterns would be present with more variables. We engineered this toy model to have a concise parameter count of 28, considering its larger 10-dimensional size, while also ensuring that it had a high degree of stiffness. As shown in Figure 6, the model's 10 variables exhibit varying time scales, contributing to its stiffness. We generated training data using the same integration technique and discretize-then-optimize approach outlined in Example 1. We tested the same integration schemes as before for varying numbers of training data points ranging from 10 to 1000 time points. For each trial, we trained the polynomial neural network, recovered the equation learned by the polynomial neural network, and then assessed the accuracy of the identified parameters by calculating the fractional relative error. Tables 2 through 6 display the equations recovered with a limited dataset of n = 17 using the various stiff integration schemes. Figure 7 shows the fractional relative error vs the number of training data points. The findings from this experiment are consistent with those seen in the 1D stiff linear system. The IF Euler method demonstrates remarkable accuracy in reproducing the correct equations, attributable to its precise time integration for linear systems. The IF Euler method excels in accuracy when training data is scarce. As the training data increases, it matches the performance of the Radau5 method."}, {"title": "C. Example 3: 3D Nonlinear Model", "content": "In our third example, we consider the following stiff nonlinear system of ODEs:\n$\\frac{dy_1}{dt} = -500y_1 +3.8y_2+1.35y_3,$\\n$\\frac{dy_2}{dt} = 0.82y_1-24y_2+7.5y_3,$\\n$\\frac{dy_3}{dt} = -0.5y_1 +1.85y_2 \u2013 6.5y_3,$\\n(20)\ny_1(0) = 15, y_2(0) = 7, y_3(0) = 10, t \u2208 [0,5].\nFigure 8 shows the training region for this model. We specifically created this stiff neural ODE toy problem to be three-dimensional and contain only linear and quadratic terms. This increases the difficulty of numerical integration slightly but remains a small enough model for a comprehensive analysis of the model's nine identified parameters.\nWe generated training data using the same integration technique and discretize-then-optimize approach outlined in Example 1. We varied n, the total number of data points, to study how different amounts of training data affect the accuracy of the inferred parameters. We trained the polynomial neural ODE on each dataset, recovered the corresponding equations from the polynomial neural ODE, and assessed the accuracy of the inferred parameters by computing their fractional relative error. Tables 7, 8, 9, and 10 show the recovered equations for datasets consisting of 1467, 369, 94, and 48 data points respectively, while Figure 9 illustrates how the fractional relative error changes with the amount of training data. Although the exponential integrating factor Euler method is more cost-effective, the tables and figures highlight that it achieves the lowest overall performance for this model."}, {"title": "D. Example 4", "content": "$\\frac{dx}{dt}= y,$\\n$\\frac{dy}{dt}= 1000y - 1000x\u00b2y - x,$\\n(21)\nx(0) = 1, y(0) = 0, t \u2208 [0,1300]\nAs a nonconservative system with nonlinear damping, the Van der Pol model is a widely used tool for studying relaxation oscillations108. Its applications span across multiple fields, including neuroscience, seismology, and speech analysis, modeling phenomena such as neuron activity, fault line tremors, and vocal cord vibrations109\u2013112. The model's stiffness can be adjusted through the parameter u, with higher values making the system stiffer, ideal for testing neural ODEs designed for stiff problems. In our study, we set \u03bc = 1000, creating an extremely stiff ODE system. The method for generating training data remained unchanged from the process outlined in Example 1. Figure 10 shows the training data, revealing rapid transients with large spikes in values followed by stiff regions where changes are minimal. Every implicit integration technique we tried failed to properly train the stiff neural ODE on this complex system, with convergence issues disrupting the process entirely. Even when we provided training data with more frequent time intervals to improve stability, the increase in computational demand made this approach unfeasible and did not solve the underlying convergence problem.\nTable 11 summarizes the equations recovered for different amounts of training data, and Figure 11 shows the impact of the amount of training data on the fractional relative error of the parameters. Compared to the implicit solvers, the exponential integrating factor Euler method is remarkably inexpensive, needing only the matrix exponential calculation and no iterations. In both this case and Example 3, the performance of the method is limited by its first-order accuracy. Nonetheless, it was the only method to achieve stable training of the extremely stiff Van der Pol oscillator, maintaining stability regardless of the density of the time points in the training data."}, {"title": "IV. CONCLUSION", "content": "This paper presents a new method for training stiff neural ODEs based on the explicit exponential integrating factor Euler method. Our prior study (Ref. 63) pioneered the successful training of stiff neural ODEs, showcasing robust training and accurate recovery of stiff dynamics using single-step implicit methods such as backward Euler, trapezoidal method, Radau3, and Radau5. These implicit approaches handle stiffness effectively but involve solving a nonlinear system at each time step, which can be computationally intensive if many iterations are needed for convergence. Convergence challenges intensify for these methods as the system's dimensionality grows, making their use more problematic. The explicit exponential integrating factor Euler method offers an advantage over implicit methods by needing just one matrix exponential computation per integration step, whose cost scales at O(n\u00b2) in the best case, potentially offering a more cost-effective alternative to solving nonlinear equations.\nOur results demonstrate that the IF Euler method provides stability and cost-efficiency compared to implicit methods, which struggled with stability when learning the stiff Van der Pol oscillator. In contrast, the IF Euler method successfully learned the model at large step sizes without stability issues. The findings also show that the IF Euler method is hindered by its first-order accuracy, leading to reduced precision when training neural ODEs with a fixed step size. A higher-order method would better minimize errors, though we were unable to identify a suitable higher-order explicit exponential integration method that worked (see Ref. 75). Despite its stability, its first-order accuracy results in too many steps in adaptive step size configurations, limiting its practicality for use in adaptive ODE solvers for neural ODE training.\nOur most notable finding was the exceptional performance in learning stiff linear systems of ODEs. The matrix exponential nearly perfectly solves the linear ODEs, making the exponential integration approach particularly powerful for recovering accurate linear equations. Examples 1 and 2 demonstrate that this approach scales effectively as the dimensionality of the linear system increases. The IF Euler method excels in accuracy when training data is very limited and competes with Radau5 as the dataset size grows.\nThe exploration of explicit exponential integration methods provides a pathway for creating efficient, stable solvers for training stiff neural ODEs. This paves the way for applying these solvers to stiff neural PDEs, MeshGraphNets, physics-informed neural networks, and other data-driven methods that require repeatedly solving and differentiating through a differential equation."}]}