{"title": "MPVO: Motion-Prior based Visual Odometry for Point Goal Navigation", "authors": ["Sayan Paul", "Ruddra dev Roychoudhury", "Brojeshwar Bhowmick"], "abstract": "Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.", "sections": [{"title": "Introduction", "content": "Autonomous visual navigation in novel indoor environments is a fundamental skill for robots to perform further intelligent downstream tasks like finding and retrieving an object, rearranging various stuff, etc. This has been the focus of computer vision and robotics researchers for a long time. To bring together the community's efforts and standardize the evaluation framework and metrics, Anderson et al. [1] proposed the task of PointGoal navigation. In PointNav, (illustrated in Fig. 1), the agent or robot is initialized in a previously unseen environment and tasked to reach a goal location specified with respect to its initial location, i.e. go to (\u2206x, \u2206y). The action space of the agent is discrete and mainly consists of 4 types of action : move_forward, turn_right, turn_left and stop (to end the episode). A pointnav episode is considered successful if the agent stops within a pre-determined distance of the goal location (say for e.g. 0.36 metres) and within the maximum number of time-steps allowed in an episode (say for e.g. 500 time-steps). Apart from success, the agent is evaluated via navigation metrics like SPL [1] and SoftSPL [8].\nThis point-goal navigation task can either be approached via map-based methods (where the agent simultaneously maps an unexplored area, localizes within it and then plans a path towards the goal) [7,15] or via recent map-less end-to-end reinforcement learning based methods [8, 19,31]. Under the assump-tion of an ideal scenario, i.e. perfect localization using noiseless GPS+Compass, noiseless egocentric RGB-D sensors and absence of actuation noise; this Point-Nav task (v1) is fully solved by both map-based and map-less approaches. But the real-world is not ideal, so PointNav v2 came into existence where the agent needs to localize itself (absence of GPS+Compass sensors), has noisy RGB-D ob-servations and noisy actuations. Under this noisy setting, both map-based and map-less approaches need to focus first on accurate localization of the agent, before navigating. Recent map-less learning based approaches try to solve this by breaking down the task into two parts - learning visual odometry (VO) (for localization) and learning navigation policy (for actions) separately. During infer-ence, this VO model can be used as a drop-in replacement for the GPS+Compass sensor with navigation policies trained using ground-truth pose in simulation.\nNow, VO has been studied in Computer/Robot Vision literature for a long-time and many matured solutions exist but this PointNav task definition makes this problem harder to solve. Due to the agent's discrete action space and large motion per action (default: 0.25m forward and 30 deg turns), VO needs to be estimated in a wide camera baseline setting, i.e. the two views are wide-apart and the overlapping region is less. Most of the VO methods, both traditional and learned variants assume availability of frame-pairs with a large overlap or in other words, a narrow baseline setting. But for practical robot navigation, wide-baseline VO is necessary because situations might arise where the robot motion is fast and/or observation processing or transmission FPS is low. Recent learned VO methods used with RL-based navigation policies in map-less approaches [19,31] have tried to solve this problem and has achieved robust performance but suffer from sample-inefficiency, embodiment specificity and dataset specificity. It requires huge datasets and compute resources to train such a model and it can't be zero-shot transferred to any other embodiment or dataset.\nSo, we propose a robust and sample-efficient novel VO pipeline based on motion priors available while an agent is navigating an environment. We found out from our experiments that simple motion priors like action prior from agent's controller or planner, or coarse pose prior from another geometric pose estimator, etc can help the VO model learn faster using fewer samples than that required by state-of-the-art (SoA) learned methods. This finding not only reduces compute resources needed to train one model but also helpful in scenarios when the target domain data is scarce and costly to collect and curate. Instead of solving for model generalization, we tweaked these models to be more sample-efficient.\nWe also propose a training-free action-prior based geometric pose estimator or VO module for embodied agents which shows superior performance compared to frame-to-frame VO baselines created using state-of-the-art geometric modules, in standalone evaluation. With this novel geometric VO module, we show that motion priors can be effectively utilized to improve relative pose estimates from sparse feature matching methods. We use this module to estimate a coarse pose which serves as a better motion prior than action prior to train our neural VO model."}, {"title": "Related Work", "content": "Autonomous visual navigation for indoor robots has been studied for many years [10, 12, 18]. Recently, due to the advances in deep learning and computer vision, there has been a renewed interest in the use of learning to design naviga-tion policies for a variety of downstream tasks like PointNav [1], ObjectNav [4], AreaNav [2], Rearrangement [3], Vision-and-Language based tasks [20], etc.\nVisual Navigation: Traditional approaches decompose this task into several sub-tasks such as localization and mapping [6, 14], followed by planning and control [13, 21]. Though these methods can work well when their hyperparame-ters are hand-tuned properly, errors in one sub-module can propagate to other downstream sub-modules and affect navigation performance adversely. Recent end-to-end learned navigation policies [19,28] alleviate some of these issues and can even outperform traditional methods with sufficient data and training. But they require huge compute resources and sometimes pose problems in generaliza-tion. To circumvent these problems, some modular learned approaches [7,12,15] combine the best of both worlds to retain the benefits of learning the sub-tasks and also the traditional decomposition of the pipeline."}, {"title": "Visual Odometry", "content": "VO has been solved using both traditional and learned approaches in the last decade. Most VO methods can be categorized as Sparse or Dense depending on whether the method uses sparse features like keypoints, lines, etc for feature matching or it uses the whole image to determine the op-tical flow, photo-metric error, etc. Now, sparse methods are generally better suited for wide-baseline settings due to their ability to handle large viewpoint variations, which is our topic of interest. Traditional sparse approaches typi-cally use handcrafted keypoint feature descriptors like SIFT [17], ORB [6], etc followed by correspondence matching and pose estimation using geometric meth-ods. Recent learned sparse methods have introduced learned feature descriptors like SuperPoint [9], DISK [25], etc and learned correspondence matchers like LightGlue [16], SuperGlue [22], etc which perform better than their traditional counterparts. VO methods based on these learned submodules perform well on aggregated relative pose estimation metrics but suffer in trajectory performance due to the agent encountering difficult frame pairs containing featureless plain walls, high depth noise, repetitive patterns, etc. Our proposed geometric VO module leverages motion-priors to constrain and improve its relative pose es-timates in such difficult scenarios and perform better than SoA baselines as observed in our experiments.\nOn the other hand, there has been a surge of dense end-to-end learned VO methods [26, 27, 32] in the recent years which directly input image-pairs into CNN based models to regress the relative poses or CNN-RNN models to regress the absolute poses of the camera trajectory. Most of these methods either require narrow camera baseline and/or trained on outdoor autonomous driving datasets which consists less challenging camera trajectories compared to indoor mobile robot scenarios.\nIn the embodied-AI research community, renewed interest in learning bet-ter navigation policies have led to progress in VO models [19,31] which take wide-baseline noisy RGB-D image pairs as input and regress the relative pose as output. They exhibit robust performance when used to estimate robot trajecto-ries for pointgoal and other downstream navigation tasks. But they suffer from sample-inefficiency requiring huge compute resources and data. Compared to these methods, our proposed pipeline learns to regress relative pose from frame pairs in a sample-efficient way using the best of the both paradigms - sparse geometric methods and dense learned methods."}, {"title": "Method", "content": "Our overall pipeline [Fig. 2] consists of two blocks: a visual odometry method (VO) and a navigation policy. This is structured similar to the common prac-tice [19,31] of designing pointgoal navigation agents in a map-less setting. The VO method estimates the relative pose between the previous and the current observations, then updates the current goal coordinates using that. The naviga-tion policy determines at each time-step which action to take to reach the goal using this updated goal coordinates and the current observation."}, {"title": "Visual Odometry", "content": "Our VO method consists of two key building blocks: one Geometric Coarse Pose Estimator module (GCPE) [Fig. 3] and another Neural Fine Pose Regression model (NFPR) [Fig. 4]. The pipeline takes as input - two noisy RGB-D frames (Ot-1 or Oa from previous timestep and Ot or Os from current timestep) as observed by the agent and an action prior $T_{ap}$ (for e.g. 0.2 m forward, 30 deg right, etc) as provided by the agent's planner or policy. Then, it outputs the relative SE(2) pose $T_{pred}$ (\u2206x,\u2206y, \u25b3\u03b8) between the two RGB-D frames.\nThe GCPE module attempts to estimate a coarse relative pose $T_{cp}$ between the two RGB-D frames by performing sparse visual feature point correspondence matching, then sampling candidate relative poses in the SE(2) locality of the given action prior $T_{ap}$ followed by selecting the best scoring candidate pose using a heuristic cost function that re-weighs the 3D-3D point correspondences. It then iteratively refines this pose estimate following the same procedure using the last iteration's pose estimate as it's new prior in the current iteration. Due to noisy depth and lack of good point correspondences in feature-less walls and repetitive textures, this refined pose estimate is still coarse.\nSo, a neural network model (NFPR) is used to regress the fine pose $T_{pred}$ from the same RGB-D pair and using the above coarse pose estimate and the initial action prior as it's motion priors. This coarse pose estimated by the GCPE module helps improve the overall accuracy and sample-efficiency of the neural model, as compared to directly regressing the fine pose from only RGB-D pair and the action prior."}, {"title": "Geometric Coarse Pose Estimator (GCPE)", "content": "The GCPE module [Fig. 3] first detects and describes sparse visual keypoints in each of the RGB images (Ia, Ib) using an off-the-shelf keypoint detector and descriptor (like SuperPoint [9], SIFT [17], etc). Then it uses Nearest Neighbour Similarity Ratio (NNSR) (see ratio test in [5]) (also known as Lowe's ratio) to match the keypoints (p\u0430, \u0420\u044c) and output a set of correspondences $C_{ab}$. Only the top-m correspondences $C^m_{ab}$ according to the scores determined by the ratio test are kept. These 2D point correspondences are back-projected to 3D by using the pixel-aligned depth maps (Da, Db) and the camera intrinsics K.\nThen the coarse relative pose between the two views is estimated using Algorithm 1, where a relative pose sampler function generates candidate poses from a normal distribution using the action prior $T_{ap}$ as the mean (\u03bc) and a given large enough standard deviation (\u03c3) hyper-parameter. The sampler func-tion alternatively samples rotations (\u25b3\u03b8) and translations (\u2206x, \u25b3y) during each iteration. Given the candidate poses $T_{samples}$ and the correspondences $C^m_{ab}$, the Motion-Prior based Correspondence Weighing (MPCW) function weighs each 3D-3D correspondence $C^i_{ab}$ by transforming the points $p_a$ from the first view to the second view using the candidate poses, transforming the points $p_b$ from the second view to the first view using the inverse of the candidate poses and then calculating the inverse symmetric 3D Endpoint Error (3D-EPE) with their cor-responding points $p^r_b$ and $p^r_a$ of the other views respectively. The final weight of each correspondence $W^i_{C_{ab}}$ is the product of this inverse symmetric 3D-EPE and the best correspondence weights $W^{j-1(best)}_{Cab}$ from the previous iteration. These weights bias the search for the best pose estimate in iteration j around the SE(2) locality of the best pose estimate of iteration j 1. For the first iteration, the previous correspondence weights are 6, i.e. not taken into consideration.\n$W^i_{C_{ab}} = \\frac{W^{j-1(best)}_{Cab}}{ (T_{sampled}(p_a) - p^r_b)^2 + (T^{-1}_{sampled}(p_b) - p^r_a)^2 }$ (1)\nThe score of each candidate relative pose is determined by the sum of the weights of all the correspondences. The pose with the highest score is chosen to be the best pose estimate and used as the prior (\u03bc) in next iteration's sampling stage. The standard-deviation (\u03c3) is halved at the end of each iteration to nar-row down the search space. This iterative refining of the coarse pose estimate continues till percentage increase of best pose score with respect to the previous iteration is higher than a given threshold $\\epsilon_{score}$."}, {"title": "Neural Fine Pose Regression (NFPR)", "content": "The NFPR module [Fig. 4] is a deep neural network model which takes the RGB-D frame pair, action prior $T_{ap}$ and coarse pose estimate from GCPE module $T_{cp}$ as input and regresses the fine relative pose $T_{pred}$ as output. It consists of a ResNet-18 based visual feature encoder followed by a compression block and a 3-layer MLP pose decoder. Our model architecture is based on [19] and we introduce simple modifications which enhance the accuracy and sample-efficiency of the model.\nUsing the action prior $T_{ap}$ and depth maps of the two views, we generate coarse masks for the overlapping region between the two views. We warp the RGB (Ia) of the first view using the depth map (Da) onto the second view using the action prior $T_{ap}$ and warp the RGB (It) of the second view using the depth map (Dr) onto the first view using the inverse of the action prior $T_{ap}^{-1}$ to generate the overlap masks for the first and second views respectively. Then the RGB and Depth images of both views are masked using their corresponding overlap masks, and stacked channel-wise to be fed into the visual encoder of the model. These masked visual observations help focus the model learn dense feature matching in the overlapping regions. We condition the pose decoder MLP by concatenating the raw pose values of the action prior (\u2206xap, \u25b3yap, \u25b3\u03b8ap) and coarse pose prior (\u0414\u0425 \u0441\u0440, \u0414\u0443\u0441\u0440, 0cp) to the output of the first FC layer. Note, we do not use the action embedding as mentioned in [19]. The regressed fine pose of the NFPR module is considered as the output of the VO method $T^{pred}_{t-1}$."}, {"title": "Navigation Policy", "content": "The relative poses from $T^{pred}_0$ to $T^{pred}_{t-1}$ predicted by the VO method till timestep t is integrated to estimate the current agent pose which is further used to update the goal coordinates at the previous timestep $g_{t-1}$ to the current time-step $g_t$. This along-with the current observations Ot is fed to a Reinforcement Learning (RL) based navigation policy to determine the next primitive action (forward, left, right, stop) for the agent to take towards the goal location. We adopt the same navigation policy as used by [19, 28], consisting of a half-width ResNet-50 visual encoder and a 2-layer LSTM. Only depth images are given as input, discarding RGB (according to common practise). We use the pre-trained model weights as provided by [19] and apply the same pre-processing to the observations as mentioned in the above work. For details, please refer to [19]."}, {"title": "Experiments", "content": "We use the Habitat Simulator [23], Gibson Scene dataset [29] and the PointNav v2 task dataset from Habitat-Lab \u00b9 for training and evaluation of the overall pointgoal navigation task. For training the Visual Odometry model (NFPR), we generate a static dataset D = (Ot\u22121, Ot, $T_{ap}$, $T_{cp}$, $T_{gt}$) using an oracle agent (has access to ground-truth map) and shortest path follower policy to unroll the trajectories from which the RGB-D pairs (Ot\u22121, Ot), the action prior $T_{ap}$ and the ground-truth relative pose $T_{gt}$ are uniformly sampled and stored. During this dataset generation phase, the GCPE module is used to compute the coarse motion prior $T_{cp}$ and cache it as part of the dataset sample tuple to be used directly during the training phase. This reduces training time by not having to compute the motion prior for the same observations repeatedly in each epoch. The training dataset (50k to 400k) has been collected by uniformly sampling 20% of the observations from the oracle trajectories of the PointNav v2 task episodes in the 72 scenes of the Gibson 4+ training split. The validation dataset (10k) has been collected by uniformly sampling 75% of the observations from the oracle trajectories of the PointNav v2 task episodes in the 14 scenes of the Gibson 4+ validation split."}, {"title": "Training Details", "content": "The Neural Fine Pose Regression (NFPR) model is trained on the 400k dataset for 50 epochs with batch size of 128, using Adam optimizer with a learning rate"}, {"title": "Evaluation Details", "content": "To evaluate the performance of the VO module (GCPE and NFPR) in the con-text of pointgoal navigation, we use the pipeline as shown in Fig. 2 and test it against the validation split of the Gibson PointNav v2 task which consists of 994 episodes. For the RL based navigation policy, we use the pretrained model checkpoints as provided by [19].\nThe agent is evaluated primarily using 4 different navigation metrics :-\n1. Success (Si): A binary metric which considers an episode to be successful (S\u2081 = 1) if the agent stops within 0.36m (2 x agent-radius) of the point-goal, otherwise not (Si = 0).\n2. Success weighted by Path Length (SPL) [1]:-\nSPL = $\\frac{1}{N} \\sum_{i=1}^N S_i \\frac{l_i}{max(p_i, l_i)}$\nwhere, Si is the binary indicator of success, pi is the agent's path length, and li is the shortest path length (geodesic distance) for each episode i. And N is the total number of episodes.\n3. Soft-SPL [8]: This metric replaces binary success with the progress towards goal.\nSoftSPL = $\\frac{1}{N} \\sum_{i=1}^N (1 - \\frac{d_f}{d_o}) (\\frac{l_i}{max(p_i, l_i)})$\nwhere, do, is the initial distance to goal and dy is the distance to goal at the end of the episode (on both successes and failures) for each episode i.\n4. Distance To Goal $d_g$ : It is the geodesic distance to the goal when the agent issued the \"stop\" command.\nTo understand specifically the VO model's performance, we have also in-cluded 3 pose metrics:-\n1. Mean Relative Pose Error (RPE) - Rotation [24] : This metric computes the rotation error per frame-pair.\nRPE (Rot.) = $\\frac{1}{N} \\sum_{i=1}^N \\frac{1}{M} \\sum_{j=1}^M cos^{-1} ( \\frac{Trace((R^{pred}_{gt})^{-1}R^{gt}) - 1}{2} )$\nwhere, $R^{pred}_{gt}$ and $R^{gt}$ are the rotational components of the final predicted relative pose $T^{pred}$ and the ground-truth relative pose $T^{gt}$ between the frame-pair at time-step j of episode i. And M is the total number of timesteps per episode.\n2. Mean Relative Pose Error (RPE) - Translation [24]: This metric computes the translation error per frame-pair.\nRPE (Translation) = $\\frac{1}{N} \\sum_{i=1}^N \\frac{1}{M} \\sum_{j=1}^M ||t^{pred}_{gt} - t^{gt}||_2$\nwhere, $t^{pred}_{gt}$ and $t^{gt}$ are the translational components of the final predicted relative pose $T^{pred}$ and the ground-truth relative pose $T^{gt}$ between the frame-pair at time-step j of episode i.\n3. Mean Absolute Trajectory Error (ATE) - Translation [24]: This metric helps to understand the overall drift of the trajectory due to the accumulation of pose errors at each time-step.\nATE (Translation) = $\\frac{1}{N} \\sum_{i=1}^N \\frac{1}{M} \\sum_{j=1}^M ||tabs^{pred}_{gt} - tabs^{gt}||_2$\nwhere, $tabs^{pred}_{gt}$ and $tabs^{gt}$ are the translational components of the abso-lute pose of the predicted trajectory $Tabs^{pred}_{gt}$ and the absolute pose of the ground-truth trajectory $Tabs^{gt}$ at time-step j of episode i. These abso-lute poses are computed by integrating frame-wise relative poses from 0-th timestep to the j-th timestep of the agent's trajectory.\n$Tabs^j = Tabs^{j-1} \\cdot T^{rel j-1}$"}, {"title": "Results", "content": "We evaluate and compare the results of our pipeline (GCPE + NFPR) with the state-of-the-art (SoA) VO model from [19]. We also compare GCPE module's performance with different geometric pose estimator baselines created using SoA sub-modules. In all the experiments, we have used the same RL-based navigation policy from [19] to study the impact of various visual odometry methods on pointgoal navigation."}, {"title": "GCPE results and baseline comparisons:-", "content": "The Geometric Coarse Pose Estimator is integrated with the Navigation Policy and evaluated on the valida-tion split of the Gibson PointNav v2 task. The hyperparameters of the GCPE module were determined empirically using data from 1 out of the 14 scenes in the Gibson validation split. The standard deviations xo, Yo and \u03b8\u03c3 used by the pose-sampler function were set to 0.06m, 0.06m and 4.0deg."}, {"title": "Neural Fine Pose Regression (NFPR) Ablations", "content": "To gain insights about which modifications contribute to our model's superiority, we ablate the three major modifications in various combinations and evaluate agent's performance against the Gibson PointNav v2 validation split. We also perform the ablations at varying dataset sizes (50k, 200k and 400k) to confirm whether the benefits remain consistent or not."}, {"title": "Effect of Coarse Pose Prior (CP)", "content": "As evident from Tab. 4, the inclusion of CP benefits the model a lot as compared to only action-prior (AP) and also with overlap masks (APOM), hence the need for GCPE or any other coarse pose estimator is established. This is due to the fact that CP is much more accurate than AP, so it helps the model learns the residual \u25b3pose needed to make it close enough to the ground-truth (GT) pose, thereby converging in the right direction and decreasing the training loss."}, {"title": "Effect of Action Prior (AP)", "content": "Using only coarse motion-priors (CP) from a visual pose estimation module (like GCPE) to train the NFPR model may lead to loss plateauing upon prolonged training. This can be attributed to the fact that CP being a stochastic motion prior can hinder the training loss converge to the global minimum. We found out that simply feeding the action prior along-with the coarse motion prior to the pose decoder block provides a deterministic biasing effect to the model optimization. This helps the model learn better for a longer time on larger datasets. It can be noticed that CP alone performs better than AP+CP on 50k dataset but this gains diminish and the above problems arise when increasing dataset size. Note, we don't use action embedding as in [19], instead prefer to use action priors (AP) directly as we found the later to be more effective in most cases (check row-1 of Tab. 4 vs row-1 (Partsey. etal which uses action embedding) of Tab. 3)."}, {"title": "Effect of Action Prior based Overlap Masks (APOM)", "content": "In case of the usual unmasked RGB-D inputs, the model needs to learn and figure out the overlapping region between the two views in order to predict an accurate relative pose. This is a hard problem for simple CNNs and is usually approached by using attention mechanism of transformers. In order to keep a lightweight CNN-based model and yet help the model focus or attend to the overlapping region of the two views, we explicitly mask the RGB-D inputs by re-projecting one view to another and vice-versa using the action prior pose. This hard-attention helps the model perform better as evident from Tab. 4. Note, we could have also used coarse motion prior (CP) to generate the overlap masks but we observed from our experiments that APOM performs better than CPOM due to the highly stochastic nature of CP which hinders the model from learning consistent patterns."}, {"title": "Conclusion", "content": "In this work, we propose MPVO, a robust and sample-efficient VO pipeline for use in pointgoal navigation of embodied agents. It's based on the effective utiliza-tion of motion priors available during agent navigation. We have demonstrated it using action priors from the agent planner, but other motion priors like wheel odometry, IMU, etc can also be utilized to improve its efficacy further. We have conducted extensive experiments to show that MPVO performs better and is more sample-efficient (upto 2x) than SoA. Our training-free geometric coarse pose estimator (GCPE) also performs better than SoA baselines in standalone evaluation. Though we have shown our VO pipeline's usage in the context of pointgoal navigation, this can be used in other embodied navigation tasks such as ObjectNav, Rearrangement, etc. We hope that this work motivates further research in utilizing motion priors to improve VO estimates for navigation tasks."}]}