{"title": "Reinforcement Learning for Sustainable Energy: A Survey", "authors": ["Koen Ponse", "Felix Kleuker", "M\u00e1rton Fej\u00e9r", "\u00c1lvaro Serra-G\u00f3mez", "Aske Plaat", "Thomas Moerland"], "abstract": "The transition to sustainable energy is a key challenge of\nour time, requiring modifications in the entire pipeline of\nenergy production, storage, transmission, and consumption.\nAt every stage, new sequential decision-making challenges\nemerge, ranging from the operation of wind farms to the\nmanagement of electrical grids or the scheduling of electric\nvehicle charging stations. All such problems are well suited\nfor reinforcement learning, the branch of machine learning\nthat learns behavior from data. Therefore, numerous stud-\nies have explored the use of reinforcement learning for sus-\ntainable energy. This paper surveys this literature with the\nintention of bridging both the underlying research communi-\nties: energy and machine learning. After a brief introduction\nof both fields, we systematically list relevant sustainabil-\nity challenges, how they can be modeled as a reinforcement\nlearning problem, and what solution approaches currently\nexist in the literature. Afterwards, we zoom out and iden-\ntify overarching reinforcement learning themes that appear\nthroughout sustainability, such as multi-agent, offline, and\nsafe reinforcement learning. Lastly, we also cover standard-\nization of environments, which will be crucial for connecting\nboth research fields, and highlight potential directions for\nfuture work. In summary, this survey provides an extensive\noverview of reinforcement learning methods for sustainable\nenergy, which may play a vital role in the energy transition.", "sections": [{"title": "Introduction", "content": "Driven by population growth and higher per capita power\nuse, an already rising global power demand is expected to in-\ncrease further in the coming years. According to the Statis-\ntical Review of World Energy 2023 [1], currently more than\n70% of primary energy\u00b9 is derived from fossil fuels. The\ncurrent share of fossil fuels in the existing energy landscape\nneeds to be replaced by sustainable counterparts to mitigate\nenvironmental effects, such as global warming [2], and to be\nless dependent on finite world resources. More generally,\nwe need to meet the (energy) needs of the present without\ncompromising the ability of future generations to meet their\nown needs [3] a common way to define sustainable en-\nergy. In other words, we must replace non-sustainable pri-\nmary energy sources with suitable sustainable counterparts,\nas depicted in Figure 1."}, {"title": "Related Work", "content": "Various articles have surveyed the use of machine learning\nfor sustainable energy [5-11]. In general, these surveys iden-\ntify much potential for machine learning methods in the\nsustainable energy transition, on a wide range of applica-\ntions. However, these overviews primarily focus on super-\nvised learning techniques, where we try to forecast certain\nproperties, such as climate predictions. Although some sur-\nveys include reinforcement learning and control methods [5],\nthis is generally not the main focus.\nSeveral surveys do specifically cover reinforcement learn-\ning methods for sustainable energy [4, 12, 13], but these\ntypically zoom in on a specific subfield of the entire sus-\ntainability pipeline, such as the electricity grid [4, 12] or\ndemand response [13]. In contrast, the present survey cov-\ners all steps of sustainability, from production (e.g., solar\npanels, wind farms) to storage (e.g., hydrogen), transport\n(e.g., electricity grids) and consumption (e.g., smart build-\nings, electrical vehicles). The present paper thereby provides\nan integrated view of the entire sustainability chain, whose\nindividual challenges are often closely intertwined.\nThere are two additional motivations for our survey. First\nof all, the developments in machine learning for sustainable\nenergy move incredibly fast. A search using the Arxiv API\nwith keywords \"Reinforcement Learning\" and \"Sustainable\nEnergy\" reveals 1798 papers, of which 1486 (83%) have been\npublished in or after 2020. The field has therefore moved in-\ncredibly fast, and previous surveys of reinforcement learning\nand sustainable energy [4, 12, 13] have not covered this large\npart of the literature.\nMoreover, we also observe that previous overview pa-\npers predominantly originate from the 'energy literature'\n[9, 10, 12, 13]. Machine learning and sustainable energy\nare of course two separate research fields, and the bridge\nbetween two communities often takes effort from both sides.\nIn general, we observe energy researchers have started ex-\nploring machine learning techniques for their problems, but\npure machine learning researchers have more trouble enter-\ning the field, probably because they lack clear benchmarks\nand problem definitions (see Section 7 as well). An addi-\ntional goal of this survey is therefore to provide a bridge from\nthe machine learning perspective, also terminology-wise\nin hope of finding common ground."}, {"title": "Areas of Sustainable Energy", "content": "Energy-related processes often follow the same pattern: en-\nergy is produced, stored, transported, and finally consumed.\nThis segmentation of energy provides a natural taxonomy,\nas shown in Figure 2. All four areas of this taxonomy pro-\nvide distinct challenges and opportunities for sustainability\nimprovements through reinforcement learning. The remain-\nder of this section introduces the main set of challenges that\nappear in each of the four areas. Note that although we\nattempt to separate problems into their respective areas for\nclarity, real-world problems may deal with multiple parts of\nthe energy chain and need to be optimized together."}, {"title": "Generation", "content": "To ensure that our society is powered by sustainable energy,\nit is important that the energy produced comes from renew-\nable sources. These sources include (1) Hydropower, which\nproduces electricity primarily by converting the potential\nenergy of water, stored in reservoirs, through dam infras-\ntructure. (2) Solar power, producing electricity from solar\nradiation. (3) Wind power, capturing kinetic energy to con-\nvert into electricity. More niche areas include (4) Tidal and\n(5) Geothermal power. Furthermore, because of its closed\ncarbon cycle, (6) energy from biomass is also considered re-\nnewable. Lastly, (7) nuclear fusion would feature enough\ncharacteristics of renewable energy, due to the plentiful sup-\nply of fuel (hydrogen) and the absence of harmful long-term\nwaste products an attribute that distinguishes it from\nnuclear fission, which we exclude from our study due to\nthese inherent byproducts. Similarly, we exclude any en-\ndeavors solely aimed at optimizing fossil power plants from\nthis study."}, {"title": "Storage", "content": "Energy is often produced non-locally, requiring us to trans-\nport it in time (storage) and space (transmission) to reach\nthe consumer at the right time and location. Storing elec-\ntricity requires us to convert it into a different form of po-\ntential energy. For example by pumping water into lakes\nof hydro power facilities. Electricity can also be stored in\nchemical (inner) energy, like batteries or hydrogen. Often,\ngeographical factors such as natural height and access to\nwater heavily influence the choice of energy storage. This\nsurvey includes EV batteries and high capacity storage, but\nexcludes small-scale batteries because the immediate impact\non sustainable energy is not known."}, {"title": "Transmission", "content": "Sustainable energy is primarily transported over electricity\ngrids, which move electrical energy from producers to con-"}, {"title": "Consumption", "content": "Historically, we have mostly balanced energy supply and de-\nmand through supply (aided by storage solutions). However,\nthe rise in global electricity useage and ongoing electrifica-\ntion of society drive us to seek for more effective methods\nof managing and reducing energy demand. With recent in-\ncreases in energy prices, such optimizations are likely to be-\ncome a growing field of interest.\nTypical consumers of energy include houses and offices,\nwhere the cost of energy is primarily driven by heating and\ncooling installations. Different challenges are found in what\nwe categorize as mobile consumers, encompassing mainly\nelectric vehicles (EV's) and their required charging stations.\nFinally, our last category of energy consumers is the indus-\ntry sector. Here, reinforcement learning sustainable energy\nmethods may be designed to address highly specific needs,"}, {"title": "Reinforcement Learning Basics", "content": "In many of the domains that are discussed in the previous\nsection, we are faced with optimization problems such as\nmaximizing energy generation, minimizing power usage, or\noptimizing power allocation. These optimization tasks of-\nten require us to decide on multiple actions to obtain good\naverage performance over a long time horizon. These prob-\nlems are known as sequential decision-making problems, for\nwhich we may employ reinforcement learning. Reinforce-\nment learning is a machine learning approach for finding an\noptimal policy by interacting with an environment. It is\noften used for sequential decision-making problems, where\nactions from the past influence states into the future.\nInformally, reinforcement learning problems consist of an\nagent and an environment. The environment is in a state,\nand after an agent chooses an action, the environment fol-\nlows a transition function to determine both the new state\nand a reward, a numerical value indicating how \"good\" the\nnew state is. The goal of the agent is to find a so-called pol-\nicy of optimal actions for each state, thereby solving the re-\ninforcement learning problem by sampling the environment\nwith its actions.\nMore formally, the sequential decision-making problem\ncan mathematically be defined as a Markov decision pro-\ncess (MDP) [21, 22], defined as a tuple (S, A, p, r, po, Y).\nThe state space S is the set of all states, the action space\nA is the set of all possible actions, p is the transition dy-\nnamics distribution p : S \u00d7 A \u2192 \u25b3(S), (s, a) \u2192 p(s'|s,a),\nr is the reward function that maps transitions into rewards\nr : S \u00d7 A \u00d7 S \u2192 R, po \u2208 \u2206(S) is the initial state distribu-\ntion and y \u2208 (0, 1) is the discount factor, which governs the\nimportance of future rewards. In an MDP, the transition\ndynamics distribution is Markovian, meaning that the tran-\nsition to a next state, given an action only depends on the\ncurrent state, i.e. p(st+1|St,at,...,ao, So) = p(St+1|st, at).\nTo provide some intuition, we briefly present some exam-\nples on how to formulate energy problems as an MDP in\nTable 1.\nTo select actions in any given state, a policy is used, that\nis, a mapping \u03c0 : S \u2192 \u2206(A), s \u2194 \u03c0(\u03b1|s). Alternating se-\nquences of states and actions are usually denoted as trajec-\ntories T = (so,A0, S1,...,ST), and each policy induces a dis-\ntribution \u03c1\u03c0 (\u03c4) over such trajectories in an MDP. For a given\ntrajectory, the return Gt is defined as the total discounted\nrewards from time-step t (resp. state s = st) onwards\nGt = \u2211yk-tr(Sk, ak, Sk+1).\nk=t\nValue functions for a given state or a state-action pair are\ndefined as the expected return given a fixed state or a state-\naction pair, respectively:\nV(8) = \u0395\u03c0[Gt|st = 8]\nQ\u03c0(\u03c2, \u03b1) = \u0395\u03c0[Gt|St = s, at a].\nSolving an MDP is defined as finding a policy \u03c0* such that\nit maximizes the expected return of trajectories:\n\u03c0* \u2208 arg max Er\u223c\u03c1\u03c0 [V\u03c0(80)].\n\u03c0\nFinding an optimal policy is usually done in an iterative\nfashion: Knowledge about the values Q under a given pol-\nicy can be used to improve the policy, for example, by acting\ngreedily w.r.t. the current values, naturally being denoted\nas a policy improvement step. This, in turn, requires a new\nevaluation of the values under this new policy, a step usually\ncalled policy evaluation. Generally, this scheme of alternat-\ning between policy evaluation and policy improvement is\ncalled Generalised Policy Iteration (GPI).\nThe policy evaluation step can be done by utilizing the so-\ncalled Bellman-equations [23], looping over all state-action\npairs and updating the value functions; a method known as\nDynamic Programming [22]. However, solving an MDP with\nDynamic Programming requires access to a known model of\nthe environment, That is, pandr must be known, which,\nfor most real-world applications, is not the case.\nIn reinforcement learning, we therefore assume a setting\nin which the transition dynamics p and reward function r are\nunknown. Additionally, instead of looping over each state-\naction pair, in reinforcement learning, we allow an agent to\ncollect experiences, i.e. trajectories, by interacting with an\nenvironment following a policy \u03c0. These experiences can\nthen be used to learn value functions V or Q, in what\nis known as value-based methods, or directly learn an ex-\nplicit policy \u03c0, in what is known as policy-based methods.\nSome methods use the obtained experiences to learn both\nthe values and an explicit policy. These methods belong to\nthe class of actor-critic [24] methods and are particularly\npopular in some of the current state-of-the-art algorithms\n[25-30].\nWell-known value-based methods, such as Q-learning [31]\nand SARSA [32], store their learned value functions in a ta-\nble, in computer memory. As a table entry is required for"}, {"title": "Applications of Reinforcement Learning in Sustainable Energy", "content": "This section focuses on the use of reinforcement learning, as\nintroduced in Section 4, in the different sustainable energy\nareas that are introduced in Section 3. We will go over\neach area of our taxonomy, discuss sub-areas, and highlight\nproblems that are currently being worked on together with\nhow researchers have so far addressed these problems.\nFirst, Section 5.1 covers generation: predominantly hy-\ndro, solar and wind, and also smaller fields such as tidal,\nbiomass, fusion and geothermal. Section 5.2 dives into rein-\nforcement learning approaches in energy storage solutions,\ndiscussing batteries, hydrogen and pumped hydro storage.\nNext, Section 5.3 discusses ways optimize energy consump-\ntion and help balance the grid from the energy demand side.\nBuildings, electric vehicles and the industry sector will be\nfeatured here. Finally, Section 5.4 discusses energy grids,\nwhich play a crucial role in connecting all previous com-\nponents (Figure 2). Although all components could be op-\ntimized/learned together, grid literature typically assumes\nthe other components have some static controller."}, {"title": "Generation", "content": "A natural angle to approach sustainable energy is to improve\nthe efficiency of inherently sustainable sources, mentioned in\nSection 3. Reinforcement learning can help in optimizing the\ncontrol and operation of such energy generation facilities,\nthereby boosting the efficiency. This, in turn, would allow\nto increase the share of these sources and consequently shift\nthe primary energy landscape in the desired direction."}, {"title": "Hydropower", "content": "Hydropower accounts for 16% of the globally produced elec-\ntricity (not to be confused with energy), contributing the\nlargest share of all renewable energy sources [1, 54]. Al-\nthough hydropower is an excellent source of sustainable\nenergy, geological requirements make it comparatively less\nscalable compared to other renewable sources, such as wind\nand solar energy. Economically viable hydropower poten-\ntial in some areas has now largely been exploited [54, 55],\nand the further enhancement would require substantial in-\nvestment. Kleiven et al. [55] propose an investment model\nusing reinforcement learning to determine an optimal up-\ngrade capacity along with its optimal point in time; this\ndecision is based on projected electricity prices and water\ninflows, modeled as a Markov Decision Process.\nOther models aim to improve the economic viability and\nefficiency of hydropower plants. Xu et al. [56] propose a\ndeep Q network method, where the water level and inflow\nrepresent the state space. The objective is to maximize the\ntotal generated energy by adjusting the release of water.\nAnother work has investigated maximizing total generated\nelectricity in a multi-reservoir (yet single-agent) setting [57].\nA similar approach, focused on optimizing total revenue\nearned is also proposed [58]. Here, the generated energy\nis multiplied by a forecasted electricity price, introducing\nan additional element of uncertainty."}, {"title": "Wind Power", "content": "With a 7% share of global electricity production, wind power\naccounts for the second highest production of renewable\nsources [1]. Due to variability in wind speed (and angle), a\nprimary challenge in the optimization of wind power plants\nlies in accurately forecasting wind speed (shifts in direction\ncan usually be adjusted for in real time). Recently, rein-\nforcement learning has been proposed for these prediction\ntasks [59, 60], as apposed to supervised learning that has\nbeen the standard. When coupled with a battery system to\ncompensate for periods of low wind, reinforcement learning\ncan find policies to maintain a stable power supply based on\nthese wind predictions [61].\nNote that it is important not only to adapt to future wind\nspeed predictions, but also to optimize power output and\nbattery charging load in real-time, responding to current\nwind conditions. This is sometimes referred to as Maxi-\nmum Power-Point Tracking, presenting the optimal load to\nthe generator depending on wind conditions [62]. Reinforce-\nment learning has demonstrated promising results in find-\ning optimal policies under variable wind conditions [62, 63].\nFurthermore, the application of reinforcement learning al-\nlows the scaling up of optimization parameters without sig-\nnificantly increasing the inference time, enabling innovative\ndesign proposals for wind turbines [64].\nLastly, note that, as wind farms grow in size, the com-\nplexity of the optimization challenge also intensifies. The\noptimal operational point for this group of turbines is usu-\nally not a linear aggregate of the optima of the individual\nturbine due to interaction effects, such as one turbine being\nin the wind shadow of another turbine. To identify the opti-\nmal operational point for the entire collection under specific\nwind conditions, multi-agent reinforcement learning meth-\nods may offer more optimal solutions [65]."}, {"title": "Solar Power", "content": "Electricity generated from solar power constitutes approxi-\nmately 4.5% of global electricity production, positioning it\nas the third most prolific renewable source [1, 54]. How-\never, solar power might be the best candidate for scaling up\nbecause of the abundance of untapped potential to deploy\nphotovoltaic (PV) cells.\nA widely discussed subject in the realm of reinforcement\nlearning for solar power is again the so-called Maximum\nPower-Point Tracking, where the aim is to maximize the\npower produced of a set of PV cells under non-optimal con-\nditions, such as shading [66\u201373]. Photovoltaic systems have\na unique global optimum under ideal conditions, and multi-"}, {"title": "Tidal Power", "content": "In terms of installed capacity, tidal energy is less significant\ncompared to solar, wind and hydro. However, we have seen a\nsignificant amount of reinforcement learning research in this\ntechnology. Three primary technologies are used to gener-\nate energy from tidal flows. (1) Tidal turbines are similar\nto their wind-driven counterparts and capture the kinetic\nenergy inherent in tidal flows [79]. (2) Wave energy con-\nverters leverage wave motions, resisted by a power take-off,\nto convert kinetic energy into electricity [80]. A more spe-\ncialized approach involves (3) Tidal Range Structures, which\ngenerate power by artificially inducing a difference in water\nlevel between the ocean and a confined area. Turbines then\ngenerate energy by allowing water to balance out this dis-\ncrepancy [81]. As in hydropower, the installation of tidal\npower is limited by geographical constraints (coastal areas).\nWave energy converters consist of a small floating body\nsubjected to wave forces, with its movements countered by\nan electric or hydraulic power take-off system. The control-\nlable damping coefficient that influences the resistance of"}, {"title": "Biomass", "content": "Lastly, the three remaining energy sources\nbiomass,\ngeothermal energy, and nuclear fusion identified as in-\nherently sustainable, are not extensively explored in the re-\ninforcement learning literature. However, we believe that\nreinforcement learning has significant potential to impact\ncontrol applications within these domains.\nBiomass processes transform biological matter from\nplants and animals into carbon-based energy carriers, such\nas ethanol or biogas (methane). Biomass processes are\nalso denoted as Waste-to-Energy (WtE). Different conver-\nsion methods have been studied, such as thermochemical,\nphysicochemical, and biochemical processes [86, 87]. The\npotential application of reinforcement learning in control-\nling these processes is presented by Faridi et al. [88]. In this\nwork, the authors introduce a model-based deep reinforce-\nment learning approach for finding an optimal controller for\na thermochemical gasification process of biomass. Another\nstudy [89] focuses on improving the efficiency of a recovery\nboiler, a device designed to convert a byproduct of the pa-\nper industry into synthetic fuel. The authors used a tabular\nreinforcement learning approach with the specific objective\nof reducing the heat transfer rate."}, {"title": "Geothermal Energy systems", "content": "Geothermal Energy systems use internal heat of the\nEarth to generate energy, commonly by using naturally ex-\nisting high-pressure water or steam reservoirs. Although\ngeothermal energy systems are only available in areas with\nsuitable seismic activity, there are various fields around the\nworld [90]. Because of its advantage of offering a more re-\nliable production in comparison to solar and wind power,"}, {"title": "Nuclear Fusion", "content": "Nuclear fusion is a promising clean energy source. Much\nof the research effort of harnessing nuclear fusion is aimed at\ncontrolling the very high temperature of the fusion plasma,\noften in tokamaks, a type of experimental fusion reactor. Re-\nsearchers are exploring automated control methods, predict-\ning and mitigating disturbances in magnetic fields, ensuring\nstable and efficient plasma operations under high-pressure\nconditions [94-100]. Deep reinforcement learning techniques\noptimize various parameters and control schemes in toka-\nmak plasmas, intended to improve the shape, duration, and\ntemperature of plasma conditions [94, 96, 97]."}, {"title": "Storage", "content": "While fossil fuels naturally and efficiently store energy in\nchemical form, allowing flexible matching of supply with de-\nmand, renewable energy sources do not. Consequently, we\nmust improve solutions to store the energy generated from\nrenewable sources in order to match supply and demand."}, {"title": "Pumped Hydro Energy Storage", "content": "An additional characteristic of some hydropower plants is\nthe ability to operate as energy storage repositories. These\nare commonly called pumped hydro energy storage (PHES)\nsystems and work by letting water flow down to generate\nelectricity; and pumping water back up into a reservoir to\nstore potential energy. The installed capacity of PHES is\napproximately 10% of the aggregate installed capacity ded-\nicated to hydropower [54]. Toufani et al. present a Markov\nDecision Process formulation with a focus on maximizing\ncashflows in converting hydropower facilities into PHES sys-\ntems [101, 102]. A related MDP formulation forms the\nbackbone of the research by Tubeuf et al. [103], wherein\nthey study a PHES system with specific attention to safety\nconsiderations. The authors develop a digital twin of the\nphysical turbine employed for pre-training the reinforcement\nlearning model.\nIn a parallel study [104], the focus is on replacing a\nconventional PID controller (Proportional, Integral, and"}, {"title": "Batteries", "content": "Batteries are perhaps the most widely recognized chemical\nstorage system. In this work, we exclude solid-state chemical\nstorage systems such as lithium-ion batteries on the basis of\nthe raw materials used in their production. We refer inter-\nested readers to a survey on reinforcement learning in bat-\ntery storage systems [105]. In contrast to solid-state chemi-\ncal storage systems, liquid-based chemical storage systems,\nspecifically redox-flow batteries, are considered to be well\nsuited to address large-scale energy storage challenges [106],\nin a sustainable way. Within this domain, Sowndarya et al.\n[107] introduce a reinforcement learning framework, based\non AlphaZero [108], to identify new stable candidates. Rein-\nforcement learning has also shown promise in the construc-\ntion of models to predict redox-flow battery state variables,\ncrucial for the widespread deployment of such systems [109]."}, {"title": "Hydrogen", "content": "Additionally, energy can also be stored in the form of\ngaseous chemicals, most notably hydrogen. Unlike in the\naforementioned systems, where conversion, storage, and re-\nconversion occur within the same system, hydrogen storage\nprocesses are typically distributed. Electrolysers, responsi-\nble for the electrochemical conversion of water into hydrogen\nand oxygen using electricity as the energy source\n2H2O2 H2 + O2,\nface a design challenge in identifying suitable catalysts. Al-\nthough we are not aware of reinforcement learning work\non hydrogen electrolyses, reinforcement learning has shown\npromise in catalysis, which may be transferable to hydrogen\nsystems [110, 111]. Other studies explore employing rein-\nforcement learning to reduce operational costs by optimiz-\ning maintenance schedules [112] or enhance operational effi-\nciency in dynamic magnet field-assisted electrolyzers [113].\nBy the laws of chemistry, inverting reaction (1) will release\n(electrical) energy; a process facilitated by devices known as\nfuel cells. Reinforcement learning-based approaches have\nbeen applied to improve the operational efficiency of solid\noxide fuel cells [114] and proton-exchange membrane fuel"}, {"title": "Consumption", "content": "When considering sustainable energy solutions, we com-\nmonly investigate energy production. However, improving\nenergy consumption practices can also contribute signifi-\ncantly to achieving our sustainability objectives, and again,\nmany control problems must be addressed. In the following\nsections, we consider three distinct categories of energy con-\nsumers. We first focus on buildings, where the key functions\nare heating, ventilation, and air conditioning systems. Sub-\nsequently, mobile consumption is discussed, which includes\nelectric vehicles and their charging infrastructure. Finally,\nwe discuss the industrial sector, characterized by specialized\nenergy requirements. Certain segments of the industry sec-\ntor still depend on non-sustainable energy sources for parts\nof their product chain. Consequently, modifications in these\nproduction chains are necessary in order to rely solely on\nsustainable primary energy sources in the future."}, {"title": "Houses & offices", "content": "Globally, buildings use an estimated 30% of final energy\n[118]. Due to this large power demand and an increasing\nnumber of smart home appliances entering people's homes,\nthere is a growing interest in optimizing energy-intensive\nsystems in houses & offices. In this area, reinforcement\nlearning has gained a lot of attention recently due to its\nadaptability and ability to learn without data from a reward\nsignal. Generally, we can subdivide the existing literature\ninto systems that optimize a single building, and those that\ninvestigate the capability of buildings for demand response\na topic we will also discuss in Section 5.4.\nA large portion of the existing literature attempts to op-\ntimize controllable elements in a single building. The goal\nhere is usually to minimize the energy cost while maintain-\ning a desired level of human comfort [119-133]. These works\nprimarily focus on optimizing HVAC systems (heating, ven-\ntilation, air conditioning), as they are the largest control-\nlable contributor to total energy consumption in buildings.\nHowever, other works extend the action space to a wider ar-\nray of appliances [120, 134], while sometimes also optimiz-\ning for additional objectives such as air quality [135-137]\nor luminescence [138]. All works in this field vary in their\nuse of algorithms, but almost all implement some form of\nmodel-free deep reinforcement learning, often a value-based\nDQN method [121-123, 134, 136, 139\u2013142]. Model-based\nsolutions are sparse [130-132], but Jeen et al. has shown\npotential for zero-shot model-based solutions [132], whereby\nthe agent quickly adapts to any new building without the\nneed for pre-training or a simulator, which is near-impossible\nto obtain for every building in the world.\nMany works employ an online learning approach-based\non a simulated environment, not on data for which there\nexists a wide range of open source Gym-based [143] simula-\ntors [144-155], some still actively developed [144, 147, 148],\nthat often leverage building simulators such as Energy-\nPlus [156]. However, others attempt to learn through of-\nfline data [130, 139] or partially use real data in their\ntraining loop, such as building occupancy or weather data\n[122, 129, 134, 137].\nDemand response systems do not take the overall capacity\nof the network for granted and instead investigate whether\nbuildings can play an active role in its overall load. By\ntimely coordination of energy usage, the idea is that build-\nings can collectively flatten energy peaks throughout the\nday. It should be noted that in demand response solutions,\nthe energy consumption and comfort level of each individual\nbuilding are still considered an important part of the opti-\nmization. Again, different simulators exist [157, 158], simu-\nlating multiple buildings and their combined effect on a grid.\nOf particular interest in this field is CityLearn [158], an ac-\ntively developed Gym environment for demand response of"}, {"title": "Mobility", "content": "Electric vehicles (EV's) and their charging stations provide\nboth new challenges and opportunities. The intermittent\nload and (combined) large battery provide excellent stor-\nage and/or passive balancing opportunities. However, EV\narrival and departure times are beyond the control of the\ncharging station, which presents new challenges in charging\nand discharging batteries at the right time. In this section,\nwe discuss problems in the control of charging stations, EV\nbattery optimization (sometimes referred to as powertrain\ncontrol), and navigating EV's in a city whenever they need\nto charge.\nIn the mobility domain, most interest has been on deep\nreinforcement learning methods, disregarding tabular ap-\nproaches. Primary objectives are to meet the energy de-\nmands of electric vehicles within a desired time, while max-\nimizing the profits of a charging station [162-168]. This is\nusually done in a single-agent fashion, where a single charg-\ning station is controlled or a single agent controls multiple\ncharging stations. However, some works study cooperative\nmulti-agent settings where more charging stations are con-\nsidered [164, 166]. In the mobility domain, action spaces\ntypically consist of the power output of the charging sta-\ntions, ranging from a simple total power output [162, 163],\nto a more detailed per-car output [164, 166]. More elaborate\nmethods focus on energy price prediction [166, 167, 169],\ndecide on the energy selling price [164, 168], include power-\ngenerating systems such as solar panels and wind [163], or\nexplicitly consider the load on the macro-grid [163]. It has\nalso been suggested that systems that include varying en-\nergy prices implicitly optimize to balance the grid [168]. Al-\nthough most of the works focus on commercial charging sta-\ntions, one can also optimize the charging station at home,\nusing the car as a battery and selling the electricity back to\nthe grid [169].\nIn addition to controlling charging stations, other works\nfocus on navigating electric vehicles more efficiently to a\ncharging station. Here, an aggregator system observes city\ntraffic, charging station data, and electric vehicle data and\ndispatches the most optimal routes to vehicles that need\ncharging [170, 171]. Alternatively, the reinforcement learn-"}, {"title": "Industry", "content": "Industrial consumption often has a distinct consumption\nprofile and often a high volume of energy consumption. For\ninstance, data centers are, like houses & offices, primar-\nily concerned with keeping a building within an acceptable\ntemperature while minimizing long-term energy cost. Here,\ncooling is again considered crucial to control as close to 50%\nof the total energy used in data centers is used for cooling\n[180", "181": "with Deepmind in-\nstalling model-based reinforcement learning systems in some\nof their centers [182"}]}