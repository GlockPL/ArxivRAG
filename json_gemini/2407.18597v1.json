{"title": "Reinforcement Learning for Sustainable Energy: A Survey", "authors": ["Koen Ponse", "Felix Kleuker", "M\u00e1rton Fej\u00e9r", "\u00c1lvaro Serra-G\u00f3mez", "Aske Plaat", "Thomas Moerland"], "abstract": "The transition to sustainable energy is a key challenge of our time, requiring modifications in the entire pipeline of energy production, storage, transmission, and consumption. At every stage, new sequential decision-making challenges emerge, ranging from the operation of wind farms to the management of electrical grids or the scheduling of electric vehicle charging stations. All such problems are well suited for reinforcement learning, the branch of machine learning that learns behavior from data. Therefore, numerous studies have explored the use of reinforcement learning for sustainable energy. This paper surveys this literature with the intention of bridging both the underlying research communities: energy and machine learning. After a brief introduction of both fields, we systematically list relevant sustainability challenges, how they can be modeled as a reinforcement learning problem, and what solution approaches currently exist in the literature. Afterwards, we zoom out and identify overarching reinforcement learning themes that appear throughout sustainability, such as multi-agent, offline, and safe reinforcement learning. Lastly, we also cover standardization of environments, which will be crucial for connecting both research fields, and highlight potential directions for future work. In summary, this survey provides an extensive overview of reinforcement learning methods for sustainable energy, which may play a vital role in the energy transition.", "sections": [{"title": "Introduction", "content": "Driven by population growth and higher per capita power use, an already rising global power demand is expected to increase further in the coming years. According to the Statistical Review of World Energy 2023 [1], currently more than 70% of primary energy\u00b9 is derived from fossil fuels. The current share of fossil fuels in the existing energy landscape needs to be replaced by sustainable counterparts to mitigate environmental effects, such as global warming [2], and to be less dependent on finite world resources. More generally, we need to meet the (energy) needs of the present without compromising the ability of future generations to meet their own needs [3] a common way to define sustainable energy. In other words, we must replace non-sustainable primary energy sources with suitable sustainable counterparts, as depicted in Figure 1.\nThe backbone of scaling up the total energy supply in a sustainable landscape are renewable energy sources, such as wind or solar. However, in contrast to fossil power plants, these sources do not provide the energy on demand, which poses a variety of optimization challenges. While traditionally we were to fit the supply according to demand, with renewable energy sources, the supply needs to be allocated in an optimal way to guarantee the best usage and stability of electrical grids. Components within a grid are faced with new optimization challenges as well. For example, storage systems such as batteries could help balance the load on the grid. Another big avenue is the electrification of the transport sector, further intensifying the challenge of optimal energy distribution, e.g. in the form of EV Charging. The shift away from fossil fuels also requires a tremendous scale-up of installed capacity of renewable energy sources, rendering optimal installation and operation of these even more important.\nReinforcement learning, a major branch of Machine Learning, aims to solve sequential decision tasks and is well suited to address many of the optimization and control challenges in the field of sustainable energy. Machine learning in general would help in many regards in mitigating test time costs and recognizing patterns in large amounts of data. Reinforcement learning in particular can aid in finding optimal strategies, usually called policies, just by interacting with an environment and receiving partial feedback. In contrast to supervised learning, reinforcement learning does not require control actions to be individually labeled, but instead learns from the outcomes of its actions through trail-and-error. This in turn allows it to potentially outperform human solutions. As such, reinforcement learning is used in the field of sustainable energy with the hope of significantly enhancing the efficiency and reliability of these systems [4].\nThis survey aims to connect the machine learning (reinforcement learning) community with the sustainable energy community. As such, we will be approaching the field from both angles in different sections. For researchers coming from the machine learning domain, we group challenges in the sustainable energy field in an easy-to-understand taxonomy and point to research that has been undertaken to address these challenges. Researchers from the energy domain are presented with a general introduction into reinforcement learning, pointing to possibly relevant literature. Additionally, we group the currently existing literature in reinforcement learning challenges that need to be addressed in the various energy problems.\nWe find that the field is still relatively young, and major reinforcement learning literature has largely not yet found its way into applied research. Notable topics that would require more work, in order to get to real-world deployment, are safe reinforcement learning and offline reinforcement learning. Furthermore, we observe a very wide and diverse number of benchmarks (environments) in the research field, while we should strive to more standardization. This standardization may prove to be the most important bridge for the two research fields to come together. When this happens, we see a lot of potential for reinforcement learning in the energy field.\nNote that the transition to sustainable energy involves both technical and economic challenges. For this survey, we focus on the former, discussing the technical aspects of generation, storage, transport, and consumption of sustainable energy. Of course, practical deployment will also require economic tools, such as sustainable energy trading, logistics, and scheduling. However, these challenges are not specific to the energy transition and therefore fall outside the scope of this survey.\nOur contributions are as follows: (1) We provide an overview of the whole energy chain and possible options for reinforcement learning optimization within this chain. We aim to do this in a way that is (2) specifically designed to connect the two research fields, the energy field and the machine-learning field, by repeatedly switching focus points. Lastly (3), we identify pitfalls, bottlenecks, and promising directions for future research.\nThe remainder of this survey is structured as follows. We start with an overview of related surveys on machine learning and sustainable energy, and how they compare to the present paper (Section 2). Then, Sections 3 and 4 provide a broad overview of both key topics: Section 3 introduces the sustainable energy landscape (intended for researchers from the reinforcement learning community), while Section 4 present an overview of reinforcement learning for energy researchers. Next, The core of the survey follows in Sections 5 and 6. The former, Section 5, surveys the full range of sustainable energy applications in which reinforcement learning methods have been applied. The structure here comes from the energy side, focusing on production, storage, transport and consumption of sustainable energy. Then, Section 6 again flips the view to the reinforcement learning perspective, discussing the overarching reinforcement learning themes we encounter along the full sustainable energy chain. Afterwards, 7 discusses benchmarking and performance metrics, a major topic in all of machine learning, and a crucial topic in the bridge between both fields. Lastly, Section 8 summarizes and discusses our findings, including recommendations and possible directions for future research."}, {"title": "Related Work", "content": "Various articles have surveyed the use of machine learning for sustainable energy [5-11]. In general, these surveys identify much potential for machine learning methods in the sustainable energy transition, on a wide range of applications. However, these overviews primarily focus on supervised learning techniques, where we try to forecast certain properties, such as climate predictions. Although some surveys include reinforcement learning and control methods [5], this is generally not the main focus.\nSeveral surveys do specifically cover reinforcement learning methods for sustainable energy [4, 12, 13], but these typically zoom in on a specific subfield of the entire sustainability pipeline, such as the electricity grid [4, 12] or demand response [13]. In contrast, the present survey covers all steps of sustainability, from production (e.g., solar panels, wind farms) to storage (e.g., hydrogen), transport (e.g., electricity grids) and consumption (e.g., smart buildings, electrical vehicles). The present paper thereby provides an integrated view of the entire sustainability chain, whose individual challenges are often closely intertwined.\nThere are two additional motivations for our survey. First of all, the developments in machine learning for sustainable energy move incredibly fast. A search using the Arxiv API with keywords \"Reinforcement Learning\" and \"Sustainable Energy\" reveals 1798 papers, of which 1486 (83%) have been published in or after 2020. The field has therefore moved incredibly fast, and previous surveys of reinforcement learning and sustainable energy [4, 12, 13] have not covered this large part of the literature.\nMoreover, we also observe that previous overview papers predominantly originate from the 'energy literature' [9, 10, 12, 13]. Machine learning and sustainable energy are of course two separate research fields, and the bridge between two communities often takes effort from both sides. In general, we observe energy researchers have started exploring machine learning techniques for their problems, but pure machine learning researchers have more trouble entering the field, probably because they lack clear benchmarks and problem definitions (see Section 7 as well). An additional goal of this survey is therefore to provide a bridge from the machine learning perspective, also terminology-wise in hope of finding common ground."}, {"title": "Areas of Sustainable Energy", "content": "Energy-related processes often follow the same pattern: energy is produced, stored, transported, and finally consumed. This segmentation of energy provides a natural taxonomy, as shown in Figure 2. All four areas of this taxonomy provide distinct challenges and opportunities for sustainability improvements through reinforcement learning. The remainder of this section introduces the main set of challenges that appear in each of the four areas. Note that although we attempt to separate problems into their respective areas for clarity, real-world problems may deal with multiple parts of the energy chain and need to be optimized together.\nTo ensure that our society is powered by sustainable energy, it is important that the energy produced comes from renewable sources. These sources include (1) Hydropower, which produces electricity primarily by converting the potential energy of water, stored in reservoirs, through dam infrastructure. (2) Solar power, producing electricity from solar radiation. (3) Wind power, capturing kinetic energy to convert into electricity. More niche areas include (4) Tidal and (5) Geothermal power. Furthermore, because of its closed carbon cycle, (6) energy from biomass is also considered renewable. Lastly, (7) nuclear fusion would feature enough characteristics of renewable energy, due to the plentiful supply of fuel (hydrogen) and the absence of harmful long-term waste products an attribute that distinguishes it from nuclear fission, which we exclude from our study due to these inherent byproducts. Similarly, we exclude any endeavors solely aimed at optimizing fossil power plants from this study.\nEnergy is often produced non-locally, requiring us to transport it in time (storage) and space (transmission) to reach the consumer at the right time and location. Storing electricity requires us to convert it into a different form of potential energy. For example by pumping water into lakes of hydro power facilities. Electricity can also be stored in chemical (inner) energy, like batteries or hydrogen. Often, geographical factors such as natural height and access to water heavily influence the choice of energy storage. This survey includes EV batteries and high capacity storage, but excludes small-scale batteries because the immediate impact on sustainable energy is not known.\nSustainable energy is primarily transported over electricity grids, which move electrical energy from producers to con-"}, {"title": "Reinforcement Learning Basics", "content": "In many of the domains that are discussed in the previous section, we are faced with optimization problems such as maximizing energy generation, minimizing power usage, or optimizing power allocation. These optimization tasks often require us to decide on multiple actions to obtain good average performance over a long time horizon. These problems are known as sequential decision-making problems, for which we may employ reinforcement learning. Reinforcement learning is a machine learning approach for finding an optimal policy by interacting with an environment. It is often used for sequential decision-making problems, where actions from the past influence states into the future.\nInformally, reinforcement learning problems consist of an agent and an environment. The environment is in a state, and after an agent chooses an action, the environment follows a transition function to determine both the new state and a reward, a numerical value indicating how \"good\" the new state is. The goal of the agent is to find a so-called policy of optimal actions for each state, thereby solving the reinforcement learning problem by sampling the environment with its actions.\nMore formally, the sequential decision-making problem can mathematically be defined as a Markov decision process (MDP) [21, 22], defined as a tuple $(S, A, p, r, p_0, \\gamma)$. The state space S is the set of all states, the action space A is the set of all possible actions, p is the transition dynamics distribution $p : S \\times A \\rightarrow \\triangle(S), (s, a) \\rightarrow p(s'|s,a)$, r is the reward function that maps transitions into rewards $r : S \\times A \\times S \\rightarrow \\mathbb{R}$, $p_0 \\in \\triangle(S)$ is the initial state distribution and $\\gamma \\in (0, 1)$ is the discount factor, which governs the importance of future rewards. In an MDP, the transition dynamics distribution is Markovian, meaning that the transition to a next state, given an action only depends on the current state, i.e. $p(s_{t+1}|s_t,a_t,...,a_0, s_0) = p(s_{t+1}|s_t, a_t)$. To provide some intuition, we briefly present some examples on how to formulate energy problems as an MDP in Table 1.\nTo select actions in any given state, a policy is used, that is, a mapping $\\pi : S \\rightarrow \\triangle(A), s \\mapsto \\pi(a|s)$. Alternating sequences of states and actions are usually denoted as trajectories $\\tau = (s_0, a_0, s_1, ..., s_T)$, and each policy induces a distribution $\\rho_{\\pi}(\\tau)$ over such trajectories in an MDP. For a given trajectory, the return $G_t$ is defined as the total discounted rewards from time-step t (resp. state $s = s_t$) onwards\n$G_t = \\sum_{k=t}^T \\gamma^{k-t}r(s_k, a_k, s_{k+1}).$\nValue functions for a given state or a state-action pair are defined as the expected return given a fixed state or a state-action pair, respectively:\n$V^{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|s_t = s]$\n$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t|s_t = s, a_t = a].$\nSolving an MDP is defined as finding a policy $\\pi^*$ such that it maximizes the expected return of trajectories:\n$\\pi^* \\in \\arg \\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\rho^{\\pi}}[V^{\\pi}(s_0)].$\nFinding an optimal policy is usually done in an iterative fashion: Knowledge about the values Q under a given policy can be used to improve the policy, for example, by acting greedily w.r.t. the current values, naturally being denoted as a policy improvement step. This, in turn, requires a new evaluation of the values under this new policy, a step usually called policy evaluation. Generally, this scheme of alternating between policy evaluation and policy improvement is called Generalised Policy Iteration (GPI).\nThe policy evaluation step can be done by utilizing the so-called Bellman-equations [23], looping over all state-action pairs and updating the value functions; a method known as Dynamic Programming [22]. However, solving an MDP with Dynamic Programming requires access to a known model of the environment, That is, p and r must be known, which, for most real-world applications, is not the case.\nIn reinforcement learning, we therefore assume a setting in which the transition dynamics p and reward function r are unknown. Additionally, instead of looping over each state-action pair, in reinforcement learning, we allow an agent to collect experiences, i.e. trajectories, by interacting with an environment following a policy $\\pi$. These experiences can then be used to learn value functions V or Q, in what is known as value-based methods, or directly learn an explicit policy $\\pi$, in what is known as policy-based methods. Some methods use the obtained experiences to learn both the values and an explicit policy. These methods belong to the class of actor-critic [24] methods and are particularly popular in some of the current state-of-the-art algorithms [25-30].\nWell-known value-based methods, such as Q-learning [31] and SARSA [32], store their learned value functions in a table, in computer memory. As a table entry is required for"}, {"title": "Applications of Reinforcement Learning in Sustainable Energy", "content": "This section focuses on the use of reinforcement learning, as introduced in Section 4, in the different sustainable energy areas that are introduced in Section 3. We will go over each area of our taxonomy, discuss sub-areas, and highlight problems that are currently being worked on together with how researchers have so far addressed these problems.\nFirst, Section 5.1 covers generation: predominantly hydro, solar and wind, and also smaller fields such as tidal, biomass, fusion and geothermal. Section 5.2 dives into reinforcement learning approaches in energy storage solutions, discussing batteries, hydrogen and pumped hydro storage. Next, Section 5.3 discusses ways optimize energy consumption and help balance the grid from the energy demand side. Buildings, electric vehicles and the industry sector will be featured here. Finally, Section 5.4 discusses energy grids, which play a crucial role in connecting all previous components (Figure 2). Although all components could be optimized/learned together, grid literature typically assumes the other components have some static controller.\nA natural angle to approach sustainable energy is to improve the efficiency of inherently sustainable sources, mentioned in Section 3. Reinforcement learning can help in optimizing the control and operation of such energy generation facilities, thereby boosting the efficiency. This, in turn, would allow to increase the share of these sources and consequently shift the primary energy landscape in the desired direction."}, {"title": "Reinforcement Learning Challenges", "content": "While the previous section summarized the literature from the sustainable energy point of view, we now revert our viewpoint and focus on the overarching reinforcement learning themes that appear throughout the literature. These central reinforcement learning topics include multi-agent RL, partial observability, model-based RL, offline RL, and safe RL. The connection between both directions (sustainability problems and reinforcement learning solution methods) is visualized in Table 2. Clearly, these reinforcement learning topics are not mutually exclusive, e.g. one might deal with a partially-observable multi-agent setting.\nConventionally, reinforcement learning is concerned with finding the optimal policy of just a single agent. However, many problems are modeled as multi-agent problems where any number of agents act in the same environment, jointly affecting its state space [48]. As a single agent is now no longer solely affecting the transition function, each agent will perceive a higher amount of unpredictability, increasing the state space and destabilizing the learning process. Sometimes agents may communicate, or cooperate. However, in some scenarios, there may be delays in communication, agents may not wish to share preferences, or there are privacy concerns [233]. Furthermore, distributed multi-agent environments may also be used as a means to model large state-action spaces. Distributed state-action spaces are smaller and easier to train on. A popular method for large multi-agent state spaces is to train algorithms according to centralized training, decentralized execution (CTDE). Here, multi-agent systems can exchange certain attributes during training, to increase efficiency, but this interaction is removed in deployment.\nIn sustainable energy reinforcement learning research, problems are often modeled as multi-agent problems, often in a collaborative manner. Frequently, these studies resort to improved trainability by splitting up larger state-action spaces over multiple agents, or CTDE. This is for example done in building control, controlling multiple appliances or multiple zones [120, 123, 125, 134, 137, 139], EV charging stations [164, 166], hydrogen refueling stations [179], energy management in grids [233, 241], voltage control [249] and power flow optimization [219], as well as in energy generation. Here, for example, windparks may be modeled as distributed multi-agent systems, splitting up the state-action\nA standard Markov Decision Process (Section 4) assumes perfect information. However, most real-world problems are actually Partially-Observable Markov Decision Processes (POMDPs) [254]. Partial observability refers to the fact that the current observation often does not capture all information of the ground-truth state of the system [255]. For example, in a first-person view navigation task, the current observation does not provide information about the environment behind us, and in a card game one agent may not know the hidden cards of their opponent. This partial observability may be mitigated by incorporating additional information from historical observations, i.e., a form of memory. However, taking our entire history into account quickly becomes computationally infeasible.\nPartial observability has been studied extensively in the reinforcement learning literature, for example for policy estimation [256, 257], value function estimation [258, 259], and the dynamics model [260, 261]. Key methods in deep learning that are used to address partial observability include windowing/framestacking [262], recurrent neural networks [263-265], transformers [266], external memory methods"}, {"title": "Benchmarks, baselines and performance metrics", "content": "So far, we have discussed various sustainable energy problems that lend themselves to reinforcement learning solutions (Section 5), and discussed various reinforcement learning areas that are prevalent in the current literature (Section 6). Successful progress in any field of research often benefits from standardization. In this section, we examine different benchmarks (environments), baselines (algorithms), and performance metrics used throughout the sustainable energy landscape within the field of reinforcement learning.\nStandardized benchmarks are a cornerstone of the progress in machine learning, facilitating the comparison of different methods on identical tasks and enabling a fair assessment of approaches. In reinforcement learning a significant advancement in this regard was realized with the introduction of Gym [143] and subsequently Gymnasium [307], which standardized an API for reinforcement learning environments. The field of sustainable energy encompasses a broad spectrum of challenges, as highlighted in Section 5. Despite (or because of) this diversity, numerous efforts have been made to create standardized environments. For ensuring wide usage and compatibility, ideally, they should also meet specific needs, including broad scope coverage, active maintenance, and integration with common frameworks like Gym/Gymnasium. An overview of these environments is given in Table 7, in which some of the main simulators are given that are ready to use in a Python-based reinforcement learning pipeline.\nNotable in Table 7, the building control problem has the widest variety of simulators available, possibly due to the popularity of the EnergyPlus building simulator [156]. Of further particular interest is SustainGym [308], which occurs in multiple settings. SustainGym is a library consisting of multiple sustainable energy Gymnasium environments that are, at the time of writing, still actively maintained. We encourage researchers entering the field to investigate the use of SustainGym for their studies and to consider developing new environments for its framework. Some of these benchmarks originate in challenges hosted in the past, such as the CityLearn challenge [159\u2013161] or the L2RPN (Learning to Run a Power Network) challenge [309-311]. These challenges serve as open benchmarks for continued submission beyond the original deadlines of these challenges.\nIn reinforcement learning, benchmarks define the environment and are used to measure the performance of an algorithm to solve a task. However, for a fair comparison, the performance of other solutions for the same environment is required, which we refer to as the baselines. Baselines often come in two different flavors: either as alternative non-reinforcement learning-based algorithms or as state-of-the-art reinforcement learning algorithms. In the realm of sustainable energy, the former baselines are usually chosen as classical optimization techniques, which are used in real-world applications. In contrast, the latter are usually used"}, {"title": "Discussion and Future Work", "content": "Matching supply with changes in demand is one of the major challenges in sustainable energy. New elements have arrived in the energy chain, such as batteries, smart grids, and smart appliances. This has led to a significant increase in the need for control and optimization of often interconnected - decision problems, a topic for which reinforcement learning methods are very well suited.\nThe combination of reinforcement learning in sustainable energy is still young, and the richness and fast growth of the landscape has resulted in a scattered field in terms of environments and benchmarks. A large amount of research in the field uses undisclosed, problem-specific environments or its own hand-crafted environment, often not open-sourcing the source code. This leads to redundant efforts and impedes the reproducibility of results.\nAs such, we believe that all areas in the field would greatly benefit from well-designed and general environments that receive long-term maintenance and can be continuously built up on. A promising software package in this regard is SustainGym [308], which attempts to standardize a variety of different sustainable energy environments. Such a standardization would allow for easier use of standardized algorithm implementations (such as Stable-Baselines and CleanRL). Furthermore, the entry barrier for reinforcement learning engineers would be reduced as they no longer need to focus on building realistic and relevant environments.\nWe note a flourishing amount of research in the field, often aimed at an initial demonstration of the potential of reinforcement learning. Often, well-known algorithms are used, such as tabular methods or DQN. A deeper look into the problem situation and the reinforcement learning literature may well be worthwhile to achieve better results. As the field matures, more interdisciplinary teams will arise, knowledge of the energy and algorithms field will integrate, and we expect more breakthrough results to appear.\nSpecifically, we will discuss some important reinforcement learning methods that may well be important for further progress. Firstly, some papers have accurately identified that the construction of simulators is not always feasible [132]; this may especially be true in the consumption domain, due to the diversity of consumption patterns. Model-based reinforcement learning methods may aid sustainable energy challenges as they first build a transition and re-"}, {"title": "Conclusion", "content": "This survey provides a comprehensive overview of the available reinforcement learning approaches to sustainable energy challenges. We first of all observe that the research field has grown rapidly in recent years, and there are many sustainability challenges to which reinforcement learning is applicable. However, we also observe that most papers originate from energy researchers that start to apply reinforcement learning methodology, while reinforcement learning researchers are less present probably because they struggle to understand the relevant underlying problems and the way\nto model them. Therefore, to mature the field, we likely need more interaction between researchers from both communities. A key direction for integration would be the development of better benchmarks, that is, standardized test environments, on which we can test and compare reinforcement learning approaches. Even then, real-world deployment will probably also require the development of new core methodology, most notably in safe and offline RL. In short, this survey identifies a large potential for reinforcement learning to contribute to the sustainable energy transition. Given the urgency in solving these problems, we hope to see the field mature and interdisciplinary work thrive."}]}