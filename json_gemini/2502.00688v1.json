{"title": "High-Order Matching for One-Step Shortcut Diffusion Models", "authors": ["Bo Chen", "Chengyue Gong", "Xiaoyu Li", "Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song", "Mingda Wan"], "abstract": "One-step shortcut diffusion models [Frans, Hafner, Levine and Abbeel, ICLR 2025] have\nshown potential in vision generation, but their reliance on first-order trajectory supervision is\nfundamentally limited. The Shortcut model's simplistic velocity-only approach fails to cap-\nture intrinsic manifold geometry, leading to erratic trajectories, poor geometric alignment, and\ninstability-especially in high-curvature regions. These shortcomings stem from its inability to\nmodel mid-horizon dependencies or complex distributional features, leaving it ill-equipped for\nrobust generative modeling. In this work, we introduce HOMO (High-Order Matching for\nOne-Step Shortcut Diffusion), a game-changing framework that leverages high-order supervi-\nsion to revolutionize distribution transportation. By incorporating acceleration, jerk, and be-\nyond, HOMO not only fixes the flaws of the Shortcut model but also achieves unprecedented\nsmoothness, stability, and geometric precision. Theoretically, we prove that HOMO's high-order\nsupervision ensures superior approximation accuracy, outperforming first-order methods. Em-\npirically, HOMO dominates in complex settings, particularly in high-curvature regions where\nthe Shortcut model struggles. Our experiments show that HOMO delivers smoother trajectories\nand better distributional alignment, setting a new standard for one-step generative models.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep generative models have exhibited extraordinary promise across various types of\ndata modalities. Techniques such as Generative Adversarial Networks (GANs) [GPAM+14], autore-\ngressive models [Vas17], normalizing flows [LCBH+22], and diffusion models [HJA20] have achieved\noutstanding results in tasks related to image, audio, and video generation [KES+18, BRL+23].\nThese models have attracted considerable interest owing to their capacity to create invertible and\nhighly expressive mappings, transforming simple prior distributions into complex target data dis-\ntributions. This fundamental characteristic is the key reason they are capable of modeling any data\ndistribution. Particularly, [LCBH+22, LGL22a] have effectively unified conventional normalizing\nflows with score-based diffusion methods. These techniques produce a continuous trajectory, often\nreferred to as a \u201cflow\", which transitions samples from the prior distribution to the target data\ndistribution. By adjusting parameterized velocity fields to align with the time derivatives of the\ntransformation, flow matching achieves not only significant experimental gains but also retains a\nstrong theoretical foundation.\nDespite the remarkable progress in flow-based generative models, such as the Shortcut model\n[FHLA25], these approaches still face challenges in accurately modeling complex data distributions,\nparticularly in regions of high curvature or intricate geometric structure [WET+24, HWA+24].\nThis limitation stems from the reliance on first-order techniques, which primarily focus on aligning\ninstantaneous velocities while neglecting the influence of higher-order dynamics on the overall flow\ngeometry. Recent research in diffusion-based modeling [Che23, HG24, LLLY24] has highlighted the\nimportance of capturing higher-order information to improve the fidelity of learned trajectories.\nHowever, a systematic framework for incorporating such higher-order dynamics into flow matching,\nespecially within Shortcut models, remains an open problem.\nIn this work, we propose HOMO (High-Order Matching for One-Step Shortcut Diffusion), a\nrevolutionary leap beyond the limitations of the original Shortcut model [FHLA25]. While Shortcut\nmodels rely on simplistic first-order dynamics, often empirically struggling to capture complex\ndata distributions and producing erratic trajectories in high-curvature regions, HOMO shatters\nthese barriers by introducing high-order supervision. By incorporating acceleration, jerk, and\nbeyond, HOMO not only addresses the empirical shortcomings of the Shortcut model but also\nachieves unparalleled geometric precision and stability. Where the Shortcut model falters-yielding\nsuboptimal trajectories and poor distributional alignment\u2014HOMO thrives, delivering smoother,\nmore accurate, and fundamentally superior results.\nOur primary contribution is a rigorous theoretical and empirical framework that showcases the\ndominance of HOMO. We prove that HOMO's high-order supervision drastically reduces approxi-\nmation errors, ensuring precise trajectory alignment from the earliest stages to long-term evolution.\nEmpirically, we demonstrate that the Shortcut model's first-order dynamics fall short in complex\nsettings, while HOMO consistently outperforms it, achieving faster convergence, better sample\nquality, and unmatched robustness.\nThe contributions of our work is summarized as follows:\n\u2022 We introduce high-order supervision into the Shortcut model, resulting in the HOMO frame-\nwork, which includes novel training and sampling algorithms.\n\u2022 We provide rigorous theoretical guarantees for the approximation error of high-order flow\nmatching, demonstrating its effectiveness in both the early and late stages of the generative\nprocess.\n\u2022 We demonstrate that HOMO achieves superior empirical performance in complex settings, es-"}, {"title": "2 Related Works", "content": "Diffusion Models. Diffusion models have garnered significant attention for their capability to\ngenerate high-fidelity images by incrementally refining noisy samples, as exemplified by DiT [PX23]\nand U-ViT [BNX+23]. These approaches typically involve a forward process that systemati-\ncally adds noise to an initial clean image and a corresponding reverse process that learns to\nremove noise step by step, thereby recovering the underlying data distribution in a probabilis-\ntic manner. Early works [SE19, SME20] established the theoretical foundations of this denois-\ning strategy, introducing score-matching and continuous-time diffusion frameworks that signif-\nicantly improved sample quality and diversity. Subsequent research has focused on more effi-\ncient training and sampling procedures [LZB+22, SSZ+24a, SSZ+24c], aiming to reduce com-\nputational overhead and converge faster without sacrificing image fidelity. Other lines of work\nleverage latent spaces to learn compressed representations, thereby streamlining both training\nand inference [RBL+22, HWSL24]. This latent learning approach integrates naturally with mod-\nern neural architectures and can be extended to various modalities beyond images, showcasing\nthe versatility of diffusion processes in modeling complex data distributions. In parallel, re-\ncent researchers have also explored multi-scale noise scheduling and adaptive step-size strate-\ngies to enhance convergence stability and maintain high-resolution detail in generated content\nin [LKW+24, FMZZ24, RCK+24, JZXG25, LYHZ24]. There are more other works also inspire our\nwork [XZC+22, DWB+23, PBHDE+23, WSD+23, WCZ+23, SSZ+24b, SSZ+24d, WXZ+24, CL24,\nKKN24, CLL+25b, CLL+25a, CXJ24, WCY+23, FJL+24, LZW+24, HWL+24].\nFlow Matching. Generative models like diffusion [SDWMG15, HJA20, SME20] and flow-\nmatching [LCBH+22, LGL22a] operate by learning ordinary differential equations (ODEs) that map\nnoise to data. To simplify, this study leverages the optimal transport flow-matching formulation\n[LGL22a]. A linear combination of a noise sample xo ~ N(0,I) and a data point x1 ~ D defines\nxt:\n$x_t = (1-t)x_0 + tx_1, $\n$v_t = x_1 - x_0,$\nwith vt representing the velocity vector directed from 20 to 21. While vt is uniquely derived from\n(xo, x1), knowledge of only xt renders it a random variable due to the ambiguity in selecting (x0, x1).\nNeural networks in flow models approximate the expected velocity \u016bt = E[vt | xt], calculated as an\naverage over all valid pairings. Training involves minimizing the deviation between predicted and\nempirical velocities:\n$\\tilde{v}_{\\theta}(x_t, t) \\sim \\mathbb{E}_{x_0, x_1 \\sim \\mathcal{D}} [v_t | x_t] $\n$L_F(\\theta) = \\mathbb{E}_{x_0, x_1 \\sim \\mathcal{D}} [||V_\\theta(x_t, t) - (x_1 - x_0)||^2].$\n(1)\nSampling involves first drawing a noise point xo ~ N(0, I) and iteratively transforming it into a\ndata point 21. The denoising ODEs, parameterized by e(xt,t), governs this transformation, and\nEuler's method approximates it over small, discrete time steps.\nHigh-order ODE Gradient in Diffusion Models. Higher-order gradient-based methods\nlike TTMs [KP92] have applications far exceeding DDMs. For instance, solvers [DNG+22] and"}, {"title": "3 Preliminary", "content": "This section establishes the notations and theoretical foundations for the subsequent analysis.\nSection 3.1 provides a comprehensive list of the primary notations adopted in this work. Section 3.2\nelaborates on the flow-matching framework, extending it to the second-order case, with critical\ndefinitions underscored.\n3.1 Notations\nWe use Pr[] to denote the probability. We use E[] to denote the expectation. We use Var[] to\ndenote the variance. We use ||x||p to denote the lp norm of a vector x \u2208 R\", i.e. ||x||1 := \u2211i=1 |xi|,\n||x||2 := (\u03a3i=1x3)1/2, and ||x||\u221e := maxi\u2208[n] |xi|. We use f(x) = O(g(x)) or f(x) \u2264 g(x) to denote\nthat f(x) \u2264 C. g(x) for some constant C > 0. We use N(0, I) to denote the standard Gaussian\ndistribution.\n3.2 Shortcut model\nNext, we describe the general framework of flow matching and its second-order rectification. These\nconcepts form the basis for our proposed method, as they integrate first and second-order informa-\ntion for trajectory estimation.\nFact 3.1. Let a field xt be defined as\nxt = Atx0 + Btx1,\nwhere at and \u1e9et are functions of t, and x0,x1 are constants. Then, the first-order gradient it and\nthe second-order gradient \u00eft can be manually calculated as\n$\\dot{x_t} = \\dot{A_t}x_0 + \\dot{B_t}x_1,$\n$\\ddot{x_t} = \\ddot{A_t}x_0 + \\ddot{B_t}x_1.$\nIn practice, one often samples (x0,x1) from (\u03bc\u03bf, \u03c0\u03bf) and parameterizes xt (e.g., interpolation)\nat intermediate times to build a training objective that matches the velocity field to the true time\nderivative it.\nDefinition 3.2 (Shortcut models, implicit definition from page 3 on [FHLA25]). Let \u2206t = 1/128.\nLet xt be current field. Let t \u2208 N denote time step. Let u\u2081(xt,t,d) be the network to be trained. Let\nd\u2208 (1/128,1/64,...,1/2,1) denote step size. Then, we define Shortcut model compute next field\nxt+d as follow:\n$x_{t+d} =\n\\begin{cases}\nx_t + u_1(x_t, t, d)d & \\text{if } d \\geq 1/128, \\\\\nx_t + u_1(x_t, t, 0)\\Delta t & \\text{if } d < 1/128.\n\\end{cases}$\""}, {"title": "4 Methodology", "content": "When training a flow-based model, such as Shortcut model, using only the first-order term as\nthe training loss has several limitations compared to incorporating high-order losses. (1) Firstly,\nrelying solely on the first-order term results in a less accurate approximation of the true dynamics,\nas it captures only the linear component and misses important nonlinear aspects that higher-order\nterms can represent. This can lead to slower convergence, as the model must implicitly learn\ncomplex dynamics without explicit guidance from higher-order terms. (2) Additionally, while the\nfirst-order approach reduces model complexity and the risk of overfitting, it may also limit the\nmodel's ability to generalize effectively to unseen data, particularly when the underlying dynamics\nare highly nonlinear. (3) In contrast, including higher-order terms enhances the model's capacity\nto capture intricate patterns, improving both accuracy and generalization, albeit at the cost of\nincreased computational complexity and potential overfitting risks.\nThen, we introducing our HOMO (High-Order Matching for One-step Shortcut diffusion model).\nThe intuition behind this design is to leverage high-order dynamics to achieve a more accurate\nand stable approximation of the field evolution. By incorporating higher-order losses, we aim to\ncapture the nonlinearities and complex interactions that are often present in real-world systems.\nThis approach not only improves the fidelity of the model but also enhances its ability to generalize\nacross different scenarios.\nDefinition 4.1 (HOMO Inference). Let \u2206t = 1/128. Let xt be the current field. Let t \u2208 N\ndenote the time step. Let 41,01 (\u00b7) and u2,02(\u00b7) denote the HOMO models to be trained. Let d \u2208\n(0, 1/128, 1/64, ...,1/2,1) denote the step size. Then, we define the HOMO computation of the\nnext field xt+d as follows:\n$x_{t+d} =\n\\begin{cases}\nx_{t+d} .u_1(x_t, t, d) + \\frac{d^2}{2} \\cdot u_2(u_1(x_t, t, d), x_t, t, d) & \\text{if } d \\geq 1/128, \\\\\nx_t + \\Delta tu_1(x_t, t, 0) + \\frac{(\\Delta t)^2}{2} \\cdot u_2(u_1(x_t, t, 0), x_t, t, 0) & \\text{if } d < 1/128.\n\\end{cases}$\nThe self-consistency target is to ensure that the model's predictions are consistent across dif-\nferent time steps. This is crucial for maintaining the stability and accuracy of the model over\nlong-term predictions.\nDefinition 4.2 (HOMO Self-Consistency Target). Let 41,01 be the networks to be trained. Let xt\nbe the current field and xt+d be defined in Definition 4.1. Let t \u2208 N denote the time step. Let\nd\u2208 (0,1/128,1/64,...,1/2,1) denote the step size. Then, we define the Self-Consistency target as\nfollows:\n$x_{t+d}^{target} = \\frac{u_{1,\\theta_1}(x_t, t, d)/2 + u_{1,\\theta_1}(x_{t+d}, t, d)/2}$\nThe second-order HOMO loss is designed to optimize the model by minimizing the discrepancy\nbetween the predicted and true velocities and accelerations. This loss function ensures that the\nmodel not only captures the immediate dynamics but also the underlying trends and changes in\nthe system.\nDefinition 4.3 (Second-order HOMO Loss). Let xt be the current field. Let t \u2208 N denote the time\nstep. Let t target be defined by Definition 4.2. Let 41,01(\u00b7) and 42,02(\u00b7) denote the HOMO models\nto be trained. Let d\u2208 (0,1/128,1/64,...,1/2,1) denote the step size. Let true and true be the"}, {"title": "5 Theoretical Analysis", "content": "In this section, we will introduce our main result, the approximation error of the second order flow\nmatching. The theory for higher order flow matching is deferred to Section D.\nWe first present the approximation error result for the early stage of the diffusion process. This\nresult establishes theoretical guarantees on how well a neural network can approximate the first\nand second order flows during the initial phases of the trajectory evolution.\nTheorem 5.1 (Approximation error of second order flow matching for small t, informal version of\nTheorem D.1). Let N be a value associated with sample size n. Let To := N-Ro and T* := N-51-8\nwhere Ro, \u03ba, \u03b4 are some parameters. Let's be the order of smoothness of the Besov space that the\ntarget distribution belongs to. Under some mild assumptions, there exist neural networks $1, \u03a62\nfrom a class of neural networks such that, for sufficiently large N, we have\n$\\int (||\\phi_1(x, t) - \\dot{x}_{true} ||^2 + ||\\phi_2(x, t) - \\ddot{x}_{true} ||^2)p_t(x)dx \\\\\n\\leq (a_t^2 \\log N + \\beta_t^2)N^{-\\frac{2s}{\\epsilon}} + \\mathbb{E}_{x\\sim P_t} [||\\dot{x}_{true} - \\ddot{x}_{true}||^2]$\nholds for any t \u2208 [T0,3T*]. In addition, $1,$2 can be taken so we have\n$||\\phi_1(\\cdot, t)||_{\\infty} = O(|a_t|\\sqrt{\\log n} + |\\beta_t|),$\n$||\\phi_2(\\cdot, t)||_{\\infty} = O(|a_t|\\sqrt{\\log n} + |\\beta_t|).$"}, {"title": "6 Experiments", "content": "This section presents a series of experiments to evaluate the effectiveness of our HOMO method\nand assess the impact of each loss component. Our results demonstrate that HOMO significantly\nimproves distribution generation, with the higher-order loss playing a key role in enhancing model\nperformance.\n6.1 Experiment setup\nWe evaluate HOMO on a variety of data distributions and different combinations of losses. We\nwould like to restate that the HOMO with first order loss and self-consistency loss is equal to\nthe original One-step Shortcut model [FHLA25], i.e., M1+SC. Furthermore, M1+M2+SC and\nM1+M2+M3+SC are our proposed methods.\nFor the distribution dataset, in the left-most figure of Figure 1, we show an example of an eight-\nmode Gaussian distribution. The source distribution \u03c0\u03bf and the target distribution \u03c0\u2081 are con-\nstructed as mixture distributions, each consisting of eight equally weighted Gaussian components."}, {"title": "6.2 Mixture of Gaussian experiments", "content": "We analyze the performance of HOMO on Gaussian mixture datasets [LSSZ24] with varying modes\n(four, five, and eight). The most challenging is the eight-mode distribution, where HOMO with all\nthree losses (M1+M2+SC) produces the best results, achieving the lowest Euclidean distance.\nThe eight-mode Gaussian mixture distribution dataset (Figure 1 (a) ) contains eight Gaussian\ndistributions whose variance is 0.3. Eight source mode (brown) positioned at a distance Do =\n6 from the origin, and eight target mode (indigo) positioned at a distance Do = 13 from the\norigin, each mode sample 100 points. HOMO optimized with first-order, second-order, and self-\nconsistency losses is the only model that can accurately learn the target eight-mode Gaussian\ndistribution, achieving high precision as evidenced by the lowest Euclidean distance loss among all\ntested configurations. We emphasize the importance of the second-order loss. Without it, the model\nstruggles to accurately capture finer distribution details (Figure 1 (f)). However, when included,\nthe model better matches the target distribution (Figure 1 (h)).\nWe further analyze how each loss contributes to the final performance of the HOMO. (1) The\nfirst-order loss enables HOMO to learn the general structure of the target distribution, but it\nstruggles to capture finer details, as shown in Figure 1 (b) and Figure 1 (g). (2) The second-order\nloss can lead to overfitting in the target distribution, as shown in Figure 1(c). When used alone, the\nsecond-order loss may cause the model to focus too much on details and lose sight of the broader\ndistribution. (3) The self-consistency loss enhances the concentration of the learned distribution,\nas shown in Figure 1 (d). Without the self-consistency loss, as shown in Figure 1 (e), the learned\ndistribution becomes more sparse."}, {"title": "6.3 Complex distribution experiments", "content": "In this section, we conduct experiments on datasets with complex distributions, where we expect\nour HOMO model to learn the transformation from a regular source distribution to an irregular\ntarget distribution."}, {"title": "6.4 Third-order HOMO", "content": "As discussed in previous sections, second-order HOMO has shown great performance on various\ndistribution datasets. Therefore, in this section, we further investigate the performance of HOMO\nfor adding an additional third-order loss.\nWe begin by introducing the dataset we used in this section. We use three kinds of datasets:\n2 Round spin, 3 Round spin, and Dot-Circle datasets. In 2 Round spin dataset and 3 Round spin\ndataset, we both sample 600 points from Gaussian distribution with 0.3 variance for both source\ndistribution and target distribution. In Dot-Circle datasets, we sample 300 points from the center\ndot and 300 points from the outermost circle, combine them as source distribution, and then sample\n600 points from 2 round spin distribution.\nThe qualitative results from the experiments (Figure 3) demonstrate that the additional third-\norder loss enables HOMO to better capture more complex target distributions. For instance, the\ncomparison between Figure 3 (c) and Figure 3 (d), as well as between Figure 3 (g) and Figure 3 (h),\nillustrates how the third-order loss enhances the model's ability to fit more intricate distributions.\nThese findings are consistent with the quantitative results presented in Table 3.\nThe trend of incorporating higher-order loss terms to improve model performance highlights\nthe importance of introducing higher-order supervision in modeling the complex dynamics of dis-\ntribution transformations between the source and target distributions. By capturing higher-order\ninteractions, the model is better equipped to understand and adapt to the intricate relationships\nwithin the data, leading to more accurate and robust learning. This approach underscores the value\nof enriching the model's training objective to handle the complexities inherent in real-world data\ndistributions."}, {"title": "7 Conclusion", "content": "In this work, we introduced HOMO (High-Order Matching for One-Step Shortcut Diffusion), a\nnovel framework that incorporates high-order dynamics into the training and sampling processes\nof Shortcut models. By leveraging high-order supervision, our method significantly enhances the"}, {"title": "3.1", "content": "Let a field be defined as"}, {"title": "where", "content": "and function constants. Then, the gradient and second-order gradient can be manually calculated as"}, {"title": "1, 3", "content": "Next, we show that there exist some such that"}, {"title": "C.1 Definitions of Besov Space", "content": "Definition Modulus of Smoothness. Let \u03a9 be a domain in For a function with the -th modulus of smoothness of is defined by"}, {"title": "where", "content": "the finite difference operator is given by"}, {"title": "2 B", "content": "Definition B-Spline. Let , and set . The Besov seminorm of is defined as"}, {"title": "D.1 Approximation Error of Second Order Flow Matching for Small t", "content": "Theorem Approximation error of second order flow matching for small formal version of Theorem. Under Assumptions and if the following holds\nThere exist neural networks such that for sufficiently large we have"}, {"title": "Then", "content": "there exists neural networks such that, for sufficiently large we have"}, {"title": "By", "content": "Lemma there is such that"}, {"title": "Next", "content": "we can show that there exists some such that"}, {"title": "Next", "content": "we can show that there exists some such that"}, {"title": "Then", "content": "there exists and satisfying such that, and thus is complete."}, {"title": "D.3", "content": "of Second Order Flow Matching for Large"}, {"title": "Let", "content": "be arbitrary, under Assumptions holds"}, {"title": "and", "content": "if the following holds\nThen there exist neural networks such that"}, {"title": "B", "content": "Work\nIn this section, we discuss more related work which inspire our"}]}