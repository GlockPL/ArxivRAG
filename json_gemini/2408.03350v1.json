{"title": "miniCTX: Neural Theorem Proving with (Long-)Contexts", "authors": ["Jiewen Hu", "Thomas Zhu", "Sean Welleck"], "abstract": "We introduce miniCTX, which tests a model's ability to prove formal mathematical\ntheorems that depend on new definitions, lemmas, or other contextual information\nthat was not observed during training. miniCTX contains theorems sourced from\nreal Lean projects and textbooks, each associated with a context that can span tens\nof thousands of tokens. Models are tasked with proving a theorem given access\nto code from the theorem's repository, which contains context that is helpful or\nneeded for the proof. As a baseline for miniCTX, we introduce file-tuning, a simple\nrecipe that trains a model to generate a proof step conditioned on the preceding\nfile contents. File-tuning substantially outperforms the traditional neural theorem\nproving approach that fine-tunes on states alone. Additionally, our file-tuned model\nimproves performance on the standard miniF2F benchmark, achieving a pass rate\nof 33.61%, which is a new state-of-the-art for 1.3B parameter models. Alongside\nminiCTX, we offer NTP-TOOLKIT for automatically extracting and annotating\ntheorem proving data, making it easy to add new projects into miniCTX to ensure\nthat contexts are not seen during training. miniCTX offers a challenging and\nrealistic perspective on evaluating neural theorem provers.", "sections": [{"title": "1 Introduction", "content": "Formal theorem proving in interactive theorem provers (ITPs) provides a testbed for evaluating the\nreasoning capabilities of large language models (LLMs). Theorem proving capabilities can then\ndirectly translate to automation for mathematicians, such as via tools that complete or formalize\nproofs [1-4]. However, despite their promise, we see a gap between current language model-based\nprovers and the complexity of real-world theorem proving.\nOur motivating observation is that theorems and proofs depend on various forms of context, such\nas newly-defined definitions and lemmas. For instance, to prove results about a square, one might\nfirst formalize a definition of a rectangle, prove some results about rectangles, then specialize them\nto a newly-defined square [5] (Figure 1). However, existing methods for training and evaluating\nLLM-based theorem provers do not take such context into account. For example, benchmarks often\nfocus on proving standalone competition problems (e.g., miniF2F [6]) or theorems from a library that\nthe model has trained on (e.g., mathlib [7, 8]), and LLM-based provers are trained to accept only a\nproof state as input, making them unaware of new theorems and definitions [9].\nFrom a language modeling perspective, a key challenge is the sheer size of potentially useful context.\nFor instance, proving a theorem in the Prime Number Theorem project without having seen it during\ntraining can require reasoning over a long context of interdependent definitions and theorems [10].\nTherefore, theorem proving with newly-defined contexts offers a challenging testbed for long-context\nreasoning, in addition to more accurately reflecting practical proving. With these considerations"}, {"title": "2 Theorem proving with context", "content": "Formal theorem proving involves two stages: defining mathematical objects and facts relevant to the\ndesired result, then stating and proving the result itself. For language model-based provers to function\nas useful tools in this real-world setting, they need to be able to work with new information such as\nnew definitions or lemmas. For example, a system suggesting proofs in the Prime Number Theorem\nproject should be familiar with the project's definition of \"square\". Unfortunately, current language\nmodel-based provers are trained on a static snapshot of data, and are hence unaware of any context\nthat was created after the model was trained (Figure 1). As a result, it remains unclear whether these\nmodels can work with new information, and how to enable this capability.\nContext-dependent proving. We study context-dependent theorem proving, where the goal is for\na model to generate proofs y for new theorems x, based on a context c that includes background\ninformation such as definitions, lemmas, or natural language comments. Formally,\narg max E_{(x,c)~p}E_{y~M(\\cdot|x,c)}v(x, C, y),\nM\nwhere (x, c) ~ p denotes a (theorem, context) pair from a context distribution p, M is a model that\nproduces a proof y, and v returns 1 if the proof is correct and 0 otherwise (implemented by Lean\u00b2).\nIn this paper, we treat a Lean repository as a context distribution. Each context c is some subset of\ncode in the repository that does not contain the theorem's formal proof. In this paper we take c to be\nthe code that precedes the theorem x in a file. This context includes new definitions, lemmas, and\ncomments that are relevant to the theorem but does not show the theorem's formal proof.\nEvaluating context-dependent proving. To measure a model's performance on context-dependent\nproving, a model is tasked with generating a proof y given a theorem x and its preceding code c.\nGiven a language model, we can test three kinds of generalization by ensuring the following:\n\u2022 Theorem-level generalization: the proof must not occur in the model's training data.\n\u2022 Context-level generalization: the code c and proof must not occur in the training data.\n\u2022 Project-level generalization: the entire repository must not occur in the training data."}, {"title": "3 miniCTX: a benchmark for theorem proving with context", "content": "We develop miniCTX, a Lean 4 theorem proving benchmark of theorems that depend on newly-\ndefined lemmas, definitions, and proofs from within a project. miniCTX is currently based on\n376 theorems from four projects: recent Lean formalizations of the Prime Number Theorem\n(PrimeNumberTheoremAnd) [10] and the Polynomial Freiman-Ruzsa Conjecture (PFR) [14], prob-\nlems from an introductory text on theorem proving (HTPI) [15], and recent results from Lean's\nmathematical library (Mathlib) [16].\nEach theorem in miniCTX is accompanied by the following data, formatted in JSON:\n1. Theorem statement,\n2. Preceding file contents up to the theorem statement,\n3. Metadata, including:\n(a) File name,\n(b) Project commit and version,\n(c) Commit at which the theorem and its file was added,\n(d) Position of the theorem and number of premises preceding it,\n(e) Proof length and type,\n(f) Whether the statement or proof uses definitions or lemmas from within the file or\nrepository.\nSee example in \u00a7A.1. We make the dataset available at 131ab/miniCTX, along with corresponding\nevaluation code based on the Lean REPL [17]. The dataset has an Apache 2.0 license.\nPrime Number Theorem. PrimeNumberTheoremAnd [10] is a project started in January 2024 that\nformalizes the prime number theorem in Lean as well as related concepts, such as residue calculus\non rectangles in C. We find the files Rectangle. lean and ResidueCalcOnRectangles.lean\nsuitable for our purpose of testing context-dependent theorem proving, especially when we use\npreceding file content as context, as each file is self-contained within the project and contains new\ndefinitions (rectangles, squares) and many interdependent lemmas. See \u00a7A.2 for an illustration\nof such lemmas. In addition, most theorems from ResidueCalcOnRectangles. lean rely on the\ndefinitions from Rectangle.lean, which serves as a perfect example of cross-file dependencies.\nWe extracted 87 theorems from these files. Assuming that a model was trained prior to January 2024,\nthis split guarantees the evaluation of project-level, context-level, and theorem-level generalization.\nPFR. PFR [14] is a project started in November 2023 that formalizes a proof of the Polynomial\nFreiman-Ruzsa (PFR) conjecture. We included 54 theorems from PFR. We find that proofs of\ntheorems in PFR tend to be much more monolithic and longer in length than those in Mathlib or other\nlibraries. PFR also defines custom mathematical concepts and notations (such as Ruzsa distance) and\na proof typically depends on many lemmas in PFR outside the current file. All of the theorems were\nadded to PFR after November 2023. Assuming that the model was trained prior to this date, this split\nguarantees the evaluation of project-level, context-level, and theorem-level generalization.\nRecent Mathlib Commits. Lean's mathematical library, Mathlib [16], is a community-maintained\nLean repository including mathematical concepts as well as programming APIs and common tactics.\nIt is the single largest Lean library that users contribute to, and is therefore representative of the\nproduction environment in which neural theorem provers are deployed. Mathlib is a long-standing\nproject, and it is common practice to train language model-based provers on Mathlib. It is therefore\nlikely that Mathlib source files have been observed during training. However, Mathlib is frequently"}, {"title": "3.1 Core challenges: dependencies, length, constraints, and varied content", "content": "miniCTX introduces several challenges for language-model based provers.\nFile and repository dependencies. First, a theorem's statement or proof may have contextual\ndependencies, i.e., requiring the use of definitions or lemmas from the new repository. Assuming that\nthe model has not observed the repository during training, a model must have some mechanism for\ntaking in the new definitions and lemmas, and must have the ability to produce proofs that, intuitively,\nrequire understanding the new context. As a proxy indicator of having dependencies, we record\nwhether the statement or proof uses definitions or theorems from within the file or repository.\nLong context length. A second challenge is the length of the context in miniCTX. As seen in\nTable 1, the average context length is over 26,000 tokens. Intuitively, the model must reason over\nlong contexts containing information that is needed to produce a proof. This differs from proving\nstandalone competition problems in miniF2F, which have an average context length of 153 tokens.\nConstraints. Third, real-world proofs may have particular constraints, such as disabling common\nautomation tools and defining new tactics, or custom definitions and syntax such as in PFR. A model\nmust be able to adapt and produce these context-dependent proofs.\nVaried content and difficulty. A final challenge is that miniCTX has varied mathematical content in\nits theorems, thus requiring a flexible automated proving system. This ranges from writing explicit\ncalculation proofs in HTPI to writing sophisticated proofs in PFR. One axis of variation is the length\nof the human-written proof (if available), which we record and treat as a proxy for difficulty."}, {"title": "4 File-tuning for theorem proving with context", "content": "Writing a proof can be seen as a sequential process ((x_1,y_1),(x_2, y_2), . . .) of states x_t and tactics\ny_t. A state contains what is left to prove (the goal), and available information (the hypotheses).\nA tactic transitions the proof to a new state. If the state contains no remaining goals, the proof is\ncomplete. Concretely, a user applies tactics by writing Lean code, Lean keeps track of the state, and\nthe development environment shows the state and the written code.\nThe traditional approach to training a language model for\ntheorem proving is to train a model on (state, tactic) pairs,\ni.e., train it to predict the next step of a proof (i.e., the\ntactic), given the state provided by the proof assistant (i.e.,\nthe proof state) [9, 7, 19, 8]. A drawback to this approach\nis that at test time, the model is not aware of new context\noutside of the proof state, such as new lemmas. We will see\nlater on that models trained with this state-tactic approach\nfail at context-dependent proving.\nAs a stronger baseline for context-dependent proving, we\npresent file-tuning, a simple recipe that trains with (pre-\nceding file context, proof state, next-tactic) tuples instead\nof training with (proof state, next-tactic) pairs (Figure 2).\nThis lets the model use new definitions, theorems, or other\ninformation that are defined prior to the current tactic at\ntraining or at test time. In practice, file-tuning requires extracting contexts and proof states from Lean.\nWe describe our toolkit for doing so next."}, {"title": "4.1 NTP-TOOLKIT for automated data extraction and evaluation", "content": "We provide NTP-TOOLKIT to extract training data, evaluation data, and interactive Lean evaluation.\nData extraction. NTP-TOOLKIT contains a general-purpose data extraction tool that extracts\nexamples from an arbitrary Lean 4 repository and formats them into examples for language-model\nfine-tuning. The tool is implemented in Lean based on Kim Morrison's lean-training-data [20].\nSpecifically, NTP-TOOLKIT takes in a configuration file with one or more Lean repositories specified.\nEach repository is transformed into next-tactic and full proof examples stored in JSON Lines files.\nThe next-tactic data is suitable for making file-tuning examples of the form (context, state, next-\ntactic). The full proof data is suitable for making evaluation or training examples of the form (context,\ntheorem, proof). See Appendix \u00a7C.1 for the JSON formats provided by the data extraction.\nInstruction-tuning data. NTP-TOOLKIT re-formats extracted data into input-output examples for\nfile-tuning and state-tactic tuning. Each example is preceded with a natural language instruction\nthat describes the task (generating a next-tactic) and the available inputs (e.g., file contents, proof\nstate). Data of this form is often referred to as \u201cinstruction-tuning data\". The specific instructions and\nformats that we used are shown in the Appendix (\u00a7C.2). Users can also adjust the formats as needed.\""}, {"title": "5 Experiments", "content": "First, we describe our procedure for file-tuning a language model on examples extracted from Mathlib.\nSecond, we evaluate the file-tuned model and other baselines on miniCTX. We study performance\nalong various axes, such as dependencies, context length, difficulty, and the type of information in the\ncontext. Our experiments show that traditional state-tactic models struggle in the context-dependent\nsetting compared to file-tuned models that can use context at proving time. Finally, our investigation\nreveals several open challenges and future directions that we discuss in \u00a75.5"}, {"title": "5.1 File-tuning on Mathlib", "content": "Data extraction. We ran NTP-TOOLKIT's next-tactic extraction on a snapshot of Mathlib, yielding\n307,049 examples available at 13lab/ntp-mathlib. We then ran NTP-TOOLKIT's instruction tuning\nscript on these examples, yielding file-tuning examples and state-tactic examples. For the file-\ntuning examples, as an initial method for handling the long Lean files, we either truncate the\nmiddle of an input file so that the file contents is 1024 tokens, or take only the preceding 1024\ntokens, with the strategy selected at random for each example. The state-tactic examples are\nat 13lab/ntp-mathlib-instruct-st. The union of file-tuning and state-tactic examples are at\n13lab/ntp-mathlib-instruct-context, split into 583k train, 15k dev, and 15k test examples."}, {"title": "5.2 Evaluation setup", "content": "We evaluate models for the task of tactic-based theorem proving: given a theorem statement, a model\ngenerates one tactic at a time while receiving states from the proof assistant. We use a standard\nbest-first search strategy [9, 7, 22, 8, 1] which prioritizes partial proofs based on the model's average\nlog probabilities. This search method is parameterized by the number of generated tactics per iteration\nS, and the maximum number of iterations T. We use the setting from [23, 1] (S = 32, and T = 100).\nWe evaluate five types of baselines: (1) pass@1 full proof generation using GPT-40: we prompt the\nmodel with only the theorem statement and require it to generate a complete proof (see Appendix\n(\u00a7C.2) for details of the prompts and few-shot examples); (2) pass@1 full proof generation with\nin-file context using GPT-4o: we supplement the theorem statement with up to 8000 tokens of in-file\ncontext; (3) the file-tuning model described in (\u00a75.1); (4) the state-tactic model described in (\u00a75.1);\nand (5) a state-tactic prompting model: we prompt a pre-trained language model with (state, tactic)\nexamples, following [23]. We use the Llemma-7b model [23]."}, {"title": "5.3 Top-level results", "content": "File-tuning maintains performance on competition problems. First, we evaluate on miniF2F,\na standard benchmark based on competition problems that do not require the use of context. We\nadapt it to the context-dependent setting by simply providing the model with the miniF2F import"}, {"title": "5.4 Analysis", "content": "We analyze the file-tuned and state-tactic models on miniCTX further along several axes, including\nthe kinds of contextual dependencies, the difficulty, and the content made available in the context.\nFile-tuning especially helps on problems with dependencies. We use the miniCTX metadata\nto categorize theorems from Prime, PFR and Mathlib splits based on their in-file dependencies.\nFigure 3 shows the performance on problems that depend on in-file definitions, in-file theorems,\nboth, or neither. We also show miniF2F as an additional reference point for problems without in-file\ndependencies. The file-tuned model shows a marked improvement over the state-tactic tuned model\nin all scenarios that have dependencies (outlined in black). The gap is much larger than on problems"}, {"title": "5.5 Discussion and future challenges", "content": "In addition to general improvements in performance, we comment on some specific open challenges.\nMaking better use of long-contexts. Our file-tuning method simply truncates contexts to be within\na token budget (1024 in our experiments), which can discard useful contextual information. We"}, {"title": "6 Related Work", "content": "Formal theorem proving with language models. GPT-f [9] pioneered the use of language models\nfor theorem proving via next tactic prediction given the current proof state, a technique adopted by\nmany subsequent methods [24, 7, 19, 22, 1, 23]. ReProver [8] conditions each generation on retrieved\npremises, while Draft-sketch-prove [25] conditions each generation on an informal proof. Baldur [26]\nfine-tunes a model with 50 lines of the preceding file content as context, but unlike file-tuning trains\nthe model to generate a full proof without proof states. More broadly, machine learning for formal\ntheorem proving is an active research area; see [27, 28] for surveys.\nTheorem proving data extraction. Several tools extract training data from interactive theorem\nprovers, including CoqGym [29] for Coq, PISA [24] for Isabelle, LeanStep [7] for Lean 3, and\nLeanDojo [8] for Lean 3 and 4. Recently, lean-training-data [20] provides tools for extracting\nproof states and other information using Lean 4 metaprogramming, which we anecdotally found to be\neasiest to modify and fastest among Lean 4 data extraction tools. Our ntp-toolkit adds 3 new tools\non top of this code, along with a pipeline for running on any Lean projects and instruction tuning.\nTheorem proving benchmarks. Theorem proving methods are typically evaluated in two settings:\n(1) standalone competition problems [6] or textbook [30] problems; (2) holding out theorems from a\nmathematical library that the model is trained on, such as Mathlib for Lean [7, 22, 8] or the Archive\nof Formal Proofs for Isabelle [24, 26]. The first does not test the use of context, while the second\ntests only theorem-level generalization. miniCTX is designed to test the use of context as well as\ntheorem-level, context-level, and project-level generalization across several mathematical domains."}, {"title": "7 Conclusion", "content": "We studied the realistic setting of proving theorems that depend on new information and project\nconstraints, and formulated an evaluation framework for testing generalization using real Lean\nprojects. We built miniCTX, and found that the predominant method for training neural theorem\nprovers fails to enable context dependent proving. Our file tuning method provides a strong starting\npoint for the new challenges opened by our investigation into theorem proving with context."}, {"title": "A miniCTX Examples", "content": "Here we give some examples of the miniCTX and its sources to illustrate the format of the data and\nhow and why we collect certain theorems."}, {"title": "A.1 Example Entry", "content": "An entry in the miniCTX dataset consists of the theorem statement, preceding file contents, and\nmetadata information. For example, given the following theorem s_eq_pow_two in context:"}, {"title": "A.2 PrimeNumberTheoremAnd Example", "content": "We collect theorems from the Rectangle. lean file in PrimeNumberTheoremAnd. The following\nexcerpt from Rectangle.lean demonstrates the scenario that often arises in a theorem proving\nenvironment where context is critical to producing a proof:"}, {"title": "B Additional datasets", "content": "In addition to problems in miniCTX, we also evaluated other datasets that are not included due to\ncopyright reasons."}, {"title": "B.1 Math2001", "content": "Math2001 [15] contains the Lean code for the book The Mechanics of Proof by Heather Macbeth, an\nintroductory text on mathematical theorem proving with accompanying Lean code. Each chapter of\nThe Mechanics of Proof covers an introductory topic and walks through how to write the associated\nmathematics in Lean, along with exercises. The topics include proofs by calculation, proofs with\nstructure, parity and divisibility, logic, induction, number theory, functions, sets, and relations. A\nunique aspect of Math2001 is that it disables common Lean automation for pedagogical purposes.\nFor example, a student must write out an equality proof in detail, with each step justified. It also\ndefines new tactics and definitions separate from the common Lean libraries. Typically a file in the\ntextbook will show examples of such proofs, followed by exercises for a student to complete. We\ncan view this as a form of contextual adaptation: a model must prove the theorem according to the\nconstraints of the textbook. Math2001 has 41 files that include examples and exercises. We selected 1\nto 2 theorems from each file (depending on the length of the file), for a total of 50 theorems. Of these,\n31 have no proof in the Math2001 repository, hence testing theorem-level generalization."}, {"title": "C NTP-TOOLKIT and file-tuning details", "content": "NTP-TOOLKIT contains a general-purpose data extraction tool that extracts examples from an arbitrary\nLean 4 repository and formats them into examples for language-model fine-tuning. The tool is\nimplemented in Lean based on Kim Morrison's lean-training-data."}, {"title": "C.1 Data extraction", "content": "NTP-TOOLKIT contains a general-purpose data extraction tool that extracts\nexamples from an arbitrary Lean 4 repository and formats them into examples for language-model\nfine-tuning. The tool is implemented in Lean based on Kim Morrison's lean-training-data."}, {"title": "C.2 Input-output formatting.", "content": "Below we show the inputs and outputs for file-tuning and state-tactic tuning. In the paper we refer to\nthe natural language description at the beginning of the input as an \u201cinstruction\", and refer to a set of\ninputs and outputs as described below as \u201cinstruction-tuning data\"."}, {"title": "C.2.1 File tuning.", "content": "Given an example containing a state, next-tactic, and preceding file contents (srcUpToTactic), the\ndata is formatted as:"}, {"title": "C.2.2 State-tactic tuning.", "content": "Given an example containing a state and next-tactic, the data is formatted as:"}, {"title": "C.2.3 GPT4-0 prompt", "content": "For full proof generation task with only theorem statement, we use the following prompt:\nFor full proof generation task with additional infile context, we use the following prompt:"}, {"title": "D Additional results and analysis", "content": ""}, {"title": "D.1 Label distribution", "content": ""}, {"title": "D.2 Performance by proof length and context length.", "content": ""}, {"title": "D.3 Example of modifying proofs from context", "content": "The File-tuned model is able to utilize the proofs in the context. Here is an example of the model\nmaking minimal modification to the proofs from the context:"}, {"title": "D.4 Example of using theorems from context", "content": "The File-tuned model is able to utilize the stated theorems in the context. Here is an example of the\nmodel using the previously defined theorem in the proof:"}, {"title": "D.5 Example of using unseen definitions", "content": "The State-tactic-tuned model is able to utilize the even unseen definitions that appear in the proof\nstate. In the following example Set. uloo is a newly defined definition, which is never seen for\nstate-tactic tuned model:"}, {"title": "E Dataset Hosting, Licensing, and Maintenance", "content": "miniCTX is released on HuggingFace: 13lab/miniCTX5, distributed under the Apache 2.0 license.\nData extraction tool NTP-TOOLKIT is released on GitHub: cmu-13/ntp-toolkit, under the MIT\nlicense. We note that the underlying data for the individual splits of miniCTX are also released under\nthe Apache 2.0 license. We include the licensing information in the dataset repository. We plan to\nregularly update and maintain the dataset to include examples from new projects."}, {"title": "F Annotation Guideline", "content": "With the NTP-TOOLKIT, users can extract and annotate new theorems from any valid Lean project."}, {"title": "F.1 Preliminary", "content": "The evaluation code relies heavily on the Lean REPL, which operates within the project environment.\nTherefore, it is essential that the project builds without any errors. Additionally, the version of Lean\nused in the project should match the version supported by the REPL. While the Lean REPL supports\nversions \u2265 4.3.0, for the best experience, we recommend evaluating projects that use Lean versions\nbetween 4.5.0 and 4.7.0."}, {"title": "F.2 Using the NTP-TOOLKIT", "content": "The NTP-TOOLKIT is designed to facilitate the extraction and annotation of theorem proving data\nfrom Lean projects. To use the toolkit for annotating theorems, follow these steps:\n1. Installation: Clone the NTP-TOOLKIT repository from GitHub to your local machine.\nEnsure that you have the required dependencies installed, as listed in the repository's\nREADME file.\n2. Configuration: Configure the toolkit to point to the directory of your Lean project. Make\nsure that your project is using a compatible version of Lean as per the guidelines in the\nprevious section.\n3. Data Extraction: Run the data extraction script provided by the toolkit. Specify the\nfull_proof_training_data option to focus the extraction on datasets suitable for train-\ning models with full proofs.\nFor detailed commands and additional options, please refer to the README file in the NTP-\nTOOLKIT repository."}, {"title": "F.3 Evaluation", "content": "We provide a comprehensive evaluation pipeline in the MINICTX-EVAL repository, supporting both\ntactic-prediction and full-proof generation tasks. Users should place the extracted JSONL file from\nthe NTP-TOOLKIT into the data folder. To run an evaluation task, execute the task script by\nspecifying the dataset path, the corresponding project path, and the path to the Lean REPL. This\nsetup ensures that the evaluation is conducted within the correct environment and with the necessary\ndata inputs."}]}