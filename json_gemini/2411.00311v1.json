{"title": "Client-Customized Adaptation for Parameter-Efficient Federated Learning", "authors": ["Yeachan Kim", "Junho Kim", "Wing-Lam Mok", "Jun-Hyung Park", "SangKeun Lee"], "abstract": "Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients. One potential solution to bypass such constraints might be the use of parameter-efficient fine-tuning (PEFT) in the context of FL. However, we have observed that typical PEFT tends to severely suffer from heterogeneity among clients in FL scenarios, resulting in unstable and slow convergence. In this paper, we propose Client-Customized Adaptation (C2A), a novel hypernetwork-based FL framework that generates client-specific adapters by conditioning the client information. With the effectiveness of the hypernetworks in generating customized weights through learning to adopt the different characteristics of inputs, C2A can maximize the utility of shared model parameters while minimizing the divergence caused by client heterogeneity. To verify the efficacy of C2A, we perform extensive evaluations on FL scenarios involving heterogeneity in label and language distributions. Comprehensive evaluation results clearly support the superiority of C2A in terms of both efficiency and effectiveness in FL scenarios.", "sections": [{"title": "1 Introduction", "content": "The advent of large-scale pre-trained language models (PLMs) for natural language processing (NLP) has led to exceptional performance across a broad spectrum of domains. However, the high memory requirements for PLMs impede their applicability to resource-constrained environments. These challenges are particularly evident in federated learning (FL), where model weights are transmitted between the server and clients to preserve data privacy (Kone\u010dn\u00fd et al., 2016; McMahan et al., 2017).\nWhile recent FL studies have expanded the application of PLMs in various tasks, such as text classification (Zhu et al., 2020; Qin et al., 2021; Weller et al., 2022), language modeling (Chen et al., 2019), and question answering (Chen et al., 2021), communicating the training model among clients requires huge computational resources and bandwidth, presenting a significant challenge in terms of practicality.\nParameter-efficient fine-tuning (PEFT) approach is thereby a promising strategy for reducing communication costs in FL. Through tuning only a small fraction of parameters, such as adapter-based tuning (Houlsby et al., 2019; Hu et al., 2022; Mahabadi et al., 2021a), bias tuning (Zaken et al., 2022), and prompt-tuning (Lester et al., 2021), PEFT approaches significantly enhance the memory efficiency in centralized scenarios. However, the feasibility of PEFT in decentralized scenarios has not been well explored.\nHence, we investigate the applicability of typical PEFT approaches in FL scenarios. Specifically, we measure the performance and client drifts (Karimireddy et al., 2020; Li et al., 2021) of PEFT approaches in FL. Our discoveries are as follows: (1)"}, {"title": "2 PEFT in FL Scenario", "content": "typical PEFT approaches show large performance degradation in FL scenarios as the degree of non-IID increases; (2) these approaches usually suffer from large client drifts in non-IID scenarios, resulting in slow convergence and detrimental model performance. The above observations reveal that adopting PEFT in FL is not trivial, and posing the necessity to address large client drift.\nTo overcome the identified limitations, we propose a novel hypernetwork-based FL framework, Client-Customized Adaptation (C2A), that leverages the information of different data distributions on clients. Our key idea is to generate the adapter parameters tailored to each client via hypernetworks by taking the information of client data distribution, rather than naively fitting a single global adapter to all heterogeneous data distributions (Figure 1). By learning to adopt the different data distributions to generate adapters for each client, C2A enables robust training for various non-IID conditions while sharing knowledge among clients. Moreover, in order to manage the large number of parameters associated with hypernetworks, we introduce factorized hypernetworks, thereby significantly reducing the number of parameters without sacrificing the performance.\nWe carefully design the experimental setting to verify the efficacy of C2A on realistic FL scenarios, considering on both label and language heterogeneous. The experimental results show clearly that C2A can be robust to the heterogeneity of clients, thereby leading to the state-of-the-art results on diverse non-IID setups. In addition, our framework shows a significant enhancement in training efficiency across a range of downstream tasks. Finally, we demonstrate that our C2A successfully mitigates the large client drifts among local clients in non-IID scenarios. A summary of our main contributions is as follows:\n\u2022 We investigate the effectiveness of PEFT among various FL scenarios. To the best of our knowledge, our work is one of the few researches for adapting PEFT in FL.\n\u2022 We propose Client-Customized Adaptation (C2A), a novel hypernetwork-based framework that strengthens the robustness of adapter concerning FL heterogeneity.\n\u2022 We demonstrate that C2A works quite well on various non-IID scenarios while preserving the benefits of efficiency in PEFT."}, {"title": "2.1 Background of FL", "content": "The goal of federated learning is to collaboratively train a single global model without sharing any private data between clients. To this end, FL proceeds through the communication of training models between clients and the server in a round-by-round manner. For each round, the server first distributes a single global model \u03b8 to a set of sampled clients, participating clients then perform local optimization on their own data. Upon the completion of the optimization, the server again aggregates all locally-trained models to update the global model. Formally, let the dataset of the i-th client be $D_i$, the above process for updating the global model can be formulated as follows:\n$$\\theta = \\sum_{i=1}^K \\alpha_i \\cdot L(D_i; \\theta),$$\nwhere $L(D_i;\\theta)$ is the function that returns the trained model based on the given dataset and the initial model, K is the number of participating clients, and $\u03b1_i$ is the contributing factor of the client i to build a global model, which is typically determined by the dataset size of each client, i.e., $\u03b1_i = \\frac{|D_i|}{\\sum_l |D_l|}$.\nWhile there are various aggregation methods, we focus on FedAvg due to its wide applicability in the FL community (Karimireddy et al., 2020; Li et al., 2021; Luo et al., 2021).\nHowever, utilizing cumbersome PLMs for the communication process of FL poses two challenges. Firstly, the function L(\u00b7) requires high computing resources due to the large number of trainable parameters associated with PLMs. Secondly, in the aggregation step (i.e., weighted summation), significant network bandwidth is required to transmit and receive the models. Therefore, it is crucial to find an optimal solution that can mitigate these constraints, providing a more efficient and less resource-intensive mechanism for FL with PLMs."}, {"title": "2.2 Impact of Heterogeneity on PEFT", "content": "To verify the applicability of PEFT in federated context, we conduct a preliminary investigation in which only small components (e.g., adapters, prompt embeddings, biases) are fine-tuned on local data and subsequently shared between clients. The experimental configuration comprises 100 clients engaged in the task of multilingual news classification (Liang et al., 2020).\nWe first examine the robustness of PEFT on heterogeneous data distribution between clients, which is common in real-world scenarios. We report the test accuracy of the global model with respect to the increasing heterogeneity. The overall results are depicted in Figure 2(a). In the non-federated scenario (i.e., IID), the existing PEFT methods manage to achieve strong performances comparable to that of the full fine-tuning. However, as the level of heterogeneity increases, the performances of the PEFT methods significantly lag behind that of the full fine-tuning. This verifies that PEFT methods exhibit greater susceptibility to heterogeneity than full fine-tuning.\nTo gain a deeper understanding of the susceptibility, we further analyze the local optimization of the PEFT methods. Specifically, we measure the CKA similarity (Kornblith et al., 2019) of the logits between the training model and the global model on the IID and non-IID setups. Figure 2(b) shows the results. Comparing between IID and non-IID setups, all PEFT methods noticeably deviate from the global model on non-IID. This indicates that the model gradually converges to the client optima while drifting apart from the global model's optima, which are believed to be more generalized (Li et al., 2021). This observation aligns with prior results (Luo et al., 2021), and we suspect that such deviation attributes the slow and unstable convergence."}, {"title": "3 C2A: Client-Customized Adaptation", "content": "In this section, we elaborate on the proposed framework in detail. The core strategy is to generate customized PEFT modules tailored to each client to mitigate the negative impact of heterogeneity among clients. To achieve this, we first derive latent vectors to represent the data distribution of each client (Section 3.2). The resulting embeddings are then conditioned on the hypernetworks so as to generate parameters of the PEFT modules tailored to each client (Section 3.3). Regarding on the large number of parameters induced from hypernetworks, we effectively factorize the weights of the hypernetworks (Section 3.4)."}, {"title": "3.1 Adapter Architecture", "content": "We start with defining the structure of the PEFT modules to be generated. While lots of different modules have been proposed, we focus on Adapter (Houlsby et al., 2019), given its versatility across domains, such as vision-and-image (Sung et al., 2022) and audio (Hou et al., 2021), as well as its demonstrated efficacy in performing given tasks. The adapter consists of down- and up-projection functions that are interleaved between self-attention layers and feed-forward layers within every block of the PLMs. The adapting process can be formulated as:\n$$A^l(x) = U^lGeLU(D^lx) + x$$\nwhere $D^l \\in \\mathbb{R}^{r \\times d}$ and $U^l \\in \\mathbb{R}^{d \\times r}$ are the weights for the down- and up-projection in the l-th layer of PLMs, respectively, d is the hidden dimension of PLMs, and r is the bottleneck dimension."}, {"title": "3.2 Construction of Client Embeddings", "content": "To represent the characteristics of the clients, we consider two different types of information: 1) label embeddings and 2) context embeddings.\nLabel Embeddings The label embedding plays a role in conveying the explicit information of class distribution on each client. Since mini-batches are generally sampled by uniform distribution, the label distributions on mini-batches can sufficiently represent the data distributions of clients. Thus we construct label embeddings from the label distributions of the mini-batches. Let the mini-batches of the client i be $B \\subset D_i$, the label embeddings can be derived as follows:\n$$L(B) = W_Lavg([y_1; ...; y_{|B|}]) + b_L,$$\nwhere yi is a one-hot label vector for the instance $x_i$, [;] denotes the concatenating function, avg(\u00b7) denotes the average pooling within mini-batches, $W_L \\in \\mathbb{R}^{C \\times t}$ and $b_L \\in \\mathbb{R}^t$ are the linear transformation weights and biases for the number of classes C and t is the dimensionality of input embeddings. It is important to note that, since the labels for test data are not accessible, we opt for a uniform distribution for the inference phase to generate adapters that are not biased toward dominant classes.\nContext Embeddings Considering the contextual information in data can also provide an enhanced understanding of each client by taking a more comprehensive viewpoint (e.g., languages, text styles). Specifically, the contextual information is extracted from every layer to generate layer-specialized adapters. Inspired by the sentence embeddings (Li et al., 2020), context embeddings are extracted by averaging word vectors over the lengths with l2 normalization. Let the resulting vectors of the sample $x_i$ from the l-th layer of PLMs be $f^l(x_j)$, the context embeddings of the l-th layer are derived as follows:\n$$F'(B) = W_Fmax([f'(x_1); ...; f'(x_{|B|}]) + b_F,$$\nwhere max(\u00b7) denotes the max-pooling across the batch, and $W_F \\in \\mathbb{R}^{d \\times t}$ and $b_F \\in \\mathbb{R}^t$ are the linear transformation weights and biases, respectively.\nClient Embeddings The comprehensive client embeddings $I^l_i$ are constructed by summing up two types of embeddings. Additionally, we add layer-index embeddings into the client embeddings of each layer, further encouraging the generator to encode more diverse layer-wise information (Van Aken et al., 2019; de Vries et al., 2020)."}, {"title": "3.3 Client-conditional HyperNetworks", "content": "Based on the client embeddings, we tailor adapters to each heterogeneous client. Drawing inspiration from the concept of hypernetworks (Ha et al., 2017) that generates parameters based on given input embeddings, we introduce the client-conditional hypernetworks, which generate adapter parameters by taking the client embeddings $I^l_i$ as inputs. Formally, the parameters of the adapters (i.e., $U^l, D^l$) are generated by following the function of hypernetworks:\n$$(U^l_B, D'^l_B) := h(I^l_B) = (W_U, W_D)I'^l_B,$$\nwhere $I^l_i$ is the input embeddings with dimensionality t, $W_U \\in \\mathbb{R}^{(r \\times d) \\times t}$, $W_D \\in \\mathbb{R}^{(d \\times r) \\times t}$ are the weights for the hypernetworks. Note that the hypernetworks are shared between different layers with the layer-specific information that are encoded to the input embeddings."}, {"title": "3.4 Factorization of HyperNetworks", "content": "While customized adapters can be generated from the aforementioned hypernetworks, hypernetworks typically comprise a relatively large number of parameters. We thus factorize the proposed hypernetworks into two smaller weights. Moreover, the resultant matrices from the factorized components are l2 normalized, such that the generated parameters are not biased towards any of the local majority classes in the client's data distribution (Zhong et al., 2021). Formally, the up-projection weights in Eq. (5) are reconstructed by two factorized components as follows:\n$$U_B = W_U L_B = \\sigma(F_U S_U)I^l_B$$\nwhere $F_U \\in \\mathbb{R}^{d \\times s}$ and $S_U \\in \\mathbb{R}^{s \\times (r \\times t)}$ indicate the factorized components from $W_U$ with latent factor s, \u03c3(\u00b7) denotes the Frobenius normalization.\nFor factorization, the latent factor s plays a crucial role in determining the complexity and expressivity of the resulting adapters. To allow for a larger dimensionality of latent factors, the two projection weights are tied similarly as if the tied auto-encoder (Alain and Bengio, 2014), i.e., $D'^l_B = U^{l T}_B$. This strategy enables to halve the memory requirements without compromising the task accuracy."}, {"title": "3.5 Aggregation Phase for C2A", "content": "Upon the completion of the training phase on each client data, the respective trained models are transmitted back to the centralized server to update the global model (Eq. (1)). Considering that the training models for C2A are hypernetworks, each client sends the parameters associated with the hypernetworks and the layer-index embeddings to the server in order to update the global hypernetworks."}, {"title": "4 Evaluation", "content": "In this section, we evaluate the efficacy of our C2A on two realistic FL scenarios: 1) heterogeneity in label distributions, and 2) heterogeneity in both label and language distributions."}, {"title": "4.1 Datasets", "content": "To simulate the two challenging scenarios, we mainly consider two text classification datasets, 20Newsgroup (Lang, 1995) and XGLUE-NC (Liang et al., 2020), which have recently served as benchmarks for evaluating FL for NLP (Lin et al., 2022; Weller et al., 2022).\n20Newsgroup The dataset comprises 18,000 news posts that pertain to 20 distinct topics. Given its larger categorical space (i.e., 20 labels) than the typical sentiment analysis datasets, it is favored to the verification for the important factor of the label distribution heterogeneity scenarios.\nXGLUE-NC The dataset includes 10,000 posts written in multiple languages that pertain to 10 news categories. This diversity in languages adds an extra layer of complexity to the FL. The dataset comprises five languages: English, Spanish, French, German, and Russian. Furthermore, due to the varying categorical distribution between languages (e.g., the English dataset is skewed towards Sports, while the French dataset is skewed toward News), the distribution shifts among clients are naturally introduced to the dataset."}, {"title": "4.2 Non-IID Client Partitioning", "content": "Building upon the two datasets, we adopt two non-IID partitioning strategies to inject heterogeneity into the label and language distributions.\nLabel Distribution. Following the benchmark setup (Lin et al., 2022), we apply Dirichlet distribution Dir(\u03b2) to the datasets in reorganizing the data into the non-IID label distribution circumstance. The value \u03b2 controls the degree of non-IID, the smaller the \u03b2, the more likely the clients in holding examples from only one class. Thus, we eventually construct a FL dataset respecting the label heterogeneity scenarios.\nLanguage Distribution. Following the language setup in (Weller et al., 2022), we randomly divide clients into five distinct groups, with each group being exclusively dedicated to a specific language. Subsequently, we split the dataset of each language in the same manner with the strategy of non-IID label distribution, which is more challenging and not even being explored in previous works."}, {"title": "4.3 Federated Learning Setup", "content": "Baselines and Implementations Following the previous work (Lin et al., 2022), we use the uncased version of DistilBERT (Sanh et al., 2019) with 66M parameters. We compare C2A with six strong baselines, which include Adapter (Houlsby et al., 2019), LORA (Hu et al., 2022), Compacter (Mahabadi et al., 2021a), Prompt-tuning (Lester et al., 2021), BitFit (Zaken et al., 2022), and AdaMix (Wang et al., 2022), to encompass a broad range of PEFT methods. These modules are optimized by AdamW (Loshchilov and Hutter, 2019) with the searched learning rate ranging from {2e-4, 3e-4, 4e-4, 5e-4}.\nLocal Optimization and Aggregation We assign 100 clients for each dataset and randomly selected 25% of the clients to join the local optimization in each round. During the local optimization,"}, {"title": "4.4 Main Results", "content": "To thoroughly evaluate each baseline on various FL setups, we start from a non-federated setup and progressively increase the level of heterogeneity by manipulating \u03b2. The results are shown in Table 1 (20Newsgroup) and Table 2 (XGLUE-NC).\nThe proposed method, C2A, achieves the state-of-the-art performance for almost all setups. Specifically, despite that AdaMix uses multiple adapters for ensemble, our model improves the respective performance by 3% on both datasets. It is also noteworthy that while most PEFT approaches manage to achieve fair performance in non-FL scenarios, their performances significantly decrease as the degree of heterogeneity increases. In contrast, our C2A shows only marginal performance degradation even for high degree non-IID settings. Moreover, in the multilingual setting, C2A achieves a comparable performance to full fine-tuning. These results indicate that C2A is more resilient to heterogeneity in decentralized scenarios."}, {"title": "5 Further Analysis on C2A", "content": "In order to gain a deeper understanding of the benefits of C2A, we perform a series of analytical experiments utilizing XGLUE-NC with a value of \u03b2 = 0.5, which represents the most challenging setup within our experimentation."}, {"title": "5.1 Ablation Studies", "content": "We conduct ablation studies to explore the contributions brought by each component of C2A. Specifically, we focus on the effect of client embeddings, which are composed of label embedding (LE), context embedding (CE), and factorization. Detailed results are presented in Table 3.\nClient Embedding. We observe that omitting either of the embeddings does hurt the model performance. Notably, comparing \"w/o LE\" to \"w/o CE\", ablating context embedding leads to more significant performance degradation. We suspect this is because that context embedding can provide more discriminating information of each client through implicit representations, such as language types, and text styles. Moreover, removing all the embeddings shows the worst performance, which demonstrates that our C2A with the client embeddings can generate more suitable adapters for each client.\nFactorization. To examine the impact of factorization, we first compare it with the C2A results neglecting factorization. Despite using only half the parameters, our model achieves comparable performance as the model without factorization. In addition, we observe that omitting normalization significantly hurts performance. The results demonstrate that our normalization alleviates the performance drop by factorization."}, {"title": "5.2 Local Epochs vs. Communication Rounds", "content": "One of the crucial aspects in FL is communication efficiency. A simple way to achieve such efficiency is to reduce communication rounds while increasing local epochs. However, the increased local updates can result in greater susceptibility to client drifts (Li et al., 2021). Thus we examine the trade-off between local epochs and communication rounds, as shown in Figure 4. We compare C2A with three baselines under the same number of model updates (local epochs \u00d7 communication rounds). We observe that increasing the local epochs leads to worse performance due to the detrimental effect of client drift. Nevertheless, C2A clearly outperforms the other baselines in all settings. This further verifies the potency of C2A in mitigating the negative effects of the drift caused by excessive local updates, and shows that C2A can be efficiently trained with only a few rounds of communication."}, {"title": "5.3 Communication Cost for Target Accuracy", "content": "In FL scenarios, the communication between clients typically continues until the model attains a target accuracy or the allocated budgets are exhausted. As such, attaining the target accuracy with minimal communication rounds is crucial for reducing the total costs in practical FL. To analyze the baselines through the lens of such communication efficiency, we compare the number of required communications to reach the targeted performance for each baseline. The results are shown in Table 4. Our proposed C2A consistently performs the best over the baselines on all target accuracy. Specifically, C2A reaches the targeted performance approximately two times faster than the vanilla adapter. These results show that C2A engages fewer communication costs with less requirement on the parameters and communication rounds."}, {"title": "5.4 Scalability of C2A", "content": "We evaluate whether C2A can be scaled to larger PLMs. To this end, we adopt all PEFT baselines to XLM-ROBERTa with 278M parameters. The results are summarized in Table 5. We observe that our C2A still outperforms the baselines by a large margin. Specifically, our C2A achieves 3.1 points improvement compared with the adapter model. These results indicate that our approach can be well generalized to larger models."}, {"title": "5.5 Robustness to Client Drifts", "content": "In order to showcase the robustness of C2A in non-IID scenarios, we employ CKA similarity to quantify the drift from the global model. Figure 2 shows that C2A is superior to other baselines in effectively reducing client drift. This justifies our hypothesis that creating tailored modules for each client is more effective in non-IID scenarios compared to a one-size-fits-all approach in training a single module for all clients."}, {"title": "6 Related Work", "content": "Recent works on PEFT can be categorized into two lines of work: (1) tuning a subset of the existing parameters within the PLMs, including head fine-tuning (Lee et al., 2019), and bias tuning (Zaken et al., 2022), (2) tuning with a small amount of additional trainable parameters, such as adapters (Houlsby et al., 2019; Mahabadi et al., 2021a; Wang et al., 2022), prefix-tuning (Li and Liang, 2021), prompt-tuning (Lester et al., 2021), and low-rank adaption (Hu et al., 2022). Previous studies showed that PEFT achieves comparable performance compared to fine-tuning using only a small set of parameters. Given the advances brought by previous studies focused on centralized datasets, attention towards decentralized scenarios in FL remains under-explored. Yet, we discover that current PEFT approaches suffer from client drifts on non-IID setup, resulting in serious performance degradation in FL. Different from previous studies, we focus on improving the robustness of PEFT in decentralized scenarios by generating client-customized adapters."}, {"title": "6.2 Federated Learning for NLP", "content": "While much attention for FL has been focused on the field of computer vision, recent efforts have been done in applying FL to NLP tasks. For example, FedNLP (Lin et al., 2022) introduced benchmarks for evaluating FL methods and performed systematic analysis in the context of PLMs. Weller et al. (2022) examined FL in multilingual scenarios, where each client uses different languages. Similarly, several works attempted to extend the setting toward diverse tasks. For example, Chen et al. (2021) adopted FL for question answering, and Qin et al. (2021) proposed an aspect-based sentiment analysis method to enhance the performance under the restriction of data isolation. However, to the best of our knowledge, none of the prior works has been done on tackling the training complexity of FL on PLMs, which is directly related to the practicality."}, {"title": "6.3 Hypernetworks in PEFT", "content": "Prior studies have demonstrated that utilizing hypernetwork (Ha et al., 2017) is conducive to more efficient fine-tuning for PLMs in centralized scenarios. For instance, Hyperformer (Mahabadi et al., 2021b) and HyperPrompt (He et al., 2022) generated task-specific parameters by incorporating task-specific and layer-specific information on multi-task learning. Moreover, for multi-lingual learning, Hyper-X (\u00dcst\u00fcn et al., 2022) learned about the task and language-specific embeddings for generating adapters. While most previous works have been conducted for improving the efficiency of PEFT by utilizing the hypernetwork, they only focused on multi-task or multi-lingual situations. Instead, our work mitigates the client drifts issue of PEFT in federated scenarios by incorporating the data distributions of each client."}, {"title": "7 Conclusion", "content": "In this paper, we have observed significant performance degradation for typical PEFT approaches in decentralized scenarios. By carefully designed analysis, we have also shown that typical PEFT suffers from large client drifts, resulting in slow convergence and performance degradation. To address these issues, we have proposed C2A, a novel hypernetwork-based FL framework, which generates client-customized adapters by incorporating the data distribution of each client. Our experimental results show that C2A achieves state-of-the-art results in various decentralized scenarios. Moreover, we have verified that C2A successfully mitigates the large client drift problem among local clients in FL scenarios."}, {"title": "8 Limitations", "content": "While we show that C2A successfully improves the effectiveness and efficiency of PEFT in FL, we have mainly focused on improving the effectiveness of the vanilla adapter. However, it is an open question whether our framework can improve other PEFT approaches, such as prompt tuning(Lester et al., 2021), and LoRA (Hu et al., 2022). Although we didn't analyze whether our framework can generate parameters for alternative PEFT, one recent approach reveals that hypernetworks can generate parameters for various types of PEFT in multi-task learning (He et al., 2022; \u00dcst\u00fcn et al., 2022). Likewise, as C2A generates parameters with hypernetwork, we believe that C2A is highly expected to improve the performance of any alternative PEFT modules."}, {"title": "Ethics Statement", "content": "This study covers work that utilizes PLMs, which have a wide variety of positive applications, such as the application to summarization or language understanding. At the same time, there are a number of ethical concerns with PLMs in general, including concerns regarding the generation of biased or discriminative text (Bordia and Bowman, 2019), the leakage of private information from training data (Carlini et al., 2021), and the environmental impact of training or tuning them (Strubell et al., 2019).\nOur framework attempts to train PLMs with minimal changes made to their pre-existing parameters in FL scenarios. Our work is believed to bring some insights into the two ethical dimensions: privacy and environment. First, with respect to private information leakage, although our work has not addressed address the privacy issue in the pre-train process, our FL framework can mitigate the data privacy issues in the fine-tuning stages. In addition, with respect to environmental impact, our work may obviate the need for full fine-tuning, which may also significantly reduce the cost in terms of memory or deployed servers."}, {"title": "Supplementary Appendix", "content": "We analyze the effect with varied dimensions of the client embeddings and factorization in C2A. The detailed results are presented in Figure 5.\nEffect of dimensions for client embeddings. To investigate the effect of dimensions for client embeddings, we investigate the number of dimensions in C2A ranging from 1,4,8, and 32, during training. The results are shown in Figure 5(a). We observe that using a larger dimension of embeddings for adapters improves the training efficiency. Specifically, the model using eight dimensions shows the best performance. Thereby, we adopt a client embedding size of 8 in all our models.\nEffect of dimensions for factorization. Figure 5(b) represents the impact of latent dimensions for adapters in C2A. The dimension of factorization size 64 appears to be the best. Based on these results, we use an embedding size of 64 in all our models.\nB Implementation details for C2A\nWe implement C2A in Pytorch using four RTX 3090 GPUs for experiments with detailed hyperparameter configurations as follows. We set the dimensionality of latent factors to s = 64 and client embeddings size of eight in all our models. Besides, for the low-rank dimension, we use a dimension of 16. We report the average results for all models of four random fine-tunings."}]}