{"title": "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos", "authors": ["Sharath Girish", "Tianye Li", "Abhinav Shrivastava", "David Luebke", "Amrita Mazumdar", "Shalini De Mello"], "abstract": "Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored. It requires incremental on-the-fly updates to a volumetric representation, fast training and rendering to satisfy real-time constraints and a small memory footprint for efficient transmission. If achieved, it can enhance user experience by enabling novel applications, e.g., 3D video conferencing and live volumetric video broadcast, among others. In this work, we propose a novel frame-work for Quantized and Efficient ENcoding (QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residu-als between consecutive frames at each time-step without imposing any structural constraints on them, allowing for high quality reconstruction and generalizability. To efficiently store the residuals, we further propose a quantization-sparsity frame-work, which contains a learned latent-decoder for effectively quantizing attribute residuals other than Gaussian positions and a learned gating module to sparsify position residuals. We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene. It acts as a guide for effective sparsity learning and speeds up training. On diverse FVV benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all metrics. Notably, for several highly dynamic scenes, it reduces the model size to just 0.7 MB per frame while training in under 5 sec and rendering at ~350 FPS.", "sections": [{"title": "Introduction", "content": "The dynamic world that we perceive around us is not 2D, but rather 3D. Unlike 2D videos, which are ubiquitous, the question of how to effectively capture, encode and disseminate free-viewpoint videos (FVV) of dynamic 3D scenes, which can be viewed at any instance of time and from any viewpoint, has intrigued computer vision and graphics researchers for much time. Free-viewpoint video transmission, if achieved, has the potential to transform and enrich user experience in profound ways by offering novel immersive experiences, e.g., FVV video playback and live streaming, 3D video conferencing and telepresence, gaming, virtual spatial tutoring and teleoperation, among others.\nThe underlying problem of reconstructing FVV involves learning a 6D plenoptic function of a dynamic scene P(x, d, t) from sparse multiple views acquired over a window of time, with x \u2208 R\u00b3 being a position in 3D space, d = (0, $) a viewing direction and t an instance of time. Neural volumetric representations, which learn a 5D plenoptic function of a scene P(x, d) at a fixed time instance, e.g.,"}, {"title": "Related Work", "content": ""}, {"title": "Traditional Free-viewpoint Video", "content": "Ever since early FVV work such as [27], a series of geometry-based FVV methods [12, 57] has been pushing for high reconstruction quality and streamable performance. However, their rendering and compression quality rely on the accuracy of a sophisticated pipeline of geometry reconstruction [20, 28], tracking [36], and texturing [61]. They also require high-end hardware for capturing complex and dynamic appearance [6, 14, 26]. Purely image-based rendering [10, 13, 35, 53] relaxes the requirement for geometric accuracy. Although methods such as [4, 95] support view interpolation with layered representations in the dynamic setting, they require a high count of views as input to ensure interpolation quality."}, {"title": "Neural and Gaussian-based Free-viewpoint Video", "content": ""}, {"title": "Offline Methods.", "content": "Compared to the traditional representations, the emergence of neural rep-resentations [47, 54, 86, 67, 72] opened a new door for capturing FVV for dynamic hu-mans [21, 31, 32, 39, 63, 82, 94] and monocular videos [22, 43, 44, 73, 75, 85]. In this work, we focus on general dynamic scenes [92] from multiple views to push the quality of streamable FVV without requiring a strong human prior [40, 48] or a very constrained input. [16, 62, 74, 80] model the scene dynamics via explicit deformation. Although suitable for motion analysis, they inevitably face a trade-off between motion accuracy and visual quality [74]. To tackle this, [41, 52, 59] use a spatial-temporal formulation via time-conditioned latent codes to implicitly encode the 4D scene, enabling reconstruction of topological changes and volumetric effects. [5, 19, 66] factorize the 4D scene into multiple space-time feature planes and achieves higher model compactness and training efficiency. [68, 76] decompose the 4D scene into static and dynamic volumes. [1, 46, 77, 87, 93] incorporate efficient NeRF representations [9, 19, 91] for higher fidelity. Although, these NeRF-based method achieve high compactness, they suffer from low rendering efficiency, even when converted [68] to a more efficient NeRF formulation [9, 55]. Seeing their great potential for efficiency, recent works extend 3D Gaussian representations [29] to dynamic scenes, with temporal attributes [42, 89], generalized 4D Gaussians [88] and a hybrid representation [83]. While these methods achieve high quality in modeling 3D dynamic scenes, they, together with the aforementioned NeRF-based methods, are mostly offline, i.e., they require all the input video and a long time for training, which is inherently difficult for streaming applications."}, {"title": "Online Methods.", "content": "Online reconstruction for FVVs is relatively under-explored, as it imposes addi-tional challenges of on-the-fly reconstruction using only local temporal information instead of the full recordings. Furthermore, toward the goal of streamable FVVs, the encoding system is evaluated by multiple metrics including compression rate, encoding and rendering speed and visual quality. [50] tracks dense 3D Gaussians by solving their motion over time. Visual quality and dynamic appearance is not their focus. [23] models motion by rendering scene dynamics, however their method is not optimized for efficiency. [45] focuses on generalizable NeRF reconstruction and shows good promise to adapt to a new frame but has a high memory footprint due to an MVSNet-style neural network [8, 90]. [37] accelerates training and rendering speed with a special tuning strategy and sparse voxels, however, their representation still has high temporal redundancy. [81] proposes an incremental training scheme with natural information partitioning and achieves high compression, but its encoding is slow. Several works [78, 79, 84] use video codec-inspired encoding paradigms for data efficiency. [78] achieves a decent compression rate and near interactive rendering with compact motion and residual grids. However, their training requires 10 minutes per frame. [79] focuses on real-time decoding, streaming and rendering instead of on-the-fly encoding. [84] performs grouped training on a hybrid representation of triplanes and volume grid. While achieving high compression rate, their fixed encoding paradigm and aggressive quantization limits their reconstruction quality along-with low rendering speeds. [69] is the closest work to ours for streaming FVV via 3D-GS. They encode the position and rotation residuals via an Instant-NGP [55] based transformation cache. While achieving faster training and rendering speeds than prior work, they have high data redundancy due to a fixed structured modeling. Additionally, they focus on geometric transformations only and"}, {"title": "QUEEN: Quantized Efficient Encoding for Streaming FVV", "content": "A solution for streamable FVV must have low-latency encoding (training) and decoding (rendering), and low data bandwidth (memory) for transmission on a common network infrastructure. Motivated by these constraints, we aim to generate streamable FVVs with compact representations that are fast to train and render incrementally. In this section, we first provide an overview of 3D-GS (Sec. 3.1). In Sec. 3.2, we propose a compression framework to efficiently represent and train Gaussian attribute residuals at each time step. Sec. 3.3 discusses utilizing an approach based on viewspace gradient differences to achieve greater efficiencies. An overview of our method is shown in Fig. 1."}, {"title": "Preliminary: 3D Gaussian Splatting", "content": "Our efficient representation for dynamic scenes is based on 3D Gaussian Splatting (3DGS) [29]. Given multi-view images I, a 3D scene is modeled by a set of Gaussians with attributes A.\nThe shape of each Gaussian i is defined by its mean p\u2081 \u2208 R\u00b3 and covariance matrix \u03a3\u2081. The covariance matrix is represented by \u03a3\u2081 = R\u2081S\u00bfS\u00bfR, where R\u2081 is a rotation matrix parameterized by a quaternion vector qi \u2208 R4, and the scale matrix S\u00bf is a diagonal matrix with elements s\u00bf \u2208 R\u00b3. Each Gaussian also contains opacity 0\u2081 \u2208 [0,1] and spherical harmonic coefficients hi for view-dependent appearance with dimensions based on the number of degrees.\nFor rasterization, 3D Gaussians are projected into 2D Gaussians for any given view. Given a camera with intrinsic matrix K and viewing transform W, the 2D mean and covariance are:\nRepresentation.\nRendering."}, {"title": "Attribute Residual Compression", "content": "T-1\nGiven a multi-view image sequence {It}f=01, our goal is to reconstruct the dynamic scene via Gaussian attributes At for each time-step t. We model the attributes based on the trained attributes from the previous time-step t -1 as\n where Rt consists of learnable residuals for each attribute (in Fig. 1 (gray block)). For time-step t = 0, we perform vanilla Gaussian splatting training to obtain attributes A0. This sequential formulation allows us to freely and adaptively update the residuals Rt on-the-fly with incoming streaming training views, without any structural constraints as in prior works [69]. However, representing the 4D scene with uncompressed residuals is still highly inefficient. As residuals have low magnitudes in comparison with the attributes themselves, they can be efficiently compressed, for which we propose a novel quantization-sparsity framework.", "latex": ["At = At-1 + Rt,"]}, {"title": "Attribute Residual Quantization", "content": "There exists spatial redundancy within the Gaussian attributes of the same time-step. Nearby Gaussians have highly correlated residuals for shape, orientation and appearance. To reduce the storage cost of the residuals, we propose to utilize a quantization framework during training [24].\nAt each time-step t, we represent the residuals via quantized latents and a shared compact decoder. Specifically, to obtain the residuals for each category\u00b2 ri \u2208 RM, we maintain corresponding quantized integer latents l\u00bf \u2208 Z\u00b9 for each Gaussian i. These latents are passed through a shared linear decoder D with learnable parameters D \u2208 RM\u00d7L to obtain the decoded attribute residual ri. Such a compact decoder has small time and memory costs due to few parameters and arithmetic operations. To allow differentiable training of the integer latents via gradient optimization, we use a continuous approximation \u00ce\u00bf \u2208 RL instead. \u00ce\u00bf are rounded to the nearest integer values for the forward pass but can still receive backpropagated gradients via the Straight-Through Estimator (STE) [3]:", "latex": ["1\u2081 = STE(\u00ce\u00bf), ri = D(li; D) = D. float(1)."]}, {"title": "Position Residual Gating", "content": "Sparse Representation. While most of the attribute residuals can be quantized effectively with our proposed method in Sec 3.2.1, we observe that the position residuals are sensitive to quantization and require high precision during rendering\u00b3. Storing all the full-precision position residuals, however, still results in high per-frame memory costs. To tackle this, we propose a learned gating methodology, which enforces sparsity in the residuals instead of quantization. This mechanism allows us to set a vast majority of the position residuals to zeros, while maintaining full-precision non-zero values. Specifically, we represent the positional residual for each Gaussian i as Api = gilpi, where the scalar gi is the learnable gate variable and Ip\u2081 \u2208 R\u00b3 is the learnable pre-gated residual in full precision during training. After training, the sparse Ap\u2081 can be efficiently stored via sparse matrix formats [60] to reduce memory costs. Thus, our goal is to encourage the sparsity for the variable gi across all", "latex": ["Api = gilpi"]}, {"title": "Viewspace Gradient Difference for Adaptive Training", "content": "Real-world dynamic scenes con-tain high amounts of temporal redundancy with only a fraction of the content changing between consecutive time-steps. The proposed quantization-sparsity framework can learn to iden-tify Gaussians corresponding to static scene content and set their residuals to 0. How-ever, they still forward/backward pass through static regions result-ing in wasted training computa-tion. Additionally, initializing the gates with 1s requires more iterations for convergence. We thus propose a proxy metric to identify Gaussians, which are static or dynamic at the start of training. We use this metric to initialize our gates while also identifying dynamic image regions to perform local rendering in, during training.\nThe ground-truth (GT) training images contain information of the dynamic scene content, which we leverage to separate static and dynamic Gaussians. A simple pixel difference between consecutive frames does not account for illumination changes and is a noisy signal for the geometric position residuals. 3D-GS utilizes 2D viewspace gradients\nViewspace Gradient Difference.\nL to identify poorly fitted Gaussians based on the reconstruction loss L between the rendered (\u00ce), GT image (I).\n\u0434\u0440\nMore concretely, after training Gaussians at time-step t - 1 with an MSE loss, Lt\u22121 and 2D Gaussian means p't-1, we compute the MSE loss for the next time-step Lt and then compute the gradient difference. The score vector dt is the average of gradient differences across all training views v:"}, {"title": "Efficient End-to-end Learnable Residuals", "content": "For an incrementally updating online approach, it is important to make sure the initial frame is well reconstructed. COLMAP used to initialize the positions of Gaussians can result in sparse 3D points for regions with sparse camera views. Hence, we use an off-the-shelf monocular depth estimation network to estimate point locations in these empty regions and predict a more complete initial point cloud. Further details and results are in the supplementary.\nWe train separate decoders and quantized latents {Lc, Dc|c\u2208 {q, s, o, h}} for all attributes except position. For position, we learn the gate parameters a and positional residuals Lp. All variables are end-to-end differentiable. The total loss function that we minimize is the reconstruction loss (Eq. 3) and the sparsity gate regularization loss (Eq. 8):\nInitial Frame Reconstruction.\nEnd-to-end Training."}, {"title": "Experiments", "content": "We evaluate our method on two challenging FVV video datasets. (1) Neural 3D Videos (N3DV) [41] consists of six indoor scenes with forward-facing 20-view videos. (2) Immersive Videos [4] consists of seven indoor and outdoor scenes captures with 46 cameras. In both datasets, the central view is held out for testing. We implement QUEEN on [29]. We train for 500 and 350 epochs for the first time-step, and for 10 and 15 epochs for the subsequent time-steps, for N3DV and Immersive, respectively, on an NVIDIA A100 GPU. One epoch contains all training views. We evaluate visual quality in terms of average frame-wise PSNR, SSIM, and LPIPS (VGG) across all videos. We also compute the average storage size and training time for each time-step, and the rendering speed. Additional details are provided in the supplementary materials."}, {"title": "Quantitative Comparisons", "content": "We compare QUEEN against state-of-the-art existing online FVV methods (3DGStream [69], StreamRF [37] and TeTriRF [84]) on N3DV and Immersive (Tab. 1). 3DGStream [69] is the overall best-performing prior method. Since 3DGStream [69] was originally run on an older NVIDIA V100 GPU on N3DV, we re-run 3DGStream on an NVIDIA A100 GPU on both N3DV and Immersive and denote it as 3DGStream* in Tab. 1 for consistency with QUEEN. For brevity, in Tab. 1 we additionally compare against only selected top-performing offline FVV methods. We include a more extensive comparison to all existing offline FVV methods in the supplementary (Tab. 8). Lastly, we evaluate three variants of QUEEN: QUEEN-s (small), QUEEN-m (medium) and QUEEN-1 (large), with residuals trained for 6, 8 and 10 epochs, respectively."}, {"title": "Qualitative Comparisons", "content": "In Fig. 3 we compare the reconstruction results of the various methods. On N3DV, we reconstruct finer details than 3DGStream, e.g., the hand and the dog, and minimize artifacts such as the tongs in the top scene or the coffee and metal tumbler in the bottom scene. TeTriRF produces blurry outputs, e.g., the cap or metal tumbler in the bottom scene. On Immersive, we better model illumination changes and new scene content such as the person (first patch) and the flame (third patch) in the top scene or the face in the bottom scene (second patch) versus 3DGStream."}, {"title": "Ablations", "content": ""}, {"title": "Effect of Updating Appearance Attributes.", "content": "To ablate the importance of updating the Gaussian appearance attributes (color and opacity) per frame in QUEEN, we run experiments on N3DV for 2 settings: (1) learning only geometric attribute residuals (position, scale and covariance) with appearance residuals set to zero and (2) learning all residuals per frame (Tab. 3). Updating only geometric attributes results in a drop of 0.4 dB PSNR versus updating all attributes. Visually, for the Flame Steak scene in N3DV (Fig. 4), updating all attributes results in the highest quality, while fixing opacity introduces artifacts at the edge of the flamethrower. Fixing color, additionally, results in a significant drop in PSNR (-0.6 dB) producing a discolored flame (rightmost column)."}, {"title": "Effect of Attribute Compression and Masked Training.", "content": "We show results for five variants of QUEEN with incrementally added sub-components: (1) a baseline with uncompressed residual training (Sec. 3.2), (2) adding quantization to all attributes except position (Sec. 3.2.1), (3) adding gating of position residuals (Sec. 3.2.2), (4) gate initialization with viewspace gradient differences and (5) masked image training (Sec. 3.3). Results are summarized in Tab. 2 for both N3DV and Immersive datasets. Compressing attributes and gating position residuals results in significant model size reduction on both datasets (60\u00d7,40\u00d7). This is further reduced by gate initialization with"}, {"title": "Conclusion", "content": "We proposed QUEEN, a framework to model 3D dynamic scenes for online FVV using 3D-GS. We utilized an attribute residual framework, which freely updates all parameters leading to better modeling of complex scenes. We show that the residuals can be successfully compressed via our learned quantization-sparsity mechanism, which adapts to the dynamic scene content to achieve very small model sizes, improved training and rendering speeds, and improved visual quality. In future work, we aim to extend QUEEN for sparse view reconstruction or sequences with long duration."}, {"title": "Appendix", "content": "We provide supplementary results (Appendix A), additional implementation details (Appendix B), discussion on limitations and future work (Appendix C) and the broader impact (Appendix D) of our approach. We recommend the reader to watch the supplementary video hosted on our project website: https://research.nvidia.com/labs/amri/projects/queen for a visual comparison of the results of the various methods as well as more details of this project."}, {"title": "Supplementary Results", "content": ""}, {"title": "Quantization vs. Gating", "content": "We evaluate the effect of the gating framework in comparison to the quantization framework for the position residuals. We perform gating or quantization on the position residuals while quantizing all other attributes. Results, averaged on the N3DV dataset, are shown in Figure 7. We vary training epochs per frame from 6 to 15 for gating and 6 to 25 for quantization to obtain trade-off curves. Increased training epochs result in higher reconstruction quality (PSNR) but longer training time or a larger model size. The left figure shows the PSNR versus size tradeoff while the right figure shows PSNR versus training time tradeoff. We see that, in both cases, the gating framework produces much better tradeoff curves than quantization, with PSNR values more than 0.2dB higher at similar sizes. When increasing the number of training iterations, quantization still improves in quality albeit at a slower rate requiring more training iterations for convergence. This demonstrates that position attributes are more sensitive to quantization errors and require full precision. It justifies our choice of learning to sparsify them as opposed to quantizing them. However, this does not translate to the other geometric attributes, scaling and rotation, where quantization is sufficient in compressing the attributes. This is seen in the results in Table 5 on the Exhibit scene from the Immersive dataset. Quantizing both rotation and scaling results in the lowest storage memory per frame at a similar PSNR and slightly higher training time."}, {"title": "Accuracy-memory Tradeoff", "content": "We consider the effect of varying different loss coefficients to trade off between accuracy and memory. In Figure 7 we explore the tradeoff between PSNR and size by varying the number of training iterations. We can also control the amount of sparsity in the scene by varying the Areg loss coefficient. As visualized in Fig. 8 (b), we find that increasing Areg leads to higher sparsity or lower memory but also lower reconstruction quality.\nWe further experiment with an additional regularization loss to reduce the entropy of the latents. We observe that lower entropy corresponds to lower memory, but also lower reconstruction quality. While learnable probability models can successfully reduce entropy, as shown by [25], these models have higher time and memory costs during training. We instead observe that the probability distribution of the various attribute residuals at each time-step is unimodal and is close to a Laplacian or Gaussian distribution. As a unimodal distribution has entropy proportional to the variance [11], we enforce a loss on the standard deviation of the latents with a tradeoff parameter Astd controlling the effect of this regularization loss. Fig. 8(a) shows results on the N3DV dataset by varying Astd. We observe that increasing Astd reduces the entropy costs, leading to lower memory costs, but lower reconstruction quality, and vice versa."}, {"title": "Effect of Quantization Latent Dimension", "content": "We provide additional analysis on the effect of latent dimension for the various attributes in Table 6. In general, latent dimension does not have a significant effect on reconstruction quality or model size. Increasing the latent dimension can lead to lower per-dimension entropy due to our learnable quantization framework and hence still maintains the overall total size for the latent. We find that varying the total number of iterations (Appendix A.1) or the entropy loss/variance coefficient (Appendix A.2) are more effective knobs for trading off between quality-memory or quality-time."}, {"title": "Adaptive Sizes.", "content": "Our quantization-sparsity framework automatically allocates more memory for frames with larger scene changes (higher frame difference). Notice that the frame difference spikes in the Immersive scenes (bottom row) correlate with frame-wise model size, which increases to allow for modeling of more temporal variations."}, {"title": "Framewise PSNR and Size", "content": "A key advantage of our quantization-sparsity framework is its adaptability to scene content. We visualize the per frame sizes for 2 scenes each from N3DV and Immersive along with the visual frame difference between consecutive frames in Figure 9. We see that our approach allocates variable model sizes for each frame unlike 3DGStream [69], which uses a fixed-sized InstantNGP structure. Additionally, higher frame differences result in larger model sizes and vice versa, as seen in the top row. This shows that our method is capable of allocating more bits to frames with large scene changes. This is especially evident from the spikes in Immersive's scenes in the bottom row, which correlate with the model size.\nNext, we show the stability of our approach at recovering from large scene variations corresponding to the frame difference spikes as mentioned above. We visualize the reconstructed test-view PSNR for each frame, for 2 scenes each from N3DV and Immersive, along with the frame difference between consecutive frames in Figure 10. A large L1 error such as around frames 175 (top left), frames 225 (top right), frames 75 (bottom left) or frames 90 and 290 (bottom right) does lead to drops in PSNR. However, our PSNR recovers in subsequent frames showing the stability of our framework with large scene variations present."}, {"title": "Effect of Improved Point Cloud Initialization.", "content": "Consistent geometry for the 3D scene in the first frame is important to learn accurate residuals for the attributes of the subsequent frames. The COLMAP-generated point cloud initialization can be incomplete for regions that are textureless or are not sufficiently captured in multiple cameras. This is visualized in the top row of Figure 11. The boundaries of the scene consist of limited training view cameras as shown by the white box leading to sparse or no points in these regions by COLMAP initialization. The densification stage in 3DGS is unable to recover from this producing erroneous rendered depth or geometry and also leads to low quality image reconstruction."}, {"title": "NeurIPS Paper Checklist", "content": ""}], "latex": ["p = I(\u0440\u2081; K, W), = JWE;WTJT,", "\u0109(x) = \u2211ciai (1 \u2013 5),", "Ai = exp(-x--x-pi)),", "L = ALD-SSIM + (1 \u2212 )L1.", "At = At-1 + Rt,", "1\u2081 = STE(\u00ce\u00bf), ri = D(li; D) = D. float(1).", "Api = gilpi", "\u011di = Sigmoid(logai/r),", "\u011fi = \u011di (1 - Yo) + Yo, gi = min(1, max(0, \u011fi)).", "Lreg = \u2211 Sigmoid (log ai (10g", "dt = ( ( ( (,), = L(), =). (", "pi = |dt|+ median_(|dt|)", "Ltotal = L + AregLreg,", "p = \u03a0(\u0440\u2081; \u041a, W),"]}