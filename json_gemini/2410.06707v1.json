{"title": "Calibrating Verbalized Probabilities for Large Language Models", "authors": ["Cheng Wang", "Gyuri Szarvas", "Georges Balazs", "Pavel Danchenko", "Patrick Ernst"], "abstract": "Calibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the \"logit\" by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.", "sections": [{"title": "1 Introduction", "content": "Large Language models (LLMs), such as GPT-4, Claude\u00b9, Mistral have demonstrated to be versatile tools with strong capabilities across many natural language processing (NLP) tasks. Their performance is grounded on very large numbers of parameters and the refinement to align more closely with human preferences using reinforcement learning from human feedback (RLHF;), direct preference optimization (DPO;), etc. This provides LLMs with the flexibility to adapt to new tasks through natural language instructions (prompts) and contextual data (in-context learning) without requiring further training or parameter updates.\nWhile these LLMs achieved impressive performance on a wide range of tasks, recent studies suggest that the RLHF fine-tuned LLMs are poorly calibrated (Tian et al., 2023; Zheng et al., 2023; Zhao et al., 2021; Xiong et al., 2024). This prevents a reliable assessment of the confidences of LLM outputs, which is a key requirement in risk-sensitive domains such as medicine (Penso et al., 2024) and finance (Bahnsen et al., 2014), where one wants to prefer high-confidence decisions and avoid low-confidence actions. Unfortunately, calibrating LLMs is challenging, particularly for black-box LLMs, which only provides closed-source APIs and thus users have no access to model output such as token-level probabilities.\nRecently, Tian et al. (2023) proposed prompting large language models (LLMs) to directly generate prediction confidence scores, a concept they refer to as verbalized confidence (Lin et al., 2022). In this approach, the language model is instructed to respond with both the predicted label and a numeric confidence score within the range of [0, 1], or with a natural language expression indicating confidence, such as \"highly likely.\" This procedure is known as confidence elicitation.\nhttps://www.anthropic.com/claude"}, {"title": "2 Methods", "content": "We describe our proposed approach to calibrate LLMs generated verbalised probability for discriminative tasks. First, we introduce a prompt for generating probability distributions over categorical labels; Then, we further identify the issue of applying post-hoc calibration method \u2013 temperature scaling (TS) \u2013 to verbalised probability, both theoretically and empirically. Last, we demonstrate the approach of estimating logits from the verbalised probability."}, {"title": "2.1 Prompting to Generate Probability Distributions", "content": "We used the following prompt template to explicitly ask LLMs to output probability distributions over binary labels. We adapted the basic prompt structure from (Tian et al., 2023) and tried to guide the LLM to reach our goal step by step (Xiong et al., 2024). This empirically provides better performance in terms of both accuracy and more structured responses. We started with an easier step by asking the LLM to give a label, then asked it to consider the confidence of this label, and finally to give the full probability distribution over labels. This creates an order of incremental difficulty, where each step builds on the previous one. Lastly, we added constraints to ask the LLM to output only the probability (the part we are interested in). See appendices A.1 and A.2 for the exhaustive list of prompts we used."}, {"title": "2.2 Temperature Scaling with Verbalized Probability", "content": "Temperature Scaling (TS; Guo et al., 2017) is one of the most representative post-hoc calibration methods. It uses a temperature parameter \\(\\tau\\) to calibrate the softmax probability:\n\\(p_i = \\frac{exp(z_i / \\tau)}{\\sum_{j=1}^k exp(z_j / \\tau)}, i \\in [1...k] \\)\nwhere \\(\\tau > 0\\) is used as a scaling factor that is applied to unnormalised activation value (logits) \\(z_i, i \\in [1...k]\\). TS controls the model's confidence by adjusting the sharpness of distribution so that the model prediction is not too certain (over-confident) or too uncertain (underconfident).\nIn the context of black-box LLMs, access to logits and raw probabilities is unavailable. While LLMs can generate verbalized probabilities, requesting them to produce logits is unrealistic due to the unbounded nature of logits. For discriminative task, we argue that directly performing TS on LLM generated verbalized probabilities lead to re-softmaxing.\nDefinition 1 Re-softmaxing is a sequential operation of applying SOFTMAX function twice on the same data \\(z = \\{z_1, ..., z_i, ...z_K\\}\\), i.e., \\(p = SOFTMAX(z)\\), \\(q = SOFTMAX(p)\\).\nLet's first understand the effects of re-applying the softmax function (i.e. re-softmaxing) towards temperature scaling (TS)-based calibration\nProposition 1 Given a categorical probability distribution \\(p = \\{p_1, ..., p_i, ..., p_K\\}\\) over K (K > 1) classes, and a re-softmaxed probability distribution \\(q = SOFTMAX(p) = \\{q_1, ..., q_i, ..., q_K\\}\\), for \\(i \\in [1, K]\\), the \\(q_i\\) is bounded: \\(0 < \\frac{1}{K-1+e} \\le q_i < \\frac{e}{K-1+e} < 1\\)\nProposition 2 In the context of calibrating verbalized probability using temperature scaling (TS), let \\(T_p\\) and \\(T_q\\) represent the optimal temperature values for calibrating p and q respectively. It follows that \\(T_q < T_p\\)."}, {"title": "2.3 Invert Softmax Trick", "content": "The standard softmax takes activation value (logits) \\(z = \\{z_1, z_2, ..., z_k\\}\\) as input and returns a probability distribution \\(p = \\{p_1,p_2,\\dots,p_K\\}\\) over K classes:\n\\(SOFTMAX(z_i) = p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\) (2)\nFor a given softmax probability distribution p, reverting eq.(2) to obtain exact raw logits is not possible because the softmax is many-to-one (non-bijective) function. Thus we propose to obtain a logit proxy by inverting the softmax probability (Smith et al., 2017; Artetxe et al., 2018; Balanya et al., 2024). Here, we follow the implementation in (Pav, 2022):\n\\(INV\\_SOFTMAX(p_i) = z_i = log p_i + c\\) (3)\n\\(s.t. c = -\\frac{1}{K}\\sum_{i} log p_i \\) (4)\nwhere c is a constant scalar. In practice, we found that the selection of c doesn't affect any calibration metrics (that is, we set c = 0 or c = 1 and obtained the same results). Then calibration can be performed by scaling the logit proxies:\n\\(p = SOFTMAX(INV\\_SOFTMAX(p)/\\tau) \\) (5)\nwhere \\(p\\) is a calibrated probability from \\(p\\). \\(\\tau\\) is tuned on the validation set by minimizing the Negative-Log Likelihood(NLL) or Expected Calibration Error(ECE).\nDiscussion. The LLM generated probability distribution over classes \\(\\{p_1, p_2, \\dots, p_K\\}\\) is supposed to be subject to \\(\\sum_{i}^{K} p_i = 1\\). In practice, depending on individual capability, as shown in Section 3.2, some LLMs fail to produce probability distributions that sum to 1, which leads to unreliable probability calibrations. Equation (5) brings the additional advantage of re-normalizing the inverted probabilities (logits) before performing calibration."}, {"title": "3 Experiments", "content": "Our experiments focus on multi-class classification task in a zero-shot learning setup. This is a general setup where we prompt LLMs without any fine-tuning steps involved. More specifically, we design experiments to accomplish the following objectives:\n\u2022 Examine the capability of LLMs in generating probability distributions on discriminative tasks in a zero-shot inference setup.\n\u2022 Assess the effectiveness of applying temperature scaling with the invert softmax trick when calibrating LLMs with temperature scaling on verbalised probabilities.\n\u2022 Understand the properties of calibrated verbalised probability in controlling Precision-Recall threshold.\nDatasets. In our experiments we used three text classification datasets that are available for research purposes and publicly hosted on Huggingface:"}, {"title": "3.1 Improved calibration with temperature scaling on the estimated logits", "content": "Table 3 presents the zero-shot performance of Claude (v2, v3) and Mixtral on the test sets. It demonstrates that calibrating logit proxy with TS improves calibration performance as compared to uncalibrated. This can be seen from the reduced ECE scores as well as the confidence histogram plots (the 2nd and 4th column) in Figure 4. The TS reduces the gap between average confidence and accuracy. In generating probability distribution, we experimented with different token temperature (T = 0 and T = 1) in configuration. We observed that T = 0 generally gives better accuracy as well as calibration scores across models and datasets."}, {"title": "3.2 Black-box LLMs have strong capability in generating probability distribution", "content": "To examine how LLMs understand confidence scores, which is supposed to \u201csoftmax probability\" and the capability of LLMs to generate probability distributions, we define a success criteria by computing the percentage of generating a probability distribution which sums to 1."}, {"title": "3.3 Claude outperforms Mixtral on both predictive and calibration performance", "content": "To make a comparison across LLMs, we used the aggregated dataset as described in Table 2 (bottom). Table 4 presents both predictive and calibration performance. On IMDB datasets, the three LLMs gives similar accuracy, v3 achieves the best ECE score. On other two datasets, we see the performance drops of Claude-v2 and Mixtral, particularly, the latter has roughly 3.6% and 10.8% lower accuracy as compared to Claude-v3. In terms of calibration performance, Mixtral also shows worse performance to Claude-v2 and v3."}, {"title": "3.4 Ablation study of generating probability with 2 decimal is beneficial", "content": "Table 5 presents the performance comparison. We can observe that, in general, the generated probability with 2 decimal is beneficial. Though it gives bit worse NLL and ECE, MCE score is largely improved. The Figure 5 (a) reports the probability difference, >95% samples have probability difference <0.2. However, we can clearly see better distributed probability of using 2 decimal, which leads to smoother Precision-Recall curve as shown in Figure 5 (b) and (c))."}, {"title": "4 Related Work", "content": "Calibration with accessible probabilities. Guo et al. (2017) and Mukhoti et al. (2020) showed that the miscalibration of large modern neural networks (NNs) is related to overfitting on the likelihood of the training dataset. To tackle this, many methods have been proposed such as focal loss (Lin et al., 2017), which acts as a maximum entropy regularizer (Mukhoti et al., 2020). Label smoothing (M\u00fcller et al., 2019), Bayesian approach (Maddox et al., 2019), meta-learning (Bohdal et al., 2021), the Gumbel-softmax Trick (Jang et al., 2017; Wang et al., 2021; Pei et al., 2022), and kernel-based methods (Kumar et al., 2018). The recent survey summarizes these approaches (Wang, 2023).\nCalibration with inaccessible probabilities. For closed-source LLMs, the aforementioned methods are not applicable. To address this limitation, LLMs are prompted to generate verbalized confidence over their outputs (Lin et al., 2022). This technique was introduced by Tian et al. (2023) to infer probabilities in Question-Answering (QA) tasks, demonstrating better calibration than conditional probabilities. Rivera et al. (2024) integrated this approach with sampling methods for uncertainty quantification. Xiong et al. (2024) further extended it by combining it with sampling strategies to generate multiple responses and employing an aggregation approach for response consistency.\nAnother line of work from Ulmer et al. (2024) focuses on training an auxiliary model to calibrate verbalized probabilities. Upon obtaining confidence scores from generation, the authors employ Platt scaling (Platt, 1999; Guo et al., 2017) to adjust these scores. Conceptually different to those work, we focus on prompting LLMs to generate"}, {"title": "5 Discussion and Conclusion", "content": "We demonstrated that LLMs are able to generate probability distribution, which offers access to model verbalized confidence. We then identified the re-softmax issue on verbalized probability on discriminative tasks. We demonstrated that performing temperature scaling on reverted verbalized probability can effectively improve LLMs predictive calibration. This is achieved by leveraging revert softmax trick. The empirical results on three public datasets and one internal dataset evidently demonstrate the effectiveness of our proposed method.\nWe also showed the beneficial of generated probability with 2 decimal, and revealed the possible high variances for some examples. Measuring and turning this variance into an asset in terms of assigned label and probability in order to further improve robustness of LLM labels and calibration of verbalized probabilities is future work."}, {"title": "Limitations", "content": "As mentioned in Sec. 3, one of the limitations of the current approach is that black-box LLMs can fail to precisely follow instructions and output something other than confidence scores, even when explicitly asked to do so. This forces future users to either tune the prompts to diminish this behavior as much as possible, or to implement additional postprocessing logic to extract the actual scores. However, even when doing so there are no guarantees that the LLM will return well-formed outputs for every input."}]}