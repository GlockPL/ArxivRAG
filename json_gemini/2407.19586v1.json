{"title": "Is Generative AI an Existential Threat to Human Creatives? Insights from Financial Economics", "authors": ["Jiasun Li"], "abstract": "With the phenomenal rise of generative AI models (e.g., large language models such as GPT or large image models such as Diffusion), there are increasing concerns about human creatives' futures. Specifically, as generative models' power further increases, will they eventually replace all human creatives' jobs? We argue that the answer is \"no,\" even if existing generative AI models' capabilities reach their theoretical limit. Our theory has a close analogy to a familiar insight in financial economics on the impossibility of an informationally efficient market [Grossman and Stiglitz (1980)]: If generative AI models can provide all the content humans need at low variable costs, then there is no incentive for humans to spend costly resources on content creation as they cannot profit from it. But if no human creates new content, then generative AI can only learn from stale information and be unable to generate up-to-date content that reflects new happenings in the physical world. This creates a paradox.", "sections": [{"title": "Introduction", "content": "The past year has been drawing many people's attention to generative AI applications such as conversational AI bot ChatGPT, as well as many other transformer-based Large Language Model (LLM) applications, such as Google's Gemini (formerly known as Bard), Meta's Llama 2, diffusion-based text-to-image solutions like Stable Diffusion or DALL\u00b7E 3, and transformer/diffusion-based video generation applications such as OpenAI's Sora. All these generative applications allow a user to give a simple instruction (\u201cprompt\u201d), and the AI would then quickly generate a coherent textual answer, graphical image, or video clip (\u201ccompletion\u201d). Although the pre-training process of such large models may be prohibitively costly, once trained the models can generate new content in response to prompts with negligible costs and at a significantly faster speed than human content creators.\nThe power of generative AI models has thus been argued to impose an existential threat to human content creators (including journalists, graphic designers, film directors, or even academic researchers). Will generative AI models take over creative jobs, just like how automation and machines take over manual jobs (e.g., Zhang (2019))? Futuristically, as generative AI models further improve and reach their theoretical performance limit, will they completely replace human content creators?\nIn this paper, we address this very question of whether generative AI models could completely replace human content creators. We give a resounding \u201cno\u201d answer based on a simple intuition borrowed from a familiar insight in financial economics: Specifically, recall Grossman and Stiglitz (1980) has articulated that a strong-form efficient market as defined in Fama (1976) is impossible to obtain, and we argue that a close analogy exists between the impossibility of a fully informationally-efficient market and a fully generative AI dominated creatives market. The analogy is so close that even ChatGPT can \u201csee\u201d it, as to be"}, {"title": "Backgrounds", "content": "To appreciate how our theory accurately captures the reality associated with generative AI models, it is imperative to adequately understand how generative AI models function. Since these models are still relatively new, and their inner workings may not be immediately familiar to non-AI researchers (such as financial economists), Section 2.1 first provides the necessary technical background.\nOn the other hand, to appreciate the real-world implications of our analysis, it is also helpful to realize that the fear of generative AI replacing human creatives is not purely theoretical. To this end, Section 2.2 quotes recent news articles reporting human creatives' responses to such concerns as motivational examples."}, {"title": "A crash course on generative AI models", "content": "In this section, we provide a high-level introduction to how generative AI models work. For exposition ease, we will use the GPT model as an example.\nSince this part of the paper is not our novel contribution (although a self-contained explanation of how generative AI models work is important for understanding the rest of the paper), to remain unbiased, we again leverage the power of ChatGPT to answer the prompt of \"how does ChatGPT work?\u201d For readers' ease, we will underscore the key takeaway within the ChatGPT's answer (\u201ccompletion\u201d), which appears at the end of this completion. Other than the underscored part crucial for the understanding of the rest of the paper, impatient readers may safely skip the rest of this subsection without loss of continuity."}, {"title": "Anecdotal evidence", "content": "The concern over generative AI replacing human creatives is not merely an academic theorization, but is rather an imminent reality. The following excerpt from a SeekingAlpha article on August 14th, 2023 gives a vivid example:\nWith generative AI poised to reshape the publishing industry, companies are\nscrambling to take positions on the latest technology. What does it mean for\ntheir operations? How will it impact their workforce? And when can they begin\nto expect to benefit from, or be harmed by, the new developments? These ques-\ntions have rattled publishers in nearly every sector of the media industry, from\nonline content and the printed word to movies and even music.\nSnapshot: The knee-jerk reaction of many firms was to band together to stave\noff any threats to their bottom line. Hollywood actors and writers have gone on\nstrike in part over fear that they might lose or share revenue with machines, with\nthe walkout now going on for more than 100 days. Publishers like The New York"}, {"title": "A Reduced-form Model", "content": "We first build a simple static model to illustrate our core idea. The model in this subsection is intentionally kept to be highly reduced-form to capture the essence of how the low-cost \"parrot\" nature of generative AI prevents it from completely replacing human creatives. Appendix ?? sketches a more microfounded model to capture the dynamic interaction between human creatives and generative AI in a more full-fledged model."}, {"title": "Agents", "content": "There is a unit continuum of consumers for creative content. Among them, a fraction \u03bb of agents are human creatives, who can incur a cost \\(C > 0\\) to create new content for their own consumption (for simplicity, we do not consider the possibility of hiring other human creatives for content creation or alternatively, we can view the current model as treating content creators and consumers as the same type of agents) while a fraction \\(1 - \\lambda\\) of agents only use generative AI with a lower cost \\(c < C\\) for their content needs. In subsequent discussions, we may simply call the second type of agent generative AI for brevity. Note that the fraction \u03bb will be endogenously determined in equilibrium."}, {"title": "Preferences", "content": "Normalizing a consumer's utility when consuming no content (either from human creation or generative AI) to zero, we denote the benefit from consuming human research as R. We assume that R > C, as otherwise, the world will trivially end up in an uninteresting as well as unrealistic situation of no content creation/consumption. We denote the payoff to an agent who conducts human content creation as \\(\\Pi_{human} = R - C > 0\\).\nOn the other hand, we denote the benefit from consuming generative AI-created research as r. Therefore, the payoff to an agent who uses generative AI \\(\\Pi_{AI} = r(\\lambda) - c\\). Here, we writer(\u03bb) to highlight that r is a function of \u03bb, the extent to which human creatives produce content. More formally, we hold the following assumptions for the function r:"}, {"title": "Assumption 1", "content": "The benefit from consuming generative AI-created content, r(\u03bb), is an increasing function of \u03bb, the extent to which human creatives produce content.\nAssumption 1 naturally follows how generative AI models work as we have previously explained in Section 2.1. To see this, notice that generative AIs are clever \u201cparrots\u201d that learn from human-generated content. Keeping the algorithms of a generative AI model fixed, the performance of the generative AI model depends on how comprehensive and up-to-date its training sample is. To the extent that more content created by human creatives (that is, a higher \u03bb) helps make the generative AI model's training sample more comprehensive and up-to-date, which in turn improves its performance, the benefit it brings to content consumers r(\u03bb) also increases with the intensity of human content creation \u03bb."}, {"title": "Assumption 2", "content": "r(0) = 0 and r(1) \u2248 R.\nAssumption 2 also naturally follows how generative AI models operate as previously explained in Section 2.1. To see this, notice that when \u03bb = 0, no content is created by humans so that the AI will have no input for its training purpose and therefore not be able to produce anything meaningful. We thus have r(0) = 0. Regarding the value of r(1), we consider two cases: (1) If the AI model is perfect, in the sense that the AI is assumed to learn everything that has been created by humans so far, then we would have r(1) = R. Otherwise, (2) if the AI is imperfect, then r(1) < R. Since we are more interested in the case in which generative AI's capability reaches its theoretical limit, we shall assume that \\(r(1) = R - \\epsilon\\), where \\epsilon is some infinitesimal amount. Combining the two cases, we assume that \\(r(1) \\approx R\\)."}, {"title": "Equilibrium characterization", "content": "We now proceed to solve for the equilibrium outcomes of the game, characterized by a couple of lemmas that lead to the main proposition.\nLemma 1. Generative AI completely taking over human creatives' jobs cannot be an equilibrium outcome. In other words, \u03bb > 0 in equilibrium."}, {"title": "Proof", "content": "We prove the lemma by contradiction. Suppose otherwise, then \u03bb = 0, and each generative AI user gets \\(r(0) - c = -c < 0\\). If one of them were to deviate toward human content creation, then his payoff would become \\(R - C > 0\\), which is profitable. Therefore, \u03bb = 0 cannot hold in equilibrium."}, {"title": "Lemma 2", "content": "It is not an equilibrium for generative AI to not be adopted, i.e., in equilibrium \u03bb < 1."}, {"title": "Proof", "content": "We again prove the lemma by contraction. Suppose otherwise, then \u03bb = 1, and each human content creator gets R-C. If he were to deviate toward using generative AI, his payoff would be \\(r(1) - c = R - \\epsilon - c > R - C\\). Therefore, \u03bb = 1 cannot hold in equilibrium."}, {"title": "Given Lemma 1 and 2", "content": "we have that the equilibrium must be interior, that is, generative AI and human content creators must coexist. Therefore, in equilibrium we must have \\(\\Pi_{human} = \\Pi_{AI}\\), or \\(R - C = r(\\lambda) - c\\).\nProposition 3. There exists a unique \u03bb* satisfying \\(R - C = r(\\lambda^*) - c\\), which characterize the unique equilibrium in our model economy"}, {"title": "Proof", "content": "Notice that the right-hand side of equation \\(R - C = r(\\lambda) - c\\) (with respect to \u03bb) is monotonically increasing, is lower than the left-hand side when \u03bb = 0, and is higher than the left-hand side when \u03bb = 1.\nFrom the definition of \u03bb*, we can immediately obtain the following comparative statics:\nThe equilibrium level of the fraction of human content creators, \u03bb*, increases in the value of human-generated content R, decreases in the cost of producing human-generated content C, decreases in the marginal cost of AI content generation c, and decreases in the performance of generative AI. All these comparative statistics should be intuitive."}, {"title": "Conclusion", "content": "In this paper, we make a simple point, that generative AI will not completely take over human content creators. The core idea shares a lot of resemblance with the active-passive tension in traditional financial markets. Our result serves to demonstrate that some fundamental ideas in financial economics could still be highly relevant as we enter an era of AI.\nThere are many extensions of our core model that future research may further explore. For example, we so far assume that content consumers all create content for themselves, while in reality, they may hire other parties (e.g., professional creatives) for their content needs; Future research may follow the framework in Admati and Pfleiderer (1990) to further tease out the equilibrium outcome. We have also so far ruled out potential heterogeneity in all the content created; Future work may leverage insights from Hellwig (1980) or Diamond and Verrecchia (1981) to fully develop the case with heterogeneous contents. Our model so far has assumed only one type of content desired by the population; When the interactions among multiple content markets (e.g., general text, general image, or domain-specific contents) are considered, one may adopt ideas from Admati (1985). Furthermore, to the extent that content consumers may act upon what they learn, which in turn affects the overall informational landscape, one may further explore the feedback loop between human-generated content, AI-generated content, and the real distribution of information in the physical world, leveraging insights from Chen, Goldstein and Jiang (2007); Edmans, Goldstein and Jiang (2015); Goldstein and Yang (2017), etc. Finally, we may further derive dynamic interactions between human creatives and generative AI, highlighting the fact that generative AI models are trained on lagged samples, and their values decay if no additional input are generated by human content creators.\nA key friction behind why generative AI cannot completely replace humans is that AI cannot learn all input humans can perceive. In other words, existing generative models need to rely on the first-hand content created by humans to generate second-hand information. The conclusion of our paper may change, however, if this fundamental friction is no longer present. One foreseeable way this friction may disappear is when generative AIs can directly learn from the physical world. The fact that many AI models are now increasingly multimodal or even omni-model, and that sensor technologies further improve (including human-machine connections as recent progress from Neuralink promises), then our conclusions may need to be qualified \u2013 however, that would be a discussion of certain more advanced AI paradigms that we have not yet seen today."}]}