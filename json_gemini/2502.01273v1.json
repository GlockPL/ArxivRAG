{"title": "Analysis of Student-LLM Interaction in a Software Engineering Project", "authors": ["Agrawal Naman", "Ridwan Shariffdeen", "Guanlin Wang", "Sanka Rasnayaka", "Ganesh Neelakanta Iyer"], "abstract": "Large Language Models (LLMs) are becoming in-creasingly competent across various domains, educators are showing a growing interest in integrating these LLMs into the learning process. Especially in software engineering, LLMs have demonstrated qualitatively better capabilities in code summarization, code generation, and debugging. Despite various research on LLMs for software engineering tasks in practice, limited research captures the benefits of LLMs for pedagogical advancements and their impact on the student learning process. To this extent, we analyze 126 undergraduate students' interaction with an AI assistant during a 13-week semester to understand the benefits of AI for software engineering learning. We analyze the conversations, code generated, code utilized, and the human intervention levels to integrate the code into the code base. Our findings suggest that students prefer ChatGPT over CoPilot. Our analysis also finds that ChatGPT generates responses with lower computational complexity compared to CoPilot. Furthermore, conversational-based interaction helps improve the quality of the code generated compared to auto-generated code. Early adoption of LLMs in software engineering is crucial to remain competitive in the rapidly developing landscape. Hence, the next generation of software engineers must acquire the necessary skills to interact with AI to improve productivity.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative large language models (LLMs) have become crucial in education, excelling in tasks from math problem-solving [1] to dialog-based tutoring [2] and aiding software engineering projects [3]. Their versatility has made them highly sought after in educational settings. In software engineering, LLMs particularly excel in tasks like code summarization [4], test generation [5], program analysis [6], code review [7], bug fixing [8], and code generation [9]. Despite growing interest in AI for education, research remains limited on how students use LLMs for open-ended tasks in software engineering projects.\nIn this work we examine the interaction between undergraduate students and AI assistants in a software engineering course. Students were tasked with using AI to develop a Static Program Analyzer (SPA) for a custom programming language. Over a 13-week semester, teams of six students undertook various tasks, from requirement engineering to user acceptance testing. They received unlimited premium access to Microsoft CoPilot and OpenAI ChatGPT. At semester's end, we collected all AI-driven conversations, code, and artifacts, along with student-annotated code metadata, for analysis. We examine the collected data to answer the following research questions:\n\u2022 Is there a significant difference between code generated by ChatGPT and CoPilot? \u2192 We compare code complexities using various metrics.\n\u2022 How does the code evolve during a conversation between a student and AI? \u2192 We analyze conversation logs and extract code for each conversation.\n\u2022 What is the impact of using AI assistant on their learning outcomes? \u2192 We analyze the conversation volume, final code output, and evolution of the prompting techniques.\n\u2022 Does the interaction between the student and AI result in a positive engagement? \u2192 We perform sentimental analysis across each conversation.\nA total of 126 undergraduate students in 21 groups, gener-ated 730 code snippets (172 tests and 558 functionality imple-mentations) using CoPilot and ChatGPT. We also collected 62 ChatGPT conversations that generated code, amounting to 318 messages between students and ChatGPT. Of the total 582,117 lines of code across all teams, 40,482 lines of code (6.95%) were produced with an LLM's help.\nUpon analysis, Copilot-generated code is longer and more complex (i.e. higher Halstead Complexity) than ChatGPT's, making it harder to interpret. Despite initial assumptions, student feedback shows no significant difference in the integra-tion effort required for both Copilot and ChatGPT-generated code. Furthre analyzing the conversation logs, we identified that through feedback ChatGPT generated code meets project needs with minimal refinement. Sentiment analysis of the con-versation reveals on average the conversation ends on a pos-itive note. Indicating conversational-based assistance generate code requiring minimal manual refinement. Over the semester, we also observed a noticeable improvement in the quality of the prompts generated by students, demonstrating their growing ability to craft more effective and precise prompts for better outcomes.\nBased on the observations from our study, we discuss design considerations for a future educational course tailored to using AI assistants for software engineering. These considerations include promoting students to learn better prompting strategies and evolving the use of AI assistants beyond merely being a tool for code generation. Our contribution lies in providing an in-depth analysis of how students use ChatGPT in a project-based software engineering course."}, {"title": "II. METHODOLOGY", "content": "Project Description: In compliance with institutional guidelines, approval for our research was obtained from the Departmental Ethics Review Committee (DERC) before conducting the study. The undergraduate level software engineering course within which this study is conducted involves a 13-week long, robust software development project, where 126 students are tasked with building a Static Program Analyzer (SPA), with three distinct milestones (MS1, MS2, MS3) where the delivery of a functional SPA is expected. The SPA is capable of performing analysis on a course specific custom programming language. The structure of the project is similar to the project used in [3], where the SPA is further subdivided into:\n\u2022 A Source Parser (SP) which analyzes the custom lan-guage to extract abstractions such as design entities.\n\u2022 A Program Knowledge Base (PKB), responsible for stor-ing the extracted information.\n\u2022 A Query Processing Subsystem (QPS), which is able to handle queries written in an SQL-like language for querying the PKB, and provide responses to the user.\nThroughout the development phase, students were granted organizational access to the paid versions of ChatGPT via both the \"Chat\" and \"Playground\" interfaces, enabling close mon-itoring of their usage. Additionally, students were also able to access GitHub Copilot features through their institutional GitHub Pro accounts. Access to both of these LLM code generators is funded by the university. Students are also actively encouraged to utilize LLMs and integrate them into their development cycle, and the usage of their organizational access was also reserved strictly for the purposes of this project. Through this setup, we are able to obtain data regarding students' interactions with LLMs, and also the conversational history and information about prompts that were used on ChatGPT.\nCode extraction and ChatGPT conversations: Following our initial work [3] we extracted LLM-generated code snippets used by the students at each milestone, which was achieved by requiring students to tag the LLM-generated code utilized in their project with the following information:\n\u2022 Generator used to obtain the output code.\n\u2022 Level of human intervention required to modify the code.\n\u2022 Link to the conversation (only for ChatGPT)\nThe tagging and collection of student data, as well as the definitions of human intervention levels (0, 1, and 2), follow our previous work in [3]: level 0 (no changes), level 1 (10% or fewer lines changed), and level 2 (more than 10% of the lines changed). This paper introduces a new aspect by including links to student-LLM conversations.\nThe collected data at each milestone is cumulative, re-flecting students' iterative development of their SPA over the semester. We also gathered data on students' use of ChatGPT, including the prompts and generated code, to analyze how conversational interactions affect the quality and usability of LLM-generated code.\nOverview on Analysis: This study explores how students in a software engineering course interact with LLMs like ChatGPT and GitHub Copilot, to use the LLM generator tools to help them in their development process, particularly in the context of how approaches to code generation and the generated outputs evolve. By examining how students interact with these LLMs, adapt generated code, and refine their prompting strategies, we aim to reveal the dynamics of human-AI collaboration in SE education. Here we unpack a multi-layered analysis that spans the quality and complexity of LLM-generated code, the processes of integrating LLM-generated code within student repositories, and the evolving conversational interactions between students and LLMs across milestones.\nTo comprehensively analyze the quality of LLM-generated code, we used a set of four distinct metrics: total lines of code (LOC), cyclomatic complexity, maximum control flow graph (CFG) depth, and the Halstead effort metric. These metrics provide insights into the sophistication and structural intricacies of the code produced by LLMs.\n\u2022 Total Lines of Code (LOC): Serves as a basic indicator of code verbosity and has been used to estimate the programming productivity of a developer.\n\u2022 Cyclomatic Complexity: Measures the number of linearly independent paths within the code, and evaluates its logical complexity. Higher cyclomatic complexity can be indicative of maintainability challenges.\n\u2022 Maximum Control Flow Graph (CFG) Depth: Measures the depth of nested structures within the code. Increased CFG depth can reflect the presence of deeply nested loops or conditional statements, which may complicate code comprehension and maintenance.\n\u2022 Halstead Effort: Estimates the mental effort required to understand and modify the generated code. Higher values suggest that the code may be more challenging to understand and maintain.\nThis work extends our previous research [3] by adding a new dimension of sentiment analysis enabled by the collected prompts, providing insights into student - AI interactions. We also introduced new metrics, offering deeper analysis of how code quality and usability vary across different generation approaches."}, {"title": "III. RESULTS", "content": "A. Analysis of LLM Usage\nWe first analyzed the code snippets generated using LLMs across each milestone for each team. 5 teams did not use any LLMs for code generation tasks despite providing premium access for the project. Out of the remaining 16 teams, 12 used LLMs to generate a moderate number (>10) of code snippets. Among these, 6 primarily relied on Copilot, 5 heavily utilized ChatGPT, and 1 team used both tools equally.\nAnalyzing across milestones significant decline can be ob-served in usage of both ChatGPT and Copilot by all teams."}, {"title": "B. Analysis of LLM Generated Code", "content": "We analyzed 730 code snippets generated using ChatGPT and Copilot. Table II summarizes the types of code snippets produced by both tools, categorized into those for testing pur-poses and functionality implementation. The analysis shows that students primarily relied on AI assistants for functionality implementation, with moderate usage for generating test cases.\nWe further analyzed the complexity of AI-generated code across the three project milestones (MS1, MS2, and MS3) using metrics such as lines of code and cyclomatic complexity. Analysis of AI-generated code revealed a trend towards higher complexity, particularly in code generated by Copilot, as shown by skewed density plots in Figure 1. This suggests that AI assistance may lead to more complex solutions, although the majority of student-generated code remained moderately complex. Although the average complexity (cy-clomatic complexity and total lines) of student-generated code remained moderate, the analysis revealed that AI assistance, particularly Copilot, occasionally produced highly complex solutions, sometimes exceeding the average values by 40 to 50 times. This suggests that AI-generated code, while often effective, has the potential to introduce unnecessary complexity if adopted without careful review and refinement.\nCopilot generated significantly more outliers than GPT across all complexity metrics, indicating a tendency toward producing more complex and verbose code. This difference likely stems from Copilot's auto-completion approach, which favors extensive code generation based on common patterns, potentially leading to inflated complexity compared to GPT's more concise and conversationally guided output.\nGPT's conversational interface allows for iterative refine-ment of code, enabling students to guide the model towards simpler and more maintainable solutions. Conversely, Copi-lot's auto-completion approach, while efficient, can lead to overly complex code due to the lack of nuanced interaction. Additionally, the study's analysis of GPT-generated code is more precise due to the ability to track exact model outputs, while Copilot's contributions are assessed through student modifications, highlighting a difference in how interactions with each tool are measured.\nWe also analyzed students' efforts to integrate AI-generated code into the project based on reported manual intervention ratings. For Copilot-generated code, the majority (53.6%) re-quired minor intervention (level 1), while a significant portion (30.0%) required moderate intervention (level 2), indicating a higher demand for user input to refine or simplify the code. Only 15.2% of Copilot-generated code required no interven-tion. In contrast, ChatGPT-generated code more often aligned"}, {"title": "C. Generated Code vs Integrated Code", "content": "Our next analysis focuses on how students modify and inte-grate ChatGPT-generated code into their project repositories. Copilot-generated code is excluded, as it lacks the conver-sational context that evolves the code. This analysis aims to uncover patterns in student adaptations, examining whether they enhance, simplify, or otherwise alter the initial code provided by ChatGPT. We compared each team's repository code with the corresponding ChatGPT-generated code for each conversation. We identified the segment of ChatGPT code with the highest average similarity to the repository and used it as a reference. Our analysis revealed multiple instances of reuse of ChatGPT-generated code snippets in various parts of the team repository. While most students used a generated code once, some used it multiple times, demonstrating its adaptability.\nIn 95 instances the generated code was used only once, indicating that the majority of students found LLMs useful for generating task-specific code. In 23 instances students reused the generated code twice in the repository and in"}, {"title": "D. In-depth Analysis of Conversations", "content": "Similarity measurements on the ChatGPT conversations were used to determine how the generated code evolved during a conversation and ultimately integrated. The histogram in Figure 6 reveals that most conversations typically consist of just one or two messages, with a smaller number extending beyond 15 messages. The longest conversation was 50 messages. This distribution shows an overall downward trend, indicating that longer conversations are less frequent.\nFor each code snippet in the repository, we identified the ChatGPT conversation that generated it by comparing the similarity of GPT-produced code snippets within conversations to the tagged repository code. Conversations were analyzed separately based on varying lengths to account for the tendency of shorter conversations to show high similarity at smaller indices. This separate analysis helped prevent a skew towards smaller indices. For conversations shorter than 20 interactions, we calculated the average index of the code with the highest similarity to the repository code, excluding reused code to avoid skewed values. Conversations averaging zero similarity, suggesting significant modifications or irrelevant outputs, were omitted. In cases of ties in maximum similarity across conver-"}, {"title": "E. Prompt Analysis", "content": "We conducted sentiment analysis for student prompts uti-lizing the VADER (Valence Aware Dictionary and Sentiment Reasoner) tool [10]. VADER is effective for analyzing the sentiment of short texts, such as prompts, which enables us to determine whether users generally felt positive, neutral, or frustrated during their interactions with the LLM."}, {"title": "IV. THREATS TO VALIDITY", "content": "Our analysis is based on voluntarily collected student self-reports, which may include underreporting or selective dis-closure, introducing potential bias. Although GitHub Copilot offers a chat feature, it was not widely used; it primarily served for code completion and debugging. The chat functionality was not significant. Moreover, the VADER tool for sentiment analysis often misclassifies technical terms as neutral, resulting in many prompts receiving scores near zero due to frequent technical language. Despite these limitations, the analysis offers valuable insights into sentiment trends and the emotional tone of user interactions."}, {"title": "V. RELATED WORK", "content": "A. LLMs in SE Education (LLM4SE Edu)\nThe increased popularity and accessibility of LLMs are prompting significant changes to approaches to software en-gineering education, with an emphasis on adaptive learning strategies and ethical considerations. [11] underscores the need for SE education to evolve in response to LLM advancements, advocating for combining technical skills, ethical awareness, and adaptable learning strategies. AI-powered tutors, such as those based on LLMs, have also shown promise in delivering timely and personalized feedback in programming courses. [12] has also found LLMs to be feasible in classifying student needs in SE educational courses, presenting a cost effective alternative to traditional tutor support demand. However, [13] highlights challenges such as generic responses and potential student dependency on AI, warranting further discussions on the cost-effectiveness of using LLMs in SE education. Sim-ilarly, [14] finds that Gamified learning environments, when augmented with LLMs, can boost student engagement but may inadvertently lead to over-reliance, undermining the learning process. The StudentEval benchmark also introduces novice prompts, shedding light on non-expert interactions and reveal-ing critical insights into user behavior and model performance [15]. Work has also been done on programming assistants that do not directly reveal code solutions [16], providing design considerations for future Al education assistants.\nB. LLMs in Software Engineering (LLM4SE)\nLLMs have been employed in tools designed to im-prove code comprehension directly within integrated develop-ment environments (IDEs). These tools utilize contextualized,"}, {"title": "C. LLMs in Education", "content": "LLMs are promising to reshape pedagogy, by offering solutions for personalized learning and scalable assessment practices. A systematic review of LLM applications in smart education highlights their role in enabling personalized learn-ing pathways, intelligent tutoring systems, and automated educational assessments [23]. LLMs have also been evaluated for their utility in grading programming assignments, with research demonstrating that ChatGPT provides scalable and consistent grading, rivaling traditional human evaluators [24]. Our work extends beyond these existing work in the fol-lowing aspects: we performed a study on the interaction between LLMs and Software Engineering students working on a complex project, conducting a comprehensive suite of analyses on both the prompts and generated code produced in these interactions, differing from the existing literature in the scope of analysis, a focus on the effects of the conversational nature of LLM code generators, as well as the examination of user sentiments via prompts they used to generate code."}, {"title": "VI. SUMMARY", "content": "Research Objectives and Contributions: Our paper ex-plores the integration of Large Language Models (LLMs) in software engineering education, focusing on how student teams interact with AI tools throughout a multi-milestone academic project. We analyzed tool usage, code complexity, refinement, and student prompting behavior to uncover pat-terns in AI-aided code development throughout the educational process. Our study provides actionable insights for educators to optimize Al tool usage in Software Engineering curricular.\nSummary of Findings: Most of the teams utilized AI dur-ing development. Copilot was preferred for auto-completion, while ChatGPT excelled in iterative refinement of more complex solutions. Al usage declined across milestones, as students relied on LLMs more at the early stages of the project. Copilot's outputs were often more complex, while ChatGPT produced more concise and understandable solutions. The"}]}