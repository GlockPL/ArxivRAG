{"title": "MuDiT & MuSiT: Alignment with Colloquial Expression in Description-to-Song Generation", "authors": ["Zihao Wang", "Haoxuan Liu", "Jiaxing Yu", "Tao Zhang", "Yan Liu", "Kejun Zhang"], "abstract": "Amid the rising intersection of generative AI and human artistic processes, this study probes the critical yet less-explored terrain of alignment in human-centric automatic song composition. We propose a novel task of Colloquial Description-to-Song Generation, which focuses on aligning the generated content with colloquial human expressions. This task is aimed at bridging the gap between colloquial language understanding and auditory expression within an AI model, with the ultimate goal of creating songs that accurately satisfy human auditory expectations and structurally align with musical norms. Current datasets are limited due to their narrow descriptive scope, semantic gaps and inaccuracies. To overcome data scarcity in this domain, we present the Caichong Music Dataset (CaiMD). CaiMD is manually annotated by both professional musicians and amateurs, offering diverse perspectives and a comprehensive understanding of colloquial descriptions. Unlike existing datasets pre-set with expert annotations or auto-generated ones with inherent biases, CaiMD caters more sufficiently to our purpose of aligning AI-generated music with widespread user-desired results. Moreover, we propose an innovative single-stage framework called MuDiT/MuSiT for enabling effective human-machine alignment in song creation. This framework not only achieves cross-modal comprehension between colloquial language and auditory music perceptions but also ensures generated songs align with user-desired results. MuDiT/MuSiT employs both a fine-tuned", "sections": [{"title": "1 Introduction", "content": "Songs are an integral part of human culture, functioning as a universal medium for communication. The creation of music necessitates significant time and effort from composers to attain the anticipated results. Recently, the swift development of datasets and generative models has catalyzed rapid advances in automatic song composition. Nonetheless, as far as we are aware, at a time when the alignment efforts between generative AI in the arts and human are becoming increasingly crucial, the alignment work in automatic song composition AI has not received adequate attention. AI alignment typically refers to ensuring that AI systems can maintain consistency with human values during their operation, which is a concern highly regarded by researchers such as Geoffrey Hinton and Ilya Sutskever (former Chief Scientist of OpenAI). In the context of automatic song composition tasks, this translates into whether AI models can fully comprehend human colloquial descriptions, whether the generated songs meet the auditory expectations desired by humans, and whether they conform to human anticipations for song structure. 'Humans' here does not exclusively pertain to musical professionals but also encompasses a vast number of amateurs. This implies that beyond understanding professional musical vocabulary, AI models need to achieve cross-modal understanding between casual lexical text and auditory perception levels. By doing so, they can better grasp the expressions conveyed by amateurs and fulfill their demands more effectively.\nPrevious research has primarily concentrated on specific aspects of song creation, such as converting text to music and musical scores to songs, yet it has not addressed the entire creative process. MusicGen leverages a single-stage Transformer language model to facilitate text-to-music production, employing efficient token interleaving patterns enabled by quantization-based audio codecs. Melodist accomplishes the generation from music score to song through a multi-stage process that includes synthesizing singing voices from"}, {"title": null, "content": "inputted lyrics and melodies, creating accompaniments based on these voices, and blending the vocal tracks with the instrumental accompaniment. Concurrently, platforms specializing in description-to-song generation from the industrial sector have gained considerable attention owing to their impressive functionalities. Suno and SkyMusic adopt two-stage generation methods based on language and diffusion models; however, they do not grant access to their datasets and code bases. Udio and Stable Audio opt for a single-stage diffusion model approach to undertake this task. Nevertheless, they still confront challenges such as inadequate comprehension of input text descriptions and a lack of structure in the generated songs.\nTo bridge this research gap, we introduce the task of Colloquial Description-to-Song Generation aimed at the human-machine alignment in automatic song composition. This departs from traditional automatic songwriting in that: 1) It necessitates a comprehensive understanding of Colloquial Descriptions since people typically employ colloquial language instead of professional descriptions; 2) It demands a deep capture of a song's musical structure, including musical sections and rhyming structures. These unique characteristics present several challenges in colloquial description-to-song generation:\n1) Data Scarcity. They require high-quality and large-scale paired datasets that encompasses various dimensions such as colloquial descriptions, musical structure, genre, and emotion. Despite the numerous datasets proposed thus far, including those derived from automatic annotations using Music Information Retrieval (MIR)\nalgorithms or Large Language Models (LLMs), as well as manual annotations , these existing datasets exhibit several issues that hinder their effectiveness for colloquial description-to-song generation. Firstly, current manually annotated datasets are often limited to expert annotations and have a narrow descriptive scope, which significantly diverges from the more varied descriptions provided by the general public. Secondly, there is a significant semantic gap between datasets obtained through MIR algorithms and complex human descriptions. Thirdly, due to limitations in algorithm performance, automatic annotation-based datasets cannot achieve complete accuracy, and existing manually annotated datasets, where each entry is annotated by only one person, are prone to inaccuracies caused by human errors or biases.\n2) Human-Machine Alignment. It is far from enough for current models to fully understand colloquial descriptions provided by common people and to accurately learn the alignment between people's descriptions and model's behaviours. Thus, in addition to the above paired datasets, an effective framework for colloquial description-to-song generation should be proposed to capture the human-machine alignment.\nSpecifically, this study adopts supervised learning techniques alongside a dual perspective of professional and amateur annotators, and constructs the training process using an end-to-end single-stage approach. This ensures that the AI-generated song harmonizes with human desires, while also enabling the AI model to comprehend and incorporate the standard structure of human song."}, {"title": null, "content": "To address the challenge of data scarcity, we first create a music annotation platform, named Caichong Multitask Annotation Platform (CaiMAP) that facilitates efficient annotation of both professional and colloquial music descriptions and employs a multi-person, multi-stage quality assurance process to guarantee the precision and uniformity of annotations. We have invited groups of professional and amateur annotators who employ two different sets of tags when labeling but annotate the same song on this platform. This leads to two distinct perspectives for each song, culminating in a comprehensive, highly precise dataset that aligns with public consensus: the Caichong Music Dataset (CaiMD). This dataset stands as the first open-source repository for Chinese colloquial music descriptions, covering various dimensions such as colloquial descriptions (style, emotion, etc.) and musical structure, designed to furnish exhaustive data for the fine-tuning of end-to-end models.\nTo handle the challenge of human-machine alignment, we propose a novel single-stage framework for colloquial description-to-song generation known as MuDiT/MuSiT. First, we adopt QLORA as the parameter-efficient fine-tuning (PEFT) method, and fine-tune a LLM model Qwen-14B to generate lyrics with additional information, including musical sections and rhyming structure, from the corresponding colloquial description. For lyrics and additional structural information, we apply a cross-attention mechanism, treating them as conditions to capture the correlations between the lyrics and the audio. Given that the vocabulary and phrases in the Chinese colloquial descriptions have not appeared in the texts used for training existing open-source text-to-audio comparison pre-trained models, we have trained a MuChin Cross-Modal Encoder utilizing the CaiMD dataset, modeled after architectures analogous to CLAP and MuLan. Subsequently, We employ MuChin to process the text description as a conditional input, standardizing the vector length before concatenating it with random noise. We then use these vectors as input, employ transformer-based diffusion models (DiT and SiT), that operate in the latent space of the Variational Autoencoders (VAE), to generate high-quality, well-structured songs that align with colloquial descriptions. For lyrics and additional structural information, we apply a cross-attention mechanism, treating them as conditions to capture the correlations between the lyrics and the audio. And the utilization of VAE and HIFI-GAN for decoding the song content into Mel spectrograms and converting them into WAV audio files.\nIn training, we initially perform supervised pre-training on DiT/SiT using a private large-scale lyrics-song audio paired dataset and unsupervised pre-training for VAE and HIFI-GAN. Subsequently, on the task of colloquial description-to-song, the DiT/SiT are fine-tuned based on the CaiMD dataset. This process enhances their capability to generate well-structured songs that align with human colloquial descriptions.\nIn terms of experimental metrics, we have adopted the Fr\u00e9chet Audio Distance (FAD) as a measure to evaluate the quality of the music produced by large-scale music models. We have also developed new metrics to assess the alignment between songs generated by music models and their colloquial descriptions, as well as to evaluate the structure of the generated lyrics. Additionally, considering the unique aspects of music, an amalgamated approach of subjective and objective evaluations was adopted."}, {"title": null, "content": "In our main experiments, we conducted comparisons with contemporary leading large-scale music models. The results demonstrate that our MuSiT and MuDiT models exhibit superior performance in comparison with open-source frameworks such as MusicGen and Stable Audio when trained on data sets of equivalent sizes. As compared to proprietary music generation models like Suno and Udio, our models are nearly on par in terms of generation quality but present better alignment with colloquial descriptions of songs, successfully mirroring non-expert descriptors. When taking into account our discrepancy in data volume and parameter size relative to these counterparts, our models maintain their standing as the most optimal. The performance disparity between MuSiT and MuDiT corroborates our hypothesis: SiT, with its superior capabilities in handling time-series data particularly adept at managing continuous temporal variations and complex dynamic processes is more suitable for music generation compared to DiT.\nAdditionally, supplementary experiments were carried out. We conducted an experimental evaluation of existing LLMs for the task of structured lyric generation, including Qwen, Baichuan-2, GLM-130B, and GPT-4, as well as a version of Qwen that was fine-tuned on the CaiMD dataset. The experiments demonstrate that the fine-tuned Qwen, despite having fewer parameters, significantly outperformed larger closed-source baseline models in structural scores, successfully aligning with the human perception of the song structure in pop music. We also verified that MuChin, in contrast to comparative pre-trained models like MuLan and CLAP that were not fine-tuned on colloquial description-to-audio data, could effectively enhance the alignment of the output songs from MuSiT and MuDiT with colloquial descriptions. In response to evaluating the advanced nature of the CaiMD dataset, we also designed experiments to test its efficacy. Assuming identical pre-training data, we utilized other existing text-to-music datasets to conduct fine-tuning training separately. Results revealed that models fine-tuned with the CaiMD dataset vastly surpassed others in their capacity to interpret colloquial descriptions, capturing the intentions and emotional nuances of typical users with greater precision. This validation supports a widely recognized notion, \"A small, well-annotated dataset often outperforms a larger, poorly anno-tated one in model training.\" In the final part of this paper, we conducted an experiment to assess whether there is a significant disparity in understanding and description of music between professionals and amateurs. The results indicate that these two groups demonstrate varied levels of interpretive differences across different dimensions and types of music, with the perspectives of professionals not resonating with amateur enthusiasts.\nOur contributions are summarized as follows:\n\u2022 We introduce a novel task of colloquial description-to-song generation, an essen-tial aspect of alignment works between generative AI and humans in the field of art. This task necessitates that AI models fully comprehend human colloquial descriptions to generate songs that precisely meet human expectations.\n\u2022 We created the CaiMAP, implementing a multi-person, multi-stage quality assur-ance process to guarantee the precision and uniformity of annotations. Based on this platform, the CaiMD was constructed through manual annotations made from both professional musicians and amateurs, offering distinct perspectives."}, {"title": null, "content": "Unlike other existing public datasets, CaiMD is better suited for fine-tuning end-to-end colloquial description-to-song generation models, thereby aligning with amateur expressions and fulfilling the general public's demands. We are in the process of progressively open-sourcing this dataset.\n\u2022 We propose a novel single-stage framework for colloquial description-to-song generation, referred to as MuDiT/MuSiT, to achieve human-machine alignment. The cross-modal comprehension between colloquial descriptions and auditory musical perceptions achieved by MuDiT/MuSiT enables it to align with the general public during song generation. Moreover, in addition to utilizing a fine-tuned LLM for lyrics creation, MuDiT/MuSiT exclusively employs one DiT/SiT model for end-to-end generation of other musical aspects such as melody, vocals, harmony, rhythm, and instrumentation, as well as subsequent mastering pro-cesses. This integrated approach ensures that all generated musical components are sonically harmonious and cohesive."}, {"title": "2 Background", "content": "2.1 Text-to-Song Generation\nText-to-song generation aims to create song pieces based to the descriptive text. Previous works typically focus on specific aspects of song creation, such as text-to-music , lyrics-to-melody and music score-to-song , but do not encompass the entire creative process. For text-to-music generation, models such as MusicLM and MusicGen utilize quantization-based audio codecs to obtain residual codebooks and employ language models and diffusion models for high-quality audio music generation. For lyrics-to-melody generation, SongMASS and TeleMelody take lyrics as input and output melodies, ensuring complex and subtle lyric-melody correlations. For music score-to-song generation, Melodist adopts a two-stage method that consists of singing voice synthesis from given music score and vocal-to-accompaniment synthesis based on the singing voice and natural lan-guage prompts. Meanwhile, text-to-song generation platforms from industry, such as Suno, SkyMusic, Stable Audio, and SongR have attracted wide attention due to their impressive capabilities. Stable Audio implements an autoencoder to compress waveforms, a T5-based text embedding for text conditioning, and a transformer-based diffusion model to generate stereo songs. However, these platforms usually do not provide access to their datasets and methodologies, which may limit reproducibility and pose challenges for future research."}, {"title": "2.2 Annotated Music Datasets", "content": "2.2.1 Automatic Annotation\nAutomatic annotation-based datasets employ existing MIR algorithms to extract musi-cal attributes from symbolic music or audio music. Then the extracted attributes are either incorporated into complete descriptive texts or regarded as descriptive tags."}, {"title": null, "content": "MSD collects a million of music data, along with audio, MIDI, and tags retrieved by Echo Nest Analyze API (MIR toolkit). POP909 presents a dataset containing audio, lead sheets, and other music attributes like keys and beats. MuseCoco and Mustango extract features from the original audio and then utilize ChatGPT to incorporate them as descriptions. MuLaMCap in Noise2Music utilizes an LLM to generate a set of music descriptive texts, and then employs MuLan, a text-music embedding model to match these texts with music audio in the datasets. Despite this, there is a considerable semantic gap between datasets obtained through automatic annotation and complex human descriptions, which will reduce the accuracy of the datasets and limit model's performance."}, {"title": "2.2.2 Manual Annotation", "content": "Manual annotation-based datasets collect descriptions or tags from music websites, while others include data annotated by professional musicians. Hooktheory is a music website where users upload audio with their annotations such as melodies, chords, and beats. MTG and Mousai use corresponding tags of music on music websites as descriptive tags, while ERNIEMusic uses comments of music as music descrip-tions, and establish datasets upon these. MusicLM presents a dataset, MusicCaps, including music descriptions annotated by professional musicians. However, current datasets annotated manually are confined to expert annotations and limited descrip-tive scopes, which significantly diverge from the descriptions provided by the general public . And existing manual annotation-based datasets, where each entry is annotated by only one person, can also be prone to inaccuracies caused by human errors or biases."}, {"title": "2.3 Transformer-Based Diffusion Models", "content": "Traditional diffusion models generally employ U-Net architectures, which are lim-ited by the inductive biases of CNNs that struggle to effectively model the spatial correlations of signals and are not sensitive to scaling laws. However, transformer-based diffusion models (DiT) successfully overcome these limitations and have shown significant advantages in areas such as speech generation , image gen-eration , and video generation . Meanwhile, scalable interpolant transformers (SiT), built on the backbone of DiT, employ a flexible interpolant framework that connects two distributions more effectively than standard diffusion models, achieving remarkable results in efficiency and performance. However, the application of these transformer-based diffusion models to text-controlled song gener-ation on large-scale audio datasets has yet to be verified, and their capacity to adapt to additional control information remains unresolved."}, {"title": "3 Methodology", "content": "This study adopts supervised learning techniques alongside a dual perspective of professional and amateur annotators, and constructs the training process using an"}, {"title": null, "content": "end-to-end single-stage approach. This ensures that the AI-generated song harmonizes with human desires, while also enabling the AI model to comprehend and incorporate the standard structure of human songs."}, {"title": "3.1 Task Definition", "content": "In this paper, we present a novel generation task, Colloquial Description to Song, which aims to create songs based on given colloquial descriptive text. Unlike previous works that focus on specific aspects of text-to-song generation, our newly defined task encompasses the entire creative process, from abstract description to full song production. Additionally, it possesses distinctive characteristics different from other tasks:\n\u2022 Fully understanding colloquial language. As people typically use colloquial rather than professional descriptions, it is essential to support both colloquial and professional descriptions. However, colloquial descriptions are more challeng-ing to understand and process, necessitating the availability of relevant datasets and models for high-quality generation.\n\u2022 Sufficiently capturing musical structures of songs. The musical structure of a song primarily includes musical sections and rhyming structure. And gener-ating songs with musical structures is crucial for ensuring coherence and overall quality."}, {"title": "3.2 Caichong Music Dataset Construction", "content": "3.2.1 CaiMAP: Caichong Multitask Annotation Platform\nWe developed the CaiMAP which adopts a novel multi-stage, multi-annotator assurance approach to enhance the accuracy of music annotations and their corre-spondence with commonly understood semantics.\nBy engaging both amateurs and professionals in the annotation process, we ensured a robust representation of perspectives. This approach facilitated the creation of the CaiMD, a comprehensive dataset characterized by multi-dimensional and high-precision annotations that integrate both professional and colloquial descriptions."}, {"title": "1) Interface of CaiMAP.", "content": "To bring the complex designs to life, we developed the CaiMAP, which harmonizes this series of tasks and systems. This section will provide a brief overview of the platform.\nAccount and Login. The platform features a sophisticated access control system that allocates specific roles to each user account. Users are able to log into their accounts to review and perform their assigned tasks, and subsequently submit their results for evaluation.\nAnnotation Interface. After logging in and selecting a piece of music, annotators access a dedicated interface tailored for the task. This interface features a media player with adjustable progress bar and playback speed, alongside a specialized text box. It also includes a comprehensive lexicon and search tool, allowing users to select or search for descriptive terms needed for music annotation."}, {"title": "2) Annotation and Assurance Pipeline.", "content": "The specific annotation pipeline is shown as Fig. 1 and will be introduced in this section.\nScreening & Structure Annotation Phase. In the screening phase, annotators are required to screen the data carefully. Music pieces with poor audio quality or content involving pornography or violence that are unsuitable for the dataset should be skipped.\nIn the structure annotation phase, the platform presents the complete lyrics sen-tence by sentence, and annotators are required to insert musical section tags between the lyrics. Annotators are also required to check the accuracy of the pre-annotated phonemes and rhymes for each line. If any inaccuracies are found, they should provide their own annotations.\nStructure Quality Assurance Phase. To ensure the accuracy of the annotations, we implemented a quality assurance mechanism. Each piece of data undergoes anno-tation by two separate annotators. Subsequently, the platform autonomously verifies the congruence of the annotations. If they align, the platform seamlessly integrates the data into the dataset for the subsequent phase. In instances of disparities, both sets of annotations are referred to a quality assurance inspector for resolution. The inspector determines the correct annotation or submits an independent correction if necessary.\nDescription Annotation Phase. Data that successfully clears the structure qual-ity assurance phase becomes eligible for utilization in the music description phase."}, {"title": null, "content": "During this phase, to guarantee attentive listening and thoughtful music descriptions, annotators must listen to each music piece without interruption. Specifically, annota-tors are prohibited from writing any textual descriptions within the initial 30 seconds of the music piece. Copy and paste content is also not allowed. Additionally, limita-tions are imposed on the number of tags that can be entered and on the word count of user-generated entries.\nDescription Quality Assurance Phase. Since music description annotation involves subjective judgments and is challenging to assess, the platform employs a randomized selection process, choosing 20% of the annotation results from each anno-tator for submission to quality assurance inspectors for scoring. These scores are then logged in the platform's backend. Annotated data that successfully pass the sampling quality assurance are submitted into the dataset, whereas those that do not meet the standards are rejected.\nAdmin Spot-Check & Settlement Phase. Administrators can monitor the real-time progress of each group's work and make payments accordingly, depending on the outcomes of quality assurance checks. Annotators who consistently achieve high pass rates for their annotations will be rewarded additionally, whereas those with lower pass rates will incur penalties, thus motivating them to annotate diligently.\nTo determine whether the inspectors are competent in their work, administra-tors also have the access to randomly selected samples of their work for secondary verification."}, {"title": "3.2.2 Annotation Process", "content": "To effectively annotate music with both amateur and professional descriptions, we have engaged 213 individuals familiar with Chinese music, comprising 109 amateur enthusiasts and 104 professionals. This diverse group was recruited through campus and public efforts and includes 144 males and 69 females, aged 19 to 35 years. We have organized these participants into four groups, each assigned specific tasks as follows:\n\u2022 Professional Group. Annotate musical sections, rhyming structure and pro-vide professional descriptions.\n\u2022 Amateur Group. Provide colloquial descriptions.\n\u2022 Inspector Group. Evaluate structure annotations, and score music descrip-tions.\n\u2022 Administrator. Address and provide feedback on inquiries from various groups, and conduct random spot-checks of the groups' outcomes."}, {"title": "1) Quality Assurance Mechanisms.", "content": "Annotation Task Classification. Annotation tasks are categorized into two types: Type A (objectively assessed) and Type B (subjectively assessed) as shown in Table 1. For Type A tasks, two annotators are assigned per song, while Type B tasks require only one. The tasks are completed in phases: initially, the Structure Annotation Phase (Type A), followed by the Music Description Annotation Phase (Type B). Only annotations that pass quality assurance in each phase are included in the dataset. Discrepancies in Type A are resolved by inspectors who determine the accuracy or correct errors, whereas in Type B, inspectors score annotations on a scale of 0-100.\nClassification of Annotators. Annotators are rigorously screened and grouped based on accuracy or scores. Those with persistent low performance receive warnings or are excluded from future tasks. High performers are rewarded, and intermediate performers may face penalties.\nAdditional Quality Assurance Measures. For content suitability, annotators exclude data with inappropriate content or poor quality. During Type A tasks, engage-ment is measured by time spent and interactions with the music player. For Type B, annotators must listen attentively to the entire song, avoiding premature interactions with the platform or copying text."}, {"title": "2) Individual Grouping and Training.", "content": "Detail the grouping and training method for each group of individuals.\nGrouping. During the structure annotation phase, involving Type A tasks, each data piece requires dual annotations, engaging 104 professionals. From these, 11 individuals are selected as quality assurance inspectors based on their exceptional expertise and detailed evaluations of their resumes. The remaining 93 serve as annotators. In the music description annotation phase, Type B tasks require only one annotation per data piece, thus involving fewer participants. Here, 109 amateurs annotate using colloquial terms, while the same 93 professionals provide more technical descriptions. The 11 inspectors continue their roles, and an experienced, communicative member from our team is appointed as the platform administrator to oversee the process.\nTraining. Annotators start by pre-annotating a small dataset of 20 entries, learn-ing platform features and proper annotation practices, including correcting common errors. Inspectors receive more in-depth training, mastering the platform and develop-ing consistent evaluation standards. They review the same dataset after annotators, making judgments based on guidelines and adjusting as needed. Discrepancies in scores over 10 points are discussed in meetings to standardize evaluations. This cycle continues until inspectors consistently align in their assessments."}, {"title": "3) Automatic Annotation.", "content": "The performance of algorithms designed for annotating textual descriptions, lyrics, and musical sections is often unsatisfactory due to their dependence on subjective human evaluations. Additionally, other types of data such as phonetic alignment, vocal separation, and audio-to-MIDI conversion do not reliably match human perception. Manual annotation of these elements is challenging and time-consuming."}, {"title": null, "content": "However, advanced algorithms now exist that efficiently handle these tasks, as detailed in next section. Consequently, we employ data pre-processing algorithms for automatic annotation, eliminating the need for manual effort and seamlessly integrating this processed content into our dataset.\nMusic Genre Clustering: To reduce subjective bias and promote diverse descrip-tions across music genres, we employ MERT, a pre-trained music audio encoder, to process and cluster the encoded data into 1000 unique audio clusters. This data is then evenly distributed among annotators, ensuring a balanced variety of music gen-res for labeling. This method ensures that each cluster is annotated by a diverse range of annotators, greatly enhancing the diversity and depth of the annotated data.\nVocal & Track Separation: To prepare the dataset for tasks like accompaniment generation, melody generation, and vocal synthesis, we use Demucs to separate vocals from the musical accompaniment in audio files. Additionally, we isolate indi-vidual instrument tracks, including drums and bass, to meet the needs of a broader range of music-related tasks.\nPhonemic Level Alignment in Audio-Lyrics: To prepare audio-lyrics pairs for vocal synthesis, we use the Montreal Forced Aligner (MFA) to achieve phonemic level alignment.\nAutomatic Pre-Annotation: To enhance the efficiency of future manual annota-tions, we've introduced software for automatic pre-annotation of lyric-related tasks. This includes a specialized program for pre-annotating rhyming structure and a fine-tuned version of Qwen for identifying the main themes in lyrics. These pre-annotations provide a foundation for manual review, allowing annotators to assess and refine the automatic annotations or use them as guidelines for their annotation efforts.\nLead Sheet Transcription: To support symbolic music tasks using MIDI, we tran-scribe audio into lead sheets using Sheet Sage, software that employs the Jukebox encoding model . After the aforementioned processes, the corpus encompassed by CaiMD is depicted in Fig. 2."}, {"title": "3.3 MuDiT/MuSiT Framework", "content": "3.3.1 Framework Overview\nWe propose a novel end-to-end generation model called MuDiT/MuSiT that converts colloquial descriptions into songs. The MuDiT/MuSiT comprises several components: a fine-tuned LLM designed to generate structured lyrics; MuChin for cross-modal text-to-audio encoding; transformer-based diffusion models (DiT and SiT ) to generate songs that align with colloquial descriptions; and the use of VAE and HIFI-GAN to decode the song content into Mel spectrograms and convert them into WAV audio files.\nFirst, the system uses a fine-tuned LLM to generate structured lyrics from the user's colloquial descriptions. The generated lyrics serve as conditional input to the"}, {"title": null, "content": "cross-attention module of the DiT model. Next, the system employs MuChin for cross-modal text-to-audio encoding, converting the user's text descriptions into description embeddings. These embeddings, concatenated with random noise, are fed into the DiT model and undergo cross-attention learning with the lyrics. Following this, the system integrates transformer-based diffusion models, namely DiT and SiT, which are responsible for generating VAE latent variables. These VAE latent variables are used as indices for audio sampling in the VAE audio space. Finally, the system uses VAE and HIFI-GAN to decode the generated song content into Mel spectrograms and convert these spectrograms into high-quality WAV files. The VAE and HIFI-GAN work together to maintain the fidelity and richness of the audio, ensuring that the final output is a polished and professionally sounding song."}, {"title": "3.3.2 Transforming Noise to VAE Space", "content": "In the original audio data space, each point represents meaningful musical content. We aim for an audio space that resembles a simple distribution, such as a normal dis-tribution. However, the original audio data space is complex and high-dimensional, necessitating an audio space transformation to convert the irregular space into a regular distribution."}, {"title": null, "content": "During training, we first perform unsupervised pre-training on the VAE and HIFI-GAN using a large private dataset. By using the VAE encoder, we transform the song audio into VAE latent vectors. The VAE audio data space acts like a dictionary with a distribution similar to a normal distribution, and the VAE latent vectors serve as indices to locate the corresponding audio content within the VAE space. Therefore, the training data for DiT needs to be a subset of the VAE training data.\nSince the VAE latent space is inherently discontinuous and discrete, not every position contains meaningful musical content. Noise is distributed in the meaningless positions between the discrete points. During inference of the DiT/SiT, the original random noise does not lie within the VAE latent space. However, by iteratively sub-tracting the noise, it gradually approaches a meaningful latent point within the VAE latent space. Subsequently, we use the VAE decoder to convert this latent point into a MEL spectrogram, and then use HIFI-GAN to transform it into a waveform."}, {"title": "3.3.3 MuChin Cross-Modal Encoder", "content": "The text-audio contrastive pre-training model is crucial for the AI's understanding of colloquial descriptions and cannot be replaced with other text encoders trained on professional vocabulary.\nGiven that the vocabulary and phrases in the Chinese colloquial descriptions have not appeared in the texts used for training existing open-source text-to-audio com-parison pre-trained models, we have trained the MuChin utilizing the CaiMD dataset, modeled after architectures analogous to CLAP and MuLan. MuChin is a cross-modal encoder for word-audio pairs. When the user inputs text description prompts, MuChin converts them into description embddings, which are then concat with random noise and fed into the DiT model. This step ensures that the input text description prompt is transformed into a dense vector representation that captures the semantic nuances necessary for music generation.\nDuring the training of the MuChin model, we randomly segmented the lengths of professional and colloquial descriptions in the CaiMD dataset, as well as the lengths of the song audio. This allows MuChin to perform text-audio contrastive pre-training effectively, adapting to inputs of varying text and audio lengths."}, {"title": "3.3.4 Fine-tuned Lyric LLM", "content": "Based on considerations of model parameters, we selected the provenly effective Qwen-14B-Chat-Int4 model for lyric generation within the context of Mandarin Chinese. Our training data encompass themes extracted from lyrics along with manually anno-tated verses featuring musical sections and rhyming structures. This dataset was employed to fine-tune the model for the task of generating lyrics that included musical sections and rhyming structure from text prompts.\nSpecifically, we converted Chinese characters into pinyin to serve as the text input for the DiT model, enhancing its generalization capabilities and robustness. The gen-erated outcomes comprise tags for musical sections such as <verse>, <chorus>, and <bridge>. The DiT model is responsible for creating songs that adhere to these labeled musical structures."}, {"title": "3.3.5 Control Conditions for DiT/SiT", "content": "We applied DiT to the task of song generation, adopting this new standardized architecture to open up more possibilities for cross-domain research."}, {"title": "1) Application of Self-Attention Mechanism.", "content": "Text Description Prompt Condition. We utilize MuChin to process text descrip-tions into description embeddings. After normalizing the vector lengths, we con-catenate these embeddings with noisy samples. The concatenated vectors serve as noised latents, which are then processed by the multi-head self-attention mechanism in DiT/SiT. This multi-head self-attention mechanism enables the model to concur-rently attend to different segments of the input latent vectors, effectively capturing the complex dependencies between text descriptions and song audio content.\nAudio Prompt Condition. The noisy sample in Fig. 5 is divided into two parts: the prompt and the target. The prompt part boasts high extensibility. It can accept user-supplied a cappella, instrumental solos, or reference song audios to serve as con-trolling conditions, enabling the continuation and generation of songs containing the corresponding content or style. Interestingly, sounds like tapping on a desk or ani-mal barks can also be input and incorporated into certain musical elements of the final song. We use source separation technology on the original song to obtain vocals, drums, chord instruments, and bass instruments as part of the training data, thereby achieving this control effect."}, {"title": "2) Application of Cross-Attention Mechanism.", "content": "Lyric & Song Structure Condition. Due to the variable length of vectors for lyrics and additional structural information, direct processing via self-attention is not feasible. However, the presence of the cross-attention mechanism allows DiT/SiT to leverage the advantages of Transformers while retaining the functional benefits of diffusion models. This enables DiT/SiT to perform multi-head cross-attention learning with variable-length lyric content and audio content, treating them as parallel streams to capture the correlations between lyrics and audio. Specifically, we map the music section labels as a single token into the lyric dictionary rather than embedding it separately. This approach prevents the disruption of the temporal relationship between lyrics and musical sections."}, {"title": "3.3.6 Training and Inference of DiT/SiT", "content": "During the training of DiT/SiT, we employ DDPM with random timesteps, while during inference, we use DDIM with sequential timesteps (progressively from t to 0)."}, {"title": "1) Pre-training Phase.", "content": "We performed supervised pre-training on DiT/SiT using a large private dataset of paired \"lyrics-song audio\". The lyrics text serves as a supervision signal through cross-attention, while the song audio is used as training data in the form of VAE latent vectors.\nConsidering that during the training phase, lyrics timestamps can be used to align audio windows with corresponding lyrics. However, during the inference phase, lyrics provided by users or LLMs often lack precise timestamps. This results in the model being unable to assign appropriate lengths of lyrics text to each audio window during inference. The variability in singing speed is more pronounced compared to speech synthesis, which has relatively consistent speed, making it challenging to predict the approximate time range based on the number of words in the text.\nTherefore, we ultimately decided not to adopt the window-based audio generation method commonly used in largespeech generation models. Instead, we opted to gen-erate the entire length of the song in a single pass. During the training phase, we do not randomly segment the audio; rather, we use the entire song audio as training data. To address the issue of variable length, we add padding to the end of each audio segment to standardize the length for training purposes.\nThis approach not only resolves the challenge of dividing lyrics text into windows but also brings two additional benefits: 1) Compared to window-based generation, it better enables the DiT/SiT model to capture the overall musical structure of the song. 2) During the training phase, there is no need for timestamped lyrics data (.LRC); only the regular lyrics data (.TXT) is required."}, {"title": "2) Fine-tuning Phase.", "content": "Finally, for the task of colloquial description to song generation, we fine-tuned the DiT/SiT model based on the CaiMD dataset. This fine-tuning enables the DiT/SiT"}, {"title": null, "content": "model to generate well-structured songs that align with human colloquial expressions based on user input of colloquial descriptions and lyrics generated by the fine-tuned lyric LLM.\nConsidering that users input description prompts of varying lengths, from single words to full paragraphs, during inference, we ensure consistency between training and inference. To achieve this, we randomly segment the professional and amateur description annotations in the CaiMD dataset into varying lengths during the training of DiT/SiT. Each segmented word, phrase, or sentence is converted into a correspond-ing vector via MuChin and then concatenated along the length dimension, keeping the hidden dimension unchanged. As a result, the DiT/SiT model is exposed to descrip-tion prompts of different lengths during training, allowing it to accommodate varying lengths of text inputs during inference."}, {"title": "3.3.7 Differential Benefits of SiT over DiT", "content": "The DiT sub-model can be replaced by the SiT sub-model, and both share a similar overall neural network architecture. However, SiT introduces a new interpo-lation framework and improves the sampling mechanism, providing a more detailed exploration of the diffusion process.\nInterpolation refers to the data transformation path during the perturbation of raw data into Gaussian noise. DiT operates on discrete time steps, achieving distri-bution transformation by defining step-dependent discrete time decay and fixed noise coefficients, assuming adjacent distributions convert at a constant rate, making the transformation rigid. SiT generalizes this to continuous time, allowing the model to establish more flexible and smooth connections between the original data and Gaus-sian distributions, discarding prior assumptions in the discrete process, and choosing better-performing interpolation functions. Clearly, the continuous diffusion process aligns better with the token continuity in music. Based on this, we designed new con-tinuous interpolation functions. Specifically, for the original data \\(x^* \\sim p(x)\\)and noise \\(\\epsilon\\sim N(0,1)\\), the transformation at any time \\(x_t\\) can be expressed as:\n\\(x_t = A_t x^* + \\sigma_t \\epsilon\\),\nwhere the coefficients are:\n\\(A_t = cos(\\frac{\\pi t}{2})\\)\n\\(\\sigma_t = sin(\\frac{\\pi t}{2})\\)\nThis design ensures adaptability in music generation and mitigates infringement issues.\nRegarding the sampling mechanism, traditional DiT relies on a deterministic sam-pling process, whereas SiT introduces randomness, decoupling the diffusion coefficients between inference and training. This design reduces the risk of overfitting and improves the model's generalization capability. Specifically, SiT defines the relationship between the velocity field (rate of distribution change) and the score function (quality of gen-eration) based on the reverse-time stochastic differential equation. This allows the"}, {"title": null, "content": "estimation of complex score functions through simpler velocity fields, which can be learned by neural networks. Consequently, we can introduce significant flexibility in the music generation process, adapting to complex data distributions and avoiding the generation of mechanical-sounding music."}]}