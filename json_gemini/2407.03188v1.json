{"title": "MuDiT & MuSiT: Alignment with Colloquial Expression in Description-to-Song Generation", "authors": ["Zihao Wang", "Haoxuan Liu", "Jiaxing Yu", "Tao Zhang", "Yan Liu", "Kejun Zhang"], "abstract": "Amid the rising intersection of generative AI and human artistic processes, this study probes the critical yet less-explored terrain of alignment in human-centric automatic song composition. We propose a novel task of Colloquial Description-to-Song Generation, which focuses on aligning the generated content with colloquial human expressions. This task is aimed at bridging the gap between colloquial language understanding and auditory expression within an AI model, with the ultimate goal of creating songs that accurately satisfy human auditory expectations and structurally align with musical norms. Current datasets are limited due to their narrow descriptive scope, semantic gaps and inaccuracies. To overcome data scarcity in this domain, we present the Caichong Music Dataset (CaiMD). CaiMD is manually annotated by both professional musicians and amateurs, offering diverse perspectives and a comprehensive understanding of colloquial descriptions. Unlike existing datasets pre-set with expert annotations or auto-generated ones with inherent biases, CaiMD caters more sufficiently to our purpose of aligning AI-generated music with widespread user-desired results. Moreover, we propose an innovative single-stage framework called MuDiT/MuSiT for enabling effective human-machine alignment in song creation. This framework not only achieves cross-modal comprehension between colloquial language and auditory music perceptions but also ensures generated songs align with user-desired results. MuDiT/MuSiT employs both a fine-tuned", "sections": [{"title": "1 Introduction", "content": "Songs are an integral part of human culture, functioning as a universal medium for communication. The creation of music necessitates significant time and effort from composers to attain the anticipated results. Recently, the swift development of datasets and generative models has catalyzed rapid advances in automatic song composition. Nonetheless, as far as we are aware, at a time when the alignment efforts between generative AI in the arts and human are becoming increasingly crucial, the alignment work in automatic song composition AI has not received adequate attention. AI alignment typically refers to ensuring that AI systems can maintain consistency with human values during their operation, which is a concern highly regarded by researchers such as Geoffrey Hinton and Ilya Sutskever (former Chief Scientist of OpenAI 2). In the context of automatic song composition tasks, this translates into whether AI models can fully comprehend human colloquial descriptions, whether the generated songs meet the auditory expectations desired by humans, and whether they conform to human anticipations for song structure. 'Humans' here does not exclusively pertain to musical professionals but also encompasses a vast number of amateurs. This implies that beyond understanding professional musical vocabulary, AI models need to achieve cross-modal understanding between casual lexical text and auditory perception levels. By doing so, they can better grasp the expressions conveyed by amateurs and fulfill their demands more effectively.\nPrevious research has primarily concentrated on specific aspects of song creation, such as converting text to music and musical scores to songs, yet it has not addressed the entire creative process comprehensively. MusicGen leverages a single-stage Transformer language model to facilitate text-to-music production, employing efficient token interleaving patterns enabled by quantization-based audio codecs. Melodist accomplishes the generation from music score to song through a multi-stage process that includes synthesizing singing voices from"}, {"title": "2 Background", "content": "2.1 Text-to-Song Generation\nText-to-song generation aims to create song pieces based to the descriptive text. Previous works typically focus on specific aspects of song creation, such as text-to-music , lyrics-to-melody and music score-to-song, but do not encompass the entire creative process. For text-to-music generation, models such as MusicLM and MusicGen utilize quantization-based audio codecs to obtain residual codebooks and employ language models and diffusion models for high-quality audio music generation. For lyrics-to-melody generation, SongMASS and TeleMelody take lyrics as input and output melodies, ensuring complex and subtle lyric-melody correlations. For music score-to-song generation, Melodist adopts a two-stage method that consists of singing voice synthesis from given music score and vocal-to-accompaniment synthesis based on the singing voice and natural lan-guage prompts. Meanwhile, text-to-song generation platforms from industry, such as Suno, SkyMusic, Stable Audio, and SongR 7, have attracted wide attention due to their impressive capabilities. Stable Audio implements an autoencoder to compress waveforms, a T5-based text embedding for text conditioning, and a transformer-based diffusion model to generate stereo songs. However, these platforms usually do not provide access to their datasets and methodologies, which may limit reproducibility and pose challenges for future research.\n2.2 Annotated Music Datasets\n2.2.1 Automatic Annotation\nAutomatic annotation-based datasets employ existing MIR algorithms to extract musi-cal attributes from symbolic music or audio music. Then the extracted attributes are either incorporated into complete descriptive texts or regarded as descriptive tags."}, {"title": "3 Methodology", "content": "This study adopts supervised learning techniques alongside a dual perspective of professional and amateur annotators, and constructs the training process using an"}, {"title": "3.1 Task Definition", "content": "In this paper, we present a novel generation task, Colloquial Description to Song, which aims to create songs based on given colloquial descriptive text. Unlike previous works that focus on specific aspects of text-to-song generation, our newly defined task encompasses the entire creative process, from abstract description to full song production. Additionally, it possesses distinctive characteristics different from other tasks:\nFully understanding colloquial language. As people typically use colloquial rather than professional descriptions, it is essential to support both colloquial and professional descriptions. However, colloquial descriptions are more challeng-ing to understand and process, necessitating the availability of relevant datasets and models for high-quality generation.\nSufficiently capturing musical structures of songs. The musical structure of a song primarily includes musical sections and rhyming structure. And gener-ating songs with musical structures is crucial for ensuring coherence and overall quality."}, {"title": "3.2 Caichong Music Dataset Construction", "content": "3.2.1 CaiMAP: Caichong Multitask Annotation Platform\nWe developed the CaiMAP which adopts a novel multi-stage, multi-annotator assurance approach to enhance the accuracy of music annotations and their corre-spondence with commonly understood semantics.\nBy engaging both amateurs and professionals in the annotation process, we ensured a robust representation of perspectives. This approach facilitated the creation of the CaiMD, a comprehensive dataset characterized by multi-dimensional and high-precision annotations that integrate both professional and colloquial descriptions.\n1) Interface of CaiMAP.\nTo bring the complex designs to life, we developed the CaiMAP, which harmonizes this series of tasks and systems. This section will provide a brief overview of the platform.\nAccount and Login. The platform features a sophisticated access control system that allocates specific roles to each user account. Users are able to log into their accounts to review and perform their assigned tasks, and subsequently submit their results for evaluation.\nAnnotation Interface. After logging in and selecting a piece of music, annotators access a dedicated interface tailored for the task. This interface features a media player with adjustable progress bar and playback speed, alongside a specialized text box. It also includes a comprehensive lexicon and search tool, allowing users to select or search for descriptive terms needed for music annotation.\n2) Annotation and Assurance Pipeline.\nThe specific annotation pipeline is shown as Fig. 1 and will be introduced in this section.\nScreening & Structure Annotation Phase. In the screening phase, annotators are required to screen the data carefully. Music pieces with poor audio quality or content involving pornography or violence that are unsuitable for the dataset should be skipped.\nIn the structure annotation phase, the platform presents the complete lyrics sen-tence by sentence, and annotators are required to insert musical section tags between the lyrics. Annotators are also required to check the accuracy of the pre-annotated phonemes and rhymes for each line. If any inaccuracies are found, they should provide their own annotations.\nStructure Quality Assurance Phase. To ensure the accuracy of the annotations, we implemented a quality assurance mechanism. Each piece of data undergoes anno-tation by two separate annotators. Subsequently, the platform autonomously verifies the congruence of the annotations. If they align, the platform seamlessly integrates the data into the dataset for the subsequent phase. In instances of disparities, both sets of annotations are referred to a quality assurance inspector for resolution. The inspector determines the correct annotation or submits an independent correction if necessary.\nDescription Annotation Phase. Data that successfully clears the structure qual-ity assurance phase becomes eligible for utilization in the music description phase.\nDuring this phase, to guarantee attentive listening and thoughtful music descriptions, annotators must listen to each music piece without interruption. Specifically, annota-tors are prohibited from writing any textual descriptions within the initial 30 seconds of the music piece. Copy and paste content is also not allowed. Additionally, limita-tions are imposed on the number of tags that can be entered and on the word count of user-generated entries.\nDescription Quality Assurance Phase. Since music description annotation involves subjective judgments and is challenging to assess, the platform employs a randomized selection process, choosing 20% of the annotation results from each anno-tator for submission to quality assurance inspectors for scoring. These scores are then logged in the platform's backend. Annotated data that successfully pass the sampling quality assurance are submitted into the dataset, whereas those that do not meet the standards are rejected.\nAdmin Spot-Check & Settlement Phase. Administrators can monitor the real-time progress of each group's work and make payments accordingly, depending on the outcomes of quality assurance checks. Annotators who consistently achieve high pass rates for their annotations will be rewarded additionally, whereas those with lower pass rates will incur penalties, thus motivating them to annotate diligently.\nTo determine whether the inspectors are competent in their work, administra-tors also have the access to randomly selected samples of their work for secondary verification.\n3.2.2 Annotation Process\nTo effectively annotate music with both amateur and professional descriptions, we have engaged 213 individuals familiar with Chinese music, comprising 109 amateur enthusiasts and 104 professionals. This diverse group was recruited through campus and public efforts and includes 144 males and 69 females, aged 19 to 35 years. We have organized these participants into four groups, each assigned specific tasks as follows:\nProfessional Group. Annotate musical sections, rhyming structure and pro-vide professional descriptions.\nAmateur Group. Provide colloquial descriptions.\nInspector Group. Evaluate structure annotations, and score music descrip-tions.\nAdministrator. Address and provide feedback on inquiries from various groups, and conduct random spot-checks of the groups' outcomes.\n1) Quality Assurance Mechanisms.\nAnnotation Task Classification. Annotation tasks are categorized into two types: Type A (objectively assessed) and Type B (subjectively assessed) as shown in Table 1.\nFor Type A tasks, two annotators are assigned per song, while Type B tasks require only one. The tasks are completed in phases: initially, the Structure Annotation Phase (Type A), followed by the Music Description Annotation Phase (Type B). Only annotations that pass quality assurance in each phase are included in the dataset.\nDiscrepancies in Type A are resolved by inspectors who determine the accuracy or correct errors, whereas in Type B, inspectors score annotations on a scale of 0-100.\nClassification of Annotators. Annotators are rigorously screened and grouped based on accuracy or scores. Those with persistent low performance receive warnings or are excluded from future tasks. High performers are rewarded, and intermediate performers may face penalties.\nAdditional Quality Assurance Measures. For content suitability, annotators exclude data with inappropriate content or poor quality. During Type A tasks, engage-ment is measured by time spent and interactions with the music player. For Type B, annotators must listen attentively to the entire song, avoiding premature interactions with the platform or copying text.\n2) Individual Grouping and Training.\nDetail the grouping and training method for each group of individuals.\nGrouping. During the structure annotation phase, involving Type A tasks, each data piece requires dual annotations, engaging 104 professionals. From these, 11 individuals are selected as quality assurance inspectors based on their exceptional expertise and detailed evaluations of their resumes. The remaining 93 serve as annotators. In the music description annotation phase, Type B tasks require only one annotation per data piece, thus involving fewer participants. Here, 109 amateurs annotate using colloquial terms, while the same 93 professionals provide more technical descriptions. The 11 inspectors continue their roles, and an experienced, communicative member from our team is appointed as the platform administrator to oversee the process.\nTraining. Annotators start by pre-annotating a small dataset of 20 entries, learn-ing platform features and proper annotation practices, including correcting common errors. Inspectors receive more in-depth training, mastering the platform and develop-ing consistent evaluation standards. They review the same dataset after annotators, making judgments based on guidelines and adjusting as needed. Discrepancies in scores over 10 points are discussed in meetings to standardize evaluations. This cycle continues until inspectors consistently align in their assessments.\n3) Automatic Annotation.\nThe performance of algorithms designed for annotating textual descriptions, lyrics, and musical sections is often unsatisfactory due to their dependence on subjective human evaluations. Additionally, other types of data such as phonetic alignment, vocal separation, and audio-to-MIDI conversion do not reliably match human perception. Manual annotation of these elements is challenging and time-consuming.\nHowever, advanced algorithms now exist that efficiently handle these tasks, as detailed in next section. Consequently, we employ data pre-processing algorithms for automatic annotation, eliminating the need for manual effort and seamlessly integrating this processed content into our dataset.\nMusic Genre Clustering: To reduce subjective bias and promote diverse descrip-tions across music genres, we employ MERT , a pre-trained music audio encoder, to process and cluster the encoded data into 1000 unique audio clusters. This data is then evenly distributed among annotators, ensuring a balanced variety of music gen-res for labeling. This method ensures that each cluster is annotated by a diverse range of annotators, greatly enhancing the diversity and depth of the annotated data.\nVocal & Track Separation: To prepare the dataset for tasks like accompaniment generation, melody generation, and vocal synthesis, we use Demucs to separate vocals from the musical accompaniment in audio files. Additionally, we isolate indi-vidual instrument tracks, including drums and bass, to meet the needs of a broader range of music-related tasks.\nPhonemic Level Alignment in Audio-Lyrics: To prepare audio-lyrics pairs for vocal synthesis, we use the Montreal Forced Aligner (MFA) to achieve phonemic level alignment.\nAutomatic Pre-Annotation: To enhance the efficiency of future manual annota-tions, we've introduced software for automatic pre-annotation of lyric-related tasks. This includes a specialized program for pre-annotating rhyming structure and a fine-tuned version of Qwen for identifying the main themes in lyrics. These pre-annotations provide a foundation for manual review, allowing annotators to assess and refine the automatic annotations or use them as guidelines for their annotation efforts.\nLead Sheet Transcription: To support symbolic music tasks using MIDI, we tran-scribe audio into lead sheets using Sheet Sage , software that employs the Jukebox encoding model .\nAfter the aforementioned processes, the corpus encompassed by CaiMD is depicted in Fig. 2."}, {"title": "3.3 MuDiT/MuSiT Framework", "content": "3.3.1 Framework Overview\nWe propose a novel end-to-end generation model called MuDiT/MuSiT that converts colloquial descriptions into songs. The MuDiT/MuSiT comprises several components: a fine-tuned LLM designed to generate structured lyrics; MuChin for cross-modal text-to-audio encoding; transformer-based diffusion models (DiT and SiT ) to generate songs that align with colloquial descriptions; and the use of VAE and HIFI-GAN to decode the song content into Mel spectrograms and convert them into WAV audio files.\nFirst, the system uses a fine-tuned LLM to generate structured lyrics from the user's colloquial descriptions. The generated lyrics serve as conditional input to the\ncross-attention module of the DiT model. Next, the system employs MuChin for cross-modal text-to-audio encoding, converting the user's text descriptions into description embeddings. These embeddings, concatenated with random noise, are fed into the DiT model and undergo cross-attention learning with the lyrics. Following this, the system integrates transformer-based diffusion models, namely DiT and SiT, which are responsible for generating VAE latent variables. These VAE latent variables are used as indices for audio sampling in the VAE audio space. Finally, the system uses VAE and HIFI-GAN to decode the generated song content into Mel spectrograms and convert these spectrograms into high-quality WAV files. The VAE and HIFI-GAN work together to maintain the fidelity and richness of the audio, ensuring that the final output is a polished and professionally sounding song.\n3.3.2 Transforming Noise to VAE Space\nIn the original audio data space, each point represents meaningful musical content. We aim for an audio space that resembles a simple distribution, such as a normal dis-tribution. However, the original audio data space is complex and high-dimensional, necessitating an audio space transformation to convert the irregular space into a regular distribution."}, {"title": "3.3.3 MuChin Cross-Modal Encoder", "content": "The text-audio contrastive pre-training model is crucial for the AI's understanding of colloquial descriptions and cannot be replaced with other text encoders trained on professional vocabulary.\nGiven that the vocabulary and phrases in the Chinese colloquial descriptions have not appeared in the texts used for training existing open-source text-to-audio com-parison pre-trained models, we have trained the MuChin utilizing the CaiMD dataset, modeled after architectures analogous to CLAP and MuLan. MuChin is a cross-modal encoder for word-audio pairs. When the user inputs text description prompts, MuChin converts them into description embddings, which are then concat with random noise and fed into the DiT model. This step ensures that the input text description prompt is transformed into a dense vector representation that captures the semantic nuances necessary for music generation.\nDuring the training of the MuChin model, we randomly segmented the lengths of professional and colloquial descriptions in the CaiMD dataset, as well as the lengths of the song audio. This allows MuChin to perform text-audio contrastive pre-training effectively, adapting to inputs of varying text and audio lengths."}, {"title": "3.3.4 Fine-tuned Lyric LLM", "content": "Based on considerations of model parameters, we selected the provenly effective Qwen-14B-Chat-Int4 model for lyric generation within the context of Mandarin Chinese. Our training data encompass themes extracted from lyrics along with manually anno-tated verses featuring musical sections and rhyming structures. This dataset was employed to fine-tune the model for the task of generating lyrics that included musical sections and rhyming structure from text prompts.\nSpecifically, we converted Chinese characters into pinyin to serve as the text input for the DiT model, enhancing its generalization capabilities and robustness. The gen-erated outcomes comprise tags for musical sections such as <verse>, <chorus>, and <bridge>. The DiT model is responsible for creating songs that adhere to these labeled musical structures."}, {"title": "3.3.5 Control Conditions for DiT/SiT", "content": "We applied DiT to the task of song generation, adopting this new standardized architecture to open up more possibilities for cross-domain research.\n1) Application of Self-Attention Mechanism.\nText Description Prompt Condition. We utilize MuChin to process text descrip-tions into description embeddings. After normalizing the vector lengths, we con-catenate these embeddings with noisy samples. The concatenated vectors serve as noised latents, which are then processed by the multi-head self-attention mechanism in DiT/SiT. This multi-head self-attention mechanism enables the model to concur-rently attend to different segments of the input latent vectors, effectively capturing the complex dependencies between text descriptions and song audio content.\nAudio Prompt Condition. The noisy sample in Fig. 5 is divided into two parts: the prompt and the target. The prompt part boasts high extensibility. It can accept user-supplied a cappella, instrumental solos, or reference song audios to serve as con-trolling conditions, enabling the continuation and generation of songs containing the corresponding content or style. Interestingly, sounds like tapping on a desk or ani-mal barks can also be input and incorporated into certain musical elements of the final song. We use source separation technology on the original song to obtain vocals, drums, chord instruments, and bass instruments as part of the training data, thereby achieving this control effect."}, {"title": "3.3.6 Training and Inference of DiT/SiT", "content": "During the training of DiT/SiT, we employ DDPM with random timesteps, while during inference, we use DDIM with sequential timesteps (progressively from t to 0).\n1) Pre-training Phase.\nWe performed supervised pre-training on DiT/SiT using a large private dataset of paired \"lyrics-song audio\". The lyrics text serves as a supervision signal through cross-attention, while the song audio is used as training data in the form of VAE latent vectors.\nConsidering that during the training phase, lyrics timestamps can be used to align audio windows with corresponding lyrics. However, during the inference phase, lyrics provided by users or LLMs often lack precise timestamps. This results in the model being unable to assign appropriate lengths of lyrics text to each audio window during inference. The variability in singing speed is more pronounced compared to speech synthesis, which has relatively consistent speed, making it challenging to predict the approximate time range based on the number of words in the text.\nTherefore, we ultimately decided not to adopt the window-based audio generation method commonly used in large speech generation models. Instead, we opted to gen-erate the entire length of the song in a single pass. During the training phase, we do not randomly segment the audio; rather, we use the entire song audio as training data. To address the issue of variable length, we add padding to the end of each audio segment to standardize the length for training purposes.\nThis approach not only resolves the challenge of dividing lyrics text into windows but also brings two additional benefits: 1) Compared to window-based generation, it better enables the DiT/SiT model to capture the overall musical structure of the song. 2) During the training phase, there is no need for timestamped lyrics data (.LRC); only the regular lyrics data (.TXT) is required.\n2) Fine-tuning Phase.\nFinally, for the task of colloquial description to song generation, we fine-tuned the DiT/SiT model based on the CaiMD dataset. This fine-tuning enables the DiT/SiT"}, {"title": "3.3.7 Differential Benefits of SiT over DiT", "content": "The DiT sub-model can be replaced by the SiT sub-model, and both share a similar overall neural network architecture. However, SiT introduces a new interpo-lation framework and improves the sampling mechanism, providing a more detailed exploration of the diffusion process.\nInterpolation refers to the data transformation path during the perturbation of raw data into Gaussian noise. DiT operates on discrete time steps, achieving distri-bution transformation by defining step-dependent discrete time decay and fixed noise coefficients, assuming adjacent distributions convert at a constant rate, making the transformation rigid. SiT generalizes this to continuous time, allowing the model to establish more flexible and smooth connections between the original data and Gaus-sian distributions, discarding prior assumptions in the discrete process, and choosing better-performing interpolation functions. Clearly, the continuous diffusion process aligns better with the token continuity in music. Based on this, we designed new con-tinuous interpolation functions. Specifically, for the original data x* ~ p(x)and noise \n$\\epsilon$~ N(0,1), the transformation at any time xt can be expressed as:\n$x_t = a_t x^* + \\sigma_t \\epsilon$,\nwhere the coefficients are:\n$a_t = \\cos( \\frac{\\pi t}{2}) \\\\ \\sigma_t = \\sin(\\frac{\\pi t}{2})$\nThis design ensures adaptability in music generation and mitigates infringement issues.\nRegarding the sampling mechanism, traditional DiT relies on a deterministic sam-pling process, whereas SiT introduces randomness, decoupling the diffusion coefficients between inference and training. This design reduces the risk of overfitting and improves the model's generalization capability. Specifically, SiT defines the relationship between the velocity field (rate of distribution change) and the score function (quality of gen-eration) based on the reverse-time stochastic differential equation. This allows the"}]}