{"title": "Certified Adversarial Robustness via Partition-based Randomized Smoothing", "authors": ["Hossein Goli", "Farzan Farnia"], "abstract": "A reliable application of deep neural network classifiers requires robustness certificates against adversarial perturbations. Gaussian smoothing is a widely analyzed approach to certifying robustness against norm-bounded perturbations, where the certified prediction radius depends on the variance of the Gaussian noise and the confidence level of the neural net's prediction under the additive Gaussian noise. However, in application to high-dimensional image datasets, the certified radius of the plain Gaussian smoothing could be relatively small, since Gaussian noise with high variances can significantly harm the visibility of an image. In this work, we propose the Pixel Partitioning-based Randomized Smoothing (PPRS) methodology to boost the neural net's confidence score and thus the robustness radius of the certified prediction. We demonstrate that the proposed PPRS algorithm improves the visibility of the images under additive Gaussian noise. We discuss the numerical results of applying PPRS to standard computer vision datasets and neural network architectures. Our empirical findings indicate a considerable improvement in the certified accuracy and stability of the prediction model to the additive Gaussian noise in randomized smoothing.", "sections": [{"title": "Introduction", "content": "While deep neural network (DNN) classifiers have attained state-of-the-art performance in benchmark image and sound recognition tasks, they are widely known to be vulnerable to minor perturbations to their input data, commonly regarded as adversarial attacks. Since the introduction of adversarial perturbations in [1-3], adversarial attack and defense methods have been extensively studied in the literature. In particular, the development of defense methods with certified robustness against norm-bounded perturbations has received significant attention over recent years.\nA standard approach to certifiably robust classification is applying randomized smoothing to the DNN classifier, where the DNN's input is perturbed by a random perturbation. As demonstrated in [4], an additive Gaussian noise with an isotropic covariance matrix \\(\\sigma\u00b2I\\) can result in certified robustness against perturbations with bounded L2-norm, where the certified prediction radius depends on the product of the noise standard deviation and the confidence score of the DNN classifier under the additive noise. Consequently, while a higher noise variance may seem to offer a greater certified radius, the resulting drop in the prediction confidence under a higher noise variance could lead to a weaker robustness certificate. Therefore, a potential idea for boosting the certified robustness radius of the randomized smoothing approach is to improve the classifier's prediction confidence score, which can be interpreted as the visibility of the input image, for a noisy image under a high noise variance.\nIn this work, we propose the Pixel Partitioning-based Randomized Smoothing (PPRS) method to improve the visibility of noisy images and, as a result, the certified prediction radius of deep neural network classifiers. Following the proposed PPRS method, the classifier performs a partitioning of the noisy pixels and assigns the mean pixel intensity of the partition's pixels to the pixels in every pixel partition. If the choice of the pixel partitions correlates with the image semantics, then one can expect that the partitioning-based averaging"}, {"title": "Related Work", "content": "Certified robustness against adversarial attacks has been extensively studied in the literature. The randomized smoothing approach in [4] provides a standard framework to certify the predictions of a general classifier against adversarial noise. This approach has been extended to adversarial perturbations with bounded Lo-norm [8,9], L\u2081-norm [10], and L\u221e-norm [11]. The randomized smoothing technique has also been utilized for non-classification tasks, including the image segmentation [12] and community detection [13]. In addition, [14] proposes a general framework, extending [4]'s method to find a robust certified radius for any noise distribution on different norms (L\u221e, L\u2081 and L2). [15] proposes a new approach to certified robustness that uses training data and a direction oracle encoding information about the decision boundary, which is only partially based on randomized smoothing to certify robustness.\nRegarding the extensions of the randomized smoothing approach, [16] provides a framework extending [4]'s method, but as also discussed in the paper it may not be sufficiently effective against L2-norm-bounded perturbations. [17] provides a novel approach to randomized smoothing in settings with multiplicative parameters of input transformations where a gamma correction perturbation is utilized. Concerning the applications of certified robustness, [18] provides an adversarial framework with certified robustness guarantees for time series data using its statistical features.\nOn the other hand, the effectiveness of randomized smoothing has been evaluated and examined in [19]. This paper reports the gap between certified accuracy of the randomized smoothing methods and the robustness achieved by standard adversarial training methods. [19]'s findings are consistent with our observation of the dropped confidence score of the classifier for noisy data, and the proposed PPRS attempts to close [19]'s observed gap. Also, [20] discusses the curse of dimensionality of randomized smoothing, which can be mitigated by applying our proposed partitioning scheme. This property of compact feature representations and their effects on adversarial robustness and generalizability have also been discussed in [21]."}, {"title": "Preliminaries", "content": null}, {"title": "Classification and Adversarial Attacks", "content": "Consider a labeled sample \\((x, y)\\) where \\(x \\in X \\subset \\mathbb{R}^d\\) denotes a \\(d\\)-dimensional feature vector and \\(y \\in Y = \\{1,2,..., k\\}\\) is a \\(k\\)-ary label. A prediction function \\(f : X \\rightarrow Y\\) maps the input \\(x\\) to a label in \\(Y\\). We use the standard 0/1-loss to evaluate the performance of the classifier on a labeled sample, i.e. the loss will be zero if the prediction is correct and, otherwise, will be one:\n\\[l_{0/1} (f(x), y) =\\begin{cases}0 & \\text{if } f(x) = y,\\\\1 & \\text{if } f(x) \\neq y.\\end{cases}\\]\nHowever, standard neural network classifiers have been frequently observed to lack robustness against norm-bounded adversarial perturbations [1-3]. To generate an adversarial perturbation \\(\\delta \\in \\mathbb{R}^d\\) with \\(\\epsilon\\)-bounded norm, one can solve the following optimization problem:\n\\[\\max_{\\delta: ||\\delta|| \\leq \\epsilon} l(f(x + \\delta), y),\\tag{1}\\]"}, {"title": "Certified Robustness via Randomized Smoothing", "content": "Certifying a classifier's robustness against norm-bounded perturbations is required for high-stake machine learning applications. The randomized smoothing approach in [4] is a standard framework to achieve certified adversarial robustness. Following this approach, we consider an isotropic Gaussian vector \\(Z \\sim \\mathcal{N}(0, \\sigma^2I_{d \\times d})\\), where \\(I_{d \\times d}\\) denotes the \\(d\\)-dimensional identity matrix, and define the following prediction rule \\(f^{GS(\\sigma)}\\) for a given prediction rule \\(f\\):\n\\[f^{GS(\\sigma)}(x) := \\underset{c \\in Y}{\\operatorname{argmax}} \\mathbb{P}(f(x + Z) = c).\\]\nIn other words, \\(f^{GS(\\sigma)}\\) outputs the most likely label for a noisy version of \\(x\\) perturbed with the zero-mean Gaussian vector \\(Z\\). [4] proves the following robustness guarantee for the defined \\(f^{GS(\\sigma)}\\):\nTheorem 1 (Theorem 1 from [4]). Consider a prediction rule \\(f : X \\rightarrow Y\\). For sample \\(x \\in X\\) classified as \\(f^{GS(\\sigma)} (x) = c_A\\), we define the prediction confidence score as \\(C(x) = (\\Phi^{-1}(p_A(x)) \u2013 \\Phi^{-1}(p_B(x)))\\) where \\(\\Phi^{-1} : (0,1) \\rightarrow \\mathbb{R}\\) is the inverse-CDF of the standard Gaussian distribution \\(\\mathcal{N}(0,1)\\) and \\(p_A(x), p_B(x)\\) are defined as:\n\\[p_A(x) = \\mathbb{P}(f(x + Z) = c_A),\\]\n\\[p_B(x) = \\underset{c \\neq c_A}{\\operatorname{max}} \\mathbb{P}(f(x + Z) = c).\\]\nThen, \\(f^{GS(\\sigma)} (x + \\delta) = c_A\\) for every L2-norm-bounded perturbation \\(||\\delta||_2 \\leq \\sigma C(x)\\).\nIn the next sections, we will discuss the trade-off between the standard deviation parameter \\(\\sigma\\) for the additive Gaussian noise and the averaged prediction confidence score \\(c(x)\\) under the random Gaussian perturbation."}, {"title": "Partition-based Randomized Smoothing", "content": "As discussed earlier, the certified prediction robustness in the randomized smoothing approach is the product of the standard deviation of the Gaussian noise and the prediction confidence score under the additive noise. However, if \\(\\sigma\\) is chosen to be moderately large, the average prediction confidence score could drop significantly, leading to a smaller certified prediction radius and hence lower certified accuracy. Therefore, one idea to improve the certified performance by randomized smoothing is to develop a method to improve the visibility of images affected by additive Gaussian noise.\nTo achieve this goal, we propose the Pixel Partitioning-based Randomized Smoothing (PPRS), according to which we partition the pixels into multiple groups and use the averaged pixel intensities within each partition. Mathematically, we group the pixels into \\(p\\) partitions in \\(S(x) := \\{S_1(x),\u2026, S_p(x)\\}\\) where \\(S_i(x)\\) denotes the subset of pixels corresponding to the \\(i\\)th superpixel. We define the partition averaging matrix \\(A_s \\in [0, 1]^{d \\times p}\\) as\n\\[A_{i,j} :=\\begin{cases}\\frac{1}{|S_j|} & \\text{if Pixel } i \\in S_j\\\\0 & \\text{otherwise.}\\end{cases}\\tag{2}\\]\nNote that the PPRS transformation of an image \\(x\\) will be \\(A_sx\\), resulting in the following definition of the PPRS-based classification rule.\n\\[PPRS(f, x) := f (A_sx).\\tag{3}\\]\nApplying the PPRS transformation to noisy image \\(x + Z\\), where \\(Z \\sim \\mathcal{N}(0,\\sigma^2I)\\), results in an effective noise standard deviation \\(\\frac{\\sigma}{\\sqrt{|S_j|}}\\) for the pixels in partition \\(S_j\\) because the Gaussian noise for \\(S_j\\'s\\) pixels are"}, {"title": "Numerical Results", "content": "We numerically evaluated our proposed PPRS methodology on three standard image datasets: MNIST [22], Fashion MNIST [23], CIFAR-10 [24], and ImageNet [25]. In the experiments, we used standard pre-trained"}, {"title": "Numerical Comparison of PPRS vs. Baseline Randomized Smoothing Methods", "content": "Figure 2 visualizes the original and noisy versions of four randomly selected ImageNet samples in the experiments. These examples suggest that the PPRS method could relatively increase the visual quality of the samples and, as a result, the confidence score of the neural net classifier. In addition, in the Appendix, we provide additional numerical results supporting that the proposed superpixel-based algorithm will be more effective under higher noise variances, since grouping of pixels helps to reduce the effective noise variance and improve input image's visibility, as shown in Figure 2.\nIn addition, Figure 3 shows ImageNet-based and CIFAR-10-based comparisons between certified accuracy scores achieved by the vanilla randomized smoothing (RS) and PPRS methods for different standard deviation parameters of the additive Gaussian noise. These plots indicate that using the same noise standard deviation, the super-pixel-based randomized smoothing can achieve a higher certified accuracy at the same robustness radius, which could be attributed to the higher visibility of perturbed samples after the partition-based averaging in PPRS."}, {"title": "Ablation Studies", "content": "Super-pixel Schemes. To study the effect of choosing different super-pixel schemes, as shown in Figure 6, we computed and visualized the super-pixels obtained by three different super-pixel schemes: Felzenszwalb's method [5], Quickshift [6], and SLIC [7]. Note that the Quickshift method is a local mode-seeking algorithm that functions based on the color and location of the pixels. On the other hand, Felzenszwalb's method is a graph-based segmentation algorithm. The empirical results in Figure 6 indicate that the super-pixel-based partitioning followed by PPRS is flexible to the choice of the super-pixel scheme and can perform satisfactorily using different schemes. All the mentioned methods performed effectively compared to vanilla randomized smoothing, validating our hypothesis on the impacts of partitioning pixels on the robustness offered by randomized smoothing.\nSuper-pixel Size. We tested the effect of the number of super-pixel components on the PPRS results using the SLIC super-pixel method. As expected, when the number of components was selected to be significantly large, super-pixels were shrunk to nearly one pixel, and the super-pixel-based PPRS method was reduced to the vanilla randomized smoothing. On the other hand, when the number of super-pixels was too few, the partitions did not capture the details of the input image, leading to semantically less meaningful images. As shown in Figure 10, the number of components should be properly tuned for every dataset. Furthermore, Figures 7 and 9 show PPRS-recovered versions of noisy MNIST, Fashion MNIST, and ImageNet samples using different numbers of super-pixel.\nPPRS noise hyper-parameters. In our experiments, we tested the impact of noise hyperparameters and observed that the proposed super-pixel-based method is more effective under higher noise levels as shown in Figure 11. Performing computations with super-pixel variables naturally leads to some bias in the neural network's classification, while the noise reduction will become more effective. Therefore, by including higher"}, {"title": "Conclusion", "content": "In this work, we proposed a partitioning-based randomized smoothing algorithm to achieve higher certified robustness scores. Our method is based on grouping the input features into semantically meaningful partitions that can preserve the visibility of the input image. We discussed how such a partitioning scheme could result in a lower input dimension in the application of the Gaussian smoothing method, and proposed applying standard super-pixels to perform the partitioning-based randomized smoothing. Our numerical results suggest the improvements in certified accuracy achieved by the super-pixel-based approach. A relevant future direction is to extend the proposed framework to non-computer vision settings, e.g. to text and audio-related classification problems. Moreover, applying the PPRS approach to non-Gaussian randomized smoothing methods will be another interesting topic for future exploration."}, {"title": "Limitations and Broader Impact", "content": "As discussed in the paper, the proposed super-pixel-based robust classification method applies only to image classification problems where the input image can be clustered into a group of semantically meaningful"}, {"title": "Appendix", "content": null}, {"title": "Proof of Theorem 2", "content": "We begin by showing the following lemma.\nLemma 1. Suppose that matrix \\(A_s\\) denotes the linear transformation following the group averaging of partitions in \\(S(x) := \\{S_1(x),\u2026, S_p(x)\\} \\). Here, we define every \\((i, j)\\)th entry of \\(A_s \\in [0, 1]^{d \\times p}\\) as \\(A_{i,j} = \\frac{1}{|S_j|}\\) if the \\(i\\)-th pixel belongs to \\(S_j\\) with size \\(|S_j|\\) and \\(A_{i,j} = 0\\) otherwise. Then,\n\\[A A^\\intercal = A_s A^\\intercal_s\\]\nProof. To show this theorem note that for every \\(i, j\\)\n\\[A^A^\\intercal_{li,j}\\]\n\\[=\\sum_{k=1}^d A_{s ik}A_{s kj}\\]\n\\[= \\sum_{k \\in S(i) \\cap S(j)}A_{s ik}A_{s kj}\\]\n\\[= \\begin{cases}0 & \\text{if } S_i \\neq S_j\\\\ \\frac{1}{|S_i| \\frac{1}{|S_j|} = \\frac{1}{|S_i|} & \\text{if } S_i = S_j\\end{cases}\\]\n\\[= A^\\intercal_{li,j}\\]\nThe above completes the lemma's proof.\nUsing the above lemma, we show that the map \\(h_s(x) := A_s(x)x\\) satisfies the following for every vectors \\(x, \\delta\\)\n\\[||h_s(x + \\delta) - h_s (x + A_s(x)\\delta) ||\\]\n\\[= ||A_{s(x + \\delta)} (x + \\delta) - A_{s(x + A_s(x)\\delta)} (x + A_s(x)\\delta) ||\\]\n\\[= ||A_{s(x + \\delta)} (x + A_{s(x + \\delta)}\\delta) \u2013 A_{s(x + A_s(x)\\delta)} (x + A_s(x)\\delta) ||\\]\n\\[\\leq ||A_{s(x+\\delta)} (x + A_{s(x+\\delta)}\\delta) \u2013 A_{s(x+A_s(x)\\delta)} (x + A_{s(x+\\delta)}\\delta) ||\\]\n\\[+ || A_{s(x+A_s(x)\\delta)} (x + A_{s(x+\\delta)}\\delta) \u2013 A_{s(x+A_s(x)\\delta)} (x + A_s(x)\\delta) ||\\]\n\\[= || (A_{s(x+\\delta)} - A_{s(x+A_s(x)\\delta)}) (x + A_{s(x+\\delta)}\\delta) || + ||A_{s(x+A_s(x)\\delta)} (A_{s(x+\\delta)} - A_s(x))\\delta||\\]\n\\[\\leq \\rho ||I \u2013 A_{s(x)}||_2 ||(|\\delta||) + \\rho||A_{s(x+A_s(x) \\delta)} ||_2 ||\\delta||_2 \\]\n\\[<2\\rho||\\delta||^2 + \\rho||\\delta|| |\\delta||.\\]\nTherefore, given every \\(x\\), we consider the certified robustness guarantee of Corollary 1 for the fixed partitioning matrix \\(A_s(x)\\), which shows the labeling assigned by the Gaussian smoothed version of \\(h_s(x)(x + A_s(x)\\delta)\\) remains unchanged if \\(||A_s(x)\\delta|| \\leq \\sigma C_{PPRS(f^{GS(\\sigma)})(x)}\\). On the other hand, Theorem 1 from [4] together with the above bound imply that under a perturbation \\(\\delta\\) satisfying \\(2\\rho||\\delta||^2 + \\rho||\\delta|| |\\delta|| \\leq \\sigma C_{PPRS(f^{GS(\\sigma)})(x)}\\) the labels assigned by \\(f^{PPRS}\\) to \\(x + \\delta\\) and \\(x + A_s(x)\\delta\\) will be the same. Therefore, assuming that \\(|\\delta|| + 2\\rho||\\delta||^2 + \\rho||\\delta|| |\\delta|| \\leq \\sigma C_{PPRS(f^{GS(\\sigma)})(x)}\\), the certified robustness will hold, which under the assumption that \\(max\\{|\\delta||, |\\delta||\\} < 1\\) will imply that if \\((1 + 3\\rho)|\\delta|| \\leq \\sigma C_{PPRS(f^{GS(\\sigma)})(x)}\\), the certified robustness will hold and the prediction for \\(x\\) and \\(x + \\delta\\) will be identical."}, {"title": "Additional Numerical Results and Visualizatons", "content": "To further investigate our method, we will also compare it with Yang et al. [14] proposed framework for certifying a robust radius for a given classifier. They introduce the notion of Wulff Crystal and spherical level sets and propose a general framework that can use different distribution noises and find a correspondent robust radii. Because our work focuses on L2 norm we choose Exponential, PowerLaw, and Gaussian distributions and follow Yang et al. method. We compare these baselines using different o's in Fig. 12 and Fig. 13.\n\u2022 Exponential \\(x\\) e\\(^{-\\frac{1}{2} ||x||^2 }\\)\n\u2022 Gaussian \\(x\\)e\\(^{- ||x||^2 }\\)\n\u2022 Power Law \\(x\\) (1 + ||x||^2)^\\(- \\sigma\\)\nFig. 14 demonstrates more samples and their respective Certified Radius when using PPRS vs RS (100 samples for estimating the class and 1000 samples for estimating the radius using (1)). Also here we chose Resnet 101 as the base classifier. As shown in the figures, we believe the Superpixel Algorithm is more effective in Images with simpler geometry where similar clusters of pixels are more apparent."}]}