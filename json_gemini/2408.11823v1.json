{"title": "Mamba-Spike: Enhancing the Mamba Architecture with a Spiking Front-End for Efficient Temporal Data Processing", "authors": ["Jiahao Qin", "Feng Liu"], "abstract": "The field of neuromorphic computing has gained significant attention in recent years, aiming to bridge the gap between the efficiency of biological neural networks and the performance of artificial intelligence systems. This paper introduces Mamba-Spike, a novel neuromorphic architecture that integrates a spiking front-end with the Mamba backbone to achieve efficient and robust temporal data processing. The proposed approach leverages the event-driven nature of spiking neural networks (SNNs) to capture and process asynchronous, time-varying inputs, while harnessing the power of the Mamba backbone's selective state spaces and linear-time sequence modeling capabilities to model complex temporal dependencies effectively. The spiking front-end of Mamba-Spike employs biologically inspired neuron models, along with adaptive threshold and synaptic dynamics. These components enable efficient spatiotemporal feature extraction and encoding of the input data. The Mamba backbone, on the other hand, utilizes a hierarchical structure with gated recurrent units and attention mechanisms to capture long-term dependencies and selectively process relevant information. To evaluate the efficacy of the proposed architecture, a comprehensive empirical study is conducted on both neuromorphic datasets, including DVS Gesture and TIDIGITS, and standard datasets, such as Sequential MNIST and CIFAR10-DVS. The results demonstrate that Mamba-Spike consistently outperforms state-of-the-art baselines, achieving higher accuracy, lower latency, and improved energy efficiency. Moreover, the model exhibits robustness to various input perturbations and noise levels, highlighting its potential for real-world applications.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in deep learning have led to significant breakthroughs in various domains, including computer vision, natural language processing, and speech recognition. One of the key architectures driving this progress is the Transformer [21], which has become the backbone of state-of-the-art foundation models. Recently, the Mamba architecture [8] has emerged as a promising alternative to Transformers, offering improved efficiency and performance by leveraging structured state spaces and selective attention mechanisms [9].\nHowever, despite the success of these architectures, they often struggle with efficiently processing temporal data, such as video streams or sensor readings.\nSpiking Neural Networks (SNNs) have gained attention as a biologically inspired approach to handling temporal information [17]. SNNs process data through discrete spikes, enabling energy-efficient computation and inherent temporal dynamics [9]. Recent works have demonstrated the potential of SNNs in various applications, including object recognition, gesture classification, and speech recognition [11]. Integrating SNNs with conventional deep learning architectures has shown promise in enhancing the efficiency and performance of temporal data processing [18,4]. For instance, [18] proposed a hybrid SNN-ANN architecture that combines the benefits of both paradigms, achieving improved"}, {"title": "2 Related Work", "content": "Various spiking neuron models have been proposed, such as the Leaky Integrate-and-Fire (LIF) model [7] and the Spike Response Model (SRM) [6], which describe the dynamics of membrane potential and spike generation. Spike-Timing-Dependent Plasticity (STDP) [2] is a biologically-inspired learning rule that modulates synaptic weights based on the relative timing of pre- and post-synaptic spikes. However, STDP-based learning often struggles with complex tasks and large-scale networks [17]. Alternatively, gradient-based learning methods, such as surrogate gradient descent [24], have been proposed to enable end-to-end training of SNNs using backpropagation. These methods have shown promising results on various datasets and tasks [23,5].\nEvent-based sensors, such as Dynamic Vision Sensors (DVS) [15] and silicon cochleas [19], have gained attention for their ability to capture sparse and asynchronous events in real-time. These sensors mimic the functioning of biological"}, {"title": "3 Proposed Mamba-Spike Architecture", "content": "The Mamba-Spike architecture consists of two main components: a spiking front-end and the Mamba backbone (Figure 2). The spiking front-end is responsible for processing raw temporal data, such as event-based sensor inputs or time-series signals, and encoding them into a sparse spike representation. This representation is then fed into the Mamba backbone, which leverages the power of selective state spaces and linear-time sequence modeling [8] to efficiently process the sparse spike sequences.\nThe integration of the spiking front-end with the Mamba backbone offers several advantages. First, the sparse spike representation reduces the computational burden on the Mamba backbone, enabling more efficient processing of long temporal sequences. Second, the event-driven nature of the spiking front-end allows for real-time processing of asynchronous data, such as those generated by event-based sensors. Finally, the temporal dynamics and precise timing information encoded in the spike representation can be effectively captured and utilized by the Mamba backbone, leading to improved performance on temporal tasks."}, {"title": "3.1 Spiking Front-end Module", "content": "The spiking front-end module consists of three main components: event-based data encoding, spiking neuron models, and spatial-temporal feature extraction (Figure 3). Each component is designed to efficiently process and encode temporal data while preserving relevant information for downstream processing.\nEvent-based Data Encoding Schemes The first step in the spiking front-end is to encode raw temporal data into spike events. For event-based sensors,"}, {"title": "3.2 Interface between Spiking Front-end and Mamba Backbone", "content": "To seamlessly integrate the spiking front-end with the Mamba backbone, we introduce an interface layer that converts the sparse spike representation into a format compatible with the Mamba architecture. This interface layer serves two"}, {"title": "4 Experiments and Results", "content": "This section presents the experimental setup and results of the Mamba-Spike architecture on various datasets and tasks. We evaluate the performance of our proposed model and compare it with state-of-the-art baselines. We also conduct ablation studies to investigate the impact of different design choices on the model's performance."}, {"title": "4.1 Datasets and Tasks", "content": "We evaluate the Mamba-Spike architecture on a range of datasets and tasks that involve temporal data processing. These include both neuromorphic datasets:"}, {"title": "4.2 Baseline Models and Evaluation Metrics", "content": "We compare the performance of the Mamba-Spike architecture with the following state-of-the-art models:\nSLAYER[20]: This is a spike-based learning algorithm that employs a temporal credit assignment scheme to train deep spiking neural networks.\nDECOLLE[10]: This model uses a combination of local learning rules and global gradient-based optimization to train multi-layer spiking networks.\nSpiking-YOLO[11]: This is a spiking-based object detection model that adapts the YOLO architecture for event-based processing.\nMamba[8]: This is the original Mamba architecture without the spiking front-end, used as a baseline to evaluate the impact of the spiking front-end on performance.\nWe evaluate the models using standard metrics such as accuracy, F1 score, and latency. For neuromorphic datasets, we also report the energy efficiency of the models in terms of the number of spikes generated per sample."}, {"title": "4.3 Results and Analysis", "content": "Table 1 presents the performance comparison of the Mamba-Spike architecture and the baseline models across various datasets, including neuromorphic datasets (DVS Gesture and TIDIGITS) and standard datasets (Sequential MNIST and CIFAR10-DVS).\nOn the DVS Gesture dataset, our proposed model achieves the highest accuracy of 97.8%, outperforming the state-of-the-art models by a significant margin. The Mamba-Spike architecture also generates fewer spikes per sample compared to the other spiking models, indicating higher energy efficiency. For the TIDIGITS dataset, the Mamba-Spike architecture maintains its superior performance, achieving an accuracy of 99.2% and surpassing all baseline models. Transitioning to the standard datasets, the Mamba-Spike architecture continues to demonstrate its effectiveness. On the Sequential MNIST dataset, our model achieves an accuracy of 99.4%, outperforming both the Mamba baseline and the LSTM and GRU models. Notably, the Mamba-Spike architecture also exhibits the lowest latency of 15 ms, indicating faster processing of the sequential data compared to the other models. Finally, on the CIFAR10-DVS dataset, the Mamba-Spike architecture once again achieves the highest accuracy of 92.5%, surpassing the performance of the SLAYER, DECOLLE, Spiking-YOLO, and Mamba baseline models. This demonstrates the ability of the spiking front-end to effectively capture and process the spatial and temporal information in the event streams, leading to improved object recognition performance.\nThe results presented in Table 1 clearly demonstrate the effectiveness of the Mamba-Spike architecture across a diverse range of datasets and tasks involving temporal data processing."}, {"title": "4.4 Ablation Studies", "content": "We conduct ablation studies to investigate the impact of different design choices in the Mamba-Spike architecture. Specifically, we examine the effect of the spiking front-end and the influence of spiking neuron models and parameters.\nImpact of Spiking Front-end on Performance To evaluate the contribution of the spiking front-end to the overall performance of the Mamba-Spike architecture, we compare the performance of the model with and without the spiking front-end on the DVS Gesture and CIFAR10-DVS datasets. The results are shown in Table 2."}, {"title": "Effect of Spiking Neuron Models and Parameters", "content": "We study the effect of different spiking neuron models and their parameters on the performance of the Mamba-Spike architecture. We compare the Leaky Integrate-and-Fire (LIF) and Spike Response Model (SRM) neurons with varying membrane time constants on the TIDIGITS dataset. The results are shown in Figure 5."}, {"title": "5 Conclusion", "content": "In this paper, we introduced the Mamba-Spike architecture, which integrates a spiking front-end with the Mamba backbone for efficient temporal data processing. The spiking front-end enables the model to process event-based data directly and extract meaningful spike representations, while the Mamba backbone leverages its selective state spaces and linear-time sequence modeling capabilities"}]}