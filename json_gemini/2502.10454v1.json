{"title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMS", "authors": ["Yinghui Li", "Jiayi Kuang", "Haojing Huang", "Zhikun Xu", "Xinnian Liang", "Yi Yu", "Wenlian Lu", "Yangning Li", "Xiaoyu Tan", "Chao Qu", "Ying Shen", "Hai-Tao Zheng", "Philip S. Yu"], "abstract": "Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of \"proof by counterexamples\" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, COUNTERMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that COUNTERMATH is challenging, indicating that LLMs, such as OpenAI ol, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.", "sections": [{"title": "1. Introduction", "content": "Mathematics, as a fundamental aspect of reasoning, has garnered significant research interest. Recent studies have demonstrated that Large Language Models (LLMs) exhibit strong mathematical reasoning abilities (OpenAI, 2023; Google, 2024; Yang et al., 2024; Shao et al., 2024; Ying et al., 2024; Chern et al., 2023; Luo et al., 2023; Yu et al., 2024a). Enhancing the mathematical reasoning capabilities of LLMs has become a prominent and fundamental topic within the LLMs research community.\nCurrently, there are two main paradigms for enhancing the mathematical reasoning capabilities of LLMs. The first involves synthetic generation based on seed math questions (Yu et al., 2023; Li et al., 2024a). For example, WizardMath (Luo et al., 2023) introduces a variety of math instructions to generate math questions of different complexities using GPT-3.5. The second approach leverages formal mathematical languages to train LLM-based theorem provers, such as Lean 4 (Moura & Ullrich, 2021). For instance, Draft-Sketch-Prove (Jiang et al., 2023), Hunyuan-Prover (Li et al., 2024c), and Lean-STaR (Lin et al., 2024a) interact with formal languages through informal proofs, automatic formalization, and natural language thoughts for theorem proving.\nThe two methods above enable LLMs to develop problem-solving skills either by training on massive similar problems, or by gaining proficiency through exposure to similar proof processes (Mirzadeh et al., 2024; Yu et al., 2024b). In both cases, these approaches enhance LLMs' mathematical reasoning abilities through training, where proficiency is achieved through familiarity, akin to \u201cdrill-based\" learning in human mathematics learning. However, relying solely on intensive-practice by inundating LLMs with math problems is neither sufficient nor essential for true mathematics learning. In other words, drill-based learning alone does not foster a deep understanding of mathematical concepts in either humans or LLMs.\nAs illustrated in Figure 1, for human mathematics learning, \u201cexample-based\" learning is a more important strategy than drill-based learning. In particular, for mathematical proofs,"}, {"title": "2. Related Work", "content": "Math Benchmarks. Recently, the number of math-related benchmarks has increased drastically (Amini et al., 2019; Yang & Deng, 2019; Zheng et al., 2022; Hendrycks et al., 2021; Cobbe et al., 2021; Frieder et al., 2023; Liu et al., 2024; He et al., 2024; Lu et al., 2024). The most influential ones are MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), which focus on arithmetic reasoning at the high school competition level and grade school level, respectively. Moreover, other benchmarks such as MathBench (Liu et al., 2024) and OlympiadBench (He et al., 2024) are also blends of problems sets from various competitions and standard examinations, which are used to test human students' abilities of utilizing the math knowledge and certain tricks to solve complex application-based problems. However, mathematicians are more expecting LLMs to help them in literature review, idea generation, proof-checking and collaborative writing as they focus on a broader spectrum of mathematical activities rather (Frieder et al., 2024). To better accommodate the true need for math research, some formal theorem proving benchmarks like Putnam-Bench (Tsoukalas et al.), CoqGym (Yang & Deng, 2019) and MiniF2F (Zheng et al., 2022) are also proposed recently in a combination of formal mathematical languages compilers (e.g. Coq, Lean), which could be viewed as the important math benchmarks for developing Mathematics Mechanization (Wu, 2001; Wu & Gao, 2007)."}, {"title": "3. COUNTERMATH", "content": "3.1. Data Construction\nOur dataset is constructed from a series of math textbooks focusing on counterexamples in different fields such as Algebra (Hu, 1983), Topology (Wang & Yang, 2000), Real Analysis (Wang, 1989) and Functional Analysis (Wang, 1994). We have obtained the authors' consent to use their publications solely for academic research purposes. As the raw data sources are in Chinese, we also translate our dataset into English, creating a mathematical conceptual reasoning benchmark based on counterexamples, named as COUNTERMATH. Each data point includes a statement, rationale, judgement (i.e., whether the statement is True or False by its rationale), and field.\nAs illustrated in Figure 2, we first recruited several Chinese annotators from specific vendors to extract statement-rationale pairs from the aforementioned textbooks using an OCR tool, which yielded 1,274 statement-rationale pairs. Next, the experts, among the authors, who have the bachelor's degrees in applied mathematics manually checked all the data points from the previous stage, annotated each statement as True or False based on its rationale, and filtered out ambiguous pairs. This resulted in 1,216 data samples as"}, {"title": "3.2. Data Analysis", "content": "Overview Since the dataset has been constructed from textbooks in four different fields, the distribution of statement-rationale pairs is shown in Figure 3a. Moreover, the distribution of judgements is presented in Figure 3b. We observe that most statements are labeled as True. This may be because the data is sourced from mathematical textbooks, where most statements are phrased correctly to avoid misleading readers, especially novices in mathematics. In general, as shown in Figure 7, the statements often involve several college-level mathematical concepts or properties, focusing on nuanced understandings of mathematics. In real-world applications, due to the concise formulations, these statements are frequently used as interview questions in math-related graduate programs. This also contributes to the diversity of mathematical testbeds for contemporary LLMs, fostering research in conceptual mathematical reasoning (Klymchuk, 2010)."}, {"title": "4. Benchmark Settings", "content": "Baselines We are testing following large language models with COUNTERMATH. For open-weight models, we"}, {"title": "5. Conceptual Finetuning", "content": "5.1. Training Data Engineering Framework\nFilter-based Data Collection To validate our approach, we conduct supervised fine-tuning to enhance the model's"}, {"title": "6. Analysis and Discussions", "content": "6.1. Evaluation Results without Finetuning\nWe selected a range of advanced mathematical LLMs with varying parameter sizes to evaluate their conceptual reasoning abilities on our benchmark. Table 1 summarizes their performance across various metrics. From the results, we derive the following findings and insights:\nJudgement Performance The performance on the automatic evaluation metric F1 reflects the models' fundamental conceptual reasoning abilities, specifically their capacity to correctly determine the truth or falsehood of a given statement. While open-source models exhibit some performance, their overall performance is relatively low around 30. Even the advanced Qwen-2.5-Math-72B-Instruct achieves only 41.8, falling behind commercial models. Notably, the math-"}, {"title": "6.2. Results with Finetuning", "content": "To further validate the effectiveness of counterexample reasoning for conceptual reasoning, we construct a training dataset designed for counterexample-based reasoning, conduct training, and evaluate the performance, as shown in Table 2 and Table 3.\nEvaluation on Our Benchmark We evaluate the approach on our benchmark to verify whether counterexample reasoning effectively enhances a model's conceptual reasoning capabilities. In addition to the base model, we include comparisons with the hint prompt, where explicit hints encouraging LLM to reason by example were provided. For a fair comparison, we use identical prompts to generate outputs during the experiments. The results indicate that, with just 1,025 training samples, the fine-tuned model outperforms all base models in judgement F1 score. Furthermore, the trained model demonstrates superior example-based conceptual reasoning abilities, showing improvements in both the quantity and quality of examples compared to base models. On the other hand, although the constructed training data are refined to align closely with our COUNTERMATH distribution, some discrepancies remain. Since our exploration involves only the simple SFT strategy on a limited dataset, the model performs slightly worse on some metrics, which can be considered an acceptable limitation.\nEvaluation on Out-of-Distribution Benchmarks To assess the generalizability of the fine-tuned model, we further evaluate its performance on out-of-distribution (OOD) benchmarks. This aims to verify whether the model's counterexample reasoning capability, which is valid on our COUNTERMATH, can transfer to other benchmarks and deliver broader performance improvements. Using identical prompts and configurations for fairness, we compare the base models and fine-tuned models on OOD benchmarks MATH and GSM8K. The results reveal that the fine-tuned model outperforms the base models on both benchmarks, even surpassing larger models like the 72B-parameter model."}, {"title": "6.3. Used Tokens Analysis", "content": "The relationship between model performance and token usage efficiency is a critical factor in understanding the trade-offs inherent in model design. As depicted in Figure 6, we analyzed the connection between the mean token ratios (%), which represent the efficiency of token usage relative to ground truth, and the F1 (macro) score, which reflects the predictive performance of the model. The Mean Token Ratio (%) is calculated by dividing the number of tokens actually used by the model during inference by the number of tokens in the ground truth answer. Reasoning models such as \"23=01-preview\" and \"16=QwQ-32B-Preview\" utilize a significantly large number of tokens during inference, but this extensive token consumption does not lead to a corresponding improvement in F1 (macro) scores. This suggests that the benchmark task is highly difficult, where simply increasing the length or detail of token reasoning does not necessarily enhance performance. Models like \"24=Qwen-max\" and \"22=GPT-40\" demonstrate a commendable balance between token usage and performance. These models achieve relatively high F1 (macro) scores while maintaining token consumption close to the ground truth token ratio. This indicates their ability to perform accurate reasoning efficiently, without overly relying on additional token usage."}, {"title": "7. Conclusion", "content": "In this work, we address the limitations of drill-based learning in mathematical LLMs by introducing COUNTERMATH, a counterexample-based reasoning benchmark. Unlike existing datasets, COUNTERMATH evaluates models on their ability to distinguish nuanced mathematical concepts through example-driven reasoning. Our key contributions include constructing a high-quality dataset with 1,216 university-level counterexample-based proofs, benchmarking state-of-the-art mathematical LLMs to reveal their conceptual reasoning gaps, and developing an automated framework for counterexample data generation and training. Experimental results show that current LLMs struggle with counterexample-based reasoning, particularly in topology and real analysis, highlighting areas for future research. Furthermore, our fine-tuned model, trained on only 1,025 examples, significantly outperforms baseline models, demonstrating the effectiveness and generalizability of counterexample-driven learning in mathematical reasoning."}, {"title": "Impact Statement", "content": "This paper introduces a conceptual mathematical benchmark aimed at advancing research on Large Language Models (LLMs) in the realm of genuine mathematical reasoning. The dataset utilized in this work is derived from publications with copyrights reserved and authors' permission only for academic research purposes. It is important to acknowledge that our experiments and evaluations rely heavily on LLMs, but this study does not fully explore or mitigate potential biases inherent in their outputs. Addressing these biases and ensuring model alignment with social values remain critical challenges. This underscores the importance of conducting comprehensive evaluations that consider diverse dimensions of human society and their implications."}, {"title": "A. Details for Data Curation", "content": "Annotation Cost In the first-stage annotation, there were three annotators recruited who have at least bachelor's degrees in engineering or science-related majors. On average, each of them has annotated about 416 statement-rationale pairs from the given textbooks. They were working through the vendor's provided platform with the provided OCR tool and annotation examples. The cost for the first-stage crowd-sourced annotation was about $1306.\nAnnotation Process The annotation process consists of two stages. In the first stage, the recruited annotator were asked to annotate statement, rationale (i.e. the response for supporting the statement, typically examples or counterexamples), field (i.e. algebra, topology, real analysis or functional analysis), and txt (i.e. mappings to original annotations for the validation use). During annotation, they were provided with several annotation examples created by authors, demonstrating the annotation targets. Moreover, we also asked annotators to focus on statements related to proving or disproving and existence of certain mathematical objects and ignore those statements without clear or complete answers such as definitions for certain advanced and complicated concepts. For the second stage, authors with at least bachelor's degrees in applied mathematics were checking the LaTeX formats and typos in statements and rationales, keeping the rationales that correct and clearly support or deny the statements with examples or counterexamples. Besides, we also added another element in annotated data points, judgement, to show whether the statement is true or false by its rationale. One full annotation example is shown in Figure 7. It should be noted that we have modified some statements (less than 5%), which are easily revised to be the reverse, to make them False for diversity because nearly all statements are stated as True in the original textbooks. In all, we tried to make sure that all statements and rationales were concise and related."}, {"title": "B. Details for Experimental Settings", "content": "Summary for Open-weight Baselines The summary of the open-weight baseline models is shown in the following Table 4. From Table 4, it is evident that current math-focused LLMs are built on a variety of base models, with Mistral being the most commonly used, followed by Llama2, Qwen2-Math, and Deepseek-Math. Notably, most models utilize data generation and augmentation strategies centered around MATH and GSM8K, which has accelerated the saturation of these benchmarks and highlighted the limitations of current \u201cmathematical reasoning\" capabilities in LLMs.\nFurthermore, most academia-developed models are often constrained to supervised fine-tuning (SFT) only due to limited computation resources. Recent efforts, such as Eurus-2-PRIME and Rho-Math, have begun to explore advanced pre-training and post-training techniques, which have been (implicitly) validated by companies as effective in enhancing mathematical reasoning. However, the opacity surrounding engineering details in technical reports (OpenAI, 2023; Shao et al., 2024; Yang et al., 2024) hinders progress toward genuine mathematical reasoning in LLMs, which is critical for helping math researchers with true mathematical research.\nConsequently, our proposed conceptual mathematical benchmark, COUNTERMATH, is a timely and significant"}, {"title": "Details of Prompts", "content": "The used prompts are summarized as follows. We follow the corresponding tokenizers for chat-template-based prompts with the instruction as system prompt and statement as user input."}, {"title": "Details of Evaluation Prompt", "content": "The evaluation prompts used are as follows:"}, {"title": "C. Details for Training Data Engineering Framework", "content": "In constructing our training dataset, we utilized GPT to filter and refine the data. Below, we outline the specific prompt designs used in this process:"}]}