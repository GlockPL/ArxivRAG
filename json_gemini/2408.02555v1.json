{"title": "MESHANYTHING V2: ARTIST-CREATED MESH GENERATION WITH ADJACENT MESH TOKENIZATION", "authors": ["Yiwen Chen", "Yikai Wang", "Yihao Luo", "Zhengyi Wang", "Zilong Chen", "Jun Zhu", "Chi Zhang", "Guosheng Lin"], "abstract": "We introduce MeshAnything V2, an autoregressive transformer that generates Artist-Created Meshes (AM) aligned to given shapes. It can be integrated with various 3D asset production pipelines to achieve high-quality, highly controllable AM generation. MeshAnything V2 surpasses previous methods in both efficiency and performance using models of the same size. These improvements are due to our newly proposed mesh tokenization method: Adjacent Mesh Tokenization (AMT). Different from previous methods that represent each face with three vertices, AMT uses a single vertex whenever possible. Compared to previous methods, AMT requires about half the token sequence length to represent the same mesh in average. Furthermore, the token sequences from AMT are more compact and well-structured, fundamentally benefiting AM generation. Our extensive experiments show that AMT significantly improves the efficiency and performance of AM generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Due to the controllable and compact advantages of meshes, they serve as the predominant 3D representation in various industries, including games, movies, and virtual reality. For decades, the 3D industry has relied on human artists to manually create meshes, a process that is both time-consuming and labor-intensive.\nTo address this issue, very recently, a line of work (Nash et al., 2020; Alliegro et al., 2023; Siddiqui et al., 2023; Weng et al., 2024; Chen et al., 2024a;b) has focused on automatically generating Artist-Created Meshes (AMs) to replace manual labor. Inspired by the success of large language models (LLMs), these approaches treat AMs as sequences of faces and learn to generate them with autoregressive transformers (Vaswani et al., 2017) in a manner similar to LLMs. Unlike methods that produce dense meshes in a reconstruction manner, these methods learn from the distribution of meshes created by human artists, thereby generating AMs that are efficient, beautiful, and can seamlessly replace manually created meshes.\nAlthough these methods have achieved some success, they still face significant challenges. One major limitation is that current methods (Nash et al., 2020; Alliegro et al., 2023; Siddiqui et al., 2023; Weng et al., 2024; Chen et al., 2024a;b) cannot generate meshes with a large number of faces. Specifically, the maximum number of faces that can be generated is currently limited to 800. The primary reason for this limitation is the inefficiency of the current tokenization methods. These methods treat a mesh as a sequence of faces, where each face consists of three vertices, and each vertex typically requires three tokens to represent. Consequently, each mesh is tokenized into a sequence nine times the number of its faces, resulting in substantial computational and memory demands. Besides, the resulting token sequence is highly redundant, which harms sequence learning and reduces performance.\nThis work aims to address this issue. We introduce a novel mesh tokenization method named Adjacent Mesh Tokenization (AMT), which processes meshes into more compact and well-structured token sequences, thereby improving both efficiency and performance. AMT achieves this by representing each face with one vertex instead of three whenever possible. As shown in Fig. 1 and Alg. 1, during the tokenization process, after encoding a face, AMT finds and encodes its adjacent face, which shares an edge, requiring only one additional vertex to represent the adjacent face. When an adjacent face cannot be found, AMT adds a special token & to the sequence to mark this event and restarts from a face that has not been encoded yet. Ideally, because AMT uses only one vertex to represent a face, the sequence length can be reduced to nearly one-third.\nTo test the effectiveness of AMT, we conducted extensive experiments in the MeshAnything (Chen et al., 2024b) setting. Our experiments on Objaverse (Deitke et al., 2023) demonstrated that AMT can, on average, reduce the sequence length by half, thereby reducing the computational load and memory usage of the attention block by nearly four times. Moreover, the performance of our model also improved due to the compact and well-structured token sequence from AMT. Besides, AMT can also be applied to unconditional or other conditional mesh generation settings (Siddiqui et al., 2023), and its effectiveness is not affected by the use of VQ-VAE (Van Den Oord et al., 2017)."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 ARTIST-CREATED MESH GENERATION", "content": "Diverging from previous works that produce dense meshes, recent works have focused on generating meshes created by human artists, i.e., Artist-Created Meshes (AMs) (Nash et al., 2020; Alliegro et al., 2023; Siddiqui et al., 2023; Chen et al., 2024a; Weng et al., 2024; Chen et al., 2024b). These methods process meshes into ordered face sequences and learn to generate this sequence. (Nash et al., 2020) first proposed using autoregressive transformers to sequentially generate vertices and faces. (Siddiqui et al., 2023) use VQ-VAE to learn a mesh vocabulary and then learn this vocabulary"}, {"title": "2.2 3D GENERATION", "content": "In recent years, 3D generation has gradually become one of the mainstream research directions in the field of 3D research. This area focuses on generating diverse, high-quality 3D assets for the 3D industry. Generative Adversarial Networks (GANs) (Wu et al., 2016; Achlioptas et al., 2018; Goodfellow et al., 2020) produce synthetic 3D data by training a generator and a discriminator network to distinguish between generated and real data. Very recently, a new line of works (Hong et al., 2023; Liu et al., 2024; Shi et al., 2023; Li et al., 2023; Tang et al., 2024; Wang et al., 2024; Tochilkin et al., 2024; Wei et al., 2024; Xu et al., 2024) directly generate 3D assets in a feed-forward manner. (Hong et al., 2023) pioneer these methods and use a transformer to directly regress the parameters of 3D models given conditions. Besides, applying diffusion models (Ho et al., 2020) to directly generate 3D assets has also been widely researched (Zhou et al., 2021; Nichol et al., 2022; Alliegro et al., 2023; Lyu et al., 2023; Liu et al., 2023; Zhang et al., 2024). (Zhang et al., 2024) lead the SOTA of current 3D generation methods by first generating high-quality 3D shapes with DiT and then producing detailed textures with material diffusion models.\nAs mesh is a crucial component in 3D generation, Artist-Created Mesh Generation (Nash et al., 2020; Alliegro et al., 2023; Siddiqui et al., 2023; Chen et al., 2024a; Weng et al., 2024; Chen et al.,"}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 ADJACENT MESH TOKENIZATION", "content": "In this section, we detail Adjacent Mesh Tokenization (AMT), a novel tokenization method for Artist-Created Mesh (AM) generation. Compared to previous methods, AMT processes the mesh into a more compact and well-structured token sequence by representing each face with a single vertex whenever possible. For simplicity, we describe AMT on triangle mesh. But it is worth noting that AMT can be easily generalized to the generation of meshes with variable polygons.\nTokenization is a crucial part of sequence learning, as it processes various data formats, such as text, images, and audio, into token sequences. The processed tokens are then used as ground truth inputs for training the sequence model. During inference, the sequence model generates a token sequence that is subsequently detokenized into the target data format. Therefore, tokenization plays a vital role in sequence learning, determining the quality of the data sequence that the sequence model learns from.\nWe first illustrate the tokenization methods used in previous methods (Nash et al., 2020; Alliegro et al., 2023; Siddiqui et al., 2023; Chen et al., 2024a; Weng et al., 2024; Chen et al., 2024b). Although there are slight differences in detail, the previous tokenization methods can be unified as follows: Given a mesh M, vertices are first sorted in ascending order based on their z-y-x coordinates, where z represents the vertical axis. Next, faces are ordered by their lowest vertex index, then by the next lowest, and so on. The mesh is then viewed as an ordered sequence of faces:\n$M := (f_1, f_2, f_3, ..., f_N),$\nwhere $f_i$ represents the i-th face in the mesh, and N is the number of faces in M.\nThen, each $f_i$ is represented as an ordered sequence of three vertices v:\n$f_i:= (v_{i1}, v_{i2}, v_{i3}),$\nwhere $v_{i1}, v_{i2}, and v_{i3}$ are the vertices that form the i-th face $f_i$ in the mesh. It is worth noting that $v_{i1}, v_{i2}, and v_{i3}$ have already been sorted and have a fixed order.\nSubstituting Equation equation 2 into Equation equation 1 gives:\n$M := ((v_{11}, v_{12}, v_{13}), (v_{21}, v_{22}, v_{23}),..., (v_{N1}, v_{N2}, v_{N3})) = Seq_y$\nDue to the sorting, the resulting $Seq_y$ is unique and its length is three times the number of faces in the mesh. It is evident that $Seq_y$ contains a significant amount of redundant information, as each vertex appears as many times as the number of faces it belongs to.\nTo resolve this issue, we propose Adjacent Mesh Tokenization (AMT) to obtain a more compact and well-structured $Seq_y$ than previous method. Our key observation is that the main redundancy of $Seq_y$ comes from representing each face with three vertices as in equation 2. This results in vertices that have already been visited appearing redundantly in $Seq_y$. Therefore, AMT aims to represent each face using only a single vertex whenever possible. As shown in Fig. 2 and Alg. 1, AMT efficiently encodes adjacent faces during tokenization, using only one additional vertex. When no adjacent face is available, as illustrated in the last step of Fig. 2, AMT inserts a special token & into the sequence to denote this event and restarts the process from a face that has not yet been encoded. To detokenize, simply reverse the tokenization algorithm as described in Alg. 1.\nIn the ideal case, where the special token \"&\" is rarely used, AMT can reduce the length of $Seq_y$ obtained by previous methods to nearly one-third. Of course, in extreme cases, such as when each face in the mesh is completely disconnected from others, AMT performs worse than previous methods. However, since the datasets Deitke et al. (2023); Chang et al. (2015) used for AM generation are created by human artists, the meshes generally have well-structured topologies. Thus, the overall"}, {"title": "3.2 MESHANYTHING V2", "content": "In this section, we introduce MeshAnything V2. It is equipped with AMT and scales up its maximum generated face count from 800 to 1600. Without increasing the number of parameters, MeshAnything V2 achieves shape conditioned Artist-Created Mesh (AM) generation with significantly better performance and efficiency. We also use it as an example to demonstrate how AMT can be applied to mesh generation.\nFollowing (Chen et al., 2024b), MeshAnything V2 also targets generating AMs aligned to a given shape, allowing integration with various 3D asset production pipelines to achieve highly controllable AM generation. That is, we aim to learn the distribution: $p(M|S)$, where M represents the AM and S represents the 3D shape condition.\nAs in (Chen et al., 2024b), V2 uses point clouds as the shape condition input S. We also use the same point cloud-Artist-Created Mesh data pairs (M, S) collected in (Chen et al., 2024b). The target distribution $p(M|S)$ is learned with a decoder-only transformer with the same size and architecture as in (Chen et al., 2024b). To inject S into the transformer, we first encode it with a pretrained point cloud encoder (Zhao et al., 2024) into a fixed-length token sequence $T_S$ and then set it as the prefix of the transformer's token sequence. We then process paired M into mesh token sequence $T_M$. It is concatenated to the point cloud token sequence as the transformer's ground truth sequence. After training the transformer with cross-entropy loss, we input $T_S$ and let the transformer autoregressively generate the corresponding $T_M$, which is then detokenized into M.\nThe key difference between (Chen et al., 2024b) and our method is the way we obtain $T_M$. Instead of the naive mesh tokenization method used in (Chen et al., 2024b), we process M with the newly proposed Adjacent Mesh Tokenization (AMT) and obtain a more compact and efficient sequence $Seq_y$. Following (Chen et al., 2024a), we discard the VQ-VAE and directly use the discretized coordinates from $Seq_y$ as token indices. We then add a newly initialized codebook entry to represent the & in the AMT sequence. Finally, we sequentially combine the coordinate token sequence and the special token for & to obtain the mesh token sequence $T_M$ for transformer input. As mentioned in Sec. 3.1, it is worth noting that whether or not VQ-VAE is used does not affect the application and effectiveness of AMT."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "The main experimental setting of MeshAnything V2 remains consistent with (Chen et al., 2024b), except we equipped it with the newly proposed Adjacent Mesh Tokenization (AMT). We still use OPT-350M (Zhang et al., 2022) as our autoregressive transformer (Vaswani et al., 2017) and the pre-trained point encoder from (Zhao et al., 2024). We trained on the same combined dataset collected in (Chen et al., 2024b), except this time we used meshes with fewer than 1600 faces as the experiment dataset instead of 800. This resulted in a dataset containing 100K point cloud and mesh pairs. We randomly sampled 2K data samples as the evaluation dataset. To accommodate meshes with more faces, we sampled 8192 points instead of 4096 for each point cloud. Besides, unlike (Chen et al., 2024b), we update the point encoder from (Zhao et al., 2024) during training because we find its accuracy insufficient for handling complex meshes with up to 1600 faces.\nMeshAnything V2 is trained with 32 A800 GPUs for four days. The batch size per GPU is 8, resulting in a total batch size of 256."}, {"title": "4.2 QUALITATIVE EXPERIMENTS", "content": "We present the qualitative results of MeshAnything V2. As shown in Fig. 1, MeshAnything V2 effectively generates high quality Artist-Created Mesh aligned to given shapes. When integrated with various 3D assets production pipelines, V2 successfully achieves highly contrabllable AM generation."}, {"title": "4.3 QUANTITATIVE EXPERIMENTS", "content": "Averaged Token Sequence Length. We randomly sample 10k mesh samples with fewer than 1600 faces from our dataset and tokenize them into token sequences using the previous tokenization method and AMT separately. In transformer Vaswani et al. (2017) learning, a vertex requires several tokens to represent it, whereas the special symbol & only requires one token. Therefore, the loss incurred by AMT when it cannot find the next adjacent face is relatively small. Since three tokens are used to represent a vertex in (Chen et al., 2024b;a) as well as in this paper, we calculate the final input token length for the transformer based on this standard. For each sample, we divide the length of the token sequence obtained by our method by the length obtained by the previous method, and then average the results to determine the average reduction in token sequence length achieved by AMT. Across the aforementioned 10k mesh samples, we found this ratio to be 0.4973, indicating that the AMT method significantly reduces length of token sequence. Additionally, this experiment only demonstrates the superiority of the AMT algorithm in shortening sequence length; its more compact and well-structured sequence characteristics also provide further advantages for sequence learning.\nAblation Study. We ablate the effectiveness of AMT by comparing the results of MeshAnything V2 with its variant without AMT. The variant follows exactly the same settings as V2, except that AMT"}, {"title": "5 LIMITATIONS AND CONCLUSION", "content": "In this work, we present MeshAnything V2, a shape-conditioned Artist-Created Mesh (AM) generation model that generates AM aligned to given shapes. V2 significantly outperfroms MeshAnything (Chen et al., 2024b) in both performance and efficiency with our newly proposed Adjacent Mesh Tokenization (AMT). Different from previous methods that use three vertices to represent a face, AMT uses a single vertex whenever possible. Our experiments demonstrate that AMT aver-agely reduces the token sequence length by half. The compact, and well-structured token sequence from AMT greatly enhances sequence learning, thereby significantly improving the efficiency and performance of AM generation.\nLimitations. Although there is a large improvement over V1, the accuracy of MeshAnything V2 is still insufficient for industrial applications. More efforts are needed to improve the model's stability and accuracy."}]}