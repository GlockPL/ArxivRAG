{"title": "Local and Global Feature Attention Fusion Network for Face Recognition", "authors": ["Wang Yu", "Wei Wei"], "abstract": "Recognition of low-quality face images remains a challenge due to invisible or deformation in partial facial regions. For low-quality images dominated by missing partial facial regions, local region similarity contributes more to face recognition (FR). Conversely, in cases dominated by local face deformation, excessive attention to local regions may lead to misjudgments, while global features exhibit better robustness. However, most of the existing FR methods neglect the bias in feature quality of low-quality images introduced by different factors. To address this issue, we propose a Local and Global Feature Attention Fusion (LGAF) network based on feature quality. The network adaptively allocates attention between local and global features according to feature quality and obtains more discriminative and high-quality face features through local and global information complementarity. In addition, to effectively obtain fine-grained information at various scales and increase the separability of facial features in high-dimensional space, we introduce a Multi-Head Multi-Scale Local Feature Extraction (MHMS) module. Experimental results demonstrate that the LGAF achieves the best average performance on 4 validation sets (CFP-FP, CPLFW, AgeDB, and CALFW), and the performance on TinyFace and SCFace outperforms the state-of-the-art methods (SoTA).", "sections": [{"title": "1. Introduction", "content": "Face recognition (FR) has gained widespread applications in many fields. However, effectively extracting identity information from low-quality face images remains challenging for face representation networks. Generally, low-quality face images are defined as those with extreme pose, face occlusion, poor contrast, and low resolution [1]. Key factors that influence FR performance include pose, lighting, occlusion, age, expression, etc [2]. Under such circumstances, the variability of face images may significantly limit the performance of FR. Therefore, the FR research of different head pose [3], face occlusion [4], various age [5], or low-resolution [6] scenes has been widely concerned.\nTo generalize further, we categorize the causes of low-quality face images into three main categories in reality (as depicted in Fig.1): (A) Partial facial region missing. (B) Substantial deformation in partial facial regions. (C) Joint influence of the two above. The factors resulting in partial facial region missing (Category A in Fig.1) mainly stem from extreme head pose, facial occlusion, and brightness. This phenomenon leads to the attenuation of face structural information and increases the difficulty of FR. Significant deformation occurs in partial facial regions (Category B in Fig.1), such as expression and wrinkles caused by age changes, which will introduce noise to the local features. Excessive attention to these local regions often affects the performance of FR. When missing and deformation jointly occur (Category C in Fig.1), such as in low-resolution or with motion blur images, the attenuation of identity information is accompanied by noise, posing significant challenges for FR.\nRecently, the main research directions of FR are divided into optimizing data preprocessing methods [7, 8], optimizing loss functions [9, 10] and optimizing feature representation networks [11, 12]. In this paper, we focus on the advancements of feature representation optimization in FR, which is mainly divided into two categories: global feature representation network and local feature representation network. With the development of deep neural networks, face representation networks based on global features have been widely applied due to their robustness. They primarily use classical CNN architectures directly or slightly modified as the backbone, such as ResNet[13] and Mobilenet[14]. The VPL model proposed by Deng et al. [15] focused on the sample-to-sample comparisons within a classification framework, enabling the model to further explore the optimal solution. Based on the Embedding Unmasking Model (EUM), [16] generates embeddings for faces in the presence of masks that resemble unmasked faces of the same identity. After the vision transformers (ViTs) model was proposed, Transformer-based models have been gradually applied to FR. Su et al. [12] proposes a new plug-and-play hybrid token Transformer (HOTformer) module based on ViTs to recognize key facial semantics by cooperation of atomic and holistic tokens. However, the face representation networks based on global features are vulnerable to partial facial region missing and background noise, which attenuates global facial information. As a result, they tend to suffer performance degradation in category A of Fig.1.\nTherefore, based on the idea that local patches play an important role in FR when the global face appearance changes significantly, some researchers have paid more attention to attention-based local feature representation networks in recent years. Wang et al. [17] introduced the HPDA networks, designed to extract facial features of different local regions at various scales. The CQA-Face proposed by Wang and Guo [11] emphasizes non-critical but still discriminative local regions to comprehensively explore useful facial parts and suppress noisy parts in a global scope. Although attention-based local feature representation networks assist models in capturing more effective face information under extreme pose or occlusion (category A in Fig.1), their reliance on partial facial region information makes them more vulnerable when local regions undergo deformation, such as expression or age change (category B in Fig.1) than global feature extraction networks.\nObserving the FR process of low-quality facial images under different categories, we find that when samples are primarily affected by missing facial regions (category A in Fig.1), the global structural information decays rapidly with the deepening of the missing degree, and local information is more stable than global information due to focusing only on specific facial areas. Consequently, people tend to prioritize identity recognition through local region similarity. When the samples have significant facial region deformation (category B in Fig.1), the local face information is attenuated due to noise, while global information is more conducive to recognition. We define the effectiveness of features for FR tasks as feature quality. Low-quality images caused by different categories have biases in local and global feature quality.\nHowever, the existing FR models have not considered the feature quality bias nor designed an effective method to utilize the characteristics to help the model obtain more effective facial information. To solve this issue, we introduce the Local and Global Feature Fusion (LGF) module. This module drives feature norm as a proxy for feature quality to dynamically measure the attention between local and global features at a relatively low cost. When the structural information of global features is weakened due to incomplete facial regions, the LGF module prioritizes effective local face information to enhance face features. Conversely, when local features introduce noise due to deformation in certain face areas, more attention is directed towards global features with greater robustness to complement facial information and improve overall feature quality.\nIn addition, we propose a Multi-Head Multi-Scale Local Feature Extraction (MHMS) module. Assuming the distribution of local information at different scales across various spatial positions, we calculate spatial attention for multiple scales respectively to obtain information distributed across various visual fields. Channel attention is implemented to enhance local information and suppress noise. In addition, the multi-head structure ensures that the model can obtain sufficiently rich local face features, enhancing the separability of identity information in high-dimensional space. By connecting the MHMS, the Global Feature Extraction (GFE) module, and the LGF, we develop a new face recognition network architecture: the Local and Global Feature Attention Fusion (LGAF) network. Our main contributions are threefold:"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Local Feature Extraction", "content": "Attention-based local feature representation networks in FR have gradually attracted the attention of the academic community. The Local and multi-Scale Convolutional Neural Networks (LS-CNN) was proposed by Wang and Guo [18], which is one of the early works that introduced attention to face representation. The Hierarchical Pyramid Diverse Attention (HPDA) network [17] used a Pyramid Diverse Attention (PDA) to extract face features of different scales and utilized a Hierarchical Bilinear Pooling (HBP) to fuse information from multiple scales. However, the PDA with different-size pooling operations may lose some fine-grained features. The HBP did not consider the effectiveness difference of various scale features and introduced redundant information. The CQA-Face [11] extracted more local face features that are not the most important but still discriminative. When the key region is occluded, it can also rely on the features of other regions for FR. However, it did not account for identity information that may appear dynamically at different scales. To solve the problems, we propose the MHMS module. It uses convolution kernels at various scales to simultaneously obtain local feature maps in different visual fields and implements spatial attention emphasis on effective regions for multiscale feature maps, respectively. The Squeeze-and-Excitation (SE) module [19] in the MHMS is utilized to measure which scales of local features the model should focus on. We employ a multi-head structure to help the model obtain more local information and enhance the separability in high-dimensional space."}, {"title": "2.2. Feature Norm", "content": "Parde et al. [1] proved that face features based on convolutional neural networks retain face pose information. Ranjan et al. [20] proposed for the first time that there was a certain relationship between the L2-norm of face features and face image quality. Meng et al. [21] used feature norm as an indicator to measure the quality of the given face image, and implemented fine-grained constraints within the class. Kim et al. [22] found that the feature norm has the characteristics of fast convergence, long-term stability, and a high correlation with image quality (IQ) score. In contrast to our elaborate categorization of the factors influencing the quality of facial features (such as posture, age variation, and so on), image quality in the AdaFace is a combination of attributes that indicates how faithfully an image captures the original scene. Based on the idea that face image quality affects the difficulty of sample recognition, the AdaFace used the feature norm as a proxy to make the model pay more attention to the samples with low image quality.\nHowever, the AdaFace only explores the correlation between feature norm and IQ score. To further explore the properties of feature norm, we calculate and analyze the trends in feature norm variations caused by partial facial region missing, deformation in partial facial regions, and joint influence of the two above, resulting in low-quality facial images, as detailed in Sec.3.1."}, {"title": "3. Methods", "content": "The LGAF model comprises five key components: the backbone network, the local feature extraction module, the global feature extraction module, the feature fusion module, and the classification layer. The ResNet100 is selected as the backbone to extract face feature maps $C(x_i)$ from processed face images $x_i \\in X^{w \\times h \\times c}$ where h, w, and c refer to the height, width, and number of channels, respectively. The local feature extraction module adopts a Multi-Head Multi-Scale Local Feature Extraction (MHMS) module, composed of several MSNet networks. A single MSNet implements spatial attention at each scale to obtain more efficient local information. Additionally, it utilizes channel attention to minimize redundancy and enhance the representation. The multi-head structure formed by multiple MSNet modules ensures that the model comprehensively captures rich facial information. The Global Feature Extraction (GFE) module transforms the face feature map $C(x_i)$ into a compact and discriminative global feature $Y_i$ through linear transformation, where $Y_i = G(C(x_i))$ and $G(\u00b7)$ represents a projection, implemented with a fully connected layer. To dynamically measure the attention between local and global features, we propose a Local and Global Feature Fusion (LGF) module. The LGF module aims to achieve information complementation between local and global features to reduce feature quality biases. The network structure of the LGAF is shown in Fig.2."}, {"title": "3.1. Feature Norm", "content": "Although previous work has revealed partial properties of feature norm, it is not comprehensive. To further explore the relationship between feature norm and the causes of low-quality images, we designed three experiments. Considering the reliability and convenient access of the data source, we use the data of pose variation, expression variation, and different motion blur intensity to represent the low-quality face data mainly caused by partial facial region missing, deformation in partial facial regions, and joint influence of the two above (as shown in Fig.1)."}, {"title": "3.1.1. Pose and feature norm", "content": "Employing head pose as a controlled variable, we focus on the relationship between head yaw angles and feature norm, which represents the relationship between feature norm and the degree of facial region missing. We select face images under natural light from the Multi-PIE dataset [23] to explore the variation trend between feature norm and the partial facial regions missing caused by pose variation.\nAs shown in Fig.3a, the larger the head yaw angle, the larger the facial missing area. To mitigate individual differences affecting the experimental results, we calculate the global and local average feature norm of different absolute yaw angles (Fig.4a). When the absolute yaw angle is in the range of 0 to 45 degrees and 60 to 90 degrees, the local feature norm is relatively stable and does not change significantly with the expansion of the face region missing. When the absolute yaw angle is between 45 and 60 degrees, almost half of the face information is lost, and the local feature norm decreases greatly. In contrast, the global feature norm nearly decreases linearly with the change of absolute yaw angle. The experiment shows that the local feature norm is relatively stable for partial face region missing within a certain range, while the global feature norm is easily affected. As shown in Fig.4d, we analyze the correlation between the absolute head yaw angle within 0 to 45 degrees and the norm of local and global features, yielding correlation scores of 0.028 and -0.441, respectively."}, {"title": "3.1.2. Expression and feature norm", "content": "We illustrated the relationship between facial local deformation intensity and feature norm by using the relationship between different expression and feature norm. From the RaFD dataset [24], we selected facial data with natural, disgust and contempt labels. Contempt, disgust and angry are expressed as:\n$Contempt = AU14,$\n$Disgust = AU9 + AU10 + AU25,$\n$Angry = AU4 + AU5 + AU7 + AU17 + AU23 + AU24,$\nwhere AU4, AU5, AU7, AU9, AU10, AU14, AU17, AU23, AU24 and AU25 correspond respectively to brow lower, upper lid raise, lid tighten, nose wrinkle, upper lip raise, dimple, chin raise, lip tighten, lip press and lips part. Based on the number of action units involved, we consider three different expression as three intensities of facial distortion changes, in order: angry, disgust, contempt and natural (Fig.3b). As shown in Fig.4b, the correlation score between facial local distortion intensity and local feature norm is -0.43, while the correlation score between facial local distortion intensity and global feature norms is 0.03 The experiments indicate that as the intensity of facial distortion increases, there is an almost linear decline in local feature norm, while global feature norm remains relatively stable."}, {"title": "3.1.3. Motion blur and feature norm", "content": "It is observed that motion blur is one of the main causes of low-quality face images. The increase of motion blur intensity may not only lead to substantial deformation in partial facial regions (such as the eye shape changes at Length 12 in Fig.3c), but also result in the loss of partial facial regions (such as the eyebrow region information missing at Length 18 in Fig.3c). We generate motion-blurred images from face images of the frontal face under flash light in the Multi-PIE dataset [23]. As shown in Fig.4c, we use moving interval pixel lengths of 0, 6, 12, and 18 representing multiple blurring levels. As motion blur intensity increases, local and global feature norm nearly exhibit a decreasing trend, and their correlation scores are -0.456 and -0.584, respectively (as depicted in Fig.4d).\nThe above three experiments prove that the feature norm can serve as a proxy to evaluate local and global feature quality. (a) In cases of low-quality samples primarily caused by missing local facial regions (e.g., extreme pose, occlusion, etc., as depicted in category A of Fig.1), local features are more conducive to recognition due to the focus on local similarity, while global features deteriorate with information loss. The global feature norm also decreases with the expansion of the missing degree, and the local feature norm remains relatively stable. (b) For samples affected only by deformation in partial facial regions (e.g., expression, age change, makeup, etc., as shown in category B in Fig.1), excessive emphasis on local features may lead to mismatching, while global features, with better robustness, are more favorable for identity recognition. As the intensity of deformation increases within local face regions, there is a clear response from the local feature norm, while the global feature norm remains relatively stable. (c) When the samples are simultaneously impacted by missing and deformation within partial facial regions (such as low-resolution images shown in category C of Fig.1), the quality of local and global features decreases due to high variability, and there is a high correlation between feature norm and motion blur intensity. Therefore, we treat the feature norm as a proxy for local and global feature quality to help the model measure feature quality within the LGF module.\nFurthermore, we conduct a statistical analysis of the feature norm distribution on TinyFace dataset, which is a low-quality dataset reflecting real-world unconstrained scenarios. In Fig.5, we visually display a portion of the samples, revealing that many faces with low global feature norm are notably absent. Additionally, face images with low local feature norm exhibit varying degrees of facial deformation. These results demonstrate that the correlation between the causes of low-quality images in the real world and their local and global feature norm further validates the use of feature norm as an indicator of feature quality."}, {"title": "3.2. Local and Global Feature Fusion", "content": "For low-quality face images caused by different factors, local features and global features have their own advantages and disadvantages. Driven by face feature quality, we propose a Local and Global Feature Fusion (LGF) module to realize effective information complementation between local and global features without excessive cost to the network. However, the selection of facial feature quality evaluation metric plays a key role. We consider a simple and reliable way to measure the quality of feature embeddings. It is assumed that the influence of partial face regions missing and deformation on the sample are independent of each other. When the sample is missing part of the face region, as the missing region expands, the method should be sensitive to the global features while maintaining stability concerning local features. When the sample is mainly affected by the local deformation of the face, with the deepening of the local deformation, the method should display sensitivity towards local features, but stable to global features.\nInspired by [25, 26], we measure the local or global feature energy to evaluate the effective amount of information contained in a feature, and then use it as a proxy for feature quality. The formula for calculating the feature energy is as follows:\n$E(T)=\\frac{||T||}{\\sqrt{n}}$\n$ = \\sum_{i=1}^{n}(\\tau_i)^2, j\\in (l, g),$ \nWhere $E(\u00b7)$ denotes the feature energy, $T^{l} = Y$ represents the local feature, $T^{g} = Y$ represents the global feature, and $T_i$ denotes the components of $T$. In the calculation form, the feature energy has the same calculation form as the feature norm, considering alignment with previous research, it is uniformly referred to as the feature norm in this paper.\nThrough the experiments in Sec.3.1, it is proved that the feature norm has a similar change trend and can be used as a proxy of the feature quality.\nLocal feature norm $Z^l$ and global feature norm $Z^g$ are calculated by the Feature Quality Assessment (FQA) module, where $Z^l = ||Y||$ and $Z^g = ||Y||$. We normalize the feature norm using the batch statistics $\\mu_z$ and $\\sigma_z$ as follows:\n$\\hat{Z} = h\\cdot clip \\left(\\frac{Z - \\mu_z}{\\sigma_z}, -1, 1\\right)+1, \\qquad j \\in (l, g),$ \nwhere $clip\\left(\\frac{Z - \\mu_z}{\\sigma_z}, -1, 1\\right)$ clips the value to [-1, 1], and $\\hat{Z} \\in [0,2]$. The distribution of $\\hat{Z}$ is approximately unit Gaussian, and we use a hyperparameter h to regulate the distribution concentration. Following the 3-sigma rule based on the unit Gaussian distribution, we determine that setting the hyperparameter h to 0.333 ensures that the vast majority of samples fall within the range of -1 to 1. Batch statistics are updated by the exponential moving average (EMA) to ensure stability. t indicates t-th step, then\n$\\mu_z = \\alpha \\mu_z + (1-\\alpha)\\mu_z^(t-1),$\n$\\sigma_z = \\alpha \\sigma_z + (1-\\alpha)\\sigma_z^(t-1),$\nwhere a is set to 0.01 for momentum.\nThe attention between local and global features is calculated as follows:\n$\\gamma_j = \\frac{e^{\\hat{Z}_j}}{\\sum_{j\\in {l,g}}e^{\\hat{Z}_j}},$\nand $\\gamma_j$ represents the attention weight for features. When partial facial region missing is the dominant factor of low image quality, the model will allocate more attention to local features to keep the recognition performance stable. When substantial deformation in partial facial regions leads to low-quality images, the model will allocate more attention to global features to avoid excessive focus on local similarities that may result in mismatching.\nThe new face feature after the LGF module is shown as follows:\n$k_i = \\gamma Y + \\psi Y,$\n$k_i$ as a new face feature is normalized and fed to the classification layer to calculate the probability of each class. Through the LGF module, the model dynamically allocates the attention weight between local and global features, which is beneficial to obtain more discriminative and high-quality features to help the model achieve stable recognition performance."}, {"title": "3.3. Multi-Head Multi-Scale Local Feature Extraction Module", "content": "Existing methods for local feature extraction lack effectiveness in obtaining and enhancing local information across various scales. Inspired by [27], we propose a Multi-Scale Local Feature Extraction Network (MSNet), which utilizes multi-scale convolution operations to capture local features. Compared to the LS-CNN network, we believe that local features at different scales have distinct spatial positional distributions. Therefore, while allowing the MSNet module to comprehensively consider information from various range neighboring locations during attention computation, we extract spatial attention for local feature maps at each scale to emphasize crucial local regions. Moreover, the SE module is employed to implement channel attention for filtering redundant information. By leveraging spatial and channel dual-dimensional attention, it achieves a thorough exploration of identity information at various scales. Considering that the information of key regions cannot always be effectively obtained (e.g. key parts are occluded), a single local feature is vulnerable. To address this issue, we adopt the multi-head structure for the MHMS module to extract multiple local features simultaneously. It helps the model maintain stable recognition performance and increases separability in high-dimensional space. Even when the facial features of some regions are difficult to extract, the local information of the remaining regions can still help the model for identity recognition.\nWe use convolution operations of various kernel sizes to extract the face feature maps given by the backbone network. Based on the idea that local features at different scales have varying spatial distributions, we measure the spatial attention at each scale using the LANet proposed by Wang and Guo [18]. The LANet consists of two layers of 1x1 convolution operations for calculating local spatial attention. Regions judged as important by the LANet are assigned higher attention, while less discriminative regions receive lower attention. Through this module, we can focus on the local regions that are beneficial to recognition and reduce the noise brought by less relevant regions. Let t be set to 1, P(\u00b7) be a LANet, and GAP(\u00b7) denote a global average pooling operation. P(\u00b7) is defined as follows:\n$P(\u03c9) = \u03c9\u03a6 (\u03a6_1(GAP(\u03c9))),$\nand the local feature maps obtained after implementing spatial attention at each scale are represented as follows:\n$\\mathcal{L}_j = P(\\Phi_j(C(x_i))),$\nwhere $\\Phi_j(\u00b7)$ is the convolution operation to obtain local feature maps at various scales and $\\Phi_j(w) = W_j w$. Specifically, j represents the size of the equivalent convolution kernel.\nSince the key local information of FR is scattered at different scales, local face features are highly discriminative at some scales, but local information at other scales may lead to confusion. Therefore, concatenating them directly may lead to performance degradation. We adopt the SE module to apply channel attention to the feature maps of different scales, which will enhance the effective representation of local features. S(\u00b7) represents the SE module and the local feature provided by the k-th MSNet module is as follows:\n$\\mathcal{L}^k = F(S (\\Omega(\\mathcal{L}_1^k, ..., \\mathcal{L}_n^k))),$\nwhere \u03a9(\u00b7) represents a concatenation operation and F(\u00b7) represents a projection of local features.\nA single local feature is vulnerable to partial facial region missing. Therefore, we adopt a multi-head structure to extract b local features simultaneously. When effective local features cannot be extracted from certain regions, the remaining local features can still contribute to stable recognition. The local features obtained by the MHMS are as follows:\n$\\Psi_i = \\Omega(\\mathcal{L}_1, ..., \\mathcal{L}_b),$"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets and Implementation Details", "content": ""}, {"title": "4.1.1. Training set", "content": "We use MS1MV2 [28] and WebFace4M [29] as our training datasets. MS1MV2 has a total of 5.8M face images in 85K individuals by cleaning noise labels from MS-Celeb-1M [30] dataset. WebFace4M is a dataset comprising 10% of the data from WebFace260M, and it contains a total of 206K individuals and 4.2 million face images."}, {"title": "4.1.2. Test set", "content": "We select CFP-FP [31], CPLFW [32], AgeDB [33], and CALFW [34], which are widely utilized FR benchmarks for 1:1 verification, to assess the recognition performance of the models in low-quality scenes caused by diverse factors. The CFP-FP dataset is designed to gather face images under various poses in unconstrained scenarios. Each identity encompasses 10 frontal face images and 4 side faces with distinct poses, aiming to evaluate the performance of face verification in the real world. The CPLFW dataset collects 3,000 positive face pairs from the Internet with pose differences while minimizing other attribute disparities for studying the issue of pose invariance in FR. The AgeDB dataset collects face images of celebrities of different ages via the Internet. The dataset incorporates a considerable number of face images of varying ages, ranging from 1 to 101 years old. The CALFW dataset is a validation set of faces that vary with age in unconstrained scenarios and focuses on those affected by age variability in FR.\nTinyFace [35] is a low-resolution face dataset collected from public web data for 1: N recognition tests. The images were collected under circumstances of uncontrolled pose, illumination, occlusion, and background. SCface [36] was captured in an uncontrolled indoor environment, and images from commercial cameras with different resolutions simulate real-world conditions, concentrating on different surveillance use case scenarios."}, {"title": "4.1.3. Training setting", "content": "We resize the original pictures into 112x112 images and apply the same data augmentation strategy as in [22]. Random rectangle cropping, photometric augmentation, and rescaling operations are applied with a probability of 0.2. These operations expand the quantity of low-quality images in the training dataset. Random rectangular cropping and photometric augmentation enhance the impact of the partial facial region missing from the original image. Image rescaling involves resizing the image to a smaller size and back, resulting in blurriness. This operation simulates the low-quality images caused by low resolution. ResNet100 is selected as the backbone network, and ArcFace loss and CosFace loss as the loss functions to evaluate the performance of the LGAF, respectively. For loss functions, the scale parameter s is 64, and the margin parameter m is set to 0.4. The model is trained using the SGD optimizer with an initial learning rate of 0.1 and step scheduling at 12, 20, and 24 epochs."}, {"title": "4.2. Ablation and Analysis", "content": "We use ResNet100 and MS1MV2 as the backbone and training set for ablation experiments, and the ArcFace loss as a default loss function. The ablation experiments investigate the following three aspects: (a) Effects of the MHMS, the GFE, and the LGF; (b) The settings of parameter b; (c) Effects of different fusion modes in the LGF."}, {"title": "4.2.1. Effects of the MHMS, the GFE, and the LGF", "content": "To investigate the effectiveness of our proposed module, CFP-FP, CPLFW, AgeDB, CALFW, and TinyFace datasets are used for performance evaluation. In Method I and II of Tab.la, we explore the effectiveness of the MHMS module and the GFE module. In particular, when only global features are extracted, the model degenerates to ArcFace. When extracting only local features (Method I), the recognition rate of CFP-FP, CPLFW, and CALFW is better than only extracting global features (Method II). The performance of the MHMS and the GFE on CFP-FP and AgeDB datasets reveals that the MHMS network excels on the validation set with significant pose variations (CFP-FP), while the GFE network outperforms on the validation set with substantial age variations (AgeDB). This further proves that global features are more discriminative when facial regions are significantly deformed, while local features are more conducive to FR where part of the facial region is missing. For TinyFace (As shown in Tab.1b), closed-set ranking retrieval demonstrates a marked enhancement in the performance of Method I compared to II.\nBy disabling the LGF module, Method III of Tab.la combines both local and global features directly by adding them together and normalizing them to obtain new fused features. However, compared with Method I and II, Method III improves the recognition rate by 0.19%, 0.16%, and 0.11% respectively in closed-set ranking retrieval on TinyFace (as described in Tab.1b), but does not produce the best performance on CFP-FP, AgeDB or CALFW. These suggest that simply combining local and global features does not directly enhance model recognition capability; it may be attributed to introducing redundant information.\nTo effectively complement local and global information and improve the quality of face features, we propose an LGF module. To verify the effectiveness of the LGF module, the performance comparison of the LGAF after ablating the LGF module is shown in Tab.la for Method III and LGAF. Experimental results verify the effectiveness of the LGF module, and the highest performance is hit in all four 1:1 verification datasets. The recognition rate of the LGAF on the validation set with pose changes (CFP-FP) is improved from 98.64% to 98.77%. It maintains comparable recognition rates as Method II on validation sets with age variation (AgeDB) while achieving the best performance in all closed-set ranking retrieval for TinyFace."}, {"title": "4.2.2. The settings of parameter b", "content": "For parameter b ablation, we analyze the performance of the LGAF with various b values. The results are detailed in Tab.2. We set the parameter b to 2, 4, 8, and 16. As the value of b increases, the average performance of four verification sets initially improves and then decreases. When b is set to 2, average performance hits its lowest point; when b is 4, the average recognition rate achieves its peak at 96.67%. Consequently, we select b to 4."}, {"title": "4.2.3. Effects of different fusion modes in the LGF", "content": "Regarding the LGF module, we explore three feature quality measures and three ways of fusing local and global features, aiming to find a more appropriate fusion strategy.\nWe compare baseline, energy-based (i.e., feature norm), and entropy-based [37] feature quality measures. As the Tab.3 shows the feature quality evaluation metric based on energy (feature norm) achieves the best performance on all four validation sets. This also demonstrates the effectiveness of the feature norm.\nWe evaluate three fusion approaches: (a) Concatenating local and global features with different attention; (b) Using the adapted iAFF [38] (a multi-feature fusion module) to achieve fusion between local and global features; (c) Adding local and global features with different attention. The average recognition rate on four 1:1 verification sets is used as a performance metric. As shown in Tab.4, the experimental results indicate that employing a complex fusion network does not help the model obtain the best recognition performance and adds excessive cost. While the adapted iAFF feature fusion module improves the average performance from 96.05% to 96.45%, it does not help the model achieve the highest average performance. The attention-based addition of features elevates the average performance from 96.45% to 96.67% and achieves the best performance on all four validation sets. Consequently, we choose the attention-based addition mode in the feature fusion module of the LGAF. This method not only helps the model achieve excellent recognition performance but also does not incur excessive costs."}, {"title": "4.3. Comparison with SoTA Methods", "content": "To compare with recent state-of-the-art methods", "1": 1, "follows": "nLocal feature representation network. CQA-Face [11", "17": "extracts local features at different scales", "28": "as baseline. Based on the feature norm information", "21": "and AdaFace [22", "40": "proposes to adaptively emphasize misclassified samples. URL [41", "43": "further constrains the separation of positive and negative examples by a learnable threshold in the training phase. The above models use a CNN-based backbone network to extract global features. Furthermore", "37": "based on vision transformer (ViTs) proposes patch-level data augmentation and difficult sample mining strategies to alleviate the overfitting problem of ViT in FR.\nAs shown in Tab.5"}, {"1": 1}]}