{"title": "Bridging Internal Probability and Self-Consistency for Effective and Efficient LLM Reasoning", "authors": ["Zhi Zhou", "Yuhao Tan", "Zenan Li", "Yuan Yao", "Lan-Zhe Guo", "Xiaoxing Ma", "Yu-Feng Li"], "abstract": "Recent advancements in large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, single-shot inference often yields unreliable results for complex reasoning tasks, leading researchers to explore multiple reasoning paths through methods such as perplexity and self-consistency. In this paper, we present the first theoretical error decomposition analysis of these techniques, breaking down their error into estimation error and model error. Our analysis reveals a fundamental trade-off: perplexity methods suffer from substantial model error due to the absence of a proper consistency function, while self-consistency exhibits high estimation error due to a slow error convergence rate. To overcome these limitations, we propose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines Perplexity Consistency, which seamlessly integrates LLM perplexity with self-consistency, and Reasoning Pruning, which eliminates low-probability reasoning paths to effectively prevent the degeneration of estimation error reduction. Theoretical analysis demonstrates that RPC not only accelerates the convergence rate of estimation error to an exponential level but also holds strong potential for further reducing model error. Extensive empirical evaluations on seven benchmark datasets confirm that RPC can significantly improve reasoning performance, sample efficiency, and confidence reliability.", "sections": [{"title": "1. Introduction", "content": "Recently, large language models (LLMs) have shown significant progress in various applications such as problem solving (Lewkowycz et al., 2022; Li et al., 2024a), planning (Valmeekam et al., 2023; Deng et al., 2024), and decision making (Ouyang & Li, 2023; Sblendorio et al., 2024), demonstrating their reasoning capabilities. Since single-shot inference is not always reliable, especially in complex reasoning tasks, one often requires the LLM to produce multiple reasoning paths, facilitating its reasoning performance.\nWhen multiple reasoning paths for a given problem are available, the reasoning performance is determined by the confidence estimation for each result. To achieve this, perplexity methods (Chen et al., 1998; Murugadoss et al., 2025) apply LLMs' internal probability to estimate the confidence of the reasoning path. Although the internal probability is quite accurate, the reasoning path confidence is highly insufficient to distinguish each answer, thereby greatly limiting the effectiveness of perplexity methods (Chen et al., 2024). In contrast, self-consistency methods (Wang et al., 2022; Chen et al., 2023b) switch to establish the answer confidence using a pre-defined consistency function. However, the answer confidence cannot be directly derived from the internal probabilities of LLMs, necessitating the use of Monte-Carlo estimation, which significantly degrades the convergence rate (Aggarwal et al., 2023; Wan et al., 2024; Wang et al., 2024).\nTo better understand the limitations of current methods and to guide the development of an effective and efficient LLM reasoning approach, we formulate the LLM reasoning problem and present a theoretical analysis that decomposes the reasoning error into two components: Estimation Error and Model Error. Self-consistency methods, which rely on the Monte-Carlo estimation, achieve only a linear estimation error reduction rate with respect to the sample size. The linear convergence rate leads to the method requiring a large sampled budget. For instance, implementing self-consistency with 64 samples on the MATH dataset using the GPT-4 API costs approximately $2000 (Li et al., 2024c), rendering it extremely expensive for both researchers and organizations. As to perplexity methods, their estimation error converges exponentially as they use the internal probability of LLMs. The exponential convergence rate ensures that perplexity methods can work well even in a very limited sample budget, while its final convergent result is far from satisfactory"}, {"title": "2. Problem and Analysis", "content": "In this section, we start by outlining the problem formulation of LLM reasoning through sampling multiple reasoning paths. Then, we provide a theoretical analysis that decomposes LLM reasoning performance into estimation error and model error. Finally, we present experimental results verifying our theoretical analysis. Our theoretical and empirical analysis motivates our follow-up method design."}, {"title": "2.1. Problem Formulation", "content": "Given a reasoning problem $(x, y)$, where $x$ represents the input query, and $y$ represents the ground-truth answer. The LLM generates a reasoning path $t = (t_1,..., t_m)$ by sequentially sampling tokens according to the conditional probability distribution $p(t_i|x, t_{<i})$, where $m$ denotes the length of the reasoning path. The probability of generating the reasoning path $t$ is defined as $p(t|x)$, a.k.a the confidence of the reasoning path $t$. An answer extraction function $g(\\cdot)$ maps the reasoning path to the final answer $\\hat{y} = g(t)$, and the reasoning correctness is evaluated by the indicator function $[\\hat{y} = y]$. We can extend the probability to the answer $\\hat{y}$, i.e., the answer confidence, denoted as $p(\\hat{y} | x)$.\nThe confidence essentially represents the probability that the reasoning path $t$ or answer $\\hat{y}$ is correct, which enables LLMs to select the most reliable solution among multiple candidates. Nevertheless, enumerating all reasoning paths or answers is unfeasible; we have to estimate the LLM confidence based on finite $n$ sampled reasoning paths instead. Furthermore, to measure the reasoning performance of LLMs, we use the squared error of confidence estimation $p(t | x)$ to the reasoning path $t$:\n$E(p) = (p(t|x) \u2013 \\mathbb{I}[g(t) = y])^2$.\nIf we can extend the confidence estimation to the answer $\\hat{y}$,"}, {"title": "2.2. Theoretical Analysis", "content": "To maximize the reasoning performance of LLMs, self-consistency methods (denoted as SC) (Xiong et al., 2024; Abbasi-Yadkori et al., 2024; Becker & Soatto, 2024) often sample $n$ reasoning paths $t_1, . . ., t_n$, and then estimate the probability of each answer by\n$\\hat{p}^{(Sc)} (\\hat{y} | x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}[\\hat{y}_i = \\hat{y}], \\ \\hat{y}_i = g(t_i)$.\nThen, the reasoning error of the SC method for a given problem $(x, y)$ can be computed by\n$E(\\hat{p}^{(Sc)}) = E_{t_i \\sim p(t|x)} [(\\hat{p}^{(Sc)} (\\hat{y} | x) - \\mathbb{I}[y = y])^2]$\n$= E_{t_i \\sim p(t|x)} [(\\sum_{i=1}^{n} \\mathbb{I}[\\hat{y}_i = \\hat{y}] - \\mathbb{I}[y = y])^2$.\nTo illustrate the key factors affecting the reasoning error, we provide an error decomposition in the following proposition.\nProposition 1 (SC Reasoning Error Decomposition). For any input $x$ with ground-truth answer $y$, let $\\hat{p}^{(Sc)} (\\hat{y}|x)$ denote the estimated probability of $\\hat{y}$ by SC. Then, the reasoning error $E(\\hat{p}^{(Sc)})$ can be divided into two components:\n$E(\\hat{p}^{(Sc)}) = \\frac{1}{n} p(\\hat{y}|x)(1 - p(\\hat{y} | x))$\n\\text{Estimation Error}\n$+ (p(\\hat{y} | x) \u2013 \\mathbb{I}[\\hat{y} = y])^2$.\n\\text{Model Error}\nRemark 1. The detailed proof is provided in Appendix A.1. The estimation error refers to the error caused by the finite sampling from the LLM probability, while the model error indicates the LLM's limited reasoning capability. Note that the estimation error of SC reduces to only the variance as the sampling is unbiased. This proposition demonstrates that, aside from the model error, which is determined by the LLM's inherent reasoning capabilities, the reasoning error is bounded by the estimation error. Moreover, the estimation error reduction rate of the sample size is linear, resulting in a large error margin when the sampling is insufficient.\nTo effectively offset the estimation error, we switch to analyze the reasoning error of perplexity methods (denoted as PPL). In contrast to the SC method that estimates the answer probability using the Monte-Carlo estimation, PPL directly utilizes the internal probability of LLMs $p(t_i|x)$ for the sampled reasoning path $t_i$. Therefore, for given the unique set of $n$ sampled reasoning paths $\\mathcal{R} = \\{t_1,...,t_n\\}$, the estimated probability of each reasoning path $t$ is\n$\\hat{p}^{(PPL)} (t | x) = \\begin{cases}\n  p(t_i|x), & \\text{if } t = t_i \\\\\n  0, & \\text{otherwise}\n\\end{cases}$\n$= \\sum_{t \\in \\mathcal{R}} \\mathbb{I}[t = t_i] p(t | x)$.\nSimilarly, we also use the mean squared error to measure the reasoning performance of PPL:\n$E(\\hat{p}^{(PPL)}) = E_{t_i \\sim p(t/x)} [(\\hat{p}^{(PPL)} (f|x) - \\mathbb{I} [\\hat{y} = y])^2]$\n$= E_{t_i \\sim p(t/x)} [(\\sum_{t \\in \\mathcal{R}} \\mathbb{I} [t = t_i] p(t|x) - \\mathbb{I}[g(t) = y])^2$.\nNow, we can obtain the following proposition.\nProposition 2 (PPL Reasoning Error Decomposition). For any given input $x$ with ground-truth answer $y$, let $\\hat{p}^{(PPL)} (f|x)$ denote the estimated probability of $f$ by PPL method. Then, the reasoning error $E(\\hat{p}^{(PPL)})$ can be divided into two components:\n$E(\\hat{p}^{(PPL)}) = (1 - p(f|x))^n p(f|x)(2\\mathbb{I}[\\hat{y}_i = y] \u2013 p(f|x))$\n\\text{Estimation Error}\n$+ (p(f|x) \u2013 \\mathbb{I}[g(t) = y])^2$.\n\\text{Model Error}\nRemark 2. The detailed proof is provided in Appendix A.1. Compared with SC, the estimation error of PPL decreases exponentially, which is much faster. However, the model error of PPL is usually larger than that of SC in practice. In Appendix A.2, we provide Proposition 3 to demonstrate that SC achieves a smaller model error than PPL in the ideal case, due to the advantages of the consistency function."}, {"title": "2.3. Empirical Observations", "content": "To confirm our theoretical results, we conduct some initial experiments on the GSM8K dataset using the InternLM-MATH-Plus 7B model. We limit the sample size $n$ from 1 to 6 and plot the accuracy curves and the estimation error in Figure 2. Additionally, we include an ablative version called by Na\u00efve-SC. Na\u00efve-SC applies the Monte-Carlo estimation, which is consistent with SC, but its consistency function is degraded to a Na\u00efve version to the reasoning path matching rather than the answer matching, which is consistent with PPL. In other words, the reasoning error $E(\\hat{p}^{(Na\u00efve-Sc)})$ of"}, {"title": "3. Methodology", "content": "Based on our theoretical and empirical analysis, we propose a new method called Reasoning-Pruning Perplexity Consistency (RPC). Specifically, we first integrate internal LLM probability into the self-consistency framework, paving the way for a confidence estimation function called Perplexity Consistency (PC). This function utilizes LLM probabilities to reduce estimation error more efficiently while maintaining a low model error. Our further analysis guides the design of a new Reasoning Pruning (RP) module that addresses the limitations of PC by filtering out reasoning paths with low probabilities. Figure 3 shows the overall framework."}, {"title": "3.1. Perplexity Consistency", "content": "To improve the efficiency of estimation error reduction, we propose PC, which directly leverages the LLM's prediction probability like PPL, obtaining the benefit of an exponential convergent rate; and also applies the consistency function of SC, to minimize the model error. Formally, for the unique set of $n$ sampled reasoning paths $\\mathcal{R} = \\{t_1,...,t_n\\}$, the estimated probability of the answer is\n$\\hat{p}^{(Pc)} (\\hat{y} | x) = \\sum_{t \\in \\mathcal{R}} \\mathbb{I}[g(t) = \\hat{y}]p(t|x)$,\nwhich calculates the cumulative probability of all unique reasoning paths whose answer is $\\hat{y}$. Therefore, the mean squared error of PC is\n$E(\\hat{p}^{(Pc)}) = E_{t_i \\sim p(t/x)} [(\\hat{p}^{(Pc)} (\\hat{y} | x) - \\mathbb{I}[\\hat{y} = y])^2]$.\nNow, we present the following theorem, which explores the reasoning error decomposition of PC.\nTheorem 3 (PC Reasoning Error Decomposition). Assume that $k = |\\{t | g(t) = \\hat{y}\\}|$ and define $a := 1 - p(\\hat{y}|x)$. Then, the reasoning error $E(\\hat{p}^{(Pc)})$ of PC can be divided into two components:\n$E(\\hat{p}^{(Pc)}) = ap(\\hat{y} | x) (2\\mathbb{I}[\\hat{y} = y] \u2013 (1 + a)p(\\hat{y}|x))$\n\\text{Estimation Error}\n$+ (p(\\hat{y}|x) \u2013 \\mathbb{I}[\\hat{y}_i = y])^2$.\n\\text{Model Error}\nRemark 4. The proof is presented in Appendix A.3. The theorem states that PC successfully fuses the strengths of PPL and SC: it achieves the same level of model error as SC while ensuring the same convergence rate as PPL in the estimation error. Particularly, the convergence rate can be computed as $a^n p(\\hat{y}|x) = (1 \u2013 p(\\hat{y}|x))^n p(\\hat{y}|x)$.\nThe convergence rate is primarily influenced by the magnitude of $p(\\hat{y}|x)$. In most scenarios, it remains exponential, facilitating rapid estimation error reduction. However, when $p(\\hat{y} | x) \\rightarrow 0$ and $np(\\hat{y}|x) < 1$, we only have $a^n \\rightarrow \\frac{1}{1+np(\\hat{y} | x)}$ (Kozma, 2021), resulting in the convergence rate unexpectedly degenerating to a linear result."}, {"title": "3.2. Reasoning Pruning", "content": "Our analysis of the convergence rate in estimation error suggests some possibility for further improving the estimation error reduction efficiency. Specifically, the rate degenerates when $p(\\hat{y} | x)$ is too small. Therefore, we propose directly pruning these low-probability answers instead of sampling, called by Reasoning Pruning (RP).\nReasoning pruning essentially aims to set $p(\\hat{y} | x) = 0$, i.e., when the cumulative probability of all its corresponding reasoning paths are very low, making the estimation error vanish. Although pruning low-probability answers can boost the efficiency of estimation error reduction, it also induces a level of model error. In other words, it may ignore some correct answers of low probability, thus implicitly degrading the performance of LLM reasoning.\nIdeally, if we know the probability $p(y|x)$ that the LLM can generate the correct answer, the optimal threshold for pruning should be $\\tau = p(y|x)$. In this case, one can obtain not only an exponential convergence rate but also a significant reduction in model error, as all incorrect answers $\\hat{y}$ satisfying $p(\\hat{y}|x) < \\tau$ are eliminated. However, to obtain this optimal error reduction, two challenges need to be resolved: (1) we only have an estimate of $p(\\hat{y} | x)$ since the accurate value is unknown; and (2) we cannot know the ground-truth probability $p(y | x)$, making the threshold difficult to determine.\nFor the first problem, we propose directly pruning reasoning paths instead of answers in RP, with the following theorem.\nTheorem 5 (Effectiveness of Reasoning Path Pruning). Assume that the optimal threshold $\\tau = p(y|x)$, and let $k = |\\{t_i | g(t_i) = \\hat{y}, i = 1,..., n\\}|$, which refers to the size of samples whose answer is $\\hat{y}$. Hence, RP achieves the optimal error reduction with at least the probability\n$1 - exp\\left(-2 k k^2 (1 - \\frac{\\tau}{a})^2\\right)$.\nRemark 6. The proof is included in Appendix A.4. The theorem provides a guarantee that RP can achieve the optimal error reduction for each given problem $(x, y)$ at a high prob-"}, {"title": "4. Experiments", "content": "In this section, we first conduct experiments to answer the following research questions:\nRQ1: Efficiency. How does RPC reduce the number of samples required to achieve comparable performance through faster convergence?\nRQ2: Efficacy. How does RPC improve reasoning performance compared to existing methods?\nRQ3: Reliability. How does RPC enhance the reliability of confidence estimation compared to existing methods?\nAdditional discussions are devoted to further demonstrating the effectiveness of RPC. Due to space limitations, supplementary experimental results are included in Appendix D."}, {"title": "4.1. Experimental Setting", "content": "In this section, we briefly introduce the comparison methods, datasets, and details of the implementation. The experimental settings can be found in Appendix C."}, {"title": "4.2. Empirical Results", "content": "RQ1: Efficiency. How does RPC reduce the number of samples required to achieve comparable performance through faster convergence?\nWe evaluate our proposed RPC against the standard self-consistency method using four mathematical benchmark datasets with the InternLM-2-MATH-Plus 7B model. For the MATH dataset, we set the reasoning path size to 64, while we set the number of reasoning paths to 128 for the other datasets with SC. We then record the best performance and minimum sampling requirements for SC. For both RPC and our Perplexity Consistency module (denoted as PC), we report the minimum number of samples needed to match or exceed the performance of the SC in Table 1.\nThe results of PC indicate improved convergence rates compared to SC in several cases, while maintaining similar rates in others. These findings support the rapid convergence and degeneration issues of PC in Theorem 3. RPC shows significant efficiency improvements by requiring fewer samples to achieve comparable performance relative to Sc. Moreover, the degeneration issues observed in PC are effectively addressed in RPC, validating both the effectiveness of the Reasoning Pruning module and our Theorem 5.\nRQ2: Efficacy. How does RPC improve reasoning performance compared to existing methods?\nWe evaluate the performance of PC and RPC in Figure 4 across various sample budgets. The results demonstrate that RPC achieves better performance than both PPL (which relies on internal LLM probabilities) and SC (which employs Monte Carlo sampling). The detailed accuracy results, including mean and standard deviation in Table 2 support these findings.\nWe also analyze the performance of PC separately. The results indicate that PC has a faster convergence rate than SC, which aligns with Theorem 3. The significant performance gains from PC to RPC, as shown in Figure 9a and Figure 9b, validate the effectiveness of the Reasoning Pruning module. This suggests that Reasoning Pruning helps reduce model errors when the LLM exhibits good alignment by eliminating incorrect reasoning paths that carry low LLM probability scores.\nRQ3: Reliability. How does RPC enhance the reliability of confidence estimation compared to existing methods?\nTo evaluate the reliability of confidence estimation, we analyze the ECE results of RPC and comparison methods in Table 2. ECE measures the difference between predicted"}, {"title": "4.3. Further Discussion", "content": "Results on Code Generation Tasks. To investigate whether our proposed approaches can generalize to other reasoning tasks, such as code generation tasks, we evaluate RPC and comparison methods on three code generation benchmarks, as illustrated in Figure 6. The results show that RPC achieves the highest accuracy across all datasets, demonstrating its effectiveness in reasoning tasks beyond mathematics."}, {"title": "5. Related Work", "content": "This paper is related to the two research topics, i.e., LLM Reasoning Boosting and LLM Confidence Estimation."}, {"title": "LLM Reasoning Boosting.", "content": "Recent research has developed various methods to improve LLM reasoning capabilities. CoT (Kojima et al., 2022) proposes the \"Let's think step by step\" prompt to guide LLMs"}, {"title": "LLM Confidence Estimation.", "content": "The confidence estimation for LLM can be categorized into three types: (1) perplexity confidence, (2) verbalized confidence, and (3) self-consistency confidence. Perplexity confidence (Huang et al., 2023; Duan et al., 2024) utilizes the geometric mean of LLM prediction probabilities (i.e., perplexity (Chen et al., 1998; Blei et al., 2003)) to evaluate model adherence (Murugadoss et al., 2025) and prompt quality (YAO et al., 2024). Verbalized confidence (Kadavath et al., 2022; Xiong et al.,"}, {"title": "6. Conclusion", "content": "In this paper, we address a foundational challenge in LLM reasoning: determining the most reliable answer from multiple reasoning paths by measuring LLM confidence. We present a theoretical framework to decompose the reasoning error into estimation error and model error, revealing that perplexity methods suffer from substantial model error due to lacking consistency function while self-consistency suffers from high estimation error because of a slow error convergence rate. To tackle this limitation, we introduce Reasoning-pruning Perplexity Consistency (RPC), a confidence estimation method with two key components: Perplexity Consistency utilizes internal LLM probabilities to achieve faster estimation error convergence in major cases. Reasoning Pruning prunes low-probability reasoning paths to prevent the remaining degeneration cases. Our theoretical analysis and extensive experiments demonstrate that RPC achieves superior error convergence rates and reasoning performance compared to existing methods."}, {"title": "Limitations and Future Work.", "content": "One limitation of this work is that we have only taken an initial step toward improv-"}, {"title": "Impact Statement", "content": "This work advances the efficiency and effectiveness of LLM reasoning with multiple reasoning paths. Our method can benefit various applications requiring reliable artificial intelligence reasoning, such as mathematical problem-solving and code generation. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Theoretical Results", "content": ""}, {"title": "A.1. Proof of Proposition 1 and Proposition 2", "content": "Proof. (SC) First, we denote the sampling probability distribution of the LLM as $p(\\hat{y} | x)$, and the confidence function as $\\hat{p}^{(Sc)} (\\hat{y} | x) = \\sum_{i=1}^{n} \\mathbb{I}[\\hat{y}_i = \\hat{y}]$, where $\\hat{y}_1, ..., \\hat{y}_n$ are sampled on the distribution $p(\\hat{y}|x)$. Apply the error decomposition, we have\n$E(p^{(Sc)}) = E_{\\hat{y}_i \\sim p(\\hat{y} |x)} [(\\hat{p}^{(Sc)} (\\hat{y} | x) - \\mathbb{I}[y = y])^2]$\n$= E_{\\hat{y}_i \\sim p(\\hat{y} |x)} [(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{I}[\\hat{y}_i = \\hat{y}] - \\mathbb{I}[y = y])^2]$\n$= E_{\\hat{y}_i \\sim p(\\hat{y} |x)} [(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{I}[\\hat{y}_i = \\hat{y}] - p(\\hat{y} | x) + p(\\hat{y} | x) \u2013 \\mathbb{I}[\\hat{y}_i = y])^2]$\n$= \\frac{1}{n} p(\\hat{y}_i | x)(1-p(\\hat{y}_i | x)) + (p(\\hat{y} | x) \u2013 \\mathbb{I}[\\hat{y}_i = y])^2$.\n(PPL) Another way is to use the confidence function to build the sampling probability distribution, i.e.,\n$\\hat{p}^{(PPL)} (f|x) = \\begin{cases}\n  p(t|x), & \\text{if there is } t_i = t \\\\\n  0, & \\text{otherwise}\n\\end{cases}$\n$= \\sum_{i=1}^{n} \\mathbb{I}(t_i = t)p(x)$.\nNow, we have\n$E_{\\hat{y}_i \\sim p(t_i|x)} [\\hat{p}^{(PPL)} (f|x) \u2013 p(f|x)] = -(1 \u2013 p(t|x))^n p(t|x)$\n$E_{\\hat{y}_i \\sim p(t_i|x)} [(\\hat{p}^{(PPL)} (f|x) \u2013 p(f|x))^2] = (1 \u2013 p(f|x))^n p(f|x)^2$.\nHence,\n$E(p^{(PPL)}) = E_{t_i \\sim p(t|x)} [(\\hat{p}^{(PPL)} (f | x) \u2013 \\mathbb{I}[\\hat{y} = y])^2]$\n$= E_{t_i \\sim p(t|x)} [(\\hat{p}^{(PPL)} (f|x) - p(t | x) + p(f|x) \u2013 \\mathbb{I}[g(t) = y])^2]$\n$= -(1-p(f|x))^n p(f|x)^2 + 2(1 \u2013 p(t|x))^n p(t|x)\\mathbb{I}[g(t) = y] + (p(f|x) \u2013 \\mathbb{I}[g(t) = y])^2$\n$= (1 \u2013 p(t|x))^n p(f|x)(2\\mathbb{I}[g(t) = y] \u2013 p(f|x)) + (p(f|x) \u2013 \\mathbb{I}[g(t) = y])^2$.\nHence, we complete the proof."}, {"title": "A.2. Model Error Comparison in Ideal Case", "content": "Proposition 3 (Comparison of Model Errors). Consider a setting with infinite sampling of reasoning paths ($n \\rightarrow \\infty$) where each incorrect reasoning path leads to a unique answer", "satisfy": "n$\\mathcal{E"}, {"perplexity": "n$\\mathcal{E}_{Model}^{(Sc)} = \\sum_{\\hat{y} \\in \\{g(t_i)|i=1...n\\}} (p^{(Sc)} (\\hat{y} | x) \u2013 \\mathbb{I}[\\hat{y}  I[y = y"}]}