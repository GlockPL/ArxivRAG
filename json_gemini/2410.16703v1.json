{"title": "PLDR-LLM: LARGE LANGUAGE MODEL FROM POWER LAW\nDECODER REPRESENTATIONS", "authors": ["Burc Gokden"], "abstract": "We present the Large Language Model from Power Law Decoder Representations (PLDR-LLM),\na language model that leverages non-linear and linear transformations through Power Law Graph\nAttention mechanism to generate well-defined deductive and inductive outputs. We pretrain the\nPLDR-LLMs of varying layer sizes with a small batch size of 32 and ~8B tokens from the Refined-\nWeb dataset, and show that they achieve competitive performance in zero-shot and few-shot settings\ncompared to scaled dot-product LLMs of similar model size reported in the literature. We show\nthat deductive outputs of PLDR-LLMs can be used to compare model characteristics or improve\nthe performance by introducing the Directed Acyclic Graph (DAG) loss as a metric and regularizer.\nOur results indicate that the initial maximum learning rate and warm-up steps have a lasting impact\non deductive outputs throughout the pretraining. We provide a detailed description of PLDR-LLM\narchitecture, its implementation and the pretraining procedure.", "sections": [{"title": "1 Introduction", "content": "Generative Pretrained Large Language Models (LLMs) demonstrated breakthrough improvements in performance on\na wide range of Natural Language Processing (NLP) tasks such as common-sense reasoning, question answering,\nreading comprehension, and inference [Radford et al., 2018, 2019, Brown et al., 2020, Touvron et al., 2023a,b]. The\nperformance gains in LLMs are typically achieved by increasing model size and number of tokens for pretraining.\nScaling laws for LLMs are extensively studied to optimize model size, number of tokens and the amount of compute\nrequired [Kaplan et al., 2020, Hoffmann et al., 2022]. The quality of the training dataset also plays an important\nrole in improving the model performance. Filtered and deduplicated web data [Penedo et al., 2023] and curated high\nquality data from books and technical papers [Touvron et al., 2023b, Gao et al., 2020] considerably improves the model\nperformance. For LLMs with sub-billion parameter range, performance was improved with deeper models and weight\nsharing strategies, as smaller models have a large share of parameter size accounted for input and output embedding\nlayers [Liu et al., 2024].\nCurrent state-of-the-art LLMs rely on a model architecture that has deep layers of transformer decoders [Vaswani et al.,\n2017] trained on large amount of text data in an unsupervised fashion. The transformer is a transductive model that\nutilizes scaled dot-product (SDP) attention for its encoder-decoder implementation to capture long range relationships\nwithin a context and avoid a bottleneck condition observed in recurrent neural networks. SDP attention leverages\nthe linear transformations and the LLMs formed on SDP transformer decoder representations are also transductive in\nnature.\nPower Law Graph Attention (PLGA) [Gokden, 2021, 2019] can be used to extend LLM architectures to have well-\ndefined deductive and inductive outputs and utilize non-linear transformations besides linear ones in attending the\ninput. They were first utilized in Power Law Graph Transformer architecture to demonstrate competitive performance\nfor machine translation tasks [Gokden, 2021]. The deductive outputs provide characteristic model parameters to\nobserve the model response during inference as well as to regularize during pretraining to improve performance other\nthan scaling the model size and number of tokens for pretraining. Regularizing deductive outputs also presents a means\nto modify model behavior as compared to solely scaling the size and data for performance as they can directly affect\nthe model weights.\nFor deductive outputs to be interpretable, they need to have a meaningful connection to input samples and retain\nthis connection as much as possible through the design of the model architecture. This is achieved by treating the\ninput sentence as a weighted graph $G = (V, E)$ where tokens are nodes and N-dimensional embeddings form a\nfeature vector for each node. The power law graph attention utilizes both non-linear and linear transformation steps\nto learn characteristics of the feature space as part of model parameters. The attention first learns a metric tensor for\nthe manifold through a deep residual neural network. This metric tensor $A_{LM}$ is a representation for the structure\nof the N-dimensional manifold. The metric tensor can also be interpreted as a weighted adjacency matrix where\nembedding space dimensions are treated as vertices. Power law coefficients $P$ are then learned to define strength and\nrange of the interaction between each embedding dimension. Finally, the interactions of each dimension with all other\ndimensions in the manifold are learned as the energy-curvature tensor $G_{LM}$, which is a superposition of potentials.\nThe final attention $E_{LM}$ is calculated by projecting the query and key vectors on $G_{LM}$. Attention $E_{LM}$ acts on the\ninput as a linear transformation to generate an output in the form of graph G. The metric tensor, power coefficients\n(and the potential tensor, $A_P = A_M$) and energy-curvature tensor form part of the deductive outputs of power law\ngraph attention we will focus in this implementation. This treatment enables us to approach the deductive outputs as\nrepresentations of graphs that shape the manifold.\nIn this paper, we present LLM architectures with deductive-inductive outputs that are designed by using the Power\nLaw Decoder Representations (PLDR) which are based on the decoder layer of Power Law Graph Transformer. We\nmake the following contributions:\n\u2022 A new deductive-inductive large language model architecture, the Large Language Model from Power Law\nDecoder Representations (PLDR-LLM) is proposed and implemented.\n\u2022 PLDR-LLMs are shown to have competitive results compared to reference scaled dot-product LLMs of simi-\nlar model size that are pretrained with larger amount of tokens, larger batch size and longer context length in\nthe literature. We demonstrate that model is highly robust to gradient noise and a global batch size of 32 (16\nbatches per rank) and ~8B tokens are enough to pretrain competitive LLMs with a context length of 1024\ntokens.\n\u2022 We study Directed Acyclic Graph (DAG) loss [Zheng et al., 2018] of deductive outputs as a metric and\nregularizer to observe PLDR-LLM characteristics during training and show that deductive outputs can be\nregularized to approximate a DAG and improve benchmark scores over an unregularized base model.\n\u2022 We introduce a semi-positive activation function we refer to as iSwiGLU which is SwiGLU with identity\nmatrix as weights and no bias.\n\u2022 Implementation of PLDR-LLM architecture and framework is available at:\nhttps://github.com/burcgokden/LLM-from-Power-Law-Decoder-Representations"}, {"title": "2 Approach", "content": "Our training approach is similar to other generative LLMs that implement attention based decoders [Radford et al.,\n2019, Touvron et al., 2023a,b]. We trained the PLDR-LLMs autoregressively in an unsupervised manner while min-\nimizing the cross-entropy loss. In addition to evaluating zero-shot and few-shot benchmarks, we did ablation studies\non the DAG loss of deductive outputs and applied DAG regularization on PLDR-LLMs."}, {"title": "2.1 Model Architecture", "content": "The model architecture is based on Power Law Graph Transformer with following parameters and modifications to\nattention implementation:\n\u2022 The $d_k = d_{model}/h$ parameter is set at 64 for each model, for embedding dimension $d_{model}$ and number of\nattention heads h.\n\u2022 The ReLU activation in the residual neural network for learning metric tensor $A_{LM}$ and feedforward network\nis replaced with SwiGLU activation function [Shazeer, 2020, Dauphin et al., 2017]. For metric learner we\nuse 8 layers of residual units with 2 SwiGLU feedforward networks (FFNs) in each unit. The layer sizes\nfor gated-linear and linear-only layers of a single SwiGLU network were 170:64 ([34dk]:dk) and 300:112\nfor PLDRv5 and PLDRv9 flavors of the model, respectively. For the feedforward network at the end of\neach attention layer, the gated-linear layer size was set as 34dmodel following the same scaling approach in\n[Shazeer, 2020].\n\u2022 The LeakyRELU activation applied after query and key projections onto $G_{LM}$ is removed from the model.\n\u2022 The ReLU activation at the end of metric learner was replaced with iSwiGLU activation function: $x \\sigma(x)$\nwhere $\\sigma()$ is the Swish (SiLU) activation function [Ramachandran et al., 2017, Elfwing et al., 2018].\n\u2022 The positional embeddings are replaced with rotary embeddings [Su et al., 2021] at each layer of attention\nmechanism.\n\u2022 LayerNorm [Ba, 2016] layers were added after the embedding layer and factoring of its output by $\\sqrt{d_{model}}$\nand after the linear self-attention of query vectors that is input to the residual network.\n\u2022 Drop-out was not used at any layer."}, {"title": "2.2 Directed Acyclic Graph (DAG) Regularization", "content": "We add mean absolute value of DAG loss of deductive outputs $A_{LM}, A_P, G_{LM}$ from all attention heads to the cross-\nentropy loss for pretraining of several PLDR-LLMs. This loss condition aims to reduce the number of cyclic paths of\nany length to zero for deductive outputs and acts as a regularizer. We also observe DAG loss as a metric to compare\nmodels without regularization. DAG loss was first introduced in [Zheng et al., 2018] for causal modeling of graphs\nwith a smooth and differentiable loss function.\nA square matrix $M \\in R^{d \\times d}$ is a Directed Acyclic Graph if it satisfies the condition:\n$tr(e^{MM}) \u2013 d = 0$  (1)\nThe the DAG regularizer $DLR(A_{LM}, A_P, G_{LM})$ added to the PLDR-LLM cross-entropy loss is defined as:\n$\\begin{aligned}\nD_L(M) &= \\frac{1}{B,L,h} \\log \\frac{tr (M^OM)}{d_k}  (2)\\\\\nDLR(A_{LM}, A_P, G_{LM}) &= \\lambda_{DAG1}D_L(A_{LM}) + \\lambda_{DAG2}D_L(A_P) + \\lambda_{DAG3}D_L(G_{LM})  (3)\n\\end{aligned}$\nwhere M is a tensor and B, L are the batch size and number of decoder layers. $\\lambda_{DAG1}, \\lambda_{DAG2}$ and $\\lambda_{DAG3}$ are\ncoefficients that determine the strength of regularization."}, {"title": "3 Dataset", "content": "We used the first ~8B tokens from the RefinedWeb dataset for pre-training. RefinedWeb is a publicly available, high\nquality web based English dataset with extensive deduplication and filtering [Penedo et al., 2023]. The tokenizer we\nused was SentencePiece unigram tokenizer [Kudo and Richardson, 2018, Kudo, 2018] with vocabulary size of 32000.\nWe trained the tokenizer model by randomly sampling from a portion of RefinedWeb dataset. The tokenizer was set\nto split all digits into single units and to fallback to bytes to decompose unknown UTF-8 characters following similar\napproach in [Touvron et al., 2023a]. The tokenizer allows padding and only an \"[END]\" token was added at the end\nof sentence during tokenization.\nEach sample in the dataset was first tokenized and formed into larger batches which were then concatenated. The\nconcatenated samples were chunked into contiguous sets of 1024 tokens and batched with final batch size of 16\nper rank. Occasionally appearing chunks with tokens less than 1024 are padded with \"[PAD]\" token. The model\nimplementation was designed to ignore padding during pretraining and evaluation of metric values."}, {"title": "4 Experiments", "content": "We evaluated PLDR-LLMs with varying size of model parameters from 104M to 260M by changing the layer size,\nnumber of heads and residual unit SwiGLU FFN layer sizes (Table 1). Each PLDR-LLM was pretrained with first ~8B\ntokens of RefinedWeb dataset in the same order and on a single epoch. For ablation studies, we used the PLDRv5-2\nas a base model. We pretrained the base model with low learning rate, longer warm-up steps and a different tokenizer\nmodel trained with same parameters on the RefinedWeb dataset. The DAG regularization was applied on a 5-layer,\n14-head PLDR-LLM on the deductive inputs and compared to a base model (PLDRv5-2) with same hyperparameters\nwithout any regularization. DAG regularization strength was skewed from 0.001 to 1 for $\\lambda_{DAG1}$ and from 0.005 to\n0.05 for $\\lambda_{DAG2}$ and $\\lambda_{DAG3}$ on PLDRv5-DAG models (Table 2).\nWe observed the training loss/accuracy and DAG loss of deductive outputs and evaluated the PLDR-LLMs on a range\nof tasks for commonsense reasoning, question answering and language understanding. The models were evaluated\nwith tinyBenchmarks version of datasets [Maia Polo et al., 2024", "2024": "for zero-shot and few-shot performance. The PLDR-LLM implementation used in our experiments\nis not optimized for fast inference, and tinyBenchmarks datasets provide a quick way to evaluate model for few-"}]}