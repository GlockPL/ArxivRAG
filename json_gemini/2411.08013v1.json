{"title": "Investigating the Effectiveness of Explainability Methods in Parkinson's Detection from Speech", "authors": ["Eleonora Mancini", "Francesco Paissan", "Paolo Torroni", "Mirco Ravanelli", "Cem Subakan"], "abstract": "Speech impairments in Parkinson's disease (PD) provide significant early indicators for diagnosis. While models for speech-based PD detection have shown strong performance, their interpretability remains underexplored. This study systematically evaluates several explainability methods to identify PD-specific speech features, aiming to support the development of accurate, interpretable models for clinical decision-making in PD diagnosis and monitoring. Our methodology involves (i) obtaining attributions and saliency maps using mainstream interpretability techniques, (ii) quantitatively evaluating the faithfulness of these maps and their combinations obtained via union and intersection through a range of established metrics, and (iii) assessing the information conveyed by the saliency maps for PD detection from an auxiliary classifier. Our results reveal that, while explanations are aligned with the classifier, they often fail to provide valuable information for domain experts.", "sections": [{"title": "I. INTRODUCTION", "content": "Parkinson's disease (PD) is a progressive neurodegenerative disorder primarily marked by the deterioration of dopamin-ergic neurons in the midbrain. This degeneration leads to a range of motor and non-motor symptoms, including tremors, bradykinesia, cognitive impairment, and depression [1]-[3]. Importantly, during the prodromal stages of PD, patients often start to exhibit speech impairments, which can serve as early indicators of the disease [4], [5]. Given the non-invasive, cost-effective, and automated nature of speech analysis, researchers have increasingly focused on this approach as a promising avenue for the early detection of Parkinson's disease [6].\nDespite substantial advances in PD classification using speech analysis, model interpretability remains underexplored. In clinical settings, explainable AI (XAI) is essential for providing clear, clinically relevant insights, crucial for the acceptance of automated systems in clinical trials. Many XAI techniques exist for interpreting model predictions, with post-hoc explanation methods among the most widely used [7]. Here, we focus on two different sets of approaches: Perturbation-based and Gradient-based post-hoc explanation methods. Perturbation-based methods assess feature impor-tance by modifying input data, while gradient-based methods use gradients of predictions with respect to inputs [8]. Both approaches support local and global explanations, are model-agnostic, and offer valuable insights into the model's behaviour. This paper systematically evaluates several key perturbation and gradient-based techniques to determine their effectiveness in highlighting PD-relevant speech features, aim-ing to enhance transparency and clinical utility in PD detection from speech.\nOur experimental results show that, although explanations are aligned with the classifier, they often fail to provide insights that are truly informative for domain experts. These methods may lack the level of interpretability required for practical use, emphasizing the need for more effective explain-ability approaches that connect model behavior with human understanding in specialized domains."}, {"title": "II. RELATED WORK", "content": "Various studies have explored automated techniques for identifying speech impairments and using them to predict the progression of Parkinson's disease. For example, researchers have applied Convolutional Neural Networks (CNNs) on spec-trograms of patients' speech to detect dysarthria, assess its severity and distinguish between PD patients and healthy controls [9], [10]. Some approaches use a combination of one-dimensional and two-dimensional CNNs to capture temporal and frequency information [11], [12]. However, some studies suggest that while these biomarkers are effective, models employing non-interpretable features often outperform those built on more interpretable characteristics [13].\nUnfortunately, much of the existing research prioritizes performance metrics over interpretability. As a result, although interpretable speech-based biomarkers have been shown to be useful for Parkinson's diagnosis [14], little effort has been devoted to comprehensive analysis of explainability in PD de-tection models. Among the most commonly used XAI methods are perturbation and saliency-based approaches. Some studies employed methods such as GradCAM and EigenCAM to visu-alize important features. However, these works do not include rigorous quantitative validation of their interpretability. Other approaches, including those using SHAP [15], face challenges in explainability due to the complex input features such as Mel-frequency cepstral coefficients (MFCCs)\u2014that do not directly correlate with auditory concepts relevant to PD.\nIn contrast, there has been increasing interest in inter-pretability for audio and speech applications in recent years. Key works have introduced methods like layer-wise rele-vance propagation [16], masked additive white noise [17], [18], and Guided Backpropagation [19] to understand im-portant spectrogram features. Additionally, SLIME [20], [21], and AudioLIME [22], [23] have explored feature importance within predefined regions of the spectrogram. Recent advance-ments, such as Listen-to-Interpret (L2I) [24], L-MAC [25], and LMAC-TD [26], have focused on generating listenable explanations in spectrogram and time domains, underscoring the value of interpretability in audio analysis.\nIn response to the limitations highlighted in PD detection, our work evaluates the effectiveness of XAI methods in the aforementioned context in a systematic way, particularly of perturbation and saliency-based approaches. By comparing various techniques, we aim to provide insights into their effectiveness and limitations, highlighting the potential for explainable AI in clinical applications for Parkinson's disease."}, {"title": "III. METHODOLOGY", "content": "Our methodology to compare and evaluate XAI methods consists of (i) obtaining attributions and saliency maps using mainstream interpretability techniques (Section III-A), (ii) evaluating the explanatory power of such saliency maps (and their combination) quantitatively via a range of metrics defined in the literature (Sections III-B and III-C), and (iii) evaluate the information conveyed by saliency maps for the PD detection task from an auxiliary classifier (Section III-D).\nMany SSL-based pre-trained models for audio operate di-rectly on the raw waveform. This is the case also for our PD detection model, HuBERT [27]. In this case, explaining the model predictions in the classifier's input domain (i.e. on the waveform Xw) results in explanations that are hard to interpret visually. We transform the waveforms into a time-frequency representation Xf using the short-time Fourier transform (STFT). This transformation allows us to compute attributions (A) on the spectrogram of the input audio via saliency-based interpretability techniques. Before inputting the audio to the model, we convert the spectrogram back to the time domain using the inverse short-time Fourier transform (ISTFT) and the phase information of the original sample Xw. We apply the following interpretability techniques to generate these maps:\nSaliency [28], SmoothGrad [29], Integrated Gradients (IG) [30], Guided GradCAM [31], Guided Backpropagation [32], and Guided SHAP [33]."}, {"title": "B. Quantitative Analysis through Explainability Metrics", "content": "To quantitatively evaluate the quality of the saliency maps, we employ several explainability metrics. Specifically, we adopt metrics previously used in the L-MAC [25], LMAC-ZS [34] and LMAC-TD [26] studies. We use Average Increase (AI), which measures the percentage of samples for which we observe an increase in the classifier's confidence for the interpretation with respect to the input sample, and Average Decrease (AD), which measures the confidence drop when masking the input with the mask, and Average Gain (AG), similar to AI. Beyond these metrics, we use the Faithfulness (FF) metric defined in the L2I paper [24] and the input fidelity (Fid-In) metric defined in the PIQ paper [35]. We also use the Sparseness (SPS) [36] and Complexity (COMP) [37] metrics to evaluate the conciseness of the explanations. We invite the reader to refer to the cited papers for further details."}, {"title": "C. Exploring Overlap in Explainability Methods", "content": "A reasonable expectation when explaining a classifier is that good explanations are aligned with the classifier's predictions, regardless of the process that generated them. To verify this hypothesis, we investigate the overlap among different attri-bution methods to assess whether combining them provides further insights into the model's decisive process. Given a dataset element (Xf, y), we extract two saliency maps A1, A2 using two different explanation techniques (e.g. Saliency and Smoothgrad) and combine them via intersection and union. An increase in explainability metrics through overlapping attributions would suggest that combining methods captures more comprehensive feature representations."}, {"title": "D. Classification Using Saliency Maps and Selective Metrics", "content": "Finally, to examine whether the saliency maps derived from each attribution method highlight information pertinent to distinguishing between PD and healthy controls (HC), we train a classifier on the generated explanations. This test is similar to RemOve And Retrain [38]; in our case, however, we measure the amount of information in the explanations in a single iteration. Additionally, we introduce a new set of metrics, called Selective Metrics, that scales the standard classification metrics depending on the explanation selective-ness. These metrics penalize explanations replicating the input audio without selectively highlighting the portions of the input relevant to the classification.\nTo define selective metrics, we combine the classification performance (e.g. accuracy) and the average of the attribution mask. Numerically, for a dataset $D = \\{(X_f,y)_i\\}_{i=1}^N$ of $N$ elements and metric $M(X_f, y)$ (i.e. accuracy, where $X_f$ is the STFT domain input audio), this computes as:\n$SM(D) = \\frac{1}{N} \\sum_{(X_f,y) \\in D} M (X_f, y) (1 - Av(A))$ \nwhere $Av(\\cdot)$ denotes the mean that is calculated on the normalized attributions A (which is in [0, 1]). This adjustment rewards classifiers that achieve high accuracy while focusing on smaller, relevant input parts. This is particularly relevant to facilitate a comparison between attribution strategies."}, {"title": "IV. EXPERIMENTS", "content": "Standard PC-GITA (s-PC-GITA) is a dataset consisting of recordings from 100 individuals, split evenly between two groups: 50 people with Parkinson's disease (PD) and 50 healthy controls (HC). Each group includes 25 men and 25 women, with the PD group diagnosed by a neurologist, while the HC group shows no signs of PD or other neurodegenerative conditions. Participants range in age from 31 to 86 years, with recordings captured in a sound-proof booth at Cl\u00ednica Noel in Medell\u00edn, Colombia. The original recordings were sampled at 44.1 kHz with 16-bit resolution and downsampled to 16 kHz for this study, as in [39] and [6]. We use the same splits as those employed by [6] for inference on the test sets of the 10 folds and present the results averaged across these folds; our replicated results, which align with the original paper, are shown in Table II. As in [6], the speech tasks from both datasets considered in this work are diadochokinetic (DDK) exercises, read sentences, and monologues. Additional dataset details are available in [40]."}, {"title": "B. PD Detector", "content": "In this study, we build upon recent advancements in the field by relying on a study that explores exploiting foundation models and speech enhancement for Parkinson's disease detection from speech in real-world operative conditions [6]. In [6], they use SSL models HuBERT [27] and WavLM [41] as foundational backbones for PD detection, extracting high-level representations from raw waveform inputs through a convolutional and transformer-based encoder. By leveraging pre-trained layers optimized through dynamic weighted summation and an attention pooling head, the model captures discriminative features essential for PD detection, refining these representations with fully connected layers to enhance task-specific accuracy. Following this work, we replicate their results using only the s-PC-GITA datasets and apply explain-ability techniques to the model."}, {"title": "C. Saliency Maps Classifier", "content": "We employ a CNN14 classifier [42] to evaluate saliency maps, taking log-mel spectrograms as input. This classifier is trained for a binary classification task with the following hyperparameters: batch size of 32, learning rate of 0.002, and 50 training epochs.\nTo train and evaluate the classifier, we construct a dataset from the masks computed on the original test set. For each fold, this dataset is split into training, validation, and test sets with a ratio of 70/15/15, ensuring that the label distribution is stratified to maintain balanced classes across the splits. For comparison, we also train the CNN14 classifier on the original spectrograms from the test set to evaluate performance differ-ences between saliency map-based inputs and the unmodified spectrograms. The CNN14 classifier training is done using the SpeechBrain 1.0 toolkit [43]."}, {"title": "V. RESULTS", "content": "As shown in Table II, both the HuBERT Base and WavLM Base models obtain comparable results on PD detection. For this reason, we conduct our experiments on HuBERT in this paper. We note that the same experimental protocol and methodology can be applied to interpreting WavLM. Our code is publicly available and can be accessed through our companion website\u00b9, together with additional spectrogram visualizations."}, {"title": "A. Faithfulness Metrics", "content": "In Table I, we present the quantitative evaluation results of the HuBERT-base model averaged over 10 folds on s-PC-GITA. We observe that all the XAI approaches show compara-ble performance overall. The variability in the metrics can be attributed to the high sensitivity of classifier representations to individual subjects, as evidenced by the high standard deviations in Table II. This subject-specific variability leads to fluctuations in faithfulness metrics across folds comparable to those observed on the classification performance. For AI, AD, AG, and FF, Integrated Gradients performs best, while for Fid-In, SPS, and COMP, Guided Grad-CAM stands out. This also aligns with the results presented in Table III, in which Guided Grad-CAM shows the lowest mask mean, aligning with its superior performance in SPS and COMP. We note that the small FF values is due to classifier's uncertainty, since we observed small values in the predicted logit outputs. Overall, we see that the explanations are aligned with the classifier, suggesting that masking the audio spectrogram can be an effective way of highlighting the regions of the input audio associated with the predicted label."}, {"title": "B. Overlap between Explainability Methods", "content": "Fig. 2 shows a scatter plot illustrating how faithfulness metrics vary with increasing intersection-over-union (IoU) be-tween explanations from different methods, specifically using intersection as the combination strategy. For the results of the overlap based on the union strategy, we do not observe a linear trend with increasing IoU values. However, we observe that for some metrics (e.g. AD), union is a more effective strategy on average. We focused on two metrics, AD and FF, which showed the most variation across methods. A similar trend is observed on other metrics; we invite the reader to refer to the companion website for the results. Overall, the trend suggests that combining attribution strategies is most effective when the attributions are already well-aligned, supporting our hypothesis that greater mask overlap improves faithfulness metrics."}, {"title": "C. Saliency Maps Classifier", "content": "In Table III, we report the classification accuracy and F1-score, together with the corresponding selective metrics. In most cases, training the classifier on the explanations results in higher classification performance with respect to training on the original data. This suggests that training on the ex-planations provides better generalization capabilities, possibly because by incorporating explanations, the classifier gains reduced context dependency, thus enhancing its generalization capabilities. Among the XAI methods, Integrated Gradients stands out, excelling in faithfulness metrics and achieving the highest selective accuracy, confirming its overall effectiveness compared to other strategies."}, {"title": "D. Qualitative Analysis", "content": "Quantitative results show that explanations align with the classifier outputs and provide helpful information for distin-guishing between PD and HC. In Figure 1, we present a sam-ple explanation generated with Guided GradCAM, Integrated Gradients and their combination. In general, we observed that the saliency maps focus on high-frequency regions, potentially reflecting attention to specific phonemes. Nonetheless, these maps are not easily interpretable by humans. We conclude that despite current XAI techniques provide faithful explanations as spectrograms, further research is needed to render expla-nations more insightful for domain experts. Relevant works in this direction include [44], which relies on additional data modalities."}, {"title": "VI. CONCLUSION", "content": "In this paper, we show that popular post-hoc explanation methods can generate faithful explanations for PD detection. Nonetheless, they fail to generate explanations that domain ex-perts can easily understand. We, therefore, suggest that future work should explore approaches that would simplify an input spectrogram such as semantically enriching the spectrogram through phoneme discretization of the spectrogram, creating a direct link between speech biomarkers in PD research and the saliency maps, or investigating methods like listenable explanations (similar to what is proposed in L-MAC) for more intuitive insights."}]}