{"title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent", "authors": ["Tencent Hunyuan Team"], "abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest open-\nsource Transformer-based mixture of experts model, with a total of 389 billion\nparameters and 52 billion activation parameters, capable of handling up to 256K to-\nkens. We conduct a thorough evaluation of Hunyuan-Large's superior performance\nacross various benchmarks including language understanding and generation, logi-\ncal reasoning, mathematical problem-solving, coding, long-context, and aggregated\ntasks, where it outperforms LLama3.1-70B and exhibits comparable performance\nwhen compared to the significantly larger LLama3.1-405B model. Key practice\nof Hunyuan-Large include large-scale synthetic data that is orders larger than in\nprevious literature, a mixed expert routing strategy, a key-value cache compres-\nsion technique, and an expert-specific learning rate strategy. Additionally, we\nalso investigate the scaling laws and learning rate schedule of mixture of experts\nmodels, providing valuable insights and guidances for future model development\nand optimization. The code and checkpoints of Hunyuan-Large are released to\nfacilitate future innovations and applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large language models (LLMs) have significantly advanced the field of artificial\nintelligence, proving their effectiveness across numerous fields such as NLP, CV, Speech, and\nAI4Science. Starting from the emergence of ChatGPT (OpenAI, 2022), lots of powerful LLMs have\nbloomed (Achiam et al., 2023; Gemini et al., 2023; Touvron et al., 2023; OpenAI, 2024; Dubey\net al., 2024; Qwen, 2024a), which inexorably bring in new ways for people to collect and process\ninformation, broadly impacting our daily lives. As the demand for more sophisticated AI systems\ncontinues to grow, researchers are exploring new techniques and paradigms to push the boundaries\nof model size and performance. One approach that stands out is the Mixture of Experts (MoE)\nmodel, which synergizes multiple specialized submodels to deliver superior performance in diverse\ntasks with dynamic activated experts (Lepikhin et al., 2020; Fedus et al., 2022; Wang et al., 2024a),\nachieving more efficient training and inference. There is a current trend observed that more and more\nMoE-structured LLMs have been constructed and open-sourced to facilitate the LLM community\n(Mistral, 2024; DeepSeek-AI, 2024; Yang et al., 2024; Jamba et al., 2024).\nTencent's AI chatbot, Yuanbao (yuanbao.tencent.com), has also adopted MoE as the neural architec-\nture of the trillion-parameter flagship LLM since February 2024. Due to its exceptional capabilities\nin reading, writing, and searching, the MoE-based Hunyuan model and Yuanbao chatbot are assisting\nusers in working effortlessly and enjoying a more vibrant life. The MoE-powered Hunyuan models\nhave also enhanced thousands of scenarios within Tencent's applications, enabling Tencent to better\nserve its billions of users.\nIn addition to serving users with the premium models, another way that contributes to the community is\nopen-sourcing. Open-source models can greatly promote the spreading of technology and flourishing"}, {"title": "2 Pre-Training", "content": "In this section, we will describe the details of pre-training Hunyuan-Large, including (a) data and\ntokenizer, where high-quality data largely contributes to the model performance, (b) model structure,\nconsisting of our proposed KV cache compression, expert routing, and expert-specific learning rate\nscaling strategies, and (c) pre-training recipes, introducing the detailed pre-training schedule as well\nas our guidebook of explorations on MoE scaling laws. These techniques build the foundation of\nHunyuan-Large's remarkable capability in pre-training."}, {"title": "2.1 Data and Tokenizer", "content": "We first give the overview of our data, which is viewed as the fuel of our powerful model, with its\npreprocessing steps and data synthesis strategies essential for the quantity and quality of data. We\nalso introduce the tokenizer employed for converting text data into an appropriate format suitable for\nHunyuan-Large."}, {"title": "2.1.1 Data Processing and Synthesis", "content": "To start with, we provide a brief overview of the used pre-training data, and then delve deeper into\nthe specifics of our synthetic data generation process, which is essential for acquiring capabilities\nalso verified in various LLMs (Dubey et al., 2024; Abdin et al., 2024; Liu et al., 2024).\nData Overview and Processing. We aim to create a high-quality, safe, and diverse training dataset\nfor pre-training, primarily consisting of Chinese and English languages for practical demands. We\nfilter the data based on criteria such as writing quality, educational value, and toxicity to ensure\nits high quality. Additionally, we anonymize all privacy-sensitive data and other harmful data. We\nhave also implemented an elaborate system of category labels, which allows us to flexibly adjust the\nproportions of various types of data in the training dataset.\nData Synthesis. Besides the existing natural text corpus, we construct large amounts of synthetic\ndata to specifically boost the knowledge acquisition against the relative capability deficiency merely\nlearned from natural data. To make full use of synthetic data to enhance model performance, we\nmainly focus on the mathematics, coding, low-resource, and high-educational-value fields that are\ngood supplements to naturally distributed corpus, meeting three key requirements of quality, diversity,\nand quantity.\nAs shown in Figure 1, we synthesize high-quality instruction data through a four-step process,\nincluding instruction generation, instruction evolution, response generation, and response filtering."}, {"title": "2.1.2 Tokenizer", "content": "The tokenizer is a vital component for effectiveness and efficiency in pre-training, which should\nbalance two critical factors: (a) achieving a high compression rate for efficient training and inference,\nand (b) maintaining an appropriately large vocabulary to ensure adequate learning of each word em-\nbedding. In Hunyuan-Large, we carefully consider both aspects and employ a vocabulary consisting\nof 128K tokens. This token vocabulary is a combination of 100K tokens from the tiktoken tokenizer\n(OpenAI, 2023) and an additional 28K tokens specifically designed to enhance Chinese language\nsupport. Notably, when compared to the LLama3.1 tokenizer, our new tokenizer exhibits improved\ncompression rates, increasing from 2.78 to 3.13 characters per token."}, {"title": "2.2 Model Structure", "content": "Hunyuan-Large is equipped with superior model structure and training strategies to achieve impressive\nLLM capabilities. We first show the overview of model architecture and hyper-parameters, and then\ndelve into the KV cache compression, expert routing strategy, and expert-specific learning rate scaling\nused in our model with details."}, {"title": "2.2.1 Overview of Hunyuan-Large", "content": "The model structure of Hunyuan-Large mainly follows the classical MoE structure that uses multiple\nexperts to replace the original FFN in Transformer. Tokens will be assigned to different experts, and\nonly a small ratio of experts will be activated in training. Hunyuan-Large consists of both shared\nand specialized experts. We use Rotary Position Embedding (RoPE) for position learning (Su et al.,\n2024) and SwiGLU for activation (Shazeer, 2020)."}, {"title": "2.2.2 KV Cache Compression", "content": "To alleviate memory pressure of KV cache and reduce the cost during inference, we jointly integrate\ntwo classical strategies for KV cache compression: (a) Grouped-Query Attention (GQA) (Ainslie\net al., 2023), which uses an intermediate number of KV heads to form head groups, compressing\nKV cache from the head aspect, and (b) Cross-Layer Attention (CLA) (Brandon et al., 2024), which\nshares the KV cache between adjacent layers, compressing KV cache from the layer aspect. In\nHunyuan-Large, we set 8 groups of KV heads for GQA, and share KV cache every 2 layers, jointly\nconsidering both effectiveness and efficiency."}, {"title": "2.2.3 Expert Routing Strategy", "content": "Shared and Specialized Experts. The expert routing strategy of MoE is essential to efficiently\nactivate each expert's capability while maintaining a relatively balanced load. Conventional routing\nstrategies, such as the classical top-k routing strategy, selects the top-k scoring experts to process\neach token (Jiang et al., 2024; Qwen, 2024b). Hunyuan-Large adopts a mixed routing strategy, that\nuses both a shared expert consumed by all tokens and several routable experts employing the classical\ntop-k routing strategy \u00b9. Hunyuan-Large sets 1 expert as the shared expert to capture the common\nknowledge required by all tokens. Besides, 16 specialized experts are allocated to dynamically\nlearn domain-specific knowledge, activating the top-1 scoring specialized expert for each token.\nRecycle Routing. Conventional top-k routing often cooperates with a capacity factor that defines the\nmaximum load of an expert in MoE, where tokens of overloaded experts are discarded during training.\nA larger capacity factor results in less dropped tokens but reduced training efficiency. Excessive\ntoken dropping may cause the loss of crucial information, which in turn negatively impacts training\nstability. To address this problem and achieve more balanced training in efficiency and stability, we\ndevelop a new recycle routing strategy for tokens discarded during the original top-k routing process,\nas displayed in Figure 2. This technique entails an additional random allocation for tokens originally\nrouted to overloaded experts to other specialized experts which have not exceeded their capacity. This\napproach strives to preserve vital information while simultaneously optimizing training efficiency,\nthus ensuring the overall effectiveness and efficiency of model training."}, {"title": "2.2.4 Expert-Specific Learning Rate Scaling", "content": "We adopt AdamW (Loshchilov & Hutter, 2019) as our optimizer. To expedite training, we can\nincrease the learning rate in tandem with the growth of batch size in pre-training. Previous work\nhas explored the square root scaling (Krizhevsky, 2014) or linear scaling (Goyal et al., 2017) when\ndiscovering the optimal learning rate based on the batch size for SGD-style optimizers. Recent work\nhas elucidated a more appropriate connection between the optimal learning rates and batch sizes for\nAdam-style optimizers in LLMs. According to Li et al. (2024a), the optimal learning rate \\( \\epsilon_{opt}(B) \\) for\na batch size B is calculated as:\n\n\\( \\epsilon_{opt}(B) = \\sqrt{\\frac{2 \\epsilon_{max}}{B_{noise} + B}} \\)\n\nHere, \\( \\epsilon_{max} \\) represents the learning rate of AdamW. \\( B_{noise} \\) indicates the trade-off point between\ntraining speed and data efficiency noted in Kaplan et al. (2020).\nHowever, in Hunyuan-Large, different experts are imbalanced in the aspect of trained tokens (e.g.,\ncomparing the shared expert with other specialized experts). The number of tokens processed by"}, {"title": "2.3 Pre-Training Recipes", "content": "The effectiveness of LLM pre-training is not solely determined by the dataset and model structure,\nbut also significantly ascribed to the pre-training recipes obtained from empirical experiments. We\nfirst explore the scaling laws of MoE functioned as a guidebook for our model design. Secondly,\nwe introduce the detailed process of annealing and long-context pre-training, which further enhance\nLLM's capability."}, {"title": "2.3.1 MoE Scaling Law", "content": "Initially, we investigate the scaling laws of MoE models to identify optimal settings and gain insights\nbefore pre-training. Typically, the training compute budget for dense models is estimated using\n\\( C = 6ND \\), where N represents the number of parameters and D denotes the training tokens.\nHowever, for MoE models with longer sequences (e.g., 8K, 32K, and 256K), the compute budget\nformula varies due to attention complexity and sparse activation. Upon meticulous computation, we\nascertain the precise compute budget C for MoE models, where N in our formula represents the\nnumber of activated parameters, is as follows:\n\n\\( C \\approx 9.59ND + 2.3 \\times 10^8D. \\)\n\nDrawing on the insights of Kaplan et al. (2020) and Li et al. (2024a), we acknowledge that batch size\nB has a significant impact on compute budget C during training. To isolate this effect and derive\nprecise estimates, we employ the critical batch size \\( B_{crit}(L) \\), which optimizes the trade-off between\ntime and computational efficiency, ultimately resulting in minimal compute budget \\( C_{min} \\):"}, {"title": "2.3.2 Learning Rate Scheduling", "content": "An optimal learning rate schedule is crucial for effective and stable training. Hunyuan-Large's\nlearning rate schedule is delineated into three sequential phases: an initial warmup phase, succeeded\nby a prolonged phase of gradual decay, and culminating in a concise annealing phase.\nThe merit of the extended phase of gradual decay is its adeptness at balancing the exploration of the\nsolution space with the convergence toward an optimal solution. By sustaining an elevated learning\nrate during the initial pre-training phase, the model is enabled to efficaciously navigate through\ndiverse regions of the solution space, thereby averting premature convergence to suboptimal local\nminima. The incremental reduction in the learning rate as training progresses ensures a methodical\nconvergence to a more optimal solution.\nIn the concluding 5% of the pre-training tokens, we introduce a brief annealing phase, wherein\nthe learning rate is reduced to one-tenth of its peak value. This approach facilitates the model in\nmeticulously fine-tuning its parameters, thereby achieving a superior degree of generalization and,"}, {"title": "2.3.3 Long-Context Pre-Training", "content": "After the annealing phase, Hunyuan-Large is trained on longer sequences (up to 256K tokens) to\nenable its long-context capability. Specifically, the long-context pre-training phase contains two\nstages (i.e., gradually increases the token length as 32K\u2192256K). We adopt ROPE (Su et al., 2024)\nfor building position embeddings, and scale the RoPE base frequency to 1 billion during the 256K\npre-training stage inspired by Xiong et al. (2023).\nFor the data, we solely rely on natural long-context data obtained from books and codes (comprising\nnearly 25% of the corpus) and mix it with normal-length pre-training data (nearly 75%) to form our\nlong-context pre-training corpus, sharing similar conclusions observed in Gao et al. (2024). We also\ndiscover that it does not require too much training for LLM to acquire long-context capabilities. In\neach of the 32K and 256K stages, we employ a long-context pre-training corpus of approximately\n10 billion tokens. The long-context pre-training at each stage can achieve satisfactory long-context\nabilities, while maintaining good LLM capabilities on tasks with normal lengths."}, {"title": "3 Post-Training", "content": "Based on the pre-trained model of Hunyuan-Large, we further conduct a post-training stage that\naims to enhance task-specific capabilities and align LLM to human preference. This stage contains a\nsupervised fine-tuning (SFT) phase and a Reinforcement Learning from Human Feedback (RLHF)\nphase on elaborately selected datasets and outputs of current policy models (Bai et al., 2022). The\nfollowing subsections contain (a) the data selection, preprocessing, and training process of SFT, (b)\nthe techniques and training strategies of Direct Preference Optimization (DPO) in RLHF."}, {"title": "3.1 Supervised Fine-Tuning", "content": "The performance of SFT strongly depends on the quality of instruction data related to various types of\nLLM capabilities. In SFT, we concentrate on the detailed data collection and processing manners that\nensure the effectiveness of Hunyuan-Large's post-training, along with the training settings of SFT."}, {"title": "3.1.1 Overview of SFT Data", "content": "The central goal of SFT is further enhancing its performance across multiple key capabilities based\non the corresponding well-selected data. These capabilities primarily encompass mathematics,\ncoding, logical reasoning, knowledge-based question answering, agent behavior, text generation,\nNLP comprehension, industrial applications, role-playing, long-text capabilities, etc. We recognize\nthat improving these abilities not only enables the model to be more adept in practical applications,\nbut also better satisfies users' diverse needs across multiple scenarios. Simultaneously, we place great\nemphasis on data security, striving to ensure that the model aligns with human values under most\ncircumstances. The overall SFT data volume exceeds 1 million."}, {"title": "3.1.2 Data Collection and Processing", "content": "The key techniques of SFT data collection and processing mainly include instruction extraction,\ninstruction generalization, instruction balancing, and data quality controlling.\nInstruction Extraction. To enhance the breadth and diversity of the instruction set, we develop an\ninstruction extraction model specifically for domains such as mathematics, logical reasoning, and\nknowledge-based question answering, whose primary goal is to effectively extract data suitable for\ninstruction tuning from publicly available data sources (e.g., web pages, encyclopedias, etc.). The\nextracted data includes both instructions and corresponding reference answers. We develop many\nspecialized models as instruction extractors. With the help of these model, we successfully extract a\nlarge set of natural instructions from public data. These instructions play a crucial role as the seed to\nenhance the final model's generalization performance and diversity.\nInstruction Generalization. We propose an instruction generalization method to obtain more\ndiverse and complex instructions in large quantities. Specifically, we design and train an instruction\ngeneralization system capable of generalizing targeted instructions while gradually increasing their\ndifficulty and complexity levels. The central recipe of this system lies in training the model by\nsynthesizing numerous mappings between simple and complex instructions. In addition, we construct\na well-structured instruction taxonomy with its corresponding classification models, which aims\nto analyze and balance the distribution of various instruction types in SFT data. Armed with this\ninstruction taxonomy, our instruction generalization system can supplement the original data on\nspecific weak instructions of targeted types.\nInstruction Balancing. Through the instruction extraction and generalization processes, we accu-\nmulate more than 10 million instructions. Instruction balance is essential for enhancing the model's\nperformance across various scenarios. However, many generated instructions have very similar\nsemantic meanings and the instruction type distribution is naturally unbalanced. To enhance the\ninstruction complexity while maintaining balanced instruction distributions, we attach labels for\neach instruction. These labels encompass multiple dimensions. By meticulously tagging these\nlabels, we can more accurately understand and analyze the characteristics of our instruction sets. By\nensuring adequate amounts and balanced distribution of different types of instructions during the SFT\nprocess, we can effectively alleviate overfitting or underfitting problems on specific instruction types,\nthereby improving the model's generalization capabilities and adaptability across diverse application\nscenarios.\nData Quality Controlling. The quality of SFT data is the foundation of superior performance. We\nmainly conduct the following three methods to ensure the high quality of our SFT data."}, {"title": "3.1.3 Training Details", "content": "In SFT, we fine-tune the pre-trained model based on the high-quality data (more than 1 million) for a\ntotal of 3 epochs. The learning rate decays from 2e-5 to 2e-6. To mitigate overfitting during SFT, we\nutilize an attention dropout of 0.1 and a hidden dropout of 0.2. We find that, compared to the dense\nmodels, the MoE architecture of Hunyuan series could benefit more from incorporating suitable\ndropout rates."}, {"title": "3.2 Reinforcement Learning from Human Feedback", "content": "To align Hunyuan-Large with human preferences, we further train our SFT model using DPO (Rafailov\net al., 2024). We adopt a single-stage training strategy that integrates both offline and online training,\nwhich demonstrates superior controllability and overall performance. In this integrated approach, we\nutilize a pre-compiled preference dataset to enhance controllability, while simultaneously employing\nthe current policy model to generate multiple responses for each prompt and our reward model to\nselect the most and least preferred responses.\nTo enhance training stability, we incorporate an SFT loss term on the chosen response, similar to the\napproaches in (Dubey et al., 2024; Adler et al., 2024). This addition helps stabilize DPO training\nby preventing a decrease in the log probability of chosen responses. Furthermore, we implement an\nexponential moving average strategy to mitigate reward hacking and reduce alignment tax (Ouyang\net al., 2022), ensuring a more stable training process across a larger dataset."}, {"title": "4 Model Evaluations", "content": "We conduct extensive evaluations of Hunyuan-Large to demonstrate its effectiveness. The following\nexperiments concentrate on our pre-trained language model (in Sec. 4.1) and post-trained language\nmodel (in Sec. 4.2) on various tasks in Chinese and English, including math and reasoning, code,\nreading comprehension, commonsense, long context, and aggregated task, etc., where Hunyuan-Large\nachieves excellent performance among tasks in both pre-training and post-training."}, {"title": "4.1 Evaluations on Pre-Trained Model", "content": "In this section, we report the performance of Hunyuan-Large's pre-trained model on various types of\nwidely-used benchmarks, verifying the power of the fundamental capability of our model."}, {"title": "4.1.1 Benchmarks and Experimental Settings", "content": "Key Benchmarks. We evaluate Hunyuan-Large on a large number of widely-used benchmarks of\nvarious tasks, including commonsense understanding, machine comprehension, question answering,\nmath and reasoning, coding, and aggregated tasks, in both English and Chinese. Specifically, we\nchoose MMLU (Hendrycks et al., 2021), MMLU-Pro (Wang et al., 2024b), BBH (Suzgun et al., 2022),\nCMMLU (Li et al., 2023), and C-Eval (Huang et al., 2024) for the aggregated evaluations. HellaSwag\n(Zellers et al., 2019), CommonsenseQA (Talmor et al., 2019), and WinoGrande (Sakaguchi et al.,\n2021) are adopted to measure our model on commonsense understanding, while PIQA (Bisk et al.,\n2020) is specially for physical related commonsense. We also select DROP (Dua et al., 2019),\nC3 (Sun et al., 2020) and NaturalQuestions(Kwiatkowski et al., 2019) to evaluate LLMs on their\nbasic capabilities on classical NLP tasks such as question answering and reading comprehension.\nARC-C (Clark et al., 2018), TriviaQA (Joshi et al., 2017) are added as QA tasks that require certain\nbackground related to science and updated world knowledge. Finally, we evaluate LLMs on GSM8k\n(Cobbe et al., 2021), MATH Hendrycks et al. (2021), and CMATH (Wei et al., 2023) to measure the\nmathematics capability, and on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for\ncoding, which are representative and essential LLM abilities.\nEvaluation Settings and Competitors. We follow the commonly used evaluation settings (e.g.,"}, {"title": "4.1.2 Model Performance of Pre-Training", "content": "Table 3 illustrates the performance of Hunyuan-Large and other competitive pre-trained models. In\ngeneral, Hunyuan-Large achieves the best overall performance compared to both Dense and MoE\nbased competitors having similar activated parameter sizes. For aggregated benchmarks such as\nMMLU, Hunyuan-Large not only surpasses the performance of the LLama3.1-405B model but does so\nwith a significantly lower count of activation parameters, achieving an impressive 3.2% improvement.\nHunyuan-Large also shows superior performance in commonsense understanding and reasoning"}]}