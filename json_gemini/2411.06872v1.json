{"title": "Multi-Modal interpretable automatic video captioning", "authors": ["Antoine Hanna-Asaad", "Decky Aspandi", "Titus Zaharia"], "abstract": "Video captioning aims to describe video contents using natural language format that involves understanding and interpreting scenes, actions and events that occurs simultaneously on the view. Current approaches have mainly concentrated on visual cues, often neglecting the rich information available from other important modality of audio information, including their inter-dependencies. In this work, we introduce a novel video captioning method trained with multi-modal contrastive loss that emphasizes both multi-modal integration and interpretability. Our approach is designed to capture the dependency between these modalities, resulting in more accurate, thus pertinent captions. Furthermore, we highlight the importance of interpretability, employing multiple attention mechanisms that provide explanation into the model's decision-making process. Our experimental results demonstrate that our proposed method performs favorably against the state-of the-art models on commonly used benchmark datasets of MSR-VTT and VATEX.", "sections": [{"title": "1 Introduction", "content": "Video captioning has the goal of automatically generating sentence to describe the video content [42,13]. This area of research has emerged as an important area with multiple applications ranging from content indexing and retrieval [24,33] to assist individuals with visual impairments [45]. However, complex nature of the video presents a significant challenge for captioning tasks. Compared to image captioning [7,44,22], videos consists of temporal sequences where multiple actions and events occur in the same time or in rapid succession. Additionally, audio cues, like as speech, music, and environmental sounds, introduce an extra layer of complexity that need to be managed to generate coherent caption.\nDespite making substantial progress, existing methods in this task is still considered inadequate in capturing the local and global representation. Various works often relies only on raw-pixels and one modality only [47,26,54,58] and often neglect other modalities such as audio, speech or audio-caption. To address this, recent multi-modal approaches [8,39,15] integrate various modalities, however, work to relate such modalities is still lacking. This can be problematic, given that there exist depedency between these modalities on the video"}, {"title": "2 Related Works", "content": "Video Captioning attempts to produce a natural language sentence that describes the action, within a given video observation [2]. The first attempt on this task can be traced to the rule-based method [6] that introduced the subject, verb, and object concepts within the caption sentencese. In their method, an encoder-decoder framework was used to extract and refine visual features, and a decoder is added to operate on top of those features to generate captions.\nFurthermore, several other approaches started to utilize a pre-trained 2D and 3D-CNN [53,1,28] as their visual backbone to benefit from the learned features during training with large dataset. Subsequently, they used RNNs or LSTMs methods for caption generation [43,12] to model the sequence, natural to these video data. Another method such as [54], proposes the hierarchical multi-modal attention that selectively attends to a certain modality when generating descriptions. However, when compared to the attention mechanism [40], those methods"}, {"title": "Multi-Modal Captioning", "content": "Recent advancements in the field artificial intelligence have shown that using multi-modal information for the input can greatly improve the quality of the their prediction [4,55]. One method example for captioning task is ActBERT [59] which uses a BERT-style [10] with mask-modelling objective applied to both video and ASR (Automatic Speech Recognition) as input in their pipeline. This method indeed achieves a superior accuracy compared to a single modality, however its limitation is from the use of ASR that has high innacuracy in the transcription process which can affect the result. In recent years nevertheless, we have seen a convergence toward a unified model that also further exploit the attention mechanisms to combine the modalities. Some example include [9,8,23], which encode modalities by using different pre-training objective such as masked language modeling [36,57], masked-frame-modeling [17] or video-text matching [18,35,25], that lead to current state-of-the-art result."}, {"title": "Explainability", "content": "Explainability in captioning task is a considered as an important area of research with the aim of improving the understanding of inner workings mechanism from the existing captioning methods. One of the primary methods for achieving explainability in this task is through attention mechanisms, that it allows to trace the models focus during prediction. Specifically, these mechanisms enable the model to focus on specific parts of an input modality (e.g. image) when generating each word of the caption, providing insights into which spatial regions of the modality are considered important. Xu et al. [52] introduced an attention-based model for image captioning that generates attention maps, highlighting relevant areas in the image input for each generated word, thus making the model's decision process more interpretable. Lastly, another notable technique that allows the explanation on the visual domains is Grad-CAM [31] (Gradient-weighted Class Activation Mapping) and Integrated Gradients [37] which can be used to produce saliency maps. These maps show the gradient of the output with respect to the input image that helps in understanding which regions that contribute most to the generated caption."}, {"title": "3 Methodology", "content": "Figure 1 provides an overview of our Multi-modal Interpretable Captioner (MICap) that includes several key components: the input encoders f(.) and g(.), the cross modal fusion block \u03c8(.) and the caption decoder h(.). This approach is inspired from the previous works [17,48,20] with the key modifications involving our novel combiner block and the involvement of additional modality of the audio captions.\nThe input of our pipelines consists of sequence of images that provides the visual information, and the audio based captions that gives the information from audio points of views. These both modalities are first fed into Modalities Encoders (that consists of Video and Audio encoders) to extract their features. Then, these features are fed into the Modalities Combiner for the alignment and fusion operation, that yields both aligned and combined features. These features then fed into the CaptionDecoder, that further merged with first token of the input text to generate the final video caption. The details of each process will be described on the following section."}, {"title": "3.1 Modalities Encoders", "content": "Given a video, V = {I1, I2, ..., IN} where N is the number of frames, and each I\u00a1 \u2208 Rh\u00d7w\u00d7c and a set of audio caption A, we obtain the generated caption \u0177 as follow:\n\u0177 = hdecoder (fv-enc (V), gac-enc(A))\nwe first extract corresponding visual features (x) using the fu for every frames with a pre-trained CLIP-ViT-B/16 [29]. Then, we further consider temporal information between each frames using set of learnable embeddings \u03a9\u03c5 associated to each frame. For audio encoder gac, we use the BERT model [10] to process the generated audio captions (from MS-CLAP [11]) to extract the audio feature xa. These operations can be seen below:"}, {"title": "3.2 Modalities Combiner", "content": "Inspired by other video understanding methods of [38,34], we fuse both of video and audio modality with the use of a co-attention transformer block [40] that consists of two internal encoders in paralel. This encoder processes different modality, e.g. image and audio, thus resulting in correlations between modalities (video-audio and audio-video). For both the video-audio branch and audio-video branch, we have :\nzv = Transformer(xv, xa), za = Transformer(xa, xv)\nThis operation can be repeated L times. In our case, the transformer layer is composed of:\nTransformer(X, Z) = FFB(MHA(X, Z, Z))\nwhere MHA is multiple heads of attention functions and a Feed-Forward Block (FFB). MHA is the correlation functions that consists of several elements including Na query vector of dimension d, Q \u2208 RN\u00d7D, coming from X and N key-value pairs, K\u2208 RN\u00d7D,V \u2208 RN\u300f\u00d7D_coming from Z. Given these, attention is defined as the scale-dot product of queries Q and keys K that further aggregated by the value V:\nAtt(Q, K, V) = Softmax(-QKT/-)V\nThen, we aggregate the features to be used for further caption generation process:\nZva = norm([zv; za])\nwhere norm(.) denote the Layer Normalisation operation and [;] denote the concatenation operation."}, {"title": "3.3 Caption Decoder", "content": "Given the previously correlated features Zva, then we employ an auto-regressive decoder h(.) and we use the commonly used loss objective of minimizing the negative log-likelihood to generate the target video caption. During training, the decoder operates on the sequences of tokens up to t-1 tokens, denoted as X<t. During testing, we use the start token [CLS] to initiate the auto-regressive generation of caption. The aggregated features of both zu and za are then injected during generation through a cross attention mechanism.\n\u0177 = h(x<t, zva)\n\u0177 = argmax(softmax(W \u00b7 Transformer(x<t, zva)))\nwhere the Transformer block is defined in eq. 3.2 and W is projection matrix used to project the result back into our vocabulary size. Here, we initialize our CaptionDecoder model from a trained BERT model [10] and we use causal masking to prevent information leakage during training. Finally, we use beam-search with a beam width of five for the caption generation during inference."}, {"title": "3.4 Loss Functions and Models Training", "content": "To optimise our predictions, we employ two main losses, namely the caption loss and the contrastive loss. The caption loss concerns the quality of the predicted captions to the ground truth caption and defined as :\nLcaption = -Ex~pdata log Po (Xt|X<t, zva)\nwhere Pdata corresponds to our data distribution, pe represent our decoder, It denote the ground truth caption at step t, x<t denote the previous t-1 tokens, and e correspond to the trainable parameters.\nWe then use contrastive loss to extract more relevant features by evaluating their value in embedding space, subject to the contrasting criteria. This criteria assumes that extracted visual and audio features that belongs to the same video to have similar representation in embedding space compared to ones that come from different video, and vice versa. To do this, we perform the alignment operation [14] that is common in another relevant task (e.g. video retrieval [3,50] ) during optimisation. This operation effectively aligns the features frames and audio caption from the same video to be as close as possible. This operations starts by performing pooling operation on the output of the cross-module for the video and audio\nCv = Poolvideo(zv), Ca = Poolaudio(za)\nwhere \"pool\" refers to obtaining the hidden state corresponding to the first token followed by a Tanh activation on the projected token. Then for each video"}, {"title": "3.5 Implementation Details", "content": "For pre-processing stage, we sample the frames from each video clip using ffmpeg library 3 and we resize and crop the frames into an image of size of 224 \u00d7 224 \u00d7 3, while the corresponding, extracted audio-caption is padded with S value of 67 long. We train all models using a single NVIDIA-3080Ti for approximately one to five days for MSR-VTT and VATEX repectively. We use a learning rate of 1e-5 for the text decoder and the modalities combiner, a learning rate of 5e-5 for the input encoders, with \u0430\u0442value (for the contrastive loss) set to 0.07. We use gradient accumulation and half precision to speed up the training process and reduce the memory overhead of our model simultaneously. The implementation of our methods can be found on the following repository4."}, {"title": "4 Experiment Results", "content": "Two commonly used video captioning datasets are considered for the evaluations:\n1. MSRVTT [51] dataset which contains 10k open-domain video clips for video captioning. Each video clip is annotated with 20 captions. To comply with Youtube copyright regulations, we are left with 6, 8k video clips."}, {"title": "4.2 The impact of Fusion and Contrastive Loss", "content": "Here we present the comparison against five versions of our method to highlight the impacts of each of our approaches:\nBLIP [17] is external method with similar network arrangement to ours that operates mainly with the use of visual modality.\nAudio Raw is the results which directly compare the audio-caption input by [11] against the ground truth test caption.\nAudio-Based is our method that uses audio-caption as input and trained with both AudioCaption encoder and CaptionDecoder.\nVision-Based is our method that uses the videos as input and trained with both the VideoEncoder encoder and CaptionDecoder.\nFusion is our model variant that uses video and audio modality as the input and is trained using only the captioning loss.\nMICap is our full model that uses video and audio modalities, including our proposed combined loss."}, {"title": "4.3 Visual and Audio Interpretability", "content": "To gain insights into the inner workings of our video captioning model, we further analyze our modality processing blocks with respect the their video caption prediction. There are three targeted blocks for the analysis: First the input video applied to the Video-Audio block with respect to the output (generated caption). Second is the attention mechanisms of audio-captions, applied to the Audio-Video branch in respect to the generated caption. Lastly the self attention during the predictions (our decoder block). This analysis will allow us to inspect"}, {"title": "4.4 Explorations of features space on Contrastive loss", "content": "Figure 3 gives us a vizualization of features from our Fusion (without contrastive loss) and Full model after applying t-SNE method between positives and negatives pairs for different features pairs (audio-caption and video frames). Here we can see that the features between the video and audio pair (from the same instance) are located on distinct area. However after the use of contrastive loss (MICap), they are aligned closed together. This observation has been show on other study (e.g. video retrieval), for example BLIP that contrast visual and video caption. However, remarkably this also applies when we substitute the video caption and with the audio caption, with the main difference is that the audio-caption is always accessible during inference (true video caption as input"}, {"title": "4.5 State of the Art Comparisons", "content": "Table 2 shows the results of different methods on the test set of MSR-VTT and VATEX dataset. As a note, we have to re-run all evaluated model on both datasets, to take into account the change happening in the dataset (number of videos is reduced), by using the respective implementation that are publicly available.\nGiven the results, we can see that our proposed approach achieves best or second highest points in terms of CIDEr/METEOR and BLEU@4/ROUGE-L respectively for MSR-VTT and VATEX, demonstrating its competitiveness. Compared to the state of the art model like SwinBERT, our method still compared favorably, with the average margin of less than 5 points accross datasets. However, SwinBERT performance can be attributed from the fact use VideoSwin [21] as its backbone, which was pre-trained on the Kinetics dataset [16], of which the VATEX [49] dataset is a subset. This large amount of dataset used during their training can largely impacts their results. Despite of this, we can still see that our approach achieves superior in two key metrics such as B@4 and Rouge-L on MSR-VTT and VATEX. This can come from the strategic use of attention mechanisms, enhancing the model's ability to focus on relevant features.\nFig. 4 provides comparison of methods focus of our method, GIT and SwinBERT, by computing both GradCAM and evaluating attention weight of each method. In this example, our model focuses on more elements like bodies, people, and objects (balloons). where the other method lack the focus, as can be seen where GIT, especially SwinBERT attend to almost all region of the input images, including the background. Thus, we can see that our model provides more accessible explanations for predictions in the audio modality compared to other models.\nThe improved focus can be attributed to the incorporation of audio modality, which provides us with temporal information for understanding dynamic video content. For example, our model's audio captions offer keywords like \"ball,\" directing visual attention to relevant areas, as evidenced by the model's focus on the ball in both audio and visual domains. Additionally, our model more accurately explains video scenes, such as correctly identifying a group of people playing."}, {"title": "5 Conclusion", "content": "In this work, we introduce a novel multi-modal captioning method equipped with interpretability component to address the lack of the utilization of such modalities for video caption generations. Our approach utilizes both of the visual and audio information to benefit from each modalities characteristics. We do this by first encoding both modalities inputs with the use of encoders, followed by the aggregation of these features through our modalities combiner, that leverages on the use of attention mechanisms. Subsequently, we use auto-regressive caption decoder to produce the captions. We further incorporate contrastive loss, during feature representation, to improve the quality of the method by aligning both audio and video to exploit their characteristics of their modality.\nTo show the benefit of our approach, we perform multiple comparisons on well known established datasets to show the benefits of our approach progressively throughout of different modalities, fusion mechanisms and further including the contrastive loss, where we also show the capabilities of the method to explain their prediction in both modalities. In our experiment, we show that the model is able to show focus on spatial modality (-video) and text modality (audio-caption). Comparison with other methods shows the competitive results of our approach, that is our method manages to achieve high scores on different quantitative metrics while simultaneously focuses on relevant areas upon each of modalities input. This in overall demonstrates our high accurate results and explainability capacity of our proposed approach. Future work will focus on scaling our method with larger datasets and finding meaningful feature representations to further improve the quality of the generated captions."}], "equations": ["\u0177 = hdecoder (fv-enc (V), gac-enc(A))", "xv = CLIP(vi) + Nv\u2081 = f(V) \u2208 RT\u00d7N\u00d7D\nVi", "xa = g(A) \u2208 RS\u00d7Drg", "\u0396\u03c5Transformer(xv, xa), za = Transformer(xa, xv)", "Transformer(X, Z) = FFB(MHA(X, Z, Z))", "Att(Q, K, V) = Softmax(-QKT/-/)V", "Zva = norm([zv; za])", "\u0177 = h(x<t, zva)", "\u0177 = argmax(softmax(W \u00b7 Transformer(x<t, zva)))", "Lcaption = -Ex~pdata log Po (Xt|X<t, zva)", "Cv = Poolvideo(zv), Ca = Poolaudio(za)", "qa2v(\u03b1)exp(s(\u03b1, \u03bd\u03b9)/\u03c4)\n<, q2a (v) =\u03a3\u03af-1 exp(s(\u03b1, \u03bd\u03b9)/\u03c4)\u1fbd\nLs(a, v) =\u2022 Cv\n||Ca||2||Cv2\u2208 [-1,1]", "Lnce = E(a,v)~B[H(yv2a(v), qv2a(v)) + \u0397(ya2v(a), qa2v (a))]", "L = Lnce + Lcaption"]}