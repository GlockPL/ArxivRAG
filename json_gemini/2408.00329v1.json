{"title": "OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial Attack", "authors": ["Kuo Gai", "Sicong Wang", "Shihua Zhang"], "abstract": "Deep neural networks (DNNs) are vulnerable to small adversarial perturbations of the inputs, posing a significant challenge to their reliability and robustness. Empirical methods such as adversarial training can defend against particular attacks but remain vulnerable to more powerful attacks. Alternatively, Lipschitz networks provide certified robustness to unseen perturbations but lack sufficient expressive power. To harness the advantages of both approaches, we design a novel two-step Optimal Transport induced Adversarial Defense (OTAD) model that can fit the training data accurately while preserving the local Lipschitz continuity. First, we train a DNN with a regularizer derived from optimal transport theory, yielding a discrete optimal transport map linking data to its features. By leveraging the map's inherent regularity, we interpolate the map by solving the convex integration problem (CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse architectures of ResNet and Transformer, making it suitable for complex data. For efficient computation, the CIP can be solved through training neural networks. OTAD opens a novel avenue for developing reliable and secure deep learning systems through the regularity of optimal transport maps. Empirical results demonstrate that OTAD can outperform other robust models on diverse datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) are the most crucial com-ponent of the artificial intelligence (AI) field. DNNs are rapidly becoming the state-of-the-art approaches in many tasks, i.e., computer vision, speech recognition, and natural language processing. Theoretical explorations of DNNs inspire the understanding of deep learning and development of new algorithms [1]\u2013[7]. However, DNNs are vulnerable to adversarial attacks, i.e., a well-chosen small perturbation of the input data can lead a neural network to predict incorrect classes.\nVarious strategies have been proposed to enhance existing models [8]. These strategies include adversarial training [9]\u2013[11], where adversarial examples are generated during training and added to the training set. However, these modified models often exhibit vulnerabilities against strong adversaries [12]\u2013[15], primarily because DNNs require large gradients to represent their target functions and attacks can always take advantage of the gradients to construct adversarial examples.\nTo stop playing this cat-and-mouse game, a growing body of studies have focused on certified robustness.\nOne straightforward approach to certified robustness in-volves constraining their Lipschitz constant. Existing ap-proaches to enforce Lipschitz constraints can be categorized into three groups: soft regularization [16]\u2013[18], hard con-straints on weights [19]\u2013[22] and specifically designed acti-vations [23]\u2013[25]. However, compared to standard networks, these approaches show suboptimal performance even on sim-ple datasets like CIFAR10. The strict Lipschitz constraints during training may hinder the model's ability to find a more effective Lipschitz function. Additionally, the target function is not Lipschitz everywhere, especially in classification problems on continuous data distributions where the function cannot be Lipschitz at the boundary between two classes.\nIn this paper, we propose a novel two-step model named OTAD to combine the strengths of the mentioned approaches (Fig. 1). The objective is to achieve a robust and accurate learned function at the terminal stage of training without enforcing Lipschitz constraints throughout the entire training process. Inspired by optimal transport theory, we leverage the theory that the optimal transport map is the derivative of a convex function $\\phi$ and possesses regularity properties, implying the map $\\nabla \\phi$ is locally Lipschitz under moderate conditions. Based on this, we can learn the discrete optimal transport map through neural networks during training and compute the robust output of the model that satisfies the local Lipschitz property.\nIn detail, we first employ a DNN to acquire the optimal transport map from data to the feature for classification (Fig. 1). Gai and Zhang [26] have demonstrated that ResNet with weight decay tends to approximate the Wasserstein geodesics during training. Therefore, we first utilize ResNet to obtain a discrete optimal transport map $T$ from data points to their features. $T$ can accurately classify the training data due to the approximation power of ResNet. Subsequently, we employ a robust model based on the discrete optimal transport map $T$ instead of the learned ResNet. For arbitrary given input $x$ in the inference process, our objective is to find an appropriate feature $y$ such that a Lipschitz function $f$ exists, satisfying $f$ being consistent with the discrete optimal transport map on the training set and $f(x) = y$. Given a set $\\{(x_i, T(x_i))\\}_{i \\in I}$, the goal is to find a convex and smooth function $g$ such that $g(x_i) = \\phi(x_i)$. This problem can be formalized into a convex integration problem (CIP). We demonstrate that solving a quadratically constrained program (QCP) based on recent advances in first-order methods [27] can find a solution to the CIP and yield a feasible value of $y$.\nHowever, the QCP is much slower than the inference of a DNN. To address this issue, we train a Transformer"}, {"title": "II. RELATED WORKS", "content": "Adversarial training aims at resisting adversarial attacks by worst-case risk minimization. Consider a classifier neural network $g$ with trainable parameters $\\theta$, let $L(g(x), y)$ denote the classification loss on data $x$ and its label $y$, then the objective of $g$ is\n$\\min_\\theta \\mathbb{E}_{p(x,y)}[\\max_{x' \\in B(x)} L(g(x'), y)"}]}