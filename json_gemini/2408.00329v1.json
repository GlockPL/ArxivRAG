{"title": "OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial Attack", "authors": ["Kuo Gai", "Sicong Wang", "Shihua Zhang"], "abstract": "Deep neural networks (DNNs) are vulnerable to small adversarial perturbations of the inputs, posing a significant challenge to their reliability and robustness. Empirical methods such as adversarial training can defend against particular attacks but remain vulnerable to more powerful attacks. Alternatively, Lipschitz networks provide certified robustness to unseen per-turbations but lack sufficient expressive power. To harness the advantages of both approaches, we design a novel two-step Optimal Transport induced Adversarial Defense (OTAD) model that can fit the training data accurately while preserving the local Lipschitz continuity. First, we train a DNN with a regularizer derived from optimal transport theory, yielding a discrete optimal transport map linking data to its features. By leveraging the map's inherent regularity, we interpolate the map by solving the convex integration problem (CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse architectures of ResNet and Transformer, making it suitable for complex data. For efficient computation, the CIP can be solved through training neural networks. OTAD opens a novel avenue for developing reliable and secure deep learning systems through the regularity of optimal transport maps. Empirical results demonstrate that OTAD can outperform other robust models on diverse datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) are the most crucial com-ponent of the artificial intelligence (AI) field. DNNs are rapidly becoming the state-of-the-art approaches in many tasks, i.e., computer vision, speech recognition, and natural language processing. Theoretical explorations of DNNs inspire the understanding of deep learning and development of new algorithms [1]\u2013[7]. However, DNNs are vulnerable to adversarial attacks, i.e., a well-chosen small perturbation of the input data can lead a neural network to predict incorrect classes.\nVarious strategies have been proposed to enhance existing models [8]. These strategies include adversarial training [9]\u2013[11], where adversarial examples are generated during train-ing and added to the training set. However, these modified models often exhibit vulnerabilities against strong adversaries [12]\u2013[15], primarily because DNNs require large gradients to represent their target functions and attacks can always take advantage of the gradients to construct adversarial examples. To stop playing this cat-and-mouse game, a growing body of studies have focused on certified robustness.\nOne straightforward approach to certified robustness in-volves constraining their Lipschitz constant. Existing ap-proaches to enforce Lipschitz constraints can be categorized into three groups: soft regularization [16]\u2013[18], hard con-straints on weights [19]\u2013[22] and specifically designed acti-vations [23]\u2013[25]. However, compared to standard networks, these approaches show suboptimal performance even on sim-ple datasets like CIFAR10. The strict Lipschitz constraints during training may hinder the model's ability to find a more effective Lipschitz function. Additionally, the target function is not Lipschitz everywhere, especially in classification problems on continuous data distributions where the function cannot be Lipschitz at the boundary between two classes.\nIn this paper, we propose a novel two-step model named OTAD to combine the strengths of the mentioned approaches (Fig. 1). The objective is to achieve a robust and accurate learned function at the terminal stage of training without enforcing Lipschitz constraints throughout the entire training process. Inspired by optimal transport theory, we leverage the theory that the optimal transport map is the derivative of a convex function \u03c6 and possesses regularity properties, implying the map \\(\\nabla \\varphi\\) is locally Lipschitz under moderate conditions. Based on this, we can learn the discrete optimal transport map through neural networks during training and compute the robust output of the model that satisfies the local Lipschitz property.\nIn detail, we first employ a DNN to acquire the optimal transport map from data to the feature for classification (Fig. 1). Gai and Zhang [26] have demonstrated that ResNet with weight decay tends to approximate the Wasserstein geodesics during training. Therefore, we first utilize ResNet to obtain a discrete optimal transport map T from data points to their features. T can accurately classify the training data due to the approximation power of ResNet. Subsequently, we employ a robust model based on the discrete optimal transport map T instead of the learned ResNet. For arbitrary given input x in the inference process, our objective is to find an appropriate feature y such that a Lipschitz function f exists, satisfying f being consistent with the discrete optimal transport map on the training set and f(x) = y. Given a set \\(\\{(x_i,T(x_i))\\}\\}_{i\\in I}, the goal is to find a convex and smooth function g such that g(x_i) = \\(\\varphi(x_i)\\). This problem can be formalized into a convex integration problem (CIP). We demonstrate that solving a quadratically constrained program (QCP) based on recent advances in first-order methods [27] can find a solution to the CIP and yield a feasible value of y.\nHowever, the QCP is much slower than the inference of a DNN. To address this issue, we train a Transformer named CIP-net as an alternative to the optimization algorithm for efficient computation. Theoretically, we derive an upper bound of the Lipschitz constant of the Transformer block, demonstrating the strong performance of CIP-net.\nTo further improve the performance of OTAD, we extend it with various architectures and metric learning. First, we adapt OTAD to Transformer-based architecture such as ViT [28], as Transformers employ residue connections in forward propagation, and the invariant dimension of the feature aligns well with the optimal transport setting. Second, since finding neighbors is a critical step in OTAD, the \\(l_2\\) distance may not effectively characterize the similarity of data closing to a manifold in high dimensional space. Consequently, we explore metric learning to find more suitable neighbors and explore the trade-off between accuracy and vulnerability in such cases. We implement various experiments to test the proposed OTAD model and its variants under different settings. Empirical re-sults demonstrate superior performance to adversarial training methods and Lipschitz networks across diverse datasets.\nThe rest of this paper is organized as follows. In section 2, we present the background of our method: the optimal transport theory and regularity of the optimal transport map. In section 3, we develop the Optimal Transport-based Adversarial Defense model (OTAD) and its implementation details. In section 4, we perform extensive experiments to demonstrate the defense ability of OTAD."}, {"title": "II. RELATED WORKS", "content": "Adversarial training aims at resisting adversarial attacks by worst-case risk minimization. Consider a classifier neural network g with trainable parameters \u03b8, let \\(L(g(x), y)\\) denote the classification loss on data x and its label y, then the objective of g is\n\\begin{equation}\n\\min_{\\theta} \\mathbb{E}_{p(x,y)}[\\max_{x' \\in \\mathcal{B}(x)} L(g(x'), y)]\n\\tag{1}\n\\end{equation}\nwhere \\(p(x, y)\\) is the joint distribution of data and labels, \\(\\mathcal{B}(x)\\) is a neighborhood of x. However, it is impossible to search the whole region since the loss surface of networks is complex. To approximate it, adversarial training methods [9]\u2013[11], [29]\u2013[33] add adversarial examples (often found by gradient descent) to the training set during training. After training, the model is robust to the type of attack chosen in the training process. The defense can still be \u2018broken\u2019 by stronger adversaries [12]\u2013[15]. To escape the cat-and-mouse game, training neural networks with bounded Lipschitz continuity has been considered a promising way to defend against attacks."}, {"title": "A. Adversarial training", "content": "Adversarial purification aims at reconstructing the clean data point by conventional transform [34], [35] or generative model before classification [36], [37]. Let P be the distribution of clean data. For adversarial noise corrupted data point \\(x+n_{ad}\\), generative model-based adversarial purification trains a network G to project the noisy data back to the manifold of clean data distribution, i.e., \\(G(x + n_{ad}) = x\\). The classifier C is trained on P and will classify \\(G(x + n_{ad})\\) with better accuracy.\nAdversarial purification can defend unseen attacks because the generative model G is trained independently from adver-sarial attacks \\(n_{ad}\\) and classifiers C. However, we can still construct adversarial examples by taking the gradients of the generative process. Such adversarial noise will not be purified [37]. The effectiveness of adversarial purification methods is highly related to the performance of generative models, such as energy-based models (EBM) [38], generative adversarial networks (GAN) [39], [40] and auto-regressive generative models [41]. The diffusion model is a rising effective genera-tive model. Diffusion-based adversarial methods have achieved competitive performance on large-scale image datasets [37], [42], [43]. For small datasets (e.g., single-cell gene expression data) with specific noise or missing values, it can be hard to train an effective generative model, indicating that adversarial purification may not be a general method for various datasets."}, {"title": "B. Adversarial purification", "content": "Training neural networks under a Lipschitz constraint puts a bound on how much its input can change in proportion to a change in its input. Existing methods fall into three categories: soft regularization, hard constraints for weights, and specifically designed activations.\nRegularizations such as penalizing the Jacobian of the network [16]\u2013[18] can constrain the Lipschitz constant locally but don't provably enforce a global Lipschitz constraint. Thus, adding such regularizations cannot solve the fragility of adversarial attacks on neural networks.\nOn the other hand, as 1-Lipschitz functions are closed under composition, it suffices to constrain 1-Lipschitz affine transformations and activations. Several prior works [19]\u2013[21] enforce the Lipschitz property by constraining the spectral norm of each weight matrix to be less than one. Another method [22] projects the weights closer to the manifold of orthogonal matrices after each update. These models provably satisfy the Lipschitz constraint but lack expressivity to some simple Lipschitz functions.\nTo enhance the expressivity of the Lipschitz network, Anil et al. [23] proposes a gradient norm-preserving activation func-tion named GroupSort and proves that the networks with this activation and matrix-norm constraints are universal approxi-mators of Lipschitz function. Singla et al. [44] further provides some tricks such as certificate regularization to boost the robustness. Zhang et al. [24], [25] propose to use \\(l_0\\)-distance function which is also proved to have universal Lipschitz function approximation property. Though many improvements have been made in this direction, the performance of the above Lipschitz networks is still not satisfactory even on simple datasets like CIFAR10, partially because the strict Lipschitz constraint in the training process impedes the model from finding a better Lipschitz function. Fundamentally different from pursuing Lipschitz constraints in the training process, we propose a novel model that provides robustness by regularity property of optimal transport map while employing powerful architectures like ResNets and Transformers.\nRandomized smoothing is another way to obtain a (proba-bilistic) certified robustness guarantee. This technique uses a Gaussian smoothed classifier, which predicts the most likely label when Gaussian noise is added to the input of the original classifier. Conversely, OTAD integrates neighborhood information but finds robust output satisfying local Lipschitz properties instead of using the original classifier, which may be vulnerable to attacks."}, {"title": "C. Lipschitz, networks and random smoothing", "content": "Optimal transport theory provides valuable tools for quanti-fying the closeness between probability measures, even when their supports do not overlap. \\(\\mathcal{P}_2(\\mathbb{R}^d)\\) denotes the set of Borel probability measures with finite second-order moment. For two probability measures \\(\\mu,\\nu \\in \\mathcal{P}_2(\\mathbb{R}^d)\\), let \\(\\Pi(\\mu,\\nu)\\) denote the set of all joint distributions \\(\\pi(x,y)\\) whose marginals are \\(\\mu\\) and \\(\\nu\\) respectively. The Wasserstein distance is defined as the solution of the Kantorovich problem [45]:\n\\begin{equation}\nW_2(\\mu, \\nu) := \\left(\\inf_{\\pi \\in \\Pi(\\mu, \\nu)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} ||x - y||^2d\\pi(x,y)\\right)^{1/2}\n\\tag{2}\n\\end{equation}\nIn the Monge formulation, maps are considered instead of joint distributions. The Borel map T pushes forward \\(\\mu\\) to \\(\\nu\\), i.e., \\(T_{\\sharp}\\mu = \\nu\\). For any set \\(A \\subset \\mathbb{R}^d\\), \\(T_{\\sharp}\\mu(A) = \\mu(T^{-1}(A))\\). If the feasible map T exists, the Monge formulation is equivalent to the Kantorovich formulation:\n\\begin{equation}\nW_2(\\mu, \\nu) = \\inf_{T:T_{\\sharp}\\mu=\\nu} \\int_{\\mathbb{R}^d}||x - T(x)||^2d\\mu(x)^{1/2}\n\\tag{3}\n\\end{equation}\nThe Brenier theorem [46] asserts that if \\(\\mu\\) is absolutely continuous, there always exists a convex function \\(\\varphi\\) such that \\(\\nabla \\varphi_{\\sharp}\\mu = \\nu\\) and \\(\\nabla \\varphi\\) is the optimal transport map sending \\(\\mu\\) to \\(\\nu\\). This convex function \\(\\varphi\\) is called a Brenier potential between \\(\\mu\\) and \\(\\nu\\).\nConsider a constant speed geodesic \\((\\mu_t)\\) on \\(\\mathcal{P}_2(\\mathbb{R}^d)\\) induced by an optimal transport map T connecting \\(\\mu\\) and \\(\\nu\\):\n\\begin{equation}\n\\mu_t = ((1 - t) I + tT)_{\\sharp}\\mu, \\mu_1 = \\nu\n\\tag{4}\n\\end{equation}\nThe continuity equation for \\(\\mu_t\\) is given by:\n\\begin{equation}\n\\frac{d}{dt} \\mu_t + \\nabla \\cdot (v_t\\mu_t) = 0\n\\tag{5}\n\\end{equation}\nwhere \\(v_t\\) is a vector field on \\(\\mathbb{R}^d\\). By Benamou-Brenier formula in [47], the constant speed geodesic can be recovered by minimizing the following energy function leading to the third definition of the Wasserstein distance\n\\begin{equation}\nW_2(\\mu, \\nu) = \\left(\\inf_{v} \\int_0^1 \\int_{t\\mathbb{R}^d} ||v_t(x)||^2d\\mu_t(x)dt\\right)^{1/2}\n\\tag{6}\n\\end{equation}"}, {"title": "III. METHOD BACKGROUND", "content": "Here, the infimum is taken among all solutions \\((\\mu_t, v_t)\\) satis-fying continuity with \\(\\mu_0 = \\mu\\) and \\(\\mu_1 = \\nu\\).\nRegularity in optimal transport is usually understood as the property that the map \\(\\nabla \\varphi\\) is L-Lipschitz, equivalent to \u03c6 being L-smooth. Assume \\(\\mu\\) and \\(\\nu\\) are supported on a bounded open set with their density function on the support set bounded away from zero and infinity. When the target domain is convex, Caffarelli [48] proved that the Brenier map can be guaranteed locally Lipschitz. The optimal transport map exhibits discontinuities at singularities when the target domain is non-convex. Nevertheless, the map remains locally Lipschitz on the support set excluding a Lebesgue negligible set [49].\nIn this paper, we consider regularity (smoothness) and curvature (strong convexity) as the conditions that must be enforced when computing the optimal transport map. Our objective is to find a potential function \u03c6 that is l-strongly convex and L-smooth, i.e.,\n\\begin{equation}\nl||x - y|| \\leq ||\\nabla\\varphi(x) - \\nabla\\varphi(y)|| \\leq L||x - y||\n\\tag{7}\n\\end{equation}"}, {"title": "A. Optimal transport", "content": "Let f denote the learned function at the terminal stage of the training process, \\(f^*\\) denote the target function that maps the training data points to their labels and \\(\\{x_1,\\dots,X_n\\}\\) denote the training data points i.i.d. sampled from distribution \\(\\mathbb{P}_r\\). The primary objective of robust learning is to ensure that the learned function f simultaneously achieves the following two goals:\n*   Well-approximation of the target function on the training data, i.e., the loss function\n    \\begin{equation}\n    \\sum_{i=1}^n \\mathcal{L} (f(x_i), f^* (x_i))\n    \\tag{8}\n    \\end{equation}\n    is minimized.\n*   Control over the change in output under input pertur-bations. That requires the learned function is Lipschitz continuous with a small constant L. For any pairs of inputs \\(x_1, x_2 \\in \\mathbb{R}^d\\), the Lipschitz continuity condition is given by:\n    \\begin{equation}\n    ||f(x_1) - f(x_2)|| \\leq L||X_1 - X_2||\n    \\tag{9}\n    \\end{equation}\nOTAD utilizes DNNs with residual connections to approximate the target function on the training data and enforces the local Lipschitz property by solving a CIP problem."}, {"title": "IV. OTAD", "content": "Consider the classification problem, we first train a m-block ResNet R(\u00b7) and a classifier H(\u00b7) to classify the dataset \\(\\{x_i\\}_{i=1}^n\\) with labels \\(\\{y_i\\}_{i=1}^n\\). Given input x, the forward propagation of the k-th block of R is defined as\n\\begin{equation}\nx_k = x_{k-1} + R_k(x_{k-1}), k\\in \\{1,2,\\dots,m\\},x^0 = x\n\\tag{10}\n\\end{equation}\nwhere \\(R_k\\) denotes the shallow network inside the k-th block of R. Let R(x) be the last-layer features, i.e., \\(R(x) = x_m\\).\n\\begin{equation}\n\\frac{\\partial \\mu}{\\partial t} + \\nabla \\cdot (v_t\\mu_t) = 0\n\\tag{11}\n\\end{equation}\nAccording to the Benamou-Brenier formula in (6), when \\(\\mu_0\\) and \\(\\mu_1\\) are fixed, the Wasserstein geodesic curve induced by optimal transport map can be recovered by minimizing the energy\n\\begin{equation}\n\\int_0^1 \\int_{\\mathbb{R}^d} || v_t ||_2^2 d\\mu_t dt\n\\tag{12}\n\\end{equation}\nA discretization of this energy concerning the ResNet R(\u00b7) is\n\\begin{equation}\n\\sum_{i=1}^n \\int_0^1 || v_t ||_2^2 dt = \\sum_{i=1}^n \\sum_{k=1}^m || R_k (x_i^{k-1}) ||^2\n\\tag{13}\n\\end{equation}\nUsing the energy as a regularizer, the objective to train the ResNet R(\u00b7) and the linear classifier H(\u00b7) is\n\\begin{equation}\n\\min_{\\Theta} \\sum_{i=1}^n \\mathcal{L}(H(R(x_i)), y_i) + \\alpha \\sum_{i=1}^n \\sum_{k=1}^m ||R_k(x_i^{k-1})||_2^2\ns.t. x_i^k = x_i^{k-1} + R_k(x_i^{k-1}), k \\in \\{1,2,\\dots, m\\}, x_i^0 = X_i\n\\tag{14}\n\\end{equation}\nwhere \\(\\mathcal{L}\\) is the loss function for classification, \u0398 denotes all the trainable parameters in H(R(\u00b7)), and \u03b1 is the hyperparameter to balance the two terms. Gai and Zhang [26] demonstrate that the weight decay operation plays a similar role with the regularizer in (14) for a Wasserstein geodesic. Let \\(z_i = R(x_i)\\). If the problem (14) is well solved, then we obtain a network-based classifier H(R(\u00b7)) with high training accuracy and a discrete optimal transport map from the data \\(x_i\\) to feature \\(z_i\\).\nThough the network R(\u00b7) learns the discrete optimal trans-port map T, it can still be vulnerable to small perturbations. Therefore, we need to find a more robust function f with f(x) = \\(z_i\\). Let h denote the potential function of f, i.e., f(\u00b7) = \\(\\nabla h(\\cdot)\\). Since the optimal transport map is locally Lipschitz and has singularities when the target domain is non-convex, then for a given test data point x', we can only trust training data near x' to constrain f(x'). Let \\(N_K(x')\\) be the set of K nearest neighbors of x' from the training dataset. Assume h is l-strongly convex L-smooth on the neighborhood of x', then finding appropriate value \\(\\nabla h(x')\\) can be formulated to a convex integration problem:\nDefinition 1: Let I be a finite index set and \\(F_{I,L}\\) denote the class of l-strongly convex and L-smooth function on \\(\\mathbb{R}^d\\). Given a set \\(S = \\{(x_i, z_i)\\}_{i\\in I}\\), the \\(F_{I,L}\\) convex integration problem is finding a function \\(f \\in F_{I,L}\\), such that \\(z_i = \\nabla f(x_i)\\) for all i\u2208 I.\nFirst, we need to determine the existence of such a function with respect to \\(N_K(x')\\). We introduce the definition of \\(F_{I,L}\\)-integrable in [27].\nDefinition 2: Consider the set \\(S = \\{(x_i, z_i)\\}_{i\\in I}\\). The set S is \\(F_{I,L}\\)-integrable if and only if there exists a function \\(f \\in F_{I,L}\\) such that \\(z_i = \\nabla f(x_i)\\) for all i \u2208 I."}, {"title": "A. ResNet-based OTAD", "content": "Input: data \\(\\{x_i\\}_{i=1}^n\\), labels \\(\\{y_i\\}_{i=1}^n\\), hyperparameter \u03b1\nOutput: features \\(\\{z_i\\}_{i=1}^n\\), classifier H(\u00b7), ResNet R(.)\n1: repeat\n2: minimize\n\\sum_{i=1}^n \\mathcal{L}(H(R(x_i)), y_i) + \\alpha \\sum_{i=1}^n \\sum_{k=1}^m || R_k (x_i^{k-1}) ||_2^2\nby gradient descent\n3: until convergence\n4: Let \\(z_i = R(x_i)\\), \\(i \\in \\{1,2,..., n\\}\\)\nInput: data \\(\\{x_i\\}_{i=1}^n\\), features \\(\\{z_i\\}_{i=1}^n\\), test data x', constant l, L and K,\nstepsize \\(\\delta_1\\), \\(\\delta_2\\), classifier H(.)\nOutput: predict feature z' and label y'\n1: \\(L_t = L, l_t = l\\)\n2: while 1 do\n3: Compute the set \\(N_K(x')\\) by the \\(l_2\\) distance between x' and \\(x_i\\)\nFind feasible Brenier potential values \\(\\{u_i\\}_{i=1}^{K\\}\\) on \\(N_K(x')\\) satisfying inequalities (15) with \\(L_t, l_t\\)\n4: if \\(\\{u_i\\}_{i=1}^{K\\}\\) exists then\n5: Compute z' by solving the problem (16) with L,l\n6: break\n7: else\n8: \\(L_t = L_t + \\delta_1, l_t = l_t - \\delta_2\\)\n9: end if\n10: end while\n11: Let \\(y' = H(z')\\)\nAccording to Theorem 3.8 in Taylor [27], if the set \\(\\{(x, z)|x \\in N_K(x'), z = T(x)\\}\\) is \\(F_{I,L}\\)-integrable, testing equals to find feasible values of \\(u_i := \\nabla h(x_i)\\) satisfying\n\\begin{equation}\n\\forall x_i, x_j \\in N_K(x'), u_i \\geq u_j + \\langle z_j, x_i - x_j \\rangle + \\frac{l}{2} ||x_i - x_j||^2 - \\frac{1}{2(1-l/L)} ||z_i - z_j||^2\\tag{15}\n\\end{equation}\nIf \\(\\{(x, z)|x \\in N_K(x'), z = T(x)\\}\\) is \\(F_{I,L}\\)-integrable, let h be the desire l-strongly convex L-smooth function. For test data x', we can find a feasible value of \\(v = h(x')\\) and \\(z' = \\nabla h(x')\\) by solving the following QCP problem (Theorem 3.14 in [27])\n\\begin{equation}\n\\min_{v \\in \\mathbb{R}, z' \\in \\mathbb{R}^d} v\ns.t. \\forall x_i \\in N_K(x'), v \\geq u_i + \\langle z_i, x' - x_i \\rangle + \\frac{l}{2} ||x'- x_i||^2 - \\frac{L}{2(1-l/L)} ||z' - z_i - \\frac{l}{L} (x' - x_i)||^2\\tag{16}\n\\end{equation}\nThen we obtain the feature z' used to classify x'. If \\(\\{(x,z)|x \\in N_K(x'),z = T(x)\\}\\) is not \\(F_{I,L}\\)-integrable, we repeat the procedures above with smaller l and larger L until we find feasible values. The solution always exists when l is sufficiently small and L sufficiently large. The training and testing scheme of ResNet-based OTAD is summarized in Algorithms 1 and 2."}, {"title": "B. ResNet-based OTAD", "content": "ResNet-based OTAD requires training an m-block dimension-invariant ResNet. Although dimension-invariant ResNets can be easily implemented for various data and tasks, their fixed dimensionality throughout the forward propagation limits their expressive power. To address this, we extend OTAD to the popular Transformer architecture [50], named OTAD-T. Unlike ResNets, Transformers embed the input into a high-dimensional space, and subsequent Transformer blocks maintain this dimensionality, resulting in a model with good expressive power while keeping the dimensionality unchanged.\nDue to the residual connections in the Transformer, the forward propagation of the Transformer can also be viewed as a discretization of a continuity equation. Thus, it approximates the Wasserstein geodesic curve induced by the optimal trans-port map under the regularizer of (12) at the terminal phase of training. One can use the discrete optimal transport map learned by the Transformer and do the same test procedure as ResNet-based OTAD.\nIn this paper, we focus on the Vision Transformer (ViT) [28]. Given an input image \\(x_i\\), ViT first divides the input image into N non-overlapping patches \\(\\{X_{i,p}\\}_{p\\in N}\\), then each patch undergoes linear embedding E(\u00b7), where it is projected into a higher-dimensional space. The sequence of patch embeddings \\(\\{E(X_{i,p})\\}_{p\\in N}\\) is added positional information by an position operator P(\u00b7), resulting \\(\\{P(E(x_{i,p}))\\}_{p\\in N}\\). The embeddings of patches \\(\\{P(E(X_{i,p}))\\}_{p\\in N}\\) are fed into the standard Trans-former blocks and obtain features \\(\\{Z_{i,p}\\}_{p\\in N}\\). Let\n\\begin{equation}\nX_i = \\begin{bmatrix}\nP(E(x_{i,1}))\\\\\nP(E(X_{i,2}))\\\\\n:\\\\\nP(E(X_{i,N}))\n\\end{bmatrix}, Z_i = \\begin{bmatrix}\nZ_{i,1}\\\\\nZ_{i,2}\\\\\n:\\\\\nZ_{i, N}\n\\end{bmatrix},\n\\tag{17}\n\\end{equation}\nThe forward propagation of the k-th block of ViT is\n\\begin{equation}\nx_i^{kAttn} = x_i^{k-1} + Attn^k(x_i^{k-1})\nx_i^k = x_i^{kAtt} + MLP^k(x_i^{kAtt})\n\\tag{18}\n\\end{equation}\nwhere \\(Attn^k\\) and \\(MLP^k\\) denote the attention and MLP block in the k-th block of ViT. The forward propagation of the Transformer can also be viewed as a discretization of a geodesic curve in Wasserstein space. The corresponding discrete optimal transport map transforms \\(\\{x_i\\}_{i=1}^n\\) into the feature \\(\\{z_i\\}_{i=1}^n\\). Then for given test data x', we can embed it and search a neighborhood set in \\(\\{x_i\\}_{i=1}^n\\). Finally, we estimate an output feature through solving a problem analogous to (16)."}, {"title": "C. Transformer-based OTAD", "content": "In the procedure of OTAD, the most time-consuming step is solving the CIP, particularly the QCP problem for the feature of test data. Traditional QCP solvers like MOSEK [51] can be time-consuming when solving large-scale QCPs.\nRecently, neural networks have been applied to various optimization problems [52], [53], achieving faster solving speeds and even higher accuracy than traditional solvers. Neural networks can efficiently capture complex patterns and dependencies in data, making them well-suited for high-dimensional and large-scale optimization tasks. To improve the inference speed of OTAD-T, we designed an end-to-end neural network to replace traditional solvers for solving the entire convex integration problem. This method is called OTAD-T-NN.\nSpecifically, the inputs of CIP are the embeddings of test data, its neighbors in training set \\(N_K()\\) and the features corresponding to the neighbors \\(Z(x) = \\{z_i|x_i \\in N_K(x)\\}\\). The output is the solution of the QCP solver, i.e., the esti-mated feature of test data. Since the attention block in the Transformer can learn the complex relation between tokens, we train a Transformer to align the inputs and outputs of the CIP, using the solutions from the QCP solver as the training data. We use the MSE loss to fit the QCP solver's solutions. The resulting Transformer is called CIP-net. Assume we have a training set S, here \\(N_K(x)\\) is the neighborhoods set in S excluded, then we can train the CIP-net by\n\\begin{equation}\n\\min_{\\Theta} \\sum_{x\\in S} || CIP-net(x, N_K(x), Z(x)) - QCP(x)||^2\\tag{19}\n\\end{equation}\nWhen the process of solving the CIP is replaced by CIP-net, the resulting OTAD-T-NN becomes differentiable. Then the gradient with respect to inputs can be used to construct adversarial examples. However, experiments show that OTAD-T-NN remains robust, indicating that the robustness of OTAD is not only due to gradient obfuscation. As a result of that, we need to understand the robustness of a Transformer block by bounding its Lipschitz constant.\nHowever, dot product self-attention has been proven to be not globally Lipschitz [54], which results in OTAD-T-NN not being globally Lipschitz. Nonetheless, adversarial robustness is more concerned with locally Lipschitz, meaning the Lips-chitz constant within the range of adversarial examples. We provide the upper bound for the local Lipschitz constant of dot-product multihead self-attention when the input is bounded.\nTheorem 1: Given a sequence \\(X_1,X_2,\\dots,X_N \\in \\mathbb{R}^D\\), the input \\(X = [X_1,X_2,\\dots,X_N]^T \\in \\mathbb{R}^{N\\times D}\\) is bounded by \\(||X||_F < M\\). For \\(1 < r < R\\), R is the number of head, let \\(Q^{(r)}, K^{(r)}, V^{(r)} \\in \\mathbb{R}^{D \\times D/R}\\), and \\(W \\in \\mathbb{R}^{D\\times D}\\), assume all parameters are bounded by\n\\begin{equation}\n\\max_{r=1,\\dots, R} \\{||Q^{(r)}||_F, ||K^{(r)}||_F, ||V^{(r)}||_F, ||W||_F\\} \\leq M_e.\n\\end{equation}\nThe multi-head self-attention F is defined by\n\\begin{equation}\nF(X) = [f^1(X),\\dots, f^R(X)]W,\n\\end{equation}\nwhere \\(f^{(r)}\\) is single-head dot product self attention defined by\n\\begin{equation}\nf^{(r)}(X) := \\textrm{softmax} \\left( \\frac{XQ^{(r)} (XK^{(r)})^T X V^{(r)}}{\\sqrt{D/R}} \\right).\n\\tag{20}\n\\end{equation}\nThen F with bounded input is Lipschitz with the following bound on \\(Lip_2(F)\\):\n\\begin{equation}\nLip_2(F) \\leq \\sqrt{R} M_e \\sqrt{M} \\frac{M^2(\\sqrt{N} + 1) + \\sqrt{N}}{\\sqrt{D/R}}.\n\\tag{21}\n\\end{equation}"}, {"title": "D. Neural network for CIP", "content": "Proof: Note that the local Lipschitz constant \\(Lip_2(f) = \\sup_{||X||_F\\leq M} ||J_f(X)||_2\\), we can bound \\(Lip_2"}]}