{"title": "SAM-SP: Self-Prompting Makes SAM Great Again", "authors": ["Chunpeng Zhou", "Kangjie Ning", "Qianqian Shen", "Sheng Zhou", "Zhi Yu", "Haishuai Wang"], "abstract": "The recently introduced Segment Anything Model (SAM), a\nVisual Foundation Model (VFM), has demonstrated impres-\nsive capabilities in zero-shot segmentation tasks across di-\nverse natural image datasets. Despite its success, SAM en-\ncounters noticeably performance degradation when applied\nto specific domains, such as medical images. Current efforts\nto address this issue have involved fine-tuning strategies, in-\ntended to bolster the generalizability of the vanilla SAM.\nHowever, these approaches still predominantly necessitate\nthe utilization of domain specific expert-level prompts during\nthe evaluation phase, which severely constrains the model's\npracticality. To overcome this limitation, we introduce a novel\nself-prompting based fine-tuning approach, called SAM-SP,\ntailored for extending the vanilla SAM model. Specifically,\nSAM-SP leverages the output from the previous iteration\nof the model itself as prompts to guide subsequent itera-\ntion of the model. This self-prompting module endeavors to\nlearn how to generate useful prompts autonomously and al-\nleviates the dependence on expert prompts during the eval-\nuation phase, significantly broadening SAM's applicability.\nAdditionally, we integrate a self-distillation module to en-\nhance the self-prompting process further. Extensive experi-\nments across various domain specific datasets validate the ef-\nfectiveness of the proposed SAM-SP. Our SAM-SP not only\nalleviates the reliance on expert prompts but also exhibits\nsuperior segmentation performance comparing to the state-\nof-the-art task-specific segmentation approaches, the vanilla\nSAM, and SAM-based approaches.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed the remarkable evolution of\nFoundation Models, deemed as a generalized AI paradigm\n(Bommasani et al. 2021). These Foundation models have\ndemonstrated exceptional performance across various real-\nworld downstream tasks with minimal human intervention.\nExamples include the renowned ChatGPT (Ouyang et al.\n2022), GPT-4 (Achiam et al. 2023), CLIP (Radford et al.\n2021), and BLIP (Li et al. 2022), etc., all of which achieved\nremarkable success with minimal human intervention. How-\never, these models were not specifically engineered for im-\nage segmentation (Minaee et al. 2021), an important com-\nputer vision task. The recent introduced Segment Anything\nModel (Kirillov et al. 2023) (SAM), a Foundation Visual\nModel for image segmentation, aims to address a range of\ndownstream segmentation tasks. Trained on the extensive\nSA-1B dataset (Kirillov et al. 2023), which encompasses\nover 1 billion masks from 11 million images, SAM gener-\nates high-quality object masks and has demonstrated distin-\nguished capabilities across diverse natural image datasets.\nNotably, SAM supports the flexible prompts from users, in-\ncluding point, box, and mask prompts. The zero-shot perfor-\nmance of SAM is impressive, often competitive with or even\nsuperior to prior fully supervised segmentation approaches\n(Kirillov et al. 2023) across various natural datasets.\nGiven these advancements, there is a natural inclination\nto apply SAM to specific domains, such as medical images\n(Litjens et al. 2017; Hesamian et al. 2019), remote sensing\nimages (Jiang et al. 2022; Shafique et al. 2022), and road\ndamage images (Yang et al. 2019; Fan et al. 2019), etc.\nFor instance, precise medical image segmentation is crucial\nfor various clinical applications, including disease diagno-\nsis, treatment planning, and disease progression monitoring.\nHowever, significant domain gaps exist between natural and"}, {"title": "", "content": "medical images, including blurred boundaries and low con-\ntrast in medical images. Experimental results from previous\nstudies (Ji et al. 2023; He et al. 2023; Zhou et al. 2023b;\nMa et al. 2024; Mattjie et al. 2023; Ning, Zhou, and Yu\n2023; Ahmadi et al. 2023) have indicates that directly ap-\nplying SAM to these domains usually leads to noticeable\nperformance degradation, with SAM significantly underper-\nforming compared to traditional task-specific medical image\nsegmentation approaches like U-net (Ronneberger, Fischer,\nand Brox 2015) and U-net++ (Zhou et al. 2020).\nTo address these challenges and broaden SAM's appli-\ncation range, recent approaches (Ma et al. 2024; Wu et al.\n2023) attempt to fine-tuning the vanilla SAM model with\ndomain-specific data. For example, MedSAM (Ma et al.\n2024) utilizes a simple fine-tuning method on a medical\nimage dataset to adapt SAM to medical image segmenta-\ntion tasks, while SAM-Med2D (Cheng et al. 2023) employs\nadaptation techniques (Chen et al. 2022) on the image en-\ncoder of SAM. Despite these improved performance in spe-\ncific domains compared to the vanilla SAM, they still heav-\nily rely on the precision of provided prompts during in-\nference (Wu et al. 2023) as shown in Fig 1. In fact, their\nperformance may degrade to the level of the vanilla SAM\nif the provided prompts are not sufficiently accurate (Wu\net al. 2023). Unfortunately, in contrast to natural images,\nthe availability of precise prompts is difficult for specific\ndomains, such as medical images, which heavily rely on\nexpert domain knowledge. This limitation may hinder the\nwidespread application of SAM. To abbreviate the need for\nexpert prompts, we pose a question: can we make SAM per-\nform well without any user prompts during inference? In-\nspired by the training strategy of SAM, where SA-1B (the\ntraining dataset of SAM) only includes automatically gener-\nated masks by SAM itself as ground truth (Kirillov et al.\n2023), we attempt to produce reliable prompts indepen-\ndently without relying on expert prompts during inference.\nConsequently, we introduce a SAM-based segmentation\nframework with a novel proposed Self-Prompting strategy,\ncalled SAM-SP, allowing the model to produce prompts\nautonomously, eliminating the need for user prompts dur-\ning inference. SAM-SP builds upon the basic framework\nof the vanilla SAM, which encodes the input image using\na ViT (Dosovitskiy et al. 2020) based image encoder. Fur-\nthermore, we employ a low-rank-based finetuning strategy\n(LoRA) (Hu et al. 2021), which allows SAM to better adapt\nto downstream tasks and makes training more efficient com-\npared to full fine-tuning. And then the mask decoder uti-\nlize the image embeddings obtained by the image encoder\nto produce initial predictions. To alleviate the dependence\non expert prompts during inference, we incorporate a Self-\nPrompting module to generate prompts by the model itself.\nSpecifically, the Self-Prompting module utilizes the first pre-\ndictions to generate prompts for guiding the subsequent it-\neration of the model. In next iteration, the prompt encoder\nencodes the generated prompts into prompting embeddings,\nwhich are then fed to the mask decoder to obtain refined pre-\ndictions. Though the supervised segmentation signals during\ntraining, SAM-SP learns to produce reliable and meaningful\nprompts by itself. Additionally, we utilize a segmentation"}, {"title": "", "content": "based self-distillation module to further enhance the training\nof the self-prompting module. We leverage the later predic-\ntions as a teacher to guide previous predictions in our self-\ndistillation module. SAM-SP gains mutual benefits from the\nself-distillation module. On the one hand, the final predic-\ntion provides additional supervision signals to guide the pre-\nvious prediction of SAM-SP. On the other hand, the previous\npredictions provide more accurate prompts for subsequent\npredictions. We conduct extensive experiments on diverse\nand publicly available datasets. The results indicate that our\nproposed SAM-SP achieves satisfactory performance with-\nout user prompts, compared to state-of-the-art task-specific\nsegmentation approaches, the vanilla SAM, and SAM-based\napproaches.\nOur contributions in this paper are summarized as fol-\nlows: (i) We highlight the significance of diminishing re-\nliance on expert prompts when deploying a visual founda-\ntion model in specific domains, such as the Segment Any-\nthing Model (SAM) in medical images. (ii) We introduce\nSAM-SP, a SAM-based framework which incorporates a\nnovel self-prompting module, extending the vanilla SAM\nto explore its capability across various downstream tasks,\nwithout relying on expert prompts during inference. We fur-\nther integrate a self-distillation module to enhance the self-\nprompting module. (iii) We build and release a novel and\nchallenging segmentation dataset Seg-GPR for SAM, aim-\ning for subgrade distress segmentation and collected by a 3-\nD ground penetrating radar. (iv) We conduct comprehensive\nevaluations of the proposed SAM-SP on diverse and publicly\navailable datasets, which demonstrate superior performance\nwithout using any user prompts, compared to state-of-the-\nart task-specific segmentation approaches, the vanilla SAM,\nand SAM-based approaches."}, {"title": "Related Work", "content": "Vision Foundation Model (LVM)\nRecent Large Language Model (Zhao et al. 2023a) like Chat-\nGPT (Ouyang et al. 2022), GPT-4 (Achiam et al. 2023)\nhave demonstrated exceptional performance in various NLP\ntasks. Similarly, Vision Foundation Models (LVM) have re-\nceived huge attention. For example, CLIP (Radford et al.\n2021) utilizes a contrastive learning strategy trained on\n400 million image-text pairs collected from the internet,\nachieving excellent performance in a wide variety of down-\nstream tasks. Inspired by pre-training strategies in NLP\n(Kenton and Toutanova 2019), MAE (He et al. 2022) em-\nploys an asymmetric encoder-decoder structure, aiming to\nreconstruct original images from random subsets of patches.\nExperimental results of MAE demonstrate the excellent\ntransfer performance in downstream tasks. In another line,\nDALLE (Ramesh et al. 2021) is built based on a large au-\ntoregressive transformer, designed to generate images from\ntextual descriptions. GPT-4 with Vision (GPT-4V) extends\nthe capabilities of GPT-4 by incorporating visual modalities,\nenabling users to analyze image inputs (Achiam et al. 2023).\nDistinct from previous VFMs, Segment Anything Model\n(Kirillov et al. 2023) (SAM) is the first VFM, specifically\ndesigned for segmentation tasks. Trained on the large SA-1B"}, {"title": "SAM and SAM-based approaches", "content": "dataset, SAM demonstrates exceptional zero-shot segmenta-\ntion performance in natural images.\nSAM and SAM-based approaches\nTo extend SAM's applicability to specialized domains, re-\ncent approaches have aimed to improve the vanilla SAM.\nNotably, MedSAM (Ma et al. 2024) is the first attempt to\nextend SAM to medical images, employing a simple fine-\ntuning method with a large collected medical image dataset.\nSAM-Med2D (Cheng et al. 2023) and SAM-Adapter (Chen\net al. 2023) both utilize adaptation techniques (Chen et al.\n2022) on the image encoder of SAM, instead of the sim-\nple fine-tuning. The Medical SAM Adapter (Wu et al.\n2023) (Med-SA) introduces Space-Depth Transpose (SD-\nTrans) and Hyper-Prompting Adapter (HyP-Adpt) modules\nto achieve prompt-conditioned adaptation for medical im-\nage segmentation. SAMed (Zhang and Liu 2023) applies\na LoRA-based (Hu et al. 2021) fine-tuning strategy to\nSAM's image encoder, jointly fine-tuning it together with\nthe prompt encoder and mask decoder of SAM without user\nprompts. SAMUS (Lin et al. 2023) extends SAM by incor-\nporating a parallel CNN branch, aiming to inject local fea-\ntures into the image encoder of SAM through cross-branch\nattention. The adapter technique (Chen et al. 2022) is also\nused in SAMUS. Though these SAM-based approaches im-\nprove the generalization abilities of SAM, they predomi-\nnantly ignore the difficulties of precise prompts in specific\ndomains, particularly in medical datasets. Consequently, we\nproposed a new framework SAM-SP which aims to pro-\nduce prompts automatically during inference. Additionally,\nwe highlight the difference between our approach and the\nrelated work by Wu, Zhang, and Elbatel, which introduces a\nsimple logistic regression as the Self-Prompt Unit. The unit\npredicts a coarse mask from the image embeddings, and then\nuses it to obtain the bounding box as the prompt. In con-\ntrast, our SAM-SP learns to generate the prompts from the\noutput of the SAM mask decoder, resulting in more precise\nprompts than those produced by simple logistic regression.\nFurthermore, we employ LoRA-based fine-tuning for better\nadaptation to downstream tasks, whereas only the logistic\nregression component is trainable in their framework (Wu,\nZhang, and Elbatel 2023), which may lead to limited gener-\nalization ability."}, {"title": "Recap of the Segment Anything Model", "content": "Before delving into the details of SAM-SP, we first review\nthe basic framework of the first visual segmentation foun-\ndation model Segment Anything Model (SAM) (Kirillov\net al. 2023), containing three main components: an image\nencoder, a prompt encoder, and a lightweight mask decoder.\nThe image Encoder leverages an MAE (He et al. 2022) pre-\ntrained Vision Transformer (ViT) (Dosovitskiy et al. 2020)\nto encoder the input images. The prompt encoder is designed\nto encoder the user-provided prompts, which supports mul-\ntiple types of prompts, including both sparse (points, boxes)\nand dense (masks) prompts. Then, the mask decoder com-\nbines the image and prompt embeddings to predict the target\nmask. In this paper, we use \\(E()\\), \\(P()\\) and \\(M()\\) to denote"}, {"title": "Methodology", "content": "image encoder, prompt encoder, and mask decoder, respec-\ntively. As discussed above, the vanilla SAM struggles in spe-\ncific domains and relies on the domain specific expert-level\nprompts during inference.\nMethodology\nOverview of Our Method\nAs illustrated in Figure 2, our proposed SAM-SP aims to\nenhance the segmentation capability of SAM in specific do-\nmains while reducing the reliance on expert prompts. SAM-\nSP is an end-to-end framework built on the vanilla SAM,\ncontaining three additional modules: LoRA-based fine-\ntuning, the Self-Prompt module and the Self-Distillation\nmodule.\nLORA-based Fine-tuning\nAs discussed above, significant domain gaps usually arise\nbetween downstream datasets (e.g. medical images) and nat-\nural images, for which the vanilla SAM is engineered. To\nmitigate these gaps and improve the generalization abili-\nties of SAM, fine-tuning has been widely adopted in pre-\nvious works (Ma et al. 2024; Cheng et al. 2023; Wu et al.\n2023; Zhang and Liu 2023; Lin et al. 2023). Instead of\nfine-tuning the all parameters of SAM, we freeze the im-\nage encoder, and only fine-tune the prompt encoder and\nmask decoder to minimize computational costs, given that\nthe image encoder dominates the majority (more than 95%)\ncomputational overhead in SAM. To further enhance the\nquality of image embeddings from the image encoder, we\nemploy a low-rank-based fine-tuning strategy (LoRA) (Hu\net al. 2021), which approximates the low rank update of\nthe parameters in the image encoder. LoRA enables SAM\nto adapt effectively to downstream tasks, while maintaining\ntraining efficiency compared to full fine-tuning. Addition-\nally, LoRA merges the trainable matrices with the frozen\nweights when deployed, introducing no additional inference\nlatency. For a pre-trained weight matrix \\(W \\in \\mathbb{R}^{d \\times k}\\) in\nthe SAM image encoder, we suppose its update as follows:\n\\[\n\\hat{W} = W + \\Delta W = W + BA,\n\\]\nwhere \\(\\hat{W} \\in \\mathbb{R}^{d \\times k}\\) denotes\nthe updated weight matrix, and a low-rank decomposition\n\\(\\Delta W = BA, A \\in \\mathbb{R}^{r \\times d},B\\in \\mathbb{R}^{d \\times r}\\) models this weight\nupdate. We set 4 as the default rank of the low-rank decom-\nposition in our implementation.\nSelf-Prompting Module\nAs depicted in Fig 2, we introduce a Plug-and-Play Self-\nPrompting module to alleviate the need of expert prompts.\nPrevious works typically simulate user prompt according the\nexpert annotations during the training stage. For example,\nMedSAM (Ma et al. 2024), Med-SA (Wu et al. 2023) and\nSAM-Med2D (Cheng et al. 2023) all employ the ground\ntruth prompts during both training and evaluation. However,\nthis may lead to models becoming overly reliant on provided\nprompts. To reduce this reliance and achieve training-test\nmatching, we do not use any user prompts during both train-\ning and evaluation. The prompt encoder in SAM can work\nwithout any prompt input, and will update a default embed-\nding during training (Kirillov et al. 2023). Formally, given"}, {"title": "", "content": "an image x, we first obtain the image embedding \\(E(x)\\) via\nthe image encoder \\(E()\\) in SAM with LoRA. The vanilla\nprediction \\(y_{0}\\) of our model without prompts is given by:\n\\[\n\\hat{y}_{0} = M(E(x), P(none))\n\\]\nTo further alleviate the need of expert prompts, our model\ngenerates the prompts by itself without manual input.\nSpecifically, we use the vanilla prediction \\(y_{0}\\) from the first\niteration to generate a prompt to guide the subsequent iter-\nation. And a box prompt is produced using the maximum\nand minimum coordinates of the predicted mask \\(y_{0}\\), and\nwe denote this self-prompting process as \\(SP()\\). Then, the\nprompt \\(SP(\\hat{y}_{0})\\) produced by the model itself is into the\nprompt encoder \\(P()\\), and the resulting self-prompt embed-\nding \\(P(SP(\\hat{y}_{0}))\\) combined with the image embedding \\(E(x)\\)\nis fed into the mask decoder \\(M()\\), formulated as:\n\\[\ny_{1} = M(E(x), P(SP(\\hat{y}_{0})))\n\\]\nwhere \\(y_{1}\\) denotes the predicted result from the self-\nprompting process. This module enables the model to learn\nproducing the prompts by itself, further reducing reliance on\nexpert prompts.\nNotably, the obtained image embeddings are used repeat-\nedly during the self-prompting process, without adding ob-\nvious additional computational costs. And the introduced\nself-prompting module does not introduce any learnable pa-\nrameters, keeping SAM-SP efficient.\nSelf-Distillation Module\nInspired by the recent progresses in Knowledge Distillation\n(Gou et al. 2021; Wang and Yoon 2021), we utilize a seg-\nmentation based self-distillation module to further enhance\nthe training of the self-prompting process. As mentioned\nearlier, we obtained the final predicted mask \\(y_{1}\\) from the\nself-prompting process. Additionally, we also obtain the the\nvanilla predictions \\(y_{0}\\) from our model. Specially, we employ\nthe final prediction \\(y_{1}\\) as the teacher to guide the previous\nprediction \\(y_{0}\\) as the student. In this self-distillation mod-\nule, SAM-SP gains mutual benefits from both the teacher"}, {"title": "", "content": "and student. The final prediction provides additional super-\nvisory signals to guide the previous prediction. Simultane-\nously, the previous predictions help to refine the subsequent\npredictions by providing more accurate prompts. The self-\ndistillation process is formulated as: \\(L_{SD} = KL(\\hat{y}_{0}, \\hat{y}_{1})\\),\nwhere \\(L_{SD}\\) denotes the knowledge distillation loss, with\nKullback-Leibler Divergence as our default implementation.\nNotably, the self-distillation module also does not intro-\nduce any learnable parameters and is deprecated during eval-\nuation, further keeping SAM-SP efficient.\nTraining and Evaluation\nThe overall training loss of SAM-SP is computed as:\n\\[\nL = Dice(\\hat{y}_{1}, y) + \\alpha L_{SD}\n\\]\nwhere y symbolizes the ground truth, \\(Dice()\\) denotes Dice\nloss, \\(\\alpha\\) serves as the weighting factor. Notably, our SAM-SP\ndoes not use any user-provided prompts during both training\nand inference.\nExperiments\nIn this section, we aim to validate the performance of our\nproposed SAM-SP model without the use of user-provided\nprompts during inference. To comprehensively assess the\neffectiveness of SAM-SP compared to state-of-the-art ap-\nproaches, we conduct extensive experiments across 10 pub-\nlicly available datasets.\nBenchmark Dataset\nMedical Image Segmentation. Medical image segmenta-\ntion is a critical and challenging task. In this evaluation,\nwe select Polyp Segmentation (Jha et al. 2020) and Skin\nLesion Segmentation (Berseth 2017) tasks. For Polyp Seg-\nmentation, we use 5 datasets: Kvasir-SEG (Jha et al. 2020),\nClinicDB (Bernal et al. 2015), ColonDB (Tajbakhsh, Gu-\nrudu, and Liang 2015), Endoscene (V\u00e1zquez et al. 2017),\nand ETIS-LaribDB (Silva et al. 2014). Following previous\nsettings (Fan et al. 2020), we adopt 900 and 548 images from"}, {"title": "", "content": "the ClinicDB and Kvasir datasets as the training set, and the\nremaining 64 and 100 images are employed as the respective\ntest sets. To evaluate the generalization performance, we test\nthe model on three unseen datasets: EndoScene, ColonDB\nand ETIS. For skin lesion segmentation, two skin lesion\ndatasets are used: ISIC 2017 (Berseth 2017) with 2050 der-\nmoscopy images, and ISIC 2018 (Codella et al. 2019) with\n2694 dermoscopy images. To ensure a fair comparison, we\nfollow the 7:3 train/test split strategy as outlined in (Ruan\net al. 2023).\nRemote Sensing Images. Remote sensing images (Zhang\net al. 2020) typically cover a wide scope with complex back-\ngrounds and diverse noise interference. ORSSD dataset (Li\net al. 2019) contains 600 images for training and the rest 200\nimages for testing. EORSSD dataset (Zhang et al. 2020) is\ndivided into two parts, where 1400 images for training and\n600 images for testing.\nDistress detection. Distress detection plays a crucial com-\nponent in transportation infrastructure. We build a novel sub-\ngrade distress detection dataset collected by an 3-Dimension\nground penetrating radar (GPR) which is wildly employed in\ninfrastructure health monitoring (Kim et al. 2021; Liu et al.\n2021; Zhou et al. 2023a). This GPR dataset, named seg-\nGPR, consists of 395 images, with an 80/20 split for training\nand testing.\nImplementation Details and Baselines\nWe compare our proposed SAM-SP to several baselines, in-\ncluding task-specific segmentation approaches, the vanilla\nSAM, and SAM-based approaches. We re-implemented all\nSAM-base approaches using their open-source code, in-\ncluding SAM-Med2D (Cheng et al. 2023), Medical SAM\nAdapter (Med-SA) (Wu et al. 2023) and SAMed (Zhang\nand Liu 2023). For a fair comparison, these approaches are\ntrained on the same training set of a dataset, and we uni-\nformly use the ViT-B as the image encoder for SAM and\nSAM-based approaches. Our paper mainly focus the down-\nstream performance without any use prompt. Consequently,\nno prompts are accessible for any approach during both\ntraining or inference periods, unless otherwise specified. We\nemploy the AdamW optimizer (Loshchilov and Hutter 2019)\nin all experiments. For the learning rates and weight decays,\nwe both make a gird search in the set {1e-2,1e-3,1e-4,1e-"}, {"title": "", "content": "5}. Similarly, we search for the optimal number of training\nepochs of different approaches in the range {100, 200, 300}.\nSpecially, we employ the warmup strategy (He et al. 2016)\nto stabilize the training process. Due our proposed SAM-\nSP is cost-effectiveness, SAM-SP can be run on a single\nNVIDIA RTX 3090 GPU. We use two wildly adopted evalu-\nation metrics: Dice Similarity Coefficient (DICE) and Inter-\nsection over Union (IoU), defined as: \\(Dice = \\frac{2TP}{FP+2TP+FN}\\)\nand \\(IOU = \\frac{TP}{TP+FP+FN}\\), respectively."}, {"title": "Main Results", "content": "Quantitative results. In Table 1, we summarize the quali-\ntative results of our SAM-SP compared to other methods on\nthe Polyp Segmentation task. Approaches in the top row are\nthe task-specific segmentation approaches, and approaches\nin the middle row are SAM and SAM-based approaches.\nSpecially, in our implementation, we provide user prompts\nfor the vanilla SAM considering that it is a zero-shot Seg-\nmentation, following previous works (Wu et al. 2023). Here,\nSAM denotes the SAM with point prompts provided dur-\ning inference, sampled within the ground-truth box. Simi-\nlarly, Table 2, 3 and 4 summarize the quantitative results of\nour SAM-SP with other methods on Skin lesion segmenta-\ntion, Remote Sensing Images Segmentation, Distress seg-\nmentation respectively. Firstly, observed from these quanti-\ntative results, our proposed SAM-SP achieve distinguished\nperformance compared to the vanilla SAM and a series of\nSAM-based approaches across most downstream segmenta-\ntion tasks, even without any prompts provided. For example,\nSAM-SP surpasses SAMMed2D with an obvious improve-\nment on CVC-300, Kvasir, ColonDB and ETIS-LaribDB"}, {"title": "", "content": "datasets in the Polyp Segmentation task. Similarly, SAM-\nSP outperform SAMed with an obvious improvements on\nboth the ISIC2018 and ISIC2017 datasets in the Skin lesion\nsegmentation task. The similar performance improvements\nalso be found on ORSSD, EORSSD and Seg-GPR Dataset.\nThese improvements can be attributed to the our proposed\nself-prompting and self-distillation modules in SAM-SP,\nwhich enable the model to produce prompts autonomously,\nthus reducing reliance of user prompts during inference.\nSecondly, unfortunately, the vanilla SAM struggles to seg-\nment these domain-specific targets, and its performance\nis significantly lower than other SAM-based approaches.\nThough we provide the ground-truth based point prompts for\nSAM during inference, SAM still can not achieve the satis-\nfied performances. Especially for our built Seg-GPR, it is a\nvery challenging for the vanilla SAM, while our SAM-SP\nperform well in this dataset. Thirdly, our quantitative results\nalso indicate that task-specific segmentation approaches still\nperform very competitively and, in some cases, still better\nthan SAM-based approaches. For instance, while our pro-\nposed SAM-SP surpasses the state-of-the-art task-specific\nsegmentation approaches in Skin lesion segmentation and\nRemote Sensing Images Segmentation tasks, it does not\noutperform the SOTA Polyp Segmentation tasks, such as\nSSFormerPVT and PolypPVT, which are specifically de-\nsigned for the Polyp Segmentation with sophisticated net-\nwork structures. How to allow Vision Foundation Models\nsurpass the all SOTA task-specific approaches deserves fur-\nther investigation.\nQualitative results. We showcase the qualitative results of\nour proposed SAM-SP in Figure 3 compared to the vanilla\nSAM and a SAM-based method SAMed. The top two rows\nshow samples from Polyp Segmentation dataset, the middle\ntwo rows from the Skin lesion segmentation dataset, and the\nbottom two from the Seg-GPR dataset. Ours-1 denotes the\nfirst prediction of SAM-SP, and Ours-final means the final\nprediction of SAM-SP. These results indicate that, due to\nthe absence of fine-tuning, the zero-shot SAM can not pro-\nduce reliable segmentation results. Though SAMed achieves\nbetter segmentation results compared to the vanilla SAM, it\nstill usually produce unclear boundary, as shown in 3(a). In\ncontrast, our SAM-SP utilize the proposed self-prompting\nprocess to produce prompts autonomously, thus exhibiting\nmore accurate predicted masks which closely resemble the\nground truths in these samples from different datasets. Spe-\ncially, we observe that our first output still performs compet-\nitively in some cases, which contributes to that our proposed"}, {"title": "", "content": "self-distillation module allow later prediction guide previous\nprediction. In summary, our proposed SAM-SP consistently\nperforms well and produce satisfactory and high-quality seg-\nmentation results across various datasets, without relying on\nthe user prompts during inference.\nAblation Studies\nAs described earlier, our proposed SAM-SP inherits the ba-\nsic structure of the vanilla SAM, and contains three addi-\ntional modules: LoRA-based fine tuning, the self-prompting\nmodule, and the self-distillation module. Consequently, we\ninvestigate the effectiveness of each module in SAM-SP and\nsummarize the experimental results in Table 5. These abla-\ntion results indicate that LoRA-based fine tuning (denoted as\nLORA) provides the first performance boost compared to the\nvanilla SAM. The further performance boost occurs when\nthe self-prompting module (denoted as SP) is introduced,\nwhich underscores its effectiveness. Finally, the employ-\nment of self-distillation module (denoted as KD) brings the\nadditional performance gains, based on previous modules.\nThe self-distillation module allows the previous and later\nprediction of SAM-SP gain benefits mutually. In summary,\nby coupling these three modules, SAM-SP achieves the sat-\nisfactory segmentation performance across various down-\nstream datasets without relying on user-provided prompts.\nModel Analysis\nSelf-Prompting Varieties In our implementation of\nSAM-SP, we typically conduct the self-prompting process\nonly once by default. Consequently, we investigate the im-\npacts with multiple self-prompting processes in our pro-\nposed SAM-SP, and summaries experimental results with\nvarying numbers of self-prompting processes in Figure 4.\nFirstly, observed from these results, the first self-prompting\nprocess brings the most significant performance boost com-\npared to the second and third. Secondly, the more times\nof self-prompting processes may yield diminishing returns\nwhich may due to the convergence of output results in our\nself-prompting module. Therefore, for a balance between\nperformance and efficiency, we default to a single self-\nprompting process in SAM-SP.\nTraining Varieties As described previously, our SAM-SP\nand other SAM-based approaches do not use any prompts\nduring both training and testing phases, achieving consis-\ntency between training and testing conditions. To investigate\nwhether the different prompt strategies during training af-\nfect the inference performance or not. Following previous\nworks (Ma et al. 2024; Cheng et al. 2023), we conduct eval-\nuations on Seg-GPR with different choices of prompt strate-\ngies during training, including: no prompt (None), Ground-\ntruth Random Point Prompt (Randompoint) and Ground-\ntruth Center Point Prompt (GTpoint). Notably, we still en-\nsure that no prompts are provided during inference. As sum-\nmarized in Fig 5, SAM-SP, along with other SAM-based ap-\nproaches, performs best when no prompts are used during\ntraining, compared to other training prompt strategies. Inter-\nestingly, although GTpoint provides more accurate ground-\ntruth information, it leads to worse inference performance."}, {"title": "", "content": "This counter-intuitive results may be due to that accurate\nprompts provided during training makes the model overly\nreliant on provided prompts, thus leading to significant per-\nformance degradation when no prompts are available during\ninference. Additionally, the use of GTpoint prompts severely\ndegrades the inference performance of SAM-Med2D. On\nthe contrary, our SAM-SP experience only minor perfor-\nmance changes with GTpoint prompts. This may be at-\ntributed to our proposed self-prompting module, which en-\ndeavor to produce reliable prompts autonomously, reducing\nreliance on externally provided prompts.\nConclusion\nIn this paper, we first emphasize the significance of reduc-\ning reliance on expert prompts when applying visual founda-\ntion models to specific domains, such as the Segment Any-\nthing Model (SAM), which broadens the application range\nof models. To alleviate the need for expert prompts in vari-\nous specific domains, we introduce a"}]}