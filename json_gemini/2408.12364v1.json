{"title": "SAM-SP: Self-Prompting Makes SAM Great Again", "authors": ["Chunpeng Zhou", "Kangjie Ning", "Qianqian Shen", "Sheng Zhou", "Zhi Yu", "Haishuai Wang"], "abstract": "The recently introduced Segment Anything Model (SAM), a Visual Foundation Model (VFM), has demonstrated impressive capabilities in zero-shot segmentation tasks across diverse natural image datasets. Despite its success, SAM encounters noticeably performance degradation when applied to specific domains, such as medical images. Current efforts to address this issue have involved fine-tuning strategies, intended to bolster the generalizability of the vanilla SAM. However, these approaches still predominantly necessitate the utilization of domain specific expert-level prompts during the evaluation phase, which severely constrains the model's practicality. To overcome this limitation, we introduce a novel self-prompting based fine-tuning approach, called SAM-SP, tailored for extending the vanilla SAM model. Specifically, SAM-SP leverages the output from the previous iteration of the model itself as prompts to guide subsequent iteration of the model. This self-prompting module endeavors to learn how to generate useful prompts autonomously and alleviates the dependence on expert prompts during the evaluation phase, significantly broadening SAM's applicability. Additionally, we integrate a self-distillation module to enhance the self-prompting process further. Extensive experiments across various domain specific datasets validate the effectiveness of the proposed SAM-SP. Our SAM-SP not only alleviates the reliance on expert prompts but also exhibits superior segmentation performance comparing to the state-of-the-art task-specific segmentation approaches, the vanilla SAM, and SAM-based approaches.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed the remarkable evolution of Foundation Models, deemed as a generalized AI paradigm (Bommasani et al. 2021). These Foundation models have demonstrated exceptional performance across various real-world downstream tasks with minimal human intervention. Examples include the renowned ChatGPT (Ouyang et al. 2022), GPT-4 (Achiam et al. 2023), CLIP (Radford et al. 2021), and BLIP (Li et al. 2022), etc., all of which achieved remarkable success with minimal human intervention. However, these models were not specifically engineered for image segmentation (Minaee et al. 2021), an important computer vision task. The recent introduced Segment Anything"}, {"title": "Related Work", "content": "Vision Foundation Model (LVM)\nRecent Large Language Model (Zhao et al. 2023a) like Chat-GPT (Ouyang et al. 2022), GPT-4 (Achiam et al. 2023) have demonstrated exceptional performance in various NLP tasks. Similarly, Vision Foundation Models (LVM) have received huge attention. For example, CLIP (Radford et al. 2021) utilizes a contrastive learning strategy trained on 400 million image-text pairs collected from the internet, achieving excellent performance in a wide variety of downstream tasks. Inspired by pre-training strategies in NLP (Kenton and Toutanova 2019), MAE (He et al. 2022) employs an asymmetric encoder-decoder structure, aiming to reconstruct original images from random subsets of patches. Experimental results of MAE demonstrate the excellent transfer performance in downstream tasks. In another line, DALLE (Ramesh et al. 2021) is built based on a large autoregressive transformer, designed to generate images from textual descriptions. GPT-4 with Vision (GPT-4V) extends the capabilities of GPT-4 by incorporating visual modalities, enabling users to analyze image inputs (Achiam et al. 2023). Distinct from previous VFMs, Segment Anything Model (Kirillov et al. 2023) (SAM) is the first VFM, specifically designed for segmentation tasks. Trained on the large SA-1B"}, {"title": "SAM and SAM-based approaches", "content": "To extend SAM's applicability to specialized domains, recent approaches have aimed to improve the vanilla SAM. Notably, MedSAM (Ma et al. 2024) is the first attempt to extend SAM to medical images, employing a simple fine-tuning method with a large collected medical image dataset. SAM-Med2D (Cheng et al. 2023) and SAM-Adapter (Chen et al. 2023) both utilize adaptation techniques (Chen et al. 2022) on the image encoder of SAM, instead of the simple fine-tuning. The Medical SAM Adapter (Wu et al. 2023) (Med-SA) introduces Space-Depth Transpose (SD-Trans) and Hyper-Prompting Adapter (HyP-Adpt) modules to achieve prompt-conditioned adaptation for medical image segmentation. SAMed (Zhang and Liu 2023) applies a LoRA-based (Hu et al. 2021) fine-tuning strategy to SAM's image encoder, jointly fine-tuning it together with the prompt encoder and mask decoder of SAM without user prompts. SAMUS (Lin et al. 2023) extends SAM by incorporating a parallel CNN branch, aiming to inject local features into the image encoder of SAM through cross-branch attention. The adapter technique (Chen et al. 2022) is also used in SAMUS. Though these SAM-based approaches improve the generalization abilities of SAM, they predominantly ignore the difficulties of precise prompts in specific domains, particularly in medical datasets. Consequently, we proposed a new framework SAM-SP which aims to produce prompts automatically during inference. Additionally, we highlight the difference between our approach and the related work by Wu, Zhang, and Elbatel, which introduces a simple logistic regression as the Self-Prompt Unit. The unit predicts a coarse mask from the image embeddings, and then uses it to obtain the bounding box as the prompt. In contrast, our SAM-SP learns to generate the prompts from the output of the SAM mask decoder, resulting in more precise prompts than those produced by simple logistic regression. Furthermore, we employ LoRA-based fine-tuning for better adaptation to downstream tasks, whereas only the logistic regression component is trainable in their framework (Wu, Zhang, and Elbatel 2023), which may lead to limited generalization ability."}, {"title": "Recap of the Segment Anything Model", "content": "Before delving into the details of SAM-SP, we first review the basic framework of the first visual segmentation foundation model Segment Anything Model (SAM) (Kirillov et al. 2023), containing three main components: an image encoder, a prompt encoder, and a lightweight mask decoder. The image Encoder leverages an MAE (He et al. 2022) pre-trained Vision Transformer (ViT) (Dosovitskiy et al. 2020) to encoder the input images. The prompt encoder is designed to encoder the user-provided prompts, which supports multiple types of prompts, including both sparse (points, boxes) and dense (masks) prompts. Then, the mask decoder combines the image and prompt embeddings to predict the target mask. In this paper, we use $E()$, $P()$ and $M()$ to denote"}, {"title": "Methodology", "content": "Overview of Our Method\nAs illustrated in Figure 2, our proposed SAM-SP aims to enhance the segmentation capability of SAM in specific domains while reducing the reliance on expert prompts. SAM-SP is an end-to-end framework built on the vanilla SAM, containing three additional modules: LoRA-based fine-tuning, the Self-Prompt module and the Self-Distillation module."}, {"title": "LORA-based Fine-tuning", "content": "As discussed above, significant domain gaps usually arise between downstream datasets (e.g. medical images) and natural images, for which the vanilla SAM is engineered. To mitigate these gaps and improve the generalization abilities of SAM, fine-tuning has been widely adopted in previous works (Ma et al. 2024; Cheng et al. 2023; Wu et al. 2023; Zhang and Liu 2023; Lin et al. 2023). Instead of fine-tuning the all parameters of SAM, we freeze the image encoder, and only fine-tune the prompt encoder and mask decoder to minimize computational costs, given that the image encoder dominates the majority (more than 95%) computational overhead in SAM. To further enhance the quality of image embeddings from the image encoder, we employ a low-rank-based fine-tuning strategy (LoRA) (Hu et al. 2021), which approximates the low rank update of the parameters in the image encoder. LoRA enables SAM to adapt effectively to downstream tasks, while maintaining training efficiency compared to full fine-tuning. Additionally, LoRA merges the trainable matrices with the frozen weights when deployed, introducing no additional inference latency. For a pre-trained weight matrix $W \\in R^{d \\times k}$ in the SAM image encoder, we suppose its update as follows: $\\widehat{W} = W + \\Delta W = W + BA$, where $\\widehat{W} \\in R^{d \\times k}$ denotes the updated weight matrix, and a low-rank decomposition $\\Delta W = BA$, $A \\in R^{r \\times d}$,$B\\in R^{d \\times r}$ models this weight update. We set 4 as the default rank of the low-rank decomposition in our implementation."}, {"title": "Self-Prompting Module", "content": "As depicted in Fig 2, we introduce a Plug-and-Play Self-Prompting module to alleviate the need of expert prompts. Previous works typically simulate user prompt according the expert annotations during the training stage. For example, MedSAM (Ma et al. 2024), Med-SA (Wu et al. 2023) and SAM-Med2D (Cheng et al. 2023) all employ the ground truth prompts during both training and evaluation. However, this may lead to models becoming overly reliant on provided prompts. To reduce this reliance and achieve training-test matching, we do not use any user prompts during both training and evaluation. The prompt encoder in SAM can work without any prompt input, and will update a default embedding during training (Kirillov et al. 2023). Formally, given"}, {"title": "Self-Distillation Module", "content": "Inspired by the recent progresses in Knowledge Distillation (Gou et al. 2021; Wang and Yoon 2021), we utilize a segmentation based self-distillation module to further enhance the training of the self-prompting process. As mentioned earlier, we obtained the final predicted mask $y_1$ from the self-prompting process. Additionally, we also obtain the the vanilla predictions $y_0$ from our model. Specially, we employ the final prediction $y_1$ as the teacher to guide the previous prediction $y_0$ as the student. In this self-distillation module, SAM-SP gains mutual benefits from both the teacher"}, {"title": "Training and Evaluation", "content": "The overall training loss of SAM-SP is computed as:\n$\\mathcal{L} = Dice(\\widehat{y}_1, y) + \\alpha \\mathcal{L}_{SD}$ (3)\nwhere $y$ symbolizes the ground truth, $Dice()$ denotes Dice loss, \u03b1 serves as the weighting factor. Notably, our SAM-SP does not use any user-provided prompts during both training and inference."}, {"title": "Experiments", "content": "In this section, we aim to validate the performance of our proposed SAM-SP model without the use of user-provided prompts during inference. To comprehensively assess the effectiveness of SAM-SP compared to state-of-the-art approaches, we conduct extensive experiments across 10 publicly available datasets."}, {"title": "Benchmark Dataset", "content": "Medical Image Segmentation. Medical image segmentation is a critical and challenging task. In this evaluation, we select Polyp Segmentation (Jha et al. 2020) and Skin Lesion Segmentation (Berseth 2017) tasks. For Polyp Segmentation, we use 5 datasets: Kvasir-SEG (Jha et al. 2020), ClinicDB (Bernal et al. 2015), ColonDB (Tajbakhsh, Gurudu, and Liang 2015), Endoscene (V\u00e1zquez et al. 2017), and ETIS-LaribDB (Silva et al. 2014). Following previous settings (Fan et al. 2020), we adopt 900 and 548 images from"}, {"title": "Implementation Details and Baselines", "content": "We compare our proposed SAM-SP to several baselines, including task-specific segmentation approaches, the vanilla SAM, and SAM-based approaches. We re-implemented all SAM-base approaches using their open-source code, including SAM-Med2D (Cheng et al. 2023), Medical SAM Adapter (Med-SA) (Wu et al. 2023) and SAMed (Zhang and Liu 2023). For a fair comparison, these approaches are trained on the same training set of a dataset, and we uniformly use the ViT-B as the image encoder for SAM and SAM-based approaches. Our paper mainly focus the downstream performance without any use prompt. Consequently, no prompts are accessible for any approach during both training or inference periods, unless otherwise specified. We employ the AdamW optimizer (Loshchilov and Hutter 2019) in all experiments. For the learning rates and weight decays, we both make a gird search in the set {1e-2,1e-3,1e-4,1e-5}. Similarly, we search for the optimal number of training epochs of different approaches in the range {100, 200, 300}. Specially, we employ the warmup strategy (He et al. 2016) to stabilize the training process. Due our proposed SAM-SP is cost-effectiveness, SAM-SP can be run on a single NVIDIA RTX 3090 GPU. We use two wildly adopted evaluation metrics: Dice Similarity Coefficient (DICE) and Intersection over Union (IoU), defined as: $Dice = \\frac{2TP}{FP+2TP+FN}$ and $IOU = \\frac{TP}{TP+FP+FN}$, respectively."}, {"title": "Main Results", "content": "Quantitative results. In Table 1, we summarize the qualitative results of our SAM-SP compared to other methods on the Polyp Segmentation task. Approaches in the top row are the task-specific segmentation approaches, and approaches in the middle row are SAM and SAM-based approaches. Specially, in our implementation, we provide user prompts for the vanilla SAM considering that it is a zero-shot Segmentation, following previous works (Wu et al. 2023). Here, SAM denotes the SAM with point prompts provided during inference, sampled within the ground-truth box. Similarly, Table 2, 3 and 4 summarize the quantitative results of our SAM-SP with other methods on Skin lesion segmentation, Remote Sensing Images Segmentation, Distress segmentation respectively. Firstly, observed from these quantitative results, our proposed SAM-SP achieve distinguished performance compared to the vanilla SAM and a series of SAM-based approaches across most downstream segmentation tasks, even without any prompts provided. For example, SAM-SP surpasses SAMMed2D with an obvious improvement on CVC-300, Kvasir, ColonDB and ETIS-LaribDB"}, {"title": "Ablation Studies", "content": "As described earlier, our proposed SAM-SP inherits the basic structure of the vanilla SAM, and contains three additional modules: LoRA-based fine tuning, the self-prompting module, and the self-distillation module. Consequently, we investigate the effectiveness of each module in SAM-SP and summarize the experimental results in Table 5. These ablation results indicate that LoRA-based fine tuning (denoted as LORA) provides the first performance boost compared to the vanilla SAM. The further performance boost occurs when the self-prompting module (denoted as SP) is introduced, which underscores its effectiveness. Finally, the employment of self-distillation module (denoted as KD) brings the additional performance gains, based on previous modules. The self-distillation module allows the previous and later prediction of SAM-SP gain benefits mutually. In summary, by coupling these three modules, SAM-SP achieves the satisfactory segmentation performance across various downstream datasets without relying on user-provided prompts."}, {"title": "Model Analysis", "content": "Self-Prompting Varieties In our implementation of SAM-SP, we typically conduct the self-prompting process only once by default. Consequently, we investigate the impacts with multiple self-prompting processes in our proposed SAM-SP, and summaries experimental results with varying numbers of self-prompting processes in Figure 4. Firstly, observed from these results, the first self-prompting process brings the most significant performance boost compared to the second and third. Secondly, the more times of self-prompting processes may yield diminishing returns which may due to the convergence of output results in our self-prompting module. Therefore, for a balance between performance and efficiency, we default to a single self-prompting process in SAM-SP.\nTraining Varieties As described previously, our SAM-SP and other SAM-based approaches do not use any prompts during both training and testing phases, achieving consistency between training and testing conditions. To investigate whether the different prompt strategies during training affect the inference performance or not. Following previous works (Ma et al. 2024; Cheng et al. 2023), we conduct evaluations on Seg-GPR with different choices of prompt strategies during training, including: no prompt (None), Ground-truth Random Point Prompt (Randompoint) and Ground-truth Center Point Prompt (GTpoint). Notably, we still ensure that no prompts are provided during inference. As summarized in Fig 5, SAM-SP, along with other SAM-based approaches, performs best when no prompts are used during training, compared to other training prompt strategies. Interestingly, although GTpoint provides more accurate ground-truth information, it leads to worse inference performance."}, {"title": "Conclusion", "content": "In this paper, we first emphasize the significance of reducing reliance on expert prompts when applying visual foundation models to specific domains, such as the Segment Anything Model (SAM), which broadens the application range of models. To alleviate the need for expert prompts in various specific domains, we introduce a novel self-prompting based fine-tuning approach SAM-SP, tailored for extending the capabilities of the vanilla SAM model. Specifically, SAM-SP leverages its own output from the previous iteration as prompts to guide the subsequent iteration of the model. This self-prompting module endeavors to learn how to generate effective prompts autonomously and alleviates the dependency on expert prompts during evaluation, significantly broadening SAM's applicability. Additionally, we integrate a self-distillation module to further enhance the self-prompting process. Extensive experiments across vari-"}]}