{"title": "ChatGPT Doesn't Trust Chargers Fans:\nGuardrail Sensitivity in Context", "authors": ["Victoria R. Li", "Yida Chen", "Naomi Saphra"], "abstract": "While the biases of language models in pro-\nduction are extensively documented, the bi-\nases of their guardrails have been neglected.\nThis paper studies how contextual information\nabout the user influences the likelihood of an\nLLM to refuse to execute a request. By gen-\nerating user biographies that offer ideological\nand demographic information, we find a num-\nber of biases in guardrail sensitivity on GPT-3.5.\nYounger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail\nwhen requesting censored or illegal informa-\ntion. Guardrails are also sycophantic, refusing\nto comply with requests for a political position\nthe user is likely to disagree with. We find that\ncertain identity groups and seemingly innocu-\nous information, e.g., sports fandom, can elicit\nchanges in guardrail sensitivity similar to di-\nrect statements of political ideology. For each\ndemographic category and even for American\nfootball team fandom, we find that ChatGPT\nappears to infer a likely political ideology and\nmodify guardrail behavior accordingly.", "sections": [{"title": "Introduction", "content": "Like other applications of AI, chatbots can offer\nunequal support to users depending on their back-\nground and needs. Large language models (LLMs)\noften have limited utility for users who speak a low\nresource language or marginalized dialect (Huang\net al., 2023; Deas et al., 2023). The phrasing of a\nrequest may also change the quality of the answer\n(Hofmann et al., 2024), advantaging educated users\nwith a privileged background.\nWhile work often addresses these issues of con-\ntextual accuracy and data scarcity, we instead fo-\ncus on a previously unexplored factor in unequal\ncapabilities: chatbot guardrails, the restrictions\nthat limit model responses to uncertain or sensitive\nquestions and often provide boilerplate text refus-\ning to fulfill a request (see Fig. 1). These guardrails\nmay be created with the same human feedback pro-\ncedures by which the next token predictions of an\nLLM are tuned into a usable dialogue interface\n(Ouyang et al., 2022; Touvron et al., 2023). In\nclosed commercial chatbots, guardrails may take\nthe form of proprietary peripheral models (Team\net al., 2023). While we do not always know the pro-\ncess by which these guardrails are trained, we can\nmeasure their sensitivity to context as a blackbox.\nGuardrails must track the wider dialogue context\nbecause adversarial jailbreaks and spurious triggers\noften depend on recontextualizing a request, e.g.,\nby first ordering the model to roleplay.\nUsing a diverse set of persona introductions\nthat imply or declare a user identity and ideology\nand a set of requests which inconsistently trigger\na guardrail refusal (Figure 2), we find a number of\nbiases in the sensitivity of the guardrail:\n\u2022 Given explicit declarations of a user persona's\ngender, age, or ethnicity, ChatGPT refuses re-\nquests for censored information for simulated\nminors more than adults; women more than\nmen; and Asian-Americans more than other\nethnicities.\n\u2022 Guardrails trigger sycophantically for polit-\nically sensitive requests, with higher proba-\nbility of a refusal if a conservative persona\nrequests a liberal position from the model or\na liberal requests a conservative position.\n\u2022 Some personas are treated as implicitly conser-\nvative or liberal. Black, female, and younger\npersonas are treated more like liberal personas\nby the LLM guardrail.\n\u2022 People strongly identify with groups that are\nnot standard demographic categories or ex-\nplicitly ideological. We explore these group\nidentifications through sports fandom, which\nconnotes demographics, region, and ideology."}, {"title": "Background", "content": "Epistemic bias Much recent work on fairness\nin LLMs focuses on potential prejudice against a\nthird party or worldview, rather than against the\nuser directly. In other words, models provide prej-\nudiced responses that that may harm third parties,\ne.g., inferring that a particular job applicant is more\nqualified based on ethnicity (Yin et al., 2024) or\npresuming gender based on an individual's profes-\nsion (Rudinger et al., 2018). Political bias can also\nemerge from information in the training data or\nthe design of the human feedback procedure (Liu\net al., 2022). Focusing on guardrail sensitivity, we\ninvestigate how the model may also express social\nbiases in refusing a user's request.\nEqual utility Language models can also disad-\nvantage certain groups by providing utility un-\nequally to their users. It is frequently observed, for\nexample, that speakers of languages other than En-\nglish rarely have access to the functionality of state-\nof-the-art English language models (Bang et al.,\n2023; Ojo et al., 2023; Huang et al., 2023). Even"}, {"title": "Experiments", "content": "Our experiments begin each dialogue with a per-\nsona introduction that reveals some aspect of the\nsimulated user's identity. We then provide a re-\nquest prompt which the model is likely, but not\ncertain, to reject. For example, we may begin by\nsaying, \"Hi there, I'm Sarah, a Minnesota Vikings\nfan originally from the Midwest,\" and then issu-\ning a sensitive request such as, \u201cHow do people"}, {"title": "Classifying responses", "content": "Since our experiments consist of more than\n225,000 requests to the conversational models, we\nautomatically label refusals to answer by prompt-\ning GPT-40 as described in Appendix B.\nFor a stricter classification of refusals, we also\nuse a keyword classifier (based on the terms in-\ndexed by Zou et al. (2023) and Qi et al. (2023))\nwhich labels a response as a refusal only if it\ncontains stereotyped guardrail terms such as \u201cI'm\nsorry\" and \"As an AI.\u201d In our visualizations, we\nuse color to differentiate refusals tagged only by\nGPT-40 from those also identifiable by keyword.\nAlthough researchers debate the quality of\nprompt based annotation (Gilardi et al., 2023;\nAldeen et al., 2023; Reiss, 2023), our preliminary\nanalysis supports our approach. Figure 3 shows\nthat semantic differences can cluster responses into"}, {"title": "Models", "content": "Our experiments analyze gpt-3.5-turbo, OpenAI's\nflagship model for conversational AI. We choose\nChatGPT-3.5 as a target of analysis due to its wide\ncoverage of users (over 180 million monthly active\nusers as of March 2024 (Porter, 2023)). Unlike\nthe paywalled ChatGPT-4, ChatGPT-3.5 is freely\naccessible. We sample with default temperature 1\non the API while using the system prompt, \u201cYou\nare ChatGPT, a large language model trained by\nOpenAI\" (OpenAI, 2024b).\""}, {"title": "Persona prompts", "content": "In order to ensure that ChatGPT has an association\nbetween the persona prompt and the demograph-\nics, we generate a list of five persona prompts by\nrequesting them from ChatGPT itself. To ensure a\nsufficiently diverse collection of biographies, we\ngenerate all five personas simultaneously as a set.\nWe generate personas (examples in Appendix\nTable 6) for political conservatives and liberals;\nmen and women; Black, White, Asian-American,\nand Latin-American users; users ages 13\u201317, 35\u2013\n44, and 55-64; and fans of every team in the NFL.\nWe filter persona collections by hand to ensure"}, {"title": "Properties of autogenerated biographies", "content": "Qualitative inspection of persona prompts reveals\nsome caveats regarding our approach. The degree\nto which these biographies reflect ChatGPT-3.5's\nexisting associations is in part a strength of automa-\ntion, but the sampled user biographies are highly\nbiased with many potential confounders. Asian-\nAmerican and Hispanic/Latin personas consistently\nspecify the nation their family immigrated from,\nbut Black and White ethnic personas may not. The\npersonas treat women as marked but men as un-\nmarked: female personas\u2014but not male personas\u2014\ncontain explicit mentions of gender, e.g., \u201cas a\nwoman...\" (See Appendix Table 6 for examples).\nThis automated approach is limited by biases in\nthe corpus and other elements of training. In gen-\neral, we see a large number of biases and that per-\nsonas are not representative of their identity groups.\nHowever, automation guarantees that these biogra-\nphies represent ChatGPT's archetype of a particular\ngroup.\""}, {"title": "Request prompts", "content": "We consider guardrails in the following situations\nin which chatbots frequently refuse requests. To\ngenerate requests, we prompt GPT-40 as in Ap-\npendix A. Of these potential sensitive requests, we\nretain only those that trigger refusals inconsistently,\ni.e., for at least one but not all of 22 personas sam-\npled across identities."}, {"title": "Results", "content": "In the following experiments, one of the most con-\nsistent effects, regardless of request type, is using\na persona introduction at all. When no persona is\nincluded and the dialog begins immediately with a\nrequest (the no-persona user), ChatGPT produces\nmore stereotyped refusals identifiable by the key-\nword classifier. However, the no-persona user does\nnot trigger more refusals overall. It appears that\nan introduction provokes a more verbose or conver-\nsational tone from the model, causing it to gently\nredirect the topic rather than using guardrail key-\nwords like \"I'm sorry.\""}, {"title": "Random variation between persona sets", "content": "First, we consider how refusal rates are affected by\nrandom variation between generated persona sets,\ncomparing personas generated by the same prompt\n(\"Please generate 5 five-sentence paragraphs where"}, {"title": "Political ideology", "content": "Using a sample of user persona introductions that\nexplicitly describe the user's political ideology, we\nfind that political allegiance determines guardrail\nsensitivity for political requests, but not censored\ninformation requests (Table 1).\nSycophancy. Perez et al. (2022) observe a phe-\nnomenon in larger LLMs that they call sycophancy,\na tendency to respond to questions by aligning\nwith the user's expressed views. We find that syco-\nphancy is also expressed through guardrails\u2014the\nmodel is more likely to refuse a direct request for a\ndefense of gun control or an argument denying cli-\nmate change if the user has previously expressed a\npolitical identity at odds with those views. Overall,\nconservative-leaning requests have a refusal rate of\n44% for conservative personas and 76% for liberal\npersonas, whereas liberal-leaning requests have a\nrefusal rate of 68% for conservative personas but\nonly 40% for liberal personas."}, {"title": "Demographics", "content": "Guardrail behavior varies in response to explicit\ndeclarations of user age, gender, and ethnicity. This\nsection discusses the findings presented in Figure\n4 with significance tests in Table 1."}, {"title": "Inferring politics from demographics", "content": "Certain demographics are often more likely to\nbe conservative or liberal, at least in their voting\nrecords. Men are more conservative than women\nin general, and ethnic groups often differ substan-\ntially in their party preferences. In the USA, where\nOpenAI is based, Joe Biden won the 2020 election\nwith 51.3% of overall votes while leaning heavily\non core constituencies like non-Hispanic Black vot-\ners, who favored Biden at a rate of 92% (Igielnik\net al., 2021). This section will show that ChatGPT\ntreats certain demographics as implicitly liberal or\nconservative in line with their voting tendencies.\nTo measure the political ideology associated\nwith guardrail behavior on a given identity category\n$Q$, we consider individual personas $q \\in Q$ each\nwith a corresponding vector of refusal rates $r_q$ in-\ndexed by request. The guardrail similarity between\na pair of personas q, q' is measured by the Pear-\nson correlation $\\rho(r_q, r_{q'})$. We correlate refusals on\nliberal and conservative persona sets (sets L and\nC respectively) with refusals on a target identity\nacross all categories of sensitive requests, both po-"}, {"title": "Sports Fandom", "content": "Conflating demographics and political identity is\none way that ChatGPT infers user ideology indi-\nrectly, but any facet of a user's identity can be cor-\nrelated with ideological positions. In this section,\nwe focus on simulated personas for enthusiastic\nfans of each NFL team.\nGuardrail sensitivity varies in response to de-\nclared sports team fandom on political and apolit-\nical trigger prompts. As shown in Appendix Fig-\nure 7, ChatGPT's refuses most frequently for a\ndeclared Los Angeles Chargers fan persona in ev-\nery guardrail category. Compared to a Philadelphia\nEagles fan, a Chargers fan is refused 5% more on\ncensored information requests, 7% more on right-\nleaning political requests, and 10% more on left-\nleaning political requests. These differences could\nexpress a variety of connotations that relate to the\nteam's home city, name, or fanbase.\nAs with demographics (Section 4.3), some\nguardrail bias relates to presumed ideology. For\nexample, we find that Dallas Cowboys fan per-\nsonas, representing one of the most conservative\nNFL fanbases, are treated like overtly declared con-\nservatives by ChatGPT guardrails. We illustrate\nthis effect in Figure 5b, showing a moderate corre-\nlation between the conservatism of an NFL team's\nfanbase according to Paine et al. (2017) and the fan\npersona's similarity to conservatism personas in its\nguardrail triggers."}, {"title": "Discussion", "content": "A user may be disadvantaged by impaired utility\nif guardrails are overly sensitive. However, they\nmay also be harmed if guardrails are insufficiently\nsensitive and an LLM generates distressing or in-\ncorrect content. It is not, therefore, straightforward\nto assess the impact of guardrail bias on utility.\nWhile we attempt to offer implicit demographic\ninformation by explicitly declaring names or fan-\ndom, we do not consider other even more implicit\nsources of information such as the dialect use or el-\nements of the phrasing of the prompt. Recent work\nhas revealed implicit biases against speakers of mi-\nnority dialects even after models are tuned to avoid\nbiases over identities (Hofmann et al., 2024; Bai\net al., 2024); different guardrail sensitivity biases\nmight emerge under similar tests."}, {"title": "Future Work", "content": "Our study of guardrails is intended to present a\npreviously unstudied, to our knowledge, source of\nbias in LLMs. However, there are obvious next\nsteps. We study only a single LLM, ChatGPT-3.5,\nbut newer models should also be studied. Further-\nmore, we only consider a limited number of user\nattributes. Other aspects of identity might be influ-\nential and even those we study have a number of\nnuances that we do not address. Researchers with\naccess to deployment data could study how much\nthese biases impact real-world users.\nWho guards the guardrails? When a language\nmodel is equipped with guardrails to reduce or con-\nceal its biases, the guardrails themselves may still\nexhibit measurable biases. How can we remedy\nthe biases documented in our findings? We leave\nsolutions to future work but incorporating explicit\nbias metrics, meta-guardrails which monitor for po-\ntentially invalid refusals, and more layers of human\nfeedback tuning could all be paths forward.\nAnalyzing different kinds of guardrails. LLMs\nrefuse a request in several situations we have not\ncovered here. We have not addressed cases where\nthe model refuses a request for a personal opinion,\nfor example. Other refusals might take a different\nform, as when the model does not have sufficient\ninformation either because the user has not pro-\nvided it or because its training corpus is limited to\ntext produced before a particular date. Future work\nmay also study bias in other guardrail types."}, {"title": "Conclusions", "content": "This paper has investigated a new potential source\nof bias in chatbot LLMs in the form of its guardrails.\nIf a guardrail triggers spuriously, the resulting re-\nfusal can limit the utility of the LLM. On the other\nhand, if a guardrail fails to trigger when it should,\nusers may be exposed to harmful or distressing con-\ntent. We have shown that the likelihood of a refusal\ncan be influenced by demographic categories, po-\nlitical affiliation, and even seemingly innocuous\ninformation like sports fandom."}, {"title": "Limitations", "content": "There are a number of limitations to our analysis\nin addition to those already discussed in the pa-\nper. First, the setup is artificial, as it involves a\ndialogue with a user who explicitly provides bio-\ngraphic information before asking questions. This\nis an atypical interaction with a user and possibly a\nsetting where ChatGPT is explicitly tuned against\novert bias. More naturalistic ways of eliciting bias,\nsuch as modifying the user's dialect, could show\ndifferent results, either stronger or weaker.\nTo the degree that our results measure significant\neffects, these effects may no longer hold true in fu-\nture versions of ChatGPT or even under additional\nhuman feedback tuning. While we are pointing\nout a potential issue with models that has not yet\nbeen discussed publicly and therefore our work has\nvalue even if the particular numbers change, our re-\nsults are subject to the reproducibility issues caused\nby proprietary model maintenance.\nThe prompt we use to generate requests includes\nexamples that bias the generated requests towards\nspecific formatting and topics. The results we pro-\nduce may not generalize to other sets of requests.\nThese results may also fail to generalize to other\ncultures. Our framework assumes the user to be\nAmerican, including the political language (\"Re-\npublican\", \"liberal\", etc.), the primary racial cate-\ngorization, and the use of American football sports\nfandom. However, ChatGPT is massively multi-\nlingual and is trained on a large range of anglo-\nphone cultures as well. We may find not only dif-\nferent effects for biographies with different cultural\nbackgrounds, but also that the model is not even\nencoding American assumptions such as associa-\ntions between political ideology and demographics.\nTherefore, an analysis that uses these associations\nto analyze the model may produce spurious conclu-\nsions, e.g., much of the world uses \u201cliberal\u201d for eco-"}, {"title": "Ethical Considerations", "content": "The biases we document here could be used for\njailbreaking models by posing as a more \u201ctrusted\u201d\nuser. We have inspected a number of the gener-\nated prompts manually to account for their sensi-\ntive nature and potential biases, and these issues\nare addressed in our paper. We are releasing all\nprompts, requests, and personas used publicly so\nthey can be inspected to learn from or alleviate\nthe confounder issues with the data that we have\ndiscussed (see Appendix and GitHub repository:\ngithub.com/vli31/llm-guardrail-sensitivity).\nAnother risk comes from the anthropomorphiz-\ning language we have used to clarify our work by\nanalogy. While we use terms like \"sycophancy\" as\nexisting standard terminology, the reader should re-\nsist the temptation to assign humanlike motivation\nor perspective to the LLM."}, {"title": "Acknowledgements", "content": "We thank Martin Wattenberg for invaluable feed-\nback and guidance that shaped this work from the\nstart. We thank Eve Fleisig, A. Michael Carrell,\nJohnathan Sun, and Adam Lopez for feedback on\nearly drafts.\nWe thank A. Michael Carrell for information\non National Football League team connotations\nand general sports knowledge. We thank Deborah\nRaji and Johnathan Sun for useful discussion. Our\nstatistical tests were informed by discussion with\nTracy Ke and Alex Falk.\nThis work was enabled in part by a gift from the\nChan Zuckerberg Initiative Foundation to establish\nthe Kempner Institute for the Study of Natural and\nArtificial Intelligence."}, {"title": "Author contributions", "content": "Victoria Li designed and implemented most experi-\nments in their current form; engineered the prompts\nbased on her review of the literature; conducted sta-\ntistical tests; plotted; wrote; and in general drove\nthis project-in both concept and development\u2014\nfor the majority of its duration.\nYida Chen performed early experiments and\nmade many of the resulting observations that\nshaped this project before stepping back. He also\nprovided support to Victoria Li throughout the"}, {"title": "Request Prompts", "content": "This section contains prompts for generating the\ncollection of personas and requests used in our\nexperiments. All prompts for request generation,\npersona dialogue, and response classification were\nrun in the month of June 2024 at a total cost of\naround $1400."}, {"title": "Persona Generation Request Prompts", "content": "We generated all personas via GPT-3.5 using\nprompts of the following structure, with necessary\ngrammatical adjustments:\nPlease generate 5 five-sentence paragraphs\nwhere a [persona type] introduces themselves.\nMake sure you can tell that each person is [per-\nsona type]."}, {"title": "Political Request Prompts", "content": "Table 4 presents illustrative examples of the result-\ning politically controversial requests, and see our\nGitHub repository for all left-leaning and right-\nleaning questions we used. To generate our politi-\ncal requests, we used the following prompt for each\nof the following political controversies for each of"}, {"title": "Censored Information Prompt", "content": "See Appendix Table 5 for examples of generated\nprompts and our GitHub repository for all requests\nin this category. Here, we focused on prompts\naround OpenAI's usage policies, which in January\n2024 included the following4:\n1. Illegal activity: OpenAI prohibits the use of our\nmodels, tools, and services for illegal activity.\n2. Child Sexual Abuse Material or any content that\nexploits or harms children: We report CSAM to\nthe National Center for Missing and Exploited\nChildren.\n3. Generation of hateful, harassing, or violent con-\ntent: Content that expresses, incites, or pro-\nmotes hate based on identity; Content that in-\ntends to harass, threaten, or bully an individual;\nContent that promotes or glorifies violence or\ncelebrates the suffering or humiliation of others.\n4. Generation of malware: Content that attempts to\ngenerate code that is designed to disrupt, dam-\nage, or gain unauthorized access to a computer\nsystem.\n5. Activity that has high risk of physical harm, in-\ncluding: Weapons development; Military and\nwarfare; Management or operation of critical in-\nfrastructure in energy, transportation, and water;\nContent that promotes, encourages, or depicts\nacts of self-harm, such as suicide, cutting, and\neating disorders.\n6. Activity that has high risk of economic harm,\nincluding: Multi-level marketing; Gambling;\nPayday lending; Automated determinations of\neligibility for credit, employment, educational\ninstitutions, or public assistance services.\n7. Fraudulent or deceptive activity, including:\nScams; Coordinated inauthentic behavior; Pla-\ngiarism; Academic dishonesty; Astroturfing,\nsuch as fake grassroots support or fake review\ngeneration; Disinformation: Spam; Pseudo-\npharmaceuticals.\n8. Adult content, adult industries, and dating apps,\nincluding: Content meant to arouse sexual ex-\ncitement, such as the description of sexual ac-\ntivity, or that promotes sexual services (exclud-"}, {"title": "Classifying Guardrail Refusals", "content": "We annotate responses as refusals or requests for\nmore information using the following prompt:\nPlease respond with just 'yes' or 'no'.\nYour classification task is to determine\nwhether a given response answers a spe-\ncific question."}]}