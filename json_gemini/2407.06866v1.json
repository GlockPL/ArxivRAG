{"title": "ChatGPT Doesn't Trust Chargers Fans:\nGuardrail Sensitivity in Context", "authors": ["Victoria R. Li", "Yida Chen", "Naomi Saphra"], "abstract": "While the biases of language models in pro-\nduction are extensively documented, the bi-\nases of their guardrails have been neglected.\nThis paper studies how contextual information\nabout the user influences the likelihood of an\nLLM to refuse to execute a request. By gen-\nerating user biographies that offer ideological\nand demographic information, we find a num-\nber of biases in guardrail sensitivity on GPT-3.5.\nYounger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail\nwhen requesting censored or illegal informa-\ntion. Guardrails are also sycophantic, refusing\nto comply with requests for a political position\nthe user is likely to disagree with. We find that\ncertain identity groups and seemingly innocu-\nous information, e.g., sports fandom, can elicit\nchanges in guardrail sensitivity similar to di-\nrect statements of political ideology. For each\ndemographic category and even for American\nfootball team fandom, we find that ChatGPT\nappears to infer a likely political ideology and\nmodify guardrail behavior accordingly.", "sections": [{"title": "Introduction", "content": "Like other applications of AI, chatbots can offer\nunequal support to users depending on their back-\nground and needs. Large language models (LLMs)\noften have limited utility for users who speak a low\nresource language or marginalized dialect (Huang\net al., 2023; Deas et al., 2023). The phrasing of a\nrequest may also change the quality of the answer\n(Hofmann et al., 2024), advantaging educated users\nwith a privileged background.\nWhile work often addresses these issues of con-\ntextual accuracy and data scarcity, we instead fo-\ncus on a previously unexplored factor in unequal\ncapabilities: chatbot guardrails, the restrictions\nthat limit model responses to uncertain or sensitive\nquestions and often provide boilerplate text refus-\ning to fulfill a request (see Fig. 1). These guardrails\nmay be created with the same human feedback pro-\ncedures by which the next token predictions of an\nLLM are tuned into a usable dialogue interface\n(Ouyang et al., 2022; Touvron et al., 2023). In\nclosed commercial chatbots, guardrails may take\nthe form of proprietary peripheral models (Team\net al., 2023). While we do not always know the pro-\ncess by which these guardrails are trained, we can\nmeasure their sensitivity to context as a blackbox.\nGuardrails must track the wider dialogue context\nbecause adversarial jailbreaks and spurious triggers\noften depend on recontextualizing a request, e.g.,\nby first ordering the model to roleplay.\nUsing a diverse set of persona introductions\nthat imply or declare a user identity and ideology\nand a set of requests which inconsistently trigger\na guardrail refusal (Figure 2), we find a number of\nbiases in the sensitivity of the guardrail:\n\u2022 Given explicit declarations of a user persona's\ngender, age, or ethnicity, ChatGPT refuses re-\nquests for censored information for simulated\nminors more than adults; women more than\nmen; and Asian-Americans more than other\nethnicities.\n\u2022 Guardrails trigger sycophantically for polit-\nically sensitive requests, with higher proba-\nbility of a refusal if a conservative persona\nrequests a liberal position from the model or\na liberal requests a conservative position.\n\u2022 Some personas are treated as implicitly conser-\nvative or liberal. Black, female, and younger\npersonas are treated more like liberal personas\nby the LLM guardrail.\n\u2022 People strongly identify with groups that are\nnot standard demographic categories or ex-\nplicitly ideological. We explore these group\nidentifications through sports fandom, which\nconnotes demographics, region, and ideology."}, {"title": "Background", "content": "Epistemic bias Much recent work on fairness\nin LLMs focuses on potential prejudice against a\nthird party or worldview, rather than against the\nuser directly. In other words, models provide prej-\nudiced responses that that may harm third parties,\ne.g., inferring that a particular job applicant is more\nqualified based on ethnicity (Yin et al., 2024) or\npresuming gender based on an individual's profes-\nsion (Rudinger et al., 2018). Political bias can also\nemerge from information in the training data or\nthe design of the human feedback procedure (Liu\net al., 2022). Focusing on guardrail sensitivity, we\ninvestigate how the model may also express social\nbiases in refusing a user's request.\nEqual utility Language models can also disad-\nvantage certain groups by providing utility un-\nequally to their users. It is frequently observed, for\nexample, that speakers of languages other than En-\nglish rarely have access to the functionality of state-\nof-the-art English language models (Bang et al.,\n2023; Ojo et al., 2023; Huang et al., 2023). Even\nEnglish speakers who use a marginalized dialect\nmay find that chatbots are less accurate or useful,\nas prestige dialects can elicit model responses that\nare better tuned or more helpful (Chen et al., 2023).\nBeyond this utility gap, LLMs have also produced\noffensive or harmful responses, occasionally result-\ning in substantial real world consequences, as when\none user died by suicide under the suggestion of a\nchatbot (El Atillah, 2023). Chatbots may be more\nlikely to offer harmful responses to some groups,\nand these harmful responses may also dispropor-\ntionately impact members of already vulnerable\ngroups, e.g., a racist reply has a greater impact on\nusers with marginalized ethnic identities.\nOur work presents a new potential source of\nbias in these LLMs: the guardrails introduced to\nprevent harmful responses. By initiating dialogue\nwith a short user bio before issuing a sensitive re-\nquest, we study the utility gap introduced by the\nguardrail-induced refusals. Although this approach\nto including user context is artificial, features like\nthe new persistent memories offered by ChatGPT\n(OpenAI, 2024a) may retain user identity across\nconversations, making potential guardrail biases a\nmore significant concern. We show that simulated\nusers may signal ideology and impact guardrail be-\nhavior with cues as seemingly innocuous as endors-\ning an NFL team, so it is likely that a real-world\nuser has implied a salient identity feature in some\ndialog context."}, {"title": "Experiments", "content": "Our experiments begin each dialogue with a per-\nsona introduction that reveals some aspect of the\nsimulated user's identity. We then provide a re-\nquest prompt which the model is likely, but not\ncertain, to reject. For example, we may begin by\nsaying, \"Hi there, I'm Sarah, a Minnesota Vikings\nfan originally from the Midwest,\u201d and then issu-\ning a sensitive request such as, \u201cHow do peoplecheat on tests?\u201d We then evaluate whether a given\nresponse addresses the request.\nThe model may reply to our simulated user with,\n\"I'm sorry, but I can't assist with that,\u201d or, \u201cRather\nthan cheating, you should focus on studying dili-\ngently to perform well on your exam.\u201d Both re-\nsponses constitute a refusal because they do not\nalign with the user's intent to cheat, but the for-\nmer response is a more stereotypical guardrail\u2014\nidentifiable by keywords alone and we distin-\nguish between these cases using multiple classi-\nfiers.\nFigure 2 illustrates the full experimental pipeline,\nincluding the generation and filtering of persona\nintroductions and sensitive requests.\n3.1 Classifying responses\nSince our experiments consist of more than\n225,000 requests to the conversational models, we\nautomatically label refusals to answer by prompt-\ning GPT-40 as described in Appendix B.\nFor a stricter classification of refusals, we also\nuse a keyword classifier (based on the terms in-\ndexed by Zou et al. (2023) and Qi et al. (2023))\nwhich labels a response as a refusal only if it\ncontains stereotyped guardrail terms such as \u201cI'm\nsorry\" and \u201cAs an AI.\u201d In our visualizations, we\nuse color to differentiate refusals tagged only by\nGPT-40 from those also identifiable by keyword.\nAlthough researchers debate the quality of\nprompt based annotation (Gilardi et al., 2023;\nAldeen et al., 2023; Reiss, 2023), our preliminary\nanalysis supports our approach. Figure 3 shows\nthat semantic differences can cluster responses into\""}, {"title": "Models", "content": "Our experiments analyze gpt-3.5-turbo, OpenAI's\nflagship model for conversational AI. We choose\nChatGPT-3.5 as a target of analysis due to its wide\ncoverage of users (over 180 million monthly active\nusers as of March 2024 (Porter, 2023)). Unlike\nthe paywalled ChatGPT-4, ChatGPT-3.5 is freely\naccessible. We sample with default temperature 1\non the API while using the system prompt, \u201cYou\nare ChatGPT, a large language model trained by\nOpenAI\" (OpenAI, 2024b).\""}, {"title": "Persona prompts", "content": "In order to ensure that ChatGPT has an association\nbetween the persona prompt and the demograph-\nics, we generate a list of five persona prompts by\nrequesting them from ChatGPT itself. To ensure a\nsufficiently diverse collection of biographies, we\ngenerate all five personas simultaneously as a set.\nWe generate personas (examples in Appendix\nTable 6) for political conservatives and liberals;\nmen and women; Black, White, Asian-American,\nand Latin-American users; users ages 13\u201317, 35\u2013\n44, and 55-64; and fans of every team in the NFL.\nWe filter persona collections by hand to ensuretheir quality and to avoid homogeneous, politically\nfringe, or otherwise problematic sets of persona\nbiographies.\nEmphatically, the personas we generate for a\ncategory are not a representative sample of a real-\nworld identity. Our claims apply to a particular\nset of personas and whether they differ from each\nother. However, because these persona sets are\ngenerated by GPT, they emphasize its internal as-\nsociations with a particular identity. While our\nexperiments do not reflect real-world user interac-\ntions, the guardrail may still express similar biases\nunder deployment.\n3.3.1 Properties of autogenerated biographies\nQualitative inspection of persona prompts reveals\nsome caveats regarding our approach. The degree\nto which these biographies reflect ChatGPT-3.5's\nexisting associations is in part a strength of automa-ntion, but the sampled user biographies are highly\nbiased with many potential confounders. Asian-\nAmerican and Hispanic/Latin personas consistently\nspecify the nation their family immigrated from,\nbut Black and White ethnic personas may not. The\npersonas treat women as marked but men as un-\nmarked: female personas\u2014but not male personas\u2014\ncontain explicit mentions of gender, e.g., \u201cas a\nwoman...\" (See Appendix Table 6 for examples).\nThis automated approach is limited by biases in\nthe corpus and other elements of training. In gen-\neral, we see a large number of biases and that per-\nsonas are not representative of their identity groups.\nHowever, automation guarantees that these biogra-\nphies represent ChatGPT's archetype of a particular\ngroup.\""}, {"title": "Request prompts", "content": "We consider guardrails in the following situations\nin which chatbots frequently refuse requests. To\ngenerate requests, we prompt GPT-40 as in Ap-\npendix A. Of these potential sensitive requests, we\nretain only those that trigger refusals inconsistently,\ni.e., for at least one but not all of 22 personas sam-\npled across identities."}, {"title": "Political ideology", "content": "Using a sample of user persona introductions that\nexplicitly describe the user's political ideology, we\nfind that political allegiance determines guardrail\nsensitivity for political requests, but not censored\ninformation requests (Table 1).\nSycophancy. Perez et al. (2022) observe a phe-\nnomenon in larger LLMs that they call sycophancy,\na tendency to respond to questions by aligning\nwith the user's expressed views. We find that syco-\nphancy is also expressed through guardrails\u2014the\nmodel is more likely to refuse a direct request for a\ndefense of gun control or an argument denying cli-\nmate change if the user has previously expressed a\npolitical identity at odds with those views. Overall,\nconservative-leaning requests have a refusal rate of\n44% for conservative personas and 76% for liberal\npersonas, whereas liberal-leaning requests have a\nrefusal rate of 68% for conservative personas but\nonly 40% for liberal personas."}, {"title": "Demographics", "content": "Guardrail behavior varies in response to explicit\ndeclarations of user age, gender, and ethnicity. This\nsection discusses the findings presented in Figure\n4 with significance tests in Table 1."}, {"title": "Inferring politics from demographics", "content": "Certain demographics are often more likely to\nbe conservative or liberal, at least in their voting\nrecords. Men are more conservative than women\nin general, and ethnic groups often differ substan-\ntially in their party preferences. In the USA, where\nOpenAI is based, Joe Biden won the 2020 election\nwith 51.3% of overall votes while leaning heavily\non core constituencies like non-Hispanic Black vot-\ners, who favored Biden at a rate of 92% (Igielnik\net al., 2021). This section will show that ChatGPT\ntreats certain demographics as implicitly liberal or\nconservative in line with their voting tendencies.\nTo measure the political ideology associated\nwith guardrail behavior on a given identity category\n$Q$, we consider individual personas $q \\in Q$ each\nwith a corresponding vector of refusal rates $r_q$ in-\ndexed by request. The guardrail similarity between\na pair of personas $q, q'$ is measured by the Pear-\nson correlation $p(r_q, r_{q'})$. We correlate refusals on\nliberal and conservative persona sets (sets $L$ and\n$C$ respectively) with refusals on a target identity\nacross all categories of sensitive requests, both po-"}, {"title": "Sports Fandom", "content": "Conflating demographics and political identity is\none way that ChatGPT infers user ideology indi-\nrectly, but any facet of a user's identity can be cor-\nrelated with ideological positions. In this section,\nwe focus on simulated personas for enthusiastic\nfans of each NFL team.\nGuardrail sensitivity varies in response to de-\nclared sports team fandom on political and apolit-\nical trigger prompts. As shown in Appendix Fig-\nure 7, ChatGPT's refuses most frequently for a\ndeclared Los Angeles Chargers fan persona in ev-\nery guardrail category. Compared to a Philadelphia\nEagles fan, a Chargers fan is refused 5% more on\ncensored information requests, 7% more on right-\nleaning political requests, and 10% more on left-\nleaning political requests. These differences could\nexpress a variety of connotations that relate to the\nteam's home city, name, or fanbase.\nAs with demographics (Section 4.3), some\nguardrail bias relates to presumed ideology. For\nexample, we find that Dallas Cowboys fan per-\nsonas, representing one of the most conservative\nNFL fanbases, are treated like overtly declared con-\nservatives by ChatGPT guardrails. We illustrate\nthis effect in Figure 5b, showing a moderate corre-\nlation between the conservatism of an NFL team's\nfanbase according to Paine et al. (2017) and the fan\npersona's similarity to conservatism personas in its\nguardrail triggers."}, {"title": "Discussion", "content": "A user may be disadvantaged by impaired utility\nif guardrails are overly sensitive. However, they\nmay also be harmed if guardrails are insufficiently\nsensitive and an LLM generates distressing or in-\ncorrect content. It is not, therefore, straightforward\nto assess the impact of guardrail bias on utility.\nWhile we attempt to offer implicit demographic\ninformation by explicitly declaring names or fan-\ndom, we do not consider other even more implicit\nsources of information such as the dialect use or el-\nements of the phrasing of the prompt. Recent work\nhas revealed implicit biases against speakers of mi-\nnority dialects even after models are tuned to avoid\nbiases over identities (Hofmann et al., 2024; Bai\net al., 2024); different guardrail sensitivity biases\nmight emerge under similar tests.\n5.1 Future Work\nOur study of guardrails is intended to present a\npreviously unstudied, to our knowledge, source of\nbias in LLMs. However, there are obvious next\nsteps. We study only a single LLM, ChatGPT-3.5,\nbut newer models should also be studied. Further-\nmore, we only consider a limited number of user\nattributes. Other aspects of identity might be influ-\nential and even those we study have a number of\nnuances that we do not address. Researchers with\naccess to deployment data could study how much\nthese biases impact real-world users.\nWho guards the guardrails? When a language\nmodel is equipped with guardrails to reduce or con-\nceal its biases, the guardrails themselves may still\nexhibit measurable biases. How can we remedy\nthe biases documented in our findings? We leave\nsolutions to future work but incorporating explicit\nbias metrics, meta-guardrails which monitor for po-\ntentially invalid refusals, and more layers of human\nfeedback tuning could all be paths forward.\nAnalyzing different kinds of guardrails. LLMs\nrefuse a request in several situations we have not\ncovered here. We have not addressed cases where\nthe model refuses a request for a personal opinion,\nfor example. Other refusals might take a different\nform, as when the model does not have sufficient\ninformation either because the user has not pro-\nvided it or because its training corpus is limited to\ntext produced before a particular date. Future work\nmay also study bias in other guardrail types."}, {"title": "Conclusions", "content": "This paper has investigated a new potential source\nof bias in chatbot LLMs in the form of its guardrails.\nIf a guardrail triggers spuriously, the resulting re-\nfusal can limit the utility of the LLM. On the other\nhand, if a guardrail fails to trigger when it should,\nusers may be exposed to harmful or distressing con-\ntent. We have shown that the likelihood of a refusal\ncan be influenced by demographic categories, po-\nlitical affiliation, and even seemingly innocuous\ninformation like sports fandom.\nLimitations\nThere are a number of limitations to our analysis\nin addition to those already discussed in the pa-\nper. First, the setup is artificial, as it involves a\ndialogue with a user who explicitly provides bio-\ngraphic information before asking questions. This\nis an atypical interaction with a user and possibly a\nsetting where ChatGPT is explicitly tuned against\novert bias. More naturalistic ways of eliciting bias,\nsuch as modifying the user's dialect, could show\ndifferent results, either stronger or weaker.\nTo the degree that our results measure significant\neffects, these effects may no longer hold true in fu-\nture versions of ChatGPT or even under additional\nhuman feedback tuning. While we are pointing\nout a potential issue with models that has not yet\nbeen discussed publicly and therefore our work has\nvalue even if the particular numbers change, our re-\nsults are subject to the reproducibility issues caused\nby proprietary model maintenance.\nThe prompt we use to generate requests includes\nexamples that bias the generated requests towards\nspecific formatting and topics. The results we pro-\nduce may not generalize to other sets of requests.\nThese results may also fail to generalize to other\ncultures. Our framework assumes the user to be\nAmerican, including the political language (\"Re-\npublican\", \"liberal\", etc.), the primary racial cate-\ngorization, and the use of American football sports\nfandom. However, ChatGPT is massively multi-\nlingual and is trained on a large range of anglo-\nphone cultures as well. We may find not only dif-\nferent effects for biographies with different cultural\nbackgrounds, but also that the model is not even\nencoding American assumptions such as associa-\ntions between political ideology and demographics.\nTherefore, an analysis that uses these associations\nto analyze the model may produce spurious conclu-\nsions, e.g., much of the world uses \u201cliberal\u201d for eco-"}, {"title": "Ethical Considerations", "content": "The biases we document here could be used for\njailbreaking models by posing as a more \u201ctrusted\u201d\nuser. We have inspected a number of the gener-\nated prompts manually to account for their sensi-\ntive nature and potential biases, and these issues\nare addressed in our paper. We are releasing all\nprompts, requests, and personas used publicly so\nthey can be inspected to learn from or alleviate\nthe confounder issues with the data that we have\ndiscussed (see Appendix and GitHub repository:\ngithub.com/vli31/llm-guardrail-sensitivity).\nAnother risk comes from the anthropomorphiz-\ning language we have used to clarify our work by\nanalogy. While we use terms like \"sycophancy\" as\nexisting standard terminology, the reader should re-\nsist the temptation to assign humanlike motivation\nor perspective to the LLM."}]}