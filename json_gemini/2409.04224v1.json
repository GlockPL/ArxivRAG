{"title": "Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement\nLearning Framework", "authors": ["Daniel J. Tan", "Qianyi Xu", "Kay Choong See", "Dilruk Perera", "Mengling Feng"], "abstract": "Multi-organ diseases present significant challenges due to\ntheir simultaneous impact on multiple organ systems, neces-\nsitating complex and adaptive treatment strategies. Despite\nrecent advancements in AI-powered healthcare decision sup-\nport systems, existing solutions are limited to individual or-\ngan systems. They often ignore the intricate dependencies\nbetween organ system and thereby fails to provide holistic\ntreatment recommendations that are useful in practice. We\npropose a novel hierarchical multi-agent reinforcement learn-\ning (HMARL) framework to address these challenges. This\nframework uses dedicated agents for each organ system, and\nmodel dynamic through explicit inter-agent communication\nchannels, enabling coordinated treatment strategies across or-\ngans. Furthermore, we introduce a dual-layer state represen-\ntation technique to contextualize patient conditions at var-\nious hierarchical levels, enhancing the treatment accuracy\nand relevance. Through extensive qualitative and quantitative\nevaluations in managing sepsis a complex multi-organ dis-\nease-our approach demonstrates its ability to learn effective\ntreatment policies that significantly improve patient survival\nrates. This framework marks a substantial advancement in\nclinical decision support systems, pioneering a comprehen-\nsive approach for multi-organ treatment recommendations.", "sections": [{"title": "Introduction", "content": "Multi-organ diseases are characterized by the sequential or\nsimultaneous impairment of multiple organ systems (Asim,\nAmin, and El-Menyar 2020). They present significant chal-\nlenges in clinical management due to their complexity, the\ndifficulty of balancing therapeutic trade-offs, and the poten-\ntial for life-threatening outcomes in critically ill patients.\nTreating these diseases requires a holistic approach that\naccounts for the interdependencies between different or-\ngan systems (Tian et al. 2023). Existing guideline-based\napproaches treat organ systems in isolation and rely on\none-size-fits-all recommendations (Whelehan, Conlon, and\nRidgway 2020), which fail to address the complexities of\nmulti-organ diseases. One example of such a disease is\nCOVID-19, which, while primarily affecting the respiratory\nsystem, can also lead to dysfunction in the immune, nervous,\nand gastrointestinal systems (Thakur et al. 2021; Bhadoria\nand Rathore 2021). Another example is sepsis, a serious con-\ndition triggered by the body's dysregulated response to in-\nfection. Sepsis can lead to widespread inflammation, coag-\nulation abnormalities, and metabolic disruptions, cascading\ninto multi-organ dysfunction, which requires comprehensive\nand adaptive treatment strategies (Greco et al. 2017).\nRecent advances in artificial intelligence, particularly in\nreinforcement learning (RL), have shown promise in opti-\nmizing clinical decision-making for complex diseases. RL's\ncapacity to learn adaptive policies from high-dimensional,\ncomplex data makes it a powerful tool for treatment rec-\nommendations. Notably, Komorowski et al., pioneered the\nuse of deep RL for sepsis treatment, developing a model\nthat learns optimal treatment policies from electronic health\nrecords (EHRs) of intensive care unit (ICU) patients (Ko-\nmorowski et al. 2018). Subsequent work in this area has ex-\nplored new model-free approaches, such as those based on\nDueling Double Deep Q-Networks (D3QN) (Raghu et al.\n2017; Wu et al. 2023), as well as model-based approaches\n(Raghu, Komorowski, and Singh 2018). Beyond sepsis, RL\nhas also been applied to chronic disease management, such\nas in the work by Zheng et al. (Zheng et al. 2021), which\nproposed an RL-based method for personalized diabetes\nand multimorbidity management by modeling health out-\ncomes like glycemia, blood pressure, and cardiovascular dis-\nease risk. Liu et al., trained an RL model to recommend\nthe dosage of oral antidiabetic drugs and insulin (Liu et al.\n2020b). Additionally, Wang et al., proposed a model-based\nRL framework consisting of a patient model paired with a\npolicy model to optimize insulin regimens through the anal-\nsis of glycemic response. (Wang et al. 2023).\nDespite the multi-organ nature of many major diseases,\nexisting RL approaches have predominantly focused on rec-\nommending treatments targeting a single organ system. For\nexample, current sepsis-targeted solutions primarily address\nonly the cardiovascular system, neglecting other relevant or-\ngan systems (Liu et al. 2020a). This is a significant limita-\ntion, as treatments for one organ system can significantly\ninfluence the efficacy or safety of treatments for another.\nFor instance, recommending vasopressors (VAs) to stabilize\nblood pressure may not be feasible in a patient with concur-\nrent renal dysfunction, as doing so could increase renal im-\npairment (Yagi et al. 2021). While there have been AI-based"}, {"title": "Methodology", "content": "We decompose the intricate multi-organ recommendation\ntask into a clinically meaningful hierarchy of sub-tasks, rec-\nommending seven treatments across three organ systems (\nFigure 1). This structured decomposition significantly re-\nduces the overall complexity by confining each sub-task\nto its own subspace, only coordinating with other subtasks\nwhen necessary.\nAt the top level, the root agent (MRt) selects from five\nprimary actions: No-treatment (Art), Neuro-only (Areu),\nCardio-only (Aar), Renal only (Aken) or Organ mixture\n(AMix). Including the frequently utilized Art option at\nthe root level substantially simplifies the process by reducing\nthe overall action space and eliminating the need to consider\nthis non-intervention option in subsequent tasks.\nOrgan-specific actions (Aneu, Apar, Aken) invoke re-\nspective agents (MNeu, Mcar, MRen), each operating\nwithin dedicated treatment subspaces. Subsequently, MNeu\nchooses from four possible actions: S1-only (Aeu), S2-\nonly (Au), S3-only (Aeu), or a mixture of them\n(AMix), each invoking a specialized sub-agent (Meu"}, {"title": "RL Components", "content": "RL States: Accurate patient understanding is essential for\neffective multi-organ treatment recommendations, which re-\nquire integrating complex interdependencies of physiologi-\ncal features and organ functions. To address this, we propose\na hierarchical patient representation approach that acknowl-\nedges the varying significance of raw features at different\nanalytical levels.\nAt the root level, Unified State Representations are\nlearned to extract broad health indicators and their dynam-\nics, providing a foundational understanding of patient sta-\ntus. This representation is used for broader decision-making\nprocesses at the root level, and sets the stage for subsequent,\nmore granular recommendations. At the organ levels, they\nare refined to learn Targeted State Representations, tailored\nto unique physiological requirements and and interrelation-\nships of specific organs. For instance, in cardiac treatments,\nthe embeddings prioritize features like ejection fraction and\ncardiac enzyme levels, capturing essential cardiac health in-\ndicators. In contrast, renal treatments focus on features like\nglomerular filtration rate and electrolyte balances, crucial for\nassessing renal function. This approach ensures that organ-\nspecific recommendations are precise and relevant.\nThis dual-layer hierarchical representation strategy bal-\nances broad applicability with detailed specificity, enhanc-\ning decision-making capabilities by considering each or-\ngan's condition within the overall health context. The hier-\narchical structure comprises the following levels:\nUnified State Representations: Inspired by the represen-\ntation learning proposed by Perera et al., (Perera, Liu,\nand Feng 2023) we learn the unified representations as\nfollows. Each patient at time t is represented by their\nraw d-dimensional feature, $X_t = {x_{t,1},x_{t,2},...,x_{t,d}} \\in\nR^d$ (for details on $x_t$, see Appendix Section Feature\nProcessing). At the root agent level, these features\nare transformed using dense latent embeddings $ERt\n= {e_t^1,e_t^2,..., e_t^d} \\in R^{dxk}$. Each k-dimensional latent\nembedding vector $e_t^i \\in R^k$ transforms its corresponding\nraw feature into a more informative dense latent represen-\ntation. The resultant patient-specific embeddings are rep-\nresented as $FRt = {f_t^1, f_t^2,\u2026\u2026, f_t^d} \\in R^{dxk}$, where\n$f_t^i = (x_{t,i}e_t^i)$. These latent embeddings $e_t^i$ are generic\nat the root agent level, providing a holistic understanding of\nthe patient by transforming each feature into a homogeneous\nlantent space. The vector $f_t^i$ denotes the transformed repre-\nsentation specific to the patient at time t.\nTo capture the complex interdependencies among these\nfeatures, a higher-order interaction layer is introduced. This\nlayer computes the element-wise product between all pairs\nof embeddings in FRt resulting an interaction matrix GRt E\nRkxd(d+1)/2. Final output of the layer consists of both first-\nand second- order interactions, denoted by HR = (FRt |\nGRt), which is then aggregated via sum pooling to generate\nan observation vector as $oRt = \\Sigma^{d(d+3)/2}_{d(d+3)} HR \\in R^k$.\nMoreover, given the importance of patient trajectory in-\nformation for understanding the patient's current context, a\ntemporal contextual state vector cft. This vector captures\nthe recent history of the patient's states by applying an expo-"}, {"title": "Q Learning", "content": "Traditional RL frameworks modeled by Markov Decision\nProcesses (MDPs), often assume that actions occur instan-\ntaneously with uniform execution times. This assumption\nsimplifies modeling, but fails to capture the complexities of\nreal-world scenarios with actions that span multiple steps\nand vary in duration.\nTherefore we utilize an integration of an options frame-\nwork with semi-MDPs and decentralized MDPs within a\nstructured hierarchical system. This approach enables an\nagent to invoke options that ranges from primitive (one-\nstep) to extended (multiple-step) actions at any point. Then\nthe agent regains control to initiate another option only af-\nter the previous one completes. Accordingly, the traditional\naction-value function is adapted to an option-value function\n$Q^{\\pi}(s, o)$ which indicates the value of invoking option o in\nstate s under a policy \u03c0. This is defined as the expected sum\nof discounted future rewards:\n$Q^*(s, 0) = E{r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \u00b7\u00b7\u00b7 | e^{\\pi} (o, s,t)}$\nwhere y is a discount factor and $e^{\\pi}(o, s, t)$ is the execution\nof option o in state s from time t to its termination. Un-\nlike standard options framework, our proposed solution con-\ntains both independent and cooperative agents controlling\noptions, as detailed in Hierarchical Decomposition section.\nThus, the root level Q-values are updated as follows:\n$Q(s, 0) \u2190 Q(s, 0) + \\alpha [Q_{tgt}(s, 0) \u2013 Q(s, 0)]$                                                            (1)\nwhere $Q_{tgt}(s, 0)$ is determined based on the controlling\nagent and the option o as follows:\n$Q_{tgt} (S, 0) = $\n$\\begin{cases}\nr, & \\text{if o = Art} \\\\\n\\max_{a\\in AN_{eu}} Q_{Neu}(s,a), & \\text{if o = Aneu} \\\\\n\\max_{a\\in A_{Car}} Q_{Car}(s,a), & \\text{if o = Acar} \\\\\n\\max_{a\\in A_{Ren}} Q_{Ren}(s,a), & \\text{if o = ARen} \\\\\n\\max_{a\\in AOMix} Q_{OMix}(s,a), & \\text{if o = A_{OMix}}\n\\end{cases}$\nwhere r is the immediate reward received for choos-\ning no-action (Art), and ANeu, Acar, and Aren are the\naction spaces for the Neuro, Cardio and Renal agents,\nwhereas Art A Mix is their combined action space (ANeu \u00d7\nAcar X ARen). Value functions $Q_{Neu}(s,a)$, $Q_{Car}(s,a)$,\nand $Q_{Ren} (s, a)$ for the corresponding agents are updated via\nTemporal Difference (TD) learning as follows:\n$Q_{Neu}(s, a) \u2190Q_{Neu}(s, a) + \\alpha [yNeu \u2013 Q_{Neu}(s,a)]$\n$Q_{Car}(s, a) \u2190Q_{Car}(s, a) + \\alpha [yCar - Q_{Car}(s,a)]$                                                      (2)\n$Q_{Ren}(s, a) \u2190Q_{Ren}(s, a) + \\alpha [yRen \u2013 Q_{Ren}(s,a)]$\nwhere $yx$\n= r + ymaxa' $Qx(s',a';\\theta^\\land)$ for X \u2208\n{Neu, Car, Ren} are TD targets computed from target net-\nworks with parameters $\u03b8^\\land$. The Q values for each system are"}, {"title": "Experiments", "content": "We collected data on 30,440 patients under Sepsis-3 crite-\nria from the popular MIMIC-IV database (Johnson et al.\n2020). Data spans from 24 hours pre-diagnosis to 48 hours\npost-diagnosis forming a maximum 72 hour window per pa-\ntient, barring death or ICU discharge. Patient state data col-\nlected includes 48 physiological measurements including vi-\ntal signs, laboratory test results, severity scores and demo-\ngraphics (see Appendix, Section Dataset). State and actions\ndata were aggregated into four-hourly windows to generate\nuniform patient data sequences. The patient trajectories were\nrandomly split into 75% training and 25% testing sets."}, {"title": "Baselines", "content": "We evaluate our model performance against a variety of\nbaselines, including both single- (D3QN-S and SoftAC-S)\nand multi-agent systems under independent (D3QN-O) and\ncooperative (QMix-O and QMix-T) learning approaches:\n\u2022 Clinician: Policy derived from clinician's recorded ac-\ntion trajectories in the test set.\n\u2022 Single D3QN (D3QN-S) (Raghu et al. 2017): State-of-\nthe-art single-agent Dueling DQN predicting all action\ncombinations in a flattened action space. We used the\noriginal implementation and tuned hyperparameters, and\nsimplified the problem to only 3 quantiles per treatment.\n\u2022 Single SoftAC (SoftAC-S) (Haarnoja et al. 2018): A\nsingle-agent Soft Actor-Critic predicting all action com-\nbinations in a flattened action space. We used the original\nimplementation and tuned hyperparameters, and simpli-\nfied the problem to only 3 quantiles per treatment.\n\u2022 Organ-specific D3QN (D3QN-O): Three independent\nD3QN agents for Neuro, Cardio and Renal systems.\nModels are trained using all available samples, including\nthose treated exclusively for the target organ and those\nmixed with other organ treatments. We present average\nquantitative metric values across agents for comparisons.\n\u2022 Treatment-specific D3QN (D3QN-T): Six independent\nD3QN agents for S1, S2, S3, IV, vaso, and diuretic-s/dialysis. Models are trained using all available sam-\nples, including those using single-treatments and mixed-\ntreatments. Average quantitative metrics are reported.\n\u2022 Organ-coordinated QMix (QMix-O): Trained end-to-\nend with three cooperative agents, each corresponding to\nan organ system operating exclusively within its factored\naction spaces. Predicts treatments across organs by coop-\neration, using a QMix mixing network.\n\u2022 Treatment-coordinated QMix (QMix-T): Uses six co-\noperative treatment-level agents (S1, S2, S3, IV fluids,\nvasopressors, diuretics/dialysis), learning cooperatively\nvia a QMix network.\nAll models used same state representations, action dis-\ncretization methods, and reward functions. Detailed feature\nprocessing information and model parameters are available\nin Appendix, Section Model Parameters. All codes for data\nprocessing and model training are included in Supplemen-\ntary materials and will publicly available upon acceptance."}, {"title": "Results and Discussion", "content": "We present and discuss our experimental results in the form\nof answers to four key research questions:\nRQ1: Does the proposed solution effectively learns a su-\nperior treatment policy? We evaluate the performance of\nour learned policies using various off-policy quantitative and\nqualitative metrics (see Table 1) as follows:\n1) Average Returns: The performance of learned policies\nis quantitatively evaluated using their estimated average re-\nturns. We use VCWPDIS (V), the average return from com-\nmon sub-trajectories in both learned and clinician policies\n(Thomas 2015). Effective sample size (ESS) is the num-\nber of common sub-trajectories, capped at the total number\nin the clinician-policy. ESS measures the confidence in the"}, {"title": "Conclusions", "content": "We introduce a hierarchical multi-agent reinforcement\nlearning framework for the complex and first-of-its-kind\nmulti-organ treatment recommendations, setting a new\nbenchmark in clinical decision support systems. Mimick-\ning real world collaborative clinical settings, our solution\neffectively decomposes the complex treatment process into\na clinically meaningful hierarchy of subtasks. Each subtask\nis managed by specialized agents operating within dedi-\ncated subspaces. It supports both independent and coopera-\ntive agent functionality through robust inter-and intra- organ\ncommunications. Moreover, a dual-layer state representation\ntechnique is proposed to support advanced contextualiza-\ntion needed at multiple levels in the hierarchy. We evaluate\nour solution on the non-standardized and multi-dimensional\nsepsis treatment recommendation. Comprehensive quanti-\ntative and qualitative evaluation showed that our solution\nconsistently outperformed baselines, significantly improv-\ning the patient survival and effectively managing task com-\nplexity. Furthermore, learned policy closely followed suc-"}, {"title": "Technical Appendix", "content": "For each 4-hour window, we computed the\ntotal dosage for each treatment by multiplying its infusion\nrate with the overlapping duration, and adding any IV push\nvolumes. Treatments with multiple drugs were converted\ninto their standard equivalents (i.e., vasopressors into nore-\npinepherine (Kotani et al. 2023), S2 to fentanyl (McPherson\net al. 2010), and diuretics to furosemide (Konerman et al.\n2022)) (see Table 1)."}]}