{"title": "Using AI Alignment Theory to understand the potential pitfalls of regulatory frameworks", "authors": ["Alejandro Tlaie"], "abstract": "The objective of this paper is to leverage insights from Alignment Theory (AT) research, which primarily focus on the potential pitfalls of technical alignment in Artificial Intelligence, to critically examine the European Union's Artificial Intelligence Act (EU AI Act). In the context of AT research, several key failure modes - such as proxy gaming, goal drift, reward hacking or specification gaming - have been identified. These can arise when AI systems are not properly aligned with their intended objectives. The central logic of this report is: what can we learn if we treat regulatory efforts in the same way as we treat advanced AI systems? By applying these concepts to the EU AI Act, this project uncovers potential vulnerabilities and areas for improvement in the regulation, ensuring it effectively addresses the complexities and risks associated with AI technologies.", "sections": [{"title": "1 Introduction", "content": "The advent of Artificial Intelligence (AI) technologies promises transformative changes across numerous domains, from healthcare and education to finance and law enforcement. However, the rapid deployment and integration of these technologies may also introduce profound challenges and risks, necessitating a thoughtful and robust regulatory approach. This paper seeks to critically examine the recently introduced European Union's Artificial Intelligence Act (EU AI Act) [1] using insights derived from AI Alignment Theory (AT), a conceptual framework primarily concerned with the pitfalls of technical alignment in AI systems [2, 3]. AT explores various modes of misalignment in AI. These phenomena may occur when AI systems, even those engineered to high technical standards, fail to adhere to their intended ethical or operational objectives. Such misalignments can lead to unintended consequences, which, in the realm of AI, might range from inefficiencies [2] to catastrophic risks [4], through violations of privacy [5] or discriminatory outcomes [6, 7].\nThis paper proposes flipping the common logic and evaluating the EU AI Act as if it were an advanced AI system itself. By applying the principles of AT to the AI Act, the aim is to identify potential vulnerabilities and areas where the regulation might be enhanced. This methodological stance offers a unique perspective that contrasts with traditional legal analyses, focusing instead on dynamic interactions between regulatory frameworks and the technologies they intend to govern.\nThe EU AI Act [1] is one of the most comprehensive regulatory efforts on achieving a safer AI, a pioneering effort aimed at creating a comprehensive legal framework for the management and oversight of AI technologies within the European Union. It categorizes AI systems according to their risk levels (see Appendix A) and sets out corresponding requirements to mitigate those risks. Despite its ambitions, there are concerns that the Act may not fully encapsulate the complexity of real-world AI applications or effectively guard against the rapid evolution of AI capabilities.\nAs there already is a rich body of literature in AT, we will heavily draw on it for lenses to look through. While the focus is on four main issues identified in AT research (proxy gaming [8], goal"}, {"title": "2 AI systems for Education", "content": "This section examines the current regulatory framework outlined in the AI Act, evaluating its effectiveness in addressing the complex challenges posed by AI-driven educational services. It also identifies potential areas for regulatory improvement to better protect individual rights and ensure equitable access to education. For a more detailed discussion on specific use cases and potential failure modes from AT, refer to Appendix A.\nThe integration of AI systems into education marks a pivotal point where technology intersects with key aspects of individual autonomy, career opportunities, and personal development. Education serves as the bedrock for accessing essential services, such as housing, loans, or health insurance, as well as for determining societal roles and responsibilities. Given the profound impact of AI in this domain, rigorous oversight is essential to prevent misuse and to uphold fairness."}, {"title": "2.1 Current legal considerations", "content": "According to the AI Act classification, deploying AI Systems for Education is considered a High risk level. As in the case of any high risk use case, the regulation states (Art. 8-17) that AI providers must abide to the general prescriptions detailed in A.3. In particular, this is what the regulation says about deploying these tools in the context of Education (Art. 57):\nThe deployment of AI systems in education is important to promote high-quality digital education and training and to allow all learners and teachers to acquire and share the necessary digital skills and competences, including media literacy, and critical thinking, to take an active part in the economy, society, and in democratic processes. However, AI systems used in education or vocational training, in particular for determining access or admission, for assigning persons to educational and vocational training institutions or programmes at all levels, for evaluating learning outcomes of persons, for assessing the appropriate level of education for an individual and materially influencing the level of education and training that individuals will receive or will be able to access or for monitoring and detecting prohibited behaviour of students during tests should be classified as high-risk Al systems, since they may determine the educational and professional course of a person's life and therefore affect that person's ability to secure a livelihood."}, {"title": "2.2 Looking through the AT lens", "content": "Given that education relies on what the societal demands are and that these are, in part, shaped by the way in which citizens are educated, we think it might be informative to inspect this feedback loop through the lens of Goal Drift [9]:"}, {"title": "2.3 Open issues in the current legislation and potential solutions", "content": "We believe that some issues when using AI systems in Education are shared with those arising from the use of AI in Recruitment (see A.4). Particularly: Focus on measurable metrics over true merit-based selection and Inflexible assessment criteria are overlapping issues between these use cases. It is thus to be expected that developing a tailored regulation to deal with one of them can serve as a guideline for the other.\nApart from those two shared classes of potential problems, we have identified the following ones, specific to AI in Education:\nImbalanced curriculum requirements: How to ensure that the AI system supports a balanced curriculum that includes subjects that are more subjective to evaluate (like arts, humanities, or social sciences) in addition to more objectively assessed ones (STEM subjects). Potential solutions include: I) Developing rubrics that assess not only factual knowledge but also critical thinking, creativity, and interpretative skills in subjects like arts and humanities [11]. II) Incorporating qualitative assessment methods (e.g. peer reviews, reflective essays, and portfolio assessments) to evaluate student performance in subjective areas [12]. III) Designing AI systems to facilitate projects that integrate multiple disciplines, encouraging students to apply STEM concepts in arts and humanities contexts, and vice versa. IV) Encouraging collaborative assignments [13] that require students to work in teams across different subjects, fostering a more comprehensive understanding and appreciation of various fields.\nIncreased monitoring and surveillance: The current version of the AI Act has requirements for record-keeping and monitoring that do not fully address the potential negative impacts of increased surveillance on students' mental health and sense of autonomy. This can create a stressful learning environment. Institutions could: I) Establish clear safeguards to limit the extent and intrusiveness of monitoring [14]. II) Implement policies that restrict monitoring to only what is necessary for educational purposes [15]. III) Conduct regular assessments to monitor the impact of AI systems on student mental health and adjust practices accordingly [16]."}, {"title": "3 Conclusions", "content": "The European Union's Artificial Intelligence Act (EU AI Act) represents a significant step towards regulating AI technologies and mitigating associated risks. However, this paper has identified several potential vulnerabilities and areas for improvement through the lens of Alignment Theory research. Key issues might arise when regulatory systems are not properly aligned with their intended objectives. In this particular case, these potential issues could undermine the AI Act's goals of ensuring safety, fairness, and ethical behavior in AI deployment.\nIn this paper, we have explored four specific risky use cases (one in the main text and three more in the Appendix A) and shown how the current regulatory effort might be misaligned with its objectives, due to different failure modes \u2013 inspired by relevant literature from AI Alignment Theory. In the main case (Section 2), we argue that systems might comply with regulatory requirements but fail to adapt to societal changes, and might indeed accelerate goal drifts. As potential solutions, we propose to address I) Imbalanced Curriculum Requirements and II) Increased Monitoring and Surveillance.\nUndoubtedly he path forward requires a collaborative and adaptive approach to regulation, ensuring that as AI technologies advance, they do so in a manner that upholds the values and principles of our society. We believe that if the EU proactively addresses the identified vulnerabilities and incorporating the proposed solutions, the EU AI Act can serve as a robust framework for the responsible governance of AI. This will not only protect individuals and communities from potential harms but also pave the way for innovative and beneficial AI applications that enhance the quality of life across Europe. The commitment to continuous improvement and ethical oversight will be key to achieving these goals, ensuring that the EU remains at the forefront of AI safety and governance."}, {"title": "A Supplemental Material", "content": ""}, {"title": "A.1 Definitions from AI Alignment Theory", "content": "There is an extensive body of literature from which we can draw concepts or inspiration. Although we have focused on four main ideas, by no means they are the only ones that can potentially be informative when inspecting regulatory frameworks. We have taken all of the following definitions from relevant sources in the Alignment Theory (AT) literature:\nProxy Gaming: behavior by which AI systems exploit measurable \u201cproxy\u201d goals to appear successful, but act against our intent [8]. In the context of the AI Act, developers might focus on meeting the compliance requirements set out in the regulation rather than genuinely ensuring safety or ethical behavior.\nGoal Drift: scenario where an AI's objectives drift away from those initially set, especially as they adapt to a changing environment [9]. As the global AI paradigm evolves, the EU's initial regulatory goals might shift, potentially leading to unintended consequences.\nReward Hacking: phenomenon where optimizing an imperfect proxy reward function, leads to poor performance according to the true reward function [8]. The incentive structures created by the regulation might be exploited in ways that do not align with its intended goals.\nSpecification Gaming: behavior that satisfies the literal specification of an objective without achieving the intended outcome [10]. AI systems might technically comply with the legislation but fail to achieve the intended outcomes."}, {"title": "A.2 Risk levels from the EU AI Act", "content": "The AI Act introduces four different risk levels for AI systems, and different rules apply to systems belonging to each of these levels [1]:\nUnacceptable risk: These applications are incompatible with fundamental human rights in general and with EU values in particular. Accordingly, systems that are related to these topics (see examples), will be banned in the EU. Examples of this class are subliminal manipulation, biometric characterization of persons through sensitive features, emotional state assessment of a person...\nHigh risk: systems that are predominantly utilized in critical sectors (healthcare, transportation, law enforcement, ...). They must pass rigorous conformity assessments to verify their accuracy, robustness, and cybersecurity. Deployment of these systems is heavily regulated to minimize associated risks. Some examples include energy supply, recruitment tools, credit scoring, grading technology for education...\nLimited risk: Systems with particular needs of transparency, so that the user is explicit about being aware that they are interacting with an AI. According to the regulation chatbots or image generators are examples of this risk level.\nMinimal/no risk: anything that does not already fall into any of the previous three categories. Minimal compliance needs. AI-based systems that are already deployed belong to this risk level, such as spam filters, AI-enabled video games..."}, {"title": "A.3 Remote Biometric Identification Systems (RBIS) in public spaces", "content": ""}, {"title": "Current legal considerations", "content": "The definition that the AI Act introduces (Art. 17) for RBIS, considered to be an Unacceptable risk, is:\n[...] an Al system intended for the identification of natural persons without their active involvement, typically at a distance, through the comparison of a person's biometric"}, {"title": "Looking through the AT lens", "content": "In this case, Proxy Gaming [8] is an interesting tool with which to frame potential issues that might be derived from this legislation:"}, {"title": "Open issues in the current legislation and potential solutions", "content": "Derived from the previous example and general insights provided by the Proxy Gaming framing, there are several potential open issues that the regulation, in its current form, does not consider:\nSelective processing: The regulation mandates accuracy and non-discrimination but does not address the potential for systems to selectively process data to maintain these metrics. This can result in a focus on ideal conditions, neglecting the need for the system to perform well across diverse real-world scenarios. As a potential solution: Establish regulatory requirements that mandate AI systems to demonstrate consistent performance across a range of conditions, including low light, crowded environments, and varying image qualities. This can be achieved by: I) setting standardized tests that evaluate system performance in different real-world scenarios [17, 18]. II) requiring AI developers to provide detailed performance reports under diverse conditions as part of the compliance documentation [19].\nHandling of challenging inputs: Similarly to the previous point, the current version of the regulation requires high accuracy but does not ensure that systems are equally effective in less-than-ideal conditions, such as low light or crowded spaces. Potential solution: Incorporate robustness testing [20] in the regulatory framework that specifically evaluates AI systems' performance in challenging conditions. This can include: Testing under various environmental factors like lighting, weather conditions, and crowd density or including stress tests that simulate extreme scenarios to ensure the system's reliability.\nEthical use and compliance: While the regulation prohibits certain uses and mandates compliance, it does not provide sufficient oversight mechanisms to ensure that systems do not exploit loopholes to maintain compliance metrics. This can be potentially mitigated if one ensures that the datasets used for training, validation, and testing include diverse and representative samples. This can be implemented by: I) Requiring a minimum percentage of training data to come from challenging environments. II) Mandating periodic updates to training datasets to reflect real-world variations and emerging conditions.\nImpact on privacy and fundamental rights: The regulation emphasizes compliance with technical standards but does not sufficiently address the potential impacts on privacy and fundamental rights in varying deployment contexts. Potential solutions may include: I) Implementing regular audits and compliance reviews conducted by independent authorities [21] to monitor the ethical use of AI systems for RBIS. This can be achieved by: I) Establishing a schedule for periodic audits to ensure ongoing compliance. II) Creating a framework for surprise inspections and random checks to deter complacency and unethical practices. III) Requiring detailed reporting from AI developers and deployers that includes information on system performance, use cases, and instances of non-compliance [22]. IV) Including requirements for thorough impact assessments that evaluate the potential effects on privacy and fundamental rights before deploying AI systems [23].\nPrescriptions for high risk use cases\nThese are the general rules that these systems have to comply with:"}, {"title": "A.4 AI Systems for Recruitment", "content": ""}, {"title": "Current legal considerations", "content": "The AI Act considers that AI Systems in Recruitment is a case of a High risk (AIA - 2) scenario. This is what the regulation says about deploying these tools (Art. 57):\n[... Al systems for recruitment] should also be classified as high-risk, since they may have an appreciable impact on future career prospects, livelihoods of those persons and workers' rights. [...] Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination [...]. Al systems used to monitor the performance and behaviour of such persons may also undermine their fundamental rights to data protection and privacy."}, {"title": "Looking through the AT lens", "content": "Recruitment is a context in which automated metrics could be socially and economically dangerous, hence, we will resort to Reward Hacking [8] as a tool to inspect this part of the law."}, {"title": "Open issues in the current legislation and potential solutions", "content": "Focus on measurable metrics over true merit-based selection: The AI Act mandates accuracy and non-discrimination but does not address the potential for systems to prioritize easily measurable proxies (like keyword matching) over genuine merit-based selection. This can result in a focus on candidates who can game the system rather than those who are genuinely the best fit for the role. Potential solutions might rely on:\n\u2022 Holistic evaluation metrics: Develop and implement more comprehensive evaluation metrics [24, 25] that go beyond keyword matching to assess candidates' true skills and experiences. This can include: I) Using advanced natural language processing (NLP) techniques to understand the context and substance of resumes. II) Incorporating behavioral assessments, skills tests, and situational judgment tests to evaluate candidates holistically.\n\u2022 Continuous learning and adaptation: Ensure that the AI system is capable of learning and adapting its evaluation criteria based on feedback from successful hires and their job performance. This can be achieved by: I) Regularly updating the AI model with data from post-hire performance reviews. II) Implementing machine learning algorithms that can adjust their criteria based on real-world outcomes [26].\nInflexible assessment criteria: The regulation requires relevant and representative datasets but does not mandate flexibility in assessment criteria to accommodate diverse expressions of skills and experiences. This rigidity can disadvantage candidates who do not use standard resume formats or buzzwords, even if they are highly qualified. We can think of the following solutions:\n\u2022 Flexible and adaptive criteria: Design AI systems with flexible criteria that can recognize diverse ways of demonstrating skills and experience. This can include: I) Implementing algorithms that can parse and understand various resume formats and styles. II) Using"}, {"title": "A.5 AI systems in financial profiling", "content": ""}, {"title": "Current legal considerations", "content": "According to the AI Act classification, deploying AI Systems in financial profiling is a case of a High risk (AIA - 2) context. Particularly, the regulation currently states (Art. 58):\n[...] Al systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk Al systems, since they determine those persons' access to financial resources or essential services such as housing, electricity, and telecommunication services. [...] Moreover, Al systems intended to be used for risk assessment and pricing in relation to natural persons for health and life insurance can also have a significant impact on persons' livelihood and if not duly designed, developed and used, can infringe their fundamental rights and can lead to serious consequences for people's life and health, including financial exclusion and discrimination.\nYet, in some cases this caution does not apply, as the same article (Art. 58) specifies:\n[...] However, Al systems provided for by Union law for the purpose of detecting fraud in the offering of financial services and for prudential purposes to calculate credit institutions' and insurance undertakings' capital requirements should not be considered to be high-risk under this Regulation.\nHence, even if we fully overlook those cases that are not considered to be high-risk by this legislation, there are several unattended points that need further oversight, as we shall show next."}, {"title": "Looking through the AT lens", "content": "Badly implemented financial profiling could rapidly lead to dystopian societies; consequently, we will use the idea of Specification Gaming [10] to identify potential issues in this section of the AI Act:"}, {"title": "Open issues in the current legislation and potential solutions", "content": "Building on this analysis of specification gaming, it becomes clear that the AI Act, while a regulatory advancement, leaves several critical gaps unaddressed. These gaps not only allow for the exploitation of regulatory requirements but also perpetuate deeper systemic issues such as reliance on proxy indicators, bias, and lack of transparency:\nOver-reliance in proxy indicators. The regulation mandates accuracy and transparency but does not address the potential for AI systems to over-rely on easily measurable but potentially unfair proxy indicators. To potentially solve this, the competent authorities could I) develop and mandate the use of metrics that provide a more panoramic view of an individual's financial health, beyond just easily measurable indicators [28]; II) ensure that the indicators used by AI systems are regularly reviewed and adjusted to reflect fairness and equity considerations [29].\nBias perpetuation. Although the current regulation emphasizes non-discrimination, it does not provide sufficient mechanisms to continually assess and mitigate biases embedded in historical data. To mitigate this issue, we suggest that developers I) implement continuous monitoring mechanisms to detect and mitigate biases periodically [30]; II) develop protocols for correcting identified biases, including retraining or fine-tuning AI models with de-biased data [31].\nTransparency and Explainability. While the AI Act does require transparency, it does not ensure that the decision-making processes of AI systems are fully explainable to users. We encourage policymakers to I) require AI systems to provide clear, understandable explanations for their credit decisions to affected individuals [32]; II) implement educational programs to help users understand AI-driven credit assessments and their rights under the regulation [33]."}]}