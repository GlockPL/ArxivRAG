[{"title": "S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity", "authors": ["Xinyu Yang", "Jixuan Leng", "Geyang Guo", "Jiawei Zhao", "Ryumei Nakada", "Linjun Zhang", "Huaxiu Yao", "Beidi Chen"], "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S2FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S2FT accomplishes this by \u201cselecting sparsely and computing densely\". It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S2FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S2FT saves training memory up to $3\\times$ and improves latency by 1.5-2.7$\\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S2FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.", "sections": [{"title": "Introduction", "content": "Recently, Large Language Models (LLMs) have achieved significant success [16, 1, 64]. With these models being applied in diverse domains, full fine-tuning (FT) is commonly employed to enhance their downstream capabilities [54, 6, 71]. However, retraining all parameters comes with three drawbacks: (i) Full FT suffers from catastrophic forgetting, where a model forgets pre-trained knowledge while acquiring new information [44, 8]. (ii) As the model and dataset sizes grow at scale, full FT becomes increasingly computation-demanding and memory-intensive [68]. (iii) It is impractical to store and serve thousands of fine-tuned LLMs on modern GPUs if each requires full parameter storage [77, 58].\nParameter-efficient fine-tuning (PEFT) methods propose to address these bottlenecks by updating a small fraction of parameters [21]. Rather than merely reducing the number of learnable parameters, an ideal PEFT method should possess three key properties to be practically effective and efficient:\nHigh Quality: It should exhibit both memorization and generalization capabilities, balancing the acquisition of new information from fine-tuning tasks with the retention of pre-trained knowledge.\nEfficient Training: It should minimize the memory footprint for model gradient and optimization states, and further translate such memory efficiency into less computation and fine-tuning speedup.\nScalable Serving: It should avoid adding inference overhead when serving a single PEFT model. For multiple models, new parameters should be partially stored as adapters to save memory, and allows for effective fusion [75], fast switch [33], and efficient parallelism [58] among thousands of adapters."}, {"title": "Memorization or Generalization?", "content": "In this section, we evaluate the memorization and generalization capabilities of various fine-tuning methods, including full FT, LoRA, and SpFT. We hypothesize that SpFT can generalize better to downstream tasks. To support this hypothesis, we present detailed observations and analyses. Further theoretical analysis about the generalization capabilities of the S2FT family can be found in Section 4.\nHypothesis. We hypothesize that SpFT offers superior generalization than both full FT and LoRA, while maintaining comparable memorization to LoRA with the same number of trainable parameters.\nExperimental Setup. We fine-tune the Llama3-8B on the Math10K data [28] using SpFT, LoRA, and full FT. In addition to training losses, accuracies are measured on downstream tasks in LLM-Adapters, including near out-of-distribution (OOD) generalization on both easy (i.e, MultiArith, AddSub, SingleEq, MAWPS) and hard (i.e, GSM8K, AQUA, SVAMP) arithmetic reasoning tasks, and far OOD generalization on commonsense reasoning ones. For PEFT methods, we set three ratios of trainable parameters (p = 10%, 1%, 0.1%) and search for the optimal hyperparameters on the valid set. In SpFT, trainable parameters are selected randomly with given ratios. See details in Appendix C.\nObservations. Figure 2 indicates several key findings. First, SpFT achieves lower training losses than LoRA when using the same ratio of trainable parameters, especially at very small ratios. This gap arises from the more complex optimization process in LoRA, which requires the simultaneous updating of two matrices [23]. Second, we observe both elevated training loss and reduced average accuracy on easier math tasks as the ratio decreases, suggesting a positive correlation between memorization abilities and trainable parameters. Notably, with only 10% of the parameters updated, PEFT methods learn comparable memorization abilities to full FT when trained on a 10k-sample dataset.\nWhen generalizing to complex mathematical problems or commonsense reasoning tasks, the performance ranking emerges as: SpFT > Full FT > LoRA. SpFT effectively transfers reasoning abilities to commonsense domains, while LoRA exhibits significant performance drops in far OOD generalization. This indicates (i) freezing a larger fraction of the parameters can retain more pre-trained abilities, and (ii) approximating high-dimensional gradients with low-rank decomposition may overfit fine-tuned data and hinder the model from generalization. Since LLMs are pre-trained on high-quality data, SpFT emerges as the preferred choice for fine-tuning on task-specific data of varying quality."}, {"title": "The S2FT family of methods", "content": "While SpFT demonstrates strong generalization ability and good overall performance in Section 2, its unstructured nature poses challenges for efficient training and scalable serving on modern hardware (e.g., GPU). This is because of the need for sparse operations when storing and computing weights, gradients, and optimization states, which are significantly slower than their dense variants on GPU. This motivates our investigation into structured sparsity approaches that utilize only dense operations:\nCan structured sparsity improve hardware efficiency while preserving performance by selecting sparsely but computing densely? If so, how far can the flexibility of selection be pushed in this context?\nTo answer this question, we design a family of Structured Sparse Fine-Tuning (S2FT) methods with dense-only computations, making PEFT effective, efficient and scalable. We begin by discovering the coupled structure in LLMs in Section 3.1. Leveraging this property, Section 3.2 introduce the selection and permutation strategies of S\u00b2FT, with overall pipeline illustrated in Figure 1b. In Section 3.3, we present our partial back-propagation algorithm that enables end-to-end training latency reduction."}, {"title": "Discover Coupled Structures in LLMs", "content": "We initiate our pursuit of flexible structured sparsity by examining the coupled structures in LLMs."}, {"title": "Sparse Selection and Permutation", "content": "At this point, all coupled structures within the model have been identified. The subsequent sparse selection and permutation processes are straightforward, with overall pipeline illustrated in Figure 1b.\nMHA Module: There are four linear layers in a MHA module: $Q, K, V, O \\in \\mathbb{R}^{d \\times d}$. For a model with $h$ attention heads, each head $i \\in [h]$ has its own projections denoted as $Q_i \\in \\mathbb{R}^{d \\times d_h}, K_i \\in \\mathbb{R}^{d \\times d_h}, V_i \\in \\mathbb{R}^{d \\times d_h}$, and $O_i \\in \\mathbb{R}^{d_h \\times d}$, where $d_h = d/h$ is the dimension per head. Let $S_{MHA} \\subseteq [h]$ denote a small subset of attention heads. By permuting $S_{MHA}$ to the beginning of each weight matrix, we are able to update these selected heads using dense-only operations, while keeping the other ones frozen.\nFFN Module: There are three linear layers in an FFN module: $U, G \\in \\mathbb{R}^{k\\times d}$ and $D \\in \\mathbb{R}^{d\\times k}$. In S2FT, only a few channels require gradient updates. Let $S_{FFN}\\subseteq [d]$ denote the selected channels. We can permute $S_{FFN}$ to the beginning of each weight matrix and only fine-tune this compact subset.\nNext, we provide several strategies for identifying and selecting important subsets in each module.\n1.  S2FT-R (S2FT): In this strategy, a subset of channels is randomly selected and set to be trainable.\n2.  S2FT-W: This variant selects subsets based on the magnitude of the weights for linear layers.\n3.  S2FT-A: This variant selects subsets based on the magnitude of activations on a calibration set.\n4.  S2FT-S: Top-K subsets are ranked and selected by the product of weight and activation magnitudes.\n5.  S2FT-G: This variant selects subsets based on the magnitude of gradients on a calibration set.\nHere, 1 and 2 can be applied directly without pre-processing. 3 and 4 only require a forward pass on a small calibration dataset. While 5 necessitates a backward pass on this dataset, it does not store optimization states and can mitigate memory footprints for activations through gradient checkpointing [18]. By default, we use S2FT-R for a fair comparison and discuss other variants in Section 5.4."}, {"title": "Partial Back-propagation Algorithm", "content": "Finally, we introduce our partial back-propagation algorithm with only two line modifications in PyTorch. our algorithm stores trainable channels based on their start and end positions, thereby improving training efficiency by eliminating redundant forward activations and backward calculations."}, {"title": "Theoretical Analysis", "content": "In this section, we theoretically explore why S2FT demonstrates stronger generalization capabilities compared to LoRA. We consider a pre-trained L-layer deep linear networks, which has been widely used to facilitate the theoretical analysis of complex DNNs [57, 30, 43, 22, 34, 5]. Let $f_{pre}(x) := W_{L}^{pre}W_{L-1}^{pre}...W_{1}^{pre}x$ be the pre-trained deep linear network, where $W_l^{pre} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$, with $d_0 = p$ and $d_L = q$. We fine-tune the l-th layer with low-rankness level $r < min{d_l, d_{l-1}}$ or sparsity level $s = \\lfloor r \\cdot d_l + d_{l-1} \\rfloor$. Denote a class of adaptation with parameters $U \\in \\mathbb{R}^{d_l \\times d}$ and $V \\in \\mathbb{R}^{d_{l-1} \\times d}$ as\n$f_{l,U,V}(x) := W_{L}^{l-1} (W_l^{pre} + UV^T)W_{l+1}^{pre}W_{l-1}^{0}x,$\nwhere $W_l^{0} := W_1^{pre}W_2^{pre}...W_{l-1}^{pre} \\in \\mathbb{R}^{d_L \\times d_{l-1}}$ and $W_l^{L} := W_{l+1}^{pre}W_{l+2}^{pre}...W_{L}^{pre} \\in \\mathbb{R}^{d_L \\times d_0}$ with $W_0^{pre} = I_p$ and $W_L^{pre} = I_q$. In a transformer-based LLM, each row of $W_l$ can represents either the parameters in a single head for the MHA module or that in a single channel for the FFN module.\nGiven n observations $(x^{(i)}, y^{(i)}) \\in \\mathbb{R}^p \\times \\mathbb{R}^q$, we fine-tune $f_{pre}$ by minimizing the empirical risk $R_n(f_{l,U,V}) := (1/n) \\sum_{i \\in [n]} || y^{(i)} - f_{l,U,V}(x^{(i)}) ||_2^2$ via gradient descent. For LoRA, we train both low-rank matrices $(U, V)$ in Equation (3) with $d \\leftarrow r$. For S2FT, we train only $V$ in Equation (3) with $d \\leftarrow s$ and fixed $U \\leftarrow U^{S2FT} := [e_{a_1}; e_{a_2}; ... ; e_{a_s}]$, which specifies s rows to fine-tune, where $S = {a_1,...,a_s} \\subset [d_l]$ and $e_a$ is the a-th standard basis. Motivated from the results that gradient descent has implicit regularization [74, 19, 5], we directly consider the minimum norm solutions.\nWe consider a multiple linear regression setting. Assume that the in-distribution training data $(x^{(i)}, y^{(i)}) \\in \\mathbb{R}^{p+q}$ and out-of-distribution test data $(x^{(o)}, y^{(o)}) \\in \\mathbb{R}^{p+q}$ are generated i.i.d. according to\n$y^{(k)} = B^{(k)}x^{(k)} + \\epsilon^{(k)}, k \\in {i, o},$\nwhere $B^{(k)} \\in \\mathbb{R}^{q\\times p}$ is the coefficient matrix, $x^{(k)}$ and $\\epsilon^{(k)}$ are mean zero sub-Gaussian signal and noise with covariance matrices $\\Sigma_x^{(k)}$ and $\\Sigma_\\epsilon^{(k)}$, respectively. The generalization capacity is measured by the fine-tuned model's excess risk $\\mathcal{E}(f) := \\mathbb{E}[||y^{(o)} - f(x^{(o)})||_2^2] - \\inf_{f'} \\mathbb{E}[||y^{(o)} - f'(x^{(o)})||_2^2]$.\nFor these OOD data, LoRA suffers from forgetting, while S2FT can maintain pre-training knowledge.\nAssumption 4.1 (Distribution Shift). Assume that $\\Sigma_x^{(o)} = \\Sigma_x^{(i)} = \\Sigma_x$ for some $\\Sigma_x \\in \\mathbb{R}^{p\\times p}$, and $|| (W_{l+1}^{pre}U^{S2FT})(W_{l+1}^{pre}U^{S2FT})^+(B^{(o)} - B^{(i)})\\Sigma_x^{1/2} ||_F^2 < \\epsilon^2 \\mathcal{E}^{(o)}(f_{pre})$ for some $\\epsilon > 0$.\nAssumption 4.1 states that while the covariate distribution remains unchanged, the label distribution conditioned on covariates may shift, but not exceeding a factor of $\\epsilon^2$ of the OOD risk of $f_{pre}$. This holds for fine-tuning with proper channel selection, where primarily the output distribution is changed.\nTheorem 4.2 (Out-of-distribution Excess Risk, Informal). Suppose Assumption 4.1 holds. Consider $n \\rightarrow \\infty$. If $B^{(i)} = W_{l+1}^{pre}\\tilde{B}^{(i)}W_{l-1}^{pre}$ holds for some $\\tilde{B}^{(i)} \\in [\\mathbb{R}^{d_l\\times d_{l-1}}$, and $s, r < rank(\\Sigma_x)$, then,\n$\\mathcal{E}^{(o)}(f_{l,U^{S2FT}, V^{S2FT}}) \\leq (1 + 3\\epsilon^2)\\mathcal{E}^{(o)}(f_{pre}), \\quad \\mathcal{E}^{(o)}(f_{l, U^{LoRA}, V^{LoRA}}) \\geq ||(B^{(o)} - B^{(i)})\\Sigma_x^{1/2}||$.\nTheorem 4.2 indicates that the OOD risk of S\u00b2FT is bounded above by that of $f_{pre}$, while that of LORA is bounded below by the label shift magnitude. If $f_{pre}$ already has low risk for OOD tasks, and the label shift is significant, S\u00b2FT is expected to outperform LoRA. Essentially, when the OOD task deviates significantly from the FT distribution, LoRA may forget the pre-trained knowledge and overfit to the FT data, compromising its generalization capabilities. See formal statements in Theorem F.8."}, {"title": "Experiments", "content": "In this section, we conduct a series of experiments across three diverse benchmarks covering more than 20 datasets. Our goal is to provide a rich picture of how S2FT performs in different scenarios. Here, we compare our method with different fine-tuning strategies and categories including: (i) Full fine-tuning (FT), (ii) reparameterized fine-tuning: LoRA [27], DORA [38], and Galore [76], (iii) adapter-based fine-tuning: Series Adapter [26], Parallel Adapter [24], and LoReFT [67], (iv) prompt-based fine-tuning: Prefix-Tuning [36], (v) sparse fine-tuning: LISA [48]. For a fair comparison, we keep a comparable number of trainable parameters in S2FT to that of LoRA. The design choices for trainable parameter allocations in S2FT will be detailed in Section 5.4. All other hyperparameters are selected via cross-validation. Detailed setups and baseline descriptions are provided in Appendix E."}, {"title": "Commonsense Reasoning", "content": "Dataset Descriptions. The commonsense reasoning dataset comprise eight subsets: BoolQ [12], PIQA [9], SocialQA [56], HellaSwag [73], WinoGrande [55], ARC-challenge [13], ARC-easy [13], and OpenbookQA [46]. Following the experimental setup of LLM-Adapters [28], we split each dataset into training and test sets. Subsequently, we combine the training data from all eight tasks into a single fine-tuning dataset and evaluate performance on the individual test dataset for each task.\nResults. Table 1 showcases that S2FT consistently outperforms existing PEFT methods across the LLAMA-7B/13B, LLaMA2-7B and LLAMA3-8B models. When compared to LoRA and DoRA, it achieves average performance gains of 4.6% and 2.8%, respectively. Additionally, S2FT also demonstrates superior performance against recent approaches including Galore, LoReFT, and LISA, with accuracy improvements of at least 1.0%. Remarkably, despite using less than 1% of trainable parameters, our method surpasses full FT by 0.5%. The 3.0% improvement observed on the LLaMA3-8B suggests that maintaining most pre-trained parameters frozen enables better generalization to test distributions."}, {"title": "Arithmetic Reasoning", "content": "Dataset Descriptions. We followed Hu et al. [28] and evaluated S2FT on seven math reasoning tasks, including MultiArith [53], GSM8K [14], AddSub [25], AQUA [37], SingleEq [31], SVAMP [50] and MAWPS [32]. Our fine-tuning employed the Math10K dataset [28], which combines training sets from GSM8K, MAWPS, and AQuA, augmented with LM-generated chain-of-thought steps. Therefore, these three tasks are considered ID, while the remaining four are classified as OOD tasks.\nResults. As showcased in Table 2, S2FT consistently outperforms other PEFT methods for different models. On average, it achieves improvements of 1.3% and 0.9% over LoRA and DoRA, respectively. These results highlight the versatility and effectiveness of our approach across a diverse range of tasks. Additionally, we observe substantial improvements even when compared to Full FT for the LLAMA3-8B model, particularly on complex tasks such as GSM8K and AQuA. This suggests that S2FT better preserves the original reasoning capabilities of this stronger model while acquiring new skills from the fine-tuning data, thereby validating the enhanced generalization ability of our method."}, {"title": "Instruction Following", "content": "Dataset Descriptions. To further showcase S2FT's superior generalization ability, we employ the instruction-following fine-tuning task with Alpaca GPT-4 dataset, which comprises 52k samples generated by GPT-4 [2] based on inputs from Alpaca [63]. Performance is measured on MT-Bench [78], featuring 80 high-quality, multi-turn questions designed to assess LLMs on eight different aspects.\nResults. Table 3 offers a comprehensive evaluation of Full FT, LoRA, LISA, and S\u00b2FT across various tasks in the MT-Bench benchmark, including Writing, Roleplay, Reasoning, Code, Math, Extraction, STEM, and Humanities. It is observed that S2FT > LISA > Full FT > LoRA/Galore > Vanilla for both Mistral-7B and LLama2-7B. This is because sparse FT methods like S\u00b2FT and LISA retain more pre-trained knowledge while acquiring new skills on the FT dataset, thereby generalizing better to diverse tasks in MT-Bench. Moreover, our method outperforms LISA due to its fine-grained and flexible selection strategy, enabling all layers to learn to follow instructions on the full fine-tuning set."}, {"title": "Design Choices for Trainable Parameter Allocations", "content": "Finally, we detail how S2FT distribute trainable parameters across layers, modules, and channels.\nUniform across Layers: Following Chen et al. [10], we allocate parameters to each layer uniformly."}, {"title": "Analysis", "content": "Having demonstrated the strong generalization capability and overall performance of S2FT, we now further explore its training efficiency and serving scalability compared to other fine-tuning techniques."}, {"title": "Training Efficiency", "content": "To evaluate training efficiency, we examine two crucial metrics: peak memory footprint and average training latency. These numbers are measured on a single Nvidia A100 (80G) SXM GPU. We keep a comparable number of parameters for all methods. To obtain the average latency, we fine-tune the model for 50 runs, each run including 200 iterations, with 10 warmup runs excluded in measurement.\nIn Figure 5, we thoughtfully profile S\u00b2FT on various model sizes, sequence lengths, and batch sizes. Compared to Full FT, S2FT saves 1.4-3.0$\\times$ memory, and speedups fine-tuning by 1.5-2.7 times. When benchmarked against other PEFT methods, S2FT establishes new standards for efficiency, offering average reductions of 2% in memory usage and 9% in latency. Notably, S\u00b2FT outperforms the widely adopted LoRA, achieving about 10% improvement in both matrics by avoiding the need to store new parameters and perform additional calculations. Our partial back-propagation algorithm further improves efficiency by saving unnecessary forward activations and backward calculations."}, {"title": "Serving Scalability", "content": "While S2FT avoids additional inference overhead for a single fine-tuned model through in-place gradient updates, we will now discuss its scalability for serving thousands of fine-tuned models. To begin, we introduce the unmerged computation paradigm of S\u00b2FT: Given a pre-trained weight matrix $W^{pre} \\in \\mathbb{R}^{d\\times k}$ and its corresponding fine-tuned weight matrix $W$ with sparsity level $s$, we define the weight difference as $\\Delta W = W - W^{pre}$. Similar to Section 4, $\\Delta W$ can be decomposed into the product of a weight matrix $V \\in \\mathbb{R}^{k\\times s}$ and a permutation matrix $U \\in \\mathbb{R}^{d\\times s}$. This decomposition allows us to \u201cunmerge\" an adapter $\\Delta W = UV^T$ from $W$, thereby sharing similarities with other adapters during inference. Following Zhong et al. [79], we consider three different adapter composition scenarios:\nAdapter Fusion. To combine knowledge from multiple trained adapters, we employ weighted fusion when fine-tuning is impractical due to limited data access or computational resources. However, this approach degrades performance. In Table 5, we compare the effectiveness of LoRA and S\u00b2FT when combining adapters trained separately on commonsense and arithmetic reasoning tasks, where we consider both fine-tuning overlapped and non-overlapped parameters for different adapters in S\u00b2FT. Our results show that S2FT with non-overlapped parameters achieves the best performance, while the overlapped variant shows inferior results. This is because S2FT (non-overlap) modifies orthogonal low-rank spaces for different tasks. Similarly, LoRA largely retains task-specific capabilities during adapter fusion by optimizing low-rank projection matrices to create separate spaces for each adapter."}, {"title": "Limitations", "content": "While our work demonstrates the effectiveness of S2FT for LLM fine-tuning, several promising directions remain unexplored. First, extending S2FT to other architectures with coupled structures, such as CNNs and RNNs, can broaden its applicability. Second, verifying our approach beyond language tasks, particularly in large vision/multi-modal models, will enhance its versatility. Third, exploring more selection strategies can provide deeper insights into optimal fine-tuning protocols due to the controllability in S2FT. Fourth, scaling our method to larger models requires further experiments. Finally, although our work confirms the feasibility of scalable and efficient deployment during inference, developing a practical serving system for S2FT remains an important next step."}, {"title": "Broader Impacts", "content": "Since our work focuses on PEFT, it leads to a reduction in hardware resource and energy consumption. Given the growing adoption of LLMs across diverse domains and the corresponding surge in fine-tuning demands, S\u00b2FT should represent an important step toward more sustainable AI development."}, {"title": "Detailed Experimental Setups for Section 2", "content": "In this study, we used SpFT, LoRA, and Full FT to fine-tune the LLAMA-3-8B model on the Math10K dataset [28]. The Math10K dataset combines training sets from GSM8K [14], MAWPS [32], and AQUA [37], augmented with chain-of-thought steps generated by language models. We conducted training for 3 epochs with a batch size of 64. For both PEFT methods-SpFT and LoRA-we fine-tune with three ratios of trainable parameters for all linear layers: p = 10%, 1%, 0.1%. The model's performance is evaluated on both arithmetic and commonsense reasoning tasks, representing near out-of-distribution (OOD) and far OOD generalization scenarios, respectively. The arithmetic reasoning dataset comprises seven subtasks: MultiArith [53], GSM8K, AddSub [25], AQUA, SingleEq [31], SVAMP [50], and MAWPS. The commonsense reasoning dataset includes eight subtasks: BoolQ [12], PIQA [9], SocialQA [56], HellaSwag [73], WinoGrande [55], ARC-challenge [13], ARC-easy [13], and OpenbookQA [46]. Based on task complexity within arithmetic reasoning (accuracy > 90%), we group MultiArith, AddSub, SingleEq, and MAWPS as easy subtasks, while the remaining ones are classified as hard subtasks. This stratification enables us to evaluate whether the model develops advanced reasoning abilities beyond memorizing basic arithmetic operations from the training data."}, {"title": "Detailed Selection Strategies in Section 3", "content": "For the five selection strategies described in Section 3.2, we will detail the methods for identifying and selecting important subsets within each linear layer of both MHA and FFN modules in LLMs.\n1.  S2FT-R (S2FT): In this strategy, we will randomly select some heads for the MHA modules and select a few channels for the FFN modules. For the output projection, all channels in the selected heads will be included to enable dense-only computation. In the up and gate projections, we will select a subset of columns, while for the down projection, a few trainable rows will be chosen.\n2.  S2FT-W: This variant selects subsets based on the weight magnitudes (i.e., $||W||_2$) in the MHA and FFN modules. We will test subsets corresponding to both the largest and smallest weights.\n3.  S2FT-A: This variant selects subsets based on the magnitude of activations (i.e., $||A||_2$) on a calibration set, using 1% of the fine-tuning data. Since collecting activations requires only forward passes, this approach maintains the same memory footprint as inference and incurs a negligible increase in training time. Similarly, we evaluate both the largest and smallest activation variants.\n4.  S2FT-S: The Top-K subsets are ranked and selected by the product of the weight and activation magnitudes (i.e, $||W||_2 \\cdot ||A||_2$). The activation values are collected in a manner similar to S2FT-A.\n5.  S2FT-G: This variant selects subsets based on the magnitude of gradients on the calibration set. Since gradients are collected without updating the model, we calculate and discard gradients layer by layer during back-propagation similar to Galore [76], requiring minimal additional memory."}, {"title": "Detailed Experimental Setups and Hyperparamters for Section 5", "content": "The detailed selection strategies and number of trainable parameters are presented in Section 5. Additional hyperparameter configurations for all tasks are provided in Table 6. We maintain the same hyperparameter settings across the LLaMA-7/13B, LLaMA2-7B, LLAMA3-8B, and Mistral-7B models."}, {"title": "Proofs for Theoretical Results in Section 4", "content": "Here we provide proofs for the results in Section 4."}, {"title": "Notation", "content": "For a vector a, let $||a||$ be the $l_2$ norm of a. For $d_1 \\geq d_2$, denote a set of orthogonal matrices by $\\mathcal{O}_{d_1,d_2} := {R \\in \\mathbb{R}^{d_1 \\times d_2} : R^TR = I_{d_2}}$. For a matrix $A \\in \\mathbb{R}^{d_1 \\times d_2}$, let $||A||_F$ and $||A||_{op}$ be the Frobenius norm and spectral norm of A, respectively. Denote the condition number of $A$ by $\\kappa^*(A) := ||A||_{op}/\\lambda_*(A)$. Let $A^\\dagger$ be Moore-Penrose inverse of A. For a symmetric matrix $A$, denote its effective rank by $r_e(A) := tr(A)/||A||_{op}$. Note that $r_e(A) \\leq rank(A)$ always holds. For $a, b \\in \\mathbb{R}$, we let $a \\vee b := max(a, b)$ and $a \\wedge b := min(a, b)$. For a matrix $A \\in \\mathbb{R}^{d_1 \\times d_2}$, let $SVD_r(A) := \\Phi_r(A)\\Lambda_r(A)\\Psi_r(A)$ be the top-r singular value decomposition of A, where $\\Phi_r(A) \\in \\mathcal{O}_{d_1,r}$ and $\\Psi_r(A) \\in \\mathcal{O}_{d_2,r}$ are top-r left and right singular vectors of A, respectively, and $\\Lambda_r(A) = diag(\\lambda_1(A), ..., \\lambda_r(A)) \\in \\mathbb{R}^{r\\times r}$ is a diagonal matrix of singular values of A, where $\\lambda_j(A)$ denotes the j-th largest singular value of A. Define $\\Phi_*(A) := \\Phi_{rank(A)}(A)$ and $\\Psi_*(A) := \\Psi_{rank(A)}(A)$ as the left and right singular vectors of A corresponding to non-zero singular values, respectively. Define the smallest positive singular value of A as $\\lambda_*(A) = \\lambda_{rank(A)}(A)$ and let $\\Lambda_*(A) = \\Lambda_{rank(A)}(A)$.\nFor a deep learning model fine-tuned on n i.i.d. samples $(x^{(i)}, y^{(i)}) \\in \\mathbb{R}^p \\times \\mathbb{R}^q$, we say an event $\\mathcal{F}$ occurs with high probability when $\\mathbb{P}(\\mathcal{F}) = 1 - exp(-\\Omega(log^2(n + p + q)))$."}, {"title": "Setup", "content": "We consider multivariate regression task. Using n i.i.d. samples $(x^{(i)}, y^{(i)}) \\in \\mathbb{R}^p \\times \\mathbb{R}^q$ from in-distribution task, we fine-tune a pre-trained network $f_{pre} : \\mathbb{R}^p \\rightarrow \\mathbb{R}^q$ for better prediction."}, {"title": "Deep Linear Networks", "content": "We consider deep linear networks of the form $x \\rightarrow W_LW_{L-1}... W_1x : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$, where $W_l \\in \\mathbb{R}^{d_l \\times d_{l-1}}$, with $d_L = q$ and $d_0 = p$. In comparison to multi-head attention transformers, each row of $W_l$ can be viewed as corresponding to the parameters in a single head. Let $f_{pre}(x) = W_L^{pre}W_{L-1}^{pre}...W_1^{pre}x : \\mathbb{R}^p \\rightarrow \\mathbb{R}^q$ represent a pre-trained neural network. We denote $W_l^{0} := W_1^{pre}W_2^{pre}...W_{l-1}^{pre} \\in \\mathbb{R}^{d_L \\times d_{l-1}}$ as the weights up to the l-th layer, and $W_l^{L} := W_{l+1}^{pre}W_{l+2}^{pre}...W_L^{pre} \\in \\mathbb{R}^{d_L \\times d_0}$ as the weights above the l-th layer, with the promise that $W_0^{pre} = I_p$. Deep linear networks have been widely used to facilitate the theoretical analysis of modern complex deep neural networks [57, 30, 43, 22, 34, 5]."}, {"title": "Fine-Tuning", "content": "We employ $l_2$ distance as the error metric. Given a pre-trained network $f_{pre"}, "we fine-tune its l-th layer by minimizing the empirical in-distribution risk $R_n(f) := (1/n) \\sum_{i\\in[n"]}, {}]