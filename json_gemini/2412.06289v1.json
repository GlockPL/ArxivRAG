{"title": "S2FT: Efficient, Scalable and Generalizable LLM\nFine-tuning by Structured Sparsity", "authors": ["Xinyu Yang", "Jixuan Leng", "Geyang Guo", "Jiawei Zhao", "Ryumei Nakada", "Linjun Zhang", "Huaxiu Yao", "Beidi Chen"], "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training,\nor scalable serving, but not all three simultaneously. To address this limitation, we\ninvestigate sparse fine-tuning and observe a remarkable improvement in generaliza-\ntion ability. Utilizing this key insight, we propose a family of Structured Sparse\nFine-Tuning (S2FT) methods for LLMs, which concurrently achieve state-of-the-\nart fine-tuning performance, training efficiency, and inference scalability. S2FT\naccomplishes this by \u201cselecting sparsely and computing densely\". It selects a few\nheads and channels in the MHA and FFN modules for each Transformer block,\nrespectively. Next, it co-permutes weight matrices on both sides of the coupled\nstructures in LLMs to connect the selected components in each layer into a dense\nsubmatrix. Finally, S2FT performs in-place gradient updates on all submatrices.\nThrough theoretical analysis and empirical results, our method prevents overfitting\nand forgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after instruction\ntuning. Using our partial backpropagation algorithm, S2FT saves training memory\nup to 3\u00d7 and improves latency by 1.5-2.7\u00d7 compared to full FT, while delivering\nan average 10% improvement over LoRA on both metrics. We further demonstrate\nthat the weight updates in S2FT can be decoupled into adapters, enabling effective\nfusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) have achieved significant success [16, 1, 64]. With these\nmodels being applied in diverse domains, full fine-tuning (FT) is commonly employed to enhance their\ndownstream capabilities [54, 6, 71]. However, retraining all parameters comes with three drawbacks:\n(i) Full FT suffers from catastrophic forgetting, where a model forgets pre-trained knowledge while\nacquiring new information [44, 8]. (ii) As the model and dataset sizes grow at scale, full FT becomes\nincreasingly computation-demanding and memory-intensive [68]. (iii) It is impractical to store and\nserve thousands of fine-tuned LLMs on modern GPUs if each requires full parameter storage [77, 58].\nParameter-efficient fine-tuning (PEFT) methods propose to address these bottlenecks by updating a\nsmall fraction of parameters [21]. Rather than merely reducing the number of learnable parameters,\nan ideal PEFT method should possess three key properties to be practically effective and efficient:\nHigh Quality: It should exhibit both memorization and generalization capabilities, balancing the\nacquisition of new information from fine-tuning tasks with the retention of pre-trained knowledge.\nEfficient Training: It should minimize the memory footprint for model gradient and optimization\nstates, and further translate such memory efficiency into less computation and fine-tuning speedup.\nScalable Serving: It should avoid adding inference overhead when serving a single PEFT model. For\nmultiple models, new parameters should be partially stored as adapters to save memory, and allows\nfor effective fusion [75], fast switch [33], and efficient parallelism [58] among thousands of adapters."}, {"title": "2 Memorization or Generalization?", "content": "In this section, we evaluate the memorization and generalization capabilities of various fine-tuning\nmethods, including full FT, LoRA, and SpFT. We hypothesize that SpFT can generalize better to\ndownstream tasks. To support this hypothesis, we present detailed observations and analyses. Further\ntheoretical analysis about the generalization capabilities of the S2FT family can be found in Section 4.\nHypothesis. We hypothesize that SpFT offers superior generalization than both full FT and LoRA,\nwhile maintaining comparable memorization to LoRA with the same number of trainable parameters.\nExperimental Setup. We fine-tune the Llama3-8B on the Math10K data [28] using SpFT, LoRA, and\nfull FT. In addition to training losses, accuracies are measured on downstream tasks in LLM-Adapters,\nincluding near out-of-distribution (OOD) generalization on both easy (i.e, MultiArith, AddSub,\nSingleEq, MAWPS) and hard (i.e, GSM8K, AQUA, SVAMP) arithmetic reasoning tasks, and far\nOOD generalization on commonsense reasoning ones. For PEFT methods, we set three ratios of\ntrainable parameters (p = 10%, 1%, 0.1%) and search for the optimal hyperparameters on the valid\nset. In SpFT, trainable parameters are selected randomly with given ratios. See details in Appendix C."}, {"title": "3 The S2FT family of methods", "content": "While SpFT demonstrates strong generalization ability and good overall performance in Section 2, its\nunstructured nature poses challenges for efficient training and scalable serving on modern hardware\n(e.g., GPU). This is because of the need for sparse operations when storing and computing weights,\ngradients, and optimization states, which are significantly slower than their dense variants on GPU.\nThis motivates our investigation into structured sparsity approaches that utilize only dense operations:\nCan structured sparsity improve hardware efficiency while preserving performance by selecting\nsparsely but computing densely? If so, how far can the flexibility of selection be pushed in this context?\nTo answer this question, we design a family of Structured Sparse Fine-Tuning (S2FT) methods with\ndense-only computations, making PEFT effective, efficient and scalable. We begin by discovering the\ncoupled structure in LLMs in Section 3.1. Leveraging this property, Section 3.2 introduce the selection\nand permutation strategies of S\u00b2FT, with overall pipeline illustrated in Figure 1b. In Section 3.3, we\npresent our partial back-propagation algorithm that enables end-to-end training latency reduction."}, {"title": "3.1 Discover Coupled Structures in LLMs", "content": "We initiate our pursuit of flexible structured sparsity by examining the coupled structures in LLMs."}, {"title": "Structure Dependency in LLMs", "content": "Inspired by prior work on structured pruning [45, 17], our study\nstart by building the dependencies between activations and weights for LLMs. Let $A$ denote an\nactivation and $W$ denote a weight in the model. We define $In(A)$ as the set of parameters that directly\ncontribute to the computation of $A$, and $Out(A)$ as the set of parameters that depend on $A$ in the com-\nputation of subsequent activations. The dependency between structures can be defined as follows:\n\n$W_1 \\in In(A) \\wedge Deg^+(W_1) = 1 \\Rightarrow A \\text{ is dependent on } W_1$\n\n$W_2 \\in Out(A) \\wedge Deg^-(W_2) = 1 \\Rightarrow W_2 \\text{ is dependent on } A$\n\nwhere $Deg^+(W_1)$ represents the out-degree of weight $W_1$, and $Deg^-(W_2)$ represents the in-degree\nof weight $W_2$. Each equation represents a unqiue directional dependency between activations and\nweights. When both equations hold simultaneously, a coupled structure exists between $W_1$ and $W_2$.\nIn Figure 3, we employ deep linear networks to illustrate two types of coupled structures in LLMs:\nBasic Structures: In Figure 3a, these structures exist in both the multi-head attention (MHA) and\nfeed-forward network (FFN) modules. Taking LLaMA as an example, in the MHA module, we\nconsider the Query (Q), Key (K), and Value (V) projections as $W_1$, and the Output (O) projection\nas $W_2$, while $Softmax(QKT)V(x)$ acting as the activation between weight matrices. Similarly, in\nthe FFN module, the Up (U) and Gate (G) projections function as $W_1$, with the Down (D) projection\ncorresponding to $W_2$. Here, $U(x) \\bullet SwiGLU(G(x))$ serves as the activations connecting $W_1$ and $W_2$.\nResidual Structures: In Figure 3b, this type of coupled structures exists between the MHA and FFN\nmodules. We further consider how residual connections influence the activations in these structures.\nPermutation Invariance of Coupled Structures. Figure 3 demonstrates that $W_1$ and $W_2$ can be\nco-permuted using the same order, which only affects the order of activations between them while\npreserving the original output from the coupled structure. Since residual dependencies require an\nadditional run-time step to permute the residuals, we will focus on basic dependencies in our method."}, {"title": "3.2 Sparse Selection and Permutation", "content": "At this point, all coupled structures within the model have been identified. The subsequent sparse\nselection and permutation processes are straightforward, with overall pipeline illustrated in Figure 1b.\nMHA Module: There are four linear layers in a MHA module: $Q, K, V, O \\in \\mathbb{R}^{d \\times d}$. For a model with\n$h$ attention heads, each head $i \\in [h]$ has its own projections denoted as $Q_i \\in \\mathbb{R}^{d \\times d_h}, K_i \\in \\mathbb{R}^{d \\times d_h}$,\n$V_i \\in \\mathbb{R}^{d \\times d_h}$, and $O_i \\in \\mathbb{R}^{d_h \\times d}$, where $d_h = d/h$ is the dimension per head. Let $S_{MHA} \\subseteq [h]$ denote a\nsmall subset of attention heads. By permuting $S_{MHA}$ to the beginning of each weight matrix, we are\nable to update these selected heads using dense-only operations, while keeping the other ones frozen.\nFFN Module: There are three linear layers in an FFN module: $U, G \\in \\mathbb{R}^{k \\times d}$ and $D \\in \\mathbb{R}^{d \\times k}$. In\nS2FT, only a few channels require gradient updates. Let $S_{FFN} \\subseteq [d]$ denote the selected channels. We\ncan permute $S_{FFN}$ to the beginning of each weight matrix and only fine-tune this compact subset.\nNext, we provide several strategies for identifying and selecting important subsets in each module.\n1. S2FT-R (S2FT): In this strategy, a subset of channels is randomly selected and set to be trainable.\n2. S2FT-W: This variant selects subsets based on the magnitude of the weights for linear layers.\n3. S2FT-A: This variant selects subsets based on the magnitude of activations on a calibration set.\n4. S2FT-S: Top-K subsets are ranked and selected by the product of weight and activation magnitudes.\n5. S2FT-G: This variant selects subsets based on the magnitude of gradients on a calibration set.\nHere, 1 and 2 can be applied directly without pre-processing. 3 and 4 only require a forward pass\non a small calibration dataset. While 5 necessitates a backward pass on this dataset, it does not store\noptimization states and can mitigate memory footprints for activations through gradient checkpoint-\ning [18]. By default, we use S2FT-R for a fair comparison and discuss other variants in Section 5.4."}, {"title": "3.3 Partial Back-propagation Algorithm", "content": "Finally, we introduce our partial back-propagation algorithm with only two line modifications in\nPyTorch. our algorithm stores trainable channels based on their start and end positions, thereby\nimproving training efficiency by eliminating redundant forward activations and backward calculations."}, {"title": "4 Theoretical Analysis", "content": "In this section, we theoretically explore why S2FT demonstrates stronger generalization capabilities\ncompared to LoRA. We consider a pre-trained $L$-layer deep linear networks, which has been widely\nused to facilitate the theoretical analysis of complex DNNs [57, 30, 43, 22, 34, 5]. Let $f_{pre}(x) :=$\n$W_{L}^{pre}W_{L-1}^{pre}... W_{1}^{pre}x$ be the pre-trained deep linear network, where $W_l^{pre} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$, with $d_0 = p$\nand $d_L = q$. We fine-tune the $l$-th layer with low-rankness level $r < min\\{d_l, d_{l-1}\\}$ or sparsity level\n$s = [r.\\frac{d_l+d_{l-1}}{2}]$. Denote a class of adaptation with parameters $U \\in \\mathbb{R}^{d_l \\times d}$ and $V \\in \\mathbb{R}^{d_{l-1} \\times d}$ as\n$f_{l,U,V}(x) := W_{L}^{l+1}(W_{l}^{pre} + U V^T)W_{l-1}^{l-1}x,$\nwhere $W_{L}^{l+1} := W_{L}^{pre}W_{L-1}^{pre} ... W_{l+1}^{pre} \\in \\mathbb{R}^{dL \\times d_l}$ and $W_{l-1}^{l-1} := W_{l-1}^{pre}W_{l-2}^{pre}.... W_{0}^{pre} \\in \\mathbb{R}^{d_{l-1} \\times d_0}$ with\n$W_0^{pre} = I_p$ and $W_L^{pre} = I_q$. In a transformer-based LLM, each row of $W_l^{pre}$ can represents either the\nparameters in a single head for the MHA module or that in a single channel for the FFN module.\nGiven $n$ observations $(x^{(i)}, y^{(i)}) \\in \\mathbb{R}^p \\times \\mathbb{R}^q$, we fine-tune $f_{pre}$ by minimizing the empirical risk\n$R(f_{l,U,V}) := (1/n) \\Sigma_{i \\in [n]} ||y^{(i)} - f_{l,U,V}(x^{(i)})||^2$ via gradient descent. For LoRA, we train both\nlow-rank matrices (U, V) in Equation (3) with $d \\leftarrow r$. For S2FT, we train only V in Equation (3)\nwith $d \\leftarrow s$ and fixed $U \\leftarrow U^{S2FT}:= [e_{a_1}; e_{a_2}; ... ; e_{a_s}]$, which specifies $s$ rows to fine-tune, where\n$S = \\{a_1,...,a_s\\} \\subset [d_l]$ and $e_a$ is the a-th standard basis. Motivated from the results that gradient\ndescent has implicit regularization [74, 19, 5], we directly consider the minimum norm solutions.\nWe consider a multiple linear regression setting. Assume that the in-distribution training data $(x^{(i)},$\n$y^{(i)}) \\in \\mathbb{R}^{p+q}$ and out-of-distribution test data $(x^{(o)}, y^{(o)}) \\in \\mathbb{R}^{p+q}$ are generated i.i.d. according to\n$y^{(k)} = B^{(k)}x^{(k)} + \\epsilon^{(k)}, k \\in \\{i, o\\},$\nwhere $B^{(k)} \\in \\mathbb{R}^{q \\times p}$ is the coefficient matrix, $x^{(k)}$ and $\\epsilon^{(k)}$ are mean zero sub-Gaussian signal and\nnoise with covariance matrices $\\Sigma_{x^{(k)}}$ and $\\Sigma_{\\epsilon^{(k)}}$, respectively. The generalization capacity is measured\nby the fine-tuned model's excess risk $E(f) := E[||y^{(o)} - f(x^{(o)})||^2] - inf_{f'} E[||y^{(o)} - f'(x^{(o)})||^2]$.\nFor these OOD data, LoRA suffers from forgetting, while S2FT can maintain pre-training knowledge.\nAssumption 4.1 (Distribution Shift). Assume that $\\Sigma_{x^{(i)}} = \\Sigma_{x^{(o)}} = \\Sigma_x$ for some $\\Sigma_x \\in \\mathbb{R}^{p \\times p}$, and\n$|| (W_{l+1}^{pre})^{-l-1}U_{S2FT})(B^{(o)} - B^{(i)})\\Sigma_x^{1/2}||_F^2  < \\epsilon^2 \\varepsilon^{(o)} (f_{pre})$ for some $\\epsilon > 0$.\nAssumption 4.1 states that while the covariate distribution remains unchanged, the label distribution\nconditioned on covariates may shift, but not exceeding a factor of $\\epsilon^2$ of the OOD risk of $f_{pre}$. This\nholds for fine-tuning with proper channel selection, where primarily the output distribution is changed.\nTheorem 4.2 (Out-of-distribution Excess Risk, Informal). Suppose Assumption 4.1 holds. Consider\n$n \\rightarrow \\infty$. If $B^{(i)} = W_{l+1}^{pre}B^{(i)} W_{l-1}^{pre}$, holds for some $\\tilde{B}^{(i)} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$, and $s < rank(\\tilde{B}^{(i)})$, then,\n$\\varepsilon^{(o)} (f_{l,U_{S2FT}, V_{S2FT}}) \\leq (1 + 3\\epsilon^2)\\varepsilon^{(o)} (f_{pre}),$\\n$\\varepsilon^{(o)} (f_{l, U_{LORA}, V_{LORA}}) \\geq ||(B^{(o)} - B^{(i)}) \\Sigma_x^{1/2}||$.\nTheorem 4.2 indicates that the OOD risk of S\u00b2FT is bounded above by that of $f_{pre}$, while that of\nLORA is bounded below by the label shift magnitude. If $f_{pre}$ already has low risk for OOD tasks, and\nthe label shift is significant, S\u00b2FT is expected to outperform LoRA. Essentially, when the OOD task\ndeviates significantly from the FT distribution, LoRA may forget the pre-trained knowledge and overfit\nto the FT data, compromising its generalization capabilities. See formal statements in Theorem F.8."}, {"title": "3.3 Partial Back-propagation Algorithm", "content": "Finally, we introduce our partial back-propagation algorithm with only two line modifications in\nPyTorch. our algorithm stores trainable channels based on their start and end positions, thereby\nimproving training efficiency by eliminating redundant forward activations and backward calculations."}, {"title": "4 Theoretical Analysis", "content": "In this section, we theoretically explore why S2FT demonstrates stronger generalization capabilities\ncompared to LoRA. We consider a pre-trained $L$-layer deep linear networks, which has been widely\nused to facilitate the theoretical analysis of complex DNNs [57, 30, 43, 22, 34, 5]. Let $f_{pre}(x) :=$\n$W_{L}^{pre}W_{L-1}^{pre}... W_{1}^{pre}x$ be the pre-trained deep linear network, where $W_l^{pre} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$, with $d_0 = p$\nand $d_L = q$. We fine-tune the $l$-th layer with low-rankness level $r < min\\{d_l, d_{l-1}\\}$ or sparsity level\n$s = [r.\\frac{d_l+d_{l-1}}{2}]$. Denote a class of adaptation with parameters $U \\in \\mathbb{R}^{d_l \\times d}$ and $V \\in \\mathbb{R}^{d_{l-1} \\times d}$ as\n$f_{l,U,V}(x) := W_{L}^{l+1}(W_{l}^{pre} + U V^T)W_{l-1}^{l-1}x,$\nwhere $W_{L}^{l+1} := W_{L}^{pre}W_{L-1}^{pre} ... W_{l+1}^{pre} \\in \\mathbb{R}^{dL \\times d_l}$ and $W_{l-1}^{l-1} := W_{l-1}^{pre}W_{l-2}^{pre}.... W_{0}^{pre} \\in \\mathbb{R}^{d_{l-1} \\times d_0}$ with\n$W_0^{pre} = I_p$ and $W_L^{pre} = I_q$. In a transformer-based LLM, each row of $W_l^{pre}$ can represents either the\nparameters in a single head for the MHA module or that in a single channel for the FFN module.\nGiven $n$ observations $(x^{(i)}, y^{(i)}) \\in \\mathbb{R}^p \\times \\mathbb{R}^q$, we fine-tune $f_{pre}$ by minimizing the empirical risk\n$R(f_{l,U,V}) := (1/n) \\Sigma_{i \\in [n]} ||y^{(i)} - f_{l,U,V}(x^{(i)})||^2$ via gradient descent. For LoRA, we train both\nlow-rank matrices (U, V) in Equation (3) with $d \\leftarrow r$. For S2FT, we train only V in Equation (3)\nwith $d \\leftarrow s$ and fixed $U \\leftarrow U^{S2FT}:= [e_{a_1}; e_{a_2}; ... ; e_{a_s}]$, which specifies $s$ rows to fine-tune, where\n$S = \\{a_1,...,a_s\\} \\subset [d_l]$ and $e_a$ is the a-th standard basis. Motivated from the results that gradient\ndescent has implicit regularization [74, 19, 5], we directly consider the minimum norm solutions.\nWe consider a multiple linear regression setting. Assume that the in-distribution training data $(x^{(i)},$\n$y^{(i)}) \\in \\mathbb{R}^{p+q}$ and out-of-distribution test data $(x^{(o)}, y^{(o)}) \\in \\mathbb{R}^{p+q}$ are generated i.i.d. according to\n$y^{(k)} = B^{(k)}x^{(k)} + \\epsilon^{(k)}, k \\in \\{i, o\\},$\nwhere $B^{(k)} \\in \\mathbb{R}^{q \\times p}$ is the coefficient matrix, $x^{(k)}$ and $\\epsilon^{(k)}$ are mean zero sub-Gaussian signal and\nnoise with covariance matrices $\\Sigma_{x^{(k)}}$ and $\\Sigma_{\\epsilon^{(k)}}$, respectively. The generalization capacity is measured\nby the fine-tuned model's excess risk $E(f) := E[||y^{(o)} - f(x^{(o)})||^2] - inf_{f'} E[||y^{(o)} - f'(x^{(o)})||^2]$.\nFor these OOD data, LoRA suffers from forgetting, while S2FT can maintain pre-training knowledge.\nAssumption 4.1 (Distribution Shift). Assume that $\\Sigma_{x^{(i)}} = \\Sigma_{x^{(o)}} = \\Sigma_x$ for some $\\Sigma_x \\in \\mathbb{R}^{p \\times p}$, and\n$|| (W_{l+1}^{pre})^{-l-1}U_{S2FT})(B^{(o)} - B^{(i)})\\Sigma_x^{1/2}||_F^2  < \\epsilon^2 \\varepsilon^{(o)} (f_{pre})$ for some $\\epsilon > 0$.\nAssumption 4.1 states that while the covariate distribution remains unchanged, the label distribution\nconditioned on covariates may shift, but not exceeding a factor of $\\epsilon^2$ of the OOD risk of $f_{pre}$. This\nholds for fine-tuning with proper channel selection, where primarily the output distribution is changed.\nTheorem 4.2 (Out-of-distribution Excess Risk, Informal). Suppose Assumption 4.1 holds. Consider\n$n \\rightarrow \\infty$. If $B^{(i)} = W_{l+1}^{pre}B^{(i)} W_{l-1}^{pre}$, holds for some $\\tilde{B}^{(i)} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$, then,\n$\\varepsilon^{(o)} (f_{l,U_{S2FT}, V_{S2FT}}) \\leq (1 + 3\\epsilon^2)\\varepsilon^{(o)} (f_{pre}),$\\n$\\varepsilon^{(o)} (f_{l, U_{LORA}, V_{LORA}}) \\geq ||(B^{(o)} - B^{(i)}) \\Sigma_x^{1/2}||$.\nTheorem 4.2 indicates that the OOD risk of S\u00b2FT is bounded above by that of $f_{pre}$, while that of\nLORA is bounded below by the label shift magnitude. If $f_{pre}$ already has low risk for OOD tasks, and\nthe label shift is significant, S\u00b2FT is expected to outperform LoRA. Essentially, when the OOD task\ndeviates significantly from the FT distribution, LoRA may forget the pre-trained knowledge and overfit\nto the FT data, compromising its generalization capabilities. See formal statements in Theorem F.8."}, {"title": "5 Experiments", "content": "In this section, we conduct a series of experiments across three diverse benchmarks covering more\nthan 20 datasets. Our goal is to provide a rich picture of how S2FT performs in different scenarios.\nHere, we compare our method with different fine-tuning strategies and categories including: (i) Full\nfine-tuning (FT), (ii) reparameterized fine-tuning: LoRA [27], DORA [38], and Galore [76], (iii)\nadapter-based fine-tuning: Series Adapter [26], Parallel Adapter [24], and LoReFT [67], (iv) prompt-\nbased fine-tuning: Prefix-Tuning [36], (v) sparse fine-tuning: LISA [48]. For a fair comparison, we\nkeep a comparable number of trainable parameters in S2FT to that of LoRA. The design choices for\ntrainable parameter allocations in S2FT will be detailed in Section 5.4. All other hyperparameters are\nselected via cross-validation. Detailed setups and baseline descriptions are provided in Appendix E."}, {"title": "5.1 Commonsense Reasoning", "content": "Dataset Descriptions. The commonsense reasoning dataset comprise eight subsets: BoolQ [12],\nPIQA [9], SocialQA [56], HellaSwag [73], WinoGrande [55], ARC-challenge [13], ARC-easy [13],\nand OpenbookQA [46]. Following the experimental setup of LLM-Adapters [28], we split each\ndataset into training and test sets. Subsequently, we combine the training data from all eight tasks\ninto a single fine-tuning dataset and evaluate performance on the individual test dataset for each task.\nResults. Table 1 showcases that S2FT consistently outperforms existing PEFT methods across the\nLLAMA-7B/13B, LLaMA2-7B and LLAMA3-8B models. When compared to LoRA and DoRA, it achieves\naverage performance gains of 4.6% and 2.8%, respectively. Additionally, S2FT also demonstrates\nsuperior performance against recent approaches including Galore, LoReFT, and LISA, with accuracy\nimprovements of at least 1.0%. Remarkably, despite using less than 1% of trainable parameters, our\nmethod surpasses full FT by 0.5%. The 3.0% improvement observed on the LLaMA3-8B suggests that\nmaintaining most pre-trained parameters frozen enables better generalization to test distributions."}, {"title": "5.2 Arithmetic Reasoning", "content": "Dataset Descriptions. We followed Hu et al. [28] and evaluated S2FT on seven math reasoning tasks,\nincluding MultiArith [53], GSM8K [14], AddSub [25], AQUA [37], SingleEq [31], SVAMP [50]\nand MAWPS [32]. Our fine-tuning employed the Math10K dataset [28], which combines training\nsets from GSM8K, MAWPS, and AQuA, augmented with LM-generated chain-of-thought steps.\nTherefore, these three tasks are considered ID, while the remaining four are classified as OOD tasks.\nResults. As showcased in Table 2, S2FT consistently outperforms other PEFT methods for different\nmodels. On average, it achieves improvements of 1.3% and 0.9% over LoRA and DoRA, respectively.\nThese results highlight the versatility and effectiveness of our approach across a diverse range of\ntasks. Additionally, we observe substantial improvements even when compared to Full FT for the\nLLAMA3-8B model, particularly on complex tasks such as GSM8K and AQuA. This suggests that\nS2FT better preserves the original reasoning capabilities of this stronger model while acquiring new\nskills from the fine-tuning data, thereby validating the enhanced generalization ability of our method."}, {"title": "5.3 Instruction Following", "content": "Dataset Descriptions. To further showcase S2FT's superior generalization ability, we employ the\ninstruction-following fine-tuning task with Alpaca GPT-4 dataset, which comprises 52k samples gen-\nerated by GPT-4 [2] based on inputs from Alpaca [63]. Performance is measured on MT-Bench [78],\nfeaturing 80 high-quality, multi-turn questions designed to assess LLMs on eight different aspects.\nResults. Table 3 offers a comprehensive evaluation of Full FT, LoRA, LISA, and S\u00b2FT across various\ntasks in the MT-Bench benchmark, including Writing, Roleplay, Reasoning, Code, Math, Extraction,\nSTEM, and Humanities. It is observed that S2FT > LISA > Full FT > LoRA/Galore > Vanilla for\nboth Mistral-7B and LLama2-7B. This is because sparse FT methods like S\u00b2FT and LISA retain\nmore pre-trained knowledge while acquiring new skills on the FT dataset, thereby generalizing better\nto diverse tasks in MT-Bench. Moreover, our method outperforms LISA due to its fine-grained and\nflexible selection strategy, enabling all layers to learn to follow instructions on the full fine-tuning set."}, {"title": "5.4 Design Choices for Trainable Parameter Allocations", "content": "Finally, we detail how S2FT distribute trainable parameters across layers, modules, and channels.\nUniform across Layers: Following Chen et al. [10], we allocate parameters to each layer uniformly."}, {"title": "6 Analysis", "content": "Having demonstrated the strong generalization capability and overall performance of S2FT, we now\nfurther explore its training efficiency and serving scalability compared to other fine-tuning techniques."}, {"title": "6.1 Training Efficiency", "content": "To evaluate training efficiency, we examine two crucial metrics: peak memory footprint and average\ntraining latency. These numbers are measured on a single Nvidia A100 (80G) SXM GPU. We keep a\ncomparable number of parameters for all methods. To obtain the average latency, we fine-tune the\nmodel for 50 runs, each run including 200 iterations, with 10 warmup runs excluded in measurement."}, {"title": "6.2 Serving Scalability", "content": "While S2FT avoids additional inference overhead for a single fine-tuned model through in-place\ngradient updates, we will now discuss its scalability for serving thousands of fine-tuned models. To\nbegin, we introduce the unmerged computation paradigm of S\u00b2FT: Given a pre-trained weight matrix\n$W^{pre} \\in \\mathbb{R}^{d \\times k}$ and its corresponding fine-tuned weight matrix $W$ with sparsity level $s$, we define the\nweight difference as $AW = W-W^{pre}$. Similar to Section 4, $AW$ can be decomposed into the product\nof a weight matrix $V \\in \\mathbb{R}^{k \\times s}$ and a permutation matrix $U \\in \\mathbb{R}^{d \\times s}$. This decomposition allows us to\n\"unmerge\" an adapter $AW = UV^T$ from $W$, thereby sharing similarities with other adapters during\ninference. Following Zhong et al. [79], we consider three different adapter composition scenarios:\nAdapter Fusion. To combine knowledge from multiple trained adapters, we employ weighted fusion\nwhen fine-tuning is impractical due to limited data access or computational resources. However, this\napproach degrades performance. In Table 5, we compare the effectiveness of LoRA and S2FT when\ncombining adapters trained separately on commonsense and arithmetic reasoning tasks, where we\nconsider both fine-tuning overlapped and non-overlapped parameters for different adapters in S2FT.\nOur results show that S2FT with non-overlapped parameters achieves the best performance, while the\noverlapped variant shows inferior results. This is because S2FT (non-overlap) modifies orthogonal\nlow-rank spaces for different tasks. Similarly, LoRA largely retains task-specific capabilities during\nadapter fusion by optimizing low-rank projection matrices to create separate spaces for each adapter."}, {"title": "7 Related Work", "content": "PEFT methods reduce the fine-tuning cost for large models", "groups": "nAdapter-based Fine-tuning introduces additional trainable module into the original model. Series\nAdapters insert components between MHA or FFN layers [51, 26", "24": ".", "67": "was introduced to directly learn\ninterventions on hidden representations. However, they introduce additional latency during inference.\nPrompt-based Fine-tuning adds randomly-initialized soft tokens to the input (usually as a prefix)\nand train their embeddings while freezing the model weights [36, 40, 35", "LoRA[27": "and its recent variants like DORA[38", "80": "and FLORA [59"}]}