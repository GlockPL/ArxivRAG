{"title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings", "authors": ["Lingyu Gao"], "abstract": "Text classification, a classic task in natural language processing (NLP), involves assigning predefined categories to textual data and is crucial for applications ranging from sentiment analysis to spam detection. This thesis advances text classification by harnessing the intrinsic knowledge of Pretrained Language Models (PLMs) to address three challenging scenarios: distractor selection for multiple-choice cloze questions, improving robustness for prompt-based zero-shot text classification, and demonstration selection for retrieval-based in-context learning.\nFirstly, we focus on selecting distractors for multiple-choice cloze questions, ensuring that they are misleading yet incorrect. We assess the relationship between human experts' annotations (accept/reject) and various features, including context-free features (e.g., word frequency) and context-sensitive features (e.g., conditional probabilities of fill-in-the-blank words). We utilize pretrained embeddings and follow annotation instructions for context-free feature design, and we find that using contextualized word representations from PLMs as features drastically improves performance over traditional feature-based models, even rivaling human performance (Chapter 3).\nSecondly, prompt-based zero-shot approaches are highly sensitive to the choice of prompts, even with task descriptions. We propose to exploit the intrinsic knowledge of the model by providing domain-independent label descriptions. We craft small datasets that describe task labels with related terms, short templates, dictionary definitions, and more. This approach achieves an average improvement of 17-19% in accuracy over traditional zero-shot methods across multiple datasets. It is robust to variations in patterns and verbalizers and proves effective across different text domains, even outperforming few-shot out-of-domain learning in multiple settings (Chapter 4).\nLastly, we consider PLMs' existing knowledge of the task-specific label space of both in-context learning demonstrations and test inputs. We find that using demonstrations that are misclassified by the models, particularly those that lie near the decision boundary of the test examples, leads to better performance. Additionally, considering output label space is more important than semantic similarity, and our methods help reduce model confusion. Extensive experiments on fine-grained classification tasks show that our method improves F1 macro scores by up to 2.6% over traditional retriever-based approaches (Chapter 5).\nIn conclusion, by leveraging contextualized word representations for distractor selection, and focusing on zero-shot and few-shot tasks that emphasize strategic demonstration selection, this thesis demonstrates the effective use of PLMs to enhance performance and robustness in text classification.", "sections": [{"title": "Introduction", "content": "Consider a news article that begins with \u201cClimate scientists tell a conference that greater efforts should be made to pull CO2 from the atmosphere.\u201d Where would it be categorized? Most likely, under Tech News rather than Sports News. Similarly, the title of an email might indicate whether it is a scam, and the correctness of a math problem solution can be judged given the question and answer.\nThese scenarios highlight the essence of text classification\u2014a fundamental task in natural language processing (NLP) where the objective is to assign predefined categories to a piece of text, whether it be a phrase, a sentence, a paragraph, or a series of long documents [2]. Text classification is ubiquitous in daily life, underpinning a broad range of applications from sentiment analysis to toxic text filtering. At the same time, it could be challenging due to the complex dependencies and inherent ambiguity of natural language.\nTraditional text classification models often require extensive labeled datasets and manual feature engineering. To classify a series of textual inputs, we first map them to real-valued vectors, a process known as feature extraction [2]. Early models heavily relied on manually designed features, and often benefited from traditional pretrained word representations such as Word2Vec [3, 4] and GloVe [5]. These static word embeddings map the discrete words to continuous vector space, and maintain intrinsic pretrained knowledge, such as word analogies (e.g., \u201cking\u201d - \u201cman\u201d + \u201cwoman\u201d \u2248 \u201cqueen\u201d). However,\nthese static word embeddings fail to capture the context-dependent nature of polysemous words, and the small-scale models trained from scratch often struggle with generalization and require large, domain-specific labeled datasets to achieve satisfactory performance, especially on complex tasks.\nRecent advancements in deep learning, particularly in transformer architecture [6] and large-scale pretraining, have achieved inspiring success in NLP fields. Pretrained language models (PLMs), such as ELMo [8], GPT-2 [9] and BERT [10], have demonstrated the ability to capture intricate patterns within large corpora and retain vast amounts of knowledge during training [11, 12]. They can be used directly or adapted as needed to enhance text classification tasks, and their encoded intrinsic knowledge facilitates strong performance in zero-shot scenarios, where we don't have available training data.\nIn this thesis, we explore three challenging settings in text classification, focusing particularly on harnessing PLMs and leveraging their intrinsic knowledge for the task. These settings are:\n\u2022 Distractor Analysis and Selection for Multiple-Choice Cloze Questions (Section 1.1). To tackle the challenge of generating misleading yet incorrect distractors for cloze questions, we develop models that utilize features designed with internal word representations derived from PLMs.\n\u2022 Label-Description Training for Zero-Shot Text Classification (Section 1.2). To address the difficulty of generalizing to unseen labels, we craft small finetuning datasets that describe task labels. This approach significantly improves model robustness and performance by exploiting the models' intrinsic knowledge.\n\u2022 Ambiguity-Aware In-Context Learning with Large Language Models (Section 1.3). To deal with the sensitivity of PLMs to prompts in in-context learning, we select effective demonstrations by considering misclassified demonstrations and resolving model"}, {"title": "Feature engineering with PLMs for distractor selection", "content": "Distractor selection involves determining whether an incorrect answer (distractor) is plausible enough to challenge the test-taker without making the question unanswerable. Consider this cloze question: \u201cThe bank will  its customers of the new policy.\u201d with its correct answer being \u201cnotify\u201d. We need to decide whether \u201ccollaborate\u201d is a good distractor here. Compared to predicting correct answers, designing questions with appropriate distractors is more complex. While we can retrieve questions and correct answers from plain text, selecting optimal distractors\u2014those that are similar to the correct answers but still incorrect [13]\u2014is challenging.\nIn real-world scenarios, the annotators who select the distractors are often domain experts following specific instructions that could be transformed into features. In practice, a feature-based lightweight model is sometimes preferred. But how do we design a feature-based lightweight model while using the PLMs' knowledge at the same time? How much gain would the PLMs bring?\nFeature engineering is the practice of constructing or selecting suitable features with domain knowledge to improve model performance [14]. For this cloze question task, the features could be one or more words before or after the blank, part-of-speech (POS) tags of the previous word, frequency of the candidate words, etc. While we could directly input word frequency to the model, the text needs to be mapped to appropriate representations.\nIn contrast to static word embeddings, contextualized word representations derived from PLMs are functions of the entire textual input [11], making them context-sensitive and potentially better for feature design. For instance, the polysemous word \u201cbank\u201d could"}, {"title": "Improving robustness for prompt-based zero-shot clas-sification", "content": "The emergence of PLMs has given rise to a pretrain-and-finetune paradigm [15], which achieves impressive performance but typically requires labeled data from downstream tasks. In zero-shot text classification, where such datasets are unavailable, it becomes challenging for models to generalize to new, unseen labels during training.\nOne approach to address this challenge is to provide the model with task descriptions, exploiting the intrinsic knowledge of PLMs to solve zero-shot tasks without supervision [12, 16, 17, 18, 19]. The core idea is transforming text classification into language modeling, i.e., prompt-based classification.\nAmong the various prompt-based methods, the pattern-verbalizer approach [18] (detailed in Section 2.2.2) is a notable example. It converts the task into a cloze question to match the pretraining task format. In this method, a pattern constructs the prompt from the textual input with a single mask token, and the verbalizer maps each label to a word from the model's vocabulary. For instance, to classify the restaurant review \u201cOverpriced, salty and overrated!\u201d, a pattern like \u201cthe restaurant is [MASK]\u201d is appended to the review. The model then predicts the most probable verbalizer (e.g., \u201cgood\u201d for positive sentiment and \u201cbad\u201d for negative) for the [MASK] position. While this approach is commonly associated with encoder-based models like RoBERTa [20], autoregressive models can generate the next word or phrase based on the prompt, adhering to the same underlying idea of"}, {"title": "Demonstration selection for in-context learning", "content": "In-context learning (ICL) is a tuning-free approach where the input-output examples (known as demonstrations) are concatenated with the textual input [17]. ICL preserves the generality of the LLMs as it doesn't change the model parameters [27]. However, the length of the input prompt is usually limited, and only a few demonstrations could be included. Since PLMs are sensitive to the prompts, selecting good demonstrations becomes a crucial research question.\nOne effective strategy is leveraging semantic similarity between the ICL demonstrations and test examples with a text retriever [28]. The retriever can either be an off-the-shelf one such as [29, 30, 31, 32], or a retriever trained specifically for that task [28, 33]. Compared to a static set of demonstrations, this dynamic and context-sensitive approach leads to substantial improvements and makes PLMs less sensitive to factors such as demonstration ordering [34].\nHowever, Lyu et al. [35] indicates that there is a copy effect where the language model's predictions are significantly influenced by demonstration inputs that closely resemble the test input. This suggests that the retrieval-based approach depends heavily on the retriever. Off-the-shelf retrievers may not be ideal for some tasks, and tuning the retriever involves a finetuning process similar to traditional finetuning, which undermines the tuning-free benefit of ICL. Additionally, this approach can be sub-optimal without considering the PLM's existing knowledge about the task, especially with respect to the output label space.\nMotivated by uncertainty sampling\u2014a technique in active learning where the model selects the data points it is most uncertain about\u2014we aim to resolve model ambiguity about test example labels in this thesis by conducting zero-shot experiments in advance.\nThrough extensive experimentation on three text classification tasks, we find that"}, {"title": "Organization of the Thesis", "content": "The thesis is organized as follows:\n\u2022 Chapter 2: Background of pretrained language models' architectures and text classification approaches with PLMs that are adopted in this thesis, including the pattern-verbalizer approach and in-context learning.\n\u2022 Chapter 3: Distractor Analysis and Selection for Multiple-Choice Cloze Questions for Second-Language Learners. It presents the challenges of selecting effective distractors and details the method and the performance gain of utilizing contextualized word representations from PLMs for features. This chapter is based on [36].\n\u2022 Chapter 4: Label-Description Training for Zero-Shot Text Classification. It explains the creation of small finetuning datasets that describe task labels for topic and sentiment classification. This chapter is based on [37].\n\u2022 Chapter 5: Ambiguity-Aware In-Context Learning with Large Language Models. It shows that using PLM's existing knowledge, such as the model prediction of demonstrations and test examples, is important to improve model performance and resolve model ambiguity. This chapter is based on [38].\n\u2022 Chapter 6: Summary of the thesis, including a synthesis of the contributions and potential future work."}, {"title": "Text Classification with Pretrained Language Models", "content": "This chapter provides an overview of the architectures of pretrained language models (PLMs) and existing approaches for text classification tasks using PLMs."}, {"title": "Pretrained Language Models", "content": "Pretrained language models are language models that have been trained on large-scale corpora using self-supervised learning techniques [15]. While there is a rich history of work in using large-scale language models [39, 40] and pretraining [41, 42], the widespread adoption and use of PLMs as the default tool for NLP began with the introduction of ELMO [8]. Subsequently, it was further popularized by a series of work such as GPT [43] and BERT [44]. The primary training target of these models involves predicting or reconstructing tokens based on contextual information. This approach aligns with two crucial aspects of language use from a psycholinguistics perspective: comprehension (the ability to understand) and production (the ability to generate)."}, {"title": "ELMo", "content": "ELMO (Embeddings from Language Models) was introduced in 2018 as a novel type of deep contextualized word representation rather than a model for finetuning. However, it improved the state of the art on several NLP benchmarks by integrating deep contextual word representations with existing task-specific architectures.\nELMO is constructed using a bidirectional language model (biLM) and a task-specific layer. The biLM is constructed by two LSTM (Long Short-Term Memory) networks [45]: one is in the forward direction and one in the backward direction. Assume we are given a series of textual inputs {$x_1,\u00b7\u00b7\u00b7,x_N$}, these LSTM networks predict the probability of a token $x_t$ given its history and future context, respectively, as shown below:\n$p(x_1, x_2,..., x_N) = \\sum_{t=1}^{N} p(x_t | x_1, x_2,...,x_{t-1})$\n$p(x_1, x_2,..., x_N) = \\sum_{t=1}^{N} p(x_t | x_{t+1}, x_{t+2},...,x_N)$\nIn the pretraining process, the token representations\u00b9 and softmax layer parameters of the two LSTMs are tied, and the objective is to maximize the joint log-likelihood of both the forward and backward directions. When used for a downstream task, the contextualized word representation can be obtained through a learned linear combination of all the layer representations of the word. As ELMo adopts two LSTM layers, the first layer is more suitable for part-of-speech (POS) tagging, while the second layer is better for word sense prediction.\nThis makes ELMo a feature-based approach, as the ELMo representations are typically used as additional input features for other models. However, the authors of ELMo also mention that finetuning the biLM on domain-specific data improves model performance in some cases."}, {"title": "GPT and BERT", "content": "Feature engineering played a crucial role in early NLP tasks, leading researchers to initially use pretrained language models' (PLMs) contextualized embeddings for feature design. However, as PLMs have become increasingly powerful, the necessity for extensive feature engineering has diminished.\nGPT. GPT (Generative Pre-trained Transformer) is a unidirectional (left-to-right) decoder-based model, making it particularly well-suited for natural language generation tasks. GPT uses token and absolute positional embeddings to map input text into a vector space, and the embeddings are directly input to the decoder without the encoder structure. It is trained autoregressively to predict the next token given a sequence of textual inputs, but its transformer layers can also serve as contextualized representations.\nUnlike ELMo, which applies task-specific layers on top of the pretrained representa-tions, GPT aims to learn a universal representation and requires minimal changes to the model architecture when transferring to new tasks. For the classification task given a series of textual inputs {$x_1,\u00b7\u00b7\u00b7,x_N$}, and the corresponding labels {$y_1,\u2026\u2026\u2026, y_c$} \u2208 L, where L is the set of all possible labels, the following loss, which includes a weight \u03bb, is applied:\n$L = \\sum_{(x,y)} log P(y|x_1,..., x_N) + \\lambda \\sum_t log P(x_t | x_1,..., x_{t-1})$\nAfter proposing GPT, OpenAI scaled up the model parameter size, used more data for pretraining, and introduced GPT-2 [9] and GPT-3 [47] with a few modifications. They emphasized the importance of unsupervised multitask learning in Radford et al. [9] and introduced \"in-context learning\" in Brown et al. [47].\nBERT. BERT (Bidirectional Encoder Representations from Transformers), on the other hand, is an encoder-based model. It is bi-directional, with all layers conditioned on both left and right context. Compared to GPT, BERT has an advantage in tasks that require incorporating context from both directions.\nBERT is pretrained on two unsupervised tasks: Masked LM and binarized next sentence prediction (NSP). For the Masked LM (MLM) task (a cloze-style training objective that is crucial for training this bidirectional language model), 15% of the tokens are randomly sampled for prediction. To mitigate possible mismatch between pretraining and finetuning data, the sampled token in the input can be:\n\u2022 Replaced by a special token [MASK] (80%)\n\u2022 Replaced by a random token (10%)\n\u2022 Left unchanged (10%)\nAs BERT doesn't include the transformer decoder, it uses an MLM head to predict the masked token. Regarding special tokens aside from [MASK], BERT inserts a [CLS] token at the beginning of every input example, which can be used for sentence-level classification with a classification head. It also uses a [SEP] token as a separator to distinguish between text segments, such as sentences.\nBERT can be used in a feature-based manner without finetuning, as each transformer layer in BERT provides a contextualized representation of each token. A common approach is to combine these layers with a weighted sum.\nComparison to ELMo. Unlike ELMo, which follows a feature-based approach, GPT and BERT belong to the finetuning approach. Another key difference lies in their model"}, {"title": "Other Recent PLMs", "content": "With the field's rapid evolution, many PLMs have been introduced for both general usage and specific domains, such as finance [48] and medicine [49]. Due to their scaled-up parameter sizes, these models are often referred to as large language models (LLMs) rather than PLMs.\nRegarding model architectures, a few modern variants of BERT, such as RoBERTa and DeBERTa [50], are commonly used as encoder-based models for tasks such as text classification and natural language inference. Encoder-decoder models, such as T5 [51] and BART [52], are suitable for sequence-to-sequence (seq2seq) tasks, such as machine translation and text summarization. Many recent LLMs are decoder-based, e.g., GPT-3.5 [53], the Llama series [54, 55, 56], and Google's PaLM and PaLM 2 [19, 57]. Reasons for the popularity of decoder-only models at large scale may include their simpler architecture, strong zero-shot generalization after self-supervised training [58], and ease of use for general-purpose generation tasks [59]."}, {"title": "Text Classification with PLMs", "content": "Text classification involves assigning predefined categories to textual data. Finetuning on in-domain data generally achieves good performance [60]. However, the increasing size of PLMs makes finetuning challenging. Additionally, the lack of sufficient data in specific domains for finetuning has prompted research into data-efficient methods [61, 62], addressing zero-shot or few-shot scenarios. Zero-shot refers to situations where the model is tested on new classes or tasks it hasn't seen during training, and few-shot refers to scenarios where only a few examples are available for the task (or class) of interest."}, {"title": "Finetuning", "content": "The finetuning approach involves adjusting the model's parameters or implementing techniques like prompt tuning or parameter-efficient methods (e.g., adapters [63], LoRA [64], and QLoRA [65]) to minimize parameter changes. This approach is also used to calibrate pretrained models to reduce biases [66]. However, finetuning potentially makes the model to become less generalizable. For example, a question-answering model may not achieve high performance on a classification task [67].\nOne method to address this is instruction-tuning [67, 68, 69], where the model is finetuned on multiple tasks and datasets to learn to follow instructions, thereby enhancing cross-task generalization. This method is different from multi-task finetuning, as ablation studies show that natural instructions are crucial [68]. Prepending inputs with natural language instructions yields better zero-shot results compared to using the task and dataset names; moreover, using inputs without any templates leads to the worst performance [68].\nImproving the model's ability to follow instructions will also help it adapt to the user's needs to perform specific tasks, especially for those unlikely to appear naturally in the unsupervised pre-training data. In Wei et al. [68], they phrase the natural language inference (NLI) task as a more natural question, which achieves better performance. Another common approach to aligning models with human preferences is reinforcement learning with human feedback (RLHF) [70], which is often conducted after instruction tuning. Recently, some work has adopted high-quality synthetic feedback data generated by LLMs [71].\nWhile these finetuning methods have achieved great success, Zhou et al. [72] argue"}, {"title": "Prompting", "content": "Prompting involves providing a language model with a textual input (prompt) in inference time to perform tasks. The content of a prompt depends on the use case and can include task descriptions/instructions and input-output examples. This approach leverages the pretrained capabilities of the model to handle various tasks without gradient updates. However, the model is sensitive to the input prompts [23, 24, 73], which can be challenging for practitioners to design effectively in true zero-shot settings.\nPattern-verbalizer Approach. The pattern-verbalizer approach [18] is a prompt-based method suitable for zero-shot and few-shot scenarios. However, its main advantage lies in data efficiency.\nThis approach transforms text classification into a language modeling task, utilizing the pretrained capabilities of models like BERT. For example, given a restaurant review, a prompt might be \u201c[CLS] Overpriced, salty and overrated! The restaurant is [MASK]. [SEP]\u201d The model predicts the masked word based on the context, mapping it to a predefined label using a verbalizer. These verbalizers, such as \u201cgreat\u201d for positive reviews or \u201cawful\u201d for negative reviews, should be semantically related to the corresponding labels.\nIt is known that this approach is sensitive to the pattern and verbalizer choices. When Schick and Sch\u00fctze [18] focuses more on combining different prompt patterns, Gao et al. [74] explores automatically generating prompts and selecting the verbalizer. They also consider including demonstrations (input-output pairs) in the prompt when finetuning over a small number of examples. For instance, consider the following: \u201c[CLS] Overpriced,"}, {"title": "Summary", "content": "In this chapter, we give an overview of PLMs' architectures used in this thesis, and existing approaches (both finetuning and prompting) for text classification tasks with PLMs. In Chapter 3, we address the challenges of selecting distractors for cloze questions using contextualized word representations derived from PLMs. In Chapter 4, we tackle the issue of model sensitivity to prompts and propose the use of small finetuning datasets. In Chapter 5, we focus on improving the selection of demonstrations in prompting, proposing a solution to model ambiguity by considering model predictions."}, {"title": "Distractor Analysis and Selection for Multiple-Choice Cloze Questions for Second-Language Learners", "content": "In this chapter, we focus on selecting distractors for multiple-choice cloze questions, i.e., deciding whether a candidate is selected or not with a binary classifier. This task is challeng-ing because the distractors should be attractive enough to mislead test-takers, yet still be incorrect in terms of the knowledge being tested. In our case, it is a mixture of vocabulary knowledge and contextual understanding, and the distractors could be either semantically or syntactically inappropriate, contributing to the difficulty. Moreover, annotated data has inherent limitations because there is no single right choice; instead, many choices are possible. This variability makes traditional supervised learning challenging. However, pretrained language models could naturally pick up on signals from their training corpora that correlate with distractor quality. We can then leverage this pretrained knowledge with a small amount of supervised data.\nGiven the complexity of the selection rules, we design a range of features, both context-free and context-sensitive, including contextualized word representations from PLMs.\nRemarkably, our strongest model matches human performance.\nThis chapter is based on [36]."}, {"title": "Introduction", "content": "Multiple-choice cloze questions (MCQs) are widely used in examinations and exercises for language learners [81]. The quality of MCQs depends not only on the question and choice of blank, but also on the choice of distractors, i.e., incorrect answers. While great improvements are achieved in question answering and reading comprehension, selecting good distractors is still a problem. Different from selecting the best ones in most of the NLP tasks, distractors, which could be phrases or single words, are incorrect answers that distract students from the correct ones.\nAccording to Pho et al. [82], distractors tend to be syntactically and semantically homogeneous with respect to the correct answers. Distractor selection may be done manually through expert curation or automatically using simple methods based on similarity and dissimilarity to the correct answer [83, 84]. Intuitively, optimal distractors should be sufficiently similar to the correct answers in order to challenge students, but not so similar as to make the question unanswerable [85]. However, past work usually lacks direct supervision for training, making it difficult to develop and evaluate automatic methods.\nTo overcome this challenge, Liang et al. [81] sample distractors as negative samples for the candidate pool in the training process, and Chen et al. [86] sample questions and use manual annotation for evaluation.\nIn this thesis, we experiment on two datasets of MCQs for second-language learners with distractor selections annotated manually by human experts. Both datasets consist of instances with a sentence, a blank, the correct answer that fills the blank, and a set of candidate distractors. Each candidate distractor has a label indicating whether a human annotator selected it as a distractor for the instance. The first dataset, which we call MCDSENT, contains solely the sentence without any additional context, and the sentences are written such that they are understandable as standalone sentences. The second dataset, MCDPARA, contains sentences drawn from an existing passage and therefore also supplies the passage context."}, {"title": "Related Work", "content": "Existing approaches to distractor selection use WordNet [87] metrics [86, 88], word embedding similarities [89], thesauruses [90, 91], and phonetic and morphological similarities [92]. Other approaches consider grammatical correctness, and introduce structural similarities in an ontology [93], and syntactic similarities [94]. When using broader context, bigram or n-gram co-occurrence [95, 96], context similarity [83], and context sensitive inference [97] have also been applied to distractor selection.\nBased on these heuristic features, Liang et al. [81] assemble these features and apply neural networks, training the model to predict the answers within a lot of candidates. Yeung et al. [85] further applies BERT for ranking distractors by masking the target word.\nAs we have two manually annotated datasets that have different lengths of contexts, we adopt both word pair features and the context-specific distractor probabilities to build our feature-based models. Moreover, we build both ELMo-based and BERT-based models, combining them with our features and measuring the impact of these choices on performance."}, {"title": "Datasets", "content": "We define an instance as a tuple (x, c, d, y) where x is the context, a sentence or paragraph containing a blank; c is the correct answer, the word/phrase that correctly fills the blank; d is the distractor candidate, the distractor word/phrase being considered to fill the blank; and y is the label, a true/false value indicating whether a human annotator selected the distractor candidate. We use the term question to refer to a set of instances with the same values for x and c."}, {"title": "Data Collection", "content": "We build two datasets with different lengths of context. The first, which we call MCDSENT (\u201cMultiple Choice Distractors with SENTence context\u201d), uses only a single sentence of context. The second, MCDPARA (\u201cMultiple Choice Distractors with PARAgraph context\"), has longer contexts (roughly one paragraph).\nOur target audience is Japanese business people with TOEIC level 300-800, which translates to pre-intermediate to upper-intermediate level. Therefore, words from two frequency-based word lists, the New General Service List (NGSL; [98]) and the TOEIC Service List (TSL; [99]), were used as a base for selecting words to serve as correct answers in instances. A proprietary procedure was used to create the sentences for both MCDSENT and MCDPARA tasks, and the paragraphs in MCDPARA are excerpted from stories written to highlight the target words chosen as correct answers. The sentences are created following the rules below:\n\u2022 A sentence must have a particular minimum and maximum number of characters.\n\u2022 The other words in the sentence should be at an equal or easier NGSL frequency level compared with the correct answer.\n\u2022 The sentence theme should be business-like.\nAll the MCDSENT and MCDPARA materials were created in-house by native speakers of English, most of whom hold a degree in Teaching English to Speakers of Other Languages (TESOL).\""}, {"title": "Distractor Annotation", "content": "We now describe the procedure used to propose distractors for each instance and collect annotations regarding their selection.\nA software tool with a user interface was created to allow annotators to accept or reject distractor candidates in MCDSENT and MCDPARA. Distractor candidates are sorted automatically for presentation to annotators in order to favor those most likely to be selected. The distractor candidates are drawn from a proprietary dictionary, and those with the same part-of-speech (POS) as the correct answers (if POS data is available) are preferred. Moreover, the candidates that have greater similarity to the correct answers are preferred, such as being part of the same word learning section in the language learning course and the same NGSL word frequency bucket. There is also preference for candidates that have not yet been selected as distractors for other questions in the same task type and the same course unit.\nAfter the headwords are decided through this procedure, a morphological analyzer is used to generate multiple inflected forms for each headword, which are provided to the annotators for annotation. Both the headwords and inflected forms are available when computing features and for use by our models.\nSix annotators were involved in the annotation, all of whom are native speakers of English. Out of the six, four hold a degree in TESOL. Selecting distractors involved two-step human selection. An annotator would approve or reject distractor candidates suggested by the tool, and a different annotator, usually more senior, would review their selections. The annotation guidelines for MCDSENT and MCDPARA follow the same criteria. The annotators are asked to select distractors that are grammatically plausible, semantically implausible, and not obviously wrong based on the context. Annotators also must accept a minimum number of distractors depending on the number of times the correct answer appears in the course."}, {"title": "Annotator Agreement", "content": "Some instances in the datasets have multiple annotations, allowing us to assess annotator agreement. We use the term \"sample\" to refer to a set of instances with the same x, c, and d. Table 3.2 shows the number of samples with agreement and disagreement for both datasets. Samples with only one annotation dominate the data. Of the samples with multiple annotations, nearly all show agreement."}, {"title": "Distractor Phrases", "content": "While most distractors are words, some are phrases, including 16% in MCDSENT and 13% in MCDPARA. In most cases, the phrases are constructed by a determiner or adverb (\u201cmore\u201d, \u201cmost\u201d, etc.) and another word, such as \u201cmost pleasant\u201d, \u201cmore recently\u201d, and 'More Utility'. However, some candidates show other patterns, such as noun phrases \u201cSouth Pole\u201d, erroneously-inflected forms \u201ccome ed\u201d and other phrases (e.g. \u201cPromises Of\u201d, \u201cNo one\").\""}, {"title": "Dataset Preparation", "content": "We randomly divided each dataset into train, development, and test sets. We remind the reader that we define a \u201cquestion\u201d as a set of instances with the same values for the context"}, {"title": "Features and Analysis", "content": "We now analyse the data by designing features and studying their relationships with the annotations."}, {"title": "Features", "content": "We now describe our features. The dataset contains both the headwords and inflected forms of both the correct answer c and each distractor candidate d. In defining the features below based on c and d for an instance, we consider separate features for two types of word pairs:\n\u2022 headword pair: correct answer headwords and candidate headwords\n\u2022 inflected form pair: correct answer and candidate"}, {"title": "Correlations of Features and Annotations", "content": "The Spearman correlations between feature values and labels are presented in Table 3.4. We compute Spearman correlations between features and the T/F annotations, mapping"}, {"title": "Label-Specific Feature Histograms", "content": "Figure 3.1 - 3.4 shows histograms of feature values for each label on inflected form pairs and headword pairs for MCDSENT and MCDPARA. Since the data is unbalanced, the"}, {"title": "Probabilities of Distractors in Context", "content": "We use BERT [44] to compute probabilities of distractors and correct answers in the given contexts in MCDSENT. We insert a mask symbol in the blank position and compute the"}, {"title": "Models", "content": "Since the number of distractors selected for each instance is uncertain, our datasets could be naturally treated as a binary classification task for each distractor candidate. We now present models for the task of automatically predicting whether a distractor will be selected by an annotator. We approach the task as defining a predictor that produces a scalar score for a given distractor candidate. This score can be used for ranking distractors for a given question, and can also be turned into a binary classification using a threshold. We define"}, {"title": "Feature-Based Models", "content": "Using the features described in Section 3.4, we build a simple feed-forward neural network classifier that outputs a scalar score for classification. Only inflected forms of words are used for features without contexts, and all features are concatenated and used as the input of the classifier. For features that use BERT, we compute the log-probability of the distractor and the log of its rank in the distribution. For distractors that consist of multiple subword units, we mask each individually to compute the above features for each subword unit, then use the concatenation of mean, min, and max pooling of the features over the subword units. We refer to this model as $M_{feat}$."}, {"title": "ELMo-Based Models", "content": "We now describe models that are based on ELMo [8] which we denote $M_{ELMO}$. Since MCDPARA instances contain paragraph context, which usually includes more than one"}, {"title": "BERT-Based Models", "content": "Our final model type uses a structure similar to $M"}]}