{"title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models\nfor Challenging Text Classification Settings", "authors": ["Lingyu Gao"], "abstract": "Text classification, a classic task in natural language processing (NLP), involves assigning\npredefined categories to textual data and is crucial for applications ranging from senti-\nment analysis to spam detection. This thesis advances text classification by harnessing\nthe intrinsic knowledge of Pretrained Language Models (PLMs) to address three chal-\nlenging scenarios: distractor selection for multiple-choice cloze questions, improving ro-\nbustness for prompt-based zero-shot text classification, and demonstration selection for\nretrieval-based in-context learning.\nFirstly, we focus on selecting distractors for multiple-choice cloze questions, ensur-\ning that they are misleading yet incorrect. We assess the relationship between human\nexperts' annotations (accept/reject) and various features, including context-free features\n(e.g., word frequency) and context-sensitive features (e.g., conditional probabilities of fill-\nin-the-blank words). We utilize pretrained embeddings and follow annotation instruc-\ntions for context-free feature design, and we find that using contextualized word rep-\nresentations from PLMs as features drastically improves performance over traditional\nfeature-based models, even rivaling human performance (Chapter 3).\nSecondly, prompt-based zero-shot approaches are highly sensitive to the choice of\nprompts, even with task descriptions. We propose to exploit the intrinsic knowledge of\nthe model by providing domain-independent label descriptions. We craft small datasets\nthat describe task labels with related terms, short templates, dictionary definitions, and\nmore. This approach achieves an average improvement of 17-19% in accuracy over tra-\nditional zero-shot methods across multiple datasets. It is robust to variations in patterns\nand verbalizers and proves effective across different text domains, even outperforming\nfew-shot out-of-domain learning in multiple settings (Chapter 4).\nLastly, we consider PLMs' existing knowledge of the task-specific label space of both\nin-context learning demonstrations and test inputs. We find that using demonstrations\nthat are misclassified by the models, particularly those that lie near the decision bound-\nary of the test examples, leads to better performance. Additionally, considering output\nlabel space is more important than semantic similarity, and our methods help reduce\nmodel confusion. Extensive experiments on fine-grained classification tasks show that\nour method improves F1 macro scores by up to 2.6% over traditional retriever-based ap-", "sections": [{"title": "Introduction", "content": "Consider a news article that begins with \u201cClimate scientists tell a conference that greater efforts\nshould be made to pull CO2 from the atmosphere.\u201d 1 Where would it be categorized? Most\nlikely, under Tech News rather than Sports News. Similarly, the title of an email might\nindicate whether it is a scam, and the correctness of a math problem solution can be judged\ngiven the question and answer.\nThese scenarios highlight the essence of text classification\u2014a fundamental task in\nnatural language processing (NLP) where the objective is to assign predefined categories\nto a piece of text, whether it be a phrase, a sentence, a paragraph, or a series of long\ndocuments [2]. Text classification is ubiquitous in daily life, underpinning a broad range\nof applications from sentiment analysis to toxic text filtering. At the same time, it could be\nchallenging due to the complex dependencies and inherent ambiguity of natural language.\nTraditional text classification models often require extensive labeled datasets and\nmanual feature engineering. To classify a series of textual inputs, we first map them to\nreal-valued vectors, a process known as feature extraction [2]. Early models heavily relied\non manually designed features, and often benefited from traditional pretrained word\nrepresentations such as Word2Vec [3, 4] and GloVe [5]. These static word embeddings map\nthe discrete words to continuous vector space, and maintain intrinsic pretrained knowl-\nedge, such as word analogies (e.g., \u201cking\u201d - \u201cman\u201d + \u201cwoman\u201d \u2248 \u201cqueen\u201d). However,"}, {"title": "Feature engineering with PLMs for distractor selection", "content": "Distractor selection involves determining whether an incorrect answer (distractor) is\nplausible enough to challenge the test-taker without making the question unanswerable.\nConsider this cloze question: \"The bank will ________ its customers of the new policy.\" with its\ncorrect answer being \"notify\". We need to decide whether \"collaborate\" is a good distractor\nhere. Compared to predicting correct answers, designing questions with appropriate\ndistractors is more complex. While we can retrieve questions and correct answers from\nplain text, selecting optimal distractors\u2014those that are similar to the correct answers but\nstill incorrect [13]\u2014is challenging.\nIn real-world scenarios, the annotators who select the distractors are often domain\nexperts following specific instructions that could be transformed into features. In practice,\na feature-based lightweight model is sometimes preferred. But how do we design a feature-\nbased lightweight model while using the PLMs' knowledge at the same time? How much\ngain would the PLMs bring?\nFeature engineering is the practice of constructing or selecting suitable features with\ndomain knowledge to improve model performance [14]. For this cloze question task, the\nfeatures could be one or more words before or after the blank, part-of-speech (POS) tags of\nthe previous word, frequency of the candidate words, etc. While we could directly input\nword frequency to the model, the text needs to be mapped to appropriate representations.\nIn contrast to static word embeddings, contextualized word representations derived\nfrom PLMs are functions of the entire textual input [11], making them context-sensitive\nand potentially better for feature design. For instance, the polysemous word \u201cbank\u201d could"}, {"title": "Improving robustness for prompt-based zero-shot clas-sification", "content": "The emergence of PLMs has given rise to a pretrain-and-finetune paradigm [15], which\nachieves impressive performance but typically requires labeled data from downstream\ntasks. In zero-shot text classification, where such datasets are unavailable, it becomes\nchallenging for models to generalize to new, unseen labels during training.\nOne approach to address this challenge is to provide the model with task descriptions,\nexploiting the intrinsic knowledge of PLMs to solve zero-shot tasks without supervision\n[12, 16, 17, 18, 19]. The core idea is transforming text classification into language modeling,\ni.e., prompt-based classification.\nAmong the various prompt-based methods, the pattern-verbalizer approach [18] (de-\ntailed in Section 2.2.2) is a notable example. It converts the task into a cloze question to\nmatch the pretraining task format. In this method, a pattern constructs the prompt from\nthe textual input with a single mask token, and the verbalizer maps each label to a word\nfrom the model's vocabulary. For instance, to classify the restaurant review \u201cOverpriced,\nsalty and overrated!\u201d, a pattern like \u201cthe restaurant is [MASK]\" is appended to the review.\nThe model then predicts the most probable verbalizer (e.g., \u201cgood\u201d for positive sentiment\nand \"bad\" for negative) for the [MASK] position. While this approach is commonly associ-\nated with encoder-based models like RoBERTa [20], autoregressive models can generate\nthe next word or phrase based on the prompt, adhering to the same underlying idea of\""}, {"title": "Demonstration selection for in-context learning", "content": "In-context learning (ICL) is a tuning-free approach where the input-output examples\n(known as demonstrations) are concatenated with the textual input [17]. ICL preserves\nthe generality of the LLMs as it doesn't change the model parameters [27]. However, the\nlength of the input prompt is usually limited, and only a few demonstrations could be\nincluded. Since PLMs are sensitive to the prompts, selecting good demonstrations becomes\na crucial research question.\nOne effective strategy is leveraging semantic similarity between the ICL demonstrations\nand test examples with a text retriever [28]. The retriever can either be an off-the-shelf one\nsuch as [29, 30, 31, 32], or a retriever trained specifically for that task [28, 33]. Compared\nto a static set of demonstrations, this dynamic and context-sensitive approach leads to\nsubstantial improvements and makes PLMs less sensitive to factors such as demonstration\nordering [34].\nHowever, Lyu et al. [35] indicates that there is a copy effect where the language model's\npredictions are significantly influenced by demonstration inputs that closely resemble the\ntest input. This suggests that the retrieval-based approach depends heavily on the retriever.\nOff-the-shelf retrievers may not be ideal for some tasks, and tuning the retriever involves\na finetuning process similar to traditional finetuning, which undermines the tuning-free\nbenefit of ICL. Additionally, this approach can be sub-optimal without considering the\nPLM's existing knowledge about the task, especially with respect to the output label space.\nMotivated by uncertainty sampling\u2014a technique in active learning where the model\nselects the data points it is most uncertain about\u2014we aim to resolve model ambiguity\nabout test example labels in this thesis by conducting zero-shot experiments in advance.\nThrough extensive experimentation on three text classification tasks, we find that"}, {"title": "Organization of the Thesis", "content": "The thesis is organized as follows:\n\u2022 Chapter 2: Background of pretrained language models' architectures and text classi-\nfication approaches with PLMs that are adopted in this thesis, including the pattern-\nverbalizer approach and in-context learning.\n\u2022 Chapter 3: Distractor Analysis and Selection for Multiple-Choice Cloze Questions for\nSecond-Language Learners. It presents the challenges of selecting effective distractors\nand details the method and the performance gain of utilizing contextualized word\nrepresentations from PLMs for features. This chapter is based on [36].\n\u2022 Chapter 4: Label-Description Training for Zero-Shot Text Classification. It explains\nthe creation of small finetuning datasets that describe task labels for topic and\nsentiment classification. This chapter is based on [37].\n\u2022 Chapter 5: Ambiguity-Aware In-Context Learning with Large Language Models.\nIt shows that using PLM's existing knowledge, such as the model prediction of\ndemonstrations and test examples, is important to improve model performance and\nresolve model ambiguity. This chapter is based on [38].\n\u2022 Chapter 6: Summary of the thesis, including a synthesis of the contributions and\npotential future work."}, {"title": "Text Classification with Pretrained Language Models", "content": "This chapter provides an overview of the architectures of pretrained language models\n(PLMs) and existing approaches for text classification tasks using PLMs."}, {"title": "Pretrained Language Models", "content": "Pretrained language models are language models that have been trained on large-scale\ncorpora using self-supervised learning techniques [15]. While there is a rich history of\nwork in using large-scale language models [39, 40] and pretraining [41, 42], the widespread\nadoption and use of PLMs as the default tool for NLP began with the introduction of\nELMO [8]. Subsequently, it was further popularized by a series of work such as GPT\n[43] and BERT [44]. The primary training target of these models involves predicting or\nreconstructing tokens based on contextual information. This approach aligns with two\ncrucial aspects of language use from a psycholinguistics perspective: comprehension (the\nability to understand) and production (the ability to generate)."}, {"title": "ELMo", "content": "ELMO (Embeddings from Language Models) was introduced in 2018 as a novel type of\ndeep contextualized word representation rather than a model for finetuning. However, it\nimproved the state of the art on several NLP benchmarks by integrating deep contextual\nword representations with existing task-specific architectures.\nELMO is constructed using a bidirectional language model (biLM) and a task-specific\nlayer. The biLM is constructed by two LSTM (Long Short-Term Memory) networks [45]:\none is in the forward direction and one in the backward direction. Assume we are given\na series of textual inputs ${x_1,...,x_N}$, these LSTM networks predict the probability of a\ntoken $x_t$ given its history and future context, respectively, as shown below:\n$p(x_1,x_2,...,x_N) = \\sum_{t=1}^N p(x_t|x_1,x_2,...,x_{t-1})$\n$p(x_1,x_2,...,x_N) = \\sum_{t=1}^N p(x_t|x_{t+1},x_{t+2},...,x_N)$\nIn the pretraining process, the token representations\u00b9 and softmax layer parameters of\nthe two LSTMs are tied, and the objective is to maximize the joint log-likelihood of both the\nforward and backward directions. When used for a downstream task, the contextualized\nword representation can be obtained through a learned linear combination of all the layer\nrepresentations of the word. As ELMo adopts two LSTM layers, the first layer is more\nsuitable for part-of-speech (POS) tagging, while the second layer is better for word sense\nprediction.\nThis makes ELMo a feature-based approach, as the ELMo representations are typically\nused as additional input features for other models. However, the authors of ELMo also\nmention that finetuning the biLM on domain-specific data improves model performance\nin some cases."}, {"title": "GPT and BERT", "content": "Feature engineering played a crucial role in early NLP tasks, leading researchers to initially\nuse pretrained language models' (PLMs) contextualized embeddings for feature design.\nHowever, as PLMs have become increasingly powerful, the necessity for extensive feature\nengineering has diminished.\nGPT. GPT (Generative Pre-trained Transformer) is a unidirectional (left-to-right) decoder-\nbased model, making it particularly well-suited for natural language generation tasks.\nGPT uses token and absolute positional embeddings to map input text into a vector space,2\nand the embeddings are directly input to the decoder without the encoder structure. It is\ntrained autoregressively to predict the next token given a sequence of textual inputs, but\nits transformer layers can also serve as contextualized representations.\nUnlike ELMo, which applies task-specific layers on top of the pretrained representa-\ntions, GPT aims to learn a universal representation and requires minimal changes to the\nmodel architecture when transferring to new tasks. For the classification task given a series\nof textual inputs ${x_1,...,x_N}$, and the corresponding labels ${y_1,........, y_c} \\in L$, where L is\nthe set of all possible labels, the following loss, which includes a weight \u5165, is applied:3\n$L = \\sum_{(x,y)} log P(y|x_1,...,x_N) + \\lambda  \\sum_{t} log P(x_t|x_1,...,x_{t-1})$\nAfter proposing GPT, OpenAI scaled up the model parameter size, used more data\nfor pretraining, and introduced GPT-2 [9] and GPT-3 [47] with a few modifications. They\nemphasized the importance of unsupervised multitask learning in Radford et al. [9] and\nintroduced \"in-context learning\" in Brown et al. [47]."}, {"title": "Other Recent PLMs", "content": "With the field's rapid evolution, many PLMs have been introduced for both general usage\nand specific domains, such as finance [48] and medicine [49]. Due to their scaled-up\nparameter sizes, these models are often referred to as large language models (LLMs) rather\nthan PLMs.\nRegarding model architectures, a few modern variants of BERT, such as RoBERTa\nand DeBERTa [50], are commonly used as encoder-based models for tasks such as text\nclassification and natural language inference. Encoder-decoder models, such as T5 [51]\nand BART [52], are suitable for sequence-to-sequence (seq2seq) tasks, such as machine\ntranslation and text summarization. Many recent LLMs are decoder-based, e.g., GPT-3.5\n[53], the Llama series [54, 55, 56], and Google's PaLM and PaLM 2 [19, 57]. Reasons for the\npopularity of decoder-only models at large scale may include their simpler architecture,\nstrong zero-shot generalization after self-supervised training [58], and ease of use for\ngeneral-purpose generation tasks [59]."}, {"title": "Text Classification with PLMs", "content": "Text classification involves assigning predefined categories to textual data. Finetuning\non in-domain data generally achieves good performance [60]. However, the increasing\nsize of PLMs makes finetuning challenging. Additionally, the lack of sufficient data in"}, {"title": "Finetuning", "content": "The finetuning approach involves adjusting the model's parameters or implementing\ntechniques like prompt tuning or parameter-efficient methods (e.g., adapters [63], LoRA\n[64], and QLoRA [65]) to minimize parameter changes. This approach is also used to\ncalibrate pretrained models to reduce biases [66]. However, finetuning potentially makes\nthe model to become less generalizable. For example, a question-answering model may\nnot achieve high performance on a classification task [67].\nOne method to address this is instruction-tuning [67, 68, 69], where the model is\nfinetuned on multiple tasks and datasets to learn to follow instructions, thereby enhancing\ncross-task generalization. This method is different from multi-task finetuning, as ablation\nstudies show that natural instructions are crucial [68]. Prepending inputs with natural\nlanguage instructions yields better zero-shot results compared to using the task and dataset\nnames; moreover, using inputs without any templates leads to the worst performance [68].\nImproving the model's ability to follow instructions will also help it adapt to the\nuser's needs to perform specific tasks, especially for those unlikely to appear naturally in\nthe unsupervised pre-training data. In Wei et al. [68], they phrase the natural language\ninference (NLI) task as a more natural question, which achieves better performance.\nAnother common approach to aligning models with human preferences is reinforcement\nlearning with human feedback (RLHF) [70], which is often conducted after instruction\ntuning. Recently, some work has adopted high-quality synthetic feedback data generated\nby LLMs [71].\nWhile these finetuning methods have achieved great success, Zhou et al. [72] argue"}, {"title": "Prompting", "content": "Prompting involves providing a language model with a textual input (prompt) in inference\ntime to perform tasks. The content of a prompt depends on the use case and can include\ntask descriptions/instructions and input-output examples. This approach leverages the\npretrained capabilities of the model to handle various tasks without gradient updates.\nHowever, the model is sensitive to the input prompts [23, 24, 73], which can be challenging\nfor practitioners to design effectively in true zero-shot settings.\nPattern-verbalizer Approach. The pattern-verbalizer approach [18] is a prompt-based\nmethod suitable for zero-shot and few-shot scenarios. However, its main advantage lies in\ndata efficiency.6\nThis approach transforms text classification into a language modeling task, utilizing\nthe pretrained capabilities of models like BERT. For example, given a restaurant review, a\nprompt might be \u201c[CLS] Overpriced, salty and overrated! The restaurant is [MASK]. [SEP]\"\nThe model predicts the masked word based on the context, mapping it to a predefined\nlabel using a verbalizer. These verbalizers, such as \u201cgreat\u201d for positive reviews or \u201cawful\u201d\nfor negative reviews, should be semantically related to the corresponding labels.\nIt is known that this approach is sensitive to the pattern and verbalizer choices. When\nSchick and Sch\u00fctze [18] focuses more on combining different prompt patterns, Gao et al.\n[74] explores automatically generating prompts and selecting the verbalizer. They also\nconsider including demonstrations (input-output pairs) in the prompt when finetuning\nover a small number of examples. For instance, consider the following: \u201c[CLS] Overpriced,\""}, {"title": "Summary", "content": "In this chapter, we give an overview of PLMs' architectures used in this thesis, and existing\napproaches (both finetuning and prompting) for text classification tasks with PLMs. In\nChapter 3, we address the challenges of selecting distractors for cloze questions using"}, {"title": "Distractor Analysis and Selection for Multiple-Choice Cloze Questions for Second-Language Learners", "content": "In this chapter, we focus on selecting distractors for multiple-choice cloze questions, i.e.,\ndeciding whether a candidate is selected or not with a binary classifier. This task is challeng-\ning because the distractors should be attractive enough to mislead test-takers, yet still be\nincorrect in terms of the knowledge being tested. In our case, it is a mixture of vocabulary\nknowledge and contextual understanding, and the distractors could be either semantically\nor syntactically inappropriate, contributing to the difficulty. Moreover, annotated data\nhas inherent limitations because there is no single right choice; instead, many choices are\npossible. This variability makes traditional supervised learning challenging. However,\npretrained language models could naturally pick up on signals from their training corpora\nthat correlate with distractor quality. We can then leverage this pretrained knowledge with\na small amount of supervised data.\nGiven the complexity of the selection rules, we design a range of features, both context-\nfree and context-sensitive, including contextualized word representations from PLMs.\nRemarkably, our strongest model matches human performance."}, {"title": "Introduction", "content": "Multiple-choice cloze questions (MCQs) are widely used in examinations and exercises\nfor language learners [81]. The quality of MCQs depends not only on the question and\nchoice of blank, but also on the choice of distractors, i.e., incorrect answers. While great\nimprovements are achieved in question answering and reading comprehension, selecting\ngood distractors is still a problem. Different from selecting the best ones in most of the\nNLP tasks, distractors, which could be phrases or single words, are incorrect answers that\ndistract students from the correct ones.\nAccording to Pho et al. [82], distractors tend to be syntactically and semantically homo-\ngeneous with respect to the correct answers. Distractor selection may be done manually\nthrough expert curation or automatically using simple methods based on similarity and\ndissimilarity to the correct answer [83, 84]. Intuitively, optimal distractors should be suf-\nficiently similar to the correct answers in order to challenge students, but not so similar\nas to make the question unanswerable [85]. However, past work usually lacks direct\nsupervision for training, making it difficult to develop and evaluate automatic methods.\nTo overcome this challenge, Liang et al. [81] sample distractors as negative samples for\nthe candidate pool in the training process, and Chen et al. [86] sample questions and use\nmanual annotation for evaluation.\nIn this thesis, we experiment on two datasets of MCQs for second-language learners\nwith distractor selections annotated manually by human experts. Both datasets consist\nof instances with a sentence, a blank, the correct answer that fills the blank, and a set of\ncandidate distractors. Each candidate distractor has a label indicating whether a human\nannotator selected it as a distractor for the instance. The first dataset, which we call\nMCDSENT, contains solely the sentence without any additional context, and the sentences\nare written such that they are understandable as standalone sentences. The second dataset,\nMCDPARA, contains sentences drawn from an existing passage and therefore also supplies\nthe passage context."}, {"title": "Related Work", "content": "Existing approaches to distractor selection use WordNet [87] metrics [86, 88], word embed-\nding similarities [89], thesauruses [90, 91], and phonetic and morphological similarities\n[92]. Other approaches consider grammatical correctness, and introduce structural simi-\nlarities in an ontology [93], and syntactic similarities [94]. When using broader context,\nbigram or n-gram co-occurrence [95, 96], context similarity [83], and context sensitive\ninference [97] have also been applied to distractor selection.\nBased on these heuristic features, Liang et al. [81] assemble these features and apply\nneural networks, training the model to predict the answers within a lot of candidates.\nYeung et al. [85] further applies BERT for ranking distractors by masking the target word.\nAs we have two manually annotated datasets that have different lengths of contexts,\nwe adopt both word pair features and the context-specific distractor probabilities to\nbuild our feature-based models. Moreover, we build both ELMo-based and BERT-based\nmodels, combining them with our features and measuring the impact of these choices on\nperformance."}, {"title": "Datasets", "content": "We define an instance as a tuple (x, c, d, y) where x is the context, a sentence or paragraph\ncontaining a blank; c is the correct answer, the word/phrase that correctly fills the blank; d\nis the distractor candidate, the distractor word/phrase being considered to fill the blank;\nand y is the label, a true/false value indicating whether a human annotator selected the\ndistractor candidate.\u00b9 We use the term question to refer to a set of instances with the same\nvalues for x and c."}, {"title": "Data Collection", "content": "We build two datasets with different lengths of context. The first, which we call MCDSENT\n(\"Multiple Choice Distractors with SENTence context\u201d), uses only a single sentence of\ncontext. The second, MCDPARA (\u201cMultiple Choice Distractors with PARAgraph context\"),\nhas longer contexts (roughly one paragraph).\nOur target audience is Japanese business people with TOEIC level 300-800, which\ntranslates to pre-intermediate to upper-intermediate level. Therefore, words from two\nfrequency-based word lists, the New General Service List (NGSL; [98]) and the TOEIC\nService List (TSL; [99]), were used as a base for selecting words to serve as correct answers\nin instances. A proprietary procedure was used to create the sentences for both MCDSENT\nand MCDPARA tasks, and the paragraphs in MCDPARA are excerpted from stories written\nto highlight the target words chosen as correct answers. The sentences are created following\nthe rules below:\n\u2022 A sentence must have a particular minimum and maximum number of characters.\n\u2022 The other words in the sentence should be at an equal or easier NGSL frequency\nlevel compared with the correct answer.\n\u2022 The sentence theme should be business-like.\nAll the MCDSENT and MCDPARA materials were created in-house by native speakers of\nEnglish, most of whom hold a degree in Teaching English to Speakers of Other Languages\n(TESOL).\""}, {"title": "Distractor Annotation", "content": "We now describe the procedure used to propose distractors for each instance and collect\nannotations regarding their selection.\nA software tool with a user interface was created to allow annotators to accept or reject\ndistractor candidates in MCDSENT and MCDPARA. Distractor candidates are sorted\nautomatically for presentation to annotators in order to favor those most likely to be\nselected. The distractor candidates are drawn from a proprietary dictionary, and those\nwith the same part-of-speech (POS) as the correct answers (if POS data is available) are\npreferred. Moreover, the candidates that have greater similarity to the correct answers are\npreferred, such as being part of the same word learning section in the language learning\ncourse and the same NGSL word frequency bucket. There is also preference for candidates\nthat have not yet been selected as distractors for other questions in the same task type and\nthe same course unit.2\nAfter the headwords are decided through this procedure, a morphological analyzer is\nused to generate multiple inflected forms for each headword, which are provided to the\nannotators for annotation. Both the headwords and inflected forms are available when\ncomputing features and for use by our models.\nSix annotators were involved in the annotation, all of whom are native speakers of\nEnglish. Out of the six, four hold a degree in TESOL. Selecting distractors involved\ntwo-step human selection. An annotator would approve or reject distractor candidates\nsuggested by the tool, and a different annotator, usually more senior, would review their\nselections. The annotation guidelines for MCDSENT and MCDPARA follow the same\ncriteria. The annotators are asked to select distractors that are grammatically plausible,\nsemantically implausible, and not obviously wrong based on the context. Annotators also\nmust accept a minimum number of distractors depending on the number of times the\ncorrect answer appears in the course."}, {"title": "Annotator Agreement", "content": "Some instances in the datasets have multiple annotations, allowing us to assess annotator\nagreement. We use the term \"sample\" to refer to a set of instances with the same x, C,\nand d. Table 3.2 shows the number of samples with agreement and disagreement for both\ndatasets. Samples with only one annotation dominate the data. Of the samples with\nmultiple annotations, nearly all show agreement."}, {"title": "Distractor Phrases", "content": "While most distractors are words, some are phrases, including 16% in MCDSENT and\n13% in MCDPARA. In most cases, the phrases are constructed by a determiner or adverb\n(\u201cmore\u201d, \u201cmost\u201d, etc.) and another word, such as \u201cmost pleasant\u201d, \u201cmore recently\u201d, and\n'More Utility'. However, some candidates show other patterns, such as noun phrases\n\u201cSouth Pole\u201d, erroneously-inflected forms \u201ccome ed\u201d and other phrases (e.g. \u201cPromises\nOf\u201d, \u201cNo one\").\""}, {"title": "Dataset Preparation", "content": "We randomly divided each dataset into train, development, and test sets. We remind the\nreader that we define a \u201cquestion\u201d as a set of instances with the same values for the context"}, {"title": "Features and Analysis", "content": "We now analyse the data by designing features and studying their relationships with the\nannotations."}, {"title": "Features", "content": "We now describe our features. The dataset contains both the headwords and inflected\nforms of both the correct answer c and each distractor candidate d. In defining the features\nbelow based on c and d for an instance, we consider separate features for two types of\nword pairs:\n\u2022 headword pair: correct answer headwords and candidate headwords\n\u2022 inflected form pair: correct answer and candidate"}, {"title": "Correlations of Features and Annotations", "content": "The Spearman correlations between feature values and labels are presented in Table 3.4.\nWe compute Spearman correlations between features and the T/F annotations, mapping"}, {"title": "Label-Specific Feature Histograms", "content": "Figure 3.1 - 3.4 shows histograms of feature values for each label on inflected form pairs\nand headword pairs for MCDSENT and MCDPARA. Since the data is unbalanced, the"}, {"title": "Probabilities of Distractors in Context", "content": "We use BERT [44] to compute probabilities of distractors and correct answers in the given\ncontexts in MCDSENT. We insert a mask symbol in the blank position and compute the"}, {"title": "Models", "content": "Since the number of distractors selected for each instance is uncertain, our datasets could\nbe naturally treated as a binary classification task for each distractor candidate. We now\npresent models for the task of automatically predicting whether a distractor will be selected\nby an annotator. We approach the task as defining a predictor that produces a scalar score\nfor a given distractor candidate. This score can be used for ranking distractors for a given\nquestion, and can also be turned into a binary classification using a threshold. We define"}, {"title": "Feature-Based Models", "content": "Using the features described in Section 3.4, we build a simple feed-forward neural network\nclassifier that outputs a scalar score for classification. Only inflected forms of words are\nused for features without contexts, and all features are concatenated and used as the\ninput of the classifier. For features that use BERT, we compute the log-probability of the\ndistractor and the log of its rank in the distribution. For distractors that consist of multiple\nsubword units, we mask each individually to compute the above features for each subword\nunit, then use the concatenation of mean, min, and max pooling of the features over the\nsubword units. We refer to this model as Mfeat."}, {"title": "ELMo-Based Models", "content": "We now describe models that are based on ELMo [8] which we denote MELMO. Since\nMCDPARA instances contain paragraph context, which usually includes more than one"}, {"title": "BERT-Based Models", "content": "Our final model type uses a structure similar to MELMO but using BERT in place of ELMo\nwhen producing contextualized embeddings, which we denote by MBERT and MBERT(l)\ngiven different types of context. We also consider the variation of concatenating the features\nto the input to the classifier, i.e., the first variation described in Section 3.5.2. We omit the\ngru+c and all variations here because the BERT-based models are more computationally\nexpensive than those that use ELMo."}, {"title": "Experiments", "content": "We now report the results of experiments with training models to select distractor candi-\ndates."}, {"title": "Evaluation Metrics", "content": "We use precision, recall, and F1 score as evaluation metrics. These require choosing a\nthreshold for the score produced by our predictors. We also report the area under the\nprecision-recall curve (AUPR), which is a single-number summary that does not require\nchoosing a threshold."}, {"title": "Baselines", "content": "As the datasets are unbalanced (most distractor candidates are not selected), we report the\nresults of baselines that always return \u201cTrue\u201d in the \u201cbaseline\" rows of Tables 3.6 and 3.7.\nMCDSENT has a higher percentage of true labels than MCDPARA."}, {}]}