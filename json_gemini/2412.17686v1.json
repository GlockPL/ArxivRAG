{"title": "Large Language Model Safety: A Holistic Survey", "authors": ["Dan Shi", "Tianhao Shen", "Yufei Huang", "Zhigen Li", "Yongqi Leng", "Renren Jin", "Chuang Liu", "Xinwei Wu", "Zishan Guo", "Linhao Yu", "Ling Shi", "Bojian Jiang", "Deyi Xiong"], "abstract": "The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies.\nThis survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions.\nOur findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at a GitHub repository.", "sections": [{"title": "1 Introduction", "content": "In 1950, Alan Turing posed the question, \u201cCan machines think?\" (Turing, 1950). At that time, the prevailing answer was \u201cno.\u201d Over the subsequent decades, building an artificial intelligence (AI) system capable of human-like thinking has become a long-standing pursuit in AI research, with substantial efforts dedicated to achieving this vision (Minsky, 1961; Brooks, 1991; McCarthy et al., 2006; LeCun et al., 2015; Lake et al., 2016). Such sustained endeavors have driven AI from theoretical explorations to practical applications. The early stages of AI focus on symbolic reasoning and rule-based systems to replicate human intelligence (McCarthy, 1960), such as rule-based machine translation\u00b2 and expert systems (Hayes-Roth, 1983). However, symbolic AI relies heavily on human-crafted rules, making it difficult to scale to new, dynamic, or complex environments. In the 1980s and and 1990s, increased data availability and computational power have shifted AI research towards data-driven approaches, allowing machines to automatically discover features from data to fulfill specific tasks. These stages have witnessed the resurgence of machine learning with substantial progress made in artificial neural networks, support vector machines and random forests. Although these machine learning approaches achieve promising performance on specific tasks, they often depend on feature engineering and struggle with handling complex, real-world problems. Since the 2010s, significant advances in neural architectures, computational resources, and access to extensive training data have led to a resurgence of artificial neural networks with deeper structures, typically referred to as deep learning. Deep learning represents a major milestone in AI, enabling the achievement of surpassing human-level performance in tasks such as image classification (Krizhevsky et al., 2012), machine translation (Sutskever et al., 2014), and protein structure prediction (Jumper et al., 2021). However, early deep learning models are often trained on task-specific data, resulting in models that are limited to a narrow range of capabilities. This limitation persists until the emergence of large language models (LLMs).\nThe development of LLMs in recent years marks a significant advancement towards the goal of \"making machines think\", as these models demonstrate exceptional proficiency in human language understanding and generation, thereby significantly reducing barriers to communication between humans and AI (Brown et al., 2020; OpenAI, 2023a; Guo et al., 2023b; Dubey et al., 2024; Yang et al., 2024a; DeepSeek-AI et al., 2024; Sun et al., 2024b). Compared to models from the early stages of deep learning, LLMs contain several orders of magnitude more parameters. Moreover, their training data typically encompass multiple tasks, domains, languages, and modalities, which contributes to their extensive capabilities. Although LLMs already exhibit more human-like intelligence than any preceding AI systems (Bubeck et al., 2023; Shi et al., 2024b), their performance continues to advance without signs of slowing down. Furthermore, due to their broad capacity and exceptional performance, LLMs have been deployed in numerous real-world applications.\nHowever, the continuous improvement and widespread deployment of LLMs in real-world scenarios have raised significant concerns regarding their safety (Brundage et al., 2018; Weidinger et al., 2021; Bommasani et al., 2021). Concerns on the risks posed by intelligent machines date as far back as the 1950s (Wiener, 1950), largely focusing on the social and ethical implications of AI. Nonetheless, unlike earlier concerns, which are largely speculative and theoretical due to the absence of highly capable AI systems, the extensive capabilities of LLMs present concrete risks. Recent studies indicate that, LLMs may generate inappropriate content that may be offensive or hateful (Gehman et al., 2020; Deng et al., 2022). Additionally, LLMs can also exhibit stereotypes and social biases (Gallegos et al., 2023; Liang et al., 2021; Huang & Xiong, 2024a; Salecha et al., 2024), compromise individual privacy (Li et al., 2023a; Staab et al., 2024), or violate ethical and moral standards (Weidinger et al., 2021; Abdulhai et al., 2023). Moreover, they can be exploited by malicious users to threaten national safety and public safety, such as through the design of weapons or the manipulation of public opinion (Soice et al., 2023; Buchanan et al., 2021).3 Notably, as LLMs become increasingly proficient at performing tasks, there is an emerging trend suggesting that these models may develop self-replication and self-preservation capabilities, and exhibit desires for power and resources (Gabor et al., 2022; Perez et al., 2023). This potential evolution could result in unforeseen and potentially harmful consequences for human society. Alarmingly, these challenges cannot be mitigated merely by scaling up the models or increasing the data and computational resources used for training (Wei et al., 2023a).\nIn view of these, a growing consensus that focusing on LLM safety is not only essential but also urgent has been reached across governments, medias, and AI communities. For instance, concerns have been raised about the risks of deploying LLMs in sensitive areas like healthcare and law, where even slight mistakes in outputs could have significant consequences. The need for human oversight and careful evaluation of LLM outputs is hence highly emphasized (Sterz et al., 2024).\nFurthermore, a number of leading experts argue that proactive measures are necessary to build trust and prevent misuse. They suggest that without proper safety measures, LLMs could be exploited to produce misinformation or be manipulated for economic or political gains (Mozes et al., 2023; Ferdaus et al., 2024). The rapid evolution of LLMs makes addressing these risks a priority to ensure the safe integration of capable AI systems into society.\nIn view of the urgent need for LLM safety technologies, strategies and national/global policies, we provide a comprehensive overview into LLM safety. We take a holistic view of LLM safety, including safety technologies, resources, evaluations, roadmaps, strategies, policies, etc., and organize them into two dimensions: basic areas of LLM safety and related areas to LLM safety. The first dimension covers major risk areas/categories that the development and deployment of LLMs give rise to. Our analysis emphasizes the evaluation of risks associated with LLMs across various scenarios, which is consistent with the view taken by several recent AI safety reports (e.g., International Scientific Report on the Safety of Advanced AI (Bengio et al., 2024a)) and safety institutes (e.g., AI Safety Institute (AISI) (AISI, 2024)). This includes a thorough examination of value misalignment, robustness against targeted attacks, scenarios of both intentional and unintentional misuse, and potential risks posed by advanced AI systems that operate independently or autonomously in complex environments. Through a systematic review and analysis of these critical areas, we aim to provide researchers and policymakers with a holistic perspective on the current landscape of LLM safety research, identify existing research gaps, and propose potential directions for future exploration.\nIn the dimension of related areas to LLM safety, we investigate the substantial risks that agents powered by LLMs, despite their exceptional problem-solving and task-planning capabilities, pose to humans and society. Moreover, our exploration encompasses the technology roadmaps and strategies to LLM safety employed by leading AI companies and institutions in practice. We also delve into interpretability methodologies for comprehensively studying and mitigating the unsafe behaviors of LLMs by examining their internal mechanisms. Finally, we expand our discussion to encompass national governance and global cooperation, investigating the multifaceted dimensions of AI governance. This includes international collaboration, technological oversight, ethical considerations, and compliance frameworks. Our goal is to deepen the understanding of the challenges and opportunities that AI governance entails, ultimately promoting technological development for the benefit of humanity.\nWe anticipate that this survey would serve as a valuable and thorough reference for researchers, policymakers, and industry practitioners, facilitating a better understanding of the current status and challenges of LLMs in relation to safety. By critically analyzing the shortcomings of existing research and policy practices, we aspire to inspire future endeavors in research, development and policymaking related to LLM safety."}, {"title": "1.1 LLM Safety Definition", "content": "In this survey, we distinguish LLM safety from security although LLMs could be used for aiding cybersecurity attacks or other security tasks. We refer LLM safety to as the responsible development, deployment, and use of LLMs to avoid causing unintended/intended harms. This definition involves ensuring that LLMs do not produce harmful outputs, such as biased, offensive, or unethical content, and safeguarding them from misuse in malicious activities, such as data manipulation or adversarial attacks. In contrast, LLM security focuses on protecting LLM systems from external threats like hacking, denial-of-service attacks, or data breaches. In summary, LLM safety is more about the ethical and responsible usage of LLMs, while LLM security is concerned with defending LLM systems from technical threats (Ayyamperumal & Ge, 2024)."}, {"title": "1.2 Paper and Source Selection", "content": "We investigate LLM safety in the context of LLMs and generative AI, focusing on publications within the fields of natural language processing (NLP) and AI. Main sources for our survey include venues such as ACL, EACL, NAACL, EMNLP, COLING, CONLL, SIGIR, IJCAI, AAAI, NeurIPS, ICML, and ICDM, as well as unpublished research disseminated through preprint platforms like arXiv. Relevant papers are identified using keywords such as \u201c\u0391\u0399 Safety\", \"Large Language Model\u201d, \u201cSafety\u201d, and \u201cAI Risks\". For specific subfields or subcategories, targeted keywords are used for literature search. For instance, terms like \"Malicious Use\u201d, \u201cMisuse\u201d and \u201cMisinformation\u201d are employed to retrieve studies related to Misuse."}, {"title": "1.3 Related Work", "content": "The rapid advancements in LLMs have drawn significant global attention to the importance of AI safety, thereby stimulating considerable interests within the research community and prompting efforts to survey the current state of this field. However, much of the recent literature has concentrated on specific aspects or levels of safety risks associated with LLMs. For instance, some studies have focused on reviewing alignment methods for LLMs (Ji et al., 2023; Shen et al., 2023), while others have primarily addressed catastrophic risks, such as potential misuse (Hendrycks et al., 2023). A few surveys have investigated a relatively broader scope. Cui et al. (2024) approach the issue from the perspective of four fundamental modules of LLM systems (input module, language model module, toolchain module, and output module), analyzing potential risks associated with each module and discussing corresponding mitigation strategies. Our survey adopts a similar classification method which is based on different LLM stages (e.g., Data Processing, Pre-training, Post-training and Post-processing Stages) when summarizing approaches to address safety problems in specific areas, such as privacy and toxicity. However, we believe this classification method is insufficient to cover all safety-related issues. For example, the discussion of robustness against attacks and catastrophic misuse is not covered in (Cui et al., 2024), nor does it include specific governance proposals and policy recommendations. Chua et al. (2024) sequentially introduce the working principles of LLMs, research challenges of generative models, and classifications of alignment and safety in their review, while Chen et al. (2024a) structure their framework for AI safety around three pillars: Trustworthy AI, Responsible AI, and Safe AI. In contrast, we provide a more detailed categorization system, dividing safety issues into several domains, each with multiple subdomains, which facilitates a clearer understanding and response to the safety challenges of LLMs. Additionally, we go beyond technical aspects of safety by investigating governance and policy, offering a more comprehensive perspective.\nBy offering well-defined subdomains and detailed evaluation techniques, this survey aims to provide practical guidance for safety researchers and practitioners focused on LLM-specific risks. Such focused methodology ensures that safety measures and strategies proposed are directly applicable to the emerging issues within LLM deployment. Moreover, policy recommendations and governance suggestions investigated in this survey aim to bridge the gap between safety technologies and regulation frameworks. These recommendations facilitate the creation of more adaptive and robust regulatory strategies, ensuring that policies remain"}, {"title": "2 Taxonomy", "content": "This survey aims to provide a systematic organization of a wide variety of safety concerns, risks, and strategies associated with LLMs. By identifying and categorizing these risks, we offer a well-structured taxonomy for understanding the broad spectrum of challenges raised by the development and deployment of LLMs. The taxonomy, illustrated in Figure 1, structures the current landscape of LLM safety into two dimensions: basic areas of LLM safety which cover key risk areas of LLMs, and related areas to LLM safety which identify essential areas closely related to LLM safety."}, {"title": "2.1 Basic Areas of LLM Safety", "content": "We identify four key risk areas of LLMs in the first dimension: Value Misalignment, Robustness to Attack, Misuse and Autonomous AI Risks, as illustrated in Figure 1. For each key risk area, we further identify subdomains, shedding light on the multifaceted challenges of LLM safety and strategies for evaluating and mitigating associated risks.\n\u2022 Value Misalignment (Section 3): This section delves into the multifaceted landscape of LLM safety stemming from misalignment between LLMs and human intentions, values, and expectations. It includes four core sub-realms: social bias, privacy, toxicity, and ethics and morality. By systematically analyzing their impacts, origins, evaluation methods, and mitigation strategies, this section provides a comprehensive understanding of the key concerns associated with these sub-realms.\n\u2022 Robustness to Attack (Section 4): This risk area explores the robustness of LLMs against adversarial attacks, focusing on jailbreaking techniques and red teaming methods, which examines various strategies for bypassing safety mechanisms, and manual or automated adversarial testing for identifying vulnerabilities of LLMs. Furthermore, it discusses defense strategies against these threats, which include external safeguards designed to protect LLMs from malicious inputs, as well as internal protections that involve modifying the LLMs themselves to enhance their resilience. These strategies are essential for improving the safety and robustness of LLMs, though challenges remain in balancing effectiveness with model complexity.\n\u2022 Misuse (Section 5): For this risk area, we focus on reviewing severe threats posed by LLMs when exploited by malicious actors, which elucidate multiple facets of human society and public safety. On the one hand, LLMs could be used for a wide range of illegal aims, including facilitating cyberattacks and producing biological, chemical, and nuclear weapons that threaten human safety. On the other hand, the generation of erroneous or misleading texts by LLMs can be exploited to disseminate harmful misinformation on social media and news platforms, significantly influencing public opinion, political processes, and societal trust. Additionally, the remarkable capabilities of state-of-the-art LLMs to generate realistic audio and video content from text have intensified ethical, social, and safety concerns surrounding deepfake technology, which has historically had detrimental effects on society.\n\u2022 Autonomous AI Risks (Section 6): In addition to the above three key risk areas, we further explore growing concerns surrounding the development of autonomous AI with LLMs. As LLMs advance toward human-level capabilities, concerns on societal and ethical risks associated with autonomous AI have reemerged, particularly regarding risks of advanced LLMs deployed online or in autonomous or semi-autonomous environments. Such risks include but are not limited to pursuing a number of convergent instrumental goals (e.g., self-preservation, power-seeking) (Benson-Tilsen & Soares, 2016), deception and situational awareness. Theoretically formalizing/validating such risks and empirically detecting and evaluating them pose significant challenges for frontier AI/LLM safety.."}, {"title": "2.2 Related Areas to LLM Safety", "content": "When granted the autonomy to use tools, perform actions, and interact with the environment, LLM-powered agents can demonstrate highly efficient and automated task-solving capabilities. However, this autonomy also brings the risk of making unpredictable or uncontrollable decisions. Beyond external aspects of LLMs (e.g., interactions between LLM-driven agents and environments), the prevalence and severity of various safety risks have prompted investigations into the internal mechanisms of LLMs. These efforts aim to address critical issues related to transparency and interpretability regarding both LLM capability and safety, which arise due to the black-box nature of LLMs.\nIn the perspective of safe deployment and application, many AI/LLM companies and research institutions allocate a lot of resources to implement various safety techniques to safeguard deployed LLMs from unsafe behaviors, e.g., generating biased, toxic, or immoral responses. In addition to technical measures, as LLMs increasingly permeates various sectors, the establishment of a comprehensive and robust governance framework at a high level has become an imperative. Such a framework should not only ensure that the development and deployment of LLMs adhere to globally recognized ethical standards but also foster international cooperation and regulatory coordination to achieve mutual benefits and shared prosperity in global AI technology.\nIn light of these considerations, we further discuss four important areas related to LLM safety: Agent Safety, Interpretability for LLM Safety, Technology Roadmaps / Strategies to LLM Safety in Practice and Governance, in addition to the key risk areas of LLMs.\n\u2022 Agent Safety (Section 7) examines the risks associated with two primary categories of LLM-powered agents: language agents and embodied agents. While these agents offer tremendous potential for automation and innovation across various sectors, they also present a range of concerns, including the potential for malicious use, misalignment with human values, privacy invasion, and unpredictable behaviors. This section delves into these risks in detail, exploring their implications for society, economy, and individual privacy. Additionally, it outlines mitigation strategies and resources aimed at enhancing the safety and reliability of LLM-driven agents. As LLMs continue to evolve, understanding and addressing these risks become crucial for ensuring the responsible development and deployment of AI technologies.\n\u2022 Interpretability for LLM Safety (Section 8) emphasizes the role of interpretability in enhancing the safety of LLMs used in critical fields. It highlights how interpretability helps make LLMs' decision-making processes transparent, enabling better evaluation and control. Key benefits include improving performance, addressing biases, and ensuring safe outputs. It introduces a taxonomy of interpretability for LLM safety, covering understanding model capabilities, safety auditing, and aligning LLMs with human values. The risks of interpretability research are also discussed, including the dual-use of technology, adversarial attacks, misunderstanding or over-trusting explanations, and the potential for interpretability to accelerate uncontrollable risks.\n\u2022 Technology Roadmaps / Strategies to LLM Safety in Practice (Section 9) delineates the current landscape of safety measures and strategies employed by various prominent LLMs. This section elaborates and compares the safety roadmaps implemented by key industry players, including OpenAI, Anthropic, Baidu, Google DeepMind, Microsoft, 01.AI, Baichuan, Tiger Research, Alibaba Cloud, DeepSeek-AI, Mistral AI, Meta, Shanghai AI Laboratory and Zhipu AI, to ensure the reliability and safety of LLMs in practical applications. Additionally, it also discusses the contributions of certain research institutions that, despite not releasing LLMs, are actively engaged in AI safety research and development.\n\u2022 Governance (Section 10) delves into the multifaceted realm of AI governance, exploring proposals, policies, and visions that collectively shape the future of AI development and deployment. As AI continues to rapidly evolve and integrate into various aspects"}, {"title": "3 Value Misalignment", "content": "As LLMs become increasingly sophisticated and ubiquitous, their potential for profound societal impact has brought critical safety considerations to the forefront of attention across various domains. This section provides a comprehensive survey on the safety concerns emerging from potential value misalignment in LLMs, which represents a multifaceted phenomenon where LLMs generate outputs that diverge from human ethical standards, societal norms, and fundamental moral principles. This misalignment manifests through four critical dimensions: social bias, privacy, toxicity, and ethics and morality."}, {"title": "3.1 Social Bias", "content": "Bias and fairness in LLMs, have emerged as critical areas of concern. They often manifest in various forms, perpetuating stereotypes or unfairly disadvantaging certain social groups. Social bias is one of the most pervasive forms of bias in LLMs, and it can take on multiple dimensions, from derogatory language and stereotypes to prejudice across different social and cultural groups. In the following sections, we will explore this issue in more depth, beginning with a discussion on the definition of social bias and its safety implications, followed by an analysis of how social bias manifests throughout the LLM lifecycle and elaborations on methods available for mitigation as well as evaluation frameworks necessary to ensure fairness and safety."}, {"title": "3.1.1 Definition and Safety Impact", "content": "Social bias in LLMs refers to the reinforcement of stereotypes and inequalities through language, which can lead to harmful outcomes for specific social groups. Such biases manifest in multiple ways, from the use of derogatory language and perpetuation of negative stereotypes to LLM performance disparities across language and social groups. For instance, the Social Categories and Stereotypes Communication (SCSC) framework (Beukeboom & Burgers, 2019) demonstrates how language can maintain social categories, which reinforces social biases. A common and troubling example is the association of the term \u201cMuslim\u201d with terrorism in LLMs, which reflects and amplifies anti-Muslim biases in these models (Abid et al., 2021). Such biases, when embedded in widely used systems, pose a significant safety risk by perpetuating harmful narratives that could increase societal divisions and marginalization.\nFrom a safety perspective, the impact of social bias goes beyond just harmful language. LLMs can generate toxic content, such as hate speech or violent language, which not only harms targeted individuals or groups but also creates broader societal risks (Dixon et al., 2018). This introduces direct safety concerns, as the propagation of hate speech or discriminatory content can exacerbate tensions, leading to real-world harms. Furthermore, biased LLM outputs in sensitive applications like healthcare, education, or law can result in biased decisions that"}, {"title": "3.1.2 Social Bias in the LLM Lifecycle", "content": "Bias can infiltrate various stages of the LLM lifecycle, from training data to deployment, each contributing to the overall safety of the final model.\nTraining Data LLMs could be trained on non-representative samples, which limits their ability to generalize across different social groups. For instance, biases in training data have led to skewed results in hate speech detection, particularly against African American Vernacular English (AAVE) (Gallegos et al., 2023). Even well-curated datasets may reflect"}, {"title": "3.1.3 Methods for Mitigating Social Bias", "content": "Social bias mitigation in LLMs can be approached through multiple strategies across different phases in the lifecycle of LLMs. This section explores four distinct stages in which bias mitigation techniques can be applied: pre-processing, pre-training, post-training, and inference."}, {"title": "3.1.4 Evaluation", "content": "Metrics A number of metrics have been developed to quantify and analyze biases across different aspects of LLMs. Embedding-based metrics like WEAT (Caliskan et al., 2017) and SEAT (May et al., 2019) are commonly used to measure biases in word and sentence embeddings. These metrics work by comparing the distances between biased and unbiased representations in the form of embeddings, providing insights into how deeply biases are ingrained in language representations learned by LLMs (Panch et al., 2019; Shumailov et al., 2023). Probability-based metrics offer another approach to bias evaluation, focusing on the probabilities assigned to tokens or sequences during model inference. For example, masked token probabilities and pseudo-log-likelihoods (e.g., Crows-pair (Nangia et al., 2020)) are used to assess the likelihood of generating biased tokens, thereby revealing underlying biases in the decision-making process of language models. Finally, generated text-based metrics evaluate biases in the actual outputs generated by LLMs (Guo & Caliskan, 2021; Webster et al., 2020). These metrics often involve comparing the distribution of tokens associated with different social groups or using classifiers like Perspective API 5 to detect and quantify harmful content. Additionally, lexicon-based analyses can be applied to generated text to identify and score biased language, further contributing to a comprehensive understanding of bias in LLM outputs.\nBenchmarks A range of benchmarks has been created to assess bias in LLMs, providing essential data for evaluating fairness across different tasks. Masked token benchmarks like Winogender (Rudinger et al., 2018), Winobias (Zhao et al., 2018), and GAP (Webster et al., 2018) are designed to test bias by predicting the most likely words in masked sentences, helping to reveal gender and other social biases embedded in models (Nangia et al., 2020;"}, {"title": "3.1.5 Future Directions", "content": "As LLMs continue to evolve and become integrated into critical decision-making processes, addressing bias and ensuring safety will require multifaceted and proactive approaches. Below are several future directions that can help advance efforts in mitigating social bias and enhancing safety in LLMs.\nMulti-Objective Optimization for Fairness and Safety Balancing performance, fair- ness, and safety in LLMs is challenging, especially when optimizing models for token prediction accuracy might inadvertently amplify biases. A promising future direction is the development of multi-objective optimization frameworks that prioritize both fairness and safety alongside performance metrics. By modifying loss functions and training objectives, models can be optimized to reduce bias while still maintaining high accuracy. Furthermore, ongoing research into fairness-aware algorithms that incorporate societal safety as a key objective could ensure that LLMs are better aligned with ethical and legal standards.\nInterdisciplinary Collaboration and Ethical AI Governance Effective bias mitigation and safety in LLMs require collaboration across different disciplines, including machine learning, ethics, law, and social sciences. Developing frameworks for cross-disciplinary collaboration, where technical experts and policy makers work together, will be crucial for ensuring that AI systems are deployed responsibly and ethically. Moreover, ethical AI governance frameworks that provide clear guidelines and regulations for bias detection, model auditing, and risk mitigation should be established. These frameworks could ensure that LLM developers are held accountable for the societal impacts of their systems, fostering greater transparency and trust in AI systems.\nIntegration of Societal and Cultural Awareness Future LLMs should aim to integrate societal and cultural awareness into their decision-making process. By embedding knowledge of diverse social and cultural contexts into LLMs, they can become more sensitive to the nuances of different communities and avoid generating outputs that perpetuate harmful stereotypes. This requires training LLMs not only on diverse datasets but also on datasets that capture the subtleties of different cultural and social practices, languages, and dialects."}, {"title": "3.2 Privacy", "content": "Protecting the privacy of LLMs is a fundamental aspect of maintaining their overall safety. We hence provide an overview of the current status of privacy issues in LLMs (e.g., privacy leakage in LLMs, privacy protection methods for LLMs)."}, {"title": "3.2.1 Preliminaries", "content": "Privacy protection has always been a key issue in the field of AI as privacy leakage could lead to serious consequences. Many legal and regulatory requirements (Rigaki & Garcia, 2020; Mireshghallah et al., 2020; Sousa & Kern, 2023; Guo et al., 2022) have been established. For example, the European Union has introduced the General Data Protection Regulation (GDPR), which sets strict guidelines for the collection, transmission, storage, management,"}, {"title": "3.2.2 Sources and Channels of Privacy Leakage", "content": "We then discuss the sources of private risks and the channels of privacy leakage, aiming to provide a deep understanding of the underlying causes of privacy breaches in large language models.\nSources of Privacy Risks We roughly identify two major sources of privacy risks for LLMs. The first is training data from the Internet (Piktus et al., 2023a; Li et al., 2023c). Web-crawled data have been widely used as a major data source to train LLMs. However, data crawled from the Internet inevitably contain non-public, private, or sensitive information."}, {"title": "3.2.3 Privacy Protection Methods", "content": "Privacy protection methods for LLMs can be roughly divided into three categories according to the life cycle of LLMs (Guo et al., 2022; Sousa & Kern, 2023).\nPrivacy Protecting at the Data Processing Stage Most of these methods aim to remove sensitive information at the data processing stage. Traditional methods like de- identification (Meystre et al., 2014) and anonymization (Majeed & Lee, 2020) are widely used to achieve this goal. For instance, names, addresses, and other PII can be generalized or replaced with pseudonyms or placeholders, making it hard to identify individuals while still preserving the dataset's structure and inner dependencies. Additionally, aggregation techniques can be applied to reduce the granularity of data, such as grouping inference queries by day or week, instead of storing individual query details. This reduces the risk of re-identification and limits the potential privacy exposure. In addition to de-indentification and anonymization, Lee et al. (2021) find that removing duplicate data from pre-training corpora can effectively reduce LLMs' memorization of training data. Although filtering or reformulating private data is a direct way to eliminate privacy risks, it is still difficult to completely remove private or sensitive data.\nPrivacy Protecting at the Pre-training or Fine-tuning Stage A variety of methods are used at in the pre-training or fine-tuning stage to reduce the degree to which LLMs memorize training data. The differential privacy gradient optimization method reduces the probability of model memorization by adding noise to the gradient of private data (Li et al., 2021; Wu et al., 2022), but such methods can negatively impact of LLMs on downstream tasks. During the model fine-tuning phase, alignment techniques are also applied to enable LLMs to refuse to respond to queries involving privacy issues. However, it remains challenging to balance user experience and privacy protection (Staab et al., 2023).\nPrivacy Protecting at the Model Pre-deployment Stage Methods that adjust model parameters by editing or light retraining are proposed to protect privacy during the pre- deployment stage. Machine unlearning methods involve light parameter retraining to help LLMs forget private information (Eldan & Russinovich, 2023; Chen & Yang, 2023; Yao et al., 2023). These methods focus on selectively removing specific knowledge from an LLM,"}, {"title": "3.3 Toxicity", "content": "Toxicity in LLMs refers to the generation of harmful or offensive content. As LLMs are increasingly integrated into sensitive domains and everyday applications, mitigating toxicity has become a critical concern to ensure safe and responsible deployment. In this section, we explore the methods for toxicity mitigation and the evaluation benchmarks commonly used to assess and reduce the toxicity of LLMs."}, {"title": "3.3.1 Definition and Safety Impact", "content": "Building upon the discussion of bias, toxicity in LLMs focuses on the generation of content that is explicitly harmful, offensive, or inappropriate. This includes hate speech, abusive expressions, threats, or any content that can directly inflict emotional or psychological harm on individuals or groups. While bias often refers to the implicit reinforcement of stereotypes and systemic inequalities (Gallegos et al., 2023), toxicity is characterized by its overtly harmful nature, which can escalate hostility, disrupt social harmony, and threaten safety.\nToxicity in LLMs arises through various mechanisms. First, it can stem from the presence of toxic language in training data, where harmful patterns are learned and subsequently reproduced (Shen et al., 2023). Second, the generalization capabilities of LLMs can lead to unintended toxicity, as LLMs may amplify offensive patterns in contexts where these patterns are not originally present. Third, interactive generation processes\u2014particularly in unsupervised or conversational settings\u2014can produce toxic outputs due to ambiguous prompts or adversarial user interactions (Bianchi & Zou, 2024).\nAddressing toxicity in LLMs is critical not only for ensuring socially responsible AI but also for safeguarding users who interact with these models. Unlike misinformation, which involves the spread of false or misleading content, toxicity centers on language that is inherently harmful in tone or intent, regardless of its factual correctness. For instance, an LLM that generates an accurate but aggressively phrased response may still cause significant harm if it fails to account for the emotional and social impact of its language.\nThe safety implications of toxicity are multifaceted. Toxic outputs can lead to the marginal- ization of vulnerable groups, contribute to societal polarization, and erode trust in AI systems. Furthermore, when toxic language is normalized or amplified by LLMs, it can influence public discourse in harmful ways, fostering an environment where hostility and abuse become more prevalent. This presents unique challenges for LLMs deployed in sensitive domains such as finance, mental health support, and public-facing applications, where the stakes for harm are particularly high (Valdez-Valenzuela & G\u00f3mez-Adorno, 2024; Chung et al., 2023b; Nazi & Peng, 2024; Chen et al., 2024c).\nIn combating toxicity, techniques such as toxicity detection and mitigation frameworks, post-processing filters, and reinforcement learning from human feedback (RLHF) (Bai et al., 2022b) have been explored. These methods aim to reduce the likelihood of toxic outputs while preserving the fluency and relevance of LLM-generated responses. However, striking a balance between minimizing toxicity and maintaining the versatility of LLMs remains a significant challenge in practice.\nIn the subsequent section 5.2 on misinformation, we will explore how the fluency and human- like quality of LLMs contribute to the generation and dissemination of false or misleading content. Together, toxicity, bias, and misinformation represent critical dimensions of LLM safety, each with distinct yet interconnected implications for the ethical and responsible deployment of these models."}, {"title": "3.3.2 Methods for Mitigating Toxicity", "content": "Pre-training Phase Training data from web sources often contains toxic content. To mitigate this, existing detoxification methods typically apply toxicity filters during the pre- training phase to remove data with high toxicity scores from the training set (Welbl et al., 2021). For instance, Ngo et al. (2021) calculate the conditional likelihood of each document in the training set, relative to user-defined trigger phrases containing harmful content, using a pre-trained model to assess the toxicity of the document. However, filtering pre-training data in this manner may lead to a degradation in the general capabilities of LLMs"}]}