{"title": "TokenSynth: A Token-based Neural Synthesizer\nfor Instrument Cloning and Text-to-Instrument", "authors": ["Kyungsu Kim", "Junghyun Koo", "Sungho Lee", "Haesun Joung", "Kyogu Lee"], "abstract": "Abstract--Recent advancements in neural audio codecs have enabled\nthe use of tokenized audio representations in various audio generation\ntasks, such as text-to-speech, text-to-audio, and text-to-music generation.\nLeveraging this approach, we propose TokenSynth, a novel neural\nsynthesizer that utilizes a decoder-only transformer to generate desired\naudio tokens from MIDI tokens and CLAP (Contrastive Language-Audio\nPretraining) embedding, which has timbre-related information. Our\nmodel is capable of performing instrument cloning, text-to-instrument\nsynthesis, and text-guided timbre manipulation without any fine-tuning.\nThis flexibility enables diverse sound design and intuitive timbre con-\ntrol. We evaluated the quality of the synthesized audio, the timbral\nsimilarity between synthesized and target audio/text, and synthesis\naccuracy (i.e., how accurately it follows the input MIDI) using objective\nmeasures. TokenSynth demonstrates the potential of leveraging advanced\nneural audio codecs and transformers to create powerful and versatile\nneural synthesizers. The source code, model weights, and audio demos are\navailable at: https://github.com/KyungsuKim42/tokensynth\nIndex Terms-Neural synthesizer, instrument cloning, text-to-\ninstrument, neural audio codec, transformer, MIDI", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in generative models, particularly in the\naudio domain, have led to remarkable progress in tasks such as\ntext-to-speech, text-to-audio, and text-to-music synthesis [1]\u2013[6].\nThese models are predominantly based on diffusion models [7]\nand transformer architectures [8], with the latter being well-suited\nfor real-time audio tasks due to their autoregressive nature. The\ndevelopment of neural audio codecs [9]\u2013[11], which compress audio\ninto a sequence of discrete tokens with a significantly lower frame\nrate than the original audio sample rate and reconstruct high-quality\naudio from these tokens, has been instrumental in enabling the use\nof transformers for audio generation [1]\u2013[4].\nWhile music generation models have made significant strides [3],\n[12], [13], they still face limitations in terms of user controllability\nwhen generating fully mixed music. In contrast, the real-world music\ncreation process often involves composing multiple tracks using\ndifferent instruments, then mixing them to create the final piece.\nThis highlights the importance of considering instruments as essential\nbuilding blocks in music production.\nUsing instruments as basic compositional units aligns with the\nnatural music creation process. In this context, efforts to expand"}, {"title": "II. RELATED WORK", "content": "Numerous attempts have been made to implement synthesizers\nusing neural networks, primarily focusing on synthesizing the sounds\nof predefined instruments [17] or enabling limited timbre control by\ninterpolating between their timbres [14], [15], [18]."}, {"title": "III. METHOD", "content": "TokenSynth is a neural codec language model that autoregressively\ngenerates audio tokens a \u2208 {1,...,Ka}N\u00d7D conditioned on a\ntimbre embedding e \u2208 RdCLAP and MIDI tokens m \u2208 {1,..., Km}M.\nFig. 1 illustrates the overall architecture of TokenSynth during both\nthe training and inference phases. We first explain how we extract the"}, {"title": "A. Representations", "content": "1) Timbre Embedding: We utilize a pretrained CLAP model [29],\na cross-modal representation learning model inspired by CLIP [35],\nto obtain timbre embeddings. Trained on a large number of audio-\ntext pairs, CLAP can generate embeddings from both audio and text\ninputs, mapping them into a shared embedding space. This shared\nrepresentation enables TokenSynth to simultaneously perform instru-\nment cloning, text-to-instrument synthesis, and text-guided timbre\nmanipulation.\nTo evaluate the pretrained CLAP model's ability to capture rich\ntimbre information, we trained an MLP classifier on the extracted\nembeddings to classify 953 instruments, achieving a 90.4% top-\n1 validation accuracy. This result confirms that CLAP features are\nsufficient for conditioning instrument timbre.\nSince the dimensions of the CLAP encoder and token embeddings\ndiffer, we introduce a projection layer proj(.) using a 2-layer MLP\nand define \u00ea = proj(e) \u2208 Rdemb as the timbre embedding.\n2) MIDI Tokenization: MT3 [36] pioneered the tokenization of\nMIDI data for transformer-based processing, and subsequent studies\nhave adopted this approach. We also adopt this MIDI tokenization\nmethod with minor modifications.\nEach note is represented using four token types:\nTherefore, a MIDI file with n notes is represented by a sequence\nof tokens m = m1:\u043c \u2208 {1, ..., Km}M, where the sequence length\nis M = 4n.\n3) Audio Tokenization: We adopt the Descript Audio Codec\n(DAC) [11] as the neural audio codec for audio tokenization. DAC\nemploys techniques such as periodic activation functions, complex\nSTFT discriminators, and quantizer dropout to achieve higher com-\npression rates while preserving audio quality. For single-channel"}, {"title": "B. Training", "content": "The model architecture and training method follow the standard\ndecoder-only transformer approach except the model predicts D\nnumber of tokens at each timestep. We use D number of softmax\nlayers to calculate the probability of tokens for each codebook.\nTraining is performed by minimizing the cross-entropy loss for\nthe next token prediction task, similar to a typical decoder-only\ntransformer. The cross-entropy loss is calculated for each codebook\nas follows:\nThe model architecture and training method follow the standard\ndecoder-only transformer approach, except that the model predicts\nD tokens at each timestep. We use D softmax layers to compute\nthe token probabilities for each codebook. Training is performed by\nminimizing the cross-entropy loss for the next-token prediction task,\nas in a typical decoder-only transformer. The total cross-entropy loss\nacross all codebooks is calculated as follows:\nTo prevent the transformer from capturing performance-related in-\nformation from the timbre embedding e, we ensure that the reference\naudio used to extract e and the target audio to be generated share\nthe same instrument but differ in performance. This encourages the\ntransformer to generate audio tokens that accurately represent the\ngiven MIDI sequence with the timbre encoded in the embedding.\nAdditionally, we train an unconditional audio generation model for\napplying classifier-free guidance during inference and an instrument-\nagnostic music transcription model to evaluate synthesis accuracy.\nThe unconditional model generates audio tokens without timbre or\nMIDI conditioning, while the transcription model is trained without\nthe timbre embedding and with the conditioning order of MIDI and\naudio tokens reversed. Unlike MusicGen [3], these models are trained\nindependently without parameter sharing with the main model."}, {"title": "C. Inference", "content": "During inference, we first extract the timbre embedding using the\nCLAP encoder. As CLAP is a cross-modal encoder, we can use either\na reference audio, text, or multiple references and interpolate them.\nThis interpolation between cross-modal representations enables text-\nguided timbre manipulation, allowing the modification of a reference\naudio's timbre using text.\nGiven the timbre embedding and MIDI tokens, we generate audio\ntokens using the top-p sampling method (nucleus sampling).\n1) First-Note Guidance: To more strongly enforce the timbre\ncondition, we optionally apply the classifier-free guidance method by\nextrapolating the logit values inputs to the final softmax layer-as\nsuggested in MusicGen [3].\nHowever, applying classifier-free guidance at every time step can\nlead to unintended noise. Specifically, when both the conditional and\nunconditional models predict silent audio tokens, the guidance may\ninadvertently lower the logit value of the silence token, resulting in\nthe sampling of unintended non-silent tokens.\nTo address this issue, we introduce the \"first-note guidance\"\ntechnique, where classifier-free guidance is applied only at the time"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To enable zero-shot instrument cloning on unseen instruments,\nthe model must learn a general timbre representation by training\non various instruments. Synthesizing arbitrary MIDI sequences also\nrequires diverse MIDI data. We adopt the synthetic approach by\nKim et al. [28], combining the NSynth and Lakh MIDI datasets to\ngenerate synthetic instrument audio. We rendered 10,000 five-second\npolyphonic audio samples per instrument in NSynth, producing 9.53\nmillion MIDI-audio pairs for training and 530,000 pairs for testing.\nTo enhance timbre diversity, we augmented the audio using a dig-\nital effect chain with random parameters. Following Koo et al. [37],\nwe randomly applied EQ, distortion, and algorithmic reverb with a\n0.5 probability, using parameters from predefined ranges. Identical\neffects were applied to each reference-target pair to maintain timbre\nconsistency. This process doubled the dataset size."}, {"title": "B. Model", "content": "We used the pre-trained 44kHz version of DAC and CLAP with the\ncheckpoint music_audioset_epoch _15_esc_90.14.pt,\nfreezing both models during training.\nThe decoder-only transformer has 12 layers, 16 heads, 1024\nembedding, 4096 feed-forward dimensions, and 0.1 dropout. The\nmodel has 175M parameters in total.\nModels were trained on both the original and augmented datasets,\nreferred to as TokenSynth and TokenSynth-Aug, respectively. Each\nmodel was trained for one epoch using the Adam optimizer [38]\nwith a learning rate of 1e-4, \u03b2\u2081 = 0.9, \u03b22 = 0.999, and a batch size\nof 8. This resulted in 1.2M steps for TokenSynth and 2.4M steps\nfor TokenSynth-Aug. Mixed-precision training was used to reduce\nmemory usage [39], [40]."}, {"title": "C. Evaluation Metrics", "content": "We computed multi-scale spectral (MSS) loss, CLAP score, and\nF-score for objective evaluation. MSS loss measures the overall\nsynthesis accuracy, considering both instrument cloning performance\nand adherence to the MIDI condition. We used the same parameters\nas suggested in DDSP to calculate MSS loss.\nCLAP score measures timbral similarity between synthesized and\ntarget audio. For instrument cloning, it is computed as the cosine\nsimilarity between CLAP embeddings of the target and synthesized\naudio. In text-to-instrument tasks, the reference text is used to extract\nthe CLAP embedding instead of the target audio, as there is no ground\ntruth target audio.\nF-score evaluates adherence to the MIDI condition. Synthesized\naudio is transcribed into MIDI note sequences using the audio-\nconditional MIDI transcription model described in Section III-B.\nThe F-measure is calculated based on standard criteria for music\ntranscription, including offset timing [41]."}, {"title": "A. Instrument Cloning", "content": "In this experiment, we synthesized audio by inputting 200 reference\naudio-MIDI pairs per instrument in the test set, using top-p sampling"}, {"title": "C. Text-guided Timbre Manipulation", "content": "Since the CLAP embeddings extracted from the reference audio\n(ea) and reference text (et) reside in a shared embedding space, it\nis possible to interpolate between ea and et to obtain an embedding\nea = a x et + (1-a) x ea for timbre conditioning. This enables\nthe manipulation of the reference audio's timbre in the direction\nof the text description, providing highly flexible timbre control\nbeyond simply cloning the reference audio's timbre. Furthermore, it\nis possible to interpolate or even extrapolate between multiple CLAP\nembeddings.\nSince the CLAP embeddings extracted from the reference audio\n(ea) and reference text (et) share the same embedding space, they\ncan be interpolated to obtain a timbre embedding ea = a x et +\n(1-a) x ea. This allows flexible timbre manipulation, blending\nthe reference audio's timbre with the text description beyond simple\ntimbre cloning. Additionally, interpolation between more than two\nembeddings or even extrapolation between multiple CLAP embed-\ndings is possible.\nAs it is difficult to quantitatively evaluate the performance\nof this task we refer readers listen to the audio demos at\nhttps://github.com/KyungsuKim42/tokensynth."}, {"title": "VI. LIMITATIONS AND CONCLUSION", "content": "TokenSynth has several limitations. It cannot perform real-time\nsynthesis as it requires the complete MIDI sequence before generating\naudio tokens. Additionally, due to the stochastic nature of autore-\ngressive sampling, the model may not strictly follow the input MIDI\nnotes. The MIDI tokenization also uses only four velocity values,\nlimited by the NSynth dataset, resulting in less precise velocity\ncontrol compared to other methods.\nDespite these limitations, TokenSynth shows strong potential by\nenabling zero-shot instrument cloning, text-to-instrument synthesis,\nand text-guided timbre manipulation without fine-tuning. Our ex-\nperiments validate its effectiveness in cloning timbres from unseen\ninstruments and highlight the advantages of training with augmented\ndata for improved performance and generalization across diverse\ntimbres and audio conditions."}], "equations": ["L = \\sum_{n=1}^{N} \\sum_{d=1}^{D} - \\log P_{\\theta}(a_{n,d}|e, m, a_{1:n-1,1:D})\\tag{1}"]}