{"title": "BF-STVSR: B-Splines and Fourier\u2014Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution", "authors": ["Eunjin Kim", "Hyeonjin Kim", "Kyong Hwan Jin", "Jaejun Yoo"], "abstract": "Enhancing low-resolution, low-frame-rate videos to high-resolution, high-frame-rate quality is essential for a seamless user experience, motivating advancements in Continuous Spatial-Temporal Video Super Resolution (C-STVSR). While prior methods employ Implicit Neural Representation (INR) for continuous encoding, they often struggle to capture the complexity of video data, relying on simple coordinate concatenation and pre-trained optical flow network for motion representation. Interestingly, we find that adding position encoding, contrary to common observations, does not improve-and even degrade-performance. This issue becomes particularly pronounced when combined with pre-trained optical flow networks, which can limit the model's flexibility. To address these issues, we propose BF-STVSR, a C-STVSR framework with two key modules tailored to better represent spatial and temporal characteristics of video: 1) B-spline Mapper for smooth temporal interpolation, and 2) Fourier Mapper for capturing dominant spatial frequencies. Our approach achieves state-of-the-art PSNR and SSIM performance, showing enhanced spatial details and natural temporal consistency. Our code will be available soon.", "sections": [{"title": "1. Introduction", "content": "Enhancing low-resolution, low-frame-rate videos to high-resolution, high-frame-rate quality is crucial for delivering seamless user experiences. To address this, deep learning approaches for Video Super-Resolution (VSR) [3, 4, 43, 44, 46] and Video Frame Interpolation (VFI) [1, 8-10, 13, 18, 38, 47, 50] have been extensively studied. VSR typically enhances spatial resolution of target frames by leveraging information from neighboring frames, while VFI improves temporal resolution by predicting inherent motion in video data. However, many existing methods are limited by fixed scaling factors set during training, which restricts their flexibility in real-world applications.\nOn the other hand, Implicit Neural Representation (INR) has recently garnered attention for its capability to represent signals continuously through a multi-layer perceptron (MLP), making it a promising approach for super-resolution (SR) tasks [5, 15, 22, 32]. Building on these advancements,"}, {"title": "2. Related Work", "content": "recent studies have extended INR to video data to achieve Continuous Spatial-Temporal Video Super Resolution (C-STVSR), which enables spatial and temporal interpolation simultaneously at arbitrary scales [6, 7]. VideoINR [7] was the first method to map spatiotemporal coordinates (x, y, t) to backward motion field, facilitating backward warping of spatial features to any temporal coordinate. MOTIF [6] improved on this by replacing the backward warping with forward warping, using softmax splatting [31]. In addition, to facilitate the learning in an explicit way, MoTIF supply forward optical flow maps estimated between reference frames as contextual information, using RAFT [42].\nWhile VideoINR and MoTIF successfully integrate INR into the C-STVSR task, they have notable limitations. Specifically, they generate target features by encoding latent features that are simply concatenated with target coordinates, without employing advanced position encoding techniques. This is surprising, given that various position encoding methods such as Fourier encoding [27, 41]-are well-established and widely used in tasks like image SR with INR due to their effectiveness, having become a conventional process [16, 22, 32, 48]. This straightforward approach may fall short in capturing the nuanced details of spatial and temporal features, especially for motion features, which are inherently complex and dynamic. Consequently, both models struggle to retain high-frequency information in the encoded spatial features, a well-known limitation referred to as spectral bias [34, 41], resulting in the generation of lower-quality frames.\nInterestingly, however, we find that simply adding position encoding does not improve and even degrade-performance in these models, an unexpected outcome that contrasts with the general success of position encoding in enhancing INR applications [12, 19, 28, 45]. This issue becomes particularly pronounced when combined with pre-trained optical flow networks. We conjecture that, while these networks provide useful guidance for motion representation, integrating them with position encoding can inadvertently limit the model's flexibility to fully leverage diverse video information.\nTo address these limitations, we propose BF-STVSR, a framework consisting of two modules: B-spline Mapper (BM) and Fourier Mapper (FM), each designed to handle temporal and spatial features. First, B-spline Mapper (BM) utilizes B-spline function, well-known established method for constructing smooth curves or surfaces [32]. This approach well-suited for capturing the continuous nature of video motion. Next, Fourier Mapper (FM) represents spatial features by estimating dominant frequency information of input video frames, effectively capturing fine details. Additionally, based on our earlier analysis, rather than directly integrating a pre-trained optical flow network into our framework, we incorporate it implicitly as a loss function during training to guide the network. This design allows the model to fully leverage the rich information inherent in video data, yielding enhanced interpolated outputs.\nIn summary, our contributions are as follows: (1) We propose BF-STVSR, a framework that independently address the spatial and temporal axes by leveraging their distinct characteristics. To achieve this, (2) we design two dedicated components, B-spline Mapper (BM) for temporal motion representation and Fourier Mapper (FM) for spatial feature representation. (3) We achieve state-of-the-art performance in C-STVSR, demonstrating the effectiveness of our approach through extensive experiments."}, {"title": "2.1. Arbitrary Single Image Super Resolution", "content": "Single Image Super Resolution (SISR) methods [23, 24, 37, 54] have achieved impressive performance, but their reliance on fixed scales limits their applicability in real-world scenarios. To address this, several studies have proposed methods to perform super resolution at arbitrary scales [5, 22, 26, 32, 51]. LIIF [5] introduced an Implicit Neural Representation (INR) approach for super resolution, representing images continuously through local implicit functions and enabling arbitrary scale upsampling. [26, 51] further used position encoding to address the spectral bias problem [34]. Recently, LTE [22] proposed identifying dominant Fourier bases from latent features to effectively capture fine details and address spectral bias. Similarly, BTC [32] employed B-spline bases instead of Fourier bases to mitigate the Gibbs phenomenon observed in Screen Content Image Super Resolution. Inspired by these methods, we explore effective position encoding techniques for C-STVSR task, which reflect the characteristics of video data."}, {"title": "2.2. Spatial-Temporal Video Super Resolution", "content": "While conventional Video Super-Resolution (VSR) [2-4, 36] and Video Frame Interpolation (VFI) [8-11, 13, 18, 20, 21, 30, 31, 33, 35, 38, 53] perform interpolation along either spatial or temporal axis, Spatial-Temporal Video Super Resolution (STVSR) conducts interpolation along both axes. Haris et al. [14] have introduced a unified framework for addressing STVSR and Xiang et al. [47] have proposed to use bidirectional deformable ConvLSTM. Although these studies demonstrate impressive performance in STVSR, they both have the limitation of only addressing STVSR at fixed scales. Recently, two works [6, 7] have been proposed for Continuous Spatial-Temporal Video Super Resolution (C-STVSR), which performs interpolation at arbitrary scales along both spatial and temporal axes. VideoINR [7] is the first work on C-STVSR, which takes space-time coordinates as input and maps the corresponding RGB value in continuous manner using INR. Following"}, {"title": "3. Method", "content": "this, MOTIF [6] generates temporal features using optical flow and performs forward warping to predict the interpolated high-resolution frame features. Although these studies effectively tackle the C-STVSR task, relying solely on MLPs for spatial and temporal modeling leads to difficulties in learning the characteristics of the video. In this work, we adopt Fourier basis and B-spline basis functions to model spatial and temporal features of video data to address the aforementioned difficulties."}, {"title": "3.1. Overview", "content": "The overall flow of our method, BF-STVSR, is illustrated in Figure 2 (a). Our framework is built upon the pipeline of MoTIF [6]. Given two low-resolution frames $I_\\mathcal{L}, I_\\mathcal{L} \\in \\mathbb{R}^{3\\times H\\times W}$, our goal is to generate a high-resolution intermediate frame $I_\\mathcal{H} \\in \\mathbb{R}^{3\\times sH\\times sW}$ at any time $t \\in [0,1]$ with an arbitrary scale s. The encoder E first takes the low-resolution frames as input and produces three latent features: $F_\\mathcal{L}^\\mathcal{F}, F_\\mathcal{L}^{(0,1)}, F_\\mathcal{L}^\\mathcal{F} \\in \\mathbb{R}^{C\\times H\\times W}$. Here, $F_\\mathcal{L}^\\mathcal{F}$ and $F_\\mathcal{L}^\\mathcal{F}$ represent the latent features of $I_\\mathcal{L}^t$ and $I_\\mathcal{L}^t$, while $F_\\mathcal{L}^{(0,1)}$ serves as a template feature for the intermediate frame, incorporating information from both input frames. Next, the latent features $F_\\mathcal{L}^\\mathcal{F}, F_\\mathcal{L}^\\mathcal{F}$ are processed by two mappers. The B-spline Mapper (Section 3.2) predicts high-resolution optical flows to target time t, producing $M_\\mathcal{H}^\\mathcal{L}, M_\\mathcal{H}^\\mathcal{L} \\in \\mathbb{R}^{2\\times sH\\times sW}$, while the Fourier Mapper (Section 3.3) estimates high-resolution spatial features at target scale s, resulting in $F_\\mathcal{H}^\\mathcal{F}, F_\\mathcal{H}^\\mathcal{F} \\in \\mathbb{R}^{C\\times sH\\times sW}$. Finally, the high-resolution features $F_\\mathcal{H}^\\mathcal{F}, F_\\mathcal{H}^\\mathcal{F}$ are temporally propagated to the target time t using forward warping based on the predicted optical flow $M^\\mathcal{L\\rightarrow t}_\\mathcal{H}, M^\\mathcal{H\\rightarrow t}_\\mathcal{H}$, generating intermediate features $F_\\mathcal{H}^\\mathcal{F}_t$. These warped features are then concatenated with target time t and $F_\\mathcal{L}^{(0,1)}$, a nearest-neighbor upsampled version of $F_\\mathcal{L}^{(0,1)}$, and decoded to produce the high-resolution intermediate frame $I_\\mathcal{H}^F$."}, {"title": "3.2. Temporal B-spline Mapper", "content": "Accurately predicting the motion inherent in a video is key to generating a visually plausible intermediate frame from two neighboring frames. While various techniques have been proposed in video frame interpolation (VFI) studies [8, 11, 18, 20, 31, 33, 35, 53] to predict motion accurately, most of them are designed for fixed scale interpolation. Therefore, applying these methods to the C-STVSR task, which requires handling arbitrary scales, is not straightforward. To overcome this challenge, VideoINR [7] and MOTIF [6] use implicit neural representations (INR) with MLPs that take spatiotemporal coordinates as input, enabling motion modeling at arbitrary target times t and scales s. While INR-based motion modeling provides flexibility in motion prediction, we find that it often struggles to capture the complex, dynamic nature of motion in video."}, {"title": "3.3. Spatial Fourier Mapper", "content": "To better represent inherent motion, we introduce B-spline Mapper, which leverages the B-spline representation. B-spline bases are widely known for their effectiveness in modeling continuous signals [32], making them well-suited for capturing smooth, continuous motion in videos, where objects move smoothly and continuously, rather than in jerky manner. The detailed process of B-spline Mapper is described in Figure 2 (b). We modify the Space-Time Local Implicit Neural Functions (ST-INF) from MoTIF [6], resulting in our B-spline Mapper. Similar to ST-INF, B-spline Mapper predicts high-resolution forward motion vectors $M_\\mathcal{H}^\\mathcal{L\\rightarrow t}, M_\\mathcal{H}^\\mathcal{L\\rightarrow t}$ and reliability maps $Z_\\mathcal{H}^\\mathcal{L\\rightarrow t}, Z_\\mathcal{H}^\\mathcal{L\\rightarrow t}$ at arbitrary time $t \\in [0,1]$. A key difference is that our B-spline Mapper takes encoded features $F_\\mathcal{L}^\\mathcal{F}, F_\\mathcal{L}^\\mathcal{F}$ as input, rather than optical flows from an external network (e.g., RAFT [42]).\nIn addition, rather than directly predicting motion vectors to the target time t, our B-spline Mapper $p_\\theta$ models the video's inherent motion by predicting the B-spline coefficients and knots, as described in the following equation:\n$p_\\theta(z_r, d_r, t) = C_r \\odot \\beta^\\eta\\Big(\\frac{t - k_r}{d}\\Big)$\nHere, $C_r = p_C(z_r, d_r)$, $k_r = p_k(z_r, d_r)$, and $d = p_d(g)$. Specifically, $z = F(q_r)$ is the latent feature vector at the point $q_r = (x_r, y_r)$, nearest to the query coordinates $q = (x, y)$, with the reference frame time index $t_r \\in {0,1}$. The functions $p_C$, $p_k$, and $p_d$ are the estimators for the coefficients $(\\mathbb{R}^{C+2} \\rightarrow \\mathbb{R}^{C})$, knots $(\\mathbb{R}^{C+2} \\rightarrow \\mathbb{R}^{C})$, and dilation $(\\mathbb{R}^{1} \\rightarrow \\mathbb{R}^{C})$, respectively. $t = |t-t_r|$ represents the relative temporal distance of the predicted feature to the reference frame, and $d_r (= q - q_r)$ is the spatial relative coordinate between the query and reference coordinates. Finally, g is the frame interval of the input video.\nAfter linearly projecting the predicted B-spline representation using $f_\\theta$, we obtain the motion vector $\\mathcal{Z}_\\mathcal{H}^{\\mathcal{L\\rightarrow t}}(q)$ and reliability map $M_\\mathcal{H}^{\\mathcal{L\\rightarrow t}}(q)$ at the query coordinates q:\n{$Z^{\\mathcal{L\\rightarrow t}}_\\mathcal{H}(q), M_\\mathcal{H}^{\\mathcal{L\\rightarrow t}}(q)$} = $f_\\theta(p_\\theta(z_r, d_r, t))$.\nUsing the predicted motion vectors, the spatial features $F_\\mathcal{H}^\\mathcal{F}, F_\\mathcal{H}^\\mathcal{F}$ and reliability maps are propagated to the target time t via forward warping using softmax splatting [31]. Finally, we obtain intermediate latent feature $F_\\mathcal{H}^F_t$ and corresponding reliability map $Z_\\mathcal{H}^F$. By directly learning the underlying motion from the input frames instead of individually predicting each arbitrary time t, our B-spline Mapper provides a more robust and flexible motion modeling approach. Additionally, since our method does not rely on optical flow inference during the inference stage, it is more efficient compared to previous approaches like MOTIF [6]."}, {"title": "3.3. Spatial Fourier Mapper", "content": "Even with the robust motion modeling provided by the B-spline Mapper, the quality of the interpolated feature $F_\\mathcal{H}^F$ depends significantly on the features propagated from $F_\\mathcal{H}^\\mathcal{F}$ and $F_\\mathcal{H}^\\mathcal{F}$. VideoINR [7] and MoTIF [6] rely on simple MLPs to interpolate the latent features $F_\\mathcal{L}^\\mathcal{F}$ and $F_\\mathcal{L}^\\mathcal{F}$. However, implicit neural functions often struggle with capturing high-frequency details, leading to poor quality in the interpolated features, as noted in several studies [27, 34, 41].\nTo address this issue, LTE [22] demonstrated that using Fourier bases for spatial feature modeling significantly improves performance in arbitrary-scale super-resolution by effectively capturing dominant frequencies. Inspired by this approach, we integrate a similar strategy into our Fourier Mapper. The detail process is illustrated in Figure 2 (c). The Fourier Mapper $g_\\phi$ predicts the dominant frequency and its amplitude of the Fourier bases for spatial features:\n{$F^F(q), F^F(q)$} = $f_{of}(g_{\\phi}(z_r, d_r))$.\nwhere $\\text{g}_\\phi (z_r, \\delta_r) = A_r \\cdot \\begin{bmatrix} cos(\\pi F_r \\delta_r) \\\\  sin(\\pi F_r \\delta_r) \\end{bmatrix}$\nHere, $A_r = g_a(z_r)$ and $F_r = g_f(z_r)$. Same as B-spline Mapper, $z_r = F(q_r)$ is the nearest latent feature vector from the query coordinates $q = (x,y)$ and $d_r(= q - q_r)$ is the relative coordinates in spatial domain. The $g_a$ and $g_f$ are the amplitude estimator ($\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{2C}$) and the frequency estimator ($\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{2C}$), respectively. By predicting dominant frequencies of query points in latent space, Fourier Mapper improves the frequency details of the interpolated features $F^F$ and $F^F$. After all, by linearly projecting it to Ff and Ff using $f_{of}$, which in turn enhances the quality of $F_\\mathcal{H}^F$. Note that, unlike LTE, which interpolates the estimated amplitude and frequency from low-resolution domain, our method estimates each coefficients in high-resolution domain and does not estimate the phase."}, {"title": "3.4. Training Objective", "content": "We follow [6] to train our model end-to-end :\n$\\mathcal{L} = \\mathcal{L}_{char}(\\hat{I}, I_\\mathcal{H}) + \\beta \\sum_{i=0}^{1} \\mathcal{L}_{char}(M^i, M_\\mathcal{H}^i)$\nwhere $\\mathcal{L}_{char}$ is Charbonnier loss and $\\beta$ is a hyper-parameter which set as 0.01. The $\\hat{I}$ is the ground-truth high-resolution frame at time t and $M_\\mathcal{H}^i$ is the predicted optical flow from the RAFT [42]. Note that we use RAFT only to guide our B-spline Mapper during training."}, {"title": "4. Experiments", "content": "Spatial scale\nTemporal scale\nMOTIF [6]\nOurs\nx4\n\u00d76\n\u00d712\n\u00d76\n\u0394(\u00d712)\n\u0394(\u00d716)\n\u00d76\n\u0394(\u00d712)\n\u0394(\u00d716)\n\u00d76\n\u0394(\u00d712)\n\u0394(\u00d716)\n31.56/0.906 -3.79/-0.083 -5.58/-0.130 29.36/0.850 -2.58/-0.059 -4.02/-0.097 25.81/0.733 -1.09/-0.022 -1.93/-0.040\n31.68/0.908 -3.62/-0.079 -5.28/-0.123 29.44/0.851 -2.38/-0.055 -3.63/-0.089 25.78/0.728 -0.91/-0.018 -1.56/-0.033\nevaluation is performed only on the center frames (i.e., the\n1st, 4th, 9th frames).\nBaseline methods We categorize baeline models into two\ntypes\u2014continuous and fixed-scale\u2014and conduct compar-\nisons within each category. Here, Fixed-scale Space-\nTime Video Super-Resolution (Fixed-STVSR) are limited\nto super-resolving at fixed scaling factors in both axes that\nare learned during the training. First, we select two-stage\nFixed-STVSR methods that combine fixed video super-resolution models (e.g., Bicubic Interpolation, EDVR [46],\nBasicVSR [3]) with video frame interpolation models (e.g.,\nSuperSloMo [17], QVI [50], DAIN [1]). Second, we se-\nlect one-stage Fixed-STVSR method, specifically Zoom-\ningSlowMo [47]. For continuous methods, we select two-\nstage C-STVSR methods that combine continuous image\nsuper-resolution models (e.g., LIIF [5]) with video frame\ninterpolation models (e.g., SuperSloMo [17], DAIN [1]).\nLastly, we select one-stage C-STVSR methods, including\nTMNet [49], VideoINR [7], and MoTIF[6]. Note that TM-"}, {"title": "4.1. Experiments Setup", "content": "Implementation and Training Details To our knowledge, VideoINR [7] and MOTIF [6] are the only models addressing C-STVSR. Since both models employ identical training and testing scheme, we follow the same approach unless otherwise noted. We adopt the same two-stage training strategy: for the first 450,000 iterations, the spatial scaling factor is fixed as 4, while for the remaining 150,000 iterations, it is uniformly sampled from [1, 4]. We use the Adam optimizer with parameters $\u03b2_1$ = 0.9 and $\u03b2_2$ = 0.999, and apply cosine annealing to decay the learning rate from $10^{-4}$ to $10^{-7}$ for every 150,000 iterations. ZoomingSlowMo [47] is used as the encoder, with a batch size of 32, and random rotation and horizontal-flipping for data augmentation. To ensure training stability, we substitute the predicted forward motion with the ground-truth forward motion with a certain probability, starting from 1.0 and gradually reducing to 0 over the first 150,000 iterations. For B-spline Mapper, we use the three-layer SIRENs as the coefficient and knot estimators, and a single fully connected layer as the dilation estimator. In Fourier Mapper, we use three-layer SIRENS as the amplitude and frequency estimators, followed by a 3\u00d73 convolution for spatial encoding. Both B-spline Mapper and Fourier Mapper have hidden dimensions of 64, with SIREN layer dimensions set to 64, 64, and 256.\nDatasets We use the Adobe240 dataset [40] for training, which consists of 133 videos in 720P taken by hand-held cameras. We split these videos into 100 for training, 16 for validation, and 17 for test. During training, nine sequential frames are selected from the video and the 1st and 9th frames are used as input reference frames. Three frames are then randomly sampled between them and used as the target ground-truth frames. For evaluation, we use Vid4[25], Adobe240 [39], and Gopro [29] datasets. Unless otherwise specified, the default spatial scale is 4, while the temporal scaling factor varies across datasets. For Vid4, temporal scale is set to \u00d72 corresponding to single frame interpolation. For Adobe240-Average and Gopro-Average, the temporal scale is set as \u00d78, representing multi-frame interpolation. Additionally, for Adobe240-center and Gopro-Center,"}, {"title": "4.2. Quantitative results", "content": "We compare our model with Fixed-STVSR methods in Table 1. For single-frame interpolation tasks in STVSR, including Vid4, GoPro-Center, and Adobe-Center, our model achieves the best performance on all datasets except Vid4. On Vid4, TMNet outperforms other models, likely due to its training on the Vimeo90K dataset [52], which has characteristics similar to Vid4. For multi-frame interpolation tasks in STVSR, represented by GoPro-Average and Adobe-Average, our model surpasses the performance of the state-of-the-art MoTIF model, which uses a pre-trained optical flow network [42] to generate temporal features during training. This improvement suggests that the B-spline Mapper and Fourier Mapper components in our model provide more robust temporal and spatial feature representations.\nAdditionally, we compare our model with C-STVSR methods in Table 2, focusing on arbitrary-scales that were not seen during training. We perform experiments on GoPro dataset. Note that while TMNet supports continuous frame interpolation, it is limited to \u00d74 spatial super-resolution. Our results show that BF-STVSR achieves the best performance across all test cases, except for a minor disparity (0.03 dB lower PSNR) at \u00d76 temporal scale and \u00d76"}, {"title": "4.3. Qualitative results", "content": "spatial scale. This finding suggests that our B-spline Mapper effectively handles temporal interpolation even at distant time intervals. Based on Table 2, we present the performance degradation in PSNR and SSIM as the temporal scale increases from \u00d76 to \u00d7 12 and \u00d7 16 across various spatial scales in Table 3. Our results demonstrate that MOTIF experiences a more pronounced performance drop than our model under challenging conditions. Specifically, at \u00d712 spatial scale, as the temporal scale increases from \u00d76 to \u00d712, MoTIF shows a degradation of approximately -4.02 dB, while our model degrades by around -3.63 dB, indicating an improvement of nearly 0.4 dB. This suggests that our method remains robust even under extreme conditions, with temporal intervals extending up to \u00d7 16.\nIn Figure 3, qualitative results are shown, comparing our model with VideoINR and MOTIF. The results include interpolated frames for an in-distribution temporal scale (\u00d78), used during training (left), and an out-of-distribution temporal scale (\u00d76), unseen during training (right). For the in-distribution scale, BF-STVSR captures high-frequency details more effectively, particularly in the horse's hooves and the striped shape of the handrails. For the out-of-distribution scale, BF-STVSR demonstrates superior performance in dynamic motion scenes, accurately interpolating edges of the text and the man's face, where other methods produce blurry or ghosted frames. These results highlight our model's ability to perform natural motion interpolation for moving objects while effectively preserving high-frequency details. Additionally, Figure 4 shows interpolated results at an extreme scale with a spatial scale of \u00d74 and a temporal scale of \u00d712. We include interpolated frames at sampled time points (t = 0.25, 0.5, 0.75) along with residual intensity maps compared to ground truth frames. Our method produces sharper and more accurate results than MOTIF, especially in areas like the tire and the region next to the car window. Additional results for all time coordi-"}, {"title": "4.4. Ablation Study", "content": "nates are provided in the supplementary document.\nOptical Flow and Position Embeddings Table 4 compares model performance with and without pre-trained optical flow network, RAFT [42], across different combinations of our proposed B-spline Mapper and Fourier Mapper modules. The first row shows the basic MoTIF [6] configuration. As seen in the second and third row, including the optical flow network with the proposed modules degrades performance. In contrast, directly using the proposed modules to extract spatial and temporal features, without the optical flow network, improves performance across all cases (last three rows). We attribute this improvement to the proposed modules facilitating the model's effective extraction and utilization of the rich information embedded within the video, enhancing its capacity to capture complex spatial and temporal features. In addition, the fourth and fifth row in Table 4 represent the impact of the B-spline Mapper and Fourier Mapper, respectively. The fourth row is the result of using only Fourier Mapper for spatial feature encoding at BF-STVSR (last row) simply mapping the counterpart axis by concatenating latent features with target coordinates. which termed as Ours-F. The fifth row is the result of using only B-spline Mapper for temporal feature encoding at BF-STVSR (last row), which termed as Ours-B. As shown in the table, performance decreases when each mapper is used independently, but the best results are achieved when both mappers are integrated.\nDifferent basis functions We conduct additional experiments by varying the basis functions used for both axes (Table 5). In all configurations, the basis function model the spatial and temporal axes, while other components (e.g., linear projection for intermediate motion vectors and reliability maps) remain the same. Although the performance differences among these configurations are minimal (around 0.04 dB), we adopt the configuration using B-spline for temporal representation and Fourier for spatial representation, as it achieves the highest performance.\nLimitations While our method demonstrates performance improvements, there still remain certain limitations."}, {"title": "5. Conclusions", "content": "As shown in Figure 5, existing C-STVSR models, including ours, still struggle with handling large motion. Moreover, the training process of C-STVSR models is time-consuming and computationally expensive. Addressing these challenges remains an open problem for future research.\nIn this paper, we proposed BF-STVSR, a novel framework for Continuous Spatial-Temporal Video Super Resolution (C-STVSR). We introduced two specialized position encoding modules: B-spline Mapper, which leverages B-spline basis functions for smooth and accurate temporal interpolation, and Fourier Mapper, which captures dominant spatial frequencies to effectively model fine-grained spatial details. Our analysis highlights that naive position encoding can degrade performance, particularly when paired with optical flow networks, emphasizing the importance of tailored encodings for spatial and temporal axes. Through extensive experiments, we show that BF-STVSR achieves state-of-the-art performance in PSNR and SSIM metrics across diverse datasets, demonstrating enhanced spatial detail, natural temporal consistency, and robustness under challenging conditions, including extreme out-of-distribution scales."}]}