{"title": "Learning Physics Informed Neural ODEs\nWith Partial Measurements", "authors": ["Paul Ghanem", "Ahmet Demirkaya", "Tales Imbiriba", "Alireza Ramezani", "Zachary Danziger", "Deniz Erdogmus"], "abstract": "Learning dynamics governing physical and spatiotemporal\nprocesses is a challenging problem, especially in scenarios\nwhere states are partially measured. In this work, we tackle\nthe problem of learning dynamics governing these systems\nwhen parts of the system's states are not measured, specifi-\ncally when the dynamics generating the non-measured states\nare unknown. Inspired by state estimation theory and Physics\nInformed Neural ODEs, we present a sequential optimization\nframework in which dynamics governing unmeasured pro-\ncesses can be learned. We demonstrate the performance of\nthe proposed approach leveraging numerical simulations and\na real dataset extracted from an electro-mechanical position-\ning system. We show how the underlying equations fit into\nour formalism and demonstrate the improved performance of\nthe proposed method when compared with baselines.", "sections": [{"title": "1\nIntroduction", "content": "Ordinary differential equations (ODEs) are used to describe\nthe state evolution of many complex physical systems in\nengineering, biology, and other fields of natural sciences.\nTraditionally, first-principle notions are leveraged in design-\ning ODEs as a form to impose physical meaning and in-\nterpretability (Psichogios and Ungar 1992) of latent states.\nA major issue, however, is the inherent complexity of real-\nworld problems for which even carefully designed ODE sys-\ntems cannot account for all aspects of the true underlying\nphysical phenomenon (Karniadakis et al. 2021). Moreover,\nwe often require prediction of systems whose dynamics are\nnot fully understood or are partially unknown.\nIn this context, Neural ODEs (NODEs) (Chen et al. 2018)\nemerged as a powerful tool for learning complex correla-\ntions directly from the data, where residual neural networks\n(NNs) are used to parameterize the hidden ODEs' states.\nExtensions of NODE were developed to improve learning\nspeed (Xia et al. 2021; Massaroli et al. 2021) and learn-\ning longtime dependencies in irregularly sampled time se-\nries(Xia et al. 2021). A major challenge in learning NODEs\narises when latent states of interest contribute indirectly to\nthe measurements. This is the case when an unmeasured\nstate influences a measured state. In this scenario, NODE'S\nstandard solutions, which are optimized using the adjoint\nmethod (Boltyanskiy et al. 1962), are compromised. Fur-\nthermore, NODE systems may have infinitely many solu-\ntions since parameters and unmeasured states are estimated\njointly. As a consequence, even when the model is capable\nof fitting the data, unmeasured states cannot be accurately\ninferred without constraining the solution space (Demirkaya\net al. 2021). To constrain the solution space, hybrid and\nphysics informed neural ODEs were developed to incor-\nporate physical knowledge of the system being learned\nwhen available(Sholokhov et al. 2023; O'Leary, Paulson,\nand Mesbah 2022). Despite their recent success in neural\nODEs and neural networks in general, these methods lack\nthe ability to learn dynamical systems under the partial mea-\nsurements scenario when dynamics generating unmeasured\nstates are unknown. Moreover, hybrid and physics informed\nstrategies were leveraged to obtain estimations of missing\nstates under partial measurements scenario (Imbiriba et al.\n2022; Demirkaya et al. 2021; Ghanem et al. 2021). Despite\nthe lack of a clear formalization, in these works the authors\nwere imposing some kind of identifiability among states\nby adding known parts of the dynamics, resulting in hy-\nbrid first-principle data-driven models. Nevertheless, these\nworks focus on state estimation using data-driven compo-\nnents to improve or augment existing dynamics but fail to\nlearn global models and do not scale well for large models.\nIn this paper, we propose a sequential optimization ap-\nproach that at each time step solves an alternating opti-\nmization problem for learning system dynamics under par-\ntially measured states, when states are identifiable. The ap-\nproach focuses on learning unknown dynamics from data\nwhere the state related to the unknown dynamics is unmea-\nsured. Since the unobserved dynamics are unknown, we as-\nsume it is described by parametric models such as NNs. We\npropose aiding a model's training with the knowledge of\nthe physics regarding the measured states using a physics-\ninformed loss term. The proposed solution leverages the re-\nlationship between many recursive state-space estimation\nprocedures and Newton's method (Humpherys, Redd, and\nWest 2012) to develop an efficient recursive NODE learning\napproach capable of sequentially learning states and model\nparameters. The benefit of the sequential strategy is twofold:\n(1) reduce the need for accurate initial conditions during"}, {"title": "2 Related Work", "content": "Partial Measurements: In the context of data-driven ODE\ndesigns, most learning frameworks assume that all states\nare measured in the sense that they are directly measured.\nThis assumption does not reflect many real-world scenarios\nwhere a subset of the states are unmeasured. GP-SSM is a\nwell-established approach used for dynamic systems identi-\nfication (McHutchon et al. 2015; Ialongo et al. 2019). GP-\nSSM can be adapted by introducing a recognition model that\nmaps outputs to latent states to solve the problem of par-\ntial measurements (Eleftheriadis et al. 2017). Nevertheless,\nthese methods do not scale well with large datasets and are\nlimited to small trajectories(Doerr et al. 2018). Indeed, (Do-\nerr et al. 2018) minimizes this problem by using stochas-\ntic gradient ELBO optimization on minibatches. However,\nGP-SSM-based methods avoid learning the vector field de-\nscribing the latent states and instead directly learn a mapping\nfrom a history of past inputs and measurements to the next\nmeasurement.\nSimilar approaches to recognition models have been used\nfor Bayesian extensions of Neural Ordinary Differential\nEquations (NODEs). In these extensions, the NODE de-\nscribes the dynamics of latent states, while the distribu-\ntion of the initial latent variable given the measurements\nare approximated by encoder and decoder networks (Yildiz,\nHeinonen, and Lahdesmaki 2019; Norcliffe et al. 2021). The\nencoder network, which links measurements to latent states\nby a deterministic mapping or by approximating the condi-\ntional distribution, can also be a Recurrent Neural Network\n(RNN) (Rubanova, Chen, and Duvenaud 2019; Kim et al.\n2021; De Brouwer et al. 2019), or an autoencoder (Bakarji\net al. 2023). Despite focusing on mapping measurements to\nlatent states with neural networks and autoencoders, these\nworks were not demonstrated to learn parameterized models\nunder partial measurements. Moreover, this parameterized\nline of work of mapping measurement to latent states suf-\nfers from unidentifiability problem since several latent in-\nputs could lead to the same measurement. Recently, sparse\napproaches such as (Bakarji et al. 2022) merged encoder net-\nworks to identify a parsimonious transformation of the hid-\nden dynamics of partially measured latent states. Moreover,\nNonlinear Observers and recognition models were com-\nbined with NODEs to learn dynamic model parameters from\npartial measurements while enforcing physical knowledge\nin the latent space (Buisson-Fenet et al. 2022). Differently\nfrom the aforementioned methods, in this work, we propose\na recursive alternating approach that uses alternating New-\nton updates to optimize a quadratic cost function with re-\nspect to states and model parameters. Furthermore, the pro-\nposed strategy provides a systematic way to estimate initial\nconditions from historical data.\nSecond order Newton method: Despite the efficiency\nand popularity of many stochastic gradient descent meth-\nods (Robbins and Monro 1951; Duchi, Hazan, and Singer\n2011; Hinton, Srivastava, and Swersky 2012; Kingma and\nBa 2014) for optimizing NNs, great efforts have been de-\nvoted to exploiting second-order Newton methods where\nHessian information is used, providing faster convergence\n(Martens and Grosse 2015; Botev, Ritter, and Barber 2017;\nGower, Goldfarb, and Richt\u00e1rik 2016; Mokhtari and Ribeiro\n2014). When training neural networks, computing the in-\nverse of the Hessian matrix can be extremely expensive\n(Goldfarb, Ren, and Bahamou 2020) or even intractable.\nTo mitigate this issue, Quasi-Newton methods have been\nproposed to approximate the Hessian pre-conditioner ma-\ntrix such as Shampoo algorithm (Gupta, Koren, and Singer\n2018), which was extended in (Anil et al. 2020) to simplify\nblocks of the Hessian, and in (Gupta, Koren, and Singer\n2018) to be used in variational inference second-order ap-\nproaches (Peirson et al. 2022). Similarly, works in (Gold-\nfarb, Ren, and Bahamou 2020; Byrd et al. 2016) focused\non developing stochastic quasi-Newton algorithms for prob-\nlems with large amounts of data. It was shown that recursive\nthe extended Kalman filter can be viewed as Gauss-Newton\nmethod (Bell 1994; Bertsekas 1996). Moreover, Newton's\nmethod was used to derive recursive estimators for predic-\ntion and smoothing (Humpherys, Redd, and West 2012). In\nthis paper, we develop a recursive Newton method that mit-\nigates the problem of partial measurements of latent states."}, {"title": "3 Model and Background", "content": "In this section, we describe our modeling assumptions, dis-\ncuss the identifiability of latent states, and present the time\nevolution of the resulting generative model.\nModel\nIn this work, we focus on dynamical models characterized\nby a set of ordinary differential equations describing the\ntime evolution of system states x(t) and system parameters\n\u03b8(t), and a measurement equation outputting measurements\ny(t) \u2208 Y \u2282 R^{d_y} of a subset of these states. These models\ncan be described as follows (S\u00e4rkk\u00e4 and Svensson 2023):\n$\\begin{aligned}\n\\dot{\\theta}(t) &= v(t) \\\\\n\\dot{x}(t) &= f(x(t), u(t), a(x(t), \\theta(t))) + \\tilde{\\epsilon}(t) \\\\\ny(t) &= h(x(t)) + \\zeta(t)\n\\end{aligned}$$\\nwhere x(t) \u2208 X \u2282 R^{d_x} are systems states and \u03b8(t) \u2208 P \u2282\nR^{d_\u03b8} are system parameters. a : X \u00d7 P represents a sys-\ntem of hidden ODE parameterized by \u03b8(t) that needs to be\nlearned without measurement available. f : X \u00d7 P \u00d7 U\nrepresents a system of ODEs parameterized by \u03b8(t), where\neach equation describes the time dynamics of an individual\ncomponent of a dynamical system. h : X \u2192 Y represents\nthe measurement function. u(t) \u2208 U \u2282 R^{d_u} is a vector of\nexternal inputs, $\\upsilon(t) \\sim N(0,Q_\\theta), \\tilde{\\epsilon}(t) \\sim N(0,Q_x)$, and\n$\\zeta(t) \\sim N(0, R_y)$, are zero mean white noise independent\nof x(t), \u03b8(t) and y(t). The subscript t indicates vectors that\nvary through time.\nThe partial measurement problem: Ideally, states x(t)\nwould be directly measured, and thus appear as an element\nin y(t). In practice, some of these states could influence\ny(t) only indirectly by acting on other measurable states,\nwhere d_y < d_x. That is when classical training fails. In this\nwork, we are interested in learning the unknown dynamics\na(x(t), \u03b8(t)) governing unmeasured states x_h(t), where\n$\\dot{x}_h(t) = a(x(t), \\theta(t)) + \\eta(t)$$\\nwhere x_h (t) \u2282 x(t) and $\\eta (t) \\subset\\tilde{\\epsilon}(t)$. This scenario poses\nfurther challenges over the estimation process since the re-\ncovery of latent states can be compromised.\nIdentifiability of latent states: The task of recovering la-\ntent states x(t) from a sequence of measurements and in-\nputs D_N = {u(0), y(0), ..., u(N \u2212 1), y(N \u2212 1)} depends\non the relationship between measurements and latent states.\nThis problem gets even more complicated when model pa-\nrameters need to be simultaneously estimated. Recent works\ndefine different forms of identifiability to analyze scenar-\nios where latent states and parameters can be properly es-\ntimated (Wieland et al. 2021). Specifically, they define iden-\ntifiability of latent variables x(t) as follows:\nDefinition 1 (State Identifiability) We say that latent vari-\nable x(t_a) is identifiable given a parameter value \u03b8(t) and a\nmeasurement sequence y(t) \u2208 Y \u2282 R^{d_y} if (Wang, Blei, and\nCunningham 2021; Wieland et al. 2021)\n$x(t_a) \\neq x(t_b) \\implies h(x(t_a)) \\neq h(x(t_b)).$\nAlthough it is extremely difficult to provide formal guar-\nantees, it makes sense that if for a given parameter \u03b8(t),\nh(x(t_a)) = h(x(t_b)), then obtaining an estimator for true\nstate x(t) becomes extremely challenging if not unfeasible.\nSince the proposed approach in this paper relies on estimat-\ning unmeasured states to learn model parameters, state iden-\ntifiability is preferred.\nTo enforce latent variable identifiability, it is sufficient to\nensure that the measurement function h is an injective func-\ntion (Wang, Blei, and Cunningham 2021) for all \u03b8. Never-\ntheless, constructing an injective measurement function re-\nquires that d_y \u2265 d_x (Wang, Blei, and Cunningham 2021),\nwhich is not feasible when dealing with the partial mea-\nsurement problem where d_y < d_x. Moreover, for models\nwith a high number of connected parameters, such as neu-\nral networks, enforcing identifiabilities can be challenging\n(Wieland et al. 2021) and latent identifiability as defined in\nDefinition 1 is not always guaranteed, especially when the\nnumber of measured states is less than the number of latent\nstates. Note that a latent variable may be identifiable in a\nmodel given one dataset but not another, and at one \u03b8 but not\nanother (Wang, Blei, and Cunningham 2021). However, one\ncould argue that one way to impose state identifiability is to\nre-parameterize the model (Wieland et al. 2021) and incor-\nporate prior knowledge regarding the relationship of states,\nfocusing on achieving the properties stated in Definition 1."}, {"title": "Discrete Generative model", "content": "In the continuous model presented in (1),a continuous-time\ndescription for the system states and parameters is assumed\neven though the measurements are recorded at discrete time\npoints. Moreover, a function a(x(t), \u03b8(t)) was defined to de-\nscribe the NODE governing the dynamics of the unmeasured\nstates, and u(t) described the external control inputs. In what\nfollows, we will omit to use a and u(t) for notation sim-\nplicity, where f(x(t), u(t), a(x(t), \u03b8(t))) will be denoted by\nf(x(t), \u03b8(t)). The discretization of states x(t) and parame-\nters \u03b8(t) can therefore be expressed as time integration of (1)\nusing Euler-Maruyama method (S\u00e4rkk\u00e4 and Svensson 2023)\nwith uniform time step \u2206t = t_i \u2212 t_{i\u22121}:\n$\\begin{aligned}\nx(t_i)&=x(t_{i-1}) + \\int_{t_{i-1}}^{t_i}f(x(t), \\theta(t))dt + \\int_{t_{i-1}}^{t_i}\\tilde{\\epsilon}(t)dt \\\\\nx(t_i)&=x(t_{i-1}) + \\Delta t f(x(t_{i-1}), \\theta(t_{i-1})) + \\Delta t\\tilde{\\epsilon}(t_{i-1})\n\\end{aligned}$$\\nHence we define the following equation:\n$\\bar{f}_\\theta(x(t_{i-1}), \\theta(t_{i-1})) = x(t_{i-1}) + \\Delta t f(x(t_{i-1}), \\theta(t_{i-1}))$\\nIn a similar fashion of states x(t), we discretize the parame-\nters \u03b8(t) and define the following equation:\n$\\theta(i)) = \\theta(t_{i-1}) + \\int_{t_{i-1}}^{t_i} \\upsilon(t) dt = \\theta(t_{i-1}) + v(t)$$\nBased on the continuous model presented in (1) and state\ndiscretization presented in (4, 5,6), we present the discrete\ntime evolution of the system states and parameters by the\nfollowing discrete generative model:\n$\\begin{aligned}\n\\theta(t_i) &= \\theta(t_{i-1}) + v(t) \\\\\nx(t_i) &= \\bar{f}_\\theta(x(t_{i-1}), \\theta(t_{i-1})) + \\epsilon(t) \\\\\ny(t_i) &= h(x(t_i)) + \\zeta(t) .\n\\end{aligned}$$\\nwhere v(t) \u223c N(0,Q_\u03b8), \u03f5(t) \u223c N(0,Q_x), and \u03b6(t) \u223c\nN(0, Ry), with Q\u03b8 = \u2206tQ\u03b8 and Qx = \u2206tQx."}, {"title": "4 Method", "content": "The proposed approach finds the model parameters \u03b8(t) of\nhidden Neural ordinary differential equation a(x, \u03b8) describ-\ning x_h (t) \u2208 x(t) and latent states x(t) of dynamical system\ngiven a dataset D\u2261 {u(t_0), y(t_0), ..., u(t_{N\u22121}), y(t_{N-1})}\nof discrete measurements and control inputs when x(t) is\npartially measured, that is $\\bar{x}_h(t)$ is unmeasured. We for-\nmulate the problem of estimating x(t) and \u03b8(t) as an op-\ntimization problem that is similar to (Humpherys, Redd, and"}, {"title": "B Vanishing gradients", "content": "Under the joint optimization scenario according to\n(Humpherys, Redd, and West 2012), we define the joint state\nz(t_{i-1}) = [\\theta(t_i), x(t_i)] and Z(t_i) = [\u0398_{i-1}, X_{i-1}]. By fol-\nlowing the same derivation as (Humpherys, Redd, and West\n2012) we get the following update equation :\n$\\bar{Z}_i = \\bar{Z}_{i|i-1} - (\\nabla^2\\mathcal{L}(Z))^{-1}\\nabla \\mathcal{L}(Z)$$\\nthe upper rows of (45) correspond to the parameter update\nwhich takes the following form (Wan and Nelson 2001):"}, {"title": "C Models and further experiments", "content": "Yeast Glycolysis Model\nYeast glycolysis Model is a metabolic network that explains\nthe process of breaking down glucose to extract energy in\nthe cells. This model has been tackled by similar works\nin the field (Kaheman, Kutz, and Brunton 2020), (Man-\ngan et al. 2016), and (Schmidt et al. 2011). It has seven\nstates: x = [x_1 x_2 x_3 x_4 x_5 x_6 x_7], and ODEs\nfor these states are given from Eq (53), (Mangan et al. 2016)."}]}