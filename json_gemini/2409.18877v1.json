{"title": "UniEmoX: Cross-modal Semantic-Guided Large-Scale Pretraining for Universal Scene Emotion Perception", "authors": ["Chuang Chen", "Xiao Sun", "Zhi Liu"], "abstract": "Visual emotion analysis holds significant research value in both computer vision and psychology. However, existing methods for visual emotion analysis suffer from limited generalizability due to the ambiguity of emotion perception and the diversity of data scenarios. To tackle this issue, we introduce UniEmoX, a cross-modal semantic-guided large-scale pretraining framework. Inspired by psychological research emphasizing the inseparability of the emotional exploration process from the interaction between individuals and their environment, UniEmoX integrates scene-centric and person-centric low-level image spatial structural information, aiming to derive more nuanced and discriminative emotional representations. By exploiting the similarity between paired and unpaired image-text samples, UniEmoX distills rich semantic knowledge from the CLIP model to enhance emotional embedding representations more effectively. To the best of our knowledge, this is the first large-scale pretraining framework that integrates psychological theories with contemporary contrastive learning and masked image modeling techniques for emotion analysis across diverse scenarios. Additionally, we develop a visual emotional dataset titled Emo8. Emo8 samples cover a range of domains, including cartoon, natural, realistic, science fiction and advertising cover styles, covering nearly all common emotional scenes. Comprehensive experiments conducted on six benchmark datasets across two downstream tasks validate the effectiveness of UniEmoX. The source code is available at https://github.com/chincharles/u-emo.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual emotion analysis plays a crucial role in various fields such as those of human-computer interaction [1], event monitoring [2] and recommender system [3]. Currently, most methods [4]\u2013[8] for visual emotion analysis utilize image classification frameworks and learn the mapping from pixel-level data to emotion labels using end-to-end neural networks. However, these trained models often demonstrate effectiveness only on specific datasets due to the ambiguity of emotional perception [9] and the diversity of data scenarios [10], thus lacking generalizability.\nIn recent years, large-scale vision models [11], [12] have achieved tremendous success, with their performance not yet significantly limited by the capacity of the model. Inspired by the paradigm of pre-training large-scale vision models, we can learn rich semantic and visual representations from large-scale datasets. Subsequently, fine-tuning on corresponding downstream datasets helps address the issue of lack of generalizability in visual emotion analysis methods. We ask: How to learn general emotional representations from large-scale complex scenes for effective application in various downstream emotion analysis tasks?\nIn the field of computer vision, pre-training paradigms are broadly classified into two primary types: supervised pre-training methods [13] and unsupervised pre-training methods [14]. Supervised pre-training methods involve training on large-scale labeled datasets, followed by fine-tuning for specific tasks. Self-supervised pretraining methods [15] are a common form of unsupervised learning that does not rely on labeled data. They leverage inherent supervisory signals from large-scale unlabeled data by designing auxiliary tasks (i.e., pre-tasks). Depending on their design, self-supervised pre-training methods are categorized into several types: data augmentation methods (e.g., rotation [16], colorization [17], Jigsaw puzzles [18]) that generate supervisory signals; contrastive learning methods, which maximize similarity among positive samples and minimize it among negative ones; and masked image modeling methods, which utilize co-occurrence"}, {"title": "II. RELATED WORK", "content": "In this section, we review existing works related to our UniEmoX from scene-centric visual emotion analysis, person-centric visual emotion analysis, and large-scale visual model pre-training."}, {"title": "A. Scene-centric Visual Emotion Analysis", "content": "Scene-centered visual emotion analysis (SVEA) aims to derive a comprehensive emotional summary by examining both the background and target information within an image. With advancements in deep learning techniques, researchers develope end-to-end classification frameworks to map im-ages to labels. Early works, such as the progressive CNN (PCNN) [20] and the multi-level deep representation network (MldrNet) [21], focus on extracting overall image features, often neglecting the importance of local regions. SOLVER [5], Stimuli-aware [4], and MDAN [22] utilize object detection and attention mechanisms to extract both global and multi-perspective local features, thereby improving classification performance. Although current methods achieve strong per-formance on specific tasks, their complex architectures hinder deployment, and ensuring robust generalization across diverse scenarios remains a challenge."}, {"title": "B. Person-centric Visual Emotion Analysis", "content": "Person-centered visual emotion analysis (PVEA) aims to determine the emotional state of specific targets by considering both background and target information. Current methods can be broadly classified into two categories. The first category pri-marily focuses on exploring various emotion-related features to design sophisticated network structures capable of effec-tively extracting and integrating these features. Early study [7] highlights the significance of contextual information, which leads to the development of the EmotiCon [8]. EmotiCon introduces types of contexts (gait, facial features, background, and social dynamics) from a psychological perspective [7], [23]\u2013[25] to enrich emotional features. Subsequent study [26] developes diverse mechanisms for extracting both foreground and background features, integrating multi-dimensional emo-tional information to enhance the model's expressive capabil-ities. The second category focuses on improving classification performance by addressing data background bias and devel-oping innovative multi-task learning strategies. To mitigate background bias, a post-hoc adjustment method known as the Context Causal Intervention Module (CCIM) [27] proposes to counteract the influence of background information on classifi-cation outcomes. Additionally, the multi-adaptive optimization (MAO) strategy [28] introduces an adaptive optimization tech-nique that dynamically adjusts the contribution of each task to the model parameters, thereby enhancing overall classification performance. However, despite the strong performance of these methods on specific datasets, they, similar to SVEA methods, face challenges in achieving widespread deployment in complex scenarios due to their intricate structures and limited generalization capabilities."}, {"title": "C. Large-Scale Visual Model Pre-training", "content": "In computer vision, the pre-training paradigm can be divided into two main types: supervised pre-training [13] and unsuper-vised pre-training. Supervised pre-training involves training on large-scale labeled datasets, followed by fine-tuning for specific tasks. This study focuses on self-supervised pre-training, a form of unsupervised pre-training that does not rely on labeled data. Instead, it extracts supervisory informa-tion from large-scale unlabeled data by designing auxiliary tasks, known as pretext tasks. Self-supervised pre-training methods can be categorized into several types based on different design philosophies: data augmentation-based meth-ods, contrastive learning-based methods, and masked image modeling-based methods. Data augmentation-based methods utilize inherent contextual relationships between samples, such as spatial structure and the maintenance of local and global consistency, to construct supervisory signals. For example, Researchers [16] train DNNs to learn image representations by recognizing random geometric transformations. Additionally, tasks like color prediction [17], [29] and jigsaw puzzles [18] have been employed as pre-training tasks for self-supervised learning. Contrastive learning-based methods aim to maximize the similarity between positive samples while minimizing the similarity between negative samples. Starting from the simple instance discrimination task, research has evolved to include MoCoV3 [30], DINO [31] and dBOT [32], up to the more popular CLIP [12], showcasing the superiority of contrastive learning. Masked image modeling-based methods leverage the co-occurrence relationships between image patches as supervisory signals. For example, BEiT [33] uses the token output of a pre-trained tokenizer as its target, while MAE [11] directly uses the raw pixels as targets. However, the primary aim of these works is to learn low-level visual patterns. Our goal is to learn abstract visual emotion representations by integrating popular pretraining techniques, such as masked image modeling and contrastive learning, with psychology-validated theories of emotion. We address the challenges of combining theoretical methods and establish a versatile and effective emotion representation framework, UniEmoX."}, {"title": "III. METHODOLOGY", "content": "In this section, we provide an overview of the proposed UniEmoX, followed by a detailed discussion of its network architecture. The development of UniEmoX is influenced by popular pretraining techniques, such as masked image modeling and contrastive learning, as well as psychological theories of emotion. These influences contribute to methods for human-environment (scene) image reconstruction, the distilla-tion of rich semantic knowledge from CLIP, and discriminative emotional representation learning. Finally, we discuss the opti-mization objectives during the fine-tuning stage of downstream tasks."}, {"title": "A. Overview", "content": "As shown in Fig. 4, the full scene information includes various visual scene maps, encompassing the human body and its surrounding environment. We define the full scene information map as $I \\in \\mathbb{R}^{N \\times H \\times W \\times C1}$ and the human position map as $B\\in \\mathbb{R}^{N \\times H \\times W \\times C1}$. UniEmoX is trained on the large-scale emotional dataset EmoSet [34], which provides multiple label attributes for each sample. As illustrated in Fig. 4 and Fig. 5, we utilize natural language logic to link multiple label attributes, thereby constructing text descriptions for the samples, which we refer to as T. As shown in Fig. 5, during the pre-training phase, UniEmoX integrates three types of input signals: text descriptions T, full-scene images $I \\in \\mathbb{R}^{N \\times H \\times W \\times C1}$, and human position maps $B\\in \\mathbb{R}^{N \\times H \\times W \\times C1}$. Motivated by tasks in masked image modeling and the psychological exploration of emotions related to inter-actions between humans and their environment, UniEmoX's backbone processes the full-scene image I and the human position map B as inputs. Employing an encoder-decoder architecture for image reconstruction tasks, it integrates learn-able fusion features focused on scenes and human bodies. This optimization of pixel-level reconstruction enhances the network's ability to effectively represent deep latent layers. Furthermore, UniEmoX integrates I and corresponding textual descriptions T. Employing knowledge distillation techniques, UniEmoX transfers semantic knowledge from CLIP, which enhances its capability to establish a universal emotion repre-sentation framework with improved generalization. During the fine-tuning stage for downstream tasks, the parameters from the backbone network are transferred to the ViT architecture, followed by the addition of a linear classification layer tailored to the number of task labels."}, {"title": "B. Network Architecture", "content": "Human-Environment (Scene) Image Reconstruction. Inspired by Masked Autoencoders (MAE), we enhance the model's capability to automatically capture and learn implicit structures and contextual information within images by re-constructing masked regions during pre-training phase. This approach improves the model's perception of both image details and global semantics.\nIn the reconstruction branch, our UniEmoX follows the processing procedure of MAE, performing a series of op-erations on full-scene images. These operations include Patch Embedding, Positional Encoding, Random Masking, adding a CLS-token, and applying both a Vision Transformer Encoder and Decoder. Patch Embedding: The full-scene image $I \\in \\mathbb{R}^{N \\times H \\times W \\times C1}$ is divided into $\\delta_{1} = \\frac{H}{patch2} \\frac{W}{12}$ non-overlapping patches. Each patch is flattened into a one-dimensional vector and mapped to the embedding dimension through a linear transformation. Consequently, the full-scene image I is transformed into a sequence $S \\in \\mathbb{R}^{N \\times d1 \\times C2}$. Random Masking: Image patches are masked randomly based on a masking ratio $\\mu$, leaving $1 - \\mu$ of the patches unmasked to form sequence $S1 \\in \\mathbb{R}^{N \\times (1-\\mu)\\delta_{1} \\times C2}$. Positional Encoding: To dis-tinguish between different patches in the sequence, each patch is added with a unlearnable positional encoding P.E. $\\in$ $\\mathbb{R}^{N \\times (1-\\mu)\\delta_{1} \\times C2}$. Consequently, the sequence is represented as $S2 = S1 + P.E.$. CLS-token: A learnable CLS-token$\\in \\mathbb{R}^{N \\times 1 \\times C2}$ is introduced at the beginning of sequence S2. This CLS-token's purpose is to integrate global and local image information, thereby forming the input sequence $S3 \\in \\mathbb{R}^{N \\times (1-\\mu)\\delta_{1}+1 \\times C2}$. Transformer Encoder: The input sequence is processed by a Vision Transformer (ViT) Encoder, comprising multiple stacked blocks. Each block includes an attention layer, a normalization layer, and a fully connected layer, aiming to extract rich feature representations, denoted as $S4 \\in \\mathbb{R}^{N \\times (1-\\mu)\\delta_{1}+1 \\times C2}$. Transformer Decoder: After processing S4 through a linear layer, one part extracts the CLS-token $\\alpha \\in \\mathbb{R}^{N \\times 1 \\times C3}$, obtaining the feature representation of the full-scene image. The other part is processed by the decoder to obtain the sequence representation $S' \\in \\mathbb{R}^{N \\times \\delta_{1} \\times C2}$ of the reconstructed image.\nWe utilize the design of the loss function from MAE to encourage the model to learn meaningful data representation by minimizing the difference between the reconstructed image sequence S' of the masked parts and the original image sequence S. The loss function $L_{1}$ is expressed as follows\n$L_{1} = \\frac{1}{M} \\sum_{i \\in M} |Si - S'i|^{2},$ (1)\nwhere $Si$ represents the original pixel value at position i, $S'_{i}$ represents the reconstructed pixel value at position i. $M = \\mu\\delta_{1}$ is the set of masked positions, | denotes the size of the set, and $||\\cdot||_{2}$ indicates the squared error."}, {"title": "Discriminative Emotional Representation Learning", "content": "According to psychological prior theories, the cognitive process of emotion is closely tied to the interaction between an indi-vidual and their environment. However, a natural semantic gap exists between these theories and the design of deep learning models. A significant challenge lies in designing effective fusion modules that integrate the feature representations of the full-scene image I and the human position map B to derive more nuanced and discriminative emotional representations. To address this challenge, we develop four methods, each targeting different degrees of this semantic gap.\nBefore introducing these four fusion methods, we first ex-plain the process of feature representation for human position map. The full-scene image I is processed by a YOLOv3 detector [35] and undergoes an image scaling transformation to obtain the human position map $B \\in \\mathbb{R}^{N \\times H \\times W \\times C1}$. The human position map B is subjected to similar operations as I, passing through the ViT encoder and a linear layer to extract the CLS-token, denoted as $\\beta \\in \\mathbb{R}^{N \\times 1 \\times C3}$. The feature representations of the body position map $\\beta$ and the full-scene image $\\alpha$ are reduced in dimensionality to $\\mathbb{R}^{N \\times C3}$ and $\\mathbb{R}^{N \\times C3}$, respectively.\nTo fuse features $\\alpha$ and $\\beta$ from human position map B and full-scene image I, we propose four methods to ad-dress varying levels of semantic gaps. To effectively tackle strong semantic gaps, we design fusion strategies $\\Gamma_{1}$, $\\Gamma_{2}$, and $\\Gamma_{3}$, which utilize weighted sum fusion, multi-head attention mechanisms with advanced activation functions, and complex feature mapping fusion, respectively. Additionally, as shown in the Psychological Prior section in the upper right corner of Fig. 5, we develop a simple feature addition strategy as a baseline for comparison to evaluate the effectiveness of the other fusion strategies in bridging semantic gaps.\nFor the fusion strategy $\\Gamma_{1}$, multiple sets of parameters $\\lambda_{i} \\in \\mathbb{R}^{1 \\times C3}$ and $\\mu_{i} \\in \\mathbb{R}^{1 \\times C3}$ are defined for $\\forall i \\in \\{1, 2, ..., \\kappa\\}$. In $\\Gamma_{1}$, the weighted sum of features $\\alpha$ and $\\beta$ from each set $\\lambda_{i}$ and $\\mu_{i}$ is combined and then subjected to layer normalization. To prevent issues with zero values during computation, a small bias term $\\epsilon \\in \\mathbb{R}^{C3}$ is added. The fusion strategy $\\Gamma_{1}$ is expressed as follows\n$\\Upsilon = LayerNorm(\\sum_{i=1}^{\\kappa} (\\lambda_{i}\\alpha + \\mu_{i}\\beta) + \\epsilon)$. (2)\nFor fusion strategy $\\Gamma_{2}$, multiple sets of linear transformation matrices $\\eta_{i} \\in \\mathbb{R}^{C3 \\times 1}$ and $\\xi_{i} \\in \\mathbb{R}^{C3 \\times 1}$ are defined for $\\forall i \\in \\{1, 2, ..., \\kappa\\}$. In $\\Gamma_{2}$, each set of $\\eta_{i}$ and $\\xi_{i}$ performs a linear transformation on features $\\alpha$ and $\\beta$, generating multiple attention scores. These attention scores are then used to compute the weighted sum of features $\\alpha$ and $\\beta$, which are subsequently combined. The Swish [36] is then applied to the result. To prevent the occurrence of zero values during computation, a small bias term $\\epsilon \\in \\mathbb{R}^{C3}$ is added. The fusion strategy $\\Gamma_{2}$ is expressed as follows\n$\\Upsilon = Swish(\\sum_{i=1}^{\\kappa} (softmax(\\eta_{i})\\alpha + softmax(\\xi_{i})\\beta) + \\epsilon)$. (3)\nFor fusion strategy $\\Gamma_{3}$, multiple sets of linear transformation matrices $\\delta_{i} \\in \\mathbb{R}^{C3 \\times C3}$ and $\\gamma_{i} \\in \\mathbb{R}^{C3 \\times C3}$ are defined for $\\forall i \\in \\{1, 2, ..., \\kappa\\}$. This strategy primarily utilizes multi-head attention applied to two input vectors $\\alpha$, $\\beta \\in \\mathbb{R}^{N \\times C3}$, combined with a gating mechanism, to produce a refined output for feature fusion. This process can be represented as"}, {"title": "follows", "content": "$\\triangle_{1} = softmax(\\alpha\\varphi_{i}) \\odot \\alpha + softmax(\\beta\\varphi_{i}) \\beta$, (4a)\n$\\triangle_{con} = concat(\\triangle_{1}, \\triangle_{2},......., \\triangle_{\\kappa})$, (4b)\n$\\triangle_{m} = ReLU (\\triangle_{con}W_{1} + b_{1}) W_{2} + b_{2}$, (4c)\n$\\triangle_{g} = \\triangle_{m} \\sigma (\\triangle_{m}W_{3} + b_{3})$, (4d)\n$\\Upsilon = ReLU (\\triangle_{m} \\ominus \\sigma (\\triangle_{g})) + \\epsilon$. (4e)\nwhere $W_{i} \\in \\mathbb{R}^{(C3 \\times H) \\times C3}$, $W_{2} \\in \\mathbb{R}^{C3 \\times C3}$, $b1,b2 \\in \\mathbb{R}^{C}, \\sigma$ represents the Sigmoid activation function."}, {"title": "Distilling Rich Semantic Knowledge from CLIP", "content": "CLIP has demonstrated the potential for cross-modal learning be-tween language and vision. Traditionally, language and vision processing are considered independent tasks. However, CLIP simultaneously processes both types of information, lever-aging their contrastive relationship to significantly enhance the model's understanding capabilities. Inspired by the suc-cess of CLIP, we adopt two approaches to develop a robust emotional pretraining framework: first, incorporating the suc-cessful vision-language contrastive pretraining paradigm, and second, efficiently distilling the visual semantic knowledge learned by CLIP into the emotional pretraining model. To achieve these goals, we implement the following specific measures:\nWe construct descriptive texts by naturally linking label at-tribute from the large-scale emotional dataset EmoSet, thereby creating numerous emotion image-text pairs. In incorporat-ing the vision-language contrastive pretraining paradigm, we do not optimize contrastive loss as CLIP do. Given that CLIP's training process utilized extensive data and hardware resources, its training performance is outstanding. To fully leverage CLIP's pretrained model, we design a similarity-based contrastive loss function to optimize the training pro-cess. Specifically, we input descriptive text T and full-scene image I into CLIP's text encoder and image encoder, re-spectively, obtaining text features $\\Lambda \\in \\mathbb{R}^{N \\times C3}$ and visual features $\\Omega \\in \\mathbb{R}^{N \\times C3}$. We first apply L2 normalization to the CLIP-processed text features A, visual features $\\Omega$, and discriminative visual emotional representation $\\Upsilon$. This process can be represented as follows\n$X' = \\frac{X_{c}}{\\sqrt{\\sum_{c=1}^{C3}X_{c}^{2}}},$ (5)\nwhere $X \\in \\{\\Lambda, \\Omega, \\Upsilon\\}$, $\\Lambda_{c}$, $\\Omega_{c}$, $\\Upsilon_{c}$ is an N \u00d7 C3 matrix, with each row vector $\\Lambda_{n}$, $\\Omega_{n}$, $\\Upsilon_{n}$ representing the n-th row of matrix A, $\\Omega$, $\\Upsilon$, respectively.\nThen, we calculate the sample correlation matrices between the visual emotional representation $\\Upsilon'$ and the CLIP-processed text features $\\Lambda'$, as well as between the CLIP-processed visual semantic representation $\\Omega'$ and the CLIP-processed text features $\\Lambda'$. Both sample correlation matrices are subjected to L2 normalization, resulting in matrices A $\\in \\mathbb{R}^{N \\times N}$ and C$\\in \\mathbb{R}^{N \\times N}$. This process can be represented as follows\n$A = \\frac{(\\Upsilon'\\Lambda'^{T})}{\\sqrt{\\||(\\Upsilon'\\Lambda'^{T})||_{2}^{2}}},$ (6)\n$C = \\frac{(\\Omega'\\Lambda'^{T})}{\\sqrt{\\||(\\Omega'\\Lambda'^{T})||_{2}^{2}}}$. (6)\nFinally, a similarity-based contrastive loss is employed for training to align our model's vision-language contrastive simi-larity with that of CLIP. The ultimate goal is to transfer CLIP's vision-language knowledge to our model. The similarity-based contrastive loss function L2 is expressed as follows\n$L_{2} = 1-\\frac{1}{N} \\sum_{n=1}^{N} A_{n} C_{n}^{T}$ (7)\nDuring the process of distilling the visual semantic knowl-edge learned by CLIP into the emotional pretraining model, we employ a visual feature similarity loss to transfer CLIP's visual encoding capabilities to our model.\n$L_{3} = 1-\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\Upsilon_{n} \\Omega_{n}^{T}}{\\sqrt{\\||\\Upsilon|| \\|\\Omega||}}$, (8)\nDuring the training phase, the overall optimization objective is defined as $L_{t} = L_{1} + L_{2} + L_{3}$."}, {"title": "C. The Optimization Objectives in the Fine-Tuning Stage", "content": "During the fine-tuning phase of downstream tasks, we employ mixup [37] and soft targets [38] to improve the model's performance with small sample sizes, imbalanced distributions, or noisy data. Assuming the predicted value of the sample is x \\in $\\mathbb{R}^{N \\times d}$ and the target label is y \\in $\\mathbb{R}^{d}$. The soft target cross entropy loss $L_{f}$ can be expressed as follows\n$L_{f} = \\frac{1}{N}\\sum_{n=1}^{N} (-\\mathbb{X}_{n,y_{n}} + log(\\sum_{i=1}^{d}exp(\\mathbb{X}_{n,i})))$. (9)"}, {"title": "IV. EM08 DATASET", "content": "To construct the Emo8 dataset, we first select the widely-used Mikels model [39], which categorizes a broad range of emotions into eight labels: amusement, awe, contentment, excitement, anger, disgust, fear, and sadness. The first four are positive emotions, while the latter four are negative. As shown in Table I, we retrieve nine additional synonyms for each of these labels from a dictionary. We then input these 80 key-words into the Bing search engine\u00b9, obtaining approximately 13,200 raw images. At this stage, we have 13,200 raw images corresponding to the eight emotion labels. Next, we conduct a preliminary screening of the images, mainly removing those that severely mismatch the label meanings. Subsequently, we employ two individuals to meticulously validate the emotional content of the filtered images. Our criteria are stringent: an image is retained only if both individuals agree that it evoked the corresponding emotion. Ultimately, we obtain the Emo8 dataset, comprising 8,930 images across the eight emotion labels. Due to space constraints, samples of Emo8 images are provided in the attached images."}, {"title": "B. Dataset Statistics", "content": "Compared to other emotion analysis datasets, the Emo8 dataset, though not the largest in terms of image count, boasts diverse sample sources, the most comprehensive emotion annotations, and delicate emotional expressions. As shown in Table II, the Emo8 dataset contains samples in cartoon, natural, realistic, science fiction, and advertising cover styles, covering nearly all common emotional scenes. This ensures the model's generalizability across various contexts during training and validation. Additionally, our dataset's adoption of the Mikels model provides a significant advantage in the number of annotations compared to datasets using models with fewer annotations, such as Ekman's [40]. Finally, the Emo8 dataset offers more delicate emotional expressions due to a three-stage filtering process: image retrieval via the Bing search engine, initial manual screening, and meticulous manual validation. Additionally, we conduct an analysis of the segmentation of the Emo8 dataset. Ensuring a consistent feature distribution between the validation set and the training set is crucial for a good dataset segmentation principle. To validate the quality of our dataset segmentation, we perform stratified random splits at a ratio of 8:1:1, generating multiple segmentation schemes. We then calculate the color histograms of the training, validation, and test sets for each scheme and perform dimensionality reduction using PCA\u00b3. As shown in Fig. 6, we select the segmentation scheme with the most consistent distribution between the validation set and the training set as the final segmentation scheme."}, {"title": "V. EXPERIMENTS", "content": "By linking the emotional label attributes of the EmoSet [34] dataset through natural language logic, we construct 118,000 emotion text-image pairs for pretraining UniEmoX. To evalu-ate the effectiveness and generalization ability of UniEmoX, we conduct comparison experiments, ablation studies, and visual analyses on five public benchmark datasets: CAER-S [46], HECO [44], UBE [10], Emotion6 [43] and FI [45] as well as our Emo8 dataset. Due to space constraints, descriptions of downstream tasks and their corresponding datasets are provided in Appendix Section I."}, {"title": "B. Evaluation Metrics and Implementation Details", "content": "In this work, we adopt Accuracy as the evaluation metric. Accuracy denotes the ratio of correctly classified samples by the model, calculated as follows\n$Accuracy = \\frac{TP+TN}{TD},$ (10)\nwhere TP, TN, TD are the number of true positives, true negatives, and total data, respectively.\nDue to space constraints, detailed information on experi-mental parameter configurations and model efficiency analyses is provided in Appendix Sections II and III."}, {"title": "C. Comparisons with Previous Results", "content": "We compare our UniEmoX with twenty existing methods, including four traditional backbone networks based on su-"}, {"title": "D. Ablation Study", "content": "To verify the effectiveness of the various component com-binations, we conduct ablation experiments. The specific steps are as follows: we conduct 150 epochs of self-supervised pre-training, experimenting with various combinations of compo-nents. (Note that the results from 400 epochs of self-supervised pre-training, where all components are combined, are referred to as S6. The results from 200 epochs of self-supervised pre-training, following the loading of the pre-trained model with all components combined, are referred to as S7.) and then fine-tune the pretrained models on downstream datasets. The experimental results are shown in TABLE IV. Based on these results, we can draw three conclusions:\n1) By comparing S\u2081 with S4, S2 with S4, and S3 with S4, we respectively validate the effectiveness of psycholog-ical theory prior knowledge, visual semantic knowledge distilled loss function $L_{3}$ from CLIP, and reconstruction loss function $L_{1}$ in some datasets;\n2) By comparing S4 with S5, and S4 with S6, we confirm the effectiveness of the contrastive similarity loss func-tion $L_{2}$ across all datasets, which significantly improves recognition accuracy in downstream datasets;\n3) By comparing S5 with S6, we find that increasing the number of self-supervised pretraining epochs can enhance performance in downstream datasets. By com-paring S7 with S6, we find that initializing with a pre-trained model significantly enhances the performance of downstream datasets in environments with limited experimental resources.\nTo assess the impact of masking ratios and various fusion strategies on downstream emotion analysis tasks, we conduct ablation experiments. The specific steps are as follows: we perform 150 epochs of pre-training with the model using different masking ratios and fusion strategies, followed by fine-tuning the pre-trained model on downstream datasets. The experimental results are shown in TABLE V and TABLE VI. Based on these results, we can draw one conclusion:\n1) Although MAE research suggests that models pre-trained with a 0.75 masking ratio perform better during fine-tuning, we find that models pre-trained with a 0.5 masking ratio learn emotional representations more effectively in most downstream datasets, including UBE, FI, and Emo8. Conversely, models pre-trained with a"}, {"title": "E. Visualization", "content": "To verify the effectiveness of our UniEmoX more intu-itively, we perform a visual analysis of the regions the model focuses on in input images by generating heatmaps using Grad-CAM4. The specific steps are as follows: We download pretrained models for three methods (DINO, MAE, MoCo v3) as well as UniEmoX. We transfer the parameters of the pretrained models to the corresponding ViT networks and add appropriate linear layers based on the number of classification labels. We set the last block of ViT as the target layer and ran Grad-CAM to generate the respective feature maps. To ensure fairness and generality, all original images are sourced from the EmoSet test set, and we randomly select multiple image samples from different labels. The visualization results are shown in Fig. 8. Based on these results, we can draw two conclusions:\n1) Compared to other methods, UniEmoX focuses on more salient regions.\n2) Additionally, in terms of judging the emotions elicited by each sample, the salient regions identified by UniEmoX align more closely with human judgment.\nGiven that our UniEmoX is based on a masked image modeling task, we conduct a visual analysis to verify its image reconstruction capabilities. The specific steps are as follows: We primarily compare our method with the MAE. We use the pretrained MAE model provided by the original authors and our method's pretrained models obtained at different pretrain-ing stages. To ensure fairness and generality, all test images are sourced from the EmoSet test set. Using the visualization scripts provided by MAE, we visualize the reconstruction capabilities of all pretrained models on the test samples, as shown in Fig. 9. Based on these results, we can draw two conclusions:\n1) The reconstruction performance of our pretrained model appears inferior to that of MAE. This is because MAE's training process utilizes a larger dataset and focused on optimizing image reconstruction capability. In contrast, our UniEmoX uses a smaller dataset and has multiple"}, {"title": "F. Comparative Analysis of Emotion Understanding Capabilities in Multimodal Large Models", "content": "Many multimodal large models, such as GPT-40, ERNIE Bot3.5, iFlytek Spark, ChatGLM, Qwen2.5, Baichuan4 and SenseChat are currently deployed in practical applications. These models accurately interpret user intentions in com-plex conversational contexts by integrating data from various modalities. To assess their performance in emotional seman-tic understanding, we design a straightforward dialogue sce-nario. We evaluate their accuracy in emotion recognition and depth of emotional semantic understanding through image-text question-and-answer interactions. Specifically, we ask: \u201cWhich emotion does this image most strongly evoke? Please select the most intense emotional response from the following list and explain your choice: ['sadness', 'fear', 'excitement', 'disgust', 'contentment', 'awe', 'anger', 'amusement'].\" We randomly select images from the FI test set, input them into the models, and record the responses. To ensure result generalizability, we repeat the test with the UBE test set. Due to space constraints, we present analysis results for only two examples per model. Detailed results for additional examples are provided in Ap-pendix Section IV. By analyzing the performance of these multimodal models alongside our UniEmoX, we can draw several key conclusions:\n1) Multimodal large models demonstrate exceptional ac-curacy in emotion recognition and depth of emotional semantic understanding on most test samples. However, they still have limitations. For instance, some models demonstrate limited depth in emotional understanding. As shown in the left image of Fig. 10 and TABLE VII, most viewers might initially interpret the character's facial expression as indicating happiness. Consequently, models such as ChatGLM, Baichuan, and SenseChat classify it as 'Joy'. However, this interpretation is su-perficial. A more nuanced analysis reveals that the character's upward gaze suggests a reaction to some-thing unexpected above. Therefore, the most accurate emotional label for this image is 'Surprise'. Notably, ERNIE Bot, iFlytek Spark, Qwen, and GPT-40 correctly identified this emotion.\n2) As shown in the right image of Fig. 10 and TABLE VIII,\""}, {"title": "VI. CONCLUSION", "content": "We introduce UniEmoX, a large-scale pretraining frame-work guided by cross-modal semantics, designed to address the poor generalization of existing visual emotion analysis methods, which is caused by the ambiguity of emotional perception and the diversity of data scenes. Inspired by psychological research emphasizing the inseparability of the emotional exploration process from the interaction between individuals and their environment, UniEmoX integrates scene-centric and person-centric low-level image spatial structural information, aiming to derive higher-level latent representa-tions. By exploiting the similarity between paired and unpaired image-text samples, UniEmoX distills rich semantic knowl-edge from the CLIP model to enhance emotional embedding representations in a more effective manner. We have developed a visual emotional dataset titled Emo8. Emo8 samples cover a range of domains, including advertising images, natural scenes, portraits, and cartoons. Comprehensive experiments conducted on six benchmark datasets across two downstream tasks validate the effectiveness of UniEmoX."}, {"title": "VII. LIMITATION"}]}