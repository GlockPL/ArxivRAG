{"title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding", "authors": ["Shijing Hu", "Jingyang Li", "Xingyu Xie", "Zhihui Lu", "Kim-Chuan Toh", "Pan Zhou"], "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7% and a speedup ratio exceeding 8%, outperforming current SoTAs as shown in Fig. 1 (a) and (b).", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs), such as GPT-4 (Achiam et al., 2023) and LLaMAs (Touvron et al., 2023a;b), have demonstrated extraordinary capabilities and potential in various fields, such as dialogue (Zheng et al., 2023), code generation (Chen et al., 2021), and mathematical reasoning (Cobbe et al., 2021). However, autoregressive decoding, the standard approach for LLMs, generates tokens sequentially, with each token requiring a full forward pass through the entire model. Given the large size of LLMs, this process is both computationally expensive and time-consuming, limiting their practicality in time-sensitive applications.\nTo address the high computational cost of generation, speculative decoding (Leviathan et al., 2023; Chen et al., 2023a) has become widely adopted and demonstrated significant improvements in inference speed. Its core mechanism involves generating draft tokens with an efficient draft model, verifying them in parallel using the target LLM, and selecting tokens that match the target LLM's output distribution. This process allows multiple-token generation in a single forward pass of the target LLM, substantially reducing latency.\nHowever, the efficiency of speculative decoding depends critically on achieving a high acceptance rate for draft tokens, while also minimizing the computational cost of generating them. Recent mixing-based methods, such as EAGLE (Li et al., 2024a), EAGLE2 (Li et al., 2024b), and Medusa (Cai et al., 2024), address this by utilizing shallow-layer hidden states of the target LLM to guide the draft model's token predictions. These methods improve computational efficiency and align draft and target models more closely by using target features to initialize the draft model.\nDespite these advances, existing mixing-based methods like EAGLE series (Li et al., 2024a;b) face a fundamental limitation: misalignment between the training and decoding processes. During training, the draft model uses features from the target model and ground-truth tokens from training data, whereas in decoding, it relies on its own generated features and previously generated draft tokens. This discrepancy introduces two key issues: (1) feature misalignment, where the features generated by the draft model during decoding diverge from those features used during training, and (2) token misalignment, where ground-truth tokens are replaced by draft tokens, often compounding errors over multiple steps. These misalignments, akin to exposure bias (Bengio et al., 2015; Schmidt, 2019), significantly degrade the acceptance rate of draft tokens and thus impair the overall speedup performance.\nEfforts to address feature misalignment like HASS (Zhang et al., 2024) use the draft model's features to replace the target model's features during training. While this aligns training with decoding, it neglects token misalignment, which is particularly problematic in autoregressive decoding. Errors from earlier decoding steps propagate and amplify, further exacerbating token misalignment. For instance, as shown in Fig. 1 (c), EAGLE2 suffers from a token misalignment rate of 30% during training, resulting in suboptimal acceptance lengths and limiting its effectiveness. Similarly, while mitigating feature misalignment, HASS sees token misalignment escalate to 35% in later training steps, rendering it ineffective for deeper multi-forward harmonized training, e.g., forward number \u2265 3, as shown in Fig. 1 (b)."}, {"title": "Contributions.", "content": "To solve token misalignment, we propose a novel GRIFFIN framework that contains a token-alignable training strategy and a token-alignable draft model.\nFirstly, GRIFFIN develops a token-alignable training strategy to mitigate the adverse effects of token misalignment. It employs a dynamic loss masking mechanism during training that focuses only on aligned tokens. Tokens are considered aligned if their corresponding ground-truth tokens appear in the top-k predictions of the draft model. This approach excludes highly misaligned tokens from the loss computation during multiple forward passes, preventing their negative influence on model optimization. Additionally, the top-k criterion ensures consistency between training and decoding, as draft trees in the decoding phase are also constructed based on top-k predictions rather than requiring exact matches to the highest-probability token.\nSecondly, to reduce token misalignment, GRIFFIN designs a token-alignable draft model by incorporating the architectural innovation of Token-Guided Fusion (TGF) into draft model in EAGLE (Li et al., 2024a). TGF performs a two-step fusion to refine feature representations and mitigate inconsistencies between the draft and target models. By incorporating input tokens twice\u2014initially with features and later to refine them-TGF ensures that the draft model produces features more closely aligned with the target model, reducing feature and token misalignment.\nTogether, these two innovations allow GRIFFIN to achieve complementary improvements. The token-alignable draft model reduces token misalignment, enabling more aligned tokens to contribute effectively to training. Meanwhile, the token-alignable training strategy ensures that the draft model operates on inputs that are closely aligned with the ground truth, enhancing the corrective power of TGF.\nExtensive experimental results demonstrate GRIFFIN's superior performance over state-of-the-arts (SoTAs) across diverse tasks, including dialogue (MT-Bench (Zheng et al., 2023)), code generation (HumanEval (Chen et al., 2021)), and mathematical reasoning (GSM8K (Cobbe et al., 2021)). For example, Fig. 1 (a) and (b) show that on LLaMA2-7/13B, LLaMA3-8B, and Vicuna-7B, GRIFFIN achieves an average acceptance length improvement of 18% over EAGLE2 and 7% over HASS, while delivering a speedup ratio of over 20% over EAGLE2 and 8% over HASS."}, {"title": "2. Related Work", "content": "Speculative decoding (Sun et al., 2024; Miao et al., 2024; Chen et al., 2023b; Kim et al., 2024; Liu et al., 2023) accelerates LLM inference by dividing each decoding step into a draft stage and a verification stage. Existing methods differ primarily in their draft model architectures or strategies, each addressing specific challenges in speculative decoding.\nSeveral approaches focus on retrieving candidate predictions from similar contexts, such as PLD (Saxena, 2023), Lookahead (Fu et al., 2024), and CLLMs (Kou et al., 2024), which rely on prompt-based context retrieval. However, their dependence on contextual similarity limits generalization to tasks with insufficient or unavailable context. Tree-based verification mechanisms, such as Sequoia (Chen et al., 2024) and SpecExec (Svirschevski et al., 2024), improve verification via hierarchical structures but introduce computational overhead, making them less suitable for latency-critical tasks. Other works leverage databases or prior outputs for drafting, such as REST (He et al., 2023) and Ouroboros (Zhao et al., 2024), which reuse previous outputs to improve draft coherence. However, these methods are constrained by the quality and availability of external resources. Chimera (Zeng et al., 2024) and Glide (Du et al., 2024) integrate the target model into the draft model to improve token quality, but this increases computational costs.\nLightweight draft models have also been explored to im-"}, {"title": "3. Motivation: Token Misalignment", "content": "Speculative decoding (Leviathan et al., 2023; Chen et al., 2023a) accelerates text generation through a \"draft-and-verify\" mechanism. It uses two language models: a smaller efficient draft model M to generate draft tokens via multiple forward passes per cycle, and a target model T to verify and accept tokens based on their alignment with the target distribution in a single forward pass per cycle.\nUnlike standard speculative decoding which autoregressively predicts tokens, EAGLE (Li et al., 2024a) shifts autoregression to the feature level. Instead of generating tokens directly, it predicts hidden state features from the final layer of target model T before the LM head H. Let xt and xt represent the t-th ground-truth token from the training dataset and the t-th draft token generated by M, respectively. Their corresponding hidden state features from T and M are Ft and Ft.\nAs shown in Fig. 2, during training, at time step t, the target model produces features Ft for ground truth xt, allowing the draft model to use Ft and xt to generate the next features Ft+1 and tokens Xt+1. However, during decoding, the draft-"}, {"title": "4. GRIFFIN", "content": "To address the token misalignment challenge in Sec. 3, we propose GRIFFIN, a novel framework designed to mitigate token misalignment through two key components: token-alignable training elaborated in Sec. 4.1 and a token-alignable draft model elaborated in Sec. 4.2."}, {"title": "4.1. Token-Alignable Training", "content": "To resolve the token misalignment issue, we propose a token-alienable training strategy. More specifically, our training consists of multiple steps, with the number of forward passes performed by the draft model increasing at each step. That is, at training step k, the draft model performs k forward passes to generate k subsequent features and their corresponding draft tokens. The losses from these k forward passes are averaged, and a single backward pass is performed to update the parameters of the draft model. The primary benefit of performing multiple forward passes is that the draft model can utilize the tokens and features generated by itself during previous forward passes, rather than relying on the ground-truth tokens and features generated by the target model. This approach effectively aligns the training process with the decoding phase, as in training phase, the draft model could simulate the similar input conditions encountered during decoding. Then we elaborate on the first training step and its subsequent training step for token alignment.\nFirst Forward Pass: Same as the conventional autoregressive generation, draft model M needs to predict the draft tokens which are then fed into target model T for verification and acceptance. Specifically, at the t-th time step, we need to use draft model M and the LM head H in the target model to predict the t-th feature embedding Ft and the t-th draft token xt:\nFt = M(x1:t-1, F1:t\u22121), xt = H(Ft). (1)\nwhere x1:t\u22121 denotes the token sequence {x}=1 from training dataset and F1:t-1 are the feature embedding sequence {F}= generated by target model T.\nThen we resolve the token misalignment caused by the inconsistency between training and decoding phases as introduced in Sec. 3. Specifically, during decoding, EAGLE employs a tree-structured draft, where nodes at the same level of the draft tree represent different top-k draft tokens generated during the same forward pass. This tree structure enables alternative branches of top-k draft tokens to be explored if the top-1 draft token is rejected. For consistency, during training, a draft token Xt is considered aligned if its ground-truth token xt appears within the top-k predictions of the draft model, rather than relying solely on the top-1 prediction. This ensures that tokens beyond the top-1 prediction also contribute to the training process, aligning with the decoding phase, where top-k tokens can be explored if the top-1 draft token is rejected. Accordingly, we introduce a mask m\u2081 = 1 to mark this consistency. For these misaligned draft tokens like xs, their masks like m = 0. This ensures the alignment criterion during both training and decoding phases. In this way, we can use the following training loss to train the draft model M:\n$\\mathcal{L}\\_M^{(1)} = \\frac{1}{\\sum\\_{t=1}^{M} m\\_t}\\sum\\_{t=1}^{M} m\\_t l(x\\_t, \\hat{x}\\_t, F\\_t, \\hat{F}\\_t),$(2)\nwhere the loss function l(xt, xt, Ft, Ft) combines two components: a feature-level loss, computed as the l\u2081 loss between Ft and Ft, and a token-level loss, computed as the cross-entropy loss between xt and xt.\nk-th Forward Pass (k \u2265 2). Draft model M would predict k draft tokens at the k-th forward pass. Similarly, we also align the training criterion with the decoding phase. Like the first forward pass, at the t timestep of the k-th forward pass, during training, a draft token \u2611t is considered aligned if its ground-truth token xt appears in the top-k predictions of the draft model, and its mask is set m\u2081 = 1. Since the current draft token x\u2081 is decided by previous predicted draft tokens xt-k:t-1 predicted in earlier (k \u2013 1) forward passes, then if any draft token in xt-k:t-1 is misalignment, the draft token xt would likely be misalignment. So we further adjust the current mask me with the alignment masks mt-k+1:t-1 of draft tokens xt-k+1:t-1:\n$\\begin{equation}\nm\\_t = \\prod\\_{i=t-k+1}^{t-1} m\\_i.\n\\end{equation}$ (3)\nNext, to further ensure alignment between training and decoding, we replace the features Ft-k+1:t generated by the target model with the features Ft-k+1:t generated by the draft model in the previous (k \u2013 1) forward passes. Then the draft model M and the LM head H are used to generate the feature Ft and the draft token xt:\n$\\hat{F}\\_t = M(x\\_{1:t}, \\hat{F}\\_{1:t-k}, F\\_{t-k+1:t}), \\qquad \\hat{x}\\_t = H(\\hat{F}\\_t).$ (4)"}, {"title": "4.2. Token-Alignable Draft Model", "content": "To enhance the accuracy of draft tokens and mitigate token misalignment, we introduce a token-alignable draft model that systematically addresses the feature inconsistency issues in existing draft models by introducing Token-Guided Fusion (TGF). As shown in Fig. 3, we introduce TGF module before the auto-regressive layer to fuse the input features Ft and tokens xt. After processing through the auto-regressive layer, following prior work (Gui et al., 2024), we also adopt a dual-head design (TEH) to decouple the conflicting objectives of token prediction and feature generation within the draft model. Specifically, TEH generates a predict feature F1 for next-token prediction and a regress feature F1 to serve as input for subsequent forward passes.\nNow we introduce TGF module more. It is designed to improve feature consistency. Feature representations in the draft model, being continuous and derived from a high-dimensional space, often fail to fully align the ones in the target model. This misalignment persists even with extensive training because the feature-level loss cannot be reduced to zero. As a result, the draft model struggles to generate features that accurately reflect the target model's feature distribution, leading to the misalignment.\nTo address this, TGF enhances feature fusion by prioritizing token embeddings, ensuring that the draft model generates feature representations that better reflect the target model's feature distribution. As illustrated in Fig. 4, TGF follows a three-step process. 1) Initial Fusion. The input feature and token embedding (both of embedding dimension d) are concatenated along the embedding dimension and passed"}, {"title": "5. Experiments", "content": "In this way, we can also use (xt, xt, Ft, Ft) to build the loss $\\mathcal{L}\\_M^{(k)}$ in Eqn. (2).\nTraining loss. We use the above method to construct the training loss {$\\mathcal{L}\\_M^{(k)}$}$_{k=1}^{K}$ for all K forward passes, and average all training losses to train the draft model M:\n$\\min\\_{M} \\frac{1}{K} \\sum\\_{k=1}^{K} \\mathcal{L}\\_M^{(k)}.$(5)\nThis means that we forward K passes and then backward one time to train the draft model M. In all experiments, we set K = 3 which works well in practice as shown in Sec. 5."}, {"title": "5.1. Experimental Setup", "content": "Target LLMs. We evaluate our approach on a diverse set of LLMs, including LLaMA2-Chat 7B/13B, LLaMA3-Instruct 8B (Touvron et al., 2023b), and Vicuna-1.5 7B (Cheng-hao Fan & Tian, 2023). To ensure consistency and fairness, all inference processes are conducted on a single NVIDIA A100 GPU.\nTasks. We conduct evaluations across three representative generation tasks: multi-turn conversation, code generation, and mathematical reasoning. For these tasks, we utilize the MT-Bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), and GSM8K (Cobbe et al., 2021) datasets, respectively. To ensure consistency and comparability with prior works such as DistillSpec (Zhou et al., 2023) and EAGLE (Li et al., 2024a), we set the batch size to 1, and set the temperature T \u2208 {0, 1} for all experiments.\nMetrics. GRIFFIN is a lightweight acceleration method that neither fine-tunes the target LLMs' weights during training nor relaxes the acceptance conditions during decoding. As a result, the generation results remain unchanged, eliminating the need for additional quality evaluation. To measure"}, {"title": "5.2. Comparison with SoTAS", "content": "We present the acceptance lengths and speedup ratio of various methods across three datasets in Table 1. GRIFFIN consistently achieves the highest acceptance length and speedup ratio across all datasets and LLMs tested. Each GRIFFIN drafting-verification cycle generates approximately 5\u20136 tokens, significantly exceeding other methods. This is roughly three times the amount of standard speculative sampling and 1.5 times the amount of EAGLE."}, {"title": "5.3. Ablation Study", "content": "Components In this experiment, we conduct the ablation study using LLaMA2-Chat 7B. We first conduct an ablation study to analyze the impact of token-alignable training (TAT) and the token-alignable draft Model (TAD) in GRIFFIN. As shown in Table 2, removing either component leads to a substantial reduction in acceptance length. Specifically, removing TAT results in a noticeable drop in performance across all benchmarks, with a mean acceptance length reduction of 0.26 at T = 0 and 0.28 at T = 1. This highlights the importance of TAT in aligning draft tokens during the training process. Similarly, removing TAD also results in significant performance degradation, with a mean acceptance length reduction of 0.19 at T = 0 and 0.23 at T = 1. This demonstrates the critical role of TAD in mitigating the misaligned token rate. Finally, the results show that removing both components (w/o both) leads to the most severe performance drop, with mean acceptance lengths reduced by 0.83 at T = 0 and 0.83 at T = 1 compared to the full GRIFFIN model. This suggests that TAT and TAD work synergistically to maximize the acceptance length, with TAT aligning draft tokens during training process and TAD mitigating the misaligned token rate. The combination of these two components ensures that GRIFFIN achieves SOTA performance across diverse benchmarks.\nHyper-param. for TAT We first analyze the effect of the hyper-parameter k, which determines the number of top-k tokens to align. As shown in Table 3, aligning the top-k tokens (for k ranging from 1 to 10) consistently improves the acceptance length compared to not aligning tokens. Notably, aligning only the top-1 token is less effective, as it neglects many other tokens that could benefit from alignment. The acceptance length achieves its peak when k = 3, suggesting that aligning a small but sufficient number of tokens provides the optimal trade-off between alignment and generalization.\nWe further analyze the effect of increasing the number of training steps N in TAT. As shown in Table 4, increasing the training steps steadily improves GRIFFIN's acceptance length during the first 5 steps. In contrast to HASS, which plateaus after step 3 (as shown in Fig. 1b), GRIFFIN continues to improve due to its token alignment mechanism. However, as the number of aligned tokens decreases with each additional training step, the improvements become less pronounced at steps 4 and 5. To ensure a fair comparison with HASS, we choose the number of training steps N = 3 in our experiments."}, {"title": "Components of TAD", "content": "We analyze the effectiveness of TGF and TEH in reducing the misaligned token rate within TAD. As shown in Fig. 5, the misaligned token rate increases significantly when either TGF or TEH is removed. Furthermore, when both TGF and TEH are removed, the misaligned token rate increases even more dramatically, indicating that these components work synergistically to mitigate misaligned token rate for draft model.\nWe evaluate whether the effectiveness of using tokens to correct inconsistent features in Token-Guided Fusion (TGF) stems from their inherent utility or merely from an increase in model parameters. To investigate this, we conduct an ablation study (Table 5) by replacing the token embeddings used in the secondary fusion step of TGF with either raw features or the initial fused features, while keeping the model architecture and training process unchanged. When the token embeddings are replaced with raw features, the acceptance length decreases by 0.83, indicating that relying solely on features is insufficient to correct inconsistent features in TGF. This demonstrates the critical role of token embeddings in addressing feature inconsistencies. When the token embeddings are replaced with initial fused features, the acceptance length is higher compared to using raw features. This is because the initial fused features incorporate some token information, which partially helps in correcting inconsistent features. However, the acceptance length still decreases by 0.26 compared to GRIFFIN, highlighting that the full effectiveness of TGF relies on the explicit use of token embeddings in the secondary fusion step. These results demonstrate the critical role of TGF in addressing feature inconsistencies and confirm that the improvements in TGF are not merely attributable to parameter scaling but stem from the inherent utility of token embeddings in correcting inconsistent features."}, {"title": "6. Conclusion", "content": "In this paper, we introduce GRIFFIN, a token-alignable speculative decoding framework. Previous works have largely overlooked the issue of token misalignment between training and decoding. To address this, GRIFFIN incorporates a token-alignable training strategy to excludes misaligned tokens from the loss computation. Furthermore, GRIFFIN adopts a token-alignable draft model, which significantly reduces the token misalignment rate. We conducted extensive evaluations across various LLMs and datasets, comparing GRIFFIN against several SOTA speculative decoding methods. In all experiments, GRIFFIN consistently achieved the highest speedup ratios and acceptance length, demonstrating its effectiveness and efficiency.\nLimitations. GRIFFIN adopts a multi-step training process for token-alignable training, which incurs additional training overhead compared to EAGLE. However, since the draft model is trained only once, real-world applications prioritize decoding efficiency over training overhead, as inference is the primary bottleneck. GRIFFIN improves the speedup ratio by over 20% compared to EAGLE2, making the extra training cost a worthwhile trade-off for the significant inference acceleration it delivers. Furthermore, GRIFFIN's overall training overhead remains comparable to that of HASS. Under the same training cost, GRIFFIN achieves an over 8% improvement in speedup ratio compared to HASS, further highlighting its effectiveness."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of LLM acceleration. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}