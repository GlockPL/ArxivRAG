{"title": "Intelligent Artistic Typography: A Comprehensive Review of Artistic Text Design and Generation", "authors": ["Bai Yuhang", "Huang Zichuan", "Gao Wenshuo", "Yang Shuai", "Liu Jiaying"], "abstract": "Artistic text generation aims to amplify the aesthetic qualities of text while maintaining readability. It can make the text more attractive and better convey its expression, thus enjoying a wide range of application scenarios such as social media display, consumer electronics, fashion, and graphic design. Artistic text generation includes artistic text stylization and semantic typography. Artistic text stylization concentrates on the text effect overlaid upon the text, such as shadows, outlines, colors, glows, and textures. By comparison, semantic typography focuses on the deformation of the characters to strengthen their visual representation by mimicking the semantic understanding within the text. This overview paper provides an introduction to both artistic text stylization and semantic typography, including the taxonomy, the key ideas of representative methods, and the applications in static and dynamic artistic text generation. Furthermore, the dataset and evaluation metrics are introduced, and the future directions of artistic text generation are discussed. A comprehensive list of artistic text generation models studied in this review is available at https://github.com/williamyang1991/Awesome-Artistic-Typography/.", "sections": [{"title": "Introduction", "content": "Artistic text generation focuses on turning text into visual forms that increase their artistic expression or convey their meaning. It can integrate plain text with fantastic style, decoration, and appearance, creating typography that is legible, readable, and appealing. Such integration of visual representation and semantic understanding, not only attracts viewers, but also emphasizes the messages' meaning and strengthens the impact, making artistic text generation prevalent in graphic design, manga and comic book industry, advertisement, websites, consumer electronics, and social media.\nArtistic text generation can be broadly classified into two categories: artistic text stylization and semantic typography. The former primarily involves applying visual effects (i.e., text effects) to text, while the latter focuses on redesigning the shape of text to match specific objects."}, {"title": "Task Formulation", "content": "Artistic text generation is a conditional image generation problem. It typically involves a text input T and a style input S', intending to generate the corresponding artistic text T', preserving the shape of T while incorporating the style from S'.\nIn terms of the text input, T usually can be a text raster or vector image. With the recent development of cross-modality diffusion models, prompts can also be used as T to specify the desired text to be generated. For example, Stable Diffusion 3 demonstrates its powerful generative ability with an image generated using the prompt \"Epic anime artwork of a wizard atop a mountain at night casting a cosmic spell into the dark sky that says 'Stable Diffusion 3' made out of colorful energy\".\nIn terms of the style input, artistic text primarily encompasses two kinds of stylish effects: one is overlaid upon the text and the other is applied to the shape of the text itself. Therefore, artistic text generation can be divided into two major categories: artistic text stylization and semantic typography:\nArtistic Text Stylization. Artistic text stylization focuses on migrating visual effects (i.e., text effects) from S' to T, including basic effects such as color, shadows, outlines, and gradients, as well as complex texture effects like flames and water ripples. Here, S' is usually a style image. Based on S', artistic text stylization can be further divided into two subcategories: text effect transfer, which directly imitates pre-designed text effects S' by artists, and arbitrary style transfer on text, which utilizes style elements from any image S' to design entirely new text effects and is more similar to standard image style transfer tasks.\nSemantic Typography. Semantic typography primarily deals with the shape of the text, aiming to deform the text into a target semantic content. For example, transforming the letter 'S' in the word \"SNAKE\" into the shape of a snake or transforming the entire word \"umbrella\" into the shape of an umbrella. Such shape transformations enable the text to visually match its intended meaning, enhancing its expressiveness and making the conveyed message more accessible even to those unfamiliar with the language.\nIt is worth noting that these two major approaches are not mutually exclusive. They can be combined to simultaneously modify the shape and apply visual effects to create more fascinating artistic text artwork.\nMeanwhile, according to the modality, the task can be divided into static artistic and dynamic text generation. Static artistic text generation primarily focuses on generating still images of artistic text. It involves applying various static text effects and style elements to the text, commonly used for creating visually appealing typographic designs, logos, posters, and other static visual compositions. In the context of the multimedia era, incorporating motion into artistic text has gained significant attention. Dynamic artistic text generation involves generating videos or GIF animations that showcase animated artistic text. In this case, there are two main aspects to consider: text effect animation and text shape animation.\nText Effect Animation. Text effect animation focuses on studying the transfer of dynamic text effects onto the static text. In most cases, the text itself remains stationary. Sometimes, motion effects such as appearing, moving, scaling, or disappearing can be also considered as part of the text effects. In such a case, the output T' engages both text motions and animated visual effects.\nText Shape Animation. Text shape animation primarily focuses on how to animate the semantic typography to resemble the motion of the intended semantic content naturally. For example, animating the leg-kicking motion for the letter 'L' in the word \"LEG\". This involves animating the shape of the text to bring it to life and can visually convey more abstract concepts.\nWe summarize the taxonomy of the representative artistic text generation methods in Figure 4. Specifically, all methods can be roughly divided into artistic text stylization and semantic typography. In the task of artistic text stylization, two kinds of styles are considered: text effects and arbitrary style, corresponding to text effect transfer and arbitrary style transfer on text. Based on the modality, artistic text stylization can be further divided into static and dynamic artistic text stylization. Meanwhile, in the task of semantic typography, character-level and word-level design are studied. Based on the modality, semantic typography can be further divided into static semantic typography and kinetic typography. In the following sections, we will detail the main ideas of the representative methods in the order of their categories."}, {"title": "Artistic Text Stylization", "content": "Artistic text stylization involves the rendering of various visual effects, such as colors, shadows, outlines, gradients, textures, and embellishments, to enhance the aesthetics of text. First of all, text stylization pertains to image stylization, focusing on how to apply image stylization techniques to the specific content of text images. In general, the development of artistic text stylization has largely followed the trajectory of image stylization techniques. Chronologically, image stylization has moved from traditional texture synthesis to image translation leveraging deep neural networks, and most recently, to more sophisticated text-to-image techniques utilizing diffusion models. Concurrently, artistic text stylization approaches have incorporated specific designs into the above techniques to better transfer text effects and maintain the legibility of text. This section presents an overview of existing methods, highlighting their specific designs, strengths, and limitations."}, {"title": "Static artistic text stylization", "content": "Text effect transfer Stroke-based text effect transfer. Early image styl-ization methods primarily simulated how artists paint on the canvas: modeling brush strokes and synthesizing them onto a digital canvas. Corre-spondingly, early text stylization methods focused on stylizing the strokes of characters, with the most representative research being calligraphy. In calligra-phy, brush strokes vary in depth, pressure, and ink textures. Early studies concentrated on synthesizing ink textures onto brush strokes. Directly applying texture synthesis technology to characters often resulted in ink directions that did not align with the stroke directions. To address this, as illustrated in Figure 5(a), Goda et al. propose to synthesize ink textures along the edges of characters by finding textures with lengths and directions that match the strokes, yielding more natural results.\nPatch-based text effect transfer. Calligraphy represents only a small portion of text effects, characterized by relatively monochrome colors. To han-dle more diverse and colorful text effects, T-Effect introduces the first style transfer method specifically for general text effects, such as shadows, outlines, gradients, and textures. In particular, T-Effect defines a supervised text effect transfer problem: S : S' :: T : T' , where the source text effect S' in addition to its corresponding plain text S are required. The algorithms learn the transformation between them and then apply it to the target text T to synthesize the result T'. Methodologically, T-Effect builds on the texture synthesis method of Wexler et al. and its variants using random search and propagation as in PatchMatch . As illustrated in Figure 5(b), the basic texture synthesis process is to match patches between S' and T' (to match textures), as well as S and T (to match text shape), and to update each patch in T' with its best-matched patch in S'. The process iteratively matches and updates patches until convergence. To extend this general texture synthesis to text effect, T-Effect analyzes real-world text effect images and summarizes a novel text effect prior: there is a high correlation between patch patterns (i.e., color and scale) and their distances to text skeletons in high-quality text effects. This is because the artists commonly adjust the effects based on text shapes for readability. Based on this, T-Effect makes the following two modifications:\nScale-adaptive matching: Encourages the patch to be matched at their op-tical scale based on their distance to the text skeletons. This could preserve both coarse structures and texture details.\nDistribution-aware matching: Encourages the text effects of T' to share sim-ilar distribution with S'. This could effectively realize spatial-aware style transfer.\nBesides, the T-Effect further considers avoiding texture over-repetition for more natural synthesis. By incorporating priors specific to text effects into the patch match approach, T-Effect enables more plausible artistic text generation as in Figure 6(a).\nMen et al. extend T-Effect to more general interactive texture transfer applications, where S and T can be general semantic maps. Since semantic maps have large flat regions that provide few cues for valid matching, this method introduces novel structure guidance. Men et al. find that it is easier to build correspondences near the contours of the semantic maps. Based on the matched contour key points, the method propagates the structure guidance into inner flat regions to build rough correspondences, based on which textures are synthesized. Thus, this method can better transfer inner textures than T-Effect.\nHowever, the patch-based method requires iterative patch matching, result-ing in a generation time of approximately one minute per image, which is not sufficiently efficient.\nDeep-based text effect transfer. Entering the era of deep learning, re-searchers investigate the way of artistic text generation via data-driven learning. TET-GAN is one of the first deep methods and enables real-time artistic text generation after training. TET-GAN builds a text effect dataset with paired plain text images and the corresponding artistic text images. Then, the prob-lem becomes to learn a supervised image-to-image translation as in pix2pix . However, directly applying pix2pix can only learn one style at a time, which is less efficient in practice. To this end, TET-GAN introduces the idea of style-content distentanglement into text effect transfer with separate con-tent and style encoders as shown in Figure 5(c). Specifically, TET-GAN builds a multi-task framework trained with three tasks:\nGlyph reconstruction: The network is trained to reconstruct the plain text image T so that it learns the glyph features.\nArtistic text destylization: The network is trained to infer the glyph features from the artistic text images S' so that it learns to disentangle the content representation.\nArtistic text stylization: The network is tasked to transfer the text effects of S' onto T, obtaining the output that approaches the ground truth T'. The network will learn to disentangle the style representation and combine it with the content representation for style transfer.\nWith this design, TET-GAN can learn hundreds of text effects in a single net-work. To further improve the practicality, it proposes a few-shot text effect fine-tuning strategy to efficiently extend the model to new text effects with only several and even one reference text effect image available.\nTraining on multiple tasks boosts the versatility of the model, which also increases the model complexity. Huang et al. find that training a pix2pix network to map a channel-wise concatenated input of S, T and S' to the output T' is enough for multi-style transfer. This is useful when complicated functions like style extension and interpolation featured in TET-GAN are not required.\nThe aforementioned methods mainly treat text effects as a whole. However, some text effects have exquisite decor that needs special consideration. These decorative elements usually have very different styles from the base text effects. To address this problem, Wang et al. propose to learn to separate, transfer and recombine the decors and the base text effects. The framework contains a network for decorative element segmentation, text effect transfer, and structure-based decor recomposition. During recomposition, the decorative elements are divided into two classes based on their importance. Insignificant elements are repeatable and randomly scattered on the text, while the significant elements are placed based on their spatial distributions in the original S'. The method can produce professional artistic typography on English letters and simple symbols as shown in Figure 6(b). Ma et al. further extend this work to complex Chinese characters."}, {"title": "Arbitrary style transfer on text", "content": "Text effect transfer is a well-defined problem with inputs S, S', and T, and the ground truth output T'. However, for arbitrary style transfer on text, we do not have the plain text version for S', and usually the ground truth output T' is not available, posing an unsupervised image-to-image translation problem for researchers. Furthermore, unlike text effects, there is a significant visual discrepancy between the plain text and the colorful style images. Therefore, this task is more challenging. The key is to find appropriate correspondences between the two distant domains.\nPatch-based style transfer on text. UT-Effect is one of the earliest methods to transfer arbitrary style onto text. To build valid correspondences be-tween S' and T, it proposes extracting a binary mask S from S' based on texture removal , super-pixel extraction and clustering. The region with higher saliency is set as the foreground corresponding to the text region in T while the remaining region is the background. Then the unsupervised problem becomes a supervised problem as proposed in T-Effect . However, there is still obvious structure discrepancy between S and T. For example, S might be maple leaves of different shapes, while T is the plain text with rigid edges. Directly synthesizing maple textures into T will result in unnatural maple boundaries. To solve this problem, UT-Effect proposes a two-stage style transfer framework:\nStructure transfer: UT-Effect transfers the structure styles of S onto T, ob-taining T that shares similar boundaries with S while maintaining the glyph of T. To achieve this, UT-Effect proposes a legibility-preserving structure transfer method, which uses a patch-based shape synthesis technique to adjust the shape of the stroke ends while preserving the shape of the stroke trunk for legibility.\nTexture transfer: For the translation problem S : S' :: \u00ce : \u00ce', UT-Effect leverages the patch-based texture synthesis technique of T-Effect and introduces a new saliency term to guide patch matching. The saliency term encourages pixels inside the text to find salient textures for synthesis and keeps the background less salient, which makes the artistic text stand out from the background.\nGAN-based style transfer on text. Applying deep learning to arbitrary style transfer on text is challenging since there is generally no large-scale ground truth data for model training. To solve this problem, Shape-Matching GAN proposes a one-shot bidirectional shape-matching framework to establish an ef-fective glyph-style mapping at various deformation levels without paired ground truth. It includes two stages:\nBackward transfer: After extracting the structure map S from the style image S' as in UT-Effect , the first stage backward transfers the shape style of the text to the structure map, obtaining its sketchy or simplified version S, whose contour style is similar to the plain text.\nForward transfer: The second stage learns the forward mapping from the sketchy structure map S to the original structure map S, and further to the original style image S'. The network learns to characterize the shape and texture features of the style image in the training phase and transfers these features to the target text in the testing phase.\nTo enable training on a single style image, Shape-Matching GAN randomly crops S, S, and S' into sub-image pairs to obtain enough data. Another key contribu-tion of Shape-Matching GAN is the style degree control mechanism. Specifically, there is a trade-off between legibility and artistry: The stylistic degree or shape deformations of a glyph need manipulation to resemble the style subject in S', while the glyph legibility needs to be maintained so that the stylized text is still recognizable. People's diverse preferences make it difficult to define an op-timal style degree. Shape-Matching GAN introduces an extra parameter l to control the style degree freely to allow users to select the most desired one. During backward transfer, l is used to control the simplification degree of S to build multi-degree paired data, thus during forward transfer, the model will learn multi-degree structure transfer conditioned on l.\nBased on Shape-Matching GAN several improvements are proposed. Chen et al. and Zhang et al. use erosion and dilation to remove the unnecessary artifacts along the contour to generate cleaner structure transfer results. Zhang et al. utilize multi-resolution style images borrowed from pyramid features for better texture transfer. Xue et al. directly train a dataset generation network to synthesize various paired data to overcome the problem of limited data.\nIntelligent Typography finds that it is hard for a 1\u00d7 network to learn a robust pixel-to-pixel level relationship from a single style image S' due to over-fitting. To overcome this issue in ShapeMatching GAN-based methods, In-telligent Typography develops a novel 2\u00d7 magnification network to smartly con-vert the problem of complex style transfer into texture expansion and super-resolution, which relieves the pressure of one-shot learning dramatically. To bet-ter transfer the style effects with relatively complex texture and structure, Intelli-gent Typography proposes a coarse-to-fine framework with two stages: prototype generation and detail refinement. Prototype generation generates a coarse-level stylized prototype from the given mask and tailored texture with the 2\u00d7 magni-fication network. Then, a structure network and a texture network are proposed to refine the details of the prototype. With the above design, Intelligent Typog-raphy can generate exquisite images with vivid artistic text details and clear backgrounds.\nDiffusion-based style transfer on text. Recently, the increase in data scale and model representation capabilities has ultimately led to the emergence of large diffusion models . Diffusion models bring new opportunities for artistic text generation. Diffusion models exhibit unprecedented expressive power, pro-viding diverse style support and interactive generation control with the help of large language models . Anything to Glyph is one of the recent diffusion-based, text-driven artistic text synthesis methods. It leverages the generative power of pre-trained Stable Diffusion and segmentation models to gen-erate a paired dataset and train a diffusion model called Position Predictor to predict an object's position mask S in S'. Then, during testing, Denoising Score Matching is applied to update the latent code of the Position Predictor so that its denoising result is similar to the target text shape T while maintaining the consistency with the prompt. Given the updated latent code, which repre-sents an initial structure transfer result (like\u00ce in UT-Effect ), pre-trained Stable Diffusion is used to synthesize textures onto it under the guidance of the prompt. Anything to Glyph is especially good at generating artistic text images composed of multiple instances of objects specified by the prompt such as stones, leaves, and eggs as shown in Figure 6(g)."}, {"title": "Joint artistic style and font style transfer", "content": "The font is an important style of text complementary to the text effects. Different from text effects, large-scale text images with different fonts can be easily generated, which is suitable for deep learning. With the rapid development of deep generative models, font gen-eration has become a hot topic and made great progress. In addition to the large-scale supervised learning, researchers focus on investigat-ing more challenging few-shot font generation and handwriting genera-tion . Font generation can effectively design and produce new typefaces that can be used in various digital and print media. The generation process can automate the creation of font styles, weights, and variations, making it easier to produce large font families with consistent design characteristics. This section will briefly review representative approaches that combine font transfer and text effect transfer. High-quality joint transfer results are shown in Figure 6(d)\nMC-GAN is one of the first deep-based few-shot joint font and text effect transfer models. It contains a glyph network for font generation and an orna-mentation network for text effect transfer. MC-GAN stacks these two networks and jointly trains them to realize an end-to-end solution. Specifically, the glyph network is pre-trained on a large-scale dataset so that it can predict the coarse glyph shapes of the missing English letters from a few stylized English letter examples. The full model with two networks is then fine-tuned on the stylized examples to learn to refine and stylize coarse glyph shapes into clean and well-designed letters.\nZhang et al. further divide the glyph network of MC-GAN into a coarse-level image-to-image translation network and a fine-level stack network. In terms of text effect transfer, it utilizes the widely-used Neural Style Trans-fer . On the other hand, Yuan et al. find that using edge and skeleton information of T as auxiliary input of the glyph network could better infer the shape of the font.\nAGIS-Net proposes to disentangle the content and style representations with two encoders and two decoders. Two encoders learn to extract the content feature and style feature, respectively. Then two collaborative decoders generate the glyph shape image and final artistic text image simultaneously based on the extract features. The disentanglement ensures a few-shot multi-content and multi-style generation.\nInspired by the AdaIN-based style representation , FET-GAN views font style and text effect style as a whole and models them with channel-wise means and standard deviations as in . The content feature extracted from the source image is stylized through AdaIN operation and further decoded to obtain the final stylized artistic text image.\nZhu et al. also propose a few-shot end-to-end framework that extracts and combines content and style representations for joint font and text effect transfer. The key idea is that text content and style are not fully independent. Therefore, it calculates the similarity between the reference glyph and the target glyph to assign weights for the style features of each referenced glyph. Then, the weighted style representation and the content representation are fused to generate the final artistic text image.\nDSE-Net argues that treating font style and text effect style as a whole would limit the transfer of complex styles. It presents a disentangled style en-coding network, with three different networks to extract the font feature, text effect feature, and glyph content feature, respectively. Finally, a cross-layer fu-sion mechanism is proposed to fuse the features adaptively to generate the final output.\nGenText extends TET-GAN with an extra task of font transfer. It treats the plain text as an artistic text image with special text effects, thus unifying the destylization and stylization tasks within a single network. Then, GenText uses an encoder network to extract the content code and the style code from artistic text images, a font transfer network to fuse the content code of the plain text and the style code of the plain text with reference font for font transfer, and a text effect transfer network to fuse the content code of the plain text and the style code of the artistic text for stylization and destylization. Then, artistic text can be generated by sequentially performing font transfer and stylization.\nZhu et al. propose an artistic text style transfer model based on multi-factor disentanglement and mixture. The model contains three encoders to ex-tract the text effect, font, and glyph representations, and further employs adver-sarial training and one-factor swap training strategies to disentangle the three representations. The disentanglement enables several tasks of font transfer, text effect transfer, joint transfer, and style removal."}, {"title": "Dynamic artistic text stylization", "content": "Compared to static artistic text, dynamic artistic text is more attractive and is widely used in a variety of media such as films, advertisements, and video clips. While static artistic text stylization and general video image style transfer have been extensively studied, a few ap-proaches have studied dynamic artistic text stylization, which is re-viewed in this section.\nDifferent from general video style transfer that migrates static style from a style image onto a content video, dynamic artistic text stylization aims to trans-fer dynamic style from a style video S' = {S_1, S_2, ..., S'_N} onto a text image T as illustrated in Figure 6(e) (f), where N is the total frame number. Therefore, it is not straightforward to apply the optical flow guidance widely used in video style transfer to dynamic artistic text generation. Instead, dynamic artistic text stylization approaches mainly focus on spatial-temporal style representation modeling and transfer.\nDynamic text effect transfer DynTypo extends the NNF search of PatchMatch to the spatial-temporal domain. Instead of searching the Nearest-neighbor Field (NNF) for text effect synthesis in a frame-by-frame manner, the main idea of DynTypo is to simultaneously optimize the text effect coherence across all frames to find a common NNF for all temporal frames. Specifically, DynTypo stacks patches at the same position but across an entire video into a patch cube and matches patches at a cube level. After matching, the entire cubes are directly used to synthesize the output video. DynTypo first detects keyframes based on the intensity of text effect dynamics and then limits the procedure of cube matching within the keyframes to maintain both spatial and temporal consistencies. DynTypo further combines PatchMatch with Simulated Annealing, to add more priority to the patches near the text contours. With the above designs, DynTypo achieves impressive results (e.g., Figure 6(e)) in dynamic text effect transfer.\nHowever, it is hard for a single global NNF to transfer complex dynamic text effects such as moving samples that shift across source videos. To better model the inter-frame correlation and transfer complex dynamic text effects, DynTex-ture proposes to combine PatchMatch with the advanced Transform-ers . Specifically, DynTexture decomposes the dynamic text effect transfer task into two stages.\nFirst frame generation: DynTexture adopts patch-based text effect trans-fer with distance map guidance to generate the first frame.\nFull video generation: The synthesized first frame is decomposed into structure-agnostic patches, which are then encoded to tokens. Then, Transformers equipped with VQ-VAE are exploited to predict the discretized token sequences, leveraging its high capability of capturing the long-distance de-pendencies between frames. All predicted patches are assembled into each frame by a Gaussian weighted average merging strategy to obtain the final full video result.\nArbitrary dynamic style transfer on text In the scope of arbitrary style transfer, Shape-Matching GAN++ extends Shape-Matching GAN to video domains. Shape-Matching GAN learns the forward mapping from the sketchy structure map S to the original structure map S in a patch level. In Shape-Matching GAN++, to learn spatial-temporal shape mappings, patches across consecutive frames are learned together, just as the patch cude in Dyn-Typo , so that the model could learn inter-frame motion patterns. To gen-erate long-term motions, Shape-Matching GAN++ divides the full video into multiple K-frame video clips and focuses on the short-term motion patterns of these video clips. It defines the short-term motion pattern as the shape dynamics between the frames within a video clip. During training, the model learns the shape mappings between the first K-1 frames and the last frame of a video clip, which is actually a frame prediction task. During testing, the model repeatedly predicts the next frame based on previous frames and propagates the short-term motion patterns to achieve long-term motion patterns. Figure 6(f) presents an example of the dynamic style transfer result by ShapeMatching GAN++."}, {"title": "Semantic Typography", "content": "Semantic Typography focuses on transforming the text shape to visually rep-resent specific objects, themes, or concepts. This section investigates different techniques, including shape morphing, deformation, and adaptive typography, to realize this meaningful and visually appealing art form."}, {"title": "Static semantic typography", "content": "Character-level semantic typography Ornamental Clipart Generation.\nOrnamental clipart generation aims to identify suitable images that correspond to the semantics of words and meticulously assemble them under the direction of glyph strokes as shown in Figure 7(a), which is considerably time-consuming when carried out manually. Zhang et al. present an automatic framework for creating such ornamental typefaces. Specifically, the framework features an interactive interface for the glyph stroke segment with scribbles from the user, thus fulfilling their personalized requirements. In terms of image selection, a semantic-shape similarity metric is established to concurrently account for both word semantics and stroke form. An optional structural optimization step based on gradient descent is implemented to yield results with enhanced glyph structure and aesthetic appeal.\nTrick or TREAT is another retrieval-based method for ornamental clipart generation. It leverages cliparts to resemble the glyphs of characters to express the semantic features of words. To better retrieve the similarity between letters and cliparts, it trains an auto-encoder based on AlexNet in an unsupervised manner to automatically learn a hidden feature space. The corresponding cli-parts will be retrieved based on the distance in the hidden space. Trick or TREAT generates results that have good legibility, semantics, and creativity. However, as with all retrieval-based methods, the diversity of the output is inherently de-pendent on the input data, which may introduce some limitations in the richness of the generated content.\nSemantic Character Generation. With the development of generative models in recent years, diffusion models have demonstrated exceptional perfor-mance in image synthesis tasks . The robust cross-modal under-standing and rich semantic priors encapsulated make diffusion models highly adaptable for semantic typography. As shown in Figure 7(b), Iluz et al. pioneer the integration of a pre-trained Stable Diffusion model into the seman-tic character generation process. Focusing on the geometric transformation of glyphs, they employed the SDS loss and DiffVG to tailor diffusion models for the artistic vectorized glyph shape generation. Additionally, they incorpo-rated the as conformal as possible (ACAP) loss to regulate the extent of character deformation and established a tone preservation loss to maintain the structural integrity of the original glyph. This approach not only enhances the adaptability of diffusion models but also ensures that the generated characters retain their distinctive aesthetic and semantic qualities.\nIn contrast to the approach of Iluz et al. , DS-Fusion places a greater emphasis on texture and color features by directly processing glyph images in raster form. Guided by style prompts and glyph images, DS-Fusion integrates the glyph shape with style images derived from a pre-trained Stable Diffusion model , which is conditioned with style prompts. A CNN-based discrimi-nator is employed to provide implicit supervision of the generation process by examining the feature maps of both the glyph shape and the generated image in latent space. It is noteworthy that DS-Fusion, when supplied with an entire word as a glyph reference, is also adept at producing word-level stylized results that exhibit harmonious character combinations and a vivid artistic expression.\nWordArt Designer presents a user-controllable artistic semantic charac-ter design system with a large language model (LLM) engine . It contains three modules SemTypo, StyTypo, and TextTypo. SemTypo employs charac-ter parameterization and rasterization techniques akin to those used by Iluz et al. , to manipulate semantic features and deform selected parts of character strokes. StyTypo refines the smoothness and stylization details by leveraging the depth2image approach of latent diffusion models . In TextTypo, a Con-trolNet is conditioned with Canny edges, depth maps, scribbles, and the original text image, enabling it to render textures that align with semantic styles and glyphs. The LLM engine generates structured text prompts based on user descriptions, feeding into the aforementioned models and thereby amplifying the creative diversity.\nWord-level semantic typography Calligram Generation. A calligram is a creative arrangement of words or letters that forms a visual image, conveying both meaning and aesthetics, shown in Figure 7(c). Xu et al. first introduce a warp-based method to integrate letters into the subject region of an image, thereby crafting calligrams that possess semantic features alongside logical glyph stroke deformation. This method employs an interactive approach to divide the container into subregions, which are subsequently filled by warping letters. While this approach can produce calligrams with coherent letter shape arrangements, it sometimes struggles with legibility.\nTo address this issue, Zou et al. conduct a crowd-sourced study aimed at refining the guidance for glyph deformation to enhance letter legibility. They introduced a fully automatic method for generating letter layouts, aligning letters based on correspondence, and applying deformation.\nWord Painting Generation. Word painting represents a form of compos-ite artwork, characterized by the adaptive fusion of visual structure and texture derived from a source image with semantic features extracted from a textual source, shown in Figure 7(d). Xu et al. formulate this task as approxi-mating the primary structure of a reference image using ASCII characters. This method surpasses traditional tone-based techniques by capturing structural and semantic nuances through an innovative alignment-insensitive shape similarity metric. Maharik et al. introduce a technique for crafting micrographics, a distinctive variant of word painting that comprises minuscule letters. Departing from the conventional single horizontal text layout of ASCII art, this method emphasizes the design of low curvature and smooth vector fields devoid of singu-larities, facilitating the synthesis of word layouts that conform to regional shapes while maintaining high text readability. Additionally, a warping procedure for text height and width is proposed, enabling the adjustment of text shape to seamlessly integrate with the image contours.\nPicWords is an automatic word painting generation framework that em-ploys non-photorealistic rendering (NPR) techniques. It processes the input im-age by segmenting the binary silhouette into distinct patches, each designed to encapsulate a keyword. By ranking the patches and keywords, this approach es-tablishes a keyword-patch correspondence that serves to accentuate significant keywords, ensuring that key semantic information is effectively conveyed. Zhang et al. introduce a method that utilizes a smooth vector field for patch segmentation, coupled with a Support Vector Machine (SVM)-based visual at-tention model to optimize the aesthetic arrangement of text. This visual atten-tion model is trained to generate a saliency map for a given image, strategically positioning keywords that are closely related to the theme in areas that natu-rally draw the viewer's attention. Compared with prior work , this approach is capable of producing results that are imbued with more profound semantic information, enhancing both the visual appeal and the narrative depth of the generated word paintings."}, {"title": "Kinetic typography", "content": "Kinetic typography is a dynamic and expressive art form that combines motion graphics with typography to convey emotions, narratives, or messages in a vi-sually engaging way. It involves animating text so that it moves, transforms, or interacts with other elements in a sequence or scene. Early works focus on ex-ploring the interaction between dynamic forms and content to enrich emotional expression, with various kinetic typography system designs for animating text . Wakey-Wakey presents an automatic framework for align-ing dynamic text motions with GIF animation. However, these methods lack effective measures to combine semantic features with textual dynamics.\nLiu et al. introduce a novel approach to generate text animation with semantic features. In contrast to the work of Iluz et al. , they have devel-oped an end-to-end model designed to mitigate conflicts with prior knowledge. As shown in Figure 7(e), this model leverages neural displacement fields and vector representations to deform letters, thereby conveying semantic meanings and rendering them in dynamic movements that respond to user prompts. Xie et al. propose a method to animate compact word clouds, expanding the scope from single-word animations to multiple words that express semantic emotions."}, {"title": "Applications", "content": ""}, {"title": "Graphic Design", "content": "WordArt and LOGO design is a task involving the transformation of text input into semantically rich typography, incorporating various elements and layouts, shown in Figure 8(a). Numerous studies have successfully addressed the genera-tion of diverse WordArt and LOGO designs, utilizing a wide range of elements and layouts .\nSimilarly, poster design entails the arrangement and styling of provided text, subsequently generating the designed text on an input image, shown in Fig-ure 8(b). Several works have achieved advancements in this area."}, {"title": "Scene Text Editing", "content": "Scene text editing has emerged as a notable research area in recent years, focus-ing on the transformation of text within a source image to align with a target reference text while preserving the original style and background, shown in Fig-ure 8(d). Numerous studies have proposed methodologies to generate high-quality text images despite varying backgrounds and fonts.\nDiffusion models have demonstrated significant potential in editing images of arbitrary topics, including scene text. To fully exploit this potential, several works have applied diffusion models to the task of scene text editing.\nThe subfield of scene-style text editing, addressed in some research , entails modifying specific attributes of text within an image while either pre-serving or altering its style. These attributes include rotation, font, color, and content, allowing for versatile text manipulations across various scene images. Moreover, scene text editing can be extended to video applications . En-hancing scene text editing in videos necessitates preserving geometric integrity, appearance consistency, and temporal"}]}