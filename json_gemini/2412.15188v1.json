{"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "authors": ["Weijia Shi", "Xiaochuang Han", "Chunting Zhou", "Weixin Liang", "Xi Victoria Lin", "Luke Zettlemoyer", "Lili Yu"], "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.", "sections": [{"title": "Introduction", "content": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023; Koh et al., 2024; Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024), and Unified-IO (Lu et al., 2022, 2024) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens.\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023; Sun et al., 2023; Wu et al., 2024). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities.\nTo address this challenge, we introduce Llama Fusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023; Chen et al., 2023; Liang et al., 2024; Liu et al., 2024a), LlamaFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks. As shown in Figure 1, we employ modality-specific query-key-value (QKV) projections and feed-forward networks (FFNs) to process text and image data separately while still allowing for cross-modal interactions in the joint self-attention layer. By freezing the text modules while finetuning the image modules, we preserve the language-only capabilities of pretrained LLMs while giving a head start to the learning of visual understanding and generation. Compared to pretraining multimodal generative models from scratch, this approach avoids the need to include text-only data in the training process, significantly reducing the computational demands.\nTo evaluate the effectiveness of our approach, we conduct comprehensive experiments comparing Llama Fusion with Transfusion in controlled settings. Specifically, we initialize our Llama Fusion architecture with a pretrained Llama-3 8B model (Dubey et al., 2024) and continue training on the same image data as in Transfusion (Zhou et al., 2024). Compared to Transfusion, LlamaFusion achieves a 20% improvement in image understanding, 3.6% improvement in image generation while using only 50% of the FLOPs. It also preserves Llama-3's text-only performance that outperforms Transfusion by 11.6%. Figure 2 presents images generated by LlamaFusion. Additionally, we further demonstrate that this framework can adapt existing vision-language models (e.g., LLaVA) with multimodal generation ability.\nThrough ablation studies, we analyze the key architectural decision for LlamaFusion: separating both self- attention and FFNs for different modality data while freezing weights for the pretrained language modality. We show that naive finetuning of the dense pretrained LLMs on multimodal data (no separation) leads to a catastrophic forgetting of their original language capabilities. Furthermore, deep separation proves to be more effective than shallow separation (using modality-specific FFNs only), with both approaches outperforming models with no separation.\nOverall, LlamaFusion has the following key features: (1) Compute reuse: It leverages existing computational investments in text-only LLMs when developing multimodal generative models. This eliminates the need to retrain on text-only data, significantly reducing computational demands. (2) Performance preservation and transfer: It completely preserves the strong text-only performance of pretrained LLMs and facilitates a better learning of image understanding and generation in the multimodal generative setup."}, {"title": "Background: Transfusion", "content": "Transfusion (Zhou et al., 2024) is a single unified multimodal model that is capable of text generation, image understanding, and image generation tasks, by jointly predicting next tokens in language and diffusing image"}, {"title": "Language Modeling", "content": "Given a sequence of discrete language tokens \u00e6txt = xtxt,...,xt, a language model @ represents its joint probability by P(xtxt) = 1 Po(xtxt | \u00e6tat). This formulation sets up an autoregressive task, where each token xtxt is predicted based on its preceding tokens \u00e6. The language model is learned by minimizing the cross-entropy between Pe and the observed data distribution, which is commonly referred to as the LM loss:\n$L_{LLM} = E_{x^{txt}} [-log P_{\\theta}(x_t^{txt} | x_{<t}^{txt}, x^{img})]$ \nOptionally, if there exists image data preceding the language tokens (e.g., image-caption data), Transfusion adds the representation of \u00e6img as additional condition to the objective. More details of representing ximg are presented below."}, {"title": "Image Diffusion", "content": "Given a raw image, Transfusion first encodes the image into a sequence of continuous latent representation ximg with a pretrained and frozen VAE tokenizer (Kingma, 2013). It then employs Denoising Diffusion Probabilistic Models (i.e., DDPM) to learn to reverse a gradual noise-addition process added in the forward process (Ho et al., 2020). In the forward diffusion process, a Gaussian noise \u0454 ~ N(0, I) is added to the image representation ximg over T steps, creating a sequence of noisy image representations x0,x1, ..., XT. Specifically, at each step t, the noisy image representation is given by:\n$x_t^{img} = \\sqrt{\\bar{\\alpha}_t} x_0^{img} + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\nHere \u0101t follows a common cosine schedule (Nichol and Dhariwal, 2021).\nIn the reverse process, the diffusion model \u025b\u0e2d(\u00b7) with parameters 0 learns to predict the added noise e given the noisy data x at timestep t and a context xtxt that can include text prompts such as captions to the image diffusion:\n$L_{DDPM} = E_{x^{img},t,\\epsilon} [|| \\epsilon - E_\\theta (x_t^{img}, t, x^{txt})||^2]$\nThe Transfusion architecture contains U-Net downsampler and upsampler to reduce the dimension of \u00e6img. The U-Net downsampler transforms the image into fewer patches before the main Transformer modules while the upsampler projects them back to the original dimension of ximg after the Transformer."}, {"title": "Training Objective", "content": "During training, Transfusion is optimized to predict both the LM loss on the text input \u00e6tat and the diffusion loss on the image input ximg. These two losses are combined using a hyperparameter \u5165:\n$L_{Transfusion} = L_{LLM} + \\lambda \\cdot L_{DDPM}$"}, {"title": "Llama Fusion", "content": "One notable feature of Transfusion is that it has the same architecture as mainstream LLMs (e.g., Llama (Touvron et al., 2023)) while being capable of text generation, image understanding, and image generation together, through an end-to-end training (Equation 4). Zhou et al. (2024) trains Transfusion from scratch using"}, {"title": "Model Architecture", "content": "In response to the challenge above, we propose Llama Fusion, a framework that combines a pretrained, text-only Llama model with a dedicated image transformer for visual generation and understanding, enabling each modality to be processed through independent weights. By freezing the text modules while finetuning the visual modules, we preserve its language-only capabilities while giving the learning of visual understanding and generation a boost start.\nLlama Fusion is a decoder-only model consisting of N transformer layers. As shown in Figure 1, central to the design are the modality-specific attention layer and Feed-Forward Network (FFN), each handling only data from its corresponding modality. Without loss of generality, we describe Llama Fusion below in a configuration with a single transformer layer, folding residual connections and layer normalization directly into the self-attention and FFN. The inputs to the model are text tokens xtxt and noisy image representations\n$x_t^{img} = \\sqrt{\\bar{\\alpha}_t} x_0^{img} + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$. We use blue for text-specific modules and red for image-specific modules."}, {"title": "Input projection", "content": "The input text tokens xtxt are projected by a linear embedding layer to a sequence of text hidden states htat. The noisy image x img are projected to a sequence of image representations himg via a U-Net downsampler.\n$h_{in}^{txt} = Proj_{text}(x^{txt})$\n$h_{in}^{img} = UNet\\_Down^{img}(x^{img}, t)$\nThen the text hidden states htxt or image hidden states himg are fed into the following attention layer."}, {"title": "Modality-specific self-attention", "content": "We create separate attention matrices for each modality. Specifically, the text hidden states hirt and image hidden states himg are converted into their respective queries, keys, and values via separate Q, K, V matrices. The pre-attention layer normalization is also modality-specific and is folded into the QKV functions.\n$\\hat{h}^{txt}, h^{txt}, h^{txt} = QKV_{text}(h_{in}^{txt})$\n$\\hat{h}^{img}, h^{img}, h^{img} = QKV_{img}(h_{in}^{img})$\nWe enable cross-modal attention by concatenating the queries, keys, and values from both image and text modalities into unified sequences. The attention-weighted values at text and image token positions are then projected back into the hidden state dimension using separate O weights for each modality.\n$h_{out}^{txt} = O_{text}(softmax(\\frac{\\hat{h}_{Q}^{txt}[\\hat{h}_{K}^{txt} \\oplus \\hat{h}_{K}^{img}]^T + M}{\\sqrt{d}})[h^{txt} \\oplus h^{img}])$\n$h_{out}^{img} = O_{img}(softmax(\\frac{\\hat{h}_{Q}^{img}[\\hat{h}_{K}^{txt} \\oplus \\hat{h}_{K}^{img}]^T + M}{\\sqrt{d}})[h^{txt} \\oplus h^{img}])$\nwhere o denotes concatenation. M represents a hybrid attention mask same as in Transfusion (Zhou et al., 2024) with a causal mask applied to text tokens and a bi-directional mask applied to image tokens. This design allows for self-attention within and across modalities, encouraging cross-modality integrations."}, {"title": "Modality-specific feed-forward network", "content": "After the attention layer, we employ modality-specific FFNs to process text and image data separately. The pre-FFN layer normalization is also modality-specific and is folded in the FFN functions.\n$h^{txt}_{FFN} = FFN_{text}(h_{out}^{txt})$\n$h^{img}_{FFN} = FFN_{img}(h_{out}^{img})$"}, {"title": "Output projection", "content": "Finally, after N layers of self-attention and FFNs, the resulting hidden states are projected either to logits in text via language model's output layer, or to predicted noise in image via a U-Net upsampler.\n$P_{logits} = LM\\_Head_{text}(h_{FFN}^{txt})$\n$\\epsilon_{pred} = UNet\\_Up^{img}(h_{FFN}^{img}, t, h_{in}^{img})$\nSame as Transfusion, the output Plogits and Epred are passed through the language modeling loss (Equation 1) and DDPM loss (Equation 3) respectively. All parameters in the text modules along with self-attention and FFN parameters in the image modules are initialized from the pretrained Llama model. During optimization, we decouple the learning rates for the text and image parameter groups: a text learning rate, Ntext, is used for {Projtext, QKVtext, Otext, FFNtext, LM-Headtext }, and an image learning rate, Nimg, for {UNet-Downimg,QKVimg, Oimg, FFNimg, UNet-Upimg}. To preserve the model's performance on text-only benchmarks, we use Ntext = 0 (freezing text modules) for our main experiments and explore different configurations in \u00a75."}, {"title": "Experiments", "content": "In this section, we describe the details of our training setup (\u00a74.1) and evaluation setup (\u00a74.3). Results in \u00a74.4 show that LlamaFusion outperforms Transfusion trained from scratch in the FLOPs match setting on text-only, image understanding and generation benchmarks."}, {"title": "Training Setup", "content": "Data Following Transfusion (Zhou et al., 2024), we use the same collection of 380M Shutterstock image- caption data, where each image is center-cropped and resized to 256 \u00d7 256 pixels. We order the captions before images (i.e., emphasizing image generation conditioned on texts) 80% of the time, and order the images before captions for the rest.\nModel Details For image tokenization, we use the same VAE encoder2 as Transfusion to compress an image of 256 \u00d7 256 pixels into a 32 \u00d7 32 \u00d7 8 tensor. These tensors are then passed into a 2-block U-Net downsampler (Ronneberger et al., 2015) to further reduce dimensions, resulting in a sequence of 256 patches (tokens). Both text-specific and image-specific Transformer modules are initialized from the pretrained Llama-3 8B model (Dubey et al., 2024). The U-Net downsampler and a corresponding U-Net upsampler are trained from scratch, together containing 0.27 billion parameters. Like Transfusion, LlamaFusion uses a maximum context length of 4096 tokens.\nOptimization In our main experiments, to preserve the language-only performance, we freeze the text modules (ntext = 0) while training only the image modules using an AdamW optimizer (\u03b2\u2081 = 0.9, \u03b22 = 0.95, \u20ac = 1 \u00d7 10\u22128) with a learning rate Nimage = 1 \u00d7 10-4. The learning rate follows a cosine decay schedule with a 4000-step warmup period before gradually decreasing to 1.5 \u00d7 10\u22125."}, {"title": "Controlled Comparison with Transfusion", "content": "Our key model comparisons are with the original Transfusion 7B model (Zhou et al., 2024), which was trained for 250K steps on 0.25T language-only tokens (text data) and 0.25T image-captions tokens (image data).\nSince we freeze the text module during training, we can exclude text data from our training process while maintaining language capabilities. This design choice allows us to explore two training configurations for a controlled comparison with Transfusion: In the first configuration, we match the amount of 0.25T image data used by Transfusion while leaving out the text data. As a result, this variant of Llama Fusion uses approximately half the total FLOPs of Transfusion. In the second configuration, we match Transfusion by using the same total FLOPs,\nAdditionally, for the language-only tasks, we report the performance of Llama-3 8B model to demonstrate that our model is able to maintain its strong text performance."}, {"title": "Evaluation Setup", "content": "Following Transfusion, we evaluate LlamaFusion on language-only, image understanding, and image generation tasks.\nLanguage-only We evaluate the model's language abilities using four tasks from the standard Llama evaluation suite (Dubey et al., 2024), including Hellaswag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and WinoGrande (Sakaguchi et al., 2021). We report accuracy on these benchmarks."}, {"title": "Results", "content": "Table 1 compares two variants of LlamaFusion against Transfusion. On language-only benchmarks, Llama Fusion keeps the strong performance of Llama-3 since we freeze all text modules. For image understanding, LlamaFusion substantially surpasses Transfusion, with a 20% improvement. In image generation tasks, Llama Fusion also shows superior results in both FID and CLIP scores.\nIn Figure 3, we benchmark the performance of LlamaFusion and Transfusion throughout the training. 4 We observe a consistent advantage of LlamaFusion over Transfusion during the entire training, for image captioning and generation. These results suggest that LlamaFusion effectively leverages the pretrained language modules from Llama while developing strong image abilities. Although Llama Fusion has twice as many parameters as Transfusion, it uses same FLOPs since only half of the parameters are activated for each input token from an arbitrary modality."}, {"title": "Analysis", "content": "Central to LlamaFusion is our modality separation techniques, which employs the design of modality-specific modules and decoupled learning rates for language and image modules. Our architectural ablation (\u00a75.1) demonstrates the importance of the design for maintaining model performance across both modalities. Additionally, we showcase LlamaFusion's ability to generalize to image-to-image generation through image editing tasks, which require simultaneous understanding of both input images and textual prompts (\u00a75.2). We further showcase that this recipe could be used for adapting"}, {"title": "Architecture Ablations", "content": "To evaluate different design choices, we conduct ablation studies using small-scale variants of Llama Fusion. Our analysis focuses on the impact of modality separation by comparing three designs: (1) no separation (a single dense model), (2) shallow separation (using modality-specific FFNs only), and (3) deep separation (using both modality-specific FFNs and attention mechanisms, our final LlamaFusion).\nNo separation (dense model) We begin our experiments with the dense Llama-3 8B model trained using the Transfusion recipe. This dense model maintains a unified structure where most components are shared across modalities (a single set of QKV, O and FFN process both texts and images), with the exception of U-Net upsampler and downsampler. For training, we use a text learning rate (ntext) for the components"}, {"title": "Results", "content": "Naive finetuning of dense pretrained LLMs for multimodal generation compromises their original language capabilities. When directly finetuning Llama-8B (no separation) using the Transfusion recipe, we observe significant performance trade-offs between image and text capabilities (Figure 4). With equal learning rates for text and image components (Ntext = 1), the model shows continuous improvement in image understanding and generation. However, this comes at a substantial cost to language capabilities, with performance on HellaSwag"}, {"title": "Image editing", "content": "Llama Fusion, our unified multimodal generative model, is naturally well-suited for tasks involving interleaved data types, such as image editing. Following Transfusion, we finetune LlamaFusion on the same dataset of 8K image editing examples, each consisting of an original input image, a prompt detailing the desired edit, and a resulting image that reflects the specified changes. In Figure 7, we apply the finetuned Llama Fusion to input images and editing prompts from the MagicBrush (Zhang et al., 2024) test set. Qualitative results demonstrate that LlamaFusion performs effectively in these image-editing scenarios, complementing its strong capabilities in text-only, image understanding, and image generation tasks."}, {"title": "LLaVA Fusion: extending LlamaFusion to vision-language models", "content": "Llama Fusion continues training the language-only pretrained LLM Llama with the Transfusion recipe. Can this recipe be extended to on vision-language models (VLMs) such as LLaVA (Liu et al., 2024d,c) and Qwen-VL (Bai et al., 2023) as well? In this section, we extend the recipe of Llama Fusion to VLMs, preserving"}, {"title": "Related Work", "content": "Unified Models for Multimodal Generation Recent work has extensively explored unified frameworks for multimodal generation, including text generation, image understanding, and image generation. While texts are commonly represented as discrete tokens across models, approaches to representing images especially for image generation-vary significantly. For instance, methods in (Lu et al., 2022; Yu et al., 2023; Lu et al., 2024; Team, 2024; Xie et al., 2024; Wu et al., 2024), represents images using vector-quantized discrete tokens (Van Den Oord et al., 2017; Esser et al., 2021; Lee et al., 2022). An alternative method, adopted by (Sun et al., 2024; Ge et al., 2024), employs continuous embeddings that require a separate diffusion model for decoding. In this work, we build upon Transfusion (Zhou et al., 2024), which integrates autoregressive generation for texts with diffusion for images within a single, end-to-end model.\nModel Sparsity Model sparsity through Mixture of Experts (MoE) (Shazeer et al., 2017; Muennighoff et al., 2024; Fedus et al., 2022; Lepikhin et al., 2020) has proven highly effective in improving LLM training efficiency. This approach has recently been extended to multimodal models (Shen et al., 2023; Lyle and Pascanu, 2024; Lin et al., 2024a; He et al., 2024a), particularly to address potential conflicts between different modalities. For example, recent efforts (Chen et al., 2023; Lin et al., 2024b; Wang et al., 2021, 2022) replace standard Transformer FFNs with modality-specific experts, enabling separate processing paths for different modalities. Our work takes this concept further by using modality-specific attention mechanisms. Concurrent work (Liu et al., 2024a; Liang et al., 2024) demonstrates the effectiveness of this deeper separation in multimodal pretraining and image generation.\nReuse of LLMs in Multimodal Training Based on the strong language capabilities of LLMs, some recent models on multimodal generation initializes their models from pretrained, language-only LLMs. For example, (Ge et al., 2023; Sun et al., 2023; Dong et al., 2023; Xie et al., 2024; Wu et al., 2024; He et al., 2024b) continued training upon the weights of language-only LLMs (Touvron et al., 2023) or vision LLMs without visual generation capabilities (Bai et al., 2023). The main focus of our work is to effectively reuse pretrained LLMs for multimodal generation, particularly with the Transfusion recipe, without any compromise on the LLMs' existing text-only capabilities."}, {"title": "Conclusion", "content": "We present LlamaFusion, a framework designed to equip LLMs with multimodal generative capabilities. By using Llama-3 for text generation and integrating parallel transformer modules for image diffusion, Llama Fusion efficiently reuses compute invested in pretrained LLMs.\nLlama Fusion's modular design enables independent developments of language and vision modules, de-risking the complexities associated with a large-scale, joint-modality pretraining. While LlamaFusion is currently built upon text-only LLMs, it can benefit further from existing visual understanding LLMs (Liu et al., 2023; Dai et al., 2023; Liu et al., 2024b; Zhu et al., 2024), inheriting the strong multimodal understanding ability while enabling generating interleaved text and visual content."}]}