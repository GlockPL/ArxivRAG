{"title": "ChatLogic: Integrating Logic Programming with Large Language Models for Multi-Step Reasoning", "authors": ["Zhongsheng Wang", "Jiamou Liu*", "Qiming Bao", "Hongfei Rong", "Jingfeng Zhang"], "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are available at https://github.com/Strong-AI-Lab/ChatLogic.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in large language models (LLMs) such as ChatGPT-3.5, GPT-4 [1], and Llama2 [2] have significantly enhanced their capabilities in various industries, proving invaluable in solving complex real-world problems. These models revolutionize sectors like customer service, healthcare, and education through their nuanced contextual comprehension and advanced conversational abilities. However, LLMs face notable challenges in multi-step logic reasoning tasks.\nWhile these models excel in content generation, they consistently struggle to produce coherent responses in tasks requiring multi-step reasoning. Their training methodology, primarily based on the 'next-token prediction' approach, limits their ability to apply logical rules and deep contextual understanding essential for such tasks. For example, Figure 1 shows how to let LLMs find a reasonable explanation path as the judgment result of the problem in the known randomly disrupted proposition sequence. This represents a critical area for improvement in current LLMs.\nFurther complicating this issue is the inherent token limitation of LLMs, which becomes apparent in continual dialogues [3]. The token caps in models like GPT-3.5 and GPT-4, while extendable through engineering prompts or technologies like Recursive Model Training [4], still pose a significant constraint. This limitation is particularly pronounced in multi-turn conversations, a common feature in multi-step logic reasoning tasks.\nTo address these limitations, innovative approaches such as external memory augmentation are being explored [5]. This method involves integrating LLMs with extensive databases to enhance their reasoning capabilities [6]. However, this integration brings its own challenges, such as the potential embedding of biases from the retrieval models into the LLMs [7], which could affect their accuracy and stability."}, {"title": "II. RELATED WORK", "content": "Our work introduces ChatLogic, a framework that augments LLMs with a logical reasoning engine to enhance their infer- ential capabilities. We have innovatively implemented a \u2018Mix- shot Chain of Thought' technique in this framework. This approach significantly enhances the performance of LLMs by combining various prompt engineering methods. Mix-shot CoT efficiently guides the model through logical reasoning steps, achieving improved problem-solving with minimal re- source consumption. ChatLogic is designed to be compatible with existing LLMs, significantly increasing their accuracy, especially in high-precision scenarios. The framework orches- trates the functioning of an LLM, enabling it to efficiently generate responses across various tasks.\nAt the heart of ChatLogic is the transformation of natural language into logical symbols, a process executed through pyDatalog. The primary objective of ChatLogic is to reinforce the stability of the reasoning process, ensuring that LLMs can handle intricate reasoning tasks with enhanced reliability and precision. The main characteristics of our framework are summarized below:\n\u2022 ChatLogic, by combining LLMs with pyDatalog, trans- lates natural language queries into logic programs, en- hancing inference accuracy. This improvement is notably evident in multi-step reasoning tasks, as demonstrated on datasets such as PARARULE-Plus\u00b9, CONCEPTRULES V1, and CONCEPTRULES V2.\n\u2022 ChatLogic mitigates information loss, effectively address- ing the long sequence limitation prevalent in adopting LLMs for multi-step reasoning tasks.\n\u2022 ChatLogic incorporates automated enhancements for logic program execution, including a syntax correction module. This module refines a logic program by learning from previous executions, significantly improving the practical application and effectiveness of the generated code.\nLLMs are considered to have reasoning abilities similar to human cognition [8]. Despite facing challenges in multi- step logical tasks involving contemporary information or com- plex logical sequences [9], emerging approaches like self- consistency [10] show promise in enhancing performance, particularly in areas such as arithmetic and common sense reasoning. The effectiveness of causal reasoning pathways [11] is also crucial, ensuring that the output of LLMs is accurate but also transparent and verifiable. However, the most impactful method is the Chain of Thought (CoT) [12], which reveals the intermediate reasoning steps used by these models in problem-solving, allowing for continual self-correction of logical thinking, thereby greatly enhancing the rationality of their reasoning capabilities. But they all more or less exposed the weakness of extracting effective content from long and disordered information. We are trying to find ways to improve this shortcoming effectively.\nLLMs have demonstrated the ability to generate code in various programming languages to meet users' specific needs [13]. However, how to directly apply the generated code to actual environments remains an issue to be resolved. In terms of optimization, the SELF-DEBUGGING approach [14] leads the post-code generation phase. It endows LLMs with the ability to debug their output, reinforcing the theme of continual refinement in the generated code. LOGIC-LM [15] creates a deterministic symbolic solver that expresses reasoning in a specific symbolic format to obtain actual results. The existing flaw we hope to improve is that while LLMs may not fully understand demo samples or generate \"fantasies\" that deviate from reality due to lack of pyDatalog knowledge in the pre- training data, they can still simply \"imitate\" Produce high-precision output. Our ultimate goal is to generate code that perfectly meets the requirements, can be deployed directly locally, and get inference results through the user's computer with a basic Python language environment installed.\nPrompt engineering in LLMs functions akin to psychologi- cal suggestions, guiding the model towards specific predictions [16]. Few-shot learning emphasizes training models with the least labelled data for optimized task performance. Notably, models like GPT-3 can handle tasks with a few examples, comparable to fine-tuned models [17]. Enhanced by prompt engineering, their reasoning capabilities are magnified. Zero- sample prompt [18] completely relies on the model's huge intrinsic knowledge and training corpus and assumes full responsibility for solving the problem. This means that we do not need to make any downstream task-related fine-tuning of LLMs; we only provide task content and expect good perfor- mance of LLMs. Surprisingly, although guidance is limited, in many cases, it often produces results that exceed expectations. In addition, zero-sample CoT [19] is also considered to be the best inference prompt currently; by using a simple prompt: 'Let's think step by step', the specific prompt and the corre- sponding two-stage key answer extraction prompt technology has significantly improved multiple inference-related zero-shot tasks. surpassed previous zero-shot learning.\nAlthough not every situation using Zero-shot CoT results in optimal content output, in most cases targeting specific down- stream tasks, LLMs have shown potential in the current field to effectively perform tasks using small sample techniques and combining external enhancement symbols [20]. In ChatLogic, we create independent prompt templates for different links in the framework and call them independently. Preliminary results suggest a promising direction by emphasizing LLMs' inherent reasoning capabilities and combining them with basic symbolic rules."}, {"title": "III. TASK DEFINITION", "content": "In our experience with advanced LLMs such as Llama2 and GPT-4, we've noticed their impressive ability to convert text into formal structures like math equations [21] and programming languages [22]. However, these models sometimes struggle with complex, multi-step reasoning tasks. The chal- lenge escalates as the reasoning depth increases, and LLMs often miss key reasoning steps.\nAcknowledging these characteristics, our primary goal is to boost the capability of LLMs to effectively represent problems in logic programming languages, particularly pyDatalog. This Python library integrates the logic programming paradigm and is particularly useful for declarative reasoning and complex querying. It allows for sophisticated rule-based logic and in- ference to be seamlessly incorporated into Python applications, enhancing their capabilities for decision-making processes.\nWe specify the multi-step deductive reasoning problem as Fact, Rule, and Query. The data sets used in the paper experiments all have this format:\n\u2022 Fact: A Fact $F = \\{f_1, f_2\u2026\u2026,f_n\\}$ is a sequence of sentences, each declarative sentence $f_i$ having a subject, a predicate verb, and an object, like 'Bob is poor' and 'Dogs like cats'. Predicates have negative expressions.\n\u2022 Rule: A Rule $R = \\{r_1,r_2\u2026,r_n\\}$ is a sequence of declarative sentences with conditional judgment, includ- ing initial features and inferential features, like 'If some- one is poor then they are bad.'\n\u2022 Query: A Query Q is also a declarative sentence consis- tent with the sentence expression format.\nTo accomplish this goal, we aim to address the following two subtasks.\nOur preliminary objective is to exploit LLMs' one-shot gen- eralization and zero-shot thinking capabilities C (the collective name for both technologies). To achieve this, we aim to fa- miliarize the model with the intricacies of symbolic language, specifically utilizing pyDatalog. We introduce them to this language through meticulously crafted examples that cover all potential edge cases. The structured pyDatalog syntax and these detailed prompts example set S are provided to improve skill C and are essential to guide LLMs in understanding and handling multi-step inference.\npyDatalogCode = C(LLM(F, R, Q), S)\nThe code generated with high precision is executed by the local compiler to obtain accurate multi-step inference results.\nResult = Local Execution(pyDatalogCode)\nThe translation of LLMs from text to code [23] is often imperfect on the first try and may contain errors. We aim to design a specific module M in ChatLogic that ensures high-accuracy alignment of natural language and translation codes. The generated code should be easily executable locally, producing the desired results directly. Essentially, it further improves code generation quality so that it can be performed accurately and reasonably.\npyDatalogCode = M(C(LLM(F, R, Q), S'))"}, {"title": "IV. CHATLOGIC", "content": "This section provides a detailed overview of the ChatLogic framework, particularly emphasising the finer details of its constituent parts and our innovative strategy regarding Mix- shot CoT.\nThe ChatLogic framework comprises four primary phases: input processing, Semantic Correction, Syntax Correction, and local execution response, as meticulously illustrated in Figure 2. The entire process, from problem input to result output, is depicted in the image as a demonstration. It is worth noting that the initial version of the logic code generated by LLMs at the beginning, after continuous revision through two or more iterations within two modules (semantic and syntactic correction), as a direct code solution for multi-step reasoning Executability is significantly improved, resulting in more precise results. Moreover, the enhanced refinement of this code substantially bolsters its executability, contributing to improved system performance and accuracy. The blue and green boxes are the intermediate self-correction processes that appear in the semantic correction and syntax correction modules and are generated correspondingly by the ChatLogic framework during the experiment.\nAlgorithm 1 delves deeper into the comprehensive algorith- mic process for querying response data in ChatLogic. Apart from the locally executed part, all sub-tasks within ChatLogic are controlled and driven by LLMs acting as components. It consists of two loops, each corresponding to the content of two correction phases. We observe that LLMs excel at semantic corrections, and with limited modifications, correct text translations can be achieved. In lines 5 and 6 of the code, we employed zero-shot CoT to assist in determining the textual similarity of two propositions. Based on the judgment, we up- date the 'DifferentFlag' label, which influences the progression of the loop process of 'Semantic Correction'. However, syntax corrections are unreliable; they may get stuck in an infinite loop, repeatedly performing meaningless tasks. To address this issue, we consider introducing an upper loop limit. Although this somewhat diminishes ChatLogic's inferential capabilities, it significantly enhances the framework's robustness, making it better suited for multi-step deductive reasoning tasks.\nOur innovative mix-shot CoT (Chain of Thought) approach represents a groundbreaking hybrid methodology, blending the strengths of zero-shot CoT and one-shot learning to create a more versatile and effective learning paradigm for language models. As mentioned before, zero-shot CoT uses LLMs' generative capabilities and chain thinking patterns to complete natural language tasks without targeted training or fine-tuning. One-shot learning provides a well-written task completion sample manually to guide LLMs to imitate the same workflow to achieve high-precision task completion. At its core, mix- shot CoT leverages the language model's innate ability for autonomous sub-task identification; a character follows estab- lished patterns accurately es it with the precision of one-shot learning through strategically chosen demonstration examples."}, {"title": "V. EVALUATION", "content": "In this section, we conduct experiments to evaluate the ef- fectiveness of ChatLogic-augmented LLMs. Our experimental results show that ChatLogic+LLMs significantly outperform baseline LLMs, highlighting the advantages of using logical symbols to enhance the multi-step reasoning capabilities of LLMs.\nAll reasoning questions in PARARULE-Plus adhere to the closed-world assumption, totalling approximately 400,000 samples. It features linguistic information in two contextual scenarios: People and Animal. For this dataset, we conducted our experiment by randomly selecting 50 instances from each depth level in both the Animal and People categories, combining them to form a set of 100 test cases for each depth level, ranging from Depth=2 to Depth=5.\nIn addition to PARARULE-Plus, we also incorporate the CONCEPTRULES V12 and CONCEPTRULES V23 datasets in our study. These datasets contain samples that require multi-step reasoning, with depths up to 3, making them suitable for evaluating models' capabilities in complex reasoning tasks. They are available in both simplified and full versions. For each version of the CONCEPTRULES datasets, we initially consolidated all data from the train, test, and dev sets into a single pool. We randomly sampled 100 instances from this unified dataset for our tests.\nIn our experiments with ChatGPT, GPT-4, and Llama 2-7B, we aimed to establish a baseline for the reasoning capabilities of LLMs as documented in the literature [24]. A significant part of our study was the implementation of ChatLogic, a framework designed to enhance the accuracy of these models' reasoning. This involved testing configurations like ChatGPT vs. ChatLogic (ChatGPT) across uniform scenarios using instances from the PARARULE-Plus and CONCEP- TRULES datasets. The crux of our hypothesis is that if models augmented with ChatLogic demonstrate improved reasoning performance across various difficulty levels compared to their baseline, it would be a strong indicator of ChatLogic's effec- tiveness. Such results would suggest that ChatLogic could be a valuable addition to the field of artificial intelligence and natural language processing, affirming its utility in advancing the reasoning capabilities of LLMs.\nIn the ChatLogic framework invocation, ensuring the con- trollability of the text is paramount. For ChatGPT and GPT-4, the model invocation versions are respectively \u201cgpt-3.5-turbo\u201d and \u201cgpt-4\u201d, and both have the hyperparameter temperature set to 0 so that we can have more control over what the LLM outputs. For Llama 2-7B, using an NVIDIA GeForce RTX 3090 with 24GB of memory, we utilized the transformer- based version meta-llama/Llama-2-7b-chat-hf 4 provided by Huggingface."}, {"title": "VII. ABLATION STUDY", "content": "We have introduced two modules to improve code execution from semantic and syntax perspectives. To demonstrate their role in aiding LLMs' multi-step reasoning, we will separately assess how each module affects the code's executability. We anticipate a gradual increase in successful executions, which would validate the effectiveness of our approach.\nFor the PARARULE-Plus dataset, we increased our data sampling to 100 random samples from the entire dataset, ensuring that the selection was shuffled entirely, with no specific order applied to parameters such as 'Depth (2-5)' and 'Pattern (Animal & People)'. For the CONCEPTRULES V1 and V2 datasets, we thoroughly shuffled all data from the 'simplified' and 'full' versions of each dataset and randomly selected 100 samples. To assess the executability rate of the generated code, we deployed it locally, evaluating solely based on the absence of error messages and the correctness of the output content.\nThe results are presented in Table IV. In our comparison, we examined the enhancements in code execution rate achieved by LLMs with the gradual integration of different modules. Relative to the baseline, both modules demonstrated incremen- tal improvements in execution rates. The Syntax Correction module still proved valuable despite Llama 2 not utilizing as much code text for pre-training as GPT-3.5 and GPT-4. While it didn't significantly increase the execution rate, its contri- bution to refining code quality is noteworthy. Furthermore, it's important to highlight that GPT-4, due to its extremely advanced capabilities in some subtasks, has seemingly reached a 'performance ceiling' on the current datasets. This suggests that the dataset's limitations may somewhat constrain its potential."}, {"title": "VIII. LIMITATION AND FUTURE WORK", "content": "Through experimental evaluations on multiple mainstream LLMs, we observed that ChatLogic+LLMs outperformed native LLMs in performance. The impressive performance demonstrates the effectiveness of our work. However, some issues have also been exposed. PARARULE-Plus is based on the closed-world assumption in question-answering data. Additionally, datasets like CONCEPTRULES V1 and V2, which are also artificially constructed, lack natural linguistic expression, which may not fully represent real-world complex- ities. When confronted with more complex sentences in the context of an open-world assumption, importing, integrating, and inferring external knowledge information that is expressed differently still poses challenges. Despite the valuable results from our experiments in enhancing code reliability, it's essen- tial to acknowledge that the optimization module's applica- bility is currently limited to specific datasets. The carefully designed prompt sample cases are also optimized for specific data sets and are not a universal prompt template. Future developments should focus on creating adaptable optimization components [25] to address a wider array of scenarios and data sources."}]}