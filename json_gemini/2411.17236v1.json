{"title": "From Graph Diffusion to Graph Classification", "authors": ["Jia Jun Cheng Xian", "Sadegh Mahdavi", "Renjie Liao", "Oliver Schulte"], "abstract": "Generative models such as diffusion models have achieved remarkable success in state-of-the-art image and text tasks. Recently, score-based diffusion models have extended their success beyond image generation, showing competitive performance with discriminative methods in image classification tasks (Zimmermann et al., 2021). However, their application to classification in the graph domain, which presents unique challenges such as complex topologies, remains underexplored. We show how graph diffusion models can be applied for graph classification. We find that to achieve competitive classification accuracy, score-based graph diffusion models should be trained with a novel training objective that is tailored to graph classification. In experiments with a sampling-based inference method, our discriminative training objective achieves state-of-the-art graph classification accuracy.", "sections": [{"title": "1. Introduction", "content": "Recent breakthroughs with generative models have enabled outstanding performance in challenging tasks in various modalities such as image generation (Saharia et al., 2022), speech generation (Le et al., 2023), and natural language processing (OpenAI et al., 2024). For text, early GPT models (Radford & Narasimhan, 2018; Radford et al., 2019) showed that generative training not only provides excellent text generation performance, but can be competitive to models discriminatively trained in the downstream task. Later improvements showed that generative training significantly outperforms all alternative approaches (Brown et al., 2020; OpenAI et al., 2024). In the image domain, Li et al. (2023) show that text-to-image diffusion models hold promise for zero-shot classification on images without any additional discriminative training. Zimmermann et al. (2021) train score-based diffusion models for zero-shot classification and show competitive performance with SOTA discriminative models in the CIFAR-10 data set. Collectively, these previous works show how sophisticated generative models can be leveraged for classification tasks.\nWhile there has been extensive research on the applicability of generative models for the classification tasks on image and text domains, this question has remained unexplored in the graph domain. Therefore, it is natural to ask: Do generative models bring the same classification competitiveness to graphs? This work takes a step toward answering this question: we show that generative models are indeed strong baselines for graph classification. In particular, our contributions are as follows.\n\u2022 We find that purely generatively trained graph diffusion models do not perform well as zero-shot classifiers using exact likelihood for inference. We therefore develop a new discriminative training objective, based on the generative ELBO likelihood approximation, that leads to strong classification performance as well as high-quality graph generation.\n\u2022 Our base diffusion model is not permutation invariant. We show that both training and inference can be improved by randomly sampling adjacency matrices from the isomorphism class of the training/test graph.\n\u2022 We conduct extensive experiments on three training loss objectives, each paired with two different inference methods, using the K-Regular synthetic dataset and the real-world IMDB-BINARY and PROTEIN datasets. We identify an optimal combination of training objectives and inference methods that is efficient and achieves superior performance across our baselines."}, {"title": "2. Related Work", "content": "Graph Neural Networks and Graph Classification. Graph Neural Networks (GNNs) ((Wang et al., 2019; Xu et al., 2019a; Hamilton et al., 2017; Ying et al., 2018; Simonovsky & Komodakis, 2017)) have emerged as effective architectures to process graph-structured data. The core operation of a GNN layer involves updating the representation of each node. The new node representation is updated by aggregating the information from the neighbor of each node and its own node representation. The final node representations can be used for downstream tasks such as graph classification. Errica et al. (2019) provided a fair comparison of various graph classifiers ((Wang et al., 2019; Xu et al., 2019a; Hamilton et al., 2017; Ying et al., 2018; Simonovsky & Komodakis, 2017)). We adopt their evaluation process and framework (details in Section 6 and Appendix A.1.2).\nDiffusion Models. Diffusion models (Song et al. (2021); Ho et al. (2020); Kingma et al. (2023); Karras et al. (2022); Rombach et al. (2021)) are generative model that operate through a dual process comprising forward and backward steps. The forward process involves introducing noise into the data, with the noise level denoted by o(t) at each time step t. Conversely, the backward process aims to denoise the data, transitioning from a noisy state to a clean one. Variational Diffusion Models (Ho et al., 2020; Kingma et al., 2023; Karras et al., 2022; Rombach et al., 2021) model the discrete time backward process by learning the evidence lower bound (ELBO) of the backward process. Song et al. (2021) utilize continuous-time diffusion, based on Stochastic Differential Equations (SDEs), and demonstrate that the data likelihood function of a scored-based diffusion model can be accurately estimated by solving an ordinary differential equation. This type of diffusion has demonstrated significant success, particularly in the field of image processing.\nGraph generative diffusion models. Motivated by the success of diffusion models on image generation, several studies have been conducted to extend these models to the graph domain (Jo et al. (2022); Vignac et al. (2023); Kong et al. (2023); Chamberlain et al. (2021)). Vignac et al. (2023) propose an equivariant discrete denoising diffusion model to generate graphs. Kong et al. (2023) use a node-absorbing diffusion process for discrete graph space generation. Other graph diffusion models ((Corso et al., 2023; Igashov et al., 2022; Xu et al., 2022)) focus on molecular data and achieve remarkable results. Yan et al. (2023) introduce SwinGNN, a non-equivariant score-based diffusion model that achieves state-of-the-art results on several benchmarks. Given the strong generation performance of SwinGNN, we use this architecture as our backbone graph generative model, and show how to adapt the SwinGNN model for classification tasks.\nGenerative Classifiers. For non-graph i.i.d. data, it is well known that a classification model can be derived from a generative model (?). The advantages and disadvantages of a generative approach to classification have been extensively researched, both theoretically and empirically (Ng & Jordan, 2001). A key finding is that generative classifiers approach their maximum accuracy faster with fewer data-points than discriminative classifiers. Because graph data are much more complex than i.i.d. data, much work has gone into developing sophisticated generative models for graphs. It is desirable to leverage these models for graph classification. To our knowledge, ours is the first work to study the generative approach for graph classification.\nDiffusion generative classifier. One line of research studied how to adapt diffusion models for classification tasks in the image domain. Zimmermann et al. (2021) add the class label as a conditioning variable to score-based diffusion models and take advantage of the fact that the exact computation of likelihood is possible for score-based models to perform image classification tasks. Li et al. (2023) leverage trained text-to-image diffusion models (such as Stable Diffusion (Rombach et al., 2021)) to perform zero-shot classification. This method relies on estimating the class-conditional likelihood by computing an evidence lower bound (ELBO) on an image and its candidate labels. Our work is related to both works since we consider diffusion models for classification. It is different in 1) the modality we study (graph vs. image/text), 2) the loss function we use to train the diffusion model."}, {"title": "3. Background on Graph and Diffusion Models", "content": "We introduce background and notation for graphs and for graph diffusion models."}, {"title": "3.1. Graph", "content": "A graph is a collection of nodes (or vertices) and edges connecting pairs of nodes, often used to model relationships and structures in various scientific fields or social networks. In this work, we assume that all graphs are undirected and unweighted. Formally, a graph G is defined as an ordered pair G = (V, E): V is the set of vertices, such as V = {U1, U2,..., Un}. E is the set of edges, where each edge is an unordered pair {vi, vj} in the case of an undirected graph.\nThe adjacency matrix A of a graph G is an n \u00d7 n matrix where n is the number of nodes. The element Aij is 1 if there is an edge between the nodes vi and vj, and 0 otherwise. The node attribute matrix X is an n x d matrix where each row corresponds to a node and the columns represent d attributes associated with that node. A permutation of a graph involves rearranging the nodes of the graph according to some permutation. This is represented mathematically by a permutation matrix P, a square n \u00d7 n matrix where each row and each column contains exactly one entry of 1 and all other entries are 0. The permuted adjacency matrix A' and the permuted node attribute matrix X' is given by:\nA' = PAPT and X' = PX. These transformations ensure that the structural and attribute properties of the graph are preserved under the node permutation, providing a powerful tool for analyzing graph isomorphisms and symmetries."}, {"title": "3.2. Diffusion Models", "content": "Diffusion model involves a forward process and a reverse process. The forward process adds time-dependent noise to data, which transforms the ground true unknown distribution q(x0), through a Markov process X0, X1,..., XT-1, into a easy-to-sample distribution q(x\u012b), usually multivariate standard Gaussian. The ground truth distribution is thus modelled as\n$q(x_0) = \\int [q(x_t)\\prod_{t=1}^T q(x_{t-1}|x_t)]dx_{1:T}.$\nThere are two basic types of diffusion models, which different motivations, but essentially equivalent final forms. A Variational Diffusion Model (VDM) models q(x0) with a forward process q(xt-1|xt) and a learned reverse process P\u03b8(xt-1|xt). The evidence lower bound (ELBO) associated with a VDM\n$log q(x_0) \\ge E_q [log \\frac{P_\\theta (x_{0:T})}{q(x_{1:T}|x_0)}].$\nFollowing previous work, this ELBO can be transformed into the training loss objective\n$E_{q(x_0),t~U[0,1],p_\\theta (x|x)} [||D_\\theta(x,t)-x||_F^2],$\nwhere q(x0) is the ground truth data distribution, x is the noisy data, p\u03b8(t) (x|x) is the noisy data distribution at time step t under noise level function \u03c3(t), the notation D\u03b8(x, t) denotes the denoising network and ||.||F is the Frobenius norm.\nThe other type are Score-based models (Song et al., 2021), which ususally define the forward and backward process via stochastic differential equations (SDEs). The equations usually have the following form:\ndx+ = f(x, t) dt + g(xt, t) dWt\ndx- = [-f(xt,t)+g(xt, t)\u00b2\u2207x\u2081 log pt(xt)] dt+g(xt, t) dW+\nwhere dx+ refers to the forward process, dx\u012b refers to the backward process, f (xt, t) is the drift coefficient, g(xt, t) is the diffusion coefficient, and dWt denotes the standard Wiener process. It learns to approximate the gradient of the log-probability of the denoising process. The training loss objective for score-based diffusion models is defined as follows:\n$E_{q(x_0),q(x_t|x_0)} [|| \\nabla_{x_t} log q(x_t|x_0) - S_\\theta(x_t, t) ||^2]$"}, {"title": "3.3. Graph diffusion Model", "content": "Graph diffusion models extend diffusion models to the graph domain. An example is DiGress (Vignac et al., 2023), which operates on discrete node types (like atom types C, N, O) and edge types (such as single, double, or triple bonds). In DiGress, introducing noise into a graph involves multiplying by a transition matrix that facilitates transitions between types. Another approach is GeoDiff (Xu et al., 2022), which applies diffusion to molecular graph data represented in 3D coordinates, gradually transforming them into Gaussian noise during the forward process. The reverse process denoises a random sample into a valid set of atomic coordinates. Our target task is graph classification, so deploy a graph diffusion model that naturally facilitates the likelihood computation for a test graph, similar to the generative classifiers described for the image domain by (Song et al., 2021; Zimmermann et al., 2021). The analogy to image generation motivates using SwinGNN (Yan et al., 2023), which treats graphs akin to images. Given the adjacency matrix A of a graph G, SwinGNN transforms A into another adjacency matrix. The transformed adjacency matrix then undergoes a forward and backward stochastic process. Treating a graph as an image naturally allows for exact likelihood computations. The choices for the noise function in this diffusion model are \u03c3(t) = 0 and \u03c3(t) = t, where the corresponding drift term is f(A,t) = 0 and the diffusion coefficient is g(t) = \u221a2\u03b4(t)\u03c3(t). In the graph domain, Equation 4 and Equation 5 of the Stochastic Differential Equation (SDE) is given by:\ndA+ = \u221a2tdW,\ndA+ = \u22122t\u2207 a log pt(A)dt + \u221a2tdW\nSimilar to SwinGNN and EDM (Yan et al., 2023; Karras et al., 2022), we adopt network preconditioning to improve training dynamics, and repamatrize the denoising network D\u03b8 as F\u03b8.\nThe resulting training loss is\n$E_{\\sigma,A, \\tilde{A}} [|| F_{\\theta}(D_\\sigma(\\tilde{A}, \\sigma)) - A||_F^2]$\nwhere A is the noisy adjacent matrix, \u03c3 is the noisy level. For simplicity of notation and implementation, we use a wrapper D\u03c3 for the preconditions and reparameterization.\n$D_\\sigma(\\tilde{A}, \\sigma) = c_s(\\sigma) \\tilde{A} + c_\\sigma(\\sigma)F_\\theta(c_i(\\sigma)\\tilde{A}, c_n(\\sigma))$\nThen the training loss objective becomes similar to a common VDM objective (for more details please see the Appendix A.1.1):\n$E_{\\sigma,A,\\tilde{A}} [||F_\\theta(D_\\sigma(\\tilde{A}, \\sigma)) - A||_F^2]$"}, {"title": "4. Training Graph Diffusion Models for Graph Classification", "content": "Let M := {(G(i), y(i)) | 1 \u2264 i \u2264 m} be a training dataset of size m, where the i-th example consists of an input graph G(i) and a discrete label y(i) \u2208 {1,2,...,C'}. Our main goal is to build a generative classifier to classify graphs. To achieve this, we introduce a novel training objective and model checkpoint selection method, which we explain in this section.\nTraining Objective. Consider a graph-label pair (G, y) \u2208 M. Let A be the adjacency matrix of the observed graph G. Using Bayes' theorem, we can derive a graph class probability from a class-conditional graph probability:\n$p(y|A) = \\frac{p(A|y) p(y)}{\\sum_i p(A|y_i) p(y_i)} = [Softmax(L)]_y$\n$L_j := ln p(A|y_j) + ln p(y_j) for all i \u2208 {1, 2, . . ., C'}$\nEquation (14) follows from Equation (13), with Softmax denoting the softmax function. Uniform class priors p(yj) can be omitted from Equations (13) and (14).\nWe investigate three plausible training objectives for a generative classifier model."}, {"title": "5. Classification Model", "content": "Let G be a graph to be classified with adjacency matrix A. We use the Bayes' theorem formula in Equation (13) to derive class probabilities p(y|A) from class-conditional probabilities p(A|y). We investigate two basic methods for estimating p(Aly).\nApproximate Inference. Approximate Inference uses the variational approximation to the class-conditional graph log-likelihood, as in the LCLF of Equation (16):\n$In p(A|y) \u2248 -L_{CLF}(A, y, \u03b8)$\nExact Inference One of the strengths of a score-based diffusion model is that exact likelihood computation is possible. In the image domain, Zimmermann et al. (2021) show how a trained class conditional score-based diffusion model with SDE can be used as zero-shot classifiers with impressive classification accuracy. Adapted from Equation 7 and Equation 8, given a trained class-conditional SwinGNN D\u03b8, the exact likelihood computation based on ODE is as follows:\n$In p(A|y) = ln p_T A+\\int_{0}^{T} f_\\theta(A(t), t, y) dt -\\int_{0}^{T} \\nabla \\cdot f_\\theta(A(t), t, y) dt,$\nwith\n$f_\\theta(A(t),t,y) = \\frac{A(t) - D_\\theta(A(t),t,y)}{t},$\nwhere A is the adjacency matrix of the input graph, A(t) is the adjacency matrix at time t of the stochastic process, y is the graph class label and D\u03b8(\u00c3, y, t) is the output of the class-conditioned denoising network.\nTest-time Data Augmentation with Random Permutations Recall that the SwinGNN model is not permutation-invariant. We can view the class probability of a graph as the expectation of the class probability of the adjacency matrices that represent it: p(G|y) \u2248 EA\u2208\u03a0(G)[p(A|y)] where II(G) is the isomorphism class of the graph's adjacency matrices. Our use of permutations during training can be viewed as approximating the training graph probability by sampling from the isomorphism sets of the training graphs.\nAt test time, we propose to utilize a similar permutation trick, and predict the class label based on several permutations of the graph. The predictions from different permutations are combined via majority vote to output the final solution. The majority vote avoids sensitivity to outlier permutations that might produce extreme probabilities. The exact working mechanism of permutation sampling is shown in Algorithm 1. We apply permutation sampling with both approximate and exact inference."}, {"title": "6. Experiments", "content": "Setup. We use SwinGNN as our backbone model, the choices of the diffusion schedule and parameter of the models is aligned with previous work (Karras et al., 2022; Yan et al., 2023), see Section 3.3 and Appendix A.1.1. All results are done using the 10-fold split same as (Errica et al., 2019). The values are reported as the mean of the 10-fold values, and the standard deviation are reported as the standard deviation of those values. More detail in Appendix A.1.2.\nModel Checkpoint Selection. Our training approach includes a model checkpointing strategy: For each fold, we train the model for a predetermined number of epochs T, saving the model after every 100 epochs. We then choose the best model based on its accuracy on the validation set. However, evaluating the validation set accuracy through exact inference (described in Section 5) involves solving ODEs and is consumes a significant amount of computation time. For example, assessing the accuracy of a single checkpoint on the IMDB-B dataset's validation set takes about 20 minutes per fold. While it would be beneficial to evaluate numerous checkpoints, the substantial time requirement restricts us to a limited selection. Conversely, evaluating the variational approximation loss LCLF from Equation (16) is considerably quicker, as approximate inference is much less time-intensive compared to solving ODEs. For instance, calculating the classification loss on the validation set takes approximately 0.5 seconds. This efficiency results in a speedup of over 2000 times in checkpoint selection compared to ODE solving. Therefore, using approximate inference allows for the evaluation of many more checkpoints, facilitating the selection of the most effective model based on performance on the validation set.\nDatasets. We consider three datasets (1) a synthetic K-regular graph dataset (i.e., in each graph all nodes have the same degree K). Graphs fall into a category of 4-regular and 6-regular, and the task is a binary graph classification task. (2) IMDB-B (Yanardag & Vishwanathan, 2015) dataset, which consists of ego-graphs of IMDB actors/actresses. In each graph, two nodes are connected if their corresponding actors/actresses have occurred in a movie. The task is binary classification of graphs into two Genres of Action and Romance. IMDB-BINARY (FEATURES) is the IMDB-BINARY dataset that each node has the number of degrees as their features. (3) PROTEINS (Borgwardt et al., 2005) dataset, a binary classification dataset consist of proteins that are classified as enzymes or non-enzymes. For IMDB-B and PROTEINS, we use the same split as Errica et al. (2019) to ensure a fair comparison with the GNN baselines.\nBaselines. For IMDB-BINARY, we include the same baselines as Errica et al. (2019).\nOur three training objectives (Section 4) and two inference methods (Section 5) define a space of 6 possible designs. One of these designs is a zero-shot classifier baseline that has been previously applied to images. The combination of using LDEN as a training objective with exact inference for classification is analogous to the approach of (Zimmermann et al., 2021) for image classification."}, {"title": "6.1. Results", "content": "Generative classifier is competitive with discriminative baselines. Table 1 shows the comparison of our method with various baselines. As shown in the Table, For IMDB-BINARY without node features, our method achieves better average compared to the best discriminative model (Graph-SAGE), while showing higher variance. When we train with node feature, our method shows both better average and lower variance then previous SOTA (GIN). This competitive performance shows promise in generative models for classification tasks.\nMore inference-time permutations improve test accuracy. Table 2 demonstrates how increasing the number of permutations increase accuracy on all three datasets experiment, where we achieve from 2% to more than 10% gain across the board by increasing the number of permutations from 1 to 5. Furthermore, Figure 4 shows the clear trend in accuracy gain with higher number of permutations, although the gains get saturated at a certain threshold (100 in our case). Both results show the benefit of inference-time permutation for classification.\nApproximate Inference pair with LCLF is the best classifier combination. Table 2 also shows that among three training objective strategies (LCLF, LDEN, LSUM), using the LCLF pair with Approximate Inference outperforms all other methods. It achieve better results for all four datasets and all other baslines."}, {"title": "6.2. Ablation Studies", "content": "Training Permutation. We also perform an ablation study to evaluate the impact of Training Permutation on model performance. We assess the validation set accuracy trends during training for models trained with and without Training Permutation on the IMDB-BINARY dataset using the LCLF loss objective, as depicted in Figure 3. The results indicate that models trained with permutation achieve higher accuracy more rapidly and maintain superior performance compared to their non-permuted counterparts, corroborating our findings in Table 2. Nonetheless, the benefits of Training Permutation are not universally consistent; further investigations into its effects across different settings are detailed in Appendix A.2.1.\nInference Permutation. While Table 2 indicates that test accuracy generally improves with an increased number of permutations, the enhancement is not consistently linear as the number of permutations grows. Consequently, we investigate how varying the number of permutations impacts performance on the IMDB-BINARY dataset. The results for Approximate Inference are illustrated in Figure 4. The plots reveal that a higher number of permutations does not detrimentally affect performance; however, excessively large permutation counts may be redundant, as performance gains become marginal. It is important to note that the specific effects are highly dependent on the data split and the dataset used. For results pertaining to Exact Inference, refer to Appendix A.2.2."}, {"title": "7. Conclusion", "content": "Our study demonstrates the significant potential of generative models for graph classification tasks, thereby expanding the applications of generative approaches beyond the text and image domains explored so far. We presented a novel training objective that preserves generative capabilities while enhancing classification performance. Our proposed inference technique, involving graph random-permutation majority voting, was shown to improve classification accuracy. Furthermore, we addressed the challenge of utilizing exact inference for score-based diffusion models during training by utilizing a variational likelihood approximation, which allows more efficient and more powerful model selection. We found that permutation-based approximate inference offers an efficient training and testing method with state-of-the-art graph classification accuracy.\nA valuable direction for future work would be to consider fine-tuning a generatively trained diffusion model with our discriminative training objective. Fine-tuning combines (some of) the efficiency advantages of zero-shot generative classifiers with the accuracy of discriminatively trained generative classifiers.\nGiven how well established generative classifiers are for non-graph i.i.d. data, we believe that generativegraph classification is a promising new approach that will inspire new methodologies that take advantage of the unique strengths of graph generative models."}, {"title": "A. Appendix", "content": "A.1. Experiment\nA.1.1. DIFFUSION SET UP\nFor the backbone model of our experiments, we use the exact same architecture as SwinGNN (Yan et al., 2023), except for the class conditioning, we add an embedding layer for each label. The class label embedding would sum up with the noise embedding before passing to each layer. And then, it would go through a reparamerterzation discussed in (Karras et al., 2022), specifically, we have the objective function\nwhere\n$c_s(\\sigma) = \\frac{\\sigma^2}{\\sigma^2 + \\sigma_0^2},$\n$c_o(\\sigma) = \\frac{\\sigma \\sigma_0}{\\sqrt{\\sigma^2 + \\sigma_0^2}},$\n$\\tau(\\sigma) = \\frac{1}{\\tau_0(\\sigma)^2},$\n$c_n(\\sigma) = \\frac{1}{4 \\sqrt{\\tau(\\sigma)}},$\n$\\lambda(\\sigma) = \\frac{1}{\\tau_0(\\sigma)^2},$\n$\\sigma_0 = 0.5,$\n$ln(\\sigma) \\sim N(\\rho_{mean}, \\rho_{std}),$\n$\\rho_{mean} = -1.2,$\n$\\sigma_{min} = 0.002,$\n$\\rho = 7,$\n$S_{min} = 0.05,$\n$S_{max} = 80,$\n$S_{noise} = 1.003,$\n$S_{churn} = 40,$\n$N = 256,$\n$t_i = (max(\\frac{i}{N}, \\tau_{min})^{-\\frac{1}{2}} + \\sigma_{max})^-2,$\n$\\gamma_i = \\{\n1 if $S_{min} i \\leq S_{max},$\nmin(S_{churn}, \\sqrt{\\frac{2}{\\gamma_i^2}}-1) otherwise,$\n}\nA.1.2. DATASETS AND EXPERMENT DETAIL\nFor the K-regular dataset, we generate 500 4-regular graphs and 6-regular graphs. We use a split ratio of 1/1/8 for the test/validation/train sets for this synthetic dataset, and a training of 3 hours lead to us a perfect classification accuracy.\nFor IMDB-BINARY and PROTEINS, we use the same split as Errica et al. (2019), where we also use the 10-fold CV for model selection and evaluation. IMDB-BINARY with feature is IMDB-BINARY dataset with degree attach to each node.\nSince SwinGNN treat the graph as an image using its Adjacency matrix and Node attribute matrix, the model becomes more resource-intensive when the graphs grow in size, which causes efficiency issues to the model hyperparameter selection. For example, the largest graph in the Protein dataset consists of 620 nodes, but the average number of nodes is 39.1. If we set our input dimension as 620, we need to pad all input graphs to 620 nodes, resulting in an Adjacency matrix of size 620 * 620. The SwinGNN model then needs to process all input graphs in the size of a 620 * 620 resolution image, which is highly inefficient, both in terms of training and inference. To overcome this issue, we select the appropriate size to be able to fit most graphs in the dataset. We omit the graphs with more nodes than our predefined cutoff. To be fair with the other methods, in inference time, for each graph in the testing and validation set that we omit, we consider the instance as an incorrect classification. This would give us a lower bound of our models test accuracy, ignoring its potential classification ability of classify large graph, in trading of efficiency. Specifically, for IMDB-BINARY, we use 128 nodes as the maximum number of nodes; For PROTEINS, we use 192.\nAs training generative model is more expensive and especially in our case, which we randomly permute the training set resulting the cost of training the model more expansive, we slightly modify some of the setting. This would only make our performance sub-optimal in trade of time and resource. Specifically, We do not perform any hyper-parameter tuning for each"}, {"title": "A.2. Permutation Experiments", "content": "A.2.1. TRAINING PERMUTATION\nNote that since it is unfair and would give bad performance for models train without permutation to inference with permutation, we modify the behaviour in inference time for training without permutation. Instead of permuting p times, they just classify p graphs that is randomly add noise according the the training noise distribution. This process aligned with the work in (Li et al., 2023)\nWe also carried out a training permutation experiment with LDEN and LSUM on the IMDB-BINARY dataset, as shown in Figure 5 and Figure 6.\nUnlike the curve in Figure 3, which training with permutation consistently outperforms training without permutation. Figure 6 shows comparable performance, but training without permutation achieves better accuracy faster. Figure 5, on the other hand, shows that training without permutation is better. Overall, LCLF with permutation still performs the best. Their performance is poor and may result from the model having trouble leaning the features that could help in classification. Making permutation in this case makes these learning harder. This suggests that for other loss objectives, training with permutation may not always be helpful in classification,\nA.2.2. TESTING PERMUTATION\nWe also show the inference time permutation experiment on IMDB-BINARY dataset, as shown in Figure 7. We could see that, for all three ways of training objectives LDEN, LCLF, and Lsum, using the exact inference w with increasing number of permutations could benefit. Consistent with message we get from to the case in Figure 4, it may not be beneficial to have a higher number of permutation numbers. Unlike in Approximate Inference, Exact Inference takes much more computation resource for the ODE solver (which is repeated called our neural network), it is hard to afford a permutation number as high as hundreds for Exact Inference."}]}