{"title": "LORA DONE RITE: ROBUST INVARIANT TRANSFORMATION EQUILIBRATION FOR LORA OPTIMIZATION", "authors": ["Jui-Nan Yen", "Si Si", "Zhao Meng", "Felix Yu", "Sai Surya Duvvuri", "Inderjit S. Dhillon", "Cho-Jui Hsieh", "Sanjiv Kumar"], "abstract": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLMs that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, which leads to weight updates that depend on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which achieves transformation invariance while being computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements over existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yields 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).", "sections": [{"title": "1 INTRODUCTION", "content": "Low-Rank Adaptation (LoRA) (Hu et al., 2022) is a popular parameter-efficient method for fine-tuning Large Language Models (LLMs). By freezing the pretrained weights and injecting trainable low-rank matrices into each layer, LoRA significantly reduces memory requirements and mitigates overfitting in limited data settings. More formally, letting $W \\in \\mathbb{R}^{m\\times n}$ be a weight matrix in an LLM, LORA freezes $W$ and introduces a low-rank matrix $Z$ added to $W$, where $Z$ is represented by the multiplication of two rank-r matrices $A$ and $B$, i.e.,\n$Z = AB^T \\in \\mathbb{R}^{m\\times n}, A \\in \\mathbb{R}^{m\\times r}, B\\in\\mathbb{R}^{n\\times r}, r < \\min(m, n)$.\n(1)\nThe matrices $A$ and $B$ will be referred to as LoRA factors in this paper. Recent research has explored numerous variations and improvements over the classic LoRA algorithm (Valipour et al., 2023; Zhang et al., 2023b; Liu et al., 2024; Yaras et al., 2024).\nDespite being widely used in practice, we find that applying standard optimizers to LoRA leads to updates that are not \u201ctransformation invariant\u201d. By definition of LoRA in (1), the same update $Z$ can be decomposed in multiple ways, i.e., $Z = A_1B_1 = A_2B_2$. Ideally, an optimizer should yield the same update to $Z$ regardless of the specific factorization. However, commonly used optimizers with diagonal preconditioners like Adam (Kingma & Ba, 2014), Adagrad (Duchi et al., 2011), RMSProp"}, {"title": "2 TRANSFORMATION INVARIANCE FOR LORA OPTIMIZATION", "content": "We now introduce the concept of transformation invariance in LoRA training and demonstrate that most existing optimizers, when applied to LoRA, do not satisfy this property. This deficiency leads to inefficient learning in practice."}, {"title": "2.1 DEFINITION OF TRANSFORMATION INVARIANCE", "content": "As introduced in formulation (1), LoRA adds a low-rank matrix $Z = AB^T$ to the original weight matrix $W$ and learns the LORA factors $A \\in \\mathbb{R}^{m\\times r}, B\\in \\mathbb{R}^{n\\times r}$ to minimize the fine-tuning loss. Observe that many different LoRA factors $(A_1, B_1), (A_2, B_2)$ can represent the same finetuned weight,\n$Z = A_1B_1^T = A_2B_2^T$.\n(2)\nWhen an optimizer is applied to train LoRA, it will produce different updates, $\\delta A_1, \\delta B_1$ or $\\delta A_2, \\delta B_2$, based on the specific parameterization used. Even though $(A_1, B_1)$ and $(A_2, B_2)$ represent the same finetuned weight $Z$, the updates using different parameterizations can produce different updates to $Z$. This suggests a serious inconsistency and implies that the update could be suboptimal under some parameterizations.\nBased on this observation, we propose that LoRA optimization should ensure transformation invariance, defined as follows:\nDefinition 1 (Transformation Invariance). Let $(A_1, B_1)$ and $(A_2, B_2)$ be any two pairs of LoRA factors that satisfy (2). An optimizer exhibits transformation invariance if its updates, $(\\delta A_1, \\delta B_1)$"}, {"title": "2.2 EXISTING OPTIMIZERS ARE NOT SCALAR SCALE INVARIANT", "content": "We now show that neither gradient descent nor Adam is scalar scale invariant (2), and in fact, almost all the existing optimizers are not scalar scale invariant when applied to LoRA optimization.\nLet us consider the following example:\n$A_2 = sA_1, B_2 = (1/s)B_1,$\n(6)\nthen by using chain rule on $Z = A_1B_1^T = A_2B_2^T$, we have\n$\\nabla_{A_1} = \\nabla_Z B_1, and \\nabla_{A_2} = \\nabla_Z B_2$.\n(7)\nWe can rewrite $\\nabla_{A_2}$ as:\n$\\nabla_{A_2} = (1/s) \\nabla_Z B_1 (since B_2 = (1/s)B_1)$\n$= (1/s)\\nabla_{A_1}.$\n(8)\nTo analyze gradient descent, we set the following in condition (4):\n$\\delta A_1 := -\\nabla_{A_1}, \\delta A_2 := -\\nabla_{A_2}, \\delta B_1 := -\\nabla_{B_1} and \\delta B_2 := -\\nabla_{B_2}$\nThe first term on right hand side of condition (4) can be rewritten as follows:\n$\\delta A_2B_2 = -\\nabla_{A_2}B_2^T$\n$= -(1/s)\\nabla_{A_1}B_2^T$, by (8)\n$= -(1/s^2)\\nabla_{A_1}B_1^T$, (since $B_2 = (1/s)B_1$)\n$= (1/s^2)\\delta A_1B_1^T$.\nThus the first terms on both sides of condition (4) have a scale difference of $1/s^2$. Similarly, we can show that the second terms have a scale difference of $s^2$, while the third terms remain identical. Therefore, gradient descent is not scalar scale invariant, and the gradient can be arbitrary large when $s$ goes to 0 or infinity.\nCan this issue be mitigated by adaptive updates such as Adam? The answer is no. To see this, let\n$\\delta A_1 := -\\nabla_{A_1}/(\\sqrt{\\nabla_{A_1}^T \\nabla_{A_1}})^{1/2}$"}, {"title": "2.3 BENEFITS OF TRANSFORMATION INVARIANCE", "content": "Why is transformation invariance important? Beyond the mathematical argument that different parameterizations of the same weight update should be equivalent, we demonstrate that transformation invariance leads to more efficient feature learning.\nThe concept of efficient feature learning, introduced in (Hayou et al., 2024), describes the asymptotic training behavior of LoRA as the network width grows. As discussed earlier, for LoRA, the update to the matrix $Z = AB^T$ can be decomposed into three parts\n$\\delta Z = (A+ \\delta A)(B^T+ \\delta B^T) - AB^T$\n$\\delta AB^T + A\\delta B^T + \\delta A\\delta B^T$,\nwhere the third term is typically negligible as it depends on the square of the learning rate. Efficient feature learning requires that both $\\delta AB^T \\vec{x}$ and $A\\delta B^T \\vec{x}$ are of magnitude $O(n^0) = O(1)$ with respect to the network width $n$, where $\\vec{x}$ is the input embedding. In other words, let the scale be $O(n^a)$, then it neither explodes ($a > 0$) nor diminishes ($a < 0$), when the network width $n$ grows.\nHayou et al. (2024) show that conventional optimizers do not satisfy efficient feature learning. This can be seen from Figure 1, where the weight norm for factor B changes significantly while the weight norm for factor A barely changes.\nUnder mild assumptions, we can show that a transformation-invariant optimizer guarantees efficient feature learning. The proof is given in Appendix."}, {"title": "3 OUR PROPOSED OPTIMIZER", "content": "We now present our proposed algorithm, which satisfies transformation invariance, and achieves significant improvements over previous LoRA optimizers in many empirical tasks."}, {"title": "3.1 DIAGONAL PRECONDITIONING IS NOT ENOUGH FOR TRANSFORMATION INVARIANCE", "content": "In this section we show that diagonal preconditioning is not enough to achieve transformation invariance. Recall that the LoRA factors $A \\in \\mathbb{R}^{m\\times r}$ and $B \\in \\mathbb{R}^{n\\times r}$. When updating $A$, most existing optimizers utilize the following update:\n$\\text{vec}(\\delta A) = -P \\text{vec}(\\nabla_A),$\n(9)\nwhere $\\text{vec}(\\cdot)$ lists the elements of matrix in a vector in column-major order and $P\\in \\mathbb{R}^{mr\\times mr}$ is a symmetric positive definite preconditioning matrix. Diagonal preconditioning methods like"}, {"title": "3.2 ACHIEVING TRANSFORMATION INVARIANCE", "content": "To achieve transformation invariance, we begin by recognizing that the LORA factors, $A$ and $B$, can be decomposed into their respective orthogonal bases and magnitudes:\n$A = U_A R_A, B = U_B R_B,$\nwhere $U_A$ and $U_B$ can be obtained through polar decomposition.\nNote that the gradients of $A$ and $B$,\n$\\nabla_A = \\nabla_Z B and \\nabla_B = \\nabla_Z A,$\ndepend on both the basis and the magnitude. To achieve transformation invariance, we introduce the concept of \"unmagnified gradients\u201d distinguished from standard gradients by the symbol $\\bar{\\nabla}$:\n$\\bar{\\nabla} A := \\nabla Z U_B = \\nabla_A R_B^{-T}, \\bar{\\nabla} B := \\nabla Z U_A = \\nabla_B R_A^{-T}.$\n(11)"}, {"title": "3.3 THEORETICAL ANALYSIS", "content": "Following previous work (Gupta et al., 2018; Feinberg et al., 2023), we provide a convergence analysis of the proposed algorithm within the online optimization framework (Hazan et al., 2016; Shalev-Shwartz et al., 2012)."}, {"title": "4 RELATED WORK", "content": "Related Optimizers. Adaptive first-order optimizers like Adagrad (Duchi et al., 2011) utilize accumulated second moments, essentially diagonal preconditioners, to scale updates for each coordinate. This approach, adopted by optimizers like Adam (Kingma & Ba, 2014) and RMSProp (Tieleman & Hinton, 2012), has become standard for training deep neural networks, including LoRA,"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "We evaluated the proposed LoRA optimizer against other optimizers across a range of datasets. This included the Super-Natural Instructions dataset, a comprehensive collection of diverse NLP tasks, as well as four standard LLM benchmarking datasets.\nWe compare the following optimizers:"}, {"title": "6 CONCLUSIONS", "content": "Current LORA optimization techniques lack transformation invariance, which implies that equivalent LoRA parameterizations can yield significantly different updates. This hinders efficient feature learning and often leads to suboptimal solutions in practice. We introduce a novel, transformation-invariant optimization algorithm with comparable time and memory overhead to Adam. Our algorithm consistently achieves higher accuracy than existing LoRA optimizers across diverse datasets and models."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 HYPERPARAMETERS", "content": "Table 5 shows our hyperparameters. We set weight decay and dropout probability to 0 as our early experiments suggest that setting a non-zero value does not improve the performance of the baselines."}, {"title": "A.2 DATASET", "content": "Table 6 shows the summary information of the LLM benchmarking datasets. We use the test set to evaluate ArcChallenge, as it is much larger than the development set."}, {"title": "A.3 PROOF OF THEOREM 1", "content": "Let $||A_1|| = \\theta(n^x), ||B_1|| = \\theta(n^y), ||\\nabla Z|| =\\theta(n^z), \\eta = \\theta(n^d)$, where $\\eta$ is the learning rate and n is the network width. Since $Z = A_1B_1$, from chain rule we know $\\nabla A = \\nabla Z B$ and $\\nabla B= \\nabla Z^TA$. Since the update rule is symmetric, we can express the updates as\n$||\\delta A_1|| = \\theta(n^{xa+yb+zc+d}), ||\\delta B_1|| = \\theta(n^{xb+ya+zc+d})$.\nIf the update rule is scalar scale invariant, then for any $A_2 = n^xA_1, B_2 = n^{-x}B_1$ we have\n$||\\delta A_1||||B_1|| = ||\\delta A_2||||B_2||,$\nwhich means\n$xa + (y + 1)b + zc + d = x(a + d) + (y + 1)(b - d) + zc + d,$\nthus $xd - (y + 1)d = 0$ for all $d$, which means $y = x - 1$. Consequently, we have\n$||\\delta A_1|| ||B_1|| = \\theta(n^{xa+(y+1)b+zc+d}) = \\theta(n^{xa+xb+zc+d}).$"}, {"title": "A.4 PROOF OF THEOREM 2", "content": "For matrix $X_A \\in \\mathbb{R}^{m \\times r}, H_A \\in \\mathbb{R}^{r \\times r}$, we call them consistent if\n$X_A \\bar{U}_B \\in \\mathbb{R}^{m \\times n}$\nand\n$\\bar{U}_B^T H_A \\bar{U}_B \\in \\mathbb{R}^{n \\times n}$\nare respectively the same across all equivalent LoRA pairs.\nFirst, one should note the fact that\n$\\bar{U}_B \\bar{U}_B^T$\nis the same across all equivalent pairs. Thus,\n$\\bar{U}_B(\\nabla_A)^T \\bar{\\nabla} A \\bar{U}_B^T = \\bar{U}_B(\\nabla_Z B^T)^T \\bar{\\nabla} Z \\bar{U}_B^T = \\bar{U}_B^T \\nabla_Z \\bar{\\nabla} Z \\bar{U}_B$\nimplies $(\\bar{\\nabla} A)^T \\bar{\\nabla} A$ is consistent.\nThis combined with the fact that $P_A \\nabla_{A_{t-1}} P_A$, is consistent if $\\nabla_{A_{t-1}}$ is consistent and that $\\nabla_{A_0}=0$ implies $\\nabla_A$ is consistent.\nLastly, since\n$\\bar{U}_B(\\nabla_{A_t} + \\rho_A I)^{-1/2} \\bar{U}_B^T = (\\bar{U}_B \\nabla_{A_t} \\bar{U}_B^T + \\rho_A \\bar{U}_B \\bar{U}_B^T)^{-1/2},$\nboth $S_A$ and $M_{A_t}$ are consistent, which completes our proof."}, {"title": "A.5 PROOF OF THEOREM 3", "content": "For convenience, for matrix $X \\in \\mathbb{R}^{m \\times r}, H \\in \\mathbb{R}^{r \\times r}$, we define\n$||X||_H = Tr(X H X^T)^{1/2}$.\nWe also utilize the following lemma for online optimization."}]}