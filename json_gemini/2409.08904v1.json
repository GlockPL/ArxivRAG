{"title": "AnyBipe: An End-to-End Framework for Training and Deploying Bipedal Robots Guided by Large Language Models", "authors": ["Yifei Yao", "Wentao He", "Chenyu Gu", "Jiaheng Du", "Fuwei Tan", "Zhen Zhu", "Junguo Lu"], "abstract": "Training and deploying reinforcement learning (RL) policies for robots, especially in accomplishing specific tasks, presents substantial challenges. Recent advancements have explored diverse reward function designs, training techniques, simulation-to-reality (sim-to-real) transfers, and performance analysis methodologies, yet these still require significant human intervention. This paper introduces an end-to-end framework for training and deploying RL policies, guided by Large Language Models (LLMs), and evaluates its effectiveness on bipedal robots. The framework consists of three interconnected modules: an LLM-guided reward function design module, an RL training module leveraging prior work, and a sim-to-real homomorphic evaluation module. This design significantly reduces the need for human input by utilizing only essential simulation and deployment platforms, with the option to incorporate human-engineered strategies and historical data. We detail the construction of these modules, their advantages over traditional approaches, and demonstrate the framework's capability to autonomously develop and refine controlling strategies for bipedal robot locomotion, showcasing its potential to operate independently of human intervention.", "sections": [{"title": "I. INTRODUCTION", "content": "With the integration of advanced algorithms, enhanced physical simulations, and improved computational power, robotics has made significant strides [1], [2]. These innovations enable robots to perform tasks ranging from industrial automation to personal assistance with unprecedented efficiency and autonomy [3], [4]. As industrial robotics matures, researches focus on humanoid robots, particularly in replicating human-like characteristics and enabling robots to perform human-oriented tasks [5]. Correspondingly, bipedal robots can be used to simulate the morphology of human lower limbs, thus providing a method to explore the locomotion skills of humanoid robots [6], [7].\nControl strategies for bipedal robots typically leverage either traditional control methods or reinforcement learning (RL) methods [8], [9]. Traditional approaches rely on problem abstraction, modeling, and detailed planning, while RL employs reward functions to iteratively guide robots toward task completion. Through repeated interactions with the environment, RL enables robots to refine control strategies and acquire essential skills, particularly excelling in trial-and-error learning in simulated environments, where robots adapt to complex terrains and disturbances.\nDespite these advancements, training and deploying RL algorithms remains challenging. Effective reward design requires careful consideration of task-specific goals and the incorporation of safety constraints for real-world applications [10], [11]. This complexity demands significant engineering effort in training, testing, and iterative refinement. Although reward shaping and safe RL offer potential solutions [12], [13], they often rely on prior experience, complicating the reward design process. Furthermore, bridging the gap between simulations and real-world conditions-the \"Sim-to-Real\" challenge-remains difficult [14]. Techniques such as domain randomization [15], which randomizes physical parameters to enhance agent robustness, and observation design, which facilitates task transfers across varied terrains, prove formidable, but still require real-world testing and human feedback for model selection and fine-tuning [7], [16], [17].\nHohe integration of large language models (LLMs) into robotics offers the prospect of reducing human work in such process. Known for their capabilities in code generation and missing planning, LLMs are being widely applied to complex robotics applications [18]. For instance, they play a pivotal role in embodied intelligence by enabling the dynamic creation of action tasks [19]. Recent developments have explored the utility of LLMs in improving reward function design and refining from simulation performance-key areas that reduce the need for human intervention [20]. However, a comprehensive framework that designs suitable policies and automatically implements properly trained models in real-world remains lacking. To address this issue, we propose a novel framework that leverages LLMs to optimize the entire training-to-deployment process. The framework minimizes human involvement, enables autonomous design, training, and deployment of RL algorithms, and supports exploring strategies from scratch or enhancing existing ones.\nWe present AnyBipe\u00b9, the first end-to-end framework for training, simulating, and deploying bipedal robots, guided by LLMs. This framework consists of three interconnected modules and embodies a closed-loop process for continual refinement and deployment."}, {"title": "II. RELATED WORKS", "content": "Reinforcement Learning for Bipedal Robots. Reinforcement learning (RL) has achieved significant success in enabling legged robots to learn locomotion and motion control through data-driven methods, allowing them to adapt to diverse environmental challenges [21]\u2013[26]. Research has introduced RL-based locomotion strategies, training in simulation environments like MuJoCo [27] and Isaac Gym [28]. Recent studies have shifted these techniques, originally designed for quadruped robots and robot arms, to bipedal robots like Cassie [29], [30]. Additional approaches explore imitation learning [31], motion planning [32], and robust RL strategies [7], enabling bipedal robots to perform tasks like running, stair climbing, and complex maneuvers [33]\u2013[35]. Building on these advancements, our work utilizes the Isaac Gym environment and proposes a semi-supervised RL framework to transfer the original work and effectively improve the learning performance.\nLarge Language Model Guided Robotics. Large language models (LLMs) have demonstrated considerable capabilities in task understanding [36], [37], semantic planning [38], [39], and code generation [40]\u2013[42], allowing them to be effectively integrated into a variety of robotic tasks. LLMS automate environmental analysis [43], [44], reward functions design [45], [46], and task-action mappint [47], [48]. However, challenges such as data scarcity, real-time performance, and real-world integration remain [49]. In addition, due to the lack of awareness of actual data, LLM-driven code design sometimes needs to be optimized through human feedback. Our framework addresses the existing limitations by adopting environmental characteristics and safety constraints as priors, and uniquely combines homomorphic feedback from real-world applications, reducing the need for human feedback in the process.\nSim-to-real Training and Deploying Techniques. The gap between simulated environments and real-world conditions, known as the \"reality gap,\" presents significant challenges for deploying RL strategies in robotics [50], [51]. Techniques such as domain randomization [52]\u2013[54] and system identification [55]\u2013[57] are widely used to address this issue. Researchers have proposed sim-to-real solutions for bipedal robots to handle tasks such as turning and walking [58], [59]. Recent work has also integrated LLMs to enhance environmental modeling and reward function design, making simulations more reflective of real-world complexity [20]. However, most approaches still rely on separate training in simulation and real-world evaluation, often using human feedback to assess sim-to-real effectiveness. Our work extends these techniques by introducing an evaluation loop that continuously monitors sim-to-real performance during deployment."}, {"title": "III. METHODS", "content": "In this section, we introduce the AnyBipe framework in detail. The framework consists of three modules aiming to automate reward design, training and simulation, and deployment feedback to minimize the need for manual work. The framework requires the robot's URDF model, a basic task description, an RL training platform, and a ROS-based system for communication and control [60], along with an SDK for sensor and actuator data, as compulsory input. Optional elements include a manual reward function template, a teacher model realized with previous work, and custom environmental observations for evaluation. AnyBipe generates reward functions accordingly and trains policies supervised by the teacher model. A group of top strategies, determined by evaluation criteria and simulation results, are deployed via ROS. After validating in the ROS Gazebo simulation, policies that pass the safety check undergo real-world testing, and the best strategy is then selected. Evaluation result across various environments and the selected best policy are compiled to feedback, guiding iterative improvements in reward generation.\nWe refer to the reward function design in the Eureka algorithm [45], which enables LLMs to autonomously improve and iterate reward functions with predefined environmental and task descriptions $D(T)$. However, initial usability issues require multiple iterations for viable training code. Furthermore, Eureka often overlooks discrepancies between training $T$ and real environments $R$, resulting in computationally expensive but minimally effective reward functions that may induce undefined behaviors [61]. The framework also lacks comprehensive safety considerations for tasks such as bipedal movement, despite attempts to integrate safety through Reward-Aware Physical Priors (RAPP) and LLM-led Domain Randomization [20].\nTo address key issues, we developed a robust context establishment mechanism that effectively tackles underdesigned reward functions and safety constraints. Our approach classifies prompts into two categories: General and Task-Specific. For general tasks, we provide coding tips, function templates, and predefined templates that facilitate code compilation, training feedback, and evalu feedback. We also integrate reference tables from Isaac Gym for environmental variables like motor torque, torque limits, and feet height. These tables prevent LLMs from the customization of non-existent observations while enhancing the utilization of environmental variables in $D(T)$ in order to ensure that LLMs consider actionable constraints during reward function design. Our experimental results affirm that LLMs can seamlessly incorporate these safety restrictions and environmental variables, thus designing highly effective reward functions, evidencing their exceptional context tracking and directive following capabilities [62], [63].\nFurthermore, the Task-Specific module allows users to define custom prompts for specific tasks, facilitating the rapid generation of viable code and standardization of reward calculations. Users have the flexibility to use trainable artificial reward functions as templates or employ various computational paradigms to enhance the accuracy and applicability of reward assessments.\nTo refine the evaluation of designed reward functions for completing the improvement loop, we introduced a comprehensive reward function evaluation scheme. This scheme not only tracks changes in rewards and observations throughout the training but also integrates a homomorphic evaluation model to closely assess real-world robot performance. This model ensures a high correlation between real-world outcomes and theoretical reward functions, enabling LLMs to intuitively identify the most impactful components of the reward functions.\nIn this section, we detail the adaptations applied to guide RL training towards desired actions using frameworks provided by Legged Gym\u00b2, employing the Proximal Policy Optimization (PPO) algorithm [64] as our foundation. We assume the existence of a baseline policy $\\pi_{ref}$, which could be derived from traditional control methods or previous RL techniques.\nTo enhance the PPO algorithm, we modify the objective function given below:\n$L^{CLIP} (\\theta) =E_{t}[min(r_{t} (\\theta) A_{t}, clip(r_{t} (\\theta), 1 \u2013 \\epsilon, 1 + \\epsilon)\u00c2_{t})\n+ \\beta KL [\\pi_{ref}(S_{t}), \\pi_{\\theta}(\\cdot|S_{t})]]$,\nwhere $r_{t} (\\theta) = \\frac{\\pi_{\\theta} (a_{t}/s_{t})}{\\pi_{gold} (a_{t} S_{t})}$ denotes the probability ratio, $\u00c2_{t}$ is an estimator of the advantage at time t, $\\epsilon$ is a small positive number, and $\\beta$ is a coefficient that measures the divergence between the reference policy and PPO policy.\nThis integration enables control over the similarity between the trained policy and the reference policy. However, abstracting the reference policy $\\tau_{ref}$ into the same probabilistic framework as the PPO policy presents challenges. Despite this, given the deterministic nature of actions $a_{t}$ for a specific state $s$ and previous action $a_{t-1}$, and assuming sufficient environmental observations, we can approximate the distribution $\\pi_{ref}$ as a Dirac distribution, and the differences between $\\pi$ and $\\tau_{ref}$ can be described as follows:\n$E_{t} [KL(\\tau_{ref}, \\pi_{\\theta})] \\approx \\frac{1}{N} \\sum_{i=1}^{N} [log(\\sqrt{2\\pi \\sigma_{0}^{2}}) + \\frac{(A_{ref} \u2013 \\mu_{\\theta,i})^{2}}{2\\sigma_{0}^{2}},$ \nStrategies are deployed in a ROS-based robotic framework (R) using ONNX. Given that the training (T) and real environments (R) are isomorphic, we define a homomorphism $F:T \\rightarrow R$, ensuring the real-world evaluation metric R = F(R) mirrors the reward function. Our automated script aligns reward structures with observed real-world data, informing users of any mismatches."}, {"title": "IV. EXPERIMENTS", "content": "Our experiments were conducted on a six-degree-of-freedom bipedal robot from Limx Dynamics, GPT-40 was selected as the LLM base, with training performed on an NVIDIA GTX 3090Ti.\nAnyBipe enables an autonomous cycle from reward function generation to training, deployment, and optimization, facilitating user-driven training of bipedal robot RL algorithms without pre-existing strategies."}, {"title": "V. CONCLUSION", "content": "AnyBipe proposes an end-to-end framework for training and deploying bipedal robots, which utilizes a state-of-the-art LLM to design reward functions for specific tasks. The framework provides interfaces that allow users to supply coding references and integrate pre-existing models to assist in training. Additionally, it incorporates feedback from both simulated and real-world test results, enabling the execution of Sim-to-Real tasks entirely without human supervision. We validated the effectiveness of each module, as well as the system's ability to guide the robot in learning locomotion in complex environments, continuously improving the model by either designing new reward functions from scratch or refining existing ones. Furthermore, this framework exhibits potential for transferability to other robotic task planning scenarios. Our future work will focus on improving the current framework in three key areas: first, extending it to a broader range of robotic applications to verify its generalizability; second, testing its effectiveness across more tasks beyond locomotion; and third, enhancing the model evaluation process by incorporating visual and conceptional feedback to achieve more comprehensive state estimation."}, {"title": "APPENDIX", "content": "We have KL divergence between normal distribution \u039d(\u03bc\u03b8, \u03c3\u03b8) with PDF q(x) = \u221a2\u03c0\u03c32exp(-(x-\u03bc\u03b8)22\u03c32), and Dirac distribution with PDF p(x) = \u03b4(x - aref), the KL divergence can be written as\nKL(\u03c0ref | q) =\u222b p(x) log p(x)q(x) dx=\u222b \u03b4(x - aref) log \u03b4(x - aref)q(x) dx=\u222b \u03b4(x) log \u222b \u03b4(x - aref)q(x) dx= 0 - log q(aref)= log(1/\u221a2\u03c0\u03c32) + (Aref \u2013 \u03bc\u03b8)22\u03c32\nWe first prove that \u222b \u03b4(x) log \u03b4(x)dx = 0. Let u = log(x), v = \u222b \u03b4(x)dx = F(x), where | is the step function, we havem(x) := \u222b \u03b4(x) log \u03b4(x)dx = udv = uv - \u222b vdu{\u222b\u221e0.u-0.du = 0, x 0.Then we show why we don't calculate the normal KL divergence KL(\u03c0\u03b8 | Tref). This is because the term\u222b p(x) log p(x)dxis-\u221e, there is no way for normal calculation. Since the backward propagation functions normally, we choose the inverse form, which approximately measures the difference between two distributions.Also, in practical, we tend to ignore the term which controls the size of \u03c3\u03b8, using this term directly instead:\nLdist = \u03a3 (Aref \u2013 \u03bc\u03b8,i)22\u03c32\u03b8i"}]}