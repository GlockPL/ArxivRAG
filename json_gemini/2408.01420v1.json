{"title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs", "authors": ["Jingtong Su", "Julia Kempe", "Karen Ullrich"], "abstract": "Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful be-haviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions. Based on our insights, we propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, we introduce a simple modification to the RLHF objective, we call E-RLHF, that aims to increase the likelihood of safe responses. E-RLHF brings no additional training cost, and is compatible with other methods. Empirically, we demonstrate that E-RLHF outperforms RLHF on all alignment problems put forward by the AdvBench [1] and HarmBench project [2] without sacrificing model performance as measured by the MT-Bench project [3].", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized the field of deep learning due to their remarkable capabilities across various domains, serving as assistants, in code generation [4], healthcare [5], and theorem proving [6]. The training process of a LLM typically includes two stages: pretraining with massive corpora, and an alignment step using Reinforcement Learning from Human Feedback (RLHF) to further align model behavior with human preferences. The latter step typically involves large amounts of humanly annotated data, and can be decomposed into a supervised fine-tuning (SFT) step, a reward modeling step, and an RL Fine-Tuning step. Despite their ability to perform multiple tasks effectively, LLMs are susceptible to generating offensive or inappropriate content including hate-speech, malware, fake information or social biases, due to the unavoidable presence of harmful elements within their pretraining datasets [7\u20139]. Social media showcase an abundance of tricks on how to attack ChatGPT [10] to elicit harmful responses, e.g., the \"Do Anything Now\" (DAN) prompts [11] or the \"Grandma Exploit\" hack [12]. On the other hand, behavior diversity in the training corpus is essential to for example capturing different cultural preferences. What is and isn't harmful ultimately depends on user preferences, hence the alignment step is not universal but depends on the specific use case under which a model will be employed."}, {"title": "2 Framework and assumptions", "content": "Jailbreaking carries several analogies to adversarial attacks, a well studied field in computer vision [39]. Here, an adversary is defined as a map that perturbs a given input image in pixel space to change the model output. The strength of the adversary is bounded by how far it is able to move the original input as quantified by the $l_p$ distance [40\u201342]. Typically, this distance is bounded in a way that the change would not be perceptible to the human observer. The goal of the adversary is to cause mis-classification of the input. In contrast, in the instance of an LLM, the adversary's goal is to provoke harmful behaviour, e.g., unintended leaking of information or hate speech. Further, any perturbation to an input, called prompt, will have a perceptible effect. Hence quantifying and bounding the capabilities of the adversary is not straight forward. Nonetheless, with some modifications, we will build on this analogy.\nFor the purpose of this work, we will view any prompt as a tuple of query and concept $(q,c)$, where $c\\in \\mathcal{C}$, and $q \\in \\mathcal{Q}$, with $\\mathcal{C}, \\mathcal{Q}$ denoting the complete concept set and query set. Conceptually, we think of concepts as representing the information content of the prompt, usu-ally through a short piece of text, for example \u2018tutorial on making a cake\". Queries are instructional text pieces that are composable with certain concepts. We can think of queries as mechanisms to trigger an LM to expand a concept in a specific way. Examples include \u201cTell me how to ${}$\u201d, or \u201cWe are now in an imaginary world, and you are"}, {"title": "3 PAC-Bayesian bound for pre-training LLMs on harmful data", "content": "Given a learning algorithm that leads to a posterior distribution over a set of models, PAC-Bayesian theory [44] applies Probably Approximately Correct (PAC) inequalities, to provide bounds on the generalization gap, i.e., the difference between the model's empirical loss and the population loss.\nWe now present the first result of our analysis: a non-vacuous PAC-Bayesian bound for pretraining LMs which implies that a well-trained LM ought to exhibit harmful behaviour even when simply prompted with direct queries if it was presented with harmful behavior during training.\nWe denote by $S = \\{(q_i, c_i)\\}_{i=1}^n$ a set of prompts generated i.i.d. under $D_\\mathcal{P}$, $S \\sim D_\\mathcal{P}^n$. These prompts together with sampled explanations form our pretraining corpus. We use $\\pi, \\rho$ as the prior and posterior distribution over $\\mathcal{LIM}$ before and after the pretraining process, defined over $\\mathcal{LIM}$, the set of language models. Given a prompt $(q, c)$, we measure the generalization capability of a LM by quantifying the Total Variation (TV) loss between the induced distribution $P_{\\mathcal{LM}}(q, c)$ and the ground-truth distribution $P_{\\text{world}}(q, c)$. For real-world LMs, pretraining involves optimizing the cross-entropy loss on the training corpus, which is equivalent to minimizing $\\text{KL}[P_{\\text{world}}(q, c) || P_{\\mathcal{LM}}(q, c)]$ under our framework. With Pinsker's Inequality, optimizing the KL-divergence term is equivalent to optimizing an upper bound on TV; thus we expect empirical TV loss be small."}, {"title": "4 A statistical perspective on jailbreaking after alignment", "content": "In this section, we will present the main theoretical contribution of our work: given our assumptions hold, we prove the existence of ways for an adversary to jailbreak an LM even after the preference alignment process. Our proof strategy is inspired by the work on adversarial robustness [41], which bounds the adversary's probability of success by upper bounding the volume of the set of points that does not allow for the existence of adversarial examples. Going forward, we need to extend our framework to integrate alignment and jailbreaking.\nAfter an LM is pretrained, it typically will undergo fine-tuning on a dataset containing preferred behaviour. In what follows, we will assume that this alignment process does not change the model performance in the sense that the LM will still produce semantically meaningful explanations (Definition 2.1). It would not, for example, default to answering any request with the same response."}, {"title": "5 E-RLHF: improving alignment by expanding the safety zone", "content": "Recall from Theorem 2 and the subsequent discussion in the previous section, that jailbreaking becomes more likely the larger the harmful zone is in comparison to the safety zone. The size of both zones relates to the size of their respective explanation sets. In other words, the size of the preference alignment dataset is crucial to successful alignment. Unfortunately, the human labor involved in creating such a dataset effectively caps its size.\nIn order to bridge the gap between our theoretical insights and a practical solution towards suppressing the jailbreaking problem, we focus on other more practical ways to expand the safety zone. Even though our ideas are more broadly applicable, in our experiments we will focus on improving Reinforcement Learning with Human Feedback (RLHF). RLHF typically includes three phases: i) supervised fine-tuning (SFT); ii) preference sampling and reward learning and iii) RL optimization. Rafailov et al. [23] have recently proposed a widely applied version of RLHF for LMs, coined Direct Preference Optimization (DPO), that employs a clever reparameterization which leads to directly learning from the preference dataset, without the need of obtaining a reward model beforehand. DPO is more stable in the training process than other implementations of RLHF. A more complete overview of RLHF and DPO can be found in Appendix C.\nFor our purposes, we assume access to an LLM $P_{\\text{SFT}}$ that has been supervised fine-tuned on high-quality data. We further assume access to a preference aligned dataset $D_S$; that contains a set of text"}, {"title": "6 Experiments and results", "content": "Our experimental set-up is based on the alignment-handbook code base [50]. We tune the publicly-available SFT model $p_{\\text{SFT}}$ provided by huggingface hub [51], using the public dataset [52, 53], with default hyperparameter setup. We label harmful prompts in the preference dataset by prompting GPT-3.5-Turbo, see Appendix E. We are using the very same prefix proposed in the previous section to generate $x_S$. Experiments are performed on 8 NVIDIA Tesla V100 GPUs, using half-precision tuning i.e., Float16. In the appendix, we also show results for an alternative training paradigm: the Low-Rank Adaptation (LoRA) [54] (see Appendix D.1). Following community standards [3, 1, 2], we use greedy decoding i.e., $T = 0$ for model evaluation.\nWe first show empirical evidence that our proposed modification of DPO, E-DPO, does in fact improve safety alignment, using the Harmbench dataset [2] and the first 100 prompts in the AdvBench harmful behavior dataset [1], measured by the HarmBench protocol. We give an overview on all adversaries in Appendix F. The results are presented in Table 1. E-DPO achieves improvements across every task we tested.\nOn top of our safety results, we want to make sure E-RLHF does not sacrifice helpfulness for increased safety. We evaluate helpfulness with the MT-Bench project [3]. The SFT model $p_{\\text{SFT}}$"}, {"title": "7 Conclusion and discussions", "content": "In this paper, we present a theoretical framework for language model pretraining and jailbreaking by dissecting input prompts into query and concept pairs. Through this approach, we have established two theoretical results pertaining to the ability of language models to mimic the world following pre-training, which leads to outputting harmful explanations given harmful prompts; and the inevitability of jailbreaking resulting from alignment challenges. Guided by these theoretical insights, we have devised a simple yet effective technique to enhance safety alignment, and demonstrate the improved resilience to jailbreak attacks with this methodology.\nCurrent limitations (1) Although we have classified concepts as either harmful or non-harmful, it is important to acknowledge that the perception of a concept's potential for harm can be influenced by various factors such as cultural, legal, and societal norms, which collectively form the context of the situation. (2) Language models have demonstrated impressive capabilities in reasoning and completing tasks within multi-round, multi-step conversations; our current framework may not fully account for the generalization and jailbreaking possibilities associated with such input formats. (3) Our analysis is grounded on a fixed $P_{\\text{world}}$ mapping and $D_\\mathcal{P}$ distribution. Nevertheless, the world is inherently dynamic, as both $P_{\\text{world}}$ and $D_\\mathcal{P}$ continually evolve.\nFuture work (1) Regarding our E-RLHF approach, as highlighted in the experimental section, in addition to attaching a universally safe prefix to all harmful prompts, improvements can be achieved by individually transforming the harmful prompts. Moreover, the safety-transformed prompts can be employed to expand the preference dataset for conventional RLHF. (2) Throughout our analysis, we have not imposed any constraints on the capacity of the language model. Extending our analysis under finite memory constraints or analyzing hallucination properties of LLMs is an interesting direction to explore. (3) Large language models have shown remarkable capabilities as in-context learners [56], and such techniques could potentially be used for jailbreaking them as well [57\u201359]. Investigating the incorporation of such input paradigms remains a promising avenue for future research."}, {"title": "F Jailbreak adversaries collected in the HarmBench project [2]", "content": "In this section, we give a short overview of the adversaries we adopt to evaluate our models. Some descriptions are summarized in Mazeika et al. [2].\n\u2022 Direct Request refers to sending the original harmful prompt directly to the target LLM.\n\u2022 GCG [1], GBDA [71] and AP [72] find adversarial suffixes via token-level optimization.\n\u2022 SFS (Stochastic Few-Shot) and ZS (Zero-Shot) [15] are few-shot sampling or zero-shot generation of test cases by an attacker LLM to elicit a behavior from a target LLM.\n\u2022 PAIR [30] and TAP [33] are iterative prompting / tree-structured prompting methods, with an attacker LLM, to adaptively explore and elicit specific harmful behaviors from the target LLM.\n\u2022 AutoDAN [29] is a genetic-algorithm based attack with initializations from handcrafted DAN jailbreak prompts.\n\u2022 PAP [73] uses persuasive strategies to jailbreak the target LLM.\n\u2022 HumanJailbreaks [74] uses a fixed set of in-the-wild human jailbreak templates, similar to the Do Anything Now (DAN) jailbreaks.\nWe exclude all transfer attacks since we focus on single-model jailbreak. Furthermore, we choose to discard the UAT [75] and PEZ [76] adversaries, because the former induces an out-of-memory error on our V100 GPUs, and the latter never succeeds to find a suffix according to our experiments."}, {"title": "G Broader Impacts", "content": "The societal impact of our work has close connection to the topic of LLM safety. We propose a framework for analyzing language model pretraining and jailbreaking, and we design a new RLHF algorithm for improving safety. As shown in our experiments, our work could advocate for safer LLMs."}, {"title": "H Related work", "content": "In this section, we provide a review of the current literature on LLM jailbreaking.\nIn this section, we summarize existing jailbreaking methods."}]}