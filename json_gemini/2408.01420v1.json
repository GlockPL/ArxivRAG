{"title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs", "authors": ["Jingtong Su", "Julia Kempe", "Karen Ullrich"], "abstract": "Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful be- haviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empir- ical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modify- ing the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions. Based on our insights, we propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, we introduce a simple modification to the RLHF objective, we call E-RLHF, that aims to increase the likelihood of safe responses. E-RLHF brings no additional training cost, and is compatible with other methods. Empirically, we demonstrate that E-RLHF outperforms RLHF on all alignment problems put forward by the AdvBench [1] and HarmBench project [2] without sacrificing model performance as measured by the MT-Bench project [3].", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolutionized the field of deep learning due to their remarkable capabilities across various domains, serving as assistants, in code generation [4], healthcare [5], and theorem proving [6]. The training process of a LLM typically includes two stages: pretraining with massive corpora, and an alignment step using Reinforcement Learning from Human Feedback (RLHF) to further align model behavior with human preferences. The latter step typically involves large amounts of humanly annotated data, and can be decomposed into a supervised fine-tuning (SFT) step, a reward modeling step, and an RL Fine-Tuning step. Despite their ability to perform multiple tasks effectively, LLMs are susceptible to generating offensive or inappropriate content including hate-speech, malware, fake information or social biases, due to the unavoidable presence of harmful elements within their pretraining datasets [7\u20139]. Social media showcase an abundance of tricks on how to attack ChatGPT [10] to elicit harmful responses, e.g., the \"Do Anything Now\" (DAN) prompts [11] or the \"Grandma Exploit\" hack [12]. On the other hand, behavior diversity in the training corpus is essential to for example capturing different cultural preferences. What is and isn't harmful ultimately depends on user preferences, hence the alignment step is not universal but depends on the specific use case under which a model will be employed."}, {"title": "Framework and assumptions", "content": "Jailbreaking carries several analogies to adversarial attacks, a well studied field in computer vision [39]. Here, an adversary is defined as a map that perturbs a given input image in pixel space to change the model output. The strength of the adversary is bounded by how far it is able to move the original input as quantified by the $l_p$ distance [40\u201342]. Typically, this distance is bounded in a way that the change would not be perceptible to the human observer. The goal of the adversary is to cause mis- classification of the input. In contrast, in the instance of an LLM, the adversary's goal is to provoke harmful behaviour, e.g., unintended leaking of information or hate speech. Further, any perturbation to an input, called prompt, will have a perceptible effect. Hence quantifying and bounding the capabilities of the adversary is not straight forward. Nonetheless, with some modifications, we will build on this analogy.\nFor the purpose of this work, we will view any prompt as a tuple of query and concept $(q,c)$, where $c \\in C$, and $q \\in Q$, with $C, Q$ denoting the complete concept set and query set. Con- ceptually, we think of concepts as representing the information content of the prompt, usu- ally through a short piece of text, for example \u2018tutorial on making a cake\". Queries are instructional text pieces that are composable with certain concepts. We can think of queries as mechanisms to trigger an LM to expand a concept in a specific way. Examples include \u201cTell me how to {}\u201d, or \u201cWe are now in an imaginary world, and you are"}, {"title": "PAC-Bayesian bound for pre-training LLMs on harmful data", "content": "Given a learning algorithm that leads to a posterior distribution over a set of models, PAC-Bayesian theory [44] applies Probably Approximately Correct (PAC) inequalities, to provide bounds on the generalization gap, i.e., the difference between the model's empirical loss and the population loss. We now present the first result of our analysis: a non-vacuous PAC-Bayesian bound for pretraining LMs which implies that a well-trained LM ought to exhibit harmful behaviour even when simply prompted with direct queries if it was presented with harmful behavior during training.\nWe denote by $S = \\{(q_i, C_i)\\}_{i=1}^n$ a set of prompts generated i.i.d. under $D_p$, $S \\sim D_p^n$. These prompts together with sampled explanations form our pretraining corpus. We use $\\pi, \\rho$ as the prior and posterior distribution over $ILIM$, the set of language models. Given a prompt $(q, c)$, we measure the generalization capability of a LM by quantifying the Total Variation (TV) loss between the induced distribution $P_{LM}(q, c)$ and the ground-truth distribution $P_{world}(q, c)$. For real-world LMs, pretraining involves optimizing the cross-entropy loss on the training corpus, which is equivalent to minimizing $KL[P_{world}(q, C)||P_{LM}(q, c)]$ under our framework. With Pinsker's Inequality, optimizing the KL-divergence term is equivalent to optimizing an upper bound on TV; thus we expect empirical TV loss be small."}, {"title": "A statistical perspective on jailbreaking after alignment", "content": "In this section, we will present the main theoretical contribution of our work: given our assumptions hold, we prove the existence of ways for an adversary to jailbreak an LM even after the preference alignment process. Our proof strategy is inspired by the work on adversarial robustness [41], which bounds the adversary's probability of success by upper bounding the volume of the set of points that does not allow for the existence of adversarial examples. Going forward, we need to extend our framework to integrate alignment and jailbreaking.\nAfter an LM is pretrained, it typically will undergo fine-tuning on a dataset containing preferred behaviour. In what follows, we will assume that this alignment process does not change the model performance in the sense that the LM will still produce semantically meaningful explanations (Definition 2.1). It would not, for example, default to answering any request with the same response."}, {"title": "E-RLHF: improving alignment by expanding the safety zone", "content": "Recall from Theorem 2 and the subsequent discussion in the previous section, that jailbreaking becomes more likely the larger the harmful zone is in comparison to the safety zone. The size of both zones relates to the size of their respective explanation sets. In other words, the size of the preference alignment dataset is crucial to successful alignment. Unfortunately, the human labor involved in creating such a dataset effectively caps its size.\nIn order to bridge the gap between our theoretical insights and a practical solution towards suppressing the jailbreaking problem, we focus on other more practical ways to expand the safety zone. Even though our ideas are more broadly applicable, in our experiments we will focus on improving Reinforcement Learning with Human Feedback (RLHF). RLHF typically includes three phases: i) supervised fine-tuning (SFT); ii) preference sampling and reward learning and iii) RL optimization. Rafailov et al. [23] have recently proposed a widely applied version of RLHF for LMs, coined Direct Preference Optimization (DPO), that employs a clever reparameterization which leads to directly learning from the preference dataset, without the need of obtaining a reward model beforehand. DPO is more stable in the training process than other implementations of RLHF. A more complete overview of RLHF and DPO can be found in Appendix C.\nFor our purposes, we assume access to an LLM $P_{SFT}$ that has been supervised fine-tuned on high- quality data. We further assume access to a preference aligned dataset $D_s$; that contains a set of text"}, {"title": "Experiments and results", "content": "Our experimental set-up is based on the alignment-handbook code base [50]. We tune the publicly- available SFT model $p_{SFT}$ provided by huggingface hub [51], using the public dataset [52, 53], with default hyperparameter setup. We label harmful prompts in the preference dataset by prompting GPT-3.5-Turbo, see Appendix E. We are using the very same prefix proposed in the previous section to generate $x_s$. Experiments are performed on 8 NVIDIA Tesla V100 GPUs, using half-precision tuning i.e., Float16. In the appendix, we also show results for an alternative training paradigm: the Low-Rank Adaptation (LoRA) [54] (see Appendix D.1). Following community standards [3, 1, 2], we use greedy decoding i.e., $T = 0$ for model evaluation.\nWe first show empirical evidence that our proposed modification of DPO, E-DPO, does in fact improve safety alignment, using the Harmbench dataset [2] and the first 100 prompts in the AdvBench harmful behavior dataset [1], measured by the HarmBench protocol. We give an overview on all adversaries in Appendix F. The results are presented in Table 1. E-DPO achieves improvements across every task we tested.\nOn top of our safety results, we want to make sure E-RLHF does not sacrifice helpfulness for increased safety. We evaluate helpfulness with the MT-Bench project [3]. The SFT model $P_{SFT}$"}, {"title": "Conclusion and discussions", "content": "In this paper, we present a theoretical framework for language model pretraining and jailbreaking by dissecting input prompts into query and concept pairs. Through this approach, we have established two theoretical results pertaining to the ability of language models to mimic the world following pre- training, which leads to outputting harmful explanations given harmful prompts; and the inevitability of jailbreaking resulting from alignment challenges. Guided by these theoretical insights, we have devised a simple yet effective technique to enhance safety alignment, and demonstrate the improved resilience to jailbreak attacks with this methodology.\nCurrent limitations (1) Although we have classified concepts as either harmful or non-harmful, it is important to acknowledge that the perception of a concept's potential for harm can be influenced by various factors such as cultural, legal, and societal norms, which collectively form the context of the situation. (2) Language models have demonstrated impressive capabilities in reasoning and completing tasks within multi-round, multi-step conversations; our current framework may not fully account for the generalization and jailbreaking possibilities associated with such input formats. (3) Our analysis is grounded on a fixed $P_{world}$ mapping and $D_p$ distribution. Nevertheless, the world is inherently dynamic, as both $P_{world}$ and $D_p$ continually evolve.\nFuture work (1) Regarding our E-RLHF approach, as highlighted in the experimental section, in addition to attaching a universally safe prefix to all harmful prompts, improvements can be achieved by individually transforming the harmful prompts. Moreover, the safety-transformed prompts can be employed to expand the preference dataset for conventional RLHF. (2) Throughout our analysis, we have not imposed any constraints on the capacity of the language model. Extending our analysis under finite memory constraints or analyzing hallucination properties of LLMs is an interesting direction to explore. (3) Large language models have shown remarkable capabilities as in-context learners [56], and such techniques could potentially be used for jailbreaking them as well [57\u201359]. Investigating the incorporation of such input paradigms remains a promising avenue for future research."}, {"title": "Definitions", "content": "Definition 2.1. (Notions of Harmfulness)\n*   (Direct Queries and Direct Prompts) We refer to a prompt as direct if it stems from $D_p$, i.e., $(q, c) \\in supp(D_p)$. The query of a direct prompt is called a direct query.\n*   (Harmful Concepts and Harmful Set) Given a concept c, the associated harmful set of ex- planations is denoted as $E_h(c) := \\{e|e \\in supp(P_{world}(\\cdot,c)) \\land e \\text{ is harmful}\\}$. In accor- dance with Assumption 2.1, with a threshold $\\eta$, a concept c is harmful if $\\forall q$ s.t. $(q, c) \\in dom(P_{world}), \\sum_{e:e\\in E_h(c)} P_{world}(e|q, c) \\ge 1-\\eta$. We refer to the set of all possible harmful concepts as $C_h \\subseteq C$.\n*   (Safe Set) $\\forall c \\in C_h$, there exists a corresponding safe set $E_s(c) \\subset E$ that we wish $P_{LM} (q, c)$ to be concentrated on. It includes safe explanations existing in $supp(P_{world}(\\cdot, c))$, and explanations designed by humans, e.g., with the template beginning with \u201cSorry.\u201d\n*   (Semantically meaningful) We call explanations in $E_h(c) \\cup E_s(c)$ as semantically meaningful for the $(q, c)$ prompt.\n*   (Mixture decomposition of $D_p$) With these notions, we can decompose $D_p = \\alpha D_{ph} + (1-\\alpha) D_{p \\bar{h}}$ (where $supp(D_{p_h})$ includes all direct prompts with a harmful concept, and $supp(D_{p\\bar{h}})$ includes the complement) as a mixture over direct prompts with a harmful concept and the non-harmful counterpart.", "definition": true}, {"title": "Definitions", "content": "Definition 3.1. (TV empirical loss and population loss)\n$l_{TV}(P_{LM}, (q, c)) := TV(p_{world}(q, c), P_{LM}(q, c))$.\nGiven an LM and a set of data S, the empirical loss $R_S(P_{LM})$ and population loss $R(P_{LM})$ are defined as\n\\begin{align}\n  R_S(P_{LM}) := \\frac{1}{n} \\sum_{i=1}^n l_{TV}(P_{LM}, (q_i, C_i));\n  R(P_{LM}) := E_{S \\sim D} [R_S(P_{LM})] = E_{(q,c) \\sim D_p} [l_{TV}(P_{LM}, (q, c))] .\n\\end{align}", "definition": true}, {"title": "Definitions", "content": "Definition 4.1. (Induced Distribution on Simplex, $\\gamma_c$)\nUnder the assumption that the LM outputs semantically meaningful explanations (Assumption 4.1), with a fixed prompt $(q, c)$ and a posterior distribution $\\gamma$ over $ILIM$, the corresponding induced distribution: $P_{LM(q, c)}$ where $LM \\sim \\gamma$ is supported over a subset of the output simplex $\\Delta_n^{n_c-1}$. This distribution is denoted as $\\gamma_{(q,c)}$, or $\\gamma_c$ when the reference to q is clear from context.\nDefinition 4.2. (Harmful Zone and Safety Zone) For a given harmful concept c and a fixed LM, the output simplex is divided into a safety zone and a harmful zone, $H_s$ and $H_h$, where a pre- defined threshold $p \\in [0,1]$ is used to quantify the distinction: $P_{LM}(q,c) \\in H_h$ if and only if $\\sum_{e:e\\in E_h (c)} P_{LM} (e/q, c) \\ge p$, and otherwise $P_{LM}(q, c) \\in H_s$.\nDefinition 4.3. (Jailbreaking) Given a harmful concept c and a query $q'$, the prompt $(q', c)$ jailbreaks the LM iff $P_{LM} (q', c) \\in H_h$. We call such a prompt $(q', c)$ and query $q'$ a jailbreaking prompt and jailbreaking query, respectively.\nDefinition 4.4. ($\\epsilon$-expansion) Given a set $A \\subset \\Delta_n-1$ and a distance measure $d$, the $\\epsilon$-expansion set $A(\\epsilon, d)$ is defined as\n$$A(\\epsilon, d) := \\{t|t \\in \\Delta_n^{-1} \\land \\exists y \\in A s.t. ||y - t||_d < \\epsilon\\}$$.", "definition": true}, {"title": "Definitions", "content": "Lemma B.4. (Volume of n-simplex)10 For any dimension n, the volume of the n-element probability simplex: $\\Delta^{n-1}$, in the n \u2212 1-dimensional space is\n$$\\frac{\\sqrt{n}}{(n - 1)!}$$.\nWe define the projected probability simplex as follows.\nDefinition B.2. (Projected probability simplex) Given $\\Delta^{n\u22121}$, the corresponding projected probability simplex, $\\Delta^{n\u22121}_{pr}$, is defined as a subset of $R^{n\u22121}: \\{x \\in R^{n\u22121}|\\sum^{n\u22121}_{i=1} x_i \\le 1, \\forall i \\in [n \u2212 1]\\}$."}, {"title": "Lemmas", "content": "Lemma B.1. (Hoeffding's Lemma) for random variable $X \\in [a, b]$ with probability 1, the following holds:\n$$E[exp(\\lambda X)] \\le exp(\\lambda EX + \\frac{1}{2} \\lambda^2 (b - a)^2)$$.\nLemma B.2. (Hoeffding's Lemma, Multivariate) for random variables $Z = f(x_1, \\cdots, x_n)$ where f has the bounded difference property, the following holds:\n$$E[exp(\\lambda(EZ \u2212 Z))] < exp(\\frac{1}{2} \\lambda^2 \\sum_{i=1}^n C_i^2)$$.\nLemma B.3. Empirical Loss defined in Definition 3.1 satisfies the bounded difference condition with constant $c = 1$, \u2200i.\nLemma B.5. (Transformation of probability simplex) Given a proper probability density function $v(x)$ defined on $\\Delta^{n\u22121}$, it is equivalent to the distribution defined on $\\Delta^{n\u22121}_{pr}$ with density $\\frac{v(x)}{\\sqrt{n}}$: \u2200A \u2208 Borel($\\Delta^{n\u22121}_{pr}$), let B = $\\{x \\in \\Delta^{n\u22121} : x_{1:n\u22121} \\in A\\}$. Then $\\int_A v(x)dx = \\int_B \\frac{v(x)}{\\sqrt{n}} dx$. Specifically, this implies $\\frac{volume(A)}{volume(\\Delta^{n\u22121}_{pr})} = \\frac{volume(B)}{volume(\\Delta^{n\u22121})}$.\nLemma B.6. (Gaussian cdf Tail Bound, Gordon [62]) Denote $\\phi(\u00b7)$ as the standard Gaussian pdf. When x > 0,\n$$\\frac{x}{x^2 + 1} \\phi(x) < 1 - \\Phi(x) \\le \\frac{1}{x} \\phi(x)$$\n$$\\frac{x}{x^2 + 1} \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} < 1 - \\Phi(x) \\le \\frac{1}{x} \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}}$$"}, {"title": "Theorems", "content": "Theorem 1. (PAC-Bayesian Generalization Bound for Language Models.) With $\\alpha$ as in Definition 2.1, consider a set of language models $ILIM$, with prior distribution $\\pi$ over $ILIM$.\nGiven any $\\delta \\in (0, 1)$, for any probability measure $\\rho$ over $ILIM$ such that $\\rho, \\pi$ share the same support, the following holds with probability at least 1 \u2212 $\\delta$ over the random draw of $S$:\n\n$E_{LM\\sim \\rho}[R(P_{LM}) \u2013 R_S(P_{LM})] \\le \\frac{1}{\\alpha} [E_{LM\\sim \\rho} R_S (P_{LM}) + \\epsilon]$.\nTheorem 2. (Jailbreak is unavoidable) Assume that an LMs output semantically meaningful expla- nations (Assumption 4.1). Given any $\\gamma$ posterior distribution over LIM, choose a harmful concept c with a direct prompt $(q, c)$ and a threshold p (Definition 2.1), to define the corresponding induced distribution $\\gamma_c$ (Definition 4.1) and division over output simplex (Definition 4.2). An $\\epsilon$-bounded adversary (Assumption 4.2) can find a jailbreaking prompt (Definition 4.3) with probability at least\n\nby using either the direct prompt, such that $P_{LM}(q, c) \\in H_h$; or\nby finding an $\\epsilon$-bounded query $q'$, such that $P_{LM}(q', c) \\in H_h$.\nHere, $\\Phi(\u00b7)$ is the standard Gaussian cdf, $\\gamma_s := max_{x \\in H_s } \\gamma_c(x)$, $\\Upsilon_s := max_{x \\in H_s-H_h(\\epsilon,d)} U(x)$ with U(x) the uniform dis- tribution over $\\Delta^{n\u22121}$, and $a_\\epsilon := a + \\sqrt{n \u2212 1} \\epsilon$, where a writes analytically as $a = \\frac{|E_h (c)| \u2212 1 \u2212 (n \u2212 1)p}{\\sqrt{(n \u2212 1)p(1 \u2212 p)}}$"}, {"title": "PPO", "content": "Following, proximal policy optimization (PPO) [65] is commonly adopted across these implemen- tations, forming the basis of current state-of-the-art language models. The KL-constrained RL Fine-Tuning (RLFT) objective takes the form:\nmax  $E_{x\\sim D_s,e\\sim P_{LM}(\\cdot|x)} [r(x, e)] \u2013 \\beta D_{KL}(P_{LM}(\\cdot|x)||P_{ref}(\\cdot|x))$.\nHowever, PPO tuning can suffer from instability [66] and implementation complication [67]. To overcome these issues, a series of work propose to skip the reward modeling step and learn directly from the preference dataset, with the representative pioneering work by Rafailov et al. [23], namely Direct Preference Optimization (DPO). We summarize the derivation of the DPO objective below, and generalize the objective to the one we use in our experiments, i.e., E-DPO."}, {"title": "Formulas", "content": "$$L_{RLHF"}, "P_{LM}) = -E_{x\\sim D_s,e\\sim p_{LM}(x)} [r(x, e)] + \\beta D_{KL} (P_{LM}(x)||P_{SFT}(x))$$\n$L_{E-RLHF} (P_{LM}) = -E_{x\\sim D_s,e\\sim p_{LM}(x)} [r(x, e)] + \\beta D_{KL} (P_{LM}(x)||P_{SFT}(x_s))$$\n$L_{E-DPO} (P_{LM}) = -E_{(x,e_w,e_l)\\sim D_s} \\log \\sigma(\\beta \\log \\frac{P_{LM}(e_w | x)}{P_{SFT} (e_w | x_s)} \u2013 \\beta \\log \\frac{P_{LM}(e_l | x)}{P_{SFT} (e_l | x_s)}) $$\n$P_{RLFT}(e|x) = \\frac{1}{Z'(x)} P_{ref}(e|x) \\exp(\\beta r(x, e))$$\n$r(x, e) = \\beta \\log \\frac{P_{RLFT}(e|x)}{P_{ref}(e|x)} + \\beta \\log Z'(x)$$\n$P_{DPO} = arg \\min_{P_{LM}} -E_{(x,e_w,e_l)\\sim D} [\\log \\sigma(\\beta \\log \\frac{P_{LM}(e_w | x)}{P_{ref} (e_w|x)} \u2013 \\beta \\log \\frac{P_{LM}(e_l | x)}{P_{ref} (e_l|x)} )]$$\n$r(x, e) = \\beta \\log \\frac{p^*(e|x)}{P_{ref}(e|x_s)} + \\beta \\log Z(x)$$\n$P_{E-DPO} = arg \\min_{P_{LM}} -E_{(x,e_w,e_l)\\sim D} [\\log \\sigma(\\beta \\log \\frac{P_{LM}(e_w | x)}{P_{ref} (e_w|x_s)} \u2013 \\beta \\log \\frac{P_{LM}(e_l | x)}{P_{ref} (e_l|x_s)}) ]$$,", "equation", true]}