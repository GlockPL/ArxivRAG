{"title": "LM Agents for Coordinating Multi-User Information Gathering", "authors": ["Harsh Jhamtani", "Jacob Andreas", "Benjamin Van Durme"], "abstract": "This paper introduces PEOPLEJOIN, a benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, PEOPLEJOIN agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. PEOPLEJOIN comprises two evaluation domains: PEOPLEJOIN-QA, focused on questions about tabular data, and PEOPLEJOIN-DOCCREATION, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic \"organizations\" of 2\u201320 users, simulating natural multi-user collaboration scenarios. We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using PEOPLEJOIN.", "sections": [{"title": "1 Introduction", "content": "In today's fast-paced and interconnected world, effective collaboration is essential for achieving complex tasks and making informed decisions (Papachristou et al., 2023; Gemp et al., 2024). Many decision-making, content creation, and information-gathering tasks require collecting information from multiple people. For example, preparing a list of interns across teams in an organization by reaching out to the leader of each team; preparing a newsletter for project updates might necessitate coordinating with multiple contributors; identifying a suitable time to meet might require several rounds of negotiations (Lin et al., 2024). Identifying what information is available, judiciously determining who to contact, asking precise questions, and compiling research results can be a challenging and time-consuming process\u2014 especially when real-time interaction between team members is difficult to coordinate.\nAt the same time, recent large language models (LLMs), such as GPT-4 (OpenAI, 2023), Phi-3 (Abdin et al., 2024), LLaMa (Touvron et al., 2023), and Gemini (Team et al., 2023), are becoming a crucial building block in developing automated agents that can assist human users with complex tasks (Xi et al., 2023; Wang et al., 2024; Butler et al., 2023). These tasks include chat applications for assisting individual users with searching and summarizing information (such as in Microsoft Copilot Chat\u00b2), and even supporting these users in workplace decision-making (Butler et al., 2023; Kim and Hsu, 2024). Could these agents be extended to improve collaboration among multiple users?\nIn this paper, we introduce PEOPLEJOIN, an evaluation framework for studying effectiveness of LLM-powered agents to assist with multi-user collaboration tasks. Each PEOPLEJOIN task takes place within a fictitious organization with 2-20 employees, some of whom possess a collection of documents necessary to solve some task. One of the users (the initiating user) communicates the task to an agent (Fig. 1). Agents have direct access to the initiating user's documents, and can engage in conversations with other users to gather relevant information. They must rely on limited descriptions of other users, and potentially previous interactions, to determine who to contact for a given task. PEOPLEJOIN comprises two families of tasks: PEOPLEJOIN-QA and PEOPLEJOIN-DOCCREATION, derived from the SPIDER (Yu et al., 2018) and MULTINEWS (Fabbri et al., 2019) datasets respectively. It evaluates agents' ability to answer questions involving complex relational"}, {"title": "2 Challenges in Effectively Steering Multi-User Information Gathering", "content": "The problem of answering user queries by synthesizing information distributed across heterogeneous data sources is most often studied through the lens of database systems (Zaniolo, 1997). Work on query optimization and federated databases (Sheth and Larson, 1990) has sought to address the specific question of how to efficiently answer structured queries without access to a centralized knowledge store. The problem we study in PEOPLEJOIN may be viewed as a generalization of this task to the setting where the relevant information is possessed by people, not structured knowledge bases, and must be obtained via conversation rather than structured queries. An agent to help a user with such requests must address several challenges:\n\u2022 Information fragmentation: In a typical organization, information is often siloed across multiple users, because of differing roles and responsibilities. Some requests may require gathering information from multiple people.\n\u2022 Partial observability: To gather this information, it is often necessary to first determine which collaborators hold relevant information, under incomplete and potentially imprecise information of what information each collaborator might have. Agents for collaborative decision-making might have to engage in multi-turn conversations with various users, refining and adapting requests as needed.\n\u2022 Communication costs: Requests for information require human effort to process and answer; effective collaboration requires efficient communication: effective agents should judiciously send information requests to other collaborators, and avoid asking questions that are likely to be unanswerable.\n\u2022 Complex reasoning and planning: Efficient"}, {"title": "3 Data", "content": "Each PEOPLEJOIN domain comprises a set of organizations. Each organization contains a set of collaborators, and each collaborator has privileged access to a set of documents. The benchmark provides LLM-based simulators for each collaborator, a search interface that can be used to find collaborators, and a messaging interface that can be used to ask collaborators about their documents. Then an agent must take as input a query from one collaborator, use the search and messaging interfaces to interact with other collaborators, and finally return an answer to the originator.\nDrawing analogies between multi-user collaboration tasks and existing multi-datasource tasks commonly studied in NLP, we develop PEOPLEJOIN by re-purposing existing high-quality resources for database question answering (to produce PEOPLEJOIN-QA) and multi-document summarization (to produce PEOPLEJOIN-DOCCREATION)."}, {"title": "3.1 PEOPLEJOIN-QA", "content": "The PEOPLEJOIN-QA dataset evaluates LM agents' abilities to answer questions by aggregating information from multiple collaborators. We construct it by re-purposing SPIDER (Yu et al., 2018), a text-to-SQL benchmark. We transform SPIDER into a multi-user information gathering task by recasting SPIDER tables as \u201cdocuments\", distributed among several users, and interpreting SPIDER questions as queries from an initiating user to an AI agent. In this scenario, answering questions requires identifying which users possess the relevant pieces of information (similar to selecting tables in a database), and then engaging in multi-turn conversations with these users to ask targeted questions (akin to constructing joins between tables).\nSPIDER consists of a set of 200 databases, with a total of over 10K questions. Each database in SPIDER is transformed into an \u201corganization\u201d containing a set of 2-20 distinct users, each with access to a distinct set of documents.\nDocuments Each table in a SPIDER database is converted to one or more documents.\u00b3 We additionally apply the following transformations to elicit a diverse set of information-gathering behaviors:\n1. Split Documents: One of the randomly selected tables is split into two parts (each containing half the rows). This simulates a scenario in which information about a given topic is distributed across multiple individuals. For instance, in Fig. 2, the information in the table department is split between Alice and Dante.\n2. Redirection: We construct scenarios in which a (\"redirecting\") user does not have direct access to some information (e.g. Chen in Fig. 2), but does have knowledge of which other (\"target\") user might have this information (Dante in Fig 2). To answer questions about these tables, agents cannot always contact knowledgeable users directly, and must navigate organizational knowledge hierarchies to find them. Information about other users is available to the redirecting user as an additional document.\n3. Missing Information: In each database, we omit a randomly selected table, making a subset of the queries associated with that organization unanswerable, simulating a scenario in which required information is simply not present (Levy et al., 2017; Rajpurkar et al., 2018) in the organization.\nIn PEOPLEJOIN-QA, each user is allocated one document, and no two users have access to the same document. After we have assigned each organization member a set of documents, we populate the collaborator search interface with hints about what information they might have access to (e.g. Chen likely has information about teacher salaries). We begin by constructing templated descriptions specifying the table name and names of columns, then use GPT-4 to convert these to simpler English statements using a few-shot prompting setup.These transformations by design sometimes result in imprecise or incomplete descriptions, simulating the challenges of selecting a good subset of people to contact under limited information. For example,"}, {"title": "3.2 PEOPLEJOIN-DOCCREATION", "content": "The PEOPLEJOIN-DOCCREATION task evaluates agents not on structured QA, but instead on more open-ended document creation tasks. We derive it from MULTINEWS (Fabbri et al., 2019), a multidocument summarization dataset consisting of sets of news articles on a related topic and single summaries that aggregate information across the articles. We distribute source news articles across multiple users, and require agents to gather these documents (or excerpts from them) and combine them into a target summary.\nTask and Evaluation As in PEOPLEJOIN-QA, each organization is derived from underlying MULTINEWS problem instances. Here, however, multiple problem instances are combined into a single organization: some users have articles on one subject, some users have articles about multiple subjects, and some may have no articles at all. Each organization possesses information about 3 topics, and contains 1\u20137 users, with documents randomly partitioned across users.\nAlso as in PEOPLEJOIN-QA, we create user descriptions for collaborator search by presenting user documents to GPT-4 and querying it for a list of keywords that the user is knowledgeable about (e.g. governor election, GOP, health care).\nStatistics Because of the relatively large size of the documents that must be exchanged to complete these tasks, we construct 200 test instances distributed across 67 organizations. Summaries are"}, {"title": "4 Baseline Agent Architectures", "content": "To demonstrate the usefulness of PEOPLEJOIN as a research platform, we develop and evaluate a reference LM-powered agent implementation to perform tasks by coordinating interactions, retrieving relevant information, and posing targeted queries to other organization members. We consider an event-based reactive agent, which is triggered by user actions: upon getting a message from any organization member, the agent follows ReAct-style prompting loop (Yao et al., 2023), taking actions, making observations, and performing reflection, until it decides to pause and wait for a next event, or terminate the session."}, {"title": "4.1 Actions", "content": "The agent can perform a few types of actions. Document Retrieval: agents have access to documents accessible to the initiating user, by invoking a function search_documents(query: str). Documents are indexed using a standard BM25 index, and the tool call returns a fixed number (upto 3) of documents with the highest matching score. People Retrieval: agents can search through a repository of employee profiles and knowledge areas, by invoking a function search_relevant_people(query: str). However, these expertise profiles may be outdated or imprecise, requiring the agent to navigate uncertainty while coordinating queries. As in document retrieval, descriptions are retrieved using a standard BM25 index. A fixed number (up to 10) of highest-scoring results are returned. Sending Messages: the agent is capable of exchanging messages with any person in the organization. Person Resolution: the agent can resolve a person name to get their user ids, to be used to send messages to them. Turn and Session Completion: agent can mark the current turn or the entire session as completed. Signatures of Python functions corresponding to the allowed actions are provided in the prompt. See Appendix A.1 for the full set of action descriptions."}, {"title": "4.2 Observations and Reflection", "content": "After each action is taken, the agent receives a textual observation. These include retrieved documents or descriptions of collaborators. As is typical in LLM-based agent architectures, these observations are simply appended to the agent's prompt. Before invoking additional actions, the agent may perform reflection actions, corresponding to text-based (\"scratchpad\u201d or \u201cchain-of-thought\") reasoning about its future plans. Our agent represents reflection as tool calls that return no value but remain in the agent's prompt at future timesteps."}, {"title": "4.3 Prompt Structure", "content": "The prompt has 3 parts: action descriptions (outlined above); exemplars; and interaction history.\nExemplars: In each domain, we manually annotated four exemplars (See Appendix A.2 for a full exemplar) with events, actions, and observations. The exemplars are designed to reflect all relevant phenomena in the domain in question, such as dealing with fragmented information, handling unanswerable questions, and managing redirection.\nInteraction History: An event (receiving a message from an employee) triggers LLM into a loop of action prediction, observation, and reflection, till an end of turn or session is predicted. Actions are executed immediately after they are predicted; events, action, and observation are incrementally appended in the prompt in the order in which they occur (see Appendix A)."}, {"title": "5 Evaluation", "content": "PEOPLEJOIN provides metrics for evaluating the efficiency and correctness of user interactions."}, {"title": "5.1 Outcome Metrics", "content": "The most important measure of an agent's effectiveness is its ability to provide the correct response to the user's query. We characterize correctness in different ways for the domains within PEOPLEJOIN.\nAnswer match: For PEOPLEJOIN-QA, we prompt an LLM-based evaluator to compare the agent's final response to the reference answer and output a score in {0,50,100}, where a score of 100 refers to a perfectly matched score (all the expected information was present), a score of 50 refers to a partial match (for example, if only few of the expected list of items were correctly provided), while a score of 0 refers to incorrect results (for example, if the agent claimed it could not find the requested information but gold answer suggests otherwise). The score is predicted by an LLM (gpt-4-turbo),"}, {"title": "5.2 Efficiency Metrics", "content": "An effective agent should not only produce correct answers, but do so while minimizing effort from collaborators. We quantify this using three metrics. Message count (Msg): measures the total number"}, {"title": "5.3 Information Source Metrics", "content": "In both PEOPLEJOIN-QA and PEOPLEJOIN-DOCCREATION, the gold set of documents required to answer a task correctly are known, which also allows us to infer the optimal set of people an agent must contact to arrive at the correct outcome. We collect the set of distinct users contacted by the agent, then compute the precision (P-Prec) and recall (P-Rec) relative to the ground-truth people set, averaged across queries."}, {"title": "6 Experiments", "content": "The PEOPLEJOIN framework includes user simulators that represent collaborators within an organization, along with scaffolding code that enables"}, {"title": "6.1 Results on PEOPLEJOIN-QA", "content": "The max score on Match metric across all methods is only 54.8 (Table 1), achieved by Reactive when used with gpt-4-turbo, demonstrating the overall challenging setup. Moreover, for the same configuration, P-Prec and P-Rec scores are 0.61 and 0.89 respectively, demonstrating scope of further improvement in optimal selection of people to contact. Comparing LLM choices for Reactive, gpt-4-turbo performed better than gpt-40, while phi-3-medium is generally worse on Match and information source selection. Finally, Reactive generally performs similar or better than Reactive-NoRef across LLMs on Match, efficiency, and optimal selection of information sources, demonstrating the usefulness of a reflection step.\nAdditional Comparisons: To put these results in perspective, we additionally compare with following techniques:\n(1) MessageAllOnce, an agent that is encouraged (through prompt instructions and exemplars) to message each person in the organization exactly once, with the same question the user asked. MessageAllOnce results highlight the importance of judiciously choosing who to contact (MsgCnt"}, {"title": "6.2 Results on PEOPLEJOIN-DOCCREATION", "content": "On PEOPLEJOIN-DOCCREATION, among the LLM choices, gpt-4-turbo performs better than gpt-40, which in turn performs better than phi-3 (Table 2). In contrast to results in PEOPLEJOIN-QA, Reactive and Reactive-NoRef variants perform similar, suggesting no usefulness of the reflection step in the document creation task. On this task, an IdealAgent should obtain G-Eval scores of 5, MsgCnt of 6.3, MsgSize of 1592, and #People of 1.7. These results indicate that the document creation task is also challenging,"}, {"title": "6.3 Case Study with Human Participants", "content": "The experiments discussed above rely on simulated users. To complement this, we conducted a human evaluation study in which real users took on the roles of certain collaborators in the experiment. The goal of this study was to assess whether the agents perform the task with similar efficacy when interacting with human users compared to a fully simulated environment. Like simulated users, human participants (Appendix B.4) had access to the documents associated with their assigned personas. Messages from the agent indicated that they were generated by an automated system. While participants were free to respond as they saw fit, they were instructed to engage as respectful colleagues within a business setting.\nThe study was conducted on 100 randomly selected examples from the PEOPLEJOIN-QA dataset. In each instance, one collaborator role was played by a human participant. To ensure meaningful interaction, rather than selecting personas randomly\u2014 which could result in cases where the human collaborator was not contacted by the agent- we specifically picked the human collaborator to be among the gold set of individuals the agent needed to contact for the test example in question."}, {"title": "7 Related Work", "content": "AI-mediated collaboration and negotiations: Recent research in human-AI collaboration has explored various strategies for facilitating decision-making and negotiations among multiple users. Lin et al. (2024) examines how AI assistants can assist humans through natural language interactions to make complex decisions, such as planning a multi-city itinerary or negotiating travel arrangements among friends. Gemp et al. (2024) focus on how game-theoretic approaches that can guide LLMs in tasks like meeting scheduling and resource allocation. Past work (Papachristou et al., 2023) has also explored the role of LLMs in facilitating group decisions, such as selecting a meeting time or venue, where LLM agents analyze individual preferences from conversations. In contrast, PEOPLEJOIN focuses on LM agents for coordinating multi-user information gathering.\nMulti-hop reasoning and task decomposition: In our setup, an agent needs to compile information from multiple sources, a theme shared with prior work in multi-hop QA (Welbl et al., 2018; Yang et al., 2018) and multi-document summarization (Liu et al., 2018; Fabbri et al., 2019). Past work on solving complex tasks by decomposing them (via prompting) into simpler sub-tasks (Wolfson et al., 2020; Khot et al., 2022; Jhamtani et al., 2024) is also relevant. Compared to such past work, our setup requires additional steps of finding the relevant users, posing apt questions, compiling the gathered information, and doing so with minimum communication overhead possible."}, {"title": "8 Conclusions and Future Directions", "content": "PEOPLEJOIN is a new benchmark designed to evaluate the role of language model (LM) agents in facilitating collaborative information gathering within multi-user environments. It comprises two domains, PEOPLEJOIN-QA and PEOPLEJOIN-DOCCREATION, which challenge LM agents to handle tasks related to question-answering and document creation. Experiments with popular LM agent architectures revealed both their potential and limitations in accurately and efficiently completing"}, {"title": "Limitations", "content": "PEOPLEJOIN consists of two tasks and is in one language (English). Future work could explore further expanding the domains and supported languages. We make the simplifying assumption that an agent in our setup can engage only in dyadic conversations. Exploring more topologies such as group chats (Wu et al., 2023) would bring-in additional challenges. We designed the domains and the experiment setup to study the effectiveness of the LM agents on a diverse set of information gathering behaviors. However, our analysis did not model all the possible factors in a real-world. Future work can explore additional factors such as turn-around speed and reliability of the response from a collaborator, how busy a person is, and various social dynamics that can be at play in organizations."}, {"title": "Ethics Statement", "content": "Allowing AI agents the capability to send messages to other users without fine-grained supervision presents a trade-off between saving user time and maintaining control. While autonomy can streamline workflows by eliminating the need for constant user confirmation, verifying key actions helps ensure accuracy and user oversight. While we studied the task in a sand-boxed environment, practitioners should carefully choose the degree of autonomy granted (for example, a more conservative approach would be to get user confirmation before every message that is sent)."}, {"title": "A Additional details on approach", "content": null}, {"title": "A.1 Action descriptions", "content": "Listing 2 shows the signatures and docstrings of Python functions corresponding to the set of allowed actions."}, {"title": "A.2 Exemplars", "content": "A fully annotated exemplar for question answering domain is provided in Listing 3, while a fully annotated exemplar for summarization domain is shown in Listing 4."}, {"title": "A.3 Action parsing failures", "content": "We observed that parsing a predicted action into an allowed Python function and its parameters fails on a small fraction of cases. In such failure cases, we append a simple string \u2018# Failed to parse the predicted action. I will predict the next action again, following the allowed function signatures only' to the prompt, and decode the next action again. This process is followed upto a maximum of three times, post which the session is terminated."}, {"title": "A.4 Overview of the prompt structure", "content": "An overview of the prompt structure is available in Table 5."}, {"title": "B Additional details on Experiment Setup", "content": null}, {"title": "B.1 Match score", "content": "Prompt outline used to compute Match score is shown in Listing 1."}, {"title": "Correlation with Human Rating", "content": "One of the authors manually labeled 50 randomly selected outputs from Reactive (with gpt-4-turbo as LLM) considering the same reference instructions and examples as in the prompt discussed above. We observe Cohen's Kappa score of 0.81 between manual judgement ratings and Match score, suggesting a high agreement of the LLM-based Match metric with human judgement ratings."}, {"title": "Stability", "content": "We conducted an analysis where we rerun the Match scores in Table 1 three times, and the maximum change we observed in any value"}, {"title": "B.2 User Simulators", "content": "User simulator prompt, shown in Listings 5, consists of a basic set of instructions at the top, followed by five examples of diverse situations a user can face (either as the initiating user, or as a teammate receiving a request). Each examples consists of a user description, the set of documents available to the user, and any conversation history so far."}, {"title": "B.3 Qualitative Examples", "content": "Listings 7 through 10 show randomly picked test examples from both the domains, demonstrating success as well as failure cases for Reactive."}, {"title": "B.4 Human Evaluation Study", "content": "Additional details about human participants: We recruited 5 participants, who each carried out 20 human-in-the-loop tasks. All the human participants are US graduates and well-versed with the English language. All participants are paid above the minimum wage requirements of the region. Participants were given the same instructions and examples as in the simulated user prompt."}, {"title": "C Additional details on datasets", "content": "SPIDER dataset is available under CC BY-SA 4.0 license.5. MULTINEWS dataset is available for research purposes.6"}]}