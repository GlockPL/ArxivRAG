{"title": "Pareto Optimal Algorithmic Recourse in Multi-cost Function", "authors": ["Wen-Ling Chen", "Hong-Chang Huang", "Kai-Hung Lin", "Shang-Wei Hwang", "Hao-Tsung Yang"], "abstract": "In decision-making systems, algorithmic recourse aims to identify minimal-cost actions to alter an individual's features, thereby obtaining a desired outcome. This empowers individuals to understand, question, or alter decisions that negatively affect them. However, due to the variety and sensitivity of system environments and individual personalities, quantifying the cost of a single function is nearly impossible while considering multiple criteria situations. Most current recourse mechanisms use gradient-based methods that assume cost functions are differentiable, often not applicable in real-world scenarios, resulting in sub-optimal solutions that compromise various criteria. These solutions are typically intractable and lack rigorous theoretical foundations, raising concerns regarding interpretability, reliability, and transparency from the explainable AI (XAI) perspective. To address these issues, this work proposes an algorithmic recourse framework that handles non-differentiable and discrete multi-cost functions. By formulating recourse as a multi-objective optimization problem and assigning weights to different criteria based on their importance, our method identifies Pareto optimal recourse recommendations. To demonstrate scalability, we incorporate the concept of e-net, proving the ability to find approximated Pareto optimal actions. Experiments show the trade-off between different criteria and the method's scalability in large graphs. Compared to current heuristic practices, our approach provides a stronger theoretical foundation and better aligns recourse suggestions with real-world requirements.", "sections": [{"title": "1 Introduction", "content": "Deep learning has been the most popular technique in Artificial Intelligence (AI) [28], [43], [41], [26], [8], [36], [35], and people increasingly rely on AI for generating decisions [42], [27]. However, the opacity of deep learning models complicates tracing and understanding decision rationales [9], [2], [34]. This issue becomes critical in systems involving safety or human activity, such as the 2018 Uber autonomous vehicle accident [22], racial bias in image restoration [15], or inappropriate advice from medical chatbots [10]. These incidents raise fundamental trust concerns in AI systems.\nThe use of recourse in decision-making and recommendation systems is driven by the need for transparency and accountability, allowing individuals to understand, question, or alter decisions that negatively impact them, such as in loan approvals, medical diagnostics, or education. By integrating recourse, AI systems align more with user needs, enabling participation and influence in decision-making, thereby promoting trust and fairness in AI applications [5]. Current algorithmic recourse methods are often gradient-based, utilizing computational efficiency to navigate high-dimensional solution spaces and identify necessary modifications [12], [19], [31], [37]. These methods incorporate constraints to ensure feasible and applicable changes, adhering to ethical and legal standards [14], [39].\nHowever, the application of recourse faces significant challenges [18], particularly in navigating the high-dimensional space of user features to pinpoint the optimal recourse. In real-world scenarios, determining the most effective recourse is complicated by the multifaceted nature of user attributes and the limitations of existing methods, which often yield suboptimal solutions that compromise various goals [38]. These methods lack thorough theoretical analysis and often rely on heuristic methods such as gradient descent, raising concerns about their interpretability and the transparency they are meant to enhance. In the following, we address those challenges in three directions.\n1. Multi-Cost Scenarios: Dealing with multi-cost scenarios in algorithmic recourse methods is complex. Real-world decisions are influenced by multiple types of costs, such as money, time, and effort. Current methods often optimize a single cost, neglecting trade-offs between different costs. Gradient-based approaches handle bi-criteria scenarios by merging loss functions, but"}, {"title": "2 PROBLEM DEFINITION", "content": "We consider a fixed predictive model $h : X \\rightarrow Y$, with $X = X_1 x X_2 ... X_d$. Each attribute $X_i$ can be either a continuous or discrete value (e.g., categories). The output can be either binary class $y = \\{0, 1\\}$ or stochastic $Y = [0, 1]$, which is the probability of the user classified into 1. The 0 and 1 represent the negative outcome and positive outcome, respectively. A corresponding application example can be a loan approval system where 0 means the \"loan denied\" and 1 means \"loan approved.\"\nGiven a set of n accessible data points $(x_1, x_2, \\ldots x_n), X_i \\in X$ and a set of k cost functions $C = \\{C_1, C_2, \\ldots c_k\\}$, one can construct a directed graph which we called actionability graph, where each node is an accessible data point (such as training samples). There is a directed edge from u to v if a feasible action exists from u to v. Each cost function $c_i : X \\times X \\rightarrow R^+$ is a metric that represents the \"distance\" of two data points. Notice that, the cost functions can be non-differentiable, an attribute of the education level is discrete and non-differentiable. Now, consider a specific points where h(s) = 0, a feasible path P in the actionability graph is the one that every edge in this path is a feasible action. By abusing the notation of cost function c, we denote the total cost of path P is c(P) for the function c, which can be other aggregation functions as long as they are metrics, such as summation or any maximum function.\nWe define path P as dominated by path Q if and only if for any cost function $C_i, C_i(P) \\geq c_i(Q)$. Our objective is to find a set of feasible paths in the graph such that the end of each path is a recourse point t, where h(t) = 1 and each path is Pareto optimal concerning all the cost functions. That is, taking summation as the example, path P is Pareto optimal if and only if there does not exist a path Q such that for any cost function c, $\\sum_{(x,y) \\in P} c(x, y) > \\sum_{(x,y) \\in Q} c(x, y)$."}, {"title": "3 MULTI-CRITERIA RECOURSE", "content": "Our approach involves handling the minimization of cost for a single edge, which can be analogized to the concept of finding the shortest path. We manage this by maintaining and updating a table to capture the trade-offs among various types of costs. This table is instrumental in recording and managing the interplay of different cost criteria on different paths. In the following, we assume the model"}, {"title": "Algorithm 1 Pareto-shortest-path", "content": "Algorithm 1 computes the shortest paths from a given source vertex to all other vertices in a graph. $D_v^l$ denotes the Pareto table of v that represents the set of current Pareto paths in iteration l for reaching vertex v from the source vertex. Line 2 to Line 5 are initialize steps. All items in $D_s^0$ are empty except for the source vertex. We set the distance to itself $D_s^0$ as zero for all cost dimensions. Line 6 to Line 11 uses Dynamic Programming to iteratively find all the Pareto paths. It iterates over the vertices excluding the source and conducts a tailored merging process, for each edge (u, v) in the graph, performs the update operation. The update operation extends $D_v^{l-1}$ to $D_v^{l}$ by first concatenating the distance from the vertex u with the multi-cost $W_{uv}$ and then pruning the dominated path. The concatenating step is the same as the standard operation in Bellman-Ford except each path in $D_v^{l-1}$ has multi-cost. The operation time here increased by at most k times. The prune operation is employed to refine the multi-cost estimate of\npaths in $D_v^l$ by comparing the paths between $D_v^{l-1}$ and $D_u^{l-1} + W_{uv}$ and only record the non-dominated paths into $D_v^{l}$. Finally, Line 11 reports all the Pareto paths for all possible recourse points t, where h(t) = 1.\nAnalysis In Algorithm 1, the time complexity analysis involves two main steps: concatenate and prune, which are repeated for at most $\\eta$ iterations. Denote $\\tau$ as the maximum size among all Pareto tables and $\\gamma$ as the maximum degree among all vertices. In the concatenation step, for each vertex v, all the incoming neighbors u of u have been concatenated via $w_{uv}$, which takes at most $O(\\gamma\\tau)$. In the pruning step, $D_v^l$ is updated to keep the Pareto paths only. This is related to finding the maxima of a point set problem [30]. Naively, we can check all pairs of paths with each criterion to remove the dominated ones, which takes $O(ky^2\\tau^2)$. However, since we are not taking an arbitrary point set but merging Pareto tables from all the incoming neighbors. During the pruning steps, the Pareto table is maintained via lexicographic orders. Thus, the running time can speed up, depending on the number of the cost functions k. When k = 2, since the first cost is always ordered we can compare the second cost directly, which takes $O(\\gamma\\tau)$. When k = 3, one can use basic sorting to remove dominated paths, which takes $O(\\gamma\\tau \\log \\gamma\\tau)$. This can be further sped up by using the data structure of van Emde Boas tree [21], takes $O(\\gamma\\tau \\log \\log \\gamma\\tau)$. When k > 3, one can sort the rest of the criteria with lexicographic order, which increases the time complexity to $O(\\gamma\\tau \\log^{k-4} \\log \\log \\gamma\\tau)$ [13]. Overall, the time complexity of Algorithm 1 is $O(\\gamma\\tau \\log^{k-4} \\log \\log \\gamma\\tau * \\eta |E|)$, where |E| represents the number of the edges.\nIn reality, the size of the Pareto table $\\tau$ is bounded by the number of different paths in the graph, this may vary depending on the specific problem instance and the number of criteria considered. Thus, one may face the scalability issue when the graph is large with high degrees. We address this issue in Section ?? and provide a solution under those circumstances, the correctness of Algorithm 1 is provided in the full version of this work.\nAlgorithm 1 finds all the Pareto optimal paths from the source to any endpoint t, where h(t) = 1."}, {"title": "4 SCALABILITY ENHANCEMENT", "content": "In Section 3, we show that finding all Pareto optimal on the actionability graph is crucial to the size of the Pareto tables $\\tau$, edge size |E|, and number of iterations $\\eta$. Since $\\eta$ corresponds to the number of hops of the output paths, which is usually as a constant (considering in reality that a long feasible path is redundant and non-interpretable), the bottleneck of the running time is mainly on $\\tau$ and |E|. Additionally, $\\tau$ is bounded by the number of different paths from each pair of nodes, which highly depends on the size of the vertices and the connectivity. To decrease the graph size and simplify the connectivity structure, one idea is to shrink the vertices of the graph such that there are only a small number of \"representative\" nodes, and the shortest path in this shrunk graph still preserves\nor approximates the distance of the original graph. This is the idea of core-set from the computational geometry perspective (see survey in [1]). The challenge here is that the cost functions are not specific but highly general. Additionally, we also need to incorporate all the k cost functions to get the Pareto optimal. Fortunately, the computation of the cost functions usually contains some structures rather than arbitrary values for any pair of points. This inspires us to utilize the idea of $\\epsilon$-net [16] to shrink the size of the graph and also ensure the quality.\nTo explicitly explain our idea, we first define the notation of shrinkable.\""}, {"title": "Definition 1.", "content": "Given G = (V,E) and a cost function c, we say a vertex i is k-shrinkable to vertex j if and only if (p, i) \u2208 E\n(p, j) \u2208 E and $c(p, j) \\leq \\kappa c(p, i)$"}, {"title": "Definition 2.", "content": "Given the approximation factor k, one can iteratively shrink all the shrinkable vertices in the graph until there is no shrinkable vertex anymore. We call this induced subgraph a shrunk graph $G_s$ and the one with the smallest cardinality is $G_\\epsilon$. Obviously, any $G_s$ preserves k-approximation factor. The shortest path between any pair of the nodes in G has another path in $G_s$ which is at most $kl$ times, where l is the number of the hops of the path. However, finding the $G_\\epsilon$ is highly non-trivial. It depends on the order of vertices in the shrinking procedure. A toy example is in the following. Consider a graph with vertices {p, i, j, r} and edges {(p, i), (p, j), (p, r)} where $c(p, i) = \\kappa c(p, j) = \\kappa^2 c(p, r)$, if j shrinks to i, then i, r are not shrinkable. On the other hand, if i shrinks to j, then r can shrink to j too. Thus, we want to have another subgraph that catches most properties of Gs and can be generated efficiently. This is the place where the $\\epsilon$-net joins into our work. In the following, we will first introduce the formal definition of $\\epsilon$-net and then show how to utilize it under our context."}, {"title": "Definition 3.", "content": "Given a range space (X,R), let A \u2282 X be a finite subset, and 0 < \u0454 < 1. Then a subset N \u2282 A is called an e-net of A w.r.t to R if\n\u2200r\u2208 R, $|r \\cap A > \\epsilon |A| \\rightarrow r \\cap N \\neq 0$"}, {"title": "Definition 4.", "content": "We define the e-net under our context. We say $G_e$ is an e-net of G if for any vertex v in some shrunk graph $G_s$ that is shrunk by more than en vertices, then either $v \\in G_e$ or $u \\in G_e$, where v is shrinkable to u.\nTo see Definition 2 and Definition 3 are equivalent, one can see the element r of a ranger \u2208 R is a subset of V which is an instance of the shrinking procedure. That is all the elements in r shrink into a point in some graph $G_s$. Thus, an e-net should include one of the points in r, which leads to Definition 3. The interpretation of $G_e$ is that it includes the majority of vertices (i.e., which is shrunk from en vertices) among all the $G_s$."}, {"title": "5 EXPERIMENT", "content": "To demonstrate our algorithm, we conducted experiments using two datasets. MNIST dataset [23] and the Adult dataset [3]. The experiments have two parts. The first part is to show the ability of our approaches to find all the Pareto optimal recourse paths with multi-criteria in different scenarios. The second part shows the quality of the Pareto recourse paths under different numbers of random samples. This supports the argument that one can use random samples as the e-net with a certain number of samples."}, {"title": "5.1 Scenarios", "content": "MNIST: Our first scenario is to find a recourse path from one digit to another digit with the higher number where the number of each picture can never go lower(e.g. 3\u21925\u2192 7 is allowed, but 3 \u2192 6 \u2192 5 \u2192 7 is not). We commence by extracting a subset from the original MNIST dataset [23]. we define cost\u2081 as the absolute difference between the images. An actionable edge exists if and only if the number of the first image is lower than the second image, aiming for a recourse path where the number incrementally increases. cost2 is the typical L2 distance between the images. The overall cost of cost\u2081 is the maximum value of cost\u2081 among all edges in the path, while the overall cost of cost2 is the sum of cost2 values among all edges.\nAdult: The Adult dataset is widely employed in predictive modeling to determine whether an individual's income exceeds $50,000 per year based on 14 features, such as age, education, occupation, hours worked per week, and others. The following outlines the specific steps and methodologies we employed in our experiment, our goal is to find the recourse path with an income of more than $50,000.\nWe established an actionable dataset based on a set of predefined actionable criteria. During this process, some criteria were designated as immutable to reflect real-world conditions. For instance, in the Adult dataset, gender was considered immutable. From this actionable dataset, we randomly selected 256 instances that met the predefined actionable criteria for further analysis. A point with the lowest predicted probability by a simple one-layered MLP model was selected as the starting point.\nTo create the actionability graph, we utilized a KNN-graph (k=4) from the standardized dataset and applied our multi-cost shortest path algorithm to explore optimal changes in three key dimensions: age, education, and hours per week. We employed the non-standardized data to calculate the shortest paths based on these criteria, aiming to identify the most optimized change strategies within these three dimensions. To ensure a realistic application, we designed different cost functions for each criterion. The first cost is the percentage of the negative value of the Kernel Density Estimation (KDE) score, which is introduced in FACE [29]. This cost is also known as the negative log-likelihood (NLL), where a lower value signifies a higher density. The remaining costs are the L1 distances between the three key dimensions. Taking \"age\" as an example, when designing the cost function, it is notable that age cannot decrease during the entire recourse process."}, {"title": "5.2 Multi-criteria paths", "content": "Figure 2 present all the Pareto optimal paths we found in MNIST with two cost functions cost1, cost2. The starting point is an arbitrary image labeled as 2 and we only sampled 256 images to construct the actionability graph. This is to simplify the graph for clear visualization. cost\u2081 represents the maximum number"}, {"title": "5.3 Scalability", "content": "This experiment is designed to see the quality of the recourse paths when the graph is too large and we have to use e-net to run our algorithm. We use the MNIST dataset and randomly sample points with sizes 128, 256, 512, 1024 and run 32 trials. We then compare the Pareto optimal paths under different sampling sizes. Figure 4 is the reported result. The x-axis is the criterion of cost\u2081 and the y-axis is the box-plot of cost2 value among 32 trials. Generally, one can see that when the number of samples increased, the quality of the Pareto path was improved too. This is reasonable since the algorithm is more likely to find a better path for lower cost2. However, the improvement is not that clear when the number of samples is large. For example, in the sampling size of 512 (blue box), the average value of cost2 when cost\u2081 = 7 and cost\u2081 = 9 is smaller than the ones with a sampling size of 1024 (purple box). We believe that is because the influence of the sampling size on the quality is not that significant and the reported cost2 value is fickle to the random sampling process. Responding to the discussion in Section 4, this supports the argument of the e-net which shows a random sampling can bound the quality when the size is large enough."}, {"title": "6 CONCLUSION", "content": "In conclusion, our algorithm proposes a novel idea that can be generalized to non-differentiable or discrete cost functions in the field of recourse. The algorithm is applied to multi-cost scenarios where users can combine their background knowledge in professional fields, set the most suitable cost function, and then finally get a more practical counterfactual. We show that our algorithm can find all the Pareto recourse plans optimally and can be scaled to a large graph with the utilization of e-net. We conduct experiments on two data sets separately. For the MNIST data set, we compared the performance of Pareto optimal paths under different sampling sizes and showed the result that transfers from one handwriting number into another and offers the whole process. For Adults, we show the diversity in each cost function, and those diverse cost functions finally lead to multi-Pareto optimal solutions."}]}