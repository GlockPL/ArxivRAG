{"title": "Accessible Smart Contracts Verification:\nSynthesizing Formal Models with Tamed LLMs", "authors": ["Jan Corazza", "Ivan Gavran", "Gabriela Moreira", "Daniel Neider"], "abstract": "When blockchain systems are said to be trustless, what this\nreally means is that all the trust is put into software. Thus, there are\nstrong incentives to ensure blockchain software is correct-vulnerabilities\nhere cost millions and break businesses. One of the most powerful ways of\nestablishing software correctness is by using formal methods. Approaches\nbased on formal methods, however, induce a significant overhead in terms\nof time and expertise required to successfully employ them. Our work\naddresses this critical disadvantage by automating the creation of a for-\nmal model-a mathematical abstraction of the software system which\nis often a core task when employing formal methods. We perform model\nsynthesis in three phases: we first transpile the code into model stubs;\nthen we \"fill in the blanks\" using a large language model (LLM); finally,\nwe iteratively repair the generated model, on both syntactical and se-\nmantical level. In this way, we significantly reduce the amount of time\nnecessary to create formal models and increase accessibility of valuable\nsoftware verification methods that rely on them. The practical context\nof our work was reducing the time-to-value of using formal models for\ncorrectness audits of smart contracts.", "sections": [{"title": "1 Introduction", "content": "Modern blockchain platforms such as Ethereum and Cosmos extend the underly-\ning consensus technology pioneered by Bitcoin towards a generalized computing\nplatform. Rather than solely reaching consensus on transactions modifying the\nstate of a ledger, these platforms also enable consensus on program executions.\nRoughly speaking, they are platforms for decentralized computing capable of\nexecuting stateful protocols called smart contracts in a trustless manner. For\nexample, one may write a smart contract for holding an auction without having\nto put one's trust into a centralized marketplace provider such as eBay."}, {"title": "2 Related Work", "content": "In this section, we provide an overview of existing research and technologies\nrelated to formal methods for blockchain systems and LLM code generation,\nand place our work within this broader context."}, {"title": "2.1 Formal Methods for Blockchain Systems", "content": "Software vulnerabilities in the blockchain world imply high financial and repu-\ntational loss. Once on-chain, the software with a fix cannot be deployed quickly.\nTherefore, there has been a lot of effort in ensuring blockchain systems are cor-"}, {"title": "2.2 LLM Code Generation and Verification", "content": "LLMs optimized for code generation showed stunning results [17,29]. One con-\ncern when using LLMs for generating code is that they are inherently stochastic\nand offer no assurance that the generated code will be error-free.\nTechniques such as PICARD [30] and Synchromesh [28] utilize parsers to\nsequentially filter out undesirable tokens at inference time. Synchromesh addi-\ntionally introduces Target Similarity Tuning (TST), a mechanism for selecting\nfew-shot demonstrations for in-context learning. While TST is highly relevant for\nall techniques which rely on few-shot prompting, we did not incorporate it in our\napproach due to the small number of few-shot demonstrations at our disposal\nand generally satisfactory performance without it. We discuss our prompting\nmethodology and approach to LLM-based code synthesis in Section 4.2.\nAt their core, these methods enforce syntactic and a limited number of\nsemantic rules by restricting the token set the LLM can sample from, based\non a provided syntax. However, the rules enforceable at the parser level are\nlimited and require an expert to encode them. Furthermore, while correct-by-\nconstruction synthesis is a valuable ideal, we found that code generation capabil-\nities of advanced LLMs have progressed to a point where existing inference-time\napproaches, especially those that rely on syntax level guarantees, are becoming\nless relevant.\nTechniques like Clover [31] cross-verify multiple LLM-generated artifacts, in-\ncluding code, documentation, and formal specifications. This bears similarity to\nour approach, in which the LLM generator gets feedback from other generated\nparts (discussed in more detail in Section 4.3). A radically different approach\nis taken by Sun et al. [32], who experiment with changing a programming lan-\nguage's grammar to make it a better fit for LLM-based generation."}, {"title": "3 Preliminaries", "content": "In this section, we cover the necessary background on model-based techniques,\nthe Quint modeling language, and CosmWasm smart contracts, which form the\nprimary focus of our work."}, {"title": "3.1 Model-based Techniques", "content": "In the context of this paper, a model is an abstract, formal representation of\na software system that can be automatically analyzed. Common examples in-\nclude 1) writing models in the TLA+ [24] modeling language and analyzing them\neither by an explicit model checker TLC [36] or by a symbolic model checker\nApalache [22]; and 2) writing models in the Alloy modeling language and an-\nalyzing them with the Alloy analyzer [20]. In this paper, we focus on models\nwritten in Quint (see Section 3.2 for a brief introduction to Quint).\nModels are in general the most useful when modeling a (distributed) proto-\ncol before the implementation. Then, verifying the model can point to protocol\nproblems early in the process (or assure us that the approach makes sense). In\nour auditing practice, we most often come in once the software is already imple-\nmented, and we focus on finding bugs. We can use models to help us find bugs\nin two ways: by verifying the code-based model, or by generating tests from the\nideal-based model.\nIn the code-based approach, we write our model to be as close as possible\nto the implementation. Then, finding violations of invariants with respect to\nthe model will correspond to finding bugs in the code. Of course, the pending\nquestion is how we can be sure that our model corresponds to the code: this is\nensured by running model-based tests. The model generates traces, which are\nthen executed against the implementation to verify conformance.\nA symmetrical situation occurs in the ideal-based approach. There, we write\na model to correspond to what we understand to be the correct functioning of\nthe software. From the model, we generate traces which are executed against\nthe implementation and after each step of the trace, the state of the model is\ncompared to the state of the implementation. If there is a mismatch, it suggests\nthere is a bug in the implementation. A similar validity question is pending here,\ntoo: how can we be sure that our model is correct? We can be assured of the\nmodel's correctness by writing and verifying the invariants that are expected to\nhold.\nThese two approaches represent somewhat extreme cases. In reality, a model,\ntests, and invariants may co-evolve as our understanding of the system improves.\nFurthermore, as noted already by many practitioners, one less tangible value of\nwriting models is that it makes our thinking about the (audited) software much"}, {"title": "3.2 Quint", "content": "Quint [10] is a modeling language based on TLA+ [24]. Quint's main advantage\ncompared to TLA+ is being significantly more in line with standard development\npractice.\nIt was designed according to the following principles:\nWhenever a language concept has a \"standard\" syntax in mainstream lan-\nguages, Quint adopts it. Conversely, when a concept is specific to modeling\nlanguages, Quint's syntax makes it explicit. As examples: Quint expresses\ninequality with != (as opposed to # and /=, used in TLA+); when a value\nneeds to be chosen nondeterministically, Quint demands a nondet modifier.\nUsers can specify types, and types are checked as soon as possible, identifying\nmistakes sooner rather than later.\nIndentation is encouraged with IDE tools, but does not carry meaning.\nThere is a clear separation between functions that modify the model state,\nand those that do not.\nOne can interact with parts of the model (i.e., in a REPL), before the whole\nmodel is finished.\nModels written in Quint remain compatible with TLA+ and transpilation is\npossible, so all existing tools for TLA+ can be used for Quint.\nWhile most of these points may seem straightforward, the accepted wisdom\namong TLA+ users was that a modeling language does not need to follow modern\nprogramming practices. Even more, that trying to stay sufficiently away from\nprogramming languages' mores is beneficial for modeling [24].\nIn our experience, Quint's principles add value by integrating modeling into\nthe development process. Quint's intuitive language and support for integration\ninto standard, ergonomic tools have been essential for bringing modeling into\npractice."}, {"title": "3.3 CosmWasm Smart Contracts", "content": "CosmWasm [2] (CW) is a smart contract platform of the Cosmos ecosystem. The\nname comes from the fact that CW contracts are compiled into WebAssembly\n(Wasm), enabling CW contracts to be written in multiple languages (the de\nfacto standard programming language for CW at this point is Rust).\nA CosmWasm smart contract consists of type definitions which define how\nthe contract's data is stored on the blockchain, and entry-point implementations\nthat operate on that data based on the messages that the contract receives.\nMultiple contracts can communicate by sending each other messages, which are\nall delivered and processed as part of the same transaction as the initial user's\nmessage."}, {"title": "4 Model Generation", "content": "As stated in the introduction, our goal is to generate a Quint model for a given\nsmart contract in three steps. In the first step, which we call mechanical gener-\nation, we use the contract's Rust source code to determine the overall structure\nof Quint model. This enables us to generate data types, function signatures, and\nparts of code that follow common CosmWasm patterns, such as receiving and\nreplying to messages and mutating the contract's internal state. However, we\nleave placeholders, which we refer to as stubs, in place of functions that encode\nthe contract's main logic. Filling these placeholders mechanically would require\none to develop a complete Rust-to-Quint transpiler, including translation of side\neffects and loops, which is an exceedingly difficult task. Additionally, mechan-\nical generation would necessarily follow the Rust code by the letter, failing to\nsufficiently abstract away from it. (In Figure 3 in Appendix C, we provide an\nexample of a stub along with the original Rust code that induced it.)\nInstead, in the second step, we prompt the LLM to complete the missing\nparts of the model. In order to help the LLM accurately complete this task, we\nprovide it with general information about Quint, user-provided NL descriptions,\nand I/O specifications. We also include relevant parts of the contract's Rust\nimplementation along with results of step 1 (primarily, stubs' signatures that\nthe synthesized code must adhere to). Most importantly, we provide the LLM\nwith few-shot examples which demonstrate how to generate Quint code.\nFinally, in the third step, we conduct a series of tests on the generated model\nto ensure its correctness, and again employ the LLM to repair any identified\nerrors based on the error details."}, {"title": "4.1 Mechanical Generation for Stubs", "content": "The mechanical generation phase analyzes a CosmWasm contract written in Rust\nand produces a corresponding Quint model with the overall structure, without\nattempting to generally translate Rust expressions. It introduces placeholders\nfor the core elements that it cannot translate. It also generates an adapter test\nfile in Rust, which reads traces produced by the generated model and replays\nthem against the implementation. The tool is called cosmwasm-to-quint and\nthe source code is available on GitHub4.\nThis section touches on the technical aspects of Rust tooling and their ap-\nplication in the generation process. Readers unfamiliar with these details may\nchoose to skip it, or refer to the Rust documentation5. Furthermore, it also\ntouches on the structure of Quint models. Appendix F provides a quick primer\non the basic structure of Quint models.\nThere are several ways to work with Rust code, from building a new parser\nto using existing translation libraries. After considering a few of those options,\nwe chose a tool called rustc_plugin [12] for that purpose. rustc_plugin is"}, {"title": "4.2 LLM-based Synthesis for Contract Logic", "content": "In this phase of the generation, we instruct the LLM to complete the Quint stubs,\ni.e., the partial model with placeholders in place of functions that implement\nthe contract logic. In addition to the stubs that resulted from the mechanical\ngeneration phase, we assume that the following three types of inputs will be\nmade available by the user.\n1. CosmWasm source code, which contains the data structures that the contract\n stores on the blockchain, Rust implementations of message handlers, and\n helper data structures such as possible error formats.\n2. Natural-language descriptions of the functionality that each stub is supposed\n to encode, which will steer the LLM's output towards Quint code that cor-\n rectly captures the contract logic.\n3. Two input-output specifications for each placeholder, which will be used to\n steer generation, and repair the generated code in case of errors.\nThe result of the mechanical generation phase is a Quint model with \"stubs\"\nin place of implementations of pure Quint functions that capture the function-\nality of the smart contract message handlers. They correspond to CosmWasm\nmessage handlers in the contract's Rust source code (and any utility functions\nthat they may use). Stubs are pure Quint functions that are intended to capture\nthe main contract logic. Pure functions are defined solely by their input-output\nbehavior and have no side effects, i.e., they do not modify or read any external\nstate implicitly, nor perform any I/O operations. The core idea of our approach\nis to factorize the complex task of generating an entire model into a set of more\ntractable pure function synthesis tasks. Crucially, the LLM may complete each"}, {"title": "4.3 Testing and Repair Phase", "content": "In this section, we outline our strategy for addressing potential errors that arise\nduring generation. As highlighted in Section 4.2, we independently repair each\nfunction in the model. Because we never generate code with side effects, it suffices\nthat the generated code correctly implements the intended behavior for all I/O\nspecifications. This stems from the fact that the model's overall structure and\nstate management code have been generated deterministically. We divide errors\ninto three distinct categories, based on the earliest point at which they can be\ndetected, namely\n1. static errors (e.g., incorrect Quint syntax), which are detected before exe-\n cuting any tests;\n2. runtime errors (e.g., calling a nonexistent method), which are detected\n when the generated is executed; and\n3. semantic errors, which are detected once the generated pure function re-\n turns an output that is inconsistent with a user-provided input-output spec-\n ification.\nAfter the initial generation is completed, we analyze it for the presence of\neither static, runtime, or semantic errors. If we detect them, our strategy is to\nenter a loop of test-and-repair rounds, as shown in Figure 1. In each round, we\ngather relevant information about the probable cause (such as error messages)\nand construct a prompt asking the LLM to repair the code based on this context.\nThe LLM's response then replaces the previous, erroneous fragment of code, and\nwe check the new version for correctness again.\nIn order to prevent an unbounded number of LLM calls, we define a finite\nrepair round \"budget\" for each error category: the amount of times the system\nmay invoke an LLM to repair a particular type of error. In all of our experiments,\nwe use a budget of 3 repair rounds for each error category, leading to a maximum\nof 9 additional LLM calls.\nAlthough this approach does not guarantee an error-free result, in practice we\nhave found it sufficiently capable of achieving our goal: reducing the developer\ntime necessary to utilize Quint models. As shown in our experimental evaluation,\nin most runs we were able to successfully generate correct code for most target\nfunctions and contracts in our benchmark, using fewer than 3 rounds of repair\non average. We outline the pseudocode for this phase (the TESTANDREPAIR\nfunction in Algorithm 1), in Algorithm 2 of Appendix B.1.\nIn the following two sections, we further explain the static and semantic\nrepair rounds. When repairing runtime errors, we use a combination of methods\nand prompts employed for static and semantic errors (details in Appendix B.2)."}, {"title": "5 Evaluation", "content": "In Section 5.1, we evaluate our Quint model generation technique on a bench-\nmark consisting of 5 smart contracts from the CosmWasm Capture the Flag\nrepository and present summary results. In Appendix C, we present more com-\nprehensive results for the entire benchmark, and perform a limited ablation study\ninvestigating the effect of natural language descriptions on performance across\nthe CTF Benchmark.\nIn Section 5.2, we present a case study applying our method to a real-world\naudit scenario. An additional case study, capturing the full model-based testing\nworkflow, can be found in Appendix A."}, {"title": "5.1 Benchmark: CosmWasm Capture the Flag", "content": "The CosmWasm Capture the Flag (CTF) repository provides the source code for\n10 smart contracts used in a capture-the-flag competition. Each smart contract\nin the repository contains a critical bug, and the goal in a CTF competition is to\nfind the bug and craft an exploit around it. For example, in CTF-01, users may\ndeposit funds into a lockup, and withdraw funds from lockups by specifying a\nlockup ID. The bug can be found in the withdraw message handler, which does\nnot check if the same lockup ID appears multiple times in the message, enabling\nusers to withdraw all funds from the same lockup twice, and thus steal funds\nfrom the contract. We focused our benchmark on contracts from the CosmWasm\nCTF repository where the vulnerability is caused by a bug in program logic"}, {"title": "5.2 Case Study: Privileged subDAO Contract", "content": "In this case study, we applied the described approach to a project from one of\nthe audits that Informal performed recently. The scope of the audit was a new\nfunctionality of Neutron's [9] governance contracts. To provide sufficient context,\nlet us describe the basic concepts of on-chain governance.\nMost blockchains, including Neutron, typically do not have a single chain\nowner. Instead, the chain is jointly owned and governed by all the token holders.\nAny user may create a governance proposal, and the token holders vote for or\nagainst it. If there is sufficient support for the proposal, it gets automatically\nexecuted on the chain. We call such an organizational structure a decentralized\nautonomous organization (DAO).\nThe whole governance process, from proposal creation to its execution, takes\na non-negligible amount of time. Furthermore, some proposals may be very tech-\nnically specialized (e.g., regular adjustment of particular chain parameters) and\nthus not sufficiently interesting for members of the DAO. To address these issues,\nthe newly added functionality introduced the concept of a privileged subDAO:\na smaller set of users authorized to vote in and execute specific changes only\n(before execution, the decision can still be overridden by members of the main\nDAO).\nThe functionality is implemented in the neutron-chain-manager contract.\nDifferent subDAOs can either have an AllowAll authorization (typically, for the\nmain or security DAO, which can do anything) or an AllowOnly([Permission])\nauthorization, which specifies a set of permissions that a subDAO can exe-\ncute. The relevant entry point functions of the contract are AddStrategy (which\ngives an authorization to a subDAO), RemoveStrategy (which removes it), and\nExecuteMessages (which executes a message if it has authorization to do so)."}, {"title": "Identifying the bug using model generation", "content": "We took the code at the audit\ncommit and wanted to see if we could find the same bug using model generation.\nWe first applied the mechanical generation step described in Section 4.1, and\nthen the model generation described in Section 4.2. The artifacts resulting from\nthis case study are available on GitHub.7\nMechanical generation reliably produced stubs as expected. There was some\nadditional cleanup work needed: removing references to external libraries or un-\nused types, and removing stubs for contracts that were out of scope. Further-\nmore, some adjustment was needed for handling message passing of the custom\nNeutronMsg type. Overall, the effort to go from the generated version to a work-\ning model stub was neither trivial nor too big.\nOut of 16 functions that there were in the contract, we identified 6 of them\nrelevant for deciding on the correct authorization. For those 6 functions, we ran\nthe model generation process.\nOut of 6 functions, 4 were immediately generated correctly. The remaining\ntwo functions were fixed with syntactic repair. The problem was that at the first\nattempt, there were issues in calling functions within the generated code (once\nthe name was hallucinated, and once the order of parameters did not match the\nsignature).\nOne interesting observation compared to the reference model (a model writ-\nten by the authors manually): at a couple of places we had to make sure that\na property holds for all elements of a list. Thus, we factored that functional-\nity out and wrote the helper function listforall (following the naming of the\nequivalent function on sets, forall). The generated model, since it always gen-\nerates only one function at a time (without the possibility of creating helper\nfunctions), generated expressions that were more complicated compared to the\nreference implementation.\nFinally, the generated Quint model sufficed to uncover the authorization bug\nthat existed in the contract. This presents the main takeaway of the case study:\nthe presented approach could indeed be used in a real-world audit.\nBeside the main takeaway, there are a couple of peculiarities worth comment-\ning on. First, we see that there was no need for semantic repair of generated\nmodel functions. This is because the authorization mostly depends on getting\nthe types right, and the logic is fairly simple.\nSecond, because our model stub ensures full generality handling CosmWasm\nbank transfers, and the analyzed contracts did not include any, the whole model"}, {"title": "6 Conclusion", "content": "In this paper, we reported on our effort to bridge the gap between model-based\ntechniques and software auditing practice. We did so by automatically gener-\nating both Quint models and model-based tests, taking a CosmWasm contract,\nI/O examples, and a high-level description as input. We demonstrated how our\ntechnique is able to generate correct models on a capture-the-flag benchmark,\nand ran a successful case study on a past audit.\nOur experience shows that LLMs can be tamed by using mechanical gener-\nation for setting up a firm frame for statistical code generation. Iterated repair\nattempts (syntactic, semantic, and runtime) were useful for correcting the LLM's\noccasional hallucinations. While the proposed method does not guarantee cor-\nrectness, our evaluation shows that it works well in practice. Furthermore, at\nevery step of the generation process (model stubs, models, test stubs, tests), the\nmethod provides useful artifacts for the users, which is very much appreciated\nby practitioners.\nThere are still aspects of our technique that need to be improved if we want\nto make it fully practical. Currently, the input-output examples we expect from\nusers are encoded as Quint files with Quint syntax. Providing a better user\nexperience there would go a long way to bring our technique to a broader audi-\nence. Additionally, mechanical generation, while fully reliable, is still restricted\nto CW contracts that do not send messages among themselves. This limits the\napplicability of the technique to simpler contracts.\nThe main practical benefit of our approach is easing auditors' way into using\nmodel-based techniques a task with a very steep learning curve prior to having\nour tool available."}, {"title": "A Case Study: Model-based Testing Workflow", "content": "We conducted a case study of the model-based testing workflow-which includes\nboth model generation and adapter generation on two smart contracts, CTF-\n02 and 04. First, we simulated Quint models generated as part of the benchmark\nfrom Section 5.1 to obtain traces. We then generated adapters that can execute\nthese traces, as discussed in Section 4.2. In Appendix D we provide further details\non the adapter generation process in this case study, including examples of state\ncomparison functions synthesized with our LLM-based method.\nListing 1.7 shows the output produced by our synthesized adapter for CTF-\n02 while processing a trace generated by our synthesized model of the contract.\nAs can be seen, the test compiles and successfully starts executing the trace,\nbut fails after two steps, uncovering a bug. The test fails due to a withdraw\nmessage in the trace sent from an unrecognized address. The Quint model is\nable to handle this situation gracefully, while the CosmWasm implementation\npanics. This difference between the model and the implementation points to a\nvulnerability in the code."}, {"title": "A.1 Uncovering the Main Vulnerability in CTF-02", "content": "The exploit in CTF-02 is triggered when an attacker attempts to unstake (re-\nclaim) more funds than they had originally staked. In a testing environment,\nan overflow error will occur in this scenario, reverting the transaction. This may\ngive developers false confidence that the exploit will similarly be prevented when\nexecuting on-chain. However, the core idea of the vulnerability is that overflow\nchecks will be disabled in the release version of the contract, making it easier\nfor developers (who primarily work within the testing environment) to overlook\nthis bug.\nIn order to uncover this vulnerability, we needed to accommodate the gen-\nerated adapter from the previous section to be more resilient against contract\npanics, such as the one from Listing 1.7. Specifically, we modified the adapter\nto unwind particular panics raised by CTF-02 (e.g., if an unrecognized token\ndenomination is received in a deposit message). In that case, we ensured that\ncorresponding transitions in the trace also report an error of the same kind in\nthe model (otherwise, we would not be alerted to a worrying mismatch between\nthe model and the implementation).\nThese modifications allowed the adapter to progress further and process more\nsteps of the trace, up until discovering the main vulnerability. As can be seen in\nListing 1.8, the adapter processed 14 steps of the trace and eventually encoun-\ntered an inconsistent state, thus uncovering the vulnerability."}, {"title": "A.2 Uncovering an Unexpected Vulnerability CTF-04", "content": "Similar to our end-to-end model-based testing case study for CTF-02, when\nexecuting a trace from the Quint model for CTF-04, our generated adapter\npanics and inadvertently uncovers a bug in CTF-04. The output of this test\nrun in shown in Listing 1.9. In this case, the problematic state transition in\nthe trace involves an unrecognized token denomination. The bug is found in the\nmint function of CTF-04's Rust implementation, where the CosmWasm utility\nfunction must_pay is called without proper error handling, resulting in a panic. 8"}, {"title": "B Testing and Repair Phase Details", "content": "In this section, we provide additional details on the testing and repair phase\nof our method that were omitted from the main paper. The pseudocode for the\ntesting and repair phase can be found in the following Section B.1 as Algorithm 2.\nIn Section B.2 we describe runtime repair rounds."}, {"title": "B.1 Full Repair Algorithm Pseudocode", "content": "We assume that GENERATEPROMPT calls in Algorithm 2 have access to similar\ncontextual information (such as the stub name, description, and CosmWasm\nsource code) as in Algorithm 1, but for the sake of brevity we do not explicitly\nstate them as arguments. The full template used by GENERATEPROMPT calls in\nthis case can be found in Appendix E.2."}, {"title": "B.2 Runtime Errors", "content": "In this section we will use the same notation as in Section 4.3, where we explain\nsemantic errors: F denotes the LLM-synthesized function, and (args;, Ri) (1 \u2264\ni \u2264 K) denote the K user-provided I/O specifications (args; and Ri are the\nuser-specified input and output, respectively).\nRuntime errors occur when the synthesized function Fattempts to execute\nillegal Quint code which was not detected by the Quint type checker (such as\nattempting to call a nonexistent method), or when certain runtime invariants\nare broken, for example if attempting to access a list with an invalid index. We\ndetect this by passing all the user-provided arguments args\u2081, one by one, to F, as\nsoon as it passes type checking. If the execution of this function crashes instead\nof returning a value, we trigger a runtime repair round."}, {"title": "E Prompt Templates", "content": "In this section, we present an informative selection of prompt templates that we\nuse to format the system's LLM queries, and to create our few-shot demonstra-\ntions dataset. The full list of templates is available in the project's repository on\nGitHub.9"}, {"title": "E.1 Full model generation user prompt template", "content": "Please complete this stub for my '@@@NAME@@@ Quint function.\n@@@DESCRIPTION@@@\nHere is the contract state type and related types:\n@@@QUINT TYPE DEFINITIONS@@@\nThe following constants are available to the function:\n@@@CONSTANTS@@@\n@@@DEC@@@\nThe following Quint imports are available:\n@@@QUINT IMPORTS@@@\nIf a module is imported as 'import Module.* from \"./lib/module\"', you\nmust use the identifiers from it directly without (so 'name' instead of\n'Module.name').\nThe '@@@NAME@@@' Quint function models a CosmWasm Rust function for\nprocessing smart contract messages.\n@@@MESSAGE HANDLERS@@@\nYour Quint code must be functional.\nAll function arguments are immutable, and you can not use mutable\nvariables.\n@@@IO EXAMPLES@@@"}, {"title": "E.2 Full repair user prompt template", "content": "Listing 1.20 presents the full prompt template we utilize for generating user mes-\nsages during the static error repair phase. The macro @@@ORIGINAL IMPLEMENTATION@@@\nexpands to a stub implementation that is being repaired (i.e., the output of the"}, {"title": "F Quint Model Structure", "content": "In this section, we give a basic overview of how a Quint model looks like. This\nis far from a complete description, but it should give you enough detail to un-\nderstand the decisions made when generating Quint stubs. For a more detailed\ndescription, refer to the Quint documentation [10].\nQuint is a modelling language. It is like a very familiar pseudocode, that\nadditionally enables us to encode state machines (the evolution of the system)\nand nondeterministic choices.\nThe state of the Quint model is defined by a set of variables, for example the\nfollowing:\nvar x: int\nvar y: bool\nThe model's evolution is described by its initial state and possible transitions\nfrom each state. In Quint, there is a special kind of function, called action, that\ncan modify state variables. Thus, we use actions to define both the initial state\nand transitions.\nAs an example, the initial state may be defined as follows:\naction init =\nall {\nx' = 0,\ny' = false\n}\nAnd transitions may be defined like this:\naction step = any {\nnondet increase = oneOf(1.to(10))\nall {\nx < 5,\ny == false,\nx' = x + increase,\ny' = true\n},\nall {\ny'= true,\nx' = 0\n}\n}\nLet us explain the above two actions. The syntax x' (read: x primed) means\n\"the value of x in the next state\". Thus, the action step defines two possible\ntransitions:\neither x < 5 and y == false, in which case increase x in the next state by\na non-deterministically chosen number between 1 and 10, and set y to true,\nor set y to true and x to 0.\nThe either-or choice is encoded by the any keyword. Furthermore, which one of\nthem is chosen is non-deterministic, but the first option can only be selected if\nits conditions are satisfied.\nThe action init is straightforward: it sets the initial values of x and y to 0\nand false, respectively.\nBeside actions, Quint has a concept of pure functions. Those are the functions\nthat may not modify the state, but can be used to calculate values.\nThe last point to mention is the concept of invariants. They are defined on\nstate variables, and Quint's simulator or model checker make sure that they hold\nat all states of the model evolution. As an example of an invariant, consider the\nfollowing simple one:\nval xAlwaysSmall = invariant x < 15"}]}