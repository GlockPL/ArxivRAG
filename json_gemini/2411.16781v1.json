{"title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing", "authors": ["Yiheng Li", "Ruibing Hou", "Hong Chang", "Shiguang Shan", "Xilin Chen"], "abstract": "Human pose plays a crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only a single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, a framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically, we apply a pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within a unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with a mixture of visual encoders, among them a pose-specific visual encoder. Benefiting from a unified learning strategy, UniPose effectively transfers knowledge across different pose-relevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building a general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPose's competitive and even superior performance across various pose-relevant tasks.", "sections": [{"title": "1. Introduction", "content": "Human pose plays a pivotal role in various human-centric applications such as VR and healthcare. Numerous studies focus on single-pose comprehension, i.e., producing posture-relevant description from a 3D body pose [14] or human image [18], as well as pose generation, i.e., creating complex 3D poses from textual descriptions [14, 27, 40] or human images [9, 15, 19, 57, 70]. Recently, a few studies have explored the relationship between pairs of poses [13, 21, 34]. These studies investigate pose-pair comprehension, where textual instruction is produced based on the differences between two 3D poses, and pose editing, where corrected 3D body pose is generated based on an initial pose and modification instruction. However, a key limitation of existing work is that pose comprehension, generation, and editing are predominantly studied in isolation. In reality, human pose cognition and communication inherently involve seamless transitions between multiple pose-relevant modalities, including 3D SMPL poses [45], textual descriptions, and human images. This highlights the need for a unified multimodal framework capable of simultaneously handling pose comprehension, generation, and editing.\nRecent years have witnessed a significant breakthrough in large language models (LLMs) [25, 30, 67] and multimodal LLMs (MLLMs), enabling general-purpose analysis of images [4, 43], videos [38, 73], motions [10, 31, 76], and audios [72, 74]. In the area of human poses, ChatPose [18], a recent innovation, leverages LLMs to generate 3D human poses from images and textual descriptions. Nevertheless, it focuses solely on single-pose generation, lacking the capacity for pose comprehension and editing. Moreover, existing MLLMs still fall short in providing comprehensive analysis of human poses, particularly concerning fine-grained part semantics and complex relationships between pose pairs. Consequently, a unified multimodal LLM that enables finer-grained pose comprehension, generation, and complex pose editing is still in highly demand.\nTwo main challenges need to be solved for building such a unified multimodal LLM framework. The first challenge is creating a unified representation space across 3D poses and texts, enabling the unification of diverse pose-relevant tasks. Existing work [18] processes 3D poses and texts differently, encoding 3D poses as continuous high-level features while tokenizing linguistic texts into discrete token sequences. This non-unified processing incurs an extra burden on LLMs to model interactions between 3D poses and texts, hindering the unifying of pose comprehension, generation and editing. The second challenge lies in achieving fine-grained pose perception within the visual branch of the multimodal framework. Most MLLMs [4, 18, 43, 65] employ CLIP [52] as their visual branch. While CLIP's visual encoder aligns well with the text embedding space through image-text contrastive learning, it struggles to capture detailed pixel-level information, such as keypoints and parsing maps, due to the global supervision provided by image captions. This limitation constrains MLLM's capabilities in fine-grained pose comprehension and generation.\nTo address these challenges, we propose UniPose, a uniform multimodal framework for human pose comprehension, generation and editing, which harnesses the powerful language generation abilities of LLMs to unify various pose-relevant tasks (Tab. 1). UniPose comprises three tires. Firstly, UniPose is equipped with a pose tokenizer for processing 3D poses and texts uniformly. Inspired by the observation that human poses exhibit a semantic coupling similar to language [31, 46, 68], we treat 3D pose as a specific language. Akin to language, the pose tokenizer compresses raw 3D pose into a sequence of discrete semantic tokens. By encoding both 3D pose and language within a shared vocabulary, we build a unified representation space across 3D poses and texts, which enables LLMs to be easily adapted to handle pose comprehension, generation, and editing. Secondly, unlike most MLLMs [4, 18, 43, 65] that solely rely on CLIP's visual encoder [52], we adopt a mixture-of-visual-encoders that combines CLIP's original visual encoder with a pose-specific visual encoder pre-trained on pose estimation task. This dual-encoder setup not only aligns visual representations with text embedding space but also enhances fine-grained pose perception, enabling more effective integration into the multimodal framework for improved pose comprehension and generation. Thirdly, we implement a mixed-attention mechanism within LLMs to handle the distinct internal logical relationships between pose and text tokens. Unlike text tokens, pose tokens encode spatial joint positions without causal dependencies, making unified autoregressive modeling suboptimal. To address this, we apply causal attention to text tokens and bidirectional attention to pose tokens. This mixed-attention strategy preserves LLM's original reasoning capabilities while enhancing contextual pose perception, enabling more effective pose generation and editing.\nTo our knowledge, UniPose is the first approach to integrate seven core tasks of pose comprehension, generation, and editing into a uniform framework. Extensive experiments demonstrate that UniPose achieves competitive performance across multiple pose-relevant tasks. Additionally,"}, {"title": "2. Related Work", "content": "Human Pose Comprehension. Pose comprehension involves generating natural language descriptions of human pose or differences between pose pairs. For single-pose comprehension, traditional methods classify basic human actions from images [77], videos [62, 63, 66], or skeletons data [2, 11, 22, 49]. However, these methods typically lack detailed descriptions of specific body part positioning. To address this gap, [14] introduces PoseScript dataset which pairs human poses with detailed body parts descriptions, and propose a pose-to-text generation model that uses cross-attention to embed pose information within a text transformer for nuanced pose descriptions. For pose-pair comprehension, [13, 21, 34] describe differences between source and target poses based on images, videos, or 3D poses. For example, PoseFix [13] uses an MLP to fuse source and target pose, then uses cross-attention in a text transformer to generate descriptions of pose differences. While these approaches enhance understanding of human poses from multimodal data, they are typically task-specific, with limited control conditions and application scenarios.\nHuman Pose Generation. Pose generation synthesizes human poses conditioned on text or images. Text-conditioned pose generation generally falls into two categories: shape-oriented [24, 56] and pose-oriented [8, 27, 40], which generate 3D poses from descriptions of body attributes (e.g., slim waist) and simple actions (e.g., running), respectively. Image-conditioned pose generation (also referred to pose estimation) includes optimization-based and regression-based approaches. Optimization-based methods [7, 16, 17, 51, 54] iteratively estimate 3D pose parameters, ensuring the projection of predicted 3D joints aligns with 2D keypoints. Regression-based methods [9, 15, 23, 33, 70] use deep neural networks to directly predict 3D pose parameters from input images. Although these methods have achieved promising results in pose generation, they lack the capability of pose comprehension and editing.\nMultimodal Large Language Models. Large Language Models (LLMs) [25, 30, 58, 71] have shown remarkable capabilities in textual comprehension and reasoning. These models have been adapted for multimodal tasks, leading to the development of multimodal large language models. For example, models like mPLUG-Owl3 [73], MiniGPT-4 [79] and LLaVA [37, 43, 44] uses a visual encoder to extract image features and a projection layer to align image embeddings with text embeddings, enhancing general visual perception. Moving towards task-specific applications, LISA [36] and Video-LISA [5] extend MLLMs for segmentation by integrating SAM [35] for generating fine-grained segmentation masks. Additionally, Show-o [69] and Transfusion [78] combine MLLMs with diffusion models [26] to unify image understanding and generation. A recent work, ChatPose [18], applies LLMs to pose-related tasks, aiming to build a versatile pose generator. However, it remains limited in its capacity for pose understanding and editing."}, {"title": "3. Method", "content": "To equip LLM with the capability to comprehend, generate, and edit human poses, we propose a unified framework named UniPose. As illustrated in Fig. 2, UniPose comprises three main components: a pose tokenizer, which quantizes original 3D poses (represented as SMPL [45] pose parameters) into discrete tokens (Sec. 3.1), a visual processor, which extracts fine-grained, pose-relevant features from visual inputs, and a pose-aware LLM, which supports unified modeling across multiple modalities (Sec. 3.2). To address pose-relevant tasks, we employ a four-stage straining scheme encompassing pose tokenizer training, pose-text alignment pre-training, vision projector pre-training, and instruction tuning (Sec. 3.3). During inference, pose tokens are decoded back to their original SMPL format by associated de-tokenizer, enabling various pose-relevant tasks to be executed via instructions (Sec. 3.3)\n3.1. Pose Tokenizer\nTo represent 3D pose in discrete tokens, we build the pose tokenizer based on Vector Quantized Variational Autoencoders (VQ-VAE) [60], as shown in Fig. 2. The pose tokenizer consists of an encoder \\(E\\), a decoder \\(D\\), along with a learnable codebook \\(B_p = {b_m}_{M=1}\\) containing \\(M\\) discrete vectors. Formally, we represent a 3D pose \\(p\\) using SMPL pose parameters, i.e., \\(p = [\\gamma,\\theta]\\) where \\(\\gamma \\in R^6\\) denotes the root orientation and \\(\\theta \\in R^{6K}\\) denotes the rotations with \\(K\\) joints. Then the pose encoder \\(&\\) that consists of several 1-D convolutional layers projects \\(p\\) into a latent embedding \\(z = &(p)\\) with \\(z \\in R^{L_p\\times d_p}\\), where \\(L_p\\) is the number of pose tokens and \\(d_p\\) is the latent dimension. Next, we transform \\(z\\) into a collection of codebook entries through discrete quantization. Specifically, the process of quantization replaces each item of \\(z\\) with its nearest entry in the codebook \\(B_p\\), obtaining the quantized latent vector \\(\\hat{z}\\in R^{L_p\\times d_p}\\) as follows:\n\\[\\hat{z} = \\underset{b_m \\in B_p}{arg\\, min} || z - b_m||_2.\\]\nAfter quantization, the pose decoder \\(D\\), consisting of several 1-D deconvolutional layers, projects \\(\\hat{z}\\) back to raw pose space as \\(\\hat{p} = D(\\hat{z})\\). Following [60], we train the pose tokenizer using the loss function \\(L_{vq} = L_r + L_e + L_c\\) where \\(L_r\\), \\(L_e\\), and \\(L_c\\) denote reconstruction loss, embedding loss and commitment loss respectively. Further training and objective details are provided in the Appendix."}, {"title": "3.2. Pose-aware Vision-Language Model", "content": "Visual Processor. Previous works [4, 43] commonly use CLIP visual encoder [52] as the visual branch. However, since CLIP is optimized by global and coarse-grained supervision signals from image captions, it struggles to capture pose-relevant details. Differently, the pose estimation task demands precise localization of human keypoints, which encourages the visual encoder to capture fine-grained pose features. Therefore, we integrate a pose-specific Vision Transformer [23], pretrained on the pose estimation task, into the visual branch, as shown in Fig. 2. Specifically, denote the CLIP visual encoder and pose-specific vision transformer as \\(f_a\\) and \\(f_b\\), respectively. Given an input image \\(x\\), we extract visual embeddings by CLIP as \\(v_a = f_a(x)\\) where \\(v_a \\in R^{L_v\\times d_a}\\), \\(L_v\\) is the number of visual patch tokens and \\(d_a\\) is its visual embedding dimension. The embedding output by pose-specific vision transformer is \\(v_b = f_b(x)\\) where \\(v_b \\in R^{L_v\\times d_b}\\). Then we concatenate the embedding output by these two encoders along the channel dimension, and apply a trainable projector layer (with projection matrix \\(W \\in R^{(d_a+d_b)\\times d}\\)) to align the dimension of the concatenated visual features to that of text features as \\(v = [v_a; v_b]W\\). Here \\(v \\in R^{L_v\\times d}\\) with \\(d\\) as text embedding dimensions of LLM. The fused visual features \\(v\\) can be concatenated with pose or text tokens as input to LLM.\nMixed Attention Mechanism. Existing LLMs [30, 53, 58] typically employ autoregressive modeling with causal attention, excelling at generating sequential data such as text and audio [72, 74]. However, pose tokens, which encode spatial positions of human joints, are inherently non-sequential, making traditional autoregressive generation suboptimal. To address this issue, we propose modeling pose tokens as a whole. Inspired by [69, 78], we modify the standard causal attention in LLM, integrating bidirectional attention for pose tokens as depicted in Fig. 2. Specifically, we apply casual attention to text sequence, but apply bidirectional attention within the pose toke sequence. To avoid information leakage, we initialize \\(L_p\\) learnable pose queries \\(Q = {q_1, ..., q_{L_p}}\\) during the generation and editing of 3D poses, as shown in Fig. 2. These queries are used to predict corresponding pose tokens in a single forward step. This design enables each pose token to attend to others within the same pose token sequence, while restricting access to only previously encountered text tokens.\nUnified Multimodal Language Model. As shown in Fig. 2, equipped with a visual processor and pose tokenizer, we can compress original visual data \\(x\\) and pose data \\(p\\) into visual feature sequence \\(v \\in R^{L_v\\times d}\\) and pose token sequence \\(u \\in R^{L_p}\\), respectively. To incorporate pose discrete tokens into LLMs, we expand the original text vocabulary \\(V_t\\) of LLM with pose vocabulary \\(V_p\\), forming a new unified text-pose vocabulary \\(V = {V_t, V_p}\\). Equipped with the unified vocabulary \\(V\\), various pose-related tasks can be formulated in a general format, where both input and output tokens are drawn from the same vocabulary, with the input optionally combined with the visual feature \\(v\\). These discrete tokens can represent natural language, 3D pose, or combination, depending on the specific task to be solved."}, {"title": "3.3. Training and Inference Paradigm", "content": "The training procedure comprises four stages, and the training paradigm of the last three stages is shown in Fig. 3. Pose Tokenizer Training. We first train a pose tokenizer using the objective \\(L_{vq}\\). The pose tokenizer encodes 3D pose as a sequence of discrete tokens, enabling seamless integration with texts within LLM. To maintain stability during LLM training, the pose tokenizer is kept frozen during the subsequent stages of training.\nPose-Text Alignment Pretraining. To enable LLM to handle discrete pose tokens, we train LLM on pose-text corpus. This process aims to align the pose and text modalities for unified reasoning within the LLM. In this stage, we consider four pose-text relevant tasks in Tab. 1, i.e., 2 pose comprehension tasks (Pose-to-Text and Pose-Diff), 1 pose generation task (Text-to-Pose) and the Pose Editing task. Based on these tasks, we train LLM using LORA [28] with the objective \\(L\\), as shown in Fig. 3a."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. For pose tokenizer training, we use the standard training split of AMASS [47] and MOYO [59], following TokenHMR [15]. For UniPose training, we integrate three types of data: (1) Text-Pose Data. We use PoseScript [14] and PoseFix [13] datasets to link language and pose modality. PoseScript [14] provides natural language descriptions paired with 3D human poses, allowing the model to understand fine-grained pose semantics. PoseFix [13] includes pairs of 3D poses and textual descriptions that specify how to modify the source pose to achieve the target pose. (2) Image-Pose Data. Following [15, 23], we use standard human pose estimation training datasets, including Human3.6M [29], MPI-INF-3DHP [48], COCO [42], and the MPII [3] dataset, and evaluate on 3DPW [61] and Human3.6M [29] test sets. (3) Image-Text Data. Since no existing dataset combines human images with pose descriptions, we create the ImageScript and ImageDiff datasets to bridge this gap in visual-textual pose comprehension. Further dataset details are provided in the Appendix.\nMetrics. We adopt the evaluation metrics from PoseScript [14] and PoseFix [13]. (1) Pose comprehension tasks. We use two types of metrics. Pose-text retrieval metric: R-Precision, which evaluates the accuracy of matching poses with corresponding descriptions. We rank the Euclidean distances between the query pose and 32 text descriptions (1 ground truth and 31 randomly selected mismatched descriptions), and report Top 1/2/3 R-Precision; Linguistic metrics: BLEU-4 [50], Rouge-L [39] and METEOR [6], which assess the quality of generated pose descriptions. (2) Pose generation tasks. We use two types of metrics. Re-"}, {"title": "4.2. Comparisons on Pose-relevant Tasks", "content": "Comparisons on Pose comprehension. We evaluate UniPose on 4 pose comprehension tasks, i.e., Pose-to-Text, Pose-Diff, Image-to-Text and Image-Diff. The comparison results are shown in Tab. 2. As seen in the table, UniPose achieves competitive performance across all evaluated tasks, highlighting its capability to address diverse pose comprehension tasks within a single model. (1) For Pose-to-Text task, we compare UniPose with PoseScript [14] on PoseScript dataset. As shown in Tab. 2, UniPose achieves slightly lower performance than PoseScript. However, PoseScript is tailored for single-pose description generation and lacks the capacity to model relationships between different poses. (2) For Pose-Diff task, we compare UniPose with PoseFix [13] on PoseFix dataset. As shown in Tab. 2, UniPose outperforms PoseFix on most metrics, demonstrating its superiority in capturing relationships between pairs of poses. (3) For Image-to-Text task, we compare UniPose with existing visual-language MLLMs, including LLaVA [43], Qwen-VL [4] and GPT4V [1], on ImageScript dataset. As shown in Tab. 2, UniPose significantly outperforms these MLLMS. The substantial gains can be attributed to the use of pose-specific visual encoder, which enables UniPose to extract fine-grained pose information from visual inputs. (4) For Image-Diff task, we compare UniPose with GPT4V on ImageDiff dataset. UniPose still outperforms GPT4V, demonstrating that UniPose not only captures fine-grained pose features from a single image but also learns the relationships between human poses across multiple images."}, {"title": "4.3. Ablation Studies & Analysis", "content": "Single-task training v.s. Multi-task training. Tab. 2, 3, 4, 5 also report the performance of UniPose training on single task (denoted as UniPose \u2020). As shown, multi-task training consistently outperforms single-task training, underscoring the importance of unifying pose comprehending, generation and editing within a single model.\nVisual Processor. We compare the impact of different vision encoders used in the Visual Processor of UniPose. In this part, the models are trained solely on Pose Estimation and Image-to-Text tasks for 2 epochs. As shown in Tab. 6, with only the CLIP-ViT encoder, the model performs poorly on pose estimation task. We argue that CLIP-ViT primarily focuses on aligning global semantic information between images and text, struggling to capture detailed human pose information. By incorporating an additional ViT model trained specifically for pose estimation, UniPose gains the ability to capture fine-grained pose details, significantly improving its performance on pose estimation task. Moreover, the pose information extracted from images enhances the performance on Image-to-Text task, enabling UniPose to generate more precise descriptions of human poses.\nAttention mechanism. We evaluate the performance of UniPose using causal attention and mixed attention. In this part, the models are trained solely on Text-to-Pose and Pose-to-Text tasks for 6 epochs. More training details are provided in the Appendix. As shown in Tab. 7, on Text-to-Pose task, the model with mixed attention achieves higher retrieval accuracy compared to casual attention. The results indicate that capturing bidirectional dependencies among pose tokens enhances pose generation. Additionally, the bidirectional attention mechanism enables single-step generation of all pose tokens, significantly accelerating inference. However, mixed attention performs worse than causal attention on Pose-to-Text task. This may be due to the in-"}, {"title": "5. Conclusion", "content": "In this work, we present UniPose, the first attempt to integrate human pose comprehension, generation, and editing within a unified framework. By employing a pose tokenizer, we build a unified representation space that bridges 3D poses and texts, enabling seamless interactions across modalities. Additionally, the mixture-of-visual encoder captures intricate pose details, thereby enhancing fine-grained pose perceptions. The mixed-attention mechanism further enhances pose generation quality while significantly accelerating inference speed. Extensive evaluations across various pose-relevant tasks demonstrate the effectiveness of UniPose in pose comprehension, generation, and editing."}, {"title": "E. Limitation", "content": "In pose estimation task, the performance of MLLMs-based models still lags behind specialized methods. We argue that these limitations may stem from the constraints imposed by the frozen visual encoder. Future research will focus on developing techniques that enable large language models to more effectively integrate pose-relevant visual features from diverse visual encoders, thereby enhancing their ability to handle complex pose estimation tasks."}, {"title": "A. Data Collection", "content": "To address the lack of datasets combining human images with pose descriptions, we present the ImageScript and ImageDiff datasets, specifically designed to bridge this gap in visual-textual pose comprehension.\nA.1. ImageScript\nImageScript dataset aims to provide accurate and detailed textual descriptions of human poses depicted in images. Existing pose estimation datasets, collectively re-"}, {"title": "A.2. ImageDiff", "content": "ImageDiff dataset is designed to provide textual descriptions of human pose differences between image pairs, enabling the model to effectively perceive and interpret pose variations across different visual inputs. Building on PoseFix [13], which introduced a pipeline for automatically generating comparative descriptions for 3D SMPL pose pairs, we propose ImageDiff, a dataset comprising image pairs, corresponding 3D pose pairs, and textual descriptions of pose differences.\nThe ImageDiff dataset consists of 52k triplets in the form of {image A, image B, text}, where the text describes how"}, {"title": "A.3. Training Data Details", "content": "We employ specific tasks and datasets for each training stage of UniPose, as summarized in Tab. 1. In details:\n\u2022 Pose-Text Alignment Pretraining Stage. We incorporate four pose-text-related tasks: two pose comprehension tasks (Pose-to-Text and Pose-Diff), one pose generation task (Text-to-Pose), and the Pose-Edit task. Drawing in-"}, {"title": "B. Implementation details", "content": "B.1. Pose Tokenizer\nWe provide a detailed explanation of the training objectives for the pose tokenizer. The pose tokenizer is trained using reconstruction loss Lr, embedding loss Le, and commitment loss Lc. To further improve the generated pose quality, we utilize vertices and position regularization in the reconstruction loss, as follows:\n\\[L_{vq} = L_r + L_e + L_c, where,\\]\n\\[L_r = \\lambda_1 || \\hat{p} - p ||_2 + \\lambda_2 || \\hat{v} - v ||_2 + \\lambda_3 || \\hat{j} - j ||_2, \\]\n\\[L_e = ||sg [z] - \\hat{z}||_2, \\, L_c = ||z - sg [\\hat{z}]||_2,\\]\nwhere \\(v\\) and \\(j\\) denotes the ground truth SMPL mesh vertices and joints positions derived from \\(p\\), \\(\\hat{v}\\) and \\(\\hat{j}\\) denotes the predicted vertices and positions derived from \\(\\hat{p}\\), \\(sg[]\\) is the stop gradient operator, and \\(\\lambda_1\\), \\(\\lambda_2\\) and \\(\\lambda_3\\) are the weighting factors.\nTraining Configurations. For the training of Pose Tokenizer, we use AdamW as the optimizer with a batch size of 256 and an initial learning rate of 2e-4. The model is trained for 240 epochs and the weighting factors \\(\\lambda_1\\), \\(\\lambda_2\\) and \\(\\lambda_3\\) are set to 20, 100, 100 respectively. We set the codebook size to 2048, representing each 3D pose with 80 discrete tokens. Following TokenHMR [15], we augment random joints with noise starting at 0.01, progressively increasing after every 5K iterations. To further enhance robustness to global orientation variations, we introduce random perturbations of -45 to 45 degrees in the z-direction and -20 to 20 degrees in the x and y directions. The effect of global orientation noise is analyzed in Sec. C."}, {"title": "B.2. Retrieval Model", "content": "To compute the Pose-Text retrieval metric, a retrieval model is required to rank a large collection of poses based on their relevance to a given textual query, and vice versa.\nPose-Text Retrieval Model consists of a pose encoder and a text encoder. For pose feature extraction, we directly employ the pose encoder from the pose tokenizer and add 1D Conv for dimensionality reduction. For the text encoder, we use a bidirectional GRU [12] with one layer for text feature extraction, with word embeddings and the text tokenizer derived from a pretrained DistilBERT [55] model. Both pose and text are encoded into 512-dimensional feature vectors. Following PoseScript [14], we adopt the Batch-Based Classification (BBC) loss as the training objective:\n\\[L_{BBC} = - \\frac{1}{B} \\sum_{i=1}^B log \\frac{exp( \\gamma \\delta(x_i, y_i))}{ \\sum_{j} exp( \\gamma \\delta(x_i, y_j))}, \\]\nwhere \\(\\gamma\\) is a learnable temperature parameter, \\(\\delta\\) is the cosine similarity function, and \\((x_i, y_i)\\) denotes pose-text pairs.\nPose Pair-Text Retrieval Model is designed for retrieving pose pairs and text in the Pose/Image-Diff task. Its architecture is similar to the pose-text retrieval model, with the key difference being that the pose encoder processes each pose in the pair separately. The extracted features are concatenated along the channel dimension and passed through multiple 1D Conv layers for dimensionality reduction. Both the pose encoder and text encoder generate 512-dimensional feature vectors, utilizing the same training objective as the Pose-Text retrieval model.\nTraining Configurations. Following PoseScript and Pose-Fix, the retrieval models are first pretrained on automatically generated captions (PoseScript-A and PoseFix-A) and then fine-tuned on human-written captions (PoseScript-H and PoseFix-H). The retrieval models are trained for 120 epochs across the pretraining and fine-tuning stages. We use the Adam optimizer, with a batch size of 512 for pretraining and 32 for fine-tuning. The learning rate is set to 2e-4,"}]}