{"title": "Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework", "authors": ["Yulong Hu", "Tingting Dong", "Sen Li"], "abstract": "This paper introduces a novel reinforcement learning (RL) framework, termed Reward-Guided Conservative Q-learning (RG-CQL), to enhance coordination between ride-pooling and public transit within a multimodal transportation network. We model each ride-pooling vehicle as an agent governed by a Markov Decision Process (MDP), which includes a state for each agent encompassing the vehicle's location, the number of vacant seats, and all pertinent information regarding the passengers on board. We propose an offline training and online fine-tuning RL framework to learn the optimal operational decisions of the multimodal transportation systems, including rider-vehicle matching, selection of drop-off locations for passengers, and vehicle routing decisions, with improved data efficiency. During the offline training phase, we develop a Conservative Double Deep Q Network (CDDQN) as the action executor and a supervised learning-based reward estimator, termed the Guider Network, to extract valuable insights into action-reward relationships from data batches. In the online fine-tuning phase, the Guider Network serves as an exploration guide, aiding CDDQN in effectively and conservatively exploring unknown state-action pairs to bridge the gap between the conservative offline training and optimistic online fine-tuning. The efficacy of our algorithm is demonstrated through a realistic case study using real-world data from Manhattan. We show that integrating ride-pooling with public transit outperforms two benchmark cases\u2014solo rides coordinated with transit and ride-pooling without transit coordination-by 17% and 22% in the achieved system rewards, respe\u0441tively. Furthermore, our innovative offline training and online fine-tuning framework offers a remarkable 81.3% improvement in data efficiency compared to traditional online RL methods with adequate exploration budgets, with a 4.3% increase in total rewards and a 5.6% reduction in overestimation errors. Experimental results further demonstrate that RG-CQL effectively addresses the challenges of transitioning from offline to online RL in large-scale ride-pooling systems integrated with transit.", "sections": [{"title": "1. Introduction", "content": "In recent years, the widespread use of transportation network companies, such as Uber, Lyft, and Didi, has raised concerns about their negative impacts on traffic and the environment. For these ride-hailing platforms, the ability to immediately respond to passenger requests is a crucial element of their success. However, this business model heavily relies on a large number of drivers who remain idle and cruise around waiting for passengers [1]. This not only leads to low vehicle utilization rates and high fuel costs but also exacerbates traffic congestion and carbon emission in urban areas. Furthermore, a more pressing concern is that ride-hailing services may compete with public transit systems. The convenience offered by ride-hailing services can divert a substantial number of riders away from public transportation [2-5], leading to what could become a vicious cycle that undermines public transit development. This may contribute to a rise in vehicle miles traveled, increased traffic congestion, reduced transit ridership, and higher emissions, which run counter to the philosophy of developing transit-oriented, multimodal transportation systems that are operationally efficient, financially sustainable, and environmentally friendly.\nTo address the aforementioned concerns, substantial research has been conducted along two major directions, including: (a) how to improve the operational efficiency of ride-hailing services; and (b) how to promote collaborative relationships between ride-hailing and public transportation systems. To achieve the first goal, a significant body of work has focused on ride-pooling services, which allow multiple passengers to share a ride in a single vehicle through a digital platform, traveling to similar or conveniently aligned destinations. By consolidating multiple orders within the same vehicle, ride-pooling can effectively enhance vehicle utilization rates, while also reducing per-person travel miles and carbon emissions. In this vein, various forms of ride-pooling services have been explored, including multi-hop ride-pooling [6], diurnal-adaptive ride-pooling management [7], and ride-pooling with passenger transfers [8]. To achieve the second objective, another stream of research has investigated the coordination between ride-hailing services and public transit systems [9-12]. The underlying premise is that ride-hailing services can either complement or compete with public transit, depending on their operational approach. To maximize the complementary effects between these two, strategies have been proposed to utilize ride-hailing services for the first and/or last leg of a journey, while leveraging public transit for the long-distance middle leg between transport hubs. This approach could potentially increase transit ridership by extending public transit access to those who are unable to reach transit facilities without the aid of ride-hailing services.\nThere exists significant synergy by combining the aforementioned two research directions, where ride-pooling services operate in conjunction with transit networks, enabling passengers to share a ride on the ride-hailing platform to cover the first and last miles of their trip, while public transportation handles the long-distance segments between transportation hubs. This arrangement is mutually beneficial for both ride-pooling and public transportation networks. In particular, a crucial determinant of the performance for ride-pooling services is the shareability of rides, which depends on whether the origins and destinations of passengers are compatible and can thus be pooled together. By integrating with transit services, we can allow significant flexibility for the ride-pooling platform to choose the drop-off point, which can significantly enhance the chances of passengers being pooled. Meanwhile, pooled rides can reduce costs for passengers and enable more people, especially lower-income passengers who are more transit-dependent, to access transit networks. This can initiate a virtuous circle for transit development, where increased ridership improves the financial condition of transit operators, who can then further enhance service quality and in return attract more passengers.\nHowever, operating ride-pooling services within multimodal transportation networks that consider inter-modal transfers poses significant research challenges. Firstly, the decision-making involved in managing large-scale multimodal networks under uncertainties is complex for ride-hailing platforms. Specifically, the platform must determine how to pool passengers, match them to vehicles, route these vehicles, and select appropriate pick-up and drop-off points for inter-modal transfers. These tasks are intricately intertwined and must be addressed in real-time amid significant uncertainties, rendering traditional model-based algorithms [13-15] inadequate. This has led to the exploration of Reinforcement Learning (RL) algorithms. However, in the realm of RL, especially when applied to the coordinated management of ride-pooling and transit networks, training models from scratch is not only time-consuming but also prone to yielding low-"}, {"title": "2. Literature Review", "content": "quality local solutions due to the enormous decision space and the trial-and-error, data-intensive nature of RL algorithms [16, 17] within the multimodal transportation system. In this context, agents must continuously interact with the environment or simulator throughout the training process, which can be exceedingly time-consuming, and this challenge is further magnified in scenarios involving thousands of agents with sophisticated operational strategies. To address these issues, some transportation researchers have proposed adopting off-policy RL techniques [18-21], such as TD learning, to learn value functions from batches of past vehicle transitions. However, these methods often suffer from significant overestimation issues due to out-of-distribution (OOD) data transitions during the offline training phase [22]. Moreover, there is a persistent gap between offline and online learning, and current offline RL methods also experience slow learning rates during subsequent online training [23], further limiting the practical application of these approaches.\nAcknowledging the challenges mentioned above and inspired by the recent progress of reward models in enabling Large Language Models (LLMs) to learn from human feedback through RL [24-26] and rewarding mechanism in ride-sharing to promote acceptable rides [27-30], we hereby propose Reward Guided Conservative Q-learning (RG-CQL). This approach leverages an offline training and online fine-tuning RL framework to effectively coordinate ride-pooling with transit, complemented with a Guider to improve the efficiency of exploration. In particular, we first formulate the ride-pooling problem in multimodal transportation networks as a Markov Decision Process (MDP), and then proceed to train an offline learning Conservative Double Deep Q Network (CDDQN) [17, 31] policy, which serves as the potential action executor, alongside a supervised learning reward estimator during the offline training phase to maximize insights from batches of past environmental transitions. Subsequently, during the online fine-tuning phase, we employ the reward estimator as the exploration guide, aiding the CDDQN in effectively and conservatively exploring unknown state-action pairs. Our extensive numerical experiments validate that the proposed RG-CQL framework markedly enhances the operational performance of multimodal transportation systems. The major contributions of our paper can be summarized as follows:\n\u2022 We addressed the coordinated management of ride-pooling with public transit under a RL framework, which is absent in the existing literature. Specifically, we modeled each ride-pooling vehicle as an agent operating under an MDP. The state of each agent includes the vehicle's location, the number of vacant seats, and all pertinent information regarding the passengers currently on board. Actions within this framework determine the drop-off locations either transit stations or final destinations for each onboard passenger. In this model, agents independently decide the drop-off locations for each matched passenger, while the rider-bundling and rider-vehicle matching is addressed centrally through bipartite matching, with edges weighted according to value functions derived from the MDP. With decisions about matching and drop-offs resolved, a separate vehicle routing problem can be independently solved and dynamically adjusted in real-time for each ride-pooling vehicle. Overall, our MDP formulation allows us to jointly determine the optimal matches between riders and vehicles, the optimal drop-off locations for each passenger, and the optimal routes for each vehicle, all under uncertainties within the multimodal transportation system.\n\u2022 We propose a pioneering Reward Guided Conservative Q-learning (RG-CQL) framework that integrates offline training with online fine-tuning to jointly optimize the operation of ride-pooling and public transit services. In the offline training, we employ CDDQN trained via offline RL phase based on historical environmental transitions, complemented by training a Guider that utilizes reward regression. Subsequently, in the online fine-tuning phase, the CDDQN acts as an action executor, while the Guider serves as an exploration guide, helping to filter out irrelevant actions and thereby enhancing the efficiency of the exploration process. This computational framework significantly enhances the system's ability to explore unfamiliar state-action pairs both effectively and cautiously. It effectively bridges the gap between conservative offline training and optimistic online fine-tuning, mitigating"}, {"title": "2.1. Order Matching for Ride-Pooling Platforms", "content": "Ride-pooling services enable individual passengers to share a ride in a single vehicle, coordinated through a digital platform, to travel to similar or conveniently aligned destinations. The operational dynamics of ride-pooling have garnered considerable attention due to their promising yet unpredictable real-time demand, as evidenced by various studies [32\u201335]. The nature of this uncertainty, coupled with the full potential of ride-pooling systems, introduces complexity into the process of coordinating vehicles with multiple passengers. Effective coordination requires not only addressing the needs of current passengers but also anticipating the needs of future riders, which includes managing new ride requests and those already being served. Initial investigations in this field have considered short-sighted, or myopic, policies that make vehicular assignments based on presently available information [32, 33]. To exemplify, [33] notably advances this by introducing the shareability graph, which identifies possible sharing opportunities between new requests and vehicles on standby. They put forward a batch-matching strategy and crafted a sequential method that divides the decision-making process into vehicle routing and passenger assignment tasks. For more efficient real-time operations, [34] reduces the complexity of the matching problem by limiting the process to pairing a single passenger with a vehicle at each time step. Recent advancements in decision-making processes have increasingly focused on integrating uncertainties related to future demand more effectively through methods like Model Predictive Control (MPC) [36\u201338], Approximate Dynamic Programming (ADP) [39\u201341], and Stochastic Programming [42]. Specifically, MPC optimizes control inputs by looking ahead and forecasting future system behavior over a set time horizon; however, its effectiveness heavily relies on the accuracy of model predictions and necessitates substantial computational resources for real-time opera-tions. ADP tackles the challenge by employing complex representation algorithms to approximate value functions, making its performance heavily dependent on the quality of the approximation model used."}, {"title": "2.2. Coordinating Ride-hailing Services with Public Transit", "content": "Another concern accompanying the development of ride-hailing studies is its negative externalities on urban transportation systems, where the most significant issue is the competitive interaction between ride-pooling and public transit. Research has shown that ride-sourcing platforms can replace transit rides, leading to increased vehicle miles traveled, exacerbated traffic congestion, reduced transit ridership, and heightened emissions [2-5]. To harmonize the ride-hailing services with public transit, several researchers attempt to coordinately dispatch ride-sourcing with public transit [9-12]. Specifically, Feng et al. [9] focused on the first-mile problem with ride-sourcing services and designed an Integer Linear Programming (ILP) Framework to coordinate ride-sourcing with metro systems. To take future uncertainty into consideration within ILP, they aim to estimate long-term expected rewards through TD learning with Online RL. [10] developed a game-theoretical framework to compute the Nash Equilibrium within the intimate interactions between the ride-hailing platform, the transit agency, and multi-class passengers with distinct income levels. Based on the proposed model and solution, they investigated two regulatory policies that can improve transport equity. Xu et al. [11] explored the strategic placement of transportation hubs in urban areas, proposing that Transportation Network Companies (TNCs) allocate Autonomous Vehicles to these hubs. Their study models five distinct trip types: Intra-Hub, Inter-Hub, Hub to Outer, Outer to Hub, and Outer to Outer. It also incorporates modelling factors such as passenger choice and waiting times. To solve the proposed model, they employed a Multi-Objective Adaptive Particle Swarm Optimization (MOAPSO) approach to search and find the optimal solutions. [12] introduced subsidies and additional bus services for passengers in high-demand areas, encouraging them to initially use other forms of public transportation to reach less congested areas before completing their journey via ride-sourcing. To determine the optimal dispatching and subsidy schemes, they developed a bi-level mixed integer programming model grounded in network flow theory and designed a corresponding iterative algorithm to effectively solve the model. Although the works above well study the problem of coordinated dispatch of ride-sourcing and public transit, their models"}, {"title": "2.3. Offline RL and its Deployment in Ride-Sourcing Dispatch", "content": "To mitigate the sample complexity inherent in the trial-and-error and data-intensive nature of online RL, offline RL, or batch RL, aims to train agents directly from pre-collected datasets instead of active interac-tions with the environment. This approach seeks to align RL with the data and time efficiency characteristic of supervised learning. Observing the promising developments in offline RL, several researchers in ride-sourcing have proposed adopting vanilla offline RL methods or off-policy RL techniques such as Q-learning and SARSA [16] to learn value functions from batches of past vehicle transitions. Specifically, [20] employs a batch training algorithm through TD learning with deep value networks to learn the spatiotemporal state-value function from historical data, optimizing repositioning actions in ride-hailing services. [18] introduces"}, {"title": "2.4. Recent Advances and Fine-tuning Dilemma in Offline RL", "content": "Recent progress in offline RL domain is mainly based on conservatism(or pessimism) [22]. One direction is to enforce the learned policy to stay close to the behavior policy e.g. by applying Lagrangian through advantage weight regression [50], or by adopting TD learning with behavior cloning and constraint [51, 52]. To exemplify, [51] employs a generative model conditioned on the state to produce actions that have been previously observed in the dataset, avoiding actions outside this distribution. This model is integrated with a Q-network that selects the highest valued action among those generated, ensuring that the actions are both plausible according to the data and valuable in terms of expected reward. [50] proposes an off-policy reinforcement learning algorithm that relies on simple supervised regression steps to optimize policy and value functions using previously collected data from a replay buffer. However this direction requires user to have pre-knowledge or estimation of sample policy for aggregated data, which might not be feasible in many real-world scenarios. Another direction is to implement critic regularization to the value function update like Conservative Q-learning [17] and Implicit Q-learning [53] to directly address extrapolation error. Specifically, [17] intentionally lower-bounds the expected value of a policy through a regularization term added to the Q-function update, which penalizes overestimation of action values under the policy being evaluated. [53] avoids directly querying the value of unseen actions by utilizing state value functions treated as random variables dependent on actions. The approach alternates between fitting an upper expectile of this random variable and updating a Q-function through SARSA-style temporal difference backup. Building on this trend, researchers have further showcased the capabilities of offline RL to learn from human demonstrations in scenarios, such as robotics [54] and multi-task learning in Atari games [55]. This progress has been achieved by incorporating advanced neural network architectures, specifically the ResNet [56] and Transformer [57] into these systems respectively.\nAlthough these exciting progress turns out to become effective for addressing extrapolation error, researcher find that there always exists a gap between offline learning and online learning. This gap results in current offline RL methods suffering from slow learning or initial unlearning during online further training [23]. To bridge this gap, several branch of efforts have been made in the offline RL community. The first approach, as proposed by [58], involves loading pre-collected data trajectories as an initial experience batch for off-policy RL methods and then conducting training as usual. While this method is simple and efficient, its performance is still upper-bounded by state-of-the-art pure offline RL pipelines. Another line of approach, discussed by [59, 60], involves training multiple value functions pessimistically during the offline training process. Afterwards during online fine-tuning, agent then performs actions based on the expected estimation of the ensemble value function. Though the ensemble value function manages to further reduce OOD overestimation, the method could become memory inefficient due to the requirements for training over tens of neural networks simultaneously. The final approach aims to lower-bound the offline training"}, {"title": "3. Problem Description", "content": "We consider using ride-pooling services to address the first-mile problem of transit services. Riders requesting trip services are assumed to be willing to use ride-pooling services on their first leg of trips and transit services on the second leg of their trips. As shown in Fig. 1, a ride-pooling vehicle could pick up a rider from her origin and deliver the rider directly to her destination or to a transit station so that the rider can take transit vehicles. Riders who continue their trip using transit would get off at the stations closest to their destinations that are accessible via walking. The transit network comprises a collection of transit stations I, along with transit routes linking these stations. Transit vehicles are assumed to follow fixed routes and schedules that are not influenced by ride-pooling services. The schedules of transit vehicles are given by a timetable that specifies a transit vehicle's arrival and departure times at each transit station $i \\in \\mathbb{Z}$. Riders dropped at transit stations are expected to follow the shortest transit route to minimize travel time to their destinations, potentially transferring between transit lines. Given the transit network and schedules, a centralized platform dispatches ride-pooling vehicles to riders and decides whether to drop off riders at their destinations or transit stations. For differentiation, we refer to a match as a \"pooling-only\" match if a rider is directly delivered to her destination and as a \"pooling-transit\" match if a rider is dropped off at a transit station and uses both ride-pooling and transit services to complete her trip. During a rider's journey on a ride-pooling vehicle, she might share a ride with other riders, depending on the route of the ride-pooling vehicle and the itinerary of other riders.\nWe consider the operation of the above coordinated ride-pooling and transit services over a finite planning horizon $T = \\{0, \\Delta t, 2\\Delta t,\\ldots,T\\}$ where $\\Delta t$ denotes time-step for decision making. The ride-pooling requests are revealed over time and arrive following a stochastic process. The platform is supposed to use a batch-matching strategy that matches a set of ride-pooling vehicles $N$ with riders $M$ that accumulated over a fixed time interval $\\Delta t$ (such as 1 minute). Each rider $m \\in M$ is characterized by parameters $(o_m, d_m, e_m, l_m)$ which specifies rider $m$'s origin location $o_m$, destination location $d_m$, service request time $e_m$, and the expected arrival time $l_m$. The trip delay of a rider is calculated as the difference between a rider's actual arrival time at her destination and her expected arrival time, including the delay incurred by ride-pooling services and by transit services (if a match is a \"pooling-transit\" match). At time $t$, each ride-pooling vehicle $n \\in N$ is characterized by its current location $I_{n,t}$, remaining capacity $v_{n,t}$ to accommodate additional riders, riders that are being delivered $p_{n,t}$, and a scheduled route for picking up and dropping off those assigned riders in $p_{n,t}$. At each decision time, the platform anticipates future potential trip demand and maximizes its expected profits by deciding (i) the matching between ride-pooling vehicles $n \\in N$ and riders $m \\in M$;"}, {"title": "4. Model Formulation", "content": "(ii) the drop-off locations (transit stations $i \\in I$ for a \"pooling-transit\" match or true destination for a \"pooling-only\" match) for each assigned rider $m \\in M$; (iii) the route and schedules that a ride-pooling vehicle follows to serve assigned riders. The matching decisions under the coordinated ride-pooling and transit services involve ride-pooling vehicles, riders, and transit stations. These decisions are expected to satisfy a set of service requirements, ensuring that riders can arrive at their destinations as soon as possible. Throughout the analysis, we assume that a ride-pooling vehicle is assigned to serve at most one rider at each decision time. This assumption does not preclude pooling possibilities among riders as we consider a dynamic system and riders assigned at present could share a ride with those assigned before. The overall goal is to efficiently link individual riders to public transit stations via ride-pooling services, thus facilitating riders' subsequent travel to final destinations while proactively managing future ride requests."}, {"title": "4.1. MDP Model", "content": "This subsection introduces the MDP model for coordinated ride-pooling and transit services with each ride-pooling vehicle being treated as an agent."}, {"title": "4.1.1. Model Elements", "content": "To streamline the model formulation, we partition the study area into a set of disjoint zones denoted by $\\mathbb{Z} = \\{1,...,z,...\\}$. Within each zone $z \\in \\mathbb{Z}$, the transit stations are represented as $I_z$. For any rider dropped off in zone $z$, her specific drop-off transit station, which optimizes travel time to her final destination $d_m$, can be uniquely determined given the set $I_z$. Consequently, the drop-off location of a rider $m$ is represented either by 0 (indicating direct drop-off at her destination) or by the zone number $z$. Building on this framework, the subsequent discussion elaborates the model components in detail, including the state $S_t$, action $A_t$, rewards $R_t$, transition function $P$, and the discount factor $\\gamma$."}, {"title": "5. RG-CQL for Value Function and Policy Learning", "content": "This section introduces RG-CQL, an offline RL pretraining and reward-guided online RL fine-tuning framework, for solving the MDP model presented in the last section. As aforementioned, introducing the coordinated \"pooling-transit\" services greatly increases state and action spaces compared to implementing \"pooling-only\" services. Consequently, implementing traditional RL methods to learn from scratch through online iterative interactions with a manually crafted simulator would lead to low training efficiency and potential local optimal solutions. RG-CQL aims to overcome this bottleneck by leveraging a diverse set of data that broadly covers the real-world on-demand ride-pooling system and state transitions. In Section 5.1, we discuss the offline phase RL method which learns the best policy in support from existing data. In Section 5.2, we innovatively introduced a module named \"Guider\", which learns to predict agents' rewards from data in the offline training stage and guides the exploration of CDDQN in the online fine-tuning/deployment stage. In Section 5.3, we summarize our RL algorithm."}, {"title": "5.1. CDDQN for Offline Learning from Pre-collected Transitions", "content": "At the offline stage, we develop CDDQN based on the idea of conservatism [22], aiming for learning Q value from existing data.\nAs the first step, a batch of observations $D$ regarding state transition are obtained from existing on-demand ride data, with $D$ containing a series of trajectories $\\tau = (s,a,r,s')$. Here, $s$ is the state of a vehicle, $a$ is a vehicle's action, $r$ is the reward, and $s'$ the vehicle's state after taking action $a$ at state $s$. To enrich the training dataset, we include not only ride-pooling data but also data on non-pooling services where riders are served individually\u00b2. Despite this expansion, the definitions of vehicle state $s$, reward $r$, and action $a$ remain consistent with those outlined in coordinated ride-pooling and transit services. In the real world, TNCs can effectively collect and organize data from past dispatch decisions as a feasible strategy. Specifically, under their current or previous policies (such as one or a combination of our four benchmark policies), TNCs can aggregate the initial status of each dispatched vehicle and the details of the matched"}, {"title": "5.2. Reward Guided CDDQN for Online-finetuning", "content": "At the online stage, we deploy reinforcement learning algorithms developed during the offline stage to interact with real-world environments or simulations, aiming for further fine-tuning RL algorithms.\nFine-tuning CDDQN directly in an online setting can lead to solution inefficiencies, such as slow learning and initial unlearning, as highlighted in existing studies [23]. This disparity arises from the divergence in Q-value estimation between offline and online learning stages. As indicated by Equation (11), the offline training stage diminishes Q-values associated with unobserved actions in the existing dataset. Conversely, during online learning, agents exhibit optimism towards unseen state-action pairs. This optimism is exemplified by the \u03f5-greedy strategy outlined in Equation (8), where a large Q-value is allocated to a randomly selected action to foster exploration. This optimism-pessimism gap complicates the balance between offline RL and online fine-tuning, especially when agents explore the environment. Fig. 4(a) illustrates the dilemma caused by such an optimism-pessimism gap between offline RL and online fine-tuning. An agent risks getting lost at the beginning of online fine-tuning if it indiscriminately explores all unseen state-action pairs and updates its strategy, potentially undermining the strengths of the original conservative offline RL policy.\nObserving this, we introduce an innovative module, referred to as \"Guider\", to resolve the issue caused by the pessimism-optimism gap. As illustrated in Fig. 4(b), the Guider aims to enhance agents' exploration by suggesting less blindly optimistic actions while maintaining the potential to discover long-term optimal policies during the fine-tuning phase. To accomplish this, the Guider employs the reward function as the foundational model-based dynamics metric. It uses the instant reward $r$ to guide agents' action choices when exploration strategy is used in order-matching. The underlying premise is that state-action pairs generating significant negative short-term rewards, especially during peak demand periods, are likely to lead to a high number of rejected orders. Consequently, these pairs are unlikely to contribute to optimal long-term operational decisions, even when considering the trade-off between immediate rewards and long-term objectives. Therefore, by eliminating these unreasonable decisions from the exploration process, we can significantly guide agents to adopt a more conservative stance during online fine-tuning, while simultaneously improving the efficiency of exploration.\nTo be more specific, during the offline stage, we train a Guider network using supervised learning in addition to the CDDQN training. The Guider network is a neural network that learns a function approximator for reward $r$ using existing data, which would be used in the online stage. The loss function for this Guider"}, {"title": "5.3. Overview of RG-CQL Framework", "content": "The overview of our RG-CQL framework is depicted in Algorithm 1 and Figure 5, which embeds the key innovative concepts of our RG-CQL method delineated in Section 5.1 and Section 5.2.\nIn Algorithm 1, Step 2 to Step 10 are dedicated to the offline training phase leveraging existing on-demand ride-hailing trip data, corresponding to the RG-CQL training module in Figure 5. Both the reward \"Guider\" and the CDDQN value function are trained based on a batch of state trajectories retrieved from existing datasets. Specifically, at each training step $t\\in \\{0, \\Delta t,\\ldots,T\\}$, Step 6 samples a batch of trajectories regarding state transitions. Step 7 and Step 8 update the network parameters of CDDQN based on sampled trajectories, progressively identifying the best policy under conservative regularization. Step 9 updates the network parameters $\\phi$ of the Guider to enhance the accuracy of the reward function regression via supervised learning.\nStep 11 to Step 28 transfer the training framework from the offline stage to the online stage for policy fine-tuning. Actions are assigned to agents based on the reward estimation from the Guider and the"}, {"title": "6. Simulation Experiments and Discussions", "content": "In this section, we evaluate our proposed approach using real-world data across various scenarios. We begin by detailing the dataset and simulation parameters in Section 6.1. Next, in Section 6.2, we assess the performance of our approach with RL algorithms used in prior ride-hailing studies. Furthermore, in Section 6.3, we evaluate the value of each component in our RG-CQL framework via ablations studies. In Section 6.4, we test the robustness of RG-CQL under variations of key heyper-parameters. Lastly, in Section 6.5, through comparing with State-of-the-art (SOTA) offline to online RL methods, we demonstrate the effectiveness of our approach in addressing offline to online gap in the context of ride-pooling with public transit. Our simulation experiments serve two purposes. First, we demonstrate that integrating ride-pooling with transit services yields superior outcomes compared to systems offering pooling-only or non-pooling services. Second, we show that the proposed RG-CQL framework surpasses commonly employed RL algorithms (Online RL, Offline RL, and Offline to Online RL) in terms of training efficiency and effectiveness."}, {"title": "6.1. Dataset and Simulation Setup", "content": "(a) Order data and study area The simualtor is built based on trip request data extracted from the dataset presented in [46, 67, 68], which is sourced from the taxi trips of New York City [69]. This dataset includes detailed trip-specific information such as pick-up and drop-off times, origin and destination geo-coordinates, trip distance, and duration. From this dataset, we extract data for trips occurring during the morning peak hour (8:00 AM - 9:00 AM) on May 4, 2016, with an average order density of around 271 trips per minute. For online fine-tuning, each training episode involves a sample of 95% of these trips, totaling approximately 15,300 orders. The study area is Central Manhattan, which is partitioned into smaller grid zones with a resolution of 800m x 800m. The average number of trip requests originating from each zone per minute is calculated based on the extracted data. In the simulation study, fifty-seven zones are selected for simulation. Fig. 6 visualizes the order density of each demand zone using a heatmap, where the sidebar denotes the average number of orders received per minute.\nDuring simulation, we treat each trip request as an order from a rider, with the pick-up time serving as the trip request time. Based on the zone partitioning, the origin and destination zones for each trip are determined."}, {"title": "4.2. Real-time Order Dispatch and Drop-off Location Choices", "content": "In this subsection", "16": "we use an $\\epsilon$-greedy strategy that leverages exploitation and exploration to address the long-term uncertainties inherent in the ride-pooling services. Specifically", "max_{a_{n,t}": "a_{n", "as": "n$\\begin{cases"}, "nw(n, m) = \\max_{a_{n,t}} Q_\\pi(S_{n,t}, a_{n,t}), \\text{ with probability } 1 - \\epsilon \\\\\nw(n,m) = Q, \\text{ with probability } \\epsilon\\end{cases}$\nWith the bipartite graph defined above, the action of agents $a_{n,t}$ and the resultant expected returns can be uniquely determined at a decision time $t$ once the selected edges linking vehicle nodes and rider nodes are established. To this end, we introduce a variable $x_{n,m}$ for each edge, which equals 1 if the edge connecting nodes m and n is selected and 0 otherwise. The following ILP program is formulated for decisions on order-dispatching and drop-off locations, which merges reinforcement learning's policy function with a bipartite matching process:\n$\\begin{aligned}\n\\max_X &\\quad \\sum_{n:n \\in \\mathbb{N}} \\sum_{m:m \\in \\mathbb{M}} w(n,m) x_{n,m},\\tag{9a}\\\\\ns.t.& \\quad \\sum_{n \\in \\mathbb{N}} x_{n,m} \\leq 1, \\quad"]}