[{"title": "Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework", "authors": ["Yulong Hu", "Tingting Dong", "Sen Li"], "abstract": "This paper introduces a novel reinforcement learning (RL) framework, termed Reward-Guided Conservative Q-learning (RG-CQL), to enhance coordination between ride-pooling and public transit within a multimodal transportation network. We model each ride-pooling vehicle as an agent governed by a Markov Decision Process (MDP), which includes a state for each agent encompassing the vehicle's location, the number of vacant seats, and all pertinent information regarding the passengers on board. We propose an offline training and online fine-tuning RL framework to learn the optimal operational decisions of the multimodal transportation systems, including rider-vehicle matching, selection of drop-off locations for passengers, and vehicle routing decisions, with improved data efficiency. During the offline training phase, we develop a Conservative Double Deep Q Network (CDDQN) as the action executor and a supervised learning-based reward estimator, termed the Guider Network, to extract valuable insights into action-reward relationships from data batches. In the online fine-tuning phase, the Guider Network serves as an exploration guide, aiding CDDQN in effectively and conservatively exploring unknown state-action pairs to bridge the gap between the conservative offline training and optimistic online fine-tuning. The efficacy of our algorithm is demonstrated through a realistic case study using real-world data from Manhattan. We show that integrating ride-pooling with public transit outperforms two benchmark cases\u2014solo rides coordinated with transit and ride-pooling without transit coordination-by 17% and 22% in the achieved system rewards, respe\u0441tively. Furthermore, our innovative offline training and online fine-tuning framework offers a remarkable 81.3% improvement in data efficiency compared to traditional online RL methods with adequate exploration budgets, with a 4.3% increase in total rewards and a 5.6% reduction in overestimation errors. Experimental results further demonstrate that RG-CQL effectively addresses the challenges of transitioning from offline to online RL in large-scale ride-pooling systems integrated with transit.", "sections": [{"title": "1. Introduction", "content": "In recent years, the widespread use of transportation network companies, such as Uber, Lyft, and Didi, has raised concerns about their negative impacts on traffic and the environment. For these ride-hailing platforms, the ability to immediately respond to passenger requests is a crucial element of their success. However, this business model heavily relies on a large number of drivers who remain idle and cruise around waiting for passengers [1]. This not only leads to low vehicle utilization rates and high fuel costs but also exacerbates traffic congestion and carbon emission in urban areas. Furthermore, a more pressing concern is that ride-hailing services may compete with public transit systems. The convenience offered by ride-hailing services can divert a substantial number of riders away from public transportation [2-5], leading to what could become a vicious cycle that undermines public transit development. This may contribute to a rise in vehicle miles traveled, increased traffic congestion, reduced transit ridership, and higher emissions, which run counter to the philosophy of developing transit-oriented, multimodal transportation systems that are operationally efficient, financially sustainable, and environmentally friendly.\nTo address the aforementioned concerns, substantial research has been conducted along two major directions, including: (a) how to improve the operational efficiency of ride-hailing services; and (b) how to promote collaborative relationships between ride-hailing and public transportation systems. To achieve the first goal, a significant body of work has focused on ride-pooling services, which allow multiple passengers to share a ride in a single vehicle through a digital platform, traveling to similar or conveniently aligned destinations. By consolidating multiple orders within the same vehicle, ride-pooling can effectively enhance vehicle utilization rates, while also reducing per-person travel miles and carbon emissions. In this vein, various forms of ride-pooling services have been explored, including multi-hop ride-pooling [6], diurnal-adaptive ride-pooling management [7], and ride-pooling with passenger transfers [8]. To achieve the second objective, another stream of research has investigated the coordination between ride-hailing services and public transit systems [9-12]. The underlying premise is that ride-hailing services can either complement or compete with public transit, depending on their operational approach. To maximize the complementary effects between these two, strategies have been proposed to utilize ride-hailing services for the first and/or last leg of a journey, while leveraging public transit for the long-distance middle leg between transport hubs. This approach could potentially increase transit ridership by extending public transit access to those who are unable to reach transit facilities without the aid of ride-hailing services.\nThere exists significant synergy by combining the aforementioned two research directions, where ride-pooling services operate in conjunction with transit networks, enabling passengers to share a ride on the ride-hailing platform to cover the first and last miles of their trip, while public transportation handles the long-distance segments between transportation hubs. This arrangement is mutually beneficial for both ride-pooling and public transportation networks. In particular, a crucial determinant of the performance for ride-pooling services is the shareability of rides, which depends on whether the origins and destinations of passengers are compatible and can thus be pooled together. By integrating with transit services, we can allow significant flexibility for the ride-pooling platform to choose the drop-off point, which can significantly enhance the chances of passengers being pooled. Meanwhile, pooled rides can reduce costs for passengers and enable more people, especially lower-income passengers who are more transit-dependent, to access transit networks. This can initiate a virtuous circle for transit development, where increased ridership improves the financial condition of transit operators, who can then further enhance service quality and in return attract more passengers.\nHowever, operating ride-pooling services within multimodal transportation networks that consider inter-modal transfers poses significant research challenges. Firstly, the decision-making involved in managing large-scale multimodal networks under uncertainties is complex for ride-hailing platforms. Specifically, the platform must determine how to pool passengers, match them to vehicles, route these vehicles, and select appropriate pick-up and drop-off points for inter-modal transfers. These tasks are intricately intertwined and must be addressed in real-time amid significant uncertainties, rendering traditional model-based algorithms [13-15] inadequate. This has led to the exploration of Reinforcement Learning (RL) algorithms. However, in the realm of RL, especially when applied to the coordinated management of ride-pooling and transit networks, training models from scratch is not only time-consuming but also prone to yielding low-"}, {"title": "2. Literature Review", "content": "2.1. Order Matching for Ride-Pooling Platforms\nRide-pooling services enable individual passengers to share a ride in a single vehicle, coordinated through a digital platform, to travel to similar or conveniently aligned destinations. The operational dynamics of ride-pooling have garnered considerable attention due to their promising yet unpredictable real-time demand, as evidenced by various studies [32\u201335]. The nature of this uncertainty, coupled with the full potential of ride-pooling systems, introduces complexity into the process of coordinating vehicles with multiple passengers. Effective coordination requires not only addressing the needs of current passengers but also anticipating the needs of future riders, which includes managing new ride requests and those already being served. Initial investigations in this field have considered short-sighted, or myopic, policies that make vehicular assignments based on presently available information [32, 33]. To exemplify, [33] notably advances this by introducing the shareability graph, which identifies possible sharing opportunities between new requests and vehicles on standby. They put forward a batch-matching strategy and crafted a sequential method that divides the decision-making process into vehicle routing and passenger assignment tasks. For more efficient real-time operations, [34] reduces the complexity of the matching problem by limiting the process to pairing a single passenger with a vehicle at each time step. Recent advancements in decision-making processes have increasingly focused on integrating uncertainties related to future demand more effectively through methods like Model Predictive Control (MPC) [36\u201338], Approximate Dynamic Programming (ADP) [39\u201341], and Stochastic Programming [42]. Specifically, MPC optimizes control inputs by looking ahead and forecasting future system behavior over a set time horizon; however, its effectiveness heavily relies on the accuracy of model predictions and necessitates substantial computational resources for real-time operations. ADP tackles the challenge by employing complex representation algorithms to approximate value functions, making its performance heavily dependent on the quality of the approximation model used.\nTo fully address the challenges associated with long-term uncertainties inherent in both the ride-pooling mode and order distributions and waive the need for specific modelling, the RL method shows promise due to its ability to learn from uncertainties through experience, its adaptability in managing large-scale and complex decision spaces, and its strong performance in recent applications [25, 43-45]. Inspired by its potential, [46] initially proposed the use of the DDQN RL method for ride-pooling operations, training it from scratch through online iterative interactions with a specially designed simulator. This enabled individual vehicles to independently learn and adapt dispatch and relocation strategies, demonstrating significant improvements in efficiency and performance over traditional methods. Building on this foundation, subsequent research has expanded online RL training to more complex ride-pooling scenarios. Specifically, [6] introduces a distributed, model-free algorithm that utilizes deep reinforcement learning for multi-hop ride-pooling, leading to significant cost reductions and enhanced fleet utilization. [7] presents an adaptive, model-free deep reinforcement learning framework for optimizing ride-pooling operations, which leverages online Dirichlet change point detection to adapt to dynamic environments. Additionally, [8] explores an optimization strategy for ride-pooling with passenger transfers using a hybrid model that combines deep RL and ILP. Despite these advancements, training RL from scratch remains challenging, inefficient, and prone to local optima in more complex ride-pooling applications, such as coordinating ride-pooling with transit, due to the trial-and-error, data-intensive nature of RL and limited exploration budgets."}, {"title": "2.2. Coordinating Ride-hailing Services with Public Transit", "content": "Another concern accompanying the development of ride-hailing studies is its negative externalities on urban transportation systems, where the most significant issue is the competitive interaction between ride-pooling and public transit. Research has shown that ride-sourcing platforms can replace transit rides, leading to increased vehicle miles traveled, exacerbated traffic congestion, reduced transit ridership, and heightened emissions [2-5]. To harmonize the ride-hailing services with public transit, several researchers attempt to coordinately dispatch ride-sourcing with public transit [9-12]. Specifically, Feng et al. [9] focused on the first-mile problem with ride-sourcing services and designed an Integer Linear Programming (ILP) Framework to coordinate ride-sourcing with metro systems. To take future uncertainty into consideration within ILP, they aim to estimate long-term expected rewards through TD learning with Online RL. [10] developed a game-theoretical framework to compute the Nash Equilibrium within the intimate interactions between the ride-hailing platform, the transit agency, and multi-class passengers with distinct income levels. Based on the proposed model and solution, they investigated two regulatory policies that can improve transport equity. Xu et al. [11] explored the strategic placement of transportation hubs in urban areas, proposing that Transportation Network Companies (TNCs) allocate Autonomous Vehicles to these hubs. Their study models five distinct trip types: Intra-Hub, Inter-Hub, Hub to Outer, Outer to Hub, and Outer to Outer. It also incorporates modelling factors such as passenger choice and waiting times. To solve the proposed model, they employed a Multi-Objective Adaptive Particle Swarm Optimization (MOAPSO) approach to search and find the optimal solutions. [12] introduced subsidies and additional bus services for passengers in high-demand areas, encouraging them to initially use other forms of public transportation to reach less congested areas before completing their journey via ride-sourcing. To determine the optimal dispatching and subsidy schemes, they developed a bi-level mixed integer programming model grounded in network flow theory and designed a corresponding iterative algorithm to effectively solve the model. Although the works above well study the problem of coordinated dispatch of ride-sourcing and public transit, their models and solutions are mainly focused on solo-rides, and thus might not generalize well to the case of ride-pooling within the context of multimodal transportation networks.\nAside from solo-rides, ride-pooling services has also been considered in the context of multimodal trans-portation systems. Along this direction, one stream of works considers the strategic planning decisions in the integrated system comprising both ride-pooling and public transit services. For instance, Zhu et al. [47] initially viewed ride-pooling services as both a complementary first- and last-mile cooperator and a competitive alternative to traditional public transit. They developed a multi-modal network model to examine commuter behaviors across various transport modes. Their findings revealed that, under conditions of low fare ratios, a significant number of public transit users might switch to ride-pooling for longer distances. They also noted that prioritizing ride-pooling excessively could reduce its demand for long distance ride-pooling and thus increasing usage of public transit. Liu et al. [48] proposed an integrated system that positions ride-pooling services as a localized demand-responsive transportation component, solely serving as a first- and last-mile feeder for public transit. They developed aspatial queuing model for demand responsive transportation component and integrate it with transit network to demonstrate the promising performance of the proposed system. [49] introduced a \u201chold-dispatch\" strategy for ride-pooling operations. In this strategy, ride-pooling vehicles are held until they accumulate a target number of ride requests. Once the threshold is met, the vehicles are dispatched to pick up passengers, following routes optimized by solving the Traveling Salesman Problem (TSP). This strategy aims to maximize overall system efficiency and costs, thereby enhancing the overall effectiveness of ride-pooling as an integral component of public transit systems.\nAnother research stream concentrates on operational aspect for coordination of ride-pooling services in conjunction with public transit. This involves dynamically pooling passengers, matching them with available vehicles, intelligently routing these vehicles, and strategically selecting pick-up and drop-off points for seamless inter-modal transfers. This complex array of tasks requires sophisticated, long-term optimal, and real-time decision-making to ensure efficient and effective transportation service integration. To address existing gaps, [14] investigated the efficacy of online solution algorithms that utilize queueing-theoretic approaches for vehicle dispatch and idle vehicle relocation, tailored specifically to these challenges. Additionally, [13] explored the potential of integrating ride-pooling with public transit, with a particular focus on the corresponding ride-matching technologies. To enhance the practical deployment of these solutions, [15] introduced an ILP formulation using a hypergraph representation of the problem and employed approximation algorithms to mitigate computational demands. However, none of the aforementioned works have fully addressed the gap in optimal operational strategies for multimodal transportation networks comprising both ride-pooling and public transit services. Specifically, [14] utilized a queuing-theoretic model that overlooks the intricate real-time matching decisions required to pair riders, who have distinct origins and destinations, with vehicles that may already carry passengers headed to specific destinations. In contrast, both [13] and [15] considered detailed real-time matching; however, the algorithms proposed in these studies are predominantly myopic, failing to consider the temporal correlations between current and future decisions, which are uncertain and might not be revealed at the time of decision-making."}, {"title": "2.3. Offline RL and its Deployment in Ride-Sourcing Dispatch", "content": "To mitigate the sample complexity inherent in the trial-and-error and data-intensive nature of online RL, offline RL, or batch RL, aims to train agents directly from pre-collected datasets instead of active interactions with the environment. This approach seeks to align RL with the data and time efficiency characteristic of supervised learning. Observing the promising developments in offline RL, several researchers in ride-sourcing have proposed adopting vanilla offline RL methods or off-policy RL techniques such as Q-learning and SARSA [16] to learn value functions from batches of past vehicle transitions. Specifically, [20] employs a batch training algorithm through TD learning with deep value networks to learn the spatiotemporal state-value function from historical data, optimizing repositioning actions in ride-hailing services. [18] introduces Fitted Q-Iteration for optimizing vacant taxi routing, leveraging archived GPS data from taxis to train models without the need for real-time environment interaction. Additionally, the framework proposed in [21] combines linear programming (LP) and tabular TD learning to dynamically reposition idle vehicles in ride-hailing systems. Here, LP manages the immediate repositioning based on a T-step lookahead prediction, while tabular TD learning is utilized to optimize long-term outcomes by learning from historical data and adjusting policies accordingly. This dual approach ensures both immediate responsiveness and strategic foresight in vehicle repositioning. Although these approaches significantly reduce computational costs and training time compared to traditional online Q-learning methods in ride-sourcing scenarios, adopting off-policy RL algorithms can be challenging due to extrapolation errors stemming from the OOD shift between the policy that collected the data and the learned policy [17]. This issue is particularly pronounced in increasingly complex ride-pooling systems where batches of transitions are likely to be incomplete for the large state-action decision space. Moreover, none of the cited works have explored the problem of offline RL policy fine-tuning in complex ride-pooling systems."}, {"title": "2.4. Recent Advances and Fine-tuning Dilemma in Offline RL", "content": "Recent progress in offline RL domain is mainly based on conservatism(or pessimism) [22]. One direction is to enforce the learned policy to stay close to the behavior policy e.g. by applying Lagrangian through advantage weight regression [50], or by adopting TD learning with behavior cloning and constraint [51, 52]. To exemplify, [51] employs a generative model conditioned on the state to produce actions that have been previously observed in the dataset, avoiding actions outside this distribution. This model is integrated with a Q-network that selects the highest valued action among those generated, ensuring that the actions are both plausible according to the data and valuable in terms of expected reward. [50] proposes an off-policy reinforcement learning algorithm that relies on simple supervised regression steps to optimize policy and value functions using previously collected data from a replay buffer. However this direction requires user to have pre-knowledge or estimation of sample policy for aggregated data, which might not be feasible in many real-world scenarios. Another direction is to implement critic regularization to the value function update like Conservative Q-learning [17] and Implicit Q-learning [53] to directly address extrapolation error. Specifically, [17] intentionally lower-bounds the expected value of a policy through a regularization term added to the Q-function update, which penalizes overestimation of action values under the policy being evaluated. [53] avoids directly querying the value of unseen actions by utilizing state value functions treated as random variables dependent on actions. The approach alternates between fitting an upper expectile of this random variable and updating a Q-function through SARSA-style temporal difference backup. Building on this trend, researchers have further showcased the capabilities of offline RL to learn from human demonstrations in scenarios, such as robotics [54] and multi-task learning in Atari games [55]. This progress has been achieved by incorporating advanced neural network architectures, specifically the ResNet [56] and Transformer [57] into these systems respectively.\nAlthough these exciting progress turns out to become effective for addressing extrapolation error, researcher find that there always exists a gap between offline learning and online learning. This gap results in current offline RL methods suffering from slow learning or initial unlearning during online further training [23]. To bridge this gap, several branch of efforts have been made in the offline RL community. The first approach, as proposed by [58], involves loading pre-collected data trajectories as an initial experience batch for off-policy RL methods and then conducting training as usual. While this method is simple and efficient, its performance is still upper-bounded by state-of-the-art pure offline RL pipelines. Another line of approach, discussed by [59, 60], involves training multiple value functions pessimistically during the offline training process. Afterwards during online fine-tuning, agent then performs actions based on the expected estimation of the ensemble value function. Though the ensemble value function manages to further reduce OOD overestimation, the method could become memory inefficient due to the requirements for training over tens of neural networks simultaneously. The final approach aims to lower-bound the offline training of a conservative Q-function with a pre-trained reference value function [23]. However, because the lower-bound of the pre-trained reference value function is crucial for the proposed framework and how inaccurate reference value function will impact the performance of the policy remains unknown, this method requires prior knowledge of the sample policy or environment system. The challenges above limit the adoption of these popular offline RL methods in our complex problem like coordinating ride-pooling with public transit.\nWith the rise and popularity of LLM like GPT-4, designing neural network and learning reward model or estimator to facilitate RL fine-tuning according to pre-collected human feedback has gained more attention than ever before. Specifically, [24] and [25] proposed training reward models based on human preferences with supervised learning to better facilitate the training of RL. As an extension, [26] attempts to encode the reward model directly into the update of the RL policy. However, none of these ideas have been applied to the specific context of multimodal transportation systems. Inspired by the aforementioned works, we propose RG-CQL, where we managed to train a Guider during offline RL stage for reward regression and utilize the Guider to guide the exploration during online fine-tuning of RL policy. Under our novel offline training and online fine-tuning RL framework, RG-CQL could learn both efficiently and effectively for the complex task of coordinating ride-pooling with transit."}, {"title": "3. Problem Description", "content": "We consider using ride-pooling services to address the first-mile problem of transit services. Riders requesting trip services are assumed to be willing to use ride-pooling services on their first leg of trips and transit services on the second leg of their trips. As shown in Fig. 1, a ride-pooling vehicle could pick up a rider from her origin and deliver the rider directly to her destination or to a transit station so that the rider can take transit vehicles. Riders who continue their trip using transit would get off at the stations closest to their destinations that are accessible via walking. The transit network comprises a collection of transit stations I, along with transit routes linking these stations. Transit vehicles are assumed to follow fixed routes and schedules that are not influenced by ride-pooling services. The schedules of transit vehicles are given by a timetable that specifies a transit vehicle's arrival and departure times at each transit station $i \\in I$. Riders dropped at transit stations are expected to follow the shortest transit route to minimize travel time to their destinations, potentially transferring between transit lines. Given the transit network and schedules, a centralized platform dispatches ride-pooling vehicles to riders and decides whether to drop off riders at their destinations or transit stations. For differentiation, we refer to a match as a \"pooling-only\" match if a rider is directly delivered to her destination and as a \"pooling-transit\" match if a rider is dropped off at a transit station and uses both ride-pooling and transit services to complete her trip. During a rider's journey on a ride-pooling vehicle, she might share a ride with other riders, depending on the route of the ride-pooling vehicle and the itinerary of other riders.\nWe consider the operation of the above coordinated ride-pooling and transit services over a finite planning horizon $T = {0, \\Delta t, 2\\Delta t,......,T}$ where $\\Delta t$ denotes time-step for decision making. The ride-pooling requests are revealed over time and arrive following a stochastic process. The platform is supposed to use a batch-matching strategy that matches a set of ride-pooling vehicles $N$ with riders $M$ that accumulated over a fixed time interval $\\Delta t$ (such as 1 minute). Each rider $m \\in M$ is characterized by parameters $(o_m,d_m,e_m,l_m)$ which specifies rider $m$'s origin location $o_m$, destination location $d_m$, service request time $e_m$, and the expected arrival time $l_m$. The trip delay of a rider is calculated as the difference between a rider's actual arrival time at her destination and her expected arrival time, including the delay incurred by ride-pooling services and by transit services (if a match is a \"pooling-transit\" match). At time $t$, each ride-pooling vehicle $n \\in N$ is characterized by its current location $I_{n,t}$, remaining capacity $v_{n,t}$ to accommodate additional riders, riders that are being delivered $p_{n,t}$, and a scheduled route for picking up and dropping off those assigned riders in $p_{n,t}$. At each decision time, the platform anticipates future potential trip demand and maximizes its expected profits by deciding (i) the matching between ride-pooling vehicles $n \\in N$ and riders $m \\in M$;"}, {"title": "4. Model Formulation", "content": "4.1. MDP Model\nThis subsection introduces the MDP model for coordinated ride-pooling and transit services with each ride-pooling vehicle being treated as an agent.\n4.1.1. Model Elements\nTo streamline the model formulation", "State": "At time $t \\in T$", "c_m": ".", "s_{N,t}": ".", "Action": "For a fully occupied ride-pooling vehicle where $v_{n", "m$": "it either drops off rider $m$ at her final destination or at an intermediate transit station $i \\in I$", "a_{N,t}": ".", "Reward": "We denote $r_{n", "endure": "n$r_{n", "min\\left{\\sum_{k": "k \\in O_{n", "max\\left{\\sum_{k": "k \\in O_{n"}, {"min\\left{\\sum_{k": "k \\in p_{n", "max\\left{\\sum_{k": "k \\in p_{n", "Function": "The transition function is denoted as $P(S_{t+1"}], "Factor": "The discount factor $\\gamma$ quantifies the present value of future rewards", "formula": "n$Q_n(S_t", "Pi$": "n$Q_n(S_t", "sum_{\\tau": "tau \\in K_t"}, "gamma^{\\tau", "R_{t+\\tau+1", "S_t", "A_t \\right]", 3, "nwhere $K_t = {0", 1, 2, "T \u2013 t", "denotes the set of time steps afterwards t until the end of planning horizon. The objective of the platform is to determine the optimal policy $\\Pi^*$ that maximizes the platform's expected discounted cumulative reward over the whole planning horizon.\nDirectly solving policy $\\Pi$ presents significant challenges due to the curse of dimensionality. Note that the platform's expected return depends on how the system states evolve", "which further relies on the actions of all agents. Also", "at each decision time", "thousands of agents might need to be simultaneously dispatched", "leading to a prohibitively high dimensional state and action space. To simplify analysis and facilitate the model solution", "we make the following two assumptions", "which are very common in the literature of multi-agent RL [6", 8, 9, 46, 64, 65, "i) agents are independent in that an agent's reward and state transition probability only depends on its own state and actions, which is independent of the states and actions of other agents; (ii) agents are homogeneous and thus share the same policy"]