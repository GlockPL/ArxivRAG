{"title": "Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion", "authors": ["Li Liang", "Naveed Akhtar", "Jordan Vice", "Xiangrui Kong", "Ajmal Saeed Mian"], "abstract": "3D semantic scene completion is critical for multiple downstream tasks in autonomous systems. It estimates missing geometric and semantic information in the acquired scene data. Due to the challenging real-world conditions, this task usually demands complex models that process multi-modal data to achieve acceptable performance. We propose a unique neural model, leveraging advances from the state space and diffusion generative modeling to achieve remarkable 3D semantic scene completion performance with monocular image input. Our technique processes the data in the conditioned latent space of a variational autoencoder where diffusion modeling is carried out with an innovative state space technique. A key component of our neural network is the proposed Skimba (Skip Mamba) denoiser, which is adept at efficiently processing long-sequence data. The Skimba diffusion model is integral to our 3D scene completion network, incorporating a triple Mamba structure, dimensional decomposition residuals and varying dilations along three directions. We also adopt a variant of this network for the subsequent semantic segmentation stage of our method. Extensive evaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show that our approach not only outperforms other monocular techniques by a large margin, it also achieves competitive performance against stereo methods.", "sections": [{"title": "Introduction", "content": "3D semantic scene completion is essential for inferring missing geometric and semantic information in scene data acquisition. This task finds numerous downstream applications in autonomous driving (Li et al. 2022; Hu et al. 2023), robotic navigation (Tian et al. 2024; Jin et al. 2024), and planning (Mei et al. 2023) etc. Recent semantic scene completion techniques use different input modalities, e.g., LiDAR, images, multi-modal, to pursue an acceptable level of performance. However, the complexity of the underlying modeling objective still makes 3D semantic scene completion an open challenge for the research community.\nSeveral studies (Zhang et al. 2018; Roldao et al. 2020; Yan et al. 2021; Xiong et al. 2023; Xu et al. 2023; Xia et al. 2023) rely on LiDAR point clouds for 3D semantic scene completion. However, LiDAR data is inherently sparse - a property conflicting with the scene completion objective. Consequently, LiDAR-based scene completion models need to make up for this disparity with additional model complexity. For instance, Xia et al. (2023) developed SCP-Net, which enhances single-frame models through dense-to-sparse knowledge distillation. However, the model computational demands become sizable. Image-based methods (Cao and De Charette 2022; Zhang, Zhu, and Du 2023; Yu et al. 2024; Zheng et al. 2024b) utilize color images for the task, promising relatively simpler solutions. However, performance level of these techniques remains a bottleneck (Zhang, Zhu, and Du 2023; Yu et al. 2024). As an example, Yu et al. (2024) introduced CGFormer, which utilizes multiple representations to encode 3D volumes from both local and global perspectives. Their method also includes a depth refinement module to enhance depth estimation accuracy. Nevertheless, the advancement still lacks an acceptable performance for the critical nature of the task.\nMulti-modal inputs are also being considered for 3D semantic scene completion (Li et al. 2019b, 2020b; Cai et al. 2021; Dourado, Guth, and de Campos 2022; Wang et al. 2023; Cao and Behnke 2024). As a representative example, FFNet (Wang, Lin, and Wan 2022) aims at addressing inconsistencies in RGB+D data and uncertainties in depth measurements. The technique employs a frequency fusion module and a correlation descriptor to capture the explicit correlation of RGB+D features. Whereas using multiple modalities helps in performance gain, it comes with a considerable computational overhead and intricacies in modeling. Moreover, achieving accurate alignment between different modalities remains a significant challenge along this direction.\nIn this work, we focus on monocular image data to devise an effective 3D semantic scene completion method that demonstrates a marked performance gain in this category. Our approach leverages a unique combination of advances in generative diffusion modeling (Rombach et al. 2022) and state space models (Gu and Dao 2023) tailored to the problem at hand. We propose a neural network, see Fig. 1, that encapsulates data processing for 3D scene completion in a conditioned latent space of a variational autoencoder (VAE). The conditioning lifts 2D scene information to 3D for subsequent multi-scale processing that prepares data for"}, {"title": "Related Work", "content": "Our contribution is in 3D semantic scene completion. For that, we devise a diffusion-based network that employs state space modeling. Hence, we organize related work by discussing advances along the key topics of diffusion models, state space models and the 3D semantic scene completion.\nDiffusion Model. These models (Ho, Jain, and Abbeel 2020; Song et al. 2020; Song, Meng, and Ermon 2020) have been widely studied recently for generative tasks such as image generation (Zhou et al. 2023; Zheng et al. 2023), image restoration (Zheng et al. 2024a), video generation (Ni et al. 2023), 3D shape generation (Shim, Kang, and Joo 2023; Li et al. 2023a), and 3D occupancy prediction (Tang et al. 2024). Diffusion models are resource-intensive and require multiple function evaluations and gradient computations, leading to high costs (Patterson et al. 2021). To mitigate that, Rombach et al. (2022) introduced a latent diffusion model, which reduces computational demands by applying the diffusion process in the latent space of an auto-encoder. In 3D, while most research has focused on individual object generation, Tang et al. (2024) has also tackled scene synthesis by employing a 3D sparse diffuser with spatially decomposed sparse kernels and a re-imagined sparse transformer head for 3D semantic occupancy prediction.\nState Space Model. State space modeling was originally developed in modern control theory (Glasser 1985) to describe dynamic systems, and has proven effective in modeling long-range dependencies. Recently, state space models have emerged as an alternative to convolutional neural networks (CNNs) and Transformers in natural language pro-\ncessing (NLP) and computer vision (Anthony et al. 2024; Li, Hong, and Fan 2024). For example, S4 (Gu, Goel, and R\u00e9 2021) shows impressive results in the vision domain using a diagonal parameter structure for efficient modeling. Subsequent models like HTTYH (Gu et al. 2022b), DSS (Gupta, Gu, and Berant 2022), and S4D (Gu et al. 2022a) continue this trend with diagonal matrices, maintaining performance with reduced costs. S5 (Smith, Warrington, and Linderman 2022) improves efficiency through parallel scanning, and MIMO SSM (Smith, Warrington, and Linderman 2022) introduces mechanisms for linear-time inference and efficient training, further refining Mamba (Gu and Dao 2023). The Mamba architecture has been applied to image classification (Li, Singh, and Grover 2024; Zhu et al. 2024), image segmentation (Anthony et al. 2024), and point clouds (Liu et al. 2024). In particular, (Liu et al. 2024) proposed Point Mamba, featuring an octree-based strategy for globally sorting raw points while preserving spatial proximity.\n3D Semantic Scene Completion. 3D semantic scene completion methods can be broadly divided into four main categories: image-based (Song et al. 2017; Yu et al. 2024), point cloud-based (Nie et al. 2021; Xiong et al. 2023), voxel-based (Yan et al. 2021; Xia et al. 2023), and multi-modality-based (Cai et al. 2021; Wang, Lin, and Wan 2022). These methods commonly employ CNNs or transformers. For example, Song et al. (2017) proposed an end-to-end 3D convolutional network with a dilation-based context module to efficiently learn context with a large receptive field. Xia et al. (2023) introduced SCPNet, which enhances single-frame model representations using dense relational semantic knowledge distillation and a label rectification technique to eliminate traces of dynamic objects. Although voxel-based methods are computationally more efficient than point-based methods, they often experience information loss. CNNs have limited receptive fields, and while transformers can expand receptive fields, they demand high memory usage. Mamba framework (Gu and Dao 2023) offers a balanced solution by expanding the receptive field without incurring high memory costs, which is promising for 3D semantic scene completion."}, {"title": "Methodology", "content": "The primary objective of monocular 3D semantic scene completion is to infer comprehensive 3D geometric and semantic information from a single image. Given an image $I \\in \\mathbb{R}^{L_1 \\times W_1 \\times 3}$, it is projected to a grid of 3D voxels in $\\mathbb{R}^{L \\times W \\times H}$. The task involves accurately completing the scene and assigning a class label to each voxel, determining whether it is empty or contains an object from one of $C$ semantic categories, represented as $c \\in 0, 1, 2, . . . , C \u2013 1$.\nProposed Network\nA schematic diagram of our proposed network is given in Fig. 1. A key component of our approach is the Skip mamba (Skimba) denoising diffusion network. This proposed network forms an integral part of the overall technique such that its architecture is also leveraged for the 3D semantic segmentation required for the scene completion task. The overall approach employs a variational autoencoder (VAE)"}, {"title": "Projection / Feature Extractor.", "content": "The use of Projection and Feature Extractor components is shown in Fig. 1. Their objective is to project 2D information to 3D, which is a challenging task due to the scale ambiguity present in a single viewpoint data available for our problem (Fahim, Amin, and Zarif 2021). The Projection component back-projects 2D features along their optical rays to create a unique 3D representation, enabling 2D-3D disentangled representations. This allows the subsequent 3D network to use high-level 2D features for detailed 3D disambiguation - a concept inspired by the approach of Cao and De Charette (2022). The Feature Extractor first converts image data into voxel representations through a projection layer. It then combines a multi-path block, proposed in SCPNet (Xia et al. 2023) with a Dimensional Decomposition Residual (DDR) block proposed by Li et al. (2019a), stacking these blocks layer-by-layer. This enables the extraction of sufficient local and global contextual features for the subsequent condition network."}, {"title": "Variational Autoencoder (VAE)/Condition Network.", "content": "As apparent from Fig. 1, our MSCB and Skimba denoising network operate in the latent space of a VAE that encapsulates the 3D scene completion network. To construct the VAE, we build on the insights of (Milletari, Navab, and Ahmadi 2016) and use an autoencoder trained with cross-entropy loss and Lovasz-softmax loss to ensure reconstructions remain on the grid manifold, avoiding the blurriness common with voxel-space losses like L2 or L\u2081. For a given voxel scene $v \\in L \\times W \\times H$, the encoder; say $E$, encodes $v$ into a latent representation $z = E(v)$. The decoder $D$ reconstructs it, yielding $\\tilde{v} = D(z) = D(E(v))$, such that $z \\in \\mathbb{R}^{l \\times w \\times h \\times c}$. The encoder downsamples the voxel scene by a factor of $f = L/l = W/w = H/h$, where $f = 4$ in our experiments. The diffusion model leverages the 3D structure of the learned latent space for efficient compression and high-quality reconstruction. The condition networks follow the encoder in their architecture to match their representation space with the VAE's latent space."}, {"title": "Multi-Scale Convolutional Block (MSCB).", "content": "Our technique uses a Multi-Scale Convolution Block (MSCB) to efficiently extract multi-scale features. Details of this block are illustrated in Fig. 2 (f). Our design is inspired by the multi-path block in SCPNet (Xia et al. 2023) and the popular VGG architecture (Simonyan and Zisserman 2014). The MSCB replaces larger convolution kernels with sequential 3 \u00d7 3 \u00d7 3 convolution layers. Two iterations replicate a 5 \u00d7 5 \u00d7 5 convolution, while three iterations approximate a 7 \u00d7 7 \u00d7 7 convolution, significantly reducing computational costs. For example, the computational cost for a 5 \u00d7 5 \u00d7 5 convolution is reduced from 125C2 to 54C2 for $C$ channels. For a 7\u00d77\u00d77 convolution this reduction is from 343C2 to 81C2. This approach effectively captures both local and global contextual information while maintaining computational efficiency. In Fig. 2(f), we only show the 3 \u00d7 3 \u00d7 3 iteration for the MSCB."}, {"title": "Skimba Denoising Network.", "content": "In the 3D Scene Completion Network, our Skimba denoising sub-network processes the MSCB output. This network is inspired by the Mamba framework (Gu and Dao 2023). Adopting state-space modeling, this framework maps a one-dimensional continuous function or sequence $x(t) \\in \\mathbb{R}$ to $y(t) \\in \\mathbb{R}$ through a hidden state $h(t) \\in \\mathbb{R}^N$. As a continuous-time framework, it re-"}, {"title": "Semantic Block:", "content": "This is another type of block devised to help process the signal under the Mamba framework. The"}, {"title": "Skimba Block:", "content": "A key component of our Skimba denoiser is the Skimba block, shown in Fig. 2(b). This block follows a Triple Mamba (TM) configuration inspired by (Yang, Xing, and Zhu 2024) to capture both direct and indirect feature connections, thereby enriching the contextual information within high-dimensional features. We propose to use Skip Triple Mamba (STM) layers with varying dilations in three distinct directions: forward, reverse, and spatial. This is depicted in Fig. 2(c). Each STM layer computes feature dependencies using distinct dilation rates of 0, 1, and 3 in three directions, ensuring that comprehensive feature relationships are effectively captured."}, {"title": "Training Objective", "content": "Due to the complex underlying task, our training objective is of composite nature. The objective function for the Skimba denoising network follows from the diffusion denoising framework, which minimizes the Expected squared error in the predicted output, defined as\n$\\mathcal{L}_{DM} = \\mathbb{E}_{x,\\epsilon \\sim \\mathcal{N}(0,1),t} [||\\epsilon - \\epsilon_{\\theta} (x_t, t)||^2]$,\nwhere $\\epsilon_{\\theta} (x_t, t)$ denotes an equally weighted sequence of denoising autoencoder, where $t = 1 . . . T$. The model predicts a denoised variant of the input $x_t$, where $x_t$ itself is a noisy version of the input $x$ at time stamp $t$. For 3D the semantic segmentation, the objective function is given as\n$\\mathcal{L} = \\mathcal{L}_{CE} + \\beta \\mathcal{L}_{Lovasz}$,\nwhere $\\beta$ is a balancing coefficient that adjusts the contribution of each loss component, $\\mathcal{L}_{CE}$ is the cross-entropy loss, and $\\mathcal{L}_{Lovasz}$ is the Lovasz-softmax loss which optimizes mIoU - a crucial performance metric in semantic segmentation. The mathematical forms of both are as follows\n$\\mathcal{L}_{CE} = - \\sum_i y_i log(\\hat{y_i})$,\n$\\mathcal{L}_{Lovasz} = \\frac{1}{C} \\sum_{c=1}^{C} J(e^c)$,\nwhere $\\hat{y_i}$ and $Y_i$ are the prediction and ground truth for the ith element of the output. $J$ denotes the Lovasz extension of the IoU, a piecewise linear function that minimizes the mIoU error, and $e (c)$ is the error vector for each class $c$ within the set of classes $C$. The objective function of the VAE is constructed by integrating $\\mathcal{L}_{CE}$, $\\mathcal{L}_{Lovasz}$, and the KL divergence regularization term."}, {"title": "Experiments", "content": "The proposed network is evaluated on two standard benchmarks for semantic scene completion; namely, the SemanticKITTI dataset (Behley et al. 2019) and SSCBench-KITTI-360 (Li et al. 2023b). We also perform ablation experiments to extensively evaluate the impact of individual components of our approach.\nDatasets. The SemanticKITTI benchmark (Behley et al. 2019) provides densely annotated urban driving scenes derived from the KITTI Odometry Benchmark (Geiger, Lenz, and Urtasun 2012). This dataset consists of voxelized scenes within a spatial volume of 51.2m \u00d7 51.2m \u00d7 64m, resulting in voxel grids of 256 \u00d7 256 \u00d7 32 with a voxel size of 0.2m. It includes 10 sequences for training, 1 for validation, and 11 for testing, encompassing 20 semantic classes. Our approach uses RGB images with 1220 \u00d7 370 resolution, cropped from the original resolution of 1226 \u00d7 370. The SSCBench-KITTI-360 dataset (Li et al. 2023b) includes 7 sequences designated for training, 1 sequence for validation, and 1 sequence for testing, encompassing 19 semantic classes. The RGB images used as inputs have a resolution of 1408 \u00d7 376 pixels. We report results on popular standard metrics for scene completion and semantic segmentation on these datasets with standard evaluation practices.\nEvaluation metrics. To assess our framework and compare it with existing works, we use established evaluation metrics for semantic scene completion as outlined by Song et al. (Guedes, de Campos, and Hilton 2018). Our evaluation focuses on two primary aspects: accurate geometric reconstruction of the scene and precise semantic segmentation of each voxel. We use Intersection over Union (IoU) to measure geometric completion performance, defined as\n$IoU_i = \\frac{TP_i}{TP_i + FN_i + FP_i}$ where $TP, FP_i,$ and $FN_i$ represent true positives, false positives, and false negatives for the\nith class, respectively. Mean Intersection over Union (mIoU) is used to evaluate semantic accuracy across all categories, defined as $mIoU = \\frac{1}{C} \\sum_{c=1}^{C} IoU_c$, where $C$ is the total number of classes."}, {"title": "Implementation Details", "content": "Our experiments were conducted using a single NVIDIA GeForce 4090 GPU with 24GB RAM. The training memory required for each network is approximately 18GB. The VAE was trained for 24 epochs using the AdamW optimizer with an initial learning rate of 3e-4. The Skimba denoiser network was trained for 43 epochs with AdamW at a le-3 learning rate and 1e-4 weight decay. The denoising step in the Skimba network was set to 100. The Skimba 3D semantic segmentation network used AdamW with a 5e-3 learning rate and le-4 weight decay. A WarmupCosineLR scheduler was applied across all training processes to gradually reduce the learning rate for optimal performance."}, {"title": "Results", "content": "Our comparative analysis assesses the performance of our proposed method against several state-of-the-art techniques. These methods include LMSCNet (Roldao et al. 2020), AICNet (Li et al. 2020a), JS3C-Net (Yan et al. 2021), MonoScene (Cao and De Charette 2022), TPVFormer (Huang et al. 2023), SurroundOcc (Wei et al. 2023), OccFormer (Zhang, Zhu, and Du 2023), IAMSSC (Xiao et al. 2024), VoxFormer (Li et al. 2023c), DepthSSC (Yao and Zhang 2023), HASSC-S (Wang et al. 2024), H2GFormer-S (Wang and Tong 2024).\nThe results (obtained from the online server) on SemanticKITTI test data for monocular 3D semantic scene completion are presented in Table 1. These results highlight that our method achieves excellent performance. Note that, whereas our method only requires Monocular input, we also include results of the approaches relying on the more informative Stereo data. It is noteworthy that our method surpasses even some of the popular Stereo methods. Specifically, the proposed method attains 46.95% IoU and 13.82% mIoU, outperforming the nearest competing monocular method by 3.21% in IoU and 1.41% in mIoU. This significant performance gain can be attributed to the exceptional capability of Skimba diffusion in completing and segmenting larger objects, such as sidewalks, buildings, vegetation, fences, and tree trunks. Additionally, the method demonstrates robust performance on smaller objects, including cars and bicycles, indicating its versatility across different object scales. It is a common practice in the literature to report results on SemanticKITTI validation set as well. We include those results in the supplementary material, demonstrating equally strong performance.\nIn Table 2, we summarize the results on the SSCBench-KITTI-360 test set. Again, our method outperforms recent monocular approaches and even some stereo-based 3D semantic scene completion methods. This consistent performance across different datasets underscores the robustness and generalizability of our approach. It is notable that this dataset has a smaller size and relatively lower sample quality, which adds to the complexity of the dataset. The dataset is more suited to methods that take LiDAR data as input, which show stronger performance on this dataset in general. Nevertheless, our method maintains its performance on this"}, {"title": "Ablation Study", "content": "We conducted systematic experiments to evaluate the impact of the various components in our framework, to quantify their contributions to the overall performance. The results of these ablation studies, detailed in Table 4, provide valuable insights into the importance of each component within the architecture. By systematically removing specific blocks, we were able to observe variations in the performance. This allows us to identify the critical elements that most significantly enhance the effectiveness of the model in the 3D semantic scene completion task. The findings from these experiments clearly indicate that both the MSCB and the Skimba are essential for achieving high performance. Their inclusion contributes to the ability of the model to accurately capture and represent complex spatial and semantic relationships. The without (w/o) Skimba refers to the triple mamba block, and does not involve the STM layer."}, {"title": "Conclusion", "content": "We proposed a 3D semantic scene completion network with a Skimba denoising diffusion sub-network. Our approach incorporates a variational autoencoder with two conditioning networks to produce lower-dimensional, perceptually equivalent symbolic spaces for input data, which effectively reduces computational demands while maintaining performance. The Mamba-inspired Skimba network captures both direct and indirect feature relationships within the data by using various skip triple dilations. This functionality enhances the ability of the network to represent the spatial and semantic structure of complex 3D scenes. Extensive evaluations on the SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that our method outperforms existing state-of-the-art methods, highlighting its effectiveness and potential for advancing 3D semantic scene completion."}]}