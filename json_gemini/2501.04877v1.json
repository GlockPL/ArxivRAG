{"title": "Real-Time Textless Dialogue Generation", "authors": ["Long Mai", "Julie Carson-Berndsen"], "abstract": "Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository\u00b9.", "sections": [{"title": "Introduction", "content": "The success of large general-purpose language models, such as ChatGPT and LLAMA, has led to significant advancements in various natural language processing (NLP) tasks, especially in challenging tasks such as open-domain dialogue generation. Chatbot applications like Replika (Replika) and Character AI (CharacterAI) have demonstrated substantial progress in this sub-task, often achieving or surpassing human-level performance in terms of semantic coherence. Despite these advancements, conversations with current chatbots often feel unnatural, especially in spoken interactions. This issue primarily arises from the design of most chatbots, which prioritize generating well-written, formal responses, without incorporating informal conversational elements such as backchannels, laughter, and other paralinguistic cues. These elements are crucial for creating a more engaging, human-like experience.\nMost spoken-based systems rely on a cascade design, consisting of three distinct components: Automatic Speech Recognition (ASR), Dialogue Response Generation (DRG), and Text-to-Speech (TTS). This sequential structure introduces challenges for providing a fluid, interactive conversation, as it assumes turn-by-turn dialogue with no overlapping speech. In real-life conversations, however, two people can speak simultaneously through parallel channels. Figure 1 shows an example of conversation with cascaded dialogue system. In this system, the dialogue context is often expanded sequentially where each speaker's utterance is concated one after another (e.g., Speaker1-Speaker2-Speaker1). This forces the response generation model to wait for the user's utterance to finish before it can generate a response, often with a delay (e.g., 800ms). This results in slow and unnatural conversational flow. Additionally, the reliance on text as an intermediate representation introduces increased latency and complexity when integrating multiple components. Generating spoken responses based on text representations also limits the expressiveness of the speech.\nThere is existing research on making spoken dialogue more naturalistic, including turn-taking prediction (Ekstedt and Skantze, 2022; Ishii et al., 2016), backchannel generation (Hara et al., 2018; Ishii et al., 2021), and duplex conversations (Lin et al., 2022). However, these approaches primarily use text as their input/output modality and focus more on task-oriented rather than open-domain generation. The work most similar to ours is dGSLM (Nguyen et al., 2023), in which the authors propose a \"textless\" model capable of generating naturalistic spoken dialogue given a conversational prompt between two speakers. However, this model can only generate dialogue offline, making it impractical for real-world applications that require generating instant responses from streaming conversational input.\nTo overcome these limitations, we propose a novel approach: a real-time, textless dialogue generation model (RTTL-DG). Unlike traditional cascade systems, which require speech-to-text conversion before generating responses, our approach integrates ASR and DRG into a unified model that directly encodes spoken conversations. The encoded information is then used to predict the chatbot's next action-such as remaining silent or speaking-at regular, short intervals (e.g., 160ms). This enables fluid turn-taking and faster response times. Moreover, our model generates speech units instead of text as the target output, resulting in more natural-sounding speech. In summary, the RTTL-DG model provides the following advantages:\n1. Real-time response generation. The RTTL-DG model provides near-instantaneous responses by directly processing streaming input from conversations with minimal delay.\n2. Textless modeling. Our approach eliminates the need for text as an intermediate representation, reducing latency introduced by the ASR system and simplifying the inference pipeline.\n3. Spontaneous expressions. The model generates natural, coherent responses, including informal conversational elements such as hesitations and laughters, which are essential for fostering human-like interactions.\n4. Fluid turn-taking. RTTL-DG enables smooth turn-taking in conversations, capable of handling interruptions and speech overlaps.\n5. Diverse response formats. The model produces responses of varying lengths, from short back-and-forth phrases to long-form storytelling formats.\nExperimental results show that the proposed RTTL-DG model provides a more interactive and natural conversational experience. While it may slightly underperform in terms of semantic coherence compared to cascade models, it excels in naturalness, responsiveness, and fluidity. We believe these advancements are crucial for developing more engaging and relatable chatbot systems. The RTTL-DG model holds significant potential across various domains, including companionship, therapy, customer service, and other areas that require human-like dialogue interactions."}, {"title": "RTTL-DG inference pipeline", "content": "Figure 2 illustrates the process by which RTTL-DG generates responses during inference. The system comprises two main components: the Dialogue Manager and the Response Generator. Both modules process continuous streaming speech inputs from both the user and the chatbot itself.\nAt regular intervals (i.e., every 160 ms), the Dialogue Manager determines the chatbot's next action based on the most recent segment of the streaming input, truncated by a sliding context window (e.g., the last 20 seconds). The Dialogue Manager must select one of four possible actions: (1) Remain silent: the chatbot continues to listen without responding, (2) Initiate speaking: the chatbot begins to generate a response, with the content provided by the Response Generator, (3) Keep speaking: if the chatbot is already speaking, this action directs it to continue its current utterance, (4) Stop speaking: if both the user and the chatbot are speaking simultaneously, this action halts the chatbot's speech. Actions (1) and (2) enable the chatbot to determine the optimal moment to respond, which does not necessarily rely on detecting the end of the user's turn (e.g., a fixed period of silence). This flexibility allows the chatbot to provide timely feedback, such as backchannels, even while the user is still speaking. Meanwhile, actions (3) and (4) ensure smooth handling of interruptions, enabling the chatbot to adapt dynamically to user input without compromising the flow of conversation.\nWhen the Dialogue Manager decides to initiate a response, the Response Generator takes over. It uses the same input features as the Dialogue Manager-the most recent segment of the streaming conversation from both speakers. Instead of generating traditional text-based responses, this model generates speech units. The discovery of these units is done by training a speech quantization model (i.e., HUBERT+KMeans (Hsu et al., 2021)) on a large amount of unlabeled speech audio. Using speech units allows the chatbot to generate more natural and dynamic expressions, including elements such as pauses, hesitations, backchannels, and even laughter. Once the Response Generator generates the speech units, a unit-to-speech model converts them into a final spoken response."}, {"title": "RTTL-DG architecture", "content": "Figure 3 illustrates the model architecture of RTTL-DG, which comprises two main components: an encoder and a decoder.\nThe encoder is designed as a streaming causal transformer, tasked with processing the continuous speech input from each speaker. It includes a convolutional feature extractor followed by eight transformer encoder layers. The convolutional feature extractor converts raw audio signals into a sequence of hidden states with dimensions $R^{T \\times D}$, where T represents the number of timesteps (or sequence length), and D is the hidden dimension, set to 768. Each timestep corresponds to a 20ms segment of audio. The transformer encoder layers further refine these hidden states into contextual representations. To improve efficiency, the number of timesteps is progressively reduced by merging two adjacent hidden states after layers 2, 4, and 6. This reduction not only speeds up computation but also enhances the semantic richness of the hidden states. Since the model processes streaming input, causal masking is applied to ensure that the encoder layers do not access future information. The final output of the encoder for each speaker has dimensions $R^{(T/8)\\times D}$. Positional embeddings and speaker embeddings, both with dimensions $R^{(T/8)\\times D}$, are then added to the encoder outputs to incorporate temporal and speaker-specific information. As the outputs from each speaker are time-aligned, they can be concatenated to produce the final encoder output $R^{(T/8)\\times 2D}$. Each timestep in this final representation corresponds to a 160ms chunk of speech, containing encoded information from both speakers. We initialize the encoder us-ing a pre-trained HUBERT model (Nguyen et al., 2023), which was trained on 2000 hours of the Fisher dataset (Cieri et al., 2004).\nThe decoder plays a dual role as the Dialogue Manager and the Response Generator. It takes the streaming input $R^{(T/8)\\times 2D}$ and, at every 160ms interval, determines the next action of the chatbot, while also generating the content of its response if required. The decoder architecture consists of an embedding layer, eight transformer decoder layers, and a projection language model head. The embedding layer converts input token indices into embeddings with dimensions $R^{S\\times 2D}$, where S is the target sequence length and 2D is the hidden dimension. These embeddings are passed through the decoder layers, which extract contextual information and attend to the encoder outputs $R^{(T/8)\\times 2D}$ from both speakers. The final contextualized decoder hidden states, with dimensions $R^{S\\times 2D}$, are projected into $R^{S\\times V}$ by the language model head, where V is the vocabulary size. The vocabulary includes special tokens for representing the chatbot's next actions: padding (0), beginning of response [BOS] (1), end of response [EOS] (2), remain silent (3), continue speaking (4), start speaking (5), and stop speaking (6). The remaining token indices in the vocabulary represent the content of the chatbot's verbal response. More specifically, when the Dialogue Manager decides that the chatbot should initiate a speech, the first token generated by the decoder should be 5, followed by tokens that form the chatbot's verbal response."}, {"title": "Training data for RTTL-DG", "content": "Figure 4 illustrates example pairs of training samples used for training the RTTL-DG model. As mentioned earlier, for each interval (i.e., 160 ms), there is a training pair of (input, output). The input is the preceding truncated conversation from both speakers, which is then encoded using the encoder introduced in Section 3. The output is a sequence of tokens containing two types of information: the next action and the response content.\nThe first token in the output represents the next action (e.g., 3 indicates remaining silent, and 5 indicates initiating a response). To assign the next action label for each input, we rely on its last segment ($s_{last}$), which is the last 160ms chunk of the input. For each speaker, we first use voice activity detection (VAD) (Gao et al.) to identify speech segments ($S_E$) and silent segments ($S_I$). If $s_{last}$ falls entirely within a silent segment, the next action is labeled as remaining silent. If $s_{last}$ lies completely within a speech segment, the action is labeled as continuing to speak. When $s_{last}$ marks the beginning of a speech segment, the action is labeled as initiating speech. If $s_{last}$ occurs at the end of a speech segment and there is overlapping speech between the two speakers, the action is labeled as stop speaking.\nIf a response is initiated, the subsequent tokens in the output represent the response content. Instead of using text tokens for the target response, we use speech units extracted from a pre-trained speech quantization model (i.e., HUBERT+Kmeans) (Nguyen et al., 2023). This approach enables the generation of more natural and expressive speech, including representations of non-verbal units such as laughter, pauses, and hesitations. To reduce the target response length and increase the semantic richness of each token, we apply deduplication (collapsing consecutive identical tokens into one) followed by byte-pair encoding to merge adjacent tokens into larger units. This significant reduction in output length facilitates faster response generation, thereby lowering overall response latency.\nWe use the Switchboard-2 Phase II corpus (Godfrey et al., 1992) to generate the training dataset. This dataset consists of 4,472 five-minute telephone conversations between two speakers discussing various topics, such as jobs, hobbies, and movies. As shown in Table 1, the amount of data in Switchboard is relatively limited\u2014approximately 372 hours. This small dataset presents significant challenges for modeling open-domain response generation, which typically requires millions of training instances (Roller et al., 2020). To address this problem, we propose a two-stage approach: first, pre-training the RTTL-DG model on a synthetic dataset to learn the response generation task, and then fine-tuning it on Switchboard data to adapt to the next action prediction task.\nTo create the synthetic dataset, we leverage the abundance of text-based two-party conversations. Initially, we extract 91,796 five-turn conversations from several datasets, including BlendedSkillTask (Roller et al., 2020), ConvAI (Logacheva et al., 2018), TopicalChat (Gopalakrishnan et al., 2023), EmpatheticDialogues (Rashkin et al., 2018), and WizardOfWikipedia (Dinan et al., 2018). We then use GPT-40 to extend each conversation by generating an additional 35 turns. Finally, a text-to-speech model is used to convert these extended text-based conversations into speech-based conversations, resulting in 5,798 hours of synthetic data, with an average duration of four minutes per conversation. The text-to-speech model employed is a multi-speaker VITS2 model (Kong et al., 2023), trained using Switchboard data to ensure minimal differences between the synthetic and real datasets."}, {"title": "Experiment settings", "content": "This section describes the setups we use to build and evaluate the proposed real-time dialogue generation models. We also describe the baseline cascaded model used for comparison."}, {"title": "Evaluation metrics", "content": "There are two settings we use to evaluate the dialogue generation model: single-turn and multi-turn settings. In the single-turn setting, we provide the model with a spoken dialogue context and ask it to predict the next action and response. In the multi-turn one, we provide the model with a seeded conversation between two speakers and ask it to complete the conversation for the next 30 seconds.\nSingle-turn metrics. As mentioned earlier, at regular intervals (i.e., every 160 ms), the model determines the chatbot's next action and generates response content based on the most recent streaming input. For the next action recognition task, we evaluate the model using Precision, Recall, and F1-score metrics. These metrics assess how effectively the model predicts one of four possible actions: remain silent (SIL), initiate speaking (SPK), keep speaking (CONT), or stop speaking (STP). The predictions are compared against ground-truth labels derived from the Switchboard test set, providing a benchmark for measuring the model's performance in understanding and managing conversational flow. For the response generation task, we evaluate the model's performance in terms of semantic coherence and naturalness. Semantic coherence measures how logical and relevant the generated responses are within the dialogue context. To assess coherence, we provide GPT-40 with a dialogue context and a generated response, then prompt it to assign a coherence score on a scale from 1 (worst) to 10 (best). We report the average coherence score, along with the percentage of responses rated as sensible-those with a coherence score of 5 or higher. We use Whisper (Radford et al., 2022) to transcribe the spoken context and response into text before prompting GPT-40 for coherence assessment.\nTo evaluate the naturalness of the spoken responses, we use the following metrics:\n1. Pitch/Energy standard deviation (PSTD/ESTD). Measures variation in pitch and energy levels. Higher values indicate more dynamic and natural-sounding speech with tonal variation.\n2. Words per minute (WPM). Tracks the pace of speech. A natural speaking speed varies by context, and speaking too fast or too slowly can sound unnatural.\n3. Filler words per minute (FWPM). Measures the frequency of filler words like \"um\" or \"uh.\" While occasional fillers are normal in speech, excessive use can make the speaker sound hesitant.\n4. Repetitions per minute (RPM). Tracks repeated words or phrases. Some repetition is natural, but excessive repetition can make speech feel redundant or robotic.\n5. Laughs/breaths per minute (LPM/BPM). Measures the frequency of laughter and breaths in speech. Occasional laughter and breathing contribute to naturalness, but excessive amounts can seem forced.\n6. Silence per minute (SPM). Measures the average length of pauses. Short pauses are natural, but too many or too few can disrupt the speech flow.\nFor each response generated by the cascaded and RTTL-DG models, we calculate the above metrics and average them across the entire test set. We then compute the same metrics for the ground truth (i.e. human-generated responses). A more effective model, in terms of naturalness, should generate statistics that more closely align with the ground truth. We use PRAAT software to calculate pitch and energy standard deviation. We prompt GPT-40 to count the number of filler words and repetitions in the transcribed text of each spoken response. For laughter and breathing events, we use a publicly pre-trained model based on the AudioSet dataset (Hershey et al., 2017).\nMulti-turn metrics. To assess chatbot performance across multiple conversational turns, we analyze interactions that mimic realistic dialogue scenarios. While deploying chatbots to engage with human participants offers valuable insights, this approach can be resource-intensive and time-consuming. To address this challenge, we employ a self-chat evaluation method. Starting with a seeded dialogue context, we proceed a conversation between two chatbots (based on the same model) that interact with each other for a set duration (i.e., 30 seconds). The resulting conversations are analyzed to evaluate how well the model performs in terms of naturalness, assessing its ability to approximate human-like dialogues. This method enables scalable testing and generates diverse conversational data without extensive human intervention.\nBuilding on prior research (Nguyen et al., 2023), we evaluate conversational naturalness using metrics such as overlap frequency, back-channels, pauses, and average gap durations. Figure 5 illustrates examples of these metrics. We first apply VAD on the generated speech for each speaker. An Inter-Pausal Unit (IPU) is defined as a continuous segment of speech from one speaker. Successive IPUs separated by silences of less than 400 ms from the same speaker are grouped into a single turn. Within each turn, pauses are identified as silent segments lasting longer than 200 ms. Overlap refers to segments where both speakers are talking simultaneously. A specific type of overlap, called back-channeling, occurs when one speaker's short IPUs (less than 1 second) are entirely contained within the other speaker's turn, such as brief acknowledgments (\"mm-hmm,\" \"okay\"). Gaps are defined as the silent intervals between turns from two different speakers. Gaps are also interpreted as latency, representing the time it takes for the next speaker to respond after the current speaker completes their turn. For overlaps, back-channels, and pauses, we count their occurrences per minute of conversation. For gaps, we calculate the average duration in milliseconds. These metrics provide a comprehensive framework to quantify the conversational dynamics and naturalness of chatbot interactions."}, {"title": "Baselines", "content": "We use a traditional cascaded dialogue generation model for comparison. This model consists of three main components: (1) an ASR system, which transcribes spoken dialogue context into text; (2) a text-based response generation model, which produces a next response based on the transcribed context; and (3) a TTS model, which converts the generated text responses back into speech. For the ASR system, we employ the state-of-the-art Whisper-Medium model (Radford et al., 2022). The text-to-speech component uses the multi-speaker VITS2 model, which is the same model described in Section 4. The text-based response generation model is a transformer encoder-decoder, initially trained on synthetic conversational data and later fine-tuned on the Switchboard dataset. Details about the dataset are provided in Section 4.\nFor single-turn evaluation, we assess the cascaded model's performance on response generation using semantic coherence and naturalness scores. In multi-turn evaluation, the cascaded model generates self-chat conversations sequentially. This means that the second speaker begins responding only after the first speaker has completed their turn. To detect the end of a speaker's turn, we apply a simple heuristic rule based on a fixed silence threshold of 800 ms, following the approach described in (Lin et al., 2022). Specifically, if there is a silence of 800 ms after a speaker finishes, it is considered the end of their turn, allowing the second speaker to proceed with generating their response."}, {"title": "Training configuration", "content": "We used the Huggingface toolkit to train all the models. For the RTTL-RG model, the encoder was initialized with a pre-trained HuBERT model trained on the Fisher dataset (Nguyen et al., 2023), while the decoder was randomly initialized. The encoder and decoder were configured with 8 layers each. The hidden dimensions were set to 768 for the encoder and 1536 for the decoder. The same hyperparameters were applied to the cascaded text-based response generation model.\nPretraining the RTTL-RG model on the synthetic dataset took approximately three days, and fine-tuning it on the Switchboard dataset required an additional day. Both processes were carried out using a single NVIDIA H100 GPU."}, {"title": "Experiment results", "content": "We present a detailed analysis of the performance and behavior of our proposed dialogue models through comprehensive experiments. The evaluation encompasses several key aspects, including next action prediction, semantic coherence, and naturalness of generated responses and dialogues. Examples of the generated outputs are available in our repository\u00b2."}, {"title": "Next action prediction", "content": "Table 2 shows the results for next action prediction on Switchboard test set. The Dialogue Manager demonstrates excellent performance in predicting the \"Remain Silent\" and \"Keep Speaking\" actions, with F1-scores of 0.85 and 0.95, respectively. These results reflect the system's proficiency in managing conversational continuity. The \"Keep Speaking\" action exhibits the highest F1-score, indicating the system excels at maintaining speech continuity when appropriate. The high precision (0.94) and recall (0.96) suggest the model effectively identifies when to continue speaking without unnecessary interruptions. This task is relatively straightforward, as the system often decides to keep speaking based on the presence of ongoing self-generated speech in recent intervals. Challenges primarily arise in scenarios with prolonged overlapping speech, y novel ideas are underrepresented the system must decide between continuing or halting. However, such scenarios are relatively rare in natural conversations, mitigating their overall impact on performance. Similarly, the model performs well in predicting when to remain silent, achieving a strong recall of 0.89, indicating that it frequently identifies opportunities to withhold responses appropriately. However, its slightly lower precision (0.81) suggests occasional over-prediction of silence, where the chatbot remains silent even when a response might be warranted. This behavior could occasionally hinder conversational engagement.\nIn contrast, the system struggles with predicting the \"Initiate Speaking\" and \"Stop Speaking\" actions, which are critical for maintaining adaptive and timely conversational responses. Predicting when to begin speaking is the most challenging task, with a precision of 0.62 and a recall of 0.43. When observing the confusion matrix, we found that a significant number of \"Initiate Speaking\" instances were misclassified as \"Remain Silent,\" leading to missed opportunities for initiating responses. This issue can create awkward conversational silences or delays in the chatbot's contributions, negatively affecting conversational flow. Multiple factors contribute to this challenge. The limited number of training examples (only 112 instances per conversation) restricts the model's ability to learn from these cases. Furthermore, open-domain conversations lack definitive rules for when to begin speaking or provide backchannels, and individual differences in speaking styles further increase the variability and noise in ground-truth labels.\nModerate performance is observed for the \"Stop Speaking\" action, with a precision of 0.75 and a recall of 0.53. While the system has some capacity to recognize when to halt speech, particularly in response to interruptions or overlapping dialogue, errors are prevalent in distinguishing \"Stop Speaking\" from \"Keep Speaking.\" Such misclassifications highlight the challenges of managing interruptions effectively, which could lead to the chatbot either talking over the user or stopping unnecessarily, both of which disrupt conversational fluidity.\nA major factor contributing to the system's challenges is the significant imbalance in class distribution. Actions like \"Initiate Speaking\" and \"Stop Speaking\" are underrepresented compared to the dominant \"Remain Silent\" and \"Keep Speaking\" classes. This imbalance limits the model's ability to generalize effectively for the rarer actions, as evidenced by their lower performance metrics. Addressing this imbalance through techniques such as advanced data augmentation techniques, resampling, or the use of loss functions tailored for imbalanced datasets could improve model performance."}, {"title": "Semantic evaluation results", "content": "Table 3 shows the semantic evaluation results for response generation task. As can be seen, both the Cascaded and RTTL-DG models achieve moderate coherence and sensible response scores, with room for improvement. The coherence scores for the Cascaded model are 6.6 and 6.4 for the synthetic and Switchboard datasets, respectively, while the RTTL-DG model achieves lower scores of 5.2 and 4.8 for the same datasets. These scores are still relatively modest, which likely reflects the limited size of the training data\u2014both synthetic and real-which are far smaller in scale than the large datasets used for training recent state-of-the-art dialogue models. This suggests that larger, more diverse datasets could improve the performance of both models.\nIn terms of sensible responses, the Cascaded model consistently outperforms the RTTL-DG model, with scores of 69% and 69% on the synthetic and Switchboard datasets, respectively. The RTTL-DG model lags behind, with sensible response rates of 45% and 48% for the same datasets. The lower sensible response rates for RTTL-DG can be attributed to the complexity of the model, which generates speech units that include pauses, hesitations, and other dynamic speech features. This adds complexity to the generation process, which may impact coherence and the model's ability to produce contextually appropriate responses.\nInterestingly, both models perform better when trained and evaluated on synthetic data as opposed to real-world datasets like Switchboard. This is likely because synthetic data is cleaner, more structured, and better organized, allowing the models to more easily learn patterns and generate coherent responses. In contrast, real-world data is inherently noisier, more variable, and often more challenging for models to handle effectively, which can account for the lower performance on the Switchboard dataset.\nTo understand why the cascaded model outperforms the RTTL-DG model in terms of coherence, we conducted an ablation study on different types of target responses used to train the response generation model on the synthetic dataset, as shown in Table 4. When using the same type of target response, the cascaded model outperforms the RTTL-DG model for both text-based and unit-based responses. This suggests that using text as an intermediate representation enhances encoding performance, resulting in better coherence scores. However, it should be noted that this approach may increase latency, as an external streaming ASR model must be used as a frontend, often causing significant delays.\nWhen comparing different types of target re-"}, {"title": "Naturalness evaluation results", "content": "Tables 6 and Table 7 present the results of naturalness evaluations for dialogue generated by the cascaded model and the RTTL-DG model on real dataset. As shown in Table 6, RTTL-DG outperforms the Cascaded model across all naturalness metrics in the single-turn evaluation. Specifically, RTTL-DG generates more natural speech variations, with pitch and energy standard deviations closer to the ground truth. It also effectively replicates natural speech disfluencies such as repetitions, breathing, and laughter-features that the Cascaded model struggles to imitate. Additionally, RTTL-DG adjusts speech rate and pauses to better align with human speech, making its responses sound more natural and less robotic compared to the faster, more monotonous speech of the Cascaded model. These improvements are largely due to RTTL-DG's use of speech units as target responses, which enables more natural outputs.\nIn multi-turn evaluations, RTTL-DG also demonstrates superior performance compared to the cascaded system across all metrics, closely aligning with results observed on real data. The cascaded model, by design, produces alternating speech turns without overlaps, back-channels, or pauses, resulting in rigid, mechanical interactions. Furthermore, the cascaded system exhibits a significant average gap or latency of 800ms between turns. This delay can cause substantial user frustration and make the interaction feel unnatural when deployed in real-world scenarios. In contrast, the RTTL-DG model excels in generating overlaps, pauses, and back-channels often at a frequency even higher than that of real conversations. The average gap duration in RTTL-DG-generated dialogues is notably lower at 393ms, compared to 518ms in real data. This suggests that responses are generated quickly and fluidly. This efficiency arises because RTTL-DG can begin formulating responses before the other speaker has finished speaking. Additionally, RTTL-DG-generated gaps are more stable and exhibit lower variance compared to those in real conversa-"}, {"title": "Effectiveness of pretraining on synthetic data", "content": "We evaluated how pre-training on synthetic data impacts the overall performance for both the Cascaded and RTTL-DG models. As shown in Table 8, pre-training with synthetic data significantly improved the response generation task for the RTTL-DG model, increasing the coherence score from 4.8 to 5.2. Notably, this improvement in response generation also enhanced the next action prediction task, with the accuracy score rising from 83% to 85%. This suggests that understanding the conversation's semantics plays a crucial role in turn-taking decisions.\nSimilarly, a more substantial improvement was observed in the Cascaded model. This can be attributed to the fact that the Cascaded model uses text as the target response, which is the same for both the pre-training and fine-tuning stages. In contrast, the RTTL-DG model uses speech units as the target response, leading to some discrepancies between pre-training and fine-tuning. This is because the speech units in synthetic speech might not consistently align with the real speech units, as mentioned earlier.\nOverall, pre-training on synthetic data resulted in significant improvements for the RTTL-DG model, suggesting that expanding the training data or designing more effective synthetic data generation and augmentation techniques could lead to further advancements."}, {"title": "Related works", "content": "Current spoken dialogue systems, while advanced in many aspects, still fall short of achieving the naturalness characteristic of human conversation. Their responses tend to lack the fluidity and spontaneity of human speech, frequently sounding stilted and overly formal. As most spoken systems are built on a cascaded design consisting of three separate components\u2014ASR, DRG, and TTS-they have difficulty managing natural turn-taking, such as handling interruptions and interpreting non-verbal cues, which disrupts the flow of interactions. Several approaches have been proposed to enhance conversational fluidity, and effective turn-taking to improve the naturalness and user experience."}, {"title": "Turn-taking modeling", "content": "Turn-taking is a fundamental aspect of human conversation, enabling seamless and intuitive interactions. Nothdurft et al. (Nothdurft et al., 2014) emphasized the challenges involved in modeling turn-taking, exploring questions such as whether it is necessary, when it should be incorporated into ongoing interactions, and how it should be implemented. Traditional dialogue systems often rely on simplistic mechanisms such as fixed silence thresholds to determine turn shifts, which can result in awkward pauses, interruptions, or overly long responses that disrupt the conversational flow. Turn-taking modeling refers to the process of predicting and managing when each participant in a conversation should speak or respond. Instead of relying solely on silence indicators, modern approaches incorporate a variety of cues to enhance turn-taking accuracy. Lexico-syntactic cues involve analyzing the structure and choice of words to predict when a speaker is about to finish their turn (Gravano and Hirschberg, 2011). Schlangen et al. (Schlangen, 2006) identify word-final pitch and intensity levels, along with N-grams-based features, as potent predictors for turn-taking. Building on this foundation, Atterer et al. (Atterer et al., 2008) enhance the accuracy of classifying words as utterance-final or non-final. They highlight features, specifically word and part-of-speech n-grams, as the most important features. Prosody, namely, the rhythm and intonation of speech, provides crucial information about the speaker's emotional state and intent, helping the system to determine natural points for turn transitions. Erik et al. (Ekstedt and Skantze, 2022) train voice activity projection models using an extensive dataset of two-person conversations to capture prosodic features relevant to turn-taking. They also conduct various analyses to show when and how prosodic information is crucial for predicting turn-taking. Gaze direction and other non-verbal cues, such as facial expressions and gestures (Ishii et al., 2016), also play a significant role in human communication and are increasingly being used to inform turn-taking decisions in dialogue systems."}, {"title": "Back-channel generation", "content": "In human conversations, back-channel responses are crucial because they provide real-time feedback to the speaker, indicating that the listener is attentive, understanding, and emotionally engaged. Examples of back-channel cues include short phrases like \"mm-hmm,\" \"yeah,\" and other non-verbal sounds of acknowledgment. Without these signals, conversations can feel one-sided and mechanical, significantly diminishing the overall user experience.\nBackchannel (BC) prediction is the task of identifying appropriate moments for a listener to provide feedback and determining the type of feedback to be given. This involves two tasks: Backchannel Opportunity Prediction (BOP) and Backchannel Category Prediction (BCP). BOP identifies when a backchannel response is appropriate, while BCP classifies the type of response. Approaches have evolved from rule-based systems to advanced neu-ral networks. Early influential work by Ward et al. (Ward and Tsukahara, 2000) and Fujie et al. (Fujie et al., 2005) relied on acoustic features like pitch and pause length, with Mel Frequency Cepstral Coefficients (MFCCs) becoming standard measures. The integration of lexical features, initially through simple embeddings like Word2Vec, led to state-of-the-art performance (Ortega et al., 2020). Combining acoustic and lexical features, especially with pre-trained models like BERT, has further improved accuracy (Jang et al., 2021). Multi-task learning approaches (Hara et al., 2018; Ishii et al., 2021), which combine BC prediction with related tasks like emotion classification and turn-taking prediction, have highlighted the benefits of leveraging the multifaceted nature of dialogue for more accurate BC prediction."}, {"title": "Duplex conversation modeling", "content": "Duplex conversation modeling integrates turn-taking and back-channel generation, which aims to make the conversational experience as seamless as possible, allowing people to speak as they would with another person. For instance, Google Duplex integrates back-channel responses"}]}