{"title": "STRUCTURAL-ENTROPY-BASED SAMPLE SELECTION FOR EFFICIENT AND EFFECTIVE LEARNING", "authors": ["Tianchi Xie", "Jiangning Zhu", "Guozu Ma", "Minzhi Lin", "Wei Chen", "Weikai Yang", "Shixia Liu"], "abstract": "Sample selection improves the efficiency and effectiveness of machine learning models by providing informative and representative samples. Typically, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Most existing methods are based on local information, such as the training difficulty of samples, thereby overlooking global information, such as connectivity patterns. This oversight can result in suboptimal selection because global information is crucial for ensuring that the selected samples well represent the structural properties of the graph. To address this issue, we employ structural entropy to quantify global information and losslessly decompose it from the whole graph to individual nodes using the Shapley value. Based on the decomposition, we present Structural-Entropy-based sample Selection (SES), a method that integrates both global and local information to select informative and representative samples. SES begins by constructing a kNN-graph among samples based on their similarities. It then measures sample importance by combining structural entropy (global metric) with training difficulty (local metric). Finally, SES applies importance-biased blue noise sampling to select a set of diverse and representative samples. Comprehensive experiments on three learning scenarios - supervised learning, active learning, and continual learning - clearly demonstrate the effectiveness of our method.", "sections": [{"title": "INTRODUCTION", "content": "Data budgets that limit sample sizes are pervasive in machine learning applications. For example, researchers and practitioners often face limited annotation and computational resources, necessitating the use of fewer but more informative samples to enhance efficiency. Similarly, in continual learning scenarios (Hou et al., 2019), the memory constraint requires fewer but more representative samples from previous tasks to effectively retain knowledge. Consequently, effective sample selection becomes crucial to improving efficiency and effectiveness in machine learning. It aims to select informative and representative samples from large datasets to accelerate training and enhance the training performance. During selection, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Existing sample selection methods primarily focus on local information, such as training difficulty and node degree (Maharana et al., 2024). Although these methods demonstrate promising performance on many datasets, they overlook the global information inherent in the graph structure. This global information, such as connectivity patterns, captures the structural properties of the whole graph (Leskovec & Faloutsos, 2006) and has been shown to be effective in improving the representativeness of selected samples (Zhang et al., 2023; Yuan et al., 2020; Zhao et al., 2021). Therefore, we aim to incorporate global information into the sample selection process to improve the quality of the selected samples.\nThe key to incorporating global information is to identify which specific metric(s) can accurately capture the global structure of the sample graph. Li & Pan (2016) propose structural entropy to evaluate the amount of information required to describe a given graph structure. The main feature of this metric is that it is robust and sensitive. First, it remains stable against minor changes like the"}, {"title": "RELATED WORK", "content": "Existing sample selection methods primarily utilize local information. They can be classified into two categories based on the information utilized: attribute-based methods and connection-based methods.\nAttribute-based methods rely on the attributes of individual samples. A commonly used attribute is the training difficulty, which is typically assessed from two perspectives: confidence and error. Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2020) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L2 norm of the error vector, and the Forgetting score (Toneva et al., 2019), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications. Based on these metrics, several sample selection methods have been developed. One simple yet effective method is selecting the most difficult samples, as they have a larger impact on model performance (Paul et al., 2021). However, this method overlooks easy samples, which are crucial for model training when data budgets are limited (Sorscher et al., 2022). To address this issue, CCS (Zheng et al., 2022) divides the dataset into strata based on training difficulty and performs random sampling within each stratum. InfoBatch (Qin et al., 2023) retains some easy samples and enhances their influence by upscaling"}, {"title": "BACKGROUND: STRUCTURAL ENTROPY OF GRAPH", "content": "Structural entropy evaluates how the nodes and edges in a graph are hierarchically organized to form communities at different levels (Li & Pan, 2016). Thus, it is effective in globally quantifying the community structure of the graph regarding its overall connectivity patterns. The calculation of structural entropy is based on an encoding tree that represents the graph's hierarchical community structure. In this tree, each node represents a community, and each leaf node corresponds to a graph node that forms a single-node community. With this encoding tree, the entropy is aggregated across communities of different levels, which provides insights into the hierarchical community structure of the graph. The entropy is calculated for each non-root node a, which considers both intra-community and inter-community connections to reflect the connectivity patterns of its community in the graph. Lower entropy indicates denser intra-community connections and sparser inter-community connections. The intra-community connection, vol(a), is quantified by the total weighted degrees of nodes, while the inter-community connection, g(a), is quantified by the total weight of the edges with exactly one endpoint in the community (outer edges). Given an encoding tree T, the structural entropy of G is the aggregation of the entropy values from all its non-root nodes:\n$$H(G,T) = \\sum_{\\alpha \\in T} \\frac{g(\\alpha)}{vol(V)} \\log \\frac{vol(\\alpha^{-})}{vol(\\alpha)},$$\nwhere \u03b1 is a non-root node, \u03b1\u00af is its parent node, g(\u03b1) is the total weight of its outer edges, and vol(V), vol(\u03b1), vol(\u03b1\u00af) represent the total weighted degrees of the nodes in V, \u03b1, and \u03b1\u00af.\nIn real-world applications, the encoding tree T may be unknown. To best capture the hierarchical community structure in such cases, the encoding tree T is constructed by minimizing the structural entropy. Obtaining an exact solution for the minimization is challenging, so a greedy strategy similar to the Huffman tree construction has been developed (Zhu et al., 2023)."}, {"title": "STRUCTURAL-ENTROPY-BASED SAMPLE SELECTION", "content": "Our sample selection method integrates global and local metrics to select informative and representative samples. Given a large set of samples, an undirected, weighted sample graph G is initially constructed to model their similarity relationships. Each sample, represented by its embedding extracted by a deep neural network, corresponds to a node in the graph. To avoid excessive edge connections, each sample is connected to its k nearest neighboring samples. The edge weight between any two samples, u and v, is their cosine similarity normalized to [0, 1]. Based on this graph, we first propose a node-level structural entropy metric to globally measure the importance of each sample. Then, it is combined with a local metric, training difficulty, to assign an importance score to each sample. Using this score, we develop an importance-biased blue noise sampling method to select a set of informative and representative samples."}, {"title": "NODE-LEVEL STRUCTURAL ENTROPY", "content": "The core of our scoring method is to define the metric at the node level. While local metrics are well studied, global metrics have received little attention. An ideal global metric for fine-grained, node-level selection should measure the connectivity patterns of a graph at the individual node level. Previous research shows that the graph-level structural entropy effectively quantifies the global connectivity patterns (Li & Pan, 2016), making it a valuable metric for sample selection. However, it only provides a single value for the whole graph, thus failing to offer detailed insights at the node level. Consequently, the key is to decompose the graph-level structural entropy to the node level.\nChen & Teng (2017) have shown that Shapley value (Shapley, 1951) is an effective method to decompose a value from the graph level to the node level. The key feature of this method is its lossless and fair decomposition of the value, ensuring that the aggregate node-level value equals the graph-level value. Inspired by this, we employ the Shapley value to derive the node-level structural entropy. Specifically, the Shapley value of a node u reflects the average increase in structural entropy when it is added to all possible subgraphs of G. As a result, this value captures the node's contribution to the global connectivity patterns.\nTo derive the Shapley value for each node, we first calculate the structural entropy for each possible subgraph of G. Then, we calculate the node's contribution to these subgraphs. Formally, let Vs denote a subset of the node set V, the Shapley value of node u is:\n$$\\phi(\u0438) = \\frac{1}{|V|-1} \\sum_{V_s \\subset V\\{u\\}} {\\begin{pmatrix} |V|-1\\\\ |V_s| \\end{pmatrix}} (H(G[V_s \\cup \\{u\\}],T) - H(G[V_s], T)),$$\nwhere G[VS] is the subgraph of G that consists of nodes in Vs and the edges between them, and $\\begin{pmatrix} |V|-1\\\\ |V_s| \\end{pmatrix}$ is the binomial coefficient.\nDirectly calculating Eq. (2) requires an enumeration of all possible subgraphs of G, which becomes intractable for a graph with a large number of nodes. To address this, we reformulate the Shapley value by considering the contribution of edges.\nProposition 1. Let G = (V, E, W) be an undirected, weighted graph. The Shapley value of node u is\n$$\\phi(u) = \\frac{1}{vol(V)} \\frac{1}{\\begin{pmatrix} |V|\\\\ 2 \\end{pmatrix}} \\sum_{(u,v) \\in E} w_{u,v} \\log vol(\\alpha_{uv}) - d(u) \\log d(u),$$\nwhere wuv is the weight of edge (u, v), \u03b1uvv is the least common ancestor of node u and v in the encoding tree T, and d(u) is the weighted degree of node u."}, {"title": "IMPORTANCE-BIASED BLUE NOISE SAMPLING", "content": "To select a set of high-quality samples, the developed global metric, node-level structural entropy, needs to be combined with an appropriate local metric. Previous research has shown that training difficulty (St) is an effective local metric in quantifying the sample's impact on model performance (Paul et al., 2021; Sorscher et al., 2022). Based on this, we employ it as the local metric. Accordingly, the overall importance score (S) of a sample is a combination of node-level structural entropy and training difficulty:\n$$S(u) = S_e(u) \\cdot S_t(u).$$\nGiven the importance scores, a straightforward solution is to select the samples with the highest scores. However, this significantly reduces the diversity of the selected samples, as the important samples tend to cluster in several narrow regions (Zheng et al., 2022). An alternative is the message passing mechanism employed by D\u00b2 Pruning: once a sample is selected, this method sends weighted messages to decrease the importance scores of its neighbors in the graph. However, the message weights are sensitive to a hyperparameter and can lead to suboptimal results if not carefully tuned. Previous research has shown that blue noise sampling achieves a good balance between randomness and uniformity by excluding overly similar samples (Xiang et al., 2019; Liu et al., 2017). This method increases sampling in low-density regions, which enhances the diversity of the selected samples. Consequently, we develop an importance-biased blue noise sampling method to select a set of informative and representative samples.\nOur sampling process contains two steps: 1) identifying the candidate sample with the highest importance score, 2) rejecting the sample if its similarity with any selected neighboring samples exceeds a threshold \u03b8; otherwise, accepting it as a selected sample. These two steps are iteratively performed until no more samples can be selected. To determine the threshold \u03b8 for a given sampling rate, we perform a binary search on \u03b8."}, {"title": "EXPERIMENTS", "content": "In this section, we first demonstrate the effectiveness of our method in three learning scenarios: supervised learning, active learning, and continual learning. We then conduct ablation studies to provide insights into our method. Finally, we conduct a qualitative analysis of the selection results."}, {"title": "SUPERVISED LEARNING", "content": "In supervised learning tasks, including image classification, text classification, object detection, and visual question answering, we aim to reduce computational costs by selecting a subset of informative and representative samples for training."}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets and models. For image classification, we use the widely used datasets, CIFAR10, CI-FAR100 (Krizhevsky, 2009), and ImageNet-1K (Deng et al., 2009). Following Maharana et al. (2024), ResNet-18 (He et al., 2015) is used for CIFAR10 and CIFAR100, while ResNet-34 (He et al., 2015) is used for ImageNet-1K. The models are trained from scratch on the selected subsets of the training set, and we report the model accuracy.\nFor text classification, we use the ANLI dataset (Nie et al., 2020), which focuses on natural language inference, and the IMDB Review dataset (Maas et al., 2011), which focuses on sentiment analysis. Following Maharana et al. (2024), we fine-tune the RoBERTa model (Liu et al., 2019) and report the accuracy on the test set for both datasets.\nFor object detection, we use the PASCAL VOC dataset (Everingham et al., 2010), which contains bounding box annotations of objects and animals. Following Choi et al. (2021), we train SSD (Liu et al., 2016) with VGG-16 (Simonyan & Zisserman, 2015) backbone from scratch and report the mAP on the test set.\nFor visual question answering, we use the CC SBU Align dataset (Zhu et al., 2024), which contains high-quality, aligned image-text pairs. Following Wei et al. (2023), we fine-tune MiniGPT-4 (Zhu et al., 2024) on this dataset, and report the average accuracy of the model on five datasets: OKVQA (Schwenk et al., 2022), IconVQA (Lu et al., 2021), DocVQA (Mathew et al., 2021), GQA (Hudson & Manning, 2019), and ScienceQA (Saikh et al., 2022).\nPlease refer to Appendix B for more details on the dataset statistics and training hyperparameters.\nBaselines. We compare our method with the state-of-the-art sample selection methods, which are either applicable to all tasks or designed for a specific task. Baselines that are applicable to all tasks include: 1) Random selection of samples, 2) Moderate coreset (Xia et al., 2023), 3) CCS (Zheng et al., 2022), 4) D2 Pruning (Maharana et al., 2024), and 5) GraphCut (Iyer et al., 2021). For image classification and text classification, the task-specific baselines include selecting the most difficult samples based on: 1) Entropy (Coleman et al., 2020), 2) Forgetting score (Toneva et al., 2019), 3) EL2N (Paul et al., 2021), 4) AUM (Pleiss et al., 2020), and 5) Variance (Swayamdipta et al., 2020). For object detection, we include selection based on the AL-MDN uncertainty (Choi et al., 2021), which captures the detector's overall uncertainty for an image. For visual question answering, we include selection based on the Instruction score (Wei et al., 2023), which evaluates an image-text pair based on image-text matching degree and text length.\nImplementation. For all tasks, we extract image embeddings using CLIP (Radford et al., 2021) and text embeddings using Sentence-BERT (Reimers & Gurevych, 2019) due to their demonstrated performance. For visual question answering, we concatenate the image and text embeddings for each sample. To measure training difficulty, we use AUM for image classification, Variance for text classification, AL-MDN uncertainty for object detection, and Instruction score for visual question answering. We ablate the different training difficulty metrics in Sec. 5.4 and observe that there is no significant performance difference among them. We also perform a grid search on the hyperparameters, such as k in the kNN-graph construction (see Appendix C for details)."}, {"title": "RESULTS", "content": "To cover a wide range of sampling rates, we select subsets that contain 1%, 2%, 5%, 10%, 20%, 50%, and 70% of the entire training set. All the results are averaged over 5 random seeds. Table 1 shows the results on four datasets that cover all the four tasks. The full results are provided in Appendix D.\nBaselines that select the most difficult samples, such as AUM and Forgetting, perform well in high-rate settings. However, these methods fall behind in low-rate settings. This is due to their limited coverage of easy samples, which are crucial for model training when fewer samples are selected (Sorscher et al., 2022). Methods that prioritize sample coverage, such as CCS and D2 Pruning, address this issue and perform well in low-rate settings. However, in high-rate settings, they cannot accurately determine the most important samples that preserve global structure. This results in a lack of representativeness in the selected samples and suboptimal performance. In contrast, our method integrates both global and local metrics to better identify important samples and employs importance-biased blue noise"}, {"title": "ACTIVE LEARNING", "content": "Active learning (Settles, 2009) aims to reduce annotation effort by selecting a set of informative and representative samples from an unlabeled pool. These samples are then labeled to train models."}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets and models. To evaluate the effectiveness of the selection methods in an active learning task, we perform image classification on ImageNet-1K. To simulate an unlabeled pool, we remove the labels from all samples during selection. After selection, we use the ground-truth labels to simulate human annotations. We train ResNet-34 from scratch on these labeled images and report the accuracy on the ImageNet-1K validation set.\nBaselines. We include the baselines from Sec. 5.1 that are applicable to unlabeled datasets: 1) Random, 2) CCS, 3) D2 Pruning, and 4) GraphCut. Additionally, we include Prototypicality (Sorscher et al., 2022) designed for unlabeled datasets. This method selects the most difficult samples based on the prototypicality score, which is defined as the distance between samples and their corresponding k-means cluster center. Difficult samples are those far from the center, as they tend to be more ambiguous than the samples closer to the center.\nImplementation. In the active learning scenario, using a pretrained supervised model like CLIP for feature extraction is not suitable, because the domain of the unlabeled data may not be covered by its pretraining data. Therefore, we extract the image embeddings with a self-supervised model, SwAV (Caron et al., 2020). In the baselines and our method, the prototypicality score is utilized to measure training difficulty."}, {"title": "RESULTS", "content": "We select unlabeled samples with rates of 1%, 2%, 5%, 10%, 20%, 50%, and 70% and report the results averaged over 5 random seeds in Table 2. In low-rate settings, other baselines perform worse than random selection due to the absence of labels, indicating their reliance on labeled data to achieve optimal performance. In contrast, our method consistently performs better than random selection and other baseline methods. This is because structural entropy compensates for missing labels by capturing community structure in datasets."}, {"title": "CONTINUAL LEARNING", "content": "Continual learning (Kirkpatrick et al., 2017) aims to alleviate the catastrophic forgetting of previously learned tasks when learning new tasks over time. We focus on the replay-based method (Hou et al., 2019), which selects a small set of informative and representative samples from previous tasks and replays them during the training of new tasks."}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets and models. We use the datasets commonly used in continual learning, including Permuted MNIST, Split MNIST, Split CIFAR10, Split CIFAR100, and Split Tiny-ImageNet. Permuted MNIST splits MNIST (LeCun et al., 1998) into 10 segments, where a fixed permutation of the pixel order is applied to all images in each segment to simulate different distributions. Thus, it contains 10 classification tasks with samples from different distributions. The other four datasets split the image classes in MNIST, CIFAR10, CIFAR100, and Tiny-ImageNet (Le & Yang, 2015) into 5, 5, 20, and 20 segments, respectively, and each segment corresponds to a different classification task. In alignment with prior studies (Borsos et al., 2020; Hao et al., 2024), we use increasingly complex models as dataset complexity grows: a two-layer MLP for Permuted MNIST, a four-layer CNN for Split MNIST, ResNet-18 for Split CIFAR10, and ResNet-18 with multi-head output (Zenke et al., 2017) for Split CIFAR100 and Split Tiny-ImageNet. The models are trained sequentially on each task while maintaining a fixed-size replay memory that contains an equal number of samples for each previous task. During training, samples from both the replay memory and the current task are used, with the replay samples weighted by a hyperparameter that controls their influence on the current task. After completing each task, a subset of samples is selected from the current task to replace a portion of the replay memory. We report the average accuracy on all tasks.\nBaselines. We include all baselines from Sec. 5.1 to select samples that update the replay memory. Additionally, we include three state-of-the-art selection methods designed for continual learning: 1) iCaRL (Rebuffi et al., 2017) that selects samples whose average embedding closely approximates the average embedding of all samples, 2) Greedy Coreset (Borsos et al., 2020) that formulates the selection as a bilevel optimization problem and greedily selects samples such that the model trained on them minimizes the loss across the entire dataset, and 3) BCSR (Hao et al., 2024) that formulates the selection as a bilevel optimization problem on the probability simplex over samples and introduces a regularizer to control the number of selected samples.\nImplementation. Consistent with the implementation in Sec. 5.1, we extract the image embeddings with CLIP and measure training difficulty with AUM. For all baselines and our method, we perform a grid search on the weight of replay samples during training to determine its optimal value (see Appendix B for details)."}, {"title": "RESULTS", "content": "Following Borsos et al. (2020), we test replay memory sizes of 100 and 200 and report the results averaged over 5 random seeds in Table 3. Our method consistently achieves better performance than baselines across all memory sizes and datasets. This is because the combination of node-level structural entropy and training difficulty captures both the global structure of samples and the model training dynamics in retaining knowledge."}, {"title": "ABLATION STUDY", "content": "We conduct ablation studies on supervised learning using CIFAR10 to evaluate the impact and behavior of each module in our method.\nEffect of modules. We ablate the two key modules in our method, node-level structural entropy (SE) and importance-biased blue noise sampling (BNS), by replacing them with alternatives. Without SE, we score samples based solely on training difficulty (TD). Without BNS, we either select samples with the highest scores (HS) or use the message passing (MP) method from D\u00b2 pruning. The hyperparam-eter controlling message weights in MP is searched as in the original paper (Maharana et al., 2024).  Table 4 shows the results. Using TD and HS yields the lowest performance. Incorporating either SE, BNS, or MP largely improves performance, demonstrating their individual effectiveness. Combining SE with either BNS or MP further improves the performance, indicating that SE complements both methods in selecting informative and representative samples. Notably, BNS achieves comparable performance to MP without the need for tuning the additional hyperparameter, demonstrating its effectiveness.\nRobustness to different training difficulty metrics. We test the effect of using different training difficulty metrics, including Forgetting, EL2N, and AUM. We also include the identity baseline, which assigns identical scores to all samples. Table 5 shows the results. Using identical scores leads to inferior performance, indicating the importance of incorporating training difficulty. Meanwhile, using AUM, Forgetting, and EL2N yields comparable performance, with average accuracies across rates differing by no more than 0.5%. This demonstrates the robustness of our method to the choice of the training difficulty metric."}, {"title": "QUALITATIVE ANALYSIS", "content": "We visualize the selection results of different methods when selecting 2% of the samples from CIFAR10 by projecting them onto a two-dimensional plane using t-SNE (van der Maaten & Hinton, 2008).  Methods that select the most difficult samples, such as AUM, oversample near several class boundaries and undersample in several classes that are easier to classify. Methods that prioritize sample coverage, such as D2 Pruning, achieve a better sample coverage but still miss critical samples near class boundaries (Fig. 4(b)). This gap indicates that these methods may not effectively preserve the global structure of samples. Our method well covers the data distribution, providing a set of informative and representative samples for model training (Fig. 4(c))."}, {"title": "CONCLUSION", "content": "In this paper, we present a structural-entropy-based sample selection method for efficient and effective learning. The key idea behind our method is to decompose graph-level structural entropy to a node-level global metric using the Shapley value. This global metric is combined with a local metric, training difficulty, for selecting informative and representative samples. The effectiveness of our method is validated by comprehensive experiments on three learning scenarios. Although our method has proven effective, future work on the following aspects is still promising. First, automating the hyperparameter selection based on data characteristics can reduce the computational costs of the grid search. Second, improving the support for multimodal data could strengthen its performance across a wider range of tasks, such as infographics VQA (Mathew et al., 2022) and building foundation models (Yang et al., 2024)."}, {"title": "PROOF OF PROPOSITION 1", "content": "We first present two lemmas essential for the proof of Proposition 1.\nLemma 1. Let G = (V, E, W) be an undirected, weighted graph and T be its encoding tree. Then the structural entropy H(G, T) can be written as:\n$$H(G,T) = \\frac{1}{vol(V)} (2 \\sum_{(\u03ba,\u03c5) \\in E} Wu,v log vol (u vv) \u2013 \\sum_{UEV}d(u) log d(u)),$$\nwhere wuv is the weight of edge (u, v), u \u2228 v is the least common ancestor of node u and v in T, and d(u) is the weighted degree of node u."}, {"title": "DATASET STATISTICS AND DETAILED EXPERIMENTAL SETTING", "content": "Image classification. The CIFAR10 and CIFAR100 datasets each consist of 50, 000 images of 32\u00d732 pixels for the training set, with an additional 10, 000 images for testing. CIFAR10 includes 10 distinct classes, while CIFAR100 includes 100 classes. The ImageNet-1K dataset includes 1, 281, 167 images across 1,000 real-world classes for training, along with 50,000 images for validation. Following common practice (Maharana et al., 2024), we trained ResNet-18 for 200 epochs on CIFAR10 and CIFAR100, and ResNet-34 for 60 epochs on ImageNet-1K. The batch size is set to 64. We use an SGD optimizer with an initial learning rate of 0.1, momentum of 0.9, and weight decay of 0.0002. We use a cosine annealing learning rate scheduler with a minimum learning rate of 0.0001.\nText classification. The ANLI dataset is a natural language inference dataset created through multiple rounds of iterative human-and-model-in-the-loop adversarial procedures. We utilize the data from the final round, which consists of 100, 459 training samples and 1, 200 test samples. Following previous work (Maharana et al., 2024), we fine-tune the RoBERTa model for 10, 000 iterations with a batch size of 16. We use the SGD optimizer with an initial learning rate of 0.1, momentum of 0.9, and weight decay of 0.0005. We use a cosine annealing scheduler with a minimum learning rate of 0.0001.\nThe IMDB Review dataset contains 25, 000 movie reviews each in the training and test splits, with each review labeled by sentiment (positive/negative). Following previous work (Maharana et al., 2024), we randomly select 2, 000 samples from the original training set due to the excessive samples in it, and use the original test set for evaluation. We fine-tune the ROBERTa model for 500 iterations with a batch size of 16. The optimizer and scheduler settings are the same as that for the ANLI dataset.\nObject detection. We train the model using the combined trainval sets of PASCAL VOC 2007 and 2012, which include 16, 551 images and 40,058 objects across 20 categories. The model is evaluated on the PASCAL VOC 2007 test set, which comprises 4, 952 images and 12, 032 objects. We train SSD (Liu et al., 2016) with VGG-16 (Simonyan & Zisserman, 2015) backbone from scratch for 80 epochs with a batch size of 64. We use an SGD optimizer with a learning rate of 0.001, momentum of 0.9, and weight decay of 0.0005. The learning rate follows a linear warm-up strategy for the first 8 epochs and is then reduced by a factor of 10 at epochs 50 and 70.\nVisual question answering. The CC SBU Align dataset contains 3, 439 high-quality, aligned image-text pairs for the fine-tuning stage (stage 2) of Mini-GPT4 (Zhu et al., 2024). The visual question answering datasets used for validation test the model's abilities in various aspects, including logical reasoning, visual reasoning, knowledge retention, and abstract understanding. We use the same setting as the second-stage fine-tuning for Mini-GPT4 (Zhu et al., 2024). We fine-tune MiniGPT-4 for 400 iterations with a batch size of 12. We use an SGD optimizer with an initial learning rate of 0.00003, momentum of 0.9, and weight decay of 0.05. We use a cosine annealing scheduler with a minimum learning rate of 0.00001."}, {"title": "ACTIVE LEARNING", "content": "We use the same setting as training on ImageNet-1K in Sec. B.1."}, {"title": "CONTINUAL LEARNING", "content": "For continual learning, we follow Borsos et al. (2020) for experiments on Permuted MNIST, Split MNIST, and Split CIFAR10, and follow Hao et al. (2024) for experiments on Split CIFAR100 and Split Tiny-ImageNet. We use increasingly complex models as dataset complexity grows: a two-layer MLP for Permuted MNIST, a four-layer CNN for Split MNIST, ResNet-18 for Split CIFAR10, and ResNet-18 with multi-head output (Zenke et al., 2017) for Split CIFAR100 and Split Tiny-ImageNet. For each task, we first randomly select M samples from all available samples. Then, we train the model on these samples for E epochs with a batch size of B. During each training iteration, all replay samples are used with a weight of \u5165. The initial learning rate is set to lrt for the first task and decays by a factor of \u03b7 for each task. All hyperparameters, except \u5165, are fixed for all baselines and our method. The value of X is determined through a grid search over {0.01, 0.1, 1, 10, 100, 1000}."}, {"title": "SELECTION HYPERPARAMETER SETTINGS", "content": "We conducted a grid search to optimize three hyperparameters: the number of neighbors k for constructing the kNN graph, the cutoff ratio \u1e9e to remove the most difficult samples, and the imbalance factory to maintain the balance between different classes.\nSelecting an appropriate value of k is crucial as it can significantly affect the quality of the sample graph and thus affect the selection result. If k is too small, the graph becomes too sparse, making the selection sensitive to noise and outliers. If k is too large, the graph will be too dense and contain many edges connecting irrelevant neighbors, making it hard to identify the most important samples in the dataset. Thus, the choice of k must strike a balance to preserve meaningful structures without introducing noise or irrelevant information.\nInspired by Zheng et al. (2022), we also search the hard cutoff ratio \u1e9e that removes \u03b2 of the most difficult samples because they are usually outliers and contain noisy samples in the dataset. In addition, we also allow negative \u1e9e during the grid search, which indicates that we will remove 3 of the easiest samples to focus on difficult samples, which has been adopted by Sorscher et al. (2022).\nMaintaining a balanced distribution of samples from different classes is also beneficial for model training. A common strategy is to enforce strict class balance (Guo et al."}]}