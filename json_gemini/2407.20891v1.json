{"title": "Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian Neural Networks", "authors": ["Bao Gia Doan", "Afshar Shamsi", "Xiao-Yu Guo", "Arash Mohammadi", "Hamid Alinejad-Rokny", "Dino Sejdinovic", "Damith C. Ranasinghe", "Ehsan Abbasnejad"], "abstract": "Computational complexity of Bayesian learning is impeding its adoption in practical, large-scale tasks. Despite demonstrations of significant merits such as improved robustness and resilience to unseen or out-of-distribution inputs over their non-Bayesian counterparts, their practical use has faded to near insignificance. In this study, we introduce an innovative framework to mitigate the computational burden of Bayesian neural networks (BNNs). Our approach follows the principle of Bayesian techniques based on deep ensembles, but significantly reduces their cost via multiple low-rank perturbations of parameters arising from a pre-trained neural network. Both vanilla version of ensembles as well as more sophisticated schemes such as Bayesian learning with Stein Variational Gradient Descent (SVGD), previously deemed impractical for large models, can be seamlessly implemented within the proposed framework, called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a dramatic reduction in the number of trainable parameters required to approximate a Bayesian posterior; and ii) it not only maintains, but in some instances, surpasses the performance of conventional Bayesian learning methods and non-Bayesian baselines. Our results with large-scale tasks such as ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLAVA demonstrate the effectiveness and versatility of Bella in building highly scalable and practical Bayesian deep models for real-world applications.", "sections": [{"title": "1 Introduction", "content": "Bayesian deep learning [1] provides mechanisms for building more robust predictive models-i) robustness to adversarial attacks; ii) unseen or out-of-distribution data and iii) a theoretical framework for estimating model uncertainty [2, 3, 4, 5, 6]. In particular, quantifying uncertainty enables more reliable decision-making and facilitates identifying potential model vulnerabilities in uncertain regions of the input space. Consequently, embracing Bayesian deep neural networks (BNNs) represents a significant stride towards building more reliable and trustworthy AI systems for various real-world applications (e.g., autonomous driving, medical image analysis, etc.). Unfortunately, their practical use is encumbered by their computational complexity.\nUnlike traditional alternatives with point estimates\u2014a single set of model parameters mapping inputs to outputs- BNNs learn the distribution of model parameters to offer a distribution over possible predictions. Consider a neural network f(x, 0) with input x and weights @ and a prior distribution on network weights p(\u03b8). The likelihood function p(D|0) determines how well the network with weights @ fits the data D [7]. Bayesian inference integrates the likelihood and the prior using Bayes' theorem to derive a posterior distribution over the space of weights, given by p (0|D) = \\frac{p(D|0)p(0)}{p(D)} and seek to compute the predictive distribution"}, {"title": "2 Related Work", "content": "Increasing network complexity has led to advances in parameter-efficient fine tuning and recent advances in exploring these methods for Bayesian learning, predominantly for language models. We discuss these in the context of our work.\nParameter-Efficient Fine Tuning. In contrast to fine-tuning all parameters, recent research proposed inserting adapters in between existing neural layers to reduce the number of trainable parameters and, subsequently, the compute (GPU consumption) [14, 15, 16]. Hu et al. [17] use a bottleneck structure to impose a low-rank constraint on the weight updates, named LoRA. The key functional difference is that LoRA can be merged with the main weights during inference, thus avoiding the introduction of any latency whilst significantly reducing the number of parameters.\nFine-Tuning Approaches and Bayesian Deep Learning. Previous research investigating the application of fine-tuning approaches for BNNs have predominantly focused on large language models (LLMs), e.g. [18, 19, 20]. Notably, marking a departure from the conventional methods relying primarily on tuning the network's parameters, these studies chose to define priors and approximate posterior over low rank attention weights. Concurrently, Yang et al. [20] introduced the concept of Laplace LoRA to incorporate Bayesian concepts to enhance the calibration of fine-tuned LLMs. However, Laplace's method relies on a Gaussian approximation of the posterior distribution. Whilst this can be effective for unimodal and symmetric distributions, the approach does not fully encapsulate the intricacies of more complex posteriors, particularly in neural networks where the posterior has multimodality and asymmetry [21]. Deep ensembles [22] typically perform better in practice compared with variational and Laplace methods, due to their ability to capture multiple modes. When employing fine-tuning, a direct application of ensembling for LoRAs was considered in [23]. Some interpretations of deep ensembles suggest that they approximate gradient flows in function spaces and that building desirable properties into an ensemble (such as repulsive behavior), is possible [24]. SVGD [25], can be viewed in a similar vein. However, while these more sophisticated, repulsive, ensembling approaches are highly impractical in the pre-training phase, we argue that their expressivity can be brought to bear precisely in tandem with low-rank fine-tuning, which is the viewpoint we adopt in this contribution.\nBuilding upon these foundations, our research represents a pioneering effort to apply the principles of repulsive ensemble-based low-rank fine-tuning to computer vision. In particular, we bridge pre-training and fine-tuning phases with recent conjectures on mode connectivity [9]. Our methodology not only capitalizes on the efficiency of fine-tuning techniques, e.g. [26, 27, 28] but also innovatively addresses the scalability challenges inherent to BNNs. Overall, our work sets a new precedent in applying Bayesian approaches to computer vision tasks by offering a scalable and efficient framework for enhancing model performance."}, {"title": "3 Background on Stein Variational Gradient Descent (SVGD) Approximations for a Bayesian Posterior", "content": "Here, we provide: i) a brief overview of methods to address the intractability of the posterior distribution p(0|D) in Bayesian learning tasks; and ii) we elaborate on our motivation for seeking a practical method to learn BNNs for large-scale tasks with SVGD.\nBayesian inference techniques have been integral to the development of neural networks, with a rich history underscored by previous works [29, 1, 30, 31, 32]. These methodologies provide a structured framework for generating reliable and interpretable estimates of uncertainty. However, the focus of prior research on Bayesian neural networks has predominantly been on the initial pretraining stages rather than on subsequent fine-tuning processes [33, 34]. This emphasis on pretraining presents significant challenges in terms of scalability and resource requirements, limiting the application of Bayesian neural networks to smaller datasets and network architectures with a minimal number of parameter particles. Consequently, the scalability issues hinder the extension of Bayesian neural networks to larger datasets, such as ImageNet [35], and more complex network architectures, like CLIP [8], which involve millions of parameters.\nVariational Inference (VI) [36, 32] and Markov Chain Monte Carlo (MCMC) [29, 31] are two primary approximate Bayesian inference frameworks. The former substitutes the true posterior with a tractable alternative while the latter involves sampling. However, accurately computing the posterior with"}, {"title": "4 Bayesian Low-Rank Learning (Bella) for SVGD", "content": "The problem with the current SVGD approach in large deep neural networks is its huge computational cost. This renders it infeasible to train efficiently and to scale to a sufficient number of parameter particles for accurately approximating the posterior distribution, which currently remains coarse. In this work, we propose to capitalize on the low-rank representations of fine-tuning in order to construct a practical and scalable variant of SVGD. Consider any dense layer, for which there is a fixed pre-trained weight matrix \u03b80 \u2208 Rd1\u00d7d2 with d1, d2 the corresponding numbers of hidden units. We consider n low-rank perturbations of do as\n\u03b8\u2081 = 00 + \u0394\u03b8\u2081 = 00 + \u0392\u03af\u0391\u0390, i = 1,...,n.\nwhere B\u00bf \u2208 Rd1\u00d7r, Ai \u2208 Rr\u00d7d2 are the low-dimensional update parameters, and r < d1, d2 is the rank of the update. Now Bella proceeds as the joint SVGD on (A, B), with updates\nAi = \u0391 - \u0395\u2211(Ai), B = Bi - \u2211(\u0392) with\n(Bi) = k (BjAj, B\u00bfA\u2081)\u2207\u0432\u2081l (f(x; 00 + BiAi), y) \u2013 \u2207\u0432\u2081k (BjAj, BiAi),\n(Ai) = k (Bj Aj, BiAi)\u2207a; l (f(x; 00 + BiAi), y) \u2013 VA\u015fk (BjAj, BiAi) .\nHere, we have placed the (improper) uniform prior on (A\u017c, B\u00bf), but other choices are possible. Uniform prior implies that we only require the gradients of the log-likelihood computed on a mini-batch of data, as well as the kernel function to encourage particle diversity. Note that the kernel function on (A, B\u017c) is given by k (00 + BjAj, 00 + B\u00bfA\u017c), which ensures that the similarity is computed on the original parameter space, and in the commonly used case of shift-invariant kernels, this simplifies to k (BjAj, BiAi). Further simplifications are obtained for specific kernel functions in particular, in the case of Gaussian RBF, while the naive implementation would require the cost of O(rd1d2) for a single kernel evaluation, we can bring it down to O(r\u00b2 (d1 + d2)) using standard trace manipulation, as described in the appendix. This procedure can be repeated across all dense layers.\nBella introduces a significant improvement in the efficiency of model training and execution. By utilizing the same pre-trained weights 00 across all parameter particles but allowing for individual low-rank adaptations \u0394\u03b8\u017c, we achieve a balance between parameter sharing and the diversity necessary for effective learning. Bella significantly reduces the parameter space from the full matrix's d1d2 to just r(d1 + d2), thereby enhancing both efficiency and scalability. This setup not only reduces the computational burden during training but also streamlines the process at inference time. The heavy lifting is done once by loading the large base model 00, and the lightweight low-rank adapters \u2206\u03b8 can be dynamically applied with minimal overhead in order to approximate the posterior predictive distribution as p(y* | x*, D) \u2248 \\frac{1}{n} \u2211_{i=1}^{n} P(y* | x, 00 + 10). This approach is particularly advantageous in large-scale models, where the weight matrices 80 are of substantial dimensions."}, {"title": "5 Empirical Experiments and Results", "content": "In this section, we provide an in-depth overview of our experimental setup, detailing the methodology, equipment, and procedures employed in the implementation of our Bella. We meticulously outline the configuration settings, dataset, and the criteria used for evaluating outcomes, ensuring a transparent and reproducible framework. Following this, we present the experimental results, offering a thorough analysis of the data obtained, including statistical evaluations and interpretations of the findings in the context of our research objectives. This comprehensive approach ensures a clear understanding of the experiment's execution and its contributions to the field."}, {"title": "5.1 Experimental Set-up", "content": "Datasets. In this research, we have employed a variety of datasets, each selected for its relevance and contribution to the field of image recognition and computer vision. These datasets include CIFAR-10, CIFAR-100 [37], CIFAR-10-C [38], STL-10 [39], CAMELYON17 [40], ImageNet [35], and DomainNet [41]. We also employ VQA v2 dataset containing billions of questions answers given hundred thousands of images [42], which is utilized in Visual Question Answering (VQA) task. Detailed information about datasets is presented in Appendix H and Appendix F.\nNetworks. In our experiments, we employed the CLIP ViT-B/32 model [43], a pre-trained variant utilizing contrastive supervision from image-text pairs, as initially introduced in the seminal CLIP research [44]. We conducted end-to-end fine-tuning, adjusting all model parameters, a strategy typically yielding higher accuracy compared to training only the final linear layer. In our methodology, we selectively fine-tune only the image encoder of the CLIP model while maintaining the text encoder frozen as it is common [45]. To elaborate, both during training and inference phases, the final output is derived through an inner product between the output produced by the image encoder and the embedding generated by the frozen text encoder. For both our ensemble-based methods which include both traditional ensemble [22] and the Stein Variational Gradient Descent (SVGD) approach [25], we employed the technique of utilizing the logits (unnormalized outputs) of the models, as described in [46]. This strategy has been consistently applied in our ensemble configurations, encompassing traditional ensembles as well as SVGD. We also employ Ensemble networks as it was shown to be a good and efficient way to approximate the posterior distribution to be baselines [21]. For VQA task, we employ the SoTA LLaVA-1.5 7B with billions of parameters to showcase the effectiveness of our Bella on large-scale network architecture. Details regarding hyper-parameters are in Appendix H."}, {"title": "5.2 Benchmark Task Performance and Cost Efficiency", "content": "The performance comparison of our Bella models with their respective base models, as well as with Vision Bayesian Lora (VBL)\u2014a derived Laplacian sampling method from [20] for vision tasks-is delineated in Table 1. The respective costs comparison on training (in terms of trainable parameters) is shown in Table 2-notably, the grey columns are the ones used for generating the results in Table 1.\nImpressively, across a spectrum of benchmark computer vision datasets such as CIFAR-10, CIFAR-100, Camelyon17, and ImageNet, the Bella models demonstrate superior performance. This is achieved with only a fraction of cost (trainable parameters) as shown in Table 2, underscoring the models' proficiency in parameter efficiency without compromising on accuracy. Further, the Bella models surpass the performance of Single models, while employing a comparable level of computational resources-see the Memory Consumption of models used, in Table 2 reporting approximately"}, {"title": "5.3 Performance on Out-Of-Disrtibution (OOD) Datasets", "content": "Assessing the robustness of machine learning systems to unseen conditions is crucial, especially their ability to generalize to out-of-distribution (OOD) data. We evaluate robutness to ODD using multiple OOD benchmarks.\nFirst, we use the DomainNet dataset, one of the most diverse domain adaptation datasets, covering a wide range of visual styles from real images to abstract art. This variety provides a challenging test bed for algorithms aiming to bridge different visual domains. Our study involves training the CLIP network on DomainNet's \u2018Real' subset and testing its generalization across various domains (see Table 3). Second, CIFAR-10-C, a corrupted version of CIFAR-10, helps assess model generalization and robustness (see Table 4). Third, the STL-10 dataset, with significant label overlap with CIFAR-10, serves as a relevant OOD test case (see Appendix B).\nBella models, with significantly better efficiency, demonstrate competitive performance against more resource-intensive implementations with Ensemble and SVGD baselines across both DomainNet and"}, {"title": "5.4 Comparing Uncertainty Estimations", "content": "Bayesian models capable of providing a theoretical basis for measuring model uncertainty. Also known as epistemic uncertainty, refers to uncertainty stemming from limitations in our knowledge or understanding of the underlying data generating process or the model itself. One of the ways to quantify model uncertainty is through mutual information estimates, following [47].\nMutual Information (MI). This is the mutual information between the output prediction and the posterior over model parameters 0, and can be used as a measure of epistemic (model) uncertainty. It can be expressed as: MI(0, y | D, x) = H[p(y | D,x)] \u2013 Ep(0\\D) H[p(y | 8, x)] If the parameters at input are well defined (e.g., data seen during training), then we would gain little information from the obtaining label, or the MI measured will be low.\nWe employ MI to measure uncertainty to investigate whether the Bella approximations of the posterior leads to uncertainty estimates commensurate with those obtained from SVGD baselines. This provides empirical evidence of a functional equivalence of the Bella approximations of the posterior to that obtained from the current computationally intensive implementation of SVGD.\nDatasets. We utilize the CIFAR-10-C task, featuring corrupted images, to examine the uncertainty of model predictions trained on the standard CIFAR-10 dataset. Additionally, we assess the uncertainty measures on the CAMELYON17 dataset, which is characterized by inherent dataset shifts within itself.\nResults. Figure 2 demonstrates the effectiveness of our approach to estimate uncertainty. Our Bella perform similarly to the SVGD base model, with a slightly better uncertainty on misclassified images of CAMELYON17 and corrupted CIFAR-10-C datasets (under brightness corruption with the maximum intensity), see details and other corruption types in Appendix C."}, {"title": "5.5 Robustness against Adversarial Examples", "content": "In this section, we examine the resilience of our proposed Bella against adversarial attacks, specifically employing the L\u221e Fast Gradient Sign Method (FGSM) across various attack budgets as detailed in Figure 3. This analysis aims to benchmark the robustness of our method in comparison to traditional models under adversarial conditions. We employ the robustness benchmark [48] to deploy the attack on CIFAR-10 test set and report results in Figure 3.\nThe findings presented in Figure 3 reveal that conventional models such as SVGD and Ensemble exhibit just slightly greater resistance to adversarial attacks. We attribute this enhanced robustness"}, {"title": "5.6 Ablation Studies", "content": "This section undertakes a series of ablation studies to examine the effects of various components within Bella. Our analysis includes an exploration of the training costs associated with different ranks and their consequent influence on model performance. Given that Bella incorporates multiple parameter particles, we also delve into how varying the number of these particles affects Bella's efficacy. Additionally, we explore the application of low-rank adapters across different layers and assess their impact. Further details on other studies are in Appendix D. We show in Table 6 that we achieve state-of-the-art performance on CAMELYON17.\nAblations on rank r. As outlined in Section 4, we substitute the network's extensive full matrix with low-rank matrices, which are defined by the 'rank' parameter (r). This section aims to assess how this parameter influences Bella's performance.\nTo comprehensively demonstrate the rank's impact, we perform our analysis on the demanding large-scale CAMELYON17 dataset. The findings, illustrated in Figure 4a, shed light on how the number of ranks affects performance. Utilizing a small rank significantly reduces the parameter space, but this constriction limits Bella's ability to learn effectively and achieve optimal performance. Conversely, enhancing the rank to 4 markedly boosts Bella's efficiency. It's observed that performance plateaus at a rank of 16, indicating a saturation point.\nAblations on the number of particles n. In this section, we delve into the influence of the quantity of parameter particles on the performance of Bella. The findings, depicted in Figure 4b, reveal an improvement in Bella's performance with an increase in the number of parameter particles."}, {"title": "5.7 Generalization to a Visual Question Answer (VQA) Task", "content": "In this section, we extend the application of our Bella to another challenging vision task, Visual Question Answering (VQA), as detailed in [49]. We leverage the state-of-the-art, pre-trained, large multi-modal model LlaVA [13, 50] for this purpose. Utilizing LlaVA transforms VQA into a process where an image and a natural-language question are inputs, and the model generates a free-form, open-ended text answer. Answering questions in VQA requires various intelligent capabilities, including not only image recognition but also complex reasoning. For this task, we employ VQA v2 [49] dataset containing 204,721 images, more than 1 Billion (1B) questions and 10B ground-truth answers in total. There are three main types of answers: Yes/No, a Number, and Other.\nModel. In our experiments, we employed our proposed Bella on top of LlaVA-1.5-7B [13, 50]. Further details about the dataset, model and metrics are deferred to Appendix F.\nResults (Performance). The primary outcomes for Yes/No and Number answer queries are detailed in Table 5, respectively. We chose these question types to ensure a fair comparison and avoid semantic mismatches in open-ended answers in Others. A distinctive feature of our Bella is its reduced uncertainty estimates to accurate predictions, coupled with increased uncertainty estimates to incorrect ones. Moreover, it surpasses the Single base model in terms of Accuracy and Exact Match metrics.\nNotably, our method is efficient, particularly in contrast to the resource-intesive baseline Bayesian models tailored for this task (e.g. [51, 52, 6]). The computational requirements for using these base-lines render the application of full SVGD or ensemble alternatives impractical, thereby highlighting the impact on practical applications by harnessing the effectiveness of Bayesian models with Bella.\nResults (Model Uncertainty and Human Confidence). Further, we investigate the relationship between model uncertainty attained from Bella and human confidence, facilitated by multiple annotators in the VQA dataset. For this, we measure the correlation between the model disagreement by measuring the predictive entropy vs the human annotations of the answers."}, {"title": "6 Conclusion", "content": "In this paper, we introduce an innovative approach for creating an efficient Bayesian Neural Network (BNN) approximation using only a base pre-trained model. Our approach, called Bella, demonstrates remarkable compatibility with full-rank BNN approximations like SVGD or Ensemble methods, outperforming single-network solutions across a range of tasks, from pure classification to out-of-distribution (OOD) generalization and uncertainty quantification. Our research paves the way for effective BNN implementation, facilitating the development of reliable and robust machine-learning models capable of scaling to large networks and datasets. Bella reinforces the effectiveness of employing such a simple, efficient and effective training method with diverse representations over the conventional single fine-tuning techniques prevalent in the field."}, {"title": "H Detailed Parameters and Datasets", "content": "In this section, we mention in detail the description for each of the dataset utilized in our experiment as below:\n\u2022 CIFAR-10: This dataset comprises 60,000 32 \u00d7 32 color images divided into 10 distinct classes, with each class containing 6,000 images. It is partitioned into 50,000 training images and 10,000 test images. The classes cover a range of subjects from animals to vehicles, providing a fundamental challenge in image classification.\n\u2022 CIFAR-10-C: This dataset is an extension of the CIFAR-10 dataset, designed to evaluate the robustness of machine learning models against common image corruptions. It contains the same 60,000 images as CIFAR-10, however, the images in CIFAR-10-C have been systematically altered using a range of corruption techniques, including noise, blur, weather, and digital effects, resulting in 19 different corruption types each at 5 severity levels. This dataset is used to test the performance of models in recognizing objects under various real-world conditions, making it a valuable tool for improving the reliability and robustness of image recognition systems.\n\u2022 STL-10: This dataset is a benchmark for evaluating image recognition algorithms, featuring 13,000 color images. This dataset is divided into 5,000 training images and 8,000 test images, distributed across 10 different classes that include a variety of objects such as animals and vehicles. Each image in the dataset is 96 \u00d7 96 pixels, offering higher resolution than many similar datasets such as CIFAR-10. The STL-10 dataset is tailored for supervised learning tasks in image recognition, providing a structured framework for developing and testing algorithms' ability to classify images into predefined categories.\n\u2022 CIFAR-100: Similar to CIFAR-10, the CIFAR-100 dataset is composed of 100 classes, each with 600 images, offering a more detailed classification challenge compared to CIFAR-10.\n\u2022 CAMELYON17: The CAMELYON17 dataset is utilized in a domain generalization context, where the domains are represented by different hospitals. The primary objective is to develop models capable of generalizing to data from hospitals not included in the training set. Focusing on binary classification, the dataset comprises 96 \u00d7 96 histopathological images as input, with the task to identify the presence of tumor tissue in the central 32 \u00d7 32 region, indicated by a binary label.\n\u2022 ImageNet (ILSVRC2012): which is a subset of the ImageNet dataset specifically used for the ImageNet Large Scale Visual Recognition Challenge in 2012, contains over 1.2 million images distributed across 1,000 different classes.\n\u2022 DomainNet: DomainNet is one of the largest and most diverse datasets available for domain adaptation studies. It contains approximately 600, 000 images across 345 categories, spanning six distinct visual domains (Real, Clip-Art, Infograph, Paint, Sketch, Quick). This diversity in domains and categories enables the dataset to simulate real-world scenarios where models must adapt to different visual representations and styles."}]}