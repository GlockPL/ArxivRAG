{"title": "Precision Knowledge Editing: Enhancing Safety in Large Language Models", "authors": ["Xuying Li", "Zhuo Li", "Yuji Kosuga", "Yasuhiro Yoshida", "Victor Bian"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but they also pose risks related to the generation of toxic or harmful content. This work introduces Precision Knowledge Editing (PKE), an advanced technique that builds upon existing knowledge editing methods to more effectively identify and modify toxic parameter regions within LLMs. By leveraging neuron weight tracking and activation pathway tracing, PKE achieves finer granularity in toxic content management compared to previous methods like Detoxifying Instance Neuron Modification (DINM). Our experiments demonstrate that PKE significantly reduces the attack success rate (ASR) across various models, including Llama2-7b and Llama-3-8b-instruct, while maintaining overall model performance. Additionally, we also compared the performance of some closed-source models (gpt-4-0613 and Claude 3 Sonnet) in our experiments, and found that models adjusted using our method far outperformed the closed-source models in terms of safety. This research contributes to the ongoing efforts to make LLMs safer and more reliable for real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented capabilities in text generation, comprehension, and task completion. However, these powerful models also present significant challenges, particularly in managing the potential generation of toxic, biased, or harmful content. This issue has become a critical concern for researchers and practitioners alike, as the deployment of LLMs in real-world applications requires robust safety measures.\nExisting approaches to address this problem include knowledge editing techniques, which aim to modify specific pieces of information or correct undesirable behaviors in LLMs without retraining the entire model. One prominent method in this field is Detoxifying Instance Neuron Modification (DINM), which has shown promise in efficiently identifying and modifying toxic regions in LLMs through single test instances. However, our research has uncovered limitations in the DINM approach, particularly in its effectiveness across different model architectures.\nThe primary finding of our study indicates that the DINM method is ineffective for some models due to the small gap between harmful (hharm) and safe (hsafe) parameters. This\nineffectiveness may stem from prompt design issues, but more\nlikely from data-related problems during model training. For\ninstance, if the training data doesn't sufficiently distinguish"}, {"title": "II. PRELIMINARIES", "content": "Knowledge editing refers to the process of modifying specific pieces of information or correcting undesirable behaviors in large language models (LLMs) without retraining the entire model. This field has gained significant attention as LLMs, such as GPT-3 and its successors, are increasingly deployed in real-world applications. The ability to alter or remove toxic content, outdated information, or biased outputs is critical for maintaining the safety and reliability of these models.\nExisting techniques in knowledge editing include approaches like ROME (Rank-One Model Editing) and MEND (Modular Editing Networks with Gradients), which enable fast and localized adjustments to model parameters without sacrificing overall performance. However, most of these techniques focus on factual knowledge correction, leaving the challenge of toxic content largely unaddressed.\nDetoxifying Instance Neuron Modification (DINM) has\nemerged as a prominent solution to this problem by targeting\nspecific neurons responsible for toxic outputs. Despite its success, DINM's simplified approach often lacks the granularity\nrequired to handle more complex toxic content in various\ncontexts. Our work extends this line of research by propos-\ning Precision Knowledge Editing (PKE), a more nuanced\ntechnique that tracks neuron weight changes and activation\npathways to edit toxic regions at a finer level of granularity."}, {"title": "B. Challenges in Toxic Content Identification", "content": "Identifying toxic content within LLMs presents multiple\nchallenges, primarily due to the context-dependent nature\nof toxicity and the subtle variations in prompts that can\ntrigger harmful outputs. Unlike factual errors, where incorrect\nknowledge can be directly identified, toxic behavior may not\nbe immediately apparent until the model generates harmful or\nbiased outputs in response to specific queries.\nAnother major challenge lies in distinguishing between\n\"safe\" and \"harmful\" parameters within the model. In our\nresearch, we found that merely measuring the gap between\nharmful parameters ($h_{harm}$) and safe parameters ($h_{safe}$) is\ninsufficient for accurate identification. This is because the\nrelationship between model parameters and output toxicity is\noften non-linear and context-dependent."}, {"title": "III. METHOD", "content": "In this section, we describe the proposed Precision Knowledge Editing (PKE) method, which enhances the accuracy of knowledge editing by focusing on identifying and modifying toxic parameter regions within large language models (LLMs). PKE builds upon the existing Detoxifying Instance Neuron Modification (DINM) technique but introduces a series of improvements to achieve more precise and effective edits. The following subsections outline the mathematical formulations and the editing process in detail."}, {"title": "A. Mathematical Formulations", "content": "To effectively edit toxic parameters in LLMs, PKE intro-duces more advanced mathematical techniques to identify key\nregions for editing. These formulations allow PKE to track\nfine-grained changes within the model, enabling targeted edits.\n1) Neuron Weight Change: One of the primary metrics used\nin PKE is the change in neuron weights. Let the weight matrix\nof the l-th layer be $W_i$, the input vector be $x$, and the output\nvector be $y$. The goal is to track how the weight matrix evolves\nbefore and after applying PKE, which is computed as:\n$AW_i = ||W_{after} - W_{before} || F$\nwhere $|| || F$ represents the Frobenius norm. The Frobenius\nnorm is chosen for its ability to measure the overall magnitude\nof the weight changes, providing insight into how much the\nnetwork has adjusted its internal parameters during knowledge\nediting.\nMotivation: By calculating the change in weight across\nmultiple layers, we can identify neurons that have undergone\nsignificant modifications. These neurons are likely contributing\nto the generation of toxic content and are prime candidates for\nfurther edits.\n2) Activation Path Tracking: The gradient of the activation\npath represents the sensitivity of the model's output to changes\nin the weights of specific layers. The gradient of the l-th layer\nis computed as:\n$gi = \\frac{\\partial\\hat{y}}{\\partial W_i}$"}, {"title": null, "content": "where y is the predicted output. The change in the activation\npath can then be determined by the norm of this gradient:\n$\\Delta a_l = |\\frac{\\partial\\hat{y}}{\\partial W_l}|$\nMotivation: Tracking the gradient of the activation path\nhelps pinpoint the layers that have the most influence on\nthe model's toxic behavior. Layers with significant gradient\nchanges are potential \"hot spots\" where toxic behavior can\nbe mitigated with minimal impact on the model's overall\nperformance.\n3) Average Toxicity Across Multiple Instances: To ensure\nthe edits generalize across multiple test instances, PKE com-\nputes the average toxicity score across several samples. For\na set of test instances $x_i$, the model produces output $\\hat{y}_i$, and\nwe assign a toxicity score $T(\\hat{y}_i)$ to each output. The average\ntoxicity in the 1-th layer is then calculated as:\n$T_l = \\frac{1}{N}\\sum_{i=1}^{N}T(\\hat{y}_i)$"}, {"title": null, "content": "where N is the number of test instances, and $Yi y$ represents\nthe model's output after the l-th layer. This step ensures that\nthe knowledge editing process reduces toxic content across\ndiverse inputs, not just for individual cases.\nMotivation: By averaging the toxicity scores, we can avoid\noverfitting the edits to specific test cases. This formulation\nensures that the model's improvements are robust and apply\nto a broader range of inputs, thereby improving the model's\ngeneral safety.\n4) Local Region Identification: One of the central objec-\ntives of PKE is to identify local regions in the model that\ncontribute disproportionately to toxic content generation. This\nis done by computing the change in activation for individual\nneurons. The change in activation for the j-th neuron in the\nl-th layer is computed as:\n$\\Delta h_{l,j} = |h_{after} - h_{before} |$\nwhere $h_{l,j}$ is the activation of neuron j in layer l. The total\nactivation change for the layer is then computed as:\n$\\Delta H_l = \\sum_{j=1}^{m_l}\\Delta h_{l,j}$"}, {"title": null, "content": "where $m_l$ is the number of neurons in layer l. This allows\nPKE to focus on the most problematic neurons within a given\nlayer.\nMotivation: By isolating and adjusting these neurons, PKE\nensures that the edits are highly localized, reducing the risk\nof overcorrecting and compromising the model's overall per-\nformance.\n5) Custom Loss Function: The custom loss function used in\nPKE is designed to balance two objectives: reducing toxicity\nand maintaining the correctness of the model's outputs. The\ntotal loss is represented as:\n$L = \\alpha T(\\hat{y}) + \\beta C(\\hat{y})$"}, {"title": null, "content": "where T(y) represents the toxicity score of the model's output,\nand C(y) represents the correctness score. $\\alpha$ and $\\beta$ are weight\ncoefficients that determine the balance between the two goals.\nMotivation: This custom loss function ensures that as PKE\nreduces toxicity, it does not inadvertently degrade the overall\nperformance or correctness of the model."}, {"title": "B. Precision Knowledge Editing (PKE) Process", "content": "PKE follows a structured process to identify, target, and\nmodify the toxic parameters within an LLM. The process can\nbe summarized in the following steps:\nStep 1: Identify Key Layers. The first step involves iden-\ntifying the key layers responsible for toxic content generation\nby tracking the activation path. The layer l* with the highest\nactivation gradient is selected as the primary target:\n$l^* = arg max \\Delta g_l$\nThis ensures that the edits focus on the layers that have the\nmost significant influence on the model's outputs.\nStep 2: Pinpoint Critical Neurons. Once the key layer is\nidentified, PKE targets the neurons within that layer that have\nundergone the most significant weight changes. The neuron j*\nwith the largest weight change is selected for editing:\n$j^* = arg max h_{lj}$\nj\nThis step helps localize the edits to the most problematic\nneurons, minimizing the risk of affecting unrelated parts of\nthe model.\nStep 3: Apply Knowledge Editing. After selecting the crit-\nical neuron, PKE modifies the weight matrix for the selected\nneuron to reduce its contribution to toxic outputs. The weight\nupdate is applied using the following rule:\n$W^{new} = W^{old} - \\eta \\cdot \\Delta W_{l^*,j^*}$"}, {"title": null, "content": "where \u03b7 is the learning rate that controls the extent of the\nadjustment.\nMotivation: By applying the updates in a controlled manner,\nPKE ensures that the edits effectively reduce toxic outputs\nwithout introducing new errors or biases into the model.\nStep 4: Re-evaluate and Iterate. After each round of\nediting, the model is re-evaluated using the test set to ensure\nthat the toxicity levels have decreased without compromising\nperformance. If necessary, the process is repeated, further\nrefining the model until an optimal balance between safety\nand accuracy is achieved.\nMotivation: This iterative process allows PKE to contin-\nuously improve the model, ensuring that the final edited\nmodel exhibits reduced toxicity while maintaining overall\nperformance."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the performance of Precision Knowledge Editing (PKE), we conducted a comprehensive set of experiments comparing it against Detoxifying Instance Neuron Modification (DINM) and vanilla (unmodified) models. Our experimental setup was designed to assess both the effectiveness of toxic content reduction and the preservation of general model capabilities.\n1) Models: We tested our approach on three large language models:\n\u2022 Llama2-7b\n\u2022 Llama-3-8b-instruct\n\u2022 Mistral-7B-Instruct-v0.3\nThese models were chosen to represent a range of architectures and sizes, allowing us to evaluate the generalizability of our method.\n2) Evaluation Metrics: We employed the following metrics:\n\u2022 Attack Success Rate (ASR): Our primary safety metric, measuring the proportion of successful attacks against the model. A lower ASR indicates better defense against adversarial prompts.\n\u2022 AlpacaEval: Used to assess the models' general capabilities, particularly their ability to follow instructions and generate appropriate responses.\n\u2022 Winrate and ROUGE-L: Additional metrics to evaluate the quality and relevance of model outputs.\n3) Judgment Model: We used GPT-4-o as our judgment model to assess attack success rates and toxic content generation. This choice was made due to GPT-4's demonstrated ability to understand nuanced content and its strong performance in content moderation tasks.\n4) Dataset: We utilized the EPASS dataset, which includes a variety of attacking methods, both adaptive and static. This dataset was chosen for its comprehensive coverage of different attack types and its ability to challenge models' robustness across various contexts."}, {"title": "V. DATASET", "content": "Our dataset consists of various categories of vulnerabilities and attack types. These include areas such as API Vulnerability, Child Safety, and Data Poisoning. The distribution of these categories is shown in Figure 1."}, {"title": "A. Results and Analysis", "content": "Our experiments demonstrated that PKE effectively reduced\nthe attack success rate (ASR) across models without signifi-\ncantly impacting their general task performance, as measured\nby AlpacaEval. Key observations include:\n1) Significant ASR Reduction: For both Llama2-7b and\nLlama-3-8b-instruct, PKE achieved substantial reduc-\ntions in ASR compared to vanilla models and models\nwith system prompts. For Llama2-7b, PKE reduced\nthe baseline ASR from 67% to 2%, outperforming\nDINM (3%). For Llama-3-8b-instruct, the improvement\nwas even more dramatic, with PKE reducing the ASR\nfrom 97.60% to 8.5%, significantly better than DINM's\n87.60%.\n2) Adaptive Attack Resistance: PKE showed superior\nperformance against adaptive attacks, particularly for"}, {"title": null, "content": "Llama-3-8b-instruct, where it reduced the ASR from\n67.06% to 4%, compared to DINM's 66.5%. This\ndemonstrates PKE's robustness against more sophisti-\ncated attack methods.\n3) Preserved General Capabilities: The AlpacaEval re-\nsults show that PKE maintained the models' general task\nperformance, with only minor fluctuations in Winrate\nand ROUGE-L scores. For example, Llama2-7b's Win-\nrate slightly improved from 5.4% to 5.37% with PKE,\nwhile the ROUGE-L score remained stable.\n4) Consistency Across Models: PKE demonstrated consis-\ntent performance improvements across different model\narchitectures, suggesting its applicability to a wide range\nof LLMs.\nThese results highlight the effectiveness of PKE in ad-\ndressing the limitations of previous methods like DINM. By\nachieving finer granularity in toxic parameter identification\nand modification, PKE offers a more robust solution for\nenhancing the safety of LLMs without compromising their\noverall functionality."}, {"title": "VI. RELATED WORK", "content": "Knowledge editing in large language models (LLMs) has\nbecome an essential area of research, particularly for correct-\ning or removing toxic or harmful content without retraining\nmodels from scratch. One prominent method, Detoxifying\nInstance Neuron Modification (DINM), stands out by ef-\nficiently identifying and modifying toxic regions in LLMs\nthrough single test instances, thereby achieving significant\nreductions in harmful outputs [3, 6]. Unlike other methods\nthat suppress toxic activations (e.g., using SFT and DPO\napproaches), DINM permanently adjusts parameters related to\ntoxicity, making it highly effective against adversarial prompts\n[2, 1]."}, {"title": "B. Dynamic Knowledge Editing Techniques", "content": "Beyond detoxification, dynamic knowledge editing tech-\nniques, like MEND (Modular Editing Networks with Gra-\ndients) and ROME (Rank-One Model Editing), have been\nexplored for their efficiency in quickly altering model outputs\nin response to changes in factual knowledge. Both methods\nutilize gradient-based updates to focus on specific parameters\nin the model, enabling fast and accurate changes to targeted\nknowledge areas while minimizing side effects [11, 7]. These\nmethods are especially effective in tasks like factual correction\nor removing biases from models [10]."}, {"title": "C. LLM Model Attacks", "content": "Despite advancements in knowledge editing, LLM model\nattacks remain a critical challenge. Methods like jailbreak\nattacks and roleplay attacks manipulate LLMs into assuming\nunauthorized roles or permissions, often resulting in the gen-\neration of harmful or inappropriate content [14, 4]. Attention"}, {"title": null, "content": "shifting attacks restructure harmful queries into benign for-\nmats, making them harder to detect [15, 16], while reformat-\nting attacks break down malicious queries into components to\nbypass safety mechanisms [17, 18]. Additionally, automated\ngeneration of jailbreak prompts and gradient-based attacks\noptimize prompts using LLM responses, further demonstrating\nthe vulnerability of models to adversarial exploitation."}, {"title": "D. Multilingual and Multimodal Knowledge Editing", "content": "Recent efforts have also extended knowledge editing tech-\nniques to support multilingual and multimodal models, allow-\ning for more inclusive and scalable edits across a range of lan-\nguages and modalities. Techniques such as KEMu (Knowledge\nEditing for Multilingual Models) and MEME (Model Edit-\ning for Multimodal Environments) leverage diverse datasets\nto make LLMs safer across language barriers and various\ninput forms, like text and images [2, 9]. These advancements\nrepresent an important step in making knowledge editing more\nadaptable across different AI applications and global contexts\n[3, 18]."}, {"title": "E. Knowledge Unlearning", "content": "Parallel to knowledge editing, research on knowledge un-\nlearning has also gained momentum as a mechanism to\neliminate harmful or outdated knowledge from LLMs. Early\nmethods focused on brute-force approaches, leading to unsta-\nble training processes and decreased model utility. However,\nnewer methods, such as Negative Preference Optimization\n(NPO) and safe unlearning, have shown potential in selectively\nremoving toxic information while maintaining overall model\nperformance [6, 10]. These approaches address the need for\nmore stable and scalable unlearning processes, particularly\nin applications requiring continuous updates to the models'\nknowledge base [3, 8]."}, {"title": "VII. CONCLUSION", "content": "In this work, we introduced Precision Knowledge Editing\n(PKE), an advanced technique for addressing the limitations\nof existing knowledge editing methods in large language\nmodels (LLMs), particularly for managing toxic content. By\nleveraging more precise methods like tracking neuron weight\nchanges and tracing activation pathways, PKE enables more\naccurate identification and modification of toxic parameter\nregions within models.\nOur experiments demonstrated that PKE significantly out-\nperforms Detoxifying Instance Neuron Modification (DINM)\nin terms of reducing the attack success rate (ASR) across\na range of models, including Llama2-7b and Llama-3-8b-\ninstruct, while maintaining overall model performance as mea-\nsured by AlpacaEval. This demonstrates PKE's adaptability\nand robustness in handling adversarial prompts, a crucial\nfeature for ensuring the safe deployment of LLMs in real-\nworld applications."}, {"title": null, "content": "The key contributions of this work include:\n1) A novel approach to toxic parameter identification that\nachieves finer granularity than previous methods."}, {"title": null, "content": "2) A comprehensive evaluation framework that assesses\nboth safety improvements and preservation of general\nmodel capabilities.\n3) Empirical evidence of PKE's effectiveness across dif-\nferent model architectures and against various types of\nattacks.\nThese advancements have significant implications for the\nfield of AI safety, offering a more scalable and efficient solu-\ntion for knowledge editing in LLMs. By providing a method\nto selectively modify toxic regions without compromising\noverall model performance, PKE paves the way for safer and\nmore reliable deployment of large language models in diverse\napplications.\nFuture work could extend PKE to address other types of\nundesirable model behaviors beyond toxicity, such as bias or\nfactual inaccuracies. Additionally, exploring the application\nof PKE to multimodal and multilingual models could further\nbroaden its impact. As LLMs continue to evolve and become\nmore integrated into various aspects of technology and society,\ntechniques like PKE will play a crucial role in ensuring their\nresponsible and safe use."}, {"title": "VIII. LIMITATIONS", "content": "While our work presents significant advancements in the\nfield of knowledge editing for LLMs, it is important to\nacknowledge the following limitations:\n1) Limited to Prompt-based Attacks: Our approach\nprimarily addresses attacks that involve variations in\nprompting. As such, it is effective only against black-\nbox attacks, where the model's internal mechanics are\nnot directly manipulated. This limits the applicability\nof our method in more advanced or white-box attack\nscenarios where attackers might have deeper access to\nmodel structures.\n2) Focus on Open-source Models: The models we evalu-\nated are all open-source, meaning our findings might not\ngeneralize well to proprietary or closed-source models.\nDifferences in architecture, training data, or optimization\nstrategies could affect how effective Precision Knowl-\nedge Editing (PKE) is in other contexts.\n3) Data Limitations: The dataset used in this study is not\nexhaustive, and while it includes several attack methods,\nit may not encompass the full range of potential vulner-\nbilities in LLMs. There is a possibility that new attack\nmethods or toxic content patterns may arise, which could\nexpose weaknesses in our current approach.\n4) Exclusion of Other Model Architectures: Our work\nfocuses on a specific set of LLM architectures. We\ndid not extend our evaluation to alternative models like\nTTT, RWKV, or Mamba, which may present different\nchallenges in terms of toxic content identification and\nknowledge editing. Further research is needed to explore\nhow PKE performs across a broader range of model\ntypes.\nThese limitations present opportunities for future research\nand refinement of the PKE method, as well as the broader\nfield of knowledge editing in LLMs."}, {"title": "IX. SYSTEM PROMPTS", "content": "During our experiments, we employed specific system\nprompts for different models to enhance safety while ensuring\nthat the models remained helpful and unbiased. Below are the\nsystem prompts used for the Llama2-7B-Chat and Mistral-7B-\nInstruct-v0.2 models:"}, {"title": "A. Llama2-7B-Chat Safety System Prompt", "content": "You are a helpful, respectful, and honest assistant.\nAlways answer as helpfully as possible, while being\nsafe. Your answers should not include any harmful,\nunethical, racist, sexist, toxic, dangerous, or illegal\ncontent. Please ensure that your responses are so-\ncially unbiased and positive in nature. If a question\ndoes not make any sense, or is not factually coherent,\nexplain why instead of answering something incor-\nrect. If you don't know the answer to a question,\nplease don't share false information."}, {"title": "B. Mistral-7B-Instruct-v0.2 Safety System Prompt", "content": "Always assist with care, respect, and truth. Re-\nspond with utmost utility yet securely. Avoid harmful,\nunethical, prejudiced, or negative content. Ensure\nreplies promote fairness and positivity.\nThese system prompts were specifically designed to guide\nthe models in providing responses that are both useful and\nsecure, prioritizing user safety and content appropriateness."}]}