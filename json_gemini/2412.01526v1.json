{"title": "Addressing Data Leakage in HumanEval Using Combinatorial Test Design", "authors": ["Jeremy S. Bradbury", "Riddhi More"], "abstract": "The use of large language models (LLMs) is widespread across many domains, including Software Engineer-ing, where they have been used to automate tasks such as pro-gram generation and test classification. As LLM-based methods continue to evolve, it is important that we define clear and robust methods that fairly evaluate performance. Benchmarks are a common approach to assess LLMs with respect to their ability to solve problem-specific tasks as well as assess different versions of an LLM to solve tasks over time. For example, the HumanEval benchmark is composed of 164 hand-crafted tasks and has become an important tool in assessing LLM-based program generation. However, a major barrier to a fair evaluation of LLMs using benchmarks like HumanEval is data contamination resulting from data leakage of benchmark tasks and solutions into the training data set. This barrier is compounded by the black-box nature of LLM training data which makes it difficult to even know if data leakage has occurred. To address the data leakage problem, we propose a new benchmark construction method where a benchmark is composed of template tasks that can be instantiated into new concrete tasks using combinatorial test design. Concrete tasks for the same template task must be different enough that data leakage has minimal impact and similar enough that the tasks are interchangeable with respect to performance evaluation. To assess our benchmark construction method, we propose HumanEval_T, an alternative benchmark to HumanEval that was constructed using template tasks and combinatorial test design.", "sections": [{"title": "I. INTRODUCTION", "content": "The proliferation of LLMs to address Software Engineering problems and tasks has been well reported in the literature [1]\u2013[3]. One of the earliest problems to be addressed by LLMS was program generation where a natural language description is provided as a prompt to an LLM and a source code solution is generated. Other problems that have been addressed include test classification, clone detection and program repair.\nAs more LLM-based techniques are developed it is impor-tant to have data and methods to compare the efficacy of these techniques as well as to measure progress as LLMs are further trained and evolved. Benchmarking is a common method for evaluating the success of LLMs to solve specific problems or tasks [4]. Benchmarking has also been used successfully in software engineering to assess LLM-based techniques [5]. Example LLM benchmarks have been developed for code generation [6], [7], software modeling [8], cs concepts in education [9], and program repair [10].\nWhile benchmarks have been utilized successfully they are not without their criticisms and limitations. For example, LLM evaluations utilizing benchmarks may exhibit bias and unfair assessment which needs to be understood and measured [11], [12]. In addition to potential bias and fairness considera-tions, benchmarks also have limitations over their lifetime due to data leakage and data contamination [13]\u2013[15]. LLMs that have experienced data leakage and contamination dur-ing training will likely exhibit inflated benchmark evaluation scores which can misrepresent their ability to address the underlying Software Engineering task as well as misrepresent their performance when compared to other LLMs that have not experienced data leakage and contamination with the benchmark data.\nTo address data leakage and evaluation fairness, we propose a new benchmark construction method where a benchmark is composed of template tasks that can be instantiated into new concrete tasks using combinatorial test design [16], [17]. Concrete tasks for the same template task must be different enough that data leakage has minimal impact and similar enough that the tasks are interchangeable with respect to evaluation. To demonstrate our approach we have developed HumanEval_T, a variant of HumanEval, that was constructed using our proposed method. We have not yet developed a full HumanEval T alternative to all of HumanEval's 164 tasks instead we have randomly sampled 10 of the HumanEval tasks and created HumanEval_T alternatives for this subset of the benchmark. Using the random subsets of HumanEval_T and HumanEval we answer the following research questions:\n1) Is there evidence of data leakage when comparing Hu-manEval_T with HumanEval?\n2) Do concrete versions of the same template task in HumanEval_T produce similar results?"}, {"title": "II. BACKGROUND", "content": "Benchmarks serve as standardized evaluation tools that enable systematic comparison of different technologies and approaches that solve the same problem or task [18]. The emergence of Large Language Models (LLMs) has introduced new challenges in benchmark design and evaluation method-ology that can impact fairness.\nData leakage represents a significant threat to LLM bench-mark integrity. As documented by Sainz et al. [14], contam-ination occurs when benchmark data appears in an LLM's training set, leading to artificially inflated performance metrics and potentially invalid scientific conclusions. This issue is particularly acute given the black-box nature of many LLMs and the difficulty in determining training data composition. For example, let's consider the HumanEval [6] benchmark which is comprised of 164 hand-crafted programming tasks. HumanEval has become a standard benchmark for evaluating code generation capabilities, however, a recent study by Li and Flanigan [15] demonstrated the risks of data contamination. They found that task contamination significantly impacted model performance, with pre-training exposure to benchmark tasks resulting in misleadingly high scores."}, {"title": "B. Combinatorial Testing", "content": "In software testing, combinatorial test design enables ef-ficient coverage of parameter spaces while minimizing test case count [17]. Previous work leveraged combinatorial test design in the construction of a concurrency bug benchmark as a way to ensure diversity while minimizing the instances of benchmark tasks [19]. More recent work has extended the use of combinatorial test design into LLM evaluation, where combinatorial testing is used to generate diverse test cases while preserving semantic consistency [20]."}, {"title": "III. RELATED WORK", "content": "To address the challenge of data leakage and fairness in Software Engineering LLM benchmarks, a number of best practices have been identified regarding the construction and maintenance of benchmarks. With respect to construction, it is important to ensure benchmark tasks do not originate from sources that are part of any LLM training data sets.The two most common strategies for ensuring this separation of benchmark data and LLM training data are to (1) hand-craft benchmark tasks (e.g., HumanEval) or (2) select benchmark tasks from private data sets that are not accessible for LLM training. With respect to benchmark operation and mainte-nance, it is important that benchmark data be actively excluded from future LLM training data sets. In cases where these benchmark construction and maintenance best practices are followed, the risk of data leakage is low. However, with the prevelence of black-box training in commerical LLMs, it is impossible to know if the benchmark data has been leaked as it relies on trust in LLM developers to abide by the best practice of excluding benchmark data from training. Even when efforts are made to exclude benchmark data sources from training data, leakage can still occur due to third-party training sources that duplicate, summarize or even mention a benchmark's data. Another approach to address benchmark fairness is to create dynamic or evolving benchmarks using agents [21], [22]. The goal of these benchmarks is to evaluate emerging capabilities or evaluate multi-faceted capabilities by dynamically evolv-ing the benchmark. This evolution has the side effect that benchmark tasks are not identical over time and thus are less likely to be impacted by data contamination. However, evolv-ing data creates challenges when comparing LLMs against historic benchmark evaluations as LLMs are continuously evaluated against an evolving standard. Furthermore, removing old benchmark tasks or modifying them creates a blind spot to possible model regression where future LLMs maybe unable to effectively solve previously evaluated tasks."}, {"title": "IV. A BENCHMARK CONSTRUCTION APPROACH TO MITIGATE DATA LEAKAGE", "content": "Recent work demonstrates that data leakage significantly impacts LLM evaluation reliability [13], [14]. We propose a novel benchmark construction methodology based on template tasks and combinatorial testing to mitigate data leakage and provide a fair evaluation. Our approach involves three key phases: template task creation, combinatorial test design and benchmark variant assembly (see Fig. 1). To demonstrate our approach we will use HumanEval problem #1 as an example. This problem tests proximity comparison in numeric arrays. The original problem states\u00b9:"}, {"title": "A. Template Task Creation", "content": "Rather than static benchmark tasks, we define template tasks that capture fundamental task properties while abstract-ing some implementation details. For example, we transform HumanEval Problem #1 into the following template tasks:\nGiven a list of <input_type> and a\n<threshold_descriptor>, check if any\ntwo <value_descriptor> are closer than\nthe given <threshold_descriptor>.\nWhere:\n\u2022 <input_type>: numbers, float values, measurements\n\u2022 <threshold_descriptor>: threshold, minimum distance, tolerance\n\u2022 <value_descriptor>: values, elements, data points\nIn the above template task we first generalized the HumanEval problem description by replacing specific details with template variables that can systematically be replacing with the original values or alternative values. For example, in the above example the <input_type> template variable can be replaced with numbers, float values or measurements. It is important to ensure that template variables are not used to replace elements that impact the difficulty or purpose of the problem. In an effort to maintain semantic behavioural equivalence we have focused on varying input/output data types, variable naming and descriptions as well as problem context and framing."}, {"title": "B. Combinatorial Task Design", "content": "Once we have defined a set of template tasks, we employ combinatorial test design techniques [17] to generate a set of concrete task instances for each template task. The goal of this approach is to systematically create varied instances while maintaining semantic equivalence and comparable complexity across them.\nIf we consider the above template task for HumanEval problem #1 and use combinatorial test design, we can sys-tematically generate a set of concrete tasks that include:\n1) Given a list of numbers and a threshold,\ncheck if any two values are closer than\nthe given threshold\n2) Given a list of measurements and a minimum\ndistance, check if any two data points are\ncloser than given minimum distance\n3) Given a list of float values and a\ntolerance, check if any two elements are\ncloser than the given tolerance\nEach variant maintains the same computational objective while presenting the problem differently. Each concrete variant also includes a manually created set of test cases that validate the same underlying computational skills, helping detect whether performance differences stem from data leakage rather than genuine problem-solving ability [6]."}, {"title": "C. Benchmark Variant Assembly", "content": "Once we have produce a set of concrete tasks for each template task using combinatorial test design we then create concrete benchmark variants \u2013 each of which contains a unique concrete task for each template task that is not used in other benchmark variants. The concrete tasks are selected randomly to compose each benchmark variant. The use of different benchmark variants reduces the impact of direct memorization from training data with a goal of maintaining evaluation fairness across different LLM evaluations."}, {"title": "V. EXPERIMENTAL ASSESSMENT OF BENCHMARK TASK CONSTRUCTION", "content": "To assess the benefits of our combinatorial test design approach to benchmark construction, we conducted experi-ments using a 10 problem subset from HumanEval [6]. Each problem was converted into a template task with combinatorial parameter variations. We evaluated the HumanEval subset and HumanEval_T variant benchmarks using four state-of-the-art LLMs: GPT-3.5, GPT-40, Claude 3.5 Sonnet and Llama 3.1. The LLMs are listed in order of release date.\nFor each template task, we generated five concrete versions through combinatorial testing [20] and assemble the concrete versions into five variant benchmarks. We measure the code generation performance using pass@1 [6], where each model gets 1 attempts to generate a correct code that is validated against a test suite. In order to obtain a more fine-grained analysis we report back the percentage of tests that pass and not just if the generated code is correct or incorrect2."}, {"title": "B. Experimental Results", "content": "We present our findings with respect to our two research questions: evidence of data leakage in HumanEval and con-sistency across HumanEval_T template-task versions."}, {"title": "1) RQ1: Evidence of Data Leakage:", "content": "To investigate poten-tial data leakage in HumanEval, we compared model per-formance between the original HumanEval benchmark and our HumanEval_T variant benchmarks. Fig. 2 and Tab. I illustrate these results when comparing the average percentage of tests passed per problem. Our analysis indicates potential data leakage when comparing the results of the HumanEval_T variants with HumanEval.\nAll models show notably higher performance on Hu-manEval compared to the HumanEval_T variants:\n\u2022 GPT-3.5: 4.75% drop (80% \u2192 75.25%)\n\u2022 GPT-40: 6.76% drop (86.25% \u2192 79.49%)\n\u2022 Claude: 11.27% drop (97.5% \u2192 86.23%)\n\u2022 Llama 3.1: 13.75% drop (93.75% \u2192 79.75%)\nThe consistent performance drop across all models indicates likely data leakage in the original benchmark. In three of the four model evaluations, HumanEval's results are identified an outlier, meaning their variation is more extreme than the expected (see Fig. 3)."}, {"title": "2) RQ2: Consistency Across HumanEval_T Variants:", "content": "To evaluate whether our HumanEval_T variants produce consis-tent results, we analyzed the variation in performance as shown in Figure 3. Recall, that consistency across HumanEval_T variants indicates that the concrete tasks generated via combi-natorial test design are likely interchangeable as tasks across benchmark variants.\nAnalysis of performance across HumanEval_T variants re-veals interesting results. GPT-3.5 shows moderate consistency ($\\sigma$ = 7.6%) with variants performing a modest 4.75% below HumanEval. GPT-4 demonstrates varied performance ($\\sigma$ = 6.9%) across variants, with a notably higher 6.76% gap from HumanEval. Claude shows the highest variation ($\\sigma$ = 8.9%) in variant performance, with an 11.27% difference from HumanEval. Llama 3.1 exhibits consistent performance across variants ($\\sigma$ = 1.8%), yet shows the most significant drop of 13.75% from the HumanEval baseline, indicating a trend of increasing performance gaps in newer models."}, {"title": "C. Discussion", "content": "The experimental results while not conclusive reveal signif-icant insights:\n1) The systematic performance difference between Hu-manEval and HumanEval_T variants suggests static benchmarks have limitations and a relatively short pe-riod of usefulness for assessing models against historic benchmark results. While all models perform well on the original HumanEval tasks, their performance on the HumanEval_T variants, which were designed to be semantically comparable, varies consistently. This result aligns with recent findings about the evolution of model capabilities and the need for dynamic evaluation approaches [15].\n2) Our combinatorial test design approach to benchmark construction enables continuous benchmark evolution through semantically comparable task variations. While further analysis with a larger set of tasks is needed, we are optimistic that our approach can allow the creation of benchmark variants that preserve task complexity while facilitating fair model comparisons over time. If proven successful, this design supports longitudinal studies of model improvement while controlling for benchmark data leakage."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "Our combinatorial test design approach to benchmark con-struction offers a systematic method for creating benchmark variants with a goal of being robust to potential data leakage of earlier variants. Initial results from our evaluation of a subset of HumanEval tasks compared with five HumanEval_T benchmark variants demonstrates this method's potential for reducing the impact of data leakage in benchmark evaluation over time and in distinguishing between enhanced model capabilities versus performance improvements based on data leakage.\nFuture work needs to be done to fully evaluate our bench-mark construction approach, first in the context of HumanEval and later in the context of other static LLM benchmarks. In the short-term we will focus on expanding HumanEval_T to encompass the complete set of 164 HumanEval tasks, while further refining our template creation process to ensure semantic comparability across concrete task versions. We also plan to investigate metrics for assessing consistency in the difficulty of concrete tasks generated from the same template tak. In the long-term we are also interested in applying our benchmark construction methodology to other benchmarks in other software engineering domains such as program repair and clone detection."}]}