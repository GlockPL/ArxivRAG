{"title": "X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models", "authors": ["Zeyi Sun", "Ziyang Chu", "Pan Zhang", "Tong Wu", "Yuhang Zang", "Xiaoyi Dong", "Yuanjun Xiong", "Dahua Lin", "Jiaqi Wang"], "abstract": "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", "sections": [{"title": "1. Introduction", "content": "Extracting knowledge from a few examples and applying it to novel tasks at inference time has long been a major challenge in machine learning. With the significant success of large language models (LLMs) like GPT-3 [8] in purely NLP tasks, it has been demonstrated that even a few examples can lead to substantial performance improvements. Previous research has attempted to adapt this capability to computer vision tasks, achieving promising results in vision-only tasks [4, 22, 73, 74] with pure vision models. However, for tasks that require high-level semantics or text prompt control, like image editing and image personalization, it is important to achieve multi-modal in-context learning. With the success of multi-modal foundation models, the research focus has shifted to unified multi-modal in-context image generation.\n\nThe field of image generation is currently dominated by diffusion models [20, 28, 40, 55, 62] like SD [57], which typically rely on a text encoder alongside a diffusion network. This structure inherently complicates support for in-context learning, as it requires multi-image understanding and reasoning capabilities. Previous approaches [16, 60, 78] like Emu [64] and SEED-X [24] that bridge diffusion and LLMs often rely on predicted embeddings by LLMs, which introduce huge information loss of image conditions, limiting their abilities to preserve details in editing or low-level vision tasks. Recently, works like Chameleon [68], Transfusion [95] and concurrent works [75, 77, 79\u201381] have proposed approaches where an LLM directly predicts VQ-VAE [71] or VAE [33] features in an auto-regressive or diffusion manner. This approach reduces information loss during image compression, preserving more visual detail while integrating the LLM\u2019s reasoning capabilities. However, limited research has explored in-context image generation on these foundation models.\n\nThe primary challenge of in-context learning on these foundation models is the substantial context length required during training. To retain the information in an image, VQ-VAE or VAE image features require a large number of tokens\u2014typically around $(\\frac{1}{16})^2$ or $(\\frac{1}{32})^2$ of the total image pixels. A single image typically requires 1024-4096 tokens. In an image-to-image in-context task, at least four images are necessary for context, leading to a prohibitive training context length. This limitation, also noted by [80], restricts support to only three images during training, making direct in-context training impractical due to the excessive context"}, {"title": "2. Related Work", "content": "Large Vision-Language Models (LVLMs). The emergence of large language models (LLMs) [1, 9, 14, 29, 49, 70] have made remarkable breakthroughs. The domain of research has increasingly turned its attention toward Large Vision-Language Models (LVLMs). Previous advances in this field focus on the integration of vision understanding capabilities with LLMs [2, 17, 30, 37, 42, 48, 51, 52, 66, 82, 91]. Recent works start to focus on integrating vision generation abilities. One early line of these works [23, 24, 31, 64, 65, 78] compress visual features with LLMs into compressed embeddings and use diffusion decoder (like SDXL [50]) to generate visual contents, However, this suffers great information loss during LLM encoding process, leading to unsatisfying results in image editing tasks. Another line of work, pioneered by Chameleon [68], uses unified image tokens from VQ-VAE [19, 71] to unify perception and generation. In this work, we aim to fully unlock the potential of Chameleon for general image generation in a unified in-context learning paradigm for both seen and unseen tasks.\n\nAuto Regressive based Image Generation. While previous state of the art image generation dominant by diffusion models [10, 20, 27, 28, 39, 40, 53, 55, 57, 62, 84], recent works on auto-regressive for image generation have shown promising results [41, 44, 45, 63, 69, 75, 79, 92]. However, these models only shows experiments results on text-to-image generation [38, 63, 67, 77, 79, 79], with limited research on other types of image generation tasks or lack of quantitative results [64, 88]. In this work, we not only enhance text-image alignment but also extend our exploration to diverse image generation tasks, including image editing, controlled image generation, and perception tasks such as semantic segmentation and depth estimation. We demonstrate that auto-regressive models can achieve competitive results across these tasks in a unified framework.\n\nIn-Context Learning. GPT-3 [8] introduced the paradigm of in-context learning, where diverse NLP tasks are reformed as text completion tasks, enhanced through prompts containing embedded examples, which significantly boosts performance on related tasks. In-context learning has also been explored in the vision domain [4, 5, 73, 74], but these models are limited to vision-only tasks, lacking multi-modal versatility. Multi-modal models like Emu-1/2 [64, 65] demonstrate in-context learning capabilities; however, their reliance on embeddings predicted by LLM from image features and the integration of an external diffusion model restrict their effectiveness in image editing and dense prediction tasks. In contrast, our approach utilizes a unified early-fusion representation based on Chameleon [68], enabling a single model to generalize across a broader array of tasks with improved performance and enhanced generalizability."}, {"title": "3. Method", "content": "3.1. In-Context Example Compression\n\nWe introduce a context-aware compression mechanism to better model in-context examples within Chameleon. As illustrated in Fig. 2. This mechanism defines three types of"}, {"title": "3.2. Task Augmentation Pipeline", "content": "As Chameleon [68] possess both text and image generation ability, we also adopt unified training on interleaved text and image generation. As illustrated in Fig. 3, to further augments the training data, We construct a data generation pipeline. Each generation task is converted into a text prediction task that focuses on describing the relationship between the input and output images. This text prediction task requires the model to interpret and articulate the relational changes between the images. Advanced vision-language models, such as GPT-4V [48] or QwenVL-2 [72], are employed for this purpose, as they can generate and interpret open-ended relational descriptions. The detailed prompts are available in Appendix C.1. Through training the model to describe differences by generating text tokens, we equip it with a deeper understanding of the relationship of input and output images, which enhances its generalization ability and improves the performance of image generation.\n\nAdditionally, we introduce a task-reversion augmentation. For each task, such as \u201cderaining\u201d (removing rain from an image), we introduce a reverse task\u2014\u201cadding rain\"-by swapping the input and output. This strategy effectively doubles the task variety, enabling the model to learn transformations in both directions and deepening its comprehension of the underlying transformation processes.\""}, {"title": "3.3. Retrieval-Augmented Image Editing", "content": "Following the spirit of Retrieval-Augmented Generation (RAG) [34]. We introduce Retrieval-Augmented Image Editing (RAIE) to enhances image editing by retrieving the relevant examples from a database. Given an input image $I_{input}$ and an instruction $Instr_{current}$, RAIE searches for the most similar instruction $Instr_{retrieved}$ and corresponding image editing pair $P_{retrieved}$ in the database. The retrieval process is defined as:\n\n$(Instr_{retrieved}, P_{retrieved}) = \\underset{(Instr, P) \\in D}{\\operatorname{argmin}} dist(Instr, Instr_{current}).$\n\nwhere we use the cosine similarity of CLIP [54] text features as the distance function dist. This retrieved example serves as in-context guidance for the model, which then"}, {"title": "4. Experiments", "content": "Though we train a unified model across different tasks, we report the data preparation process separately in each subsection for clarity. The complete training dataset comprises approximately 5 million data pairs, expanding to around 8 million pairs after task reversion and text prediction task augmentation (detailed in Appendix B). For all tasks that take an image with text as input and produce an image with text as output, we include an in-context example of the same task type. We set the batch size to 1024 and the context window size to 5120. The learning rate is set to 1e-4 with a cosine learning rate scheduler. Training is conducted on 128 NVIDIA A100-80G GPUs over 20,000 steps."}, {"title": "4.1. Text-to-Image Generation", "content": "Settings. To enhance Chameleon's [68] text-to-image generation capabilities, we utilize QWen-VL2 [72] to rewrite dense descriptive captions for 500K high-quality images filtered from the LAION-Aesthetic [59] dataset (detailed in Appendix C.4), selecting only images with an aesthetic score greater than 6. For evaluation, we adopt the GenEval [26] benchmark.\n\nResults. As shown in Tab. 1, we significantly enhance Chameleon's original text-to-image generation capabilities. Leveraging the image dense description task, our model further achieves competitive results compared to other auto-regressive models for text-to-image generation. This experiment highlights the effectiveness of unifying dense image description and image generation tasks, resulting in notable improvements, particularly in tests involving complex multi-object and color attributes. The ability to generate images that accurately follow text prompts is essential for our downstream applications like image-editing. Qualitative visualization are available in Appendix A."}, {"title": "4.2. Image Dense Prediction", "content": "Settings. We use representative datasets for dense prediction tasks: NYU-v2 [61] for depth and surface normal estimation, ADE-20K [94] for semantic segmentation, and Rain-13K [21], LOL [76], and GoPro [47] for corresponding low-level vision tasks. Full training data details are available in the Appendix B.\n\nResults. We select typical specialist model and previous vision generalist for comparison. Results are shown in Tab. 2, where our model can achieve competitive results on dense prediction task and low level vision task. Our model is the first to deliver promising results using a unified, discrete token approach. The slight performance gap on low level vision task compared to continuous feature prediction models is due to the inherent information loss in the VQ-VAE discretization process, as Chameleon [68] adopt 16x compression rate. However, by discretizing images and adopting a next-token prediction approach akin to that used in large language models, our method offers promising scalability for future advancements."}, {"title": "4.3. Image Editing with RAIE", "content": "Settings. As we introduced Retrieval-Augmented Image Editing (RAIE) in Sec. 3.3, we also prepare training data in the same way. We use publicly available UltraEdit [93] (500K) and MagicBrush [89] (8K) For the training. We use CLIP-B/32 [54] text encoder to encode the the edit instruction for each training sample and retrieval the most similar instruction feature as its neighbor sample (excluding the sample itself). During training, we input the neighboring sample as a task prompt context and prompt the model to predict the edited image. Besides generation task, we also use QWen-VL2 [72] to describe the differences between the input image and the edited image. We add these difference description tasks into the training to help model gain better understanding of images and their variations.\n\nPreparing each editing pair with a similar editing pair for in-context learning is crucial to the success of RAIE, as we frequently observe similar editing pairs in both the Ultra-Edit and MagicBrush datasets. We provide detailed analysis of this in Appendix D."}, {"title": "4.4. In-Context Learning on Novel Tasks", "content": "Settings. Following the approach of GPT-3 [46], this experiment primarily investigates the generalizability of our model on novel tasks, given only a single example as context. We choose Low light enhancement and Image derain from low-level vision, object addition and object removal from Image Editing as novel task to perform this study. During training, we remove the training data for each novel tasks. For Image derain, we remove the training of generating derained image of Rain-13K. For low light enhancement, we remove the training of generating enhanced image of Mit-5K [11] and LOL [76]. For image editing task. we filtered out the sample in Ultra-Edit [93] and MagicBrush [89] using LLama-3-Instruct-7B [18] by querying whether the instruction involves object addition or removal. Our test is perform on Rain100H [85] (Image derain), LOL-val (low light enhensement) and manually filtered 100 editing samples of Object Addition/ Removal task from publicly available instructP2P [7] dataset. For depth estimation we test a new color palette that match the distance of the pixel on image to a different color spectrum. For low level vision, we report PSNR and SSIM as image evaluation metric. For Image Editing, we report CLIP [54] score consistent with [89, 93]. For depth estimation task, we report the RMSE on the previous unseen color spectrum.\n\nResults. We report the quantitative results in Tab. 4. \"Full training\" refers to the model fully trained to corresponding training set. Both \"In-context\" and \"No In-context\" settings"}, {"title": "4.5. Other In-context Form", "content": "Settings. In addition to training our model on existing datasets, we also create two small datasets for style personalization and action preservation to demonstrate X-Prompt's ability to extract diverse contextual information. For style-personalization, we use RB-Modulation [58] to generate image pairs based on style image and further filter low quality data with QWen-VL2 [72] (detailed in Appendix C.2). For action preservation, we generate diversified human actions and use pose estimation model and ControlNet [90] to generate different person doing same action in the same pose. For each task, we generate 10K pairs for in-context learning. For style personalization, we give model an example transformation pair to prompt model perform similar transformation on an unseen image. For action and pose preservation, we give model two image of a person doing same action in similar pose and a new person description and prompt model to generate a new image.\n\nResults. We show qualitative results in Fig. 6. X-Prompt can extract both the high level semantics and low level details of the context example and perform successful transformation on new image or generation based on text prompt. This experiments demonstrate that X-Prompt can achieve diversified in-context form in arbitrary multi-modal tasks."}, {"title": "5. Discussion", "content": "In this work, we propose empowering the autoregressive foundation model Chameleon [68] to achieve unified image generation through in-context learning. We demonstrate its promising performance across tasks such as text-to-image generation, dense prediction, low-level vision, and image editing, and showcase its generalizability to previously unseen tasks when provided with in-context examples. We hope this work will pave way for this promising direction to achieve the \"GPT-3 moment\" in the unified multi-modal field in image generation.\n\nWhile promising, our work still faces several unresolved challenges. First, the VQ-VAE in our base model, Chameleon [68], introduces substantial information loss in image reconstruction at a compression rate of 16, which is a primary reason for the model's reduced performance on certain low-level vision tasks that demand high-quality image reconstruction. Second, X-Prompt achieves success-"}, {"title": "Supplementary Material", "content": "A. Qualitative Results of Text-to-Image Generation.\n\nWe visualize some text-to-image generation results of our model in Fig. 7 and comparison with other models in Fig. 10. Fig. 7 demonstrates that our model can generate images with high aesthetic qualities after training on filtered high qaulity data from Laion-Aesthetics [59]. Figure 10 clearly demonstrates that the incorporation image dense description task has significantly bolstered the model's proficiency in accurately following text prompt when compared to other models such as Emu3 [75] and Janus [77].\n\nB. Details of training data\n\nFull training data statistics are reported in Tab. 6 and Tab. 7. For each of the task in Tab. 6, we use QWenVL-2 [72] to describe the transformation between the input and output images and augments with reversed task.\n\nC. Details of Prompt template\n\nC.1. Difference description task.\n\nFor each image editing pair in Ultra-Edit [93] and MagicBrush [89], we leverage QWenVL2 [72] to describe the difference between images using the following prompt: \"Describe the differeces between two images. Use 'input image' describe the first image and 'output image' to describe the second image, describe what subtask it belongs to, choosing from [Style Transfer, Object Removal, Object Replacement, Object Addition, Object Modification, Season/ Time Change, OTHER_SUBTASK]\". We also ask QWenVL2 to label a reverse editing prompt for data augmentation.\n\nC.2. filtering data generated by RB-Modulation.\n\nTo generate and filter high-quality data, we first use FLUX to generate high-quality and stylized images based on the prompt templates in Tab. 8. However, RB-Modulation [58] occasionally performs correct style transformations but sometimes fails. To ensure quality, we further use QWen-VL2 [72] for data filtering. Due to QWen-VL2's current"}, {"title": "C.3. filtering data generated by IP-Adapter.", "content": "IP-Adapter [87] can perform layout and semantic combination on two provided images, However, the final output image can maintain different attributes (layout, semantics, texture, details) from different images in a unified but not entirely deterministic format. Given the complex attributes relationship between the input images and the output images, we employ GPT-40 to analyze and annotate these relationships. As shown in Fig. 9, GPT-40 provides high-quality, detailed descriptions of the relationships between different images. For this purpose, we annotate a dataset of 50K image pairs.\n\nC.4. Caption Rewriting on Laion-aesthetic.\n\nWe filter high-quality data from Laion-Aesthetic [59], selecting images with an aesthetic score greater than 6. For dense caption rewriting, we use QWen-VL2 [72], focusing on the relative positions, colors, and numbers of objects. To preserve caption diversity, we retain 10% of the original captions during training.\n\nD. Retrieval-Augmented Image Editing\n\nClustering similar editing pairs during training is critical to the success of Retrieval-Augmented Image Editing (RAIE) as a form of in-context learning. Fortunately, we observe that many editing instructions in MagicBrush [89] and UltraEdit [93] are highly similar to each other. As shown in Fig. 11, by pairing each editing pair with its nearest neighbor based on CLIP [54] text feature similarity, we find that many instructions are either similar or identical. This similarity is a key factor contributing to the effectiveness of RAIE.\n\nE. More Qualitative Results Visualization.\n\nWe provide more visualization on vision tasks in Fig. 8.\n\nF. Higher Resolution Reconstruction\n\nWe use the Rain-100L derained test set to evaluate the reconstruction abilities of different models. As shown in Tab. 5, increasing the input resolution significantly enhances reconstruction quality for both VQ-VAE and VAE models. This improvement arises from the fact that image compression inherently leads to a loss of detail, and providing higher-resolution input allows the model to recover previously lost details, resulting in better outputs. However, we are unable to implement X-Prompt with a 1024 resolution as Chameleon [68] is pretrained exclusively on a 512 resolution. We anticipate significant improvements across all tasks with the availability of higher-resolution early-fusion multi-modal foundation models in the future."}]}