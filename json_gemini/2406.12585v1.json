{"title": "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling", "authors": ["Yao-Ching Yu", "Chun-Chih Kuo", "Ziqi Ye", "Yu-Cheng Chang", "Yueh-Se Li"], "abstract": "Ensembling multiple models has always been an effective approach to push the limits of existing performance and is widely used in classification tasks by simply averaging the classification probability vectors from multiple classifiers to achieve better accuracy. However, in the thriving open-source Large Language Model (LLM) community, ensembling methods are rare and typically limited to ensembling the full-text outputs of LLMs, such as selecting the best output using a ranker, which leads to underutilization of token-level probability information. In this paper, we treat the Generation of each token by LLMs as a Classification (GAC) for ensembling. This approach fully exploits the probability information at each generation step and better prevents LLMs from producing early incorrect tokens that lead to snowballing errors. In experiments, we ensemble state-of-the-art LLMs on several benchmarks, including exams, mathematics and reasoning, and observe that our method breaks the existing community performance ceiling. Furthermore, we observed that most of the tokens in the answer are simple and do not affect the correctness of the final answer. Therefore, we also experimented with ensembling only key tokens, and the results showed better performance with lower latency across benchmarks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in a wide range of natural language processing tasks (Achiam et al., 2023; Touvron et al., 2023). Over time, new and more powerful LLMs are continually being released, pushing the boundaries of the LLM community (Meta, 2024; Alibaba, 2024). Due to the diversity of data sources, architectures and training methods, different LLMs have strengths and weaknesses in different tasks and contexts (Jiang et al., 2023). In addition to investing significant resources in training a superior LLM, ensembling multiple existing models is another effective way to break through the community performance ceiling (Huang et al., 2016), especially given the current trend in the open source LLM community to contribute only model weights rather than training data and procedures (AllenAI, 2024).\nTaking computer vision (CV) classification as an example, it is common to ensemble the output probability vectors of multiple models (e.g. by averaging) to achieve superior results (Krizhevsky et al., 2017). This approach remains effective even with recent CV models. Similarly, the popular decoder-only LLM architecture generates text by producing tokens one by one, with each generation step resulting in a probability vector of the length of the vocabulary. Inspired by CV, we propose to treat each generation step as a classification task, and by ensembling multiple models, we can achieve higher accuracy. There is already work that simplifies problems into binary tasks, exploiting the collective wisdom of LLMs and achieving better results, demonstrating the feasibility of this approach.\nAnother advantage is that early errors in LLMs often snowball into later errors (Zhang et al., 2023). Ensembling during generation helps prevent the generation of inaccurate tokens at each step, thereby reducing misleading cues for subsequent token generation. In this paper, we conducted experiments at several points in time between November 2023 and June 2024, ensembling available state-of-the-art (SOTA) LLMs up to each of these points. We found that this approach significantly outperformed any single model available at those times on five popular benchmarks involving subject examination, mathematics, reasoning, and knowledge-based QA.\nIn addition, we found that for text generation it seemed unnecessary to ensemble every token. For example, for the question \"What Andean animal has banana-shaped ears?\" shown in Fig. 1, the most critical part is for the LLM to generate the key token \"llama\". The initial part of the answer \"It should be _\" or \"The animal is _\" do not significantly affect the correctness of the final answer. Ideally, the step that produces the token \"llama\" is the one we want to ensemble.\nStudies in CV classification have also shown that most samples are \"simple\" and can be correctly classified by most models (Wang et al., 2017), including cost-efficient ones, making the use of expensive models wasteful. To address this, CV classification used cascade inference (Jazbec et al., 2024; Enomoro and Eda, 2021), where a gate model passes a sample to a more powerful model only if its confidence falls below a threshold, thereby improving efficiency. Obviously, it is very important for cascading that the confidence of the gate model accurately reflects the accuracy. To ensure that LLMs are also suitable as gate models, we measured the Expected Calibration Error (ECE) (Guo et al., 2017) of CV models and LLMs on ImageNet and MMLU (Hendrycks et al., 2020), as shown in Tab. 1. ECE is a metric that reflects the difference between a model's confidence and its accuracy. We found that the ECE of CV models and LLMs were close. Therefore, in this paper, we also applied the cascade inference to LLMs by ensembling only the \"key\" tokens to speed up generation. Our experiments showed that this approach consistently achieved better performance with lower latency across different benchmarks."}, {"title": "2 Analysis and Prior Work", "content": "In this chapter, we review previous LLM ensemble studies and their features, as well as the problems our approach addresses. Previous studies can be categorized as follows:\nOutput-level ensemble methods select multiple candidate models and use their complete outputs for ensembling. Jiang et al. (2023) trained an additional ranking model (PairRanker) to score each candidate output and select the best one. Lu et al. (2023) and Shnitzer et al. (2023) trained a router to select the most appropriate candidate model given a question. However, these methods are limited to the existing candidate outputs and become ineffective if all the outputs are incorrect. Other studies have trained a fusion model to blend the outputs (Jiang et al., 2023; Wang et al., 2023b), overcoming the limitation of selecting only existing candidate outputs and often achieving superior results. However, the generalization of the fusion model is a major challenge, and they cannot fully exploit the probability information from each generation step.\nWeight-level ensemble methods merge the weights of multiple models and are primarily used in multi-task learning (Yadav et al., 2024). The expectation is that the merged model will inherit capabilities across multiple tasks. However, a limitation is that the architectures of the models to be merged must be homologous, which limits the use of the capabilities of the LLM community. And it is rare to observe that the merged model outperforms the original models (Yu et al., 2023).\nTraining-level ensemble like FuseLLM (Wan et al., 2024) uses the output probability vectors of multiple models during training to ensemble as labels, rather than one-hot labels. In effect, this is a specific form of distillation that allows the model being trained to gain more information from the probability outputs of the ensembled (teacher) models. However, distillation is mainly used to improve small models, making it difficult to further improve the SOTA LLMs.\nOur work can overcome the above limitations by ensembling at each generation step, allowing the output not to be confined to the original candidate output space and homologous architectures, while fully exploiting the probability information at each step. Our experiments in Sec.4 will also show that the ensemble consistently outperforms any single model, even SOTA LLMs. The main challenge, however, is that different LLMs typically have inconsistent vocabularies, leading to different dimensions in the probability vectors produced by different models. The most intuitive solution is to take the union of the vocabularies of the ensembled LLMs, denoted $V_U$, which includes all tokens from the participating models. Then, at each generation step, the output is first mapped to this union space $R^{\\vert V_U \\vert}$ before ensembling.\nA potential problem with this approach is that different models may tokenize the same word differently, leading to conflicts. However, most mainstream LLMs use BPE or BBPE (Sennrich et al., 2015; Wang et al., 2020) to train tokenizers on sampled corpora, which tend to have similar sources (e.g. CommonCrawl) and distributions. This results in consistent tokenization for common words. For example, both Qwen1.5 and Llama3 (Bai et al., 2023; Meta, 2024) tokenize the word \"alphabetically\" into [\"\u0120alphabet\", \"ically\"]. If both models intend to output this word, they will assign a higher probability to \"Galphabet\" first. We selected several popular LLMs and tokenized 5,000 commonly used English words, and then calculated the proportion of identical tokenization results between each pair of LLMs. The proportion is above 90% for all pairs, indicating that such conflicts can be ignored in most cases."}, {"title": "3 Proposed Method", "content": "In this section, we will first introduce the overall ensemble process of our GAC framework, and then explain the details in the following subsections."}, {"title": "3.1 Overall Process of GAC", "content": "When generating text, LLMs output a probability vector of the same dimension as their vocabulary. Given n LLMs to be ensembled, we first take the union of their vocabularies and create a mapping matrix that can project the probability vectors to the union dimensions. At each generation step, all LLMs produce outputs that are mapped to the union vocabulary dimensions and ensembled to sample the next token. The tokenizer of each LLM then converts the sampled token into token IDs for the next step. As mentioned in Sec.1, not all tokens have the necessity for ensembling, so we also try to ensemble only certain key tokens."}, {"title": "3.2 Creating the Union Mapping", "content": "Given {LLM1, LLM2,...,LLMn} to ensemble, with their respective vocabularies {V1, V2, ..., Vn}, we first take the union of the vocabularies:\n$V_U = \\bigcup_{i=1}^{n} V_i$. (1)\nDuring this process, we record the positions of tokens from V\u00b9 in VU and create corresponding mapping matrices $M_i \\in \\mathbb{R}^{\\vert V_i \\vert \\times \\vert V_U \\vert}$."}, {"title": "3.3 GAC Ensembling", "content": "At the start of text generation, we convert the input prompt into token ID sequences for each LLM. We denote the tokenizer of LLMi as Ti : text \u2192 (\u03c41, \u03c42, . . ., \u03c4m), which converts the input text into a sequence of token IDs. We calculate:\n$I^i = T^i(prompt) \\quad for \\quad i = 1, ..., n$ (2)\nwhere Ii is the input token ID sequence for LLMi. For each generation step, we input Ii into LLMi to obtain $p^i(\\cdot \\vert I^i) \\in \\mathbb{R}^{\\vert V_i \\vert}$, which represents the probability vector for the next token. These vectors are then mapped to the union vocabulary dimensions and averaged:\n$q(\\cdot) = \\frac{1}{n} \\sum_{i=1}^{n} p^i(\\cdot \\vert I^i) \\cdot M^i,$ (3)\nwhere $q(\\cdot)$ is the ensemble probability vector. In Sec.4.3, we experimented with different ensemble weights and decided to use the average. We then sample a token $x \\sim q(\\cdot)$ as the result of this step. Finally, the sampled token is converted back into token IDs for each LLM and appended to Ii:\n$I^i \\leftarrow I^i \\cup T^i(x) \\quad for \\quad i = 1,..., n$ (4)\nWe repeat (3) and (4) until the stopping criteria are met, such as outputting an end-of-sentence token or reaching the maximum length, as shown in Fig.3. In our implementation, different LLMs run in parallel on different GPUs, so the duration of each step is equal to the time taken by the slowest LLM. Since we have not modified a complete forward pass, our approach is compatible with techniques such as vLLM, DeepSpeed, quantization, and hardware optimizations."}, {"title": "3.4 Ensembling Key Tokens with Threshold", "content": "As mentioned in the last part of Sec. 1, most tokens do not significantly affect the correctness of the response. From Tab.1, we can see that LLMs and CV models have similar ECE levels, suggesting that the confidence scores of LLMs may reflect accuracy to some extent. Therefore, we also experiment with ensembling only the tokens with confidence below a threshold t. We choose a model as the gate, denoted LLMg, and use its maximum probability at each step as the confidence score. During the ensemble, we replace the original (3) with:\n$q(\\cdot) = \\begin{cases} \\sum_{i=1}^{n} p^i(\\cdot \\vert I^i) \\cdot M^i & \\text{if } max(p^g(\\cdot \\vert I^g)) \\leq t \\\\ p^g(\\cdot \\vert I^g) \\cdot M^g & \\text{otherwise.} \\end{cases}$ (5)\nNote that apart from LLMg, the other LLMs are not computed at every step, so their KV caches become stale. While there has been research using partial KV caches (Barad et al., 2023), for simplicity our work disables the KV caches of all LLMs except LLMg. This is an area for improvement and is listed in our future work."}, {"title": "4 Experiments", "content": "In this section, we first present the experimental setup, including the benchmarks and hardware used (Sec.4.2). We then test the effects of different ensemble weights for GAC (Sec.4.3) and compare it with other methods (Sec.4.4). We also select SOTA LLMs available at different times for ensembling to explore the performance ceiling at each time period (Sec.4.5). Finally, we experiment with thresholded ensembling to explore variations in latency and performance (Sec.4.6)."}, {"title": "4.2 Experimental Settings", "content": "Benchmarks. GAC is not limited to specific tasks, so we tested it as broadly as possible. We selected a total of five benchmarks. For general capabilities, we chose MMLU (Hendrycks et al., 2020). For maths, we utilized GSM8K (Cobbe et al., 2021). For reasoning, we employed BBH (Suzgun et al., 2023). For knowledge capabilities, we included TriviaQA and NaturalQuestions (NQ) . Note that all scores, including those for individual models, were computed locally under the same environment to ensure fairness, using lm-evaluation-harness v0.4.1.\nHardware and Latency. Each LLM was loaded on 1(n) A100 GPU(s) according to its memory requirements, using naive model parallelism without optimization for inference. During ensembling, different LLMs were loaded on separate GPU(s) and executed in parallel, managed and communicated via Ray . We also recorded the latency (ms/token). Each model performs a \"dry run\" after being loaded onto the GPU, generating 1024 tokens to warm up CUDA before experimentation, following the practice of Mehta et al. (2024)."}, {"title": "4.3 Different Ensemble Weights", "content": "Before proceeding with further experiments, we tested different ensemble weights for GAC. We used a simple averaging of the probabilities from each model in Eq.3. We now replace Eq.3 by $\\frac{\\sum_{i=1}^{n} w^i p^i(\\cdot \\vert I^i)M^i}{\\sum w^i}$, where wi is the ensemble weight for LLMi. We set wi separately to each LLM's score on MMLU, the MMLU score minus the ECE and 1 (i.e. averaging). We selected two sets of LLMs listed in Tab.2 with different sizes for GAC ensemble. We observed no significant differences between the different weights, so for simplicity we decided to use averaging."}, {"title": "4.4 Comparison with Other Methods", "content": "Baselines. We compared GAC with existing methods. First, we considered LLM Blender (Jiang et al., 2023), which employs PairRanker to rank the outputs of candidate LLMs and GenFuser to fuse these outputs. However, we found that GenFuser refused to answer a significant proportion of the questions in our chosen benchmarks. We therefore only used PairRanker to ensure fairness. We also included other rankers, such as OAssistRM and UltraRM , which we ran in parallel on the GPUs hosting the ensemble LLMs to ensure low latency. These rankers scored the outputs and selected the best answers. Furthermore, we included the FuseLLM (Wan et al., 2024) (OpenChat-3.5-7B-Solar), which uses probability information from multiple models during training for distillation.\nModels for Ensemble. We chose two sets of models of different sizes. The smaller models included openchat-3.5 (Wang et al., 2023a) and Nous-Hermes-2-SOLAR-10.7B (NousResearch, 2024b) (teacher models for OpenChat-3.5-7B-Solar distillation). Larger models included Mixtral-8x7B-Instruct-v0.1 and Yi-34B-Chat.\nExperimental Results. We ensemble the above two sets of LLMs using both our and baseline methods, and present the results in Tab.3. Our method showed superior performance with the lowest latency for both combinations (row ids 6 and 14). For row id 8, we used openchat-3.5 as the gate model with a threshold of 0.5 (Eq.5), resulting in only 7.68% of tokens being ensembled. This slightly increased the latency from 28.01 to 31.34 ms/token, but achieved a performance close to that of SOLAR-10.7B (average score of 58.01 vs. 58.24), whose latency is 50.89 ms/token, further demonstrating the effectiveness of our method."}, {"title": "4.5 Breaking the Ceiling", "content": "In this experiment, we aimed to break the performance ceiling of the open source LLM community at different times. We chose the SOTA LLMs released between November 2023 and June 2024, as listed in the upper part of Tab.4, excluding models with more than 100 billion parameters due to hardware limitations. We then ensemble the SOTA LLMs available at different times, as shown in row ids 6-10, and observe an improvement of 3.13% to 4.47% over the best single model at each time. An exception is 2024/04/18, when Llama-3-70B-Instruct was released and significantly improved performance over the previous SOTA LLMs, resulting in a drop in performance after ensemble due to the large gap.\nHowever, with the release of Qwen2-72B-Instruct, which showed comparable performance to Llama-3-70B-Instruct, the ensemble again led to significant improvements (row id 10). In rows 11 and 12, we ensemble the top two best-performing models for each benchmark at the two most recent times, including the challenging time of 2024/04/18, and observe performance gains with this task-specific top-two ensemble even on 04/18 (row id 11). Finally, row id 12 shows the best results available for the open source community on 2024/06/07. By pushing the boundaries of the community, we can narrow the gap with proprietary models and promote the democratization of LLMs.\nSince ensembling models with large performance differences could lead to performance degradation (row id 9 in Tab.4), we also tested this hypothesis with CV models. We ensemble PVTv2-B1 with different sizes of EfficientNet on ImageNet by averaging their outputs. We observed that as the accuracy gap between the two models increased, the ensemble gains decreased and eventually became negative. This suggests that it is advisable to ensemble models with similar levels of performance."}, {"title": "4.6 Ensemble with Threshold", "content": "In this experiment, we used the thresholded ensemble (Sec.3.4) to explore variations in latency and performance. We selected models of different sizes , listed in the upper part of Tab.5, pairing a smaller model with a larger model and using the smaller model as the gate model for the ensemble. We aimed to match the performance (average score) of Qwen1.5-72B-chat and Qwen1.5-32B-chat with our ensemble, but with lower latency, and the results are shown in row ids 6-9. Interestingly, even when combining the two Qwen models themselves with a threshold of 0.5 (row id 7), where 6.31% of the tokens were ensembled, we observed slightly higher performance than Qwen1.5-72B-chat (average score increased from 60.55 to 60.96) and lower latency (102.11 to 77.86 ms/token). We believe this is a promising new way to speed up inference.\nWe also observed lower latency in row ids 6, 8 and 9 of Tab.5 with comparable performance to Qwen1.5-72B-chat or Qwen1.5-32B-chat. In addition, similar trends were observed in the MT-Bench using LLM as judge (GPT-4-0613) in Tab.6, with scores calculated using FastChat in our local environment. For the combinations of Llama-3-8B-Instruct or Phi-3-mini-4k-instruct with Llama-3-70B-Instruct, we directly adopted the output of the larger model if the probability of the smaller model was below the threshold. This is based on our observations in Fig.5, where ensembling models with large performance gaps resulted in reduced performance."}, {"title": "5 Conclusion", "content": "In this paper, we present a token-level ensembling framework called GAC, which fully exploits the probability information at each generation step. In our experiments, we have surpassed the performance ceiling of open-source SOTA LLMs available at different time periods (Sec.4.5), further narrowing the gap between open-source and proprietary models. This progress promotes the democratization of LLMs and provides new motivations for future research, enabling better exploitation of collective intelligence. In addition, we experimented with ensembling just a few tokens and found that this approach can achieve better performance with lower latency (Sec.4.6), opening up new avenues for accelerating inference."}, {"title": "Contemporaneous Works", "content": "We have noticed several contemporaneous works related to our research, all of which aim to address the vocabulary discrepancy between different models. Xu et al. (2024) proposed EVA, which trains a projection matrix between each pair of LLMs, using the overlapping tokens from their vocabularies as a bridge. DEEPEN (Huang et al., 2024) converts the output probabilities to a relative representation using anchor tokens before ensembling, and then inverts back to the original model's vocabulary space using gradient descent, which requires an additional 7% to 29% of time per generation step. In contrast, our method requires no additional training and only a single matrix multiplication and tokenization for each model during ensembling, with minimal time cost."}, {"title": "Limitation", "content": "Like other ensemble methods, the approach proposed in this paper requires more computational resources. Although different models can be run in parallel on separate GPUs, so that latency only depends on the slowest model, the overall computational load is additive, raising the threshold for use."}]}