{"title": "Code-mixed LLM: Improve Large Language Models' Capability to Handle Code-Mixing through Reinforcement Learning from AI Feedback", "authors": ["Wenbo Zhang", "Aditya Majumdar", "Amulya Yadav"], "abstract": "Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of linguistic units from two or more languages during the conversation or sometimes even a single utterance. Code-mixing introduces unique challenges in daily life, such as syntactic mismatches and semantic blending, that are rarely encountered in monolingual settings. Large language models (LLMs) have revolutionized the field of natural language processing (NLP) by offering unprecedented capabilities in understanding human languages. However, the effectiveness of current state-of-the-art multilingual LLMS has not yet been fully explored in the CM scenario. To fill this gap, we first benchmark the performance of multilingual LLMs on various code-mixing NLP tasks. Then we propose to improve the multilingual LLMs' ability to understand code-mixing through reinforcement learning from human feedback (RLHF) and code-mixed machine translation tasks. Given the high-cost and time-consuming preference labeling procedure, we improve this by utilizing LLMs as annotators to perform the reinforcement learning from AI feedback (RLAIF). The experiments show the effectiveness of the proposed method.", "sections": [{"title": "1 Introduction", "content": "Code-mixing or code-switching is a linguistic phenomenon where two or more languages are mixed together within a single utterance, conversation, or speech. It allows individuals to convey culturally specific concepts, forge connections or differentiate themselves from others, and reinforce their identities. Code-mixing is common in multilingual societies, with recent studies indicating that up to 20% of online content in regions like South Asia, parts of Europe, and Singapore is code-mixed. In regions with significant bilingual or multilingual populations, code-mixing is more than a cultural expression; it is a core component of everyday communication. This widespread use highlights the necessity for NLP systems to process and interpret code-mixed language accurately.\nIn recent years, large language models (LLMs) have shown promising performance in comprehending, producing, and interacting with human language. Trained on extensive text corpora, these models can capture a broad range of linguistic patterns and subtleties. Relying on the corpus containing independent monolingual texts in different languages, these LLMs have also achieved notable success in multilingual contexts. As a result, these multilingual LLMs have enabled cross-lingual transfer, which has proven to be a valuable method to leverage resources from high-resource languages to improve downstream task performance for other languages which have similar linguistic structures. To some extent, this property of multi-lingual large language models makes them a natural choice to handle code-mixing\nYet, existing multilingual LLMs are not specifically trained with objectives for managing CM scenarios or large-scale corpus only including CM texts. The effectiveness of current state-of-the-art multilingual LLMs has not yet been explored and analyzed on code-mixed text fully. Hence, assessing the capabilities of the current multilingual LLMs to deal with code-mixing is essential.\nThe main challenges of developing multilingual LLMs optimized for CM are not limited to the im-balanced language ratio in the pre-training corpora, the data scarcity for CM in different languages, and the trade-off between model capacity and language coverage(Philippy et al., 2023). Previous work focuses on data augmentation for CM data, fine-tunes LLMs in specified code-mixed language pairs (Zhang et al., 2023b). These methods do not utilize the text-generation capabilities of LLMs that are fundamentally designed for. The adoption of these prompt-based multilingual LLMs for code-mixing remains a challenge. This observation motivates us to explore - Can existing multilingual"}, {"title": "3 Benchmark the performance of LLMS in code-mixing scenarios", "content": "We present a more comprehensive empirical analysis of models' capability to deal with code-mixing, including a variety of task types, model architectures, and model sizes. Currently, our evaluation is limited to Hindi-English language pairs as this type of code-mixing dataset is easily accessible. However, we will extend our evaluation to other code-mixed language pairs to make our argument more convincing. In this section, we explore two code-mixing task categories: sentiment analysis (SA), and machine translation (MT)."}, {"title": "3.1 Sentiment Analysis", "content": "Sentiment Analysis is the task of understanding the sentiment expressed in the text and classifying the text into different categories. In this task, we use the Hindi-English subset from the SemEval-2020 Task 9 (Patwa et al., 2020) which is designed for the sentiment analysis of code-mixed tweets."}, {"title": "3.2 Machine Translation", "content": "Machine translation works by using advanced algorithms and machine learning models to automatically translate text or speech from one language to another. In this task, we use the English-Hinglish (code-mixed Hindi and English) dataset from MixMT 2022 shared task (Srivastava and Singh, 2022) which is designed for machine translation of code-mixed text."}, {"title": "4 Improve the capability to deal with code-mixing through Reinforcement Learning from AI Feedback(RLAIF)", "content": "Code-mixed machine translation is chosen as the basic task to realize the RLAIF procedure due to: (i) prior studies show that applying the machine translation task into the model training procedure enables the improvement of cross-lingual transfer (Lample and Conneau, 2019; Hu et al., 2020; Conneau et al., 2020). Using multi-lingual large language models to handle code-mixing benefits from the cross-lingual transfer; (ii) the expected goal of the machine translation task also focuses on improving the quality of generated texts. In the following sections, I will start with a base open-sourced LLM, demonstrate how to collect preference data and finish the RLAIF procedure (see Figure. 1)."}, {"title": "4.1 Supervised Fine Tuning", "content": "Given the parallel corpus, we apply a fixed prompt template on the parallel corpus and convert it into the training set for RLAIF. The base LLM is supervised fine-tuned (SFT) using the next-token prediction objective on this training set (Radford et al., 2019). After optimization, the SFT LLM is refined for the purpose of following instructions (Wei et al., 2021)."}, {"title": "4.2 Preference Modeling", "content": "Response Collection. Collecting human preference data is also nontrivial, time-consuming, and labor-intensive. Instead of collecting code-mixing datasets by ourselves, we propose another simple yet efficient way to collect preference. To be more specific, we demonstrate how to transform the existing code-mixing dataset into a new response dataset in the code-mixing area.\nPreference annotations through LLM. Preference data annotation often requires high demands on annotators and is plagued by inconsistencies in"}, {"title": "4.3 Model Fine Tuning through Policy Optimization", "content": "The SFT LLM is further trained to maximize the expected output from the reward model trained in the reward model training stage (a proxy for user preferences) using general-purpose reinforcement learning algorithms."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Benchmark Analysis", "content": ""}, {"title": "5.1.1 Dataset", "content": ""}, {"title": "5.1.2 Models for Semantic Analysis", "content": "Fine-tuned Models. We conduct experiments involving fine-tuning as a part of the benchmark. We fine-tune the models listed below on the training set and evaluate them on the test set. We fine-tune 4 models which are specifically designed for code-mixed inputs:\n\u2022 TAC model (Gautam et al., 2021) proposes using mBART to translate code-mixed text into monolingual English text first, and then completing the sentiment classification with ROBERTa.\n\u2022 CS-ELMo (Aguilar and Solorio, 2019) aims to transfer English knowledge from a pre-trained ELMO model to various code-switched language pairs (such as Nepali-English, Spanish-English, and Hindi-English) through the language identification task.\n\u2022 SC-CMT (Mahata et al., 2021) develops a sentiment analysis model based on the bi-directional LSTMs along with language tagging."}, {"title": "5.1.3 Models for Machine Translation", "content": "Fine-tuned Models. We conduct experiments involving fine-tuning as a part of the benchmark. we fine-tune models listed below on the training set and evaluate them on the test set. For machine translation, we fine-tune three models in total:\n\u2022 mT0 (Muennighoff et al., 2022) is the fine-tuned variant of multilingual BLOOM and mT5, obtained through multitask prompted fine-tuning. It supports about 120 different languages.\n\u2022 M2M100 (Fan et al., 2020) proposes the many-to-many multilingual translation model that can translate directly between any pair of 100 languages.\n\u2022 mBART-50 (Liu et al., 2020) is the first to pre-train a complete sequence-to-sequence model by denoising full texts in multiple languages.It supports 50 different languages."}, {"title": "5.2 Reinforcement Learning from AI Feedback", "content": ""}, {"title": "5.2.1 Dataset", "content": ""}, {"title": "5.2.2 Model", "content": "Llama-3.1-8B-Instruct. The language capability of the reward model is vital for preference modeling (Xu et al., 2024). We choose this model as the base of our experiment as it supports English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai."}, {"title": "5.2.3 Setup", "content": ""}, {"title": "5.2.4 Evaluations", "content": "To evaluate the effectiveness of the proposed method, we employed three evaluation methods: LLM comparative evaluation, BLEU metrics, and human evaluation."}, {"title": "6 Results and Discussion", "content": ""}, {"title": "6.1 Benchmark Analysis", "content": ""}, {"title": "6.2 Effetiveness of RLAIF Adaption", "content": "Table 3 shows the performance of LLM after RLAIF procedure. The higher win rate of RLAIF shows the effectiveness of utilizing RLAIF in multilingual LLMs."}, {"title": "6.3 Experiment-2", "content": "to be continued..."}, {"title": "6.4 Experiment-3", "content": "to be continued..."}, {"title": "7 Conclusion and Limitations", "content": "to be continued..."}]}