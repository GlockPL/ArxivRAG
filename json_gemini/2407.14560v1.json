{"title": "Automated and Holistic Co-design of Neural Networks and ASICS\nfor Enabling In-Pixel Intelligence", "authors": ["Shubha R. Kharel", "Prashansa Mukim", "Piotr Maj", "Grzegorz W. Deptuch", "Shinjae Yoo", "Yihui Ren", "Soumyajit Mandal"], "abstract": "Extreme edge-AI systems, such as those in\nreadout ASICs for radiation detection, must\noperate under stringent hardware constraints\nsuch as micron-level dimensions, sub-milliwatt\npower, and nanosecond-scale speed while provid-\ning clear accuracy advantages over traditional ar-\nchitectures. Finding ideal solutions means iden-\ntifying optimal AI and ASIC design choices from\na design space that has explosively expanded\nduring the merger of these domains, creating\nnon-trivial couplings which together act upon a\nsmall set of solutions as constraints tighten. It is\nimpractical, if not impossible, to manually deter-\nmine ideal choices among possibilities that eas-\nily exceed billions even in small-size problems.\nExisting methods to bridge this gap have lever-\naged theoretical understanding of hardware to\ncreate proxies for key metrics such as ASIC area\nand power and used them in neural architecture\nsearch. However, the assumptions made in com-\nputing such theoretical metrics are too idealized\nto provide sufficient guidance during the difficult\nsearch for a practical implementation. Mean-\nwhile, theoretical estimates for many other cru-\ncial metrics (like delay) do not even exist and\nare similarly variable, dependent on parameters\nof the process design kit (PDK). To address these\nchallenges, we present a study that employs in-\ntelligent search using multi-objective Bayesian\noptimization, integrating both neural network\nsearch and ASIC synthesis in the loop. This ap-\nproach provides reliable feedback on the collec-\ntive impact of all cross-domain design choices.\nWe showcase the effectiveness of our approach\nby finding several Pareto-optimal design choices\nfor effective and efficient neural networks that\nperform real-time feature extraction from input\npulses within the individual pixels of a readout\nASIC.", "sections": [{"title": "1 Introduction and Motivation", "content": "Real-time edge-AI systems are designed for infer-\nence at the edge of computing hardware, where\ndata is generated, and are crucial for applications\nrequiring swift detection, inference, and decision-\nmaking [22, 26, 39]. These systems must be\nadequately reliable while operating within strin-\ngent resource constraints, including limited area,\npower, and latency. Their design complexity is\namplified by the numerous design choices avail-\nable in both the AI and hardware spaces, the\ncomplex interactions between them, and their\ncombined effect on the deployment feasibility in\nreal-world applications [25].\nTackling the stringent requirements for ex-\ntreme edge-AI systems has spurred innovations\nin AI models, hardware, and their co-design. In\nAI design, research has concentrated on model\ncompression techniques such as quantization and\npruning [23, 17] that enhance computational ef-\nficiency without detrimentally compromising ac-\ncuracy. Similarly, innovations have emerged in\nAI accelerators customized for these AI models\nand their implementation on field-programmable\ngate arrays (FPGAs) [13, 38] and application-\nspecific integrated circuits (ASICs) [20, 32]. The\nco-design approach has taken this process further\nby simultaneously considering design choices\nfrom both AI and hardware perspectives to find\nthe best solution for both domains [5, 40].\nThis paper focuses on the co-design approach\nto addressing the challenges of extreme edge-\nAI systems. A significant challenge in co-design\nis the exponential growth in design choices and\nthe substantial compute costs required to quan-\ntify its impact on design objectives. Automated\nsearches with advanced algorithms like Bayesian\noptimization [34, 37] and reinforcement learn-\ning [1, 21] have shown promise in navigating this\ncomplex co-optimization space but remain too\nexpensive to be practical for most applications,\nespecially when quantifying the costly hardware\nobjectives. Some progress has been made using\ntheory-guided approximations to model hard-\nware objectives [9, 7], but actual implementa-\ntions often deviate significantly from these esti-\nmates, leading to sub-optimal or infeasible de-\nsigns. While improving the accuracy of these\nestimates is crucial, it is often insufficient due\nto the unique and intricate nature of more prac-\ntical hardware objectives for which theoretical\nestimates are not available.\nMany scientific applications would benefit\nfrom fast, low-latency machine learning, includ-\ning event reconstruction and triggering in parti-\ncle accelerators, denoising of gravitational wave\ndetectors, and real-time monitoring and con-\ntrol of plasma dynamics [15, 10]. ASICs rep-\nresent a natural progression from the success\nof FPGA-based AI accelerators in such scien-\ntific applications. Unlike other specialized hard-\nware, ASICs offer a unique combination of effi-\nciency, area, and power optimization, enabling\nthem to operate under the extreme conditions of\nedge computing and even advancing AI to pixel-\nlevel processing in devices such as radiation de-\ntectors [27, 29, 28, 24]. Despite these advantages,\nthe field has remained relatively unexplored due\nto the high development costs, the need for close\ninterdisciplinary collaboration with highly spe-\ncialized hardware design domains - often lack-\ning in academic and scientific settings - and the\npaucity of consumer-available options. However,\nthe growing availability of open-source tools, in-\ncreased support from government initiatives, and\nheightened scientific interest in advancing effi-\ncient AI has created a more feasible ecosystem\nfor AI-ASIC co-design.\nThe AI-ASIC co-design environment presents\nmore constraints than those typically encoun-\ntered with AI accelerators and FPGAs. While\nthis adds complexity, it also narrows the de-\nsign space, enabling a more feasible approach\nfor holistic integration, which is often challeng-\ning in broader settings. In this study, we em-\nbraced this unique context by holistically inte-\ngrating design choices across the AI and ASIC\ndomains. Instead of relying on theoretical esti-\nmates, our approach utilizes direct metrics ob-\ntained from hardware synthesis, incorporating\nthese insights into a fully automated pipeline us-\ning multi-objective Bayesian optimization. This\nmethod allows us to consider more practical and\nnecessary hardware objectives. We demonstrate\nthe effectiveness of our approach on a scientific\napplication, namely the design of on-chip neural\nnetworks for feature extraction from amplified\ncharge pulses [28]. Such pulses, which are pro-\nduced by readout ASICs designed for radiation\nsensing, encode information on the energy, direc-\ntion of arrival, and identity of particles observed\nin high-energy physics experiments [33].\nThe main contributions of this work are as fol-\nlows:\n\u2022 Realistic Estimates and Practicality:\nWe incorporate more accurate estimates of\ndesign objectives in our optimization pro-\ncess to increase the likelihood of achieving\ndesigns that are both optimal and feasible.\n\u2022 Holistic Co-design integration: We ex-\npand on the usual co-design choices, such\nas the neural network architectures and\nquantization strategies, to also simultane-\nously consider ASIC synthesis strategies,\nthus enabling a more comprehensive search\nfor practical designs."}, {"title": "2 Related Work", "content": "A common metric used to characterize the per-\nformance of digital systems the power-delay\nproduct (PDP), often expressed in inverse form\nas performance-per-watt (MIPS/W or FLOP-\nS/W) with higher values being better [6]. An-\nother common metric is the EDP, which is de-\nfined as the product of delay/operation and\nenergy/operation [19] with lower values being\nbetter. The two metrics are related by the\nfact that energy is proportional to PDP, which\nin turn implies that EDP is proportional to\n(performance)2-per-watt, i.e., emphasizes per-\nformance to a greater degree than PDP.\nEarlier work on automated hardware-aware\ndesign space exploration for AI utilized ana-\nlytical models to approximate the relationship\nbetween network hyperparameters (such as the\nnumber of layers, neurons per layer, and weight\nprecision per layer) and performance metrics\nsuch as PDP. For example, QKeras [9] utilizes"}, {"title": "3 Methodology", "content": "In our co-design method, we collate the Neu-\nral Network and ASIC design spaces and si-\nmultaneously optimize multiple objectives, us-\ning realistic metrics affected by a given set of\nchoices. The Neural network design space con-\nsists of MLP architecture and heterogeneous\nquantization configuration. ASIC design space\nconsists of the strategy used during ASIC syn-\nthesis. ASIC synthesis and Quantization Aware\ntraining within it gives a more accurate estimate\nof co-design objectives. Finally, an automated\npipeline integrates all components to do multi-\nobjective Bayesian optimization. This method\nkeeps things simple, suitable, and sufficient for\ndemonstrating our co-design approach.\n3.1 Neural Network Architecture\nWe focus on Multi-Layer Perceptron (MLP)\nmodels consisting of fully connected layers. De-\nspite their simplicity, they have been proven to\nbe universal approximators, allowing them to\napproximate the most relevant functions with\nsufficient depth and width, both of which are\nincluded in the co-design space. While these\nparameters determine the model's performance,\nthey also determine the computational cost\nwhich scales as\n$O_{MLP} = O(\\sum_{i=1}^{d} n_i n_{i+1})$\nwhere d is the depth and $n_i$ is the width of each\nlayer. Computational complexity affects rele-\nvant co-design objectives by influencing the area\nneeded for memory and computation units, the\npower consumed for data storage and processing,\nand the latency required to compute each layer's\noutput.\n3.2 Quantization Configuration\nQuantization reduces the precision of the nu-\nmerical representation of weights and activation\nfunctions from larger floating-point numbers to\nsmaller fixed-point numbers, integers, or even\nbinary numbers. Quantization can significantly\ndecrease resource consumption for an MLP with\na given width and depth without substantially\naffecting its performance. Heterogeneous quan-\ntization takes this idea further by aggressively\nquantizing less important layers to lower preci-\nsion based on the importance of the layer to the\noverall performance in the network. So, we ex-"}, {"title": "3.3 ASIC Design", "content": "Recent years have witnessed a rapid growth in\nthe availability of open-source process design\nkits (PDKs) and electronic design automation\n(EDA) tools for ASIC design. The first open-\nsource PDK, for the SkyWater 130 nm CMOS\nprocess, was launched in 2020 in collaboration\nwith Google. It has since been widely used for\na variety of ASIC designs [12], including pro-\ncessors [41], coarse-grained reconfigurable arrays\n(CGRAS) [8] and AI accelerators [30, 31].\nThe standard open-source digital ASIC de-\nsign flow for the SkyWater 130 nm process is\nOpenLANE, which is based on the OpenROAD\nproject [2] and was first introduced by Shalan\nand Edwards in 2020 [36]. OpenLANE has been\nbenchmarked against commercial EDA tools [41]\nand found to be comparable on key metrics such\nas synthesis run time, gate count, placement &\nrouting time, die area, leakage power, placement\ndensity, and maximum clock frequency. How-\never, its performance is typically 1.4\u00d7 to 1.6\u00d7\nworse for most of these metrics. On the plus side,\nOpenLANE is highly automated, thus making it\nsuitable for incorporation within Bayesian design\nspace exploration."}, {"title": "3.4 Multi-objective Bayesian Opti-\nmization", "content": "The most limiting aspects of co-designing AI and\nASIC are the high computational cost, exponen-\ntial growth in design choices, and coupled com-\npeting objectives. The computing cost is even\nexacerbated when we include ASIC synthesis to\nget more accurate estimates of the ASIC objec-\ntive metrics. Naively exploring the exponential\nand costly design space, like grid search, is in-\nefficient, if not infeasible. To handle these chal-\nlenges, we use Bayesian optimization, designed\nto minimize the number of expensive trials and\nproven to work effectively in domains including\nmachine learning hyper-parameter tuning, algo-"}, {"title": "3.5 Integration and Automation", "content": "While plenty of tools are available for NN mod-\neling and some for ASIC synthesis, there is a\nhigh barrier between tools from both co-design\ndomains. Our work bridges this much-needed\ngap through the seamless integration of these\ntools. Additionally, automation for going back\nand forth between NN training and ASIC syn-\nthesis is also lacking despite being crucial for co-\ndesign. Our work contributes to this by fully\nautomating co-design optimization using the in-\ntegrated tools. To make our tools accessible for\nfuture research, we exclusively use open-source\ntools."}, {"title": "4 Implementation", "content": "Our goal is to provide the research commu-\nnity with automated, open-source, extensible,\nand scalable tool integration that is familiar\nto researchers in both AI and ASIC domains.\nWith this in mind, our automated pipeline\nuses Optuna for Bayesian optimization, QKeras\nfor quantization-aware training, OpenLANE for\nASIC synthesis, Hydra for easy configuration\nmanagement, and Docker for containerization.\nOptuna is a popular hyperparameter opti-\nmization library that offers features like paral-\nlelization and multiple optimization algorithms\nand is already popular among ML researchers.\nQKeras provides quantization-aware training for\nheterogeneously quantized models and is among\nthe first few open-source libraries to do so.\nOpenLANE is an open-source ASIC synthesis\ntool that provides a complete RTL-to-GDSII"}, {"title": "5 Experiment", "content": "As an example of practical importance, here we\nconsider the problem of on-chip waveform pro-\ncessing for radiation detectors. The basic goal\nof waveform processing is real-time feature ex-\ntraction from pulse shapes. The latter are gen-\nerated by analog front-end (AFE) circuits that\namplify and filter the charge packets generated\nby energetic particles [33]. A generic pulse can\nbe written in the form\n$s(t) = Ap(t - t_0) + n(t)$   (2)\nwhere p(t) is a reference pulse shape, A is a scal-\ning factor known as the pulse amplitude, $t_0$ is the\ntime of arrival (TOA), and n(t) is additive noise.\nNote that A is typically proportional to particle\nenergy, so quantifying its value is a key goal of\nmany measurements (which are known as am-\nplitude spectroscopy). In other words, A is the\nmost important feature to be extracted from the\nwaveform shape, s(t). The TOA is also impor-\ntant in many cases since it encodes information\non time of flight or arrival angle. Similarly, the\nreference shape p(t) can provide additional infor-\nmation, such as particle identification (PID) or\nthe angle of arrival.\nFor simplicity, we assume that the feature ex-\ntraction is performed digitally. In this case, the\nAFE is followed by an analog-to-digital converter\n(ADC) that converts s(t) to the discrete-time se-\nquence\n$s[k] = Ap[k \u2013 k_0] + n[k]$   (3)\nwhere k denotes the sample index. The digi-\ntized reference shape, p[k], depends on the par-\nticle type, detector material, and AFE filtering\nparameters. A commonly used model for p[k]\nis the so-called CR-(RC)N pulse shape, which is\ngiven by\n$p[k] = \\frac{1}{N!}(\\frac{kT}{\\tau})^N exp(-kT/\\tau)$   (4)\nwhere T is the sampling period, N = 1,2,... is\nknown as the order of the shaping function, and\n$T$ is the decay time constant.\nFig. 2 plots p(t) in continuous time for various\nvalues of N. Note that the waveform becomes\nsmoother and more symmetric as N increases\ndue to the increased amount of low-pass filtering.\nThe peak value of p(t), which occurs at $t_{pk} =$\nNT, is given by\n$Apk(N) = \\frac{(N)^N e^{-N}}{N!}$   (5)"}, {"title": "5.1 Dataset and Models", "content": "Initial experiments were focused on datasets\nwhere both the pulse shape, p[k], and the time\nconstant, \u03c4, were kept fixed. The feature extrac-\ntion process then reduces to the single-variable\nproblem of estimating the pulse amplitude, A.\nAn optimal solution to this problem exists when\nthe noise is AWG in nature; it takes the form of\na classical matched filter followed by peak pick-\ning [24]. However, note that this solution does\nnot account for the inevitable overlap between\nnearby pulses, which is known as pileup.\nLater simulations also allowed the time con-\nstant, \u03c4, to vary randomly over the range\n[\u03c4\u03bf, 2\u03c4\u03bf], where To = T is a constant. In this\ncase, the feature extraction algorithm is asked\nto estimate both A and T, i.e., has to solve a\ntwo-variable problem.\nThe sampling rate for the simulations was ini-\ntially set to T = T. All processing was per-\nformed on data windows with $N_w$ = 32 data\npoints. Variations in pulse arrival times within\nthese windows, which are inevitable due to the\nasynchronous pulse arrivals, was minimized by\na real-time waveform alignment algorithm. This\nalgorithm operates as follows:\n\u2022 Buffer incoming data into windows of length\n2$N_w$.\n\u2022 Find the location within the buffer, k\u0442\u0430\u0445,\nwith the peak absolute value, |s[k]|.\n\u2022 Output the sub-window of length ($N_w$ + 1)\ncentered around the peak position, kmax.\n\u2022 Buffer the next set of points and repeat.\nNote that this algorithm makes no attempt to\nmitigate the effects of noise and pileup on |s[k]|.\nThus, it only performs coarse time alignment of\nthe received pulse waveforms.\nFinally, the size of the neural network used for\nfeature extraction was minimized by decimating\nthe time-aligned waveforms, i.e., by increasing\nthe sampling period from T to MT where M > 1\nis an integer. This process reduces the length of\neach data window from $N_w$ +1 to $N_w$/M + 1.\nOur experiments used a default decimation\nfactor of M = 4 during the pre-processing\nstage, which results in windows containing 9 time"}, {"title": "5.2 Co-design Space", "content": "The co-design choices for the experiment in-\ncludes: a) the number of hidden layers, b) the\nnumber of perceptrons in each hidden layer,\nc) quantization levels of weights/biases between\nlayers, d) input/output quantization levels for\nthe MLP, and e) ASIC synthesis strategies. This\nmakes the dimension of the co-design space to\nbe:\n$C = N_{Qio} \u00d7 N_{AS} \u00d7 \\sum_{d=1}^{D}(N_w \u00d7 N_Q)^d$  (7)\nwhere NQio, NA\u201d,Nw,NQ, and D are the\nnumber of choices available for input/output\nquantization, ASIC synthesis strategies, percep-\ntrons in a layer, quantization levels in a layer,\nand the number of hidden layers, respectively.\nThe range of values used in the experiment\nand consequently the dimension of the co-design\nspace is shown in Table 1."}, {"title": "6 Results", "content": "In this experiment, we defined co-design objec-\ntives, including the validation loss, area, power,\nand delay estimates reported from the ASIC syn-\nthesis. Some objectives, such as loss, area, and\ndelay, are competitive, whereas others, like area\nand power, mostly overlap. There is no optimal\nsolution in multi-objective optimization scenar-\nios like this. Instead, a set of solutions exists\nwhere each is Pareto optimal\u2014improvement in\none objective necessitates a trade-off in another.\n6.1 Pareto Fronts\nThe 3D cross-section inf Fig. 4 reveals some in-\nsight into the search space explored during it-\nerations. Early iterations, represented by blue\npoints, are dispersed widely across the graph,\nindicating an initial exploration phase of the so-\nlution space. As the iterations advance, marked\nby a shift toward red points, there is a noticeable\nclustering of data points, suggesting that the op-\ntimization algorithm is converging toward a set\nof optimal solutions and also reveals the topol-\nogy of the front (we will continue this discussion\nin a later section). Meanwhile, the topology of\nthe Pareto-front is also shown in those clusters,\nwhich brings us to the next set of figures on 2D\ncross-sections (Figs. 5 - 7), which gives more in-"}, {"title": "6.2 Optimization Dynamics", "content": "Fig. 8 shows the moving average of the optimiza-\ntion objectives as the search progresses. The\nhistogram shows the number of Pareto optimal\nsolutions during that evolution. The histogram\nshows that increasing iterations consistently find\nPareto optimal points throughout the optimiza-\ntion. The Fig. 9 shows the kernel density esti-\nmate (KDE) of objectives in Pareto fronts during\ndifferent iterations.\nThere are several key observations from the\noptimization dynamics that can be gained from\nFig. 8 and Fig. 9. First, the search increasingly\ngoes from the region where loss is low to higher,\napparent in both Fig. 8 and the top-left of Fig. 9.\nThe second peak on the validation loss that is not\nbeing pushed further is also close to the baseline\nvalidation from a model that always predicts the\naverage of the training set. This is happening\nbecause no constraint minimal performance was\nset during this round of simulation. There is\na high sensitivity of area/power with the vali-\ndation loss, whereas delay is less susceptible to\nit, both of which were also hinted previously by\nFigs. 5-7. Kernel density estimates from Fig. 9\nindicate a successful focusing of the search to-\nwards promising regions, with area/power being\nmost challenging for the optimization to push\nlower. The dynamics of delay seen in both Fig. 8\nand Fig 9 clearly show that it benefited the most\nfrom the higher iteration of optimization."}, {"title": "6.3 Convergence", "content": "Hypervolume is one of the key metrics used\nto evaluate the dynamics of the Multi-Objective\nBayesian Optimization (MOBO). The hypervol-\nume measures the extent of the objective space\ncovered by the Pareto front, bounded by a refer-\nence point, which, in this context, is the worst-\ncase scenario for each objective. A larger hyper-\nvolume indicates superior solution quality, en-\ncompassing more objective space. This metric\nis also helpful as a convergence test during the\noptimization process.The hypervolume increases\nand eventually saturates in our experiments, as\nillustrated in the Figure 10. This saturation sug-\ngests that the optimization process in our exper-\niment has effectively converged to a stable set of\noptimal solutions.\nPareto Diversity and Spacing: Addition-\nally, we use Pareto diversity and spacing to char-\nacterize the effectiveness of our method. Pareto\ndiversity ensures that the optimization covers a\nbroad range of trade-offs, enhancing decision-\nmaking with diverse solutions. Meanwhile, spac-\ning between these points measures how uni-\nformly solutions are distributed, indicating a bal-\nanced solution space exploration. In Fig. 11, we\nobserve the evolution of these metrics in our ex-\nperiment. The normalized Pareto spacing de-\ncreases, showing an increase in uniformity among\nsolutions. Meanwhile, the Pareto size increases,"}, {"title": "6.4 Optimal Designs", "content": "The visual representation in Fig. 12 represents an\naverage Multi-Layer Perceptron (MLP) architec-\nture derived from a set of Pareto optimal solu-\ntions from optimization in a co-design space with\nover two billion possible configurations. This\nMLP model embodies the collective characteris-\ntics from various configurations, differing depth,\nwidth, and quantization levels for each layer.\nThe size and color of the perceptron represent"}, {"title": "6.5 Comparison with Theory", "content": "In this section, we do another experiment to\nquantify the distinction in our approach, where\nwe used a power estimate from ASIC synthe-\nsis instead of the theoretical estimate. Fig. 14a\nshows that the theoretical estimate of the energy\nshows a strong correlation with the power esti-\nmated from the ASIC synthesis. Still, the uncer-\ntainty of its accuracy can affect the search sig-\nnificantly. To quantify the effect of unquantified\nuncertainty in the theoretical estimate, we ran\nanother round of two-objective optimization:a a)\nArea-Energy optimization guided by a theoreti-\ncal estimate of energy given by QKeras, and b)\nPower-energy optimization guided by power es-\ntimated from logic synthesis.\nTo compare the effectiveness of the two ap-\nproaches, Fig. 14b shows the hypervolume evo-\nlution during the two optimization runs, with a\nred-dashed line showing the proxy-hypervolume\nused in theory-guided optimization. While the\nred curve shows an increase and saturation like\nthe synthesis-guided hypervolume, the plotting\nof 'real hypervolume' computed using power es-\ntimate from synthesis clearly shows that theory-\ndriven optimization saturates much earlier with\nvery little Pareto diversity. This empirical obser-\nvation is the first insight into the negative impact\nof theory-driven optimization for realistic ASIC\ndesign solutions, with more in our future works."}, {"title": "6.6 In-Pixel Implementable Neural\nNetwork", "content": "We applied constraints on the trials to iden-\ntify those suitable for in-pixel implementation,\nas shown in Table 2. We chose the area con-\nstraint of 250 x 250 \u00b5m\u00b2 based on the 130nm\nprocess design kit (PDK) used. The acceptable\npower density limit of 5 W/cm\u00b2 was set to en-\nsure feasible on-chip power consumption. The\ndelay constraint of 20 ps was derived from the\nclock frequency used in our design. The val-\nidation Mean Squared Error (MSE) threshold\nof 0.044837 corresponds to the MSE of a sim-\nple baseline model that predicts the mean of the\ntraining data. These constraints collectively de-\nfine a challenging yet realistic design space for\nin-pixel neural network implementation, pushing\nour co-design approach to find optimal solutions\nthat balance performance with hardware limita-\ntions.\nApplying the constraints to the trials reveals\na Pareto front with 54 in-pixel implementable,\nequally good choices. Figure 15 plots the op-\ntimization objectives in the constrained Pareto\nsurface. The x-axis represents the area utiliza-\ntion, which is the fraction of area occupied com-\npared to the constraint set. The y-axis shows the\npower density in W/cm\u00b2. The z-axis and color\nscale represent n, a performance metric of the\nmodel defined as the ratio of the baseline MSE\nto the model's MSE (MSE of the baseline / MSE\nof the model). The top plot illustrates the rela-\ntionship between area utilization, power density,\nand n. The surface shows a complex landscape\nwith multiple peaks and valleys, indicating the\ntrade-offs between these parameters. Generally,\nwe can observe that higher \u03b7 values (better per-\nformance) tend to correspond with higher power\ndensity and area utilization, as expected for more\ncomplex models. The bottom plot focuses on\n$f_{max} = \\frac{1}{Delay}$, which represents the maximum\nachievable frequency for each design point. This\nplot reveals a different pattern of trade-offs, with\npeaks in $f_{max}$ occurring at various combinations\nof area utilization and power density.\nTo further evaluate the effectiveness of using\ntheoretical estimates for guidance versus our ap-\nproach using ASIC synthesis, we identified the\nconstrained Pareto fronts in the two-objective\n(performance and power) optimization done in\n6.5. The figure 15 clearly demonstrates that uti-\nlizing synthesis-guided metrics in the optimiza-\ntion process revealed a constrained Pareto front"}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we presented a holistic approach\nto co-designing neural networks and ASICs for\nenabling in-pixel intelligence in radiation detec-\ntors. Our key contributions include an auto-\nmated pipeline integrating neural network de-\nsign and ASIC synthesis within a multi-objective\nBayesian optimization framework, use of real-\nistic ASIC synthesis metrics rather than theo-\nretical estimates to guide the optimization pro-\ncess for practical solutions, and exploration of\na specific large co-design space including neu-\nral architecture, quantization, and ASIC synthe-\nsis strategies. We demonstrated Pareto-optimal\ndesigns meeting strict area, power, and latency\nconstraints for in-pixel implementation.\nOur results show the benefits of using circuit-\nlevel metrics from ASIC synthesis compared to\ntheoretical estimates, which can lead to less ideal\nsolutions even in cases when such approxima-\ntions are available. This approach provides more\naccurate guidance towards implementable de-\nsigns.\nThere are several promising directions for fu-\nture work. Expanding the co-design space to\ninclude additional neural architectures beyond\nMLPs could yield more powerful in-pixel pro-\ncessing capabilities. Incorporating more detailed\ncircuit-level optimization choices into the co-\ndesign process may uncover additional efficiency\ngains. Exploring more emerging technologies\ncould push the boundaries of what's possible\nwith in-pixel processing. Finally, extending this\napproach to other scientific applications requir-\ning extreme edge AI could have broad impact\nacross various fields."}]}