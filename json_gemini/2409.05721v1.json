{"title": "Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding", "authors": ["Bram Willemsen", "Gabriel Skantze"], "abstract": "We propose an approach to referring expression generation (REG) in visually grounded dialogue that is meant to produce referring expressions (REs) that are both discriminative and discourse-appropriate. Our method constitutes a two-stage process. First, we model REG as a text- and image-conditioned next-token prediction task. REs are autoregressively generated based on their preceding linguistic context and a visual representation of the referent. Second, we propose the use of discourse-aware comprehension guiding as part of a generate-and-rerank strategy through which candidate REs generated with our REG model are reranked based on their discourse-dependent discriminatory power. Results from our human evaluation indicate that our proposed two-stage approach is effective in producing discriminative REs, with higher performance in terms of text-image retrieval accuracy for reranked REs compared to those generated using greedy decoding.", "sections": [{"title": "Introduction", "content": "A visually grounded dialogue is a conversation in which speakers refer to entities in a (shared) visual context. They do so by producing referring expressions (REs). The listener is expected to use the RE to identify the target entity, i.e., the referent. Whether the listener is successful in doing so depends on several factors, one being how specific the description of the referent was. With regard to specification, there exists a trade-off between discriminatory power and efficiency. On the one hand, the aim is to produce an unambiguous expression with which a referent can be successfully identified, whereas on the other hand a cooperative speaker is expected to make their contribution as economical as possible, while still avoiding ambiguity (Grice, 1975). To illustrate, consider the three phones depicted in Figure 1. If the intention of a speaker was to produce a description based on visual content that uniquely identified the phone second from the left, \"the phone with the QWERTY keyboard\" would be underspecified, as it applies to both the intended target as well as the right-most image. To avoid underspecification, additional content could be added to the RE, possibly resulting in a description such as \"the mostly black Nokia E75 mobile phone with the side-sliding QWERTY keyboard and keypad\". This RE does set apart the target from the distractors, but is overspecified, as the description contains more content than is strictly required for identification of the referent in this context, violating the Gricean maxim of quantity (Grice, 1975). In determining form and lexical content of REs, context plays a crucial role. We will again use Figure 1 to illustrate this by example. A attempts to draw the attention of B to a specific phone by referencing its brand name. However, since B recognizes two phones to be from this brand, B asks a clarification question that focuses on color. There are two things to note here. First, the REs produced by B, in particular \u201cthe black one\u201d, only work as discriminative references due to the mention of the brand name just prior, as \u201cone\" is here a proform of \u201cnokia\u201d (the right-most phone is also black). Second is the symmetry between the REs, showing conventional preservation of form.\nFor a conversational agent to take part in visually grounded dialogue, it would preferably generate REs in a similar, context-dependent manner, as this is expected by human conversational partners. The computational modeling of this process is the do-"}, {"title": "Related work", "content": "REG, as most NLG tasks, has been subject to a paradigm shift over the years. Whereas earlier methods were mostly symbolic (e.g., Appelt, 1985; Dale and Reiter, 1995; Krahmer and Theune, 2002), most approaches proposed in more recent years are based on neural models (e.g., Mao et al., 2016; Luo and Shakhnarovich, 2017; Panagiaris et al., 2021; Sun et al., 2023). Contemporary NLG research frequently incorporates large language models (LLMs), predominantly those that are Transformer-based (Vaswani et al., 2017). A common approach to modeling downstream NLG tasks is domain adaptation via transfer learning. This is typically achieved by fine-tuning a pre-trained LLM on a task-specific dataset.\nAlthough the bulk of the computation for most downstream tasks has been delegated to the pre-training of the base model, fine-tuning may still require significant computational resources. To combat this issue, parameter-efficient fine-tuning methods have been proposed, such as Low-Rank Adaptation (LoRA, Hu et al., 2022). By freezing the pretrained model weights and instead training rank decomposition matrices that have been added to the dense layers of the network, LoRA manages to reduce the number of trainable parameters by several orders of magnitude, often without considerable adverse effects to downstream performance.\nAside from language, Transformers have shown promising results when it comes to modeling other modalities (e.g., Dosovitskiy et al., 2021; Radford et al., 2023). Of particular interest here are multimodal models that combine vision and language. VLMs such as CLIP (Radford et al., 2021) have learned to jointly embed both modalities via contrastive pretraining objectives. Their learned representations have shown to be useful for discriminative downstream vision-language tasks, such as text-image retrieval (TIR). We will hereafter refer to these models as discriminative VLMs. Other VLMs such as Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023), Kosmos-2 (Peng et al., 2024), LLaVA (Liu et al., 2023), and InternVL (Chen et al., 2024) have been introduced to address generative downstream tasks, such as image captioning and (multi-turn) visual-question answering. These generative VLMs, sometimes called multimodal LLMs (MLLMs), are able to autoregressively output text based on multimodal inputs,"}, {"title": "Method", "content": "In this work, we focus on generating REs conditioned on a multimodal dialogue context for referents that are represented by independent images. This setting bares some resemblance to that of discriminative image captioning (see e.g., Vedantam et al., 2017; Cohn-Gordon et al., 2018; Sch\u00fcz et al., 2021). REG more commonly attempts to describe objects or entities, represented by bounding boxes or segmentation masks, in single images or scenes. Spatial relations frequently become part of distinguishing descriptions in such settings as a result. Our method, however, focuses instead on generating REs based on visual content in situations that have been specifically designed for this to be challenging. We leave extending the framework to incorporate spatial relations to future work.", "sections": [{"title": "Task description", "content": "For a given referent, which is represented by an image (or images), the aim is to generate an RE (1) with which the referent can be identified and (2) which is discourse-appropriate."}, {"title": "Proposed approach", "content": "Broadly speaking, we propose a framework that consists of two components, namely a REG model and a REC model. For a visualization of this framework, see Figure 2. We approach REG as a causal language modeling problem. More specifically, we use a generative VLM that has been pretrained to handle arbitrarily interleaved sequences of text and images (Alayrac et al., 2022; Lauren\u00e7on et al., 2023) in order to condition the autoregressive generation of REs on a preceding visio-linguistic context. For the experiments presented in this paper, the generative VLM we use is IDEFICS (Lauren\u00e7on et al., 2023), an open-source implementation of Flamingo (Alayrac et al., 2022). By fine-tuning IDEFICS on visually grounded dialogue data, our aim is to satisfy the second constraint of the task, i.e., generating REs that are a good fit for the projected use context. In order to ensure the generated REs satisfy the first constraint, we evaluate their discriminatory power using a REC model. Crucially, as part of a generate-and-rerank strategy, we propose discourse-aware comprehension guiding. The motivation for the use of a discourse-aware REC model to score discriminatory power comes from the context-dependence of this property, as some REs will need to be resolved to their coreferences in order to be disambiguated and understood to be adequate mentions. For the experiments presented in this paper, we base our REC model on the conversational referent description generator (CRDG) framework of Willemsen et al. (2023)."}, {"title": "Multimodal conditioning with IDEFICS", "content": "IDEFICS is a generative VLM based on the Flamingo VLM architecture (Alayrac et al., 2022). Flamingo was introduced to handle various open-ended vision-language tasks that carry an NLG objective, with a noted focus on using few-shot multimodal in-context learning (ICL) to accomplish them. Flamingo builds on pretrained vision and language models, bridging these modalities in order to incorporate visual information in the process of predicting the next token. To condition the autoregressive generation of text on both text and images, gated cross-attention dense layers that are trained from scratch are interleaved between the frozen layers of a pretrained LLM. Images are encoded using a pretrained vision model, after which the resulting embeddings go through a process of Perceiver-based (Jaegle et al., 2021) resampling in order to encode the high-dimensional visual feature representations as fixed numbers of so-called visual tokens. The model cross-attends to this output from the resampler in order to incorporate the visual information into its predictions, enabling the modeling of text interleaved with images.\nTo use IDEFICS for our purpose, we simply take the available linguistic context, indicating with speaker tokens the identity of the speaker for each message in the dialogue history, and add the image representing the referent to the sequence in the position at which we want to generate an RE. For reference, see step 1 in Figure 2."}, {"title": "Comprehension guiding with the CRDG", "content": "Willemsen et al. (2023) frame reference resolution in visually grounded dialogue as a TIR task. They note, however, that current discriminative VLMs, typically assume that the text is descriptive of the image. As REs in dialogue can take various forms besides definite descriptions, being able to resolve coreferences, including pronouns, is often a prerequisite for successful identification of a referent. For this reason, they proposed fine-tuning a causal LLM to generate so-called referent descriptions. Referent descriptions distill all available coreferential information in the linguistic context of a given mention into a single (definite) description of the referent. These referent descriptions can then be used by a pretrained VLM to identify referents via (zero-shot) TIR. To illustrate, consider again the REs in Figure 1. If we were to attempt TIR directly with the RE \"the black one\u201d, the description is ambiguous, applying to both the target and a distractor. If we instead use its referent description \"the black nokia\", which combines information from all mentions of the referent in the available linguistic context, we now have a distinguishing description. This shows how the linguistic context is crucially important in resolving an otherwise seemingly underspecified RE and how the CRDG can resolve references regardless of form.\nWhile this framework was originally intended for REC in conversation, we propose to repurpose it as a comprehension-guiding model for REG in visually grounded dialogue. To evaluate candidate REs generated by our REG model based on their discriminatory power, we insert the candidate RE into the dialogue segment at the position at which it was generated by the REG model, marking its beginning and end in text. We then use the CRDG to autoregressively generate for this candidate RE a referent description based on the provided dialogue segment. For reference, see step 2 in Figure 2. The generated referent description is then encoded with a discriminative VLM to get a text embedding. We then compute representational similarity between this text embedding and the image embeddings of the candidate referents to rank the candidate RES. For reference, see step 3 in Figure 2. Note that the referent descriptions are only used in the process of guiding the selection of candidate REs.\nCandidate reranking Although it makes intuitive sense to deem the candidate RE that has the most discriminatory power according to the REC model to be the best available candidate, this is not necessarily always true. To clarify, consider the following: if we were to simply opt for the candidate RE that has, among the candidates, the highest probability assigned to the target image via softmax, we may be selecting an RE based of a referent description that the VLM considers to be most similar to the target image when accounting for the distractors, but that is not in itself a good description of any of the images. Despite low similarity between the images and the description in absolute terms, the relative difference just so happens to be large and in favor of the target image. As a result, we would likely be selecting a suboptimal RE.\nFor this reason, we propose to select candidate REs not just based on their text\u2192image matching (TIM) score, but rerank them based on both their TIM and image text matching (ITM) scores: here, the TIM score indicates to what extent the candidate RE describes the target image with respect to the distractor images; the ITM score indicates to what extent the candidate RE describes the target image with respect to the other candidate REs. Note that each candidate RE is represented by its referent description, as generated by the CRDG, when these scores are computed. We combine the scores by way of linear opinion pooling (see e.g., Jacobs, 1995), taking a weighted linear combination of the log softmax of the TIM and ITM logits. For each candidate RE we calculate its pooled score, $S$, as follows:\n$S_i = w_a \\cdot ln(a_i + \\epsilon) + w_b \\cdot ln(b_i + \\epsilon)$", "where": "where, for each $i$-th candidate RE, $a$ and $b$ represent its TIM and ITM softmax probabilities, respectively, each $w$ the coefficient by which $a$ and $b$ are scaled, and $\\epsilon$ a small constant that is added to avoid taking the (theoretical) log of 0. The coefficients sum to 1. We select the candidate RE with the highest $S$ for the target image. We describe a hypothetical case in Appendix A to further illustrate the rationale behind this weighted reranking."}]}, {"title": "Experiments", "content": "The dialogues used in our experiments come from the visually grounded dialogue task A Game Of Sorts (AGOS, Willemsen et al., 2022). In this \"game\", two players are presented with a set of nine images that they are asked to rank-one at a time-based on a given sorting criterion. To complete the task, they will have to agree on a ranking which they deem satisfactory. The game is played over multiple rounds with the same set of images to ensure repeated mentions of the same referents. Although the players see the same set of images, they cannot see each other's perspective. The position of the nine images on screen is randomized, forcing the players to refer to the images based on their visual content. The task was specifically designed to encourage discussions and imposes no restrictions on message content. As a result, the referring language comes embedded in considerably longer and more diverse conversations compared to those from related work. Willemsen et al. (2022) collected 15 dialogues in total: three dialogues for each one of five image categories. Images from the same set were selected to have overlapping visual attributes, in order to further complicate the production of discriminative REs. Due to the deliberate challenges to the referential process and the relatively unconstrained nature of the dialogues, the task can be considered a challenging test bed for the grounding and generation of REs in conversation.\nFor fine-tuning and evaluation of both REG and REC models, we require dialogues with REs annotated. For this purpose, we use the span-based mention annotations for AGOS from Willemsen et al. (2023). These annotations indicate the start and end of all the mention spans found in the dialogues, and the image, or images, to which they refer. We will consider these human-produced REs to be the ground truth for our study.", "sections": [{"title": "Data", "content": "The dialogues used in our experiments come from the visually grounded dialogue task A Game Of Sorts (AGOS, Willemsen et al., 2022). In this \"game\", two players are presented with a set of nine images that they are asked to rank-one at a time-based on a given sorting criterion. To complete the task, they will have to agree on a ranking which they deem satisfactory. The game is played over multiple rounds with the same set of images to ensure repeated mentions of the same referents. Although the players see the same set of images, they cannot see each other's perspective. The position of the nine images on screen is randomized, forcing the players to refer to the images based on their visual content. The task was specifically designed to encourage discussions and imposes no restrictions on message content. As a result, the referring language comes embedded in considerably longer and more diverse conversations compared to those from related work. Willemsen et al. (2022) collected 15 dialogues in total: three dialogues for each one of five image categories. Images from the same set were selected to have overlapping visual attributes, in order to further complicate the production of discriminative REs. Due to the deliberate challenges to the referential process and the relatively unconstrained nature of the dialogues, the task can be considered a challenging test bed for the grounding and generation of REs in conversation.\nFor fine-tuning and evaluation of both REG and REC models, we require dialogues with REs annotated. For this purpose, we use the span-based mention annotations for AGOS from Willemsen et al. (2023). These annotations indicate the start and end of all the mention spans found in the dialogues, and the image, or images, to which they refer. We will consider these human-produced REs to be the ground truth for our study."}, {"title": "Evaluation", "content": "We focus on evaluating single-image referents, however noting that, in principle, our proposed framework can be extended to the multi-image referent case. We adopt the cross-validation protocol used by Willemsen et al. (2023), where the AGOS dataset is partitioned along the five image sets: for each run, twelve dialogues from four image sets are used for training, and the three dialogues of the remaining image set are used for testing. We limit the context window of the dialogue to the previous seven messages for model-based experiments, and report TIR results based on the reduced visual context, i.e., not considering ranked images to be part of the candidate referents."}, {"title": "Metrics", "content": "We score the referent descriptions generated by the CRDG based on their similarity to the manually constructed ground truth labels using the same metrics as reported in Willemsen et al. (2023), i.e., the Jaccard index, BLEU (based on unigrams and bigrams) (Papineni et al., 2002), ROUGE-L (Lin, 2004), and cosine similarity between text embeddings (CosineTT). When comparing generated REs against ground truth mentions, we compute unigram-based BLEU, ROUGE-L, and cosine similarity between text embeddings (CosineTT). We report TIR performance in terms of top-1 accuracy, mean reciprocal rank (MRR), normalized discounted cumulative gain (NDCG), and cosine similarity between referent description text embeddings and target image embeddings (CosineTI). Model-based TIR results reflect the zero-shot performance of the discriminative VLM as it is used in the CRDG framework. This VLM is also used to-"}, {"title": "Human", "content": "In order to externally validate our model-based experimental results, we conduct a human subjects experiment to evaluate human TIR performance for generated REs and to compare these results to those for the ground truth. Following Willemsen et al. (2023), participants are shown the REs in the context of the unfolding dialogue. We, however, show the dialogue up until the end of the current RE for which the participant is asked to provide an answer. We evaluate with the reduced visual context. For more details, see Appendix B."}, {"title": "Comparisons", "content": "Given the focus on multimodal ICL with Flamingo (Alayrac et al., 2022), we evaluate the n-shot performance of IDEFICS in addition to its (LoRA) fine-tuned performance. We compare these variants based on outputs generated using greedy decoding. For details about the selection of support examples for ICL, see Appendix C. Further experiments use the fine-tuned variants of the model. To generate multiple candidate REs, we use beam search with a width of 6. We examine how our proposed approach using weighted reranking (Rerank), which selects candidates based on their pooled score, compares against ablated versions of the method. We contrast performance with a variant that selects the candidate with the most discriminatory power (Max disc.) and a variant without any guiding that simply selects the top beam hypothesis (Top-1). We deliberately focus on evaluating different versions of the proposed framework, as, to the best of our knowledge, existing REG models are ill-suited to handle the AGOS task setting or principally do not satisfy our discourse-appropriateness criterion. For instance, if we were to use as a baseline a model that would invariably generate context-independent, but overspecified or caption-like REs-such as discussed in Section 1 in relation to the example based around Figure 1-these may result in high TIR accuracy, but, even so, will virtually never be discourse-appropriate."}, {"title": "Implementation details", "content": "Similar to Willemsen et al. (2023), we obtain the CRDG by fine-tuning GPT-3-although davinci-002 instead of the davinci base model-using the OpenAI API. Crucially, however, our version of the CRDG is incremental as opposed to message-based. We use InternVL (Chen et al., 2024), specifically InternVL-G, as our discriminative VLM within the CRDG framework. With regard to the reranking of candidate REs, although we could treat the coefficients as learnable parameters, we instead simply set $w_a$ to and $w_b$ for the TIM and ITM scores, respectively, as we believed this to represent a reasonable trade-off between the scores for our purpose. All experiments reported in this paper that involve IDEFICS are based on the 80 billion parameter variant. We use quantized LoRA (QLoRA, Dettmers et al., 2023) for parameter-efficient fine-tuning. We modify the loss calculation by masking the loss for all tokens but the RE. We estimate, without exhaustive search, hyperparameters for IDEFICS fine-tuning using nested five-fold cross-validation. For additional details, including IDEFICS and GPT-3 hyperparameters, see Appendix D."}]}, {"title": "Results", "content": "Our results are based on 1305 of the 1319 annotated mentions of single-image referents; 14 samples were excluded as their target referents were not part of the set of candidate referents as a consequence of evaluating with the reduced visual context. Table 5 shows REs from different sources for a few dialogue samples.\nIncremental CRDG Table 1 shows the performance of the CRDG on the ground truth data. We managed to closely replicate the results reported by Willemsen et al. (2023) despite our variant of the CRDG being incremental.\nMultimodal ICL vs. fine-tuning In Table 2 we show results for candidate REs generated using greedy decoding with 1-, 2-, 4-, and 8-shot multimodal ICL and with the fine-tuned model. We found that a single example tended to be enough for the model to generate an RE, in accordance with the provided task. Adding an additional example improved performance slightly, but further increasing the number of support examples hurt performance instead. Moreover, the metrics showed a notable gap between ICL and fine-tuning, with fine-tuning averaging higher scores across the board.\nAblations Shown in Table 3 are results of the three strategies for candidate selection after beam search. With the exception of text-image cosine similarity, we observed slightly lower scores for the TIR metrics for the reranked REs in comparison with those that had the most discriminatory power. This was expected, as we actively went against taking the most discriminative candidate with our weighted reranking, which, our results suggested, did lead to higher representational similarity, on average, between referent descriptions and target images. These differences were, however, marginal.\nHuman performance We validated our model-based experimental results through human evaluation, results of which are shown in Table 4. We collected one data point per dialogue, meaning 15 data points per source of RE listed, for a total of 45 data points from 38 different participants. We contrasted TIR accuracy for REs generated with fine-tuned IDEFICS with that of ground truth mentions. We found that, although lagging behind the ground truth, the generated REs, regardless of the exact strategy, showed strong performance, far exceeding chance level (which was roughly 22%). Although both tested model-based RE variants seemed effective, our reranked REs resulted in higher accuracy than those based on greedy decoding.\nRE length We found that REs generated by our (fine-tuned) REG model tend to be shorter, on average, than the ground truth mentions. This is one indicator of our model not having been prone to generating overspecified REs, which would otherwise have had the potential to artificially inflate accuracy scores. A comparison between the average length of the generated REs and the ground truth is visualized in Figure 4 in Appendix E.\nRE content When examining the ground truth REs, we found that more than 20 percent of the included mentions contain no words that were descriptive of visual content (e.g., \u201cit\u201d, \u201cthat one\u201d), with the pronoun \u201cit\u201d accounting for roughly half of these REs. We found that such REs were selected at a similar rate when using our weighted reranking schema. It is worth nothing, however, that whenever both the ground truth and selected candidate REs contained no content words, their forms would, at times, differ (e.g., \u201cit\u201d having been selected where the ground truth was \u201cthat one\u201d)."}, {"title": "Discussion", "content": "In this paper, we explored the problem of REG in visually grounded dialogue. Our aim was to realize the generation of REs that were not only discriminative, but also appropriate for the dialogue context in which they would be used. We proposed to approach the problem from a causal language modeling perspective, where the generation of tokens would be conditioned on both text and images. By fine-tuning a generative VLM, IDEFICS (Lauren\u00e7on et al., 2023), we showed it is possible to generate REs that are indicative of the referent while suitable for the dialogue context. Notably, we were successful using a parameter-efficient fine-tuning approach (Dettmers et al., 2023) and while having relatively limited data for training (Willemsen et al., 2022). In addition, we introduced discourse-aware comprehension-guiding to evaluate whether candidate REs are discriminative given their linguistic context. By adding candidate REs to the dialogue for which they were generated, we were able to use the CRDG framework of Willemsen et al. (2023) to score candidate REs on their discourse-dependent discriminatory power. Finally, we showed that human TIR accuracy using candidate REs selected based on a weighted reranking of scores derived from this discourse-aware REC model was on average higher than for candidate REs generated through greedy decoding.\nOne of the main benefits of our approach is the ability for the REG model to generate REs that"}, {"title": "Limitations", "content": "The experiments reported in this paper were based solely around modeling the English language; it is of yet unclear whether our results would transfer to other languages. We have focused on a single, relatively small dataset for which the annotations required by our approach were available; acquiring similar annotations for other, bigger datasets would be relatively costly. We have experimented with only one generative VLM for this paper; as a result, we do not know to what extent our findings generalize to other generative VLMs. We have used a closed-source API-based method for fine-tuning of the CRDG; consequently, we are not able to make the model weights publicly available, nor is the fine-tuning process transparent. The current iteration of the CRDG is unimodal, whereas the task of resolving references in visually grounded dialogue is inherently multimodal; this limits the maximally achievable performance. Our approach is modular and, as such, likely to be affected by error propagation; a bottleneck is the CRDG framework if it overvalues inadequate candidates (false positives) or undervalues adequate ones (false negatives) with respect to their discriminatory power. We currently operate on the assumption that utterance planning has been delegated to another system; this is a complex problem and challenging to solve properly, but will likely ultimately require a more unified approach that implicitly includes REG."}]}