{"title": "ADAMANIP: ADAPTIVE ARTICULATED OBJECT MANIPULATION ENVIRONMENTS AND POLICY LEARNING", "authors": ["Yuanfei Wang", "Xiaojie Zhang", "Ruihai Wu", "Yu Li", "Yan Shen", "Mingdong Wu", "Zhaofeng He", "Yizhou Wang", "Hao Dong"], "abstract": "Articulated object manipulation is a critical capability for robots to perform various tasks in real-world scenarios. Composed of multiple parts connected by joints, articulated objects are endowed with diverse functional mechanisms through complex relative motions. For example, a safe consists of a door, a handle, and a lock, where the door can only be opened when the latch is unlocked. The internal structure, such as the state of a lock or joint angle constraints, cannot be directly observed from visual observation. Consequently, successful manipulation of these objects requires adaptive adjustment based on trial and error rather than a one-time visual inference. However, previous datasets and simulation environments for articulated objects have primarily focused on simple manipulation mechanisms where the complete manipulation process can be inferred from the object's appearance. To enhance the diversity and complexity of adaptive manipulation mechanisms, we build a novel articulated object manipulation environment and equip it with 9 categories of objects. Based on the environment and objects, we further propose an adaptive demonstration collection and 3D visual diffusion-based imitation learning pipeline that learns the adaptive manipulation policy. The effectiveness of our designs and proposed method is validated through both simulation and real-world experiments.", "sections": [{"title": "1 INTRODUCTION", "content": "Among the various categories of objects in our daily life, articulated objects are highly significant as they are common in our surroundings (such as cabinets, doors, and laptops) and their components are complex, featuring rich and diverse geometries, semantics, articulations, and functions. Therefore, learning articulated object representation and manipulation are essential while challenging for future robots in home-assistant tasks.\nAmong articulated object manipulation tasks, door manipulation is first and most thoroughly studied, as doors are most common and useful in our daily lives. Afterwards, with the release of diverse articulated object manipulation datasets and environments, various manipulation tasks (like opening, sliding, rotating, and further language-guided manipulation) on many categories of articulated objects (such as pots, lamps, and cabinets) have been studied.\nWhile previous covered various aspects of articulated object manipulation, one of the most essential features of articulated objects, the mechanisms of different parts and articulations for accomplishing the final manipulation goal, has yet to be explored. For example, a safe can be directly opened by pulling the door in previous environments, while in the real world, the robot may have to first turn the key to unlock the latch, and then pull open the door. While UniDoorManip proposes an environment with the corresponding dataset that can simulate the mechanisms of doors,\nthe mechanisms of various types of articulated objects could be much more diverse and complicated. Therefore, we build an environment that can simulate the above-described complex mechanisms of articulated object manipulation, equipping this environment with 9 categories of different objects covering 5 types of adaptive mechanisms (details described in Section 3).\nThe different mechanisms of articulated objects call for two core capabilities of the policy: (1) multi-modal action proposal and (2) adaptive manipulation from history actions. For an observed object, the manipulation policy may contain multiple modes, including different manipulation trajectories. For example, when observing the safe with its door closed, to achieve the goal of opening the door, the policy could be either directly pulling the door (when the door is unlocked) or turning the key and then pulling the door (when the door is locked), and the method should be able to model these modalities from the same visual observation.\nFurthermore, to identify and execute the accurate action from the multi-modal manipulation action candidates, it is necessary to adapt the manipulation policy based on previous actions and their corresponding outcomes. For example, when pulling the door results in no movement, the Adaptive policy should adapt from proposing multi-modal actions (either pulling the door or turning the key) to single-modal action (turning the key).\nTo support multi-modal action proposals, we take advantage of the designs of diffusion policy and its following studies, which have demonstrated modeling multi-modal distributions from only a few successful demonstrations. To empower the policy with adaptive manipulation abilities, while previous studies only collect optimal success trajectories (without any failures during the manipulation) for training, we introduce trajectories including failure actions and the recovery and adaptation actions from failures for training, as the failure actions help in revealing the accurate mechanisms and manipulation policy. Figure 1 showcases the superiority of our proposed adaptive policy learning method. The pure visual observation could not tell whether the door was locked or not. The static policy that only takes the passive one-frame visual input will randomly propose one of the multi-modal trajectory candidates. On the contrary, the adaptive policy will first try pulling the door, and then adapt the policy distribution from multi-modal to single-model accordingly, as its training data include the"}, {"title": "2 RELATED WORK", "content": "2.1 ARTICULATED OBJECT ENVIRONMENTS AND DATASETS\nTo facilitate the study of representation and manipulation of diverse and complex articulated ob- jects, DoorGym , a door manipulation environment is first introduced with diverse doors. UniDoorManip further empowers door environments with different mechanisms. PartNet-Mobility dataset first introduces multiple categories of articulated objects from PartNet , integrated with the sapien environment to support various articulated object manipulation tasks. Further, GAPartNet provides fine-grained part annotations, AKB-48 provides real-world articulated object models, and Arnold provides the environment for language-guided manipulation.\n2.2 ARTICULATED OBJECT MANIPULATION\nThere have been a series of studies studying articulated object manipulation. Where2Act first studies the point-level affordance for short-term manipulation, with affordance-based , flow-based , part-based and rl-based methods study the long-horizon manipulation. Environment-Aware Affordance further studies the manipulation with environment constraints. Where2Explore and AdaAfford study converting passive visual priors to manipulation posteriors using the few-shot interactions, which tackle the problem of exploring novel articulated object categories with novel geometries and parts, and manipulation on ambiguous kinematics and dynamics. Besides, coarse-to-fine method studies the sim2real framework for real-world manipulation, and language-guided methods explore the manipulation with language guidance. While these works mainly investigated the manipulation with simple mechanisms (such as directly opening a door or safe), in our work, we further study the policy for manipulating articulated objects with diverse and complex mechanisms, with a novel proposed environment supporting such objects."}, {"title": "3 ADAPTIVE MANIPULATION ENVIRONMENT", "content": "Previous datasets and simulation environments for articulated objects often lack diversity and realistic manipulation mechanisms. To address this issue, we developed a new environment to better explore complex mechanisms in articulated object manipulation and learn adaptive manipulation policies. Based on IsaacGym, this environment simulates these mechanisms and includes 9 categories of objects (Section 3.1) with 5 types of adaptive mechanisms (Section 3.2).\n3.1 ARTICULATED OBJECT DATASET\nIn recent years, several works have proposed datasets for articulated object manipulation. PartNet- Mobility and AKB-48 offer diverse datasets for articulated"}, {"title": "3.2 ADAPTIVE MANIPULATION MECHANISM", "content": "Most existing articulated object environments focus primarily on geometric diversity across different categories of objects. While these objects may contain multiple parts, manipulating one part typically does not impact other parts' state or joint limits, resulting in simplified manipulation mechanisms. Common actions in these environments include pushing or pulling a part, such as opening a drawer or pressing a button, which can be deduced purely from visual observation. However, real-world manipulation often depends on internal joint states that are not visible externally, necessitating adaptive manipulation policies based on feedback.\nTo better simulate real-world articulated object manipulation as well as corresponding mechanisms, we have identified five adaptive mechanisms that enhance the fidelity of our environment:\nLock Mechanism: Common in everyday objects like doors or safes, the lock mechanism requires an initial action such as rotating a key or knob or pressing a button to unlock the object before it can be opened. This mechanism tracks the key part's joint state during manipulation and updates the lock state accordingly. If the lock state transitions to \"unlock\", the door joint limit is lifted to allow opening; otherwise, the door remains locked. Since the lock state cannot be inferred visually, the robot must interact with the object to determine the lock state and adapt its policy accordingly.\nRandom Rotation Direction: When rotating a knob, cap, or handle, the direction (i.e., clockwise or counterclockwise) is determined by the internal revolute joint limit. Our environment randomly assigns the rotation direction upon initialization, preventing visual inference of the direction. The robot must attempt one direction and switch if unsuccessful.\nRotate & Slide Mechanism: This mechanism requires a part to be rotated to a specific angle before it can be lifted or pulled out, such as lifting the lid of a pressure cooker. The required rotation angle is not visually discernible, necessitating the robot to rotate the part incrementally and attempt to slide it"}, {"title": "4 \u041c\u0415\u0422\u041dOD", "content": "As illustrated in Figure 3, we propose a novel framework that learns an adaptive manipulation policy for various mechanisms from collected adaptive demonstrations. To achieve this, we leverage the annotated part poses in our dataset to generate expert manipulation trajectories in the environments, considering invisible internal states to ensure the trajectories are adaptive. Next, to model the expert trajectory distribution with high multi-modality, we employ 3D visual diffusion-based imitation learning , which learns the gradient of the action score function to generate actions.\n4.1 ADAPTIVE DEMONSTRATION COLLECTION\nOur goal is to generate adaptive demonstrations that are optimal under partial observation. For instance, when opening a microwave, the expert adaptive policy initially pulls the handle to check the lock state. If the latch is locked, the policy will push the button before opening the door. If the latch is not locked, the policy continues pulling the handle to open the door. In contrast, a static policy"}, {"title": "4.2 3D DIFFUSION-BASED ADAPTIVE POLICY LEARNING", "content": "Given the collected demonstration dataset D = {(Ot, at)}, we aim to learn a policy that models the conditional distribution P(At|Ot, At). Here At refers to the predicted action sequence At = (at, ..., at+Ta), where Ta is the action horizon. Ot refers to the observation history, including 3D point clouds and proprioception states, Ot = (ot-To, \u2026\u2026\u2026, ot), where To is the history horizon. At refers to action history, \u00c2t = (at-To\u22121, ..., at\u22121).\nHowever, conducting imitation learning on D is challenging due to its multi-modal nature: The ambiguity of the internal states of articulated objects results in multiple successful manipulation trajectories under the same visual observation. Thanks to recent progress in diffusion-based meth- ods , we can better fit the multi-modal distribution by learning the action score function.\nFollowing the Diffusion Policy , we utilize DDPM to estimate the conditional distribution P(At|Ot, \u00c2t). The DDPM scheduler performs K iterations of denoising"}, {"title": "5 EXPERIMENTS", "content": "5.1 SETTINGS AND METRIC\nWe conduct experiments in the category level covering all the 9 object categories, and collect 20 adaptive manipulation demonstrations for each object as the training data. For evaluation metric, we use success rate of manipulations. To evaluate what kind of adaptive demonstrations is the most beneficial to adaptive policy learning, we train adaptive policies on adaptive demonstrations with different numbers of adaptive trials."}, {"title": "5.3 SIMULATION RESULTS", "content": "Table 3 and Figure 5 respectively show the success rates and manipulation trajectories of different methods. Figure 4 further shows more manipulation trajectories proposed by our method. Our method outperforms all other methods and demonstrates stable and accurate action trajectories for adaptive manipulation. From the visualizations, we can observe that, for VAT-Mart, as an open-loop method, it is difficult to fit the whole trajectory space and directly predict a manipulation trajectory at a time, and the open-loop method does not support adapting the policy from previously executed actions. Using the diffusion-based imitation method, our method can more accurately model the poses of actions using the limited number of data (such advantage is also demonstrated in other diffusion-based imitation studies ). In contrast, AdaAfford suffers from the inferior multi-modal distribution modeling capability of CVAE and the increased training data requirements associated with point-level affordance learning. The Sampling-based method performs worse than AdaManip and other baselines because its sampling process because its sampling process cannot efficiently leverage priors or posteriors learned from demonstrations. While ACT and DP3 outperform affordance-based methods, they fall short of AdaManip due to the absence of an adaptive demonstration collection pipeline. Additionally, ACT is further limited by its lack of the robust multi-modality modeling capability provided by diffusion models.\nOurs w/o adaptive is not trained from demonstrations with recovery actions from failure trials, so the learned policy could not adapt from failure actions to finally achieve the goal. As illustrated in Figure 5, when the safe is locked and Ours w/o adaptive attempts to open the door, it fails to switch to rotating the knob and instead continues on the opening trajectory.\nTable 4 shows the effects of repeated adaptive trials in adaptive demonstrations for training. When only using optimal successful actions (i.e., 0 adaptive trials) under full observation, the model is the same with Ours w/o adaptive and could not have the adaptation capability. When using more than one adaptive trial at the same object state, these adaptive trials are redundant and increase the complexity of distributions to model, while not increasing the scenarios the policy can handle. Therefore, the performance decreases when the number of repeated adaptive trials increases from 1. These results validate our demonstration collection design, which limits the manipulation sequence to only one adaptive trial."}, {"title": "5.4 REAL-WORLD EXPERIMENTS", "content": "To validate the generalization of our adaptive diffusion policy to real-world scenarios, we conduct experiments on various real-world objects like Pressure Cooker, Microwave, Bottle, and Safe. For the real-world settings, we employ a Franka Emika Panda Robot Arm as our agent. To capture 3d visual observation, we position an Azure Kinect DK camera adjacent to the robot arm. Our policy takes the real-time point clouds from the depth camera, the robot state from the robot arm, and previous actions as the input, and generates the corresponding end-effector action in a close-loop fashion. We collected 35 adaptive expert demonstrations for each object by human teleoperation to train the policies, and conducted 10 evaluation trials per object.\nTable 5 presents the number of successful executions across four real-world tasks. The results in Figure 6 demonstrate that our adaptive policy can be effectively applied to real-world scenarios. To"}, {"title": "6 CONCLUSION", "content": "We study the problem of adaptive manipulation policy for manipulating articulated objects with diverse and complex mechanisms, build environments with different categories of such objects that support the various manipulation mechanisms, and propose a novel framework that learns the adaptive manipulation policy for various mechanisms from diverse adaptive demonstrations based on diffusion policy. The significance of our proposed environment and the effectiveness of the proposed adaptive policy learning framework have been demonstrated by our experiments.\nOur paper represents the initial study into the environment suitable for adaptive manipulation policy learning. Currently, our dataset encompasses 9 categories with a total of 277 objects, which we plan to expand by introducing more categories and instances to cover increasingly complex and realistic mechanisms. Additionally, incorporating deformable object manipulation into adaptive tasks represents a significant direction for future research. Moving forward, the AdaManip environment will progressively include a broader spectrum of real adaptive manipulation tasks, making it a comprehensive platform for both training and testing adaptive manipulation policies."}]}