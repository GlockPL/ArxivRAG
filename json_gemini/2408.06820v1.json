{"title": "Efficient Search for Customized Activation Functions with Gradient Descent", "authors": ["Lukas Strack", "Mahmoud Safari", "Frank Hutter"], "abstract": "Different activation functions work best for different deep learning models. To exploit this, we leverage recent advancements in gradient-based search techniques for neural architectures to efficiently identify high-performing activation functions for a given application. We propose a fine-grained search cell that combines basic mathematical operations to model activation functions, allowing for the exploration of novel activations. Our approach enables the identification of specialized activations, leading to improved performance in every model we tried, from image classification to language models. Moreover, the identified activations exhibit strong transferability to larger models of the same type, as well as new datasets. Importantly, our automated process for creating customized activation functions is orders of magnitude more efficient than previous approaches. It can easily be applied on top of arbitrary deep learning pipelines and thus offers a promising practical avenue for enhancing deep learning architectures.", "sections": [{"title": "Introduction", "content": "Nonlinearities are an indispensable component of any deep neural network. The design choice of these so-called activation functions has proven to crucially affect the training dynamics and final performance of neural networks.\n\nThe rectified linear unit (ReLU) is the most commonly used activation due to its simplicity and consistent performance across different tasks. However, it took several years of empirical research [15, 20, 27] before it was widely adopted by practitioners as an activation function in deep neural networks.\n\nDespite the desirable properties of the ReLU, other alternatives have been introduced [23, 16, 9, 18, 13, 24], each with their own theoretical or empirical justification, to address potential issues associated with the ReLU, such as the dying ReLU problem [34, 1]. These alternative activations, which are mostly variations of ReLU, lead to performance improvements in particular settings, although none is as widely adopted yet.\n\nAs evidenced by previous research, manually designing an activation function that suits a certain task is highly non-trivial and established choices (such as ReLU, GELU and SiLU) are made possibly at the cost of losing (optimal) performance. Automated search methods have been previously employed to learn activation functions (see Section 2 for details), but existing methods require thousands of function evaluations and have thus not been adopted widely in practice. If it was possible to design a customized activation function for the problem at hand for the same cost as evaluating some standard alternatives (e.g., ReLU, GELU and SiLU) while yielding better performance, this would be quickly adopted by the community. That is the goal of our paper.\n\nOur approach draws on recent developments in the rapidly growing field of Neural Architecture Search (NAS) with over a thousand papers in the last few years (see [36] for a recent survey). NAS has mostly been limited to architectural choices, such as network depth or width in macro search spaces, or (choosing among) a pre-defined set of operations on the edges of a computational cell in cell-based search spaces, in all of which the activations are fixed. Recently, gradient-based one-shot methods [22, 8, 10] have shown promise in efficiently optimizing architecture search spaces, reducing time costs by orders of magnitude compared to blackbox methods. Here, we adapt these NAS methods to mimic this success for searching activation functions by combining primitive mathematical operations.\nWe summarize our contributions as follows:\n\u2022 We implement several key adjustments to modern gradient-based architecture search methods, tailoring them to search within the space of activations. This method is then integrated with a search space design of activations which is rich enough to accommodate novel activations, yet small enough to maintain search efficiency.\n\u2022 Within a wide range of image classification tasks, with ResNet and ViT architectures, as well as language modelling tasks with GPT, we demonstrate that using gradient-based one-shot search strategies we can discover from scratch specialized activations that improve a network's performance. Notably, our approach proves orders of magnitude more efficient compared to previous methods.\n\u2022 Moreover, we investigate the transferability of the discovered activations to different models and datasets, and show that activation functions selected on a network/dataset, are among the top-performing activations on similar but larger models, as well as on new datasets.\nTo facilitate reproducibility, we make our code available here."}, {"title": "Related work", "content": "A line of research in automated activation function design utilizes gradient descent to learn \"adaptable activations\" during training together with network weights. These works rely on a sufficiently general parameterization of activation functions that is capable of approximating a wide class of functions including most existing activations. [2] use a general piecewise linear unit to approximate activations, while [14] adopt a weighted sum of polynomial basis elements. Instead, [25] rely on the Pad\u00e9 approximant (rational functions of polynomials) which shows better stability properties. Following [2], [32] also adopt a piecewise linear approximation but introduce inductive bias to restrict the parameter space and provide a balance between simplicity and expressivity, hence simplifying optimization.\n\nA separate approach [30, 3, 4, 21, 5], which is more in the spirit of NAS and further aligned with our current work, considers activation functions as hyper-parameters which are optimized in a search phase. The optimized function is then used as a fixed activation, possibly with learnable parameters, within a neural network. Contrary to gradient methods discussed previously, in this series of papers the activations within the search space are represented symbolically as combinations of basic functions. Moreover, they all utilize black-box optimization methods to explore the search space and thus require thousands of functions evaluations.\n\n[30] define the search space as a combination of basic unary and binary operations, and employ a search strategy previously developed for NAS [40]. They utilize an RNN controller to sequentially predict different components of the activation function. The RNN controller is trained with reinforce-ment learning taking the validation accuracy of a proxy network/task with the candidate activation as the reward. With a combination of exhaustive and black-box search procedures, with a budget of 10000 function evaluations, they identify the Swish function as a high-performing activation that also generalizes across a variety of tasks.\n\nAlong the same line, a number of subsequent works use evolutionary strategies to explore the space of activations. [3] define the search space as consisting of separate pieces for negative and positive input, each of which is constructed from existing, well-known, activations, including Swish and two other activations, ELiSH and HardELiSH, introduced in the same paper, inspired by Swish. [4] apply evolution to a search space similar to the one of [30]. [21] search for both activation and normalization layers jointly as a single building block. The search space consists of a Directed Acyclic Graph (DAG) with basic mathematical functions (including unary and binary operations), as well as statistical moments on the nodes. More recently, [5] used evolutionary methods to search over a more flexible combination of unary and binary operations. The set of unary operations is slightly different from [30] and includes existing high-performing activations, such as ReLU and Swish. As part of the evolutionary process, adaptable parameters are also randomly introduced in the activations which are then learned during training as any parametric activation. In a subsequent work, AQuaSurF [6] introduced a surrogate representation by combining the Fisher information matrix eigenvalues and activation outputs through UMAP embeddings. This enabled a regression algorithm to search over this space efficiently, reducing the cost to a hundred function evaluations as opposed to thousands required by previous approaches.\n\nThe black-box nature of all these optimization methods makes them computationally demanding and impractical to apply to large search spaces and modern, costly, deep learning pipelines. In this work, we instead rely on gradient descent to explore the space of activation functions. We closely follow [30], [4], and [5] to define the search space as combinations of low-level mathematical operations, as well as some existing activation functions. Contrary to previous gradient-based approaches, the search is performed in a bi-level fashion where the parameters of the activations are updated at the upper optimization level while the network weights are learned in the lower loop. This allows us to perform the optimization in the time it would require to evaluate only a few activation functions. The found activations can then be placed in the same or a different neural network which is trained from scratch."}, {"title": "Methodology", "content": "We first describe our search space for activation functions, then discuss tools from gradient-based neural architecture search (NAS) we build on, and then discuss how we adapt them for effective gradient-based activation function search."}, {"title": "The search space for activation functions", "content": "Following [30], [4] and [5], the space of activation functions is defined as a combination of unary and binary operations, which form a scalar function $f$, as shown in Figure 1. The unary and binary functions are chosen from a set of primitive mathematical operations, as listed in Figure 1 (Left). We also include several existing activation functions as unary operations to enrich the search space further as in [5].\n\nThe unary edges and binary vertices of the computational graph in Figure 1 (Right) can take any of the corresponding operations from Figure 1 (Left). In order to enable gradient-based optimization on this discrete space we continuously relax the space by assigning a weighted sum of all unary operations $\\sum_u \\upsilon_u^{(i)} u$ to the edge $(i, j)$ of the graph, and a weighted sum $\\sum_b \\beta_b^{(i)} B_b$ of binary operations to vertex $i$. Here the sums run over $u, b$ which denote respectively unary and binary operations in Figure 1 (Left), and $\\upsilon_u, \\beta_b$ are the weights with which they appear in this sum. Both sets of coefficients are constrained to lie on a simplex $\\sum_{u}\\upsilon_u^{(i)} = \\sum_{b}\\beta_b^{(i)} = 1$.\n\nThe computational cell in Figure 1 (Right) is therefore a function of the activation parameters $\\upsilon, \\beta$. This will replace the original activation (ReLU for ResNet and GELU for ViT and GPT) within the network where the gradient-based search is carried out. The parameter $\\gamma$ in Figure 1 (Left) is a learnable parameter that is trained along with the activation parameters and becomes frozen after the search is completed."}, {"title": "Tools from gradient-based neural architecture search", "content": "We first review well-established gradient-based NAS methods, which will serve as a starting point for our gradient-based activation function search.\n\nDARTS [22] was the first neural architecture search method that combined the weight-sharing idea [28] with a continuous relaxation of architecture parameters, allowing the use of gradient-descent to explore the architecture search space. This is carried out through bi-level optimization where gradient update steps are performed on continuous architectural parameters $\\alpha$ in the outer loop, while model weights $w$ are updated in the inner loop:\n\n$\\min_{\\alpha} L_{val}(w^{*}(\\alpha), \\alpha)$\n\ns.t. $w^{*}(\\alpha) = \\argmin_{w} L_{train}(w, \\alpha)$ (1)\n\nIn our specific problem of Section 3.1 $\\alpha$ will represent the collection of unary and binary parameters $(\\upsilon, \\beta)$. After the bi-level search phase is over, a final discretization step is then required to identify an architecture in the search space. The method is known to suffer from performance degradation at discretization [39].\n\nIn order to overcome the problem of large generalization errors and also encourage more exploration in the search space, DrNAS [8] formulates the differentiable architecture search as a distribution learning problem where the architecture parameters $\\alpha$ are sampled from a Dirichlet distribution $\\alpha \\sim Dir(p)$ with learnable parameters $p$.\n\nMotivated by the success of these methods in searching for top neural architectures, we employ similar search strategies to explore the space of activations. In particular, in this work, we opt to closely align with the distribution learning concept introduced in DrNAS (Algorithm 2), based on its demonstrated effectiveness in architecture search and in our initial experiments. However, given the slightly different nature of activation function spaces compared to those of neural architectures, this optimizer, at least in its original form, is not the best fit for discovering top performing activations. In the following subsection, we thus discuss how to modify it for searching the space of activations."}, {"title": "Gradient-based activation function search", "content": "Given the similarity between the space of architectures and those of activation functions, described in the previous subsection, one may hope that existing architecture search techniques can be used out of the box to efficiently explore the space of activation functions. However, na\u00efvely applying gradient-based optimizers to activation search spaces simply fails. We hypothesize that this is why this approach does not exist in the literature yet for activation function search. In order to make gradient-based optimization work for such spaces, we now introduce a series of techniques to robustify the approach.\n\nWarmstarting the search To robustify the search we introduce a short warm-starting phase during which the model weights are updated in the inner loop using the original activation, while the search cell is optimized in the outer loop. This ensures initializing the search with reasonable settings for both the network weights and the activation function parameters. After warm-starting the bi-level search continues, updating both model weights in the inner loop and activation parameters in the outer loop.\n\nConstraining unbounded operations Na\u00efvely applying gradient-based optimizers to activation search fails due to divergence of the search. This is caused by unbounded activation functions that lead to exploding gradients. To address this issue, we regularize the search space by constraining the unbounded operations in the search space. That is, operation outputs y with magnitude beyond a threshold y > l will be set to y = l sign(y). Here, we take l = 10. After these two modifications, existing NAS methods can be run reliably on the space of activations, but we can improve performance further.\n\nProgressive shrinking There are some fundamental differences between architecture spaces and those of activation functions. In particular, unlike architecture spaces, operations in the space of activations are nearly parameter free, as these are basic mathematical functions possibly with a few learnable parameters. Furthermore, different unary / binary elementary functions operate on different scales, making it challenging to rank their significance based on their coefficients.\n\nBecause of such inherent differences, it turns out that these methods do not perform well enough initially, at least to compete with existing activation baselines. Moreover the problem of performance drop at discretization, which is present in most NAS approaches, is more pronounced in the activation function space. To address these challenges we track activation parameters and at each epoch we drop a number of unary / binary operations corresponding to the lowest parameters (see Algorithm DropOps.). We choose the number of remaining operations to follow a logarithmic schedule such that at the final epoch we end up with a single unary(binary) operation on each edge(vertex), leading to a fully discretized activation. This progressive shrinking of the search cell not only improves efficacy of the approach but further expedites the search process.\n\nDrNAS with variance reduction sampling To optimize the activation cell we closely follow DrNAS, where a Dirichlet distribution Dir(p) is assigned to each edge/vertex of the search cell and the concentration parameters p are trained to minimize the expected validation loss. At each iteration, DrNAS draws activation parameters from its Dirichlet distribution. While DrNAS by default uses a single fixed sample throughout the network, in our variant, in order to reduce the variance introduced by this sampling process, we draw independent samples for each activation cell within the network."}, {"title": "Experiments", "content": "We explore high-performing activation functions across three distinct families of neural architectures: ResNet, ViT, and GPT. All examined network architectures in this study employ a single type of activation throughout the network. To conduct the search, the network's original activation function is globally replaced with the search cell in Figure 1. This activation cell is then optimized following the method outlined in Section 3.3.\n\nTo assess our method's reliability, for each model, we repeat the search procedure with five different seeds, resulting in up to five distinct activation functions. In principle, this number could be less than five due to different searches converging to the same activation or known baseline activations. However, by retaining all distinct activations, even if they were very similar, as we did in this work, this did not occur in our experiments. The identified activation functions are evaluated on the networks/datasets they are searched on and subsequently also transferred to larger models of the same type and/or applied to new datasets.\n\nFor the evaluation of each discovered activation, we train the models with it for five seeds on the train set, and report test set performance (mean \u00b1 the standard error of the mean)."}, {"title": "ResNet", "content": "Residual networks (ResNets) were introduced in [17] to mitigate the limitations of training deep neural networks and allow them to benefit from increased depth. They have since been the default in many image classification tasks.\n\nIn this section, our objective is to enhance the performance of ResNet20 trained on CIFAR10 by improving its activation functions. To achieve this, we replace the ReLU activations within ResNet20 with the search cell illustrated in Figure 1, and the exploration of the activation function space is carried out using the search strategy outlined in Section 3.3.\n\nIn all ResNet experiments, including the (inner loop) of the bi-level search and the (re)training of all models during evaluation, we utilized the PyTorch implementation provided in [19].\n\nAfter five repetitions of the search process five distinct and new activation functions were identified. The explicit formulas are given as\n\n$F_{RN}^{1}(x) = 0.4739 \\text{LeakyReLU}(\\text{LeakyReLU}(x)) + 0.5261 \\text{GELU}(x)$\n\n$F_{RN}^{2}(x) = 0.5163 \\text{LeakyReLU}(0.4945 \\text{ReLU}(x) + 0.5055 \\text{GELU}(x)) + 0.4837 \\text{GELU}(x)$\n\n$F_{RN}^{3}(x) = 0.4865 \\text{GELU}(0.4873 \\text{ReLU}(x) + 0.5127 \\text{GELU}(x)) + 0.5135 \\text{GELU}(x)$\n\n$F_{RN}^{4}(x) = 0.4756 \\text{ReLU}(x) + 0.5244 \\text{GELU}(x)$ (2)\n\n$F_{RN}^{5}(x) = 0.4591 \\text{LeakyReLU}(0.5267 \\text{LeakyReLU}(x) + 0.4733 \\text{GELU}(x)) + 0.5409 \\text{GELU}(x)$\n\nand their functional forms are visualized in Appendix E. These five activations are then retrained from scratch on the training set and their performance is evaluated on the test set."}, {"title": "Vision Transformers", "content": "After the success of the Transformer model [35] in natural language processing, Vision Transformers [11] based on the same self-attention mechanism have become increasingly popular in the vision domain. In the original ViT model GELU has been the default activation function. Here we let the automated search discover the activation that is well-suited to the ViT architecture.\n\nTo avoid computational burden, we conduct the search on the ViT-Ti [33] model which is a light version of ViT. The specific version of this model, as well as a larger variant used for evaluation in this study, is adapted from the implementation provided by [38], which we denote as ViT-tiny and ViT-small, respectively (See C for details of the architectural choices).\n\nIn the evaluation experiments of this section and in the inner loop of the search pipeline we utilized the GitHub repository [38], but employed the TrivialAugment (TA) setup [26] as the augmentation method. TA simply applies a random augmentation with a random strength to each image, and has proved to achieve state-of-the-art on a variety of image classification tasks.\n\nEquation 3 shows explicit formulas for the five novel activations found in the search process on ViT-tiny / CIFAR10. These activations are then evaluated and compared to baselines on ViT-tiny as well as the larger variant ViT-small on the three datasets CIFAR10, CIFAR100 and SVHN Core.\n\n$F_{ViT}^{1}(x) = 0.6601 \\text{GELU}(\\text{SiLU}(x) \\text{GELU}(x)) + 0.3399 x^2$\n\n$F_{ViT}^{2}(x) = 0.7322 \\text{SiLU}(0.2822 x^2 + 0.7178 \\text{GELU}(x)) + 0.2678 x^2$\n\n$F_{ViT}^{3}(x) = 0.7319 \\text{GELU}(\\text{SiLU}(x) \\text{GELU}(x)) + 0.2681 x^2$ (3)\n\n$F_{ViT}^{4}(x) = 0.6778 \\text{GELU}(\\text{SiLU}(x) \\text{GELU}(x)) + 0.3222 x^2$\n\n$F_{ViT}^{5}(x) = 0.3139 x^2 + 0.5431 \\text{GELU}(x)$"}, {"title": "Generative pre-trained transformers", "content": "To enhance the diversity of our experiments, we extend the evaluation of our approach to language modelling tasks. The best-established model in this domain is the Generative Pre-trained Transformer (GPT), which has recently achieved breakthrough performance. For the sake of simplicity, in this work we focus our analysis on Andrej Karpathy's nanoGPT, a streamlined implementation of GPT-2 [29].\n\nWe optimize the activation within a down-scaled version of this architecture with 11M parameters featuring 3 layers, 3 heads and an embedding dimension of 192, which we denote as miniGPT. We employ the TinyStories [12] dataset for training.\n\nAs before, we repeat the search five times, warm-starting it with the default GELU activation. This results in the following five new activations:\n\n$F_{GPT}^{1} = 0.4953 \\text{LeakyReLU}(x) \\text{GELU}(x) + 0.5047 \\text{ReLU}(x)$\n\n$F_{GPT}^{2} = (0.4689 \\text{GELU}(x) + 0.5311)\\text{ReLU}(x)$\n\n$F_{GPT}^{3} = (0.4662 \\text{sinh}(x) + 0.5338)\\text{GELU}(x)$ (4)\n\n$F_{GPT}^{4} = 0.4781 \\text{ReLU}(x)^2 + 0.5219 \\text{ReLU}(x)$\n\n$F_{GPT}^{5} = 0.4828 \\text{ReLU}(x)^2 + 0.5172 \\text{ReLU}(x)$\n\nall of which demonstrate lower test losses compared to GELU, as detailed in the left column of Table 6. As shown in the two right columns of this table, these improvements also transfer to two larger variants which we refer to as tinyGPT and smallGPT respectively. tinyGPT has 6 layers, 6 heads and an embedding dimension of 392, nearly tripling the size to 30M parameters, while smallGPT has 9 layers, 9 heads and an embedding dimension of 576 with 65M parameters.\n\nThe (asymptotic) ReLU(x)\u00b2 behaviour observed in $F_{GPT}^{4}$ and $F_{GPT}^{5}$ was previously identified in Primer [31] through an evolutionary search over TensorFlow programs for Transformer language models, and was determined to be the most effective modification in the architecture."}, {"title": "Conclusions", "content": "We have adapted modern gradient-based architecture search techniques to explore the space of activation functions. Our work demonstrates that our proposed search strategy, when combined with a well-designed search space, can successfully identify activation functions tailored to specific deep learning models that surpass commonly-used alternatives. Furthermore, the discovered activation functions exhibit transferability to larger models of the same type, as well as new datasets, achieving high performance.\n\nMost notably, the optimization is highly efficient, requiring very low overhead, up to only a few function evaluations in our case; this is in contrast to existing methods which require thousands of function evaluations. This makes it convenient for practitioners to employ these methods to automatically and efficiently design activation functions tailor-made for their deep learning architectures.\n\nThe method presented in this work aims to demonstrate the potential of gradient-based techniques in identifying top-performing activation functions, and as the first such work is not intended to represent the optimal pipeline for conducting such a search. While our approach, as is, may potentially already improve the strongest available models, we mostly see this work as opening the door for a host of possible follow-ups, such as improved search spaces and search methods, searching for activation functions with robust performance across workloads, or searching for activation functions with particularly strong scaling behavior to larger networks. We hope that our work lays the ground for further research and exploration in this direction."}]}