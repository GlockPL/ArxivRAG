{"title": "Efficient Search for Customized Activation Functions with Gradient Descent", "authors": ["Lukas Strack", "Mahmoud Safari", "Frank Hutter"], "abstract": "Different activation functions work best for different deep learning models. To exploit this, we leverage recent advancements in gradient-based search techniques for neural architectures to efficiently identify high-performing activation functions for a given application. We propose a fine-grained search cell that combines basic mathematical operations to model activation functions, allowing for the exploration of novel activations. Our approach enables the identification of specialized activations, leading to improved performance in every model we tried, from image classification to language models. Moreover, the identified activations exhibit strong transferability to larger models of the same type, as well as new datasets. Importantly, our automated process for creating customized activation functions is orders of magnitude more efficient than previous approaches. It can easily be applied on top of arbitrary deep learning pipelines and thus offers a promising practical avenue for enhancing deep learning architectures.", "sections": [{"title": "Introduction", "content": "Nonlinearities are an indispensable component of any deep neural network. The design choice of these so-called activation functions has proven to crucially affect the training dynamics and final performance of neural networks.\n\nThe rectified linear unit (ReLU) is the most commonly used activation due to its simplicity and consistent performance across different tasks. However, it took several years of empirical research [15, 20, 27] before it was widely adopted by practitioners as an activation function in deep neural networks.\n\nDespite the desirable properties of the ReLU, other alternatives have been introduced [23, 16, 9, 18, 13, 24], each with their own theoretical or empirical justification, to address potential issues associated with the ReLU, such as the dying ReLU problem [34, 1]. These alternative activations, which are mostly variations of ReLU, lead to performance improvements in particular settings, although none is as widely adopted yet.\n\nAs evidenced by previous research, manually designing an activation function that suits a certain task is highly non-trivial and established choices (such as ReLU, GELU and SiLU) are made possibly at the cost of losing (optimal) performance. Automated search methods have been previously employed to learn activation functions (see Section 2 for details), but existing methods require thousands of function evaluations and have thus not been adopted widely in practice. If it was possible to design a customized activation function for the problem at hand for the same cost as evaluating some standard alternatives (e.g., ReLU, GELU and SiLU) while yielding better performance, this would be quickly adopted by the community. That is the goal of our paper.\n\nOur approach draws on recent developments in the rapidly growing field of Neural Architecture Search (NAS) with over a thousand papers in the last few years (see [36] for a recent survey). NAS"}, {"title": "Related work", "content": "A line of research in automated activation function design utilizes gradient descent to learn \"adaptable activations\" during training together with network weights. These works rely on a sufficiently general parameterization of activation functions that is capable of approximating a wide class of functions including most existing activations. [2] use a general piecewise linear unit to approximate activations, while [14] adopt a weighted sum of polynomial basis elements. Instead, [25] rely on the Pad\u00e9 approximant (rational functions of polynomials) which shows better stability properties. Following [2], [32] also adopt a piecewise linear approximation but introduce inductive bias to restrict the parameter space and provide a balance between simplicity and expressivity, hence simplifying optimization.\n\nA separate approach [30, 3, 4, 21, 5], which is more in the spirit of NAS and further aligned with our current work, considers activation functions as hyper-parameters which are optimized in a search phase. The optimized function is then used as a fixed activation, possibly with learnable parameters, within a neural network. Contrary to gradient methods discussed previously, in this series of papers the activations within the search space are represented symbolically as combinations of basic functions. Moreover, they all utilize black-box optimization methods to explore the search space and thus require thousands of functions evaluations.\n\n[30] define the search space as a combination of basic unary and binary operations, and employ a search strategy previously developed for NAS [40]. They utilize an RNN controller to sequentially predict different components of the activation function. The RNN controller is trained with reinforce- ment learning taking the validation accuracy of a proxy network/task with the candidate activation as the reward. With a combination of exhaustive and black-box search procedures, with a budget of 10000 function evaluations, they identify the Swish function as a high-performing activation that also generalizes across a variety of tasks.\n\nAlong the same line, a number of subsequent works use evolutionary strategies to explore the space of activations. [3] define the search space as consisting of separate pieces for negative and positive input, each of which is constructed from existing, well-known, activations, including Swish and two other activations, ELiSH and HardELiSH, introduced in the same paper, inspired by Swish. [4] apply evolution to a search space similar to the one of [30]. [21] search for both activation and normalization layers jointly as a single building block. The search space consists of a Directed Acyclic Graph (DAG) with basic mathematical functions (including unary and binary operations), as"}, {"title": "Methodology", "content": "We first describe our search space for activation functions, then discuss tools from gradient-based neural architecture search (NAS) we build on, and then discuss how we adapt them for effective gradient-based activation function search."}, {"title": "The search space for activation functions", "content": "Following [30], [4] and [5], the space of activation functions is defined as a combination of unary and binary operations, which form a scalar function f, as shown in Figure 1. The unary and binary functions are chosen from a set of primitive mathematical operations, as listed in Figure 1 (Left). We also include several existing activation functions as unary operations to enrich the search space further as in [5].\n\nThe unary edges and binary vertices of the computational graph in Figure 1 (Right) can take any of the corresponding operations from Figure 1 (Left). In order to enable gradient-based optimization on this discrete space we continuously relax the space by assigning a weighted sum of all unary operations $\\Sigma_{\\upsilon} \\upsilon_{\\upsilon}^{(i,j)} u$ to the edge (i, j) of the graph, and a weighted sum $\\Sigma_{\\upsilon_{\\beta}} \\beta_b^{(i)} B_b$ of binary operations to vertex i. Here the sums run over u, b which denote respectively unary and binary operations in Figure 1 (Left), and $v_u, \\beta_b$ are the weights with which they appear in this sum. Both sets of coefficients are constrained to lie on a simplex $\\Sigma_{\\upsilon} \\upsilon_{\\upsilon}^{(i,j)} = \\Sigma_{\\upsilon_{\\beta}} \\beta_b^{(i)}= 1$.\n\nThe computational cell in Figure 1 (Right) is therefore a function of the activation parameters \u03c5, \u03b2. This will replace the original activation (ReLU for ResNet and GELU for ViT and GPT) within the network where the gradient-based search is carried out. The parameter \u03b3 in Figure 1 (Left) is a learnable parameter that is trained along with the activation parameters and becomes frozen after the search is completed."}, {"title": "Tools from gradient-based neural architecture search", "content": "We first review well-established gradient-based NAS methods, which will serve as a starting point for our gradient-based activation function search.\n\nDARTS [22] was the first neural architecture search method that combined the weight-sharing idea [28] with a continuous relaxation of architecture parameters, allowing the use of gradient-descent to explore the architecture search space. This is carried out through bi-level optimization where gradient update steps are performed on continuous architectural parameters a in the outer loop, while model"}, {"title": "Gradient-based activation function search", "content": "Given the similarity between the space of architectures and those of activation functions, described in the previous subsection, one may hope that existing architecture search techniques can be used out of the box to efficiently explore the space of activation functions. However, na\u00efvely applying gradient-based optimizers to activation search spaces simply fails. We hypothesize that this is why this approach does not exist in the literature yet for activation function search. In order to make gradient-based optimization work for such spaces, we now introduce a series of techniques to robustify the approach.\n\nWarmstarting the search To robustify the search we introduce a short warm-starting phase during which the model weights are updated in the inner loop using the original activation, while the search cell is optimized in the outer loop. This ensures initializing the search with reasonable settings for both the network weights and the activation function parameters. After warm-starting the bi-level search continues, updating both model weights in the inner loop and activation parameters in the outer loop."}, {"title": "Constraining unbounded operations", "content": "Na\u00efvely applying gradient-based optimizers to activation search fails due to divergence of the search. This is caused by unbounded activation functions that lead to exploding gradients. To address this issue, we regularize the search space by constraining the unbounded operations in the search space. That is, operation outputs y with magnitude beyond a threshold y > l will be set to y = l sign(y). Here, we take l = 10. After these two modifications, existing NAS methods can be run reliably on the space of activations, but we can improve performance further."}, {"title": "Progressive shrinking", "content": "There are some fundamental differences between architecture spaces and those of activation functions. In particular, unlike architecture spaces, operations in the space of activations are nearly parameter free, as these are basic mathematical functions possibly with a few learnable parameters. Furthermore, different unary / binary elementary functions operate on different scales, making it challenging to rank their significance based on their coefficients.\n\nBecause of such inherent differences, it turns out that these methods do not perform well enough initially, at least to compete with existing activation baselines. Moreover the problem of performance drop at discretization, which is present in most NAS approaches, is more pronounced in the activation function space. To address these challenges we track activation parameters and at each epoch we drop a number of unary / binary operations corresponding to the lowest parameters (see Algorithm DropOps.). We choose the number of remaining operations to follow a logarithmic schedule\u00b9 such that at the final epoch we end up with a single unary(binary) operation on each edge(vertex), leading to a fully discretized activation. This progressive shrinking of the search cell not only improves efficacy of the approach but further expedites the search process."}, {"title": "DrNAS with variance reduction sampling", "content": "To optimize the activation cell we closely follow DrNAS, where a Dirichlet distribution Dir(p) is assigned to each edge/vertex of the search cell and the concentration parameters p are trained to minimize the expected validation loss. At each iteration, DrNAS draws activation parameters from its Dirichlet distribution. While DrNAS by default uses a single fixed sample throughout the network, in our variant, in order to reduce the variance introduced by this sampling process, we draw independent samples for each activation cell within the network. Algorithm.1 outlines the pseudocode for our GRadient-based Activation Function Search (GRAFS) approach.\n\nBesides architecture parameters, the activation cell includes a few learnable parameters represented by y in Figure 1 (Left). These parameters are treated as part of activation parameters. Upon completion of the bi-level search process, if operations involving learnable y variables are identified, their values will be fixed to their final learned values."}, {"title": "Experiments", "content": "Overview\nWe explore high-performing activation functions across three distinct families of neural architectures: ResNet, ViT, and GPT. All examined network architectures in this study employ a single type of activation throughout the network. To conduct the search, the network's original activation function is globally replaced with the search cell in Figure 1. This activation cell is then optimized following the method outlined in Section 3.3.\n\nTo assess our method's reliability, for each model, we repeat the search procedure with five different seeds, resulting in up to five distinct activation functions. In principle, this number could be less than five due to different searches converging to the same activation or known baseline activations. However, by retaining all distinct activations, even if they were very similar, as we did in this work, this did not occur in our experiments. The identified activation functions are evaluated on the networks/datasets they are searched on and subsequently also transferred to larger models of the same type and/or applied to new datasets.\n\nFor the evaluation of each discovered activation, we train the models with it for five seeds on the train set, and report test set performance (mean \u00b1 the standard error of the mean)."}, {"title": "ResNet", "content": "Residual networks (ResNets) were introduced in [17] to mitigate the limitations of training deep neural networks and allow them to benefit from increased depth. They have since been the default in many image classification tasks.\n\nIn this section, our objective is to enhance the performance of ResNet20 trained on CIFAR10 by improving its activation functions. To achieve this, we replace the ReLU activations within ResNet20 with the search cell illustrated in Figure 1, and the exploration of the activation function space is carried out using the search strategy outlined in Section 3.3.\n\nIn all ResNet experiments, including the (inner loop) of the bi-level search and the (re)training of all models during evaluation, we utilized the PyTorch implementation provided in [19].\n\nAfter five repetitions of the search process five distinct and new activation functions were identified. The explicit formulas are given as\n\n$F_{R N}^{1}(x)=0.4739 \\text{LeakyReLU}(\\text{LeakyReLU}(x))+0.5261 \\text{GELU}(x)$ \n$F_{R N}^{2}(x)=0.5163 \\text{LeakyReLU}(0.4945 \\text{ReLU}(x)+0.5055 \\text{GELU}(x))+0.4837 \\text{GELU}(x)$ \n$F_{R N}^{3}(x)=0.4865 \\text{GELU}(0.4873 \\text{ReLU}(x)+0.5127 \\text{GELU}(x))+0.5135 \\text{GELU}(x)$ \n$F_{R N}^{4}(x)=0.4756 \\text{ReLU}(x)+0.5244 \\text{GELU}(x)$ \n$F_{R N}^{5}(x)=0.4591 \\text{LeakyReLU}(0.5267 \\text{LeakyReLU}(x)+0.4733 \\text{GELU}(x))+0.5409 \\text{GELU}(x)$"}, {"title": "Vision Transformers", "content": "After the success of the Transformer model [35] in natural language processing, Vision Transformers [11] based on the same self-attention mechanism have become increasingly popular in the vision domain. In the original ViT model GELU has been the default activation function. Here we let the automated search discover the activation that is well-suited to the ViT architecture.\n\nTo avoid computational burden, we conduct the search on the ViT-Ti [33] model which is a light version of ViT. The specific version of this model, as well as a larger variant used for evaluation in this study, is adapted from the implementation provided by [38], which we denote as ViT-tiny and ViT-small, respectively (See C for details of the architectural choices).\n\nIn the evaluation experiments of this section and in the inner loop of the search pipeline we utilized the GitHub repository [38], but employed the TrivialAugment (TA) setup [26] as the augmentation method. TA simply applies a random augmentation with a random strength to each image, and has proved to achieve state-of-the-art on a variety of image classification tasks.\n\nEquation 3 shows explicit formulas for the five novel activations found in the search process on ViT-tiny / CIFAR10. These activations are then evaluated and compared to baselines on ViT-tiny as well as the larger variant ViT-small on the three datasets CIFAR10, CIFAR100 and SVHN Core. The results, reported in Table 4, illustrate that all five activations outperform existing baselines on ViT-tiny / CIFAR10 providing high-performing customized activations for this task. Surprisingly, this pattern further extends to the datasets CIFAR100 and SVHN Core and the larger variant ViT-small.\n\n$F_{V I T}^{1}(x)=0.6601 \\text{GELU}(\\text{SiLU}(x) \\text{GELU}(x))+0.3399 x^{2}$ \n$F_{V I T}^{2}(x)=0.7322 \\text{SiLU}(0.2822 x^{2}+0.7178 \\text{GELU}(x))+0.2678 x^{2}$ \n$F_{V I T}^{3}(x)=0.7319 \\text{GELU}(\\text{SiLU}(x) \\text{GELU}(x))+0.2681 x^{2}$ \n$F_{V I T}^{4}(x)=0.6778 \\text{GELU}(\\text{SiLU}(x) \\text{GELU}(x))+0.3222 x^{2}$ \n$F_{V I T}^{5}(x)=0.3139 x^{2}+0.5431 \\text{GELU}(x)$"}, {"title": "Generative pre-trained transformers", "content": "To enhance the diversity of our experiments, we extend the evaluation of our approach to language modelling tasks. The best-established model in this domain is the Generative Pre-trained Transformer (GPT), which has recently achieved breakthrough performance. For the sake of simplicity, in this work we focus our analysis on Andrej Karpathy's nanoGPT, a streamlined implementation of GPT-2 [29].\n\nWe optimize the activation within a down-scaled version of this architecture with 11M parameters featuring 3 layers, 3 heads and an embedding dimension of 192, which we denote as miniGPT. We employ the TinyStories [12] dataset for training.\n\nAs before, we repeat the search five times, warm-starting it with the default GELU activation. This results in the following five new activations:\n\n$F_{G P T}^{1}=0.4953 \\text{LeakyReLU}(x) \\text{GELU}(x)+0.5047 \\text{ReLU}(x)$\n$F_{G P T}^{2}=(0.4689 \\text{GELU}(x)+0.5311) \\text{ReLU}(x)$\n$F_{G P T}^{3}=(0.4662 \\sinh(x)+0.5338) \\text{GELU}(x)$\n$F_{G P T}^{4}=0.4781 \\text{ReLU}(x)^{2}+0.5219 \\text{ReLU}(x)$\n$F_{G P T}^{5}=0.4828 \\text{ReLU}(x)^{2}+0.5172 \\text{ReLU}(x)$"}, {"title": "Conclusions", "content": "We have adapted modern gradient-based architecture search techniques to explore the space of activation functions. Our work demonstrates that our proposed search strategy, when combined with a well-designed search space, can successfully identify activation functions tailored to specific deep learning models that surpass commonly-used alternatives. Furthermore, the discovered activation functions exhibit transferability to larger models of the same type, as well as new datasets, achieving high performance.\n\nMost notably, the optimization is highly efficient, requiring very low overhead, up to only a few func- tion evaluations in our case; this is in contrast to existing methods which require thousands of function evaluations. This makes it convenient for practitioners to employ these methods to automatically and efficiently design activation functions tailor-made for their deep learning architectures.\n\nThe method presented in this work aims to demonstrate the potential of gradient-based techniques in identifying top-performing activation functions, and as the first such work is not intended to represent the optimal pipeline for conducting such a search. While our approach, as is, may potentially already improve the strongest available models, we mostly see this work as opening the door for a host of possible follow-ups, such as improved search spaces and search methods, searching for activation functions with robust performance across workloads, or searching for activation functions with particularly strong scaling behavior to larger networks. We hope that our work lays the ground for further research and exploration in this direction."}]}