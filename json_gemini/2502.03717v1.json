{"title": "Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning", "authors": ["Jaden Clark", "Joey Hejna", "Dorsa Sadigh"], "abstract": "Abstract-Expressive robotic behavior is essential for the widespread acceptance of robots in social environments. Recent advancements in learned legged locomotion controllers have enabled more dynamic and versatile robot behaviors. However, determining the optimal behavior for interactions with different users across varied scenarios remains a challenge. Current methods either rely on natural language input, which is efficient but low-resolution, or learn from human preferences, which, although high-resolution, is sample inefficient. This paper introduces a novel approach that leverages priors generated by pre-trained LLMs alongside the precision of preference learning. Our method, termed Language-Guided Preference Learning (LGPL), uses LLMs to generate initial behavior samples, which are then refined through preference-based feedback to learn behaviors that closely align with human expectations. Our core insight is that LLMs can guide the sampling process for preference learning, leading to a substantial improvement in sample efficiency. We demonstrate that LGPL can quickly learn accurate and expressive behaviors with as few as four queries, outperforming both purely language-parameterized models and traditional preference learning approaches. Website with videos: this http url.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in reinforcement learning (RL) for quadrupeds have enabled agile locomotion across many com- plex terrains through diverse and tuneable gaits [1], [2], [3]. However, choosing the appropriate gait remains an open question, and selecting the incorrect gait may inadvertently generate dangerous or erratic movements that fail to align with human expectations [4]. In the case of quadrupeds, the fastest gait might not always be suitable. For example, a more agile gait might optimize speed, but could pose dangers in social or crowded environments. This demonstrates the need for a principled method to generate human-aligned reward and objective functions.\nSelecting the correct objective is particularly challenging when there is a need to quickly adapt. In the real world robots will interact with diverse humans to complete various desired tasks. In the case of a social robot, different users desiring to interact with a robot may prefer it to move differently based on their age, comfort level with robots, or task at hand. For example, a younger child may prefer a slower and calmer gait whereas an older user may desire more expressive, rapid locomotion to accomplish a task. Here, the robot must quickly adjust its behavior to suit the expectations and needs of different users before they lose interest in the robot.\nThe need for user controlled gaits that can rapidly adapt to a user's preferences has led to a number of recent works that use a natural language due to its intuitiveness and ability to adapt to diverse environments [5], [6], [7], [8]. However, natural language based approaches are fundamentally low- resolution, and it can be challenging for humans to describe the nuanced differences between robot gaits. For example, most humans are not experts on different quadruped locomotion styles. They may not know how to differentiate pacing and trotting, or what different step frequencies look like. So, it can be challenging to specify how a robot should walk without observing and providing specific feedback for existing gaits. On the other hand, preference learning allows users to compare different model outputs to provide high-resolution feedback. This makes it effective for refining robotic behaviors to align with human expectations where direct specification of complex reward functions is impractical [9], [10], [11], [12]. Unfortunately, though learning from preferences allows for greater specificity it comes at a cost determining a users intent from comparisons alone often requires hundreds [12] or thousands [11] of queries for the most efficient approaches. Querying a human's feedback hundreds of times to tune the quadruped's behavior may be tiresome or annoying for the user, and discourage them from adjusting the behavior as needed.\nThis work presents a hybrid approach that combines the versatility of language models with the precision of preference learning for quadruped locomotion. Our core insight is that although language is a coarse mechanism of feedback, it can provide high-quality candidate solutions which vastly speed up the preference learning process.\nSpecifically, our approach Language-Guided Preference Learning leverages large language models (LLMs) to generate candidate reward functions for a quadruped using in-context prompting. Then, humans provide feedback by ranking the LLM-generated behaviors. This feedback is then used to learn a final reward function, enabling the rapid customization of robot behaviors to user-specific requirements. This approach not only accommodates the broad behavioral adaptability facilitated by LLMs but also capitalizes on the capability of preference learning to estimate a user's true intent when it may be difficult to exactly specify with natural language. Through simulations and user studies, LGPL generates a diverse array of human-aligned behaviors with as few as four queries - generating behaviors that have 53% lower L2 loss than preference learning and 62% lower L2 loss than LLM parameterization based on MSE loss with respect to a ground truth reward (LGPL MSE was 0.223, preference learning MSE was 0.455, and LLM MSE was 0.821). Furthermore, we found that users preferred the behaviors from LGPL 76% of the time when generating and tuning expressive behaviors over competing methodologies."}, {"title": "II. RELATED WORK", "content": "Our work builds on existing research in quadruped loco- motion, LLMs in robotics, and preference learning. Here, we review the areas most pertinent to our approach.\nQuadruped Locomotion. A large body of work has shown that it is possible to train agile locomotion policies via approaches such as reinforcement learning (RL) capable of generating many different gaits [13], [14], [1], [2], [3], [15]. However, such policies can be challenging to train without a sophisticated learning curriculum [3], and gaits can be difficult to specify without manually encoding their reward parameters [14]. Several works propose methods for real time specification of gaits [1], [7]. However, these works are limited as they either do not consider how a non-expert user can tune the commands for the robot, or do not allow refinement of behaviors beyond low-resolution language feedback. On the other hand, LGPL provides an efficient means for both expert and non-expert users to rapidly tune desired gaits.\nSpecifying Robot Behavior with LLMs. A large body of recent work has focused on using LLMs to generate robot behaviors [16], [17], [6], [18], [19], [8]. Some works focus on using LLMs to generate long-horizon sequences of actions for robots [17], [16], and more recent work focuses on improving existing LLMs for robot-specific tasks [20]. Most relevant to our work is literature that explore generating reward functions directly from language instructions or corrections [5], [21], [6]. Notably, several works use in-context learning to prompt an LLM to define reward parameters for a variety of robot tasks from high level commands such as \"sit\" or \"stand up\" [6], to more complex gait contact patterns [7]. While this approach effectively bridges high-level behavioral commands with lower-level locomotion, it does not focus on refining these behaviors based on user feedback, which is critical in personalized interactions. Furthermore, many robot behaviors such as differences in locomotion styles cannot be easily specified using language by a non-expert user [22]. This brings us to consider how to adaptively learn reward functions from human feedback.\nPreference Learning. Due to the challenges in manually specifying rewards for dynamical systems, several techniques have been developed to learn reward functions from hu- man feedback. These methodologies encompass learning from physical corrections [23], scalar feedback [24], rankings [25], [26], and pairwise comparisons [12], [11], [27], [28]. These works often actively generate the next query to ask from the user in order to learn the reward function in a sample efficient manner. Several methods employ preference-based methods to capture complex objectives but often suffer from sample inefficiency [11], [12]. To try and improve sample efficiency, several works develop active learning strategies, some with an LLM in the loop [29], [30], but often struggle in high- dimensional spaces [27], [31]. Moreover, active preference learning is still slow, requiring many samples and multiple feedback iterations. On the other hand real-time systems, like those for quadruped locomotion, necessitate simple and fast feedback mechanisms. Thus, instead of focusing on how to improve feedback over multiple iterations as in active learning, we instead focus on how to generate near-optimal initial reward candidates. In the next section, we show how we can use LLMs to do so efficiently for a real-time quadruped locomotion system. Here we focus on learning a reward vector that is initialized via parameters generated from an LLM given an initial language instruction. We then further adapt such reward functions to align with the non-expert user's preferences via standard preference learning techniques."}, {"title": "III. METHOD", "content": "In this section, we first describe the general problem of quadruped behavior adaptation with real-time human feed- back. Then, we describe our approach, Language-Guided Preference Learning (LGPL), which uses an LLM to generate candidate reward parameterizations as shown in Fig. 2. These candidates are then used as queries for preference learning to further adapt the rewards via rankings from human feedback.\nA. Problem Statement\nConsider a Markov Decision Process (MDP) with state space S, action space A, and dynamics P defined as (S, A, P, rw), where the w \u2208 \u03a9 serves as a task-specific parameter that defines the reward function rw. We train a task- conditioned policy \u03c0*(a|s, w) via reinforcement learning (RL) to maximize the expected reward rw across all tasks w\u2208 \u03a9. It is assumed that the distribution w is accessible during the training phase (which allows for the learning of the optimal task-conditioned policy). However, our approach deviates from standard goal-conditioned RL by positing that the ultimate desired task w* is unknown and must be inferred from human feedback instead of being sampled from a fixed distribution.\nIn this context, we define the few-shot preference-based RL challenge. Our objective is to identify the desired task w* from human feedback, while minimizing the number of user interactions required. In the real world, providing human feedback can be an arduous process [32], far too slow to adapt behaviors in real-time. Thus, to enable real-time systems we focus specifically on the few-shot setting where we have access to only a very small number of interactions with the user.\nTo efficiently learn the task w, we assume that the user provides rankings over a small number of trajectories (T1, T2,..., Tn) of length T, where n < 10. Each trajectory can be written as Ti = (S1,i, A1,i, S2,i, A2,i, ..., ST,i, AT,i). Following prior work [33], we assume that human responses follow the Bradley Terry model [34]. Thus, the probability of one trajectory being preferred over another within the ranking can be written as\nP[T1 > T2] =  frac{exp (\u2211t rw\u2217 (st,1, at,1))}{exp(\u2211t rw\u2217 (st,1, at,1)) + exp(\u2211t rw\u2217 (st,2, at,2))}\n(1)\nwhere T1 T2 indicates that trajectory T\u2081 is preferred to trajectory 72 and rw* is the users intended reward function. The users' ranking of n trajectories thus results in (n \\2) such pairwise comparisons.\nA typical approach would elicit user preferences over tra- jectories T1, T2,..., Tn of length T generated by randomly sampling from \u03a9 to generate corresponding W1,W2,...,Wn. These preferences would then be used to learn a reward model. However, when n is small sampling only a few random w to generate trajectories is likely to be uninformative as the space of possible tasks is too large to cover in only a handful of queries. As a result, prior works on few-shot preference learning often assume access to optimal demonstrations [35]. However, providing human demonstrations in our setting is unrealistic, as this amounts to knowing the desired task vector wapriori. Moreover, intelligent sampling and active learning approaches which often rely on model uncertainty are equally ineffective with only a few data points. Instead, we posit that low-quality priors (e.g. random) are the source of this inefficiency. Next, we detail how we can tap into the world knowledge of LLMs to remedy this problem by generating better candidate w.\nB. Language Parameterization\nThe core insight of our method is that we can use LLMs to act as high-information priors. Specifically, we use an LLM to generate initial candidate tasks parameterizations w, which are subsequently evaluated based on the trajectories they produce. To enable the LLM to generate realistic quadruped behaviors, we prompt the model with pairs of task parameterizations \u03c9 and their corresponding language descriptions l as provided"}, {"title": "C. Preference Learning", "content": "Given the candidate task parameterizations generated by the LLM, we rollout the policy \u03c0*(a|s,w) for each sampled can- didate (W1,W2,...,Wn) yielding trajectories (T1, T2,..., Tn) which can be visualized by a human user. After being shown all trajectories, the user ranks all wis, which in turn generates comparisons that can be used for preference learning.\nA na\u00efve approach would directly learn w from the (n \\2) comparisons generated by the use rankings. However, in the few-shot setting where n is small, this still results in only a handful of datapoints. To construct more training data, we assume that user preferences also hold over sub-segments of the trajectories [36]. Using sub-segments of length k, each trajectory T\u2081 of length T admits T k sub-segments \u03c4i, \u03c4, \u03c4, ..., where \u03c4 = (Sj,i, aj,i,..., Sj+k,i, aj+k,i). In total, this results in (T - k)\u00b2(n \\2) comparisons that can be used for preference learning, which is substantially more than if attempting to directly learn w from the original n ranked values. Slightly overloading notation, we denote the modified dataset of all sub-segment comparisons as \u010e = {(\u03c41,\u03c42,Y)}. where y = {1,2} specifies which of the two sub-segments is preferred.\nTo learn the user's intended task parameterization w* from the preference dataset D, we minimize the binary cross entropy between the empirical preference distribution and the preference model implied by the learned parameterezation w. Succinctly, our objective is:\nLpref (w) = E(\u03c41,\u03c42,y)\u223c\u010e [y(1) log (Pw[\u03c41 > \u03c42]) +y(2) log (1 \u2013 Pw[\u03c41 > \u03c42])]  (2)\nwhere Pw[\u03c41 > \u03c42] is the implied preference distribution from substituting the learned w into rw(s,a) in Eq. (1). In other words, this is the standard binary cross entropy loss where the logits are determined by the cumulative reward estimates rw over a segment 7. The goal is to maximize the probability of preferred preference segment and consequently, the predicted reward values of the preferred segment relative to the unpreferred one. By minimizing Eq. (2) we obtain an estimate of w*. Critically, at deployment time we do not need to retrain a policy. Instead, we just rollout the existing task conditioned policy \u03c0*(als,w) with w*, facilitating real- time deployment. We call our approach Language-Guided Preference Learning (LGPL).\nA requirement of LGPL is that the reward function rw is differentiable with respect to w. Fortunately, reward functions used for quadrupeds are often expressed as a sum of dif- ferentiable functions parameterized by features like velocity, orientation, and energy expenditure penalties. Thus, in LGPL we model rw as a sum of differentiable terms rw(s,a) = \u03a3jaj\u03c6j(s,a,wj) where aj are weights, \u03c6j(s,a,wj) are parameter-specific differentiable reward functions, and wis the jth element of the vector representation of the task. This not only allows us to optimize w using gradient descent, but also makes producing candidate w with an LLM easy, as it just needs to output a vector. In the next section, we provide further details about how we use LGPL for a real-time quadruped system."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we seek to answer the following questions: 1) Is LGPL more sample efficient than existing preference"}, {"title": "A. Implementation", "content": "To test the efficiency of LGPL, we design a space of tasks \u03a9 that is able to represent a broad set of behaviors. Specifically, we choose w \u2208 R5 to be the desired velocity, desired pitch, and three essential primitives: trotting P1, pacing P2, and bounding P3. We represent each gait by an indicator function where\n1{gait=Pi}(gait) = {  1 if the gait is P\u2081, 0 otherwise.\nIn practice when learning w as a continuous vector in R5 it may not strictly follow the form [v, p, 1{gait=P1},1{gait=P2},1{gait=P3}] due to the indicator variables. For deployment we set_1{gait=P{} = 1 only for the maximum of the gaits and set the rest to zero. Our initial experiments found that this space of \u03a9 was sufficient to express a broad variety of gaits desired by diverse users. We train a train an RL policy conditioned on w to maximize rw(s,a) = \u2211jaj\u03c6j(s,a,wj), where \u03c6j are the aforementioned differentiable reward factors. For example, in our case at timestept \u03c62(s,a,w2) = ||p - pt||2 where Pt is the pitch at t. For the different gaits, we set \u03c6j based on the contact pattern generator from prior work [7]. The task-conditioned policy is trained using the Nvidia Isaac Gym simulator [37] with PPO [38]. The resulting policy \u03c0*(als,w), a feed-forward neural network, outputs desired positions for each motor joint. Inputs to this network include the robot's base angular velocities, the normalized gravity vector g = [0, 0, -1] in the base frame, current joint positions and velocities, outputs from the previous timestep, and the targeted reward parameterization w. During training, \u03b3\u03c9 was uniformly randomized to ensure the resulting policy is optimal across all tasks.\nFor deployment, we used a custom LLM prompt (defined in the project page) to generate a set amount of diverse task parameterizations. In all experiments, we used GPT-4 as the LLM [39]. We then deploy our policies in the real world using the Pupper v3 quadruped. Pupper v3 has three joints per leg (hip, thigh, and calf joints), and is capable of diverse locomotion behaviors."}, {"title": "B. Is LGPL more sample efficient than existing preference learning approaches?", "content": "First, we assess how well LGPL can identify a wide variety of desired tasks with only a handful of preference queries. We compare LGPL against the following baselines:\n\u2022 Language to Reward (L2R): the LLM directly parame- terizes the reward vector as done in [6].\n\u2022 Preference Learning (PL): the human ranks queries ran- domly generated from the space of rewards the policy is trained on. These ranking are then used for preference learning [26].\n\u2022 LLM ranked: After sampling rewards as in LGPL, we ask the LLM to re-rank its own generations and use those for preference learning\nFor all methods using language, we use chain-of-thought prompting to elicit stronger behavior specification [40].\nWe consider 5 types of emotive behavior tasks in simulation \"happy\", \"sad\", \"scared\", \"angry\", and \"excited\" each corresponding to a unique w* provided by a human expert. For methods using preferences, we provide oracle preference labels by ranking trajectories according to the sum of rewards under the desired task, Et rw\u2217 (st, at). We evaluate methods on how well can predict the desired task vector by measuring the mean-squared error (MSE) with the ground truth w* across 5 seeds for 4, 8, and 12 ranked trajectories.\nOn average, we find that LGPL (depicted in orange) achieves far lower MSE with the ground truth task than standard preference learning particularly with fewer ranked trajectories, indicating that LGPL's use of an LLM prior can accelerate preference learning. Standard PL (depicted in yellow) often performs worse than simply querying the LLM when only given 4 ranked trajectories as it is unable to identify the correct task from uniform samples as seen in Fig. 3.\nWhile simply using in-context learning performs competitively in some tasks (e.g. \u201cHappy\u201d), it appears more sensitive to the language prompt, and further refinement via LPL only sometimes improves performance."}, {"title": "C. Do LGPL Rewards Align with Human Expectations?", "content": "A benefit of LGPL is its ability to be used for real- time interactions with users. To assess LGPL's ability to generate human-aligned behaviors, we conduct a user study in which participants observe videos of a quadruped executing behaviors generated by LGPL and each baseline method.\nWe generate a trajectory for each behavior for all four methods. Each trajectory is split into 5 segments. Then we query users for preferences between trajectories for a given behavior, using 3 seeds. Participants also ranked each each trajectory on a Likert scale from 1-7 for how closely it matched the desired behavior. A total of 11 users conducted the study,"}, {"title": "D. Can LGPL Adapt to Real-Time User Feedback?", "content": "Finally, we seek to evaluate how efficiently LGPL can adapt to user feedback in real world. To do so, we conduct an additional user study in which all feedback is provided by real participants. Each user describes a desired behavior in language and then subsequently ranks 4 candidates for all methods that use preference learning (LGPL, PL) to esti- mate the intended task w*. In addition to L2R, we consider an additional LLM-based baseline where we allow users to continue prompting the LLM to refine the behavior after observing rollouts of the L2R policy. We refer to this method as Language 2 Reward with Feedback (L2RF). Ultimately, users observe the final trajectories generated by all methods and assign each a Likert score from 1-7 and rank them according to how well they identify their intended behavior. The study consisted of 5 participants, and each participant evaluated all methods for two expressive tasks they described, such as fearful, nervous, playful, etc. The full study took approximately 20 minutes. Results are presented in Fig. 5 and the bottom half of Table I. We found that LGPL was preferred 76.67\u00b110.59% of the compared to baseline methods. Notably, in some cases providing language feedback to L2R often did not improve the new gait (L2RF), and L2R gaits before feedback were only preferred less 40% of the time. Thus, when relatively few (4) queries are available, LGPL generates user-preferred behaviors for novel expressive tasks better than both preference learning and L2R. Furthermore, repeatedly querying semantic feedback from users is not necessarily a good alternative as providing accurate language feedback can be more onerous than rankings, and does not necessarily generate preferred trajectories."}, {"title": "A. Conclusion", "content": "In this paper, we introduce Language-Guided Preference Learning (LGPL), a new approach for efficiently generating quadruped behaviors aligned with human preferences. By using LLMs to produce initial candidate reward functions and refining them through preference-based feedback, LGPL combines the versatility of language input with the precision of preference learning. Our experiments demonstrated that LGPL outperforms both traditional preference learning and purely language-parameterized models in terms of sample efficiency and accuracy, achieving desired behaviors with as few as four queries. Human evaluations confirmed that LGPL-generated behaviors closely match user expectations for expressions such as happiness, sadness, fear, and anger. These findings highlight the potential of LGPL to enable rapid and precise customiza- tion of robot behaviors in dynamic social environments."}, {"title": "B. Limitations and Future Work", "content": "Future work has the potential to scale LGPL to more complex real-time systems by overcoming its limitations.\n\u2022 Differentiability. LGPL requires differentiable reward functions, which potentially prohibits the expressiveness of possible tasks.\n\u2022 Chaining behaviors While LGPL can efficiently identify individual tasks, real-world expressive behaviors may involve sequences of sequential tasks.\n\u2022 Query difficulty Behaviors produced by by preference learning algorithms can be hard for humans to discern. Future work could explicitly consider legibility when generating queries.\n\u2022 Dimensionality LGPL works well with only a few queries, however increasing the dimension of the task vector w may come at the cost of sample efficiency as both LLM generation and preference learning become harder."}]}