{"title": "NARCE: A Mamba-Based Neural Algorithmic Reasoner Framework for Online Complex Event Detection", "authors": ["Liying Han", "Gaofeng Dong", "Xiaomin Ouyang", "Lance Kaplan", "Federico Cerutti", "Mani Srivastava"], "abstract": "Current machine learning models excel in short- span perception tasks but struggle to derive high- level insights from long-term observation, a ca- pability central to understanding complex events (CES). CEs, defined as sequences of short-term atomic events (AEs) governed by spatiotempo- ral rules, are challenging to detect online due to the need to extract meaningful patterns from long and noisy sensor data while ignoring irrelevant events. We hypothesize that state-based meth- ods are well-suited for CE detection, as they cap- ture event progression through state transitions without requiring long-term memory. Baseline experiments validate this, demonstrating that the state-space model Mamba outperforms existing architectures. However, Mamba's reliance on ex- tensive labeled data, which are difficult to obtain, motivates our second hypothesis: decoupling CE rule learning from noisy sensor data can reduce data requirements. To address this, we propose NARCE, a framework that combines Neural Algo- rithmic Reasoning (NAR) to split the task into two components: (i) learning CE rules independently of sensor data using synthetic concept traces gen- erated by LLMs and (ii) mapping sensor inputs to these rules via an adapter. Our results show that NARCE outperforms baselines in accuracy, gen- eralization to unseen and longer sensor data, and data efficiency, significantly reducing annotation costs while advancing robust CE detection.", "sections": [{"title": "1. Introduction", "content": "Current work has achieved great performance in short-time perception machine learning tasks, such as human activity recognition or object detection, which typically require only a few seconds of sensor data for inference. However, many real-world applications critically depend on the ability to understand high-level contextual information over extended periods of time, as humans do an ability that is crucial yet often overlooked. For example, consider a social robot that monitors the daily routine of the elderly with a camera of 30 frames per second rate that works all day. An 8-hour day- time video generates nearly one million frames; one needs to know how to compress and memorize key information to keep track of patterns that may span minutes or hours. To describe different situations, it is useful to use the concept of a complex event (CE). A complex event represents a high- level scenario with spatiotemporal rules and patterns that require aggregating and reasoning over numerous short-term activities, which we call atomic events (AEs). Fig. 1 shows examples of two common complex events, each featuring (spatial)temporal patterns of key atomic events, respectively.\nComplex events are challenging. The reasons are three-fold. First, to recognize a complex event pattern, the model must identify key atomic event occurrences while ignoring irrel-"}, {"title": "2. Related Work", "content": "Complex Event Detection (CED) has been extensively studied in traditional stream processing for structured databases. Early works employed event engines such as Finite State Machines (FSMs) to identify complex events based on predefined patterns. More re- cently, research has shifted towards CED over unstructured, high-dimensional data. Approaches integrate neural detectors with symbolic rule-based sys- tems like ProbLog, enabling backpropagation through logical rules. However, these methods suffer from computational scalability issues. Moreover, most existing approaches fo- cus on complex activities within short time spans, limiting their applicability to longer sensor traces with extended temporal dependencies.\nNeural Algorithmic Reasoners (NAR) are neural networks designed to learn algorithmic structures directly from data, enabling structured reasoning tasks such as sorting, shortest- path computation, and graph search. combines a transformer with a pre-trained NAR"}, {"title": "3. Online Complex Event Detection", "content": "3.1. Complex Event Definitions\nDefinition 3.1. Atomic events (AEs) are low-level, short- duration, and fundamental building blocks of complex events. They are typically instantaneous or span a small time window and are directly detectable by models such as image classification, object detection, or activity recognition models.\nDefinition 3.2. Complex events (CEs) are high-level events that are defined as sequences or patterns of atomic events (AEs) occurring in specific temporal or logical relationships.\nLet $\\mathcal{A} = \\{a_1, a_2, ..., a_n\\}$ denote the set of all atomic events, where $n$ is the total number of atomic events. Each $a_i$ is associated with a start time and an end time. Similarly, $\\mathcal{E} = \\{e_1, e_2, ..., e_k\\}$ denote the set of all complex events of interest, where $k$ is the total number of complex events.\nEach complex event $e_i \\in \\mathcal{E}$ is defined as:\n$e_i = R_i(\\mathcal{A}_i) = R_i(a_1^i, a_2^i, ..., a_{n_i}^i)$,\nwhere $\\mathcal{A}_i \\subset \\mathcal{A}$ is the subset of atomic events relevant to $e_i$, $R_i$ is a pattern function that defines the temporal or logical relationship among the atomic events in $\\mathcal{A}_i$, and $n_i = |\\mathcal{A}_i|$ is the number of atomic events involved in defining $e_i$. Each $e_i$ is associated with a time $t_{e_i}$, which is the specific time (or time interval) at which the pattern $R_i$ is satisfied, i.e., when the complex event $e_i$ occurs.\nTemporal Pattern Function ($R_i$): The function $R_i$ maps a subset of atomic events $\\mathcal{A}_i$ to a complex event $e_i$ by defin- ing specific patterns among the atomic events. In this work, we considered four main categories of patterns: Sequen- tial Patterns, Temporal Patterns, Repetition Patterns, and Combination Patterns. Some groups have one or more sub-categories; their definitions and examples are provided in Table 1. Importantly, all patterns considered in this work are bounded to finite states, enabling them to be represented by finite state machines (FSMs).\n3.2. Online Detection Task Formalization\nWithout loss of generality, let's assume a system receives a raw data stream $X$ from a single sensor with some modal- ities $\\mathcal{M}$. The sensor operates at a sampling rate $r$. The system processes the data stream using a non-overlapping"}, {"title": "4. Baseline CED Framework", "content": "This section is organized as follows: First, we introduce the online CED pipeline, which serves as the foundation for the subsequent parts. Second, to evaluate Hypothesis I, we propose various neural and neurosymbolic baseline archi- tectures. Third, we describe the construction and details of our CED dataset. Finally, we present experimental results to validate our hypothesis.\n4.1. Pipeline Overview\nWe propose a two-module online processing system. As is shown in Fig. 4, the system takes sensor streams as inputs and applies a non-overlapping sliding window of size $W$ to process data streams into segments. Those segments $\\{s_t\\}_{t=1}^T$ are forwarded to the following two modules:"}, {"title": "4.2. Baseline Architectures", "content": "4.2.1. REQUIREMENTS\nWe design various neural and neurosymbolic architectures to serve as the complex event detector in Fig. 4. They must meet the following requirements:\nCausal structure. Online CED requires the system to pre- dict complex events at each time step $t$ using only informa- tion from previous observed timestamps (0 to $t$). Models must have a causal structure to ensure they do not access future information.\nMinimum receptive field. For neural network architectures, the receptive field or context window of each model must be larger than the longest temporal patterns of complex events"}, {"title": "4.4. Baseline Evaluation", "content": "Experimental Setup. We train all baseline models using the AdamW optimizer with Focal Loss. TCN-based models, including Neural + TCN, have a receptive field of 8 minutes, sufficient to capture CE patterns in the 5-minute training data. Early stopping is applied based on validation loss, and results are averaged over 10 random seeds. Additional train- ing details are provided in Appendix E.2. The Pretrained Feature Encoder and the Neural AE classifier used in the experiment are described in Appendix D.\nMetrics. We evaluate performance using the F1 score for each CE class $e_i$ and report two aggregated scores:\n1. Macro F1 ($F1_{all}$): The unweighted average F1 across all classes ($e_0$ to $e_{10}$).\n2. Positive F1 ($F1_{pos}$): The average F1 over positive event classes ($e_1$ to $e_{10}$), excluding the less important \"negative\" label $e_0$. This serves as our key metric.\nA higher F1 score indicates better precision-recall balance, reflecting both correctness and completeness.\nResults. We evaluate model performance across different training set sizes, as shown in Fig. 5. The results indicate that Mamba achieves the best performance, followed by LSTM. The Neural + X models underperform compared to end-to-end models, likely due to errors and noise intro- duced by the Neural AE classifier. This also explains why the Neural AE + FSM model, despite incorporating correct human-written complex event rules, performs worse. De- tailed F1 scores for each CE, including per-class F1 scores, are provided in Table 7.\nAdditionally, we test model generalization on out-of- distribution (OOD) complex events lasting 15 and 30 min- utes, following the same CE rules but with extended tempo- ral spans. As shown in Table 2, Mamba generalizes better than LSTM to unseen test data. Training with more la- beled sensor data improves performance on 5-minute test data and enhances generalization to longer unseen traces. However, we still observe a performance drop as temporal span increases. Moreover, the data-hungry nature of these neural network baselines imposes significant real-world data collection and labeling costs."}, {"title": "5. NARCE Framework", "content": "5.1. Overview\nWe propose NARCE, a framework designed to reduce the need for labeled sensor data by decoupling complex event rule learning from sensor-specific variations, as hypothe- sized in Hypothesis II. Inspired by the Neural Algorith- mic Reasoning (NAR) paradigm, which uses Graph Neural Networks (GNNs) to represent and learn algorithms by us- ing symbolic algorithm input-output pairs for training, in NARCE, we analogously treat each complex event rule as a type of algorithm and leverage Mamba, a state-space model well-suited for long-range dependencies, to learn them.\nTo achieve this, NARCE follows a two-stage training process, as shown in Fig. 2. In Stage 1: It learns complex event rules from synthetic concept traces without using sensor data. In Stage 2: It adapts to real sensor data by training a Sensor Adapter that maps sensor embeddings into the latent space of the pretrained CE NAR.\n5.2. Stage I: Training CE NAR on Concept Traces\nIn this stage, we train the Mamba-based CE NAR to learn complex event rules independently of sensor data. Instead"}, {"title": "5.3. Stage II: Training the Sensor Adapter", "content": "Once the CE NAR is trained on concept traces, we adapt it to real sensor data by training a Sensor Adapter $f'$. The goal of the Sensor Adapter is to map raw sensor embeddings into the latent space of NAR, allowing it to process sensor inputs while preserving the learned event reasoning capabilities. To achieve this:\n\u2022 The Embedding Encoder from Stage 1 is removed, and the Sensor Adapter is introduced.\n\u2022 The CE NAR remains frozen, ensuring that the event reasoning remains intact.\n\u2022 The Sensor Adapter is trained using labeled sensor data with online CE labels to learn the mapping.\nWe use a 6-layer Mamba block for the Sensor Adapter, though other neural network models can be used. This setup enables online CE detection, allowing the model to infer complex event occurrences from raw sensor streams."}, {"title": "6. Experiments", "content": "6.1. Experimental Setup\nWe train both the CE NAR and the sensor adapter of NARCE using the AdamW optimizer with Focal Loss. Further train- ing details are in Appendix G.1. Synthetic AE concept traces are generated using the LLM Synthesizer, producing datasets of 20k, 40k, and 80k samples. The labeled sensor dataset is identical to that used in the baseline experiment.\n6.2. Evaluation\nWe compare the performance of baseline Mamba models, Neural AE + FSM, and NARCE trained on 40k pseudo AE concept traces. Results are shown in Fig. 6. Additionally, we conduct a Wilcoxon Signed-Rank Test with a = 0.05 to assess statistical significance. The results indicate that narce_4k significantly outperforms mamba_4k, while no significant difference is detected between narce_4k vs. mamba_10k and narce_2k vs. mamba_4k. This con- firms that NARCE achieves comparable or superior per- formance to baseline models while requiring significantly fewer labeled sensor samples, validating Hypothesis II. Detailed statistical hypotheses and p-values are provided in Appendix G.2.\nWe also analyze the impact of synthetic training data size on NARCE's performance. As shown in Table 3, increas- ing the number of synthetic AE concept traces improves both performance and generalization to extended CE se- quences, given the same amount of labeled sensor data. The improvement is more significant when sensor data is limited to 2000 samples. However, training with 80k concept traces provides only marginal improvement over 40k, likely due to model capacity limitations, suggesting an upper bound on its effectiveness."}, {"title": "6.3. Ablation Study", "content": "We conduct an ablation study to evaluate the effective- ness of the Focal Loss (FL) we propose for online CED tasks. In Table 4, we compare the following models: (1) a NARCE where the NAR is trained using FL, while the Sen- sor Adapter is trained with CrossEntropy Loss, (2) a NARCE where both NAR and Adapter are trained with CrossEntropy Loss, and (3) Our standard NARCE, where FL is applied to both components. The results show that FL plays a crucial role in training both the NAR and Sensor Adapter. While model (2) achieves strong performance on 5-minute data, it generalizes poorly on OOD test data (15-minute and 30- minute). In contrast, our standard NARCE with FL excels, particularly when the number of labeled sensor data samples is limited to 2,000, demonstrating better generalization and robustness across all test sets."}, {"title": "7. Conclusion", "content": "In this work, we propose NARCE, a Neural Algorithmic Reasoning framework for efficient and robust online Com- plex Event Detection. By decoupling complex event rule learning from sensor-specific variations, NARCE reduces the reliance on large-scale labeled sensor data. Extensive exper- iments validate our Hypothesis I and Hypothesis II, showing that Mamba-Based NARCE achieves comparable or superior performance to baseline models with significantly fewer la- beled sensor samples. Our results demonstrate that training with low-cost synthetic pseudo AE concept traces enhances"}, {"title": "A. Complex Event Dataset Classes", "content": "The 10 CE classes of interest are defined here."}, {"title": "B. Complex Event Simulator", "content": "The complex event simulator is used to generate CE dataset related to the CE classes defined in Table 5.\nB.1. Complex Event Simulator\nDue to the complexity of complex event patterns, each CE has infinitely many combinations of AEs over time. To generate a general distribution for complex events, we developed a stochastic CE human activity simulator to synthesize multimodal time-series data for each CE pattern.\nFig. 7 illustrates the simulator used to generate stochastic CE sequences. It consists of multiple Stages, each containing a set of Activities that occur with different probabilities. An Activity is defined by a temporal sequence of Actions. For example, the Activity \u201cUse Restroom\u201d follows the sequence: 'walk' \u2192 ('wash') \u2192 \u2018sit\u2019 \u2192 \u2018flush-toilet\u2019 \u2192 (\u2018wash\u2019) \u2192 'walk', where parentheses indicate that an Action occurs probabilistically. The duration of each Action is randomly sampled within a user-defined threshold, allowing sequences to vary in length even for the same Activity. Transitions between Stages"}, {"title": "B.2. Multimodal AEs", "content": "We synthesize multimodal sensor data for 9 Actions, corresponding to the AEs used to construct CEs. Each Action class is mapped to a pair of audio and IMU classes, as shown in the first column of Table. 6. The 9 audio and 9 IMU classes are selected from the following two datasets:\nB.3. Audio dataset\nWe utilize the ESC-70 dataset, a combination of the ESC-50 dataset and the Kitchen20 dataset. The ESC-50 dataset consists of 2000 5-second labeled environmental audio recordings, with 50 different classes of natural, human, and domestic sounds. The Kitchen20 dataset collects 20 labeled kitchen-related environmental sound clips. We also self-collected 40 additional 5-second silent sound clips for Actions that do not have sound. We downsampled the original sampling rate of those recordings is 44.1 kHz to 16 kHz.\nB.4. IMU dataset\nWe use the WISDM dataset, containing raw accelerometer and gyroscope sensor data collected from smartphones and smartwatches, at a sampling rate of 20 Hz. It was collected from 51 test subjects as they performed 18 activities for 3 minutes each. Based on the findings in a survey paper , we only use smartwatch data for better accuracy. We segment the original data samples into non-overlapping 5-second clips.\nUsing those two datasets, we generate 500 multimodal data samples of 5 seconds per Action class. To synthesize multimodal CE sensor data, the 5-second sensor data clips for every Action are concatenated according to the AE patterns of the generated stochastic CE sequences."}, {"title": "C. FSM Examples", "content": "Examples FSM Codes for e1, e2 and e3 defined in Table 5. Those implementations utilize an Extended Finite State Machine (eFSM) instead of a standard FSM to improve efficiency in counting tasks and simplify state logic. eFSMs enhance readability by incorporating variables and conditions for state transitions, making them easier to understand and manage. While any eFSM can still be represented as a standard FSM, using an eFSM allows for a more concise and structured approach to state management for easy interpretation."}, {"title": "D. Pretraining Feature Encodera and Neural AE Classifier", "content": "The Feature Encoder and Neural AE are trained at the same time. The architecture of the Feature Encoder combined with a Neural AE classifier is illustrated in Fig. 8. Given that the CE dataset is a multimodal dataset containing both inertial and IMU sensor data, the Pretrained Feature Encoder is designed to generate a fused embedding that effectively integrates these two modalities.\nD.1. Early Fusion Model\nVarious approaches can be considered for multimodal, such as early fusion, late fusion, and hybrid fusion. In our system, we employ early fusion, which involves integrating the features from both modalities immediately after extraction. Fig. 8 provides an overview of the multimodal fusion module's structure. First, the model uses pre-trained audio and IMU modules, BEATs and LIMU-Bert, respectively, to extract features from every 5-second audio and IMU clip. Then the audio and IMU embeddings undergo individual Gated Recurrent Unit (GRU) layers to obtain audio and IMU embeddings of the same dimension 128. Next, we concatenate them into an embedding of 256 and pass it through a Fusion Layer to create a joint representation of 128, which is trained alongside a downstream fully-connected NN for AE classification. Notably, during the inference phase, the Neural AE classifier is omitted, and only the output from the last hidden layer of the fusion layer is employed as the fusion embedding.\nD.2. Training\nThe BEATs model used for the audio module is already pre-trained, while the LIMU-Bert model for IMU module is pre-trained on large datasets. We freeze the parameters of both models and focus on training the other components of the multimodal fusion module. For training and testing the fusion module, we utilize the multimodal atomic action dataset introduced in Table. 6. The training process minimize the cross-entropy loss:\n$\\min L_m = \\frac{1}{\\mathcal{N}} \\sum_i^{N} \\min c_i log(\\widehat{c_i})$ where $\\widehat{c_i} = m_\\theta (d_i)$"}, {"title": "D.3. Evaluation of the Neural AE classifier", "content": "We test the classifier using the multimodal AE dataset, which is used in Neural AE + X models. The classifier achieves 95% accuracy on test set."}, {"title": "E. Baseline Experiments", "content": "E.1. Model Details\nEnd-to-end Neural Architectures:\n\u2022 Unidirectional LSTM: 5 LSTM layers with a hidden dimension of 256 (# parameters \u2248 2.5M).\n\u2022 Causal TCN: It masks information from future timestamps in convolutional operations. It has one stack of residual blocks with a last dilation rate of 32 and a kernel size of 3. The number of filters for each level is 256. The receptive field is calculated as # stacks of blocks \u00d7 kernel size \u00d7 last dilation rate = 1 \u00d7 3 \u00d7 32 = 96, which corresponds to 8 minutes, greater than the longest 5-min temporal pattern of our CE dataset, corresponding to a receptive field greater than 5 \u00d7 60\u00f75 = 60 (# seconds divided by the window size). (# parameters \u2248 4.6M)\n\u2022 Causal Transformer Encoder: with a triangular attention mask to restrict the model's self-attention to previous timestamps only, excluding information from future timestamps. The causal transformer encoder uses 6 encoder layers with a hidden dimension 128, multi-head attention with 8 heads, and positional encoding (# parameters \u2248 4.2M)\n\u2022 Mamba: We use a Mamba model with 12 SSM blocks. We made this design choice because the original Mamba paper states that two Mamba blocks are equivalent to one transformer layer. (# parameters \u2248 1.4M)\nTwo-stage Concept-based Neural Architecture. Neural AE + Xs have almost the same architecture as the Xs in End-to-end Neural Architecture, except that they take the 9-dimensional one-hot vectors as AE concepts.\nE.2. Training Details\nFor training, we utilize one NVIDIA GeForce RTX 4090 GPU and four NVIDIA H100 GPUs. All the baseline neural models are trained using the AdamW optimizer with a learning rate of 1 \u00d7 10\u22123, a weight decay of 0.1, and a batch size of 256. Focal Loss is used to address class imbalance. Early stopping is applied based on the validation loss, with training halted if no improvement is observed after a predefined patience period. The maximum epochs of training is 5000. All experiments are repeated with 10 random seeds."}, {"title": "E.3. Detailed Results", "content": "Table 7 presents the Macro F1 score, Positive F1 score, and F1 score for each class ei. All neural models are trained on 10,000 CE sensor data. We observe that all models perform poorly on e4, possibly due to the similarity between drink and eat sensor embeddings."}, {"title": "F. LLM Synthesizer", "content": "F.1. The Prompt Template\nYou are a simulator that mimics daily human activities. You output sequences of activities as live streaming. At each window of 5-second, you generate a current activity label, which represents the activity that happens during this 5-second time window. Here's an example output of a live-streaming list of activities:"}, {"title": "G. NARCE Experiments", "content": "G.1. Training Details\nWe utilize one NVIDIA GeForce RTX 4090 GPU and four NVIDIA H100 GPUs. Both the (Embedding Encoder +) NAR and Sensor Adapter are trained using the AdamW optimizer with a learning rate of 1 \u00d7 10-\u00b3, a weight decay of 0.1, and a batch size of 256. Focal Loss is used to address class imbalance. Early stopping is applied based on the validation loss, with training halted if no improvement is observed after a predefined patience period. The maximum epochs for training NAR is 5000, and for training Sensor Adapter is 10000. All experiments are repeated with 10 random seeds.\nG.2. Detailed Results - Wilcoxon Statistical Test\nComparison between narce_4k and mamba_4k. We perform a Wilcoxon Signed-Rank Test to evaluate the null hypothesis Ho: narce_4k is worse than mamba_4k. The resulting p-values are 0.02, 0.3, and 0.03 for CE 5-min, CE 15-min, and CE 30-min, respectively. Additionally, for the CE 15-min dataset, we test the null hypothesis Ho: narce_4k is better than mamba_4k, obtaining a p-value of 0.69. These results indicate that narce_4k is significantly better than mamba_4k on C\u0415 5-min and CE 30-min. However, no significant difference is observed between narce_4k and mamba_4k on CE 15-min.\nComparison between narce_4k and mamba_10k. We conduct two one-sided Wilcoxon Signed-Rank Tests to evaluate the null hypotheses: (1) H0: narce_4k is worse than mamba_10k and (2) H0: narce_4k is better than mamba_10k. However, none of the p-values for CE 5-min, CE 15-min, or CE 30-min are significant enough to reject either hypothesis. Thus, we conclude that there is no significant difference between narce_4k and mamba_10k.\nComparison between narce_2k and mamba_4k. Similarly, we conduct two one-sided Wilcoxon Signed-Rank Tests to evaluate the null hypotheses: (1) Ho: narce_2k is worse than mamba_4k and (2) H0: narce_2k is better than mamba_4k. However, none of the p-values for CE 5-min, CE 15-min, or CE 30-min are significant enough to reject either hypothesis. Thus, we conclude that there is no significant difference between narce_2k and mamba_4k.\nH. Case Study - LLM for CED\nMany existing works show that LLMs have the ability to do high-level reasoning on sensor data. We also evaluated the ability of LLMs to perform online CED tasks. We simplify the setting to give LLMs ground truth sequences of atomic event labels and only investigate LLMs' ability for complex event reasoning. In this experiment, each AE label is expressed in words representing the activity happening in each 5-second window. For example,"}, {"title": "H.3. Key Take-aways", "content": "Though LLMs have the most potential to perform well on online CED tasks, the current models still suffer from hallucinations and poor ability in long-chain reasoning. Also, as online CED usually requires timely inference at each time window, transformer-based LLMs will induce huge latency when we wait for blocks of inputs to be fed to LLM servers for processing."}]}