{"title": "Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM", "authors": ["Haizhou Wang", "Nanqing Luo", "Peng Liu"], "abstract": "Sandboxes and other dynamic analysis processes are prevalent in malware detection systems nowadays to enhance the capability of detecting 0-day malware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in modern malware samples, and sandboxes can suffer from false negatives and analysis failures when analyzing the samples with TADAs. In such cases, human reverse engineers will get involved in conducting dynamic analysis manually (i.e., debugging, patching), which in turn also gets obstructed by TADAs. In this work, we propose a Large Language Model (LLM) based workflow that can pinpoint the location of the TADA implementation in the code, to help reverse engineers place breakpoints used in debugging. Our evaluation shows that we successfully identified the locations of 87.80% known TADA implementations adopted from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs.", "sections": [{"title": "I. INTRODUCTION", "content": "Malware detection in real-world is achieved through a complex human-in-the-loop system with reverse engineers playing an essential role. In terms of quantity, the majority of the malware samples are detected and blocked by file signatures, including inline firewall blocking and anti-virus software. This is because most samples in the wild are 1-day or n-day samples, so their file signatures are already documented, either automatically or manually. Obviously, any signature-based methods are subject to 0-day malware attacks, and therefore different methods need to be adopted to detect and block 0-day malware. Although it is possible to detect 0-day malware through advanced static analysis, in practice this strategy can suffer from non-scalability and false negatives. For example, some samples are only at the initial stage of infection and will do no harm unless subsequent payloads are downloaded, which can only be confirmed to be malicious by concretely executing the samples. Therefore, many industry-level malware detection systems rely on automated dynamic analysis, or sandboxes [1], [2], to detect 0-day malware using behavior signatures and rules. Unfortunately, most malware authors are fully aware of the existence of sandboxes and dynamic analysis efforts, and therefore many of them nowadays will implement Techniques of Anti-Dynamic-Analysis (TADA) in their malware to evade dynamic analysis-based detection.\nAs we will elaborate in Section II-A, malware authors usually have two goals with TADAs implemented: evading the dynamic analysis-based detection and impeding reverse engineering effort, respectively. These two goals are directly motivated by the process of detecting malware samples in the real world, which is illustrated in Figure 1. The top half of the figure is fully automated, and the bottom half involves human labor. When a sample arrives at the system, the first step is usually to check the existing signature and hash databases to see if the sample is already known to be malicious. If the signature or hash has a match, then the system can immediately conclude that the sample is malicious. Despite the disadvantage of the"}, {"title": "II. BACKGROUND", "content": ""}, {"title": "A. Techniques of Anti Dynamic Analysis (TADA)", "content": "Existing works [9], [10], [11] had surveyed TADA comprehensively. From an attacker's perspective, the reason for implementing TADA in their malware is two-fold: evade dynamic analysis based detection and analysis obstruction. Evading dynamic analysis based detection is to ensure the malware will not be blocked so that the malicious goals can be achieved; whereas analysis obstruction is the effort to delay the reverse engineer's understanding the logic of the malware as well as understand the root cause of analysis evasion, if any.\nDynamic analysis usually happens in sandboxes based on Virtual Machines (VM) and/or other monitored environments. Since sandbox is automated dynamic analysis, it is omnipresent in malware detection systems. To evade dynamic detection, malware must not behave maliciously in sandboxes, rendering the necessity of detecting the sandbox environment. [12] shows the trend that fingerprinting and artifact detection is becoming popular in sandbox evasion, not only because it is highly effective, but also because the implementation could be extremely diverse. As a counter-measure, sandbox developers usually will try to hide the artifacts indicating sandbox and VM environment [1], [2], [3], [4], [5]. With this being the current status, essentially, a good sandbox is usually transparent, but the transparency is not free: the more transparent, the more costly. For example, cost-effective sandboxes are usually VM-based, because VMs are easy to create and destroy, no requirement of isolated hardware, and easy to scale. However, VM-based sandboxes are relatively less transparent, especially when necessary monitoring tools need to be integrated into the guest OS. In contrast, the most transparent sandboxes is bare-metal hardware based, but that would cost much more than a VM-based sandbox. Thus, due to this real-world cost effectiveness issue, for malware authors, implementing TADAs to evade detection is still considered to be worthwhile.\nAnother reason of malware having TADA is analysis obstruction. In short, failed analysis and false negative could happen when detecting 0-day malware dynamically, either due to the TADAs implemented by the malware author or missing runtime requirements. Therefore, eventually manual analysis by reverse engineer becomes necessary to confirm the verdict of a sample for which automatic analysis failed. Reverse engineers usually rely on the debuggers and other analysis software such as Process Monitor to understand the behavior of a sample. Accordingly, many TADA implemented is targeting to detect the debuggers and analysis programs (i.e., anti-debugging) [13]."}, {"title": "B. Categorization of TADAS", "content": "Depending on the context, analysts tend to categorize TADAs differently. There are two major ways to categorize TADAs: by tactic or by implementation. For sandbox builders, they often categorize TADAs by their tactics, because sandbox builders usually focus on how to hide the sandbox artifacts and make their analysis tools more transparent. For example, in [9], authors categorize the TADAs based on the evasion goals. Whereas for reverse engineers, they tend to categorize TADAs by their implementations, because reverse engineers usually need to identify the locations of the code implementing TADAS.\nCategorization based on tactics. The most popular strategy to categorize TADAs is based on their tactics. In MITRE ATT&CK tactics, there are two sub-tactics in Defense Evasion that is related to TADA:\n\u2022 T1622: Debugger Evasion\n\u2022 T1497: Virtualization/Sandbox Evasion\nBoth T1622 and T1497 aim to detect dynamic analysis efforts, with slight different focuses. On one hand, T1622 is debugger evasion, which is usually aiming to detect and impede human analysts reverse engineering efforts (debuggers are usually used by a human); on the other hand, T1497 is more about impeding automated dynamic analysis for malware evasion and detection. For example, the most well-known debugger detection on Windows platform is to check the Process Environment Block (PEB), where there exists multiple indicators that may imply the presence of a debugger and tracer. In general, we can categorize TADAs into 4 categories based on their tactics: debugger evasion, sandbox evasion, VM evasion, and analysis tool evasion. Regarding the differences between VM and sandbox evasion, a VM does not have to be a sandbox (e.g. VMs used by reverse engineers), and a sandbox does not have to be a VM (e.g. bare-metal sandboxes, emulator sandboxes).\nCategorization based on implementations. A less popular way to categorize TADA is based on the implementations. Since the implementations of TADAs are not necessarily correlated to their tactics, the implementations of TADA with different tactics can be extremely alike. For instance, by altering very little can an implementation of a running debugger detection technique turn into a VM guest addition detection. If the running debugger is detected by finding a well-known debugger process name, all the malware writer needs to do to detect VM guest addition is to switch the name of the process to detect. From a program analysis point of view, the code involving detecting running debugger and detecting running virtual machine guest addition could be virtually identical, but the purpose is very different, which is purely based on the name of the process being detected. By implementation, TADAs can be categorized into: assembly based, direct API based, and indirect API based. As the name suggested, assembly-based implementations usually achieve the TADA goals using several instructions. Usually, this kind of TADA is implemented by adding inline assembly code. Direct API-based TADAs leverage the APIs that can directly inform the program if it is executing under an analysis environment, such as the famous IsDebuggerPresent API. Usually direct API-based TADA will not involve many API calls: one or a few will be sufficient for the conclusion. In contrast, indirect API-based TADA uses APIs that seem to not be directly related to the purpose of anti-dynamic analysis (e.g. check the current username). Usually indirect API-based TADA implementation will involve a sequence of API calls, followed by logic checking their return values."}, {"title": "C. LLM For Binary Analysis", "content": "LLMs we refer to today are usually Generative Pre-trained Transformers (GPT) [14], which take tokenized natural language inputs and generate language output. These LLMs have been showing impressive performance in source code analysis, as technically programming language is just a more structured natural language that will only apply to a limited scope. However, when applying LLMs for binary analysis, it is necessary to conduct an extensive amount of feature engineering, because raw binary code are very different from the natural language. Even disassembled binary code, or assembly language, is significantly different."}, {"title": "III. MOTIVATION AND PROBLEM STATEMENT", "content": ""}, {"title": "A. Motivation", "content": "Due to the diversity of TADAs and cost-effectiveness concerns when designing the sandboxes, it is still common for malware to evade sandbox detection and eventually in need of manual reverse engineering. Common reverse engineering process for binary executables involves two major steps: 1) advanced static analysis for interesting breakpoints and memory locations, and 2) dynamic analysis using a debugger for behavior study (using breakpoints found previously). Dynamic analysis using debugger is usually necessary throughout the reverse engineering efforts, either to search for malicious behaviors or confirm conclusions drawn from advanced static analysis. As described in Section II-A, malware authors implement TADAs not only to evade detection but also to impede and delay human reverse engineering analysis. Therefore, it is extremely likely that when a reverse engineer investigates an evasive sample, he/she will encounter TADAs stopping them from properly analyzing the sample.\nThis work is motivated by the fact that the real-world malware detection system shown in Figure 1 still requires a significant amount of human effort that can be tedious for the reverse engineer, yet most parts of the system are already automated (top side in Figure 1). Based on the common two-step reverse engineering process, we aim to propose a workflow that can suggest a set of breakpoints near the implementation of TADA during the advanced static analysis stage, so that the analysis can apply patches whenever these breakpoints are triggered during the dynamic analysis stage."}, {"title": "B. Advantages of LLMs", "content": "Introduced in Section II-B, the implementations of the TADAs can be categorized into 3 kinds. As we will show later in Section V, the majority of the TADAs are implemented through indirect APIs, against which are extremely tricky to defend, because never can human analysts predict what aspects of the runtime environment malware authors are checking. Based on various analysis reports [11], [19], [10], malware may inspect username, folder and file names, disk volume number, recent file used, number of CPU cores, current running process names, current running process command lines, etc. Since modern computers and OSes are extremely complicated, the list of sandbox artifacts is endless, and consequently, it is not possible to create 100% transparent sandboxes.\nIf we scrutinize the implementations of TADAs carefully, it is notable that the majority of the runtime checks involve strings. For example, inspecting file names, user names, WMI (Windows Management Instrumentation) queries, and all other similar artifacts will presumably involve strings. Due to the volatile nature of strings, detection of TADA could suffer from false negatives when using rule-based methods, which may be tackled by adopting LLMs. Despite the controversy about LLMs' ability to truly understand humans, LLMs do show a promising ability to understand human languages, which makes them a perfect candidate for digesting the strings used in TADA implementations."}, {"title": "C. Problem Statement", "content": "As motivated in Section III-A, our goal is to suggest a set of breakpoints \u201cnear\u201d the implementation of TADA, so that the reverse engineer can bypass the TADAs and conduct root cause analysis on sandbox failures. Now we have our problem statement as follows:\nGiven a malware sample with TADA, how to reduce\nthe time and labor efforts of the reverse engineer by\nproviding a set of breakpoints.\nIn this paper, we decide to perform our analysis at the basic block (BB) level, and correspondingly, the concept of \u201cnear\u201d will be a small number of BB away from the BB where the breakpoint is located, on the control flow graph (CFG). Specifically, we want to place the breakpoints at the BBs that are parts of the TADA implementations."}, {"title": "IV. METHOD", "content": ""}, {"title": "A. Overview", "content": "To identify the breakpoints for human analysts, they need to be located near the implementation of the TADAs. Therefore, our goal can also be viewed as detecting the location of the TADA.\nWhat is the granularity of the analysis? The breakpoints are usually placed at the beginning of instructions so that the granularity of the breakpoints is at the instruction level. However, for the sake of detecting TADA implementations, instruction level could be improper, as the amount of information carried in an single instruction could be too little. The most obvious example are API calls, which usually involves more than one instructions if any arguments are presented. On the other hand, function-level could be too large, as placing a breakpoint at a function could be futile, in worse cases requiring human analyst manually inspecting thousands of instructions. Therefore, to include sufficient information for detection while keeping the definition of the breakpoint location reasonably precise, our detection granularity is at the basic block (BB) level.\nWhat input fits LLM well? This paper focuses on reverse engineering of binary programs, which is clearly inappropriate to be directly integrated into the input prompt. Essentially, both open-source and proprietary LLMs are trained using natural languages in addition to some program languages so that the LLMs cannot handle reverse engineering tasks from scratch. Common strategies to address this type of challenge include few-shot learning and fine-tuning, but given the diversity of the TADA implementations, such strategies face the challenge of"}, {"title": "B. Sample unpacking and BB extraction", "content": "Since the majority of the malware executable samples are Windows Portable Executable (PE) files, we mainly focus on PE in this work. However, it is very common to see a PE malware packed [20]. Without unpacking, one can never construct the CFG reflecting real malware logic. Therefore, the first step of our workflow is to unpack the malware samples and construct the CFG.\nThe topic of unpacking PE files [21], [22], [23] and constructing CFG has been well studied and many techniques are available. For the purpose of self-containing, here we only briefly introduce the techniques adopted during our experiments when handling real-world malware samples. Given a malware sample, we first check if it is a PE file by checking the magic bytes and parsing the header. If it is a valid PE file, we will then use Detect-It-Easy tool to find whether the file is packed. In cases where no known packer is found, we will then check the Import Address Table (IAT). If there are fewer than 5 imported libraries or 15 imported functions, we will also view this sample as a packed malware sample.\nAs for unpacking, if the malware is packed by UPX, we will just go ahead and unpack it using the UPX utility. For other packers, we will do dynamic unpacking. We first will try automatic unpacking using PE-sieve tool. Essentially, after the malware is loaded in the memory, the tool will continuously scan the memory to check whether the executable code is inconsistent with the copy on the disk. If any suspicious changes in executable segments are detected, the tool will dump the executable, which is very likely to be the unpacked malware sample. Unfortunately, in some cases, this automatic unpacking will fail due to corrupted IAT in the dump, and here our last try is to do manual unpack using x64dbg, and fix the imports using Scylla plugin.\nTo this point, if the original IAT can be parsed, constructing CFG and dumping the BBs are fairly straightforward. Existing tools include but are not limited to IDA, Angr, etc."}, {"title": "C. Feature Construction: Assembly Feature", "content": "Assembly features are features extracted solely from the assembly language, which can be acquired directly from the extracted BBs. In particular, we are looking for two specific pieces of information: mnemonics of instructions and memory accesses.\nIn assembly language, mnemonics of instruction are the most semantic-rich component, and the presence of certain mnemonics can be a strong indicator of TADAS. For example, if a BB contains both pushf and popf instructions, then it is extremely suspicious to detect a debugger. For another example, if a BB contains hardware querying instructions such as cpuid, then it could be detected if a VM exists. As for memory accesses, we particularly look for memory accesses through segment registers. On X86 Windows host,"}, {"title": "D. Feature Construction: String Feature", "content": "One key motivation for using LLM is that the string can carry many clues regarding whether a BB is TADA-related. For example, as shown in Figure 3, if a BB refers to a string such as VirtualBox, then obviously the BB is very suspicious of involving TADA. Accordingly, we include string features, such as:"}, {"title": "E. Feature Construction: API Call Feature", "content": "As the name suggested, the API call feature of a BB reflects all the API calls in the BB. Needless to mention, APIs called during malware execution are one of the most important information to understand the behavior of the sample. For our purposes, API calls are indeed critical as well, as numerous TADAs can be implemented through APIs. For example, the most famous single API that can be used to do anti-debugger is IsDebuggerPresent, which will return the value of the BeingDebugged flag in PEB 1. For another example, in many anti-sandbox techniques, malware authors will conduct detection based on computer and username, which can be very difficult to acquire without using the Windows APIs such as GetComputerName. Therefore, APIs is extremely important for detecting TADA.\nIn our API call feature, we not only include the API name, but also the arguments passed to the API. Shown in Figure 5 is an example of API call in a BB calling MessageBoxA. Since the call instruction refers DS register, after checking the IAT, we will be able to find out that this call is calling MessageBoxA. Subsequently, based on the documentation of the MessageBoxA, we know that the 4 pushed data are the 4 arguments of the call.\nAPI Name We first resolve the name of the API, which is very straightforward in cases of direct calls. In particular, for each call instruction, we check if the callee is referring to the import table. From the IAT, we will be able to resolve the API that is being called easily.\nIndirect calls are extremely challenging, since our feature extraction only uses static analysis. However, there are still simple cases where calls may be resolved, as long as the register value at the call site can be inferred. To do so, if we find an indirect call instruction in a BB, we will construct the data flow graph (DFG) for the function containing the BB. Then we trace the data flow backward from the indirect call site to see if a live variable with a concrete value can be found (e.g. value from static data sections). If so, we will check if the value is a valid address for an API, in a similar way as described earlier for direct calls.\nArguments Based on Windows _stdcall calling convention and API documentation, it is not difficult to resolve"}, {"title": "F. Leveraging LLM", "content": "Input. Our ultimate goal is to help the reverse engineers identify the location of the TADA implementations, so that breakpoints can be properly set. As described in Section IV-A, we would want to ensure the prompt consists mainly of natural language, so that the LLM can be used without fine-tuning and few-shot learning. Therefore, our prompt design starts with an initial prompt in plain English of the instructions and the specifications of the TADAs, followed by the features extracted in the form of natural language.\nOutput. At first glance, it may seem that the most intuitive model of this problem is binary classification: having the LLMs to output a binary verdict of whether a BB is TADA related or not. However, as described in Section III-B, identifying implementation of TADA is a very challenging task and is very prone to false positive, so that the best scenario for a human analyst is to have quantified results which can give them a sense of which BB should be prioritized for investigation. Instead of prompting the LLM to output a binary result, we prompt for a rating. The LLM will essentially output a number from 0 to 10, reflecting how related a BB may be related to the implementation of TADA.\nThe template of the prompt is shown below:"}, {"title": "V. EVALUATION", "content": "As described in Section III, the major goal of our work is to ease the human labor involved in identifying the locations of TADA in a malware executable binary. Accordingly, we want to focus on three research questions: 1) Can our method identify well-known TADAs? 2) Can our method detect TADA in real-world samples? and 3) How much time our method may save for the human analysts? The first two questions are essentially evaluating the effectiveness of our method: whether our method could identify the TADA after all; the third questions is evaluating the amount of human labor may be saved quantitatively.\nOur implementation is based on the workflow introduced in Section IV, which will be elaborated in the next subsection. The output of our workflow is binary for each of the BB: whether a breakpoint should be placed at the beginning of the BB. Every BB that receive an LLM rating of 7 or above is considered as positive BB and a breakpoint shall be placed."}, {"title": "A. Implementation and Malware Samples", "content": "In our experiment, most of our implementation is written in Python. In terms of the libraries and tools, we used IDAPython for disassembling, CFG construction, and IAT parsing; Miasm library for DFG construction and data flow analysis; FLOSS library for string deobfuscation emulation. As for LLM, all our experiment uses GPT4-Turbo from OpenAI. The input parameters are configured to make the model output to be deterministic.\nRegarding the malware samples involved in our experiments, they are all collected from VirusTotal using VirusTotal Intelligence Query."}, {"title": "B. Can our method identify well-known TADAS?", "content": "Despite the diversity of TADA mentioned in Section III-B, there still exists many popular and well-known TADAs, which are commonly adopted by malware authors. Thus, we first need to ensure that our method can identify the BBs implementing these popular TADAs before we explore those interesting tricky ones. We built a set of programs, each of which includes an implementation of one well-known TADA. Instead of implementing everything from scratch, we adopted publicly available implementations of these TADAS from a popular GitHub repository of malware anti-analysis techniques, al-khaser.\nIn total we collected 164 implementations of TADA from al-khaser, and created 164 programs accordingly. According to the categorizations introduced in Section II-B, 164 programs can be categorized by either tactics or implementations of the corresponding TADAs. Since all the TADAs are self-implemented, the compiled programs are not packed. All 164 programs are analyzed using the workflow described in Section IV, except for the malware unpacking components. We consider an implementation of TADA to be detected if and only if at least one BB that is a part of the implementation of the TADA is reported by our tool as positive (e.g., LLM rating greater than 7). Therefore,"}, {"title": "C. Can our method detect TADA in real-world samples?", "content": "It is also very important that our method can detect those TADAs in the real-world malware samples, especially those interesting TADAs. Here, interesting TADAs refer to those TADAs bringing \"surprise\", which will inspect unpopular and unconventional hardware and/or system artifacts. In this section, we study 4 famous real-world malware families that are known to have TADAs. Some of the techniques are introduced and documented in the analysis report due to their creativity and uniqueness. Our tool not only successfully detected the documented TADA in the sample, but also detected some techniques that are not documented in detail. For those who is interested in reproducing the experiment, the hashes are included in Appendix B.\n1) Anti-emulation in Raspberry Robin: We have done analysis on samples of Raspberry Robin (RR), which is a very active malware family. In late 2023, analysts found some RR samples were adopting an interesting method to detect Windows Defender Emulation engines [24].\nRunning our tool against the sample with anti-emulation techniques, we identify a BB that is not so obvious to be related to TADA. This BB mainly consists of the following features:\nAfter researching, we found a blog [24] which indicate the basic block is related to detecting Windows Defender Emulation engine.\n2) Disk Serial Number Check in Zebrocy: An well-known malware family called Zebrocy adopted hardware and environment checking in order to detect the sandbox environment. According to an online analysis blog [19], the malware will not exhibit malicious behavior if one of commonly used disk serial numbers is found.\nRunning our tool against the sample shown in the blog, we are able to detect the first BB of the implementation of this TADA technique. The BB mainly contains following features:\nWe confirmed our findings based on the blog post. It is shown that this is the BB querying the disk serial number, followed by a series of comparisons with known disk serial numbers used by sandboxes.\n3) Comprehensive Hardware Checks in Trickbot: Trickbot is a famous malware family that is very popular around the year of 2020. It is a trojan mainly spread by email, and has been thoroughly studied by the authors of [19]. The most interesting fact of Trickbot in terms of its anti-dynamic-analysis behavior is that this family will stare at the hardware information, carefully and extensively.\nOur tool study the sample with hash listed in [19], and found several BBs that are directly related to TADA. For examples, we found a BB contains code querying WMI:\nIn addition, this sample also tried to detect virtual environments. One example is shown in the following BB:\nBios features are also checked:"}, {"title": "D. How much time our method may save for the human analysts?", "content": "This is a difficult question to answer directly in quantitative manner. Therefore, instead we evaluate this question indirectly by investigating the number of BBs that our tool can find out of a program.\nIn this experiment, we use real-world malware samples collected from Virus Total using the Virus Total intelligence search. Specifically, we query the Virus Total for samples that are Windows PE, at least 20 vendors give malware/adware verdict and with anti-analysis tags (e.g. attack technique T1622 or T1497). In addition, samples are from April 2024 to June 2024, so that they are relatively new at the time of writing and evaluating. To make our analysis statistically significant, we have sampled in total 30 malware/adware samples from Virus Total using the query described earlier.\nWe first dissemble the samples and compute the total number of BBs of each sample. On average, the total number of BBs in a sample is 6869.31, and after analyzing all 30 samples using our tool, we found that on average we can find 12.57 BBs positive for TADA implementations. In these 30 samples, the number of detected BBs is significantly smaller than that of the total number of BBs, whose ratio is about 0.17%. We believe this ratio indicates that finding code of TADA is really finding a needle in a haystack, and our tool should be able to save a good amount of time for the human analysts."}, {"title": "VI. DISCUSSION", "content": ""}, {"title": "A. TADA in benign software", "content": "The presence of TADA does not necessarily mean that a sample is malware. In fact, many benign samples and \u201cgray\u201d samples such as adware will also have advanced TADA. For example, the binary executables for online games usually implement TADAs to detect not only debugger and cheating software, but also virtual machines and emulators, in order to protect their games against cracking for piracy. Therefore, although there are academic researches [7], [8] focusing on detecting malware partially based on the presence of TADA, this practice is extremely rare in the real world, which is prone to false positives."}, {"title": "B. Limitations", "content": "Limitation of static analysis. Since we adopt static analysis, our method shares common limitations of static analysis. This include resolving pointer alias (of indirect calls), vulnerable to file patching and packing obfuscations, and accuracy issues. Nevertheless, as shown in Figure 1, our scope focuses on reverse engineering, so static analysis is an inevitable step. Additionally, in principle, our methodology does not prevent us to extract features using dynamic analysis, and therefore, this would be the future work.\nLimitations of file types. While we are focusing on detecting TADAs in the executable, it is also note-worthy that many anti dynamic analysis efforts may be performed in different file types, such as VBA scripts and JavaScripts. For different file types, it is necessary to re-design the features that may be extracted from the file, which could be a future direction of this field.\nLimited LLM prompt engineering. In this work, we primarily focus on specifying the definition of the TADA in the prompt, and we did not adopt any advanced prompt engineering methods. One future work is to use dynamic few-shot learning during the prompt crafting phase, so that an example set of features of TADA related BB can be added in the prompt. This will enable the LLM to learn from the ground truth of existing known TADA BBs."}, {"title": "VII. RELATED WORK", "content": ""}, {"title": "A. TADA detection", "content": "Anti-debug techniques are used for malware to detect debuggers when malware decides if it stops running and prevents being analyzed. Cobra [25], focused on self-modifying and self-checking code to mitigate anti-debug techniques. Roundy and Miller [26] developed a hybrid analysis technique that combines static and dynamic analysis to overcome anti-debugging and other evasion techniques used by malware. Sunjun Lee et al. [27] proposed a dynamic binary instrumentation approach to detect and bypass anti-debugging techniques in malware analysis. ZeVigilante [28] presented a machine learning-based approach to detect the presence of anti-debugging techniques in malware samples. Xue et al. [29] introduced Malton, a binary analysis platform designed to handle various anti-analysis techniques, including anti-debugging"}, {"title": "APPENDIX A EXTRA CONTEXT USED TO AUGMENT ASSEMBLY FEATURE", "content": ""}, {"title": "A. Mnemonics", "content": "\u2022 pushf: Can be used to read/write EFLAGS register\n\u2022 pushfd: Can be used to read/write EFLAGS register\n\u2022 popf: Can be used to read/write EFLAGS register"}, {"title": "B. Segment Register Access in MS Windows for X86", "content": "\u2022 fs:0h: Current Structured Exception Handling (SEH) frame\n\u2022 fs: 4h: Stack Base / Bottom of stack (high address)\n\u2022 fs: 8h: Stack Limit / Ceiling of stack (low address)\n\u2022 fs: Ch: SubSystemTib\n\u2022 fs: 10h: Fiber data\n\u2022 fs:14h: Arbitrary data slot\n\u2022 fs:18h: Linear address of TEB\n\u2022 fs:1Ch: Environment Pointer\n\u2022 fs: 20h: Process ID (in some Windows distributions this field is used as DebugContext)\n\u2022 fs:24h: Current thread ID\n\u2022 fs:28h: Active RPC Handle\n\u2022 fs: 2Ch: Linear address of the thread-local storage array\n\u2022 fs:30h: Linear address of Process Environment Block (PEB)\n\u2022 fs:34h: Last error number\n\u2022 fs:38h: Count of owned critical sections\n\u2022 fs: 3Ch: Address of CSR Client Thread\n\u2022 fs:40h: Win32 Thread Information\n\u2022 fs: 44h: Win32 client information (NT), user32 private data (Wine)\n\u2022 fs: C0h: Pointer to FastSysCall in Wow64\n\u2022 fs:C4h: Current Locale\n\u2022 fs:C8h: FP Software Status Register\n\u2022 fs: CCh: Reserved for OS (NT), kernel32 private data (Wine)\n\u2022 fs:1A4h: Exception code\n\u2022 fs:1A8h: Activation context stack\n\u2022 fs:6E8h: Real Process ID\n\u2022 fs:6ECh: Real Thread ID"}, {"title": "APPENDIX B SAMPLE HASHES", "content": "Raspberry Robin: 242851abe09cc5075d2ffdb8e5\neba2f7dcf22712625ec02744eecb52acd6b1bf\nZebrocy: 091ffdfef9722804f33a2b1d0fe765d2c2\nb0c52ada6d8834fdf72d8cb67acc4b\nTrickbot:\n3bf0f489250eaaa99100af4fd9cce3a\n23acf2b633c25f4571fb8078d4cb7c64d\nMustung Panda: 8f9581a80cd18e2bbe33ba0fe29b\n778c8125e30f10a81071019dcd46bc3a2d34"}]}