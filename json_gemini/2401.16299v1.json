{"title": "Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation", "authors": ["Vishal Dey", "Xia Ning"], "abstract": "Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients (RCGrad), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNS for molecular property prediction.", "sections": [{"title": "1 Introduction", "content": "Accurate prediction of molecular properties is pivotal in drug discovery [Wieder et al., 2020], as it accelerates the identification of potential molecules with desired properties. Developing computational models for property prediction relies on learning effective representations of molecules [David et al., 2020]. In this regard, Graph Neural Networks (GNNs) have shown impressive results in learning effective representations for molecular property prediction tasks [Gasteiger et al., 2021; Wang et al., 2022b; Guo et al., 2023]. Inspired by the paradigm of pretraining followed by fine-tuning, widely recognized for its impact in natural language understanding [Radford et al., 2018; Wei et al., 2022], molecular GNNS are often pretrained [Hu et al., 2019] on a large corpus of molecules. Such a corpus might encompass irrelevant data for the target property prediction task. This can lead the GNNs to learn features that do not benefit the target task. Consequently, pretrained GNNs are fine-tuned with the target task to encode task-specific features. However, vanilla fine-tuning can potentially lead to poor generalization, particularly when dealing with diverse downstream tasks, limited data, and the need to generalize across varying scaffolds [Wu et al., 2018].\nTo improve generalization, auxiliary learning has recently garnered attention [Liebel and K\u00f6rner, 2018; Liu et al., 2019a; Dery et al., 2022]. Auxiliary learning leverages informative signals from self-supervised tasks on unlabeled data, to improve the performance of the target tasks. However, its application in the context of molecular graphs, specifically for molecular property prediction, remains largely unexplored. Following this line of work, in this paper, we explore how to adapt pretrained molecular GNNs by combining widely-used self-supervised tasks with the target task using respective task-specific data (with self-supervised and target task labels). However, a critical challenge in such an adaptation is caused by negative transfer[Rosenstein et al., 2005], where auxiliary tasks might impede rather than aid the target task [Ruder, 2017; Du et al., 2018].\nTo address this challenge, we develop novel gradient surgery-based adaptation strategies, referred to as Rotation of Conflicting Gradients (RCGrad) and Bi-level Optimization with Gradient Rotation (BLO+RCGrad). Such strategies mitigate negative transfer from auxiliary tasks by learning to align conflicting gradients. Overall, our adaptation strategies improved the target task performance by as much as 7.7% over vanilla fine-tuning. Moreover, our findings indicate that the developed adaptation strategies are particularly effective in tasks with limited labeled data, which is a common challenge in molecular property prediction tasks. Our comprehensive investigation of multiple adaptation strategies for pretrained molecular GNNs represents a notable contribution in addressing the limited benefit of pretrained GNNs [Sun et al., 2022], and in improving generalizability across a diverse set of downstream tasks with limited data."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Pretraining and fine-tuning GNNs", "content": "Pretraining followed by fine-tuning is widely used to leverage knowledge gained from related tasks and to improve model generalization. Typically, it involves training a model on large-scale data with self-supervised or supervised tasks, and then fine-tuning it on a small-scale labeled data. Following the success of pretraining and fine-tuning paradigm in various domains [Liu et al., 2019b; Floridi and Chiriatti, 2020], researchers have extended it to molecular GNNs [Hu et al., 2019; Hu et al., 2020; Liu et al., 2021; Wang et al., 2022b]. In this regard, researchers have designed a number of self-supervised tasks as pretraining tasks that focus on capturing diverse chemical rules, connectivities, and patterns at varying granularities: on node, subgraph and graph levels [Xia et al., 2022b]. Although pretrained GNNs showed promise in capturing diverse chemical knowledge, the challenge lies in effectively extracting this knowledge relevant to the target task, which is often non-trivial through vanilla fine-tuning. Specifically, such fine-tuning often leads to overfitting [Xia et al., 2022a]. Contrary to the observations in domains such as natural language processing (NLP) and computer vision, where pretrained models consistently yield substantial improvements, pretrained GNNs do not exhibit such improvement [Sun et al., 2022].\nThis could be due to a notable research gap in determining what self-supervised molecular tasks can better benefit the downstream target tasks. In fact, prior studies in pretraining molecular GNNs mostly leverage one or two self-supervised task(s), thereby resulting in a plethora of multiple pretrained GNNs. Interestingly, such pretrained GNNs capture different knowledge [Wang et al., 2022a] and excel in different downstream molecular property prediction tasks [Sun et al., 2022]. Additionally, Sun et al. [Sun et al., 2022] recently demonstrated that self-supervised graph pretraining does not consistently/significantly outperform non-pretraining methods across various settings. Overall, although pretrained GNNs hold promise for molecular property prediction, their benefit over non-pretrained models seems limited. To address this, some recent attempts [Xia et al., 2022a; Zhang et al., 2022] to fine-tune pretrained GNNs have largely relied on existing ideas like regularization [Xuhong et al., 2018] or update constraints [Houlsby et al., 2019] during fine-tuning. In contrast, our proposed approaches leverage auxiliary tasks to learn generalizable knowledge and prevent overfitting to the training set."}, {"title": "2.2 Knowledge Transfer with Auxiliary Learning", "content": "Knowledge transfer through auxiliary learning has demonstrated its effectiveness across a spectrum of domains [Trinh et al., 2018; Nediyanchath et al., 2020; Lee, 2021]. This paradigm, distinct from multi-task learning, aims to optimize the target task's performance while leveraging auxiliary tasks to bolster generalization [Shi et al., 2020]. Prior research in other domains has developed multiple methods to automatically learn task weights, such as using gradient similarity [Dery et al., 2021a; Du et al., 2018], using parameterized auxiliary network [Navon et al., 2020; Dery et al., 2022], using bi-level optimization and implicit differentiation [Navon et al., 2020; Chen et al., 2022], minimizing distances between task embeddings [Chen et al., 2021], or from the perspective of Nash equilibrium [Shamsian et al., 2023]. However, the application of auxiliary learning for adapting molecular GNNs to target tasks, particularly in the context of molecular property prediction, remains an under-explored area. In this study, we adopt and explore gradient similarity, gradient scaling, and bi-level optimization strategies."}, {"title": "3 Preliminaries", "content": "Motivated by the success of continued pretraining and task-specific adaptation in pretrained Large Language Models (LLMs) [Gururangan et al., 2020; Dery et al., 2021b; Yang et al., 2022], we investigate adaptation of off-the-shelf pretrained molecular GNNs to target molecular property prediction tasks. Via such an adaptation, we aim to leverage existing self-supervised (SSL) tasks designed for molecular GNNs and transfer learned knowledge from such tasks to the target task. We employ the existing SSL tasks typically used in molecular pretraining such as masked atom prediction (AM), context prediction (CP) [Hu et al., 2019], edge prediction (EP) [Hamilton et al., 2017], graph infomax (IG) [Sun et al., 2019], and motif prediction (MP) [Rong et al., 2020]. We refer to these tasks as auxiliary tasks. Intuitively, these auxiliary tasks can potentially capture diverse chemical semantics and rich structural patterns at varying granularities. By utilizing SSL objectives on target task-specific data, auxiliary tasks augment the pretrained GNNs with richer representations. Such representations, in turn, can improve the generalizability of the target property prediction task. Henceforth, the term \"GNN\" refers to an off-the-shelf pretrained molecular GNN.\nFigure 1 presents an overview of the adaptation setup. Formally, we adapt a GNN with parameters \u0398 to optimize the performance on the target task $T_t$. We achieve this by jointly training $T_t$ with auxiliary tasks $\\{T_{a,i}\\}_{i=1}^k$ through solving the following optimization problem:\n$\\min_{\\Theta,\\Psi,\\Phi_i \\in \\{1..k\\}} L_t + \\sum_{i=1}^k w_i L_{a,i},$\nwhere $L_t$ and $L_{a,i}$ denote the target task loss and i-th auxiliary task loss, respectively, \u03a8 and $\\Phi_{i \\in \\{1,...,k\\}}$ denotes task-specific learnable parameters for the target and i-th auxiliary task, respectively, and $w$ is the weight indicating the influence of the auxiliary tasks on the target task. Through the above optimization, all the parameters are simultaneously updated in an end-to-end manner. Note that the above optimization does not optimize $w$\u2013 we will introduce an approach that can additionally learn $w$ in Section 4.2. In fact, the key to effective adaptation lies in accurately determining $w$, such that the combined task gradients can backpropagate relevant training signals to the shared GNN as follows:\n$\\Theta^{(t+1)} := \\Theta^{(t)} - \\alpha \\big(g_t + \\sum_{i=1}^k w_i g_{a,i}\\big),$ where $g_t = \\nabla_{\\Theta} L_t$, and $g_{a,i} = \\nabla_{\\Theta} L_{a,i}$ denote the gradients updating \u0398 from the target and i-th auxiliary task, respectively, and \u03b1 denotes the learning rate. Our proposed adaptation strategies focus on learning such $w$ in an end-to-end manner, to dynamically combine task gradients during each update. These strategies contrast with those using fixed weights or conducting expensive grid-search to explore all possible $w$."}, {"title": "3.1 Gradient Cosine Similarity (GCS)", "content": "The first strategy to meaningfully combine task gradients is based on gradient cosine similarity (GCS) [Du et al., 2018]. Intuitively, GCS measures the alignment between task gradients during training, providing insights into the relatedness of auxiliary tasks with the target task. A high GCS indicates that the auxiliary tasks provide complementary information, and thus, can benefit the target task. Conversely, low GCS indicates potential orthogonality or even conflict between tasks. Thus, GCS can naturally quantify the relatedness of auxiliary tasks with the target task over the course of training. We compute GCS and update \u0398 as:\n$\\Theta^{(t+1)} := \\Theta^{(t)} - \\alpha \\big(g_t + \\sum_{i=1}^k \\max \\big(0, \\cos (g_t, g_{a,i})\\big) g_{a,i}\\big),$ where, max operator takes the maximum out of the two values, thereby, dropping the tasks with conflicting gradients (i.e., with negative GCS)."}, {"title": "3.2 Gradient Scaling (GNS)", "content": "We also adopt a simpler strategy of gradient scaling [He et al., 2022] to adjust the influence of auxiliary tasks with respect to the target task. Our preliminary experiments as presented in Figure 2 revealed significant differences in the scales of the task gradient norms, and thus requiring careful adjustments. This is because if the gradient of an auxiliary task is much larger than that of the target task, updates will be most dominated by such auxiliary tasks, thereby potentially resulting in worse target performance. On the other hand, if the gradient of an auxiliary task is relatively small, the training signals from such auxiliary tasks will be too weak to encode any relevant features in \u0398. Thus, following [Chen et al., 2018; He et al., 2022], we use a simple gradient scaling to dynamically adjust the influence of auxiliary tasks during updates of \u0398 as follows:\n$\\Theta^{(t+1)} = \\Theta^{(t)} - \\alpha \\Big(g_t + \\sum_{i=1}^k \\frac{||g_t||}{ \\max \\Big(1, \\frac{||g_{a,i}||}{||g_t||}\\Big)} g_{a,i}\\Big),$ where || || denotes the l-2 norm."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Rotation of Conflicting Gradients (RCGrad)", "content": "While both conflicting directions and magnitude differences of task gradients can lead to negative transfer, GCS and GNS focus separately on homogenizing either the direction or magnitude of gradients, rather than in a unified manner. To address these limitations, we develop Rotation of Conflicting Gradients (RCGrad) a novel extension of PCGrad [Yu et al., 2020] \u2013 that aligns gradients both in terms of direction and magnitude. RCGrad, which builds upon PCGrad, does not completely discard gradients conflicting with the target task, unlike GCS. Instead, RCGrad only negates the component of the conflicting gradient that is completely opposite to the target task gradient. Additionally, RCGrad explicitly learns how much of the non-conflicting component should be incorporated for the most effective knowledge transfer. This mitigates negative transfer by not only removing the conflicting component but also by learning to incorporate a portion of the non-conflicting component.\nFigure 3 demonstrates the difference between PCGrad and RCGrad. Formally, RCGrad learns to rotate auxiliary gradient $g_{a,i}$ by angle $\u03b8_i$ to yield a rotated gradient $R(\u03b8_i)g_{a,i}$, which is followed by an orthogonal projection in case of conflicts"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Materials", "content": "We perform experiments on 8 benchmark classification datasets from MoleculeNet [Wu et al., 2018]. We compare our adaptation strategies with simple baselines such as traditional fine-tuning (FT), and vanilla multi-task learning (MTL) that assigns equal weights to all auxiliary tasks; and a more advanced state-of-the-art regularization-based fine-tuning with optimal transport (GTOT) [Zhang et al., 2022]. Additionally, we consider other state-of-the-art gradient surgery-based methods (GCS, GNS, PCGrad) as baselines. We refer to this group of baselines collectively as GS methods. We use the official publicly available checkpoints\u00b9 of two GNNs: 1) supervised_contextpred [Hu et al., 2019], denoted as Sup-CP, which is pretrained via self-supervised context prediction and supervised graph-level multi-task learning, and 2) supervised [Hu et al., 2019], denoted as Sup, which is pretrained only via supervised graph-level multi-task learning. Using such different pretrained GNNs allows a controlled comparison to understand how different pretraining objectives (with and without self-supervised context prediction task) can influence the adaptation. Details on auxiliary tasks and datasets are presented in Section B in Supplementary."}, {"title": "5.2 Reproducibility and Implementation Details", "content": "Following the prior line of research [Hu et al., 2019; Liu et al., 2021], we use scaffold-split for the downstream target tasks, and use the same atom and bond features as in GTOT. All experimental details for the FT baseline follow the GTOT fine-tuning setup. Specifically, we initialized a linear projection layer on top of the pretrained GNN as the target task classifier. Across all methods, both the pretrained GNN and task-specific layers are trainable. For FT and adaptation methods, we train the models for 100 epochs with Adam optimizer with an initial learning rate \u03b1 of 0.001, we use a batch size of {32, 64, 256}, an embedding dimension of 300, and a dropout probability of 0.5 for the GNN module. For GTOT experiments, we use the optimal hyper-parameters provided for each dataset, when finetuned on Sup-CP. For MTL experiments, we assign equal weights to all auxiliary tasks. For BLO and BLO+RCGrad experiments, we use M = 3 in Algorithm 2, update $w$ every r = {5, 10, 20} update of \u0398, and use Adam optimizer with learning rate \u03b2 of 0.001 to update w. The code is available at https://github.com/vishaldeyiiest/GraphTA."}, {"title": "5.3 Comparison using Sup-CP as the pretrained GNN", "content": "Table 1 presents an overall comparison when all the auxiliary tasks are used with Sup-CP as the pretrained GNN. Our proposed adaptation strategies, specifically RCGrad and BLO+RCGrad, outperform all baselines, including other GS-based adaptation strategies, across all datasets (except ClinTox). Specifically, compared to the best fine-tuning method, GTOT, RCGrad demonstrated significant improvement of 2.4% and 4.8% in BACE and BBBP, respectively. This indicates the efficacy of our proposed rotational alignment in mitigating negative transfer and improving the generalizability of the pretrained GNN. Furthermore, BLO+RCGrad exhibits significant improvement over fine-tuning methods FT and GTOT in small-scale datasets of as much as 6.3% and 4.1%, respectively. This highlights the efficacy of bi-level optimization combined with gradient rotation in improving generalizability, especially in limited data regimes.\nAdditionally, RCGrad and BLO+RCGrad consistently outperform other gradient surgery-based (GS) methods. Specifically, compared to PCGrad, RCGrad demonstrates statistically significant improvements in ROC-AUC by 2.5%, 4.7%, 0.9% and 1.0% in ClinTox, BBBP, Tox21, and ToxCast, respectively. This improvement can be attributed to the rotation component in RCGrad, which not only resolves gradient conflicts but also actively aligns them in a direction favorable to the target task. Moreover, our proposed methods RCGrad and BLO+RCGrad learn to retain a component of the conflicting task gradients, unlike GCS which completely discards conflicting gradients. This ensures that valuable information from auxiliary tasks is not discarded, thus facilitating more effective knowledge transfer.\nConversely, BLO, which learns task weights without explicitly handling gradient conflicts, performs comparably or slightly worse than RCGrad, BLO+RCGrad, and other GS-based baselines. The suboptimal performance of BLO, especially in smaller datasets (e.g., SIDER), may be attributed to the noisy nature of task gradients, potentially leading to a poor approximation of hyper-gradients. In contrast, GNS is more robust to noisy gradients since it adjusts the scale of gradient magnitudes relative to the target task. Overall, our proposed methods consistently outperform all baselines on smaller datasets (except ClinTox), while achieving competitive performance on larger ones.\nIn contrast, MTL, which assigns equal weights to all auxiliary tasks regardless of their relevance to the target task, results in worse performance across all downstream tasks. Compared to FT, MTL exhibits deteriorations of as much as 9.1% and 20.6% in SIDER and ClinTox, respectively. This indicates that MTL leads to drastic negative transfer, where the auxiliary tasks hurt the performance of the target task. On the contrary, all adaptation strategies (including GS-based baselines) perform better than MTL with significant improvements of up to 24.2%. Furthermore, upon analyzing gradient similarities of auxiliary tasks with the target task (Figure 4), we hypothesize that AM, IG, and MP may benefit the target task better than the other auxiliary tasks."}, {"title": "5.4 Comparison using Sup as the pretrained GNN", "content": "Table 3 presents an overall comparison of adaptation of Sup as the pretrained GNN using all auxiliary tasks. Similar to our findings in the previous section, MTL again results in worse performance compared to fine-tuning methods, thus indicating negative transfer. On the other hand, our proposed methods, specifically RCGrad and BLO+RCGrad, demonstrate improved performance over fine-tuning and GS baselines. Notably, compared to the best fine-tuning baseline GTOT, BLO+RCGrad improved ROC-AUC by 6.8%, 2.2%, and 4.8% in ClinTox, BACE, and BBBP, respectively. Similarly, compared to the best GS baseline GNS, BLO+RCGrad demonstrates notable improvement of 3.0%, 11.1%, and 1.0% in SIDER, ClinTox, and BACE, respectively. Furthermore, compared to BLO, which does not explicitly handle conflicting task gradients, BLO+RCGrad yields consistent improvement across most datasets. Such consistently superior performance of BLO+RCGrad implies that aligning and extracting informative components out of conflicting task gradients is crucial to improve the generalizablity of pretrained GNNs, regardless of the specific pretraining objective.\nFollowing the similar setup of Sup-CP experiments with a selected subset of auxiliary tasks, Table 5 in Supplementary presents an overall comparison using Sup as the pretrained GNN. Compared to the previous setup with all auxiliary tasks, almost all GS baselines and our proposed method RCGrad exhibit improved performance with fewer auxiliary tasks. This suggests that using a smaller and relevant set of auxiliary tasks can lead to more efficient adaptation, which holds true across different pretrained GNNs. Furthermore, compared to the best GS baseline, GNS, our proposed methods RCGrad and BLO+RCGrad achieve better or comparable performance, particularly on smaller datasets. Additionally, BLO+RCGrad exhibits significant improvement over GCS in Tox21 and ToxCast.\nHowever, it's worth noting that when using Sup as the pretrained GNN, all methods, including RCGrad and BLO+RCGrad, yield slightly worse performance compared to when Sup-CP is used as the pretrained GNN. This observation suggests that the Sup pretrained GNN might not capture contextual chemical relationships as effectively as Sup-CP, which was pretrained additionally on the context prediction task. This subtle difference in performance indicates that the choice of pretrained GNN can have an impact on the overall adaptation process. Additional results are presented in Section B in Supplementary materials."}, {"title": "6 Conclusion and Future Work", "content": "In this study, we explored multiple adaptation strategies to improve the performance of pretrained GNNs on downstream molecular property prediction tasks. To address the poor generalization performance to such diverse downstream tasks, we introduced two novel methods, RCGrad and BLO+RCGrad, that learn to align conflicting task gradients. Our experiments demonstrate that our proposed methods consistently outperform all fine-tuning and gradient surgery-based approaches, especially on smaller datasets (except ClinTox). This suggests that the adaptation of pretrained GNNs can be a promising direction to boost target task performance, especially with limited labeled data. Our study serves as the first step in exploring the adaptation of pretrained GNNs in molecular property prediction. In future work, we will explore other adaptation strategies to alleviate noisy gradients and to improve task selection with sparser task weights. We will further investigate the benefit of adapting GNNs to diverse downstream molecular regression tasks."}, {"title": "A Details on BLO", "content": "Algorithm 1 describes the training process of BLO, and Algorithm 2 describes the computation of the gradient $\\nabla_w L_t(A)$ via approximated Hessian Inverse and vector products."}, {"title": "B Experimental Details", "content": ""}, {"title": "B.1 On Auxiliary Tasks", "content": "We describe the auxiliary tasks and share key insights behind using them:\n\u2022 Masked Atom Prediction (AM): AM [Hu et al., 2019] involves predicting the identity of masked atoms within a molecular graph. It helps the GNN to learn the local chemical context and relationships between atoms and bonds, which are crucial for understanding molecular structure and function. The embedding out of GNN is fed to a linear classifier to predict the atom type of masked atoms.\n\u2022 Edge Prediction (EP): EP [Hamilton et al., 2017] focuses on predicting the presence or absence of bonds (edges) between pairs of atoms in a molecular graph. It helps the GNN to capture essential local structural information, including connectivity and spatial arrangement of atoms within molecules. Following existing design[Sun et al., 2022], the dot product of node embeddings is used to predict the existence of a bond.\n\u2022 Context Prediction (CP): CP [Hu et al., 2019] requires the model to predict neighboring graph structures (context) based on an anchor structure. This aids the GNN in distinguishing molecular contexts, enabling the model to capture subgraph-level information. The setup of Hu et al.[Hu et al., 2019] is followed to extract and distinguish positive and negative subgraph contexts.\n\u2022 Graph Infomax (IG): IG [Sun et al., 2019] maximizes the mutual information between local (node) and global (subgraph) representations. This helps the GNN to capture structural patterns, allowing it to understand how atoms form functional groups and larger molecular substructures. The existing setup [Sun et al., 2019] is followed to train a discriminator model that distinguishes between node embeddings from the same molecular graph and those from a different graph.\n\u2022 Motif Prediction (MP): MP [Rong et al., 2020] focuses on predicting the presence of specific recurring substructures (motifs) within a molecule. It helps the GNN to identify structural motifs indicative of chemical properties or functions. This task is formulated as a multi-label binary classification problem with each of 85 motifs\u00b2 extracted from RDKIT [RDKit, online, ] as labels."}, {"title": "B.2 Dataset Overview", "content": "We perform our adaptation experiments on 8 benchmark classification datasets from MoleculeNet [Wu et al., 2018]. In this section, we give a brief overview and provide preliminary statistics of these datasets.\n\u2022 BBBP: measures whether a molecule permeates the blood-brain barrier.\n\u2022 BACE: measures whether a molecule inhibit the \u03b2-secretase 1 (BACE-1) enzyme.\n\u2022 ClinTox: contains toxicity labels for clinical drugs, facilitating the assessment of drug safety profiles across various targets. It is important to note that these labels reflect both FDA approval outcomes and clinical trial failures due to toxicity. Such outcomes are determined by not just the molecular structures of the drugs. but also by external factors such as genetic predispositions, evaluation methodologies, and environmental conditions. This complexity can make methodological comparisons challenging.\n\u2022 HIV: measures whether a molecule can prevent antiviral activity against the HIV virus.\n\u2022 MUV: compiled and refined from PubChem bioassays, evaluating compound activity across multiple targets."}]}