{"title": "Rule-based Data Selection for Large Language Models", "authors": ["Xiaomin Li", "Mingye Gao", "Zhiwei Zhang", "Chang Yue", "Hong Hu"], "abstract": "The quality of training data significantly impacts the performance of large language models (LLMs). There are increasing studies using LLMs to rate and select data based on several human-crafted metrics (rules). However, these conventional rule-based approaches often depend too heavily on human heuristics, lack effective metrics for assessing rules, and exhibit limited adaptability to new tasks. In our study, we introduce an innovative rule-based framework that utilizes the orthogonality of score vectors associated with rules as a novel metric for rule evaluations. Our approach includes an automated pipeline that first uses LLMs to generate a diverse set of rules, encompassing various rating dimensions to evaluate data quality. Then it rates a batch of data based on these rules and uses the determinantal point process (DPP) from random matrix theory to select the most orthogonal score vectors, thereby identifying a set of independent rules. These rules are subsequently used to evaluate all data, selecting samples with the highest average scores for downstream tasks such as LLM training. We verify the effectiveness of our method through two experimental setups: 1) comparisons with ground truth ratings and 2) benchmarking LLMs trained with the chosen data. Our comprehensive experiments cover a range of scenarios, including general pre-training and domain-specific fine-tuning in areas such as IMDB, Medical, Math, and Code. The outcomes demonstrate that our DPP-based rule rating method consistently outperforms other approaches, including rule-free rating, uniform sampling, importance resampling, and QuRating, in terms of both rating precision and model performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have been widely utilized across a diverse range of applications. Pretraining and fine-tuning these models typically require large and diverse datasets. Studies have found that data quality is critical for training good LLMs (Brown, 2020; Chowdhery et al., 2023; Du et al., 2022; Dubey et al., 2024; Wenzek et al., 2019). For instance, Meta's LIMA paper (Zhou et al., 2024) demonstrated that using only 1000 carefully curated data samples can achieve better performance than using the original 50k samples. Similar phenomena have been observed in other studies where selecting a subset of high-quality datasets increases the training convergence and model performance (Cao et al., 2023; Hsieh et al., 2023; Xie et al., 2024; Sachdeva et al., 2024; Zhang et al., 2023; Javaheripi et al., 2023).\nRecent studies now adopt an approach that employs LLMs to grade data quality according to a set of designed metrics (which we call rules) (Yuan et al., 2024; Wettig et al., 2024; Bai et al., 2022; Mu et al.). For example, Wettig et al. (2024) rates the pre-training data using LLMs according to four predefined rules. RedPajama (Together AI, 2023) is continuously developing a pool of rules for users to select from, which currently contains over 40 basic criteria that LLM data should satisfy. On specific aspects such as safety, Constitutional AI (Bai et al., 2022) proposed their \u201cconstitution\" a set of standard safety criteria-to generate safe synthetic data, and most recently, OpenAI's Rule-based Rewarding (Mu et al.) proposed 21 general safety rules and injected them into the RLHF (reinforcement learning with human feedback) process. This rule-based rating provides greater explainability of data quality and breaks down the challenge of assigning a data point one overall quality score into a simpler task of giving several rule-specific scores."}, {"title": "2 Related Work", "content": "LLM data selection. There are different genres of data selection approaches for LLMs. Basic filterings, such as setting thresholds on word lengths, are used in many studies to eliminate low-quality data (Soldaini et al., 2024; Wenzek et al., 2019; Raffel et al., 2020; Conneau & Lample, 2019; Penedo et al., 2023; Lauren\u00e7on et al., 2022). Fuzzy deduplication is another approach which removes repetitive or similar data samples (Allamanis, 2019; Lee et al., 2021; Abbas et al., 2023; Gao et al., 2020; Jiang et al., 2022). Another method is \u201cheuristic classification\", selecting data based on a predefined quality score, typically measured by similarity to formal sources such as Wikipedia or other human-generated, high-quality datasets (Brown, 2020; Touvron et al., 2023; Chowdhery et al., 2023; Du et al., 2022; Gao et al., 2020; Wenzek et al., 2019). In contrast to this, directly querying LLMs to rate data and use the scores as the quality indicator has become a standard practice in many studies (Li et al., 2023a; Chen et al., 2023; Bai et al., 2022; Wettig et al., 2024; Yuan et al., 2024; Dubois et al., 2024; Li et al., 2023b; Fernandes et al., 2023).\nRule-based rating. There are studies adopting a more fine-grained approach to data quality, distilling it into a finite set of metrics which we refer to as \"rules\". For instance, RedPajama (Together AI, 2023) provides over 40 quality rules that serve as basic quality metrics for the users to choose from. More pertinent to our research, there are papers that apply this rule-based idea to rate LLM data. For example, Yuan et al. (2024) assigns a score out of 5 to each data point, awarding 1 point for each of the 5 predefined criteria met. In Wettig et al. (2024), the authors designed four general rules to rate and select data for LLM pre-training. (Sun et al., 2024) proposed 16 human-crafted rules to evaluate the desirable quality of response data. The rule-based approach is also utilized in more targeted applications, such as ensuring data safety. Constitutional AI designed 16 general safety critique rules to revise synthetic data, enhancing data safety (Bai et al., 2022). This revision process involves iterative steps where a random subset of rules from the \"constitution\" (the entire set of rules) is applied. Additionally, in (Mu et al.), the score generated by an LLM grader according to a set of 21 safety rules is integrated directly into the RLHF process as an additional reward. As noted earlier in the introduction, the rules employed in the literature exhibit several critical issues. They often depend heavily on human heuristics for design, lack robust rule evaluation metrics and exploration of rule sizes, and demonstrate limited versatility for new tasks or for customization. Our goal is to address these challenges using our proposed framework."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Definitions and Notations", "content": "We introduce the definitions of the primary objects considered in our method:\n\u2022 R: the total number of available rules.\n\u2022r: the number of selected rules, using a specified rule selection method.\n\u2022 D: the set of all data samples, with its size denoted by $N \\stackrel{\\text { def }}{=}|D|$.\n\u2022BCD: a batch of data samples, randomly selected for evaluating the correlation of rules during the rule selection step, with its size denoted by $n \\stackrel{\\text { def }}{=}|B|$.\n\u2022 S\u2208 Rn\u00d7R: the rating matrix S where each entry Si,j represents the score of the i-th data sample according to the j-th rule and is constrained to the interval [0, 1].\n\u2022 \u0160\u2208 Rnxr: a submatrix of S consisting of ther selected columns from S, corresponding to the r selected rules."}, {"title": "3.2 Determinantal point process (DPP)", "content": "The optimal solution to this mathematical problem of selecting the most orthogonal subset of a set of vectors is NP-hard (Civril & Magdon-Ismail, 2007; Kulesza et al., 2012) but we use DPP sampling to provide a relatively good solution. The determinantal point process (DPP) is a probabilistic model that describes the likelihood of selecting diverse subsets from a larger set (Macchi, 1975; Borodin & Olshanski, 2000). Mathematically, a DPP is defined by a kernel matrix that describes the similarities between elements in a set. The probability of selecting a particular subset is proportional to the determinant of the corresponding submatrix of this kernel matrix. Intuitively, subsets with highly similar items (leading to higher correlation in the submatrix) have smaller determinants and are thus less likely to be chosen.\nDPP Definitions. Given a discrete ground set y, without loss of generality we let y = {1,2,..., R}, a (discrete) DPP defines a probability measure over 2, the power set of Y. Let Y be a randomly chosen subset. Then for any subset AC Y, the probability of A being chosen by a DPP is given by:\n$P(ACY) = det(KA)$\nwhere K \u2208 RR\u00d7R is a real positive-semidefinite matrix called the kernel matrix and $KA \\stackrel{\\text { def }}{=} [K]_{i,j \\in A}$ is the submatrix of K indexed by elements in A.\nKernel Matrix. Each entry Kij in the kernel matrix K describes the similarity between elements i and jin Y. For our purpose of selecting orthogonal rules, we will define K as the Gram matrix of the score vectors: $K \\stackrel{\\text { def }}{=} S^T S$.\nDPP Sampling. To sample a diverse subset using DPP, there are several existing algorithms (Hough et al., 2006; Kulesza et al., 2012; Tremblay et al., 2018) and the Python library DPPy (Gautier et al., 2019) implements some of them. The computation of the DPP sampling primarily hinges on the overhead of"}, {"title": "3.3 DPP rule-based rating algorithm", "content": "The pipeline of our rule-based data selection method is illustrated in Figure 1 and comprises the following steps:\nStep 1. Rule generation. We query GPT-4 to generate R rules. In the prompt, we include the goal, the description of the source data, and the description of the downstream task to help GPT-4 generate relevant task-related rules.\nStep 2. Rule-based rating: Recall the definitions in Section 3.1. We employ LLM, particularly Llama3-8B-Instruct (AI@Meta, 2024), to rate the batch data B according to R rules, resulting in the matrix S\u2208RnXR.\nStep 3. Rule selection using DPP: From S, we aim to select r relatively independent columns using a DPP, forming the submatrix S\u2208 Rnxr. We define the kernel matrix of DPP as follows:\n$K \\stackrel{\\text { def }}{=} S^T S \\in \\mathbb{R}^{R \\times R}$\n(2)\nwhere each entry $K_{i,j} = \\langle S_i, S_j \\rangle$ (each Si is the i-th column of S), representing the similarity between rule i and rule j. We then employ the DPP sampling algorithm to select r indices from {1, 2, . . ., R}, corresponding to the r chosen rules.\nNote that the cost of generating R rules is negligible, requiring just a single GPT-4 query, and the cost of obtaining the rating matrix S can be managed by adjusting the batch size n. The motivation to select a fixed small number of r rules is driven by the computational costs associated with using LLMs for data rating and the need to maintain a consistent dimensionality for explaining data quality. These practical considerations lead us to treat ras a hyperparameter. Discussions on the optimal choices of rare explored in Section 4 and Appendix A.6.3.\nAnother important remark is that, even with the same set of rules, they could have different correlations conditioned on a specific task or dataset. Therefore during DPP selection, instead of employing fixed representations such as semantic encodings which result in static rule representations and selections across all tasks we use task-aware score vectors to adaptively represent the rules. These vectors allow the entire pipeline to be customized for a particular downstream task.\nStep 4. Stochastic data selection: We extend the rating process to cover all data samples using the selected r rules, expanding the rating matrix \u015c from n\u00d7r to Nxr. We then aggregate these fine-grained ratings by averaging across the r columns of S, resulting in a score vector v = [V1, V2, ..., vn] that assigns a quality score to each of the N samples.\nGiven the N scores and a fixed budget of selecting k samples for training, rather than choose the traditional top-k approach, (selecting the k highest scored samples), we adopt a stochastic sampling strategy,"}, {"title": "4 Evaluation A: Evaluating Against Ground Truth Ratings", "content": "We evaluate our method in two ways: A. by comparing the rating results against the ground truth rating of the dataset. Smaller deviations from the ground truth scores indicate better performance. Specifically, we rely on pairwise comparisons generated by GPT-4 and apply the Bradley-Terry model (Bradley & Terry, 1952) to compute n scores, treating them as the ground truth. B. by training an LLM (Llama3-8B) with the selected data and assessing its performance through both general and domain-specific benchmarks. In this section, we present the first set of experiments (corresponding to Evaluation A), while the second set of experiments (based on Evaluation B) is discussed in Section 5. The low cost of Evaluation A enables us to explore various aspects such as the rule-size scaling law, different rating schemes (pairwise vs. single), and the impact of model sizes (Llama3-8B and Llama3-70B). These experiments provide preliminary evaluations of our method."}, {"title": "4.1 Experiments Setup", "content": "Datasets: We consider two datasets: Common Crawl (Common Crawl, 2024), containing raw web-crawled data, and IMDB (Maas et al., 2011), a dataset of 50K movie reviews, representing general and domain-specific settings, respectively. For each dataset, we collect the first 50 examples and apply a pairwise comparison scheme for data rating (prompt templates are available in Appendix A.5.5), which requires comparison on 2,450 ordered pairs.\nGround truth scores: Ground truth scores are generated as follows: we prompt GPT-4 to compare each pair of data samples (i, j) and then reversing the comparison for (j, i). We only keep the pairs where both comparisons are consistent, filtering out cases where GPT-4 performs poorly. After filtering, approxi-mately 1000 comparisons remain for CommonCrawl and 1800 for IMDB. From these outcomes, we calculate scores for the 50 samples using the Bradley-Terry model (Bradley & Terry, 1952) (details can be found in Appendix A.5.1).\nRating: Now with the ground truth scores, we use our rule-based approach to rate the same data. For each rule i \u2208 {1,2,..., R} (R = 50), we employ Llama3-8B-Instruct as our comparison rater and similarly use the Bradley-Terry model to compute a score vector Si \u2208 Rn (n is also 50 here), thereby forming the rating matrix S\u2208 Rn\u00d7R. Recall we denote S as the submatrix of S containing r columns indexed by the r selected rules. To assess the rating results in S against the ground truth, we compute the mean squared error (MSE):\n$\\epsilon(\\check{S}) \\stackrel{\\text { def }}{=} \\frac{1}{r} \\sum_{j=1}^r \\| \\frac{1}{n} \\check{S}\\_j - S\\_{GT} \\|_2^2$\n(4)\nwhere SGT \u2208 Rn is the ground truth score vector and $$\\check{S}\\_j$$ is the j-th column of \u0160. Furthermore, to establish comparative baselines, we implemented the same rating procedure (pairwise comparisons and score calcula-tions via the Bradley-Terry model) using both the four designed rules in QuRating (see Wettig et al. (2024)) and a rule-free approach, referred to as the \"NoRule\" setting.\nOur experiments in this section aim to address the following research questions: (Q1) Does greater rule diversity lead to more accurate ratings? (Q2) Does rule-based selection generally outperform rule-free methods? (Q3) How does our DPP-based rule selection compare to human-designed rules and ratings without rules? (Q4) Does DPP select better rules than randomly chosen ones? (Q5) How do different rating schemes and rater models impact the performance of our method?"}, {"title": "4.2 Results", "content": "Correlation of p(S) and the MSE \u20ac(S) (answer to Q1). For each r\u2208 {1,2,...,50}, we sample min{10000, (50)} sets of indices of size r, which are used to choose rules and form \u015c. We then calculate its rule correlation p(\u0160) and MSE \u20ac(\u0160). We compute their Pearson correlation and observe positive values for both IMDB and CommonCrawl datasets (see Figures 2a and 2b). This confirms that higher rule diversity is positively correlated with the accuracy of rating results. In other words, the correlation or redundancy of rules is positively correlated with the error \u20ac(S).\nRule-based v.s. Rule-free (answer to Q2): We sample 106 possible rule subsets with size r from all 50 rules and calculate the corresponding MSE, comparing it to the MSE from the NoRule setting. The results in Figures 2c and 2d demonstrate that using rule-based rating is mostly guaranteed to give better results than rating without rules, no matter applied to general data like CommonCrawl or domain-specific data like IMDB. When compared to QuRating MSE, the results show that QuRating is outperformed by most randomly selected rule subsets, highlighting the limitations of human-designed rules.\nDPP v.s. QuRating v.s. NoRule (answer to Q3). For each r\u2208 {1,2,...,50}, we use DPP to sample r rules and conduct 100 trials. Then compare the averaged MSE against the MSEs from QuRating and NoRule, recording the winning rates of the DPP rules (see Figure 3a). For the IMDB dataset, we found that once r reaches a certain threshold, DPP rules consistently achieve near-perfect winning rates against both NoRule and QuRating. Interestingly, for Common Crawl, DPP underperforms QuRating when r is too small or too large. This suggests that while QuRating rules are effective for general pre-training data, they lack the flexibility to adapt to other settings or domains.\nDPP rules v.s. Randomly selected rules (answer to Q4). We compare DPP-selected rules with randomly selected rules of the same size r, evaluating both the rule correlation p(\u0160) and the MSE \u20ac(\u0160) for their corresponding score submatrices \u0160. The results show that DPP consistently produces rules with lower correlation and lower MSE, regardless of the value of r (see Figures 3b and 3c). Another key observation is that the MSE for DPP rules increases when r is either too small or too large, with the optimal r falling somewhere in the middle. This matches our intuition: when r is too small, there are too few rules to achieve sufficient rating diversity, and when r is too large, rule redundancy can negatively affect the rating outcomes.\nVariations in rating schemes and rater models (answer to Q5). To verify that our method works across different rating schemes and rater models, we explored the following variations: 1. Pairwise v.s. individual rating. While the pairwise ratings provide more reliable comparisons, individual rating requires only O(n) computation. We observed similar results as in Section 4.2 (see Appendix A.5.3). Notably, individual ratings on the IMDB dataset showed a Pearson correlation between rule correlation p(\u0160) and MSE \u20ac(\u0160) of up to 0.6, and the winning rates show that DPP significantly outperforms both QuRating and the NoRule. 2. Llama3-8B v.s. Llama3-70B. We tested the influence of rater model capability by switching to Llama3-70B (instruction-tuned version), using the individual rating scheme on IMDB. The results are similar to earlier and we also noted a high Pearson correlation (over 0.6) between rule correlation and MSE, along with a high winning rate of DPP compared to QuRating and NoRule. Furthermore, randomly selected rules perform significantly better than both QuRating and NoRule. See Appendix A.5.4 for further details."}, {"title": "5 Evaluation B: Data Selection for LLM Fine-tuning", "content": "In this section, we follow the pipeline outlined in Section 3.3 and conduct experiments based on Evaluation B, where we train an LLM (Llama3-8B) using the selected data and assess its performance. This setup closely reflects real-world applications of LLM data selection. We benchmark our method against several baselines, including random selection, direct rating without rules, QuRating rules (Wettig et al., 2024), and DSIR (Xie et al., 2024) (a commonly used baseline for LLM data selection). We condcut experiments in this section to explore the following research questions: (Q1) How does data selected by rule-based methods enhance model fine-tuning compared to rule-free methods? (Q2) How do the rules generated by our automated framework compare to human-designed rules? (Q3) How does DPP rule selection perform compared to random rule selection?"}, {"title": "5.1 Experiments Setup", "content": "Evaluation Benchmarks. To systematically evaluate the effectiveness of our framework, we use following benchmarks: For experiments on general continued pre-training, we utilize ARC-Easy (Yadav et al., 2019), ARC-Challenge (Yadav et al., 2019), Winogrande (Sakaguchi et al., 2021), MMLU(Hendrycks et al., 2020), and SST-2 (Socher et al., 2013). Then we employ domain-specific datasets to do fine-tuning: For IMDB, we use the IMDB sentiment analysis dataset (Maas et al., 2011). For Code, we use benchmarks for code generation, including HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), Multiple-py and Multiple-cpp (Cassano et al., 2022). For Math and Medical domains, we choose subsets from MMLU corresponding to Math subject and Medical subject respectively. More details about these benchmarks are summarized in Appendix A.6.2.\nData Source. SlimPajama is a large, deduplicated, multi-corpus open-source dataset specifically de-signed for training large language models (Cerebras Systems, 2023). We randomly sampled 1 million data points (around 1 billion tokens) from SlimPajama as our initial data source D. From this pool, we employ"}, {"title": "5.2 General Continued Pre-training", "content": "We selected 20K samples from our data source using the methods described above for continued pre-training of Pythia-1B, then benchmarked the model's performance. The choice of 20K samples (approximately 20M tokens) was constrained by our GPU resources. Although 20K is significantly smaller than the pre-training corpus size, we observed improvements in benchmark results shown in Table 1, with DPP consistently leading in most metrics."}, {"title": "5.3 Domain-specifc Fine-tuning", "content": "We now focus on domain-specific fine-tuning across four domains: IMDB, Medical, Math, and Code. By selecting 20K domain-related data samples from our source for model training, we aim to enhance domain-specific task performance. As demonstrated in Tables 2 and 3, domain-specific fine-tuning yields more significant improvements than general continued pre-training, which often needs larger datasets to enhance performance due to the broader nature of the training data. Notably, rule-based methods consistently outperform rule-free approaches in general, especially when comparing against Random Select and No Rule. Within the rule-based methods, DPP demonstrated superior performance compared to using all 50 rules or selecting 10 rules randomly. This underscores the effectiveness of a rule-based strategy, which introduces more diversity in the data rating step and selects better training data. Furthermore, it also demonstrates that our application of DPP in rule selection effectively identifies a core set of high-quality rules, thereby enhancing data quality and ultimately improving model performance."}, {"title": "6 Conclusion", "content": "We have introduced an automated, rule-based framework for selecting high-quality LLM data, utilizing LLMs to generate a diverse set of rules and the DPP method to eliminate redundancy. Our work is the first to introduce an automated rule evaluation metric and we also propose a rule-based selection pipeline that demonstrates substantial generalizability across various settings, effectively overcoming the limitations of human-designed rules and addressing the challenges associated with the lack of robust rule evaluations. We first demonstrated that our approach enhances the accuracy of data ratings using a dataset with given ground truth scores. Then we conduct experiments that train LLMs with selected data and have shown that our method outperforms various other approaches, both in general pre-training and fine-tuning across four domains. The results indicate that our method successfully generates high-quality, diverse rules, and thereby improves quality of selected data, which in turn leads to improved model performance after trained with the chosen data."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Orthogonality Measures", "content": "Volume of parallelepiped. In our experiments, we also considered another measure of orthogonality, defined as the \"volume\" of the parallelepiped formed by vectors. This is mathematically described as:\n$Vol(S) \\stackrel{\\text { def }}{=} \\frac{\\sqrt{\\operatorname{det}\\left(S^T S\\right)}}{\\prod\\_{i=1}^r \\|v\\_i\\|}$"}, {"title": "A.2 DPP Sampling", "content": "Intuition by r = 2 case. Here we use the r = 2 case to illustrate the intuition behind DPP and explain why it tends to choose items that are relatively uncorrelated. Using the same notation as in 3.1, let K be the kernel matrix and Y, Y be the ground set and selected subset respectively. When r = 2, consider items A = {i, j}. Then the probability of both items being selected together is given by:\n$P(A \\subseteq Y)=K\\_{i, i} K\\_{j, j}-K\\_{i, j} K\\_{j, i} \\\\=P(i \\in Y) P(j \\in Y)-K\\_{i, j}^{2} \\\\=P(i \\text { is chosen }) P(j \\text { is chosen })-(\\text { similarity of items } i, j)^{2}$,\nsince K is symmetric by our definition. Larger similarity of i, j reduces the probability P(ACY), indicating that similar items are less likely to be chosen simultaneously. This underscores the DPP's capacity to promote diversity by favoring the selection of dissimilar items.\nDPP Sampling Algorithm: The sampling algorithm can be found in Algorithm 1 of Kulesza et al. (2012). The sampling process starts by decomposing the kernel matrix K and involves two main stages: 1. Selecting eigenvectors by sampling from a Bernoulli distribution based on the eigenvalues, and 2. Sampling a subset from the ground set using an iterative conditional distribution method to ensure diversity, as detailed in (Kulesza et al., 2012). We utilize the DPPy Python library (Gautier et al., 2019) for efficient DPP initialization and sampling.\nTime Complexity Finding the submatrix (subset of columns) of a matrix to maximize the orthogo-nality is NP-hard (Civril & Magdon-Ismail, 2007; Kulesza et al., 2012). DPP provides us a relatively good solution. In practice, the computational complexity of sampling from a DPP depends primarily on the eigendecomposition of the kernel matrix K. In our case, K \u2208 RR\u00d7R and therefore it requires O(R\u00b3) time, where R is the number of rules. In the DPPy package (Gautier et al., 2019) it uses the spectral sampler by default, so the actual run-time of our DPP implementation is O(R\u00b3)."}, {"title": "A.3 Stochastic data selection: Gumbel top-k trick:", "content": "Imagine the cases where the target dataset distribution shows a long-tail pattern with respect to our quality measure, using a deterministic quality score as the cutoff could exclude many possibly valuable data (Albalak et al., 2024). Hence, our stochastic sampling in 3 effectively balances the quality and diversity of the selected data. Nonetheless, instead of doing actual sampling according to Equation 3, we use the Gumbel top-k trick similar as in (Wettig et al., 2024), which is a sampling technique used to efficiently and probabilistically select the top-k items from a discrete probability distribution. Specifically, each item i in the distribution is assigned a score using the formula:\n$S\\_i = \\log p\\_i + g\\_i$,\nwhere pi is the probability of item i, and gi is a noise term drawn from a Gumbel distribution, which can be generated using g\u2081 = -log(-log(uz)). In other words, we could add a Gumbel noise vector to the log of the sampling probability in Equation 3 and then choose the top-k data points with the highest sums. This is statistically equivalent to sampling according to Equation 3 (Kool et al., 2019)."}, {"title": "A.4 Limitations and Future Directions", "content": "We have developed an automated, rule-based selection framework for identifying high-quality LLM data. Below, we outline some limitations of our approach and suggest potential directions for future research:"}, {"title": "A.5 Appendix for Evaluation A", "content": null}, {"title": "A.5.1 Bradley Terry Model", "content": "The Bradley-Terry model is a probabilistic model used to estimate the latent \"strength\" of teams based on pairwise competitions. The model is parameterized as follows:\n$P(i \\text { beats } j)=\\frac{v\\_{i}}{v\\_{i}+v\\_{j}}=\\frac{e^{\\beta\\_{i}}}{e^{\\beta\\_{i}}+e^{\\beta\\_{j}}}$\n(6)\nwhere exponential functions are used to model the scores $v\\_{i} \\stackrel{\\text { def }}{=} e^{\\beta\\_{i}}$ and $v\\_{j} \\stackrel{\\text { def }}{=} e^{\\beta\\_{i}}$. In other words, the difference of their scores determines the the log-odds of team i beating team j. Sometimes an intercept term a is added to adjust for any influence of the order (for example, imagine that i is the home team and has home-court advantage), then the probability becomes\n$P(i \\text { beats } j)=\\frac{e^{\\alpha+\\beta\\_{i}}}{e^{\\alpha+\\beta\\_{i}}+e^{\\beta\\_{j}}}$"}, {"title": "A.5.2 Error Metrics", "content": "Ranking-difference error. To assess the deviation of rating scores from the ground truth, instead of using the mean squared error in 4, an alternative intuitive approach is to compare the rankings derived from the data scores with those of the ground truth. This approach is based on the premise that for data selection purposes, if two sets of scores yield identical rankings, they will select the same high-scoring data samples. An example of such a ranking metric is the Kendall rank correlation coefficient (Kendall's tau) (Kendall, 1938). However, we opted against this type of metric for two critical reasons: First, it lacks the granularity needed to evaluate errors effectively. For instance, two sets of scores like [0.01, 0.98, 0.99] and [0.01, 0.02, 0.03] share exactly the same ranking yet differ significantly in their actual scores. Second, our method involves stochastic data selection, not a straightforward top-k selection, meaning that a higher score increases the likelihood of a data point being chosen. Hence, a ranking difference, which overlooks the absolute values of scores and focuses solely on their relative comparisons, is not ideal here."}, {"title": "A.5.3 Rating scheme Variation: Individual rating", "content": "Here we present the results after replacing the pair-wise rating with the direct individual rating in Section 4:"}, {"title": "A.5.4 Rater Model Size Variation: Llama3-70B-Instruct", "content": "Here we present the results after replacing the rater model from Llama3-8B-Instruct model with the stronger Llama3-70B-Instruct in Section 4:"}, {"title": "A.5.5 Prompts and Generated Rules", "content": "Comparison prompt: Below is the template used to compare two data samples according to a specific rule. For the rule-free version, simply omit the sentence involving the rule. Replace DATASET_NAME with \"IMDB reviews\" or \"Common Crawl data\" to correspond to the two data sources discussed in Section 4."}, {"title": "A.6 Appendix for Evaluation B", "content": null}, {"title": "A.6.1 Model training", "content": "For training Pythia-1B and Llama3-8B, we loaded both models using bfloat16 precision and used one NVIDIA A100-80GB for each training job. Below are the training parameters:"}, {"title": "A.6.2 Evaluation Benchmarks", "content": "In this section", "pre-training": "ARC-Challenge (15)", "2011)": "MMLU is a comprehensive multitask test comprises multiple-choice questions from a wide range of knowledge domains. It spans subjects across the humanities, social sciences, hard sciences, and other critical"}]}