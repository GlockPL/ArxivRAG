{"title": "EfficientQAT: Efficient Quantization-Aware Training\nfor Large Language Models", "authors": ["Mengzhao Chen", "Wenqi Shao", "Peng Xu", "Jiahao Wang", "Peng Gao", "Kaipeng Zhang", "Yu Qiao", "Ping Luo"], "abstract": "Large language models (LLMs) are integral to modern natural language process-\ning and artificial intelligence. However, they face challenges in managing their\nsignificant memory requirements. Although quantization-aware training (QAT)\noffers a solution by reducing memory consumption through low-bit representations\nwith minimal accuracy loss, it demands substantial training resources to optimize\nmodel weights and quantization parameters. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a novel quantization technique for\ncompressing LLMs. EfficientQAT involves two consecutive phases: Block-wise\ntraining of all parameters (Block-AP) and end-to-end training of quantization pa-\nrameters (E2E-QP). Block-AP sequentially conducts quantization-aware training\nfor all parameters in each transformer block with block-wise reconstruction, main-\ntaining efficiency by avoiding training the entire LLM. Initialized with quantized\nmodel, E2E-QP then trains only quantization parameters (step sizes) end-to-end,\nenhancing efficiency with a fixed quantized backbone and reduced trainable pa-\nrameter count. Extensive experiments demonstrate that EfficientQAT outperforms\nprevious quantization methods across a range of models, including base LLMs,\ninstruction-tuned LLMs, and multimodal LLMs, with scales from 7B to 70B pa-\nrameters at various quantization bits. For instance, EfficientQAT obtains a 2-bit\nLlama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3%\naccuracy degradation compared to the full precision (69.48 vs. 72.41). Notably,\nthis INT2 quantized 70B model obtains a 1.67 accuracy gain over the Llama-2-13B\nmodel (69.48 vs. 67.81) while requiring less memory (19.2GB vs. 24.2GB). Code\nis available at https://github.com/OpenGVLab/EfficientQAT.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) [57, 6, 12, 63, 66] have demonstrated impres-sive capabilities in diverse language tasks such as reasoning [14, 13, 70], cognitive processing [23, 63],and agent-based applications [48, 49]. However, these models are characterized by their extensiveparameters, which pose significant challenges for memory footprint and bandwidth [30, 62].\nQuantization-aware training (QAT), one of the most effective quantization techniques, works byminimizing quantization errors through training with quantization constraints. Although QAT cancompress LLMs effectively without significant performance loss, it requires training the wholeLLM on a large corpus, resulting in enormous training costs. For instance, the QAT method BitNetb1.58 [45] can achieve nearly lossless ternary quantization but requires retraining LLMs from scratchusing the full pre-trained dataset, which is impractical for extremely large models*.\nTo tackle the above challenges, we introduce a novel quantization technique named EfficientQAT,to compress LLMs effectively and efficiently. EfficientQAT surpasses existing PTQ [52, 20], Q-PEFT [64, 16] and QAT [43] methods in performance while maintaining memory efficiency duringtraining and inference, as illustrated in Figure 2a & 2b. To improve quantization efficiency andaccuracy, EfficientQAT involves two consecutive phases: Block-wise training of all parameters(Block-AP) and end-to-end training of quantization parameters (E2E-QP). Block-AP sequentiallyconducts quantization-aware training for all parameters including the original full-precision weightsand quantization parameters (step sizes and zero points) in each transformer block with block-wisereconstruction. While Block-AP trains all parameters, it preserves memory efficiency by avoidingtraining the entire LLM. Moreover, potential overfitting issues in Block-AP can be mitigated by simplyincreasing the training samples from 128 to 4096 (see Figure 4). Notably, our fully trainable paradigmof Block-AP outperforms other trainable variants (see Table 6), such as training rounding [46, 11],clipping thresholds [52], and step sizes [21, 18].\nBuilding on the robust initialization provided by Block-AP, E2E-QP fixes the quantized weights andtrains only the quantization parameters (step sizes) end-to-end, further enhancing the quantization"}, {"title": "Related Works", "content": "Post-Training Quantization of LLMs. PTQ is a pivotal technique for accelerating and deployingLLMs. Quantization approaches generally fall into two categories: weight-only quantization [22,17, 31, 30, 33, 10] and weight-activation quantization [61, 41, 60, 59, 68, 71, 1, 34, 2]. Weight-onlyquantization focuses on compressing weights into low-bit formats, reducing memory demands andenhancing the efficiency of memory-bounded computations in LLMs [38, 69]. Conversely, weight-activation quantization compresses both weights and activations, thus further decreasing the overheadassociated with matrix multiplications [38]. Recent advancements in weight-only quantization includethe introduction of vector quantization methods by QUIP#[58] and AQLM[20]. These methods haveshown promising performance but also introduce significant overhead [24]. Our research continues toexplore uniform quantization, which is preferred for its compatibility with hardware implementations.\nQuantization-Aware Training of LLMs. QAT can enhance the performance of quantized modelsbeyond what PTQ offers. However, QAT has been less explored in LLMs due to the significant trainingcosts involved. Studies such as LLM-QAT [43] and BitDistiller [19] investigate the application ofknowledge distillation within QAT contexts. Techniques like BitNet b1.58 [45] and OneBit [65]employ QAT to achieve extreme binary or ternary quantization levels. Although BitNet b1.58demonstrates near-lossless performance on models up to 3 billion parameters and 100 billion trainingtokens with ternary quantization, its applicability to larger models or datasets remains uncertain dueto prohibitive training expenses.\nQuantized Parameter-Efficient Fine-Tuning of LLMs. Techniques like QLoRA [16], INT2.1 [7],LQ-LORA [25], and LoftQ [35] quantize model parameters to low-bit representations followed by theaddition of LoRA [27] modules for fine-tuning. However, these methods require merging the LORAmodules into quantized weights, resulting in the model reverting to the FP16 format. Addressingthis issue, QA-LORA [64] redesign the LoRA module to merge seamlessly into the zero points.To our knowledge, the work most closely to our approach is PEQA [29], which employs a simpleround-to-nearest (RTN) method for low-bit quantization and fine-tunes the step sizes for downstreamtask adaptation. Despite this, the suboptimal PTQ initialization technique utilized by PEQA leads toperformance that falls short when compared to QLoRA [16] and QA-LORA [64]."}, {"title": "EfficientQAT", "content": "In this section, we introduce EfficientQAT, a novel quantization-aware training framework forLLMs that enhances memory efficiency. As illustrated in Figure 3, traditional QAT approachestrain the full-precision weights W and quantization parameters s (step sizes) and z (zero points)simultaneously in an end-to-end manner, which significantly increases the memory requirements due"}, {"title": "Block-Wise Training of All Parameters", "content": "In this section, we introduce the Block-Wise Training of All Parameters (Block-AP) approach,designed to efficiently provide an effective initialization for following end-to-end training.\nQuantization and Dequantization. Specifically, Block-AP begins with a standard uniform quantiza-tion method:\n$W_{int} = clamp(\\frac{W}{s} + z, 0, 2^N - 1),$ (1)\nwhere $\\lceil\\cdot\\rfloor$ represents the rounding operation. N is the target bit number. $W_{int}$ and W denote thequantized integer and full-precision weights, respectively. s is the scaling factor and z is the zeropoint. In the forward propagation, the quantized weights are converted back to full precision asfollows:\n$W = (W_{int} - z) \\cdot s.$ (2)\nHere, $\\hat{W}$ refers to the dequantized weights used in the forward computation. The processes ofquantization (Eq.(1)) and dequantization (Eq.(2)) are integrated within the computation graph andcan be optimized through gradient descent in a quantization-aware manner.\nBlcok-wise Quantization-aware Training. Traditional QAT methods [45, 21, 43] train the entirenetwork using Eq.(1) and Eq.(2) in an end-to-end fashion, which typically requires substantialcomputational resources and extensive data to prevent overfitting. Here we aim to enhancethe training efficiency of QAT. Previous studies, such as BRECQ [36], have demonstrated that block-wise training achieves faster convergence and requires less training time, data, and memory thanend-to-end training given a pre-trained model. Following the methodologies in BRECQ [36] andOmniQuant [52], Block-AP sequentially conducts quantization-aware training within one transformerblock before moving on to the next under a block-wise reconstruction framework.\nFull Training of Model Weights and Quantization Parameters. Unlike previous methods whichoptimize several quantization parameters such as rounding parameters [46, 11, 32], clipping parame-ters [52], and step sizes [21, 18], Block-AP behaves like QAT, training all inherent parameters fromEq.(1) and Eq.(2), including scaling factor s, zero point z, and model weights W.\nIn our Block-AP approach, a straightforward full-training regimen outperforms existing partial-training variants [46, 36, 18] with intricate designs. Traditional training methods involving roundingparameters [46, 36, 18] serve as regularization techniques, constraining the update range of integralweights to (-1,+1) to mitigate overfitting. However, this approach limits the solution space,potentially hindering the final performance of quantized models. By simply scaling the training"}, {"title": "End-to-End Training of Quantization Parameters", "content": "We further introduce the End-to-End Training of Quantization Parameters (E2E-QP), aimed atefficiently training the entire quantized model on target datasets.\nEnd-to-End Training of step sizes. Unlike traditional Quantization-Aware Training (QAT) meth-ods [43, 45] that train full-precision weights, E2E-QP begins with Wq initialized via Block-AP andfocuses solely on the training of quantization parameters (s and z). Our findings indicate that trainings, z, or both yields similar performance (see Table 7 for details). However, since training z involvesconverting it from a low-bits format to full-precision, we typically train only s by default unlessspecified otherwise to avoid additional memory overhead.\nAdditionally, within E2E-QP, there is no quantization process as per Equation (1); only the dequanti-zation process occurs as described in Equation (2). Thus, the gradient of the trainable parameter s iscomputed as $\\frac{\\partial \\hat{W}}{\\partial w}W_q - z$.\nOverall, the memory usage for training in E2E-QP is drastically reduced due to the reduced trainableparameter count. Detailed memory footprints for various model sizes and bits under E2E-QP arelisted in Table 8. For instance, the Llama-2-70B model can complete 2-bit QAT through E2E-QPusing only 34.2GB of memory. Equipped with E2E-QP, EfficientQAT is adaptable to differentscenarios by simply changing the training datasets, which includes applications such as continualpre-training and instruction-tuning [53]."}, {"title": "Experiments", "content": "This section presents extensive experiments to verify our proposed EfficientQAT. Secition 4.1 andSec 4.2 present the comparisons with quantization methods and Q-PEFT methods respectively.Section 4.4 details the training cost and inference speed-up of the proposed EfficientQAT. Section 4.3presents the comprehensive ablation studies of the proposed EfficientQAT."}, {"title": "EfficientQAT for LLMs Quantization", "content": "Training. We conduct experiments on the Llama-2 and Llama-3 models. For Block-AP, we use 4096samples from RedPajama [15] with a context length of 2048. We train each block with batch size as2 and epochs as 2, setting the learning rate of quantization parameters as 1 \u00d7 10\u22124, and the learningrate of weights as 2 \u00d7 10-5 for 2-bit and 1 \u00d7 10-5 for 3/4-bits. For E2E-QP, we also employ 4096samples from RedPajama [15] but with a context length of 4096. We train the entire model with batchsize as 32 and epoch as 1, and set the learning rate of step size as 2 \u00d7 10-5 for 2-bit and 1 \u00d7 10-5for 3/4-bits.\nEvaluation. We assess the zero-shot accuracy of five common-sense reasoning tasks using thev0.4.2 lm-evaluation-harness. The tasks include WinoGrande [50], PIQA [5], HellaSwag [70],Arc-Easy [14], and Arc-Challenge [14]. We also measure the perplexity of Wikitext2 and C4 with a2048 context length, as done in previous studies [22, 52].\nPTQ Baseline. We compare our results with PTQ methods from uniform quantization such asGPTQ [22], AWQ [37], OmniQ [52], and AutoRound [11], and vector quantization includingQuIP# [58] and AQLM [20]."}, {"title": "EfficientQAT for Instruction Tuning", "content": "Training and Evaluation. Following existing works [64, 47], we train Llama-1 models on theAlpaca dataset [53] and assess their performance by measuring average 5-shot MMLU [26] accuracyworks [64, 47]. The training hyperparameters are identical to those described in Section 4.1, exceptwe replace the RedPajama dataset [15] with Alpaca. In line with QLoRA's methodology [16], weadjust the source context length to 384 and the target context length to 128, training for 10,000 stepswith a batch size of 16.\nBaseline. We benchmark EfficientQAT against several leading methods, including QLoRA [16],QA-LORA [64], PEQA [29], and IR-QLoRA [47], across quantization setting of 2, 3, and 4 bits.Consistent with QA-LoRA [64], we also employ GPTQ [22] to quantize the fine-tuned QLORAmodels into a low-bit format without FP16 LoRA for equitable comparison.\nResults. Both Table 4 and Figure 1b indicate that EfficientQAT significantly outperforms existing Q-PEFT methods. For instance, in channel-wise quantization (group size of -1), EfficientQAT achievesmore than 3% higher accuracy than PEQA [29]. In the 2-bit quantization scenario, the superiority ofEfficientQAT is even more pronounced, surpassing QA-LORA [64] by 5.1% and 4.0% in 7B and 13Bmodels, respectively, and outperforming PEQA by 4.5% and 8.7% in the same models. Moreover,Table 4 also demonstrates that EfficientQAT outperforms both QA-LORA and QLORA with GPTQ insmaller model memory footprint (larger group size)."}, {"title": "Ablation Analysis", "content": "The EfficientQAT algorithm is comprised of two main components: Block-AP and E2E-QP. Thissection evaluates the effectiveness, trainable parameters, and training sample requirements of eachcomponent. We present the average perplexity for WikiText2 and C4 datasets, and the averageaccuracy for five zero-shot reasoning tasks, similar to Table 1."}, {"title": "Limitations", "content": "In terms of limitations, EfficientQAT demonstrates marked superiority in low-bit scenarios, such as3-bit and 2-bit quantization. However, in the 4-bit group-wise quantization, existing PTQ methodslike GPTQ [22], AWQ [37], and even the simplest RTN achieve comparable performance morequickly. Additionally, the Llama-3 quantization experiences significant performance degradation [28],a phenomenon also observed in EfficientQAT. Moreover, the 2-bit performance of EfficientQAT stillexhibits a small gap (3% to 4% accuracy) compared to full-precision models. Addressing these issuesto achieve nearly lossless performance with INT2 quantization is a key focus of our future work."}, {"title": "Broader Impact", "content": "This paper presents work whose goal is to advance the compression and acceleration of large languagemodels. There are many potential societal consequences of our work, none of which we feel must bespecifically highlighted here."}, {"title": "Gradient of Trainable Parameters in Block-AP", "content": "Block-AP, aligned with LSQ+[4], uses a straight-through estimator (STE)[3] to facilitate gradientcomputation through the rounding operation. The gradients of scaling factor s are computed asfollows:\n$\\frac{\\partial \\hat{W}}{\\partial s} = \\begin{cases} \\frac{\\lceil\\frac{W}{s} + z\\rfloor - \\frac{W}{s}}{s}, & \\lceil\\frac{W}{s} + z\\rfloor \\leq 2^{N-1}, \\\\ \\frac{\\lceil\\frac{W}{s} + z\\rfloor - \\frac{W}{s}}{s} - z, & \\lceil\\frac{W}{s} + z\\rfloor < 0, \\\\ \\frac{\\lceil\\frac{W}{s} + z\\rfloor - \\frac{W}{s}}{s} - (2^N - 1 - z), & \\lceil\\frac{W}{s} + z\\rfloor > 2^{N-1} \\end{cases}$ (3)\nand the gradient with respect to zero point z is:\n$\\frac{\\partial \\hat{W}}{\\partial z} = \\begin{cases} 0, & 0 \\leq \\lceil\\frac{W}{s} + z\\rfloor \\leq 2^{N-1}, \\\\ -1, & otherwise, \\end{cases}$ (4)\nand the full-precision weight W can also be updated through its gradient+:\n$\\frac{\\partial \\hat{W}}{\\partial W} = \\begin{cases} 1, & 0 \\leq \\lceil\\frac{W}{s} + z\\rfloor \\leq 2^{N-1}, \\\\ 0, & otherwise, \\end{cases}$ (5)"}, {"title": "Results Source of Other Method.", "content": "In this study, we present a thorough comparison of our method against existing PTQ techniques,including GPTQ [22], AWQ [37], OmniQ [52], AutoRound [11], QuIP# [58], and AQLM [20]. Wealso compare with existing QAT methods, including LLM-QAT [43], BitDistiller [19], PB-LLM [51]and DB-LLM [9]. Additionally, we also evaluate quantized parameter-efficient fine-tuning methodssuch as PEQA [29], QLoRA [16], QA-LORA [64], and IR-QLoRA [47]. The results we discussoriginate from their respective official publications, and other scholarly articles, or are derived fromour reproduction. We meticulously document the source of the results for each method as follows:\n\u2022 GPTQ, AWQ, OmniQ, AutoRound: The zero-shot accuracy results for Llama-2 modelsusing these methods are derived from the AutoRound GitHub repositorys. The perplexityresults for the Llama-2 models using GPTQ, AWQ, and OmniQ are taken from the OmniQpaper [52]. The results for Llama-3 models using AWQ"}]}