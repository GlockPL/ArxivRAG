{"title": "EfficientQAT: Efficient Quantization-Aware Training\nfor Large Language Models", "authors": ["Mengzhao Chen", "Wenqi Shao", "Peng Xu", "Jiahao Wang", "Peng Gao", "Kaipeng Zhang", "Yu Qiao", "Ping Luo"], "abstract": "Large language models (LLMs) are integral to modern natural language process-\ning and artificial intelligence. However, they face challenges in managing their\nsignificant memory requirements. Although quantization-aware training (QAT)\noffers a solution by reducing memory consumption through low-bit representations\nwith minimal accuracy loss, it demands substantial training resources to optimize\nmodel weights and quantization parameters. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a novel quantization technique for\ncompressing LLMs. EfficientQAT involves two consecutive phases: Block-wise\ntraining of all parameters (Block-AP) and end-to-end training of quantization pa-\nrameters (E2E-QP). Block-AP sequentially conducts quantization-aware training\nfor all parameters in each transformer block with block-wise reconstruction, main-\ntaining efficiency by avoiding training the entire LLM. Initialized with quantized\nmodel, E2E-QP then trains only quantization parameters (step sizes) end-to-end,\nenhancing efficiency with a fixed quantized backbone and reduced trainable pa-\nrameter count. Extensive experiments demonstrate that EfficientQAT outperforms\nprevious quantization methods across a range of models, including base LLMs,\ninstruction-tuned LLMs, and multimodal LLMs, with scales from 7B to 70B pa-\nrameters at various quantization bits. For instance, EfficientQAT obtains a 2-bit\nLlama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3%\naccuracy degradation compared to the full precision (69.48 vs. 72.41). Notably,\nthis INT2 quantized 70B model obtains a 1.67 accuracy gain over the Llama-2-13B\nmodel (69.48 vs. 67.81) while requiring less memory (19.2GB vs. 24.2GB). Code\nis available at https://github.com/OpenGVLab/EfficientQAT.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) [57, 6, 12, 63, 66] have demonstrated impres-\nsive capabilities in diverse language tasks such as reasoning [14, 13, 70], cognitive processing [23, 63],\nand agent-based applications [48, 49]. However, these models are characterized by their extensive\nparameters, which pose significant challenges for memory footprint and bandwidth [30, 62].\nQuantization-aware training (QAT), one of the most effective quantization techniques, works by\nminimizing quantization errors through training with quantization constraints. Although QAT can\ncompress LLMs effectively without significant performance loss, it requires training the whole\nLLM on a large corpus, resulting in enormous training costs. For instance, the QAT method BitNet\nb1.58 [45] can achieve nearly lossless ternary quantization but requires retraining LLMs from scratch\nusing the full pre-trained dataset, which is impractical for extremely large models*.\nCorresponding authors: shaowenqi@pjlab.org.cn; pluo@cs.hku.hk\n*BitNet b1.58 only verifies the performance on 3B models with 100B training tokens.\nPreprint. Under review."}, {"title": "2 Related Works", "content": "Post-Training Quantization of LLMs. PTQ is a pivotal technique for accelerating and deploying\nLLMs. Quantization approaches generally fall into two categories: weight-only quantization [22,\n17, 31, 30, 33, 10] and weight-activation quantization [61, 41, 60, 59, 68, 71, 1, 34, 2]. Weight-only\nquantization focuses on compressing weights into low-bit formats, reducing memory demands and\nenhancing the efficiency of memory-bounded computations in LLMs [38, 69]. Conversely, weight-\nactivation quantization compresses both weights and activations, thus further decreasing the overhead\nassociated with matrix multiplications [38]. Recent advancements in weight-only quantization include\nthe introduction of vector quantization methods by QUIP#[58] and AQLM[20]. These methods have\nshown promising performance but also introduce significant overhead [24]. Our research continues to\nexplore uniform quantization, which is preferred for its compatibility with hardware implementations.\nQuantization-Aware Training of LLMs. QAT can enhance the performance of quantized models\nbeyond what PTQ offers. However, QAT has been less explored in LLMs due to the significant training\ncosts involved. Studies such as LLM-QAT [43] and BitDistiller [19] investigate the application of\nknowledge distillation within QAT contexts. Techniques like BitNet b1.58 [45] and OneBit [65]\nemploy QAT to achieve extreme binary or ternary quantization levels. Although BitNet b1.58\ndemonstrates near-lossless performance on models up to 3 billion parameters and 100 billion training\ntokens with ternary quantization, its applicability to larger models or datasets remains uncertain due\nto prohibitive training expenses.\nQuantized Parameter-Efficient Fine-Tuning of LLMs. Techniques like QLoRA [16], INT2.1 [7],\nLQ-LORA [25], and LoftQ [35] quantize model parameters to low-bit representations followed by the\naddition of LoRA [27] modules for fine-tuning. However, these methods require merging the LORA\nmodules into quantized weights, resulting in the model reverting to the FP16 format. Addressing\nthis issue, QA-LORA [64] redesign the LoRA module to merge seamlessly into the zero points.\nTo our knowledge, the work most closely to our approach is PEQA [29], which employs a simple\nround-to-nearest (RTN) method for low-bit quantization and fine-tunes the step sizes for downstream\ntask adaptation. Despite this, the suboptimal PTQ initialization technique utilized by PEQA leads to\nperformance that falls short when compared to QLoRA [16] and QA-LORA [64]."}, {"title": "3 EfficientQAT", "content": "3.1 Method Overview\nIn this section, we introduce EfficientQAT, a novel quantization-aware training framework for\nLLMs that enhances memory efficiency. As illustrated in Figure 3, traditional QAT approaches\ntrain the full-precision weights W and quantization parameters s (step sizes) and z (zero points)\nsimultaneously in an end-to-end manner, which significantly increases the memory requirements due\nto the large number of parameters involved. To address this issue, EfficientQAT adopts a two-stage\nstrategy: block-wise training of all parameters (Block-AP) and end-to-end training of quantization\nparameters (E2E-QP). In the Block-AP phase, model parameters and quantization parameters are\ntrained block-by-block using reconstruction loss, which not only allows for precise calibration with\nfull training but also reduces memory consumption [36, 52] by block-wise training. Following this,\nthe E2E-QP phase fixes the quantized weights and trains the step sizes exclusively on target datasets,\nthus achieving quantization-aware training in a memory-efficient way. Details on Block-AP and\nE2E-QP are further described in Sections 3.2 and 3.3, respectively.\n3.2 Block-Wise Training of All Parameters\nIn this section, we introduce the Block-Wise Training of All Parameters (Block-AP) approach,\ndesigned to efficiently provide an effective initialization for following end-to-end training.\nQuantization and Dequantization. Specifically, Block-AP begins with a standard uniform quantiza-\ntion method:\nW_{int} = \\text{clamp}(\\frac{W}{s} + z, 0, 2^N - 1),\nwhere $\\lfloor\\cdot\\rceil$ represents the rounding operation. N is the target bit number. $W_{int}$ and $W$ denote the\nquantized integer and full-precision weights, respectively. s is the scaling factor and z is the zero\npoint. In the forward propagation, the quantized weights are converted back to full precision as\nfollows:\nW = (W_{int} - z) \\cdot s.\nHere, $\\hat{W}$ refers to the dequantized weights used in the forward computation. The processes of\nquantization (Eq.(1)) and dequantization (Eq.(2)) are integrated within the computation graph and\ncan be optimized through gradient descent in a quantization-aware manner.\nBlcok-wise Quantization-aware Training. Traditional QAT methods [45, 21, 43] train the entire\nnetwork using Eq.(1) and Eq.(2) in an end-to-end fashion, which typically requires substantial\ncomputational resources and extensive data to prevent overfitting. Here we aim to enhance the\ntraining efficiency of QAT. Previous studies, such as BRECQ [36], have demonstrated that block-\nwise training achieves faster convergence and requires less training time, data, and memory than\nend-to-end training given a pre-trained model. Following the methodologies in BRECQ [36] and\nOmniQuant [52], Block-AP sequentially conducts quantization-aware training within one transformer\nblock before moving on to the next under a block-wise reconstruction framework.\nFull Training of Model Weights and Quantization Parameters. Unlike previous methods which\noptimize several quantization parameters such as rounding parameters [46, 11, 32], clipping parame-\nters [52], and step sizes [21, 18], Block-AP behaves like QAT, training all inherent parameters from\nEq.(1) and Eq.(2), including scaling factor s, zero point z, and model weights W.\nIn our Block-AP approach, a straightforward full-training regimen outperforms existing partial-\ntraining variants [46, 36, 18] with intricate designs. Traditional training methods involving rounding\nparameters [46, 36, 18] serve as regularization techniques, constraining the update range of integral\nweights to (-1,+1) to mitigate overfitting. However, this approach limits the solution space,\npotentially hindering the final performance of quantized models. By simply scaling the training"}, {"title": "3.3 End-to-End Training of Quantization Parameters", "content": "samples from 128 to 4096, Block-AP effectively addresses the overfitting issue (see Figure 4 for\ndetails) while improving performance compared to training with rounding parameters. Our empirical\nfindings demonstrate the superiority of full training within our Block-AP over existing partial-training\nvariants [46, 36, 18], as shown in Table 6.\nFollowing block-wise training, we obtain the quantized model which includes quantized weights $W_q$,\nstep sizes s, and zero points z for each quantization group. The weights $W_q$ and zero points z are\nstored in a low-bit format, while step sizes s are stored in FP16. Note that s and z are shared within\ntheir respective quantization groups and constitute only a small fraction of the model's parameters,\napproximately 1.6% for a group size of 64. Moreover, the model's memory footprint is substantially\nreduced by transitioning from full-precision 16-bit weights to 2/3/4-bit quantized weights.\n3.3 End-to-End Training of Quantization Parameters\nWe further introduce the End-to-End Training of Quantization Parameters (E2E-QP), aimed at\nefficiently training the entire quantized model on target datasets.\nEnd-to-End Training of step sizes. Unlike traditional Quantization-Aware Training (QAT) meth-\nods [43, 45] that train full-precision weights, E2E-QP begins with $W_q$ initialized via Block-AP and\nfocuses solely on the training of quantization parameters (s and z). Our findings indicate that training\ns, z, or both yields similar performance (see Table 7 for details). However, since training z involves\nconverting it from a low-bits format to full-precision, we typically train only s by default unless\nspecified otherwise to avoid additional memory overhead.\nAdditionally, within E2E-QP, there is no quantization process as per Equation (1); only the dequanti-\nzation process occurs as described in Equation (2). Thus, the gradient of the trainable parameter s is\ncomputed as $\\frac{\\partial \\hat{W}}{\\partial s} = W_{int} - z$.\nOverall, the memory usage for training in E2E-QP is drastically reduced due to the reduced trainable\nparameter count. Detailed memory footprints for various model sizes and bits under E2E-QP are\nlisted in Table 8. For instance, the Llama-2-70B model can complete 2-bit QAT through E2E-QP\nusing only 34.2GB of memory. Equipped with E2E-QP, EfficientQAT is adaptable to different\nscenarios by simply changing the training datasets, which includes applications such as continual\npre-training and instruction-tuning [53]."}, {"title": "4 Experiments", "content": "This section presents extensive experiments to verify our proposed EfficientQAT. Secition 4.1 and\nSec 4.2 present the comparisons with quantization methods and Q-PEFT methods respectively.\nSection 4.4 details the training cost and inference speed-up of the proposed EfficientQAT. Section 4.3\npresents the comprehensive ablation studies of the proposed EfficientQAT.\n4.1 EfficientQAT for LLMs Quantization\nTraining. We conduct experiments on the Llama-2 and Llama-3 models. For Block-AP, we use 4096\nsamples from RedPajama [15] with a context length of 2048. We train each block with batch size as\n2 and epochs as 2, setting the learning rate of quantization parameters as 1 \u00d7 10\u22124, and the learning\nrate of weights as 2 \u00d7 10-5 for 2-bit and 1 \u00d7 10-5 for 3/4-bits. For E2E-QP, we also employ 4096\nsamples from RedPajama [15] but with a context length of 4096. We train the entire model with batch\nsize as 32 and epoch as 1, and set the learning rate of step size as 2 \u00d7 10-5 for 2-bit and 1 \u00d7 10-5\nfor 3/4-bits.\nEvaluation. We assess the zero-shot accuracy of five common-sense reasoning tasks using the\nv0.4.2 Im-evaluation-harness. The tasks include WinoGrande [50], PIQA [5], HellaSwag [70],\nArc-Easy [14], and Arc-Challenge [14]. We also measure the perplexity of Wikitext2 and C4 with a\n2048 context length, as done in previous studies [22, 52].\nPTQ Baseline. We compare our results with PTQ methods from uniform quantization such as\nGPTQ [22], AWQ [37], OmniQ [52], and AutoRound [11], and vector quantization including\nQuIP# [58] and AQLM [20]."}, {"title": "4.4 Efficiency of EfficientQAT", "content": "Training Efficiency Table 8 illustrates the required memory and time for training Lllama-2 models\nusing EfficientQAT. The results indicate that the model completes training rapidly, taking 4.8 hours\nfor the 7B model and 40.9 hours for the 70B model. Furthermore, the adoption of a quantized\nbackbone in the end-to-end quantization process (E2E-QP) significantly reduces memory usage due\nto the lower bit width. Consequently, EfficientQAT accomplishes 2-bit training for the 70B model\nusing only 34.2GB of memory.\nInference Efficiency Weight-only quantization reduces the memory requirements of LLMs and\naccelerates inference, as LLMs are predominantly memory-bound [69]. Quantization modifies the\ncomputation in the linear layers while other operations remain unchanged. To evaluate this, we\ncalculate the relative speedup in the linear layers' processing. According to Table 9, INT2 quantization\nenhances the forward-pass speed by approximately 2.9x to 4.4x. Furthermore, due to the leverage\nof standard uniform quantization, the quantized models of EfficientQAT can also achieve speedup\nthrough other toolbox, such as MLC-LLM [55], AWQ [37], and BitBLAS [54]."}, {"title": "5 Conclusion", "content": "In this study, we introduce EfficientQAT, a novel method that completes QAT with improved efficiency\nin both memory usage and training time. Through comprehensive testing, EfficientQAT proves\nsuperior to existing PTQ, QAT, and Q-PEFT methods in terms of versatility and performance across\nvarious models and quantization levels. Additionally, EfficientQAT leverages a standard uniform\nquantization, which simplifies deployment using popular toolboxes. We anticipate that EfficientQAT\nwill stimulate further research and improve the compression of Large Language Models (LLMs),\nmaking them more efficient and widely accessible."}, {"title": "A Limitations", "content": "In terms of limitations, EfficientQAT demonstrates marked superiority in low-bit scenarios, such as\n3-bit and 2-bit quantization. However, in the 4-bit group-wise quantization, existing PTQ methods\nlike GPTQ [22], AWQ [37], and even the simplest RTN achieve comparable performance more\nquickly. Additionally, the Llama-3 quantization experiences significant performance degradation [28],\na phenomenon also observed in EfficientQAT. Moreover, the 2-bit performance of EfficientQAT still\nexhibits a small gap (3% to 4% accuracy) compared to full-precision models. Addressing these issues\nto achieve nearly lossless performance with INT2 quantization is a key focus of our future work."}, {"title": "B Broader Impact", "content": "This paper presents work whose goal is to advance the compression and acceleration of large language\nmodels. There are many potential societal consequences of our work, none of which we feel must be\nspecifically highlighted here."}, {"title": "C Gradient of Trainable Parameters in Block-AP", "content": "Block-AP, aligned with LSQ+[4], uses a straight-through estimator (STE)[3] to facilitate gradient\ncomputation through the rounding operation. The gradients of scaling factor s are computed as\nfollows:\n\\frac{\\partial \\hat{W}}{\\partial s} =\\begin{cases}\n\\frac{\\tilde{W}}{s}, & \\text{if } 0 \\leq \\lfloor \\frac{\\tilde{W}}{s} + z \\rceil \\leq 2^N - 1, \\\\\n\\frac{W}{s} - z, & \\text{if } \\lfloor \\frac{\\tilde{W}}{s} + z \\rceil < 0, \\\\\n2^N-1 - z, & \\text{if } \\lfloor \\frac{\\tilde{W}}{s} + z \\rceil > 2^N - 1\n\\end{cases}\nand the gradient with respect to zero point z is:\n\\frac{\\partial \\hat{W}}{\\partial z} =\\begin{cases}\n0, & \\text{if } 0 \\leq \\lfloor \\frac{\\tilde{W}}{s} + z \\rceil \\leq 2^N - 1, \\\\\n-1, & \\text{otherwise},\n\\end{cases}\nand the full-precision weight W can also be updated through its gradient+:\n\\frac{\\partial \\hat{W}}{\\partial \\omega} =\\begin{cases}\n1, & \\text{if } 0 \\leq \\lfloor \\frac{\\tilde{W}}{s} + z \\rceil \\leq 2^N - 1, \\\\\n0, & \\text{otherwise},\n\\end{cases}"}, {"title": "D Results Source of Other Method.", "content": "In this study, we present a thorough comparison of our method against existing PTQ techniques,\nincluding GPTQ [22], AWQ [37], OmniQ [52], AutoRound [11], QuIP# [58], and AQLM [20]. We\nalso compare with existing QAT methods, including LLM-QAT [43], BitDistiller [19], PB-LLM [51]\nand DB-LLM [9]. Additionally, we also evaluate quantized parameter-efficient fine-tuning methods\nsuch as PEQA [29], QLoRA [16], QA-LORA [64], and IR-QLoRA [47]. The results we discuss\noriginate from their respective official publications, and other scholarly articles, or are derived from\nour reproduction. We meticulously document the source of the results for each method as follows:\n\u2022 GPTQ, AWQ, OmniQ, AutoRound: The zero-shot accuracy results for Llama-2 models\nusing these methods are derived from the AutoRound GitHub repositorys. The perplexity\nresults for the Llama-2 models using GPTQ, AWQ, and OmniQ are taken from the OmniQ\npaper [52]. The results for Llama-3 models using AWQ\" and GPTQ\" were obtained through\ntheir open-source implementations."}, {"title": "F_Additional Ablation Analysis", "content": "Quantization Group Size. The group size is a crucial hyperparameter in weight-only quantization.\nA smaller group size offers more granular compression and reduces quantization loss but increases\nthe number of quantization parameters required. As indicated in Table 11, a group size of 64 strikes\nan optimal balance for 2-bit quantization using EfficientQAT. It outperforms a group size of 128\nby achieving a 0.31 lower perplexity and a 0.64% higher accuracy, yet it slightly underperforms\ncompared to a group size of 32, with a marginal difference of 0.09 in perplexity and 0.14% in\naccuracy."}, {"title": "G Instruction Tuning for LVLMs.", "content": "Traditional Q-PEFT methods only do experiments on the language models. In this section, we further\nextend proposed EfficientQAT into Large vision-Language models (LVLMs) such as LLaVA [40].\nTraining and Evaluation. For the fine-tuning of large vision-language models (LVLMs), we largely\nalign with LLaVA1.5 [39], which encompass the training model, datasets, and hyperparameters\u2021\u2021.\nUnlike LLaVA1.5, which begins fine-tuning with full-precision Vicuna models using either full fine-\ntuning or LoRA-based methods [27], EfficientQAT starts with Vicuna models already quantized using\nour Block-AP method and continues with our E2E-QP fine-tuning approach. The training process\ninvolves two steps: initially freezing the LLM and pre-training a projector to align features with a\nVision Transformer (ViT), followed by end-to-end fine-tuning of both the LLM and the projector. For"}, {"title": "H Full Results", "content": "In Table 1, we present the average accuracy for five zero-shot tasks. This section offers a detailed\nbreakdown of the task-specific accuracy numbers. Specifically, Tables 13, 14, and 15 detail the\nperformance of 4-bit, 3-bit, and 2-bit quantization, respectively."}]}