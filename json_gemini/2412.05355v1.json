{"title": "MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance", "authors": ["Hidir Yesiltepe", "Tuna Han Salih Meral", "Connor Dunlop", "Pinar Yanardag"], "abstract": "In this work, we propose the first motion transfer approach in diffusion transformer through Mixture of Score Guidance (MSG), a theoretically-grounded framework for motion transfer in diffusion models. Our key theoretical contribution lies in reformulating conditional score to decompose motion score and content score in diffusion models. By formulating motion transfer as a mixture of potential energies, MSG naturally preserves scene composition and enables creative scene transformations while maintaining the integrity of transferred motion patterns. This novel sampling operates directly on pre-trained video diffusion models without additional training or fine-tuning. Through extensive experiments, MSG demonstrates successful handling of diverse scenarios including single object, multiple objects, and cross-object motion transfer as well as complex camera motion transfer. Additionally, we introduce MotionBench, the first motion transfer dataset consisting of 200 source videos and 1000 transferred motions, covering single/multi-object transfers, and complex camera motions.", "sections": [{"title": "1. Introduction", "content": "Diffusion-based video generation models have gained substantial attention for their ability to produce high-quality, diverse video content. These models, driven by advances in text-to-video generation, open new possibilities for automated and creative video synthesis [1, 5, 8, 26, 28, 33, 36].\nMotion transfer in generative models [3, 10, 30, 34, 38, 40], has become a significant research area, focusing on transferring the motion from one video to another, often guided by text prompts. Consider the complex transformation depicted in Fig. 1, where a ground vehicle's trajectory is reimagined as the flight path of an aircraft. Such motion transfer involves more than merely replacing the car with a plane. For instance, translating the movement of a car into a plane gliding over a beach, as described by the text prompt (see Fig. 1) requires a significant adjustment in environmental context. This includes transforming how the car interacts with the road to how an aircraft engages with the sky. This level of control is particularly important as it enables users to create videos with motions that are challenging to describe through text prompts alone, such as complex camera motions (see Fig. 1).\nRecent video generation and editing methods have focused on disentangling motion and appearance characteristics. Various approaches have emerged: MotionDirector [40] uses an appearance-debiased temporal loss with dual-path LoRA architecture, while DreamVideo [31], Customize-A-Video [22], and MotionCrafter [38] employ dedicated processing branches. VMC [10] combines fine-tuning and inversion techniques targeting temporal layers, and DMT [34] leverages space-time feature loss using DDIM inversion and UNet activations. Motion Inversion [30] uses motion embeddings trained from reference videos for temporal dynamics control. Despite these advancements, motion control in video generation remains challenging because of the complex interplay between spatial and temporal dimensions in video. Controlling motion is essential for applications in entertainment, advertising, and virtual reality, where specific and consistent movements are crucial to communicate a narrative or aesthetic vision.\nHowever, while these methods are effective in straightforward motion transfer tasks involving single objects without significant background or object transformations, they struggle with more challenging motion transfer tasks. They often fail to adequately transform the scene, merely replacing one object with another without aligning the scenery with the changes specified in the text prompt. Other methods may dramatically alter the scene without preserving the original motion. On the other hand, video editing methods [2, 4, 11, 20] utilize structural similarities between source and target videos. However, their performance is limited when it comes to multi-object motion transfer or managing complex camera movements. Additionally, these methods struggle with significant shape transformations, such as turning a car into a flying plane (see Fig. 1). These limitations highlight the need for more advanced motion transfer techniques that can handle a wider range of transformations and motions without being constrained by the physical similarities between the source and target videos. Such capabilities would significantly enhance the flexibility and applicability of generative models in video editing and animation, opening up new possibilities for creative and practical applications.\nIn this paper, we present Mixture of Score Guidance (MSG), a novel approach for motion transfer in diffusion-based generative video models. Our method builds upon a novel conditional score reformulation, where we formulate motion transfer as a mixture of potential energies in the score space of diffusion models. By leveraging the observation that reformulated conditional scores encode rich motion information in early diffusion timesteps, MSG successfully isolates and transfers motion patterns. We establish the mathematical connection between score mixing and Langevin dynamics, providing theoretical perspectives for stable motion transfer. Through extensive experimentation, we demonstrate that MSG enables high-fidelity motion transfer across diverse scenarios without requiring model fine-tuning or additional training data. Our work extends beyond single-motion cases to handle multiple motion sources, and complex camera motions offering a unified approach to video motion transfer. Our contributions are as follows:"}, {"title": "2. Related Work", "content": "Transformer architectures have emerged as a powerful foundation for video generation tasks. Early research scaling transformers for T2V applications, including Sora [17], CogVideo [8], CogVideoX [33] and LATTE [16], established the viability of this approach. The introduction of Diffusion Transformers [19] further cemented transformers as core components in video diffusion models. Several works have introduced specialized conditioning inputs: ControlVideo [39] leverages depth maps, DragNUWA [35] employs motion trajectories, while VideoDirectorGPT [14] and related approaches [3, 13] utilize spatial and temporal guides. T2I-based extensions include AnimateDiff [5], ModelScope [28], and InstructVideo [37]."}, {"title": "2.2. Video Motion Editing and Transfer", "content": "Video motion control research has developed along two primary paths: explicit control through bounding boxes and motion transfer from reference videos. Explicit control methods include Animate Anyone [12], Boximator [29], Peekaboo [9], and Trailblazer [15].\nAnother significant line of work focuses on transferring motion from reference videos. MotionDirector (MD) [40] made a significant advancement with its innovative dual-path LoRA architecture, effectively separating motion and appearance characteristics through specialized components that enable precise control over temporal dynamics. DreamVideo [31] and Customize-A-Video [22] further refined this separation using distinct branches for appearance and motion learning. Motion Inversion [30] introduced a novel approach by learning motion embeddings through temporal attention layers trained directly on the original video.\nVideo Motion Customization (VMC) [10] introduced a novel approach combining fine-tuning with inversion through adaptive temporal layer adjustments, achieving superior motion transfer results while maintaining temporal consistency. TokenFlow [4], ReRender-A-Video [32], and RAVE [11] explored various approaches to temporal consistency. The field has further advanced with MotionInversion (MI) [30] that enable precise control over temporal dynamics while maintaining visual quality through sophisticated motion embeddings trained from a reference video.\nA persistent challenge in motion transfer is the assumption of feature similarity between reference and target videos. DMT [34] addresses this limitation through a novel space-time feature loss, leveraging internal UNet activations for improved motion fidelity. This approach achieves superior results in maintaining temporal consistency while allowing for more diverse edited outputs compared to traditional feature-matching methods."}, {"title": "3. Background", "content": "Consider a video sequence as a high-dimensional random variable $z \\in Z$ following an unknown data distribution $p(z)$. The diffusion process gradually transforms this distribution to a known prior distribution through a forward process defined by the following stochastic differential equation:\n$dz = [f(z,t) - \\frac{g(t)^2}{2} \\nabla_z \\log p_t(z)]dt + g(t)dw_t$ (1)\nwhere the drift coefficient $f(z, t)$ is characterized by:\n$f(x,t) = - \\sigma(t) \\sigma'(t) \\nabla_z \\log p_t(z)dt$ (2)\nand the diffusion coefficient $g(t)$ takes the form:\n$g(t) = \\sigma(t) \\sqrt{2\\beta(t)}$ (3)\nThe stochastic process is driven by the standard Wiener process $d\\bar{w}_t$, while $p_t(z_t)$ represents the probability distribution of the noisy samples at time t. The boundary conditions of this distribution are given by the data distribution at the initial time, $p_0(z_0) = P_{data}(z)$, and a normal distribution with specified variance at the terminal time, $P_1(z_1) = N(0, \\maxI)$. The time-reversed stochastic process for variance-preserving (VP) conditional diffusion is formulated in [27] as:\n$dz = - \\frac{1}{2} \\beta_t z dt - \\beta_t \\nabla_z \\log p_t(z|y) dt + \\sqrt{\\beta_t}dw_t$ (4)\nBy indicating directions of increased probability, the score naturally serves as a mechanism to undo the forward diffusion process."}, {"title": "Classifier Free Guidance", "content": "Classifier-Free Guidance (CFG) [6] enhances generation quality by interpolating between conditional and unconditional score predictions, effectively balancing fidelity and diversity in the output. CFG introduces a guided score $\\nabla_z \\log p_{t,\\chi}(z|y)$ that replaces the conditional score $\\nabla_z \\log p_t(z|y)$ in (4), defined at each timestep as:\n$\\nabla_z \\log P_{t,\\chi}(z|y) = (1 - \\chi)\\nabla_z \\log p_t(z) + \\chi \\nabla_z \\log p_t(z|y)$ (5)\nwhere $\\chi > 1$ reduces to the standard conditional generation, while $\\chi > 1$ amplifies the influence of the conditioning signal, typically leading to higher-quality but potentially less diverse outputs."}, {"title": "Langevin Dynamics", "content": "As a fundamental stochastic process in statistical physics, Langevin dynamics (LD) [18, 25] enables sampling from complex probability distributions through continuous-time evolution. The dynamics follow a stochastic differential equation that combines deterministic drift with random fluctuations [23]:\n$dz = - \\frac{1}{2} \\nabla \\log p(z)dt + \\sqrt{\\epsilon}d\\bar{u}_t$ (6)\nWhen implemented with appropriate step sizes, this process naturally evolves toward its equilibrium state $p(z)$ [24], making it particularly valuable for sampling tasks. The method's practical implementation hinges on the availability of the score function $\\nabla \\log p(z)$, which, similar to diffusion models, can be estimated through neural networks."}, {"title": "4. Methodology", "content": "This section presents the theoretical foundations and formulations of Mixture of Score Guidance (MSG), a novel approach for motion transfer in diffusion models in terms of statistical mechanics and stochastic processes."}, {"title": "4.1. Score-Based Motion Transfer", "content": "Let M: Z \u2192 M be a mapping from the latent space to motion characteristics. The score function $\\nabla_z \\log p_t(z|y)$ can be separated into motion and content components through our conditional reformulation $\\nabla_z \\log p_t(z, M(z^*)|y)$:\n$\\nabla_z \\log p_t (z, M(z^*)|y) = \\nabla_z \\log p_t(M(z^*)|y) +\\nabla_z \\log p_t(z|M(z^*),y)$\nwhere M(z*) is a reference motion representation and it is a function of the reference video latent z*. This decomposition separates the score function into two meaningful components:\n(1) Motion Score: $\\nabla_z \\log p_t(M(z^*)|y)$ which is responsible for capturing how the latent affects motion characteristics and representing the gradient of log-likelihood concerning motion. The term dominates in early timesteps due to motion's hierarchical nature.\n(2) Content Score: $\\nabla_z \\log p_t (z|M(z^*), y)$ which captures content-specific information conditioned on motion and represents the residual gradient after accounting for motion. As a result it is more prominent in later timesteps."}, {"title": "4.1.2 Mixture of Score Guidance", "content": "Given a reference video with desired motion characteristics characterized by the reference condition $y^*$ with the latents $z^*$, we formulate MSG as:\n$S_{MSG}(Z_t, \\hat{z}) = \\nabla_z \\log p_t (z|y) + \\omega_{MSG}(\\nabla_z \\log p_t(z^*|y^*) \u2013 \\nabla_z \\log p_t(z))$"}, {"title": "4.1.3 Motion Trajectory Representation", "content": "Let $V \\in R^{F\\times H\\times W\\times 3}$ denote an input video sequence. Our aim is to derive a training-free motion representation through the following formulation. The forward process transforms the initial frame latents of the reference z into noised latents $z_t$ at timestep t according to:\n$z_t = \\alpha_t z_0 + \\sigma_t \\epsilon, \\ \\epsilon \\sim N(0, I)$ (10)\nwhere $\\alpha_t$ and $\\sigma_t$ are time-dependent coefficients controlling the noise schedule [7]. The noising step is controlled by the strength parameter. The conditional score function $\\nabla_z \\log p_t(z^*|y)$ is then computed via a pretrained denoising network. Through investigation of score distribution, we establish that the motion representation operator $M : Z \u2192 Z$ defined as $M(z) = \\nabla_z \\log p_t(z|y)$ captures predominant motion patterns at early diffusion timesteps $t \\ll T$. This finding is empirically validated through our analysis of reference video conditional noise patterns, as illustrated in Figure 2 and Figure 3."}, {"title": "5. Experiments", "content": "Our implementation utilizes the CogVideoX [33] model for video generation and editing. We conduct all experiments at a resolution of 720 x 480 pixels using 50 diffusion timesteps. Due to the absence of a dedicated DDIM inversion schedule in CogVideoX, we employ a stochastic inversion approach where we add controlled noise to the input video latents, regulated by a strength parameter (detailed analysis in Fig. 8). For motion transfer, our pipeline operates in two phases: first, we obtain conditional score estimates from the reference video in early timesteps ($t \\ll T$), then we apply this guidance during the generation of motion-transferred videos at the same timestep range. Throughout all experiments presented in this paper, we consistently set t to 10% of the total timesteps, as this configuration provides an effective balance between motion preservation and generation quality."}, {"title": "6. Qualitative Experiments", "content": "Our experimental results demonstrate MotionShop's versatility across diverse motion transfer scenarios. As shown in Figures 1 and 4, our method successfully handles both single and multi-object transfers. For single-object scenarios, MotionShop effectively transforms a black swan into a horse and a man riding jet ski (Fig. 4.c), maintaining realistic movement patterns and contextual elements like water splashes. In multi-object cases, our method seamlessly converts cats into birds (Fig. 4.b) and robots (Fig. 4.f). MotionShop provides flexible background control-enabling both dramatic alterations (Fig. 4.a) and preservation according to the text prompt (Fig. 4.c). Our method also supports concurrent motion controls, demonstrated by simultaneously transforming a frisbee into a coin while converting a dog into an eagle (Fig. 4.d). Additionally, MotionShop handles complex camera movements including zoom-ins, zoom-outs (Fig. 1), and rotational movements (Fig. 7), showcasing its comprehensive motion transfer capabilities."}, {"title": "7. Qualitative Comparisons", "content": "We conducted a qualitative comparison of MotionShop against MotionInversion [30], DMT [34], VMC [10], and MotionDirector [40], as shown in Table 5. Our evaluation focused on motion transfer across single/multiple objects and complex camera movements. Our experimental results reveal key differences in background handling among the methods: MotionDirector and VMC show limitations in background preservation, introducing undesirable artifacts (Table 5). In contrast, MotionShop demonstrates two distinct advantages: it enables accurate background modification when explicitly requested in the prompt (Table 5), and maintains consistent preservation of the original scene composition while properly transforming target objects (Table 5). These results indicate MotionShop's superior ability to distinguish between intentional and unintentional scene modifications. Additionally, MotionShop excels in transferring complex camera movements, including zoom-ins, zoom-outs, and rotations and their combinations such as pan-left and zoom-out (Fig. 7 bottom right), which proved challenging for DMT and MotionDirector in creative scene camera motion transfer (Fig. 6)."}, {"title": "8. Quantitative Experiments", "content": "In our quantitative evaluation, we compared MotionShop with MotionInversion [30], DMT [34], VMC [10], and MotionDirector [40] with 100 data-prompt pairs using four metrics: (1) Text Similarity, measuring frame-to-text alignment using CLIP [21], (2) Motion Fidelity [34], evaluating motion preservation using tracklet similarity between input and output videos, (3) Temporal Consistency, measuring frame-to-frame coherence via CLIP feature similarity, and (4) FID, assessing visual quality against DAVIS dataset. As shown in Table 1, MotionShop achieves state-of-the-art performance in both Motion Fidelity (0.913) and Temporal Consistency (0.928). While VMC shows marginally higher Text Similarity (0.328 vs. 0.314), our method achieves a better balance between text alignment and motion quality metrics (Table 1)."}, {"title": "9. Discussion on Quantitative Experiments", "content": "The quantitative evaluation results presented in Table 1 demonstrate the superior performance of our approach."}, {"title": "9.1. Ablation Studies", "content": "We analyze three key components: motion extraction strength, guidance timestep ratio, and guidance mechanisms. Fig. 8 (left) shows the impact of motion extraction strength parameters. At 0.6, the horse's jumping motion is insufficiently transferred; at 0.8, over-stylization distorts the motion; 0.7 achieves optimal balance between motion preservation and visual quality. Fig. 8 (right) demonstrates that applying Mixture of Score guidance at different timesteps preserves generative priors, enabling diverse yet natural jumping motions.\nFig. 9 compares three guidance approaches: Classifier-Free Guidance (CFG), Unconditional Score Guidance (USG), and our Mixture of Score Guidance (MSG). CFG struggles with motion consistency, while USG better preserves motion but lacks prompt-guided precision. MSG demonstrates superior performance, evidenced by natural medieval cat motions (left) and wolf-to-pig transformations (right) while maintaining motion characteristics. This improvement stems from our novel formulation that explicitly decomposes motion and content scores, allowing for more precise control over the transfer process."}, {"title": "10. MotionBench Dataset", "content": "We introduce MotionBench, a comprehensive motion transfer dataset designed for systematic evaluation of motion transfer capabilities. The dataset comprises 200 carefully curated source videos and 1,000 corresponding transferred sequences, combining real-world footage from DAVIS dataset (50 videos) and high-quality synthetic videos (150 videos) generated using CogVideoX [33].\nThe dataset is structured around three primary motion categories, each addressing distinct challenges in motion transfer: Single Object Motion (85 videos, 42.5%), Multiple Object Motion (65 videos, 32.5%), and Camera Motion (50 videos, 25%). Single object sequences capture diverse motion patterns from rigid mechanical movements to complex articulated motions. Multiple object scenarios evaluate preservation of spatial relationships and interaction dynamics."}, {"title": "11. Limitation and Societal Impact", "content": "Our method's performance is inherently tied to the generative priors learned by the underlying T2V model. As a result, certain target concepts and motions may fall outside the model's distribution. Additionally, any biases present in the T2V model are carried over into our approach, a drawback that any zero-shot method suffers, which may influence the quality of generated outputs for specific scenarios. Since our method enables controllable video generation, there is a potential risk of it being used to create deepfake videos that spread misinformation or deceive viewers. To mitigate these risks, we emphasize the importance of ethical use of our tool."}, {"title": "12. Conclusion", "content": "In this paper, we presented MotionShop, the first motion transfer approach in video diffusion transformers, which reformulates conditional score to decompose motion and content scores. By treating motion transfer as a mixture of potential energies, our method enables creative scene transformations while preserving motion patterns, operating directly on pre-trained models without additional training. Extensive experiments demonstrate MSG's effectiveness across various scenarios, from single/multiple object transformations to complex camera motion transfer. Our framework provides principled guidance for balancing motion and content preservation, enabling flexible motion transfer in video generation."}, {"title": "A. User Study Details", "content": "To evaluate the perceptual quality of our method, we conducted a comprehensive user study with N=50 participants recruited through Prolific.com. Following standard practices in human evaluation studies for video generation [30], we designed our study to assess three critical aspects of motion transfer quality, as illustrated in Fig. 10.\nFor each test case, participants were presented with an input video and five different edited versions, corresponding to various motion transfer methods. The evaluation criteria were as follows:\n1. Motion Fidelity: Participants were asked to identify the two edited videos that best preserved the motion patterns from the input sequence. This assessment focused on the accuracy of transferred motion dynamics and spatial relationships. The question we asked is being \"Regarding the input video, which specific edits would you consider to be among the top two most successful regarding preserving original motion?\"\n2. Temporal Consistency: Users selected the two results exhibiting the highest temporal coherence, evaluating frame-to-frame continuity and the absence of artifacts or jitter in the generated sequences. The question we asked is being \"Regarding the modified videos below, select the top 2 that have the smoothest motion.\"\n3. Text-Motion Alignment: Participants evaluated how well each generated video aligned with its corresponding text prompt, focusing on both semantic accuracy and motion appropriateness. The question we asked is being \"Which video best aligns with textual description (prompt) below.\"\nThe study compared five different approaches: our proposed MotionShop method, Space-Time Features (DMT) [34], MotionDirector (MD) [40], MotionInversion (MI) [30], and Video Motion Customization (VMC) [10]. The user study interface, shown in Fig. 11, was designed to facilitate clear comparison and intuitive interaction."}, {"title": "B. MotionBench: A Comprehensive Motion Transfer Dataset", "content": "We introduce MotionBench, the first publicly available dataset specifically designed for evaluating motion transfer capabilities in video generation models. While existing video datasets primarily focus on general video synthesis or editing tasks, MotionBench addresses the critical gap in standardized evaluation of motion transfer capabilities. The dataset comprises 200 carefully curated source videos and 1,000 corresponding motion-transferred sequences, enabling systematic evaluation across diverse motion patterns and scene compositions."}, {"title": "B.1. Dataset Composition", "content": "The 200 source videos are curated from two primary sources:\n* DAVIS Dataset (50 videos): Selected for their diverse real-world motions\n* Synthetic Videos (150 videos): Generated using CogVideoX-5B [33] model.\nThe source videos are categorized into the following motion categories:\n1. Single Object Motion (85 videos)\nThe Single Object Motion category constitutes the largest portion of our dataset (42.5% of source videos), carefully curated to capture the full spectrum of motion patterns observed in real-world scenarios. This category is subdivided into three distinct motion types:\nRigid Object Motion (35 videos): This subcategory focuses on objects that maintain their shape during motion, featuring vehicles (e.g., cars, motorcycles), toys (e.g., remote-controlled cars, mechanical toys), and mechanical objects (e.g., robotic arms, industrial machinery). These sequences are particularly valuable for evaluating a method's ability to preserve consistent object geometry while transferring motion patterns.\nNon-rigid Object Motion (30 videos): This subset encompasses objects that undergo deformation during movement, primarily featuring animals (e.g., birds in flight, running quadrupeds) and deformable objects (e.g., cloth, fluid-like materials). These sequences present more complex challenges, requiring methods to handle both global motion and local deformations simultaneously. The videos capture various natural movements including galloping, flying, and elastic deformations.\nHuman Motion (20 videos): The human motion sequences capture a diverse range of articulated movements, including walking sequences, dance performances, and various sports activities. These videos are particularly challenging as they combine both rigid (skeletal) and non-rigid (soft tissue) motion patterns. The sequences test a method's capability to preserve complex kinematic chains and natural human dynamics while transferring motion to different target subjects or characters.\nEach subcategory is carefully balanced to include both simple and complex motion patterns, varying speeds, and different environmental contexts. This structured approach enables systematic evaluation of motion transfer methods across a spectrum of complexity levels, from basic rigid transformations to highly articulated and deformable motion patterns."}, {"title": "2. Multi-Object Motion (65 videos)", "content": "The Multi-Object Motion category comprises 32.5% of our dataset, specifically designed to evaluate motion transfer capabilities in scenarios involving multiple moving entities. This category presents unique challenges in preserving spatial relationships, temporal synchronization, and complex interaction patterns. We organize these sequences into three distinct subcategories:\nInteractive Motion (25 videos): These sequences capture complex interactions between multiple objects or animals, such as predator-prey chase sequences, children playing with toys, or animals engaged in social behaviors. The defining characteristic of this subset is the causal relationship between the subjects' movements, where the motion of one entity directly influences others. These videos are particularly challenging for motion transfer as they require preserving not only individual motion patterns but also the intricate timing and spatial relationships that define the interactions. Examples include dogs playing with frisbees, cats interacting with toys, and people passing objects between them.\nIndependent Motion (20 videos): This subcategory features scenarios where multiple objects move simultaneously but independently of each other. These sequences test a method's ability to maintain distinct motion patterns while ensuring global scene coherence. Examples include traffic scenes with multiple vehicles, scenes of birds flying in different directions, and sequences of independent mechanical systems operating simultaneously. The primary challenge lies in preserving the independence of various motion patterns while maintaining their temporal alignment and avoiding unintended interactions in the transferred results.\nGroup Motion (20 videos): The group motion sequences focus on coordinated movements of multiple subjects, such as synchronized dancing, flock behaviors, or team sports activities. These videos present unique challenges in maintaining both individual motion fidelity and group-level patterns. The sequences capture various forms of collective behavior, from highly structured (e.g., marching bands, synchronized swimming) to more organic patterns (e.g., school of fish, crowd movements). The key evaluation aspect is the preservation of both individual dynamics and emergent group behavior patterns during motion transfer.\nEach subcategory is carefully curated to include varying levels of complexity in terms of the number of objects, spatial distribution, and temporal coordination. This structured organization enables comprehensive evaluation of how motion transfer methods handle scenarios ranging from simple multi-object scenes to complex, interdependent motion patterns, providing insights into their scalability and robustness in real-world applications."}, {"title": "3. Camera Motion (50 videos)", "content": "The Camera Motion category constitutes 25% of our dataset, specifically designed to evaluate motion transfer methods' capabilities in handling various camera movement patterns. This category is particularly crucial as camera motion adds an additional layer of complexity to the motion transfer task, requiring methods to maintain coherent scene composition while adapting to changing viewpoints and perspectives.\nSimple Camera Movements (20 videos): This subcategory encompasses fundamental camera operations that form the building blocks of cinematographic techniques. Each type presents unique challenges for motion transfer:\n* Pan (5 videos): Horizontal camera rotations that test a method's ability to maintain consistent object appearance and motion during lateral viewpoint changes. These sequences include landscape shots, architectural surveys, and subject tracking, with varying pan speeds and ranges.\n* Tilt (5 videos): Vertical camera rotations that challenge perspective preservation, particularly in maintaining proper scale relationships as the viewing angle changes. Examples include vertical scans of buildings, waterfalls, and ascending/descending subject movements.\n* Zoom (5 videos): Sequences involving camera focal length changes, testing a method's capability to handle continuous scale variations while preserving motion coherence. These include both zoom-in sequences revealing fine details and zoom-out shots revealing broader context.\n* Dolly (5 videos): Forward/backward camera translations that evaluate depth handling and parallax effects. These shots are particularly challenging as they require maintaining proper spatial relationships between foreground and background elements during motion transfer.\nComplex Camera Movements (30 videos): This subcategory features more sophisticated camera work that combines multiple basic movements, presenting higher-level challenges for motion transfer systems:\n* Combined Motion Patterns (15 videos): These sequences feature simultaneous execution of multiple camera movements (e.g., pan-with-zoom, tilt-with-dolly). They test a method's ability to handle compound camera transformations while maintaining scene coherence and motion fidelity. Examples include aerial shots with multiple degrees of freedom, elaborate reveal sequences, and complex establishing shots.\n* Dynamic Tracking Shots (15 videos): These sequences involve camera movements that actively follow moving subjects, requiring simultaneous handling of both camera and subject motion patterns. They present particularly challenging scenarios where the camera movement must maintain a specific spatial relationship with the tracked subject while adapting to the subject's motion. Examples include sports coverage, chase sequences, and nature documentaries.\nThe camera motion sequences are carefully selected to include variations in speed, acceleration, and motion smoothness. Additionally, they encompass different environmental contexts (indoor/outdoor, varying lighting conditions) and subject types, providing a comprehensive evaluation framework for testing motion transfer methods' robustness to camera movement. This category is particularly valuable for assessing a method's potential in real-world applications such as cinematography, virtual production, and automated video editing."}, {"title": "B.2. Motion Transfer Sequences", "content": "Our dataset includes 1,000 carefully curated motion-transferred sequences, each derived from the source videos through various transformation scenarios. These sequences are specifically designed to evaluate different aspects of motion transfer capabilities, ranging from object transformations to comprehensive scene alterations. The transfers are organized into two primary categories, each addressing distinct challenges in motion transfer tasks: 1. Cross-Category Transfers\nThis category evaluates a method's capability to transfer motion patterns across different object categories while maintaining motion fidelity. The sequences are divided into three distinct transfer types:\nObject-to-Object: These transfers focus on motion preservation across different object categories while handling significant shape and appearance variations. Examples include:\n* Vehicle-to-creature transformations (e.g., car motion applied to a mechanical horse)\n* Mechanical-to-organic conversions (e.g., robot movements mapped to flowing water)\n* Rigid-to-deformable translations (e.g., toy motion adapted to cloth-like objects)\nThese sequences test the ability to maintain motion characteristics despite fundamental changes in object properties and physical constraints.\nHuman-to-Character: This subset specifically addresses the challenging task of transferring human motion to non-human characters while preserving natural movement patterns. Examples include:\n* Human dance movements applied to animated characters\n* Sports motions transferred to fantasy creatures\n* Gesture sequences mapped to mechanical entities\nThese transfers test the preservation of complex articulated motion while adapting to different skeletal structures and movement constraints.\nAnimal-to-Object: These sequences evaluate the transfer of organic motion patterns to inorganic objects, presenting unique challenges in motion adaptation. Examples include:\n* Bird flight patterns applied to flying vehicles\n* Quadruped locomotion mapped to mechanical assemblies\n2. Scene Transformation Transfers\nThis category focuses on evaluating motion preservation within dramatically altered environmental contexts, addressing two key aspects:\nEnvironment Changes: These transfers test the ability to maintain motion fidelity while completely transforming the surrounding environment. The sequences include:\n* Context shifts (e.g., street scene to underwater environment)\n* Scale transformations (e.g., human-scale to miniature worlds)\n* Physical domain changes (e.g., terrestrial to aerial scenarios)\nThese sequences evaluate how well methods handle motion transfer when environmental physics and constraints change significantly.\nStyle Transfers: This subset focuses on artistic and stylistic transformations while maintaining motion integrity. Examples include:\n* Realistic to animated style conversions\n* Contemporary to historical aesthetic adaptations\n* Natural to fantastical scene transformations\nEach transfer category is carefully designed to test specific aspects of motion transfer capabilities, from basic motion preservation to complex scene-level transformations. The sequences vary in complexity, duration, and transformation extent, providing a comprehensive evaluation framework for assessing motion transfer methods across different scenarios and applications. This structured approach enables systematic analysis of a method's strengths and limitations in handling various types of motion transfer challenges."}, {"title": "B.3. Dataset Statistics", "content": "Key characteristics of the dataset:\n* Resolution: 720\u00d7480 pixels\n* Frame Rate: 15 FPS\n* Duration: 1-7 seconds per video (Due to the frame processing limitation of CogVideoX)\n* Total Frames: ~45,000\n* Format: MP4 (H.264 codec)\nHere we note that the duration limit stems from the CogVideoX [33] backbone. It can only process 49 frames at most."}, {"title": "B.4. Discussion", "content": "MotionBench addresses several critical requirements essential for comprehensive motion transfer evaluation. First, it achieves comprehensiveness by encompassing a diverse range of motion types and scene compositions, ensuring broad coverage of real-world scenarios. Its scalable design provides sufficient data for meaningful model training and evaluation, while maintaining standardized evaluation protocols and metrics that enable consistent comparisons across different approaches. Furthermore, the dataset's inclusion of both synthetic and real-world scenarios ensures practical applicability across various use cases. Through these carefully considered design choices, MotionBench enables researchers to systematically compare motion transfer methods, analyze motion preservation capabilities, evaluate scene composition handling, and assess temporal consistency."}]}