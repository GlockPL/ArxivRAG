{"title": "Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study", "authors": ["Rabih Chamas", "Ismail Khalfaoui-Hassani", "Timoth\u00e9e Masquelier"], "abstract": "Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' Grad-CAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models \u2013 ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa_24 and 36) and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random heatmaps for two models in our study: CAFormer and ConvFormer models, leading to low interpretability scores. es. We addressed this issue by introducing Threshold-Grad-CAM, a modification built on top of Grad-CAM that enhanced interpretability across nearly all models. The code and checkpoints to reproduce this study are available at:", "sections": [{"title": "1 Introduction", "content": "Deep learning neural networks are extremely powerful for a myriad of tasks, including image classification. However, despite being very powerful tools, they remain black box models, and understanding how they arrive at their results can be a major challenge. Explainability methods in deep learning aim to explain why a particular model predicts a particular result.\nOne application where explainability methods have been successfully in use for several years is image classification. The most popular and successful models nowadays for image classification include convolution and/or attention layers.\nWhen a model contains only convolutions, it is called a fully convolutional neural network or CNN, when it contains only multi-head self-attention (MHSA) layers, it is called a transformer or, in the context of computer vision, a vision transformer, and when a model contains both layers, it is called a hybrid model.\nDespite their very high accuracy, most of these models remain very opaque, and the lack of explicability of the latter, especially in computer vision, raises concerns about trust, fairness, and interoperability, hindering their adoption in sensitive areas such as medical diagnosis or autonomous vehicles.\nThis also applies to recent advances such as Dilated Convolution with Learnable Spacings (DCLS) [Khalfaoui-Hassani et al., 2023b], which shows promising performance gains in tasks such as image classification, segmentation, and object detection.\nWhile the accuracy of DCLS is encouraging, its black-box nature demands attention. Thus, we have been motivated to explore explainability measures and scores specifically for DCLS, with the hope of shedding light on its underlying decision-making processes.\nThe taxonomies used for explainability in the artificial intelligence field of research are diverse and constantly evolving as new approaches are discovered. However, a common way to proceed is to distinguish between two major families of model explainability methods: global methods and local methods [Speith, 2022; Schwalbe and Finzel, 2023].\nGlobal methods describe the overall behavior of the model, considering general patterns and the importance of features. Some examples include Partial Dependence Plots (PDPs) [Friedman, 2001] and SHapley Additive explanations (SHAP) [Lundberg and Lee, 2017]. Local methods, on the other hand, focus on explaining individual predictions, focusing on why the model made a particular decision for a particular input. Examples include Local Interpretable Model-Agnostic Explanations (LIME) [Ribeiro et al., 2016] and Gradient-weighted Class Activation Mapping (Grad-CAM) [Selvaraju and et al., 2017].\nGrad-CAM is a popular method that helps visualize which parts of an image are most important to the model's decision. For the needs of our study, we designed a new explainability method based on Grad-CAM that we called Thershold-Grad-CAM. This new explainability method overcomes some is-"}, {"title": "2 Methods", "content": "To quantitatively evaluate the interpretability of the models, we employed the ClickMe dataset [Linsley et al., 2019], which was introduced to capture human attention strategies in classification tasks. The dataset collection process involved a single-player online game, ClickMe.ai [Linsley et al., 2019], where players identified the most informative parts of an image for object recognition. The alignment of model-generated heatmaps with those from the ClickMe dataset measures how closely a model's attention strategy mirrors human strategy."}, {"title": "2.2 DCLS method", "content": "Although larger convolution kernels can improve performance, increasing the kernel size increases the number of parameters and computational cost. Yu and Koltun [2015] introduced dilated convolution (DC) to expand the kernel without increasing parameters. DC inserts zeros between kernel elements, effectively enlarging the kernel without adding new weights. However, DC uses a fixed grid, which can limit performance.\nKhalfaoui-Hassani et al. [2023b] presented DCLS as a new method that builds upon DC. Instead of using fixed spacings between non-zero elements in the kernel, DCLS allows learning these spacings through backpropagation. An interpolation technique is used to overcome the discrete nature of the spacings while maintaining the differentiability necessary for backpropagation."}, {"title": "2.3 Grad-CAM and Threshold-Grad-CAM", "content": "Grad-CAM is a technique that provides visual explanations for the decisions made by deep neural networks. The method uses the gradients of a target concept, propagated into the final convolutional layer of a deep neural network, to produce a localization map highlighting the regions in the input image that are crucial for predicting this concept [Selvaraju and et al., 2017]. Grad-CAM adapts to various network architectures by focusing on the last layer of interest before a classification head or pooling operation. The method is detailed in the supplementary material."}, {"title": "Threshold-Grad-CAM", "content": "In the standard implementation of Grad-CAM, a ReLU activation is applied to the weighted combination of activation maps post-summation. This is predicated on the assumption that positive features should be exclusively highlighted as they are the ones contributing to the class prediction [Selvaraju and et al., 2017]. However, our observations suggest that applying ReLU after the summation can inadvertently suppress useful signals when negative activations are present, as they may negate some positive activations when summed. This phenomenon becomes particularly pronounced in architectures such as ConvFormer and CAFormer, where we observed that the resulting heatmaps were no more informative than random heatmaps. We believe this is due to the choice of a specific activation: StarReLU in these two architectures, which depends on two learnable parameters: scale and bias.\nTo address this issue, we propose applying ReLU to the activation maps before their summation. We then normalize the heatmaps. Finally, the heatmaps are thresholded to retain values above a predetermined threshold (determined experimentally to be 0.3 for optimal results on the ClickMe dataset). The modified Grad-CAM process is described in the supplementary material. Our experiments demonstrated that this modification significantly improved the interpretability of the heatmaps generated for ConvFormer and CAFormer."}, {"title": "3 Related work", "content": "The field of interpretable and explainable Al has recently gained significant attention within the AI community. Extensive research efforts range from defining key terms such as interpretability and explainability to developing explainability methods assessing their trustworthiness and evaluating the interpretability of deep learning models. Gilpin et al. [2018] distinguished between interpretability and explainability and highlighted the challenge of achieving complete and interpretable explanations at the same time. Doshi-Velez and Kim [2017] defined interpretability as the ability to present model decisions in terms understandable to humans. In their study, Mohseni et al. [2021] utilized multi-layer human attention masks to benchmark the effectiveness of explanation methods such as Grad-CAM and LIME. Velmurugan et al. [2020] proposed functionally grounded evaluation metrics that assess the trustworthiness of explainability methods, including LIME and SHAP. Furthermore, Fel et al. [2022] employed the ClickMe dataset to investigate the alignment between human and deep neural network (DNN) visual strategies, applying a training routine that aligns these strategies, as a result enhancing categorization accuracy. In our study, we align with the interpretability definitions in the literature. We employ human heatmaps from the ClickMe dataset as ground truth to evaluate our model's interpretability."}, {"title": "4 Experiments", "content": "In this section, we present the experimental setup used to compare the performance and interpretability of our proposed models. Specifically, we calculated the top-1 accuracy of the models trained on the ImageNet1k validation dataset [Deng and et al., 2009] to assess their classification effectiveness. For interpretability, we employed Spearman's correlation as a metric to compare the alignment between human-generated heatmaps from the ClickMe dataset and the model-generated heatmaps. We assessed the interpretability of the heatmaps produced using two different methods: Grad-CAM and our proposed Threshold-Grad-CAM."}, {"title": "4.1 Results", "content": "We present the results of integrating DCLS into state-of-the-art neural network architectures and our novel update to the Grad-CAM technique. Our experiments evaluated model interpretability, which we defined as the degree of alignment between heatmaps generated by explainability methods and those derived from human visualization strategies."}, {"title": "4.2 Improvement in Model Interpretability with DCLS", "content": "Our experiments incorporated DCLS into five model architectures: ResNet, ConvNeXt, CAFormer, ConvFormer, and FastViT. We trained each model on ImageNetlk. When this is mentioned by_dcls, it means that the training has been done by replacing each depth-separable convolution of the baseline model with DCLS.\nThe results showed an enhancement in model interpretabil-ity with all models but FastViT_sa24. When equipped with DCLS, ConvNeXt improved in heatmap alignment. The score improved with both Grad-CAM and Threshold-Grad-CAM methods, as shown in Table 1 and in Figure 2.\nSince Grad-CAM generates random heatmaps on CAFormer and ConvFormer architectures, we experimented with Threshold-Grad-CAM. Similar to ConvNeXt, CaFormer and ConvFormer showed higher interpretability scores when used with DCLS. The FastViT_sa24 model showed a high interpretability score, even without incorporating DCLS, and applying DCLS didn't improve the score.\nIn addition, DCLS increased the top-1 accuracy in all models but CAFormer_s18 and FastViT_sa36 (Table 1)."}, {"title": "5 Discussion", "content": "Except for FastViT, all model families studied show two points: first, an increase in accuracy when the depthwise separable convolution is replaced by DCLS, and second, an increase in the Treshold-Grad-CAM explanability score when this same modification is made. FastViT is a special model because the test inference is performed with a kernel reparametrization identical to that of RepLKNet Ding et al. [2022]. This could interfere with the DCLS method, which is in fact a different reparametrization, and might explain why the results for this family of models were not correlated in the same way as for the other studied models. Furthermore, the results presented here are significant since we tested three different training seeds for the ConvNeXt-T-dcls model and found an accuracy of 82.49\u00b10.04 and a Treshold-Grad-CAM score of 0.7466\u00b10.004.\nFel et al. [2022] utilized the ClickMe dataset to compare human and DNN visual strategies on ImageNet [Deng and et al., 2009]. They adopted a classic explainability method: Image Feature Saliency [Jiang et al., 2015], to generate comparable feature importance maps for 84 deep neural networks (DNNs). They report that as DNNs become more accurate, a trade-off emerges where their alignment with human visual strategies starts to decrease. In contrast, our study employs Grad-CAM for analyzing DNN visual strategies. Unlike the findings of Fel et al. [2022], our use of Grad-CAM did not reveal such a trade-off. We think that this discrepancy is due to the differences in the explanatory methods used, which highlights the influence of analytical tools in interpreting DNN visual strategies.\nFurthermore, it is conceivable that models with higher interpretability scores may focus more on those image features mostly correlated with the label class. A preliminary examination of the ClickMe dataset reveals that humans tend to concentrate solely on the object representing the class label within the image, ignoring other less directly related features to the class. This behavior likely stems from a nuanced human understanding of the concepts. Therefore, alignment with human-generated heatmaps might reflect a model's robustness."}, {"title": "6 Conclusion", "content": "In this study, we investigated the interpretability of recent deep neural networks using Grad-CAM-based methods for image classification tasks. We found that employing Dilated Convolution with Learnable Spacings enhances network interpretability. Our results indicate that DCLS-equipped models better align with human visual perception, suggesting that such models effectively capture conceptually relevant features akin to human understanding. Future work could focus on investigating the explainability score of DCLS using black-box methods such as RISE."}, {"title": "A Appendix: Grad-CAM Implementation", "content": "Input: Image I, Target class c, Trained Convolutional Neural Network CNN\nOutput: Heatmap H visually highlighting influential regions for class c\n1: Forward Pass:\nProcess image I through CNN to obtain feature maps at the last convolutional layer A. Let $A^k$ be the feature map for the k-th channel.\n2: Compute Gradients:\nCompute the gradient of the loss for class c, denoted $y^c$, with respect to the feature maps A, resulting in $\\frac{\\partial y^c}{\\partial A}$.\n3: Global Average Pooling of Gradients:\nFor each feature map channel k, compute the global average of the gradients:\n$\\alpha_k = \\frac{1}{Z} \\sum_{i} \\sum_{j} \\frac{\\partial y^c}{\\partial A_{ij}^k}$\nwhere i, j index spatial dimensions and Z is the number of elements in $A^k$.\n4: Weighted Combination of Feature Maps:\nCompute the weighted sum of the feature maps using the weights $\\alpha_k$:\n$L = ReLU(\\sum_k \\alpha_k A^k)$\n5: Generate Heatmap:\nResize L to the size of the input image I to get the heatmap H.\n6: Overlay Heatmap on Original Image:\nSuperimpose H onto the original image I for visualization, adjusting the transparency to ensure visibility of underlying features."}, {"title": "B Appendix: Threshold-Grad-CAM Implementation", "content": "Input: Weighted activation maps $A^k$\nParameter: Threshold value t = 0.3\nOutput: Final heatmap H\n1: Apply ReLU Activation:\nApply the ReLU function to each weighted activation map to filter out negative values. This step prevents the cancellation of positive activations during summation:\n$A_{ReLU}^k = ReLU(A^k)$\n2: Summation of Activated Maps:\nSum the ReLU-activated maps: $S = \\sum_k A_{ReLU}^k$\n3: Normalization:\nNormalize the summed activation map S to ensure values are scaled consistently:\n$N = \\frac{S}{max(S)}$\n4: Apply Thresholding:\nApply a threshold of t to reduce noise and enhance the focus on relevant regions:\n$H = \\begin{cases} N & \\text{if } N \\geq t \\\\ 0 & \\text{otherwise} \\end{cases}$\nOur revised approach yields more coherent and focused visual explanations, as validated by quantitative assessments of the ClickMe dataset."}]}