{"title": "Effective Large Language Model Debugging with\nBest-first Tree Search", "authors": ["Jialin Song", "Jonathan Raiman", "Bryan Catanzaro"], "abstract": "Large Language Models (LLMs) show promise in code generation tasks. However,\ntheir code-writing abilities are often limited in scope: while they can successfully\nimplement simple functions, they struggle with more complex tasks. A fundamen-\ntal difference with how an LLM writes code, compared to a human programmer,\nis that it cannot consistently spot and fix bugs. Debugging is a crucial skill for\nprogrammers and it enables iterative code refinement towards a correct imple-\nmentation. In this work, we propose a novel algorithm to enable LLMs to debug\ntheir code via self-reflection and search where a model attempts to identify its\nprevious mistakes. Our key contributions are 1) a best-first tree search algorithm\nwith self-reflections (BESTER) that achieves state-of-the-art Pass@1 in three code\ngeneration benchmarks. BESTER maintains its superiority when we measure pass\nrates taking into account additional inference costs incurred by tree search. 2) A\nnovel interpretability study on what self-reflections attend to in buggy programs\nand how they impact bug fixes, which provides a deeper understanding of the\ndebugging process. 3) An extensive study on when self-reflections are effective in\nfinding bugs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities in writing code from\nnatural language task descriptions [1, 2, 3, 4, 5]. Github Copilot has been shown to boost expert\nprogrammers' productivity [6]. Code generation quality is a popular metric in evaluating foundation\nmodels [7, 8, 2, 9, 10]. However, most existing code generation evaluations ask a model to generate\na single response to a question prompt, which represents a simplistic way to measure code writing\nability as LLMs are presently unreliable at zero-shot code generation. We observe that code writing\nis an iterative process where human programmers alternate between writing code, testing code (with\nunit tests), and debugging test failures; whereas a language model would fail to generate proper code\nsimply because a single wrongly predicted token due to stochastic sampling.\nInspired by the iterative nature of human programming, we ask: will an LLM finish more coding\ntasks if we allow it to iteratively debug its implementations using unit test feedback? In this paper,\nwe answer in the affirmative by proposing a general best-first tree search framework that combines\nexecution feedback and self-reflection and improves code generation abilities of LLMs. Self-reflection\nrefers to a special generation task where an LLM reviews its earlier outputs and attempts to identify\nlogical flaws within. In code generation, we call self-reflection the process of prompting an LLM\nto produce debugging instructions and feedback when given as input the source code of a buggy\nprogram with the associated execution trace/errors/test results. Self-reflections can instruct LLMs to\nrepair a buggy program, a process we call program repair. Our search algorithm, BEst Self-reflection"}, {"title": "2 Related Works", "content": "LLMs that are capable of generating code from natural language description are ubiquitous [12, 1,\n16, 4, 3, 2, 8, 5]. We often evaluate a code generation LLM by prompting it with a task description\nand evaluate the correctness of its output by executing a suite of input-output tests [1, 12, 13].\nSeveral recent works implemented search procedures with test execution information and model\nself-reflections to improve generation quality. Reflexion [17] and self-refine [18] utilize model\nself-reflection to correct prior buggy implementations. LATS [19] uses Monte-Carlo tree search to\nsearch for a correct implementation. There is also recent work on improving self-reflection generation\nquality by finetuning models [20]. Our work proposes an alternative tree search algorithm based on\nbest-first tree search with self-reflections.\nBeyond small programming task benchmarks, researchers also developed larger scale programming\ntasks to capture real-world use cases. SWE-Bench [21] challenges LLMs to solve Github issues.\nDevBench [22] evaluates LLMs on different tasks in software development, such as project design\nand unit testing. In addition to code writing ability, retrieval-augmented generation (RAG) is often\nessential in accomplishing these tasks [21]."}, {"title": "2.2 Logical Reasoning in LLMs", "content": "Code generation belongs to the category of logical reasoning tasks, which includes mathematical\nreasoning [23, 24, 25]. Existing research works have developed many useful inference techniques to\nboost a model's reasoning ability, such as chain-of-thought prompting [26], tree-of-thoughts [27], and\ncode interpreter integration [28]. Least-to-most prompting [29] takes a divide-and-conquer approach\nto helping LLMs tackle complex problems. Tree search is another popular algorithmic framework\nto iteratively build a correct reasoning path [19, 25]. However, these techniques are fundamentally"}, {"title": "3 Methodology", "content": "In this section, we describe our proposed Best Self-reflection Tree Search (BESTER) algorithm for\ncode generation. Each code generation problem $P$ contains a natural language specification $X$ on\nits functionality and a suite of test cases $T$ to evaluate the correctness of an implementation for $X$.\nAfter running a program $S$ on $T$, we record $r \\in [0, 1]$, a score measuring the percentage of tests in $T$\nthat it passes. If $r = 1$, we find a program that passes all tests. Note that an important distinction\nexists between passing all tests and concluding that $S$ is correct according to the specification $X$: test\ncoverage can be poor, allowing a program to be incorrect on corner cases outside the test suite but\nproduce the desired outputs on the cases that were checked [38]. To obtain a more truthful measure\nof correctness, we will evaluate with a more comprehensive test suite at the end of the tree search.\nWe will provide more details in section 4.\nLanguage model self-reflections are recently gaining popularity as an approach to refining its logical\nreasoning [17, 19]. Broadly speaking, it is a technique to present a language model with an incorrect\nanswer and prompt it to identify logical issues within. For code generation tasks, logical issues\nmanifest as bugs that we wish for a model to rectify. Our approach incorporates self-reflections in a\nbest-first tree algorithm to enable iterative solution refinement. BESTER interleaves three operations:\nprogram generation, execution evaluation, and self-reflection generation. Figure 1 displays the core\noperations of our proposed algorithm. We describe each component in detail."}, {"title": "Program Generation", "content": "Given a problem $P = (X, T)$, we use a language model LM to generate a\nprogram by greedy decoding $S = LM(X)$. While existing works show that temperature sampling\nimproves code generation performances [39, 40], we choose greedy decoding to better isolate the\neffect of searching with self-reflections. We prompt the model to follow a specific format of wrapping"}, {"title": "Execution Evaluation", "content": "We measure the correctness of a program $S$ by running it with test cases in\n$T$ to compare output results, and obtain a score $r$. If $r = 1$, we find a program satisfying all test cases\nand terminate the search. If $r < 1$, we continue onto self-reflection generations."}, {"title": "Self-reflection Generation", "content": "If there are failed test cases, we emulate the debugging process by\nprompting a model to self-reflect on issues in $S$. We posit a successful debugger needs access to the\nproblem specification $X$, the buggy program $S$, and the set of its failed test cases $T_{fail} \\subset T$, thus we\nformulate a new prompt with these information $X || S || T_{fail}$. A model generates a self-reflection\n$SR = LM(X || S || T_{fail})$ to describe bugs in $S$ and potential actions to fix them. With $SR$ and the\nprevious solution $S$, we ask the model to generate a program repair $S' = LM(S || SR)$."}, {"title": "4 Experiments", "content": "We evaluate BESTER on three code generation benchmarks: HumanEval [1], MBPP [12] and\nAPPS[13]. We investigate the following questions: 1) Is best-first search with self-reflection an\neffective algorithm at solving code generation tasks? (subsection 4.2); 2) How do design decisions in\nBESTER influence its performances? (subsection 4.3); 3) Can we interpret the generation process of\nself-reflections and program repairs? (subsection 4.4)"}, {"title": "4.1 Experiment Setup", "content": "Models We evaluate BESTER with three models: GPT-4\u00b9 [2], deepseek-coder-33b-instruct\n(Deepseek) [5] and Meta-Llama-3-70B-Instruct (Llama3) [41]. All three models are finetuned\nfor instruction following, which is an important ability because in the self-reflection generation and\nprogram repair steps we need the model to carry out specific actions.\nDatasets We evaluate the correctness of a program with unit tests. For HumanEval and MBPP,\nwe use the expanded test suites in EvalPlus [42]. For each problem, we divide its test suite into two\nsubsets: a small subset for execution evaluation during search and a large one that acts as a held-out\nset to test the returned program from search. In our experiments, we label a program correct if it\npasses all cases in the held-out test set to ensure the separation between search and final evaluation.\nFor HumanEval and MBPP, we use test cases in the original dataset during search and the rest of\ncases in EvalPlus as held-out sets. For APPS, we select 300 problems with at least 10 test cases. We\nuse 3 tests during search and the rest as the held-out set.\nMetrics Pass@k is the canonical metric to evaluate code generation tasks [1]. It measures the\nprobability that at least one program from $k$ i.i.d. samples is correct. This metric is proposed for the\nsampling-based generation procedure but we can also apply it for search-based generation because of\nthe creation of held-out test sets. We ensure that during search the model will not see test cases that\nthe final evaluation is based on. When a search-based method, such as BESTER, only returns a single\nsolution to evaluate on the held-out set, then Pass@1 is a valid metric. We remark that search-based\ngeneration such as Algorithm 1 may require multiple model inferences to finish, but the Pass@1\nmetric ignores the additional computational cost. We address this limitation with a second metric\nPass@Infer that integrates the computation cost increase by measuring the test suite pass rate given a\nfixed number of model inference calls.\nBaselines We compare with three baseline methods. The first one is temperature sampling which\ndoes not utilize any code execution feedback. The second is best-first tree search with only execution\nfeedback without self-reflections (Ex. Feedback). The last one is Reflexion [17] which is a special\ncase of BESTER if we only sample a single self-reflection at each iteration.\nBESTER Parameters We run BESTER with 2 rounds of self-reflections and sample 5 self-\nreflections each round with a 0.8 temperature term. This corresponds to setting $N = 2, k = 5$ in\nAlgorithm 1. We use greedy decoding for both the initial and subsequent repair code generation.\nWe provide an ablation on the choice of $N$ and $k$ in subsection 4.3. With this parameter setting, we\nallocate 21 inference calls during tree search, 1 for the initial greedy decoding and $N \\times k = 10$ each\nfor self-reflection and program repair generation. So for Pass@Infer results in the next section, we fix\nnumber the inferences to be 21 for all methods."}, {"title": "4.3 Ablation Study", "content": "In this section, we ablate two components in the BESTER algorithm: the selection rule during tree\nsearch (Line 10 in Algorithm 1), and inference allocations (values of $N$, $k$) in tree search.\nSelection Rule Selection rule determines which program repair to follow during tree search. We\nconstruct 4 heuristic selection rules: randomly select a repair (Random), select a repair generated\nfrom a self-reflection that contains code snippets (Code), select a repair having the minimal number\nof diffs from the buggy program (Min-diff), and select one having the maximal number of diffs\n(Max-diff). Table 5 confirms that the number of passing tests is a more useful signal to guide the tree\nsearch than heuristic rules. We leave it as future work for better selection rules, for example taking\nthe entire search tree into account in making selection decisions.\nInference Allocation Two parameters in the BESTER algorithm control the shape of a search tree:\n$N$ the maximal depth to search and $k$ the number of self-reflections to sample at each iteration. In\nthe equal compute setting, using a larger value of $N$ necessitates a smaller value of $k$ accordingly.\nIs it better to search with a shallow and wide tree or a deep and narrow one? We use the same 21\ntotal inferences as in our main experiments. Subtracting the initial greedy decoding inference, we\nhave 20 left for tree search. A program repair call follows each self-reflection generation so we can\ngenerate 20/2 = 10 self-reflections during a tree search. Thus, we keep $N \\times k = 10$ in this ablation\nstudy, and study 4 configurations $(N, k) = (1, 10), (2, 5), (5, 2), (10, 1)$. Figure 2 shows Pass@Infer\ncomparisons. Configuration values in the middle tend to be better than the extremes. We find that\n$N = 2$ is empirically the best value. We leave adaptive setting values of $N$ and $k$ as future work."}, {"title": "4.4 Model Interpretations", "content": "In this section, we present novel interpretability studies into two components in BESTER: self-\nreflection generation and program repair generation. We use Captum [43] to perform input feature\nperturbation [15] to compute attribution scores. We use the log probability of a target string from\nthe language model, given an input prompt, to compute attribution scores. Mathematically, the\nattribution score for an input-output pair $(I, O)$ is $\\log P_{LM(I)}(O) - \\log P_{LM(I')}(O)$ where $P_{LM}(I)$\nis the probability distribution over output sequences when a model LM receives an input prompt\n$I$, and $I'$ is a perturbed prompt from $I$. It measures how much more likely a model generates $O$\nwhen the input is $I$ compared to $I'$. A high attribution score means that the generation of $O$ depends\nheavily on $I$. Captum computes per output token attribution scores so that we can aggregate scores\nfor parts of the output, for example, individual lines in a program."}, {"title": "5 Conclusion", "content": "In this paper, we propose BESTER a best-first tree search algorithm with self-reflections for code\ngeneration tasks. This algorithm enables language models to solve programming tasks iteratively.\nBESTER achieves state-of-the-art Pass@1 using three different langauge models and excels in the\nequal compute setting as well. We analyze the connection between self-reflection correctness and its\neffectiveness in successful repairing a buggy program. Finally, we present a novel interpretability\nstudy on self-reflection and program repair generations. Applying BESTER to other logical reasoning\ntasks is a natural next step. Real-world software engineering tasks pose a scalability challenge due to\nadditional difficulties posed by the additional context required. Integrating BESTER into an agent\nframework is an interesting future direction."}, {"title": "A Discussion", "content": "Limitations The main limitation in this study is the scale of studied code generation tasks as most\nof the solution programs have fewer than 100 lines of code. Real-life software engineering projects\ncan easily reach tens of thousands of lines so whether BESTER can handle code generation tasks\nat a much larger scale is an open question. Another limitation is that BESTER is an inference-time\nsearch technique to enhance an existing model. One could argue that model finetuning is necessary to\nimprove performance even further. We leave this as future direction.\nBroader Impacts Automatic code generation with LLMs holds promise to boost the productivity\nfor software engineers. Tools like Github Copilot are already being integrated into daily workflows.\nThe usefulness of such tools hinge on whether they can consistently satisfy their users' needs.\nBESTER improves generation quality and could make those tools more helpful. However, there is\nalways the possibility of misuse by malicious agents, e.g., to generate malware at scale. Even in\nlegitimate use cases, automatic code generation tools can introduce bugs that are costly to fix. So we\nneed to take a balanced view in order to improve code generation models."}, {"title": "B Dataset Details", "content": "In section 4, we use three datasets: HumanEval [1], MBPP [12] and APPS [13]. We use test suites in\nEvalPlus [42] for HumanEval and MBPP final evaluations. EvalPlus suite covers all 164 problems\nin HumanEval and 378 problems in MBPP. We subsample 300 problems from the training split of\nAPPS. We make sure each problem has at least 10 test cases and its type is call-based.\nThe problem indices we use for APPS are: 3329, 4460, 2772, 3433, 4135, 3697, 1613, 3837, 4133,\n3758, 3995, 4209, 3817, 3763, 3622, 4262, 3522, 4123, 1625, 4641, 1629, 4588, 2995, 3419, 4636,\n2927, 2728, 4091, 4042, 4276, 3814, 2730, 3478, 3155, 4281, 2794, 4424, 4050, 3562, 2758, 4409,\n4339, 2710, 2714, 3301, 2799, 4179, 3861, 3048, 3203, 2661, 4650, 4242, 3470, 3822, 4458, 2731,\n4378, 2842, 4035, 3769, 2700, 4523, 3565, 4069, 3826, 4531, 3066, 3606, 3222, 2899, 2695, 4440,\n3454, 3174, 2936, 4648, 4232, 2752, 3505, 3891, 3492, 4346, 4709, 4258, 3276, 3064, 2785, 4147,\n4271, 4176, 3842, 4464, 4105, 3445, 2801, 3989, 4616, 2744, 4752, 3060, 4668, 3308, 2879, 4104,\n2892, 4018, 3096, 3024, 2652, 3217, 4140, 4577, 4298, 2885, 4096, 4084, 2998, 3907, 4169, 4329,\n2860, 3461, 3229, 2975, 3016, 4556, 4260, 2828, 4007, 2949, 4596, 2957, 4046, 4509, 3164, 3768,\n3740, 3099, 2654, 2706, 3486, 2774, 4312, 4168, 4487, 4186, 3811, 2783, 3411, 4174, 4029, 3483,\n4332, 3130, 3213, 3030, 2711, 3927, 2698, 3777, 4514, 3463, 3600, 3909, 3460, 4461, 1660, 3134,\n3242, 4374, 3937, 3128, 3786, 3504, 4322, 3446, 4185, 4557, 3334, 4292, 3238, 3243, 3360, 1633,\n4244, 3475, 1643, 1611, 4450, 4445, 4144, 3166, 2845, 2793, 3878, 4022, 4365, 3356, 3274, 3605,\n4499, 4327, 3506, 3523, 3139, 3372, 3601, 2874, 3519, 3674, 3205, 4548, 4240, 2830, 3888, 3974,\n3042, 3176, 4127, 3823, 3561, 2736, 1618, 1658, 3391, 3654, 3045, 4220, 3211, 3514, 4353, 4740,\n4067, 1652, 4626, 4369, 3744, 3727, 4200, 3062, 3895, 3040, 2868, 4211, 4521, 2779, 4212, 4634,\n4667, 3163, 4664, 4044, 4682, 4685, 3966, 4724, 3306, 4643, 4484, 3862, 3594, 3036, 3881, 3094,\n3224, 4608, 4164, 4057, 4020, 2651, 4572, 4387, 3187, 4126, 4076, 4654, 2881, 1647, 3358, 3880,\n3320, 4705, 4693, 1638, 2814, 3491, 4610, 3402, 3554, 4465, 2802, 3569, 3531, 3244, 3930, 3083,\n4227, 4361, 3346."}, {"title": "C More Interpretability Results", "content": "Figure 6 and Figure 7 show mean normalized rank analyses on MBPP and APPS. We observe the\nsame results as on HumanEval in the main paper. We further confirm that diff lines having lower mean\nnormalized rank than non-diff lines is statistically significant by recording the p-value of one-tailed\nt-tests. All the p-values in Table 6 and Table 7 are smaller than 0.05."}, {"title": "D Prompts", "content": "Initial greedy decoding prompt:\nComplete {query} Keep the function signature and docstring if there is one. Wrap your code with\n```. Do not include any assert statements."}, {"title": "E Compute Resources", "content": "All our experiments use 4 NVIDIA A100 GPUs."}]}