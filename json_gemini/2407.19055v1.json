{"title": "Effective Large Language Model Debugging with Best-first Tree Search", "authors": ["Jialin Song", "Jonathan Raiman", "Bryan Catanzaro"], "abstract": "Large Language Models (LLMs) show promise in code generation tasks. However, their code-writing abilities are often limited in scope: while they can successfully implement simple functions, they struggle with more complex tasks. A fundamental difference with how an LLM writes code, compared to a human programmer, is that it cannot consistently spot and fix bugs. Debugging is a crucial skill for programmers and it enables iterative code refinement towards a correct implementation. In this work, we propose a novel algorithm to enable LLMs to debug their code via self-reflection and search where a model attempts to identify its previous mistakes. Our key contributions are 1) a best-first tree search algorithm with self-reflections (BESTER) that achieves state-of-the-art Pass@1 in three code generation benchmarks. BESTER maintains its superiority when we measure pass rates taking into account additional inference costs incurred by tree search. 2) A novel interpretability study on what self-reflections attend to in buggy programs and how they impact bug fixes, which provides a deeper understanding of the debugging process. 3) An extensive study on when self-reflections are effective in finding bugs.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities in writing code from natural language task descriptions [1, 2, 3, 4, 5]. Github Copilot has been shown to boost expert programmers' productivity [6]. Code generation quality is a popular metric in evaluating foundation models [7, 8, 2, 9, 10]. However, most existing code generation evaluations ask a model to generate a single response to a question prompt, which represents a simplistic way to measure code writing ability as LLMs are presently unreliable at zero-shot code generation. We observe that code writing is an iterative process where human programmers alternate between writing code, testing code (with unit tests), and debugging test failures; whereas a language model would fail to generate proper code simply because a single wrongly predicted token due to stochastic sampling.\nInspired by the iterative nature of human programming, we ask: will an LLM finish more coding tasks if we allow it to iteratively debug its implementations using unit test feedback? In this paper, we answer in the affirmative by proposing a general best-first tree search framework that combines execution feedback and self-reflection and improves code generation abilities of LLMs. Self-reflection refers to a special generation task where an LLM reviews its earlier outputs and attempts to identify logical flaws within. In code generation, we call self-reflection the process of prompting an LLM to produce debugging instructions and feedback when given as input the source code of a buggy program with the associated execution trace/errors/test results. Self-reflections can instruct LLMs to repair a buggy program, a process we call program repair. Our search algorithm, BEst Self-reflection"}, {"title": "Related Works", "content": "LLMs that are capable of generating code from natural language description are ubiquitous [12, 1, 16, 4, 3, 2, 8, 5]. We often evaluate a code generation LLM by prompting it with a task description and evaluate the correctness of its output by executing a suite of input-output tests [1, 12, 13]. Several recent works implemented search procedures with test execution information and model self-reflections to improve generation quality. Reflexion [17] and self-refine [18] utilize model self-reflection to correct prior buggy implementations. LATS [19] uses Monte-Carlo tree search to search for a correct implementation. There is also recent work on improving self-reflection generation quality by finetuning models [20]. Our work proposes an alternative tree search algorithm based on best-first tree search with self-reflections.\nBeyond small programming task benchmarks, researchers also developed larger scale programming tasks to capture real-world use cases. SWE-Bench [21] challenges LLMs to solve Github issues. DevBench [22] evaluates LLMs on different tasks in software development, such as project design and unit testing. In addition to code writing ability, retrieval-augmented generation (RAG) is often essential in accomplishing these tasks [21]."}, {"title": "Logical Reasoning in LLMs", "content": "Code generation belongs to the category of logical reasoning tasks, which includes mathematical reasoning [23, 24, 25]. Existing research works have developed many useful inference techniques to boost a model's reasoning ability, such as chain-of-thought prompting [26], tree-of-thoughts [27], and code interpreter integration [28]. Least-to-most prompting [29] takes a divide-and-conquer approach to helping LLMs tackle complex problems. Tree search is another popular algorithmic framework to iteratively build a correct reasoning path [19, 25]. However, these techniques are fundamentally"}, {"title": "Methodology", "content": "In this section, we describe our proposed Best Self-reflection Tree Search (BESTER) algorithm for code generation. Each code generation problem P contains a natural language specification X on its functionality and a suite of test cases T to evaluate the correctness of an implementation for X. After running a program S on T, we record $r \\in [0, 1]$, a score measuring the percentage of tests in T that it passes. If r = 1, we find a program that passes all tests. Note that an important distinction exists between passing all tests and concluding that S is correct according to the specification X: test coverage can be poor, allowing a program to be incorrect on corner cases outside the test suite but produce the desired outputs on the cases that were checked [38]. To obtain a more truthful measure of correctness, we will evaluate with a more comprehensive test suite at the end of the tree search. We will provide more details in section 4.\nLanguage model self-reflections are recently gaining popularity as an approach to refining its logical reasoning [17, 19]. Broadly speaking, it is a technique to present a language model with an incorrect answer and prompt it to identify logical issues within. For code generation tasks, logical issues manifest as bugs that we wish for a model to rectify. Our approach incorporates self-reflections in a best-first tree algorithm to enable iterative solution refinement. BESTER interleaves three operations: program generation, execution evaluation, and self-reflection generation. Figure 1 displays the core operations of our proposed algorithm. We describe each component in detail.\nGiven a problem P = (X, T), we use a language model LM to generate a program by greedy decoding S = LM(X). While existing works show that temperature sampling improves code generation performances [39, 40], we choose greedy decoding to better isolate the effect of searching with self-reflections. We prompt the model to follow a specific format of wrapping"}, {"title": "Experiments", "content": "We evaluate BESTER on three code generation benchmarks: HumanEval [1], MBPP [12] and APPS[13]. We investigate the following questions: 1) Is best-first search with self-reflection an effective algorithm at solving code generation tasks? (subsection 4.2); 2) How do design decisions in BESTER influence its performances? (subsection 4.3); 3) Can we interpret the generation process of self-reflections and program repairs? (subsection 4.4)"}, {"title": "Experiment Setup", "content": "We evaluate BESTER with three models: GPT-4\u00b9 [2], deepseek-coder-33b-instruct (Deepseek) [5] and Meta-Llama-3-70B-Instruct (Llama3) [41]. All three models are finetuned for instruction following, which is an important ability because in the self-reflection generation and program repair steps we need the model to carry out specific actions.\nWe evaluate the correctness of a program with unit tests. For HumanEval and MBPP, we use the expanded test suites in EvalPlus [42]. For each problem, we divide its test suite into two subsets: a small subset for execution evaluation during search and a large one that acts as a held-out set to test the returned program from search. In our experiments, we label a program correct if it passes all cases in the held-out test set to ensure the separation between search and final evaluation. For HumanEval and MBPP, we use test cases in the original dataset during search and the rest of cases in EvalPlus as held-out sets. For APPS, we select 300 problems with at least 10 test cases. We use 3 tests during search and the rest as the held-out set.\nPass@k is the canonical metric to evaluate code generation tasks [1]. It measures the probability that at least one program from k i.i.d. samples is correct. This metric is proposed for the sampling-based generation procedure but we can also apply it for search-based generation because of the creation of held-out test sets. We ensure that during search the model will not see test cases that the final evaluation is based on. When a search-based method, such as BESTER, only returns a single solution to evaluate on the held-out set, then Pass@1 is a valid metric. We remark that search-based generation such as Algorithm 1 may require multiple model inferences to finish, but the Pass@1 metric ignores the additional computational cost. We address this limitation with a second metric Pass@Infer that integrates the computation cost increase by measuring the test suite pass rate given a fixed number of model inference calls.\nWe compare with three baseline methods. The first one is temperature sampling which does not utilize any code execution feedback. The second is best-first tree search with only execution feedback without self-reflections (Ex. Feedback). The last one is Reflexion [17] which is a special case of BESTER if we only sample a single self-reflection at each iteration.\nWe run BESTER with 2 rounds of self-reflections and sample 5 self-reflections each round with a 0.8 temperature term. This corresponds to setting N = 2, k = 5 in Algorithm 1. We use greedy decoding for both the initial and subsequent repair code generation. We provide an ablation on the choice of N and k in subsection 4.3. With this parameter setting, we allocate 21 inference calls during tree search, 1 for the initial greedy decoding and N \u00d7 k = 10 each for self-reflection and program repair generation. So for Pass@Infer results in the next section, we fix number the inferences to be 21 for all methods."}, {"title": "Main Results", "content": "A subset of the problems in each dataset can be solved directly by the initial greedy decoding program from the problem prompt so we do not need to perform search for them."}, {"title": "Ablation Study", "content": "In this section, we ablate two components in the BESTER algorithm: the selection rule during tree search (Line 10 in Algorithm 1), and inference allocations (values of N, k) in tree search.\nSelection rule determines which program repair to follow during tree search. We construct 4 heuristic selection rules: randomly select a repair (Random), select a repair generated from a self-reflection that contains code snippets (Code), select a repair having the minimal number of diffs from the buggy program (Min-diff), and select one having the maximal number of diffs (Max-diff). Table 5 confirms that the number of passing tests is a more useful signal to guide the tree search than heuristic rules. We leave it as future work for better selection rules, for example taking the entire search tree into account in making selection decisions.\nTwo parameters in the BESTER algorithm control the shape of a search tree: N the maximal depth to search and k the number of self-reflections to sample at each iteration. In the equal compute setting, using a larger value of N necessitates a smaller value of k accordingly. Is it better to search with a shallow and wide tree or a deep and narrow one? We use the same 21 total inferences as in our main experiments. Subtracting the initial greedy decoding inference, we have 20 left for tree search. A program repair call follows each self-reflection generation so we can generate 20/2 = 10 self-reflections during a tree search. Thus, we keep N \u00d7 k = 10 in this ablation study, and study 4 configurations (N, k) = (1, 10), (2, 5), (5, 2), (10, 1). Configuration values in the middle tend to be better than the extremes. We find that N = 2 is empirically the best value. We leave adaptive setting values of N and k as future work."}, {"title": "Model Interpretations", "content": "In this section, we present novel interpretability studies into two components in BESTER: self-reflection generation and program repair generation. We use Captum [43] to perform input feature perturbation [15] to compute attribution scores. We use the log probability of a target string from the language model, given an input prompt, to compute attribution scores. Mathematically, the attribution score for an input-output pair (I, O) is $log P_{LM(I)}(O) \u2013 log P_{LM(I')}(O)$ where $P_{LM}(I)$ is the probability distribution over output sequences when a model LM receives an input prompt I, and I' is a perturbed prompt from I. It measures how much more likely a model generates O when the input is I compared to I'. A high attribution score means that the generation of O depends heavily on I. Captum computes per output token attribution scores so that we can aggregate scores for parts of the output, for example, individual lines in a program.\nWe extract triples (S, SR, S') from BESTER search trees, where S is a buggy program, SR is a self-reflection based on S, and S' is a program repair based on S and SR. We first use (I, O) = (S, SR) to study how self-reflections depend on buggy programs. Then we use (I, O) = (SR, S') to study how program repairs depend on self-reflections.\nWe first study how self-reflection gen-eration depends on buggy programs. Figure 3a shows an example from the Deepseek model. The self-reflection on the right has an attribution score (the middle column) for each line of the input buggy program on the left. Based on this self-reflection, Deepseek generates a repair program Figure 3b. We highlight in red the diff in the buggy program from the repair. In this example, the diff line has the highest attribution score which means this self-reflection depends most heavily on this line. Indeed, the self-reflection directly refers to the incorrect frequency comparison in the highlighted line.\nWe hypothesize that attribution scores of diff lines tend to be higher than those of non-diff lines. As attribution score ranges vary from problem to problem, we transform them into normalized ranks: first sort the scores in descending order then assign a line a normalized rank of $r/(R \u2013 1)$ if its attribution score ranking is r, where R is the total number of program lines. Then we compute the mean normalized ranks among diff and non-diff lines for each program, with results on HumanEval in Figure 4. The mean normalized rank of diff lines is lower than that of non-diff for both Deepseek and Llama3, which implies that diff lines on average have higher attribution scores. The differences are statistically significance (Appendix C). We carry out the same analyses for MBPP and APPS, and the same result holds. So we can conclude that self-reflections target diff lines in buggy programs.\nNext we study how self-reflections influence program repairs. We show an example in Figure 3b which is a continuation from Figure 3a. For each line in the repair program, the rightmost column shows its corresponding attribution score on the self-reflection. Again, the diff line has the highest attribution score, which means that it depends most heavily on the self-reflection. Indeed, the self-reflection contains instruction on how to fix the bug.\nFigure 5 compares mean normalized ranks of diff and non-diff lines on HumanEval. The mean normalized rank of diff lines is lower than that of non-diff lines for both Deepseek and Llama3, which implies that diff lines in general have higher attribution scores. So we can conclude that self-reflections cause targeted code diff edits."}, {"title": "Conclusion", "content": "In this paper, we propose BESTER a best-first tree search algorithm with self-reflections for code generation tasks. This algorithm enables language models to solve programming tasks iteratively. BESTER achieves state-of-the-art Pass@1 using three different langauge models and excels in the equal compute setting as well. We analyze the connection between self-reflection correctness and its effectiveness in successful repairing a buggy program. Finally, we present a novel interpretability study on self-reflection and program repair generations. Applying BESTER to other logical reasoning tasks is a natural next step. Real-world software engineering tasks pose a scalability challenge due to additional difficulties posed by the additional context required. Integrating BESTER into an agent framework is an interesting future direction."}, {"title": "Discussion", "content": "The main limitation in this study is the scale of studied code generation tasks as most of the solution programs have fewer than 100 lines of code. Real-life software engineering projects can easily reach tens of thousands of lines so whether BESTER can handle code generation tasks at a much larger scale is an open question. Another limitation is that BESTER is an inference-time search technique to enhance an existing model. One could argue that model finetuning is necessary to improve performance even further. We leave this as future direction.\nAutomatic code generation with LLMs holds promise to boost the productivity for software engineers. Tools like Github Copilot are already being integrated into daily workflows. The usefulness of such tools hinge on whether they can consistently satisfy their users' needs. BESTER improves generation quality and could make those tools more helpful. However, there is always the possibility of misuse by malicious agents, e.g., to generate malware at scale. Even in legitimate use cases, automatic code generation tools can introduce bugs that are costly to fix. So we need to take a balanced view in order to improve code generation models."}, {"title": "Dataset Details", "content": "In section 4, we use three datasets: HumanEval [1], MBPP [12] and APPS [13]. We use test suites in EvalPlus [42] for HumanEval and MBPP final evaluations. EvalPlus suite covers all 164 problems in HumanEval and 378 problems in MBPP. We subsample 300 problems from the training split of APPS. We make sure each problem has at least 10 test cases and its type is call-based.\nThe problem indices we use for APPS are: 3329, 4460, 2772, 3433, 4135, 3697, 1613, 3837, 4133, 3758, 3995, 4209, 3817, 3763, 3622, 4262, 3522, 4123, 1625, 4641, 1629, 4588, 2995, 3419, 4636, 2927, 2728, 4091, 4042, 4276, 3814, 2730, 3478, 3155, 4281, 2794, 4424, 4050, 3562, 2758, 4409, 4339, 2710, 2714, 3301, 2799, 4179, 3861, 3048, 3203, 2661, 4650, 4242, 3470, 3822, 4458, 2731, 4378, 2842, 4035, 3769, 2700, 4523, 3565, 4069, 3826, 4531, 3066, 3606, 3222, 2899, 2695, 4440, 3454, 3174, 2936, 4648, 4232, 2752, 3505, 3891, 3492, 4346, 4709, 4258, 3276, 3064, 2785, 4147, 4271, 4176, 3842, 4464, 4105, 3445, 2801, 3989, 4616, 2744, 4752, 3060, 4668, 3308, 2879, 4104, 2892, 4018, 3096, 3024, 2652, 3217, 4140, 4577, 4298, 2885, 4096, 4084, 2998, 3907, 4169, 4329, 2860, 3461, 3229, 2975, 3016, 4556, 4260, 2828, 4007, 2949, 4596, 2957, 4046, 4509, 3164, 3768, 3740, 3099, 2654, 2706, 3486, 2774, 4312, 4168, 4487, 4186, 3811, 2783, 3411, 4174, 4029, 3483, 4332, 3130, 3213, 3030, 2711, 3927, 2698, 3777, 4514, 3463, 3600, 3909, 3460, 4461, 1660, 3134, 3242, 4374, 3937, 3128, 3786, 3504, 4322, 3446, 4185, 4557, 3334, 4292, 3238, 3243, 3360, 1633, 4244, 3475, 1643, 1611, 4450, 4445, 4144, 3166, 2845, 2793, 3878, 4022, 4365, 3356, 3274, 3605, 4499, 4327, 3506, 3523, 3139, 3372, 3601, 2874, 3519, 3674, 3205, 4548, 4240, 2830, 3888, 3974, 3042, 3176, 4127, 3823, 3561, 2736, 1618, 1658, 3391, 3654, 3045, 4220, 3211, 3514, 4353, 4740, 4067, 1652, 4626, 4369, 3744, 3727, 4200, 3062, 3895, 3040, 2868, 4211, 4521, 2779, 4212, 4634, 4667, 3163, 4664, 4044, 4682, 4685, 3966, 4724, 3306, 4643, 4484, 3862, 3594, 3036, 3881, 3094, 3224, 4608, 4164, 4057, 4020, 2651, 4572, 4387, 3187, 4126, 4076, 4654, 2881, 1647, 3358, 3880, 3320, 4705, 4693, 1638, 2814, 3491, 4610, 3402, 3554, 4465, 2802, 3569, 3531, 3244, 3930, 3083, 4227, 4361, 3346."}, {"title": "More Interpretability Results", "content": "Figure 6 and Figure 7 show mean normalized rank analyses on MBPP and APPS. We observe the same results as on HumanEval in the main paper. We further confirm that diff lines having lower mean normalized rank than non-diff lines is statistically significant by recording the p-value of one-tailed t-tests. All the p-values in Table 6 and Table 7 are smaller than 0.05."}, {"title": "Prompts", "content": "Initial greedy decoding prompt:\nComplete {query} Keep the function signature and docstring if there is one. Wrap your code with```. Do not include any assert statements.\nGPT-4 evaluate self-reflection prompt: I am going to show you a programming exercise and an incorrect to the exercise. Then I am going to show you an explanation about why the incorrect solution is incorrect. Your task is to classify whether the explanation correctly identifies issues in the code. Respond yes if the explanation is correct and no otherwise. Your response should only consist of one word: \"yes\" or \"no\".\nThe exercise is query The incorrect solution is program It failed the following test cases errors The explanation is self-reflection Please analyze the explanation carefully and respond with \"yes\" or \"no\"."}, {"title": "Compute Resources", "content": "All our experiments use 4 NVIDIA A100 GPUs."}]}