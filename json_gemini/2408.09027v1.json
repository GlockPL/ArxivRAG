{"title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction", "authors": ["Kai Qiu", "Xiang Li", "Hao Chen", "Jie Sun", "Jinglu Wang", "Zhe Lin", "Marios Savvides", "Bhiksha Raj"], "abstract": "Audio generation has achieved remarkable progress with the advance of sophisticated generative models, such as diffusion models (DMs) and autoregressive (AR) models. However, due to the naturally significant sequence length of audio, the efficiency of audio generation remains an essential issue to be addressed, especially for AR models that are incorporated in large language models (LLMs). In this paper, we analyze the token length of audio tokenization and propose a novel Scale-level Audio Tokenizer (SAT), with improved residual quantization. Based on SAT, a scale-level Acoustic AutoRegressive (AAR) modeling framework is further proposed, which shifts the next-token AR prediction to next-scale AR prediction, significantly reducing the training cost and inference time. To validate the effectiveness of the proposed approach, we comprehensively analyze design choices and demonstrate the proposed AAR framework achieves a remarkable 35x faster inference speed and +1.33 Fr\u00e9chet Audio Distance (FAD) against baselines on the AudioSet benchmark.", "sections": [{"title": "Introduction", "content": "Autoregressive (AR) modeling (Achiam et al. 2023; Sun et al. 2024) has been widely used in the generation domain, which typically involves two steps - token quantization (Esser, Rombach, and Ommer 2021; Yu et al. 2021) and next-token prediction (Achiam et al. 2023; Touvron et al. 2023). Specifically, the token quantization aims to convert the inputs to a sequence of discrete tokens and the next-token prediction models the conditional distribution of one token based on previous ones. AR approaches have shown significant success in textual modeling, e.g., large language models (LLMs) (Vaswani et al. 2017; Devlin et al. 2018; Touvron et al. 2023; Achiam et al. 2023) and even visual modeling (Dosovitskiy et al. 2020; Chang et al. 2022). However, despite its effectiveness, AR-based audio generation remains under-explored.\nUnlike natural language which is discrete and can be easily tokenized into a short series of tokens, audio demonstrates more challenges to be discretized without losing perceptual quality given its long sequence and continuity nature. Previous approaches (D\u00e9fossez et al. 2022; Yang et al. 2023; Kumar et al. 2024; Zeghidour et al. 2021) leverage multi-stage residual quantization (RQ) (Lee et al. 2022) to model the raw waveform with different frequencies. However, the multi-stage RQ will significantly increase the token length, leading to difficulty in the subsequent next-token prediction. Another paradigm (Baevski et al. 2020) focuses on the semantics of the waveform and leverages pre-trained models (e.g., Hubert (Hsu et al. 2021)) to cluster the embeddings in the semantic space and then quantize the embeddings based on cluster centers. Though semantic embeddings can successfully reconstruct the waveform, the reconstruction quality and generalization capability are bottlenecked by the pre-trained encoder.\nIn addition, compared to text and images, audio waveform typically has a much longer sequence length due to the high sampling rates, such that about 960000 sequence length in 1 min audio clip with 16kHz. Since AR models predict tokens in a sequential manner, the inference cost is quadratically correlated to the sequence length, making the AR-based audio generation slow and computationally expensive, as illustrated in Fig. 1 (a)."}, {"title": "Related Works", "content": "Raw audio discretization. Before the development of Variational Autoencoders (VAEs) (Van Den Oord, Vinyals et al. 2017; Razavi, Van den Oord, and Vinyals 2019), converting continuous domains into discrete representations was a significant challenge in generative modeling. VAEs facilitate the effective quantization of inputs into structured priors using powerful encoder-decoder networks, allowing manipulation in tasks like generation and understanding (Achiam et al. 2023; Touvron et al. 2023; Caillon and Esling 2021). Recent innovations, such as VQGAN (Esser, Rombach, and Ommer 2021) and RQGAN (Lee et al. 2022), have further advanced these priors, improving model generalization and inspiring numerous works in audio discretization (Oord et al. 2016; Caillon and Esling 2021). In the audio domain, Encodec (D\u00e9fossez et al. 2022) employs an architecture similar to Sound-Stream (Zeghidour et al. 2021), using an encoder-decoder model to reconstruct audio, incorporating residual quantization and a spectrogram-style discriminator to enhance audio quality. In contrast, HIFI-codec (Yang et al. 2023) uses group residual quantization to refine the representation in the initial quantization layer. Kumar et al. (Kumar et al. 2024) have made significant contributions to audio reconstruction by introducing multi-spectrogram loss and quantizer dropout, which effectively reconstruct high-frequency details, prevent code collapse, and enhance bitrate efficiency. Building upon these advances, our work poses an important question: can we use fewer tokens to represent low-frequency information, thereby efficiently reducing the computational burden while maintaining high-quality audio reconstruction? To address this, we propose a Scale-level Audio Tokenizer, which encodes audio on different scales, capturing hierarchical features that improve both the efficiency and quality of audio generation and reconstruction.\nDiffusion-based audio generation Diffusion models (Yoon et al. 2023; Song, Meng, and Ermon 2020; Lu et al. 2022), initially introduced by Sohl-Dickstein et al., have made significant strides in recent years due to their ability to generate high-quality audio by progressively transforming noise into coherent signals. These models have been widely adopted in various audio applications, including speech synthesis, music generation (Hawthorne et al. 2022), and general audio synthesis (Kong et al. 2020), because of their robustness and flexibility in handling the complexities of audio data. This has inspired researchers to extend diffusion models to different modalities, consistently demonstrating their adaptability and strength in creating realistic and detailed synthetic outputs. However, the diffusion process inherently presents several challenges for audio generation: (1) high computational costs and significant inference times due to the iterative nature of the diffusion process, and (2) difficulty integrating with mainstream intelligent systems due to differences in representation. These challenges motivate us to explore audio generation using large language models (LLMs) (Vaswani et al. 2017; Devlin et al. 2018; Touvron et al. 2023; Achiam et al. 2023), which offer an alternative approach with potentially more efficient computational demands and compatibility with existing intelligent systems.\nAutoregressive modeling The autoregressive model (Chowdhery et al. 2023; Hoffmann et al. 2022), as a different approach from diffusion models, leverages efficient Large Language Models (LLMs) (Vaswani et al. 2017; Devlin et al. 2018; Touvron et al. 2023; Achiam et al. 2023) to generate the next tokens sequentially to construct the output. Due to its sequential nature, autoregressive models have excelled in text generation, machine translation, and other sequence prediction tasks. Recently, autoregressive models have also made significant processes in the image generation domain (Chang et al. 2022; Sun et al. 2024). By treating image pixels or patches as sequences, these models can generate high-quality images by sequentially predicting each part of the image. However, the application of autoregressive models to raw audio generation remains challenging. The primary limitation is the sheer number of tokens required to represent raw audio data. Audio signals have a high temporal resolution, meaning that accurately capturing the nuances of sound requires a large number of tokens. This results in increased computational complexity and longer generation times, making it difficult to achieve real-time audio generation with current autoregressive models. To mitigate these limitations, our paper proposes a novel direction on employing Scale-level Audio Tokenizer to encode raw audio at different scales and generate it using Acoustic AutoRegressive modeling via next-scale prediction, thereby enhancing audio generation efficiency and quality."}, {"title": "Method", "content": "Our approach consists of two major stages: (1) In the first stage, we train a Scale-level Audio Tokenizer (SAT) to convert continuous audio signals into discrete tokens using multi-scale residual quantization. (2) The second stage reformulates the Acoustic AutoRegressive modeling (AAR) in a next-scale manner and models the tokens obtained from the frozen SAT tokenizer with a transformer structure."}, {"title": "Baseline Audio Tokenizer", "content": "Audio quantization. Consider an audio signal $a \\in \\mathbb{R}^{C \\times T}$, where $C$ represents the number of audio channels and $T$ is the number of samples over the duration of the signal. Traditional approach (Kumar et al. 2024; D\u00e9fossez et al. 2022; Yang et al. 2023) in audio tokenizer often involves a 1D convolutional-based autoencoder frameworks to compress audio waveform to latent space $x \\in \\mathbb{R}^{l \\times d}$ where $l$ is the token length and then utilizes a vector quantization to quantize the latent tokens:\n$x = E(a), \\hat{x} = Q(x), \\hat{a} = D(\\hat{x})$\nwhere $E(\\cdot)$ donates encoder, $Q(\\cdot)$ a vector quantizatier, and $D(\\cdot)$ a decoder. A vector quantizer $Q$ maps each feature vector in the latent space $x$ to the closest vector in a learnable codebook $Z \\in \\mathbb{R}^{d \\times V}$ with $V$ vectors of dimension $d$. Specifically, vector quantization $\\hat{x} = Q(x)$ involves looking up the closest match for each feature vector in $x$ with vectors in $Z$ by minimizing Euclidean distance, such that\n$\\hat{x} = \\underset{z \\in Z}{\\text{argmin}}||x - z||_2$\nwhere $\\hat{x}$ represents the quantized output and $x$ is the input to the quantizer.\nHowever, due to the complexity of the audio waveform, particularly in handling frequency-specific information, a residual quantization approach is typically employed. In residual quantization, a sequence of vector quantizers $Q = \\{Q_1, Q_2, \\cdots, Q_K\\}$ is used, where each quantizer $Q_i$ iteratively quantizes the residual error from the previous step. Specifically, after each quantization step, the residual error is computed as $\\delta_i = x_i - \\hat{x}_{i-1}$ and passed to the next quantizer as the input $x_i = \\delta_{i-1}$. The final quantized representation $f$ is obtained by summing the outputs from all quantizers\n$f = \\sum_{i=1}^{K} f_i$,\nwhich is then decoded by the decoder $D(x)$ to produce the reconstructed output $\\hat{a}$.\nLoss function. To train audio quantized autoencoder, we leverage a combination of loss functions including the reconstructed time-domain loss $L_t$, reconstructed frequency domain loss $L_f$, discriminative loss $L_G$, residual quantization loss $L_{uq}$, and commitment loss $L_{commit}$ (D\u00e9fossez et al. 2022):\n$L = \\lambda_tL_t + \\lambda_fL_f + \\lambda_GL_G + L_{vq} + \\lambda_{com}L_{com}$.\nSpecifically, reconstructed time-domain loss measures the absolute difference between $a$ and $\\hat{a}$ as\n$L_t = ||a - \\hat{a}||$,\nand frequency domain loss assesses the difference over mel-spectrograms across $n$ time scales as\n$L_f = \\sum_{i=1}^{n} ||S_i(a) - S_i(\\hat{a})|| + ||S_i(a) - S_i(\\hat{a})||$,\nwhere $S_i$ represents the transformation to the mel-spectrogram at scale $i$. The discriminative loss is derived from a multi-scale STFT discriminator, as introduced in (Zeghidour et al. 2021) to ensure the model captures high-fidelity audio features across various time-frequency scales. The vector quantization loss encourages the encoded features to match the codebook vectors, and the commitment loss penalizes deviations from these vectors, ensuring that the encoder commits to the quantized space as\n$L_{vq} = \\sum_{i=1}^{r} ||S_g(x_i) - z_i||^2, L_{com} = \\sum_{i=1}^{r} ||x_i - sg(z_i)||^2$."}, {"title": "Analysis", "content": "The baseline audio tokenizer can successfully discretize audio tokens. However, due to the residual quantization, the token length representing each audio will be significant which severely hinders the efficiency in the autoregressive modeling. Considering each quantizer in residual quantization basically divides and represents the audio into different frequency bands (D\u00e9fossez et al. 2022; Kumar et al. 2024; Yang et al. 2023), we aim to further adjust the token length based on its represented frequencies, i.e., lower-frequency parts can be represented with fewer tokens. To this end, we introduce the Scale-level Audio Tokenizer to reduce the number of tokens being used."}, {"title": "Scale-level Audio Tokenizer", "content": "In Scale-level Audio Tokenizer (SAT), we employ the same encoder-decoder architecture as baseline tokenizer (D\u00e9fossez et al. 2022) but incorporate multi-scale residual quantization (MSRQ) to enhance efficiency and flexibility in audio representation. In MSRQ, as shown in Fig. 2 (a), the quantizer $Q_i$ is defined the same as the baseline setting as $\\hat{r_i} = Q_i(r_{i-1})$ while the feature map $r_{i-1}$ is first downsampled from its original dimension $l_k \\times d$ to a lower resolution $l_k \\times d$ where $K$ is the scale number of the last index and $k$ is the scale number of the correct index. After downsampling, the look-up procedure is performed to match each feature vector with the closest codebook vector $Z_i$. After the look-up, the processed quantized vector $\\hat{z_i}$ is upsampled back to the original dimension $l_k \\times d$ to ensure consistency across scales. Due to the loss of high-frequency information from downsampling, we employ a 1D convolutional layer after upsampling to restore the lost details and enhance the fidelity of the reconstructed audio. Specifically, this convolutional layer processes the upsampled feature vectors according to the equation\n$\\varphi(r) = \\gamma \\times conv(r) + (1 - \\gamma) \\times r$\nwhere $conv(.)$ applies a 1D convolution with a kernel size of 9. This design effectively combines the original features with the transformed outputs, while preserving the reparameterization inherent to vector quantization, controlled by the quantization residual ratio $\\gamma$. In the Appendix, we provide a pseudo-code for the scale-level audio tokenizer."}, {"title": "Acoustic AutoRegressive Modeling", "content": "Vanilla autoregressive modeling. Autoregressive modeling is first introduced by (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014) and quickly spread to different modalities such as image (Sun et al. 2024), video (Weissenborn, T\u00e4ckstr\u00f6m, and Uszkoreit 2019) and 3D modeling (Siddiqui et al. 2024). In autoregressive modeling, a sequence of data points is modeled as a product of conditional probabilities. For a sequence $x = (x_1, x_2,...,x_T)$, its joint distribution can be expressed and modeled as\n$p(x_1, x_2, ..., x_T) = \\prod_{t=1}^{T} P(X_t | x_1, x_2, ..., x_{t-1})$.\nThis approach is widely used across various domains due to its flexibility and ability to capture dependencies within data. For any continuous modality, it is traditional to first train a tokenizer to discretize the input into tokens, which can then be modeled using a discrete categorical distribution. This step involves mapping the continuous data to a sequence of discrete tokens $x = (x_1,x_2,...,x_T)$ that are fed into an autoregressive model to predict the next token in the sequence, based on the preceding tokens. In the context of transformers, which have become the dominant architecture for autoregressive modeling, the attention mechanism plays a crucial role in training. The attention mechanism allows the model to focus on different parts of the input sequence when making predictions. To ensure that the model adheres to the autoregressive property, where each token $x_i$ is predicted based only on previous tokens $X_1, X_2, ..., X_{i-1}$, an attention mask is applied. Mathematical, the attention mask $M$ is defined as\n$M_{ij} =\\begin{cases}1, & \\text{if } i \\leq j\\\\0, & \\text{otherwise}\\end{cases}$\nwhere guarantee the modeling's performance on predicting $x_i$ is only relevant to its preceding tokens.\nAfter the completion of training of such a model $P$ using cross-entropy loss, it can efficiently handle complex dependencies and generate new samples by sequentially predicting each token conditioned on its predecessors (Achiam et al. 2023; Touvron et al. 2023; Sun et al. 2024).\nThis capability makes autoregressive models well-suited for generating data that requires a coherent and consistent sequence. However, their capacity for audio generation is still under-explored due to the huge sequence length required for audio data. The sheer number of tokens needed to represent even short audio clips can lead to computational inefficiencies and challenges in maintaining temporal coherence. To efficiently solve such a challenge, we combine the unique property of our SAT to efficiently generate audio via scale-level Acoustic AutoRegressive modeling.\nAcoustic autoregressive modeling. To shorten the inference step, we propose Acoustic autoregressive modeling (AAR). This approach, distinct from traditional vanilla autoregressive models that predict token sequences one by one, involves predicting across different scales. Attributed by SAT, our method represents an audio sample as a series of scale-level representations:\n$R = (r_1, r_2, ..., r_K)$\nBy efficiently expressing it as joint modeling, the audio sequence is defined as:\n$p(R) = \\prod_{i=1}^{K} p(r_i | r_1, r_2, ..., r_{i-1})$\nIn this formulation, each $r_i$ represents a distinct scale in the hierarchical representation of the audio signal. The model predicts each scale by conditioning on all previously predicted scales, effectively capturing both global structures and fine-grained details of the audio. This hierarchical approach reduces the complexity associated with long sequence lengths by leveraging multi-scale dependencies, thereby enhancing the model's efficiency and ability to maintain temporal coherence. To successfully implement our method, we modify"}, {"title": "Experiment", "content": "Evaluation Metrics and Settings\nWe evaluate FAD (Kilgour et al. 2018), MEL distance (Kumar et al. 2024), and STFT distance (Kumar et al. 2024) as reference for reconstruction, and FAD (Kilgour et al. 2018), ISc and KL (Salimans et al. 2016) for generation. FAD, built upon VGGish (Chen et al. 2020), is the metric to indicate the similarity of the generated and target samples effectively. MEL distance quantifies the difference in mel-spectrogram features, and STFT distance measures the short-time Fourier transform discrepancies between the generated and target audio signals, which focus more on high-frequency information for audio. Additionally, ISc, simulating its performance on image generation, is used to evaluate the generated sample diversity and quality. KL divergence is utilized to measure the difference between the probability distributions of the generated and target samples.\nWe conducted all experiments on the AudioSet (Gemmeke et al. 2017) dataset. To effectively evaluate the performance of our audio tokenizer, we divided the original 10-second evaluation set into n segments, each matching the window size of our model for reconstruction. After reconstructing these segments, we reassembled them into a complete audio stream. For autoregressive generation, we randomly selected one segment from the evaluation set and used it as the ground truth.\nImplementation Details\nTokenizer. In stage 1, we utilize multi-scale residual quantization (MSRQ) of codebook size 1024 with the soundstream autoencoder framework (Zeghidour et al. 2021). The model is trained for 100 epochs using the Adam optimizer with \u03b2\u2081 = 0.5 and \u03b22 = 0.9. We apply a cosine learning rate scheduler with initial learning rate 3e-4 and setting the loss weights to \u03bbt = 0.1, \u03bbdf = 3, \u03bbG = 3, and \u03bbcom = 1. Our discriminator updated 2/3 times during training.\nTransformer. In stage 2, our primary focus is on scale-level acoustic autoregressive modeling. To achieve this, we employ a GPT-2-style transformer (Radford et al. 2019) with adaptive normalization (Zhang et al. 2018) and depth of 16. We utilize CLAP audio embeddings (Wu et al. 2023) as our start tokens. Since one-second audio segments often contain limited meaningful information, we opt to use 10-second audio embeddings to capture richer context, even when generating one-second clips. For training, we adopt the AdamW optimizer with a learning rate of 1e-4, using a linear learning rate scheduler. Additionally, we apply a weight decay of 0.05 and implement warmup settings with an initial warmup proportion of 0.005 and an end warmup proportion of 0.01.\nMain Results Analysis\nFor tokenization, as illustrated in Tab 1, our proposed SAT tokenizer suppresses the baseline Encodec (D\u00e9fossez et al. 2022) by 0.3 FAD, despite using fewer tokens (750 tokens v.s. 455 tokens). This demonstrates that by increasing quantization while reducing the number of tokens, we can efficiently improve reconstruction quality while using fewer tokens.\nFor the audio generation, we introduce an autoregressive model with next-token prediction as the baseline. To ensure a fair comparison, we employ two encoders (Encodec (D\u00e9fossez et al. 2022) for AR and our SAT for AAR) with similar performance. We find that our proposed AAR shows superior performance in terms of both latency and audio quality. As shown in Fig. 3, the next-scale prediction demonstrates a remarkable improvement in audio generation, achieving a 35x speed improvement (0.225s v.s. 7.866s) and generation enhancement (FAD 5.55 v.s. 6.88). More analysis of training costs is available in the Appendix."}, {"title": "Ablation Experiments", "content": "We conduct ablation experiments to validate the effectiveness of the components in SAT and AAR.\nEffect of the scale setting. To find the optimal combination of SAT configuration, we start with Encodec in 128 latent dimensions with 10 quantizers (D\u00e9fossez et al. 2022) and test multiple scales with shared codebooks of different sizes and individual codebooks for each scale. In particular, Tab 2 shows that enlarging the scale to 16 consistently improved audio quality. As illustrated in Tab 3, we tested the performance of linear, quadratic and logarithmic scheduling on 16 scales: linear scheduling provides a balanced number of tokens for each scale; quadratic scheduling focuses more on the early or late stages of the process; and logarithmic scheduling offers a more gradual progression. We believe the suboptimal performance observed in logarithmic scheduling is due to its lack of high-frequency information representation at larger scales even though it also builds a complete information flow for audio. Quadratic scheduling, in particular, proved to be more efficient, requiring fewer tokens than linear scheduling (455 v.s. 601) and also achieves comparable reconstruction performance in audio quality.\nTo further improve the model's capacity, we fixed the decoder dimension to 1024 and tested latent dimensions of 8, 16, 32, and 64. As Tab 4 indicated, our SAT achieves its superior performance in the latent dimension of 64.\nEffect of the discriminator. We explored multiple discriminator configurations to optimize the performance of our Scale-level Audio Tokenizer (SAT). As illustrated in Tab 5, we tested two different discriminator setups: one using only"}, {"title": "Conclusion", "content": "In this paper, we introduced a novel approach for audio generation using a multi-scale autoregressive model via next-scale prediction. The proposed framework leverages the scale-level audio tokenizer, which efficiently compresses audio sequences into tokenizers of varying scales, thereby improving efficiency while maintaining high fidelity. Through comprehensive experiments, we demonstrated the superior performance of our method in generating high-quality audio compared to traditional autoregressive methods.\nOur approach provides an efficient solution for audio generation. By incorporating a multi-scale residual quantization technique, the model effectively reduces the sequence length required for generation, leading to enhanced efficiency and reduced computational demands."}]}