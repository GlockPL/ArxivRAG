{"title": "One Node Per User: Node-Level Federated Learning for Graph Neural Networks", "authors": ["Zhidong Gao", "Yuanxiong Guo", "Yanmin Gong"], "abstract": "Graph Neural Networks (GNNs) training often necessitates gathering raw user data on a central server, which raises significant privacy concerns. Federated learning emerges as a solution, enabling collaborative model training without users directly sharing their raw data. However, integrating federated learning with GNNs presents unique challenges, especially when a client represents a graph node and holds merely a single feature vector. In this paper, we propose a novel framework for node-level federated graph learning. Specifically, we decouple the message-passing and feature vector transformation processes of the first GNN layer, allowing them to be executed separately on the user devices and the cloud server. Moreover, we introduce a graph Laplacian term based on the feature vector's latent representation to regulate the user-side model updates. The experiment results on multiple datasets show that our approach achieves better performance compared with baselines.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have attracted significant attention both within academic circles and across diverse industries. Their remarkable achievements span a multitude of domains, including fraud detection in social networks [32], cancer classification in biology science [20], and materials design in molecular chemistry [4]. In real-world applications, graph data tied to individuals or human behaviors often contains sensitive details. For example, the user's comments, friend list, and profiles on a social platform, as well as their purchase records, browsing history, and transactions on an economic network, are typically deemed private. With increasing emphasis on user privacy, legal restrictions like the General Data Protection Regulation (GDPR) in Europe (EU) and the Health Insurance Portability and Accountability Act (HIPAA) in the US have rendered data-sharing practices infeasible. However, the conventional graph machine learning paradigm requires uploading raw user data to a central server. This is infeasible due to privacy restrictions, hindering the deployment of many real-world graph-based applications.\nFederated learning (FL) [16] offers a collaborative learning method, allowing multiple clients to train a model without revealing their raw training data. The workflow of FL follows an iterative training procedure, including multiple communication rounds between the clients and a central server. Specifically, the central server maintains a global model and orchestrates the training process. In each round, the selected clients fetch the global model and perform several epochs of updating using their local training data. The updated local model is later uploaded to the server to produce the latest global model. The training process terminated until the model meets the pre-defined criteria."}, {"title": "Preliminary", "content": ""}, {"title": "Graph Neural Networks", "content": "Let G = (V, E, X) be an attributed graph, where V is set of nodes, and E is set of edges. The graph topology can be represented as an adjacency matrix A \u2208 R^{n\u00d7n}, Ai,j = 1 if there is an edge (i, j) \u2208 E between node i and node j, and Ai,j = 0 otherwise. Here n = |V| denotes the number of nodes. X \u2208 R^{n\u00d7d} is the feature matrix, where each row corresponds to a node's feature vector xi \u2208 R^d. Denote Y \u2208 R^{n\u00d7c} as a matrix of the one-hot labels, where each row corresponds to a one-hot vector of a labeled node and c is the number of classes. Note only a subset of the nodes Vo \u2286 V are labeled.\nGiven a graph dataset G, the K-layer GNN consists of K consecutive layers, where each layer k receives as input the node embeddings {h_i^{k-1},\u2200i \u2208 V} from layer k \u2212 1 and outputs a new node embedding h_i^k for each node i by aggregating the current embeddings of its adjacent neighbors followed by a learnable transformation as follows:\n$h_i^k:= Agg^k (\\{h_j^{k-1},\u2200j\u2208 N_i\\}),$\n$h_i^k:= Update^k (h_i^k),$\nwhere Ni is the set of neighbors of node i, including itself, in the graph G. Here Aggk(\u00b7) is the aggregator function (e.g., mean, sum, and max) for k-th layer, and Updater (\u00b7) is the k-th layer trainable non-linear function (e.g., neural network) for k-th layer. The initial embedding of each node i is its feature vector, i.e., h_i^0 = xi, and the node embeddings from the last layer's output {h_i^K, \u2200i \u2208 V} will be used for downstream tasks such as predicting the labels of nodes from the unlabeled set V \\ Vo."}, {"title": "Problem Definition", "content": "In this paper, we explore learning a GNN model under the node-level federated graph learning setting, where each node in the graph represents a user, and the node feature xi is private to user i. Furthermore, we assume a central server has access to the graph topology A as well as the label of the node in Vo, but can not observe the feature matrix X. The central server aims to cooperate with the users to train a GNN model over the graph G without requiring the private data xi to leave the users. We focus on the node classification task, where the server wants to assign labels to the remaining unlabeled nodes (users) V \\ Vo in the graph."}, {"title": "FL Objective", "content": "Let fs (\u03b8s, A,) be the server-side model, parameterized by \u03b8s, and fu (\u03b8ui, xi) be the user-side model, parameterized by \u03b8ui. The user-side model's output is Zi = fu(\u03b8ui, xi). Then, the GNN model's final output is:\n$Z = f_s(\\theta^s, A, [Z_1,\\ldots, Z_n]^T)$\n$= f_s(\\theta^s, A, [f_u(\\theta_i^u, x_1),\\ldots,\\ldots, f_u(\\theta_i^u, x_n)]^T)$.\nDefine \u03b8 = {\u03b8s}\u222a{\u03b8ui}n i=1 be the set of all trainable parameters on both user-side and server-side. Then, the goal of GNN training under node-level FL is to minimize the following classification loss:\n$min \\mathcal{L}_{CE} (\\theta) := \\sum_{i\u2208V_O} CE(Z_i(X, \\theta), Y_i)$.\nHere, CE denotes the cross-entropy loss, and Vo is the set of labeled nodes."}, {"title": "The Proposed Method", "content": ""}, {"title": "Algorithm to address (3).", "content": "We summarize the details for addressing (3) in Algorithm 1. Specifically, at the beginning of the t-th communication round, every user (node) in the graph computes the latent output of the user-side model and uploads x_i^t to the server (lines 5-6). Subsequently, the server computes the classification loss based on equations (2) and (3). Following this, the server-side model parameter \u03b8s can be updated using standard back-propagation and gradient descent (lines 8). The server then computes the gradient for the user-side model's output \\$\\frac{\\partial \\mathcal{L}}{\\partial z_i^t}\\$ and sends it to the corresponding user (line 9). The user receives \\$\\frac{\\partial \\mathcal{L}}{\\partial z_i^t}\\$ and computes the gradient of the user-side model through back-propagation (line 11). Finally, the user-side model parameter \u03b8ui is updated via gradient descent (line 12)."}, {"title": "Model Splitting Strategy", "content": "Implementing a layer-wise split of the GNN model is unfeasible in a node-level FL setting as the resultant user-side model still requires the entire graph as input. To address this problem, we start from the message-passing mechanism of the GNN layer and suggest splitting the Agg(\u00b7) and Update(\u00b7) functions of the first GNN layer. A detailed discussion of the general model splitting process can be found in Appendix B. In this section, we elucidate this concept through a simplified example of a two-layer GCN model.\nLet A = D^{\u2212\\frac{1}{2}}AD^{\u2212\\frac{1}{2}} be the normalized adjacency matrix, where A = A + I is the adjacency matrix with self-connections, Dii = \u2211j Aij is the degree matrix, and I \u2208 R^{n\u00d7n} is the identity matrix. Then, the forward pass of the standard 2-layer GCN model [14] can be expressed as follows\n$Z = softmax (\\tilde{A} ReLU(\\tilde{A}XW^{(0)})W^{(1)})$, following the proposed model splitting strategy, we split the first GCN layer into two parts: every user retains a local W(1), while A is located on the server side. The training process of the split GCN model unfolds as follows. In each round, the user-side model first maps its feature vector xi into latent representation x = xW(0), where W(0) is the weight matrix of the user i. Subsequently, xi is uploaded to the server. The server-side model computes the loss based on the topology and the received latent vectors. Let X = [x1,...,xn]T. The forward pass of the server-side model is expressed as:\n$Z = softmax (\\tilde{A}ReLU(\\tilde{A}\\tilde{X})W^{(1)})$"}, {"title": "nFedGNN with regularization", "content": "To improve the performance of nFedGNN, we revisited the foundational assumption of graph-based semi-supervised learning: the connected nodes in the graph are likely to share the same label [40, 33]. If two nodes are connected in the graph, a properly trained GNN model is likely to assign the same label to them. This suggests that the latent representations of two connected nodes should become increasingly similar as the model's layers become deeper, even if they have distinct feature vectors.\nDrawing from this observation, in the context of the split GNN model, if two users are interconnected, their respective latent representations should ideally manifest analogous patterns. However, as discussed previously, the user-side model is only responsible for one feature vector locally. Consequently, the node's latent representation shares less similarity with neighbor nodes. If we restrict the distribution of latent representations and let the connected users have a similar latent representation, the performance of the learned model should be improved. Motivated by this, we introduce the graph Laplacian regularization based on the received latent representations:\n$\\mathcal{L}_{reg} = \\frac{1}{N}\\sum_{i=1}^N\\frac{1}{|N_i|}\\sum_{j\u2208N_i}|| f_u(\\theta_i^u, x_i) - f_u(\\theta_i^u, x_j) ||^2$.\nThe overall training loss can be formulated as\n$\\mathcal{L} = \\mathcal{L}_{CE} + \\lambda \\mathcal{L}_{reg}$.\nHere, \u03bb \u2265 0 is the hyperparameter that balances the weight of classification loss and regularization."}, {"title": "Discussion", "content": "Graph Regularization: Graph regularization-based approach has a long history and is widely used for various applications. It's noteworthy that the state-of-the-art GNN models [14] relax the assumption behind the regularization-based approaches, where the connected nodes in the graph are likely to share the same label. Instead, they directly encode the topology with the feature matrix by a neural network, as the graph topology does not necessarily encode similarity but contains the information that does not appear in the feature matrix X. Nevertheless, this assumption still holds for a large portion of nodes within the graph, a fact evidenced by the success of regularization-based strategies. In nFedGNN, we use the graph Laplacian regularization to restrict the distribution of latent representation from the user-side model. The remaining layers of the GNN model (located on the server side) still encode the graph topology with the (latent)-feature matrix as the state-of-the-art GNN model. Consequently, the model still utilizes the knowledge from graph topology as state-of-the-art GNNs.\nCommunication Cost: In cross-device federated learning environments, the communication bottleneck is one of the major concerns in prior graph-level and subgraph-level FL researches [38, 13]. However, within nFedGNN, the data exchanged between the user and server each round consists solely of a latent representation vector and a corresponding gradient vector, as opposed to the entire model parameter. Typically, the length of the latent representation vector is in the range of tens to hundreds depending on the tasks and models, substantially smaller compared to the total count of neural network parameters. Thus, within the framework of nFedGNN, communication cost does not pose as a constraining factor."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "We evaluate nFedGNN on six datasets: Cora, Citesser, Pubmed [24], Chameleon, Squirrel [21], and Wiki-CS [18]. The details and statistics of these datasets can be found in Appendix C. Currently, we assume all users participate in the learning process at every communication round. The algorithm was implemented using DGL [31] with the Pytorch backend, and the training/testing data partition is the same as prior works [14, 28, 19, 18]. All experiments were conducted on a GPU server with 4 NVIDIA RTX A6000 (48GB GPU memory).\nWe employ two well-established GNN models \u2013 GCN[14] and GAT[28] \u2013 for the node classification task across all datasets. The model splitting strategy for GCN follows the strategy discussed in Section 2. For GAT, a two-layer model is utilized, and the details of the model splitting strategy are discussed in Appendix D. To ensure a fair comparison, we let the model structure be the same as the model in centralized training counterparts before splitting. Specifically, the GCN model parameters are as follows: 16 hidden neurons, 0.5 dropout rate, and a learning rate of 0.1. For GAT, the dropout rate is 0.6, the number of attention heads is 8, the number of hidden neurons is 16, and the learning rate is 0.01. For both models, we employ the Adam optimizer with weight decay as 5e-4 to update the model parameter both on the server side and the user side. The total FL round number is set to be 200 for all experiments. We run each experiment with three random seeds and report the averaged training loss and testing accuracy.\nBaseline: Given limited exploration in prior studies regarding node-level federated graph learning, establishing a baseline for comparison is challenging. A study close to ours is CNFGNN [17], which advocates for the transmission of both model parameters and latent representations between the user and the server. However, their focus is traffic prediction using time-series user data, employing an encoder-decoder architecture in their user-side model to handle sequence data, rendering their model and algorithm incompatible with our setting. Nonetheless, we adapt their conceptual framework by allowing users to upload both the model parameter and the latent vector, with the server subsequently averaging the received local models and broadcasting the updated global model to all users. It's important to note that their approach simultaneously uploads the local model and the latent representation, significantly increasing the risk of privacy leakage. Prior research [5] has shown that adversaries could infer sensitive information from model parameters."}, {"title": "Experiment Results", "content": "We first compare our method with the baseline. In Fig. 4, we illustrate the training loss and testing accuracy of the split GCN model on the Cora, Citeseer, and Pubmed datasets. Note additional experiment results on Chameleon, Squirrel, and Wiki-CS datasets can be found in Fig. 6 in Appendix E. From the figure, we can observe that nFedGNN significantly outperforms the CNFGNN on all datasets. To show the effectiveness of regularization loss, we search the best value of \u03bb within the range {0.1, 1, 10, 100, 300} for Cora, Citeseer, and Pubmed, and the range {1, 10, 100, 500} for Chameleon, Squirrel, and Wiki-CS. We select the best \u03bb for each dataset, and the optimal hyperparameter can be found in each image. In the figure, the performance improvement of nFedGNN with the regularization can be clearly observed. The best accuracy reaches 71.9% for the GCN model on the Cora dataset. Here, we also plot the testing accuracy of the GCN model on these datasets in centralized training [14] as references. Although there is still a gap between nFedGNN and the centralized GNN model in testing accuracy, it is significantly reduced compared with the baseline and the no regularization counterpart. Note our goal is not to achieve the state-of-the-art but to demonstrate the effectiveness of the proposed approach. It is worth noting that CNFGNN needs to transmit the extra local model parameters between the users and server at every FL round, necessitating more communication resources. In Appendix E, we demonstrate the comparison results for the GAT in Fig. 7, 8, similar trends and conclusions can be observed in these figures. In summary, nFedGNN has better performance, clearly proving the advantages of nFedGNN compared with baseline.\nSubsequently, we investigate the influence of \u03bb on the model convergence. As shown in Figure 5, we display the testing accuracy and training loss under different magnitudes of \u03bb. A general trend emerging from these figures is the testing accuracy increases when \u03bb is getting higher. However, when \u03bb exceeds some threshold value, the testing accuracy starts to decrease. For the training loss, a higher \u03bb, the value is larger. Another observation is the convergence speed of testing accuracy is slower with higher \u03bb, e.g., it takes more rounds for the testing accuracy to achieve the plateau. We conclude that with a higher value of \u03bb, the training task is more \u201cdifficult\u201d, and training requires more time to converge. By properly determining the value of \u03bb, the learned models' performance can be improved. In Appendix E, we also illustrate the impact of \u03bb on model convergence for the GAT in Fig. 9. Similar observations can be found in these figures. Note that due to the stochastic nature of the learning process, a few curves may not match the above observations. However, the general trend is clear, and the conclusions are reliable."}, {"title": "Communication Cost Comparison", "content": "We count the total communication cost (in MB) of both nFedGNN and CNFGNN for the GCN model and the GAT models in Table 1. This includes the cumulative communication cost spanning 200 training rounds for all users within the graph, with the data transmitted in float32 format. For nFedGNN, we record the size of the latent feature vector. For CNFGNN, we record the size of the latent feature vector and the number of user-side model parameters. In all cases, nFedGNN consumes less communication resources than CNFGNN. Moreover, the total transmitted data size is very small, verifying the conclusion that communication cost is not the bottleneck for our method."}, {"title": "Conclusion", "content": "We introduce nFedGNN, a novel federated learning algorithm for GNNs where each user only has access to the data of a single node. Our method provides an opportunity to utilize the distributed graph data without compromising user privacy. We compare nFedGNN with nFedGNN over multiple datasets and demonstrate the advantage of our method. Despite our extensive efforts, we recognize the limitations in the scope of our study. For example, we have not explored the training on larger datasets and inductive frameworks like GraphSage [9]. In the future, we will extend nFedGNN to adopt partial user participation and to provide rigorous privacy protection (e.g., differential privacy)."}, {"title": "Related Work", "content": "Federated Learning. FL has drawn great attention recently due to its benefits in privacy and communication efficiency [27]. The goal of FL is to allow multiple users to collaboratively train a model under the orchestration of a central server without sharing their raw data [12]. In conventional FL algorithms such as FedAvg [16], model training takes place on the user side, and only the model or model update is transferred between the user and a server. An aggregator on the server subsequently aggregates the models from the users for the next training round. In FL, the user's raw data is never shared.\nOne branch of FL is vertical federated learning (VFL) [36, 35]. In conventional FL [16], the user has different data samples, while the samples from different users share the same feature space. In VFL, the user owns a common sample space, but disparate feature spaces. The first few layers of the model are partitioned width-wise to accommodate different feature spaces. Each user owns a portion of the whole model corresponding to their feature spaces. The user who possesses the labels holds the remaining layers of the model and coordinates the training process.\nAnother relevant notion of FL is split learning [8, 29]. Specifically, the model is partitioned layer-wise between the users and the server. Only the intermediate results, e.g., the activation maps and related gradients, are exchanged between the user and server during training. As opposed to FL, split learning requires users to communicate with the server every iteration, incurring heavy communication overhead and high training latency.\nGCN in Distributed Setting. There has been an increasing research interest in training GNN in a distributed setting. Most of the work focuses on either graph-level FL, where small graphs are distributed among multiple parties [10, 37], or subgraph-level FL, where each party holds a sub-graph of the whole large graph [10, 39, 1, 30, 23].\nSpecifically, Baek et al.[1] propose a personalized sub-graph FL algorithm, which allows each user to train a personalized model by selectively sharing knowledge across users. Zhang et al. [39] trains a missing neighbor generator to handle the missing edges of the subgraphs across users. Wang et al. [30] employs the model-agnostic meta-learning (MAML) approach to tackle the Non-IID distributed data for the graph-based semi-supervised node classification task. Hu et al. [11] suggested an online adjustable attention mechanism to aggregate the local models. These prior works mainly focus on the setting that each user holds a graph/sub-graph, where a local GNN model could be independently trained and serves as a base model for FedAvg.\nIn this paper, we consider a more challenging case where each user represents only one node in the graph and does not have access to other's data. It is, in fact, a special but the most challenging case of subgraph-level FL and could not be addressed by any of the prior work. The closest work to ours is [17]. They consider the spatiotemporal dynamic modeling tasks and propose an approach that explicitly encodes the underlying graph structure using GNNs. Their approach is specifically crafted to fit the problem and may be unsuitable for general GNN tasks."}, {"title": "General Model Splitting Strategy", "content": "Generally, GNN can be described within the Message Passing Neural Networks (MPNN) framework [6]. As shown in (1), the hidden state of each node hi relies on the message from their neighboring nodes. Thus, user i requires hj from its neighbor users to compute hi. This leads to excessive privacy leakage. Therefore, layer-wise splitting for GNN is infeasible under the node-level FL setting. Alternatively, we seek to split the first layer of the GNN model. However, the conventional GNN layer follows the message-passing-then-encoding protocol. Directly splitting a GNN layer results in the message aggregation function Agg(\u00b7) located on the user side. To deal with this problem, we reformulated the GNN layer as an encoding-then-message-passing process:\n$h_i^k := Update^k (h_i^k),$\n$h_i^{k+1} := Agg^k (\\{h_j^k,\u2200j\u2208 N_i\\}),$\nNote (8) usually results in the same model as message-passing-then-encoding. Then we can split the first GNN layer into two parts: the update function Updatek(\u00b7) on the user side and the message aggregation function Aggk(\u00b7) on the server side. Taking the split GCN model 5 as an example, we can find the first layer's hidden state update function Updatek(\u00b7) is a linear projection with parameter W(0), and the message aggregation function Aggk(\u00b7) is the matrix multiplication with normalized adjacency matrix A. Switching the order of the Updatek(\u00b7) and Aggk(\u00b7) implies the different matrix multiplication orders. The associativity property of matrix multiplication immediately proves the same GCN model after reformulation."}, {"title": "GAT Splitting Strategy", "content": "A GAT layer comprises a learnable weight matrix W \u2208 RF'\u00d7F and a shared attention mechanism a : RF' \u00d7 RF' \u2192 R, where F and F' are the number of input and output of the GAT layer. Note that a is used to compute the attention coefficient eij, which further depends on the feature vectors of nodes i and j. Therefore, the user-side model contains the trainable weight matrix W, while the shared attention mechanism a should be located on the server side. At each training round, the user-side model first transforms the local feature vector xi into x' = Wxi, which is uploaded to the server later. The server uses x' to compute the attention coefficients eij = a(x'i, x'j) and update the model. Moreover, the GAT model usually employs the multi-head attention mechanism to stabilize the training process. To achieve this, we let the user have multiple weight matrices. Each weight matrix transforms the input feature independently: x1 = Wlx, where Wl is the weight of l-th attention mechanism on i-th user. Subsequently, the user uploads a set of latent features {xli}Ll=1 to the server, where L is the number of attention mechanisms. Finally, the output of the first GAT layer is\n$x_i = \u03c3(\\frac{1}{L} \\sum_{l=1}^L \\sum_{j\u2208N_i} \u03b1^l_{ij}x^l_j )$,\nwhere \u03c3 is the activation function and \u03b1 = softmax(e) is the normalized attention coefficients computed by the l-th attention mechanism."}]}