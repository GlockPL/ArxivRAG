{"title": "BONGARD IN WONDERLAND: VISUAL PUZZLES THAT STILL MAKE AI GO MAD?", "authors": ["Antonia W\u00fcst", "Tim Tobiasch", "Lukas Helff", "Devendra S. Dhami", "Kristian Kersting", "Constantin A. Rothkopf"], "abstract": "Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's GPT-4o, have emerged, seemingly demonstrating advanced reasoning capabilities across text and image modalities. Yet, the depth of these advances in language-guided perception and abstract reasoning remains underexplored, and it is unclear whether these models can truly live up to their ambitious promises. To assess the progress and identify shortcomings, we enter the wonderland of Bongard problems, a set of classical visual reasoning puzzles that require human-like abilities of pattern recognition and abstract reasoning. While VLMs occasionally succeed in identifying discriminative concepts and solving some of the problems, they frequently falter, failing to understand and reason about visual concepts. Surprisingly, even elementary concepts that may seem trivial to humans, such as simple spirals, pose significant challenges. Moreover, even when asked to explicitly focus on and analyze these concepts, they continue to falter, suggesting not only a lack of understanding of these elementary visual concepts but also an inability to generalize to unseen concepts. These observations underscore the current limitations of VLMs, emphasize that a significant gap remains between human-like visual reasoning and machine cognition, and highlight the ongoing need for innovation in this area.", "sections": [{"title": "1 Introduction", "content": "Visual reasoning, the ability to understand, interpret, and reason about the visual world, is a fundamental aspect of human intelligence [27]. It allows us to navigate our environment, interact with objects, and make sense of complex visual scenes. In recent years, the field of artificial intelligence (AI) has advanced rapidly toward replicating aspects of this visual reasoning, with significant focus placed on Vision-Language Models (VLMs) [5, 24, 25]. These models integrate visual and textual information to generate descriptive content, aiming to mimic how humans comprehend and reason about the world. Because of their human-like responses, VLMs often create the illusion of possessing human-like perception and intelligence. However, as recent work shows, VLMs and the Large Language Models (LLM) on which they are based have dramatic shortcomings in the case of reasoning [30] and visual perception [12, 13, 19, 34] or their combination [39, 47, 48].\nBongard problems (BPs), a class of visual puzzles that require the identification of underlying rules based on a limited set of images, provide a unique and challenging benchmark for assessing visual reasoning abilities in AI systems [4]. Conceived by Russian scientist Mikhail Bongard in 1967, these visual puzzles test cognitive abilities in pattern recognition and abstract reasoning, posing a formidable challenge even to advanced AI systems [15]."}, {"title": "3 Bongard Problems and VLMs", "content": "Bongard problems (BP), introduced 1970 by Mikhail Bongard [4], are classical visual puzzles that test for capabilities of pattern recognition, concept formation, and abstraction. Each BP consists of twelve simple black-and-white diagrams divided into a left and a right group. Usually, all images share some similarity, but for both sides, there is an opposing property or rule, respectively, that its six images have in common (and which is shared by no image of the other side).\nAn example BP is shown in Figure 2 where the properties are vertical and horizontal orientation.\nThe task is the linguistic expression of the underlying rule that distinguishes the two groups. These rules vary in complexity, ranging from simple geometric properties like the presence of a circle to more abstract or relational concepts like symmetry or the presence of a right angle. In some cases, the rule of the right side is just the negative of the left rule, like BP#24 (a circle present vs. no circle present). Still, for the majority of BPs the second rule is a more specific opposite of the first e.g., BP#6 (triangles vs. quadrangles).\nIn contrast to mainstream classification tasks, BPs differ from mainstream classification tasks due to their complexity and reliance on abstract reasoning rather than direct pattern recognition. Specifically, BPs test the ability to express distinctive and common features of images, including the pattern recognition necessary to correctly associate the features with images, as well as the ability to come up with textual rules that can characterize the meta-pattern (not within each but) across all twelve diagrams that constitute a BP.\nThis multi-modality of BPs makes them an interesting challenge for VLMs or multimodal AI in general. In this work, we use different strategies to prompt VLMs to solve BPs which will be introduced in the following.\nOpen-Ended Solving of Bongard Problems. For each BP, the model is prompted individually by providing a text prompt together with an image showing all twelve diagrams. The prompt follows the following setup:\n1. The structure of the image is described first. This includes the number of diagrams, their arrangement (left and right sides), and the fact that the diagrams are black and white with specific shapes and features.\n2. The task for the model is defined by explaining that there are two rules, one for the left side and one for the right side and that the rules should not apply to any of the diagrams on the other side.\n3. The model is instructed to evaluate step-by-step, beginning with a detailed analysis of the diagrams, followed by inferring the underlying rules.\n4. The required response format is specified as a dictionary with two entries, one for each rule.\nThe complete prompt can be found in Listing 1. The answers to the reasoning task are then compared to the ground truth\u00b2 by an LLM-Judge, as the answer setting is open-ended (cf. Listing 3 for prompt).\nMultiple Choice Setting. In this setting, rather than having the model generate the rules itself, we provide it with a set of predefined rule pairs from the BP domain. The model is then tasked with selecting the correct rule pair from these options. The prompt follows the same structure as in the previous task but includes an additional list of available rules for the model to choose from. The expected answer is the ID corresponding to the correct rule-pair (cf. Listing 1)."}, {"title": "4 Experimental Evaluation", "content": "Our intention here is to investigate to what extent state-of-the-art VLMs can solve Bongard problems. At first, we evaluated the models quantitatively on all 100 puzzles. We then compared their performance against humans and investigated them qualitatively in more detail on the base of four selected BPs. Specifically, we address the following research questions:\n(Q1) How well can state-of-the-art VLMs solve Bongard problems?\n(Q2) How do VLMs perform in comparison to humans?\n(Q3) How accurately can VLMs detect concepts required to solve BPs?\nData. For our evaluations, we considered the 100 original Bongard problems of [4]. We used the dataset variation of [10], which contains high-resolution images of the original diagrams.\nModels. We evaluated the models GPT-4o [1], Claude 3.5 Sonnet [3], Gemini 1.5 Pro 37, and LLaVA with versions v1.6-34b [25] and v1.5-13b [24]. For simplicity, the models are referred to as GPT-4o, Claude, Gemini, LLaVA 1.6, and LLaVA 1.5 in the following.\nCan VLMs solve Bongard problems? (Q1) As a first step, we aimed to assess how well current state-of-the-art VLMs can solve BPs. To do this, we tasked our selection of VLMs with solving each BP three times. The answers were evaluated by the LLM-judge, which determined whether each response correctly solved the BP. The evaluation results are shown in the top row of Table 1. Notably, GPT-4o emerged as the best performing model, with an average of 21 solved BPs. However, this performance is still surprisingly low, especially when compared to human abilities (cf. (Q2) and [23, 31]). A more detailed breakdown of which BPs were solved is provided in Table 3. It shows that even rather simple BPs with concepts like small vs. large shapes (BP#2) and vertical vs. horizontal elongated figures (BP#7) cannot be solved in most attempts. Some example responses from the VLMs for BP#7 are illustrated in Figure 2, highlighting that the models frequently fail to identify the correct rule and are often distracted by irrelevant features like wavy patterns or rectangular shapes.\nIn a further setting, we analyzed how the results change when providing the models with all existing rule pairs of the BPs and asking them to select the correct one (cf. Table 1 Multiple Choice (100)). Interestingly, this does not change the performance for GPT-4 and LLaVA-v1.5 significantly. However, Gemini and Claude's performance is better in this setting; Claude can even solve 28 BPs on average.\nTo further simplify the task, we reduced the number of options to 10 possible rule pairs and repeated the procedure. Now, the models perform better, reaching up to 69 solved BPs (cf. Table 1 Multiple Choice (10)). This is interesting since before the correct solution was present as well, but the models could not select it correctly. Therefore, it is unclear whether the models actually caught the concepts or if merely the exclusion procedure was easier. The question remains whether, with specialized context, it is possible to solve a BP if it has not been solved before. We investigated this in more detail in the context of (Q3). First, let us compare the results of the VLMs to human reasoning abilities.\nHow do VLMs perform in comparison to humans? (Q2) To compare the performance of the VLMs to human performance we considered the results from the user study of [10]. Here, 13 participants were asked to solve the first 50 BPs within 45 minutes. We aggregated the mean of the number of solved BPs for the humans and the VLMs on the"}, {"title": "How accurately can VLMs detect concepts required to solve BPs? (Q3)", "content": "We saw that the VLMs show poor performance on the BP dataset. This could stem from difficulties in perceiving the diagrams properly but also from reasoning failures, such as incorrectly formulating rules that apply distinctly to each side. None of the models was able to solve BP#16, BP#29, BP#36 and BP#55 correctly even though the conceptual complexity of the rules is rather small. This raises the question on whether the perception of the models is flawed. We investigated this in more detail by providing the individual diagrams of the BPs to the models and asking them directly for the relevant concepts. An excerpt of the responses is displayed in Figure 3 (cf. Table 7, 8, 9, 10 for all responses). When all images from the BP are correctly categorized, we take it as an indication that the VLM is able to capture the concept in principle.\nSurprisingly, we find that for BP#16 (cf. Figure 3 top left), even though some images are classified correctly, none of the models can classify all images correctly. Instead, we can see a tendency to classify one of the directions rather than the other, e.g., GPT-4o and Claude almost always predict counter-clockwise as the turning direction for all of the diagrams.\nFor BP#55 (cf. Figure 3 top right) we also see that none of the models is able to classify the concepts of all images correctly. Here we can see very similar behaviors across the different models, e.g., all models categorize the first diagram of the BP falsely, but in most of the attempts, they are able to categorize the second diagram correctly. Interestingly, we found a strong correlation between the absolute position of the circle in the diagram (left vs. right) and the models responses. We take this as an indicator, that the models did not manage to reason spatially about the position of the circle in relation to the cavity.\nFor BP#29, the models were asked to determine whether there are more shapes inside or outside the bigger shape (cf. Figure 3 bottom left). Even though the final decisions of the models were primarily correct, we saw in the answers that except for Claude, none of the models was able to count the shapes correctly. GPT-4o, for example, was convinced that image 7 has as many shapes on the outside as on the inside.\nFor the last BP, BP#36 (cf. Figure 3 bottom right), the models can identify the concept better, with some even classifying all 12 images correctly (cf. Table 9). The ground truth concept is triangle vs. circle on top and for this the perception"}, {"title": "Limitations.", "content": "While BPs are valuable for assessing abstract reasoning, they also represent a narrow and highly specialized set of challenges that may not comprehensively reflect the broad range of problems VLMs encounter in real-world applications. Additionally, the reliance on our LLM-Judge introduces some uncertainty in the evaluation process. Future work should expand these evaluations to more diverse tasks and evaluate the judge's performance and additional model architectures to address these limitations."}, {"title": "5 Conclusion and Future Work", "content": "We presented a diagnostic evaluation of VLMs using the classical Bongard problems (BPs), providing valuable insights into their current capabilities of pattern recognition and abstract reasoning. Our experimental results highlight a significant gap between human-like visual reasoning and machine cognition. Specifically, we found that VLMs are still largely unable to solve the majority of BPs, with the best-performing model, GPT-4o, solving only 21 out of the 100 BPs. Moreover, our analysis suggests that the limitations of current VLMs extend beyond just visual reasoning; they also struggle to perceive and comprehend elementary visual concepts, such as simple spirals. A model that cannot recognize the direction in which a spiral is rotating cannot reason about whether multiple spirals are rotating in the same direction. Our findings raise several critical questions: Why do VLMs encounter difficulties with seemingly simple Bongard Problems, despite performing impressively across various established VLM benchmarks [11, 21]? How meaningful are these benchmarks in assessing true reasoning capabilities?\nAn intriguing direction for future research would be analyzing the visual and textual latent spaces of the VLMs for BPs [6]. Additionally, more recent approaches for image encoding in VLMs such as CLOC [7] could be investigated or fine-tuning of CLIP [32] could be considered. Further, it could be interesting to translate the visual concepts of BPs to real-world contexts, to evaluate to what extend the visual appearance or style of the images influences the perception. For higher transparency of the model's reasoning when solving BPs, concept-based approaches could be considered, such as first discovering relevant concepts of the BPs, e.g., via VLMs or approaches like the Neural Concept Binder [36] combined with explicit reasoning, e.g., via program synthesis [40]."}, {"title": "A Experimental Details", "content": "In the following, the prompts used during the experiments are provided. The prompt for the main experiment is shown in Listing 1. The prompt for the multiple choice setting is in Listing 2 and the prompt for the LLM-judge is in Listing 3. The prompts for the second part of the experiment are provided in Listings 4, 5, 6, 7,.\n1 You are provided with a black-and-white image consisting of 12 simple diagrams. Each diagram\nrepresents shapes with specific features, such as geometric properties or higher-level concepts.\n2\n3 The 6 diagrams on the left side belong to Set A.\n4 The 6 diagrams on the right side belong to Set B.\n5\n6 ## Task:\n7\n8 Your task is to determine two distinct rules:\n9\n10 1. Set A Rule:\nIdentify a rule that applies to all diagrams in Set A.\n11 2. Set B Rule:\nIdentify a separate rule that applies to all diagrams in Set B.\n12\n13 Important: The rule for Set A must not apply to any diagram in Set B, and the rule for Set B must\nnot apply to any diagram in Set A.\n14\n15 ## Step-by-Step Process:\n16\n17 1. Diagram Analysis: Carefully describe each diagram in detail, noting any geometric properties,\npatterns, or conceptual features.\n18 2. Rule Derivation: Based on your analysis, deduce the rule for Set A and the rule for Set B,\nensuring that each rule is unique to its set.\n19\n20 ## Final Answer Format:\n21\n22 Provide the final answer using the following format:\n23\n24```python\n25\n26 answer = {\n27 'set A rule': ' (LEFT RULE)',\n28 'set B rule':\n29 }\n30\n31\n[RIGHT RULE]'\n32 Ensure that the rules are clearly defined, concise, and do not overlap between the sets."}, {"title": "B Additional Results", "content": "In the following the detailed results of the evaluations are presented. In Table 3, Table 4 and Table 5 the results for the single BPs for each model are reported. Please note that LLaVA-v1.6-34b could not be considered for Table 4 since the context size of the model was too small to consider all 100 options.\nIn Table 6 we compare the results of the VLMs on a subset of BPs against human results from [10]. Each BP has been categorized based on the ground truth rules, the possible categories are existence, size, concept, number and spatial. Existence means that the rules correspond to the existence or absence of something in the diagrams. For size, the rule is based on the size of one or multiple shapes in the BP. Concept means, that the BP tests for a specific, more abstract concept, e.g., convex and concave. Under number BPs are grouped that require some form of counting, e.g., one vs two shapes. And finally spatial takes into account BPs that require spatial reasoning, e.g., some shape is on top of the other.\nFurther, we report the classification results of the second part of the experiments for the concepts of BP#16 (Table 7), BP#29 (Table 8, BP#36 (Table 9) and BP#55 (Table 10)."}]}