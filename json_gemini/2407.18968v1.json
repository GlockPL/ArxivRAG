{"title": "Intelligence Analysis of Language Models", "authors": ["Liane Galanti", "Ethan Baron"], "abstract": "In this project, we test the effectiveness of Large Language Models (LLMs) on the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) dataset. This dataset serves as a representative benchmark for testing abstract reasoning abilities, requiring a fundamental understanding of key concepts such as object identification, basic counting, and elementary geometric principles. Tasks from this dataset are converted into a prompt-based format for evaluation. Initially, we assess the models' potential through a Zero-shot approach. Subsequently, we investigate the application of the Chain-of-Thought (CoT) (Wei et al., 2022) technique, aiming to determine its role in improving model performance. Our results suggest that, despite the high expectations placed on contemporary LLMs, these models still struggle in non-linguistic domains, even when dealing with simpler subsets of the ARC dataset. Our study is the first to concentrate on the capabilities of open-source models in this context. The code, dataset, and prompts supporting this project's findings can be found in our GitHub repository, accessible at:\nLLMsOnARC.", "sections": [{"title": "Introduction and Related Work", "content": "Evaluating artificial intelligence (AI) system intelligence through abstract and visual reasoning challenges has been a longstanding practice in the field of deep learning. An early instance is the Analogy program by Evans (Evans, 1964), which solved geometric analogy tasks using DSL. Numerous programs and benchmarks have been proposed over the years, with the ARCathon (Chollet, 2019) being the latest and most prominent example.\n\nARC dataset and ARCathon The Abstraction and Reasoning Corpus (ARC) created by Fran\u00e7ois Chollet (Chollet, 2019) is a benchmark test that measures AI's progress towards human-level abstraction and reasoning abilities. It resembles in for-"}, {"title": "Method", "content": "A straightforward approach to addressing ARC-like tasks with LLMs is through textual encoding. This technique involves transforming the 2D input-output images into a textual format. Once converted, this text becomes part of the LLM's prompt. The LLM then processes this prompt and generates a solution that hopefully corresponds to the required output."}, {"title": "Dataset", "content": "A subset of 50 ARC tasks that are solvable by ARC solver ARGA (Xu et al., 2022). This method follows what was first done in (Xu et al., 2023). This subset was curated specifically because it is shorter and \"simpler\", as these tasks can be solved using a search-based approach.\nEach task consists of a small number of demonstration examples and a small number of test examples (generally 1). Each example consists of an input matrix and an output matrix. Each matrix is of dimension nxm (n, m are the number of rows and columns of the matrix accordingly) and each element in it is a color (e.g., see figure 4). There 10 different colors. A matrix can have any height or width between 1 \u2013 30."}, {"title": "Textual encoding", "content": "Given an ARC task, i.e., 2D matrix, we converted it into text by encoding each element color numerically from \"0\" to \"9\", representing ten colors. In our encoding, numbers are space-separated, and matrix rows are marked with \"\\n\". Alternatively, colors could be encoded with letters \"a\" to \"j\" or by their names, like \"blue.\""}, {"title": "Prompting techniques", "content": "Zero-shot: For this method, the models were given the tasks as prompts and were asked to produce outputs without prior training. This approach evaluates the innate ability of LLMs to tackle new, reasoning-based challenges. It provides valuable insights into their adaptability and generalization capabilities. Such an evaluation is important in understanding how well these models can perform in real-world scenarios where they encounter unexpected problems.\nChain-of-Thought (CoT): For this method, the models were given the tasks and a step-by-step reasoning process tailored for a specific fixed task from the dataset. These were provided as prompts, and the models were then asked to generate the corresponding output. It's important to note that the models were not tested on this fixed task.\nARC tasks typically require advanced, multi-step reasoning. The CoT (Wei et al., 2022) method is designed to guide the models through the necessary steps to solve such complex tasks. By training with CoT, models are expected to better understand the multi-step problem-solving, a critical skill for successfully handling ARC dataset."}, {"title": "Results", "content": "The results indicate that all the models we have tested, showed limited effectiveness in solving the ARC tasks. The maximum number of tasks successfully solved by any model was 2 out of a total of 50 (see table 1), suggesting a notable challenge for LLMs in this particular problem domain.\nThe comparison of Zero-shot and CoT approaches in terms of performance (see table 2) suggests that the effectiveness of CoT is somewhat questionable. It is observed that models using CoT don't consistently reason correctly towards the right solution, even when they eventually provide the correct answer e.g., see tables 3 and 4. Moreover, Code Llama 7-b performed better under Zero-shot than CoT, while Code Llama 13-b showed the opposite trend.\nAnalyzing the success across specific tasks numbered 3, 14, 17, 47 (see table 2), it's observed that certain tasks (e.g., task #3, see figure 6) were consistently solved across different models and prompting methods, indicating these tasks might be easier to solve or more aligned with the models' capabilities. In contrast, tasks like #17 and #47 were rarely or never solved, pointing to their higher complexity or misalignment with the models' reasoning patterns."}, {"title": "Experimental Details", "content": "Models In our study, we evaluated several models: LLaMA (Touvron et al., 2023), a META-owned LLM known for its robust performance across a range of benchmarks; We also assessed Phind which is an adaptation of CodeLlama-34B, fine-tuned on a dataset exclusive to Hugging Face; and the recently released Mixtral (Jiang et al., 2024), famous for its speed, success across a range of benchmarks and versatility in language.\nCompute Each task was executed three times. A task was considered successfully completed by a model if it achieved success in at least one of these attempts. Due to constraints in GPU resources, all runs were conducted using 8-bit quantization on two Quadro RTX 8000 GPUs, each with 50GB of memory."}, {"title": "Discussion", "content": "In our study, we specifically investigated the capability of Large Language Models (LLMs) in tasks that require abstract reasoning. This systematic analysis is unique in its focus on open-source LLMs, utilizing Zero-shot and Chain-of-Thought (CoT) as prompting techniques. The results clearly show that the models underperform on the dataset, achieving a maximum success rate of only 2 out of 50 tasks. These findings align with recent similar research, such as the study described in (Xu et al., 2023), which exclusively evaluated GPT (OpenAI, 2020, 2023) models on the 50 tasks subset. In one of their configurations, GPT-3.5 (OpenAI, 2020) achieved success in only 3 out of 50 tasks using Zero-shot and 2 using CoT. For GPT-4 (OpenAI, 2023), the success rates were slightly higher, with 5 tasks solved using Zero-shot and 9 with CoT. An interesting observation of our study, also observed in (Xu et al., 2023), is that employing CoT sometimes resulted in poorer outcomes (as shown in tables 1 and 2). In both our study and in (Xu et al., 2023), it was evident that even when models correctly solved tasks using CoT, their reasoning processes were not necessarily sound or logical (see tables 3, 4). This suggests that while CoT can lead to correct answers, it does not always ensure a coherent or accurate reasoning, highlighting a significant limitation in its effectiveness. These outcomes suggest that LLMs generally struggle with abstract reasoning tasks, as demonstrated by their limited success on the ARC dataset.\nIt is important to underscore that the annual ARCathon competition, dedicated to advancing models capable of handling the ARC dataset, has not seen any significantly improved outcomes since 2020 with a success rate of ~ 30% on the dataset (for more detailes, see ARCathon). Notably, the solution that achieved this did not employ any deep learning models. When deep learning methods have been applied to this challenge, they have shown only minimal, if any, enhancement in performance. For instance, in a study conducted by (Xu et al., 2023), where GPT was tested on ARC, the success rate was far below 30%. This lack of advancement highlights a critical aspect: despite progress in deep learning and particularly in language models, LLMs remain challenged in abstract reasoning. Our analysis contributes to the growing body of evidence suggesting that current LLMs are a considerable distance from achieving Artificial General Intelligence (AGI). The consistent inability of LLMs to excel in abstract reasoning, even when leveraging techniques like CoT, highlights an important limitation in their current design and functioning.\nWhile our study represents a step forward in un-"}]}