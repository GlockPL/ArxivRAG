{"title": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease Detection based on Spontaneous Speech", "authors": ["Jonathan Heitz", "Gerold Schneider", "Nicolas Langer"], "abstract": "Alzheimer's Disease (AD) is a significant and growing public health concern. Investigating alterations in speech and language patterns offers a promising path towards cost-effective and non-invasive early detection of AD on a large scale. Large language models (LLMs), such as GPT, have enabled powerful new possibilities for semantic text analysis. In this study, we leverage GPT-4 to extract five semantic features from transcripts of spontaneous patient speech. The features capture known symptoms of AD, but they are difficult to quantify effectively using traditional methods of computational linguistics. We demonstrate the clinical significance of these features and further validate one of them (\"Word-Finding Difficulties\") against a proxy measure and human raters. When combined with established linguistic features and a Random Forest classifier, the GPT-derived features significantly improve the detection of AD. Our approach proves effective for both manually transcribed and automatically generated transcripts, representing a novel and impactful use of recent advancements in LLMS for AD speech analysis.", "sections": [{"title": "Introduction", "content": "In light of the global demographic shift towards an older population, Alzheimer's Disease (AD) emerges as a critical public health concern with a substantial economic burden (Weller and Budson, 2018). While there is no known cure, effective management depends on early diagnosis (Arvanitakis et al., 2019), necessitating the identification of biomarkers that are easy to collect, cost-effective, and non-invasive (Ribaldi et al., 2019). Speech and language alterations manifest as early symptoms of AD (Calz\u00e0 et al., 2021), presenting a promising avenue for real-time AD screening through speech analysis in extensive epidemiological research. For these methods to be adopted on a large scale, it is imperative to develop fully automated, reliable, and explainable systems capable of providing real-time predictions.\nThe recent rise of large language models (LLMs) has opened new possibilities of automatic natural language processing. One of the most popular and powerful LLMs is OpenAI's GPT series, the most capable model being GPT-4, which excels at a variety of traditional language processing benchmarks and beyond, including e.g. the Medical Knowledge Self-Assessment Program (OpenAI, 2023). Despite the widespread integration of GPT into various applications, the field of AD speech analysis remains relatively underdeveloped in comparison, with prevailing methodologies still predominantly reliant on conventional (acoustic and linguistic) features or older language models such as BERT (Devlin et al., 2018) (c.f. Parsapoor (2023) for a recent review on the topic). While these conventional approaches have shown impressive results distinguishing AD from control, they are ineffective in detecting some of the clinically known, but more complex and high-level symptoms of AD in speech, such as word-finding difficulties.\nIn this study, we address this gap by leveraging GPT in two distinct approaches for AD detection: a) fine-tuning a GPT model for direct classification purposes, and b) employing GPT as a rich semantic feature extractor from transcripts. The extracted features quantify complex speech alterations in AD, which existing methodologies are unable to capture. When combined with established features and Random Forest, they yield substantial improvements in AD detection, while enhancing explainability."}, {"title": "Related Work", "content": "Significant work has been done on AD classification based on spontaneous speech. Recently, the most popular datasets in the field were introduced as part of the ADRESS and ADRESSo challenges (Luz et al., 2020, 2021). While some approaches leverage information encoded in the audio signal directly, most work has found the linguistic analysis of transcripts more useful (Cummins et al., 2020). Methodologically, most such studies can be divided into a more traditional approach of feature extraction paired with the use of popular classification algorithms (e.g. Tang et al., 2023; TaghiBeyglou and Rudzicz, 2024), or fine-tuned language models, most prominently BERT (e.g. Balagopalan et al., 2020; Pan et al., 2021). The feature-based approach has the advantage of being more transparent and explainable. Fine-tuned language models, on the other hand, have been reported to produce slightly better classification performance, although the reported performance differences are usually small (Balagopalan et al., 2021) and depend on the setup (e.g. manual vs. automatically generated transcripts, Heitz et al. (2024)).\nLLMs, in particular (Chat)GPT, have been applied in a variety of medical settings, including medical competency examinations (Nori et al., 2023) and diagnosis (Hirosawa et al., 2023; Wang et al., 2023b). In the context of cognitive decline or AD, prior work has used GPT for data augmentation of speech transcripts (Cai et al., 2023) or employed it to directly classify AD from control relying on a prompt listing participant demographic information and cognitive test scores (Wang et al., 2023b). Limited work has studied GPT on spontaneous speech transcripts: Yang et al. (2023); Wang et al. (2023a) attempted to distinguish patients with mild cognitive impairment (MCI) from healthy controls, iteratively improving ChatGPT prompts. However, they used a non-standard and unbalanced dataset, did not compare performance to traditional approaches, and attempted direct (zero-shot) classification, making it impossible to combine GPT-extracted information with established features. B.T. and Chen (2024) experimented with the ADRESSo dataset, but the results are only slightly better than the random baseline. Both of these approaches queried GPT via the web-based Chat-GPT interface, which uses random sampling of responses, limiting reproducibility.\nThe novelty of our contribution is three-fold: Firstly, to the best of our knowledge, this is the first study leveraging GPT to extract linguistic features from transcripts of spontaneous speech and integrating them into an existing pipeline. Secondly, we validate the GPT-extracted features: We assess their clinical significance through a group comparison, and further scrutinize one of them by measuring its alignment with a proxy measure and human evaluations. Thirdly, our evaluation is more rigorous than prior work by using a standard dataset, querying OpenAI's GPT API with reproducible settings, assessing output stability to prompt and random seed variations, and comparing our approach to an established combination of linguistic features and Random Forest, as well as a fine-tuned GPT model. Our innovative use of GPT improves AD classification performance, with advantages in explainability."}, {"title": "Methods", "content": "All training and analysis is performed on a Linux Ubuntu machine with 8 CPUs, 32 GB RAM, and a NVIDIA Tesla T4 GPU. Our pipeline is implemented in Python 3.12, and our code for preprocessing, feature extraction, model training, and evaluation is available on our GitHub repository\u00b9."}, {"title": "Dataset and preprocessing", "content": "In this work, we use the English ADRESS dataset (Luz et al., 2020), containing audio recordings of 156 participants describing the Cookie Theft picture (Goodglass et al., 2001). The dataset is balanced with respect to diagnosis, age, and gender, and includes manual transcriptions in the CHAT annotation format (MacWhinney, 2000). Basic demographic characteristics are provided in Table 1.\nThis dataset is a subset of the DementiaBank English PITT corpus (Becker et al., 1994), with noise removal applied to the audio files (Luz et al., 2020). We find this noise removal problematic, as it also removes significant sections of speech, resulting in multiple audio files with no or very little intelligible participant voice. This renders downstream automatic speech recognition difficult. To counteract this problem, we matched the ADRESS selection"}, {"title": "Automatic Speech Recognition (ASR)", "content": "To evaluate the effectiveness of our approach in a fully automatic AD detection pipeline, we compare the use of manual transcripts to results from automatic speech recognition (ASR). We employ two pre-trained state-of-the-art ASR models: Whisper (Radford et al., 2023) and the Google Speech \"Chirp\" model (Zhang et al., 2023), both of which have reported excellent performance\u00b3.\nWe assess the quality of these transcriptions using the Word-Error-Rate (WER) (Morris et al., 2004), which quantifies differences between ASR transcripts and the manual transcripts provided as part of the dataset, and is defined as\n$WER=\\frac{I+D+S}{N}$ (1)\nwhere N denotes the number of words in the manual transcription, and I, D, and S count insertions, deletions, and substitutions of words."}, {"title": "Feature engineering", "content": "As a baseline, we use a set of 40 linguistic features which we call Established features. These features include lexical features, features based on part-of-speech (POS) tagging, and features of repetitiveness, and they have worked well in previous approaches of AD classification on speech. The inclusion criteria, the list of features, and their definitions are detailed in Appendix A."}, {"title": "GPT features", "content": "We prompt GPT-4 (OpenAI, 2023) programmatically via the OpenAI API to extract relevant features from a transcript. Reproducibility of GPT outputs is maximized by setting a seed and specifying temperature 0 in the API calls."}, {"title": "Choice of features (Prompt 1)", "content": "To initially identify relevant features, we query GPT once for five indicators of AD in spontaneous speech, without providing any transcripts. The GPT prompt and its response are provided in Appendix C. We call the resulting features GPT features, and they consist of: Word-Finding Difficulties (Anomia), Semantic Paraphasias, Syntactic Simplification, Impoverished Vocabulary, and Discourse Impairment. The free-text descriptions of these features are also provided by GPT (they are contained in Prompt 2, displayed in Figure 1).\nNote that all five indicators have previously been reported to occur more frequently in AD patients than controls (Forbes-McKay et al., 2005; Croisile et al., 1996; Forbes et al., 2002; Lindsay et al., 2021); in other words, GPT successfully identifies known symptoms of this disease, motivating its use in our application."}, {"title": "Feature extraction (Prompt 2)", "content": "For each transcript, we query GPT-4 to quantify each feature on a Likert scale from 1 (not at all fulfilled) to 7 (very strongly fulfilled), without providing information on the diagnostic label (AD or control). The scale from 1 - 7 is motivated by the \"Rating Scale Profile of Speech Characteristics\u201d of the Boston Diagnostic Aphasia Examination (BDAE, Goodglass et al., 2001), where a similar scale is used by clinicians to rate characteristics of spontaneous speech in the context of aphasia. In addition to a numerical assessment, we ask GPT for text extracts from the transcript to explain the assessment. The prompt and an example output are given in Figure 1."}, {"title": "Validation of GPT features", "content": ""}, {"title": "Clinical validation", "content": "For each feature, we perform a group comparison between AD and control, calculating Cohen's d (effect size, Cohen, 1988). In addition, we statistically test whether values in AD are significantly larger than values in the Control group using a Mann-Whitney U Test (Mann and Whitney, 1947), as it is a non-parametric test suitable for ordinal features."}, {"title": "Validation of Word-Finding Difficulties", "content": "While GPT readily quantifies the five studied indicators, it is unclear whether these assessments indeed capture the speech characteristics in question. For the feature Word-Finding Difficulties (Anomia), we try to validate this by comparing it to a deterministic proxy measure and to human ratings."}, {"title": "Validation against proxy measure", "content": "We compute a proxy feature disfluency ratio, defined as the number of disfluencies divided by the total number of spoken words. To count the number of disfluencies, we use all relevant special transcription markers provided by the CHAT format: fragmented words (e.g. \"coo\" instead of \"cookie\"), filler words (e.g. \"uhm\"), as well as explicitly coded repetitions ([/]), revisions ([//]), and pauses ((.), (..), (...)). This proxy feature is our best attempt to quantify word-finding difficulties using the available annotations and traditional methods of computational linguistics. For this reason, we expect the GPT feature Word-Finding Difficulties (Anomia) to be more highly correlated to disfluency ratio than to other features, indicating that the two features are related, and increasing our confidence in GPT successfully capturing the speech characteristic in question."}, {"title": "Agreement to human raters", "content": "We have asked two specialists (a psychologist and a speech therapist) to rate the word-finding difficulties for all subjects in our dataset, given both the audio recording and the manual transcript. These human ratings serve as a gold standard that we aim for our GPT-derived feature Word-Finding Difficulties (Anomia) to approximate. Given the subjectivity of the task, two human raters will never agree perfectly. We quantify the amount of human (dis)agreement (i.e. the inter-rater reliability) using an intraclass correlation coefficient between these two raters (Case ICC(3,1) according to Shrout and Fleiss (1979))"}, {"title": "AD classification and evaluation", "content": "We train a Random Forest (RF) classifier for binary classification (AD vs. control) on three sets of features: Our baseline Established features, our GPT features, and their union Established+GPT."}, {"title": "Fine-tuning GPT", "content": "In addition to using GPT as a feature extractor in combination with RF, we directly fine-tune GPT to distinguish AD from control. This is conceptually similar to fine-tuning BERT, a common and successful approach to the AD detection task in prior work. For fine-tuning, we use a prompt asking GPT to decide whether a transcript comes from a person with AD or a healthy person, and ground truth completions 'Dementia' or 'Healthy'. On test samples, the first token's log probability (which is provided by the fine-tuned model) is converted into a predicted AD probability, allowing the calculation of our metrics. Fine-tuning is orchestrated programmatically using the OpenAI API, with default hyperparameters\u2077."}, {"title": "Evaluation", "content": "Classification performance is assessed using stratified 10-fold cross validation (CV) on the entire dataset (156 samples), with fixed random splits. We combine the test predictions of the 10 folds and report the area under the ROC curve (AU-ROC) on their union. To quantify uncertainty of results, we provide two-sided bootstrap confidence intervals (CI)\u2078."}, {"title": "Sensitivity analysis", "content": "LLMs are known to be sensitive to slight changes in prompts (Errica et al., 2024; Gan and Mori, 2023; Atil et al., 2024). To analyse how stable our GPT feature values are subject to this prompt sensitivity, we created two variations of Prompt 2, where instructions are given in other words while keeping their meaning (full prompts are given in Appendix D). We use these to extract two additional sets of GPT feature values. We compute intraclass correlation coefficients (Case ICC(2,1) according to Shrout and Fleiss (1979)) to assess how strongly result of different prompt versions agree. In addition, we calculate the difference of feature value when using the modified prompt compared to the original Prompt 2 for each feature and participant, and report their mean absolute difference (MD), defined as\n$MD=\\frac{1}{2n} \\Sigma_{v\\in{1,2}} \\Sigma_{i\\in{1,...,n}} |f_i - f_i^v|$ (3)\nwhere for participant i among n participants, $f_i$ refers to the original feature value and $f_i^v$ represents the feature value when using the alternative prompt v.\nAnalogously, we test the sensitivity of our GPT feature values to different random seeds in the OpenAI API call.\nWe also perform a control analysis in which GPT was prompted to extract ten indicators instead of five, allowing us to evaluate the robustness of the feature selection process."}, {"title": "Results", "content": "Comparison of ASR models: Word Error Rates (WER) for both ASR models are similar, with the following median WER on the entire dataset (lower is better): Whisper: 0.35 (AD: 0.43, control: 0.31), Google Speech: 0.37 (AD: 0.40, control: 0.30). For AD classification (cf. results in Table 2), we observe a trend of Google Speech providing more useful transcripts than Whisper."}, {"title": "Clinical validation of GPT features", "content": "Validation results of our five GPT features are presented in Figure 2. We observe that GPT feature values are clearly higher for AD than control, with highly significant group differences (p-values < 10\u207b\u00b9\u2070) and large effect sizes (Cohen's d > 1.1). This confirms that all GPT features capture language characteristics that are clinically relevant to distinguish individuals with AD from healthy controls."}, {"title": "Validation of Word-Finding Difficulties", "content": "The correlation between the proxy feature disfluency ratio and Word-Finding Difficulties (Anomia) is 0.63, which is higher than the absolute correlation to any other linguistic feature (\u2264 0.55, c.f. Appendix B for a full correlation matrix). This strengthens our hypothesis that this GPT feature indeed captures difficulties in word-finding. The intraclass correlation coefficient (ICC) between the two human raters quantifying word-finding difficulty is 0.55 (CI: 0.43 \u2013 0.65), indicating moderate inter-rater reliability (Shrout and Fleiss, 1979), which highlights the inherent subjectivity of assessing high-level concepts such as \"word-finding difficulty\". The ICC between GPT and human raters is 0.53 (CI: 0.44 \u2013 0.62), with a confidence interval (CI) that overlaps with the human agreement. This indicates that GPT's assessment captures the speech characteristic in question as well as a human rater. It is important to note that the GPT assessment is based solely on the speech transcripts, whereas human raters had the advantage of basing their assessment on both audio and transcripts."}, {"title": "AD classification performance", "content": "Classification results are given in Table 2. We observe that our GPT features alone perform worse than the Established features (our baseline), when combined with RF. Fine-tuning GPT also does not outperform our baseline. However, RF with the combination of both established and GPT features (Established+GPT) outperforms the established features. The differences are statistically significant on manual transcripts and stronger on manual than on ASR transcripts, but robust across all settings. Note that confidence intervals are relatively wide. This is a result of the small number of samples in our dataset, a main limitation of this work."}, {"title": "Feature importance", "content": "Table 3 presents the 10 most important features among Established+GPT, according to mean absolute SHAP values (Lundberg et al., 2020). We observe that GPT features are highly important, representing 5 out of the top 6 features."}, {"title": "Sensitivity analysis", "content": "Table 4 displays the results for prompt and seed sensitivity. Low MD (approx. 0.2 on a scale from 1 \u2013 7) and high ICC (all > 0.79, considered excellent agreement (Cicchetti, 1994)) indicate that slight changes in prompts or random seeds have a low impact on the feature values.\nOur control analysis extracting ten GPT features (instead of five) yielded a classification performance similar to our main results (results are listed in Appendix Table 8)."}, {"title": "Running time", "content": "Running time is dominated by ASR, taking approx. 55 min (Whisper) or 30 min (Google Speech) for the entire dataset. Feature extraction and RF model training times sum up to less than 10 min. GPT fine-tuning takes around 15 min per split, with limited parallelization allowed by the API. Note that the inference time per individual is short enough to be deployed in a potential real-time application."}, {"title": "Discussion and Conclusion", "content": "In this study, we have harnessed the capabilities of GPT-4 to extract five semantic features from transcripts of spontaneous speech, which were then utilized to complement existing linguistic features within an Alzheimer's Disease (AD) detection framework. The extracted features quantify known symptoms of AD in speech, but previous efforts in linguistic feature engineering have failed to capture them due to their complex and elusive nature.\nFor example, the feature Word-Finding Difficulties (Anomia) is associated with AD (Rohrer et al., 2008), but its calculation from a given transcript is non-trivial and we are not aware of any existing linguistic feature attempting to do so. We have attempted to construct a rule-based deterministic proxy of this characteristic (disfluency ratio, based on manual annotations of disfluencies) and showed that it is correlated highly with this GPT feature. In addition, we have demonstrated an agreement of this GPT feature's values with human raters. Compared to existing linguistic features, we believe that the GPT-extracted Word-Finding Difficulties (Anomia) captures a richer concept, and a group comparison between AD and control as well as our SHAP feature importance analysis confirm the utility of this feature in AD detection. Table 5 displays three example transcripts, and the corresponding GPT-4 output for this feature. The difference in word-finding difficulty between these examples is apparent, but goes beyond the expressiveness of traditional linguistic features. Similar arguments can be made for all five features included in this study.\nThe GPT-generated features represent high-level speech alternations which are easier to grasp intuitively than many existing highly specific linguistic features. GPT-4 not only quantifies these features on a scale from 1 to 7, but also complements its assessment with explanatory notes or selected transcript excerpts (see examples responses in Figure 1 and Table 5). A potential application could provide these details to clinicians and patients. The transparent derivations and the intuitive interpretation of the features are a step towards better explainability, which is a crucial property of AI in medical applications, as it augments transparency, promotes trust of both clinicians and patients, and satisfies legal requirements such as the EU\u2019s \u201cAI Act\u201d. We want to stress that explainability of this sort is a significant advantage of our feature-based approach compared to a fine-tuned language model such as GPT (as presented in this paper) or BERT (as frequently done in prior work). While fine-tuning is elegant and can be high-performing, explainability remains difficult, hindering a potential application in clinical practice.\nOur results demonstrate how the GPT-generated features alone as well as the fine-tuning of GPT produce sub-optimal classification performance. This is in line with prior research employing GPT for AD classification, where results were unsatisfactory (B.T. and Chen, 2024; Wang et al., 2023b). We hypothesize that fine-tuning does not work better because many low-level features of language (e.g. based on word and letter counts) that are useful to distinguish AD from control cannot be effectively extracted by an LLM. For example, it has been shown that LLMs are unable to count letters (Zhang and He, 2024), making it impossible to extract a feature such as Average Word Length, which is among our most important features (cf. Table 3). However, the combination of established linguistic with GPT-generated features produces a system that significantly outperforms prior feature-based approaches. This demonstrates that the complex semantic patterns identified by GPT encompass additional significant information beyond the reach of simpler, established features, which fail to capture such depth. Moreover, they add a significantly different perspective: The maximal absolute correlation between a GPT feature and any other included linguistic feature is 0.55 and thus rather low (a full correlation matrix is given in Appendix B). Furthermore, the new GPT features are clinically relevant (cf. Figure 2) and show high feature importance compared to established features (cf. Table 3), with 5 out of the top 6 most important features given by GPT. This further strengthens the observation that they indeed capture meaningful symptoms of AD.\nOur approach is robust to slight variations in prompts or random seeds and is effective with both manual transcripts of spontaneous speech and ASR transcripts, where we recommend the use of Google Speech, as it results in better AD classification performance than Whisper. We suspect the reason to be a stronger use of a language model in Whisper's decoding pipeline compared to Google Speech, smoothing ASR outputs in a way that removes details (e.g. repetitions) from the transcripts that prove valuable to distinguish AD from control. The strong performance on ASR transcripts could enable the use of our approach in a fully automatic, low-cost, and real-time system. This makes it suitable for large-scale epidemiological studies, or might allow the identification of AD subgroups based on linguistic characteristics (similar to Park et al., 2017), paving the way for more tailored cognitive training interventions.\nIn addition to the presented method of leveraging GPT as a feature extractor and fine-tuning GPT for direct AD classification, we have experimented with other approaches of harnessing GPT for AD classification based on spontaneous speech, including direct zero-shot prediction of AD vs. control (instead of feature extraction), and the use of GPT-40 instead of GPT-4. These additional experiments showed no improvements over the presented methods (results are given in Appendix E). In this work, we show no detailed results for a fine-tuned BERT model, but results reported in our previous work are inferior to the presented method here (Heitz et al., 2024)."}, {"title": "Conclusion", "content": "In this study, we employed GPT-4 to extract linguistic features capturing known alterations in AD speech from both manually transcribed and automatically generated transcripts of spontaneous speech, integrating these within an end-to-end AD detection framework. We demonstrated that the inclusion of complex GPT-derived features enhances performance beyond what is achievable with traditional linguistic features alone, surpassing a fine-tuned GPT model. Furthermore, these new features are accompanied by explanatory snippets extracted from the transcripts, contributing to the advancement of more interpretable AI within a medical setting. Our innovative approach of combining GPT-generated features with an established classification pipeline is a novel and effective application of recent advancements in LLMs for AD speech analysis."}, {"title": "Limitations", "content": "The primary limitation of this study is the size and diversity of the dataset. Although we are convinced that the usefulness of our new GPT features would generalize to larger and more diverse datasets, further research is required to strengthen our findings and effectively test for potential biases.\nMoreover, our approach relies on GPT, a technology controlled by a commercial organization. Its large-scale deployment could entail economic and ethical risks, and may be affected unpredictably by future developments. However, powerful open-source alternatives to GPT are available and could be explored instead.\nOur approach is based on transcripts of speech, ignoring additional information contained in the audio signal. While prior research has shown that these linguistic features are more useful than audio features (Cummins et al., 2020), clinicians assessing speech disorders rely on both modalities, capturing the entirety of patients\u2019 speech. Future work should thus focus on multi-modal approaches recent developments on multi-modal LLMs are promising foundational steps into this direction.\nMore broadly, we recognize substantial potential in utilizing LLMs, such as GPT, to explain the outputs of existing predictive models. This approach can enhance the acceptance of machine learning models within the medical field by bridging the communication gap between complex (difficult to understand) models and clinicians without technical expertise."}, {"title": "CGPT prompts", "content": "In this section, we present the verbatim prompts introduced in Section 3.3.2: Prompt 1 has been used once, to initially find the five GPT features, without providing any transcripts. Prompt 2 is used for every transcript, to extract the value of these features."}, {"title": "GPT Prompt 1 - Choice of features", "content": "GPT Prompt 1, used to initially find relevant features, as well as the GPT-4 response, are given below:\nSystem message: You are a experienced doctor studying patients with Alzheimer's dementia. You know everything about this disease and how it affects language. You can analyze transcriptions of spontaneous speech and tell whether the speaker suffers from Alzheimer's Disease (dementia) or not. The transcriptions are typically created based on audio recordings of people describing the Cookie Theft Picture. In this task, participants are shown a drawing of a mother who is drying dishes next to the sink in the kitchen. She is not paying attention and has left the tap on. As a result, water is overflowing from the sink. Meanwhile, two children are attempting to take cookies from a jar when their mother is not looking. One of the children, a boy, has climbed onto a stool to get up to the cupboard where the cookie jar is stored. The stool is rocking precariously. The other child, a girl, is standing next to the stool and has her hand outstretched ready to be given cookies.\nPrompt 1: Name 5 good indicators that you would examine in these transcriptions of spontaneous speech, pointing towards Dementia\nGPT-4 response: When analyzing transcriptions of spontaneous speech, particularly in the context of the Cookie Theft Picture description task, there are several linguistic indicators that may suggest the presence of Alzheimer's dementia. Here are five key indicators to look for:\n\u2022 Word-Finding Difficulties (Anomia): Individuals with Alzheimer's often have trouble finding the right words. This can manifest as frequent pauses, use of non-specific words like \"thing\" or \"stuff,\" or circumlocutions (talking around the word without being able to name it). For example, instead of saying \"stool,\" a person with dementia might say \"the thing you sit on.\"\nImpoverished Vocabulary: The range of words used by someone with Alzheimer's may become limited. Their language may seem less rich and more repetitive, with a reliance on common and general terms rather than specific nouns or verbs.\nSyntactic Simplification: The complexity of sentence structures may decline. People with dementia might use simpler, shorter sentences and may make more grammatical errors. They might avoid complex grammatical constructions like subordination or relative clauses.\nSemantic Paraphasias: This refers to the use of incorrect words that are semantically related to the intended word. For example, a person with dementia might say \"oven\" instead of \"sink\" or \"water\" instead of \"cookies.\"\nDiscourse Impairment: There may be a noticeable decline in the ability to organize narrative speech. This can include tangential speech, difficulty maintaining a topic, and problems with coherence and cohesion. The person might jump from one idea to another without clear connections, or they might provide too much or too little information about the picture.\nIt's important to note that while these indicators can suggest the presence of Alzheimer's dementia, they are not definitive on their own. A thorough assessment by a healthcare professional, including a detailed medical history, cognitive testing, and possibly neuroimaging, is necessary for a formal diagnosis. Additionally, these symptoms can vary widely among individuals and may be influenced by other factors such as education level, language proficiency, and cultural background."}, {"title": "GPT Prompt 2: Feature extraction", "content": "GPT Prompt 2 is used to extract feature values for our GPT features. In the following, the transcript text is replaced by the placeholder {transcript}.\nThis prompt is identical to the one presented in Figure 1, additionally including the system message:\nSystem message: You are an experienced doctor studying patients with Alzheimer's dementia. You know everything about this disease and how it affects language. You can analyze transcriptions of spontaneous speech and tell whether the speaker suffers from Alzheimer's Disease (dementia) or not. The transcriptions you will see have been created based on audio recordings of people describing the Cookie Theft Picture. In this task, participants are shown a drawing of a mother who is drying dishes next to the sink in the kitchen. She is not paying attention and has left the tap on. As a result, water is overflowing from the sink. Meanwhile, two children are attempting to take cookies from a jar when their mother is not looking. One of the children, a boy, has climbed onto a stool to get up to the cupboard where the cookie jar is stored. The stool is rocking precariously. The other child, a girl, is standing next to the stool and has her hand outstretched ready to be given cookies. Examinees are instructed \"Tell me everything you see going on in this picture\".\nPrompt 2: Here are key indicators of Alzheimer's dementia in spontaneous speech:\nWord-Finding Difficulties (Anomia): Individuals with Alzheimer's often have trouble finding the right words. This can manifest as frequent pauses, use of non-specific words like \"thing\" or \"stuff\", or circumlocutions (talking around the word without being able to name it). For example, instead of saying \"stool\", a person with dementia might say \"the thing you sit on\".\nImpoverished Vocabulary: The range of words used by someone with Alzheimer's may become limited. Their language may seem less rich and more repetitive, with a reliance on common and general terms rather than specific nouns or verbs.\nSyntactic Simplification: The complexity of sentence structures may decline. People with dementia might use simpler, shorter sentences and may make more grammatical errors. They might avoid complex grammatical constructions like subordination or relative clauses.\nSemantic Paraphasias: This refers to the use of incorrect words that are semantically related to the intended word. For example, a person with dementia might say \"oven\" instead of \"sink\" or \"water\" instead of \"cookies.\"\nDiscourse Impairment: There may be a noticeable decline in the ability to organize narrative speech. This can include tangential speech, difficulty maintaining a topic, and problems with coherence and cohesion. The person might jump from one idea to another without clear connections, or they might provide too much or too little information about the picture.\nFor each of these indicators, and for the following transcript of a Cookie Theft Picture description task, please indicate how much each indicator is fulfilled by the transcript. Give numbers between 1 (not at all fulfilled) and 7 (very strongly fulfilled). In addition, give 1-3 examples from the text for your assessment (in brackets, mark verbatim quotations with \"). Give only the indicator name, the assessment (number between 1 and 7), examples from the text (in brackets), nothing else, and no explanation.\n{transcript}"}, {"title": "Alternative prompts to assess prompt sensitivity", "content": "In the following, the two alternative versions of Prompt 2 are given. They are used in our prompt sensitivity analysis described in Section 3.6."}, {"title": "Alternative 1", "content": "Prompt 2 (Alternative 1): The following are important indicators of Alzheimer's dementia in spontaneous speech:\nWord-Finding Difficulties (Anomia): Individuals with Alzheimer's often have trouble finding the right words. This can manifest as frequent pauses", "thing\" or \"stuff\", or circumlocutions (talking around the word without being able to name it). For example, instead of saying \"stool\", a person with dementia might say \"the thing you sit on\".\nImpoverished Vocabulary": "The range of words used by someone with Alzheimer's may become limited. Their language may seem less rich and more repetitive", "Simplification": "The complexity of sentence structures may decline. People with dementia might use simpler", "Paraphasias": "This refers to the use of incorrect words that are semantically related to the intended word. For example", "oven\" instead of \"sink\" or \"water\" instead of \"cookies.\"\nDiscourse Impairment": "There may be"}]}