{"title": "PearSAN: A Machine Learning Method for Inverse Design using Pearson Correlated Surrogate Annealing", "authors": ["Michael Bezick", "Blake A. Wilson", "Vaishnavi Iyer", "Yuheng Chen", "Vladimir M. Shalaev", "Sabre Kais", "Alexander V. Kildishev", "Alexandra Boltasseva", "Brad Lackey"], "abstract": "PearSAN is a machine learning-assisted optimization algorithm applicable to inverse design problems with large design spaces, where traditional optimizers struggle. The algorithm leverages a generative model's latent space for rapid sampling and employs a Pearson correlated surrogate model to predict the figure of merit of the true design metric. As a showcase example, PearSAN is applied to thermophotovoltaic (TPV) metasurface design by matching the working bands between a thermal radiator and a photovoltaic cell. PearSAN can work with any pretrained generative model with a discretized latent space, making it easy to integrate with VQ-VAEs and binary autoencoders. Its novel Pearson correlational loss can be used as both a latent regularization method, similar to batch and layer normalization, and as a surrogate training loss. We compare both to previous energy matching losses, which are shown to enforce poor regularization and performance, even with upgraded affine parameters. PearSAN achieves a state-of-the-art maximum design efficiency of ~ 97% and is at least an order of magnitude faster than previous methods, with an improved maximum figure-of-merit gain.", "sections": [{"title": "Introduction", "content": "With the impressive capabilities of modern generative models, including Generative Adversarial Networks (GANs) [1, 2], diffusion models [3-5], and Large Language Models [6-8], there is growing interest in their application across various engineering domains. GANs and diffusion models have shown potential for generating device designs with specific properties by integrating surrogate models in place of large and complex simulations. Surrogate optimization algorithms can be applied to many engineering challenges, such as topology optimization for mechanical or architectural components, novel 3D circuit layouts, and predicting failure modes in intricate systems. These models make efficient use of synthetic training datasets, helping reduce simulation costs and shorten design cycles [9-13].\nIn photonics and optoelectronics, the optimization of new compact planar devices and systems built on optical metasurfaces is a nascent engineering problem [10,14-19]. Optical metasurfaces are subwavelength-thin nanostructed films that have fine-tuned control over phase, amplitude, and polarization across various wavelengths through a composition of optimized meta-atoms. Their unique properties have sparked tremendous interest in tailoring metasurfaces for emerging applications in sensing [20-22], quantum information [11, 23], and renewable energy [24]. Traditional methods of generating optimal meta-atoms for tailored metasurfaces requires computationally expensive gradient calculations in an exponentially large design space, making direct optimization impractical [25]. Alternative to these approaches is latent optimization [10, 15, 17], an ever-growing suite of machine learning-assisted optimization algorithms that sample designs for optimization problems from a feature-rich, lower-dimensional latent space using global search algorithms such as simulated annealing [26], stochastic gradient descent, Markov Chain Monte Carlo [10, 11] or quantum-inspired optimizers [27]. Often these latent optimization algorithms use a pretrained decoder and train a surrogate model to predict the figure-of-merit of a decoded design solely from its latent vector. Then, optimizing the input to the surrogate model generates latent vectors, which are decoded into designs with low loss and good figure-of-merit. The choice of surrogate model, surrogate loss, latent space regularization, and surrogate optimizer is crucial for latent optimization performance. For example, in the original bVAE-QUBO work [10], a pretrained binary variational autoencoder (bVAE) with Gumbel-Softmax reparameterization [28] created a categorical distribution over la-"}, {"title": "Methods", "content": ""}, {"title": "Isotonic Latent Optimization", "content": "Consider a data space $X$ sampled i.i.d. $X = \\{x^{(i)}\\}_{i=1}^N$ where each point $x^{(i)}$ has a figure-of-merit (FOM) value $f(x^{(i)})$. Our goal is to train a generative model $p_{\\theta}$ with variational parameters $\\theta$ to sample new data $x \\sim p_{\\theta}(x)$ that optimizes the expected FOM $f(\\cdot)$, i.e.,\n$\\arg \\max_\\theta E_{x \\sim p_{\\theta}(x)}[f(x)].$ \t\t\t (1)\nSince the advent of variational autoencoders [30, 31], many generative models rely on extracting data features into a low-dimensional latent space $Z$ with latent variables $z \\in Z$ [5]. Then, the new data is sampled directly from the latent space $Z$ using a conditional decoder $x \\sim p_{\\theta}(x|z)$. To leverage latent sampling for optimization, we express $p_{\\theta}(x) = \\sum_z p_{\\theta}(x|z)q_{\\phi}(z)$ with a latent sampler $q_{\\phi}(z)$ and latent variational parameters $\\phi$. Then, we rewrite Eq. 1 as a latent optimization problem\n$\\arg \\max_{\\theta,\\phi} E_{z \\sim q_{\\phi}(z)}[E_{x \\sim p_{\\theta}(x|z)}[f(x)]].$ \t\t\t (2)\nThis splits our model $p_{\\theta}$ into a latent optimization sampler $q_{\\phi}$ and a decoder $p_{\\theta}(x|z)$. Rather than train the decoder being aware of the latent sampler's optimization distribution $q_{\\phi}$ [17], which can lead to sampling an unnormalized distribution at each gradient step [32] and meticulous fine-tuning problems [8], we train the latent sampler to produce optimal latent vectors under a pretrained \u201coptimization-agnostic\u201d decoder $p_{\\theta}(z)$.\nThe optimal latent sampler $q_{\\phi}$ would solve Eq. 1 by producing the latent vector for a decoded design with the maximal FOM with probability 1, however this is generally infeasible to accomplish in practice because of the typically exponentially large latent space [33]. Instead, we only make the weak assumption that our model $p_{\\theta}$ produces samples whose probability is (strictly) isotonic\u00b3 with the FOM values over the design space. We say that $p_{\\theta}$ is (strictly) isotonic with $f$ over $X$, i.e., $f(x) \\risingdot p_{\\theta}(x)$, to mean that for each pair of points $(x^{(i)}, x^{(j)}) \\in X \\times X$ we have\n$f(x^{(i)}) < f(x^{(j)}) \\rightarrow p_{\\theta}(x^{(i)}) < p_{\\theta}(x^{(j)})$.\t\t (3)\nBy Eq. 3, the model will assign larger probability to samples with larger FOM values. With the fixed decoder $p_{\\theta}(x|z)$, we expand the marginal of $p_{\\theta}$ in Eq. 3 to obtain $f(x) \\risingdot \\sum_z [p_{\\theta}(x|z)q_{\\phi}(z)]$. As we will show, to isotonically couple the latent sampler $q_{\\phi}$ to $f(x)$, we pick a sampler $q_{\\phi}$ that produces samples with probability isotonic to a surrogate function $h_{\\phi}(z)$, i.e., $h_{\\phi}(z) \\risingdot q_{\\phi}(z)$ over the latent space $Z$. Then, we train the latent sampler's surrogate function $h_{\\phi}$ to be isotonic with $f$ with respect to"}, {"title": "Pretrained Deterministic Decoder", "content": "In practice, decoders $p_{\\theta}(x|z)$ are often deterministic, meaning a fixed model $D_{\\theta}$ produces a single design from each latent vector, i.e., $D_{\\theta}(z) = x$. The deterministic decoder distribution can be rewritten as $p_{\\theta}(x|z) = P(x = D_{\\theta}(z))$ which is 1 for the decoded design and 0 for all other designs. Using this fact, we marginalize over $z$ to reduce Eq. 4 to\n$f(x) \\risingdot h_{\\phi}(D^{-1}(x)) \\risingdot q_{\\phi}(D^{-1}(x))$\n$\\leftrightarrow f(D(z)) \\risingdot h_{\\phi}(z) \\risingdot q_{\\phi}(z),$\t\t (5)\nwhere $D^{-1}$ is the inverse of $D_{\\theta}$. The deterministic decoder has the benefit of coupling the figure-of-merit directly to the latent sampler through the surrogate model.\nOur focus is now on realizing the isotonic conditions in Eq. 5. by 1) choosing a distribution $q_{f}$ that is isotonic with a trainable surrogate model $h_{\\phi}$ and 2) enforcing that the surrogate model $h_{\\phi}$ is isotonic with the FOM values, i.e., $f(x) \\risingdot h_{\\phi}(z)$, both of which are satisfied with PearSAN."}, {"title": "Variational Neural Annealing", "content": "The most popular samplers $q_{\\phi}$ which aim to produce samples isotonic with a surrogate function are Markov Chain Monte Carlo [11], simulated annealing [26], quantum samplers [34], and more recently variational neural annealing [29]. Inspired by statistical mechanics, these samplers are antitonic to an energy model $h_{\\phi}(z) \\propto q_{\\phi}(z)$ where $Z = \\{0, 1\\}^n$ and $h_{\\phi}$ is given by a pseudo-boolean polynomial,\n$h_{\\phi}(z) = \\sum_{s \\subset n} \\Phi_s \\prod_{i \\in s} z_i$\n$= \\sum_i \\Phi_i z_i + \\sum_{i<j} \\Phi_{i,j} z_i z_j + \\sum_{i<j<k} \\Phi_{i,j,k} z_i z_j z_k....$\t\t\t (6)\ntypically representing the potential interactions between spin sites [35], superconducting rings [34], etc.\nTo construct $q_{\\phi}(z)$, we implement variational classical annealing (VCA), a variant of variational neural annealing. VCA uses recurrent neural networks (RNNs) to sample $h_{\\phi}(z)$ with better practical convergence compared to Markov Chain Monte Carlo and simulated annealing [29], as we corroborate in Appendix 9. RNNs are widely used for generating sequential data, such as language modeling [36], anomaly detection [37], and biometric authentication [38] through autoregression. The RNN's structure is modeled after the chain rule of probability, where the output joint probability distribution $q_{\\phi}(z)$ can be expressed as\n$q_{\\phi}(z) = q_{\\phi}(z_1) q_{\\phi}(z_2|z_1)... q_{\\phi}(z_N|z_{N-1},...,z_1)$.\t\t (7)\nEach bit $z_i$ in the latent vector $z$ is generated by a conditional probability statement $q_{\\phi}(z_i|z_1,..., z_{i-1})$ following a Bernoulli distribution. VCA works through minimizing the sampler's variational free energy\n$G_{\\phi}(t) = E_{z \\sim q_{\\phi}(z)}[h_{\\phi}(z)] - T(t) S(q_{\\phi}),$\t\t (8)\nwhere $T(t)$ is a temperature parameter that is decreased through the annealing process from a large $T(0) = T_0$ to 0, and $S(q_{\\phi}) = - \\sum_z q_{\\phi}(z) \\log(q_{\\phi}(z))$ is the entropy. We approximate the variational free energy at temperature $T$ as\n$\\frac{1}{N_s}$\n$G(T) \\approx \\frac{1}{N_s}\\sum_{i=1}^{N_s} h_{\\phi}(z^{(i)}) + T \\log(q_{\\phi}(z^{(i)})),$\t\t (9)\nby taking $N_s$ discrete samples drawn from the RNN, i.e., $z^{(i)} \\sim q_{f}$. The $T\\log(q(z^{(i)}))$ term has the effect of enforcing more randomness in the state evolution during the beginning of training with a high $T$, allowing the model to escape local minima and increasing the entropy throughout the annealing process. As $T$ decreases, analogous to simulated annealing, the model is less likely to exhibit large, random changes of state, favoring to minimize the surrogate function, implying that $h_{\\phi}(z) \\propto q_{\\phi}(z)$.\nTo allow for the antitonicity of VCA, we replace the isotonic relations in Eq. 5 with antitonic relations while still maintaining the overall objective, i.e.,"}, {"title": "Pearson Surrogate Optimization Loss (PearSOL)", "content": "$f(D(z)) \\propto h_{\\phi}(z)$\t\t\t (10)\n$h(z) \\propto q(z)$\t\t\t (11)\n$f(D(z)) \\propto q(z),$\t\t\t (12)\nwhere Eq. 11 is satisfied by using VCA. To enforce Eq. 10, we introduce PearSOL.\nTraining an antitonic surrogate model typically involves modifying the energy matching (EM) loss, which uses a pairwise L-norm loss function [10], e.g.,\n$L_{EM}(F, H) = \\sum_i ||F_i - (-H_i)||^2,$\t\t (13)\nwhere $F = \\{f(D_{\\theta}(z^{(i)}))\\}_{i=1}^N$ represents the set of decoded FOMs and $H = \\{h_{\\phi}(z^{(i)})\\}_{i=1}^N$ represents the latent vector energies. EM minimizes the difference between $-H^{(i)}$ and $F^{(i)}$, thus achieving antitonicity, but L-norm loss can be overly sensitive in less-explored regions and unnecessarily penalizes small differences. To overcome these issues and enforce antitonicity, we propose using Pearson correlation [39, 40]:\n$L_{Pearson}(F, H) = \\frac{\\sum (F_i - \\overline{F})(H_i - \\overline{H})}{S_F S_H}$\t\t (14)\nwhere $\\overline{H}$ and $S_H$ are the mean and standard deviation of $H$, and $\\overline{F}$ and $S_F$ are defined similarly for $F$. By the Cauchy-Schwarz inequality, $L_{Pearson}(F, H) \\in [-1, 1]$, with +1 or \u22121 indicating perfect isotonic or"}, {"title": "PearSAN Overview", "content": "PearSAN is especially effective when the initial dataset $X^{(0)}$ is updated with new designs generated by the decoder, allowing the surrogate model to retrain on the FOM from the new designs. We denote the current iteration with $\\tau$ and the total iterations with $T_{max}$. As outlined in Alg. 1, we begin by sampling an initial set of latent vectors $Z^{(0)} = \\{z^{(i)}\\}$ either via an encoder $z^{(i)} \\sim E(x^{(i)}) : x^{(i)} \\in X^{(0)}$ or some prior $z^{(i)} \\sim q(z)$. Each iteration begins by training a polynomial surrogate model $h_{\\phi}^{(\\tau)}$ (Eq. 6) to optimize the PearSOL over $Z^{(\\tau)}$, $L_{PearSOL}(F^{(\\tau)}, H^{(\\tau)})$ where\n$H^{(\\tau)} = \\{h_{\\phi}^{(\\tau)}(z^{(i)}) : z^{(i)} \\in Z^{(\\tau)}\\}$\t\t (16)\n$F^{(\\tau)} = \\{f(D_{\\theta}(z^{(i)})) : z^{(i)} \\in Z^{(\\tau)}\\}$,\t\t (17)\nand $D_{\\theta}$ is a pretrained, deterministic decoder $D_{\\theta}(z)$ with a discrete latent space. Then, we train a RNN $q_{\\phi}^{(\\tau)}$ to antitonically sample the surrogate model $h_{\\phi}$, using VCA by minimizing Eq. 9. As evidenced by the success of dropout, score matching and diffusion models, adding noise throughout training can promote exploration and larger accuracy in lower probability regions of the latent space. For VCA, the high"}, {"title": "Impact of Regularization", "content": "We assess the effectiveness of PearSOL and EM in regularizing the latent space of the bAE. As outlined in Appendix 8, regularization, while not strictly necessary for PearSAN's operation, helps improve sample quality by avoiding overfitting during training. To evaluate regularization performance, we employ standard GAN evaluation metrics such as KID [43], FID [44], and Inception Score [45].\nKID and FID measure the discrepancy between the features of generated and real datasets using a pretrained neural network, with KID being less sensitive to sample size, making it preferable given our relatively small dataset of 12,000 designs. Inception Score evaluates the quality of generated samples by assessing their distinctiveness and diversity using a classification model [46]. As shown in Table 1, PearSOL outperforms EM in all metrics, indicating its ability to generalize across unseen regions of the latent space better than EM."}, {"title": "Retraining Procedure", "content": "We consider $T_{max} = 10$ iterations of PearSAN and a sampling epoch threshold at $N_{thresh} = 20$ for VCA, as indicated by convergence plots in Figure 9. For each retraining iteration, we average the FOM of all accumulated vectors across 10 experiments, excluding the first iteration where the surrogate model is under-trained due to random initialization. Figure 3 (a) shows that PearSOL consistently outperforms EM\u2075 for all retraining iterations, achieving an average of 92.31% VGG-predicted efficiency on the final"}, {"title": "Comparison with Previous Methods", "content": "We compare PearSAN's efficiency against previous methods such as direct topology optimization [15], AAE+TO, AAE+VGGNet [15], AAE+DE, AAE+rDE [17] and bVAE-QUBO [10]. To generate the optimal design for PearSAN, we took the 100 best designs from the best run of PearSAN with PearSOL and simulated them using the same finite different time domain methods as the other techniques. While previous methods evaluated 100 designs over several hours, our PearSAN model evaluated 100 designs in just 0.0033 hours, showing a significant improvement in efficiency and sampling speed. PearSOL achieved the highest efficiency of 97.02%, outperforming all prior approaches which were limited by the VGGnet for predicting the FOM \u2076. It should be noted that AAE+TO had direct access to FDTD calculations throughout training [17], whereas PearSAN had access to only the VGGnet. Comparing PearSAN to AAE+VGGnet, which is equivalent to AAE+TO except with the VGGnet, reveals that PearSAN may outperform AAE+TO either given a better FOM approximation network or direct access to FDTD simulations.\nPearSAN's efficiency and speed are orders of magnitude better than previous approaches, as shown in Table 2. This is likely due to the algorithmic performance improvements rather than hardware alone, though further study is required for a fair comparison of hardware advancements."}, {"title": "Conclusion and Outlook", "content": "In this work, we introduced PearSAN, a novel machine learning-assisted optimization algorithm for inverse design problems with large parameter spaces, as exemplified by our showcase example of optimizing the metasurface unit-cell design for thermophotovoltaics (TPVs). By pairing Pearson Surrogate Optimization Loss (PearSOL) regularization with Variational Classical Annealing (VCA) sampling, PearSAN leverages a discretized latent space (e.g., from VQ-VAEs or binary autoencoders) to gen-"}, {"title": "Future Directions", "content": "While we demonstrate PearSAN with a quadratic Boolean energy function, different physics-inspired energy models (e.g., Blume-Capel, Potts, or higher-order polynomials) may offer richer representations of the design space. Additionally, PearSAN can be adapted to continuous latent spaces (e.g., with a simple multilayer perceptron and the Adam optimizer [47].) Beyond autoregressive models for VCA, exploring different latent-space optimizers or advanced neural architectures such as State Space Machines [48] could further accelerate convergence. Beyond the TPV problem, the application of PearSAN to various nanophotonic device design tasks, such as quantum information and sensing, will help elucidate its generality and performance benefits."}, {"title": "PearSOL vs EM", "content": ""}, {"title": "Pearson Correlation vs Energy Matching", "content": "Based on the set of latent vectors that represent the original thermal emitter designs, we use Principal Component Analysis (PCA) to compare Pearson Correlation and EM. Firstly, we determine the nature"}, {"title": "PearSOL vs EM with Affine Parameters", "content": "We test whether adding positive variational affine parameters to the surrogate model, i.e., $h_{\\phi}(z) = \\Phi_{\\alpha}h_{\\phi}(z) + \\Phi_{\\beta}$, helps EM improve its latent optimization performance. The variance of reported FOM dramatically increases without a significant gain in FOM. Comparing affine EM with PearSOL in Figure 5, we see a similar trend. For both EM-affine experiments, one regularized with EM and one with PearSOL, EM either dramatically increases its variance (PearSOL regularized) or the FOM quality suffers (EM regularized). Due to the results in Figure 6, we conclude that it is better for EM to not use variational affine parameters. We expect the poor performance is likely due to increased difficulty of training $\\Phi_{\\alpha}$. The gradient of the other parameters is also proportional to $\\Phi_{\\alpha}$, resulting in greater variance in the gradient making convergence difficult. Even still, fine-tuning $\\Phi_{\\alpha}$ likely remains a challenging task."}, {"title": "Pearson Correlation Isotonicity", "content": "Consider a sample set $\\{(x_1, y_1), ...(x_n, y_n)\\}$ where $x_i, y_i \\in \\mathbb{R}$ given by evaluating the function $f : x \\rightarrow y$. Strict isotonicity is given by $I(x) \\risingdot f(x)$:\n$x_i < x_j \\rightleftarrows f(x_i) < f(x_j)$\n$\\longleftrightarrow x_i < x_j \\rightarrow y_i < y_j,$\t\t (18)\nwhere $I$ is the identity. Given two samples $i, j$, we can determine if strict isotonicity holds with an indicator,\n$m(i, j) = (x_j < x_i \\text{ and } y_j < y_i) \\text{ or } (x_j > x_i \\text{ and } y_j > y_i)$.\t\t (19)\nWe know $f$ is strictly isotonic if the differences along both axes, $(x_i - x_j) = \\Delta x$ and $(y_i - y_j) = \\Delta y$, have the same parity, i.e.,\n$\\Delta x \\Delta y > 0 \\rightleftarrows m(i, j) = \\text{True},$\t\t (20)\nwhich can be seen in Fig. 7. This gives us a natural way to enforce isotonicity through the differentiable function\n$\\gamma(X, Y) = \\frac{1}{N^2} \\sum_{i,j} \\frac{(x_i - x_j)(y_i - y_j)}{\\sigma_X \\sigma_Y}$\t\t (21)\nIf $f$ is variational, then $f$ can learn to be isotonic by maximizing $\\gamma(X, Y)$. However, $\\gamma(X, Y)$ is a pointwise function that scales $O(N^2)$. Instead, we use the Pearson correlation\n$\\rho(X, Y) = \\frac{1}{N} \\sum_{i} \\frac{(x_i - \\overline{X})(y_i - \\overline{Y})}{\\sigma_X \\sigma_Y},$\t\t (22)"}, {"title": "Pearson Correlational Loss", "content": "Within the PearSOL, the Pearson correlation $L_{pearson}$ is input into a supplemental inverse logistic curve:\n$L'_{Pearson} = -log\\bigg(\\frac{0.5(L_{Pearson} + 1)}{1 - 0.5(L_{Pearson} + 1)}\\bigg)$\t\t (24)\nThis choice ensures that the gradient approaches infinity\n$\\lim_{L_{Pxy} \\rightarrow -1} \\nabla L'_{Pearson} = \\infty,$\t\t (25)"}, {"title": "Binary Autoencoder Details", "content": "Because VCA samples a polynomial energy function over a binary space, we consider a binary autoencoder (bAE) to pre-train a decoder over a binary latent space. The bAE encoder $E(x)$ generates a Bernoulli distribution $E(x)_i = P(z_i = 1)$. After sampling $z_i = Bernoulli(E(x)_i)$, a deterministic decoder $D_\\theta(z)$ constructs a new sample $\\hat{x}$. To train the bAE, we use the objective function\n$L_{bAE} = \\lambda_1 ||x - \\hat{x}||^2_2 + \\lambda_2 L_{Perceptual} + \\lambda_3 L_{Reg},$\t\t (27)\nwhich is a linear combination between standard mean-squared-error loss, the perceptual loss $L_{Perceptual}$ [49]\u2077, and either the EM or PearSOL losses $L_{Reg}$ which we now introduce. For more details on our implementation, see Appendix 8.\nThe dataset is symmetric under reflections about the the x and y axes. Hence, we only train on one quadrant of each design and use the reflection symmetry to regenerate the complete designs after sampling. We construct a binary autoencoder with residual blocks [50], attention layers [6], and sigmoid linear units [51]. The encoder utilizes 2D maxpooling operations to halve spatial dimensions and the decoder uses 2D transposed convolutions to double spatial dimensions. Residual blocks are implemented with two convolution operations and one skip connection from the input to the output. Following Vaswani et al. [6], we utilize scaled dot-product attention, implemented as:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V$.\t\t (28)\nEach model contains 4.2 million parameters, not including the surrogate energy function, and the models are trained for 10,000 epochs over the TPV dataset. We utilize Purdue University's Gilbreth cluster, training each model with 12 NVIDIA A30 GPUs in parallel, with each model taking \u2248 15 hours.\nWe find that we can achieve adequate reconstruction quality with a latent space of size 64, whereas [10] utilized a size of 256."}, {"title": "Annealing Implementations", "content": "Alternative to VCA, we implement simulated annealing as an alternative latent optimizer. Starting at initial temperature $T(0) = 1$, we linearly decreasing the temperature to $T(n) = 0$ over $n$ iterations. We randomly initialize starting state $s_t$, and for each annealing iteration, we randomly flip a single bit to produce candidate $s'$, and we calculate the acceptance probability according to the Metropolis-Hastings rule:\n$A(s' \\rightarrow s_t) = \\min(1, \\exp\\{ \\frac{E(s_t) - E(s')}{T(t)} \\}).$\t\t (29)\nIn our comparison to VCA, we set $n = 200$ for both methods and maintain the same linear temperature schedule. For our VCA implementation, we calculate the variational free energy loss by drawing $N_s = 50$ discrete samples from the model probabilities. We transition to the next state after each gradient descent step."}, {"title": "Retraining Procedure Further Details", "content": "In our retraining procedure, we alternate between training the surrogate model to predict FOM from vectors and using VCA to discover new latent vectors, where we calculate their decoded efficiencies and append the vector-efficiency pair to the existing dataset."}]}