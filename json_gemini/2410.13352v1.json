{"title": "LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of the European Court of Human Rights", "authors": ["Odysseas S. Chlapanis", "Dimitrios Galanis", "Ion Androutsopoulos"], "abstract": "We present Legal Argument Reasoning (LAR), a novel task designed to evaluate the legal reasoning capabilities of Large Language Models (LLMs). The task requires selecting the correct next statement (from multiple choice options) in a chain of legal arguments from court proceedings, given the facts of the case. We constructed a dataset (LAR-ECHR) for this task using cases from the European Court of Human Rights (ECHR). We evaluated seven general-purpose LLMs on LAR-ECHR and found that (a) the ranking of the models is aligned with that of LegalBench, an established US-based legal reasoning benchmark, even though LAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more clearly, compared to LegalBench, (c) even the best model (GPT-40) obtains 75.8% accuracy on LAR-ECHR, indicating significant potential for further model improvement. The process followed to construct LAR-ECHR can be replicated with cases from other legal systems.", "sections": [{"title": "Introduction", "content": "The rise of Large Language Models (LLMs) has impacted several sectors, including the legal one. In the United States, LLMs are being integrated into legal research and writing tools designed for both professionals and laypeople. These advances are largely due to the effort of legal experts who contributed significantly in dataset development and manual evaluation (Guha et al., 2023; Magesh et al., 2024). Their involvement, however, is extremely costly, hence methods to construct and evaluate legal benchmarks semi-automatically are required. LegalBench (Guha et al., 2023) is an example of a widely used legal reasoning benchmark. It consists of data for 162 tasks, hand-crafted by legal experts, that evaluate six types of legal reasoning of the US legal system, making it the most reliable dataset of this kind. MMLU-Law, a subset of MMLU (Hendrycks et al., 2021) that contains three US legal tasks only, is also commonly used. Similarly, LawBench (Fei et al., 2023) and IL-TUR (Joshi et al., 2024) were created for the Chinese and Indian regions; they comprise 20 and 8 tasks, respectively. For other legal systems, at least two other large multi-task legal benchmarks have been made available (Chalkidis et al., 2022; Niklaus et al., 2023). However, they include mostly text classification tasks that do not require understanding or generating chains of legal arguments (e.g., court arguments explaining the decisions of judges) and can be solved reasonably well using smaller BERT-based models (Chalkidis et al., 2020) or even linear classifiers (Aletras et al., 2016). Hence, it is questionable if they test legal reasoning abilities.\nresent Motivated by the observations above, we introduce a novel task (\u00a72), Legal Argument Reasoning (LAR), designed to evaluate the legal reasoning skills of LLMs. The task requires selecting"}, {"title": "The LAR task", "content": "We introduce Legal Argument Reasoning (LAR), a novel task to evaluate the legal reasoning abilities of LLMs. The LLM is fed with the facts of the case (a list of sentences summarizing the events considered, see Table 1), a sequence of preceding arguments (statements) from the court proceedings, and continuation options (the correct next statement and distractors). The LLM has to select the correct next statement. (In court proceedings, 'arguments' are numbered statements documenting the legal reasoning of the court.)\nLAR requires various types of legal and commonsense reasoning that extend beyond simple pattern recognition or memorization. As demonstrated by HellaSwag (Zellers et al., 2019), even predicting"}, {"title": "The LAR-ECHR dataset", "content": "The LAR-ECHR dataset contains arguments from the proceedings of the European Court of Human Rights (ECHR). An ECHR court decision typically begins with the facts of the case, followed by the 'Law' section, where the arguments of the parties and the court are presented, followed by the court's conclusion (e.g., verdict, fines). To create the dataset, we used statements from 'Law'.\nTo ensure that LAR-ECHR is challenging and effectively evaluates legal reasoning, we select appropriate arguments based on criteria derived from the annotations of the Legal Argument Mining ECHR (LAM:ECHR) dataset (Habernal et al., 2024) (\u00a73.1). The criteria we use are described in \u00a73.2 below. Instead of generating the distractors using a language model, as in HellaSwag (Zellers et al., 2019) and LegalLens (Bernsohn et al., 2024), we draw them from similar ECHR cases using an algorithm we developed (\u00a73.3), to avoid introducing language model biases and hallucinations.\nLAR-ECHR is based on 191 ECHR court cases. From the 191 cases, we derive 403 samples (like the example of Table 1), which we split randomly into three sets: 5 samples for few-shot prompts, 98 samples for development, 300 samples for testing. In our experiments, we use only the test set, but we release the full dataset for future research. Below we describe in detail how the dataset was created."}, {"title": "The previous LAM:ECHR dataset", "content": "In the aforementioned LAM:ECHR dataset, the arguments of 373 ECHR court decisions were annotated for legal argument mining purposes. The"}, {"title": "Selection of target arguments", "content": "Here we describe the process used to select the target arguments (correct next statements) of the new dataset from the 191 cases of \u00a73.1.\nAs already noted, the 'Law' section of each case contains the arguments of the parties and the court. Actually, a case usually examines multiple issues and the 'Law' section contains the arguments of the parties, followed by the arguments of the court, separately per issue. For each issue, the arguments of the parties (in the court proceedings) are actually also written by the judges, in a way that supports the reasoning of the judges. Therefore, for each issue, the arguments (statements) of both the parties and the court actually form a reasoning chain. From that chain, we wish to focus on the arguments of the judges, especially those annotated as \u2018Application to the concrete case' in LAM:ECHR, which are the most demanding in terms of reasoning, as they consider and combine the arguments of the parties, the law, and the facts of the particular case. Those arguments are \u201cconcerned with determining the relation between the concrete case and the abstract legal norm by the subsumption of the facts of a case under a legal norm\u201d (Habernal et al., 2024). In other words, they are parts of the reasoning that the judges follow to connect the law to the facts by 'subsumption', i.e., checking if the facts meet the conditions specified by the law.\nFurthermore, in our experience, among the arguments of the judges, the first one (per issue) is the most difficult to predict; we leave an experimental validation of this claim for future work (\u00a77). Therefore, we select as target arguments those that satisfy the following criteria: (i) the argument must be annotated as \u2018ECHR' (argument of the judges), (ii) the argument must be annotated as 'Application to the concrete case', and (iii) it must be the first one (per issue) after the arguments of the parties.\nDue to the limited context length of LLMs, in LAR-ECHR the facts of each case are summarized (using GPT-40) and only the last three of the arguments preceding the target one are retained."}, {"title": "Selection of distractors", "content": "Distractors are incorrect next statements, as opposed to the target argument, which is the correct one. Some studies use synthetic distractors generated by LLMs, e.g., HellaSwag (Zellers et al., 2019) and Legalens (Bernsohn et al., 2024). We opt to use arguments from the same dataset as distractors, following the approach in EntailmentBank (Dalvi et al., 2021). This approach avoids the introduction of biases and hallucinations of LLM generators, as reported in the work of HellaSwag.\nThe most suitable distractors are algorithmically selected. The algorithm adheres to the following desiderata. (a) The distractors must be similar to the target argument, i.e., they must have roughly"}, {"title": "Experiments", "content": "We evaluate the reasoning skills of seven general-purpose LLMs using the respective web APIs and three random seeds. We employed closed-weight OpenAI models (GPT family), namely gpt-40 (L), gpt-4o-mini (S) (OpenAI et al., 2024); open-weight models by Mistral (Mistral family), namely open-mixtral-8x22b (L), open-mixtral-8x7b (M), open-mistral-7b (S) (Jiang et al., 2024); and open-weight models by Meta (Llama family), namely 1lama-3.1-70b (L), llama-3.1-8b (S) (Dubey et al., 2024) .\nWe report the average classification accuracy (over the three random seeds) and the standard deviation for each LLM on the test subset of LAR-ECHR. We also show results on two previous legal benchmarks (LegalBench, MMLU-Law) and two general benchmarks (MMLU-full, HellaSwag), as previously reported (Liang et al., 2023).\nMMLU (Hendrycks et al., 2021) is the most widely used benchmark for evaluating the knowledge and reasoning abilities of instruction following LLMs (Liang et al., 2023). MMLU-Law is a subset of MMLU that contains three legal tasks ('International Law', 'Jurisprudence', 'Professional Law'). LegalBench is the largest (in terms of tasks) benchmark for the evaluation of legal reasoning (Magesh et al., 2024). It includes 162 tasks that assess 6 different reasoning types. HellaSwag (Zellers et al., 2019) is a dataset created automatically that only contains the next statement prediction task, similar to LAR-ECHR. However, in HellaSwag the texts are collected from online articles and not chains of legal arguments, as in LAR-ECHR. In the three previous benchmarks that have multiple tasks (MMLU, MMLU-Law, Legal-Bench), we report macro-average over their tasks."}, {"title": "Zero-shot prompting", "content": "All experiments are performed in a zero-shot setting with a Chain-of-Thought (CoT) prompt (Wei et al., 2023) we designed for LAR-ECHR (Table 4). The prompt explains the provided input (facts, preceding arguments, continuation options) and the task, asking the LLM to generate an output in three steps. In the first step (\u2018Analysis'), the LLM reflects on the plausibility of each option (candidate next argument). In the second step ('Explanation'), the LLM explains its choice. In the last step ('Answer'), the LLM outputs only the letter (A, B, C, D) of its choice (to facilitate answer collection)."}, {"title": "Experimental results", "content": "Table 3 presents the accuracy of the models on the five datasets (LAR-ECHR, LegalBench, MMLU-Law, MMLU-Full, HellaSwag). The large version of GPT-40 (L) achieves the best performance on all datasets, with accuracy 75.8% on LAR-ECHR, showing that there is room for model improvement on our dataset. The rankings (in square brackets) of all models on LAR-ECHR are identical to those of LegalBench, even though the two datasets are based on different legal systems (US, EU). Within each family of models, the largest model is the best on all datasets, as expected.\nMistral (L) is the second best model on LAR-ECHR and LegalBench, with a larger performance gap from the best model (6 pp), compared to the corresponding gap on LegalBench (2.5 pp), showing that LAR-ECHR distinguishes better than LegalBench the legal reasoning abilities of the top two models. When comparing models of the same family, LAR-ECHR also distinguishes better between GPT-40 (L) and GPT-40 (S), as shown in"}, {"title": "Related work", "content": "The LAR task was inspired by the continuation task introduced by SWAG (Zellers et al., 2018) and later improved by HellaSwag (Zellers et al., 2019). It is a multiple-choice task where the model has to select the most likely continuation of an event description, such as \u201cA woman sits at a piano\u201d is followed by \"She sets her fingers on the keys\". The corpus is collected from various online sources such as wikiHow. Similarly to LAR, HellaSwag is constructed automatically, via a technique called Adversarial Filtering (AF) which selects the most persuasive LLM-generated continuations as incorrect options. It is shown empirically that accurately predicting the correct continuation of an event in HellaSwag requires skills that are closely related to commonsense reasoning. The primary differences with our work, aside from our focus on the legal domain, are: (a) we employ official content from court proceedings instead of events from online articles of varying credibility, (b) we use (based on the respective annotations) the most appropriate chain of arguments, and (c) we utilize human-generated challenging distractors.\nOur dataset builds on top of two previous works: LAM:ECHR (Habernal et al., 2024) and ECtHR B (Chalkidis et al., 2021). LAM:ECHR annotated, with the help of legal experts, the arguments of 373 ECHR decisions with actor and argument type labels, and trained and evaluated their ROBERTa-based models on both tasks. In ECtHR B the goal is to predict the articles of ECHR that were allegedly violated, given the facts of the case. To create LAR-ECHR we aligned the common instances of"}, {"title": "Conclusion", "content": "In this study, we introduced LAR, a legal reasoning NLP task that requires selecting the correct next argument made by judges in a case. We constructed a dataset for this task, called LAR-ECHR, using cases from ECHR. We evaluated seven general-purpose LLMs from three families on this dataset. The best model obtained 75.8% accuracy, indicating significant potential for further model improvement. Model rankings were identical with those of LegalBench, even though the datasets are based on different legal systems. Despite that weak models obtained a substantially lower score in LegalBench, LAR-ECHR distinguished the top models more clearly. The process followed to construct LAR-ECHR can be replicated with cases from any court proceedings, even from different legal systems."}, {"title": "Future work", "content": "The semi-automatic creation of a LAR dataset requires a few design decisions, two of which we believe are most worth investigating further: (a) the impact of not selecting only the first arguments of the judges (per issue) as target arguments (which in our experience are the most difficult to predict) and (b) the impact of the similarity threshold \u03c4 in selecting candidate distractors.\nAdditionally, we plan to extend the dataset in various directions: (a) collect and align the missing ECHR cases that are annotated from LAM:ECHR, but they do not exist in ECtHR B, (b) include the rest of the articles of ECHR, apart from articles 7 and 8, to cover other domains of legal expertise, (c) annotate more cases to increase the dataset size. These extensions could lead to the inclusion of a training set for fine-tuning LLMs. These LLMs would be either open-source LLMs or smaller BERT-based models that have shown promise in legal reasoning tasks, such as (Chalkidis et al., 2020). Even though these legal-specific models do not exhibit few-shot learning capabilities, they would be ideal baseline models.\nIndependently of this extension, we plan to evaluate more general-purpose, but also legal-specific LLMs, and update the leaderboard of the dataset. It would be insightful to measure the impact of pre-training on the same or other legal systems. To our knowledge, there is currently only one publicly released family of legal LLMs that can follow instructions, Saul-7B (Colombo et al., 2024b), Saul-54B and Saul-141B (Colombo et al., 2024a).\nFinally, the process followed to construct LAR-ECHR could be replicated with cases from other court proceedings to create new LAR datasets that"}, {"title": "Limitations", "content": "One limitation of our work has to do with the process followed to create the dataset. While the data were originally created by humans, the next statement prediction task is artificial. We employed semi-automatic techniques, based on legal expert annotations and embedding similarity of the arguments, to compile a challenging dataset. We also summarized the facts to fit in the context length of all the models. This process might have introduced biases and/or mistakes, as we have already discussed for the summaries of the facts (\u00a7 4.3). The impact of these biases could only be measured by careful examination from legal experts and extensive comparisons with different variations (e.g. summaries from other models).\nFurthermore, it should be noted that, as in many other legal NLP datasets, we are using the 'facts' of ECHR court decisions as if they are the factual information available prior to the final decision. However, due to the details of the legal process and the way that court proceedings are written, this is unrealistic (Medvedeva and Mcbride, 2023). The judges actually publish only the information that is supporting their final decision as the 'facts' of the case; not the original record that they had to consider in that process. To make the task realistic for a real-world application we should include the actual information that the parties had access to before the final judgement took place, but access to this information is very hard to get for most cases."}, {"title": "Ethics Statement", "content": "The primary objective of this research is to advance legal NLP and more specifically the use of LLMs as tools that assist-without replacing-legal professionals. A diverse set of communities can be benefited from our research: (a) the NLP community can challenge existing and future LLMs on an advanced legal reasoning dataset and even build new datasets for other courts, (b) legal practitioners can improve their understanding of the way these models make decisions and (c) the legal tech community can gain useful insights into LLM capabilities across different courts and legal systems, enabling them to design appropriate use cases and develop more accurate tools.\nMost previous work in legal NLP, including"}]}