{"title": "VL-CACHE: SPARSITY AND MODALITY-AWARE\nKV CACHE COMPRESSION FOR VISION-LANGUAGE\nMODEL INFERENCE ACCELERATION", "authors": ["Dezhan Tu", "Danylo Vashchilenko", "Yuzhe Lu", "Panpan Xu"], "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance\nacross a versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts, such\nas images or videos. While existing KV cache compression methods are effec-\ntive for Large Language Models (LLMs), directly migrating them to VLMs yields\nsuboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a\nnovel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention by dis-\ntinguishing visual and text tokens in prefill and decoding phases. Based on these\nobservations, we introduce a layer-adaptive sparsity-aware cache budget alloca-\ntion method that effectively distributes the limited cache budget across different\nlayers, further reducing KV cache size without compromising accuracy. Addi-\ntionally, we develop a modality-aware token scoring policy to better evaluate the\ntoken importance. Empirical results on multiple benchmark datasets demonstrate\nthat retaining only 10% of KV cache achieves accuracy comparable to that with\nfull cache. In a speed benchmark, our method accelerates end-to-end latency of\ngenerating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x,\nwhile reducing the memory footprint of KV cache in GPU by 90%.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision-Language Models (VLMs) have recently emerged as powerful tools for a broad range of\nmulti-modal tasks (Liu et al., 2023b; Chen et al., 2023; Bai et al., 2023). As these models improve\nat processing long visual context \u2013 such as high-resolution images, multiple images and multi-frame\nvideos (Li et al., 2024a) \u2013 the number of visual tokens increase rapidly. Consequently, deploying\nVLMs demands substantial GPU memory capacity, bandwidth, and computational resources, lead-\ning to high inference latency and cost.\nSimilarly to Large Language Models (LLMs) (Chang et al., 2024), VLMs decode tokens sequen-\ntially in an auto-regressive loop. The key and value pairs of the input prompt and of the generated\noutput tokens are stored in GPU memory (where they are known as the KV cache) and reused at\neach decoding step to avoid recomputation. As the context length grows, KV cache not only occu-\npies a larger amount of GPU memory, but also increases inference latency due to data movement\nbetween GPU's high-bandwidth memory (HBM) and its on-chip memory (SRAM) in each decoding"}, {"title": "2 BACKGROUND", "content": "In this section, we detail the inference procedure of the widely adopted VLM architecture with vision\n& language input and text output, where image tokens are projected as soft prompts (Li et al., 2024a;\nTeam et al., 2023; Islam & Moushi, 2024; Bai et al., 2023; Anthropic, 2023). We also introduce the\nformulation of KV cache compression and the approach of existing works (Zhang et al., 2024b;a;\nLiu et al., 2024a; Ge et al., 2023; Tang et al., 2024; Li et al., 2024b; Lee et al., 2024; Yu et al., 2024).\n2.1 VLM INFERENCE\nPrefill phase. The input to VLMs includes both images and language, where images are processed\nby the visual encoder to generate visual tokens. Subsequently, a projection layer, such as a simple\nmulti-layer perceptron, maps these visual tokens to a unified embedding space. Meanwhile the\nlanguage prompt is fed to a tokenizer and embedding layer to create the initial hidden state for\nlanguage tokens. For notation simplicity, we denote a sequence of m prompt tokens, including both\nvisual and language ones, as {x1,..., Xm}. These tokens are processed by the language model in\nparallel to calculate the probability of the first decoded token Po(xm+1|X1,...,xm). Simultaneously,\nthe key vectors ${k^{(1)}_{1}, ..., k^{(1)}_{m}}$ and value vectors ${v^{(1)}_{1}, ..., v^{(1)}_{m}}$ at each transformer layer l are cached\nin GPU memory to avoid recomputation in the next decoding step.\nDecoding phase. Once decoding starts, the language model in VLMs takes effect and generates\none token per step in an auto-regressive loop. At step i, the model receives the token Xm+i as input\nand calculates the probability Po(xm+i+1|X1, ..., Xm+i). Each decoding step involves generating\nnew key vectors $k^{(l)}_{m+i}$ and value vectors $v^{(l)}_{m+i}$, which are appended to the previously cached key-\nvalue pairs for each layer, resulting in ${k^{(1)}_{1}, ..., k^{(1)}_{m+i}}$ and ${v^{(1)}_{1}, ..., v^{(1)}_{m+i}}$. In case of long contexts,\nsuch as multiple or high resolution images, the key-value cache can grow significantly larger than\nthe model parameters and other intermediate tensors, making memory capacity and bandwidth major\nperformance bottlenecks.\n2.2 KV CACHE COMPRESSION\nTo address the bottleneck of storing and accessing the large KV cache during decoding, many re-\nsearchers have focused on KV cache compression to maintain only a subset of the full KV cache for\nmore efficient decoding while minimizing the accuracy loss. There are two main design dimensions\nto such algorithms: how many cache tokens should be kept at each layer, and which tokens to evict\nduring compression."}, {"title": "3 PRELIMINARY EXPERIMENT", "content": "The attention mechanism for visual and language tokens is a key aspect of VLMs. Therefore, fur-\nther optimization of the KV cache compression methods for VLMs requires careful investigation of\nthe attention patterns with relevant input prompts. Motivated by this need, we conducted prelimi-\nnary experiments to explore the VLM attention. We randomly sampled data from three multi-modal\ndatasets DocVQA (Mathew et al., 2021), MathVista (Lu et al., 2023), and Coco-Caption (Chen\net al., 2015). These datasets cover of a wide range of visual tasks, including OCR, visual diagram\nreasoning, and world knowledge. We selected one of the state-of-the-art VLMs \u2014 LLaVA-Mistral-\n7B (Liu et al., 2023b) \u2014 and recorded the attention score matrix until the generation process com-\npletes. Initially, we investigate attention sparsity across layers, and then propose two metrics to\nquantitatively assess how the model attends to visual and language tokens in each layer and evaluate\ndifferent token scoring policies. We conclude our main findings in the end of this section which lead\nto our algorithm design described in Section 4.\n3.1 MEASURING ATTENTION SPARSITY\nIn this section, we measure the sparsity of the attention score matrix in different transformer layers\nduring the prefill and decoding phases. First, we apply a filter with a relative threshold p to the\nattention score matrix A:\nThresholdFilter(A, p)ij = $\\begin{cases} A_{ij} & \\text{if } A_{ij} \\geq p \\cdot \\max_j(A_{ij}) \\\\ 0 & \\text{otherwise} \\end{cases}$\nwhere threshold p\u2208 (0,1) controls the strength of induced sparsification, following Zhang et al.\n(2024b). We also heuristically set p = 1%, such that the filtered-out scores have an insignificant\nimpact on the output of the transformer layer. As will be discussed in Section 4.1, we also need to\nmeasure sparsity at inference time, so we prefer a metric with faster runtime. ThresholdFilter\nis asymptotically faster than alternative methods of truncating a distribution (such as top-p and top-\nk), since it does not require the attention scores to be sorted. After filtration, we calculate sparsity\n\u03b3(1) \u2208 [0, 1] of layer l as count of zero entries, normalized by the size of the lower triangular portion\nof the attention scores matrix:"}, {"title": "4 VL-CACHE METHOD", "content": "Motivated by observations from preliminary experiments, we introduce our method VL-Cache,\nwhich strategically combines sparsity-aware cache budget allocation and modality-aware token scor-\ning policy to improve VLM's performance under limited KV cache budget, in terms of both accuracy\nand efficiency.\nSpecifically, we use Post-vision Attention to compute both inter-layer sparsity and intra-layer token\nimportance. The former guides how many cache tokens should be allocated at each layer, while\nthe latter dictates which k tokens within a layer should be kept due to their importance. A high-\nlevel description of our method is visualized in Figure 4. For brevity, we will use A' to denote the\nPost-vision Attention matrix in this section.\n4.1 SPARSITY-AWARE KV CACHE BUDGET ALLOCATION\nBefore determining the exact tokens to evict, we need to allocate the KV cache budget, which is\nthe percentage of the KV cache to retain at each layer. Based on our observations from Section\n3.1, we implement a sparsity-aware layer-wise KV cache allocation approach with two steps during\nthe prefill phase. First, we apply ThresholdFilter pruning (with p = 1%) to the Post-vision\nAttention scores and calculate the layer-wise sparsity. Second, given a target KV cache budget for\nthe whole model, we distribute this budget across layers based on each layer's sparsity ratio. This\nmethod optimizes the use of limited memory to store an appropriate amount of context information\nin each layer. Note that the allocation occurs only once and right after prefill phase, so the latency\noverhead can be amortized across multiple decoding steps.\n4.2 MODALITY-AWARE TOKEN SCORING POLICY\nAfter we decide the cache budget for layer l, we choose a subset of k(l) cache tokens from the full\ncache. Prior works (that were targeting KV cache compression for LLMs) have explored several\nscoring policies, such as Accumulated Attention in H2O (Zhang et al., 2024b) and Accumulated"}, {"title": "5 EXPERIMENTS", "content": "In our experiments, we evaluate VL-Cache across representative VLMs that can handle image, video\nand language inputs. We use the state-of-the-art open-source LLaVA family, including LLaVA-\nMistral-7B (Liu et al., 2023b) and LLaVA-1.6-34B (Liu et al., 2023a). As shown in Table 1, they all\nshare the same visual model (openai/clip-vit-large-patch14-336 (Radford et al., 2021)) but are fine-\ntuned from different language backbones (e.g., Mistral (Jiang et al., 2023), Nous-Hermes-2-Yi-34B\n(Research, 2023)). Also, the former model uses Grouped Query Attention (GQA) (Ainslie et al.,\n2023), while the latter model uses Multi-Head Attention (MHA) (Vaswani et al., 2017).\nImplementation Details. We use an AWS EC2 P4 instance equipped with 8 A100 40GB GPUs\nfor evaluation. First, we sample three tasks from Imms-eval (Bo Li* & Liu, 2024), including Coco-\nCaption (Chen et al., 2015), DocVQA (Mathew et al., 2021), and MathVista (Lu et al., 2023). These\ntasks are representative, and cover OCR, reasoning, and world knowledge domains. Second, we\ncompare accuracy of our approach against full-cache baselines and previous KV cache sparsification\nmethods including StreamingLLM (Xiao et al., 2023a), H2O (Zhang et al., 2024b) and PyramidKV\n(Zhang et al., 2024a). We apply KV cache sparsification in line with these baselines by retaining the\nmost recent tokens and selecting the Top-K tokens according to their corresponding scoring policies.\nAll baselines are configured with their default settings, except that the KV cache budget is scaled\nproportionally to the prompt length, and the recent token window size is fixed at 10% of this budget\nto enable a fair comparison. Finally, we benchmark latency with varying context lengths and batch\nsizes.\n5.1 ACCURACY EVALUATION\nThe accuracy evaluation results are shown in Figure 5 and Table 2. We report the average accuracy\nscore with KV cache budget varying from 1% to 100% of prompt length. Overall, VL-Cache out-\nperforms other compression methods across a range of KV cache budgets and different language\nmodel backbones.\n5.2 SPEED BENCHMARK\nIn order to show the speed advantage of VL-Cache, we measure the GPU kernel latencies of prefill\nand decoding forward passes with synthetic prompts, following the method in Kwon et al. (2023).\nWe vary the size of the prompt from 1K tokens to 128K tokens to scale our method to a very long\ncontext. Batch sizes vary from 1 to 64 and are static, meaning that all requests get prefilled and\ndecoded concurrently. We assume that the prompt template format remains similar to our accuracy\nbenchmarks, so we use the last 50 tokens of the prompt to determine which tokens to evict from\nthe KV cache. For both prefill and decoding in the baseline, we used default settings from the Hug-\ngingFace implementation \u00b9, including CUDA-based FlashAttention-v2. To optimize performance\nin our VL-Cache, we applied our Triton-based solution for self-attention forward pass, layer-wise\nsparsity evaluation, and modality-aware token scoring, as detailed in the appendix A.3. The speedup\nis calculated as $\\frac{\\text{Baseline latency}}{\\text{VLCache latency}}$\nIn Table 3, we observe that with 50 query tokens for calculating attention statistics, the overhead of\nour method is just 1-6% of the prefill latency. See Appendix A.4 for detailed measurements of the\noverhead and a discussion on how to reduce the overhead of statistics calculation for a large number\nof query tokens. During decoding, we run 99 forward passes for a total of 100 output tokens. We"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose VL-Cache, a novel KV cache compression optimized for VLMs. We\ndiscovered the unique sparsity patterns of visual and language tokens throughout the prefill and\ndecoding phases. With these observations, we introduce a modality-aware token scoring policy and\nsparsity-aware cache budget allocation to reduce KV cache size without accuracy loss. Empirical\nresults on multiple benchmark datasets demonstrate that when maintaining only 10% of the KV\ncache, our method achieves accuracy comparable to the full KV cache and outperforms all existing\nmethods. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens\nby up to 2.33x relative to the full KV cache."}, {"title": "A APPENDIX", "content": "A.1 VISION-LANGUAGE PROMPT TEMPLATE CONSTRUCTION\nConstructing prompt templates in image-based conversations is a common practice for VLMs (Li\net al., 2024a; Team et al., 2023; Islam & Moushi, 2024; Bai et al., 2023; Anthropic, 2023), as it\ninstructs language models to generate more accurate responses. For example, as illustrated in Figure\n7, the input image is processed through a visual encoder and a projection layer to generate an image\nembedding, represented by the <image>. For language input, beyond user input, a prompt template\nis employed. With the appropriate prompt template design, regardless of the original image order\nfrom user input (whether before or after language inputs), there will always be a language-based\ninstruction or question in the post-vision position, providing a strong signal for our VL-Cache to\nevict insignificant visual tokens.\nA.2 EXTENDED RELATED WORKS\nThe KV cache, while essential for transformer-based LLM and VLM family, demands significant\ncomputational resources and memory, limiting inference speed. To address these challenges, re-\nsearchers have explored various KV cache compression techniques, such as KV cache sparsification,\nquantization, or a combination of both.\nKV cache sparsification. Heavy-Hitters (H2O) (Zhang et al., 2024b) employs cumulative attention\nscores to greedily evict unimportant tokens. However, this method tends to accumulate more atten-\ntion on the initial tokens, introducing bias and negatively impacting the identification of key tokens\nduring decoding. ZipCache (He et al., 2024) further normalizes the cumulative attention scores,"}, {"title": "A.3 EFFICIENT IMPLEMENTATION", "content": "In order to discard tokens that received low attention during prefill, we need to calculate 2 statistics\nfrom the attention score matrix during the prefill stage: (1) the average attention score for each token\nin key dimension; and (2) the count of attention scores that are smaller than p% of the maximum\nwithin the query Q dimension.\nRegular attention kernels, such as FlashAttention (Dao, 2023) and PagedAttention (Kwon et al.,\n2023), do not materialize the attention scores in HBM, so we need a new memory-efficient kernel to\ncalculate the attention statistics. Overall, we aim to schedule these operations to ensure that: (1) The\nattention mask is not written to HBM; (2) the QK product is not written to HBM; (3) the attention"}, {"title": "A.4 SPEED BENCHMARK RESULTS", "content": "In this section, we report detailed metrics from the speed benchmark results. We will also describe\nthe speed benchmark methodology a little more. We used Torch Profiler to measure GPU kernel\nlatencies, and then added up the latencies from different kernels to get a total latency of a particular\noperation. This breakdown of latency by GPU kernels enabled us to 1) avoid measuring CPU latency\nthat does not relate to our method, 2) group the latency into different buckets for reporting. Empty\ncells mean that the inference server crashed due to insufficient GPU memory during prefill.\nThe following table presents the cumulative latency of all GPU kernels that are related to computing\nattention statistics from 50 query tokens, eviction scores for all prompt tokens, and copying the KV\ncache tensor after eviction of 90% of the KV cache into contiguous memory. This prefill overhead\nis stated in milliseconds:"}, {"title": "A.5 MEASURING ATTENTION TO VISUAL AND LANGUAGE TOKENS", "content": "As we have established in the Section 3.1, the attention sparsity during decoding can be predicted\nfrom attention scores in prefill phase. To further understand the importance of visual tokens and\nlanguage tokens in the prompt, we examine the division of attention from decoding tokens to these\ntwo modalities of prompt tokens."}, {"title": "Contribution", "content": "We propose Contribution for quantitative analysis and visualization of the layer wise modality-\nspecific attention patterns. Let T be the current sequence length, t be the index of the first decoded\ntoken, and A(1) \u2208 RT\u00d7T be the attention matrix at layer l for one specific head.\nContribution (1)\nmod: $\\frac{1}{T-t+1} \\frac{\\Sigma_{i>t} \\Sigma_{j \\in J_{\\text{mod}}} ThresholdFilter(A^{(l)}, p)_{ij}}{\\Sigma_{i>t} \\Sigma_{j \\in J_{\\text{all}}} ThresholdFilter(A^{(l)}, p)_{ij}}$\nHere, we use Jmod and Jall to denote KV cache indices of one selected modality (language or\nvision) and all modalities respectively in the input prompt. As shown in Figure 8, VLMs allocate\nprimary attention to visual tokens in the first layer, while in the second layer, the contributions of\nvisual and language tokens are comparable. From the third layer onward, language tokens dominate,\nwith a slight increase in visual token contribution in the middle layers. Additionally, we also propose\nCoverage to analyze the ratio of the number of tokens from a specific modality, defined as follows:\nCoverage (1)\nmod $\\frac{1}{T-t+1}\\frac{\\Sigma_{i>t}\\Sigma_{j \\in J_{mod}} TopK(A^{(l)}, k)_{ij}}{\\Sigma_{i>t}\\Sigma_{j \\in J_{all}} TopK(A^{(l)}, k)_{ij}}$\nSpecifically,\nTopK(A, k)ij = $1[A_{ij} \\in \\{ A_{(r-k+1)}, A_{(r-k+2)}, ..., A_{(r)} \\}]$"}]}