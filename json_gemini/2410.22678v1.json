{"title": "Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion", "authors": ["Ji Guo", "Hongwei Li", "Wenbo Jiang", "Guoming Lu"], "abstract": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Networks (CNN) across various computer vision tasks. However, akin to CNN, ViTs are vulnerable to backdoor attacks, where the adversary embeds the backdoor into the victim model, causing it to make wrong predictions about testing samples containing a specific trigger. Existing backdoor attacks against ViTs have the limitation of failing to strike an optimal balance between attack stealthiness and attack effectiveness.\nIn this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB) targeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively erodes pixels in areas of maximal attention gradient, embedding a covert backdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves an optimal balance between attack stealthiness and attack effectiveness, ensuring the trigger remains invisible to human detection while preserving the model's accuracy on clean samples. Extensive experimental evaluations across various ViT architectures and datasets confirm the effectiveness of AGEB, achieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data Accuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated, demonstrating minimal visual discrepancies between the clean and the triggered images.", "sections": [{"title": "I. INTRODUCTION", "content": "Vision Transformers (ViTs) have demonstrated competitive or even superior performance in a diverse array of computer vision tasks, including image classification [1], image generation [2], and object detection [3], outperforming Conventional Neural Networks (CNN). Unlike CNN, which rely on convolutional layers to extract features through hierarchical processing of local image regions, ViTs deconstruct the input image into a flattened patch sequence. These patches are the foundation for feature extraction, leveraging attention mechanisms to focus on and interpret the interrelationships between patches dynamically. This paradigm shift to utilizing attention-based mechanisms for feature extraction marks a significant divergence in methodology between ViTs and CNN, underscoring the innovative approach of ViTs in handling complex visual data.\nBackdoor attacks are well-studied vulnerabilities within CNN, supported by a substantial body of research [4], [5]. Recent studies have illuminated the vulnerability of ViTs to backdoor attacks. Pioneering this field, Subramanya et al. [6] were the first to confirm the vulnerability of ViTs to such threats and propose an inconspicuous trigger by limiting the strength of trigger perturbations. Unfortunately, their approach assumed attacker having access to training data. After that, Yuan et al. [7] developed a patch-wise trigger more effectively seize the model's attention. Advancing this domain, Zheng et al. [8] introduced a novel patch-wise trigger mechanism within TrojViT to enhance the Attack Success Rate (ASR) while minimizing the Trojan's footprint. However, it failed to achieve a truly imperceptible attack. Despite these advancements in designing triggers for ViTs that emphasize resizing triggers to patch-wise dimensions and optimizing them for attention engagement, the critical aspect of trigger stealthiness is often overlooked. Moreover, the patch-wise feature of these triggers, being localized features rather than global ones, makes them susceptible to straightforward defense strategies, such as discarding the patches with the highest attention, thereby significantly undermining their effectiveness.\nIn pursuit of an effective and stealthy backdoor attack mechanism for ViTs, this study introduces an Attention Gradient-Based Erosion Backdoor (AGEB) targeted at ViTs. AGEB capitalizes on the unique attention gradients inherent to pre-trained ViTs to subtly modify the original image. Selectively eroding pixels in regions with the highest attention gradients embed an unobtrusive signal that serves as the trigger.\nUtilizing a morphological erosion process, AGEB ensures that the alterations remain imperceptible to human observers.\nFigure 1 illustrates the subtle yet critical difference between the original and our triggered images, highlighting the strategic erosion of images in areas of intensified attention gradients. This nuanced approach leverages human cognitive biases that prioritize detecting changes in color gradients over absolute values, thus maintaining the trigger's stealthiness. AGEB addresses the limitations associated with localized, patch-wise triggers by adopting a global trigger mechanism. This enhancement improves the method's generalization ability across various ViT architectures, overcoming the challenges of stealthiness and localized trigger limitations previously overlooked in the literature.\nOur contributions can be summarised as follows:\n\u2022 We present a backdoor attack against ViTs via attention gradient-based image erosion, addressing the overlooked issues of trigger stealthiness and localization in prior studies."}, {"title": "II. RELATED WORK", "content": "The Transformer architecture was initially designed for tasks in natural language processing (NLP) [9]. Inspired by the considerable success of the Transformer in NLP, researchers have explored adapting analogous models for computer vision tasks, culminating in the proposition of the Vision Transformers (ViTs) [10].\nThe Vision Transformer restructures images into a sequence of patches, incorporates position embeddings, and appends a class token-resulting in image representations akin to those used in natural language processing. It employs the attention mechanism for feature extraction, which can be mathematically expressed as:\nAttention(Q, K, V) = softmax (\\frac{Q K^{T}}{\\sqrt{D_{k}}})V\nHere, Q denotes the query, K denotes the key, and V denotes the value. The term $D_{k}$ corresponds to the dimensions of the query and the key. In contrast to CNN, ViTs diverge in two fundamental aspects:\n\u2022 Region of feature extraction: ViTs decompose an image into a multitude of patches for feature extraction, whereas CNN typically derive features from each individual pixel.\n\u2022 Method of feature extraction: ViTs leverage an attention mechanism, as delineated by Equation 1, in contrast to the convolutional layers utilized by CNN.\nConsidering the previously discussed aspects, traditional backdoor attacks designed for CNN may be less effective when applied to ViTs. In numerous instances, ViTs demonstrate enhanced robustness against backdoor attacks compared to CNN [6]."}, {"title": "B. Backdoor Attacks", "content": "Backdoor attacks significantly threaten the integrity of Deep Neural Network (DNN) models. Extensive research has been conducted on backdoor attacks targeting CNN. Gu et al. [4] pioneered this investigation by leveraging pixel patches as triggers to create triggered images. Subsequent studies [11]\u2013[15] have explored a variety of trigger mechanisms for CNN. Recent studies [5], [16], [17] have aimed at increasing the stealthiness of attacks by utilizing triggers that are either imperceptible or resemble the input's innate characteristics.\nIn the context of ViTs, backdoor attacks have also been examined. Subramanya et al. [6] demonstrated the vulnerability of ViTs to backdoor attacks using pixel patches, akin to the approach taken by BadNets [4]. Furthering this line of inquiry, Zheng et al. [7] investigated the effects of trigger size and introduced a patch-wise trigger designed to improve the ASR for ViTs. Simultaneously, Zheng et al. [8] employed a patch-wise trigger to capture the model's attention more effectively.\nHowever, despite these advancements, the stealth and global features of triggers for ViTs frequently need to be revised to reach current research scrutiny. Patch-wise triggers, for instance, may be easily mitigated by discarding the most attention-grabbing patch. In contrast, visible triggers risk early detection, leading users to avoid using such data for model training. Even though Subramanya et al. [6] suggested an invisible trigger by constraining the amplitude of perturbations, this assumes that attackers can access the training data. Moreover, the TrojViT strategy [8]even completely omits the stealthiness of the trigger. The discernible feature of the triggered images makes the attack evident, diminishing its effectiveness in real-world scenarios where concealment is critical.\nA critical task in designing backdoor attacks for ViTs is identifying a methodology that effectively balances stealthiness and effectiveness. More specifically, the challenge lies in discovering backdoor strategies for ViTs that do not rely on localized, patch-wise triggers, thereby advancing the subtlety and potential undetectability of the attack."}, {"title": "III. THREAT MODEL AND ATTACK GOAL", "content": "We adopt the threat model consistent with numerous backdoor attacks on CNN as reported in the literature [5]. Our approach involves generating triggered samples, which are mislabeled with the target class, and incorporating them into the original training dataset before releasing it publicly. A victim developer inadvertently introduces a backdoor vulnerability upon using this tampered dataset to train their model. It is important to note that the attacker is presumed to have neither control over the training process nor any knowledge about the specifics of the victim's model. In some studies of ViT backdoor attacks, the threat model [6], [18] assumes the training process or train dataset is available for attackers, which can not be applied in the real world. Our AGEB should have the following goals:\n\u2022 Functionality-preserving. The backdoor model should have high test accuracy of clean samples. The model should have high Clean Date Accuracy (CDA) in a threat model.\n\u2022 Effectiveness. The triggered sample should be classified into the target class. In other words, the model should have a high Attack Success Rate (ASR).\n\u2022 Stealthiness. The triggered sample should be hard to distinguish from the clean sample by human eyes."}, {"title": "IV. METHODOLOGY", "content": "Figure 2 illustrates an overview of our triggered images generation process. The AGEB method is delineated into two distinct phases. Initially, the selection phase determines the pixels to be manipulated, utilizing a mask. This is achieved by evaluating whether the gradient of the last attention layer for each pixel surpasses a predefined threshold. Subsequently, the operation phase encompasses three critical manipulations: erosion of the images, mixing the eroded images with the original images alongside a distinct signal, and refining the images post-operation. For an in-depth elucidation of our approach, refer to Algorithm 1."}, {"title": "B. Pixel Selection", "content": "To determine the pixels subject to erosion, we analyze the gradient of the last attention layer. Furthermore, we posit that different ViT models exhibit analogous attention weights for identical samples, as suggested by Yuan et al. [7]. This similarity in attention weights implies that the attention gradients for the same sample across various ViT models may also exhibit congruence.\nConsider the last attention layer's output for a given pixel position (i, j) in the input sample. Let $G_{i,j}$ denote the gradient of the loss function concerning the attention score for this pixel. Then, based on the chain rule, $G_{i,j}$ can be computed as follows:\n$G_{i,j} = \\frac{\\partial L}{\\partial A_{i,j}} = \\frac{\\partial L}{\\partial a_{i,j}} \\cdot \\frac{\\partial a_{i,j}}{\\partial e_{i,j}}$\nwhere L is the loss function, $A_{i,j}$ is the attention weight for the pixel at position (i, j), and $e_{i,j}$ is the corresponding attention score. The gradient of $A_{i,j}$ concerning $e_{i,j}$ can be calculated as mentioned in the earlier example.\nThis gradient, $G_{i,j}$, is then used to update a mask M of the same size as the input sample, where each pixel's value in M indicates whether the corresponding pixel in the input sample should be eroded or preserved. For instance, one might set a threshold \u03c4 and update M as follows:\n$M_{i,j} = I(G_{i,j} > \\tau)$\nwhere I is the indicator function."}, {"title": "C. Erosion", "content": "Image erosion is a fundamental morphological operation. The basic idea behind erosion is to erode the boundaries of objects of foreground pixels.\nFor binary images, erosion is performed using a structuring element, a small shape, or a template applied to each image pixel. The central pixel is replaced by the minimum value of all the pixels under the structuring element. Mathematically, the erosion on a binary image B by a structuring element S can be defined as:\n$B \\ominus S = {z \\in Z^{2} S_{z} \\subset B}$"}, {"title": "D. Mix and Adjust", "content": "We implement a post-erosion blending strategy to enhance the CDA and ASR. After performing the erosion operation on the images, we blend the eroded images with the original images at a specific ratio. This process is mathematically formulated as follows:\n$I(x,y) = \\alpha \\cdot I_{eroded(x,y)} + (1-\\alpha) \\cdot I_{original(x,y)}$\nwhere $I(x,y)$ represents the pixel value at position (x, y) in the modified image, $I_{eroded(x,y)}$ is the pixel value at the same position in the eroded image, $I_{original(x,y)}$ is the pixel value in the original image, and \u03b1 is the blending ratio.\nTo ensure that the eroded images do not become too insignificant for the model to learn due to their reduced values, we introduce a small bias termed as sign to each eroded image, guaranteeing a lower bound. This can be represented as:\n$I_{eroded(x,y)} = I_{eroded(x,y)} + sign(\\epsilon)$\nwhere \u03b5 is a small positive constant, and $sign(\\cdot)$ ensures the adjustment is consistent with the pixel's original value."}, {"title": "V. EVALUATION", "content": "Our backdoor attack is general for various ViT models and datasets. Without loss of generality, we perform our evaluations over the CIFAR-10 [19], GTSRB [20], ImageNette datasets on ViT-b [10], Deit-t, Deit-s [1] models. All models have the same input dimensions as 3 \u00d7 224 \u00d7 224, and we reshape all input images with this dimension."}, {"title": "B. Evaluation Metrics", "content": "To thoroughly assess the impact of our backdoor attacks, we employ several evaluation metrics that focus on the attack's functionality-preserving capabilities, effectiveness, and stealthiness. These metrics are essential for understanding how the attack affects the model's performance on clean data, the success rate of the attack, and the perceptual similarity to the original images. Specifically, we define the following metrics:\n\u2022 Clean Data Accuracy (CDA): Measures the model's accuracy on a clean dataset, i.e., a dataset not containing any samples with the backdoor trigger.\n\u2022 Attack Success Rate (ASR): Determines the effectiveness of the backdoor attack by measuring the proportion of samples containing the backdoor trigger that are misclassified as the target class by the model."}, {"title": "C. Effectiveness Evaluation", "content": "In the effectiveness evaluation, we demonstrate the efficacy of our approach against backdoor attacks on various datasets.\nAs detailed in Table II, our method consistently achieved high ASR across all models, with the Deit-s model on the CIFAR-10 dataset showing an exemplary ASR of 99.78%. Furthermore, the CDA remained robust, indicating that our method effectively balances attack resistance and data integrity.\nFurther validation is evident from the results presented in Figure 3, which detail the ASR and CDA across different poisoning rates on the CIFAR-10 dataset for other models.\nNotably, for ViT-b with a 5% poisoning rate, our method reaches a peak ASR of 99.85%, while CDA remains robust at 98.63% even with increased poisoning rates, underscoring the precision of our technique. In addition, for ViT-b with a 2% poisoning rate, our method still has an ASR of more than 95% and CDA over 98%. These findings underscore the strategic advantage of applying our gradient-focused erosion, affirming its superiority in enhancing model security against backdoor threats."}, {"title": "D. Ablation Experiment", "content": "Our ablation experiments provide compelling evidence of the effectiveness of our attention gradient-based erosion method. As shown in Table III, directly targeting areas with high attention gradients for erosion significantly outperforms random pixel selection. The latter approach failed to improve the ASR or maintain CDA. However, by introducing a minimal, fixed signal (signal) and combining it with the original image (mix+signal), our attention-based method achieved the best outcomes, with an ASR of 94.78% and a CDA of 98.06%, which is a notable improvement over the baseline.\nFurthermore, the straightforward application of image erosion presents two primary challenges: First, it diminishes vital semantic information encapsulated within the original images, which is detrimental to the model's capacity to extract and learn crucial features of clean samples. Second, the success of erosion is highly dependent on the specific features of the images, such as pixel value similarities, which may lead to ineffective modifications that obstruct the model's learning of the intended trigger's features. To address the first issue, mixing the eroded segments with the untouched original image can retain critical semantic information, enhancing the model's ability to assimilate essential features from clean samples. To overcome the second hurdle, blending a minimal, constant signal ensures the modification's effectiveness across different images, facilitating the model's consistent learning of the trigger's features."}, {"title": "E. Stealthiness Evaluation", "content": "We compare the difference between the original images and the triggered images generated by classic backdoor attacks (see Figure 4). Considering those backdoor attacks [6]\u2013[8] for ViTs even ignore stealthiness, we chose to compare ours with the backdoor attack methods which are designed for CNN and known for stealthiness.\nWe observe a tiny difference between the original images and our triggered image, which we find hard to distinguish. Our method is more subtle."}, {"title": "F. Hyperparameter Selection", "content": "In our backdoor attack experiments, we meticulously chose hyperparameters to subtly balance ASR and CDA while minimizing perturbation visibility (see Figure 5 and Table IV). The decision to target the top 40% of pixels by gradient values, use a kernel size of 3, and limit modifications to a single iteration was informed by our goal to achieve effective attacks with minimal detectability. This strategy ensures that perturbations are impactful and discreet, striking a critical balance for stealthy yet potent backdoor attacks. Our approach demonstrates a nuanced method to degrade model performance on targeted tasks without harming the accuracy of clean data."}, {"title": "VI. CONCLUSION", "content": "This study introduces a novel backdoor attack for ViTs that subtly erodes pixels with the maximum attention gradient as a trigger. The triggered images exhibit only minute differences from their originals, making them exceedingly difficult for human observers to detect. Our comprehensive experiments validate that our method operates effectively across various ViT architectures and datasets, emphasizing the dual benefits of our approach: the triggers' inconspicuous feature and their global feature, which enhance both stealthiness and effectiveness. Besides, if AEGB can work other models like CNNs is also worth further exploration."}]}