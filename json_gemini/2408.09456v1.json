{"title": "In-Memory Learning Automata Architecture using Y-Flash Cell", "authors": ["Omar Ghazal", "Tian Lan", "Shalman Ojukwu", "Komal Krishnamurthy", "Alex Yakovlev", "Rishad Shafik"], "abstract": "The modern implementation of machine learning architectures faces significant challenges due to frequent data transfer between memory and processing units. In-memory computing, primarily through memristor-based analog computing, offers a promising solution to overcome this von Neumann bottleneck. In this technology, data processing and storage are located inside the memory. Here, we introduce a novel approach that utilizes floating-gate Y-Flash memristive devices manufactured with a standard 180 nm CMOS process. These devices offer attractive features, including analog tunability and moderate device-to-device variation; such characteristics are essential for reliable decision-making in ML applications. This paper uses a new machine learning algorithm, the Tsetlin Machine (TM), for in-memory processing architecture. The TM's learning element, Automaton, is mapped into a single Y-Flash cell, where the Automaton's range is transferred into the Y-Flash's conductance scope. Through comprehensive simulations, the proposed hardware implementation of the learning automata, particularly for Tsetlin machines, has demonstrated enhanced scalability and on-edge learning capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "In modern computing systems where the data processing scalability grows alongside the expansion of big data, the Von Neumann bottleneck has become a challenge [1]\u2013[3].\nThe traditional architecture relies on moving data between the memory and processing unit, leading to significant data throughput and energy costs. A new approach called in-memory computing (IMC) addresses these challenges, where data is processed directly within a memory array, eliminating the need for constant data movement between memory and processing units. The IMC has significant advantages, primarily due to the inherent characteristics of the memory cells used in IMC operations, such as having low access time, high cycling endurance, and non-volatility.\nThe emerging nonvolatile memory (NVM) devices, like resistive random-access memory (ReRAM) [3], phase-change memory (PCM) [1], and ferroelectric random-access memory (FeRAM) [2], have attracted attention for use as core devices in IMC systems. Aiming to leverage the integration of nonvolatile memory devices with the IMC can significantly improve the performance of computing systems such as Machine learning (ML) algorithms, where processing complex and large volumes of information immediately and efficiently is critical [1]\u2013[3]. Recent research has explored integrating the ReRAM array with a state-of-the-art ML algorithm, Tsetlin machines (TM) [4], [5]. TM employs a learning Automaton called the Tsetlin Automaton (TA) as its primary learning component, as illustrated in Fig. 1(c) [6]. The TA is responsible for constructing logical propositions that link input-output pairs in classification tasks. The TA behaves similarly to traditional Finite State Machines (FSMs) and incorporates learning mechanisms based on reinforcement learning principles, allowing it to undertake two actions, inclusion or exclusion, based on reward and penalty feedback during training. Upon completion of training, the TA makes a final decision to either include ('1') or exclude ('0') a specific feature. TM forms multiple clauses, each containing a set of TAs generating diverse logical propositions from exact boolean literals (features and their negations). Organized into an architecture of classes, these clauses facilitate the classification algorithm [6], [7].\nThe structure of TM, based on basic propositional logic, enabled its integration into IMC technology, where the final boolean actions of TAs are mapped onto the ReRAM array [4], [5]. However, the challenge lies in mapping the multi-state dynamics of TAs onto memristors' analog tunable resistance levels to establish an efficient, low-power IMC system [8]."}, {"title": "II. Y-FLASH BASED TSETLIN AUTOMATON IMPLEMENTATION", "content": "This section explores the application of encoding the states of TA as conductances utilizing a Y-Flash memristive device. It thoroughly examines the device's characteristics and explains the applied procedural approach for mapping the states of TA into the Y-Flash cell. For this purpose, a compact Y-Flash model is employed, considering both D2D and C2C variations in design [14].\nFig. 1(a) illustrates the Y-Flash device, which consists of two transistors connected in parallel: the read transistor (SR) and the injection transistor (SI). They share a standard drain (D) and a polysilicon floating gate. This device is fabricated using a commercial 180 nm CMOS process [12]. The SR is optimized for low-read voltage operation below 1V, with a longer channel length 0.6\u00b5m and a lower threshold voltage $(V_{th})\\approx 0.3V$. On the other hand, the SI is designed for high-voltage program/erase operations, with a shorter channel length 0.3\u00b5m and a higher $V_{th} \\approx 1.5V$. The Y-Flash device can function as a two-terminal memristor by shortening the two sources, SR and SI, see Fig. 1(b). This allows for simpler addressing, while the three-terminal configuration permits more flexibility in different operation modes, Table I. The behavior of the device's current-voltage $(I_{DS} -V_{DS})$ is shown in Fig. 1(b). Under positive bias conditions $(V_{D} > 0$ and $V_{S} = 0)$, the $(I_{DS} - V_{DS})$ relation exhibits nonlinearity due to drain-source and floating-gate voltage changes. Conversely, when voltage bias is reversed $(V_{D} = 0$ and $V_{S} > 0)$, the current flow is negligible as the floating gate is coupled to the drain, effectively switching off the transistors. This unique I-V behavior, in addition to the low reverse current, allows the device to self-select and minimize sneak-path currents in crossbar arrays without requiring a selector device.\nIn Fig. 2(a, and b), the read currents $(I_{SR})$ of the device are displayed in linear scales. A read voltage $(V_{R} = 2V)$ was applied with a pulse duration of 5ns. The reading was measured after the device was erased to the high conductance state $(HCS\\approx 2.5\u00b5S)$, which resulted in a reading current of $(I_{SR}\\approx 5\u00b5A)$. The device was then programmed to the low conductance state $(LCS\\approx 1nS)$, producing a reading current of $(I_{SR}\\approx 1nA)$.The pulse measurements may be affected by overshooting caused by parasitic capacitors in the device. Fig. 3 shows the control of Y-Flash device states, where precise voltage pulses should applied. A program pulse 5V with a 200\u03bcs width is followed by a $(0 \u2013 2)V$ sweep to read the device state. Successive programming pulses shift the device from an HCS $(\\approx 5\u00b5A)$ readout current to LCS with a $(\\approx 1nA)$. After 40 pulses, the device achieves 41 discrete conductance states. Fig. 3(a, and b) displays the readout I-V curves before and after each programming operation and the conductance measured at $V_{R} = 2V$ after each programming pulse, respectively. Similarly, the erase operation is performed by an erase pulse with an amplitude of 8V and a width of 200 \u03bcs. Fig. 3(c, and d) displays the readout I-V curves before and after each consecutive erase pulse and conductance measured\nat $V_{R} = 2V$ after each erasing pulse. The longer pulses used here mainly speed up the measurement of a complete program cycle. Reducing the program/erase pulse width, like using 10 \u00b5s, can generate over 1000 analog conductance states, providing finer control."}, {"title": "B. Mapping TM Learning Element to Y-Flash Cell", "content": "This section outlines the methodology for mapping the states of the TA onto individual Y-Flash memory cell. Each distinct state of the TA is translated into a unique conductance state (G) level within the Y-Flash cell, leveraging its ability to operate in multiple analog states. The training procedure, illustrated in Fig. 4, involves an efficient approach to adjusting the conductance. Instead of directly modifying the conductance based on instantaneous changes in TA states, an accumulating divergence counter $(\\pm DC)$ is employed. This counter accumulates the TA state differences over multiple training data points, which mitigates the need for frequent writing to the corresponding Y-Flash cell. When the accumulated DC value surpasses a specified positive or negative threshold (\u00b1 15 in this experiment), a programming or erasing pulse is issued to the corresponding TA to update its conductance. Subsequently, the DC counter is reset to zero, ensuring that the Y-Flash device remains synchronized with the TA's learning dynamics.\nTo demonstrate the methodology's effectiveness, the TM is trained using the XOR problem dataset. Fig. 5(a) displays only eight TAs of the TM training set. The decision boundary for the TA was set at 150 states (2N = 300). During the training process, which involved 5000 data points, we recorded the state transitions to capture the complexity of the learning dynamics. Notably, four of the TAs reached the 'include' action. Fig. 5(b) illustrates how our method significantly reduces these transitions to 19 pulses to mimic the TA dynamic transition in the training process, using a pulse width of 0.5 ms, thereby smoothing the mapping of TA state transition behavior. The maximum included TA reached a 2.33 \u00b5S conductance, while the minimum excluded TA reached 23.2 nS. This reduction in complexity is achieved while maintaining consistency with the state transitions of the TA. However, using a higher pulse width can realize the TA dynamic transition but with less granularity, necessitating an increase in the limits of the DC. Balancing pulse width and the DC boundaries will determine the required number of pulses, which is crucial for optimizing energy efficiency and programming accuracy. Our approach highlights the efficiency and effectiveness of the method. The integration of the Y-Flash memory cell to represent the multi-conductance states of the TA mimics complex decision-making behavior within a single device. The analog tunable nature of Y-Flash allows for precise adjustment of conductance levels, effectively capturing the accurate learning dynamics of the TA. Another advantage of this approach is utilizing a blind write method, eliminating the need to verify or fine-tune to update the conductance, thereby ensuring a fast and cost-effective write operation. The mapping procedure demonstrates that the seamless integration of TAs within Y-Flash devices holds significant potential for developing efficient and adaptive computing systems capable of handling diverse real-world tasks."}, {"title": "III. EXPERIMENT AND RESULTS", "content": "In order to evaluate the conductance of Y-Flash memris-tors, we performed the cycling performance test C2C. The cycles are illustrated in Fig. 6(a, and b), using a 200\u03bcs pulse width for erase and program operations for 250 cycles. By applying a set of program pulses, we programmed the device at Vp = 5V from HCS $(I_{SR} > 2\u00b5A)$ measured at $V_{R} = 2V$ to LCS $(I_{SR} < 1nA)$ and then erased it backward VE = 8V. The results indicated that the devices exhibited a range of LCS between $(0.8 - 0.9)nS$ and HCS between $(1 \u2013 1.08)\u03bcS$. Fig. 6(c, and d) show the cycling degradation test through the 250 cycles. The full-time is determined by multiplying the number of pulses by the pulse width. The program time increases in a step-wise manner, reaching a maximum value of 8.6 ms. Similarly, the erase time increases progressively more noticeably, reaching a maximum value of 11.2 ms. The gradual increase in both program and erase times suggests a slight degradation in device performance over repeated cycling. Despite these increases, no significant performance degradation was observed, as the devices switched between the HCS and LCS reliably over all 250 cycles. The Y-Flash memristors demonstrate robust performance with minimal degradation, maintaining reliable conductance switching throughout the testing period. Additionally, we used 100 devices to test D2D variations, as illustrated in Fig. 7.\nFollowing the transition from a target LCS of less than 1 nS to an HCS exceeding 1 \u00b5S, as well as the reverse transition, measurements were taken for both states. The LCS exhibited a range between $(0.77 - 0.99)nS$, while the HCS ranged between $(1.0 - 1.13)\u00b5S$. The mean conductance for the LCS was 0.92 nS with a standard deviation of 0.047 nS, whereas for the HCS, the mean conductance was 1.04 \u00b5S with a standard deviation of 0.027 \u00b5S. These measurements, plotted against the device number, showed some variance, but all devices functioned normally, indicating a high yield for the tested devices. This success can be attributed to the CMOS fabrication flow and the high uniformity of the analogue switching involved. In Table.II, the power calculation was derived based on the 40 conductance states presented in Fig. 3 for both programming and erasing modes, utilizing a pulse duration of 200 \u03bcs to calculate the average energy per mode pulse. Similarly, a pulse duration of 5ns was employed for the reading process, as shown in Fig. 2."}, {"title": "IV. CONCLUSION", "content": "This paper presented a new in-memory computing approach utilizing Y-Flash memristive devices, which address data movement and computational efficiency in machine learning architectures while mitigating the von Neumann bottleneck. By representing the learning automaton of the Tsetlin Machine within a single Y-Flash cell, we demonstrate a scalable and energy-efficient hardware implementation that enhances on-edge learning capabilities. Our findings highlight the potential of the Y-Flash memristor to advance the development of reliable in-memory computing for machine learning implementation. Future research will address reliability challenges in memristor devices and optimize the integration of Tsetlin Machines with Y-Flash technology for broader machine-learning applications."}]}