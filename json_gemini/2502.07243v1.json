{"title": "VEVO: CONTROLLABLE ZERO-SHOT VOICE IMITATION WITH SELF-SUPERVISED DISENTANGLEMENT", "authors": ["Xueyao Zhang", "Xiaohui Zhang", "Kainan Peng", "Zhenyu Tang", "Vimal Manohar", "Yingru Liu", "Jeff Hwang", "Dangna Li", "Yuhao Wang", "Julian Chan", "Yuan Huang", "Zhizheng Wu", "Mingbo Ma"], "abstract": "The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation. However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios. To address these issues, we propose Vevo, a versatile zero-shot voice imitation framework with controllable timbre and style. Vevo operates in two core stages: (1) *Content-Style Modeling*: Given either text or speech's content tokens as input, we utilize an autoregressive transformer to generate the content-style tokens, which is prompted by a style reference; (2) *Acoustic Modeling*: Given the content-style tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference. To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech. Specifically, we adopt VQ-VAE [1] as the tokenizer for the continuous hidden features of HuBERT [2]. We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations. Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, Vevo matches or surpasses existing methods in accent and emotion conversion tasks. Additionally, Vevo's effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.", "sections": [{"title": "1 INTRODUCTION", "content": "The imitation of voice has long been an important issue in the field of speech generation. This includes the imitation of speaker identity [3, 4], the imitation of speaking style such as accent [5, 6] or emotion [7], and a broader concept of voice cloning such as in zero-shot text-to-speech (TTS) task [8]. These techniques have a wide range of applications, including spoken language learning [5, 6, 9], voice anonymization [10], voice assistants [11, 12], and video dubbing [11, 12, 13].\nTo achieve targeted and controllable imitation over various speech attributes, many studies focuses on factorizing speech into multiple sub-spaces [14, 15, 16, 17]. In this work, we follow this idea and decompose speech into three key attributes: linguistic content (what to speak), style (how to speak), and timbre (who speaks). Based on this, we define three zero-shot speech generation tasks (Table 1): (1) *Timbre Imitation*: Given a speech as source, imitate only the timbre of the reference speech while preserving the linguistic content and speaking style. It can be adopted in voice conversion that only spectral aspects of speech are converted [3]. (2) *Style Imitation*: Given a speech as source, imitate only the speaking style of the reference speech while preserving the content and the timbre. It can be adopted in accent conversion [5] and emotion conversion [7]. (3) *Voice Imitation*: Given either a speech (i.e., conversion task) or text (i.e., synthesis task) as source, imitate both the timbre and style of the reference speech while preserving the content. It can be adopted in voice conversion that both spectral and prosodic aspects of speech are converted [3, 4] and zero-shot TTS [8]."}, {"title": "2 RELATED WORK", "content": "Controllable Voice Imitation We focus primarily on how existing works approach the imitation of two key speech attributes: timbre and style. (1) *Imitation of Timbre*: As a crucial aspect of speaker identity, timbre imitation has been extensively explored within the voice conversion (VC) field. Most studies aim to utilize the speaker-agnostic representations such as PPG features [20, 42] or some self-supervised representations [43, 44], and use models including GAN [45, 46], auto-encoder [14, 22], and diffusion models [47, 48] to achieve timbre imitation. (2) *Imitation of Style*: In terms of style imitation, accent and emotion are two widely studied attributes. For conversion tasks (with speech as input), classic approaches often involve learning the conversion between par-allel corpus [9, 19, 20, 21]. Additionally, many studies aim to obtain the style-agnostic features, such as pushing them to be close to textual transcriptions [30, 31, 32, 49]. Besides, leveraging automatic speech recognition (ASR) models can transform conversion tasks into synthesis tasks, allowing the injection of style label's embeddings into TTS models to achieve style imitation [29, 50]. In con-clusion, these existing approaches often rely on annotated data and struggle to achieve zero-shot style imitation. (3) *Imitation of both Timbre and Style*: In VC, some works suggest adopting a sequence-to-sequence formulation [51, 52] or introducing an additional modeling for prosody features [48, 53] to achieve both timbre and style imitation. However, these models still have signif-icant room for improvement in both quality and style imitation. Recent advances in zero-shot TTS have greatly improved voice imitation and cloning. They leverage large-scale in-context learning to mimic all speech attributes of a reference prompt, including timbre and style, with high quality and speaker similarity [11, 13, 16, 17, 26, 33]. Nonetheless, it is challenging to obtain the speech repre-sentations disentangled timbre and style effectively [23, 33], leading to inadequate targeted control of these attributes. For instance, using the existing representations directly for VC tasks will lead to timbre leakage, unless mitigated by timbre perturbation or an additional fine-tuning stage [11, 13].\nDisentangled Speech Representation There are many studies aim to decouple linguistic content, timbre, and style. Existing work on obtaining disentangled speech representations can generally be categorized into several approaches: (1) Knowledge distillation using auxiliary tasks such as ASR, FO prediction, and speaker verification [15, 17, 23], (2) Model architecture design based on information bottlenecks, including careful adjustments to hidden layer dimensions [14, 22] or vector quantization methods like K-means [2, 54, 55] or VQ-VAE [1, 15, 17, 39, 40], and (3) Perturbation of acoustic signals [56, 57, 58]. Besides, existing works also leverage additional learning strategies including adversarial learning [17, 23], comparative learning [23, 59], and mutual information min-imization [40, 60, 61] to enhance disentanglement effectiveness. However, existing work still has two main weaknesses. On one hand, as mentioned earlier, finding suitable representations for down-stream generation tasks that can effectively decouple timbre and style remains quite challenging. On the other hand, how to design voice imitation models that can control specific attributes based on these disentangled speech representations has been scarcely explored."}, {"title": "3 METHODOLOGY", "content": "Motivation To disentangle representations of different speech attributes, we adopt a VQ-VAE tokenizer [1] due to its demonstrated potential in disentangling high-level information within speech"}, {"title": "3.1 VQ-VAE TOKENIZER FOR HUBERT", "content": "Architecture The VQ-VAE consists of three components: Encoder, Vector Quantization (VQ), and Decoder. Formally, given the codebook E = [e1,2,..., \u0435\u043a] whose vocabulary size is K, taking HuBERT hidden features \u00e6 as input, we get the reconstructed after the three modules:\nze(x) = Encoder(x),\nzq(x) = ek, where k = arg min ||ze(x) \u2013 ej ||2,\n                                   j\nx = Decoder(zq(x)),\nwhere zq(x) is the quantized representation (i.e., token) of ze(x) after VQ. The loss function con-sists of the reconstruction loss (whose weight is \u5165) and quantization loss (whose weight is \u03b2):\nL = 1||x \u2212 x||2 + \u1e9e||ze(x) - zq(x)||2.\nNote that there is no real gradient defined for zq(x). We could utilize the straight-through gradient estimator or exponential moving average (EMA) as the optimization algorithm [1]. In this paper, we follow the design in [62, 63] and use the EMA algorithm. We describe the specific module design of VQ-VAE in Appendix B.1. Notably, the VQ-VAE model does not contain any downsampling or upsampling operations, thus preserving the sequence length of the input \u00e6. In other words, for the 50 Hz frame-level HuBERT features [2], we can also get 50 Hz frame-level tokens after VQ.\nAnalysis of the Vocabulary Size of Codebook The quantization of HuBERT hidden features by VQ-VAE can be viewed as a form of lossy compression. Inspired by AutoVC [22], we propose that the vocabulary size of the VQ codebook acts as an information bottleneck. If the input x possesses sufficient speech information, reducing the vocabulary size K from infinity to zero: (1) When K \u2192 \u221e, we consider the bottleneck to be extremely wide, capable of accommodating all information without any loss. (2) As K decreases, more low-level acoustic information begins to be lost, such as spectral features related to timbre or prosodic features related to style. At a certain reduced K, only the highest-level, most abstract information like linguistic content is preserved within x. (3) When K \u2192 0, the bottleneck becomes exceedingly narrow, filtering out even high-level information like linguistic content. We validate the above hypothesis through experiments on the zero-shot timbre imitation task (Section 4.1). Interestingly, as we progressively reduce K, we observe that timbre information is the first to be filtered out (assuming when K = K\u2084), from which we derive the content-style tokens. Subsequently, most style information is filtered, and ultimately, almost only the highest-level linguistic content information is retained (assuming when K = Kc), from which we derive the content tokens. We refer to the VQ-VAE model whose K = K, as the content-style tokenizer Qs, and the model whose K = K as the content tokenizer Qc."}, {"title": "3.2 CONTENT-STYLE MODELING (CONTENT TO CONTENT-STYLE)", "content": "During the content-style modeling stage, our goal is to transform the content token of speech (or text) into content-style tokens, which is prompted by a style reference. This can be formulated as a sequence-to-sequence generation task. For this stage, we employ a decoder-only autoregressive (AR) transformer, known for its powerful capability in such tasks [11, 34, 35]. In this section, we will focus only on cases where speech's content tokens are used as input (Figure 2). The scenarios where text serves as input will be discussed in Appendix B.3.\nDuration Reduction Given a speech input u, we denote the content and content-style tokens as Qc(u) and Qs(u). Both of them are 50 Hz frame-level representations of equal length. In the content-style modeling stage, Qs(u) is used as the output. However, instead of using Qc(u), we apply a Duration Reduction strategy to it, yielding the reduced Q\u00b4(u) as the input. Specifically, we merge the consecutive duplicate units of Qc(u) into one. For instance, if Q(u) = [e1, e1, E1, E2, \u0435\u0437, e3], it will be condensed to Q\u00b4(u) = [e1, e2, e3]. This strategy offers significant"}, {"title": "3.3 ACOUSTIC MODELING (CONTENT-STYLE TO ACOUSTIC)", "content": "During the acoustic modeling stage, prompted by a timbre reference, we aim to transform the content-style tokens to Mel spectrograms. We adopt a flow matching transformer [34, 35, 36] (Figure 3), which has been verified to be effective in in-context learning and reconstructing high-quality acoustic representations [12, 24, 27, 38].\nDuring training, given a speech u and its Mel spectrogram y1, we randomly select a part of y1 as the timbre reference (denoted as y\u00a3tx), and aim to reconstruct the other part (denoted as ymis) condi-tioned on y\u00a3tx and the content-style tokens Qs(u). In other words, we aim to model the conditional probability p(ymis|y\u00a3tx, Qs(u)). Specifically, we follow Voicebox [27] and use a temporal span masking strategy: ymis = m \u2299 y1, and ytx = (1 \u2212 m) \u2299 y1, where m is a binary temporal mask that is of the same length as y1, and \u2299 means the element-wise multiplying operation. During inference, given a source speech u\u017c and a timbre reference utr, all the source's Mel spectrogram will be masked (i.e., ymis). The input conditions become the timbre reference's Mel spectrogram (i.e., y\u00a3tr) and the concatenated content-style tokens Qs(Ui\u2295 Utr). This enables the generated target to preserve the linguistic content and style of ui, and the timbre of utr (Figure 3b)."}, {"title": "3.4 VEVO FOR VARIOUS ZERO-SHOT IMITATION TASKS", "content": "Assume that during the content-style modeling and acoustic modeling stages, we have obtained pre-trained models Mstyle and Macoustic respectively. We can then adjust only the inference pipeline to apply Vevo to various zero-shot imitation tasks. Given the source speech ui (or text Ti) and the reference ur, we can utilize the following variants of Vevo to achieve zero-shot timbre, style, and voice imitation tasks (\u201c \u21e8 M\u201d means that the model M is prompted by u to generate):\n\u2022 Vevo-Timbre for timbre imitation: Qs(Ui) ur Macoustic\n\u2022 Vevo-Style for style Imitation: Q(Ui) Ur Mstyle Ui Macoustic\n\u2022 Vevo-Voice for voice imitation (conversion task): Q(uz) ur Mstyle Ur Macoustic\n\u2022 Vevo-TTS for voice imitation (synthesis task): Q(Ti) Ur Mstyle Ur Macoustic\nFor Vevo-TTS, Q(Ti) means the tokenization for Ti, and Mstyle means the pre-trained model for content-style modeling that takes text as input. We describe its detailed design in Appendix B.3."}, {"title": "4 EXPERIMENTS", "content": "Training Data We train the English-only models on 60K hours of ASR-transcribed English au-diobooks, which is the same as the dataset used by the Voicebox English model [27]. The model Macoustic and Mstyle are trained solely with speech data. The model Mstyle, which uses text as in-put, is trained with both speech and textual transcriptions data. We begin with the publicly available HuBERT-Large model [2] to prepare the VQ-VAE tokenizer. We utilize its hidden features from"}, {"title": "5 CONCLUSION", "content": "We introduce Vevo, a versatile zero-shot voice imitation framework featuring controllable timbre and style. Vevo contains of two primary stages: content-style modeling via an autoregressive trans-former, and acoustic modeling via a flow matching transformer. Both stages are trainable through self-supervised and in-context learning, friendly to scale up. Vevo operates based on our newly proposed content and content-style tokens, generated by VQ-VAE tokenizers of HuBERT with care-fully adjusted vocabulary sizes. Pre-trained only on 60K hours of audiobook speech data without fine-tuning on style-specific corpus, Vevo outperforms state-of-the-art models of accent and emo-tion conversion fields, particularly achieving these conversions in a zero-shot manner. Furthermore, Vevo's robust performance in zero-shot voice conversion and text-to-speech tasks underscores its versatility and also highlights the broad potential of our proposed disentangled speech tokens."}, {"title": "A TERMINOLOGY CLARIFICATION", "content": "In this study, we decouple speech into linguistic content (what to speak), timbre (who speaks), and style (how to speak). Below, we will clarify our definitions and scope for timbre and style.\nTimbre Timbre is a physical concept that refers to the acoustic qualities of sound, such as the spectral envelope, which allows us to differentiate between speakers even when pitch and loudness are identical. It is primarily determined by the speaker's vocal anatomy and articulatory behaviors. Often discussed alongside timbre is speaker identity. Speaker identity is a perceptual concept \u2013 it encompasses not only timbre but also habitual speech patterns, idiosyncrasies, and other personal styles that make a speaker recognizable. While timbre lays the acoustic foundation of identity, speaker identity reflects the broader auditory impression formed by a listener.\nStyle Style refers to the expressive aspects of speech, including accent, emotion, and speaking habits, which dictate how something is said. It includes specific features such as accent and emotion, but also covers a wider array of expressive behaviors. A critical component of style is prosody, which includes features such as F0 (pitch), energy, and duration. These prosodic features govern the rhythm, stress, and intonation of speech, contributing significantly to how emotion and emphasis are conveyed. Although style encompasses prosody, it also extends beyond it, influencing not only the melodic flow of speech but also cultural and emotional expressions."}, {"title": "B DETAILS OF VEVO", "content": ""}, {"title": "B.1 VQ-VAE ARCHITECTURE", "content": "We adopt the implementation of RepCodec [62] as our VQ-VAE tokenizer, whose A and B are 45 and 1. Its architecture of encoder and decoder is shown in Figure 4. The vocabulary sizes of our content and content-style tokenizer are 32 and 4096. Their parameter counts are 59M and 63M, respectively."}, {"title": "B.2 CONTENT-STYLE MODELING", "content": "For the content-style modeling stage, we use reference-style-enhanced continuation by default. The architecture of our AR transformer is similar to LLaMA7 [35]. It has 12 layers, 16 attention heads, 2048/3072 embedding/feed-forward network (FFN) dimension. The global style encoder consists of WavLM-based representation layers and TDNN-based feature extraction layers [64, 65]. Specif-"}, {"title": "B.3 CONTENT-STYLE MODELING (TEXT AS INPUT)", "content": "Compared to Mstyle, the only difference of Mstyle is that its input becomes text tokens, rather than the duration reduced content tokens. Specifically, we adopt the Grapheme-to-Phoneme (G2P) method and use the same phonemization tokenizer as Voicebox [27]. All the hyper parameters of training and inference are same as Mstyle."}, {"title": "B.4 ACOUSTIC MODELING", "content": "For the acoustic modeling stage, we follow the flow matching model implementation of Voice-box [27]. Specifically, we randomly mask 70%-100% of the frames to create ymis. We employ the midpoint ODE solver with a step size of 0.0625 (NFE=32). The \u03c3 of the optimal transport path of flow matching is le-5. The transformer has 24 layers, 16 attention heads, 1024/4096 embedding/feed-forward network (FFN) dimension. Its parameter count is 334M.\nOur target Mel spectrogram is at 24 kHz with 100 Mel bands. It is normalized with the global mean (-5.8843) and standard deviation (2.2615) to stabilize training [27]. During training, Mel spectrogram length is capped at 1,600 frames and chunked randomly if length exceeds. We use Adam [81] optimizer with a peak learning rate of 1e-4, linearly warmed up for 5K steps and decays over the rest of training. It is trained for 500K updates. During inference, we employ the midpoint ODE solver with a step size of 0.0625 (NFE=32).\nWe apply the classifier free guidance (CFG) [82] to improve the generation quality like other works [12, 24, 27]. Specifically, we randomly drop the conditions, i.e., ytx and Qs (u), with a prob-ability of Puncond. During inference, the modified vector filed f\u00e9 becomes ft (yt, t, y\u00a3tx, Qs(u)) = (1+x) ft(yt, t, y\u00a3tx, Qs(u)) \u2013 aft(yt, t), where a is the strength of the guidance. In practice, we follow Voicebox and set Puncond as 0.2 and a as 0.7."}, {"title": "B.5 VOCODER", "content": "We use BigVGAN [67] as vocoder to synthesis waveform from Mel spectrogram. We fine-tune from the official released checkpoint bigvgan_24khz-100band using our 60K hours training data. Its parameter count is 112M."}, {"title": "C DETAILS OF BASELINES", "content": ""}, {"title": "C.1 ZERO-SHOT TIMBRE IMITATION AND VOICE IMITATION (CONVERSION TASK)", "content": "\u2022 HierSpeech++ [53]: It utilizes MMS [83] (pretrained on 500K hours of data from over 1000 languages) to extract content features. It is designed based on the VITS architecture [84], and is trained on 2.8k hours sourced from Libri-light [74] and LibriTTS [85]. We use the officially released checkpoint10 to generate samples.\n\u2022 LM-VC [52]: It is an autoregressive hierarchical transformer that predicts SoundStream [63] codecs from soft units similar to HuBERT k-means tokens [54], trained on the Libri-light dataset [74]. We obtain the generated samples from the authors.\n\u2022 UniAudio [28]: It is an autoregressive transformer capable of performing multiple audio gen-eration tasks, using 500-cluster K-means tokens from HuBERT-base (that is pre-trained on Lib-riSpeech [75]) to predict their proposed acoustic codecs, with training data comprising approx-imately 80K hours of speech and 20K hours of other audio data. We use the officially released checkpoint\u00b9\u00b9 to generate samples.\n\u2022 FACodec [17]: It adopts an auto-encoder and residual vector quantization based architecture. It decouples the raw waveform into factorized attributes through ASR, FO prediction, and speaker classification tasks, trained on the Libri-light dataset [74]. We use the released checkpoint in Amphion 12 [76, 78] (which is implemented by the authors) to generate samples."}, {"title": "C.2 ZERO-SHOT STYLE IMITATION", "content": "\u2022 ASR-AC [29]: It uses an ASR model based on wav2vec 2.013 [86] (that is pre-trained on 60K hours of Libri-light [74] and fine-tuned on 1K hours of LibriSpeech [75]) to extract the one-hot text predictions from speech, i.e., xasr in our paper (Section 4.1). It adopts a transformer encoder and a HiFi-GAN decoder to reconstruct waveforms conditioned on xasr, the accent labels, and F0, which is trained on about 700 hours of accented corpus. We use 30 samples from its demo website14 to evaluate, including English accents' conversions from British to American, British to Hindi, and Hindi to American.\n\u2022 VoiceShop [20]: To achieve accent conversion, the authors first uses an conformer-based ASR model (that is trained by 40K hours of their private corpus) to extract the hidden features (BNF). Then, they create about 300 hours of parallel conversion corpus based on a commercial accented TTS system. Finally, they adopt an encoder-decoder transformer to learn the BNF's mapping between parallel corpus. We use 17 samples from its demo website15 to evaluate, including English accents' conversions among American, British, Hindi, and Mandarin.\n\u2022 Conv-Speak [21]: The authors formulate accent conversion from source's content tokens to tar-get's content tokens. They propose to self-supervised pre-train on content tokens like BART [87], in order to relieve the requirements of parallel data. They adopt the 500-cluster K-means of HuBERT-Base16 (that is pre-trained on 1K hours of LibriSpeech [75]) as content tokens. The conversion model is trained on about 600 hours of data, including about 1 hour of parallel data. We use 24 samples from its demo website17 to evaluate, including English accents' conversions from Hindi and Mandarin to American.\n\u2022 Emovox [30]: To achieve emotion conversion, the authors design a recognition encoder to push its output (i.e., emotion-agnostic features) closely with phoneme transcriptions. The conversion model is based on a sequence-to-sequence decoder, that can reconstruct the Mel spectrogram conditioned on the emotion-agnostic features and emotion labels. The model is trained on about"}, {"title": "C.3 ZERO-SHOT VOICE IMITATION (SYNTHESIS TASK)", "content": "\u2022 VALL-E [26]: It is a classic AR model for zero-shot TTS. It utilizes the transformer to pre-dict EnCodec [89] codecs. We use the released checkpoint in Amphion 19 [76, 78] to generate samples, which is pre-trained on 45K hours of MLS English set [90].\n\u2022 Voicebox [27]: It applies the flow matching transformer to both duration model and acoustic model. We reproduce it with the help of the authors.\n\u2022 VoiceCraft [77]: It uses an AR transformer to predict EnCodec [89] codecs. Compared to VALL-E, it proposes token rearrangement and delayed stacking strategies to enhance the model learning. We use the officially released checkpoint20 to generate samples, which is pre-trained on 10K hours of Gigaspeech [91].\n\u2022 CosyVoice [24]: It proposes a semantic tokenizer that is supervised by ASR task. It contains an AR transformer to predict the semantic tokens from text, and a flow-matching transformer to predict Mel spectrograms. We use the officially released checkpoint21 to generate samples, which is pre-trained on 171K hours of in-the-wild, multilingual, and private data.\n\u2022 MaskGCT [13]: It consists of two-stage discrete diffusion models. It is based on the hidden features of w2v-bert 2.022 that pre-trained on 4.5M hours to obtain the semantic tokens. Its TTS model is trained on 100K hours of in-the-wild and multilingual data [79]. We use the released checkpoint in Amphion 23 [76, 78] to generate samples."}, {"title": "D ADDITIONAL EXPERIMENTAL RESULTS", "content": ""}, {"title": "D.1 EFFECT OF THE VOCABULARY SIZE OF THE VQ-VAE TOKENIZER", "content": "In Section 4.1, we have already demonstrated the impact of different vocabulary sizes in the VQ-VAE codebook on Macoustic (i.e., disentanglement capability). In this section, we aim to present two complementary experimental results. First, we explore the effects of a wider range of vocabulary sizes (from the smallest at 8 to the largest at 16,384) on the produced tokens. Second, we investigate the impact of various combinations of Ke and Ks on Vevo-Voice."}, {"title": "D.1.1 EFFECT ON PHONETIC DISCRIMINABILITY", "content": "We explore the phonetic discriminability of different representations, inspired by AudioLM [92]. Specifically, we measure phonetic discriminability using the ABX error rate, a distance-based metric that evaluates a set of phoneme trigrams differing only in the central phoneme (e.g., \u201cbit\u201d vs. \u201cbet\u201d). The ABX error rate assesses how often a random instance X of a trigram (\"bit\") is closer to an instance B of another trigram (\u201cbet\u201d) rather than to another instance A of the same trigram (\u201cbit\u201d)."}, {"title": "D.1.2 EFFECT ON VEVO-VOICE", "content": "We explore the effects of different (Kc, Ks) combinations on Vevo-Voice, with the results presented in Table 8. Our observations include: (1) A significant drop in intelligibility occurs when Ke changes from 32 to 16, indicating that a smaller vocabulary size for the content tokenizer leads to loss of linguistic content information; (2) When K, decreases from 4096 to 1024, all metrics decline. We hypothesize that while a reduction in K, might lessen the learning difficulty for Mstyle, a smaller K also results in a decrease in the quality of the final generated audio for Macoustic."}, {"title": "D.2 ZERO-SHOT VOICE IMITATION (SYNTHESIS TASK)", "content": "In Section 4.4, we present the performance of Vevo-TTS in zero-shot imitation (synthesis) tasks. We have detailed its comparative performance against baselines on all four evaluation sets (AB, CV, ACCENT, and EMOTION) in Table 9. We can observe that: (1) In comparison with AR baselines, Vevo-TTS exhibits a clear advantage over VALL-E and VoiceCraft across various metrics on all datasets. Compared to the state-of-the-art CosyVoice, although Vevo-TTS is trained solely on 60K hours of Audiobook data, it performs better in some metrics such as Naturalness CMOS (AB, AC-CENT, EMOTION), Speaker S-MOS (EMOTION), and notably in style imitation-related metrics like Accent S-MOS and Emotion S-MOS. This demonstrates the high effectiveness of the AR TTS model implemented using the content-style tokens proposed in this paper. (2) When compared with Non-AR baselines, Vevo-TTS falls short on WER across all datasets compared to Voicebox and MaskGCT. This underscores the stability still needed in AR models, indicating significant room for improvement."}, {"title": "E SUBJECTIVE EVALUATION", "content": ""}, {"title": "E.1 BACKGROUND OF SUBJECTS", "content": "We hired dozens of subjects on a paid basis to complete the subjective evaluations. These individuals have extensive experience in providing subjective assessments of audio generated by AI models. They have lived in English-speaking countries for extended periods and are highly familiar with various common English accents, including American, British, Hindi, and Mandarin. Each audio sample in our evaluation was rated at least ten times."}, {"title": "E.2 METRICS AND QUESTIONNAIRES", "content": "We have developed an automated subjective evaluation interface. For each item to be evaluated, users will see three components: the System Interface (i.e., the audio to be evaluated), the Question-naire, and the Scoring Criteria."}, {"title": "E.2.1 NATURALNESS MOS", "content": "System Interface One audio to be evaluated (with target text)\nQuestionnaire How human-like is the speech in the clip? Does it sound like a real human who is engaged in the topic, or does it sound like an AI that doesn't understand what is being said?\nScoring criteria 5 (A perfect imitation of human speech), 4 (Exceeds my expectations for AI voices), 3 (Meets my expectations for AI voices), 2 (A subpar representation of human speech), 1 (Very poor artificial speech)"}, {"title": "E.2.2 SPEAKER SIMILARITY MOS", "content": "System Interface One reference audio, One audio to be evaluated\nQuestionnaire Ignore the content and audio quality, just pay attention to the voice of the person. How similar is the voice to be evaluated compared to the reference voice?\nScoring criteria 5 (Excellent, sounds like exactly the same person), 4 (Good, sounds like a sim-ilar person), 3 (Fair, sounds like a slightly similar person), 2 (Poor, sounds like a different person mostly), 1 (Bad, sounds like a completely different person)"}, {"title": "E.2.3 ACCENT SIMILARITY MOS", "content": "System Interface One reference audio, One audio to be evaluated\nQuestionnaire Ignore the vocal characteristics (who is speaking), just pay attention to the accent of the speaker. Is the accent similar to the reference voice?"}, {"title": "\u0415.2.4 \u0415\u043cOTION SIMILARITY MOS", "content": "System Interface One reference audio, One audio to be evaluated\nQuestionnaire Ignore the vocal characteristics (who is speaking), just pay attention to the emotion of the speaker. Is the emotion similar to the reference voice?"}, {"title": "F ETHICS STATEMENT", "content": "As with other powerful new AI innovations, we recognize this technology brings the potential for misuse and unintended harm. We will build a highly effective classifier that can distinguish between authentic speech and audio generated with Vevo to mitigate these possible future risks."}]}