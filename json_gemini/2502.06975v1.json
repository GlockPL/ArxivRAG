{"title": "Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents", "authors": ["Mathis Pink", "Qinyuan Wu", "Vy Ai Vo", "Javier Turek", "Jianing Mu", "Alexander Huth", "Mariya Toneva"], "abstract": "As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continuous learning and long-term knowledge retention. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present a framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already covering portions of these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are rapidly expanding beyond their origins as text-completion engines. Instead, they are evolving into agentic systems capable of taking meaningful actions in complex environments (Xi et al., 2023). This transformation can enable a range of real-world applications, including autonomous research assistance (Schmidgall et al., 2025), aiding in literature reviews, data analysis, and hypothesis generation; personalized customer support (Li et al., 2024b), where they can recall prior interactions to provide consistent and tailored assistance; and interactive tutoring systems (Lin et al., 2023), which track learning progress, and revisit challenging concepts to ensure effective and personalized education. These diverse applications hint at a vast potential of LLMs to enable intelligent agents capable of meaningful and context-sensitive interaction.\nOperating and reasoning over extended timescales in dynamic interactive contexts demands that an agent not only recalls what happened, but also when, how, why, and involving whom. Such rich traces of past events, motivations, and outcomes form the basis of context-sensitive behavior-especially crucial in large-scale projects involving human stakeholders and multiple actors. For example, a future long-term LLM agent that is supposed to assist in the ongoing development of a massive software project such as Linux-which has spanned decades, encompasses over 40 million lines of code, and additionally involves countless past contributions, issues, comments, notes, and feature requests would need to continuously integrate and reason about a vast, evolving historical context while adapting to new requirements. Core necessities for this kind of system are constant computational cost per new token and a stable or improving performance over time.\nOngoing research directions attack the problem of long-term retention and adaptation from different angles and have made impressive progress. However, we are still lacking approaches that maintain relevant contextualized information over long time frames at a constant cost without degrading performance\u2014necessities for a widespread adoption of LLM agents in many long-term settings.\nMeanwhile, many biological systems solve the demands for acting in a continually evolving environment with a dedicated memory system that allows for both fast and slow learning: episodic memory (McClelland et al., 1995; Schwartz & Evans, 2001; O'Reilly & Norman, 2002; O'Reilly et al., 2014; Kumaran et al., 2016; Liao & Losonczy, 2024). In this position paper, we argue that the growing demand for LLM agents to operate effectively over extended timescales, alongside ongoing advances in long-context models, external memory systems, and efficient fine-tuning methods, makes episodic memory a timely framework to unify efforts for enabling truly long-term LLM agents.\nTo lay out the argument for this position, we proceed as follows: In Section 2, we operationalize the concept of episodic memory for LLM agents by highlighting five key properties that distinguish it from other biological types of memory that are also desirable for LLM agents. We proceed by showing how various existing approaches to improve\nLLM memory target different properties that are united in episodic memory. In Section 4, we highlight how unifying these threads under a common goal can spur more holistic progress and outline a roadmap toward implementing episodic memory. Lastly, in Section 5, we discuss alternative views under which episodic memory would not be necessary for long-term LLM agents."}, {"title": "2. Operationalizing EM for LLMs", "content": "To transfer the concept of episodic memory from cognitive science to the context of LLM agents, we highlight five properties of episodic memory that are useful for LLM agents, and that distinguish episodic memory from other memory types in animals and humans. These five properties naturally cluster into two categories: properties that concern the way that the system operates with the memory\u2014namely, long-term storage, explicit reasoning, and single-shot learning, and properties that concern the content of the stored memory-namely, instance-specific and contextualized memories. We first discuss how the combination of these five properties distinguishes episodic memory from other types of memories in animals and humans, and then detail each property and its utility for LLM agents."}, {"title": "2.1. Unique Combination of EM Properties", "content": "Episodic memory is one of multiple memory systems that exist in animals and humans, distinguished by its unique combination of properties. Other biological memory\nsystems that share some, but not all, properties of episodic memory are 1) procedural memory (Milner, 1962; Cohen & Squire, 1980), which allows for long-term storage of memories for implicit operations or task behaviors, such as producing a sequence of a particular type, rather than reasoning about the sequence; 2) semantic memory (Collins & Quillian, 1969; Tulving, 1972), which allows for long-term storage of factual knowledge and explicit reasoning with these stored memories, but lacks specificity to single instances of acquired information and its context; and 3) working memory (Baddeley, 1986; Baddeley & Hitch, 1974), which can share many of the highlighted properties of episodic memory except for the important fact that it does not allow for long-term storage. The unique combination of important properties in episodic memory makes it a promising candidate for translation to AI systems."}, {"title": "2.2. Importance of EM Properties for LLM Agents", "content": ""}, {"title": "2.2.1. EPISODIC MEMORY OPERATIONS", "content": "Long-term storage. In humans and other animals, episodic memory functions as a form of long-term memory, capable of storing knowledge throughout an individual's lifetime (Conway, 2001; Mayes & Roberts, 2001; Squire & Zola, 1996; Hampton & Schwartz, 2004). This distinguishes it from working memory, which is transient. For LLM agents, an effective episodic memory system must similarly support memory retrieval across any number of tokens. This requires mechanisms for long-term memory that maintain an agent's performance throughout a continual interaction with an environment. An adaptive long-term agent should not only prevent a degradation in performance over time-it should also be able to improve by learning new general knowledge and skills.\nExplicit reasoning. In classical theories of human memory, episodic memory is described as a subset of declarative or explicit memory (Squire & Zola, 1996; Hampton & Schwartz, 2004). A defining feature of explicit memory is the ability to reflect and reason about the memory content. In the context of LLM agents, the explicitness of memory is necessary as agents need to be able to answer direct queries about stored information or use this information in explicit internal reasoning processes.\nSingle-shot learning. A key characteristic of episodic memory, as emphasized in complementary learning systems theory, is its ability to be acquired based on a single exposure (Liao & Losonczy, 2024; Schwartz & Evans, 2001; O'Reilly & Norman, 2002; O'Reilly et al., 2014; McClelland et al., 1995; Kumaran et al., 2016; Das et al., 2024b). This fast learning enables the rapid encoding of unique experiences or events. For LLM agents, this capability is particularly crucial in environments where continual deployment may not provide multiple variations or repetitions of specific events. Certain occurrences in an environment may happen only once, necessitating an episodic memory system that is capable of effectively capturing and utilizing information from single exposures."}, {"title": "2.2.2. EPISODIC MEMORY CONTENT", "content": "Instance-specific memories. Episodic memory stores information specific to an individual sequence of events along with their distinct temporal contexts (Sugar & Moser, 2019; Colgin et al., 2008). This specificity allows episodic memory to capture details unique to a particular occurrence, enabling its application in agentic environments where reasoning about specific past actions and their consequences matters. This can include past lines of reasoning that were associated with a decision to be made by an LLM agent.\nContextual memories. Episodic memory binds context to its memory content, such as when, where, and why an event was encountered (Eichenbaum & Cohen, 2014; O'Keefe & Nadel, 1978; Eichenbaum, 2015). The ability to store many contextual relations associated with a specific event enables retrieval based on contextual cues as well as explicit recall"}, {"title": "3. Current Approaches", "content": "While many methods currently exist to modify and augment LLM memory, we argue that they fall short of the memory properties that would enable effective, long-term LLM agents. We group existing methods that seek to improve the memory of LLMs into three categories that are relevant to episodic memory:\n1. In-Context Memory methods extend the effective context length by optimizing computational efficiency and length generalization;\n2. External Memory methods augment a model's in-context memory capacity with a separate module, often with reduced GPU memory requirements and/or computational cost;\n3. Parametric Memory methods modify the LLM parameters that encode memories (primarily learned from the language modeling training data).\nIn this section, we discuss examples in each category that capture different properties of episodic memory. More importantly, we highlight their shortcomings in supporting episodic memory for LLM agents in isolation."}, {"title": "3.1. In-Context Memory", "content": "In-context memory (ICM) allows LLMs to perform single-shot, instance-specific, and contextualized learning by enabling them to directly attend to representations of encountered sequences. ICM capacity is either tightly limited or extensible but expensive to scale, often requiring sequence parallelization. Recent works seek to extend it by increasing the context window, but models struggle with length generalization beyond training exposures. We review existing methods and their limitations in addressing these challenges.\nOne active research direction focuses on extending the in-context window to handle significantly longer sequences, enabling LLMs to perform reasoning over extended contexts. This advancement brings LLMs closer to mimicking episodic memory, as it allows models to retain and utilize information across longer contexts. However, transformer-based LLMs face significant challenges, including the high computational cost of processing long sequences and limitations in length generalization. Recent research has sought to address these challenges by reducing memory usage, optimizing inference time, and improving long-sequence generation. Despite these advancements, current methods have yet to achieve robust, persistent memory capabilities necessary for long-term, open-ended, and context-aware reasoning. Below, we briefly review existing methods and their limitations.\nMemory reduction. For transformer-based LLMs, several methods aim to reduce memory and computation costs.\nSparsification and compression methods selectively retain relevant information to optimize memory usage. Sparsification strategies optimize memory by restricting attention computations to the most relevant parts of the sequence (Lou et al., 2024), reducing both storage and computational overhead. Similarly, forgetting mechanisms remove less useful tokens to maintain efficiency (Anonymous, 2024). Other compression-based approaches dynamically reduce KV cache size by storing only the most important tokens and key-value pairs (Liu et al., 2023b; Ge et al., 2024; Tang et al., 2024). Adaptive strategies further refine compression across layers (Yang et al., 2024a; Nawrot et al., 2024) or merge similar states to minimize redundancy (Liu et al., 2024a).\nQuantization methods reduce memory footprints by lowering precision or selectively storing information. Quantization techniques store key-value pairs at reduced precision (Liu et al., 2024c; Hooper et al., 2024; Yue et al., 2024; Duanmu et al., 2024), allowing for larger context windows with a relatively minimal performance degradation.\nInference time reduction. Efficiency improvements during inference focus on optimizing KV cache management and parallelization. Techniques such as paged caching (Kwon et al., 2023; Lee et al., 2024; Zheng et al., 2024) dynamically allocate memory to accommodate longer sequences without excessive overhead. Other methods leverage GPU memory pooling and adaptive chunking (Lin et al., 2024; Agrawal et al., 2024) to process extended contexts efficiently while maintaining fast retrieval and computation speeds. Other strategies improve efficiency by reusing KV tensors across layers (Ye et al., 2024a; Brandon et al., 2024)\nRecent work has aimed to introduce episodic memory in LLMs by structuring token sequences into retrievable events (Fountas et al., 2024), enhancing long-context reasoning and outperforming retrieval-based models. However, a fundamental challenge remain the increasing memory and retrieval costs: maintaining the full KV-Cache for an entire interaction history can quickly become impractical, especially in large-scale, long-duration, and multimodal applications. This limitation is inherent to KV-Cache management systems, which must retain the entire cache, leading to significant storage and computational overhead.\nTransformer alternatives to reduce both memory and inference time. In addition to optimizing KV cache storage and management, alternative architectures have been proposed to address the limitations of standard transformers in both memory and inference time.\nLinear attention (Li et al., 2020; Katharopoulos et al., 2020) approximates full self-attention using kernel-based or low-rank transformations, significantly reducing computational complexity and improving efficiency for long-sequence processing. State-space models (SSMs) (Peng et al., 2023; Gu & Dao, 2023) further achieve linear scaling for sequence handling by maintaining a fixed-size representation, making them inherently memory-efficient. Hybrid architectures (Goldstein et al., 2024) combine these techniques with transformers to compress KV-cache sizes while preserving strong performance. Other alternatives restructure the transformer architecture itself to enhance efficiency. Some models (Sun et al., 2024a) modify the decoder structure to reduce memory usage and latency, while others (Pang et al., 2024) compress sequence information into compact representations to improve inference speed and scalability.\nThese methods enhance ICM efficiency, but their reliance on compression, approximation, and selective retention comes with limited support for long-term reasoning with episodic memory. This limitation highlights the need for an external memory structure that retains past information. Methods of KV-cache optimization can also discard older context, leading to irreversible information loss and different model behavior (Kirsten et al., 2024). Generally, methods with a constant cost, like SSMs, struggle to handle a continually expanding interaction history in dynamic environments, while methods with an increasing state representation increase in both inference time and GPU memory requirements.\nLength Generalization. Length generalization refers to a model's ability to maintain understanding over long sequences, preventing degradation of performance such as"}, {"title": "3.2. External Memory", "content": "Many methods propose a separate memory module that stores information when it exceeds the effective operating span of the model. These augmented memory models are usually evaluated on tasks which require using that stored information. As such, these methods typically have long-term and explicit memory. However, they often lack information that relates the stored memories to one another-especially contextual details on how the model acquired the memory, or details to help differentiate specific instances. They are typically not evaluated for single-shot learning, especially for specific instances. And finally, there is a lack of proposals to generalize information from these instances and update parametric memory. Below we review some relevant external memory methods and elaborate on key examples to illustrate these shortcomings.\nSlot-based memory with recurrent controllers. A key advance in memory augmentation in the pre-transformer era was the formulation of learnable memory modules external to the main neural network. External memories were stored in individual slots and updated via a recurrent memory controller. These models were shown to retain longer-term information than vanilla long-short term memory (LSTM) networks. Similar memory augmentation methods have been adapted for transformers. However, these methods lack a way to store contextual details that LLM agents would need in an episodic memory, as they strongly depend on the details available in the input data. One exception devised a method to record temporal relationships between memories , but this has yet to be seen in augmented LLMs.\nDistributed vs. slot memory. An issue with slot-based memory modules is that they are capacity-limited, both by the number of slots and the dimensionality of each slot representation. While these models adopt forgetting mechanisms to mitigate this, the capacity limit still affects how long memories can be stored. Another approach addresses this downside by storing external memories in a sparse, distributed fashion  instead of in slots. Recent work integrated distributed memory in an LLM, and showed that the model can recall a greater number of facts over longer contexts, compared to baseline LLMs. While they demonstrate how they can perform one-shot memory updates (fact-editing), they do not evaluate single-shot learning of novel facts.\nRAG and GraphRAG methods. Retrieval Augmented Generation (RAG) methods maintain an external database of information that is added to the input data to augment LLM generation. Naive RAG implementations encode chunks of text using embedding models, typically without much metadata or contextual detail about the original text. And while text embedding models can capture some similarity relationships between embeddings, they do not encompass the rich set of relationships that LLM agents will likely need for most applications. GraphRAG models replace the vector embedding database with a structured graph that explicitly encodes relationships as connections between nodes. Still, these graphs encode a limited number of relationship types, even when researchers branch out beyond pre-existing datasets and learn to build the graphs directly from the input text . As such, they also lack rich contextual detail.\nExternal storage of past LLM inputs and outputs. Another type of approach maintains a database of pasts LLM inputs to avoid recomputing predictions to similar future inputs . Here, contextual information (e.g. details that differentiate specific instances) will only be stored when explicitly given in the LLM input text. That is, the memory is much more dependent on input data, limiting test-time generalization. One proposal to mitigate this formulates a long-term memory module for context that is updated with LLM activations based on the current inputs. Other approaches additionally store LLM outputs, such as generated text, summarizations , chain-of-thought steps , and extracted relation triples . One approach specialized for chat interactions stores timestamps and user personality"}, {"title": "3.3. Parametric Memory", "content": "This type of memory allows LLMs to process the information in the input to obtain well-suited output. Parametric memory values are initially learned through back-propagation with a pretraining dataset. During this process, the parametric memory tends to capture general knowledge and rules ranging from syntax to common sense and factual knowledge. Due to the sheer size of the parametric memory, the amount of data needed for pre-training is usually very large, following power laws . Generally, parametric memory is fixed after training, i.e., does not change with the input at inference time.\nA relevant research direction in parametric memory focuses on adapting LLM parameters to specific domains, tasks, or applications when given limited resources. Efficient fine-tuning methods have been developed in recent years to tackle the runtime and memory consumption of this process. Alternatively, distillation techniques have been proposed to update knowledge and propagate it through a model. A key challenge is the need for updating specific factual knowledge without interfering with other knowledge. Some facts may change over time, requiring surgical precision to update the parameters of a model. The line of work that proposes these updates is known as knowledge editing.\nEfficient Fine-tuning. Various works have been proposed to reduce the computational needs (hardware memory) of updating a model to a specific domain. Among these, Low-Rank Adaptation (LoRA)  applies additive low-rank approximation updates to shift the model parameters. Several methods proposed other ways to further improve efficiency by reducing and localizing updates. Other work learned modifications on representations instead of parameters . In all cases, fine-tuning methods require a dataset to adapt a model for a specific task or domain, i.e. they are not capable of single-shot learning or capturing instance-specific and contextually rich information. On the other hand, additional fine-tuned adapter parameters are often frozen after the fine-tuning process, supporting the long term storage of information. Moreover, these methods tend to preserve the reasoning capabilities while updating the model with newly captured information.\nKnowledge Editing. As the environment evolves over time, some factual knowledge becomes outdated (e.g., the president of a country may change after the elections). Knowledge editing methods aim to make modifications to the factual knowledge in parametric memory with targeted updates while avoiding interference with other facts. In ROME  and MEM-IT , the first step is to find relevant parameters (in MLPs) that influence the specific fact through causal interventions and then update the related parameters with low-rank model edits.\nAn alternative research direction proposes to train a hyper-network  that predicts the amount of change for each parameter given the knowledge to be edited. A different method, SERAC, stores the set of edits in an external memory, combined with a scope detector and a counter-factual model to decide when and how to apply the edits . All knowledge editing methods work on facts which are inherently context-free, making it impossible to contextualize the edited knowledge in the history of the agent-environment interaction. However, they mimic the episodic memory traits of learning from a single instance, while enabling long-term retention.\nThe problem of knowledge editing has been extended to a continual learning setting, where edits are required sequentially over time to correct a model. This leads to the sequential editing problem: hyper-network prediction quality decreases because they fail to reflect the updated model, and low-rank parameter updates interfere with one another causing catastrophic forgetting. MELO adapts dynamic LoRA to this problem and introduces a vector database to search the selections of the blocks to be dynamically activated within the LORA matrices for each layer. WISE adds duplicates of the MLP's output parameters for some layers in the network, and updates them with each new edit set. A"}, {"title": "4. Episodic Memory as a Unifying Framework", "content": "Although current work has advanced context-sensitive LLMs that are capable of handling longer sequences, it does not yet deliver efficient learning that could support long-term LLM agents. Existing methods\u2014which extend in-context (working) memory, integrate external memory, or update parametric memory-only address subsets of episodic memory's five essential properties, as discussed in Section 3. These approaches remain fragmented, impeding the immediate assimilation of new experiences and gradual improvement over time.\nWe propose that enabling episodic memory offers a unifying perspective that will combine and extend existing methods to advance the capabilities of LLM agents. By incorporating long in-context memory, external memory, and mechanisms for updating parametric memory, agents can more seamlessly adapt to new information, consolidate it, and prevent escalating costs or performance degradation during extended interactions with an environment. This view is based on Complementary Learning Systems Theory , in which episodic memory is part of a fast-learning system that stores information from individual instances. Over time, that information is consolidated into a slow-learning system that stores more stable, durable knowledge.\nIn Figure 1, we present a general architecture and framework that combines these elements under the overarching goal of enabling all five key features of episodic memory for LLM agents as detailed in Section 2. As a roadmap to enable episodic memory in LLM agents, we specifically call for four main research directions (encoding, retrieval, consolidation, and benchmarks), and formulate six research questions under these areas below."}, {"title": "4.1. Encoding", "content": "RQ1: How to store information from in-context memory in a long-term external memory store?\nAn external memory store is essential for retaining experience in a structured way that preserves the context of individual instances (Fig.1, arrow (b)). A straightforward approach is to store text chunks or embeddings in a non-parametric RAG-like database, potentially augmented with metadata for context. More structured representations, such as GraphRAG, could also facilitate context-sensitive retrieval. However, capacity constraints on these types of databases may make it necessary to rely on more compressed parametric representations.\nRQ2: How to segment continuous input into discrete episodes, and when to store them in an external memory?\nA major design question is when and how to segment a continuous stream of agent experience into episodes to be encoded into an external memory. LLMs have already been shown to be capable of segmenting text into meaningful events, in a way that is similar to humans , and recent approaches show that further bundling related segments based on model surprise can improve long-term modeling . Leveraging long-context advances can further improve encoding by providing a space in which new episodes can be equipped with a rich contextualization. Large hidden states or extended attention windows help capture high-fidelity contextual information, which can then be encoded into an external memory in a compressed format for future retrieval."}, {"title": "4.2. Retrieval", "content": "RQ3: Given an external memory, how to select relevant past episodes for retrieval and reinstatement into in-context memory for the purpose of explicit reasoning?\nTo employ past experiences in current tasks, an agent must retrieve relevant episodes at the right time and reintegrate them into its in-context memory with an adequate mechanism (Fig.1, arrow (c)). Common strategies include prepending retrieved text tokens to the input sequence (as in RAG), manipulating representational states within the transformer (e.g., memory tokens , or adapting internal representations .\nRQ4: How can retrieval mechanisms in long-context LLMs improve and accelerate the optimization of external memory retrieval and reinstatement?\nLong-context advances can be leveraged to inform when and what to retrieve at sequence lengths that are still feasible. Future research could explore tight integration of external memory with the model's forward pass  and adopt cross-architecture distillation that retain many of the desirable properties of in-context memory while reducing the resource cost."}, {"title": "4.3. Consolidation", "content": "RQ5: How to periodically consolidate external memory contents into the LLM's base parameters without forgetting previous knowledge?\nEventually, merging external memory contents into the model's parameters (Fig.1, arrow (a)) promises to allow new generalized knowledge to be used without explicit retrieval. This process both prevents external memory overflow and supports continuous adaptation of the agent's semantic and procedural backbone to the environment. Relevant techniques include context distillation, parametric knowledge editing, and localized fine-tuning methods that capture newly encountered information without catastrophic interference with other knowledge. Open questions remain about how to decide when to consolidate and how to compress many episodic instances into more abstract parametric knowledge while also retaining previous knowledge and skills."}, {"title": "4.4. Benchmarks", "content": "RQ6: What new types of benchmarks are needed to assess episodic memory in LLM agents?\nFinally, evaluating episodic memory effectiveness requires new tasks and metrics. Studies should test the recall of contextualized events after long delays, assessing how well agents remember when, where, and how events occurred. An example of such a study is the testing of instance-specific temporal order memory proposed by Pink et al. . Beyond controlled probes, benchmarks must incorporate real-world complexities: agents should demonstrate an improving task performance that is linked to encoding, retrieval, and consolidation of past experiences over extended timescales."}, {"title": "5. Alternative Views", "content": "While we argue that an explicit episodic memory framework is necessary for effective long-term and context-sensitive behavior, there are alternative perspectives suggesting that current or emerging methods might suffice in the future without the need for the concept of episodic memory to provide guidance.\nScaling in-context memory will be sufficient. One view suggests that advances in long-context methods\u2014such as improved transformers, state-space models, or other architectures with extended context windows\u2014will enable practically unlimited access to past information. Proponents claim that better positional encodings, modified attention mechanisms, and other in-context memory extensions will cover most relevant applications for LLM-based agents.\nContextualized external memory will be sufficient. A second view holds that external memory structures-such as knowledge graphs or retrieval-augmented generation (RAG) systems-could eliminate the need for an episodic memory framework. By contextualizing data chunks and storing them in structured graphs, these systems aim to incorporate past context into current tasks effectively.\n\"Infinite\" in-context memory remains a speculative prospect. Extending limited context windows to include all information needed by an agent requires foreknowledge of the maximum timespan of relevant information. For very long timespans, this will either incur prohibitive computational costs or require compression methods that may lose key details. Only relying on external memory will still incur high storage costs, and require forgetting mechanisms. An episodic memory framework addresses these constraints by periodically consolidating information into high-capacity parametric memory (Figure 1, arrow (a)). This has the added benefit of enabling LLM agents to slowly improve over time, as they continue to learn from the past before they forget it."}, {"title": "6. Conclusion", "content": "This position paper argues that to fully realize efficient long-term LLM agents, we must endow LLM agents with episodic memory. We operationalize episodic memory-a term borrowed from cognitive science-for LLMs by highlighting five key characteristics that distinguish episodic memory from other types of memory in biological systems, and argue for why each property is also important for LLM agents. We position the call for episodic memory in LLM agents in the current literature and discuss how episodic memory can serve as a unifying goal for existing research directions. Lastly, we provide a roadmap of research questions towards implementing episodic memory in LLMs. By describing the potential of this research direction, we aim to spark a community-wide shift in how we conceive and engineer long-term memory in the move towards agentic AI-one that more deeply integrates lessons from cognitive science and brings together existing approaches in ML under a unifying goal with strong promise."}]}