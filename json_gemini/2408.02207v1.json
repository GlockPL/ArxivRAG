{"title": "MARCO: A Memory-Augmented Reinforcement Framework for Combinatorial Optimization*", "authors": ["Andoni I. Garmendia", "Quentin Cappart", "Josu Ceberio", "Alexander Mendiburu"], "abstract": "Neural Combinatorial Optimization (NCO) is an emerging domain where deep learning techniques are employed to address combinatorial optimization problems as a standalone solver. Despite their potential, existing NCO methods often suffer from inefficient search space exploration, frequently leading to local optima entrapment or redundant exploration of previously visited states. This paper introduces a versatile framework, referred to as Memory-Augmented Reinforcement for Combinatorial Optimization (MARCO), that can be used to enhance both constructive and improvement methods in NCO through an innovative memory module. MARCO stores data collected throughout the optimization trajectory and retrieves contextually relevant information at each state. This way, the search is guided by two competing criteria: making the best decision in terms of the quality of the solution and avoiding revisiting already explored solutions. This approach promotes a more efficient use of the available optimization budget. Moreover, thanks to the parallel nature of NCO models, several search threads can run simultaneously, all sharing the same memory module, enabling an efficient collaborative exploration. Empirical evaluations, carried out on the maximum cut, maximum independent set and travelling salesman problems, reveal that the memory module effectively increases the exploration, enabling the model to discover diverse, higher-quality solutions. MARCO achieves good performance in a low computational cost, establishing a promising new direction in the field of NCO.", "sections": [{"title": "1 Introduction", "content": "The objective in Combinatorial Optimization (CO) problems is to find the optimal solution from a finite or countable infinite set of discrete choices. These problems are prevalent in many real-world applications, such as chip design [35], genome reconstruction [39] and program execution [16].\nIn recent years, the field of Neural Combinatorial Optimization (NCO) has emerged as an alternative tool for solving such problems [5; 34; 4]. NCO uses deep neural networks to address CO problems in an end-to-end manner, learning from data and generalizing to new, unseen instances. Researchers in this field have followed the steps of heuristic optimization, proposing the neural counterparts of constructive methods [4; 24; 25] and improvement methods [30; 10; 42].\nNeural constructive methods quickly generate an approximate solution in a one-shot manner by means of a learnt neural model. While being simple and direct, constructive methods suffer from their irreversible nature, barring the possibility of revisiting earlier decisions. This limitation becomes particularly pronounced in large problems where suboptimal initial decisions in the construction of the solution can significantly impact the final outcome. To improve the performance of these methods, recent efforts have employed techniques such as sampling, where instead of following the output of the model deterministically, a random sample is taken from a probability distribution given by the output, with the intention of obtaining better solutions and break with the deterministic behaviour, obtaining a richer set of solutions; or beam search [11], which maintains a collection of the highest-quality solutions as it explores the search space based on the output of the neural network, i.e., the probability of adding an item to the partial solution that is being constructed. Similarly, active search [4; 21] is used to update the model's weights (or a particular set of weights) during test time, in order to overfit the model to the test instance to be solved.\nAlternatively, neural improvement methods are closely linked to perturbation methods, such as local search. They start from a complete solution, and operate by iteratively suggesting a modification that improves the current solution at the present state. Unlike constructive methods, improvement methods inherently possess the ability to explore the search space of complete solutions. However, they often get stuck in local optima or revisit the same states repeatedly, leading to cyclical patterns. Recent studies [3; 17] have employed a variety of strategies inherited from the combinatorial optimization literature to tackle these drawbacks. The method by [3] keeps a record of previously per-"}, {"title": "2 Related Work", "content": "Various strategies have been developed to enhance the exploration of the search space in NCO algorithms. Most of the methods sample from the model's logits [4; 24; 25], which introduces stochasticity into the solution inference process. Beyond sampling, entropy regularization has been implemented during the training of NCO models [22], to ensure the models are not overconfident in their output. Furthermore, [19] proposed a multi-decoder system, where each decoder is trained on instances where it performs best, resulting in a set of specialized and complementary policies.\nDespite these advancements, none of these methods exploit any kind of memory mechanism, which has the potential to leverage previous experiences in the decision-making process and promote exploration.\nIn the work by [17], a tabu search algorithm [18], known for its memory-based approach to circumventing cyclical search patterns, is layered on top of a neural improvement method. The algorithm utilizes a tabu memory to track previously visited solutions. However, this memory serves merely as an external filter, preventing the selection of tabu actions without integrating historical data into the neural model's decision-making process.\nDeepACO [44] uses a neural network to learn the underlying heuristic of an ant colony optimization algorithm [6; 12]. It maintains an external pheromone matrix, indicative of promising variable decisions. However, the integration of this pheromone data is indirect; it is combined in a post-hoc fashion with the output probabilities of the model rather than being an intrinsic part of the learning process.\nCloser to our work, ECO-DQN [3] is a neural improvement method that records the last occurrence of each action. This operation-based memory approach, which simply tracks when actions were last taken, is computationally efficient, requiring only minimal storage. The drawback of this approach is that it only focuses on the actions, failing to consider the overall search context. The effectiveness of an action is often contingent on the broader state of the optimization process, a fact that operation-based memory fails to capture. Compared to this work, we save entire solutions in memory, incorporating a more holistic view of the search context to the system, at the cost of higher memory requirements."}, {"title": "3 MARCO: A Memory-Based Framework", "content": "This section introduces MARCO, the main contribution of this paper. Although the framework can be used for arbitrary CO problems, we first focus on graph-based problems, as they are ubiquitous in combinatorial optimization. In fact, from the 21 NP-complete problems identified by Karp (1972), ten are decision versions of graph optimization problems, while most of the other ones can also be modeled over graphs.\nLet \\(G = (V, E)\\) be a simple graph composed of a set of nodes \\(V\\) and a set of edges \\(E\\). Finding a solution \\(\\Theta\\) for graph problems often involves finding subsets of nodes or edges that satisfy specific criteria, such as minimizing or maximizing a certain objective function.\nBriefly, the idea of MARCO is to leverage both (1) a learnt policy defining how the current solution should be modified for exploring the search space, and (2) a memory module \\(M\\), providing information to build the policy. The policy is typically parameterized with a neural network, and especially with a graph neural network [23] when operating on graph problems. Such an architecture has been considered as highly relevant for combinatorial optimization [9]. Besides, the policy is iteratively called to modify the solution until a convergence threshold has been reached.\nThis mechanism can be integrated into both constructive and improvement methods. The main difference relates to how a solution is defined and how information is retrieved from the memory. Let \\(\\Theta_t\\) refer to a complete solution obtained after \\(t\\) iterations, and \\(\\hat{\\Theta}_t\\) refer to a partial solution, i.e., a solution where only \\(t\\) variables has been assigned, with at least one variable not assigned. In constructive methods, MARCO is capable of using a deterministic policy repeatedly, i.e., opting for the greedy action to generate multiple different constructions. Each construction starts from an empty solution"}, {"title": "3.1 Memory Module", "content": "As shown in Algorithm 1, the inference in MARCO starts with the selection of an initial solution (refer to line 2). In each optimization step, the memory module \\(M\\) is responsible for storing the visited solutions (line 6), and retrieving aggregated historical data \\(h_t\\) (line 7). The historical data (\\(h_t\\)) is aggregated with the current (partial) solution and the graph features (\\(G\\)) to form the current optimization state \\(s_t\\) (line 8). Subsequently, \\(s_t\\) is input into the model (line 9), which then proposes a set of actions that generate new solutions (line 10).\nThe specific process of retrieval is shown in Algorithm 2. To retrieve relevant solutions, MARCO employs a similarity-based search. This involves comparing the current (partial) solution (\\(\\Theta_t\\) or \\(\\hat{\\Theta}_t\\)) with each stored solution (\\(\\Theta_{t'}\\), where \\(t' < t\\)) using a similarity metric (e.g., the inner product in line 4). Intuitively, the idea is to fed the policy with the most similar solutions to the current one for executing the next exploration step. We carry out the retrieval using a k-nearest neighbors search (line 5). Rather than simply averaging the \\(k\\) most similar solutions, MARCO uses a weighted average approach, where the weight given to each past solution is directly proportional to its similarity to the current solution. This score is normalized, ranging from 0 (completely different) to 1 (identical), to represent the level of similarity (see line 6).\nCollaborative Memory\nAn additional feature enabled by MARCO is the implementation of parallel optimization threads during its inference phase. In this setup, multiple concurrent threads are run for"}, {"title": "4 Application of MARCO", "content": "In this study, we demonstrate the adaptability of MARCO to various problem types, encompassing both constructive and improvement methods. We specifically apply MARCO in two scenarios: (1) a neural improvement method for problems with binary variables, such as the Maximum Cut (MC) and the Maximum Independent Set problem (MIS); and (2) a neural constructive method for permutation problems, such as the Travelling Salesman Problem (TSP)."}, {"title": "4.1 Improvement Methods for Binary Problems", "content": "In binary optimization problems, a solution is formalized as a binary vector, denoted as \\(\\Theta \\in \\{0, 1\\}^{|V|}\\) for a problem with \\(|V|\\) variables. Each variable \\(x_i\\) represents a binary decision for the \\(i\\)th variable. Neural improvement methods are designed to optimize a problem by iteratively refining an initial complete solution \\(\\Theta_0\\), which can be generated either randomly or through heuristic methods. In the case of binary problems, the central operation is a node-wise operator that flips the value of a node in \\(\\Theta\\). The memory records visited solutions and their associated actions (e.g., a bit-flip action, consisting in flipping the value of a variable). When a new solution is generated, the model consults the memory to retrieve the actions performed in similar previous scenarios. The importance of the stored actions is given by the similarity between the current solution \\(\\Theta_t\\) and previously stored solutions \\(\\Theta_{t'}\\) with \\(t' < t\\). We compute the similarity using the inner product:\n\\[\\text{Similarity}(\\Theta_t, \\Theta_{t'}) = (\\Theta_t, \\Theta_{t'}) = \\sum_{i \\in V} (\\Theta_t)_i (\\Theta_{t'})_i\\label{eq:inner_prod}\\]\nIn this case, the aggregated memory data (\\(h_t\\)) is a vector of size \\(|V|\\), defined as the weighted average of the actions that were executed in the \\(k\\) most similar solutions (if any). See for a visual description of the inference in neural improvement methods with MARCO.\nThe reward \\(r_t\\) obtained by a neural improvement model at each step \\(t\\) is defined as the non-negative difference between the current objective value of the solution, \\(f(\\Theta_t)\\), and the best objective value found thus far \\(f(\\Theta^*)\\), i.e., \\(r_t = \\max\\{f(\\Theta_t) - f(\\Theta^*), 0\\}\\). This reward structure, prevalent in neural improvement methods [33; 42], motivates the model to continually seek better solutions. To prevent the model from cycling through the same states and encourage novel solution exploration, we incorporate a binary penalty term \\(p_r\\), activated when revisiting previously encountered solutions. The adjusted reward for each step is thus \\(r_t = r_t - W_p \\times p_r\\), where \\(W_p\\) is a weight factor for the penalty."}, {"title": "4.2 Constructive Methods for Permutations", "content": "The objective in permutation problems like the TSP is to find a permutation of nodes in a graph that maximizes or minimizes a specific objective function. Neural constructive methods build the permutation incrementally, starting from an empty solution and adding elements sequentially until a complete permutation is formed.\nIn the context of permutation problems, the solution \\(\\Theta\\) can also be conceptualized as a binary vector \\(\\Theta_b \\in \\{0, 1\\}^{|E|}\\)."}, {"title": "5 Model Architecture", "content": "Graph neural networks are particularly well-suited to parameterize policy \\(\\pi\\). We specifically use a Graph Transformer (GT) [14] coupled with a Feed-Forward Neural Network. GTs are a generalization of transformers [38] for graphs. The fundamental operation in GTs involves applying a shared self-attention mechanism in a fully connected graph, allowing each node to gather information from every other node. The gathered information is then weighted by computed attention weights, which indicate the importance of each neighbor's features to the corresponding node.\nOur model aims to be adaptable to various combinatorial problems, requiring it to assimilate both the graph's structural information and the attributes of its nodes and edges. To achieve this, we modify the GT to incorporate structural information encoded as edge features within the Attention (Attn) mechanism. This adaptation is reflected in the following equation.\n\\[\\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T + E}{\\sqrt{d_k}}\\right) V\\label{eq:attention}\\]\nIn this equation, \\(Q\\), \\(K\\), and \\(V\\) stand for Query, Key, and Value, respectively, which are fundamental components of the attention mechanism [38] and \\(d_k\\) is a scaling factor. \\(E = W_e e_{ij}\\) is a linear transformation of the edge weights, where \\(W_e \\in \\mathbb{R}^{1 \\times n_{heads}}\\) is a learnable weight matrix, and \\(e_{ij}\\) represents the edge features between nodes \\(i\\) and \\(j\\). \\(E\\) integrates edge information by being added to the attention scores and used in a dot product.\nThe final step involves processing the output of the GT through an element-wise feed-forward neural network to generate action probabilities. The output of the GT could be both node- or edge-embeddings. The performed action depends on the method in use. In our studied cases, we will use node embeddings to generate node-based actions: node-flips for improvement methods in binary problems and node addition to the partial solution for the constructive method in permutation problems. However, MARCO is also applicable to edge-based actions, such as pairwise operators (swap, 2-opt) for permutation-based improvement methods.\nWe utilize the policy gradient REINFORCE algorithm [41] to find the optimal parameter set, \\(\\pi^*\\), which maximizes the expected cumulative reward in the optimization process."}, {"title": "6 Experiments", "content": "We validate the effectiveness of MARCO across a diverse set of CO problems both binary and permutation-based: the Maximum Cut (MC), Maximum Independent Set (MIS) and the Travelling Salesman Problem (TSP)."}, {"title": "6.1 Problems", "content": "Maximum Cut (MC). The objective in MC [13] is to partition the set of nodes \\(V\\) in a graph \\(G\\) into two disjoint subsets \\(V_1\\) and \\(V_2\\) such that the number of edges between these subsets (the cut) is maximized. The objective function can be expressed as: \\(\\text{max } \\sum_{(u, v) \\in E} \\delta[\\Theta_u \\neq \\Theta_v]\\) where \\(\\Theta_u\\) and \\(\\Theta_v\\) are binary variables indicating the subset to which nodes \\(u\\) and \\(v\\) belong, and \\(\\delta\\) is a function which equals to 1 if \\(\\Theta_u\\) and \\(\\Theta_v\\) are different and 0 otherwise.\nMaximum Independent Set (MIS). For the MIS problem [28], the goal is to find a binary vector \\(\\Theta\\) that represents a subset of nodes \\(S \\subseteq V\\) in a graph \\(G\\) such that no two nodes in \\(S\\) are adjacent, and the size of \\(S\\) is maximized. The objective function can be formulated as: \\(\\text{max } |S\\) such that \\((u, v) \\notin E\\) for all \\(u, v \\in S\\).\nTravelling Salesman Problem (TSP). In TSP [27; 40], given a set of nodes \\(V\\) and distances \\(d_{u, v}\\) between each pair of nodes \\(u, v \\in V\\), the task is to find a permutation \\(\\pi\\) of nodes in \\(V\\) that minimizes the total travel distance. This is expressed as: \\(\\text{min } \\sum_{i=1}^{|V|} d_{(\\Theta_i, \\Theta_{i+1})}\\) with \\(\\Theta_{|V|+1} = \\Theta_1\\)"}, {"title": "6.2 Experimental Setup", "content": "Training For each problem, we train a unique model, using instances that vary in size. This helps the model to learn strategies that can be transferable between differently sized instances. For the MC and MIS, we used randomly generated Erdos-Renyi (ER) [15] graphs with 15% of edge probability, and sizes ranging from 50 to 200 nodes. For the TSP, fully connected graphs ranging from 50 to 100 nodes were generated, in which cities were sampled uniformly in a unit square. The total training time depends on the problem. The models for both MC and MIS required less than 40 minutes, while the one for the TSP required a significantly longer training (4 days) to reach convergence. See the supplementary material for a detailed description of the training configuration used.\nInference To evaluate the performance of MARCO, we have established certain inference parameters. For MC and MIS, we set the neural improvement methods to execute with 50 parallel threads (processing 50 solutions simultaneously), stopping upon \\(2|V|\\) improvement steps. For the TSP, we use 100 parallel initializations (as done in POMO [25]) and 20 iterations (solution constructions) for each instance. We have used \\(k = 20\\) for the similarity search. A more detailed description of the inference configuration used is reported in the supplementary material. MARCO has been implemented using PyTorch 2.0. A Nvidia A100 GPU has been used to train the models and perform inference. Exact methods and heuristics serving as baselines were executed in a cluster with Intel Xeon X5650 CPUs.\nEvaluation Data Following the experimental setup of recent works [1; 7; 45], we will evaluate the MC and MIS problems in ER graphs of sizes between 700-800, and harder graphs from the RB benchmark [43] of sizes between 200-300 and 800-1200. For TSP, we follow the setting from [24] and use randomly generated instances, with uniformly sampled cities in the unit square. We use graphs of sizes 100, 200 and 500.\nAblations We evaluate MARCO through several ablations that help us understand the impact of its different components. We begin by evaluating standalone models proposed in this work: the Neural Improvement Method (NIM) and Neural Constructive Methods (NCM), both of which operate without any integrated memory module. Next, for improvement methods, we add a NIM equipped with an operation-based memory (Op-NIM), tracking the number of steps since each action was executed lastly (imitating ECO-DQN [3]). Finally, we asses MARCO-ind, a variant of MARCO that operates without shared memory, executing multiple threads simultaneously but independently, with each thread maintaining its own separate memory.\nBaselines To assess MARCO's performance, we conduct a comprehensive comparison against a broad spectrum of combinatorial optimization methods tailored to each specific problem addressed. Our comparative analysis includes exact algorithms, heuristics, and learning-based approaches\nFor the MC, our comparison includes the GUROBI solver [20], the local search enhanced heuristic BURER [8], and ECO-DQN [3], which is a neural improvement method incorporating an operation-based memory.\nFor MIS, we also include GUROBI [20], together with KAMIS [26], a specialized algorithm for MIS; and a constructive heuristic (Greedy), that selects the node with minimum degree in each step. Furthermore, we examine also a range of recently proposed learning-based methods: DGL [7], LwD [1] and FlowNet [45].\nFor the TSP, we report results of the well known conventional solver Concorde [2], the heuristic LKH-3 [36], the Nearest Neighbor (NN) heuristic; and the learning-based"}, {"title": "6.3 Ablation Study", "content": "We evaluate the impact of the proposed memory module on our model's learning dynamics and performance. Figure 4 (a) illustrates the revisit frequency of already visited solutions during the first 100 training episodes. This metric reflects the model's ability to explore diverse solutions effectively. Figure 4 (b) shows the increase in the objective value of solutions as training progresses.\nThe results indicate that the model without memory tend to repeat actions more frequently. In contrast, incorporating an operation-based memory module reduces this repetitiveness, facilitating broader exploration. Furthermore, MARCO's memory module demonstrates superior exploration capabilities among the tested configurations.\nOverall, the study confirms that integrating the proposed memory module not only improves the model's exploration capabilities but also leads to a measurable improvement in the quality of the generated solutions."}, {"title": "6.4 Performance Results", "content": "We present the results for each studied problem in a table divided by three row-segments, the first one consisting of non-learning methods (exact and heuristic), the second with recent learning methods from the literature, and the third with the methods (MARCO and ablations) proposed in this paper. We report both the average objective value in the evaluation instance set and the time needed for performing inference with a unique instance (batch size of 1). We use ms, s and m to denote milliseconds, seconds and minutes, respectively. For learning methods, we report the results from the best performing configuration reported in the original paper. For exact solvers, we report the best found solution when the optimal solution is not achieved in a limit of 1 and 10 minutes per instance.\nMC In Table 1 we report the results for the MC. MARCO significantly outperforms GUROBI and ECO-DQN, especially in larger problem instances (ER700-800, RB800-1200). In addition, MARCO proves to be competitive against the state-of-the-art heuristic, BURER, in the studied graph instances. The ablation results show that using the proposed memory scheme is superior to (1) not using any memory module, and (2) using an operation-based memory. Moreover, using a shared memory slightly improves the performance (with respect to MARCO-ind), while the computational cost is reduced. Compared to the ECO-DQN in computational cost, MARCO reduces the time needed to perform \\(2|V|\\) improvement steps.\nMIS Table 2 summarizes the results for MIS. Here, MARCO is also able to surpass the learning methods and its ablations, obtaining a comparable performance to the exact solver. Moreover, it reduces the gap to the specialized KAMIS algorithm. While incorporating a memory module in MARCO (NIM vs. MARCO) increases the time cost, it contributes to achieving superior solutions, while NIM gets stuck in suboptimal solutions (increasing the number of steps does not increase the performance).\nTSP Results for the TSP are reported in Table 3. MARCO can obtain good inference performance in the studied instances, reaching the best found solutions for N100 and N200; and being second on N500, only surpassed by LEHD. It is important to note that our basic NCM implementation (without memory) obtains comparable results with the state-of-the-art learning method while being orders of magnitude faster. Also, MARCO improves over both NCM and the method without sharing memory (MARCO-ind)."}, {"title": "Generalization to Larger Sizes", "content": "Training NCO models with reinforcement learning is computationally intensive, leading to a common practice in the literature where models are often trained on smaller-sized instances (up to 100). While this approach is understandable due to resource constraints, it is important to consider the ability of these models to generalize to larger instances. This aspect is crucial for their applicability in real-world scenarios, where problem sizes can vary significantly.\nThe data presented in illustrate this point. Here, even a basic greedy constructive method manages to outperform more complex learning-based methods (DGL, LWD, and FlowNet) in the RB800-1200 instance. This observation underlines the importance of using simple heuristics as a sanity check to assess whether advanced models are effectively generalizing to unseen instances or larger sizes. Similarly, Table 3 reveals that a simple Nearest Neighbour heuristic is able to surpass POMO, DACT and NeuOPT in instances of 500 cities. Even though the underlying model of MARCO has been trained on smaller instances (up to 200 for MC and MIS, and up to 100 for TSP), it is able to maintain a good performance in larger graphs with a lower time cost compared to state-of-the-art heuristic solvers."}, {"title": "7 Limitations and Future Work", "content": "MARCO offers significant advancements in neural combinatorial optimization. However, it has room for improvement. A primary concern is the uncontrolled growth of its memory during the optimization process, as it continually stores all the encountered states, leading to increased computational and memory costs. To counter this, future work could focus on implementing mechanisms to prune the memory by removing redundant information.\nAnother limitation is the substantial resource requirement for storing entire edge-based solutions in memory (like in TSP). This approach, particularly for large instances, can result in high memory consumption and slower retrieval processes. A promising direction would be to represent solutions in a lower-dimensional space using fixed-size embeddings, effectively reducing the memory footprint while preserving (or even incorporating) necessary information.\nIn terms of data retrieval, MARCO currently employs a method based on a weighted average of similarity, which may not fully capture the relationships between solution pairs. A more advanced alternative to consider is the implementation of an attention-based search mechanism. This method would not only prioritize the significance of various stored solutions but could also incorporate the objective values or other distinct characteristics of these solutions to compute their relevance.\nAdditionally, while not a limitation, applying MARCO to new problems or integrating it with different NCO methods requires careful consideration in how memory information is aggregated with instance features. The nature of the data stored and retrieved can vary significantly depending on the specific problem being addressed."}, {"title": "8 Conclusion", "content": "In this paper, we have introduced the Memory-Augmented Reinforcement for Combinatorial Optimization (MARCO), a framework for Neural Combinatorial Optimization methods that employs a memory module to store and retrieve relevant historical data throughout the search process. The experiments conducted in the maximum cut, maximum independent set and travelling salesman problems validate MARCO's ability to quickly find high-quality solutions, outperforming or matching the state-of-the-art learning methods. Furthermore, we have demonstrated that the use of a collaborative parallel-thread scheme contributes to the performance of the model while reducing the computation cost."}, {"title": "A Further implementation details", "content": "In order to solve the Maximum Cut (MC), Maximum Independent Set (MIS) and Travelling Salesman Problem (TSP), MARCO receives information from three sources: the static instance information, the dynamic (partial) solution and the historical memory data."}, {"title": "A.1 MC and MIS", "content": "In MC and MIS, the instance information is given by binary edge features \\(y \\in \\mathbb{R}^{|E|}\\), which denote the existence of an edge in the set of edges \\(E\\) of the graph \\(G\\). The solution is also encoded in binary node features \\(x \\in \\mathbb{R}^{|V|}\\), indicating which of the two sets the node belongs to, and where \\(V\\) is the set of nodes. Visited solutions and performed actions (node flips) are stored in memory. The model retrieves a weighted average of the actions performed in similar solutions and this information is concatenated as an additional node feature."}, {"title": "A.2 TSP", "content": "In TSP, we follow prior works [24; 25] with some modifications:\n\u2022 While prior works only use node features (city coordinates), we also consider the use of distances between cities as edge features, since we believe the relative information is advantageous for the model.\n\u2022 We implemented a Graph Transformer (GT) [14] encoder layer that also considers the edge features.\n\u2022 We increased the model size (embedding dimension). We obtained better results scaling it. See the selected model's hyperparameters in Table 4 (lines 1-5).\n\u2022 ReLU activation function is replaced by SwiGLU [37].\n\u2022 We implemented a function to compute an arbitrary number of data augmentations (coordinate rotations).\nThe memory data is aggregated as an edge feature, and gives information about the number of times each edge has been considered in previous solutions or routes. These edge features are used in the MHA mechanism of the decoder [24; 25], where a linear projection of the memory features is added to the attentions weights.\n\\[\\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}} + E\\right) V\\label{eq:attention}\\]"}, {"title": "A.3 Training in MARCO", "content": "The training hyperparameters can be found in Table 4 (lines 6-15). We trained a unique model for each problem. Each epoch of training comprised 1000 episodes. Within each episode, 128 new instances of an arbitrary size (50-200 for MC and MIS and 20-100 for TSP) were randomly generated. The optimization of model parameters was conducted using the AdamW optimizer [29], and we clipped the gradient norm at a value of 1.0.\nThe training of improvement models (for MC and MIS) requires additional hyperparameters, such as the discount factor and episode length. This necessity comes from the reward mechanism employed in these models, which is based on the concept of discounted future rewards, i.e., focusing on maximizing future gains. A discount factor of 0.95 and an episode length of 20 steps were selected.\nWe adopted the following two-phase training pipeline for constructive models. The initial phase comprises 200 epochs of training without the incorporation of memory, following the training of POMO [25].\nSubsequently, we fix all model weights and introduce a trainable Feed-Forward network into the decoder, which processes the data retrieved from the memory. For the TSP, the processed memory content is incorporated into the edges, and it is used within the Multi-Head Attention mechanism of the decoder in the same fashion as in the GT encoder. This second training stage extends for an additional set of 50 epochs. Within each training episode during this stage, the process begins with a deterministic rollout performed with an empty memory. This is followed by executing 5 construction iterations (line 15) using identical problem instances. In these iterations, the model is penalized based on the similarity of its proposed solutions to those generated in previous iterations."}, {"title": "A.4 Inference in MARCO", "content": "The performance results discussed in the paper have been obtained using models trained with the hyperparameter values shown in Table 4 (16-21).\nLines 16-18 have common parameters for both improvement and constructive methods: the number (K) of neighbors to consider in the memory retrieval process, the number of parallel execution threads, and the maximum number of solutions stored in memory (once this number is reached the oldest solutions are replaced).\nThe Max steps parameter denotes the number of improvement steps, while Constructions denote the number of solutions built in constructive methods.\nLastly, for the TSP, we modulate the frequency of data retrieval from the memory. Instead of conducting this search at every construction step, we establish a predetermined retrieval frequency (retrieval frequency) for performing it, significantly accelerating the process, while having minimal impact on performance due to the marginal changes to the partial solution in consequent steps."}, {"title": "B Impact of k on MARCO's Performance", "content": "The number of neighbors, denoted as k, considered during the k nearest neighbor search, plays a crucial role in MARCO's"}, {"title": "C Impact of Repetition Penalty Coefficient", "content": "The penalty coefficient, as indicated in line 11 of Table 4, specifies the magnitude of the penalty applied to the reward mechanism when the algorithm suggests a repetitive action. This parameter requires careful adjustment to ensure it harmonizes with the problem's objective value in the reward function. A penalty that is too small may lead the algorithm to frequently repeat actions, whereas an excessively high penalty can generate very large gradient values. Figure 6 illustrates the results of experiments conducted with various penalty coefficients for the MC problem. We trained five distinct models for each penalty coefficient, following the methodology of the main experiments, and conducting evaluations across 100 graph instances with 200 nodes. The findings reveal that penalty coefficients of 0.1 and 1.0 yield the most favorable performance outcomes."}, {"title": "D Analysis of the Memory Module", "content": "The Graph Transformer model (GT) and the memory module are the two core elements with the higher memory consumption.\nLet us denote \\(N = |V|\\) as the number of nodes in the instance graph. The memory complexity of transformers [38], and by extension the GT, is known to scale quadratically with \\(N\\) (O(N2)).\nThe complexity associated with the memory module varies"}, {"title": "D.1 Theoretical Memory Complexity Analysis", "content": "based on the method employed for storing solutions encountered during the optimization process:\n\u2022 Node-wise storage. In MC and MIS", "as": "n\u2022 Node-wise storage: O(N2 + T \u2022 N)\n\u2022 Edge-wise storage: O(N2 + T \u00b7 N2)\nIn scenarios where \\(T\\) is considerably large, the memory requirement associated with solution storage may become the"}]}