{"title": "MARCO: A Memory-Augmented Reinforcement Framework for Combinatorial Optimization", "authors": ["Andoni I. Garmendia", "Quentin Cappart", "Josu Ceberio", "Alexander Mendiburu"], "abstract": "Neural Combinatorial Optimization (NCO) is an emerging domain where deep learning techniques are employed to address combinatorial optimization problems as a standalone solver. Despite their potential, existing NCO methods often suffer from inefficient search space exploration, frequently leading to local optima entrapment or redundant exploration of previously visited states. This paper introduces a versatile framework, referred to as Memory-Augmented Reinforcement for Combinatorial Optimization (MARCO), that can be used to enhance both constructive and improvement methods in NCO through an innovative memory module. MARCO stores data collected throughout the optimization trajectory and retrieves contextually relevant information at each state. This way, the search is guided by two competing criteria: making the best decision in terms of the quality of the solution and avoiding revisiting already explored solutions. This approach promotes a more efficient use of the available optimization budget. Moreover, thanks to the parallel nature of NCO models, several search threads can run simultaneously, all sharing the same memory module, enabling an efficient collaborative exploration. Empirical evaluations, carried out on the maximum cut, maximum independent set and travelling salesman problems, reveal that the memory module effectively increases the exploration, enabling the model to discover diverse, higher-quality solutions. MARCO achieves good performance in a low computational cost, establishing a promising new direction in the field of NCO.", "sections": [{"title": "1 Introduction", "content": "The objective in Combinatorial Optimization (CO) problems is to find the optimal solution from a finite or countable infinite set of discrete choices. These problems are prevalent in many real-world applications, such as chip design [35], genome reconstruction [39] and program execution [16].\nIn recent years, the field of Neural Combinatorial Optimization (NCO) has emerged as an alternative tool for solving such problems [5; 34; 4]. NCO uses deep neural networks to address CO problems in an end-to-end manner, learning from data and generalizing to new, unseen instances. Researchers in this field have followed the steps of heuristic optimization, proposing the neural counterparts of constructive methods [4; 24; 25] and improvement methods [30; 10; 42].\nNeural constructive methods quickly generate an approximate solution in a one-shot manner by means of a learnt neural model. While being simple and direct, constructive methods suffer from their irreversible nature, barring the possibility of revisiting earlier decisions. This limitation becomes particularly pronounced in large problems where suboptimal initial decisions in the construction of the solution can significantly impact the final outcome. To improve the performance of these methods, recent efforts have employed techniques such as sampling, where instead of following the output of the model deterministically, a random sample is taken from a probability distribution given by the output, with the intention of obtaining better solutions and break with the deterministic behaviour, obtaining a richer set of solutions; or beam search [11], which maintains a collection of the highest-quality solutions as it explores the search space based on the output of the neural network, i.e., the probability of adding an item to the partial solution that is being constructed. Similarly, active search [4; 21] is used to update the model's weights (or a particular set of weights) during test time, in order to overfit the model to the test instance to be solved.\nAlternatively, neural improvement methods are closely linked to perturbation methods, such as local search. They start from a complete solution, and operate by iteratively suggesting a modification that improves the current solution at the present state. Unlike constructive methods, improvement methods inherently possess the ability to explore the search space of complete solutions. However, they often get stuck in local optima or revisit the same states repeatedly, leading to cyclical patterns. Recent studies [3; 17] have employed a variety of strategies inherited from the combinatorial optimization literature to tackle these drawbacks. The method by [3] keeps a record of previously performed actions, while the study in [17] maintains a tabu memory of previously visited states, forbidding the actions that would lead to visit those states again.\nNeural constructive methods, neural improvement methods, and most classical optimization proposals all face a significant challenge: exploring efficiently the search space. To address this, we introduce a new framework, referred to as Memory-Augmented Reinforcement for Combinatorial Optimization, or MARCO. This framework integrates a memory module into both neural constructive and neural improvement methods. The memory records the visited or created solutions during the optimization process, and retrieves relevant historical data directly into the NCO model, enabling it to make more informed decisions.\nA key feature of MARCO is the ability to manage a shared memory when several search threads are run in parallel. By doing so, MARCO not only reduces the redundancy of storing similar data across multiple threads but also facilitates a collaborative exploration of the search space, where each thread benefits from a collective understanding of the instance.\nThe main contributions of the paper are as follows: (1) introducing MARCO as a pioneering effort in integrating memory modules within both neural improvement and constructive methods. (2) Designing a similarity-based search mechanism that retrieves past, relevant information to feed the memory and to better inform the model. (3) Presenting the parallelism capabilities of MARCO, which enables a more efficient and collaborative exploration process. (4) Illustrating the implementation of the framework to three graph-based problems: maximum cut, maximum independent set, and travelling salesman problem. Experiments are then carried out on these three problems with graphs up to 1200 nodes. The empirical results indicate that MARCO surpasses some of the recently proposed learning-based approaches, demonstrating the benefits of using information regarding visited solutions. The source code and supplementary material are available online\u00b9."}, {"title": "2 Related Work", "content": "Various strategies have been developed to enhance the exploration of the search space in NCO algorithms. Most of the methods sample from the model's logits [4; 24; 25], which introduces stochasticity into the solution inference process. Beyond sampling, entropy regularization has been implemented during the training of NCO models [22], to ensure the models are not overconfident in their output. Furthermore, [19] proposed a multi-decoder system, where each decoder is trained on instances where it performs best, resulting in a set of specialized and complementary policies.\nDespite these advancements, none of these methods exploit any kind of memory mechanism, which has the potential to leverage previous experiences in the decision-making process and promote exploration.\nIn the work by [17], a tabu search algorithm [18], known for its memory-based approach to circumventing cyclical search patterns, is layered on top of a neural improvement method. The algorithm utilizes a tabu memory to track previously visited solutions. However, this memory serves merely as an external filter, preventing the selection of tabu actions without integrating historical data into the neural model's decision-making process.\nDeepACO [44] uses a neural network to learn the underlying heuristic of an ant colony optimization algorithm [6; 12]. It maintains an external pheromone matrix, indicative of promising variable decisions. However, the integration of this pheromone data is indirect; it is combined in a post-hoc fashion with the output probabilities of the model rather than being an intrinsic part of the learning process.\nCloser to our work, ECO-DQN [3] is a neural improvement method that records the last occurrence of each action. This operation-based memory approach, which simply tracks when actions were last taken, is computationally efficient, requiring only minimal storage. The drawback of this approach is that it only focuses on the actions, failing to consider the overall search context. The effectiveness of an action is often contingent on the broader state of the optimization process, a fact that operation-based memory fails to capture. Compared to this work, we save entire solutions in memory, incorporating a more holistic view of the search context to the system, at the cost of higher memory requirements."}, {"title": "3 MARCO: A Memory-Based Framework", "content": "This section introduces MARCO, the main contribution of this paper. Although the framework can be used for arbitrary CO problems, we first focus on graph-based problems, as they are ubiquitous in combinatorial optimization. In fact, from the 21 NP-complete problems identified by Karp (1972), ten are decision versions of graph optimization problems, while most of the other ones can also be modeled over graphs.\nLet $G = (V, E)$ be a simple graph composed of a set of nodes V and a set of edges E. Finding a solution @ for graph problems often involves finding subsets of nodes or edges that satisfy specific criteria, such as minimizing or maximizing a certain objective function.\nBriefly, the idea of MARCO is to leverage both (1) a learnt policy defining how the current solution should be modified for exploring the search space, and (2) a memory module M, providing information to build the policy. The policy is typically parameterized with a neural network, and especially with a graph neural network [23] when operating on graph problems. Such an architecture has been considered as highly relevant for combinatorial optimization [9]. Besides, the policy is iteratively called to modify the solution until a convergence threshold has been reached.\nThis mechanism can be integrated into both constructive and improvement methods. The main difference relates to how a solution is defined and how information is retrieved from the memory. Let $O_t$ refer to a complete solution obtained after t iterations, and $\\hat{O}_t$ refer to a partial solution, i.e., a solution where only t variables has been assigned, with at least one variable not assigned. In constructive methods, MARCO is capable of using a deterministic policy repeatedly, i.e., opting for the greedy action to generate multiple different constructions. Each construction starts from an empty solution and each optimization step consists in extending the current partial solution, i.e., assigning an unassigned variable in the optimization problem. The policy takes as input both static information (i.e., the graph instance G) and dynamic information related to the current partial solution 6t). The memory then stores a solution once it is completed, i.e., once all the variables have been assigned. On the other hand, improvement methods feature an iterative refinement of a complete solution. Each step modifies a current (complete) solution \u03b8t, transitioning it to a subsequent solution 0t+1. In this scenario, the dynamic information is the complete solution \u03b8t. Each explored solution is recorded into the memory.\nFor both methods, the training is conducted through reinforcement learning. Each time a completed solution is reached, a reward rt is obtained, denoting how good the executed optimization trajectory has been. The reward is designed to balance two factors: (1) the quality of the solution found and (2) the dissimilarity of the new solution compared to previous solutions stored in memory.\nA general overview of MARCO's framework is illustrated in Figure 1."}, {"title": "3.1 Memory Module", "content": "As shown in Algorithm 1, the inference in MARCO starts with the selection of an initial solution (refer to line 2). In each optimization step, the memory module M is responsible for storing the visited solutions (line 6), and retrieving aggregated historical data ht (line 7). The historical data (ht) is aggregated with the current (partial) solution and the graph features (G) to form the current optimization state st (line 8). Subsequently, st is input into the model (line 9), which then proposes a set of actions that generate new solutions (line 10).\nThe specific process of retrieval is shown in Algorithm 2. To retrieve relevant solutions, MARCO employs a similarity-based search. This involves comparing the current (partial) solution (\u03b8t or \u03b8t) with each stored solution (0\u2081, where t' < t) using a similarity metric (e.g., the inner product in line 4). Intuitively, the idea is to fed the policy with the most similar solutions to the current one for executing the next exploration step. We carry out the retrieval using a k-nearest neighbors search (line 5). Rather than simply averaging the k most similar solutions, MARCO uses a weighted average approach, where the weight given to each past solution is directly proportional to its similarity to the current solution. This score is normalized, ranging from 0 (completely different) to 1 (identical), to represent the level of similarity (see line 6).\nAn additional feature enabled by MARCO is the implementation of parallel optimization threads during its inference phase. In this setup, multiple concurrent threads are run for each problem instance, collaboratively exploring the solution space. A key aspect of this functionality is the use of shared memory across all threads. This collective memory stores all the explored solutions by any thread, making it accessible to the entire group."}, {"title": "4 Application of MARCO", "content": "In this study, we demonstrate the adaptability of MARCO to various problem types, encompassing both constructive and improvement methods. We specifically apply MARCO in two scenarios: (1) a neural improvement method for problems with binary variables, such as the Maximum Cut (MC) and the Maximum Independent Set problem (MIS); and (2) a neural constructive method for permutation problems, such as the Travelling Salesman Problem (TSP)."}, {"title": "4.1 Improvement Methods for Binary Problems", "content": "In binary optimization problems, a solution is formalized as a binary vector, denoted as $\\theta \\in {0,1}^{|V|}$ for a problem with |V| variables. Each variable $x_i$ represents a binary decision for the ith variable. Neural improvement methods are designed to optimize a problem by iteratively refining an initial complete solution 00, which can be generated either randomly or through heuristic methods. In the case of binary problems, the central operation is a node-wise operator that flips the value of a node in 0. The memory records visited solutions and their associated actions (e.g., a bit-flip action, consisting in flipping the value of a variable). When a new solution is generated, the model consults the memory to retrieve the actions performed in similar previous scenarios. The importance of the stored actions is given by the similarity between the current solution $O_t$ and previously stored solutions $O_{t'}$ with t' < t. We compute the similarity using the inner product:\n$Similarity(\\theta_t, \\theta_{t'}) = (\\theta_t, \\theta_{t'}) = \\sum_{i\\EV}(\\theta_t)_i (\\theta_{t'})_i$ (1)\nIn this case, the aggregated memory data (ht) is a vector of size V, defined as the weighted average of the actions that were executed in the k most similar solutions (if any). See Figure 2 for a visual description of the inference in neural improvement methods with MARCO.\nThe reward rt obtained by a neural improvement model at each step t is defined as the non-negative difference between the current objective value of the solution, f(Ot), and the best objective value found thus far (f(O*)), i.e., rt = max{f(0+) - f(0*),0}. This reward structure, prevalent in neural improvement methods [33; 42], motivates the model to continually seek better solutions. To prevent the model from cycling through the same states and encourage novel solution exploration, we incorporate a binary penalty term pr, activated when revisiting previously encountered solutions. The adjusted reward for each step is thus rt = rt - Wp \u00d7 pr, where Wp is a weight factor for the penalty."}, {"title": "4.2 Constructive Methods for Permutations", "content": "The objective in permutation problems like the TSP is to find a permutation of nodes in a graph that maximizes or minimizes a specific objective function. Neural constructive methods build the permutation incrementally, starting from an empty solution and adding elements sequentially until a complete permutation is formed.\nIn the context of permutation problems, the solution @ can also be conceptualized as a binary vector $0^b \\in {0,1}^{|E|}$. Each element in this vector corresponds to an edge eij in the graph, indicating whether the edge is part of the solution, that is, whether node i and node j are adjacent in the permutation.\nAt each step of the permutation building process, the model operates on a partial solution, defined as a sequence 0 = (0[1], 0[2], ..., 0[k]), where k < |V| is the current number of nodes in the sequence. As the model progresses through constructing the permutation, the memory data is used to consider which edges have been selected in previously constructed solutions that are similar to p. The similarity score is performed by an inner product between the binary representations of the partial solution and the complete solutions saved in memory $O_{t'}$ , with t' <t:\n$Similarity(\\hat{\\theta_t}, \\theta_{t'}) = (\\hat{\\theta_t}, \\theta_{t'}) = \\sum_{i\\EV}(\\hat{\\theta_t})_i (\\theta_{t'})_i$ (2)\nTraining involves computing a reward once the solution is completed. The reward rt = f(Ot), given by the objective value of the solution, is adjusted by subtracting a baseline value to stabilize training. A common approach is to use the average reward across different initializations, as done in POMO for the TSP [25].\nOur initial experiments with constructive models showed that exact solution repetitions are uncommon for large instances. Therefore, instead of the binary penalty system used in improvement methods, we apply a scaled penalization based on similarity levels with stored solutions. The final reward is calculated as rt = rt - Wp \u00d7 AVGSIM(Ot, Ot'), where AVGSIM(Ot, Ot') is the average of all the inner products between the constructed solution and the k most similar stored solutions."}, {"title": "5 Model Architecture", "content": "Graph neural networks are particularly well-suited to parameterize policy \u03c0. We specifically use a Graph Transformer (GT) [14] coupled with a Feed-Forward Neural Network. GTs are a generalization of transformers [38] for graphs. The fundamental operation in GTs involves applying a shared self-attention mechanism in a fully connected graph, allowing each node to gather information from every other node. The gathered information is then weighted by computed attention weights, which indicate the importance of each neighbor's features to the corresponding node.\nOur model aims to be adaptable to various combinatorial problems, requiring it to assimilate both the graph's structural information and the attributes of its nodes and edges. To achieve this, we modify the GT to incorporate structural information encoded as edge features within the Attention (Attn) mechanism. This adaptation is reflected in the following equation.\n$Attn(Q, K, V) = softmax( \\frac{QK^T + E}{\\sqrt{d_k}} )V$ (3)\nIn this equation, Q, K, and V stand for Query, Key, and Value, respectively, which are fundamental components of the attention mechanism [38] and dk is a scaling factor. $E = W_e e_{ij}$ is a linear transformation of the edge weights, where $W_e \\in R^{1 \\times n_{heads}}$ is a learnable weight matrix, and eij represents the edge features between nodes i and j. E integrates edge information by being added to the attention scores and used in a dot product.\nThe final step involves processing the output of the GT through an element-wise feed-forward neural network to generate action probabilities. The output of the GT could be both node- or edge-embeddings. The performed action depends on the method in use. In our studied cases, we will use node embeddings to generate node-based actions: node-flips for improvement methods in binary problems and node addition to the partial solution for the constructive method in permutation problems. However, MARCO is also applicable to edge-based actions, such as pairwise operators (swap, 2-opt) for permutation-based improvement methods.\nWe utilize the policy gradient REINFORCE algorithm [41] to find the optimal parameter set, \u03c0*, which maximizes the expected cumulative reward in the optimization process."}, {"title": "6 Experiments", "content": "We validate the effectiveness of MARCO across a diverse set of CO problems both binary and permutation-based: the Maximum Cut (MC), Maximum Independent Set (MIS) and the Travelling Salesman Problem (TSP)."}, {"title": "6.1 Problems", "content": "Maximum Cut (MC). The objective in MC [13] is to partition the set of nodes V in a graph G into two disjoint subsets V\u2081 and V2 such that the number of edges between these subsets (the cut) is maximized. The objective function can be expressed as: max \u2211(u,v)\u2208\u0395 \u03b4[\u03b8u\u2260\u03b8\u03c5] where \u03b8u and \u03b8\u03c5 are binary variables indicating the subset to which nodes u and v belong, and d is a function which equals to 1 if \u03b8u and \u03b8\u03c5 are different and 0 otherwise.\nMaximum Independent Set (MIS). For the MIS problem [28], the goal is to find a binary vector 0 that represents a subset of nodes SC V in a graph G such that no two nodes in S are adjacent, and the size of S is maximized. The objective function can be formulated as: max | S such that (u, v) E for all u, v \u2208 S\nTravelling Salesman Problem (TSP). In TSP [27; 40], given a set of nodes V and distances du,v between each pair of nodes u, v \u2208 V, the task is to find a permutation @ of nodes in V that minimizes the total travel distance. This is expressed as: min = d(\u03b8\u03b5, \u03b8\u03b5+1) with 0\\v1+1 = 01"}, {"title": "6.2 Experimental Setup", "content": "Training For each problem, we train a unique model, using instances that vary in size. This helps the model to learn strategies that can be transferable between differently sized instances. For the MC and MIS, we used randomly generated Erdos-Renyi (ER) [15] graphs with 15% of edge probability, and sizes ranging from 50 to 200 nodes. For the TSP, fully connected graphs ranging from 50 to 100 nodes were generated, in which cities were sampled uniformly in a unit square. The total training time depends on the problem. The models for both MC and MIS required less than 40 minutes, while the one for the TSP required a significantly longer training (4 days) to reach convergence. See the supplementary material for a detailed description of the training configuration used.\nInference To evaluate the performance of MARCO, we have established certain inference parameters. For MC and MIS, we set the neural improvement methods to execute with 50 parallel threads (processing 50 solutions simultaneously), stopping upon 2|V| improvement steps. For the TSP, we use 100 parallel initializations (as done in POMO [25]) and 20 iterations (solution constructions) for each instance. We have used k = 20 for the similarity search. A more detailed description of the inference configuration used is reported in the supplementary material. MARCO has been implemented using PyTorch 2.0. A Nvidia A100 GPU has been used to train the models and perform inference. Exact methods and heuristics serving as baselines were executed in a cluster with Intel Xeon X5650 CPUs.\nEvaluation Data Following the experimental setup of recent works [1; 7; 45], we will evaluate the MC and MIS problems in ER graphs of sizes between 700-800, and harder graphs from the RB benchmark [43] of sizes between 200-300 and 800-1200. For TSP, we follow the setting from [24] and use randomly generated instances, with uniformly sampled cities in the unit square. We use graphs of sizes 100, 200 and 500.\nAblations We evaluate MARCO through several ablations that help us understand the impact of its different components. We begin by evaluating standalone models proposed in this work: the Neural Improvement Method (NIM) and Neural Constructive Methods (NCM), both of which operate without any integrated memory module. Next, for improvement methods, we add a NIM equipped with an operation-based memory (Op-NIM), tracking the number of steps since each action was executed lastly (imitating ECO-DQN [3]). Finally, we asses MARCO-ind, a variant of MARCO that operates without shared memory, executing multiple threads simultaneously but independently, with each thread maintaining its own separate memory.\nBaselines To assess MARCO's performance, we conduct a comprehensive comparison against a broad spectrum of combinatorial optimization methods tailored to each specific problem addressed. Our comparative analysis includes exact algorithms, heuristics, and learning-based approaches\nFor the MC, our comparison includes the GUROBI solver [20], the local search enhanced heuristic BURER [8], and ECO-DQN [3], which is a neural improvement method incorporating an operation-based memory.\nFor MIS, we also include GUROBI [20], together with KAMIS [26], a specialized algorithm for MIS; and a constructive heuristic (Greedy), that selects the node with minimum degree in each step. Furthermore, we examine also a range of recently proposed learning-based methods: DGL [7], LwD [1] and FlowNet [45].\nFor the TSP, we report results of the well known conventional solver Concorde [2], the heuristic LKH-3 [36], the Nearest Neighbor (NN) heuristic; and the learning-based methods used are the neural constructive POMO [25] enhanced with sampling and data augmentation, LEHD [31] which reports the best results among neural methods and two of the state-of-the-art neural improvement methods: DACT [33] and NeuOPT [32]."}, {"title": "6.3 Ablation Study", "content": "We evaluate the impact of the proposed memory module on our model's learning dynamics and performance. Figure 4 (a) illustrates the revisit frequency of already visited solutions during the first 100 training episodes. This metric reflects the model's ability to explore diverse solutions effectively. Figure 4 (b) shows the increase in the objective value of solutions as training progresses.\nThe results indicate that the model without memory tend to repeat actions more frequently. In contrast, incorporating an operation-based memory module reduces this repetitiveness, facilitating broader exploration. Furthermore, MARCO's memory module demonstrates superior exploration capabilities among the tested configurations.\nOverall, the study confirms that integrating the proposed memory module not only improves the model's exploration capabilities but also leads to a measurable improvement in the quality of the generated solutions."}, {"title": "6.4 Performance Results", "content": "We present the results for each studied problem in a table divided by three row-segments, the first one consisting of non-learning methods (exact and heuristic), the second with recent learning methods from the literature, and the third with the methods (MARCO and ablations) proposed in this paper. We report both the average objective value in the evaluation instance set and the time needed for performing inference with a unique instance (batch size of 1). We use ms, s and m to denote milliseconds, seconds and minutes, respectively. For learning methods, we report the results from the best performing configuration reported in the original paper. For exact solvers, we report the best found solution when the optimal solution is not achieved in a limit of 1 and 10 minutes per instance.\nMC In Table 1 we report the results for the MC. MARCO significantly outperforms GUROBI and ECO-DQN, especially in larger problem instances (ER700-800, RB800-1200). In addition, MARCO proves to be competitive against the state-of-the-art heuristic, BURER, in the studied graph instances. The ablation results show that using the proposed memory scheme is superior to (1) not using any memory module, and (2) using an operation-based memory. Moreover, using a shared memory slightly improves the performance (with respect to MARCO-ind), while the computational cost is reduced. Compared to the ECO-DQN in computational cost, MARCO reduces the time needed to perform 2|V| improvement steps.\nMIS Table 2 summarizes the results for MIS. Here, MARCO is also able to surpass the learning methods and its ablations, obtaining a comparable performance to the exact solver. Moreover, it reduces the gap to the specialized KAMIS algorithm. While incorporating a memory module in MARCO (NIM vs. MARCO) increases the time cost, it contributes to achieving superior solutions, while NIM gets stuck in suboptimal solutions (increasing the number of steps does not increase the performance).\nTSP Results for the TSP are reported in Table 3. MARCO can obtain good inference performance in the studied instances, reaching the best found solutions for N100 and N200; and being second on N500, only surpassed by LEHD. It is important to note that our basic NCM implementation (without memory) obtains comparable results with the state-of-the-art learning method while being orders of magnitude faster. Also, MARCO improves over both NCM and the method without sharing memory (MARCO-ind).\nGeneralization to Larger Sizes\nTraining NCO models with reinforcement learning is computationally intensive, leading to a common practice in the literature where models are often trained on smaller-sized instances (up to 100). While this approach is understandable due to resource constraints, it is important to consider the ability of these models to generalize to larger instances. This aspect is crucial for their applicability in real-world scenarios, where problem sizes can vary significantly.\nThe data presented in Table 2 illustrate this point. Here, even a basic greedy constructive method manages to outperform more complex learning-based methods (DGL, LWD, and FlowNet) in the RB800-1200 instance. This observation underlines the importance of using simple heuristics as a sanity check to assess whether advanced models are effectively generalizing to unseen instances or larger sizes. Similarly, Table 3 reveals that a simple Nearest Neighbour heuristic is able to surpass POMO, DACT and NeuOPT in instances of 500 cities. Even though the underlying model of MARCO has been trained on smaller instances (up to 200 for MC and MIS, and up to 100 for TSP), it is able to maintain a good performance in larger graphs with a lower time cost compared to state-of-the-art heuristic solvers."}, {"title": "7 Limitations and Future Work", "content": "MARCO offers significant advancements in neural combinatorial optimization. However, it has room for improvement. A primary concern is the uncontrolled growth of its memory during the optimization process, as it continually stores all the encountered states, leading to increased computational and memory costs. To counter this, future work could focus on implementing mechanisms to prune the memory by removing redundant information.\nAnother limitation is the substantial resource requirement for storing entire edge-based solutions in memory (like in TSP). This approach, particularly for large instances, can result in high memory consumption and slower retrieval processes. A promising direction would be to represent solutions in a lower-dimensional space using fixed-size embeddings, effectively reducing the memory footprint while preserving (or even incorporating) necessary information.\nIn terms of data retrieval, MARCO currently employs a method based on a weighted average of similarity, which may not fully capture the relationships between solution pairs. A more advanced alternative to consider is the implementation of an attention-based search mechanism. This method would not only prioritize the significance of various stored solutions but could also incorporate the objective values or other distinct characteristics of these solutions to compute their relevance.\nAdditionally, while not a limitation, applying MARCO to new problems or integrating it with different NCO methods requires careful consideration in how memory information is aggregated with instance features. The nature of the data stored and retrieved can vary significantly depending on the specific problem being addressed."}, {"title": "8 Conclusion", "content": "In this paper, we have introduced the Memory-Augmented Reinforcement for Combinatorial Optimization (MARCO), a framework for Neural Combinatorial Optimization methods that employs a memory module to store and retrieve relevant historical data throughout the search process. The experiments conducted in the maximum cut, maximum independent set and travelling salesman problems validate MARCO's ability to quickly find high-quality solutions, outperforming or matching the state-of-the-art learning methods. Furthermore, we have demonstrated that the use of a collaborative parallel-thread scheme contributes to the performance of the model while reducing the computation cost."}, {"title": "A Further implementation details", "content": "In order to solve the Maximum Cut (MC), Maximum Independent Set (MIS) and Travelling Salesman Problem (TSP), MARCO receives information from three sources: the static instance information, the dynamic (partial) solution and the historical memory data."}, {"title": "A.1 MC and MIS", "content": "In MC and MIS, the instance information is given by binary edge features $y \\in R^{|E|}$, which denote the existence of an edge in the set of edges E of the graph G. The solution is also encoded in binary node features $x \\in R^{|V|}$, indicating which of the two sets the node belongs to, and where V is the set of nodes. Visited solutions and performed actions (node flips) are stored in memory. The model retrieves a weighted average of the actions performed in similar solutions and this information is concatenated as an additional node feature."}, {"title": "A.2 TSP", "content": "In TSP, we follow prior works [24; 25] with some modifications:\n\u2022 While prior works only use node features (city coordinates), we also consider the use of distances between cities as edge features, since we believe the relative information is advantageous for the model.\n\u2022 We implemented a Graph Transformer (GT) [14] encoder layer that also considers the edge features.\n\u2022 We increased the model size (embedding dimension). We obtained better results scaling it. See the selected model's hyperparameters in Table 4 (lines 1-5).\n\u2022 ReLU activation function is replaced by SwiGLU [37].\n\u2022 We implemented a function to compute an arbitrary number of data augmentations (coordinate rotations).\nThe memory data is aggregated as an edge feature, and gives information about the number of times each edge has been considered in previous solutions or routes. These edge features are used in the MHA mechanism of the decoder [24; 25], where a linear projection of the memory features is added to the attentions weights.\n$Attn(Q, K, V) = softmax( \\frac{QK^T}{\\sqrt{d_k}} + E )V$ (4)"}, {"title": "A.3 Training in MARCO", "content": "The training hyperparameters can be found in Table 4 (lines 6-15). We trained a unique model for each problem. Each epoch of training comprised 1000 episodes. Within each episode, 128 new instances of an arbitrary size (50-200 for MC and MIS and 20-100 for TSP) were randomly generated. The optimization of model parameters was conducted using the AdamW optimizer [29], and we clipped the gradient norm at a value of 1.0.\nThe training of improvement models (for MC and MIS) requires additional hyperparameters, such as the discount factor and episode length. This necessity comes from the reward mechanism employed in these models, which is based on the concept of discounted future rewards, i.e., focusing on maximizing future gains. A discount factor of 0.95 and an episode length of 20 steps were selected.\nWe adopted the following two-phase training pipeline for constructive models. The initial phase comprises 200 epochs of training without the incorporation of memory, following the training of POMO [25].\nSubsequently, we fix all model weights and introduce a trainable Feed-Forward network into the decoder, which processes the data retrieved from the memory. For the TSP, the processed memory content is incorporated into the edges, and it is used within the Multi-Head Attention mechanism of the decoder in the same fashion as in the GT encoder. This second training stage extends for an additional set of 50 epochs. Within each training episode during this stage, the process begins with a deterministic rollout performed with an empty memory. This is followed by executing 5 construction iterations (line 15) using identical problem instances. In these iterations, the model is penalized based on the similarity of its proposed solutions to those generated in previous iterations."}, {"title": "A.4 Inference in MARCO", "content": "The performance results discussed in the paper have been obtained using models trained with the hyperparameter values shown in Table 4 (16-21).\nLines 16-18 have common parameters for both improvement and constructive methods: the number (K) of neighbors to consider in the memory retrieval process, the number of parallel execution threads, and the maximum number of solutions stored in memory (once this number is reached the oldest solutions are replaced).\nThe Max steps parameter denotes the number of improvement steps, while Constructions denote the number of solutions built in constructive methods.\nLastly, for the TSP, we modulate the frequency of data retrieval from the memory. Instead of conducting this search at every construction step, we establish a predetermined retrieval frequency ("}]}