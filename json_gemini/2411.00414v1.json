{"title": "On the Opportunities of Large Language Models for Programming Process Data", "authors": ["JOHN EDWARDS", "ARTO HELLAS", "JUHO LEINONEN"], "abstract": "Computing educators and researchers have used programming process data to understand how programs are constructed and what sorts of problems students struggle with. Although such data shows promise for using it for feedback, fully automated programming process feedback systems have still been an under-explored area. The recent emergence of large language models (LLMs) have yielded additional opportunities for researchers in a wide variety of fields. LLMs are efficient at transforming content from one format to another, leveraging the body of knowledge they have been trained with in the process. In this article, we discuss opportunities of using LLMs for analyzing programming process data. To complement our discussion, we outline a case study where we have leveraged LLMs for automatically summarizing the programming process and for creating formative feedback on the programming process. Overall, our discussion and findings highlight that the computing education research and practice community is again one step closer to automating formative programming process-focused feedback.", "sections": [{"title": "1 Introduction", "content": "Feedback can have a tremendous impact on learning and achievement [26]. The level of detail of the feedback influences its effectiveness [80], and feedback can be given at many levels ranging from targeting how to work on and complete specific tasks to considering personal characteristics and behavior [26, 36, 59]. In teaching and learning pro- gramming, automated assessment systems have been a key tool for providing feedback at a scale already for more than a half a century [30, 36, 61]. Researchers have sought to automate step-by-step guidance [78], provide hints during the programming process [55], improve programming error messages [6], and aid in providing textual feedback by grouping similar code submissions together [23, 37, 58]. To support the understanding of how novices construct programs, researchers and educators have been collecting increasing amounts of data from students' programming process [31]. Such data can be collected at multiple granu- larities, ranging from final course assignment submissions to individual keystrokes from solving the assignments [31]. Programming process data has been, for example, used to play back how students construct their programs step by step or keystroke by keystroke to create a broader understanding of the process [27, 73, 83]. So far, despite shared efforts towards providing timely feedback to students [33], the potential of fine-grained programming process data for feedback purposes is still largely untapped. Large Language Models (LLMs) are a potential tool for realizing the transformation of programming process data into actionable feedback items. Within Computing Education Research, LLMs have broadened the horizon of what computing education researchers and practitioners can achieve [65], calling even for rethinking how computer science and programming is taught [16]. Large Language Models have been shown to help in creating assignments [51, 72],"}, {"title": "2 Programming Process Data", "content": "In this section, we discuss the types and different uses of programming process data. Two surveys [18, 31] are resources for discussion of both the different granularities of raw process data and the wide variety of studies and research questions being purused using this type of data."}, {"title": "2.1 Types of data", "content": "Programming process data has been collected in various forms. Ihantola et al. [31] discuss process data at six discrete levels and Edwards et al. [18] define four levels, which are, starting from the least granular: submissions and com- mits [e.g., 2, 39]; executions, compilations, and file saves [e.g., 13, 32, 34]; line-level edits, which capture all contiguous changes in a single line in a single event [e.g., 9, 11]; and finally, keystroke and character edits [e.g., 12, 18, 45, 50]. Our interest is in using occasional snapshots of code and analyze the progression of one snapshot to another. Any granularity of process data would potentially be suitable for our purposes, but, in practice, students may not submit or commit their data often enough, nor do executions or compilations guarantee sufficient temporal resolution. How- ever, line-level and keystroke-level data are generally too fine-grained. In this paper, we use keystroke-level data but discretize it into code snapshots that immediately precede a break in keystrokes of at least five minutes. The five-minute threshold is based on a probabilistic model by Hart et al. [25] that shows that breaks of five minutes statistically indicate a 50% chance that the student has disengaged. Among the many other thresholds that have been used, which arbitrarily range from 60 seconds to one hour [35, 48, 57, 67], one study used a threshold of five minutes [42], but it pre-dated Hart's statistical model and was arbitrarily chosen. A small number of public datasets are available that would be suitable for our experiments, including Blackbox [9- 11], which has line-level edit data, and a CSEDM challenge dataset which has compile-level granularity. We chose the dataset provided by Edwards et al. [18], which is keystroke-level, because it gives us more flexibility over the temporal resolution of code snapshots."}, {"title": "2.2 Uses of programming process data", "content": "Programming process data has been used in biometrics and authentication [41, 49, 50, 52, 56] and plagiarism detec- tion [28, 70, 73], but the most common use of programming process data is in predicting course outcomes. Early"}, {"title": "3 Possibilities of Large Language Models for Programming Process Data", "content": "Large language models provide many exciting opportunities for analyzing and utilizing programming process data. One potential application of LLMs is to summarize the process into natural language. Before LLMs, most work in programming process data used derived metrics like time-on-task, number of events, etc. to analyze the process [31, 43], but here the utility of the data is only as good as the metrics derived from it. With LLMs, more abstract aspects of the process could potentially be derived. Another stream of work has looked into visualizing the programming process based on programming process data [17, 27, 73, 81], but here one issue is that the visualizations might be hard to interpret, or if the whole process is visualized keystroke-by-keystroke (see e.g. [17, 27, 81]), it is very time consuming to analyze. Another potential application of LLMs for programming process data is to give feedback to students. When the whole process is available, feedback could be given with different granularities. For example, it could be possible to give high-level feedback on the whole process (e.g., \u201cyou should run code more frequently", "you recently modified your function 'calculateSum' but there is a small bug\"). Ideally, feedback using LLMs could be given during the process, which could support students while they are working on their programs. Some recent work has looked into generating synthetic data using LLMs [24, 62, 63": "which is one potential applica- tion of LLMs for programming process data. One issue with programming process data, especially very fine-grained data such as keystroke data, is that individuals can sometimes be identified using \"keystroke dynamics\", i.e., based on their unique typing rhythms [3, 52, 75]. This is an issue for sharing data openly [18, 47], since typically data that"}, {"title": "4 Case Study: Analyzing Programming Process Data with LLMs", "content": "Here, we outline our case study of analyzing programming process data using LLMs. For the case study, we explored how well three different LLMs 1) summarize the programming process based on the programming process data and 2) generate programming process feedback for students based on the programming process data."}, {"title": "4.1 Process data and preprocessing", "content": "For the process data, we used a publicly available Python dataset published by Edwards et al. [18]. The dataset has 44 participants and 8 assignments from an introductory programming course organized at Utah State University in the United States. The dataset was collected using the PyPhanon plugin [19] for PyCharm, and it consists of 1 million unique events. For the present work, we focused on three of the assignments. Outlines of the assignments are given in Table 1 and are taken from assignment descriptions included with the dataset. Using the logs from five students working on three assignments, we construed the programming process from the logs into a sequence of state snapshots that outlined how the assignment code evolved over time from the beginning to the end. For the present analysis, we created the sequence of state snapshots by including the first and last code states"}, {"title": "4.2 Large language models and prompt engineering", "content": "For the analysis, we used Anthropic's Claude 3 Opus, OpenAI's GPT-4 Turbo, and Meta's LLaMa2 70B Chat model. The first two models are proprietary, while the third model is an open-source model. The models were selected to represent a snapshot of the state-of-the-art LLMs at the time of the analysis in March and April of 2024. We acknowl- edge that OpenAI has the GPT-4 model that is known to perform better than the GPT-4 Turbo. However, the context window size for the GPT-4 version that was available for us was limited to 8k tokens, and our analyses highlighted that this was not sufficient. Thus, we opted for the GPT-4 Turbo which has a context window size of 128k tokens. This is also a limitation in some of our analyses where we leverage the LLaMa2 70B Chat model. Our prompt engineering had the objective of having LLMs describe the process a student took to write their com- puter program and then to provide feedback to the student as if the Al were an expert teaching assistant (TA). Follow- ing prompting best practices [20], we iteratively explored and refined a range of prompts, including providing context and domain information, personalizing the responses, providing information about the format of the input data, and providing guidelines and constraints on the expected outcomes. One of the authors was in charge of the prompt engi- neering process, sharing intermediate results and observations with the other authors. During the prompt engineering process, the research team met weekly, commenting on the outputs and the prompts, and discussing further prompt improvement possibilities and briefly evaluating their impact on the outcomes. During the prompt engineering process, we observed that the models were relatively poor at incorporating times- tamp information. As an example, if a student had a break that consisted of multiple days, an LLM might suggest that the student was thinking hard about the problem for days. Thus, we omitted the timestamp data. We further explored specifying and not specifying the number of items in the feedback, and in the end resorted to not restricting the number of items for the present evaluation. The final prompt for providing feedback on the programming process is outlined in Figure 1. When comparing the prompt for providing feedback on the programming process to the prompt for summarizing the programming process, the main differences are in the persona \"[...] and an expert in summarizing how students construct their programs [...]\" and the explicit task instructions \"As an extremely good introductory programming teaching assistant, summarize"}, {"title": "4.3 Data generation", "content": "To generate the data, we used Anthropic's and OpenAI's APIs for Claude 3 Opus and GPT-4 Turbo respectively, while LLaMa2 70B Chat model was run on the HuggingFace platform. The data generation led to a total of 45 programming process descriptions (3 assignments \u00d7 5 students \u00d7 3 models) and 45 feedbacks based on the programming process, lead- ing to 90 entries. Table 3 provides descriptive statistics of the data generation process, outlining the average response time and the average response length for each of the models."}, {"title": "4.4 Evaluation", "content": "The evaluation was divided into three parts. First, one of the researchers conducted a surface-level summary evaluation of the outputs to study whether the models followed the task. This was followed by three researchers analyzing a subset of the data to form a shared understanding of the data. Finally, one of the reviewers conducted a thematic analysis of the improvement suggestions in the feedback to identify recurring themes in the outputs."}, {"title": "4.5 Results", "content": ""}, {"title": "4.5.1 Surface-level analysis.", "content": "On a surface level, the LLMs followed the instructions and provided expected outputs adequately. For the 15 inputs (5 students and 3 assignments), GPT-4 Turbo generated 15 acceptable (see below for our definition of acceptable) programming process summaries and 14 feedbacks, Claude 3 Opus generated 13 acceptable summaries and 11 feedbacks, and LLaMa2 70B Chat generated 10 acceptable summaries and 7 feedbacks. While GPT-4 Turbo and Claude 3 Opus provided a response for all student-exercise-prompt combinations, LLaMa 70B Chat failed to provide a response in two instances (one summary and one feedback) due to the prompt being too long. The main reason why a summary was not deemed acceptable was that the response focused on describing one of the code states but did not summarize the process. Similarly, the main reason when a programming process feedback was not deemed acceptable was that the feedback did not explicitly include information about the process, but provided more generic improvement suggestions such as suggesting more efficient ways to implement the algorithms, adding type hints, and so on."}, {"title": "4.5.2 Analysis of programming process improvement suggestions.", "content": "Overall, while the models did often provide similar suggestions, there were also differences in the focus and style of the feedback. The GPT-4 Turbo model often used a logical structure to the feedback consisting of generic comments, specific comments, and a conclusion. The feedbacks from Claude 3 Opus and LLaMa 70B Chat model on the other hand were typically more list-like, providing suggestions directly. When considering the programming process feedback as a whole, the models typically followed the implicit instruction of being constructive and providing feedback in a positive fashion as the feedback was directed to students. The feedback items often included something that the student was praised on, even if just for adding comments or for exploring, as shown in the following quotes (mainly from GPT-4 Turbo). \"It's great to see you organizing your code with comments, making it easier to understand the purpose of each section. Keep maintaining this practice.\" \"It's great that you started by experimenting with the random number generation in steps 1 and 3 to understand how it works. This exploratory coding is a good way to familiarize yourself with new concepts.\" The models also often captured back and forth movement where students first added code and then removed it, as well as scenarios where students started working on implementations that were not in line with the problem statement. Such feedbacks included comments that would suggest the student to step away from coding to thinking about the problem, as shown by the following quotes. \"The initial steps indicate a bit of confusion regarding the problem statement. [...] Taking time to fully understand the problem before starting to code can save a lot of time and confusion.\" \"You seem to jump into implementing parts of the solution without fully forming the algorithm in your mind or on paper first. It's important to sketch out a basic algorithm or pseudocode before you begin coding, especially for a problem with several specific steps like this one.\""}, {"title": "4.5.3 Researcher insights from programming process summarization and feedback.", "content": "The AI systems were explicitly in- structed not to include feedback on the code itself, but rather to focus on the process of writing the code. (One round of prompting includes the statement, \"Only provide feedback on how to improve the programming process. Do not provide any summary description of the programming process data and do not provide explicit suggestions on how to improve the code.\" Other prompts include similar statements.) However, the LLMs were found to have great difficulty in recognizing the difference between the process a student takes to write code and the code itself. For example, the AI gave the following process-related feedback to one student: \"Incremental Development and Testing: I noticed that you began writing large blocks of code before testing smaller units. It's important to approach complex problems by breaking them down into smaller, more manageable pieces. Once you write a small part, such as a function to calculate the sum of factors, test it thoroughly before moving on. This will help you catch errors early and understand each part of your code better. Consider implementing your code piece by piece, testing each function independently before integrating it into your main logic.\" The same feedback set also includes the following, which relates to the code itself: \"Efficiency in Your Loops and Functions: Your approach to finding the Fluky Numbers involves quite a bit of redundancy and potentially unnecessary calculations. For example, in calculating the sum of factors, think about how you might optimize this process to avoid redundant calculations or checks. In addition, I see you've modified the 'factorSum' function to return both a sum and a count of factors, but the count doesn't appear to be utilized in later logic. Always review your functions to ensure they are doing exactly what is needed for the task at hand, no more, no less. This will help keep your code efficient and focused.\" The fact that the feedback includes the word \"process\" in the feedback indicates that it may be confusing the pro- cess of code execution with the process of code writing. Every single summary and feedback output from the LLMs included something relating to the code and not process. In retrospect, this should not have been surprising to us. Even we humans tend to find it challenging to distinguish between the two. One possible reason for this is that software engineers and programming educators are attuned to thinking about code but think less about the evolution of code into its final form. Thus, when we look at a code snapshot, we tend to analyze it as a standalone product, rather than in its temporal context. Furthermore, considering it through the lens of cognitive load theory, analyzing a code snapshot relative to a prior snapshot requires roughly double the working memory, which is already stretched when reading code. From a philosophical perspective, it makes sense that LLMs suffer from this as well, as they are trained on data generally generated by humans, and there is very little data that deals with the programming process. Furthermore, the majority of generated summaries and feedback that are related to the programming process are generic. The above example that suggests that the student use incremental development and testing is typical: incre- mental development is so commonplace that the only thing keeping it from becoming cliche is that it is so rarely actually practiced. Whereas our vocabulary of things that can be improved in code is vast (e.g., variable names, com- ments, duplicated code, algorithm complexity, code organization into functions, etc), educators have far fewer can- didate improvements to the programming process, which are generally limited to incremental development, early decomposition, and planning ahead. A challenge in evaluating quality of summaries and feedback is their subjectivity. Feedback in particular, is a chal- lenge because, in some cases, educators disagree on best practices. For example, many educators encourage up-front design whereas others encourage a more agile, refactoring approach. In order to make evaluation of AI output objec- tive, the AI would need to be primed with opinions on controversial topics. Related to this, educators would need to decide which topics are the most important. In our case study, the AI responses were often excessively long, so the educator would need to limit the length of output and indicate to the Al which topics to prioritize. So what criteria should an AI be judged on? We propose five scores. The first would be a hallucination score. This would be straightforward to measure with standard multi-rater coding practices. The second score would measure how well the Al distinguishes between process and analysis of static code. As discussed, this distinction is challenging for both LLMs and humans. However, we found that determining how well the AI makes the distinction is actually not too difficult. The human evaluator needn't consult the code snapshots to determine if the LLM output addresses process or code. The third score would be specificity against genericness relative to the given submission. An AI that speaks in tropes is not useful. And finally, scores on correctness and utility would be needed. These are straightforward"}, {"title": "5 Discussion", "content": "Overall, we here make an argument that exploring the ability of LLMs to provide feedback on programming process is worthwhile. The argument is in two parts. The first is the argument that feedback itself on programming process is useful. While giving students feedback on code is common, giving them feedback on how they went about writing the code is rare. This is because process data isn't easy to collect, but also because little research has been done into best practices in code evolution in the context of novice programmers. We claim that guiding students on this front will result in less frustration, better learning, reduced attrition, and increased diversity in the computer programming community. If we accept that providing guidance to students on process is useful, then we can address the other part of the argument, that AI in general, and LLMs specifically, are well-suited to providing this feedback to students. The primary reason for this is that analyzing how a code evolved is far more difficult than analyzing a single code snapshot. Well-prompted LLMs have vast memory resources and can evaluate far more code submissions than a human, and can do it in a short amount of time. One way to make process feedback even more effective could be to combine it with a playback tool such as Keystro- keExplorer [18]. We note that in our case study, we considered only snapshots of code immediately preceding a break of at least five minutes. During our analysis, we found that this granularity often left the reader disoriented. Actually watching the keystroke-level evolution of the code between snapshots could be helpful in keeping the student oriented as they receive the AI-generated feedback, and also potentially create self-reflection opportunities, in a similar way as one could e.g. review their chess games. We see that already finding out the appropriate granularity (and data format) could significantly improve the prelim- inary results outlined in this article, not to mention the emergence of newer and more powerful LLMs. We also see that using LLMs for creating feedback on the programming process data and providing that feedback to students during the programming process could help in pinning down the points in time when feedback should be given, which has been an open question in prior programming process feedback research [33]. To begin the discussion at the conference, and beyond, we outline the following five research directions for using LLMs on programming process data: \u2022 Cost-efficient representations of programming process data for LLMs, including evaluating data representation types such as diffs and exploring the utility of different data granularities. \u2022 Providing insights from programming process data to students, exploring when and how to present the insights for maximal effectiveness, including incorporating the LLM-generated insights into existing systems such as IDEs and programming process visualizers. \u2022 Analyzing the subjectivity of LLM-generated insights and their utility to learners and instructors, and identi- fying contextual factors that contribute to the subjectivity. \u2022 Exploring the opportunities of leveraging programming process data with LLMs for supporting students in learning about the process of programming, which has been classically highlighted as one of the challenges in learning complex skills such as programming [7, 15, 77]."}]}