{"title": "Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation", "authors": ["Junxuan Yu", "Rusi Chen", "Yongsong Zhou", "Yanlin Chen", "Yaofei Duan", "Yuhao Huang", "Han Zhou", "Tan Tao", "Xin Yang", "Dong Ni"], "abstract": "Echocardiography video is a primary modality for diagnosing heart diseases, but the limited data poses challenges for both clinical teaching and machine learning training. Recently, video generative models have emerged as a promising strategy to alleviate this issue. However, previous methods often relied on holistic conditions during generation, hindering the flexible movement control over specific cardiac structures. In this context, we propose an explainable and controllable method for echocardiography video generation, taking an initial frame and a motion curve as guidance. Our contributions are three-fold. First, we extract motion information from each heart substructure to construct motion curves, enabling the diffusion model to synthesize customized echocardiography videos by modifying these curves. Second, we propose the structure-to-motion alignment module, which can map semantic features onto motion curves across cardiac structures. Third, The position-aware attention mechanism is designed to enhance video consistency utilizing Gaussian masks with structural position information. Extensive experiments on three echocardiography datasets show that our method outperforms others regarding fidelity and consistency.", "sections": [{"title": "Introduction", "content": "Echocardiography is a primary method that relies on dynamic video to obtain structural information for clinical diagnoses [24]. However, training radiologists with diagnostic skills and establishing machine learning models both suffer from limitations on video resources. Recently, video generation models have demonstrated a promising ability to solve this problem, owing to their powerful capability in modeling data distribution [22,21]. Several studies about video generation"}, {"title": "Extraction of Motion Curves", "content": "Previous studies [11,10] faced challenges in controlling the motion of specific cardiac substructures since they relied solely on the single sparse condition (LVEFs), while our approach aims to provide fine-grained control over the motion of each cardiac substructure. Therefore, we extract motion curves for the key structures (e.g., Left Ventricle, Left Atrium, Mitral Valve, etc.) as conditions.\nAs illustrated in Fig. 2, the process of extracting motion curves is as follows: (a) We employ a well-trained anatomy detector (average accuracy=85%) to identify each substructure in each frame of the echocardiogram video. (b) Subsequently, we utilize the pixel coordinates of each substructure's bounding box (bbox) as the basis for encoding the motion curves, which form the basis for encoding the motion curves, represented as $f_m \\in \\mathbb{R}^{B \\times N \\times C \\times (4 \\times 2)}$, where N represents the number of frames and C represents the categories of the substructure. Notably, any missed detected structures would be treated as learnable parameters, initialized by the network. (c) Since cardiac motion is periodic, we employ Fourier transformation (FT) to transform the pixel coordinates into a high-dimensional feature representation, denoted as $f_m \\in \\mathbb{R}^{B \\times N \\times C \\times E}$, where E represents the dimensionality of the features obtained from FT. (d) Finally, the motion embedding is passed through several multi-layer perceptron (MLP) layers. Overall, these motion curve features can be formulated as $f_m \\in \\mathbb{R}^{B \\times N \\times C \\times 1024}$"}, {"title": "Structure-to-Motion Alignment Module", "content": "Although the motion curves of the echocardiography videos are captured, it is challenging for the model to distinguish the relationship between the motion curves and the semantic information of each substructure. Recently, GLIGEN [6] has demonstrated its effectiveness in combining caption and bbox information, enhancing visual-language understanding to enable fine-grained control over specific objects in natural images. However, our preliminary experiments suggested that GLIGEN struggles to effectively represent and interpret texts related to cardiac structures in echocardiography.\nInnovatively, we replace texts with the cardiac structure features, denoted as $f_i$. As shown in Fig. 2, to obtain $f_i$, the regions of interest (ROI) that correspond to the cardiac substructures in the initial frame are cropped by the well-pretrained detector as mentioned above. Then, a pretrained CLIP image encoder [9] is utilized to transform the ROIs into structural embedding features. Then, these features pass through several MLP layers, resulting in an output denoted as $f_a \\in \\mathbb{R}^{B \\times N \\times C \\times 1024}$, which has the same shape with the extracted motion curve features $f_{im}$ mentioned in Sec. 2.1. Consequently, structural features and their corresponding motion curve features from the same category are"}, {"title": "Position-aware Attention Mechanism", "content": "Enhancing the consistency of cardiac motion is crucial in the task of cardiac video generation. To address this, we aim to inject the positional information of the cardiac structure into the cross-attention mechanism. Specifically, we design Gaussian masks that are generated based on the position of the cardiac structures. The Gaussian masks are defined as:\n$M_g (x, y) = \\frac{1}{2\\pi\\sigma^2} exp(-\\frac{(x-\\mu_x)^2 + (y-\\mu_y)^2}{2\\sigma^2}),$ (3)\nwhere x and y represent the spatial positions of the mask. $ \\mu_x$ and $ \\mu_y$ denote the center positions of the Gaussian distribution, which are the coordinates of the"}, {"title": "Experiments", "content": "Datasets and Implementations. To assess the performance of ECM, we gathered data from three sources: two private dataset from multiple hospitals and the publicly available dataset named EchoNet-Dynamic [8]. The in-house dataset includes 144 apical four-chamber (A4C) and 100 apical two-chamber (A2C) heart videos, and the public one comprises 10,030 labeled A4C echocardiography videos. For both datasets, the videos were randomly split into training (90%) and testing (10%) sets. During training, 12-frame clips were randomly sampled from each video, with a sampling interval ranging from 1 to 4 frames. For testing, videos were truncated according to different sampling intervals. The input videos from the private datasets were resized to 256x256 pixels, while those from the EchoNet-Dynamic dataset were kept at their original resolution of 112x112 pixels. All methods were implemented in PyTorch using an NVIDIA RTX 4090 GPU under same settings. The Adam optimizer was used with a learning rate of 5e-3 and 60,000 training steps.\nEvaluation Metrics. Our evaluation metrics cover both image-level and video-level evaluation. The image-level assessment includes Structural Similarity Index (SSIM) [20], Mean Absolute Error (MAE) [1], Peak Signal-to-Noise Ratio (PSNR) [3], Fr\u00e9chet Inception Distance (FID) [4] and Learned Perceptual Image Patch Similarity (LPIPS) [15]. For video-level assessment, we only considered the commonly-used Fr\u00e9chet Video Distance (FVD) [16]. Notably, to assess the consistency of cardiac structures between the generated and target videos, we introduce a new indicator calculated by the Intersection over Union (IoU) between the bboxes of the original and synthesized videos.\nMethod Comparison. The quantitative comparison of ECM and other methods is reported in Table 1. The ECM model achieves the best performance across all metrics, indicating superior image quality and high fidelity compared to other methods. It can be seen that the SEG Diffusion method, generating echocardiography videos without any control conditions, achieves poor quality. Notably, ECM model markedly outperforms the SD method under bbox/text conditions, achieving 67.8%\u2193 in FVD. This demonstrates that synthesizing 2D images with rich conditions and stitching them into videos does not yield satisfactory outcomes. The integration of a position-aware attention mechanism significantly contributes to our superior performance and enhanced video consistency."}, {"title": "Conclusion", "content": "In this study, We have presented ECM for generating echocardiography videos guided by motion curves, which reflect the movement of each cardiac structure. ECM enables to customize the generated videos by adjusting (scaling and replacing) the initial motion curve. Besides, to link the structure features with corresponding movement information, we propose the structure-to-motion alignment mechanism. Moreover, attention masks based on the position of the anatomical structures are introduced to enhance the motion consistency of each structure. Overall, our proposed ECM achieves state-of-the-art performance for generating echocardiography videos in terms of fidelity and consistency."}]}