{"title": "Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation", "authors": ["Junxuan Yu", "Rusi Chen", "Yongsong Zhou", "Yanlin Chen", "Yaofei Duan", "Yuhao Huang", "Han Zhou", "Tan Tao", "Xin Yang", "Dong Ni"], "abstract": "Echocardiography video is a primary modality for diagnosing heart diseases, but the limited data poses challenges for both clinical teaching and machine learning training. Recently, video generative models have emerged as a promising strategy to alleviate this issue. However, previous methods often relied on holistic conditions during generation, hindering the flexible movement control over specific cardiac structures. In this context, we propose an explainable and controllable method for echocardiography video generation, taking an initial frame and a motion curve as guidance. Our contributions are three-fold. First, we extract motion information from each heart substructure to construct motion curves, enabling the diffusion model to synthesize customized echocardiography videos by modifying these curves. Second, we propose the structure-to-motion alignment module, which can map semantic features onto motion curves across cardiac structures. Third, The position-aware attention mechanism is designed to enhance video consistency utilizing Gaussian masks with structural position information. Extensive experiments on three echocardiography datasets show that our method outperforms others regarding fidelity and consistency. The full code will be released at https://github.com/mlmi-2024-72/ECM.", "sections": [{"title": "Introduction", "content": "Echocardiography is a primary method that relies on dynamic video to obtain structural information for clinical diagnoses [24]. However, training radiologists with diagnostic skills and establishing machine learning models both suffer from limitations on video resources. Recently, video generation models have demonstrated a promising ability to solve this problem, owing to their powerful capability in modeling data distribution [22,21]. Several studies about video generation based on specific conditions have been investigated. The typical ones relied on canny edges or depth maps [2,19] extracted from additional videos as conditions. Nevertheless, these conditions are non-editable and lack motion-driving information. In contrast, Shi et al. [14] proposed the Motion-I2V framework to predict dense optical flow and guide video generation, maintaining both spatial and motion consistency. Wang et al. [18] then employed a self-tracking training method. Specifically, they specified the box positions of the first and last frames along with motion paths, to control the movement of objects. However, these paths are basic and coarse, inadequate for ultrasound video synthesis that demands an accurate representation of intricate structure motions.\nIn the field of echocardiography video generation, Zhou et al. [23] proposed the OnUVS framework to synthesize ultrasound videos by animating source images and leveraging motion information from driving video. Nevertheless, OnUVS faced challenges due to the anatomical structure gap between the source image and the driving video, making accurate motion control difficult. In addition, some studies have mined the structural information of the heart to guide the movement of videos. For instance, Reynaud et al. [11] developed a Generative Adversarial Network (GAN) capable of generating echocardiography videos corresponding to the left ventricular ejection fractions (LVEFs). They further employed a cascade video diffusion model conditioned on randomly sampled frames, enhancing the synthesis quality of echocardiography video [10]. Van et al. [17] utilized segmentation masks of end-diastolic (ED) frames as a condition to generate four-chamber heart videos. However, both LVEFs and ED masks are relatively sparse conditions, leading to an imbalance of information between intricate motions and limited conditions. This sparsity poses challenges for effectively controlling fine-grained cardiac structures, thus limiting the ability to capture the full complexity of heart movements and dynamics.\nTo address the above issues, we propose an explainable and controllable motion curve guided video diffusion model (ECM) that can synthesize video guided by the initial frame and motion curves (Fig. 1). Our contributions are threefold: (1) We innovatively mine the motion of the echocardiography video to obtain motion curves, which fully reflect the movement of each cardiac structure. This easily controlled approach enables the customization of videos through the modification (scaling and replacing) of the initial motion curve. (2) As the curve lacks"}, {"title": "Methodology", "content": "Fig. 2 shows the framework of ECM. During the training process, ECM takes an original video as input. A pretrained variational auto-encoder (VAE) from Stable Diffusion (SD) [12] is then utilized to downsample the video into latent features. By gradually adding noise to the latent and then learning to denoise it, the model can obtain the generated video with a pretrained VAE decoder. Remarkably, we develop the structure-to-motion alignment module to match cardiac structures with motion, yielding aligned features that condition the ECM model. Additionally, we employ Gaussian masks for each structure using a Position-aware Attention Mechanism, incorporating these into the spatial layers of the diffusion"}, {"title": "Extraction of Motion Curves", "content": "Previous studies [11,10] faced challenges in controlling the motion of specific cardiac substructures since they relied solely on the single sparse condition (LVEFs), while our approach aims to provide fine-grained control over the motion of each cardiac substructure. Therefore, we extract motion curves for the key structures (e.g., Left Ventricle, Left Atrium, Mitral Valve, etc.) as conditions.\nAs illustrated in Fig. 2, the process of extracting motion curves is as follows: (a) We employ a well-trained anatomy detector (average accuracy=85%) to identify each substructure in each frame of the echocardiogram video. (b) Subsequently, we utilize the pixel coordinates of each substructure\u2019s bounding box (bbox) as the basis for encoding the motion curves, which form the basis for encoding the motion curves, represented as $f_m \\in \\mathbb{R}^{B\\times N\\times C\\times (4\\times 2)}$, where $N$ represents the number of frames and $C$ represents the categories of the substructure. Notably, any missed detected structures would be treated as learnable parameters, initialized by the network. (c) Since cardiac motion is periodic, we employ Fourier transformation (FT) to transform the pixel coordinates into a high-dimensional feature representation, denoted as $f_m \\in \\mathbb{R}^{B\\times N\\times C\\times E}$, where $E$ represents the dimensionality of the features obtained from FT. (d) Finally, the motion embedding is passed through several multi-layer perceptron (MLP) layers. Overall, these motion curve features can be formulated as $f_m \\in \\mathbb{R}^{B\\times N\\times C\\times 1024}$"}, {"title": "Structure-to-Motion Alignment Module", "content": "Although the motion curves of the echocardiography videos are captured, it is challenging for the model to distinguish the relationship between the motion curves and the semantic information of each substructure. Recently, GLIGEN [6] has demonstrated its effectiveness in combining caption and bbox information, enhancing visual-language understanding to enable fine-grained control over specific objects in natural images. However, our preliminary experiments suggested that GLIGEN struggles to effectively represent and interpret texts related to cardiac structures in echocardiography.\nInnovatively, we replace texts with the cardiac structure features, denoted as $f_i$. As shown in Fig. 2, to obtain $f_i$, the regions of interest (ROI) that correspond to the cardiac substructures in the initial frame are cropped by the well-pretrained detector as mentioned above. Then, a pretrained CLIP image encoder [9] is utilized to transform the ROIs into structural embedding features. Then, these features pass through several MLP layers, resulting in an output denoted as $f_a \\in \\mathbb{R}^{B\\times N\\times C\\times 1024}$, which has the same shape with the extracted motion curve features $f_{im}$ mentioned in Sec. 2.1. Consequently, structural features and their corresponding motion curve features from the same category are"}, {"title": "Position-aware Attention Mechanism", "content": "Enhancing the consistency of cardiac motion is crucial in the task of cardiac video generation. To address this, we aim to inject the positional information of the cardiac structure into the cross-attention mechanism. Specifically, we design Gaussian masks that are generated based on the position of the cardiac structures. The Gaussian masks are defined as:\n$M_g(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp(-\\frac{(x-\\mu_x)^2+(y-\\mu_y)^2}{2\\sigma^2}),$\nwhere $x$ and $y$ represent the spatial positions of the mask. $\\mu_x$ and $\\mu_y$ denote the center positions of the Gaussian distribution, which are the coordinates of the\nfour corner points of the detected bbox. $\\sigma$ is the standard deviation to control the distribution width. Here, we set $\\sigma$ to 10 pixels as default. Subsequently, the Gaussian masks are resized to match the dimensions of the latent feature map. The Gaussian-weighted cross-attention mechanism is as follows:\n$\\text{CrossAttenMask}(Q, K, V, M_g) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) \\odot V,$ where $\\odot$ is the element-wise multiplication. Overall, this position-aware attention mechanism effectively integrates positional information of cardiac structures, enhancing the consistency and realism of generated cardiac motion."}, {"title": "Experiments", "content": "Datasets and Implementations. To assess the performance of ECM, we gathered data from three sources: two private dataset from multiple hospitals and the publicly available dataset named EchoNet-Dynamic [8]. The in-house dataset includes 144 apical four-chamber (A4C) and 100 apical two-chamber (A2C) heart videos, and the public one comprises 10,030 labeled A4C echocardiography videos. For both datasets, the videos were randomly split into training (90%) and testing (10%) sets. During training, 12-frame clips were randomly sampled from each video, with a sampling interval ranging from 1 to 4 frames. For testing, videos were truncated according to different sampling intervals. The input videos from the private datasets were resized to 256x256 pixels, while those from the EchoNet-Dynamic dataset were kept at their original resolution of 112x112 pixels. All methods were implemented in PyTorch using an NVIDIA RTX 4090 GPU under same settings. The Adam optimizer was used with a learning rate of 5e-3 and 60,000 training steps.\nEvaluation Metrics. Our evaluation metrics cover both image-level and video-level evaluation. The image-level assessment includes Structural Similarity Index (SSIM) [20], Mean Absolute Error (MAE) [1], Peak Signal-to-Noise Ratio (PSNR) [3], Fr\u00e9chet Inception Distance (FID) [4] and Learned Perceptual Image Patch Similarity (LPIPS) [15]. For video-level assessment, we only considered the commonly-used Fr\u00e9chet Video Distance (FVD) [16]. Notably, to assess the consistency of cardiac structures between the generated and target videos, we introduce a new indicator calculated by the Intersection over Union (IoU) between the bboxes of the original and synthesized videos.\nMethod Comparison. The quantitative comparison of ECM and other methods is reported in Table 1. The ECM model achieves the best performance across all metrics, indicating superior image quality and high fidelity compared to other methods. It can be seen that the SEG Diffusion method, generating echocardiography videos without any control conditions, achieves poor quality. Notably, ECM model markedly outperforms the SD method under bbox/text conditions, achieving 67.8%\u2193 in FVD. This demonstrates that synthesizing 2D images with rich conditions and stitching them into videos does not yield satisfactory outcomes. The integration of a position-aware attention mechanism significantly contributes to our superior performance and enhanced video consistency."}, {"title": "Conclusion", "content": "In this study, We have presented ECM for generating echocardiography videos guided by motion curves, which reflect the movement of each cardiac structure. ECM enables to customize the generated videos by adjusting (scaling and replacing) the initial motion curve. Besides, to link the structure features with corresponding movement information, we propose the structure-to-motion alignment mechanism. Moreover, attention masks based on the position of the anatomical structures are introduced to enhance the motion consistency of each structure. Overall, our proposed ECM achieves state-of-the-art performance for generating echocardiography videos in terms of fidelity and consistency."}]}