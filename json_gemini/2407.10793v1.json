{"title": "GraphEval: A Knowledge-Graph Based LLM Hallucination\nEvaluation Framework", "authors": ["Hannah Sansford", "Nicholas Richardson", "Hermina Petric Maretic", "Juba Nait Saada"], "abstract": "Methods to evaluate Large Language Model (LLM) responses and detect inconsistencies, also known as hallucinations, with\nrespect to the provided knowledge, are becoming increasingly important for LLM applications. Current metrics fall short in\ntheir ability to provide explainable decisions, systematically check all pieces of information in the response, and are often too\ncomputationally expensive to be used in practice. We present GraphEval: a hallucination evaluation framework based on\nrepresenting information in Knowledge Graph (KG) structures. Our method identifies the specific triples in the KG that are\nprone to hallucinations and hence provides more insight into where in the response a hallucination has occurred, if at all,\nthan previous methods. Furthermore, using our approach in conjunction with state-of-the-art natural language inference\n(NLI) models leads to an improvement in balanced accuracy on various hallucination benchmarks, compared to using the raw\nNLI models. Lastly, we explore the use of GraphEval for hallucination correction by leveraging the structure of the KG, a\nmethod we name GraphCorrect, and demonstrate that the majority of hallucinations can indeed be rectified.", "sections": [{"title": "1. Introduction", "content": "As the size and power of LLMs have drastically increased\nover recent years, so has the number of potential appli-\ncations. Arguably, one of the biggest blockers to imple-\nmenting these models in practice is their tendency to\nhallucinate - returning seemingly plausible, but untrue,\nresponses. Here, we focus on the problem of detecting\nhallucinations with respect to the provided context that\nthe LLM should use as its source of knowledge; detecting\nhallucinations that have deviated from the LLM's original\ntraining data is out of the scope of this work. In appli-\ncations where certainty in a response is critical, such as\nmedical diagnosis, the existence of hallucinations that\narise from a given context is especially limiting. There-\nfore, it is of utmost importance to develop successful\nmethods to detect these hallucinations and, when it is\nof interest to address or correct them, provide clarity on\nwhich aspect of the response is likely a hallucination.\nThe importance of this issue is reflected in the amount of\nresearch being published on the topic - see Ji et al. [12]\nfor a recent survey of this area.\nPerforming evaluation on natural language is a challeng-\ning task that researchers have been interested in long\nbefore hallucinations were at the forefront of the prob-\nlem. Methods have evolved a great deal from traditional\nN-gram based metrics, such as BLEU [26] and ROUGE\n[17], to much more intricate LLM-based evaluation met-\nrics with user-defined evaluation criteria, such as G-Eval\n[18]. More recently, techniques to mitigate the preva-\nlence of hallucinations in generated outputs leveraging\nRetrieval Augmented Generation (RAG) [16] and rea-\nsoning on knowledge graphs (KGs) [19, 34] have been\nproposed. The former suggested the concatenation of rel-\nevant contextual data into the prompt to ground the LLM\nresponse, while the latter enforced a more robust reason-\ning process through providing grounding information in\nKG structures [1]. As successful as these approaches have\nbeen, they do not fully circumvent the need to evaluate\nLLM outputs.\nInspired by current research harnessing KGs to provide\ngrounded LLM responses, we propose GraphEval - a hal-\nlucination detection framework based on the represen-\ntation of information in KG structures. To the best of\nour knowledge, we are the first to apply KGs to an LLM-\nbased hallucination evaluation framework, and in doing\nso we provide a higher level of insight into where in\nthe output a hallucination has occurred than any previ-\nous metrics. Additionally, we demonstrate how using\nour method in conjunction with current state-of-the-art\nhallucination detection methods improves their classi-\nfication accuracy on various benchmarks. Finally, we\nconsider the problem of hallucination correction and we\nintroduce GraphCorrect, showcasing how GraphEval can"}, {"title": "2. Problem statement", "content": "In this work we focus on the closed-domain hallucina-\ntion detection problem: the situation where we have a\ntextual output from an LLM which is generated using\nsome grounding context included in the prompt. In this\ncase, the goal is for the LLM to use the provided context\nas its only source of knowledge. The open-domain prob-\nlem, which is with respect to all factual knowledge in\nthe world, is not explored here but is briefly discussed in\nSection 8.\nWe consider hallucination detection to be a binary classifi-\ncation problem, with 0 corresponding to the LLM output\nbeing factually consistent given the provided context,\nand 1 corresponding to the output containing at least\none inconsistency. We can assess hallucination evalua-\ntion methods using a benchmarking dataset containing\nground-truth labels (usually human-annotated) to de-\ntermine whether a given context-output pair contains\nfactual inconsistencies. Throughout the paper we use\nthe terms factual, consistent, grounded and faithful in-\nterchangeably to mean containing no hallucinations with\nrespect to the context.\nFinally, we explore the problem of hallucination correc-\ntion, wherein we do not use any directly labeled dataset.\nInstead, we utilize hallucination detection frameworks to\nfirst identify hallucinations to correct, and subsequently\nrepurposing them to evaluate the corrected outputs. It is\nimportant to note that our exploration of hallucination\ncorrection only serves as an extension to our evaluation\nframework and is not the primary focus of this study."}, {"title": "3. Related work", "content": "Historically, N-gram based metrics such as BLEU [26]\nand ROUGE [17] have been the most widely used metrics\nfor natural language evaluation. However, these met-\nrics have been shown to perform poorly at the task of\nfactual inconsistency detection [21, 11]. In more recent\nyears, embedding-based metrics such as BERTScore [35]\nhave been favoured over N-gram based metrics. These\nmethods measure the similarity between two pieces of\ntext by comparing the contextualised embedding from a\ntransformer model, such as BERT [3].\nBoth N-gram and embedding-based metrics base their\nscores on how similar the text to be evaluated is to some\nreference text. This similarity objective often fails to cap-\nture the intricacies of the hallucination detection problem.\nTherefore, researchers have begun to develop new meth-\nods that are more acutely tuned to detecting inconsisten-\ncies between an LLM output and its grounding context.\nMaynez et al. [21] identified the crossover between the\ntextual entailment score in NLI tasks and consistency pre-\ndiction. This was a breakthrough at the time, producing\nhigher correlation with faithfulness than any previous\nmetrics, and paved the way for further research that cap-\\italised on NLI data and models [10, 15, 6].\nVery recently, attention has turned to leveraging LLMs\nthemselves to evaluate the consistency of LLM outputs.\nSelfCheckGPT [20] and ChatProtect [24] approach the\nproblem by considering the self-consistency within sam-\npled outputs. Since they require the generation of a large\nnumber of responses from the LLM, many consider these\nmethods prohibitively computationally expensive.\nOther LLM-based hallucination evaluation methods, such\nas G-Eval [18] and GPTScore [5], employ a different LLM\nfor evaluation than the one used to generate the LLM\nresponse that needs to be evaluated. G-Eval allows user-\ndefined evaluation criteria and uses automated chain-\nof-thought prompting and form-filling to assign scores.\nGPTScore treats the task as conditional generation, lever-\naging models like GPT-3 to assign higher probabilities to\nhigh-quality outputs by prepending evaluation instruc-\ntions to the LLM prompt. Unlike NLI models trained on\nbinary classification data, these methods produce scores\nthat are harder to interpret as probabilities and often\nrequire additional steps for inconsistency classification.\nRecent hallucination detection methods, such as\nFactScore [23] and SAFE [32], utilize large language\nmodels to break down the response into atomic or in-\ndividual facts for evaluation. These approaches have\nenabled precise identification of where hallucinations oc-\ncur within the LLM response. Each fact is automatically\nverified against a comprehensive knowledge source like\nWikipedia or scientific literature in the case of FactScore,\nor through the use of a search engine in the case of SAFE.\nFactGraph [28] is the only factuality evaluation method\nwe are aware of that utilises graph-like structures. The\nmethod is focused solely on the detection of inconsisten-\ncies in the summarization problem, decomposing both\nthe summary and the supporting documents into what\nthey call structured meaning representations (MRs). These\nMRs describe the core semantic concepts and relations,\nwhich the authors claim to be more suitable for factuality\nevaluation than the raw text."}, {"title": "4. GraphEval: Our evaluation\nmethod", "content": "GraphEval is based around the idea of representing infor-\nmation in a structured manner through KGs, and aims to\naddress the lack of explainability of previous hallucina-\ntion detection approaches, i.e. which concrete pieces of\ninformation in particular are inconsistent.\nFormally, a KG is a collection of triples $KG =$\\n$\\{(e_1,r,e_2) \\subseteq E \\times R \\times E\\}$, where E and R denote\nthe set of entities and relationships, respectively. In the\nGraphEval setting, both entities and relationships are\nsimply pieces of text. We do not make use of common\nextensions to this simple setting, such as entity and rela-\ntionship types, or attached properties.\nOur GraphEval metric consists of a two-stage procedure:\nStage 1 - Construct a KG from the LLM output\nto be evaluated.\nStage 2 Iterate through each of the triples in\nthe KG, identifying whether they are factually\nconsistent given the provided context.\nThe output is considered factually inconsistent if any of\nthe triples in stage 2 are identified as not grounded in the\ncontext. The inconsistent triple(s) may also be returned\nto provide explainability by highlighting where in the\noutput the hallucination(s) has occurred. We provide\na visualisation of this process in Figure 1 using a real\nexample from one of the benchmarks described in Section\n7.1.\nRegarding stage 1, we provide a short review of LLM-\nbased KG construction methods in Section 5, along with\nresults from our implementation. For stage 2, we leverage\nexisting techniques and employ an out-of-the-box NLI\nmodel for this task. A benefit of this approach is that it\ngives us the opportunity to make a direct comparison\nbetween the performance of the raw NLI model and the\nmodel supplemented with our KG approach. In essence,\nour method is a pre-processing step, the output of which\ncan be fed into any hallucination detection method; we\nchoose NLI models as they are computationally cheap\ncompared to LLM-based models, yet still achieve state-of-\nthe-art results. By feeding each triple into an NLI model,\nalong with the grounding context, we obtain a probability\nof containing a hallucination for each triple. Finally, we\nclassify the example as inconsistent if at least one triple\nproduces a probability greater than 0.5.\nSimilar approaches to ours have been proposed in re-\ncent literature. SummaC [15] also uses NLI-based models\nto detect inconsistencies in LLM-generated summaries.\nHowever, it distinguishes itself by segmenting both the\ncontext and the summary into their respective sentences,\nand then by passing each context-summary pair into the\nNLI model. This approach presents challenges in main-"}, {"title": "5. Construction of KGs using LLMs", "content": "Constructing KGs from unstructured textual data in-\nvolves identifying the set of entities within the text and\nthe relationships between them, resulting in a structured\nrepresentation of the information contained within the\ntext. The process can be divided into three main stages:\n1. Entity detection - the process of identifying and\nextracting entities from text.\n2. Coreference resolution - the process of finding\nof all expressions (also called mentions) in the text\nthat refer to the same entity.\n3. Relation extraction - the process of identifying\nsemantic relationships between entities.\nPreviously, researchers addressed each stage individually,\nbut with the increasing power of LLMs, there's been a\nshift towards end-to-end systems. Kumar et al. [14] sug-\ngest employing two LLM components: one for named\nentity recognition and another one for both relation clas-\nsification and direction. Similarly, Grapher [22] utilizes a\npre-trained LLM for entity extraction and relation predic-\ntion. However, these methods require users to provide\npossible relations. More recent methods like PiVE [7]\nand AutoKG [37] use LLM prompting strategies for KG\nconstruction without additional user input.\nThe aforementioned methods do not make use of some of\nthe emergent abilities of LLMs, such as in-context learn-\ning and the chain-of thought prompting strategy. We\ndecide to leverage these emergent abilities, and take a\nsimple prompt engineering approach to our KG construc-\ntion step. The techniques used can be summarised as the\nfollowing:\n\u2022 Chain-of-thought (CoT) prompting strategy. Pro-\nviding intermediate reasoning steps in the prompt\nto enable LLMs to solve more complex tasks.\n\u2022 In-context learning. A method of prompt engi-\nneering where one provides several task demon-\nstrations within the prompt, circumventing the\nneed for fine-tuning.\nThe final prompt used in our experiments can be found\nin the Appendix. We highlight to the reader that our KG\nconstruction method is not the main contribution of our"}, {"title": "6. GraphCorrect: Correction of\nhallucinations with GraphEval", "content": "While the primary focus of this work lies in hallucination\ndetection, GraphEval's breakdown of LLM outputs into\ntriples easily allows for its extension to correct hallucina-\ntions within the given context. To achieve this, we first\nidentify all triples within the KG that are likely to con-\ntain hallucinations (i.e. those with a probability greater\nthan 0.5, if any). We then employ the following two-step\nprocedure on each identified triple:\nStep 1 - Input the given triple along with the\ncontext into an LLM to correct for the potential\nhallucinations within the triple. This results in a\nnewly generated corrected triple.\nStep 2 - Input the identified triple, its corrected\ncounterpart and the initial LLM output. Selec-\ntively replace the information from the original\n(hallucination-containing) triple with the infor-\nmation from the new triple in the initial LLM\noutput.\nWe name this LLM hallucination correction method as\nGraphCorrect. The final prompts used in our experiments\nfor both step 1 and step 2 can be found in the Appendix B\nand C respectively. This systematic approach to halluci-\nnation correction offers several benefits. First, it tackles\neach identified hallucination separately, increasing the\nchances of all perceived hallucinations being corrected.\nFurthermore, it offers the advantage of exclusively alter-\ning the segments of the original text that are suspected\nto contain a hallucination, leaving other elements un-\ntouched and ensuring overall high similarity with the\noriginal text. Finally, breaking down the entire process\ninto intermediate steps ensures that the original context\nand the initial LLM output never undergo simultaneous\nprocessing within an LLM. This guarantees safeguards\nagainst both the addition of extra information and the\nloss of information in the LLM output."}, {"title": "7. Experiments", "content": "We conducted two sets of experiments: one focusing on\nhallucination detection to highlight GraphEval's perfor-\nmance and another on hallucination correction to show-\ncase the advantages of GraphCorrect. For both scenarios,\nwe utilized the SummEval [4], QAGS-C and QAGS-X\n[31] benchmarks - currently the most prevalent bench-\nmarks in relevant academic literature. All three are con-\ncerned with detecting hallucinations in LLM-generated\nsummaries and are human-annotated for factual con-\nsistency with respect to the grounding context. Table\n1 contains some statistics pertaining to each of these\ndatasets.\nSummEval The SummEval dataset consists of human\nevaluations on 16 summarization model outputs from\n100 articles from the CNN/DailyMail dataset [9]. Each\nsummary is labelled on a Likert scale from 1-5 on 4 cat-\negories: consistency, coherence, fluency and relevance.\nWe follow the TRUE benchmark [10] in taking the con-\nsistency scores and mapping a score of 5 to being fully\nconsistent, and anything lower to being inconsistent.\nQAGS The QAGS-C and QAGS-X datasets are built\nfrom the CNN/DailyMail and the XSum [25] datasets,\nrespectively. The human annotators examined the sum-\nmaries one sentence at a time, and determined the factual\nconsistency of each sentence comparing it to the original\narticle. Three annotators assessed each sentence and the\nmajority decision was recorded. Again, we follow the\nTRUE benchmark in considering a summary to be factu-\nally consistent if and only if all sentences are considered\nconsistent."}, {"title": "7.2. NLI models in GraphEval", "content": "As mentioned in Section 4, we employ NLI models to\nperform the second stage of GraphEval - checking the\nconsistency of each individual triple with respect to the\ncontext. We conduct experiments using the three most\npopular NLI-based hallucination detection models avail-\nable on HuggingFace \u00b9.\nHHEM Based on the DeBERTaV3 model [8] and ini-\ntially trained on NLI data, the hallucination evaluation\nmodel created by Vectara 2 is further fine-tuned on\ndatasets annotated for consistency. The datasets used"}, {"title": "7.3. Experimental settings", "content": "In all experiments conducted in this study necessitating\nthe utilization of an LLM, we use Claude 23, an LLM\nfrom Anthropic, through the Amazon Bedrock API 4. We\nuse the default settings for the LLM: temperature = 1,\ntop_p = 1, top_k = 250. We also refer the reader to the\nAppendix for the prompts used in this work."}, {"title": "7.4. Results", "content": "We present our results of hallucination detection for the\nthree NLI models, and their GraphEval counterparts, in\nTable 2. We report the balanced accuracy as our evalu-\nation metric, which corrects for the class imbalance in\nthe SummEval benchmark. In the case of using the NLI\nmodel directly, we classify the example as containing a\nhallucination if the NLI model returns a probability of\nmore than 0.5. When combining the NLI model with\nGraphEval, we classify the example as containing a hallu-\ncination if at least one triple fed to the NLI model returns\na probability of more than 0.5. We see that adding the\nGraphEval pre-processing step to each of the NLI mod-\nels almost always improves the balanced accuracy score,\nsometimes by a considerable amount, such as the results\nfor the SummEval and QAGS-C benchmarks in Table\n2. On average (weighting by the number of samples in\neach dataset), adding the GraphEval pre-processing step\nimproves the balanced accuracy by 6.2 (SE=1.3).\nWe hypothesise that the negligible difference between\nthe base NLI model and the model supplemented with\nGraphEval for the QAGS-X dataset is due to the average\nlength of the generated text (only 18 words, compared\nwith 49 and 63 for QAGS-C and SummEval respectively,\nsee 1). This highlights an important aspect of where the\nmost value can be found in our method. When the LLM\noutput is very short, there are less likely to be multiple\nfacts that need to be checked for consistency (which\ncan easily be done without the use of a KG) and the\nintricacies of the short sentence might even be lost in\nthe KG construction phase. On the other hand, when the\nLLM output is very long, current methods struggle to\ntest each individual fact against the context, and this is\nwhen GraphEval thrives.\nIt should be noted that even when the results for GraphE-\nval are comparable to the baseline methods, the benefit\nof using GraphEval is the identification of the specific\ntriple(s) that are inconsistent with the provided context."}, {"title": "7.4.2. Hallucination correction with GraphCorrect", "content": "Identifying the particular triple(s) likely to harbor a hallu-\ncination enables straightforward correction using Graph-\nCorrect, as described in Section 6. For each of the evalu-\nation frameworks proposed here (HHEM + GraphEval,\nTRUE + GraphEval, and TrueTeacher + GrapEval), we\ncompared GraphCorrect to a basic prompting strategy\nfor hallucination correction, serving as a baseline. The\nprompt used in this baseline approach, referred to as the\nDirect Prompt henceforth, is provided in Appendix D.\nFor each framework, we initially identify hallucinations,\ncorrect only the LLM outputs suspected of containing hal-\nlucinations using either GraphCorrect or Direct Prompt,\nand then reapply the evaluation framework to detect hal-\nlucinations in the corrected LLM outputs. Note that this\nprocedure only allows us to measure what we presume to\nbe corrected hallucinations, given the potential for errors\nin the evaluation frameworks utilized here. We report the"}, {"title": "8. Discussion", "content": "Our work focuses on detection of hallucinations in closed-domain tasks, where we are interested only in consis-tency with respect to the provided context. The GraphE-val framework could be extended to open-domain halluci-nation detection by employing agents, as in AutoKG [37],to first retrieve relevant external sources as the groundinginformation to check against.\nWe expect that in the near future, more research will beconducted on the construction of KGs from unstructuredtext, which will provide improvements to the first stage ofour procedure and ultimately the evaluation performance.Even as LLMs alone become more powerful, this willcontinue to contribute to improvements in GraphEval'sperformance.\nWe observe that, in the knowledge graph constructionphase of our procedure, it is possible that some informa-tion loss may occur. However, as shown by the resultsin Section 7.4, our method rarely leads to a reduction inbalanced accuracy. Furthermore, when it is comparableto the baseline methods, we have the added explainabilityof identifying the specific triples where the hallucinationhas occurred.\nWe believe our hallucination correction framework(GraphCorrect) shows promise and an interesting av-enue for future work. However, the effectiveness ofthe approach described in this work should be assessed man-ually, rather than relying on the convoluted use of hallu-cination evaluation frameworks (which only yield mea-"}, {"title": "9. Conclusion", "content": "We introduce GraphEval, a simple and effective pre-processing step for improving the explainability and per-formance of LLM hallucination detection metrics. Ourmethod leverages LLM's ability to extract informationfrom unstructured text and construct knowledge graphs,whose triples can be fed into out-of-the-box hallucinationdetection methods.\nWe demonstrate that GraphEval in conjunction withstate-of-the-art NLI models leads to an average improve-ment in balanced accuracy of 6.2 (SE=1.3) on three popu-lar hallucination benchmarks. Furthermore, our methodindicates which triples, in the KG representation of theLLM output, are inconsistent. To the best of our knowl-edge, this is the first application of KGs to an LLM-basedhallucination evaluation framework and we believe thesuccess of GraphEval will only grow as KG constructionmethods also improve.\nFinally, we examined the issue of hallucination correctionand showed that GraphCorrect can effectively address themajority of hallucinations found in LLM outputs whilemaintaining extremely high similarity with the originaltexts."}, {"title": "A. KG Construction Prompt", "content": "(system,\n), You are an expert at extracting\ninformation in\nstructured formats to build a\nknowledge graph.\nStep 1 Entity detection: Identify\nall entities in the raw text.\nMake sure not to miss any out.\nEntities should be basic and\nsimple, they are akin to\nWikipedia nodes.\nStep 2 Coreference resolution:\nFind\nall expressions in the text that\nrefer to the same entity. Make\nsure entities are not duplicated.\nIn particular do not include\nentities that are more specific\nversions themselves, e.g. \"a\ndetailed view of jupiter's\natmosphere\" and \"jupiter's\natmosphere\", only include the\nmost specific version of the\nentity.\nStep 3 Relation extraction:\nIdentify semantic relationships\nbetween the entities you have\nidentified.\nFormat: Return the knowledge graph as\na list of triples, i.e. [\"entity\n1\", \"relation 1-2\", \"entity 2\"],\nin Python code.\n(\"human\",\n), \"Use the given format to extract\ninformation from the\nfollowing input: {input}</input >. Skip the\npreamble and output the\nresult as a list within\npython> </python > tags.\",\n(\"human\",\n), Important Tips:\n1. Make sure all information\nis included in the\nknowledge graph.\n2. Each triple must only\ncontain three strings!\nNone of the strings\nshould be empty.\n3. Do not split up related\ninformation into separate\ntriples because this\ncould change the meaning.\n4. Make sure all brackets and\nquotation marks are\nmatched.\n5. Before adding a triple to"}, {"title": "B. Hallucination correction (step 1)", "content": "You are an expert at extracting\ninformation in structured formats\nfrom text.\nThe following triple contains\nfactually incorrect information.\nCorrect it based on the provided\ncontext,\nImportant Tips:\n1. A triple is defined as [\"\nentity 1\", \"relation 1-2\",\"entity 2\"].\n2. A triple must only contain\nthree strings! None of the\nstrings should be empty.\n3. The concatenated triple must\nmake sense as a sentence.\n4. Only return the corrected\ntriple, nothing else.\n{triple}\n{context}\nRemember, it is important that you\nonly return the corrected triple."}, {"title": "C. Hallucination correction (step\n2)", "content": "In the following context, replace the\ninformation of the old triple\nwith the information of the new\none.\nDo not make any other modification to\nthe context.\nOnly return the new context.\n{summary}</context >\n{old_triple} </old_triple >\n{new_triple} </new_triple >"}, {"title": "D. Hallucination correction\nwithout a KG", "content": "The following summary contains\nfactually incorrect information.\nCorrect it based on the context, but\ndon't change other parts of the\nsummary.\nOnly return the corrected summary,\nnothing else.\n{summary}</summary>\n{context}</context >\nRemember, do minimal changes to the\noriginal summary, don't make it\nlonger and keep as much of it\nyou can exactly the same."}]}