{"title": "Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm", "authors": ["Sattar Vakili", "Julia Olkhovskaya"], "abstract": "Reinforcement learning utilizing kernel ridge regression to predict the expected value function represents a powerful method with great representational capacity. This setting is a highly versatile framework amenable to analytical results. We consider kernel-based function approximation for RL in the infinite horizon average reward setting, also referred to as the undiscounted setting. We propose an optimistic algorithm, similar to acquisition function based algorithms in the special case of bandits. We establish novel no-regret performance guarantees for our algorithm, under kernel-based modelling assumptions. Additionally, we derive a novel confidence interval for the kernel-based prediction of the expected value function, applicable across various RL problems.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has demonstrated substantial practical success across a variety of application domains, including gaming (Silver et al., 2016; Lee et al., 2018; Vinyals et al., 2019), autonomous driving (Kahn et al., 2017), microchip design (Mirhoseini et al., 2021), robot control (Kalashnikov et al., 2018), and algorithmic search (Fawzi et al., 2022). This empirical success has prompted deeper investigations into the analytical understanding of RL, especially in complex environments. Over the past decade, significant advances have been made in establishing theoretically grounded algorithms for various settings. In this work, we focus on the infinite horizon average reward setting, also known as the undiscounted setting (Wei et al., 2020, 2021). This setting is particularly well-suited for applications that involve continuing operations not divided into episodes such as load balancing and stock market operations. In contrast to the episodic setting (Jin et al., 2020) and the discounted setting (Zhou et al., 2021), theoretical understanding of RL algorithms is relatively limited for the undiscounted setting. We develop a computationally efficient algorithm and establish its theoretical performance guarantees in the undiscounted case.\nThere is a natural progression in the complexity of RL models corresponding to the structural complexity of the Markov Decision Process (MDP). This progression ranges from tabular models to linear, kernel-based, and deep learning-based models. The kernel-based structure is an extension of linear structure to an infinite-dimensional linear model in the feature space of a positive definite kernel, resulting in a highly versatile model with great representational capacity for nonlinear functions. In addition, the closed-form expressions for the prediction and the uncertainty estimate in kernel-based models allow the development of algorithms based on nonlinear function approximation that are amenable to theoretical analysis. Kernel-based models also serve as an intermediate step towards understanding the deep learning-based models (see, e.g., Yang et al., 2020) based on the Neural Tangent (NT) kernel approach (Jacot et al., 2018)."}, {"title": "1.1 Contributions", "content": "To summarize, our contributions are as follows. We develop a kernel based optimistic algorithm for the infinite horizon average reward setting, referred to as KUCB-RL. We establish no-regret guarantees for the proposed learning algorithm, which is the first for this setting to the best of our knowledge. Specifically, in Theorem 3, we prove a regret bound of\n$\\mathcal{O}\\left(\\frac{1}{w}\\left(w + \\sqrt{T\\gamma(T; \\rho)} + \\log \\frac{1}{\\delta}\\right) \\sqrt{\\rho T\\gamma(T; \\rho) + \\rho^2 w^2 \\gamma(T; \\rho)\\gamma(T/w; \\rho)}\\right)$, at a 1 \u2013 \u03b4 confidence level, where \u03c1 is the parameter of kernel ridge regression and \u03b3(T; \u03c1) is the maximum information gain, a kernel specific complexity term (see Section 2). This regret bound translates to $\\mathcal{O}\\left(\\frac{d^3}{T}\\right)$ in the special case of a linear model, recovering the best existing results (Wei et al., 2021) in dependence on T, and improving by a factor of $d^{\\frac{1}{4}}$. When applied to very smooth kernels with exponential eigendecay such as the Squared Exponential (SE) kernel, we obtain a regret of $\\tilde{\\mathcal{O}}(T^{\\frac{1}{4}})$, with the notation $\\tilde{\\mathcal{O}}$ hiding logarithmic factors. For one of the most general cases, the kernels with polynomial eigendecay with parameter p > 1 (See Definition 1), that includes, for example, the Mat\u00e9rn family and NT kernels, we show that our regret bound translates to $\\tilde{\\mathcal{O}}(T^{\\frac{3p+5}{2(3p+4)}})$, which constitutes a no-regret guarantee. To highlight the significance of this result, we point out that no-regret guarantees for GP-UCB in the degenerate case of bandits were established only recently"}, {"title": "1.2 Related Work", "content": "The vast RL literature can be categorized across various dimensions. In addition to the average reward, episodic, and discounted settings, as well as tabular, linear, and kernel-based structures mentioned above, other notable distinctions among settings include model-based versus model-free approaches, and offline versus online versus settings where the existence of a generative model is assumed (allowing the learning algorithm to sample the state-action of its choice at each step, rather than following the Markovian trajectory). Covering the entire breadth of RL literature is challenging. Here, we will focus on highlighting and providing comparisons with the most closely related works, particularly in terms of their setting and structure.\nThe kernel-based MDP structure has been considered in several recent works under the episodic setting (Yang et al., 2020; Vakili and Olkhovskaya, 2023; Chowdhury and Oliveira, 2023; Domingues et al., 2021; Vakili, 2024). The regret bound proven in Yang et al. (2020) for the episodic setting applies only to very smooth kernels such as SE kernel. Vakili and Olkhovskaya (2023) addressed this limitation by extending the results to Mat\u00e9rn and NT families of the kernels, albeit with a sophisticated algorithm that actively partitions the state-action domain into possibly many subdomains, using only the observations within each subdomain to obtain kernel-based prediction and uncertainty estimates. Their work is also based on a particular assumption that relates the kernel eigenvalues to the size of the domain. The work of Chowdhury and Oliveira (2023) is most closely related to ours in terms of kernel-related assumptions. Specifically, our Assumption 4 is identical to Assumption 1 of Chowdhury and Oliveira (2023). They establish a regret bound of $\\mathcal{O}(\\frac{H\\gamma(N; \\rho)}{\\sqrt{N}})$ for the episodic MDP setting, where N is the number of episodes, \u03b3(N; \u03c1) is the maximum information gain, a kernel-related complexity term, H is the episode length and the value of \u03c1 is a fixed constant close to 1. However, their regret bounds do not apply to general families of kernels, such as those with polynomially decaying eigenvalues (see Section 2.2 for the definition) including Mat\u00e9rn and NT kernels, as for this family of kernels \u03b3(N; \u03c1) possibly grows faster than $\\sqrt{N}$. As a result, a no-regret guarantee cannot be established in many cases of interest. In comparison, the infinite horizon setting considered in this work is more challenging than the episodic setting as evident when comparing these settings with linear modeling. For this more challenging setting, we establish no-regret guarantees. A key element of our improved results is the novel confidence interval we utilize in our analysis (Theorem 1). This result is general and can be used across RL problems, for example, improving the results of Chowdhury and Oliveira (2023) as well.\nIn the tabular case, a lower bound of $\\Omega(\\sqrt{D|S||A|T})$ on regret was established in Auer et al. (2008) in the infinite-horizon average-reward setting, where D is the diameter of the MDP. For ergodic MDPs, Wei et al. (2020) shows a regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{t_{mix}^3 |S||A|T})$, where $t_{mix}$ is the mixing time of an ergodic MDP. Furthermore, under the broader assumption of weakly communicating MDPs, which is necessary for low regret (Bartlett and Tewari, 2012), the best existing regret bound of model-free algorithms is $\\tilde{\\mathcal{O}}(|S|^5 |A|^2 \\sqrt{T})$, achieved by the recent work of Zhang and Xie (2023). Several works have studied linear function approximation in the infinite horizon average reward setting under strong assumptions of uniformly mixing and uniformly excited feature conditions (Abbasi-Yadkori et al., 2019a,b; Hao et al., 2021). Notably, Hao et al. (2021) achieved a regret bound of $\\mathcal{O}(\\sqrt{\\frac{t_{mix}^3}{\\lambda_{\\phi}}T})$ under the linear bias function assumption, where $\\lambda_{\\phi}$ is the smallest eigenvalue of policy-weighted covariance matrix. Under the much less restrictive setting of Bellman optimality equation assumption (Assumption 1) for linear MDP, Wei et al. (2021) provides an algorithm with regret guarantee of $\\tilde{\\mathcal{O}}((dT)^{3/4})$. We also consider our kernel-based approach under this general assumption on MDP. Furthermore, for examples of infeasible algorithms in the literature, see Wei et al. (2021), Algorithm 1. There also exists a separate model-based approach to the problem where the transition probability distribution (model) is learned and used for planning, usually requiring high memory and computational complexity and utilizing substantially different techniques and assumptions. While this approach is studied under tabular settings (Bartlett and Tewari, 2009; Auer et al., 2008) and linear settings (Wu et al., 2022), it is not clear whether model-based"}, {"title": "2 Problem Formulation", "content": "In this section, we overview the background on infinite horizon average reward (undiscounted) MDPs and kernel based modelling."}, {"title": "2.1 Infinite Horizon Average Reward MDP", "content": "An undiscounted MDP is described by the tuple (S, A, r, P) where S is a state space with a possibly infinite number of elements, A is a finite action set, r : S \u00d7 A \u2192 [0, 1] is the reward function, and $P(\\cdot | s, a)$ is the unknown transition probability distribution over S of the next state when action a is selected at state s. Throughout the paper we use the notation z = (s, a) for the state-action pairs, and Z = S \u00d7 A.\nThe learner interacts with the MDP through T steps, starting from an arbitrary initial state $s_1 \\in S$. At each step t, the learner observes state $s_t$ and takes an action $a_t$ resulting in a reward $r(s_t, a_t)$. The next state $s_{t+1}$ is revealed as a sample drawn from the transition probability distribution: $s_{t+1} \\sim P(\\cdot | s_t, a_t)$.\nThe goal of the learner is to compete against any fixed stationary policy. A stationary policy \u03c0 : S \u2192 A is a possibly random mapping from the states to actions. The long-term average reward of a stationary policy \u03c0, starting from state s \u2208 S, is defined as:\n$J^{\\pi}(s) = \\liminf_{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E} \\left[ \\sum_{t=1}^{T} r(s_t, a_t) \\middle| s_1 = s, \\forall t \\geq 1, a_t = \\pi(s_t), s_{t+1} \\sim P(\\cdot | s_t, a_t) \\right]$\nWe assume that the MDP belongs to the broad class of MDPs where the following form of the Bellman optimality equation holds:"}, {"title": "Assumption 1 (Bellman optimality equation).", "content": "There exists $J^* \\in \\mathbb{R}$ and bounded measurable functions $v^* : S \\rightarrow \\mathbb{R}$ and $q^* : S \\times A \\rightarrow \\mathbb{R}$ such that the following conditions are satisfied for all states s \u2208 S and actions a \u2208 A:\n$J^* + q^*(s, a) = r(x, a) + \\mathbb{E}_{s'\\sim P(\\cdot | s, a)} [v^*(s')], \\quad v^*(s) = \\max_{a \\in A} q^*(s, a).$\nThis assumption was also used for the linear MDP case in Wei et al. (2021). By applying the Bellman optimality equation, it can be shown that a policy $\\pi^*(s) = \\arg \\max_{a\\in A} q^*(s, a)$, which deterministically selects actions that maximize $q^*$ in the current state, is the optimal policy $\\pi^* = \\arg \\max_{\\pi} J^{\\pi}$, with $J^{\\pi^*}(s) = J^*$, for all s (Wei et al., 2021).\nFor the finite state setting, Assumption 1 follows from the weakly communicating MDP assumption (see, e.g., Puterman, 1990, Chapter 9). Assumption 1 also holds under several other common conditions (Hern\u00e1ndez-Lerma (2012), Section 3.3).\nThe learner's performance is measured by regret, which is defined as the difference in total reward between the learner and the optimal stationary policy. Specifically,\n$R(T) = \\sum_{t=1}^{T} (J^* - r(s_t, a_t)).$\nWe emphasize that under Assumption 1, for any initial state $s_1 \\in S$, $J^{\\pi^*}(s_1) = J^*$, that is reflected in our regret definition.\nFor any value function $v : S \\rightarrow \\mathbb{R}$, throughout the paper, we use the notation\n$[Pv](z) = \\mathbb{E}_{s'\\sim P(\\cdot | z)} [v(s')]$\nfor the expected value function of the next state."}, {"title": "2.2 Kernel-Based Models and the RKHS", "content": "Consider a positive definite kernel $k : Z \\times Z \\rightarrow \\mathbb{R}$. Let $\\mathcal{H}_k$ be the reproducing kernel Hilbert space (RKHS) induced by k, where $\\mathcal{H}_k$ contains a family of functions defined on Z. Let $\\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}_k} : \\mathcal{H}_k \\times \\mathcal{H}_k \\rightarrow \\mathbb{R}$ and $\\| \\cdot \\|_{\\mathcal{H}_k} : \\mathcal{H}_k \\rightarrow \\mathbb{R}$ denote the inner product and the norm of $\\mathcal{H}_k$, respectively. The reproducing property implies that for all $f \\in \\mathcal{H}_k$, and $z \\in Z$, $\\langle f, k(\\cdot, z) \\rangle_{\\mathcal{H}_k} = f(z)$. Mercer theorem implies that k can be represented using a possibly infinite dimensional feature map:\n$k(z, z') = \\sum_{m=1}^{\\infty} \\lambda_m \\phi_m(z) \\phi_m(z'),$\nwhere $\\lambda_m > 0$, and $\\sqrt{\\lambda_m} \\phi_m \\in \\mathcal{H}_k$ form an orthonormal basis of $\\mathcal{H}_k$. In particular, any $f \\in \\mathcal{H}_k$ can be represented using this basis and weights $w_m \\in \\mathbb{R}$ as\n$f = \\sum_{m=1}^{\\infty} w_m \\sqrt{\\lambda_m} \\phi_m,$\nwhere $\\|f\\|_{\\mathcal{H}_k}^2 = \\sum_{m=1}^{\\infty} w_m^2$. A formal statement and the details are provided in Appendix 8. We refer to $\\lambda_m$ and $\\phi_m$ as (Mercer) eigenvalues and eigenfunctions of kernel k, respectively."}, {"title": "2.3 Kernel-Based Prediction", "content": "Kernel-based models provide powerful predictors and uncertainty estimators which can be leveraged to guide the RL algorithm. In particular, consider a fixed unknown function $f \\in \\mathcal{H}_k$. Assume a t \u00d7 1 vector of noisy observations $y_t = [y_i = f(z_i) + \\varepsilon_i]_{i=1}^{t}$ at observation points $\\{z_i\\}_{i=1}^{t}$ is provided, where $\\varepsilon_i$ are independent zero mean noise terms. Kernel ridge regression provides the following predictor and uncertainty estimate, respectively (see, e.g., Sch\u00f6lkopf et al., 2002),\n$f_t(z) = k_t(z)^T (K_t + \\rho I)^{-1} y_t, \\\\ \\sigma_t^2(z) = k(z, z) - k_t(z)^T (K_t + \\rho I)^{-1} k_t(z),$"}, {"title": "2.4 Kernel-Based Modelling in RL", "content": "In our RL setting, we use a kernel-based model to predict the expected value function. In particular, for a given transition probability distribution $P(s', \\cdot)$ and a value function $v : S \\rightarrow \\mathbb{R}$, we define $f = [Pv]$ and use past observations to form predictions and uncertainty estimates for f, as detailed in the following section. The value functions vary due to the Markovian nature of the temporal dynamics. To effectively use the confidence intervals established by the kernel-based models on f, we require the following assumption."}, {"title": "Assumption 2.", "content": "We assume $P(s' | \\cdot, \\cdot) \\in \\mathcal{H}_k$, for some positive definite kernel k, and $\\| P(s' | \\cdot, \\cdot) \\|_{\\mathcal{H}_k} \\leq 1$ for all $s' \\in S$."}, {"title": "2.5 Eigendecay and Information Gain", "content": "Our regret bounds are presented in terms of maximum information gain which is a kernel-specific complexity term. Specifically, for a kernel k and sets of observation points $\\{z_i\\}_{i=1}^{t}$, we define the maximum information gain \u03b3(t; \u03c1) as follows\n$\\gamma(t; \\rho) = \\sup_{\\{z_i\\}_{i=1}^{t} \\subset Z} \\frac{1}{2} \\log \\det \\left( I + \\frac{K_t}{\\rho} \\right),$\nwhere \u03c1 > 0, and $K_t$ is the kernel matrix defined in Section 2.3. Several works have established upper bounds on \u03b3(t; \u03c1). In the special case of a d-dimensional linear kernel, we have \u03b3(t; \u03c1) = $\\mathcal{O}(d \\log(t))$. For kernels with exponential eigendecay, including SE, \u03b3(t; \u03c1) = $\\mathcal{O}(\\text{polylog}(t))$ (Srinivas et al., 2010; Vakili et al., 2021b). For kernels with polynomial eigendecay, which represent a crucial case due to challenges in establishing no-regret guarantees in RL and bandits, and include kernels of both practical and theoretical interest such as the Mat\u00e9rn family and NT kernels, we first provide the definition below and then the bound on \u03b3."}, {"title": "Definition 1.", "content": "A kernel k is said to have a p-polynomial eigendecay if $\\forall m \\geq 1$, $\\lambda_m \\leq C m^{-p}$, for some p > 1, C > 0 where $\\lambda_m$ are the Mercer eigenvalues of the kernel in decreasing order.\nFor kernels with p-polynomial eigendecay, we have (Vakili et al., 2021b, Corollary 1):\n$\\gamma(t; \\rho) = \\mathcal{O} \\left( \\left( \\frac{t}{\\rho} \\right)^{\\frac{1}{p}} \\left(\\log \\left( 1 + \\frac{t}{\\rho} \\right) \\right)^{\\frac{p-1}{p}} \\right).$"}, {"title": "3 KUCB-RL Algorithm", "content": "In this section, we introduce our algorithm, Kernel-based Upper Confidence Bound for Reinforcement Learning (KUCB-RL). The algorithm's structure is similar to acquisition-based kernel bandit algorithms such as GP-UCB (Srinivas et al., 2010), where each action is chosen as the maximizer of an acquisition function. We construct an optimistic proxy $q_t$ for the state-action value function. At each step t, given the current state $s_t$, the action $a_t$ is selected as the maximizer of $q_t(s_t, a)$ over a. This proxy $q_t$ is derived using past observations of transitions, employing kernel ridge regression to provide a prediction and uncertainty estimate for the state-action value function over a future window of size $w \\in \\mathbb{N}$. The proxy is established as an upper confidence bound, following the principle of optimism in the face of uncertainty. The value functions are computed in batches of w steps, and the derived policies are unrolled over the subsequent w steps. The details are presented next.\nWe define a fixed window size, $w \\in \\mathbb{N}$, which represents the future interval that the algorithm will consider. For a given $t_0$ where ($t_0 \\mod w) = 0$, including $t_0 = 0$, we initialize $U_{t_0+w+1}(s) = 0, \\forall s \\in S$, reflecting the algorithm's consideration of the reward within this future window of size w. Subsequently, we recursively obtain proxies $q_t$ and $v_t$ for all steps $t \\in \\{t : t_0 + 1 \\leq t \\leq t_0 + w\\}$. Let $f_t$ denote $[Pv_{t+1}]$, $f_t$ represent the kernel ridge predictor of $[Pv_{t+1}]$, and $\\sigma_t$ be its uncertainty estimator. The predictor and the uncertainty estimator are derived using the data set $\\mathcal{D}_{t_0}$, which contains observations of past transitions up to $t_0$. We use the notation $\\mathcal{D}_t = \\{(s_j, a_j, s_{j+1})\\}_{j<t}$ for the past transitions, and also define $v_{t+1, t_0} = [v_{t+1}(s_2), v_{t+1}(s_3), \\cdots, v_{t+1}(s_{t_0+1})]$, for the values of the proxy value function at the history of state observations. We then have\n$f_t(z) = k_{t_0}(z)^T (K_{t_0} + \\rho I)^{-1} v_{t+1, t_0}, \\\\ \\sigma_t^2(z) = k(z, z) - k_{t_0}(z)^T (K_{t_0} + \\rho I)^{-1} k_{t_0}(z),$\nwhere $k_t(z) = [k(z, z_1), k(z, z_2), \\ldots, k(z, z_t)]^T$ denotes the vector of kernel values between z and $(z_j = (s_j, a_j))_{j<t}$ in the history of observations, and $K_t = [k(z_i, z_j)]_{i, j=1}^{t}$ denotes the kernel matrix.\nEquipped with the kernel ridge predictor and uncertainty estimator, we define $q_t$ as an upper confidence bound for $f_t$, as follows:\n$q_t(z) = \\Pi_{[0, w]} (r(z) + f_t(z) + \\beta(\\delta) \\sigma_t(z)), \\quad \\forall z \\in Z,$\nwhere 1 \u2212 \u03b4 represents a confidence level, and \u03b2(\u03b4) is a confidence interval width multiplier; the specific value of which is given in Theorem 3. The notation $\\Pi_{[a, b]}(\\cdot)$ is used for projection on [a, b] interval. This step is natural since with the assumption $r : Z \\rightarrow [0, 1]$ the value over a window of size w can not be more than w. We also define\n$v_t(s) = \\max_{a \\in A} q_t(s, a), \\quad \\forall s \\in S.$\nBy iteratively updating from t = $t_0$ + w to t = $t_0$ + 1, we compute the values of $q_t$ and $v_t$ for all t from $t_0$ + 1 to $t_0$ + w. Then, we unroll the learned policy over the subsequent w steps, as the greedy policy with respect to $q_t$:\n$a_t = \\arg \\max_{a \\in A} q_t(s_t, a).$\nA pseudocode is provided in Algorithm 1.\nComputational Complexity. KUCB-RL enjoys a polynomial computational complexity of $\\mathcal{O}(T^3)$, where the bottleneck is the matrix inversion step in (5) in kernel ridge regression every w steps. This is not unique to our work and is common across kernel-based supervised learning, bandit, and RL literature. Luckily, sparse approximation methods such as Sparse Variational Gaussian Processes (SVGP) or the Nystr\u00f6m method significantly reduce the computational complexity of matrix inversion step (to as low as linear in some cases), while maintaining the kernel-based confidence intervals and, consequently, the eventual rates (see, e.g., Vakili et al., 2022, and references therein). These results are, however, generally applicable and not specific to our problem."}, {"title": "4 Regret Bounds for KUCB-RL", "content": "In this section, we provide analytical results on the performance of KUCB-RL. We prove the first sublinear regret bounds in undiscounted RL setting under general assumptions based on kernel-based modelling. We first derive a novel confidence interval that is broadly applicable to the kernel-based RL problems. We then utilize this result to establish bounds on the regret of KUCB-RL."}, {"title": "4.1 Confidence Intervals for Kernel Based RL", "content": "The analysis of our algorithm utilizes confidence intervals of the form $\\left|f(z) - f_t(z)\\right| \\leq \\beta(\\delta) \\sigma_t(z)$, where $f_t = [Pv_t]$ denotes the expected value of a value function $v_t$, and $f_t$ and $\\sigma_t$ represent the kernel ridge predictor and the uncertainty estimate of $f_t$. Here, \u03b2(\u03b4) represents the width multiplier for the confidence interval at a 1 \u2212 \u03b4 confidence level. Similar confidence intervals are established in kernel ridge regression for a fixed function f in the RKHS of a specified kernel k (see, e.g., Abbasi-Yadkori, 2013; Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Vakili et al., 2021a; Whitehouse et al., 2024). In the RL context, specific considerations are required as both $f_t = [Pv_t]$ and the observation noise depend on the value function $v_t$ that varies due to the Markovian nature of the temporal dynamics. We note that in this setting, for a given value function $v : S \\rightarrow \\mathbb{R}$, the observation noise is captured by $v(s_{t+1}) - [Pv](s_t, a_t)$. A possible approach involves deriving confidence intervals that apply to a class $\\mathcal{V}$ of value functions. Such results appear in some of the existing work (Chowdhury and Oliveira, 2023; Vakili and Olkhovskaya, 2023). The result most closely related to our is Chowdhury and Oliveira (2023), which derives its confidence interval under the exact same kernel related assumptions as our work, but for the episodic MDP setting. With the same assumptions, the confidence interval that we establish is different from the one in Chowdhury and Oliveira (2023). In particular, their confidence interval is applicable to a specific value of kernel ridge regression parameter \u03c1, constrained by their proof techniques. Inspired by Whitehouse et al. (2024), which established a confidence interval for kernel ridge regression (not within the RL context) but allowed for a judicious selection of \u03c1, we prove a new confidence interval suitable for the RL setting that allows tuning parameter \u03c1. As a result, we obtain the first improved no-regret algorithms in this setting."}, {"title": "Theorem 1 (Confidence Bound).", "content": "Consider v : S \u2192 R, a conditional probability distribution $P(s|z)$, s \u2208 S, z \u2208 Z, and two positive definite kernels $k : Z \u00d7 Z \\rightarrow \\mathbb{R}$ and $k' : S \u00d7 S \\rightarrow \\mathbb{R}$, where Z = S \u00d7 A is compact subset of $\\mathbb{R}^d$. Let $f = [Pv]$ and assume $\\|v\\|_{\\mathcal{H}_{k'}} \\leq C_v$, $v(s) \\leq w, \\forall s \\in S$, and $\\|f\\|_{\\mathcal{H}_k} \\leq C_f$, for some $C_v, w, C_f > 0$. A dataset $\\{(z_i, s'_i)\\}_{i=1}^{n} \\subset (Z \u00d7 S)^n$ is provided such that $s'_i \\sim P(\\cdot | z_i)$. Let $\\lambda_m$, m = 1, 2, ... denote the Mercer's eigenvalues of k' in a decreasing order and $\\psi_m$ denote the corresponding eigenfunctions, with $\\psi_m \\leq \\psi_{\\text{max}}$ for some $\\psi_{\\text{max}} > 0$.\nLet $f_n$ and $\\sigma_n$ be the kernel ridge predictor and the uncertainty estimate of f using the observations:\n$f_n(z) = k_n(z)^T(\\rho I + K_n)^{-1}v_n, \\\\ \\sigma_n(z) = k(z, z) - k_n(z)^T(\\rho I + K_n)^{-1}k_n(z),$\nwhere $v_n = [v(s'_1), v(s'_2), ..., v(s'_n)]^T$ is the vector of observations.\nFor all z \u2208 Z and v : $\\|v\\|_{\\mathcal{H}_{k'}} \\leq C_v$, the following holds, with probability at least 1 \u2212 \u03b4,\n$\\left|f(z) - f_n(z)\\right| \\leq \\beta(\\delta) \\sigma_n(z),$\nwith $\\beta(\\delta) = \\frac{C_f + \\frac{C_v \\psi_{\\text{max}}}{\\sqrt{\\rho}} \\sum_{m=1}^{M} \\sqrt{\\lambda_m}}{C_f + \\frac{C_v \\psi_{\\text{max}}}{\\sqrt{\\rho}} \\sum_{m=1}^{\\infty} \\sqrt{\\lambda_m}} \\left( 2 \\log \\left( \\frac{1}{\\delta} \\text{det} (I + \\frac{K_n}{\\rho}) \\right) \\right)^{\\frac{1}{2}} + \\frac{4 C_v \\psi_{\\text{max}}}{\\sqrt{\\rho}} \\left( n \\sum_{m=M+1}^{\\infty} \\lambda_m \\right)^{\\frac{1}{4}}$,"}, {"title": "Assumption 3.", "content": "For the kernel k', we assume that for some $C_1, C_2$ and $q > 0, \\sum_{m=1}^{M} \\lambda_m \\leq C_1$ and, $n \\sum_{m=M+1}^{\\infty} \\lambda_m \\leq C_2 M^{-q}$ for any $M \\in \\mathbb{N}$.\nThis is a mild assumption. For example, a p-polynomial eigendecay profile with p > 1, which applies to a large class of common kernels including SE, Mat\u00e9rn and NT kernels, satisfies this assumption with $C_1 = \\frac{pC}{p-1}$, $C_2 = \\frac{pC}{p-1}$, and $q = p - 1$, where C is the constant specified in Definition 1."}, {"title": "Remark 2.", "content": "Under Assumption 3, the expression of \u03b4 in Theorem 3 can be simplified as\n$\\beta(\\delta) = \\mathcal{O} \\left( \\frac{C_f}{\\sqrt{\\rho}} + \\frac{C_v}{\\sqrt{\\rho}} \\left( \\sqrt{\\gamma(\\rho; n)} + \\left( \\frac{C_2}{C_1} \\gamma(\\rho; n) \\right)^{\\frac{1}{4}} \\right) \\right).$\nRemark 2 can be observed by selecting $M = [n^{\\frac{1}{4}}]$ in the expression of \u03b2(\u03b4), which provides a straightforward presentation of the confidence interval width multiplier.\nThe proof of Theorem 1 involves the Mercer representation of v in terms of $\\psi_m$. The expression of the prediction error $|f(z) - f_n(z)|$ is then decomposed into error terms corresponding to each $\\psi_m$. We then partition these terms into the first M elements corresponding to eigenfunctions with the largest M eigenvalues and the rest. For each of the first M eigenfunctions, we obtain high probability bounds using existing confidence intervals from Whitehouse et al. (2024). Summing up over m, and using a bound based on uncertainty estimates, we achieve a high probability bound\u2500corresponding to the second term in the expression of \u03b2(\u03b4). We then bound the remaining m > M elements based on the decay of Mercer eigenvalues\u2014corresponding to the third term in the expression of \u03b2(\u03b4). A detailed proof is provided in Appendix 6."}, {"title": "Theorem 1", "content": "is presented in a self-contained way, making it broadly applicable across various RL settings. In the following section, we apply this theorem within the analysis of the infinite horizon average reward setting to obtain a no-regret algorithm. This is the first no-regret algorithm within this setting under general kernel-related assumptions."}, {"title": "4.2 Regret Analysis of KUCB-RL", "content": "The weakest assumption regarding value functions is realizability, which suggests that the optimal value function v* either belong to the an RKHS or are at least well-approximated by its elements. In the degenerate case of bandits with |S| ="}]}