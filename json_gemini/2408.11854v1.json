{"title": "When Raw Data Prevails: Are Large Language Model Embeddings\nEffective in Numerical Data Representation\nfor Medical Machine Learning Applications?", "authors": ["Yanjun Gao", "Skatje Myers", "Shan Chen", "Dmitriy Dligach", "Timothy A Miller", "Danielle Bitterman", "Matthew Churpek", "Majid Afshar"], "abstract": "The introduction of Large Language Models\n(LLMs) has advanced data representation and\nanalysis, bringing significant progress in their\nuse for medical questions and answering. De-\nspite these advancements, integrating tabular\ndata, especially numerical data pivotal in clini-\ncal contexts, into LLM paradigms has not been\nthoroughly explored. In this study, we examine\nthe effectiveness of vector representations from\nlast hidden states of LLMs for medical diag-\nnostics and prognostics using electronic health\nrecord (EHR) data. We compare the perfor-\nmance of these embeddings with that of raw\nnumerical EHR data when used as feature in-\nputs to traditional machine learning (ML) algo-\nrithms that excel at tabular data learning, such\nas eXtreme Gradient Boosting. We focus on\ninstruction-tuned LLMs in a zero-shot setting\nto represent abnormal physiological data and\nevaluating their utilities as feature extractors\nto enhance ML classifiers for predicting diag-\nnoses, length of stay, and mortality. Further-\nmore, we examine prompt engineering tech-\nniques on zero-shot and few-shot LLM embed-\ndings to measure their impact comprehensively.\nAlthough findings suggest the raw data features\nstill prevails in medical ML tasks, zero-shot\nLLM embeddings demonstrate competitive re-\nsults, suggesting a promising avenue for future\nresearch in medical applications.", "sections": [{"title": "1 Introduction", "content": "Numerical data plays a pivotal role across various\ndomains. For instance, much of the data used for\nanalytics from electronic health records (EHRs)\nare numerical values in tabular formats, document-\ning patient demographics (e.g., age), vital signs,\nlaboratory tests, and nurse assessments. Utilizing\nnumerical data for predictive modeling has been in-\nstrumental in facilitating accurate diagnoses, risk stratifying, and outcome predictions in healthcare.\nMachine learning (ML) classifiers like gradient\nboosted have excelled\nin these tasks for making accurate clinical predic-\ntions."}, {"title": "2 Related Work", "content": "Recent studies highlight LLMs in tabular data\nanalysis: introduces\nTableLLM, which converts tables to text using a\nmanual template. studies CoT"}, {"title": "3 Datasets and Tasks", "content": ""}, {"title": "3.1 Diagnosis prediction for clinical\ndeterioration", "content": "Early warning systems often use rule-based and ML\nalgorithms to identify patients at risk of deteriora-\ntion or death without providing diagnoses. To address this,\nexperts from multiple hospitals created a dataset\nthat labels the diagnoses for patients who had a clin-\nical deterioration event during their hospitalization.\nThese expert-annotated diagnoses were performed\nwith a full review of the EHR and served as the la-\nbels for our training data. Twenty-four tabular data\nfeatures including demographics, vital signs, labs,\ninterventions, and nursing assessments were ex-\ntracted from the structured EHR (eg. tabular data).\nThey were previously identified as critical variables\nfor clinical deterioration. The\nfinal datasets encompassed EHR data from 660\nadult patients in medical-surgical ward within a\nU.S. health system. The primary diagnoses were\nSepsis, Arrhythmia (Arrhy.), and Congestive Heart\nFailure (CHF) volume overload, with prevalence\nrates of 43.18% for Sepsis, 15.30% for Arrhyth-\nmia, and 11.82% for CHF, respectively. We used\n5-fold validation on all 660 samples to generate\nfive distinct test sets."}, {"title": "3.2 Mortality and length-of-stay prediction", "content": "The MIMIC-III dataset, derived from the EHR of\nthe Critical Care Units (ICU) at Beth Israel Dea-\nconess Medical Center, has been utilized exten-\nsively in research. further developed an open-source\npipeline for extracting, preprocessing, and repre-\nsenting data from the MIMIC-III database, namely\nMIMIC-Extract. This pipeline aggregates various\ndata types, such as tabular demographic data avail-\nable at admission, vital signs with repeated mea-\nsures, laboratory test results, time-varying inter-\nvention signals, and prediction labels needed for\nclinical tasks. MIMIC-Extract introduces two clini-\ncal prediction tasks: mortality and length-of-stay\n(LOS) predictions. The mortality prediction task\nuses tabular data from the first 24-hour window of\na patient's ICU stay to predict mortality as a binary\nclassification task. The LOS prediction task, in\ncontrast, determines whether a patient's stay will\nexceed three (LOS 3) or seven days (LOS 7) based\non the same 24-hour data period. Importantly, to"}, {"title": "4 Methods and Experiment Setup", "content": "Figure 2 illustrates the study overview and experi-\nment setup. We began with a patient's tabular data\ninput, represented using the Pandas DataFrame data\nstructure (raw data). This raw data was converted\nto text using four distinct conversion methods, de-\ntailed in \u00a74.1, and LLM encoded the converted text,\nwith the last hidden states extracted to generate em-\nbedding features (\u00a74.2). These embeddings were\nsubsequently used to train various ML classifiers\non two datasets for binary prediction tasks.\nWe started with zero-shot, off-the-shelf LLMs\nfor experiments (\u00a74.3). We then investigated the\nimpact of prompt-engineering techniques and few-\nshot learning configurations on the embeddings\nand subsequent predictions (\u00a74.4). An initial inves-\ntigation was also conducted to assess the effects\nof parameter-efficient fine-tuning on LLM embed-\ndings for ML tasks, focusing on two of the models\n(\u00a74.5).\nAs baselines, we included traditional ML classi-\nfiers trained directly on raw tabular data inputs. To\nbenchmark the effectiveness of LLM embeddings,\nwe used randomly initialized embeddings of the\nsame size as the LLM-generated embeddings."}, {"title": "4.1 Table-to-text conversion", "content": "We employed four different methods to convert\nEHR tables into input formats for LLMs: NAR-\nRATIVES, JSON, HTML, and MARKDOWN. NAR-\nRATIVES provide a continuous text description of\npatient data, offering context and readability simi-\nlar to clinical notes. JSON struc-\ntures the data hierarchically, making it easy to parse\nand interpret programmatically. HTML format leverages web-based structures to\npresent the data with tags. MARKDOWN offers a lightweight markup\nlanguage that provides formatting while remaining\nreadable in plain text."}, {"title": "4.2\nEmbedding extraction methods", "content": "This section introduces the methods used to con-\nvert input text to fixed-size vector for ML input.\nWe focused on the last hidden states of LLMs (as\nin , and employed three different\nembedding extraction methods: Max Pooling cap-"}, {"title": "4.3 Selection of LLMs", "content": "We assessed a mix of general-domain models\nand models trained on medical text. Three\nwidely-used, general-domain LLMs that have\nbeen instruction-finetuned are Mistral-7B-Instruct-v0.1, Llama2-13B-chat-hf,\nLlama2-70B-chat-hf, and\nLlama3-8B-instruct. These mod-\nels are compatible with one Nvidia 80GB A100\nGPU, making them popular choices among avail-\nable LLMs. For the domain specific LLM, We se-\nlected Meditron-7B, a Llama2-\n7B based model continuously pretrained on medi-\ncal text. We also included ClinicalBERT, pre-trained on MIMIC EHR text, rep-\nresenting encoders pre-trained on clinical text base-\nline compared to decoder-only LLMs."}, {"title": "4.4 Prompt design and few-shot learning", "content": "Because the majority of LLMs we tested are\ninstruction-tuned and require varying input formats,\nwe utilized the chat templates to ensure proper\nintegration of input data. In our\nstudy, the default setting involves including only\nthe task-relevant question (shown on the right side\nof Fig 2) in the system message and the converted\nEHR data in the user input, without additional\nsystem instructions, predefined personas, or other\ncontext. Given that instruction-tuned LLMs are\nknown to be sensitive to system instructions, we\ndesigned four system instructions that vary by per-\nsona (medical professional, AI system), tasks (assess pa-\ntients, generate embeddings for ML classifiers), thinking\nstyle (chain-of-thoughts), and question type (general as-\nsessment, binary question), enabling us to explore the\ninfluence of prompt characteristics on the embed-\ndings. All prompts were paraphrased for better\nperplexity scores, following prompt optimization\nstrategies.\nTwo few-shot settings were explored besides\nzero-shot prompt engineering. We generated syn-\nthetic data for diagnosis prediction, by prompting\nGPT-4 to generate values based on the attribute\nnames in Table 1. For each target diagnosis, GPT-4\ngenerated one example confirming the diagnosis\n(positive) and one example negating it (negative).\nMoreover, GPT-4 was asked to generate CoT ex-\nplanations identifying abnormal values and their\nclinical significance. An expert physician and clini-\ncal informaticist reviewed these synthetic data pairs\nfor quality assurance. The complete set of prompts\nare presented in Table 8."}, {"title": "4.5 Parameter efficient fine-tuning", "content": "While our paper primarily focuses on evaluating\nzero-shot LLMs for numerical feature representa-\ntion, we included parameter-efficient fine-tuning\nexperiment to suggest future directions for improve-\nment. We employed QLoRA on Mistral-7B-Instruct and Llama3-8B-Instruct, us-\ning the MIMIC-Extract dataset due to its larger\ntraining set compared to the diagnosis dataset. We\ntrained Mistral with a sequence classification head\non top, saving checkpoints with the lowest valida-\ntion loss. Based on validation performance, we\noptimized the (q, k, v, o) layers with r = 16, a\nlearning rate of 3e-5, and a LoRA dropout of 0.1.\nEach model was trained for 3 epochs with early\nstopping to prevent overfitting."}, {"title": "4.6 Experiment setup", "content": "We used a 5-fold cross-validation on the diagnosis\ndataset (660 patient records), resulting in 528 pa-\ntients for training and 132 for testing per fold. For\nmortality and LOS prediction tasks from MIMIC-"}, {"title": "5 Results", "content": ""}, {"title": "5.1 Main results for diagnosis prediction", "content": "Table 4 presents AUROC scores for predicting Sep-\nsis, Arrhythmia, and CHF with different ML mod-\nels, demonstrating the effects of using LLM embed-\ndings compared to raw data features. Traditional\nML classifiers with raw features demonstrate high\nAUROC scores.\nFor LLM embeddings with zero-shot setting, we\nobserved performance gain over a randomly initial-\nized embedding approach into XGB with substan-"}, {"title": "5.2 Main results for mortality prediction and\nlength-of-stay", "content": ""}, {"title": "5.3 Comparisons across different embedding\nmethods and data conversion methods", "content": "Figure 4 presents AUROC values for different\nembedding methods and data conversion formats\nacross three models: Mistral, Meditron, and\nLlama3-8b. Max pooling achieves the highest per-\nformance for Mistral (64.62) and Meditron (62.54),\nwhile mean pooling is most effective for Llama3-\n8b (64.69). The last token method yields moder-\nate performance across all models, with AUROCS\naround 57, while first token embeddings result in\nthe lowest AUROC values, indicating a less effec-\ntive representation for these models.\nWhen encoding data with different formats, Mis-\ntral shows preference for NARRATIVES, JSON, and\nHTML. The MARKDOWN format generally yielded\nthe lowest performance across the models, particu-\nlarly for Mistral. JSON and HTML formats showed\ncompetitive performance, with JSON being slightly\nmore effective for Meditron and Llama3-8b. No-\ntably, Llama3-8b exhibited the highest variability\nacross formats, with AUROCs ranging from 57.40\n(HTML) to 66.13 (NARRATIVES)."}, {"title": "5.4 Impact of prompt engineering and\nfew-shot learning", "content": "We compared performance of Mistral and Llama3\nusing different system instructions under zero-shot\nand few-shot settings, as well as CoT examples.\nMistral, under 0-shot with a system instruction with\npersona of medical professional and the task of as-\nsessing patient condition (prompt 1 in Table 8),\nachieved an AUROC of 71.35 on Sepsis prediction,\nthe highest of all models. Llama3 with zero-shot\nprompting using prompt 1 in Table 8 showed re-\nported AUROC of 73.51 on Arrhythmia, surpass-\ning its counterpart at 71.08 but still below raw data\nXGB baseline (76.49). CoT and few-shot exhibited\nvarious performance and often resulted in lower\nAUROC scores compared to Table 4. Full results\nare provided in Appendix C."}, {"title": "5.5 Parameter efficient fine-tuning results", "content": "Table 5 presents results of Mistral and Llama3-\n8b under the QLoRA across all four tasks from\nMIMIC-Extract. The performance drops are no-\nticeable, especially in the two mortality predictions.\nTo further understand the reason behind the perfor-\nmance drops, we plotted the confusion matrices for\nLOS 3 and Mort ICU, comparing Mistral's predic-\ntions before and after QLoRA in Figure 5. For LOS\n3 prediction, the Mistral model with QLoRA shows\nan increase in true negatives and a decrease in false\npositives. However, the false negatives rises from\n1133 to 1473, and true positive drops from 918 to\n578. On the Mort ICU task, the Mistral model with\nQLORA correctly predicts no false positives, but\nit fails to predict any positive cases (0 true posi-\ntives). The performance drop can be attributed to\nthe imbalanced class distribution in the dataset, as\nthe models show a tendency to favor the majority\nclass (negative cases). During QLoRA, the LLM\nmight learn the class prevalence, biasing its rep-\nresentation and making it challenging to correctly"}, {"title": "5.6 LLM Embedding vs LLM Generation", "content": "The final experiments compare the performance of\nLLM embeddings combined with ML classifiers\nagainst direct outputs from LLMs. This compar-\nison shows that, although LLM embeddings gen-\nerally do not outperform raw data features, they\noffer a more robust and reliable solution than rely-\ning on LLMs to directly answer Yes or No ques-\ntions. Our exploration revealed significant limita-\ntions in LLM generation for binary prediction tasks.\nFor instance, Mistral frequently predicted 'Yes' for\nsepsis, arrhythmia, and CHF AORC, resulting in\nAUROC scores being 50, whereas LLM embed-\ndings achieved AUROCs of 71.12 for sepsis, 72.26\nfor arrhythmia, and 63.54 for CHF AUROC. Simi-\nlar patterns were observed from Llama3-8b results\n(Table 12). On MIMIC-Extract tasks with highly\nskewed class distributions, Mistral and Llama3-\n8b, when generating direct Yes/No answers, again\nshowed reduced ability to discriminate between\npositive and negative cases (Table 13). These find-\nings underscore the need for embeddings, which\nprovide a more nuanced and effective approach\nfor clinical predictions. We refer readers to Ap-\npendix D for more details."}, {"title": "6 Discussion", "content": "To understand the discrepancy between the two\ndata representations, we examined the training ef-\nfectiveness of raw data features and LLM embed-\ndings by controlling the training set size. Figure 6\ncompares the performance of the raw data XGB\nbaseline model with the Mistral and Meditron em-\nbeddings across different training set sizes for two\ntasks in the MIMIC dataset. The raw data XGB\nbaseline model shows a significant increase in AU-\nROC scores with larger training sets, achieving\nhigh performance. In contrast, both the Mistral and\nMeditron embeddings paired with XGB models\nexhibit much smaller improvements, consistently\nperforming lower than the raw data XGB baseline.\nThis highlights the greater effectiveness of XGB\nwhen learning from raw data features compared to\nLLM embeddings for these prediction tasks.\nOur findings suggest that raw data features pro-\nvide more informative input for ML models com-\npared to LLM-generated embeddings. While LLM\nembeddings capture complex representations, they\nmay not be as tailored for binary medical predic-\ntion tasks. Additionally, computing efficiency is an\nimportant consideration, as LLMs require signifi-\ncantly more GPU memory than raw data features.\nHowever, zero-shot LLM embeddings achieve\ncomparable performance in certain scenarios, high-\nlighting their potential for rapid deployment with-\nout extensive training. A promising direction is\ndistilling these embeddings into a smaller space\nwhile retaining their extensive knowledge. recently\nproposes LLM2Vec, a method to train decoder-only\nLLMs as text encoders with unsupervised training,\nwhich merits further investigation."}, {"title": "7 Conclusion", "content": "We present the first analysis of LLM embeddings\nfor numerical EHR data features in medical ML ap-\nplications, showing the opportunity and challenges\nof using LLM embeddings as a substitute of raw\ndata features. We hope to encourage future research\non improving LLM embeddings, particularly for\nimbalanced label prediction, and advancing health\npredictions with multi-modal data, while address-\ning interpretability and bias."}, {"title": "8 Limitation", "content": "In our study, we focused on investigating some\nof the most common LLMs, including Meditron,\nMistral, Llama2, and Llama3. Due to GPU con-\nstraints, some experiments, such as Qlora, were\nconducted on only one or two models, limiting the\ncomprehensiveness of our analysis. We did not\ninclude black-box LLMs via API because, despite\nusing fully de-identified data, both EHR datasets\nare protected under Data Use Agreement, restrict-\ning us sharing with third parties. Additionally, we\nacknowledge that we did not explore all possible\nmethods of prompting LLMs, which may have in-\nfluenced our results. Furthermore, our examination\nwas restricted to the last layers of the LLMs, poten-\ntially overlooking valuable information encoded in\nother layers.\nRegardless of these limitations, our findings are\nconsistent across models: zero-shot LLM embed-\ndings paired with machine learning classifiers gen-\nerally underperform compared to raw data features,\nthough they sometimes achieve comparable perfor-\nmance."}, {"title": "9 Ethical Statement", "content": "Following the ACL's ethical review guidelines, our\nstudy on leveraging LLMs for medical diagnosis\nwithin EHR emphasizes ethical integrity by priori-\ntizing harm avoidance, privacy protection, fairness,\ntransparency, and respect for intellectual property.\nWhile our research aims to advance medical di-\nagnostics through LLMs, there is a potential risk\nthat misinterpretations of model predictions could\ninadvertently lead to diagnostic errors or bias in\nclinical decision-making. Therefore, rigorous vali-\ndation protocols, including expert medical review\nand bias detection mechanisms are needed to en-\nsure that model predictions are both accurate and\nequitable across diverse patient populations.\nWe have rigorously ensured data de-\nidentification, obtained ethical approvals,\nactively mitigated biases, and maintained openness\nin our methodologies and findings to uphold\nhonesty and reproducibility. Our commitment\nextends to respecting intellectual property through\nproper attribution and license adherence, with\nthe overarching goal of contributing positively\nto healthcare outcomes and societal well-being.\nThis approach underscores the importance of\nrobust, secure research practices in developing\ncomputational tools for healthcare, aligning with\nour ethical responsibility to advance the field for\nthe public good."}, {"title": "C Prompt Design", "content": "To test the impact of prompt engineering on LLM\nembeddings, we designed four distinct prompts,\nvarying by persona, thinking style, and question\ntypes. Our prompt engineering strategy builds on\nprior work that utilizes perplexity to select opti-\nmal prompts. Initially, we\ncrafted meta-language prompts delineating clinical\nscenarios and tasks. We then employed GPT-4, a\nstate-of-the-art LLM, to generate five paraphrases\nfor each prompt to capture a range of expressions.\nWe evaluated each paraphrase's naturalness and\nfluency by calculating its perplexity score, select-"}, {"title": "D Results of LLM Direct Generation", "content": "We tested the ability of Mistral and Llama3 to di-\nrectly predict Yes or No answers to questions from"}, {"title": "E Parameter Grids for ML Classifiers", "content": "We conducted a comprehensive grid search\nfor hyperparameter optimization on two classi-\nfiers: XGBoost (XGB) and Logistic Regression.\nFor the XGB classifier, the parameter grid in-\ncluded nestimators set to [50, 100, 250, 500],\nmaxdepth ranging from [2, 5, 10, 15, 20],\nlearningrate values of [0.005, 0.01, 0.05, 0.1],\nand minchildweight values of [1, 2, 3]. This ex-\ntensive search aimed to identify the best combina-\ntion of hyperparameters to enhance model perfor-\nmance.\nFor the Logistic Regression classifier, we varied\nalpha with values of [0.1, 0.5, 1.0] and l1ratio with\nvalues of [0.1, 0.5, 0.9]. This grid search was de-\nsigned to fine-tune the regularization parameters to\nachieve optimal balance between model complexity\nand performance.\nGrid-searching on XGB parameters took 25-40\nminutes on GPU. On LR, it took about 25 minutes\nto search for the best parameters. Training both\nclassifiers took less than 5 minutes, even on the\nMIMIC-Extract dataset where there are more than\n16000 samples."}]}