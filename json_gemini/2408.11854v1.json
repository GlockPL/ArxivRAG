{"title": "When Raw Data Prevails: Are Large Language Model Embeddings\nEffective in Numerical Data Representation\nfor Medical Machine Learning Applications?", "authors": ["Yanjun Gao", "Skatje Myers", "Shan Chen", "Dmitriy Dligach", "Timothy A Miller", "Danielle Bitterman", "Matthew Churpek", "Majid Afshar"], "abstract": "The introduction of Large Language Models\n(LLMs) has advanced data representation and\nanalysis, bringing significant progress in their\nuse for medical questions and answering. De-\nspite these advancements, integrating tabular\ndata, especially numerical data pivotal in clini-\ncal contexts, into LLM paradigms has not been\nthoroughly explored. In this study, we examine\nthe effectiveness of vector representations from\nlast hidden states of LLMs for medical diag-\nnostics and prognostics using electronic health\nrecord (EHR) data. We compare the perfor-\nmance of these embeddings with that of raw\nnumerical EHR data when used as feature in-\nputs to traditional machine learning (ML) algo-\nrithms that excel at tabular data learning, such\nas eXtreme Gradient Boosting. We focus on\ninstruction-tuned LLMs in a zero-shot setting\nto represent abnormal physiological data and\nevaluating their utilities as feature extractors\nto enhance ML classifiers for predicting diag-\nnoses, length of stay, and mortality. Further-\nmore, we examine prompt engineering tech-\nniques on zero-shot and few-shot LLM embed-\ndings to measure their impact comprehensively.\nAlthough findings suggest the raw data features\nstill prevails in medical ML tasks, zero-shot\nLLM embeddings demonstrate competitive re-\nsults, suggesting a promising avenue for future\nresearch in medical applications.", "sections": [{"title": "1 Introduction", "content": "Numerical data plays a pivotal role across various\ndomains. For instance, much of the data used for\nanalytics from electronic health records (EHRs)\nare numerical values in tabular formats, document-\ning patient demographics (e.g., age), vital signs,\nlaboratory tests, and nurse assessments. Utilizing\nnumerical data for predictive modeling has been in-\nstrumental in facilitating accurate diagnoses (Pang\net al., 2021), risk stratifying (Zeiberg et al., 2019;\nGreen et al., 2018), and outcome predictions (Akel\net al., 2021; Chang et al., 2019) in healthcare.\nMachine learning (ML) classifiers like gradient\nboosted (Chen and Guestrin, 2016) have excelled\nin these tasks for making accurate clinical predic-\ntions (Churpek et al., 2024a; Lolak et al., 2023;\nMoore and Bell, 2022).\nRecent work shows Large Language Models\n(LLMs)' vast potential on text generation over\nstructured data input, including Chain-of-Thought\n(CoT) reasoning over tabular data (Zheng et al.,\n2023), classification on diseases (Hegselmann et al.,\n2023). LLMs have also exhibited exceptional\npromise in medical NLP tasks, evident in their stel-\nlar performance in the United States Medical Li-\ncensing Examination (MedQA) (Nori et al., 2023).\nHowever, the use of embedding representations,\nparticularly for medical diagnostics and outcome\npredictions using standard EHR numerical data,\nremains largely unexplored. In these areas, raw\ndata inputs have traditionally dominated feature\nrepresentation for ML algorithms before the era\nof LLMs. This is exemplified by their use in\ncritical applications such as mortality prediction\nand early sepsis warnings (Deng et al., 2022; Hou\net al., 2020), and patient infection (Bashiri et al.,\n2022; Bhavani et al., 2020). The potential of LLM-\nderived features as a viable alternative to raw data\nfeatures in ML applications is still unclear.\nThis study aims to address this knowledge gap by\nexamining the use of LLM embeddings for EHR\nnumerical data representation in ML algorithms.\nAlthough LLMs are renowned for text generation,\ntheir embeddings may offer multiple advantages,\nsuch as leveraging LLMs' pre-trained knowledge\nand sophisticated text understanding to enhance\ndomain-specific tasks. Moreover, using LLMs to\nrepresent tabular data allows for a unified model\nthat encodes both structured and unstructured text\nin EHRs, seamlessly integrating and contextualiz-\ning information across modalities, such as embed-\nded tables in clinical notes (Soenksen et al., 2022;\nKline et al., 2022).\nOur work presents novel examination of the im-\npact of different formats and embedding methods\non LLM last layers and ML classifiers. We focus\non open-source, zero-shot LLMs suitable for single-\nGPU systems, considering the resource limitations\nprevalent in many hospitals and academic research\nsettings. To establish a foundation for this work,\nwe probed Mistral-7B-Instruct and Llama2-13B,\ntwo open-source, general-domain LLMs, for their\nknowledge of reference ranges for vital signs and\nlab test values. We directly asked about the stan-\ndard physiological values and units of measurement\nfor 24 EHR features identified as critical predictor\nvariables for detecting clinical deterioration (Akel\net al., 2021). As in Figure 1, physician judgment\nindicates that LLMs possess this knowledge, pro-\nviding initial evidence for further investigation.\nOur study utilizes three clinical prediction tasks\nderived from two independent EHRs and four ML\nclassifier input settings. We investigate the im-\npact of table-to-text conversion formats, embed-\nding extraction methods, prompt engineering, and\nfew-shot techniques, along with early results from\nparameter-efficient fine-tuning, on the quality of\nLLM embeddings. Our main contributions are\nthreefold:\n\u2022 We present a comprehensive study exploring var-\nious factors that influence the performance of\nnumerical EHR feature embeddings generated by\nLLMs for medical ML applications.\n\u2022 Our findings show that while LLM embeddings\npaired with XGB classifiers can achieve perfor-\nmance comparable to traditional raw data fea-\ntures on some tasks, performance gaps persist,\nnecessitating further improvements to maximize\ntheir effectiveness.\n\u2022 We discuss the efficiency and robustness of LLM\nfeature representation for numerical data versus\nraw data in training ML classifiers.\nResults show that, despite external evidence indi-\ncating that LLMs possess extensive knowledge of\nmedical facts, extracting usable representations of\nthis knowledge for downstream tasks will require\nsignificant additional methodological progress."}, {"title": "2 Related Work", "content": "Recent studies highlight LLMs in tabular data\nanalysis: Hegselmann et al. (2023) introduces\nTableLLM, which converts tables to text using a\nmanual template. Zheng et al. (2023) studies CoT\nreasoning over tables. Akhtar et al. (2023) ex-\namines the abilities of LLMs on numerical data\nunderstanding. Zhu et al. (2024), closest to our\nwork, explores zero-shot LLM for structured longi-\ntudinal EHR data and finds that GPT-4 can outper-\nform XGB on clinical prediction tasks. Our study,\nhowever, uniquely focuses on open-box LLM em-\nbeddings for enhancing ML algorithms.\nRaw EHR data are commonly used in medical\nML applications, as found by a survey on medical\nML research (Si et al., 2021). They noted that labs\nand vital signs as frequent data types for patient\nrepresentation learning. Churpek et al. (2024a) in-\ntroduces an XGB algorithm predicting clinical de-\nteriorations using EHR features like demographics\nand lab values. Wang et al. (2020) used 104 clini-\ncal EHR features across various ML algorithms to\nestablish baselines for clinical tasks such as mor-\ntality predictions. Our work uses the same dataset\nand tasks as (Wang et al., 2020) to compare LLM\nembeddings against traditional ML classifier out-\ncomes on the same raw data feature baseline."}, {"title": "3 Datasets and Tasks", "content": ""}, {"title": "3.1 Diagnosis prediction for clinical\ndeterioration", "content": "Early warning systems often use rule-based and ML\nalgorithms to identify patients at risk of deteriora-\ntion or death without providing diagnoses (Churpek\net al., 2014; Kipnis et al., 2016). To address this,\nexperts from multiple hospitals created a dataset\nthat labels the diagnoses for patients who had a clin-\nical deterioration event during their hospitalization.\nThese expert-annotated diagnoses were performed\nwith a full review of the EHR and served as the la-\nbels for our training data. Twenty-four tabular data\nfeatures including demographics, vital signs, labs,\ninterventions, and nursing assessments were ex-\ntracted from the structured EHR (eg. tabular data).\nThey were previously identified as critical variables\nfor clinical deterioration (Akel et al., 2021). The\nfinal datasets encompassed EHR data from 660\nadult patients in medical-surgical ward within a\nU.S. health system. The primary diagnoses were\nSepsis, Arrhythmia (Arrhy.), and Congestive Heart\nFailure (CHF) volume overload, with prevalence\nrates of 43.18% for Sepsis, 15.30% for Arrhyth-\nmia, and 11.82% for CHF, respectively. We used\n5-fold validation on all 660 samples to generate\nfive distinct test sets."}, {"title": "3.2 Mortality and length-of-stay prediction", "content": "The MIMIC-III dataset, derived from the EHR of\nthe Critical Care Units (ICU) at Beth Israel Dea-\nconess Medical Center, has been utilized exten-\nsively in research (Johnson et al., 2016). Wang\net al. (2020) further developed an open-source\npipeline for extracting, preprocessing, and repre-\nsenting data from the MIMIC-III database, namely\nMIMIC-Extract. This pipeline aggregates various\ndata types, such as tabular demographic data avail-\nable at admission, vital signs with repeated mea-\nsures, laboratory test results, time-varying inter-\nvention signals, and prediction labels needed for\nclinical tasks. MIMIC-Extract introduces two clini-\ncal prediction tasks: mortality and length-of-stay\n(LOS) predictions. The mortality prediction task\nuses tabular data from the first 24-hour window of\na patient's ICU stay to predict mortality as a binary\nclassification task. The LOS prediction task, in\ncontrast, determines whether a patient's stay will\nexceed three (LOS 3) or seven days (LOS 7) based\non the same 24-hour data period. Importantly, to\navoid competing risk outcomes between death and\nLOS, patients who died within the 3- or 7-day LOS\nwindow were excluded from the LOS prediction.\nWe adopted the same data partitioning used\nin (Wang et al., 2020), comprising 16,700, 2,394,\nand 4,790 patient records for the training, develop-\nment, and testing sets. Each patient record includes\n104 time-varying tabular data features. More de-\ntailed demographic information can be found in the\nMIMIC-Extract study (Wang et al., 2020). The\nlabels in the MIMIC-Extract dataset are highly\nskewed, with positive label distributions of 42.82%\nfor LOS 3, 7.66% for LOS 7, 10.27% for Mort\nHosp, and 7.10% for Mort ICU."}, {"title": "4 Methods and Experiment Setup", "content": "Figure 2 illustrates the study overview and experi-\nment setup. We began with a patient's tabular data\ninput, represented using the Pandas DataFrame data\nstructure (raw data). This raw data was converted\nto text using four distinct conversion methods, de-\ntailed in \u00a74.1, and LLM encoded the converted text,\nwith the last hidden states extracted to generate em-\nbedding features (\u00a74.2). These embeddings were\nsubsequently used to train various ML classifiers\non two datasets for binary prediction tasks.\nWe started with zero-shot, off-the-shelf LLMs\nfor experiments (\u00a74.3). We then investigated the\nimpact of prompt-engineering techniques and few-\nshot learning configurations on the embeddings\nand subsequent predictions (\u00a74.4). An initial inves-\ntigation was also conducted to assess the effects\nof parameter-efficient fine-tuning on LLM embed-\ndings for ML tasks, focusing on two of the models\n(\u00a74.5).\nAs baselines, we included traditional ML classi-\nfiers trained directly on raw tabular data inputs. To\nbenchmark the effectiveness of LLM embeddings,\nwe used randomly initialized embeddings of the\nsame size as the LLM-generated embeddings."}, {"title": "4.1 Table-to-text conversion", "content": "We employed four different methods to convert\nEHR tables into input formats for LLMs: NAR-\nRATIVES, JSON, HTML, and MARKDOWN. NAR-\nRATIVES provide a continuous text description of\npatient data, offering context and readability simi-\nlar to clinical notes (Yu et al., 2023). JSON struc-\ntures the data hierarchically, making it easy to parse\nand interpret programmatically (Zhao et al., 2023).\nHTML format leverages web-based structures to\npresent the data with tags (Garc\u00eda-Ferrero et al.,\n2024). MARKDOWN offers a lightweight markup\nlanguage that provides formatting while remaining\nreadable in plain text (Zhao et al., 2023).\nTable 2 includes two NARRATIVES templates\nused to format these varied clinical measurements\ninto a standardized query. These templates detail\nthe format in which data from the EHR dataset\nare presented, integrating both laboratory results\nand vital signs into a single descriptive snapshot of\na patient's current state. Each placeholder in the\ntemplate is populated with actual data points from\npatient records, facilitating the transformation of\ntabular EHR data into a format suitable for LLM\ninput, from which we then generate embeddings.\nThe primary distinction between the templates\nfor the diagnosis prediction dataset and the MIMIC-\nExtract dataset lies in the types of values incor-\nporated. For diagnosis prediction, data are values\ncollected immediately before the early warning sys-\ntem triggers for clinical deterioration. In contrast,\nMIMIC-Extract tasks include laboratory and vital\nsigns data from the 24 hours prior to the event. We\nextracted all unique values observed during the first\n24 hours of ICU admission in chronological order,\ncompiling these into a list format. If a feature has\nno observations, it is omitted, resulting in variable\nlength sequences."}, {"title": "4.2 Embedding extraction methods", "content": "This section introduces the methods used to con-\nvert input text to fixed-size vector for ML input.\nWe focused on the last hidden states of LLMs (as\nin (Lu et al., 2021)), and employed three different\nembedding extraction methods: Max Pooling cap-\ntures the most salient features by taking the maxi-\nmum value across all token embeddings for each\ndimension (Bao et al., 2023); Mean Pooling com-\nputes the average value of the token embeddings,\nproviding a balanced representation reflecting the\noverall content (Ram et al., 2023); Last Token\nuses the embedding of the last token as the repre-\nsentation, capturing the concluding context or final\nsummary (Shani et al., 2023; Fu et al., 2023). We\nincluded embeddings extracted from first token as\na reference point despite it is not ideal due to the\nnature of decoder-only models.\nOur choice of ML classifiers comprised two tree-\nbased methods and a linear model to provide a\ncomprehensive assessment of various predictive\napproaches. Specifically, we utilized eXtreme Gra-\ndient Boosting (XGB)(Chen and Guestrin, 2016)\nand Random Forest (RF)(Breiman, 2001) as our\ntree-based classifiers due to their robustness and ef-\nficiency in handling diverse datasets with accuracy.\nAdditionally, Logistic Regression with regulariza-\ntion (LR) as our linear model was chosen for its\neffectiveness in preventing overfitting via Ridge\nand Least Absolute Shrinkage and Selection Opera-\ntor regularization(Zou and Hastie, 2005). Together,\nthese classifiers form a balanced baseline setup\nthat caters to both non-linear and linear decision\nboundaries in our data."}, {"title": "4.3 Selection of LLMs", "content": "We assessed a mix of general-domain models\nand models trained on medical text. Three\nwidely-used, general-domain LLMs that have\nbeen instruction-finetuned are Mistral-7B-Instruct-"}]}