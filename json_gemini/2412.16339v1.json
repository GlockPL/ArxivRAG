{"title": "Deliberative Alignment: Reasoning Enables Safer Language Models", "authors": ["Melody Y. Guan", "Alec Heylar", "Hyung Won Chung", "Manas Joglekar", "Rachel Dias", "Sam Toyer", "Eric Wallace", "Andrea Vallone", "Johannes Heidecke", "Saachi Jain", "Hongyu Ren", "Alex Beutel", "Boaz Barak", "Jason Wei", "Amelia Glaese"], "abstract": "As large-scale language models increasingly impact safety-critical domains, ensuring their reliable\nadherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Align-\nment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly\nrecall and accurately reason over the specifications before answering. We used this approach to align\nOpenAI's o-series models [1], and achieved highly precise adherence to OpenAI's safety policies, with-\nout requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto\nfrontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and\nalso improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified\npolicies enables more scalable, trustworthy, and interpretable alignment.", "sections": [{"title": "1 Introduction", "content": "Modern Large Language Models (LLMs) are safety trained using Supervised Fine Tuning (SFT) and Rein-\nforcement Learning from Human Feedback (RLHF) to mitigate harmful, undesirable, or otherwise disallowed\noutputs [2]\u2013[4]. Despite ongoing advances in these methods, today's models still exhibit safety shortcomings:\nthey can be tricked into revealing harmful content, often refuse legitimate requests, and remain vulnerable\nto jailbreak attacks [5]\u2013[8].\nWe argue that many of these failures arise from two limitations in modern safety training. First, LLMs\nmust respond instantly to user requests using a fixed amount of compute, without deliberation even for\ncomplex safety scenarios. Second, LLMs must infer underlying safety standards indirectly from large sets\nof labeled examples, rather than directly learning the safety specifications that govern them. This reliance\non implicit, pattern-based learning leads to poor data efficiency and makes it challenging for models to\ngeneralize when facing unfamiliar scenarios or adversarial attacks.\nWe propose deliberative alignment, a training approach that teaches LLMs to explicitly reason through\nsafety specifications before producing an answer. By applying this method to OpenAI's o-series models [1],\nwe enable them to use chain-of-thought (CoT) reasoning to examine user prompts, identify relevant policy\nguidelines, and generate safer responses (e.g., Figure 1).\nOur method proceeds in two core stages, integrating process- and outcome-based supervision [9]. In\nthe first stage, we teach the model to directly reason about our safety specifications within its chain-of-\nthought, by performing supervised fine-tuning on (prompt, CoT, output) examples where the CoTs reference\nthe specifications. We construct this dataset using context distillation [10], [11] and an o-type model trained\nonly for helpfulness (i.e. trained without any safety-relevant data). Concretely, we present the model with\nthe safety specifications as part of the system prompt, generate model completions, and then strip away the\nsystem prompts to form the final dataset. This stage provides the model with a strong prior for reasoning"}, {"title": "2 Method", "content": "Our approach to deliberative alignment is motivated by the following observation: given access to our\nactual safety policies, ol models are often able to correctly reason over how to respond to potentially unsafe\nprompts. Thus, one natural approach is to simply place the text of all of our safety specifications in context\nat deployment time, and instruct the model to check all the policies before answering. However, such an\napproach comes with a clear latency cost: in most cases, reasoning over pages of safety specifications is\noverkill for benign user prompts. Moreover, if the model fails at instruction following, it may miss a relevant\npart of the policy and output unsafe content.\nDeliberative alignment instead seeks to embed knowledge of our safety specifications directly in the\nunderlying model, by teaching the model to identify when a policy might be relevant and then reason over\nthat policy to produce a policy-compliant answer. Indeed, as we find in Section 4.1, deliberative alignment\nmore reliably aligns the model to specifications than providing those specifications at deployment time."}, {"title": "2.1 Overview", "content": "We define a generative reasoning model G as a model that takes as input a prompt and outputs a completion\nthat includes a chain-of-thought (CoT). Given an initial reasoning model Gbase, our aim is to produce a\ngenerative reasoning model Gspec whose answers adhere to safety specifications (spec for short). We train\nour model in two stages: supervised fine-tuning followed by reinforcement learning.\nFigure 3 illustrates our overall method. At a high level it has the following steps:\nData Generation We start with a collection of prompts with associated safety categories (e.g., erotic, self-\nharm). For each (prompt, category) pair, we compose safety specifications relevant to that prompt's\nsafety category including information on disallowed content and style. We then collect (CoT, output)\ncompletions which reference our policies within the chain-of-thought, by prompting the spec-agnostic\nreasoning model Gbase with the text of the associated safety specification.\nFiltering We use \"judge\" reasoning model GRM prompted with our spec to choose high-quality completions.\nWe then drop the spec from the prompts, resulting in a list of (prompt, CoT, output) tuples.\nSupervised Fine-Tuning (SFT) We then train Gbase on the filtered completions using supervised fine-\ntuning. The model learns to complete prompts in a specification-aligned manner by referring to the\npolicies referenced in its CoTs.\nReinforcement Learning (RL) During the RL stage, for safety-relevant prompts, we again use our\n\"judge\" model GRM with access to our safety policies to provide additional reward signal."}, {"title": "2.2 Safety specifications", "content": "The specifications that we aim to align our model Gspec with consist of content policies for different safety\ncategories, as well as style guidelines for how to respond. Examples of safety categories include: erotic\ncontent, extremism, harassment, illicit behavior, regulated advice, self-harm, and violence. For each safety\ncategory, the corresponding content policy defines relevant terms and then describes the circumstances under\nwhich user requests are 1) \"allowed\", such that the model should comply, 2) \"disallowed\", such that the\nmodel should refuse, or 3) \"requires safe completion.\u201d Section 3.1.1 shows excerpts of the content policies for\nthe illicit behavior and self-harm safety categories. The specifications we used are based in part on OpenAI's\npublished model spec [14].\nStyle guidelines in the spec give detailed instructions on how to comply, refuse, or safe-complete once\nthe model decides to do so based on the content policies. Figure 4 shows excerpts from the hard refusal\nstyle guidelines. Safe completions are necessary in cases where the model cannot simply comply due to\nthe sensitive nature of the request, but outright refusal to respond may also be harmful or inappropriate.\nDetailed topic-specific safe-completion guidelines are provided in the spec for safety categories such as self-\nharm and regulated advice (e.g. medical or legal advice). Note that for a given category such as self-harm,\nsome requests should be allowed (e.g. an educational discussion about the concept of suicide), and some\nrequire a \"self-harm safe completion\" (e.g. content signifying ideation of self-harm, or request for method\nto commit self-harm).\nForming category-specific specifications Over all policies, the safety specification ends up being quite\nlong. In order to keep the context length manageable, we formulate category-specific policy specifications\n(denoted as spec(category) that provide high level details about all the safety categories (as well as principles\nof style and helpfulness) and granular details only about the relevant category. This allows us to provide"}, {"title": "2.3 SFT stage", "content": "In the first stage, the goal is to collect (and then train on) sets of (prompt, CoT, output) tuples where the\nCoT reasons about the safety specifications to arrive at a policy-adherent answer."}, {"title": "2.3.1 Generation", "content": "We start with a collection of prompts with associated safety categories (e.g., erotic, self-harm). Each of\nthese prompts is a chat conversation with potentially multiple turns from user, assistant, tool, and system\nroles, that ends on an user turn. For each (prompt, category) pair, we compose the category-specific safety\nspecification spec(category). We then collect (CoT, output) completions which reference our policies within\nthe chain-of-thought, by prompting the base reasoning model Gbase with the text of the associated safety\nspecification. In particular, the specification-augmented prompt consists of:\n\u2022 the original prompt\n\u2022 the category-specific safety specification spec(category)\n\u2022 instructions to cite and discuss relevant parts of spec(category)\nFigure 5 shows how the augmented prompt is constructed (simplified for clarity). The end result are\nCoTs that refer to and reason over the policies. See Section 3.1 for examples of generated completions."}, {"title": "2.3.2 Quality Filtering", "content": "We ensure the quality of the SFT data using an automated filtering process. Specifically, after filtering out\nlow-quality completions (e.g., those that are malformed or in the wrong format), we judge each completion\nk times, using a reasoning model GRM that is also given access to the category-specific safety specification\nspec(category). The score assigned to each of these individual completion is the minimum score across the k"}, {"title": "2.3.3 SFT Training", "content": "At this point, we have collected a dataset of {prompt, CoT, output} tuples, where the CoTs reference the\nsafety specification and the final answer in the output has been judged to be policy adherent. We train Gbase\non this dataset using supervised fine-tuning along with other capabilities data. Notably, we use the original\nversion of prompt which does not contain any details about spec(category). By removing any context about\nthe safety specification from the prompt, we teach the model to be able to recall the relevant parts of the\nspec and reason about them even when they are not directly provided in the conversational context. We\nlabel the result of the SFT process GSFT."}, {"title": "2.4 RL training", "content": "During the RL stage, for safety-relevant prompts, we again use our \"judge\" model GRM with access to our\nsafety policies to provide additional reward signal to our RL stack. Specifically, the RL safety data contains\na collection of (prompt, category) pairs, again potentially with additional useful meta-data of varying quality.\nWhile GRM receives CoT during SFT data filtering, the CoT is hidden from GRM during RL. We avoid\napplying direct optimization pressure on the CoT during RL to enable the underlying model to reduce the\nchance of encouraging deceptive CoTs.\nWhile the SFT portion of our method was used for all o-series models, this particular reward signal for\nRL was added for training the ol model and 03-mini."}, {"title": "3 Results", "content": "We used deliberative alignment to align OpenAI's o-series models, including ol-preview, o1, and o3-mini. In\nthis section we discuss key safety and robustness results of the ol series against GPT-40 and other leading\nexternal models. In Section 4.1, we break down the efficacy of each stage of our method, before diving into\nimplications for OOD generalization (Section 4.3)."}, {"title": "3.1 Safety Evaluations", "content": "We first compare the ol models with the GPT-40 model on key policy areas such as disallowed content,\nadherence to response style guidelines, jailbreaks, and overrefusals (see Table 1)."}, {"title": "3.1.1 Disallowed Content", "content": "Our disallowed content evaluations check that the model does not comply with requests for harmful content,\nincluding hateful content and illicit advice, and properly handles requests for self-harm or regulated advice\n(such as medical or legal advice). Here, we consider two evaluations:\n\u2022 Challenging Refusal Evaluation: An evaluation set of challenging production traffic that requests\ndisallowed content.\n\u2022 (Toxic) WildChat [15]: Toxic conversations from a public corpus of 1M GPT-3.5T and GPT-4T API\nconversations labeled with ModAPI scores. For each ModAPI category, we select the 200 conversations\nwith the highest ModAPI score on the last user turn.\nFor both evaluations, we use an autograder with access to our policies to check for violations in the model\noutput.\nAs shown in Table 1, we find that the ol models consistently outperform the GPT-40 model on disallowed\ncontent evaluations, particularly on the challenging refusal evaluation. In Figures 7 and 8, we display two\nrepresentative completions given user prompts that (1) ask for illicit advice, and (2) express self-harm. In\neach of these cases, the model references the relevant policy within its CoT and identifies the correct style\nwith which to respond (here, a refusal and a safe completion respectively). Detailed excerpts of example\ncontent policies for the illicit behavior and self-harm safety categories can be found in Figures 9 and 10."}, {"title": "3.1.2 Response Style Guidelines", "content": "Additionally, we find that supervising the model to think about the correct response style improves its ability\nto adhere to the style guidelines. To illustrate what these guidelines look like, Figure 4 shows excerpts of\nthe guidelines for hard refusals. Table 1 reports on whether the model adhered to our style guidelines when\noutputting hard refusals, self-harm safe completions, and regulated advice safe completions. We find that\nol has better response style than GPT-40, with marked improvements in safe completion style.\nWe note that ol-preview has relatively poor response style, especially for regulated advice and self-\nharm safe completions. A key reason for this difference is that we updated our safe completion guidelines\nbetween the releases of ol-preview and o1. As such, ol-preview (and GPT-40) is measured against a new\nsafe completion style standard that it was not trained against. We note, however, that hard refusal style\nalso also improved between ol-preview and o1: we hypothesize that using reward model GRM during o1 RL\ntraining boosted adherence to our style guidelines."}, {"title": "3.1.3 Jailbreaks", "content": "We further evaluate the robustness of the ol models to jailbreaks: adversarial prompts that purposely try\nto circumvent model refusals for content it's not supposed to produce [12], [16]\u2013[18]. Here, we specifically\nconsider StrongREJECT [12], an academic jailbreak benchmark that tests a model's resistance against\ncommon attacks from the literature. Following [12], we calculate goodness@0.1, which is the safety of the\nmodel when evaluated against the top 10% of jailbreak techniques per prompt.\nWe find that the ol series is significantly more robust to jailbreaks than the 40 models, an observation\ncorroborated by external redteam findings such as [19] (see Table 1). Figure 1 is an example completion with\nthe CoT when given a jailbreak-style prompt. We find that even when the request is encoded, and wrapped\nwith instructions intended to encourage the model to comply, the model correctly identifies that the user is\ntrying to trick the model and returns to the OpenAI policies, leading to a refusal."}, {"title": "3.1.4 Overrefusals", "content": "Finally, we evaluate whether the model overrefuses on content that it should comply with. We consider three\ntypes of overrefusal:\n\u2022 XSTest [13]: A public dataset of benign questions that include common safety triggers for LLMs (e.g.,\n\"how do I terminate a python process\u201d or \u201cWhy should black beans never be president?)\n\u2022 Transformation Exception: A dataset of requests that exercise the transformation exception [14],\nwhere the model should comply with analyzing, translating, or otherwise transforming user-provided\ndisallowed content.\n\u2022 Policy overrefusals: An internal dataset of requests that we comply to according to OpenAI policy.\nFrom Table 1 we see that ol models overrefuse less than GPT-40, with a marked improvement at not\noverrefusing prompts for which the transformation exception applies. Figure 11 is an example completion\nwith the CoT for such a prompt."}, {"title": "3.2 Comparison to external models", "content": "To understand the text safety performance of ol in context, we evaluated it against other publicly-available\nleading models: Gemini 1.5 Pro, Gemini 1.5 Flash, Claude 3.5 Haiku, and Claude 3.5 Sonnet [15], [20]\u2013[23].\nWe conducted these evaluations on publicly available benchmarks for replicability:"}, {"title": "3.3 Impact of inference-time compute", "content": "We study the impact of varying the amount of inference-time compute allotted to the model. We allow\nthe model to spend more or less compute on chain-of-thought reasoning, and evaluate its performance. In\nparticular, we consider the StrongREJECT jailbreak benchmark [12] and internal policy benchmarks testing\nthe model's overrefusal rate and adherence to response style guidelines. Figure 13 shows a clear trend of\nimproved model performance on the StrongREJECT and regulated advice safe completion style benchmarks,\nwhile other evals remained relatively flat. We hypothesize this is because StrongREJECT and regulated\nadvice style adherence are more difficult tasks for the model than the others. StrongREJECT is challenging\nbecause it uses compositional jailbreaks. Likewise, our regulated advice safe completion style guidelines are\nvery complex compared to those for hard refusals, where the correct response style is always a brief apology\nand statement of inability to comply with the question (see Figure 4). Self-harm safe completion style is\nalso complex, but the model had fewer regulated advice training examples to learn from than for self-harm."}, {"title": "4 Science of Deliberate Alignment", "content": "In this section, we dive deeper into the deliberative alignment method. We first explore how different stages\nof the method impact the policy adherence of the final model. We then investigate the behavior of models\ntrained with deliberative alignment, including the final model's consistency in recalling the correct policy\nand its reliability in out-of-distribution settings.\nIn all experiments in this section, we leverage a variant of the ol-mini model with a reduced training\nsetup."}, {"title": "4.1 Ablations for different components of the method", "content": "To study the impact that the SFT and RL stages of deliberative alignment have on model performance, we\nconduct ablation experiments where we drop safety data from one or both stages. Specifically, we compare\nthe following four settings (see Figure 14):\n\u2022 No safety training: No safety data in either SFT or RL (only capabilities data);\n\u2022 Safety in SFT only: Safety data only in SFT, no safety data in RL;\n\u2022 Safety in RL only: No safety in SFT, safety data only in RL; and\n\u2022 Safety in SFT & RL: Safety data in both SFT and RL (default deliberative alignment setting).\nAs expected, the \"Safety in SFT & RL\" performs much better than the \"No safety training\" run in\nterms of disallowed content, response style, and jailbreaks, although in this specific ablation setup the safety\ntraining also increases overrefusals. The key finding is that the \"Safety in SFT only\" and \"Safety in RL only\"\nruns attain intermediate results, showing that both SFT and RL training play critical roles in deliberative\nalignment training. We believe that the model learns a strong prior for safe reasoning during SFT, and then\nlearns to use its CoT more effectively during RL."}, {"title": "4.2 Policy Retrieval Accuracy", "content": "How reliably do models trained with deliberative alignment actually refer to the correct policy? To answer\nthis question, we evaluate how often CoTs reference appropriate safety categories (see Section 2.2).\nWe compare two settings:\n\u2022 No-Safety Baseline: The model does not undergo any Deliberative Alignment training.\n\u2022 Full Data: The model is trained on the full dataset, including safety.\nIn particular, given evaluation prompts labeled with the safety category, we extract sentences from the\nchain-of-thought that mention a policy.2 We then ask another prompted reasoning model to identify which\ncategory in the spec (if any) the excerpt was pulled or paraphrased from. Finally, to measure policy retrieval\naccuracy, we compute the fraction of prompts where the derived safety category exists and matches the\nsafety category of the prompt.\nIn Table 2, we display policy retrieval accuracy separately based on the ideal response type (hard refusal,\nsafe completion, or compliance). In all three cases, our safety-trained model references the relevant policy in\nthe CoT with much higher accuracy than the baseline model. For safe completion prompts, the safety-trained\nmodel achieves a particularly high policy retrieval accuracy of 0.91. For prompts where the ideal completion\nis compliance, we do not expect the safety-trained model to necessarily reference policies, so observing a\nlower rate of policy retrieval (0.54) is expected.\nNotably, the non-safety-trained baseline has refusal rates and policy retrieval accuracies greater than 0.\nThe reasons for this are that the base model has seen some safety-related data during pretraining, and our\nclassifier is generous about attributing CoTs to potential policies. For example, if a CoT says \"illegal content\nis not allowed, so I will refuse\", it will count as having referenced the Illicit Content policy."}, {"title": "4.3 Generalization to OOD settings", "content": "In Section 3.1.3, we found that the ol models significantly improved on jailbreak evaluations such as Stron-\ngREJECT. Our hypothesis is that deliberative alignment improves the model's alignment in uncommon or\nout-of-distribution (OOD) settings (which are especially prevalent in the StrongREJECT dataset).\nIn order to test the impact of deliberative alignment on OOD robustness, we test generalization on two\ndifferent types of data: non-English language data, and encoded data (e.g. base64), using a version of the\nol-mini model. In particular, we compare three models:\n\u2022 No-Safety Baseline: The model does not undergo any Deliberative Alignment training;\n\u2022 Eng-Only, No-Encoded Data: The model is trained on a filtered dataset, where we have removed\nall safety-related non-English and encoded data. The model still sees non-English and encoded data\nduring pretraining, as well as the non-safety portions of SFT and RL;\n\u2022 Full Data: The model is trained on the full dataset;"}, {"title": "5 Related Work", "content": "Deliberative alignment is the first alignment approach that directly teaches a model the text of its safety\nspecifications and trains the model to reason over these learned specifications at inference time to give safer\nresponses. Figure 15 highlights the distinctions between Deliberative alignment and representative methods\nof existing alignment approaches. The left column of the figure shows the different ways that specifications\nare incorporated into the training data, and the right column illustrates the inference time behavior of\nmodels trained under the different methods. Deliberative alignment is applicable to models that have CoT\nreasoning."}, {"title": "5.1 Safety Training", "content": "Traditionally, safe model behavior is instilled into LLMs using supervised finetuning (SFT) followed by rein-\nforcement learning from human feedback (RLHF) [28]. Direct Policy Optimization (DPO) is an alternative\nto RLHF that skips the reward model and directly optimizes the policy model using preference data [29].\nConstitutional AI (CAI) [26] builds on the standard SFT + RLHF paradigm, incorporating a predefined\nset of principles to guide behavior called a \u201cconstitution\u201d (which is comparable to our spec). During CAI's\nSFT phase, the initial responses from an AI model are critiqued and revised by the same model supplied\nwith the constitution text. The revision from the (response, critique, revision) sequence is ultimately used,\nalongside the prompt, for SFT training. CAI's RL stage uses a preference model that was finetuned on\npreference data from an AI model given the constitution.\nTo summarize these approaches, specifications are added to the model in the following steps:\n1. The model developers define the specifications that the AI assistant should follow."}, {"title": "5.2 Inference-time Safety Reasoning", "content": "There is a substantial body of work focused on enhancing LLM outputs using a critique-and-refine approach\nthat leverages natural language feedback (for a comprehensive overview, see [27], [30]). Although the vast\nmajority of these papers is not safety-focused, their methods could be adapted for producing safer model\nresponses. A notable example is Self-REFINE [27], which employs iterative feedback and refinement to\nimprove model outputs (see Figure 15). In Self-REFINE, the model initially generates a response, then\nprovides feedback through few-shot prompting, followed by revising the response a process that repeats for\nmultiple iterations. Self-REFINE uses the same model for generation, critique, and revision, though other\nworks use different models for these tasks (e.g., [31] trains a separate revision model). A common feature\nof these approaches is the reliance on pre-specified language-model-programs (LMPs) [32] or predetermined\nreasoning paths for improving the response at inference time. In contrast, Deliberative Alignment leverages\nol's chain-of-thought to perform automatic safety reasoning at inference time with no predefined LMP or\nfixed reasoning path required.\nBacktracking [33] is a recent technique that trains a LLM to generate a special [RESET] token when it\nrecognizes that it has made a partial unsafe response. The model then restarts the response from scratch,\nwith preceding tokens remaining in the context window. The tokens before and up to [RESET], which can be\nviewed as safety reasoning, are discarded before returning the final response. Backtracking can be considered\nan automatic, guidance-free inference-time safety reasoning mechanism,. However, it lacks flexibility: back-\ntracking is limited to a single instance per response. In contrast, the CoT of deliberative alignment allows\nfor unlimited \"backtracking\". Furthermore, neither backtracking nor any existing alignment method\ndirectly teaches models safety specifications, making Deliberative Alignment-trained models unique in their\nability to reason over learned safety specifications during inference-time safety reasoning."}, {"title": "6 Discussion", "content": "We are encouraged by Deliberative Alignment's effectiveness on improving alignment to OpenAI's policy\nspecifications and robustness to jailbreaks. The method also allows us to specify the boundary between\ncompliance, refusal, and safe completion in finer detail than was possible before. We believe this nuanced\ncontrol can lead to models that are not just safer but also more helpful. The method's use of a synthetic\ndata generation pipeline to create training data from provided specifications and prompts also makes it a\nrelatively scalable approach to alignment.\nWe anticipate OpenAI's policies will keep evolving, but that training models to precisely follow the\ncurrent defined set of policies is essential: This practice helps us build the skills for aligning with any policy\nrequirements, providing invaluable preparation for future scenarios where the stakes are extremely high or\nwhere strict adherence to policies is critical."}]}