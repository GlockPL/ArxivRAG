{"title": "Taking AI Welfare Seriously", "authors": ["Robert Long", "Jeff Sebo", "Patrick Butlin", "Kathleen Finlinson", "Jacqueline Harding", "Jacob Pfau", "Jonathan Birch", "Kyle Fish", "Toni Sims", "David Chalmers"], "abstract": "In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic in the near future. That means that the prospect of Al welfare and moral patienthood of AI systems with their own interests and moral significance is no longer an issue only for sci-fi or the distant future. It is an issue for the near future, and AI companies and other actors have a responsibility to start taking it seriously. We also recommend three early steps that AI companies and other actors can take: They can (1) acknowledge that Al welfare is an important and difficult issue (and ensure that language model outputs do the same), (2) start assessing AI systems for evidence of consciousness and robust agency, and (3) prepare policies and procedures for treating AI systems with an appropriate level of moral concern. To be clear, our argument in this report is not that AI systems definitely are or will be - conscious, robustly agentic, or otherwise morally significant. Instead, our argument is that there is substantial uncertainty about these possibilities, and so we need to improve our understanding of AI welfare and our ability to make wise decisions about this issue. Otherwise there is a significant risk that we will mishandle decisions about Al welfare, mistakenly harming AI systems that matter morally and/or mistakenly caring for Al systems that do not.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 A transitional moment for AI welfare", "content": "In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic, and thus morally significant, in the near future. We also argue that AI companies have a responsibility to acknowledge that AI welfare is a serious issue; start assessing their Al systems for welfare-relevant features; and prepare policies and procedures for interacting with potentially morally significant AI systems. Plausible philosophical and scientific theories, which accord with mainstream expert views in the relevant fields, have striking implications for this issue, for which we are not adequately prepared. We need to take steps toward improving our understanding of AI welfare and making wise decisions moving forward.\nWe release this report during a transitional moment for AI welfare. For most of the past decade, AI companies appeared to mostly treat AI welfare as either an imaginary problem or, at best, as a problem only for the far future. As a result, there appeared to be little or no acknowledgment that AI welfare is an important and difficult issue; little or no effort to understand the science and philosophy of AI welfare; little or no effort to develop policies and procedures for mitigating welfare risks for AI systems if and when the time comes; little or no effort to navigate a social and political context in which many people have mixed views about AI welfare; and little or no effort to seek input from experts or the general public on any of these issues.\nRecently, however, some Al companies have started to acknowledge that AI welfare might emerge soon, and thus merits consideration today. For example, Sam Bowman, an AI safety research lead at Anthropic, recently argued (in a personal capacity) that Anthropic needs to \u201clay the groundwork for AI welfare commitments,\" and to begin to \"build out a defensible initial understanding of our situation, implement low-hanging-fruit interventions that seem robustly good, and cautiously try out formal policies to protect any interests that warrant protecting.\u201d Google recently announced that they are seeking a research scientist\u2074 to work on \u201ccutting-edge societal questions around machine cognition, consciousness and multi-agent systems\". High-ranking members of other companies have expressed concerns as well.\nThis growing recognition at AI companies that AI welfare is a credible and legitimate issue reflects a similar transitional moment taking place in the research community. Many experts now believe that Al welfare and moral significance is not only possible in principle, but also a realistic possibility in the near future. And even researchers who are skeptical of Al welfare and moral significance in the near term advocate for caution; for example, leading neuroscientist and consciousness researcher Anil Seth writes, \u201cWhile some researchers suggest that conscious AI is close at hand, others, including me, believe it remains far away and might not be possible at all. But even if unlikely, it is unwise to dismiss the possibility altogether [emphasis ours].\u201d"}, {"title": "1.2 The risks of mishandling AI welfare", "content": "When assessing the welfare and moral patienthood of nonhumans, including other animals and AI systems, we face two kinds of risk: the risk of over-attributing welfare and moral patienthood to nonhumans, and the risk of under-attributing these properties to nonhumans. Over-attribution of welfare and moral patienthood is a false positive: mistakenly seeing, or treating, an object as a subject, or a non-moral patient as a moral patient. Under-attribution of these properties is a false negative: mistakenly seeing, or treating, a subject as an object, or a moral patient as a non-moral-patient. Both of these mistakes can lead to significant costs or harms in this context, and we will need to navigate both of them with caution.\nWhen there is a clear asymmetry between competing risks for example, when false positives are far more severe than false negatives, or vice versa then we might be able to mitigate risk by simply \"erring on the side of caution\" in cases where a more complex risk assessment is either intractable or unnecessary. But when there is at least a rough symmetry between competing risks for example, when false positives and false negatives are comparably severe a simple precautionary strategy may not be possible. We may have to engage in more complex risk assessment to the extent possible, attempting to mitigate both kinds of risks in a reasonable, proportionate manner.\nHow should we think about risks involving nonhuman welfare and moral patienthood in this context? In the case of nonhuman animals, it seems plausible that the harms of under-attribution of welfare and moral patienthood are often far worse than the risk of over-attribution, which makes precautionary reasoning appropriate in those contexts. However, in the case of AI, both errors could cause grave harm, either to humans (and other animals) or to AI systems. Both kinds of harm could also scale rapidly depending on the trajectory of AI development and deployment from here. This predicament makes it difficult to simply \"err on the side of caution,\u201d which underscores the urgency of improving our understanding of these issues.\nOn the one hand, the harm of under-attributing welfare and moral patienthood to AI systems could be significant. When we mistakenly see a subject as an object, we risk harming or neglecting them unnecessarily. For example, factory farming, animal research, and other such industries kill hundreds of billions of vertebrates and trillions of invertebrates every year. And as evidence that these animals are welfare subjects and moral patients has accumulated, our species has been slow to accept it, in part because of our increasing dependence on these industries. Now that our species is finally starting to accept this evidence, it will take us decades to transform these industries, during which many more animals will suffer and die unnecessarily.\nIn the future, similar harms could follow from under-attributing welfare and moral patienthood to AI systems. The AI industry is currently at an early stage of development, and depending on the path that it takes from here, we could use even more AI systems than animals in the future, and we could scale up our use of them even more rapidly. This is particularly true in the current paradigm, which requires an enormous amount of compute for training and much less for inference. If an AI system in such a paradigm could be a welfare subject and moral patient, then many model instances could be run after training. Unlike with animals, the scale of the problem could increase by orders of magnitudes more or less instantaneously.\nOn the other hand, the harm of over-attributing welfare and moral patienthood to AI systems could be significant as well. First of all, there could be substantial opportunity costs associated with this error. At present, we lack the ability to fully care for the eight billion humans alive at any given time, to say nothing of the quintillions of other animals alive at any given time. If we treated an even larger number of AI systems as welfare subjects and moral patients, then we could end up diverting essential resources away from vulnerable humans and other animals who really needed them, reducing our own ability to survive and flourish. And if these AI systems were in fact merely objects, then this sacrifice would be particularly pointless and tragic.\nThe over-attribution of welfare and moral patienthood to AI systems could also be actively harm-ful. For example, if we treated AI systems as welfare subjects and moral patients with many of the same interests as typical adult humans, then we could end up extending them many of the same legal and political rights as typical adult humans, including the right to legal and political representation and participation. This could, in turn, empower AI systems to act contrary to our own interests, with devastating consequences for our species (although some have argued that neglect for AI systems would carry a similar risk). As with the risk of opportunity costs, this risk would apply even if these AI systems are in fact subjects. But if they were in fact merely objects, then accepting this risk would likewise be particularly pointless and tragic.\nBy default, we should not expect our \"common sense\" intuitions about Al welfare and moral patienthood to be reliable; we will not handle this issue well simply by reacting to situations as they arise. We have dispositions that can lead to under- and over-attribution of these properties in non-humans, depending on the nature of the nonhumans and our interactions with them. These include dispositions toward anthropomorphism, that is, a tendency to see nonhumans as having human traits that they lack. They also include dispositions towards anthropodenial, that is, a tendency to see nonhumans as lacking human traits that they have. Both tendencies have caused errors regarding animals, and they will likely have a similar effect regarding AI systems.\nA number of factors make us more likely to anthropomorphize nonhumans and, perhaps falsely, attribute consciousness and other such capacities to them. For instance, studies suggest that we are more likely to attribute consciousness and other such capacities to beings who move at a similar speed as humans, rather than faster or slower. We are more likely to attribute agency to beings who have the appearance of eyes, who have distinctive motion trajectories, and who engage in contingent interaction that is, behavior that is apparently self-directed. Evidence also suggests that features such as \u201ccuteness\" can encourage attributions of mental states and moral patienthood.\nMany robots or chatbots are designed to appear conscious and charismatic, and in the future, many AI systems will have bodies, life-like motion, and (at least apparently) contingent interactions. Furthermore, unlike nonhuman animals, AI systems are already increasingly able to hold extremely realistic conversations, making seemingly thoughtful contributions in realistic timeframes. These traits do not guarantee that humans will see and treat these systems as welfare subjects and moral patients, but they will increase the probability of such reactions. In fact, there have already been some prominent and others less so cases of humans becoming convinced that current chatbots are welfare subjects and moral patients.\nAt the same time, a number of factors make us more likely to engage in anthropodenial as well. For instance, when we consider the mechanisms that produce nonhuman behavior taking what Daniel Dennett has called taking a \u201cmechanistic stance\" towards nonhumans we become less likely to attribute mental states to those nonhumans. There appear to be motivational factors that encourage anthropodenial as well. For instance, those who are invested in social, political, or economic systems that subjugate nonhumans may be more likely to view these nonhumans as \"lesser than\". Similarly, those who find it useful to treat nonhumans as objects may be more likely to deny that these nonhumans are welfare subjects and moral patients.\nWhile discussions about AI welfare and moral patienthood understandably focus on AI systems like robots and chatbots that appear conscious and charismatic, many other AI systems like image generators or algorithmic trading systems lack these features. Even if such systems were in fact conscious and robustly agentic, we might not recognize these capacities in them. And as these systems become increasingly embedded in society, we might have increasingly strong incentives to view them as mere objects. Companies, governments, and other powerful actors who benefit from this technology might then promote and reinforce our objectification of these systems, attempting to frame moral consideration for these systems as fringe and unserious.\nAt present, it is an open question which kind of risk will be more likely for particular kinds of AI systems, including seemingly conscious and charismatic systems like robots and chatbots. The more advanced such systems become, the more likely both risks might become in different respects: We might over-attribute based on their behavioral similarities with humans, but under-attribute based on their architectural differences from humans. And the more economically dependent on chatbots we become, the more likely over-attribution and under-attribution might become for them in different respects as well: For example, we might over-attribute for digital \u201ccompanions\" but under-attribute for other kinds of digital minds.\nWhile further research is required for a comprehensive assessment of these risks, at least this much is plausible: Given our track record with animals and the current pace of AI development, the risk of under-attribution appears to be both reasonably likely and reasonably harmful. To the extent that we also risk over-attribution, we cannot simply avoid this risk by defaulting to treating Al systems as mere objects. We should thus accept that Al welfare is difficult to get right, and do the necessary work to improve our decisions by assessing AI systems for evidence of consciousness, robust agency, and other such capacities, and preparing policies and procedures for treating AI systems with an appropriate level of moral concern."}, {"title": "2 Routes to near-term AI welfare", "content": ""}, {"title": "2.1 Introduction", "content": "We will argue that, according to the best information and arguments currently available, there is a realistic possibility that some AI systems will be moral patients in the near future. We first consider the possibility that some AI systems will be conscious in the near future, and we then consider the possibility that some AI systems will be robustly agentic in the near future. Consciousness, robust agency, or both could suffice for moral patienthood and could exist in some near-future Al systems. In our view, while these routes toward near-future AI moral patienthood are far from certain, they are likely enough for AI companies to have a responsibility to start implementing reasonable, proportionate precautionary measures now.\nWe make structurally similar arguments for both routes. Each route depends on a normative claim and a descriptive claim:\nNear-term consciousness: key claims\nThere is a realistic, non-negligible possibility that:\n1. Normative: Consciousness suffices for moral patienthood, and\n2. Descriptive: There are computational features like a global workspace, higher-order representations, or an attention schema that both:\na. Suffice for consciousness, and\nb. Will exist in some near-future AI systems.\nNear-term robust agency: key claims\nThere is a realistic, non-negligible possibility that:\n1. Normative: Robust agency suffices for moral patienthood, and\n2. Descriptive: There are computational features like certain forms of planning, reasoning, or action-selection - that both:\na. Suffice for robust agency, and\nb. Will exist in some near-future AI systems."}, {"title": "2.2 Consciousness in near-future AI", "content": "The consciousness-based case for expecting moral patienthood in near-term AI systems is that there is a realistic, non-negligible possibility that:\n1. Normative: Consciousness suffices for moral patienthood, and\n2. Descriptive: There are computational features like a global workspace, higher-order representations, or an attention schema that both:\na. Suffice for consciousness, and\nb. Will exist in some near-future AI systems.\nWe can now survey why each premise is plausible enough to support a realistic possibility in near-future AI moral patienthood, given the best information and arguments currently available."}, {"title": "2.2.1 Does consciousness suffice for moral patienthood?", "content": "The word \"consciousness\u201d is used in many different ways in ordinary language and in various academic disciplines. In this report, we use \u201cconsciousness\u201d to mean subjective experience what philosophers call \u201cphenomenal consciousness.\u201d One famous way of elucidating \u201cphenomenal consciousness\" is to say that an entity has a conscious experience when there is \"something it is like\" for that entity to be the subject of that experience. There is a subjective \u201cfeel\u201d to your experiences as you read this report: something that it is like to see the words on the screen while, perhaps, listening to music playing through your speakers, feeling the couch underneath you, feeling the laptop or a cat or a dog on top of you.\nThe word \"sentience\u201d is likewise used in many different ways. Some uses of \u201csentience\" are synonymous with some uses of \u201cconsciousness.\" But in this report, we use \u201csentience\" to mean a particular kind of consciousness, namely positively or negatively valenced conscious experiences. Anything that feels good or bad in some way or another counts as positively or negatively valenced in the relevant sense. This can include bodily states like pleasures and pains and emotional states like hope and fear. If you find the report engaging (or the opposite), if you find the music pleasant (or the opposite), and if you find the couch comfortable (or the opposite), then you are experiencing a range of positive or negative states at the same time.\nWhy might sentience suffice for moral patienthood? The idea that sentience is a sufficient condition for moral patienthood is very plausible and widely accepted, because when you can consciously experience positive and negative states like pleasure and pain, that directly matters to you. All else being equal, your life goes better for you when you experience positive states like pleasure and your life goes worse for you when you experience negative states like pain. So there is a clear link between sentience and welfare. There is also a clear link between sentience and moral patienthood, because we have a responsibility not to harm welfare subjects unnecessarily, including and especially by causing them to suffer unnecessarily.\nTo be clear, when we say that sentience suffices for moral patienthood, we are not saying that sentience suffices for the specific kind of moral status that typical adult humans possess. Typical adult humans are both sentient and rational, which means that we have a wide range of moral rights that these capacities jointly unlock (in addition to having moral duties, though this is not our focus here). In contrast, many other animals are plausibly sentient but non-rational, which means that they plausibly lack certain moral rights, such as the right to make their own medical decisions. But for present purposes, what matters is that we at least have some duties to animals, including a duty to avoid causing them to suffer unnecessarily.\nThe idea that consciousness without valence suffices for moral patienthood, while contested, is increasingly defended as well. Some philosophers argue for this view by describing subjects who have consciousness without valence, and by asserting that these subjects plausibly matter for their own sakes. However, some of these thought experiments describe subjects who have consciousness and agency, which makes it hard to tell whether consciousness alone suffices. And while other thought experiments describe subjects who have consciousness without valence or agency (say, a subject who passively experiences color with no pleasure or desire), the idea that this subject matters for their own sake is more controversial.\nIn any case, even if consciousness is insufficient for moral patienthood in theory, it might still be sufficient or at least nearly sufficient in practice, since consciousness and valence might be closely linked. For example, it might be that consciousness necessarily involves valence, in which case the emergence of conscious AI would suffice for the emergence of sentient and morally significant AI. It might also be that even if consciousness without valence is theoretically possible, the step from conscious AI to sentient AI is much easier than the step from non-conscious AI to conscious AI. In that case, the emergence of conscious AI would be (at the very least) a significant step towards morally significant AI, warranting careful scrutiny.\nWhile this section is primarily about the consciousness route towards AI welfare and moral patienthood, we note that consciousness could be relevant for other reasons too; for instance, as noted above, whether or not consciousness suffices for moral patienthood on its own, it might suffice in combination with other capacities like robust agency that is, it might be that when entities can consciously set and pursue their own goals based on their own beliefs and desires, they matter for their own sakes (whether or not they can experience pleasure or pain). If so, then the emergence of conscious AI would increase the probability of morally significant AI for this reason (that is, as part of a consciousness and robust agency route) as well.\nIn the remainder of this section we follow our strategy in Sebo and Long (2023) by examining the route towards conscious AI in general rather than examining the routes towards specific kinds of consciousness, such as valenced consciousness. We make this choice for the sake of simplicity and specificity, given that the science of consciousness is more developed than the science of valence and given that consciousness is plausibly either sufficient for or otherwise closely linked to valence and/or moral patienthood anyway. We will then, in upcoming work, discuss these capacities in more detail and present a research agenda that examines how to identify indicators for both consciousness and sentience in LLMs and other AI systems."}, {"title": "2.2.2 Will some AI systems be conscious in the near future?", "content": "How can we tell whether Al systems are conscious? We can be confident that other (awake, adult) humans are conscious, since each of us knows that we are conscious and that other humans are behaviorally and anatomically similar to us. But we have uncertainty about why we are conscious, that is, about which features of our brains or bodies are responsible for, or associated with, consciousness. We also have uncertainty about which other animals are conscious, because of uncertainty not only about which features are associated with consciousness in humans, but also about how to extrapolate what we know about human consciousness to the nonhuman animal case. We also have significant uncertainty about how many animals' brains work.\nRecently, scientists have attempted to improve our understanding of nonhuman consciousness by searching for what \u201cmarkers\u201d of consciousness in other animals. At a high level this method proceeds as follows: We start by distinguishing between certain kinds of conscious and unconscious processing in humans say, distinguishing pain from nociception by seeing when patients do or do not report consciously feeling pain. We then identify features that correlate with conscious processing say, certain behaviors, brain regions, or patterns of neural processing. We then search for relevantly similar features in nonhumans, and we treat the presence of these features as evidence of conscious processing.\nThis method does not tell us which animals definitely are or are not conscious, and there are a variety of methodological difficulties related to identifying and extrapolating the relevant features. But this method has still allowed researchers and policymakers to make more informed estimates and decisions about animal welfare despite ongoing disagreement and uncertainty about animal consciousness. Moving forward, the same can be true for AI. Later on, we discuss how to tailor this method for AI, but for now, we can emphasize that it will be important to focus less on behavioral evidence (with a limited class of exceptions, which we discuss below) and more on internal evidence, like architectural and computational features.\nWhich kinds of architectural and computational features might indicate consciousness in AI? We can look to neuroscientific theories of consciousness for guidance. These theories use a variety of empirical methods to uncover which states and processes are associated with consciousness. While these theories tend to be framed around brain or neural states and processes (given our typical focus on humans and other animals), the ones we focus on tend to specify these states and processes in terms of the computations that they perform or the functional roles that they play. While these theories remain contested and incomplete, they can still shed light on what kinds of states and processes might be associated with consciousness in AI.\nOf course, even if we identify a variety of architectural and computational markers of consciousness in Al systems, we must still ask whether these markers suffice for consciousness. Computational functionalism is the hypothesis that some class of computations suffices for consciousness. If this hypothesis is correct, then the question is which computations suffice for consciousness and when, if ever, these computations will exist in AI. If this hypothesis is incorrect, then AI con-sciousness will remain at best a theoretical possibility until we move beyond current architectures. For example, if more biology-like functions are required for consciousness, then AI consciousness may require novel hardware that can perform those functions.\nAs we discuss below, our view is that computational functionalism is neither clearly correct nor clearly incorrect at this stage. We might lean one way or the other, but given the importance and difficulty of consciousness as a research topic, we should leave room for doubt. That means that AI consciousness assessments will need to be probabilistic rather than all or nothing at present and, plausibly, for the foreseeable future. To use a simple example, if we estimate that there is a 30-50% chance that computational functionalism is correct and a 30-50% chance that an AI system is conscious if so, then it follows that there is a 9\u201325% chance that this AI system is conscious. That would be good to know when interacting with this AI system!\nIn this precautionary spirit, some of the authors of this report (Patrick Butlin, Robert Long, and Jonathan Birch) released a paper in 2023 exploring the implications of several prominent scientific theories of consciousness viewed through the lens of computational functionalism for AI consciousness."}, {"title": "2.3 Robust agency in near-future AI", "content": "The robust agency-based case for expecting moral patienthood in near-term AI systems is that there is a realistic, non-negligible possibility that:\n1. Normative: Robust agency suffices for moral patienthood, and\n2. Descriptive: There are computational features like certain forms of planning, reasoning, or self-awareness that both:\n\u2022 Suffice for robust agency, and\n\u2022 Will exist in some near-future AI systems.\nWe will present arguments for each of these premises in turn."}, {"title": "2.3.1 Does robust agency suffice for moral patienthood?", "content": "The word \"agency\" is used in many different ways as well. In a broad sense, one might use \"agent\" to mean any entity that senses the environment and responds, which would include thermostats, or any entity that learns and pursues goals, which would include very simple RL agents that play Tic-Tac-Toe. Some might take even these basic ways of being an agent to be sufficient for moral patienthood and, to the extent that you accept this view, you likely already endorse moral consideration for AI systems without needing to read further. While this view merits consideration (and we consider it further in upcoming work), we set it aside for now. We will be arguing that AI systems could be agents in a more demanding sense.\nSpecifically, \"robust agency\" is the ability to pursue goals via some particular set of cognitive states and processes. Which ones? There are several \u201clevels\" of agency that extend beyond the mere ability to learn and pursue goals, and that could plausibly suffice for moral patienthood even when consciousness is absent. For present purposes, we highlight three such levels:\n1. Intentional agency: This is the capacity to set and pursue goals via beliefs, desires, and intentions. Roughly, if you have mental states that represent what is, ought to be, and what to do, and if these mental states work together in the right kind of way to convert perceptual inputs to behavioral outputs, then you count as an intentional agent.\n2. Reflective agency: This is intentional agency plus the ability to reflectively endorse your own beliefs, desires, and intentions. Roughly, if you can form beliefs, desires, and intentions about your own beliefs, desires, and intentions, accepting or rejecting your own attitudes and behaviors at a higher level, then you count as a reflective agent.\n3. Rational agency: This is reflective agency plus the ability to rationally assess your own beliefs, desires, and intentions. Roughly, if you can consider whether particular beliefs, desires, intentions, actions are justified and adopt principles that you can treat as rules of conduct, then you count as a rational agent.\nThere might also be different ways of realizing each capacity, with different kinds and degrees of cognitive sophistication. For example, typical adult humans have the capacity for propositional thought, which means that our thoughts can have a structure that allows for truth values and logical relations. This capacity unlocks powerful forms of intentional agency, reflective agency, and rational agency, since it allows us to develop a wide range of novel beliefs, desires, and intentions and then use evidence and reason to assess their accuracy and coherence. In contrast, while nonhuman animals appear to lack this capacity, many animals at least have a limited capacity for symbolic thought, metacognition, and planning and problem solving.\nWhy might these kinds of agency suffice for moral patienthood? First, intentional agents are potentially welfare subjects. It seems plausible that when you have desires, your life goes better for you when your desires are satisfied and worse for you when your desires are frustrated. Moreover, the satisfaction or frustration of desires can benefit or harm you whether or not you consciously experience them. On some views, this is why we can be posthumously harmed, for example. And while some people think that desire-satisfaction and desire-frustration matter only for conscious beings, others think that they matter for non-conscious beings as well, and so animals with desires deserve moral consideration whether or not they can consciously experience pain, for instance.\nReflective agency, particularly in its propositional form, then adds the ability to have desires about our own desires, which is at the root of some conceptions of free will, the self, and personal identity. Specifically, our desires become \u201cours\u201d in a new sense when we endorse them through reflection. Reflective agency also allows for new kinds of morally significant interests and relationships. When you can have mental states about other mental states, you can have at least a limited conception of how you and others think or feel. For example, reflective agents can have preferences about how they relate to each other, and all else being equal, their lives are better for them when those preferences are satisfied and worse for them otherwise.\nRational agency, particularly in its propositional form, then adds the ability to create social contracts with other rational agents (assuming that they can communicate as well), which are at the root of some conceptions of moral, legal, and political rights and responsibilities. Rational agency also allows for decisions based on judgments about reasons and principles, and this ability not only allows for new kinds of interests, but also on some views commands a kind of respect that extends beyond compassion. Indeed, this idea of respect for rational agents is at the root of the Kantian ethical theory, which rests alongside the utilitarian idea of beneficence for sentient beings as one of the two most influential ethical theories in the modern era.\nIn what follows, we discuss the route towards robust agency in general rather than the routes towards intentional, reflective, and rational agency in particular. We distinguish these levels of robust agency here to emphasize that there can be different kinds of robust agency with different kinds of moral significance, both within and across these levels. That means that when we search for robust agency in nonhumans, including animals and AI systems, it would be a mistake to anchor too much on human agency. But having now made this point, we focus on showing that AI development is currently on a path to create many computations that we associate with all of these levels of robust agency."}, {"title": "2.3.2 Will some AI systems be robustly agentic in the near future?", "content": "There are plausible routes by which we might soon have AI systems with robust agency. What it takes to be an intentional", "efforts": "Both major tech companies and startups are investing heavily in creating more agentic AI systems", "textbook": "n[RL", "domains": "n\u2022 ReAct (Yao et al.", "well)": "n\u2022 Flexible goal-setting and planning: Rather than being constrained to predefined reward functions", "reasoning": "By leveraging LLMs' broad knowledge and reasoning capabilities, language agents can navigate novel contexts, drawing from relevant insights in other contexts to inform their decisions."}]}