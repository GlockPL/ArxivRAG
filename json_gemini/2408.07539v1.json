{"title": "Cross-aware Early Fusion with Stage-divided Vision and Language Transformer Encoders for Referring Image Segmentation", "authors": ["Yubin Cho", "Hyunwoo Yu", "Suk-Ju Kang"], "abstract": "Referring segmentation aims to segment a target object related to a natural language expression. Key challenges of this task are understanding the meaning of complex and ambiguous language expressions and determining the relevant regions in the image with multiple objects by referring to the expression. Recent models have focused on the early fusion with the language features at the intermediate stage of the vision encoder, but these approaches have a limitation that the language features cannot refer to the visual information. To address this issue, this paper proposes a novel architecture, Cross-aware early fusion with stage-divided Vision and Language Transformer encoders (CrossVLT), which allows both language and vision encoders to perform the early fusion for improving the ability of the cross-modal context modeling. Unlike previous methods, our method enables the vision and language features to refer to each other's information at each stage to mutually enhance the robustness of both encoders. Furthermore, unlike the conventional scheme that relies solely on the high-level features for the cross-modal alignment, we introduce a feature-based alignment scheme that enables the low-level to high-level features of the vision and language encoders to engage in the cross-modal alignment. By aligning the intermediate cross-modal features in all encoder stages, this scheme leads to effective cross-modal fusion. In this way, the proposed approach is simple but effective for referring image segmentation, and it outperforms the previous state-of-the-art methods on three public benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Referring image segmentation [5]-[12] is one of the most fundamental and challenging vision-language tasks to highlight regions corresponding to the language description for the properties of the target object. It can be applied in var-ious applications such as human-robot interaction and image editing. Unlike the conventional single-modal segmentation (e.g., instance or semantic segmentation [13]\u2013[18]) based on fixed category conditions, referring image segmentation aims to determine the region according to various linguistic expressions that are not predetermined. In this task, the images with dense objects have complicated relationships between the target object and other objects. In addition, linguistic expressions are ambiguous because the linguistic meaning can be interpreted in various ways depending on the per-spectives. Thus, the context of the image should be taken into account to interpret the linguistic expression with the appropriate meaning. To resolve these problems, we address two main challenges in the referring image segmentation task. One is regarding the fusion that refers to the cross-modal information from each modality features. The other challenge is the alignment that embeds the vision and language features into the same space to improve the fusion performance [19].\nThe first challenging point, cross-modal fusion, makes it possible to refer to the mutual information between an image and a language expression. Several fusion approaches have been proposed for this task. The fusion approaches can be cate-gorized into late fusion and early fusion. As shown in Fig. 1(a), most typical late fusion approach [1], [2], [20] leverages the decoder architectures that fuse the final features extracted from the vision and language encoders. This approach, which depends only on the decoder architectures for the cross-modal fusion, cannot effectively utilize the rich encoder information. Recently, the early fusion approach [3], [4], [21] performs the fusion using the vision features as a query and the language features as a key-value in the intermediate stages of the vision encoder. We call this early fusion approach as vision-only early fusion. As illustrated in Fig. 1 (b-1), previous early fusion methods [3], [21] perform the vision-only early fusion by passing the final language features to each vision stage. Another vision-only early fusion method [4] provides the intermediate language encoder features to each vision stage, as described in Fig. 1 (b-2). By achieving superior improvements, these early fusion methods demonstrate that the vision-only early fusion approach is more effective than the late fusion approach for the cross-modal interactions. However, due to unidirectional early fusion, the language encoder cannot uti-lize the visual information. Therefore, the vision-only early fusion approach [3], [4] has a limitation in interpreting the unrestricted language expressions into the appropriate meaning that considers the context of the image in this task.\nThe second challenging point, alignment, is to embed the cross-modal features of each encoder into the same embed-ding space for effective vision-language feature fusion. To perform cross-modal alignment, vision-language pretraining tasks [22]-[24] use the text-to-image contrastive learning that embeds positive pairs closely and negative pairs distantly. Spe-cially, [24] demonstrates that aligning the cross-modal features before performing the fusion is beneficial for the fusion. To apply the contrastive learning to the pixel-level prediction task, the text-to-pixel contrastive learning [2] based on the conventional text-to-image method is employed to promote the relation between two modalities at the pixel-level. This scheme has improved the performance of the vision-language alignment in this task. However, these approaches use only the final features of the vision and language encoders or the output logits to align the cross-modal features. Therefore, the intermediate features of the vision and language encoders, which have rich information where low-level features contain the structural (or syntactic) information and high-level features contain the semantic global information [25], [26], cannot be sufficiently aligned to the joint embedding space.\nThis paper proposes a novel architecture for referring image segmentation, Cross-aware early fusion with stage-divided Vi-sion and Language Transformer encoders (CrossVLT), which mutually enhances the robustness of each encoder by extend-ing the early fusion approach to a language encoder as well as a vision encoder. Typically, the vision encoder consists of the multiple stages that are separated by the resolution of the feature map; such a structure, which is divided into multiple stages based on specific criteria (e.g., pooling the feature resolution), is called a stage-divided structure in this paper. Different from the vision encoders, the language encoders are not commonly designed with the stage-divided structure. To enable to bidirectionally exchange the cross-modal information of two encoders at each stage, our CrossVLT is designed with the stage-divided language and vision encoders as shown in Fig. 1 (c). Unlike previous methods [3], [4], our language en-coder considering the visual contexts provides more contextual language features to the vision stages. Our vision and language encoders can exchange richer information at each stage and jointly improve their ability to understand the context of im-ages and expressions by considering each other's perspectives. Therefore, our method advances the ability of the cross-modal context modeling to deal with the complicated images and the ambiguous language expressions. In addition, unlike the conventional scheme that relies solely on the final features for the cross-modal alignment, we introduce a feature-based alignment scheme where the low-level to high-level features participate in the cross-modal alignment. Prior to performing the fusion, our scheme can sufficiently align the intermediate cross-modal features to the joint embedding space, leading to a more effective fusion at each stage. We also employ the alignment loss to circumvent the conflict with a task loss. Our scheme better allows the vision and language encoders to learn the cross-modal correlation by improving the ability of aligning the cross-modal features.\nWe demonstrate the effectiveness of the proposed method by achieving the competitive performance on three public datasets for referring image segmentation. In addition, we empirically divide the stages of the language encoder based on the optimal position where the cross-modal alignment and cross-aware fusion with the vision features of the corresponding vision stage can be effectively performed. Our contributions are summarized as follows.\n1) We propose a novel network for referring image seg-mentation, CrossVLT, which leverages the stage-divided vision and language transformer encoders to perform the cross-aware early fusion and mutually enhances the robustness of each encoder.\n2) We introduce a feature alignment scheme that enables the low-level to high-level features of the vision and language encoders to engage in the cross-modal alignment to improve the ability of aligning the inter-mediate cross-modal features. Our scheme leads to a more effective cross-modal fusion at each stage by being applied prior to performing the fusion.\n3) Our CrossVLT is simple yet effective, and outperforms the previous state-of-the-art methods on three widely used datasets for referring image segmentation."}, {"title": "II. RELATED WORKS", "content": "Different from the conventional segmentation task [13]-[18] based on fixed categories, referring image segmentation aims to find the target object according to the unrestricted language expressions. In the referring image segmentation task, various methods [5], [7], [10], [27]\u2013[29] have been introduced to fuse vision and language features. Hu et al. [5] and RRN [27] concatenated language features encoded by LSTM [30] and vision features encoded by CNNs [31] to generate the fused features. CMSA [7] also concatenated each feature extracted from vision and language encoders, then used the self-attention to captures the long-range dependencies between visual and linguistic features. CMPC [28] adopted the bilinear fusion to associate spatial regions with correlated linguistic features of the entity and attribute words. To improve the multi-modal interaction towards more important words, Ye et al. [10] proposed a multi-modal feature encoder that fuses the visual and linguistic features by using a dual convolution LSTM framework. Liu et al. [29] used the element-wise multiplication for the fusion.\nRecent methods [1], [2], [20], [32] have used a cross-modal fusion module after extracting features from the uni-modal encoders. VLT [1] adopted a transformer encoder-decoder structure that extracts vision features in the encoder and fuses vision and language features in the transformer decoder. SeqTR [32] also utilized the transformer encoder-decoder structure after the feature extraction and fusion. Re-STR [20] used transformer-based feature extractor and the visual-linguistic transformer encoder. CRIS [2] extracted the encoder features from the image encoder and language encoder of CLIP [22] and fused them in a vision-language decoder.\nFor better cross-modal fusion, recent methods [3], [4], [21] performed the early fusion in the vision encoder stages instead of fusing after feature extraction. EFN [21] conducted the cross-attention using linguistic features as a key-value in the intermediate stages of the vision encoder. LAVT [3] also performed the language-aware visual attention using the final language features of BERT [33] as a key-value on all stages of Swin [34]. PLV [4] provided the intermediate language features to the vision stages for the language-aware visual attention. These studies have demonstrated the effectiveness of the vision-only early fusion, but the enhancement of the language encoder was not considered. Unlike previous methods, we propose the cross-aware early fusion with stage-divided vision and language transformer encoders. In our method, both vision and language encoders jointly perform the early fusion to better understand expressions for accurate segmentation."}, {"title": "B. Alignment for Vision-Language Task", "content": "Contrastive learning, which exploits positive pairs closely and negative pairs distantly, has been effectively used in vision-language tasks for alignment. In vision-language pre-training tasks, CLIP [22] and ALIGN [23] applied the vision-language contrastive loss for cross-modal matching using massive web data that consists of image-text pairs. In addition, ALBEF [24] proposed the align-before fusion framework that applies the image-text contrastive loss to the cross-modal fea-tures of the final encoder layers before fusing them. The align-before fusion framework demonstrated that the cross-modal features alignment is enhanced by considering the intermediate vision-language features. In referring image segmentation, CRIS [2] performed text-to-pixel contrastive learning using the knowledge distillation from CLIP [22] that applies the text-to-image contrastive learning.\nDifferent from previous methods, we propose the feature-based alignment using contrastive loss based on the features of the intermediate stages in each encoder. It leverages the text-to-pixel contrastive learning and the align-before fusion. However, to the best of our knowledge, this is the first attempt that considers the features of the intermediate stages for cross-modal alignment in referring image segmentation. We also explored the appropriate design of the loss that can be applied to the intermediate layers for better alignment without conflicting with the task loss."}, {"title": "III. METHODOLOGY", "content": "As illustrated in Fig. 2, we introduce CrossVLT, which is designed to mutually enhance the robustness of vision and language encoders and improve the ability of cross-modal feature alignments. First, from low-level to high-level stages, each stage of the two encoders extracts vision and language features, and these features are effectively mapped to the cross-modal embedding space by performing feature-based alignment in the intermediate layers of the two encoders. Then, the cross-aware early fusion is performed to enrich the contextual information of each modality. Finally, the simple segmentation decoder utilizes vision encoder features from each fusion block to generate a output mask for a target object."}, {"title": "A. Stage-divided Vision and Language Encoders", "content": "CrossVLT is designed with a stage-divided language en-coder as well as a stage-divided vision encoder to mutually enhance the robustness of both encoders by jointly performing early fusion. Given an image-text pair, the vision stage takes an image as an input and the language stage takes a language ex-pression as an input. Each stage is indexed as \\(i = 1,2,\u2026\u2026, n\\).\nStage-divided vision encoder. For the dense prediction task, we adopt a Swin transformer [34] organized into four stages to capture long-range visual dependencies and extract hierarchical features defined as \\(V_i \\in \\mathbb{R}^{H_iW_i \\times C_i}\\). We append a vision query fusion layer as the last layer of each vision stage, which consists of the multi-head cross-attention and feed-forward block for fusion with language features. Note that \\(H_i\\), \\(W_i\\), and \\(C_i\\) denote the height, width, and channel dimension of the feature maps at the \\(i\\)th vision stage.\nStage-divided language encoder. Unlike typical language encoders [33], [35] without dividing stages, we newly de-sign a stage-divided language encoder based on the BERT-base model [33], which is a representative transformer-based language encoder, to jointly perform cross-aware early fusion with the vision encoder. The stage-divided language encoder is divided into four stages to correspond to each stage of the vision encoder. Unlike the vision encoder, where stages are distinctly divided according to resolution, the language encoder has no standard to divide stages clearly. Empirically, the number of layers in each stage is set to [6,2,2,2], which can sufficiently extract the linguistic information at low-level. For more details, please refer to Table IV. In the 2nd to 4th language stages, we replace self-attention mechanism of the first encoder layer with the language query cross-attention mechanism for fusion with vision features. Each language stage extracts the language features \\(L_i \\in \\mathbb{R}^{T\\times D}\\), where \\(T\\) and \\(D\\) denote the length of the expression and the feature dimension. The first token of the language features is a special [CLS] token that is encoded to include representative infor-mation of all language tokens for understanding at sentence-level. The [CLS] token of \\(L_i\\) is defined as \\(\\text{CLS}_i \\in \\mathbb{R}^{1\\times D}\\)."}, {"title": "B. Cross-aware Early Fusion", "content": "The cross-aware fusion block fuses the vision and language features by traversing through each stage of both encoders al-ternately to extract robust features considering the perspective of the other modality. The contextual information of the vision features is enriched by referring to the linguistic information relevant to each pixel of the vision feature maps. As presented in Fig. 3, the vision query fusion layer of the \\(i\\)th vision stage takes the language features \\(L_i\\) and the vision features \\(V_i\\) as inputs. The fusion layer performs the multi-head cross attention using vision features as queries and language features as key-value pairs. Then, the feed-forward block extracts the language-aware vision features \\(M_i \\in \\mathbb{R}^{H_iW_i \\times C_i}\\). Specifically, the fusion process of the vision stage is described as follows:\n\\[\\begin{aligned}\n\\text{MHCA}(Q, K, V) &= \\text{Softmax}\\left(\\frac{Q\\cdot K^T}{\\sqrt{d_k}}\\right)\\cdot V,  \\\\ M_i &= \\text{MHCA}(V_i, L_i) + V_i, \\\\ M_i &= \\text{FFN}(M_i) + V_i,\nD_i &= \\text{Down}(M_i), \\\\ F^v_i &= \\text{MHCA}(D_i, L_i) + D_i,  \\\\ F^v_i &= \\text{FFN}(F^v_i) + D_i,  \n\\end{aligned}\\]\nwhere \\(Q\\), \\(K\\), \\(V\\) and \\(d_k\\) denote queries, keys, values and dimensions of keys. MHCA(\\(\\cdot\\)) and FFN(\\(\\cdot\\)) indicate the multi-head cross attention and the feed-forward block. Down(\\(\\cdot\\)) denotes downsampling. The intermediate vision features \\(M_i \\in \\mathbb{R}^{H_iW_i \\times C_i}\\) are used for the segmentation decoder and the language-aware vision features \\(M_i\\) are downsam-pled except for the final stage. The downsampled vision features evolve into the language-aware vision features \\(F^v_i \\in \\mathbb{R}^{H_{i+1}W_{i+1} \\times C_{i+1}}\\) through the multi-head cross attention and feed-forward. Then, \\(F^v_i\\) is passed to the next vision stage and the language query fusion layer of the next language stage.\nAfterward, the language query fusion layer of the \\((i + 1)\\)th language stage takes the vision features \\(F^v_i\\) and language features \\(L_i\\) as inputs. The multi-head cross attention is per-formed in this fusion layer, which uses the language features as queries and vision features as key-value pairs to understand the linguistic meanings from visual perspectives. Then, the vision-aware language features \\(F^l_i \\in \\mathbb{R}^{T\\times D}\\) are extracted by the feed-forward block and residual connections. The fusion process of the language stage is described as follows:\n\\[F^l_i = \\text{MHCA}(L_i, F^v_i) + L_i, \\quad F^l_i = \\text{FFN}(F^l_i) + L_i, \\\\]"}, {"title": "C. Feature-based Alignment", "content": "In most previous methods [2], [22], [24], the final features of the vision and language encoders are solely responsible for the cross-modal feature alignment as illustrated in Fig. 4. Unlike these methods, our feature-based alignment scheme engages the low-level to high-level features in the cross-modal alignment to more effectively embed the intermediate features of the vision and language encoders into the cross-modal embedding space. We now explain the position where the features are aligned for the effective cross-modal fusion and describe how to compute the alignment loss.\nFeature alignment position. The position of the alignment is in front of the cross-aware fusion block at each stage. That is, the vision features \\(V_i\\) and language features \\(L_i\\) are adopted to conduct the feature-based alignment. This posi-tioning makes it better for the vision and language encoders to learn the cross-modal correlation by grounding the vision and language features used for the fusion.\nFeature alignment loss. We use the text-to-pixel contrastive loss as an alignment loss for this pixel-level prediction task. As illustrated in Fig. 5, the vision features \\(V_i\\) and [CLS] token of language features \\(\\text{CLS}_i\\) are used to compute the text-to-pixel contrastive loss \\(\\mathcal{L}_{align}\\). Vision and language linear projections transform \\(V_i\\) and \\(\\text{CLS}_i\\) into the same feature dimension \\(D' , respectively. The transformed vision features \\(v_i \\in \\mathbb{R}^{H_iW_i\\times D'}\\) and the transformed language features \\(z_i \\in \\mathbb{R}^{1\\times D'}\\) are used to obtain a similarity map. The feature alignment loss is calculated as follows:\n\\[\\mathcal{L}_{align} = \\frac{1}{I}\\sum_{i=1}^{I} \\sum_{j \\in Z} \\mathcal{L}_{align}^{i,j},\\]\nwhere\n\\[\\mathcal{L}_{align}^{i,j} = \\begin{cases} -\\log(\\sigma(\\text{Sim}(z, v_i) / \\tau_i)) & j \\in Z^+ \\\\ -\\log(1 - \\sigma(\\text{Sim}(z, v_i) / \\tau_i)) & j \\in Z^-, \\end{cases}\\]\nwhere \\(Z\\), \\(Z^+\\) and \\(Z^-\\) denote the set of pixels, relevant pixels (positive) and irrelevant pixels (negative) for language expres-sion, Sim is a cosine similarity, \\(\\tau\\) is learnable temperature parameters, and \\(\\sigma\\) is a sigmoid function. We leveraged a sigmoid function with learnable parameters for loss calculation to circumvent the conflict with a task loss. \\(\\mathcal{L}_{align}\\) optimizes the networks so that relevant pixels are embedded close to the language features and irrelevant pixels are embedded far apart."}, {"title": "D. Segmentation Decoder", "content": "The segmentation decoder is designed with a simple struc-ture, where three decoder blocks are stacked to verify the effectiveness of our encoders. The decoder block consists of two layers stacked using 3\u00d73 convolution, batch normalization and a ReLU function. The decoder features are upsampled using the bilinear interpolation, and fed into the next decoder block after concatenating them with the vision features \\(M_i\\) extracted by vision query cross attention module of the \\(i\\)th vision stage. The final prediction mask is projected into a binary class mask by conducting a 1 \u00d7 1 convolution. We use the binary cross-entropy loss \\(\\mathcal{L}_{task}\\) for network training. Thus, the final loss function is as follows:\n\\[\\mathcal{L}_{task} = -\\frac{1}{N} \\sum_{j=1}^{N} \\left[ y_j \\log(\\hat{p}_j) + (1-y_j) \\log(1-\\hat{p}_j) \\right],\\]\n\\[\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\lambda \\cdot \\mathcal{L}_{align},\\]\nwhere \\(N\\), \\(y_j\\) and \\(\\hat{p}_j\\) denote the total number of the pixels, the truth value and the predicted probability for the \\(j\\)th pixel."}, {"title": "IV. EXPERIMENTS", "content": "RefCOCO & RefCOCO+. RefCOCO [42] and RefCOCO+ [42] are widely used datasets for referring segmentation and were collected from MSCOCO dataset. RefCOCO contains 19,994 images with 142,209 language expressions for 50,000 objects, and RefCOCO+ contains 19,992 images with 141,564 expressions for 49,856 objects. Each expression in RefCOCO and RefCOCO+ contains 3.5 words on average and each image contains 3.9 objects of the same category on average. Expressions in RefCOCO+ do not contain words about abso-lute locations, and thus, it is more challenging than RefCOCO.\nG-Ref. G-Ref [43] is also widely used for referring seg-mentation, which was collected from Amazon Mechanical Turk. G-Ref contains 26,711 images with 104,560 expressions for 54,822 objects. Compared to RefCOCO and RefCOCO+, G-Ref has more complex expressions containing 8.4 words on average, and thus, it is a more challenging dataset than RefCOCO and RefCOCO+."}, {"title": "B. Implementation Details", "content": "Experimental settings. Our method was implemented in PyTorch [44]. The vision encoder was initialized with Swin-B [34] pretrained on ImageNet, and the language encoder was initialized using the official pretrained weights of BERT-base [33] (uncased version). The segmentation decoder was randomly initialized. We trained the model for 40 epochs with a batch size of 16 on RTX 3090 GPU. We used the AdamW optimizer with the learning rate of 3e-4 and adopted the polynomial learning rate decay scheduler. Input images were resized to 480 x 480 pixel resolution. The maximum sentence length was set to 21 words including the [CLS] token for three datasets. During the inference, post-processing operations were not applied.\nEvaluation metrics. Following previous studies, we used the overall intersection-over-union (oIoU), the mean intersection-over-union (mIoU), and precision at 0.5, 0.7, and 0.9 thresholds. The oIoU is the ratio between the total intersection regions and total union regions of all test sets. The mIoU is the average value of IoUs between the prediction mask and the ground truth of all test sets. The precision is the percentage of test sets that have an IoU score higher than the threshold."}, {"title": "C. Comparison with the State-of-the-Art", "content": "In Table I, we evaluated our CrossVLT with previous state-of-the-art methods on three widely used datasets for referring segmentation using the oIoU metric. Our method surpassed other previous methods on all evaluation splits of all datasets. Compared to the state-of-the-art LAVT on the RefCOCO, CrossVLT had improved performance by 0.71%, 0.34%, and 1.36% on each split, respectively. Further, our model outper-formed other state-of-the-art methods on the more challenging RefCOCO+. In addition, on the G-Ref that contains the most challenging data pairs, our CrossVLT achieved performance improvements by 1.44% and 1.66% on the validation and test sets, respectively. These improvements indicate that CrossVLT effectively aligns and fuses the cross-modal features, and better understand the complex linguistic expressions and images with dense objects than the previous methods."}, {"title": "D. Ablation Study", "content": "To verify the effectiveness of the main components in our method, we conducted extensive experiments as follows.\nCross-aware early fusion and Feature-based alignment. We analyze the effectiveness of the cross-aware early fusion and the feature-based alignment. In this experiment, the base-line model performs the cross-modal fusion only at the last encoder stage. As displayed in Table II, our cross-aware early fusion improves the baseline by 2.96% and 2.4% in mIoU and oIoU scores, respectively. In addition, our alignment scheme improves the baseline by 1.73% and 1.29% in mIoU and oIoU scores, respectively. Modeling the cross-aware early fusion and feature-based alignment together shows the best performance. These results demonstrate that the cross-aware early fusion and feature-based alignment are effective for modeling the cross-modal context aggregation in referring image segmentation.\nIn Fig. 6, we visualized t-SNE results of our CrossVLT com-pared to the late fusion approach. The t-SNE results exhibit the distribution of relevant pixel tokens and irrelevant pixel tokens with language [CLS] token in cross-modal embedding space. In t-SNE of our CrossVLT, the relevant pixel tokens are embedded closer to the language token than in the late fusion results by learning the cross-modal interactions. Therefore, this result visually demonstrates the effectiveness of our approach.\nIn Table III (a), we experimented with applying the cross-aware early fusion to each encoder stage incrementally. Apply-ing the cross-aware early fusion on all stage led to significant improvements of 2.96% and 2.4% in mIoU and oIoU scores. These results indicate that cross-aware early fusion with the stage-divided both encoders enables the exchange of abundant information between the vision and language encoders. In Table III (b), we experimented with applying the feature-based alignment to each encoder stage incrementally. Applying the feature-based alignment on all stages also improved perfor-mance by 0.79% and 0.48% in mIoU and oIoU scores. These results indicate that our scheme helps to align the intermediate cross-modal features for effective fusion.\nImportance of applying the alignment and fusion to-gether in every stage. In Table III (c), we evaluated the effectiveness of applying the alignment and fusion together in every stage. The performance improved as the number of stages to which alignment and fusion are applied increased. That is, the best performance occurred when they are applied at all stages. Compared with the setting of [4], the mIoU and oIoU scores at the setting of [1, 2, 3, 4] increased by 2.23% and 1.86%, respectively. We take this as evidence that applying the fusion and alignment together at every stage is effective by considering the low-level to high-level information of the cross-modalities.\nImportance of the cross-aware fusion. In Table III (d), all models performed the early fusion to exclude the effectiveness of the early fusion and to purely verify the effectiveness of the cross-aware fusion. The 'Uni' fusion (i.e. vision-only early fusion), where only vision stages perform the early fusion with the intermediate features of the language stage, showed the degraded performance compared with the 'Bi' fusion models (i.e. cross-aware early fusion). The results indicate that the cross-aware fusion enables to capture the rich contextual information by considering each other's perspectives.\nSuitability of alignment loss. We experimented on replac-ing our loss with common segmentation auxiliary loss to help uncover the role of alignment loss. In Table III (e), apply-ing alignment loss perform much better, whereas applying common auxiliary loss on all stages rather causes conflict in learning the encoder features and degrades performance. This indicates that the design of our alignment loss is more suitable for applying to intermediate encoder features and beneficial for the cross-modal alignments.\nOptimal settings for the stage-divided language encoder. In the language encoder, determining the criteria for dividing the stages is unclear. We empirically found the optimal hyper parameters for designing an effective stage-divided language encoder. We experimented with the number of layers in each stage by limiting the total number of layers in the language encoder to 12 based on the BERT-base model [33]. As shown in Table IV, the setting [6, 2, 2, 2] performed the best under the same conditions without using pretrained weights. The setting [2, 2, 2, 6] cannot fully extract the low-level linguistic information. The setting [8, 2, 1, 1] is unsuitable for utilizing the high-level semantic information. Therefore, we adopted the optimal setting [6, 2, 2, 2].\nFair comparison using the same backbone. In Table V, we compared the proposed model with the other state-of-the-art models fairly, using the same vision and language encoders on all models. Swin-B [34] and BERT-base [33] were used as the vision and language encoders, respectively. Our CrossVLT surpassed the previous models under the same conditions. This result demonstrates that the performance improvement of CrossVLT was not the effect of the particular backbone and language encoder (i.e., Swin-B [34] and BERT-base [33]).\nComparison with different baseline methods. In Table VI, we applied our method to different state-of-the-art methods (i.e., VLT and CGFormer) that consist of more complicated cross-modal decoder. As shown in Table VI, both models combined with our method showed significant improvements in oIoU performance of 2.71% and 0.88% compared to VLT and CGFormer, respectively. These results indicate that our method can be applied to other methods and leads to additional performance gains for other methods."}, {"title": "Precision-recall analysis.", "content": "In Fig. 7, we analyzed the precision-recall (PR) curves of our model and the ablation models. The area under the PR curve (AUC-PR) summarizes the overall performance of the model across different threshold values. A higher AUC-PR indicates a better-performing model. As shown in Fig. 7, our full model (red solid) had the highest AUC-PR compared to other ablation models. From around 0.7 recall, our full model maintained its advantage in precision over the alignment ablation model (green dashed)."}, {"title": "E. Visualizations", "content": "Visualizing language features. Fig.8 showed that each stage of the language encoder captures different information. The lower stage contains the syntactic information and the higher stage captures long-range relations. Our method ex-ploits this abundant information of the intermediate features for the cross-modal fusion and alignment.\nQualitative results. In Fig. 9, we present qualitative results of our full model and other ablated models (i.e. without cross-aware early fusion, without feature-based alignment and without both components) to demonstrate the effectiveness of each component. As shown in examples of Fig. 9, our full model segmented the target regions more elaborately than the other ablated models. These qualitative results indicate that applying both the feature-based alignment and cross-aware early fusion is an effective approach for referring image segmentation."}, {"title": "The Robustness of CrossVLT.", "content": "As shown in Fig. 12 (a), when previous methods encountered language expressions with typos (e.g. \"seond\" and \"blu\"), they are confused in understanding the meaning of expressions. Additionally, as shown in Fig. 12 (b), informal language expressions (e.g. \"hitta\" and \"pic\") also made it difficult for the network to capture the context of the expressions in previous methods. Given these challenging types of expressions, our CrossVLT, unlike other state-of-the-art models, correctly determined the target regions by referring to the visual perspectives in under-standing the meaning of the expressions. These results indicate that our method enhances the robustness of each encoder and the ability of understanding the contexts."}, {"title": "V. CONCLUSION", "content": "This paper proposed a novel network for referring image segmentation, Cross-aware Early Fusion with Stage-divided Vision and Language Transformer Encoders (CrossVLT), which leverages cross-modal features alternately traversing through each stage of the two transformer encoders to mutually enhance the robustness of each encoder. We also introduced the feature-based alignment scheme that performs contrastive learning using the features of the intermediate levels in each encoder to enhance the capability of aligning the cross-modal features. Experiments demonstrated the effectiveness of our method on three public datasets. We hope our method can motivate further research for various multi-modal tasks and the feature-based alignment scheme can be applied by designing the alignment loss to suit their tasks."}]}