{"title": "N-DRIVERMOTION: DRIVER MOTION LEARNING AND PREDICTION USING AN EVENT-BASED CAMERA AND DIRECTLY TRAINED SPIKING NEURAL NETWORKS", "authors": ["Hyo Jong Chung", "Byungkon Kang", "Yoonseok Yang"], "abstract": "Driver motion recognition is a principal factor in ensuring the safety of driving systems. This paper presents a novel system for learning and predicting driver motions and an event-based high-resolution (1280x720) dataset, N-DriverMotion, newly collected to train on a neuromorphic vision system. The system comprises an event-based camera that generates the first high-resolution driver motion dataset representing spike inputs and efficient spiking neural networks (SNNs) that are effective in training and predicting the driver's gestures. The event dataset consists of 13 driver motion categories classified by direction (front, side), illumination (bright, moderate, dark), and participant. A novel simplified four-layer convolutional spiking neural network (CSNN) that we proposed was directly trained using the high-resolution dataset without any time-consuming preprocessing. This enables efficient adaptation to on-device SNNs for real-time inference on high-resolution event-based streams. Compared with recent gesture recognition systems adopting neural networks for vision processing, the proposed neuromorphic vision system achieves comparable accuracy, 94.04%, in recognizing driver motions with the CSNN architecture. Our proposed CSNN and the dataset can be used to develop safer and more efficient driver monitoring systems for autonomous vehicles or edge devices requiring an efficient neural network architecture.", "sections": [{"title": "1 Introduction", "content": "With the advancement of artificial intelligence (AI), it is being applied in various industrial fields, and among them, vehicle AI systems are emerging as one of the most prominent applications. AI used in vehicles is used as a system that assists autonomous driving or the safety of drivers and pedestrians by integrating it with the control system [2]. In particular, the EU and the United States have introduced mandatory requirements for various driver assistance systems to ensure safe road traffic by regulations on general automotive safety [1]. This has led to an urgent need to research and develop driver assistance systems (ADAS) using AI. In response to these demands, we propose N-DriverMotion, an event-based dataset for neuromorphic learning and predicting driver motions on an efficient convolutional spiking neural network (CSNN). This driver motion recognition research, comprising a neuromorphic framework for efficient CSNN configuration, implements driver safety assistance by incorporating a high-resolution event-based camera. The contributions of the proposed driver motion recognition system include the following:\n\u2022 We create an event-based high-resolution (1280x720) dataset, N-DriverMotion, for large-scale driver motion recognition using an event-based camera: We believe that this paper is the first to study driver motion recognition using a high-resolution event camera and spiking neural networks. The event-based dataset presents 13 driver motion categories classified by direction (front, side), lighting (bright, moderate, dark), and participant.\n\u2022 We present a novel simplified four-layer convolutional spiking neural network (CSNN), directly trained by the high-resolution dataset without any time-consuming preprocessing. This enables efficient adaptation to on-device SNNs for real-time inference over high-resolution event-based camera streams. Furthermore, the proposed neuromorphic vision system achieves comparable accuracy, 94.04%, in recognizing driver motions with the CSNN architecture, developing safer and more efficient driver monitoring systems for autonomous vehicles or edge devices requiring an efficient neural network architecture.\n\u2022 We configured the efficient CSNN for driver motion recognition using one of the widely used practical neuromorphic frameworks, the Intel Lava neuromorphic framework \u00b9 for real-world applications and adoptions with optimization of hyperparameter values.\nThe remainder of this paper is organized as follows. Section 2 briefly reviews related work, while Section 3 describes the proposed driver motion recognition system. Section 4 presents the implementation results and performance evaluations and Section 5 concludes this paper."}, {"title": "2 RELATED WORK", "content": "Event-based cameras have advantages over frame-based cameras on properties such as high dynamic range, high temporal resolution, low latency, and low power consumption because of their sparsity in stream events when intensity changes [28]. Therefore, they are gradually used in machine-learning vision systems for gesture learning and recognition. Hand-gesture recognition [12], gesture and facial expression recognition [8], DECOLLE [7] for deep continuous local learning, EDDD for event-based drowsiness driving detection [14], TORE [6] for time-ordered recent events, event-based asynchronous sparse convolutional networks [5], and EST [4] for end-to-end learning of representations for asynchronous event-based data have been devised on deep learning-based neural networks (DNNs) and event representations, exploiting the sparsity and temporal dispersion of event-based gestures. Riccardo et al. trained a deep neural network with a pre-processed DVS hand gesture dataset for recognizing hand gestures on a Loihi neuromorphic processor [15]. The trained DNN was converted into the spike domain for deployment on Intel Loihi [29] for real-time gesture recognition.\nUnlike DNN-based approaches that require converting event-based datasets to static patterns for processing, recent research has shifted to direct training and learning on SNNs using event-based datasets. This is because events are inherently and naturally suited for SNNs operating in continuous time. Moreover, SNNs directly exploit the temporal information of events for energy-efficient computation. SLAYER [16] has proposed an error backpropagation mechanism for offline training on SNNs. It receives events and leverages the backpropagation method to train synaptic weights and axonal delays directly. Amir et al. proposed an end-to-end event-based gesture recognition system using an event-based camera and a TrueNorth processor configured with a convolutional neural network (CNN) [19]. It recognized hand gestures in real-time from hand gesture event streams captured by a Dynamic Vision Sensor(DVS).\nEvent-based vision systems have been widely used in various applications, including vision systems for autonomous driving [11, 13] and object detection [9, 10]. These applications exploit event-based cameras to capture the spatial and temporal driving and object event data."}, {"title": "3 PROPOSED EVENT-BASED DRIVER MOTION RECOGNITION SYSTEM", "content": "In this section, we introduce efficient CSNN architecture and a driver motion learning and prediction system employing a direct training mechanism and event-based data streams. As event-based data differs from static images in that it includes time to represent events, we adopt a direct spiking training method and 3D spike convolution operation to build an efficient CSNN model.\nFor the direct training of spiking neural networks, we exploited a gradient-based training method developed in SLAYER [16]. It is generally known that the characteristic of discrete spike events hinders the differentiation in SNN. To resolve such problems, various attempts have been made to propose the approximation for the derivative of the spike function [24, 25, 26]. However, none of the following have considered the temporal dispersion between spikes\n${e^{(l)}(t) = \\begin{cases} \\frac{\\partial a^{(n_l)}(t)}{\\partial t} \\quad \\text{if } l = n_l\\\\ (\\frac{\\partial e^{(l)}}{\\partial a^{(l)}}(t)) \\cdot (E\\delta e^{(l+1)})(t) \\quad \\text{otherwise} \\end{cases}}$\n(1)\n$\\delta^{(l)}(t) = p^{(l)}(t) \\cdot (E \\delta e^{(l)})(t)$\n(2)\n$\\nabla_{W^{(l)}}E = -\\int_0^T e^{(l+1)}(t) \\cdot (\\xi \\circledast a^{(l)})(t) dt$\n(3)\n$\\nabla_{d^{(l)}}E = -\\int_0^T \\delta^{(l)}(t) \\cdot (e^{(l)}(t))dt$\n(4)\nwhere $p^{(l)}(t)$ denotes the probability density function in layer $l$ at certain time $t$, $\\Delta\\xi$ as the random perturbation, $W^{(l)}$ is the weight vector for layer $l$, $a^{(l)}$ being the spike response signal, $L(t)$ being the loss at a certain time $t$, $d^{(l)}$ being the axonal delay, $\\xi_d$ being the spike response kernel, and $\\circledast$ being the element-wise correlation operation in time. This provides effective distribution of the errors back through layers of a neural network as in DNNs. It takes account of the errors in the previous timeline, a crucial factor to be considered as spiking neuron's states relied on the previous states.\nIn our model, the event-based video streams are defined as a 3D tensor with the shape of $(u, v, t)$ where $u$ and $v$ denote the coordinate of the width and height of layer $l$, and $t$ stands as the timestamp [3]. By configuring the optimal temporal frequency, users can control $t$ from the original segment.\nA convolutional neural network (CNN) is a feed-forward network composed of multiple layers where filters convolve around the input or single layer for neurons to collect meaningful features or patterns [27]. However, as the event-based video has not only spatial information but also time, the sampled input sequence S(n) would require a 3D convolutional kernel for building a spiking neuronal feature map as shown in Figure 1. Each spike inside the kernel represents a cluster of spike trains $s_{u,v}(t)$ where the spike's coordinate is $(u, v)$, and $t$ is in range of the temporal resolution window $t$. Spikes from the kernel continually get cumulated to the neurons in the feature map, and when the spikes in the region of the kernel exceed a certain threshold, it would generate the membrane potential for a single neuron in the feature map, creating spatio-temporal dynamic patterns."}, {"title": "4 Experiments and Results", "content": "4.1 Event-based camera and data conversion\nThe Prophesee's Metavision EVK4\u00b2 camera is one of the latest event-based vision sensors that supports up to 1280x720 pixel resolution. Generally, when there exists a change in pixel values exceeding a certain threshold defined by the user, the event-based camera detects such changes in brightness asynchronously and generates events specific to that pixel [28]. Each event contains pixel information relative to the position described in the x and y coordinates, indicating a motion's change and a timestamp for the occurrence of the event. The device offers a high dynamic range (86 dB, however, it can reach over 120 dB based on low light cutoff measurement being: 0.08 lux) and a typical and maximum event rate of 1.06 giga-events per second (Geps). To allow direct usage of the spike data earned from the high-resolution event-based camera without requiring additional resources, we have minimized modification of the extractor such that it can retrieve coordinate, polarity, origin (newly implemented feature in EVK4), and timestamp information. Moreover, the most meaningful motion information was contained in the central 720x720 pixels so that the size of the input events was cropped to the 720x720 map size.\n4.2 Datasets\nOver the past years, a profusion of gesture datasets captured with a simple frame-based sensor has been presented. However, to stimulate the improvement of event-based computer vision, Hu et al. [17] strongly support the importance of the DVS dataset. Serrano et al. introduced the first labeled event-based neuromorphic vision sensor dataset which originated from the classical MNIST digit recognition dataset by moving the images along certain directions within the screen [18], with being further developed by Orchard et al. where several frame artifacts were removed with a pan-tilt unit [22]. It could be observed that generating artificial movement to static images enabled the production of event outputs, which became a crucial factor for object recognition in neuromorphic systems such as spiking neural networks [23].\nHowever, it is observed that static image recognition with event-based vision sensors is ineffective as the purpose of the usage is mainly focused on dynamic scenes. Recognizing such drawbacks, some datasets which consist of dynamic scenes have been presented. Berner et al. presented novel datasets that converted the existing visual video benchmarks for object tracking, and action/object recognition into spiking neuromorphic datasets with DAVIS camera's output [20].\nEven though these datasets show dynamic movements in a spiking neuromorphic way, according to Tan et al., a DVS camera produces microsecond temporal resolution output. In contrast, datasets produced by conventional vision tools such as video recorders or color cameras have tens of milliseconds which would cause loss in high temporal frequency during conversion. Moreover, they add that there is a high chance of subsidiary unwanted artifacts being contained during conversion [21].\n${a_{u,v}(t) = S_{u,v}(t) * \\xi_d(t)}$\n(5)\n${U_{j,k}(t) = \\sum_{m=1}^{K}\\sum_{n=1}^{K}W_{m,n}a_{m+(j-1),n+(k-1)}(t) + (S_{j,k}(t) * \\nu(t))}$\n(6)\n${S_{j,k}(t) = 1 \\text{ and } U_{j,k}(t) = 0 \\text{ when } U_{j,k}(t) \\geq V_{thr}}$\n(7)\nwhere * denotes the convolution operator, $W_{m,n}$ denotes the synaptic weights at $(m, n)$ of the kernel, $u_{j,k}$ represent the membrane potential at coordinate $(j, k)$ presented by the feature map, $K$ represents the width and height of the convolution kernel, and $\\nu(t)$ is the refractory kernel.\n4.3 Network configuration and training\nTo enable adequate training, the dataset has been converted where events would be converted to tensors consisting of x coordinate, y coordinate, polarity, and timestamp. Also, we have set the sampling time as 2.0 seconds. With such a form, it can now be conveyed as input to the CSNN during runtime.\nAll of the applications were implemented and mapped to GPU/CPU to be executed as the simulation for Loihi 2, with the Lava version 0.4.4 software framework using the Lava-DL library for training. Our CSNN models were trained with the event-based dataset using SLAYER provided by Lava-DL.\nWe have trained multiple configurations of CNN to compare the accuracy. Each network has been trained for 200 epochs with a single batch size due to the dataset's high resolution. We have selected CUBA LIF (CUrrent BAsed Leaky Integrate and Fire) as the base neuron model for the spiking CNN models. For optimization, the learning rate has been set to 3\u00d710\u207b\u00b3 and used ADAM for the optimizer. For the loss, we have chosen spike rate loss. The network configuration for the 4-layered CSNN and the neuron parameters are shown in Table 2.\n4.4 Results\nThe results from training models are shown in Figure 4 and Table 3. In the benchmark dataset for evaluation of our models, we used the DVS Gesture recognition dataset, comprising 11 hand gesture categories from 29 subjects under 3 illumination effects [19]. We tested three different SNN configurations: 1) having all layers composed of dense"}, {"title": "5 Conclusions", "content": "In this paper, we propose N-DriverMotion for driver motion learning and prediction using a high-resolution event-based camera and directly trained spiking neural networks. As demonstrated by the experimental results above, driver motions captured by the Prophesee event-based camera could be classified with high accuracy (94.04%) using a simplified convolutional spiking neural network with low complexity. From this result, N-DriverMotion can contribute to developing safety systems for driver motion recognition in challenging conditions such as low-light environments or tunnels."}]}