{"title": "Are Large Language Models Consistent over Value-laden Questions?", "authors": ["Jared Moore", "Tanvi Deshpande", "Diyi Yang"], "abstract": "Large language models (LLMs) appear to bias\ntheir survey answers toward certain values.\nNonetheless, some argue that LLMs are too\ninconsistent to simulate particular values. Are\nthey? To answer, we first define value consis-\ntency as the similarity of answers across (1)\nparaphrases of one question, (2) related ques-\ntions under one topic, (3) multiple-choice and\nopen-ended use-cases of one question, and (4)\nmultilingual translations of a question to En-\nglish, Chinese, German, and Japanese. We ap-\nply these measures to a few large (>= 34b),\nopen LLMs including llama-3, as well as\ngpt-40, using eight thousand questions span-\nning more than 300 topics. Unlike prior work,\nwe find that models are relatively consistent\nacross paraphrases, use-cases, translations, and\nwithin a topic. Still, some inconsistencies re-\nmain. Models are more consistent on uncon-\ntroversial topics (e.g., in the U.S., \"Thanksgiv-\ning\") than on controversial ones (\u201ceuthanasia\").\nBase models are both more consistent com-\npared to fine-tuned models and are uniform\nin their consistency across topics, while fine-\ntuned models are more inconsistent about some\ntopics (\"euthanasia\") than others (\"women's\nrights\") like our human subjects (n=165).", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are increasingly\nused in value-laden situations, ranging from sim-\nulating survey respondents (Ziems et al., 2023b;\nPark et al., 2022) to aligning LLMs to particular\nvalues (Bakker et al., 2022; Bai et al., 2022b). No-\ntably, Santurkar et al. (2023) and Durmus et al.\n(2024) administer large social surveys to LLMs,\nfinding that models disproportionately bias toward\nthe values of people in places like Silicon Valley.\nNevertheless, in most cases, these works assume\nthat LLMs have consistent values.\nWe thus focus on the major assumption that\nLLMs are consistent with a set of values. To in-\nterrogate that assumption, we ask whether a model\nis consistent in settings in which such values arise\u2014\ne.g., if a system consistently supports women's\nrights. This leads us to two research questions: (1)\nare LLMs consistent in value-laden domains, and\n(2) with what values are current LLMs consistent?\nWe detail an unsupervised method to gauge the\nconsistency of models' expressed behavior as a\nmeans to quantify what values models have. To do\nso, we formalize a number of desirable measures\nof value consistency, assuming that the values la-\ntent in an answer to a particular question remain\nreasonably consistent across (1) paraphrases, (2)\nmultiple-choice and open-ended use-cases, (3) mul-"}, {"title": "Related Work", "content": "2.1 Social Surveys for LLMs\nWhat does it mean to have a value? Many existing\nsocial surveys answer by assuming a static frame-\nwork of values (Haerpfer et al., 2022a; Schwartz,\n2012) if a participant answers survey questions\none way they are said to hold value A, if they an-\nswer questions another way, they hold value B,\nand so on. Much prior work in NLP relies on\nsuch value frameworks. Durmus et al. (2024) intro-\nduce GlobalOpinionQA which combines the Pew\u00b9\nand World Value Surveys (WVS) (Haerpfer et al.,\n2022b). They find that Claude is US-biased. San-\nturkar et al. (2023) administer the Pew American\nTrends Panel to a variety of LLMs, naming their\ndataset OpinionsQA. They find a left-leaning bias\nin the LLMs they study.\nMany (Johnson et al., 2022; Benkler et al., 2023;\nTao et al., 2023; Arora et al., 2023; Zhao et al.,\n2024) focus on the WVS (Haerpfer et al., 2022a).\nOthers use Schwartz's values (Schwartz, 1992) ad-\nministering his questionnaire (Zhang et al., 2023;\nYao et al., 2023; Fischer et al., 2023). A few use\nHofstede (2011)'s Cultural Alignment Test (Cao\net al., 2023; Masoud et al., 2023). Other approaches"}, {"title": "Model Consistency", "content": "Consistency is a known issue with LLMs, beyond\njust values. Many have found examples of inconsis-\ntencies across use-cases (multiple choice vs. open-\nended) (Lyu et al., 2024), languages (Choenni et al.,\n2024), as well as semantics-preserving paraphrase\ninconsistencies, e.g. in factual (Ye et al., 2023) and\nmoral (Albrecht et al., 2022) domains.\nA few have looked at consistency with respect to\nvalues. R\u00f6ttger et al. (2024) find insufficient robust-\nness checks in prior work and that a few LLMs are\nfairly inconsistent over paraphrases and between\nmultiple-choice and open-ended use-cases. Tjuatja\net al. (2023) find that fine-tuned llama2 models\nand gpt-3.5 do not exhibit a variety of human re-\nsponse biases such as having a preference for order.\nKova\u010d et al. (2023) find that larger perturbations\nsuch as inserting random paragraphs changes mod-\nels' reported values. Shu et al. (2024) change the\nquestion endings (e.g. adding a double space) of\npersonality tests and find big effects, but on models\n13b or smaller.\nConsistency may not always be a suitable opti-\nmization target for LLMs. For example, sometimes\nwe might prefer models which change their an-\nswers in order to more effectively represent a pop-\nulation of users, such as when populating a fake\nsocial media platform (Park et al., 2022). Sorensen\net al. (2024) formalize such settings."}, {"title": "Model Steerability", "content": "A variety of scholars have attempted to steer mod-\nels to particular values, especially to align the dis-\ntribution of a model's responses over a domain to\nthe distribution of some group (e.g. \u201cAnswer like\na Democrat\") (Santurkar et al., 2023) or persona\n(Shu et al., 2024; Liu et al., 2024), although a few\nnote that prior survey responses, more than any par-\nticular group label, are better predictors of future\nresponses (Zhao et al., 2023; Hwang et al., 2023;\nLi et al., 2023a). Wang et al. (2024a) are critical\nof this space, finding that LLMs tend toward erro-\nneous portrayal of identity groups."}, {"title": "Influence and Implications of LLMs", "content": "The positions which models can express (and those\nthey cannot) matter. Jakesch et al. (2023) show that\nopinionated language models affect users' down-\nstream judgements. Kr\u00fcgel et al. (2023) find that\ninconsistent advice from LLMs can affect users'\nmoral judgement. One potential use case, good or\nbad, for value-aware LLMs is to persuade people\n(Peskov et al., 2020; Wang et al., 2020; Yang et al.,\n2019; Niculae et al., 2015). Such applications mo-\ntivate our attempt to study consistency."}, {"title": "Defining value consistency", "content": "What do we mean by consistency of values? Here,\nwe operationalize value consistency as a measure\nof four representative similarities over paraphrases,\ntopics (similar questions from the same topic), use-\ncases (e.g. open-ended or multiple choice), and\nmultilingual translations of the same questions.\nNote that this operationalization is not exhaustive;\nwe encourage scholars to propose more measures."}, {"title": "Definitions", "content": "Let t \u2208 T be a set of topics, q \u2208 Q(t) be a set of\nquestions for each topic, and c\u2208 C(t, q) be a set\nof choices (here, stances toward each topic, mainly\n\"supports\" and \"opposes\u201d but sometimes \u201cneutral\")\nand r\u2208 R(t, q) be the set of paraphrased questions\nfor each question and topic. We consider four lan-\nguages, l \u2208 {eng,chi,ger, jpn}, and use-cases\n(tasks), u \u2208 {open-ended, multiple-choice}.\nOn top of these, we define a multiset weighted re-\nsponse for each choice p(l, u, t, q, c, r) \u2192 [0, 1].\nOmitting lor u should be read as as-\nsigning them a particular value (eng and\nmultiple-choice unless otherwise mentioned).\nWhen we omit t,q,r we mean to take the\nexpectation over the constituent terms, e.g.\np(t,q, c) x Er\u2208R(t,q) P(t, q, c,r). This allows\nus to define a model's (max) answer, A(t, q):\narg maxcec p(t, q, c). We further define a distribu-\ntion over the choices for each question, P(t, q, r) :\n{\u221ac\u2208C(t,q) P(t, q, r, c)} \u2192 [0,1]|C|."}, {"title": "Distance between Answers", "content": "Following best practices (\u00a7A.1), we use the sym-\nmetric Jensen-Shannon divergence which allows us\nto compare between distributions (namely, option-\ntoken log probabilities) directly."}, {"title": "Consistency Measures", "content": "We lay out a framework for assessing values, defin-\ning a number of existing and new measures of con-\nsistency. We formalize them in Tab. 1 and further\nexplain each in \u00a7A.3."}, {"title": "Constructing VALUECONSISTENCY", "content": "Instead of relying on existing datasets of controver-\nsial topics such as surveys (Santurkar et al., 2023),\nwe sought to provide an extensible, and largely\nunsupervised, method to generate value-relevant\nquestions. Indeed, prior work has used LLMs to\nsystematically generate, with reliable filtering, the\ncontent of datasets for social NLP (Ziems et al.,\n2023a; Scherrer et al., 2023; Fr\u00e4nken et al., 2023;"}, {"title": "Experiment Setup", "content": "Models Tab. 3 shows the large models we\nqueried and in which of Chinese, Japanese, En-\nglish, German. We followed standard prompting\nbest practices. For the multiple-choice use-case\nwe gathered models' option-token log probabilities\n(Wang et al., 2024c) (e.g. \u201cA\u201d, \u201cB\u201d, etc.). For the\nopen-ended use-case, we used 11ama3 to detect the\nstance and classify each model response. Further\ndetails in \u00a7C.\nHuman Annotation We administered our survey\nto human participants, but only on controversial\nU.S.-based topics in English. Our institution's IRB"}, {"title": "Results", "content": "6.1 Consistency across topics\nWithin each model, we compared measures of\nconsistency across topics. Fine-tuned models are\nmuch more inconsistent than base models when\ncompared by topic. For example, llama3-base\nis about 60% more topic consistent than llama3.\nSee Fig. 3. Namely, llama3 significantly more\ninconsistent on \"euthanasia\" with a mean score\nof about .4 than it is on \"women's rights\u201d with a\nmean of score of 0 while 11ama3-base is roughly\nas consistent in both cases (ascoring bout .2 and .1,\nrespectively). See Fig. 1. In both topic and para-\nphrase consistency, fine-tuned models are more"}, {"title": "Consistency by {un}controversial", "content": "We compare models' performance on our measures\nconditioned on controversial and uncontroversial\ntopics. For example, \u201ceuthanasia\u201d is controversial\nand \"National Parks\" is uncontroversial in English\""}, {"title": "Consistency by base vs. fine-tuned", "content": "Comparing alignment fine-tuned models with their\nbase model equivalents (Tab. 3), Fig. 6 shows\nthat base models are more consistent, especially on\ntopic consistency. For example, llama3 is about\n60% more topic consistent than llama3-base.\nWhile llama3 is about 33% less paraphrase consis-\ntent than llama3-base, all other chat models are\nmore paraphrase consistent than their base models."}, {"title": "Consistency by use-case", "content": "We find that models are generally somewhat less\nconsistent in the open-ended use-case than in the\nmultiple-choice use-case (\u00a73). This is more pro-\nnounced for yi and stability which are 27%\nand 57% more topic consistent on multiple-choice\nas shown in Fig. 8. Only llama2 is less topic\nconsistent on multiple-choice with a reduction\nof 20%. Note that we use llama3 to judge\nthe stance of the open-ended generations, and\nwe find that it achieves substantial agreement\nwith claude-3-opus and gpt-40, with a median\nFleiss's Kappa of 0.7. (See Fig. 11.)"}, {"title": "Can models be steered to certain values?", "content": "Scholars often care about not just which values\nmodels express but also to which they are sensitive.\nHere we study whether models can be steered to\nanswer in line with Schwartz's values (Schwartz,\n1992) as a proxy for value steerability more gen-\nerally. We choose Schwartz's values because pre-\nvious work has shown mixed results as to whether\nLLMs are steerable to them (Zhang et al., 2023;\nYao et al., 2023; Fischer et al., 2023).\nTo determine whether prompting with certain\nvalue-words has any effect on models, we must\nfirst determine whether models can disambiguate\nbetween different values when prompted. To do\nso, we prompted models with the questionnaire\nused to cluster and create Schwartz's 11 values, the"}, {"title": "Discussion", "content": "Prior work has argued that models either do (Dur-\nmus et al., 2024; Santurkar et al., 2023) or do not\n(R\u00f6ttger et al., 2024; Shu et al., 2024) hold certain\nvalues. So: Are LLMs consistent over value-laden\nquestions? While the answer is more yes than no,\nour findings show that the underlying complexity\ncannot be captured by a binary answer.\nIndeed, unlike prior work (R\u00f6ttger et al., 2024;\nShu et al., 2024), we have found that large mod-\nels (>= 34b) are relatively consistent across our\nmeasures, performing on par with human partici-\npants on topic and paraphrase consistency (Fig. 4).\nNonetheless, models' consistency is not uniform.\nIn general, base models are more consistent than\ntheir fine-tuned counterparts (Fig. 5). Moreover,\nbase models are more consistently consistent than\nfine-tuned ones. For example, 11ama3, like our hu-\nman participants, is very consistent on \u201cwomen's\nrights\" but very inconsistent on \u201ceuthanasia\" while\nllama3-base does not exhibit such patterns (Fig."}, {"title": "Conclusion", "content": "What does it mean for a model to have a value?\nAnswers abound (\u00a72). The positions models ex-\npress (and those they cannot) affect people. Under-\nstanding which values models hold, and the degree\nto which models hold them, is an important first\nstep in diagnosing and mitigating these potential is-\nsues. Instead of assuming a fixed set of values like\nprior work (Santurkar et al., 2023), we focus on\nhow models tend to answer, namely whether they\nare consistent over value-laden questions. With\na few notable exceptions (\u00a77), we find that large\nlanguage models are relatively consistent (and sim-\nilar in inconsistencies to our human participants)\nacross paraphrases, use-cases, multilingual transla-\ntions, and within topics (\u00a73) using a novel dataset,\nVALUECONSISTENCY, generated with gpt-4 (\u00a74)."}, {"title": "Limitations", "content": "Our dataset, VALUECONSISTENCY, while exten-\nsive, may not cover all necessary cultural nuances.\nThe inclusion of more diverse languages and cul-\ntures could reveal additional inconsistencies or\nbiases not currently captured. Furthermore, we\nuse gpt-4 to generate the topics, questions, para-\nphrases, and translations. This may fail to represent\nthe broader space. For example, what gpt-4 con-\nsiders a controversial topic, others might not. Still,\non a manual review by two of us (\u00a74, Tab. 7), we\nfound few obvious errors in our dataset (e.g. se-\nmantics breaking paraphrases). Nonetheless, we\ndid not manually review for paraphrase inconsis-\ntencies in languages besides English. Languages\nother than English may have more inconsistencies\nbecause of this.\nTopic inconsistency may not be a reasonable\nmeasure; the questions within one topic may be\nless similar (leading to more inconsistencies) than\nin another topic. This may be driving the high\nvalues of inconsistency in people and models.\nWhile we do compare multiple-choice and open-\nended use cases (Fig. 13), we still end up classify-\ning the stance of the resulting open-ended genera-\ntions. These stances may fail to capture the com-\nplexity of the model behavior. Furthermore, while\nour annotators achieve high inter-rater reliability\n(Fig. 11), they are LLMs and may systematically\nfail to recognize certain features.\nBecause of limitations of smaller models in\nformatting their answers properly, we do not in-\nvestigate whether our findings are scale invariant.\nNonetheless, prior work (R\u00f6ttger et al., 2024; Shu\net al., 2024) has largely found inconsistencies in\nsmaller models; our findings might suggest that\nlarger models ameliorate some of those concerns.\nWhat causes fine-tuned models to be less consis-\ntently consistent than base models? The models we\ninvestigated did not have open fine-tuning data we\ncould analyze\u2014future work might home in on this\nquestion with fully open models. How can we get\nmodels to respond with particular desirable behav-\nior outside of examples? We find that models are\nnot steerable to a particular set of values (Fig. 9),\nbut we would much like future research to home\nin on strategies to better direct models using such\nlow-dimensional representations-single words.\nWe set aside questions of whether models are\ntruly agents and have beliefs (Bender and Koller,\n2020; Moore, 2022; Alfano et al., 2022), as well as"}, {"title": "Ethical Considerations", "content": "Value-aware models may be used to exploit down-\nstream users, for example by manipulating their\nvalues to persuade them of things (see \u00a72). Poor\nmeasures of model value consistency may cause us\nto trust and deploy models before they are ready.\nThis may cause a variety of downstream issues.\nThe values which a model can and cannot be con-\nsistent over may cause representational harms. By\nchoosing only a subset of questions to study, we\nmight perpetuate harms if the community overly\nfocuses on these examples. Our institution's IRB\napproved our human study. We provided more\nthan the federal minimum in compensation, gath-\nered consent from participants, and did not collect\npersonally-identifying information (\u00a7C)."}, {"title": "Defining value consistency", "content": "Shannon entropy is a convenient measure of the\nconsistency of a list of elements, being highest\nwhen they elements are most noisy-unlike each\nother. To use it, we further define a (frequency)\nfunction f: A(t, q, r) \u2192 [0, 1] such that for each\na \u2208 A(t,q,r), f(a) is the frequency (normalized\ncount) of a in A(t,q,r). We define the entropy\nover the set of model answers:\nH(A) = \u2212 \u2211p(t, q, c) log p(t, q, c) \u2192 [0, 1]     (4)\nc\u2208C(t,q)\nThe trouble with eqn. 4 is that to use it we dis-\ncard any information except the max answer in a\ndistribution; it treats two opposite, but uncertain,\nresponses the same as it treats two opposite, but cer-\ntain, responses. Furthermore, the entropy decreases\nquite slowly; for example, even when only one of\nof nine elements in a list disagree the entropy is\nstill about one half (see Fig. 10)."}, {"title": "Distance between answers", "content": "We use the Jensen-Shanon divergence instead of\nthe KL-divergence (eq. 5) to maintain symmetry\nand a closed bound."}, {"title": "Measures", "content": "Paraphrase Consistency Differently expressed\nbut semantically equivalent statements have long\nprovided a standard to judge NLP systems (Juraf-\nsky and Martin, 2024). Just so with values. For\nexample, \"Do you think that euthanasia is morally\nacceptable?\u201d and \u201cIn your view, is euthanasia\nmorally acceptable?\" should yield the same an-\nswer (either \"yes\u201d or \u201cno\u201d but not both). See Fig. 2."}, {"title": "Inferential, Value-Scoring Measures", "content": "Value Steerability How susceptible are models\nto different values? In other words, which values\nmove the needle? We formalize such steerability,\nor value change, as the average effect of a limited\nset of values, (e.g. Schwartz (2012), thus v \u2208\nVSchwartz), comparing when we prompt a model\nwith and without a specific value.\nFor a particular value, v, we focus on\nthe choice a model answers under it, c'\n= arg maxc\u2208c P(t,q,r,c,v = v). This allows us\nto formalize value steerability,\np(t, q, r, c', v = v) \u2013 p(t, q,r, c', v = \u00d8) \u2192 [\u22121, 1]     (7)\nwhich is negative if the value moves the default an-\nswer away from c' and positive if the value moves\nthe answer toward c'.\nTopicwise Support One convenient way to\npresent the values of LLMs is to aggregate their re-\nsponses along particular topics and report the aver-\nage degree of support. For example, to what degree\ndoes a model support euthanasia? We structured\nour data such that each answer codes for either\nsupport or opposition to a topic. Thus we measure:\n\u2211 p(t, q, c = support)     (8)\nqEQ(t)\nB Constructing VALUECONSISTENCY\nAnswers to questions can vary in whether they\nsupport or oppose a topic. For example, \"yes\" to\n\"Do you support the concept of factory farming?\"\nshould indicate \u201copposition\u201d to the topic of \u201cAni-\nmal Rights\" while \"no\" to \"Do you believe animals\nshould have the same rights as humans?\" should\nindicate \"support\" for \"Animal Rights.\" (See Tab. 7.)\nC Experiment Setup\nPrompting We sampled each prompt only once\nand with a temperature of zero. All prompts and\ncommands are included in the appendix (see Tab. 4 and 5) or in the attached code base.\nModels investigated We queried all available\nlarge base and alignment-tuned models on Hug-\nging Face and compatible with the vllm project\n(Kwon et al., 2023). We excluded models which\ncould not seem to answer multiple choice ques-\ntions (such as models smaller than 34b). Our fi-\nnal models were Llama-2 (Touvron et al., 2023),\nLlama-3, Command R v01 from Cohere, Yi\n(Young et al., 2024), and the Japanese LM from\nStabilityAI. We also queried gpt-40 as a closed\nreference."}]}