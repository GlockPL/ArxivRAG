{"title": "Approaching Deep Learning through the Spectral Dynamics of Weights", "authors": ["David Yunis", "Kumar Kshitij Patel", "Samuel Wheeler", "Pedro Savarese", "Gal Vardi", "Karen Livescu", "Michael Maire", "Matthew R. Walter"], "abstract": "We propose an empirical approach centered on the spectral dynamics of weights\u2014the behavior of singular values and vectors during optimization\u2014to unify and clarify several phenomena in deep learning. We identify a consistent bias in optimization across various experiments, from small-scale \"grokking\" to large-scale tasks like image classification with ConvNets, image generation with UNets, speech recognition with LSTMs, and language modeling with Transformers. We also demonstrate that weight decay enhances this bias beyond its role as a norm regularizer, even in practical systems. Moreover, we show that these spectral dynamics distinguish memorizing networks from generalizing ones, offering a novel perspective on this longstanding conundrum. Additionally, we leverage spectral dynamics to explore the emergence of well-performing sparse subnetworks (lottery tickets) and the structure of the loss surface through linear mode connectivity. Our findings suggest that spectral dynamics provide a coherent framework to better understand the behavior of neural networks across diverse settings.", "sections": [{"title": "1. Introduction", "content": "Interest in neural networks has exploded in the past decade. Capabilities are rapidly improving, and deployment is ever-increasing. Yet, although issues with these technologies now have social repercussions (Bender et al., 2021; Bommasani et al., 2021), many fundamental questions regarding their behavior remain unanswered.\nFor instance, despite extensive research, we still lack a complete understanding of the implicit biases (Neyshabur et al., 2014) of neural networks trained via stochastic optimization. Even basic questions regarding the role of regularization like weight decay (Hanson & Pratt, 1988; Krogh & Hertz, 1991; Zhang et al., 2018a) have only partial answers (Van Laarhoven, 2017; Andriushchenko et al., 2023; Yaras et al., 2023b). Perhaps most vexing, we lack a complete explanation for how neural networks generalize, despite having the capacity to perfectly memorize the training data (Zhang et al., 2021). Such an explanation may allow us to design better algorithms, however a lack of understanding makes the deployment of neural networks vulnerable to uninterpretable errors across fields (Szegedy et al., 2013; Ilyas et al., 2019; Hendrycks et al., 2021; Zou et al., 2023).\nAlthough theoretical explanations have been put forward to make analysis tractable, these studies often rely on special settings like deep linear networks (Arora et al., 2018; 2019) or infinite-width systems (Jacot et al., 2018), and arguments may rely on unsubstantiated or impractical assumptions like near zero initialization. On the empirical side, a growing body of work in interpretability has attempted to reverse-engineer neural networks (Rahaman et al., 2019; Barak et al., 2022; Nanda et al., 2023), but given the difficulty of the tasks, the systems of interest have been very small-scale, and the methodology for analysis quite bespoke and difficult to scale. A third category of work aims at understanding empirical behavior from a higher level (Zhang et al., 2021; Huh et al., 2022; Yu & Wu, 2023), but while these works often study larger-scale systems, they often focus on more abstract objects like the gram matrix (Huh et al., 2022) or NTK (Fort et al., 2020), and thus do not have the granularity and predictive power of the previous two categories.\nTo bridge these gaps, we propose a task-agnostic, unifying perspective of many disparate phenomena in deep learning across many different practical tasks and architectures, including image classification with ConvNets, image generation with UNets, speech recognition with LSTMs and language modeling with Transformers. Through extensive experiments, we examine the dynamics of singular values and singular vectors of weight matrices and the spectral dynamics of weights. We show that such dynamics underlie many distinct phenomena and appear intimately tied to generalization. We are motivated to study these dynamics specifically as optimization is the fundamental process driving deep learning (Nagarajan & Kolter, 2019; Zhang et al., 2021), the matrix parameters form the core identity of any neural network, and the singular value decomposition is a fundamental way to view any matrix. We detail our specific contributions in the following paragraphs.\nAs a test bed for understanding generalization, Power et al. (2022) introduce the \"grokking\" phenomenon, where a small-scale model initially minimizes the training loss but performs poorly on validation data, then with much more training suddenly minimizes the validation loss. In particular, Nanda et al. (2023) showed that in simple modular arithmetic tasks, the specific solution learned by optimization could be reverse-engineered from the weight matrices. Although this description is precise, in Section 3, we notice a task-agnostic view of grokking, observing that the drop in validation loss during grokking coincides with the simultaneous discovery of low-rank solutions across all weight matrices in the network. The connection between rank and generalization might intuitively be through Occam's Razor. Furthermore, echoing existing works (Lyu et al., 2023; Liu et al., 2023), we find that weight decay clearly affects grokking and rank minimization: neither behavior occurs as strongly without it. Having said that, we demonstrate that increasing the training data can partially compensate for the absence of weight decay in terms of generalization and rank minimization, and in all cases, we see a correlation between low-rank matrices and generalization. Thus, we find that examining spectral dynamics provides a task-agnostic view of grokking.\nThough this suggests a connection between rank and generalization, grokking is typically studied on synthetic tasks, very small-scale models like single-layer Transformers or small MLPs, and requires very particular hyperparameter settings (Power et al., 2022; Nanda et al., 2023; Gromov, 2023; Kumar et al., 2023). If our perspective is to be useful, it needs to scale to larger systems. Thus, we turn to common empirical tasks drawn from the literature like image classification, image generation, speech recognition and language modeling as well as varied and larger networks like VGG (Simonyan & Zisserman, 2014), UNet (Ronneberger et al., 2015), LSTM (Hochreiter & Schmidhuber, 1997b) and Transformers (Vaswani et al., 2017).\nIn Section 4, we demonstrate that the spectral dynamics are biased toward effective rank minimization across various practical neural networks in complex settings. Although this behavior echoes theoretical predictions in the deep linear setting, we find that the behavior of networks disagrees with a common theoretical assumption about low-rank dynamics: alignment of singular vectors in consecutive layers (Saxe et al., 2014; Arora et al., 2018; 2019; Milanesi et al., 2021). Thus, the rank minimization mechanism may differ from what the theory describes. It is notable too that our hyperparameter settings are drawn from existing literature, thus the trend toward rank minimization coincides with well-generalizing networks across settings.\nStill, in these more practical settings, we do not see such an obvious rank minimization as in grokking, nor is there a sudden transition from memorization to generalization. One particularly notable ingredient for grokking was a very high level of weight decay. Weight decay has a long history as a regularizer explicitly penalizing parameter norm, which can be used for norm-based generalization bounds (Bartlett, 1996), but these bounds do not seem to explain the success of practical systems (Nagarajan & Kolter, 2019; Jiang et al., 2019). Recently, others have suggested an implicit effect of weight decay on parameter rank in theoretical or small-scale empirical studies (Galanti et al., 2022; Timor et al., 2023) which may be connected to generalization (Razin & Cohen, 2020). As such, we explore this effect of weight decay in practical settings.\nIn Section 5, we empirically connect rank minimization to weight decay, showing that weight decay promotes rank minimization across architectures and tasks. In addition, in some cases it also appears to promote singular vector alignment in consecutive weights despite the nonlinearities between layers, which indicates further compression of the model. Although weight decay explicitly penalizes norm, studying spectral dynamics allows us to observe the implicit effect on rank. Such an effect can help in understanding generalization as norm regularization is insufficient.\nGiven the suggestive connection between rank and generalization, we turn to the classic memorization experiments of Zhang et al. (2021). Zhang et al. (2021) demonstrated that even small networks can memorize random labels. Hence, any arguments about generalization need to take into account the structure of the data and the optimization process. In Section 6, we show that training with random labels leads to high-rank solutions, while rank with true labels is much lower. We also find that while random labels do not align consecutive layers, true labels do, which is surprising given the non-linearities between layers. Through spectral dynamics, we see a clear difference between the optimization of generalizing and memorizing networks, which provides a foothold in the climb toward better theoretical understanding.\nOur results suggest that viewing neural networks through the lens of spectral dynamics can shed light on several generalization-related phenomena, but we suspect there are broader connections. In the literature, many curious and unexplained phenomena regarding neural networks exist. We take two as case studies. First, the lottery ticket hypothesis (LTH) (Frankle & Carbin, 2018), which has found the existence of sparse sub-networks with similar performance. Such a phenomenon provides evidence that, despite ever-increasing parameter counts and energy costs, efficient smaller networks already exist. Understanding the source of such efficiency may help us alleviate deployment costs. Second, linear mode connectivity (LMC) (Nagarajan & Kolter, 2019; Frankle et al., 2020; Neyshabur et al., 2020), which finds that models sharing a portion of the optimization trajectory can be averaged together in weight-space to yield a stronger model (Wortsman et al., 2022; Ramesh et al., 2022). This phenomenon indicates that, after some training, the loss surface is quite convex in a subspace, even though the optimization problem is theoretically extremely nonconvex. As any finetuning from pre-trained models stays in this convex space (Neyshabur et al., 2020; Li et al., 2022; Sadrtdinov et al., 2023), an explanation for what underlies model-averaging would help to clarify the role of pretraining, would shed light on the commonly-used low-rank adaptation (Hu et al., 2021), and could lead to better optimization.\nIn Section 7, we find that global magnitude pruning, a standard procedure for finding lottery tickets, preserves top singular vectors and acts like a low-rank pruning. We also see that the ability to interpolate between models in LMC strongly correlates with sharing top singular vectors. With these results, we note that the two phenomena can be seen as aspects of the spectral dynamics of weights and bring them under the umbrella of prior sections.\nTo summarize the discussion above, by studying the spectral dynamics of weights, we find:\n\u2022 Grokking is intimately linked to rank minimization;\n\u2022 Rank minimization is a general phenomenon in more complex tasks;\n\u2022 Weight decay acts implicitly as a low-rank regularizer;\n\u2022 Generalizing solutions have a lower rank than memorizing ones; and\n\u2022 Top singular vectors are preserved when performing magnitude pruning while linearly interpolating between connected modes.\nAll of these phenomena and effects have previously been studied in isolation to varying degrees, but by approaching deep learning through spectral dynamics, we aim at a common language for neural networks. Code for all experiments is released at https://github.com/dyunis/\nspectral_dynamics."}, {"title": "2. Related Work", "content": "2.1. Grokking\nPower et al. (2022) first noticed a surprising phenomenon they called \"grokking\u201d where models quickly fit the training data on toy tasks, then after a long period of training, very quickly generalize on the validation data. Later, others found that this phenomenon can occur in a relaxed fashion (Thilak et al., 2022) on very simple models and different datasets (Liu et al., 2022; Gromov, 2023; Kumar et al., 2023; Xu et al., 2023) and that weight decay seems critical to cause it (Lyu et al., 2023; Liu et al., 2023; Tan & Huang, 2023). Some posit a transition from the kernel (Jacot et al., 2018) to rich (Atanasov et al., 2023) regime explains grokking (Kumar et al., 2023; Mohamadi et al., 2023). There is also empirical evidence for a connection between double descent and grokking (Davies et al., 2022) the discovery of a sparse solution (Merrill et al., 2023), the simplification of decision boundaries (Humayun et al., 2024) and the leading indicator of loss oscillation (Thilak et al., 2022; Notsawo Jr et al., 2023). None of these works have explicitly examined the connection with rank, which we do in Section 3, and provides a common framework through which to view many of these results.\n2.2. Singular Value Dynamics\nPrior work on deep linear networks (Arora et al., 2019; Milanesi et al., 2021) suggests that rank minimization may describe implicit regularization in deep matrix factorization better than simple matrix norms. See (Arora et al., 2018) (Appendix A) for a detailed argument. However, a critical assumption used in these works is \"balanced initialization.\" This means that for consecutive matrices $W_i$ and $W_{i+1}$ in the product matrix $\\Pi_j W_j$, we have $W_{i+1}^T W_{i+1} = W_i W_i^T$ at initialization. Decomposing these matrices with SVDs and leveraging orthogonality, this simplifies to $V_{i+1}^T U_{i+1} = U_i V_i^T$ where $U_i$ and $V_{i+1}$ are orthogonal matrices. Since these are orthogonal decompositions of the same matrix, their diagonals must be equivalent, allowing for the permutation of elements with the same value. This leads to $U_i = V_{i+1} O$ up to signs, where $O$ is a block diagonal permutation matrix that may permute the rows of equivalent diagonal elements. Notably, if all diagonal elements are distinct and $U_i$ and $V_{i+1}$ are square matrices, then $U_i = V_{i+1}$ up to signs.\nUnder the balanced initialization assumption, all product matrices will be aligned. Consequently, the product of the diagonals will evolve in a closed-form manner, with larger singular values growing faster than smaller ones. As shown by (Arora et al., 2019), this translates to rank-minimizing behavior with increasing depth in the matrix products. This formula is also empirically validated for linear matrix factorization problems. Similar results have been derived for tensor products and other structured settings (Saxe et al., 2014; Yaras et al., 2023a). More generally, (Ji & Telgarsky, 2019) show that for deep linear networks with infinite training alignment between layers will happen. In Section 4, we explore how these conclusions and assumptions hold for much larger, practical neural networks that are far from linear.\n2.3. Low-Rank Properties\nAnother line of research focuses on more general low-rank biases. Early work explored norms as an implicit bias (Gunasekar et al., 2017). Theoretical analyses reveal that norms or closed-form functions of weights might be insufficient to explain implicit regularization, but they do not necessarily contradict the possibility of rank minimization (Razin & Cohen, 2020; Vardi & Shamir, 2021). Numerous studies investigate low-rank biases in various matrices, including the Jacobian (Pennington et al., 2018), weight matrices (Le & Jegelka, 2021; Martin & Mahoney, 2020; 2021; Frei et al., 2022; Ongie & Willett, 2022), Gram matrix (Huh et al., 2022), and features (Yu & Wu, 2023; Feng et al., 2022). Additionally, research suggests that dynamics influence the decay of rank (Li et al., 2020; Chen et al., 2023; Wang & Jacot, 2023). Some works establish connections between weight decay and rank minimization in idealized settings (Ziyin et al., 2022; Galanti et al., 2022; Zangrando et al., 2024; Ergen & Pilanci, 2023; Parhi & Nowak, 2023; Shenouda et al., 2023). We are particularly interested in how far these connections extend in practice. In Section 5, we present empirical evidence that sometimes disagrees with, but also expands, arguments from theory and small-scale systems to much larger ones."}, {"title": "3. Grokking and Rank Minimization", "content": "Motivated by theoretical work that proposes connections between rank and generalization (Razin & Cohen, 2020) weight decay and rank (Galanti et al., 2022; Timor et al., 2023; Yaras et al., 2023b; Zangrando et al., 2024), and the importance of weight decay for grokking (Power et al., 2022; Lyu et al., 2023; Liu et al., 2023) in simple settings, we evaluate the potential connection between rank and grokking in neural networks. Examining grokking through the lens of rank (and more generally spectral dynamics) offers a complementary perspective on grokking with other descriptions such as fourier decomposition (Nanda et al., 2023), the simplification of linear decision boundaries (Humayun et al., 2024), the connection to double descent (Davies et al., 2022), and the discovery of a sparse solution (Merrill et al., 2023).\nWe largely follow the setting of Nanda et al. (2023), optimizing a single-layer Transformer for modular addition (details in Appendix A). Inspired by work in the deep linear case (Saxe et al., 2014; Arora et al., 2019; Milanesi et al., 2021; Yaras et al., 2023b), we track the evolution of singular values for individual weight matrices. To gain a high-level overview of all parameter evolutions, we compute the (normalized) effective rank of a matrix $W$ (Roy & Vetterli, 2007) with rank $R$ as\n$\\mathrm{EffRank}(W) := \\frac{\\sum_{i=1}^R \\sigma_i}{\\sigma_1},$$\n$\\mathrm{NormEffRank}(W) := \\frac{H}{\\log R},$\nwhere $\\sigma_i$'s are the singular values of matrix $W$ and EffRank($W$) is the entropy of the normalized singular value distribution. As the probability mass concentrates, the effective rank decreases. We plot NormEffRank($W$) to compare across layers and time.\nIn addition, inspired by the assumptions of balancedness made by prior work (Arora et al., 2018; 2019), we examine the alignment of consecutive weight matrices in the Transformer. To examine and quantify this alignment between consecutive matrices in a network at training time $t$, i.e.,\n$W_i = \\sum_{j=1}^R \\sigma_j(t) u_j(t) v_j^T(t),$\n$W_{i+1} = \\sum_{k=1}^R \\sigma_k(t) u_k(t) v_k^T(t),$\nwe compute,\n$A(t)_{jk} = |(u_j (t), v_k(t))|,$where the absolute value is taken to ignore sign flips in the SVD computation. We then plot the diagonal of this"}, {"title": "4. Spectral Dynamics Across Tasks", "content": "Inspired by the results on grokking and prior work on deep linear networks, which studies the evolution of the SVD of the weight matrices (Saxe et al., 2014; Arora et al., 2018; 2019; Milanesi et al., 2021; Yaras et al., 2023a), we apply the same analysis to larger, more practical systems. We show that the trends we saw in the analysis of grokking mostly hold true across networks and tasks at a much larger scale, even though our findings do occasionally deviate from theoretical predictions.\n4.1. Methodology\nOur experiments aim to examine reasonably sized neural networks across a variety of tasks. We select models and tasks that are representative of current applications. Specifically, we focus on:\n\u2022 Image classification with CNNs (VGG-16 (Simonyan & Zisserman, 2014)) on CIFAR10 (Krizhevsky, 2009);\n\u2022 Image generation through diffusion with UNets (Ronneberger et al., 2015) on MNIST (LeCun, 1998);\n\u2022 Speech recognition with LSTMs (Hochreiter & Schmidhuber, 1997b) on LibriSpeech (Panayotov et al., 2015); and\n\u2022 Language modeling with Transformers (Vaswani et al., 2017) on Wikitext-103 (Merity et al., 2016).\nTraining hundreds of runs for each of the above experiments is computationally expensive, limiting the scale of models we can explore. We primarily adopt hyperparameters from existing literature, with minor modifications for simplicity. This ensures that any correlations observed are likely a reflection of common practices, not introduced bias on our part. We hope that the broad scope of these experiments will allow for a more general perspective on neural network optimization.\nThe primary evidence in this section comes from computing the SVDs of weight matrices within the models. Consequently, we disregard 1D bias and normalization parameters in our analysis. However, previous research suggests that in some cases these parameters are not crucial for performance (Zhang et al., 2018b; Mohan et al., 2019; Karras et al., 2023). Due to the large number of matrices in these models, we present plots of individual layers' matrix parameters and statistics summarizing behavior across layers for conciseness of presentation. Hundreds of thousands of plots were generated for this study, making it impossible to include them all. Full experimental details, including hyperparameters, are available in Appendix A, where we take hyperparameters from existing settings in the literature.\n4.2. Effective Rank Minimization\nBuilding on theoretical (Saxe et al., 2014; Arora et al., 2019; Milanesi et al., 2021; Boix-Adser\u00e0 et al., 2023; Yaras et al., 2023a) and empirical (Dittmer et al., 2019; Martin & Mahoney, 2020; 2021; Boix-Adser\u00e0 et al., 2023) findings, we investigate effective rank minimization across parameters in larger models and on a more diverse variety of tasks. Figure 3 reveals a consistent trend: the effective rank of network parameters generally decreases throughout training, regardless of the specific parameter or network architecture. This suggests a progressive \u201csimplification\u201d of the network as training progresses, and echoes our previous findings in the high-data regime on modular addition.\nWe further conduct a singular-value pruning experiment to explore the relationship between low-rank behavior and model performance. We prune either the top or bottom half of the singular values for each weight matrix in the network and then evaluate the pruned model at each training step. Intuitively, we expect the top singular values to capture the most critical information for the network's function. Figure 4 confirms this hypothesis, demonstrating that the pruned parameters, without further training, can closely approximate the full model's performance. This may not have been the case. In particular, simultaneously pruning lower components across all layers may lead to losing some critical signal that must be passed between layers, or the large"}, {"title": "4.3. Alignment of Singular Vectors Between Layers", "content": "Similar to the analysis of grokking, we investigate the alignment between consecutive layers in the larger neural networks considered in this section. We not only employ the alignment matrix defined in Eqn. 3 but also derive and plot a scalar measure for alignment based on the top diagonal entries:\n$a(t) = \\frac{1}{10} \\sum_{i=1}^{10} A(t)_{ii}.$For specific details on calculating this measure in diverse architectures and complex layers (beyond fully connected layers), please refer to Appendix A.\nFigure 5 reveals a key finding: the theoretical assumption of balanced initialization, which posits aligned singular value decompositions (SVDs) between weight matrices (Arora et al., 2018; Saxe et al., 2014), does not hold true at the start of training in these larger networks. Additionally, unlike the linear case discussed in Du et al. (2018), the alignment does not appear to remain static throughout training. However, a weak signal of alignment in the top ranks develops. This trend is somewhat reminiscent of the theoretical result provided by Mulayoff & Michaeli (2020) for linear networks under the assumption of whitened input data. Still, the weakness of the observed signal means that existing theoretical models do not fully capture the complexities of real-world neural network training."}, {"title": "5. The Effect of Weight Decay", "content": "In light of the previously observed evolution of singular values, we investigate a proposed effect of weight decay. Though weight decay explicitly penalizes the norm of weights, there is evidence that complicates the connection between norm and generalization for neural networks (Razin & Cohen, 2020; Andriushchenko et al., 2023), meaning we do not have a full understanding as to why weight decay may be useful. Alternatively, some theoretical (Boix-Adser\u00e0 et al., 2023; Razin & Cohen, 2020; Yaras et al., 2023a; Timor et al., 2023; Ongie & Willett, 2022; Galanti et al., 2022; Zangrando et al., 2024) and empirical works (Galanti et al., 2022; Boix-Adser\u00e0 et al., 2023) propose a connection with the rank of matrices in constrained settings. Still, a comprehensive connection to larger empirical networks has not yet been demonstrated.\nWe speculate on the intuition of the mechanism in more practical settings. Notice in its simplest form that weight decay asks for $\\arg \\min_W L(W) + \\lambda ||W||_F^2$, where $||W||_F^2 = \\sum_i \\sigma_i^2$ with singular values $\\sigma_i$ of weight matrix $W$ with rank $R$. We saw that larger singular values of neural networks grow faster (Fig. 3 top row) and that the top singular vectors are much more useful for minimizing task loss than the bottom ones (Fig. 4). Thus, with minor weight decay regularization, one straightforward solution for the network may be to minimize the rank of a given weight matrix while preserving the top singular values to minimize $L(W)$. Timor et al. (2023) argue a similar effect as if all singular values are less than 1, the norm of activations will shrink with depth, so it will be impossible to pass signals with sufficiently deep networks. Thus it is important for a few singular values to be sufficiently large.\nFigure 6 shows that adding weight decay produces this exact low-rank behavior, while too much weight decay leads to complete norm collapse. The exact choice of \"too much\" varies across architectures and tasks, though it may be due to the use of SGD for VGG (Simonyan & Zisserman, 2014) instead of AdamW (Loshchilov & Hutter, 2017) for the rest.\nIn addition, even more surprisingly, large amounts of weight decay promote a tighter alignment in the top singular vectors of consecutive layers of the Transformer, but not other networks. We see this in Figure 7. This behavior is quite reminiscent of the balancedness condition (Arora et al., 2018; 2019; Du et al., 2018), though the Transformer considered here has nonlinearities and much more complex structures. It is curious that the trend is so strong for only this architecture. We also provide additional evidence in Appendix A where Figure 14 shows that the solutions with very high weight decay are still performant, even though they are much lower rank. Though it is difficult to argue as simple a trend as \"lower rank equals better generalization\" because one does not know the minimal rank necessary for a given task, it is notable that the role of weight decay for improving generalization is tied up with its function as a rank regularizer. In addition, although we lack precise tools to entirely interpret complex models, when there are only a few ranks per matrix it may become possible to extend analysis efforts (Nanda et al., 2023; Praggastis et al., 2022) to more complex domains."}, {"title": "6. Spectral Dynamics with Random Labels", "content": "Given the observations connecting generalization and rank thus far, and the enlightening view on the implicit effects of weight decay, we are interested in seeing whether the perspective developed sheds any light on the classic random label memorization experiments of Zhang et al. (2021).\nSimilar to Zhang et al. (2021), we train a simple MLP to fit random or true labels on CIFAR10. Please see Appendix A for the details regarding the experimental setup. Zhang et al. (2021) decay the learning rate to zero, and the random label experiments only converge late in training. Consequently, we use a constant learning rate to control this phenomenon. We see in Figure 8 that both cases are able to achieve zero error, though with different singular value evolution and alignment in the middle layer.\nSurprisingly, we see that with true labels the inner layers are low rank, while with random labels they are much higher rank. This may be explained by the shared structure in the true classes of the dataset, which manifests in the parameters. Even more surprisingly, we find here that even without weight decay, inner layers align with true labels, while with random labels, this alignment occurs and then disappears with more training. This is particularly intriguing as there are non-linearities that could theoretically separate the network from the linear case, and yet strong alignment occurs despite that. Such alignment has not yet been leveraged by existing theory, and might provide structured assumptions for new understanding. In summary, these results suggest that viewing generalization through the lens of rank may be fruitful."}, {"title": "7. Beyond Generalization", "content": "We have seen over the course of many experiments that deep models are biased toward low rank", "phenomena": "lottery tickets (Frankle & Carbin", "case": "s(t) = \\frac{1}{10} \\sum_{i} S(t)_{ii}$. In Figure 9, we see that top singular vectors converge in direction earlier than bottom vectors.\n7.2. Lottery Tickets Preserve Final Top Singular Vectors\nAs large singular vectors will become stable late in training, we wonder about the connection to magnitude pruning and the lottery ticket hypothesis. Frankle & Carbin (2018) first showed evidence for the lottery ticket hypothesis, the idea that there exist sparse subnetworks of neural networks that can be trained to a comparable performance as the full network, where the sparse mask is computed from the largest magnitude weights of the network at the end of training. Frankle et al. (2020) build further on this hypothesis and notice that, for larger networks, the masking cannot begin at initialization, but rather at some point early in training. Still, the mask must come from the end of training.\nThe reason for this particular choice of mask may be connected to the dynamics we previously observed. Specifically, at the end of training large singular values are disproportionately larger, so high-magnitude weights may correspond closely to weights in the top singular vectors at the end of training. If magnitude masks were computed at the beginning, the directions that would become the top singular vectors might be prematurely masked as they have not yet stabilized, which may prevent learning on the task.\nHere we train an unmasked VGG-16 (Simonyan & Zisserman, 2014) on CIFAR10, then compute either a random mask, or a global magnitude mask from the end of training, and rewind to an early point (Frankle et al., 2020) to start sparse retraining. Please see Appendix A for details.\nIn Figure 10, we plot the singular vector agreement (SVA, Eqn. 5) between the final model, masked and unmasked, where we see exactly that magnitude masks preserve the top singular vectors of parameters, and with increasing sparsity fewer directions are preserved. Even though prior work has remarked that it is possible to use low-rank approximations for neural networks (Yu et al., 2017), and others have explicitly optimized for low-rank lottery tickets (Wang et al., 2021; Schotth\u00f6fer et al., 2022), we rather are pointing out that the magnitude pruning procedure seems to recover a low-rank approximation.\nWe also compute the singular vector agreement (SVA) between the masked model trajectory and the original unmasked model trajectory (diagonal of Eqn. 5). We see in Figure 10 that there is no agreement between the bottom singular vectors at all, but there is still loose agreement in the top singular vectors. Thus, it seems the mask allows the dynamics of only the top singular vectors to remain similar, which we know are most important from the pruning analysis in Figure 4.\nPreserving top"}]}