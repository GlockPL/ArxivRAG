{"title": "Approaching Deep Learning through the Spectral Dynamics of Weights", "authors": ["David Yunis", "Kumar Kshitij Patel", "Samuel Wheeler", "Pedro Savarese", "Gal Vardi", "Karen Livescu", "Michael Maire", "Matthew R. Walter"], "abstract": "We propose an empirical approach centered on the spectral dynamics of weights\u2014the behavior of singular values and vectors during optimization\u2014to unify and clarify several phenomena in deep learning. We identify a consistent bias in optimization across various experiments, from small-scale \"grokking\" to large-scale tasks like image classification with ConvNets, image generation with UNets, speech recognition with LSTMs, and language modeling with Transformers. We also demonstrate that weight decay enhances this bias beyond its role as a norm regularizer, even in practical systems. Moreover, we show that these spectral dynamics distinguish memorizing networks from generalizing ones, offering a novel perspective on this longstanding conundrum. Additionally, we leverage spectral dynamics to explore the emergence of well-performing sparse subnetworks (lottery tickets) and the structure of the loss surface through linear mode connectivity. Our findings suggest that spectral dynamics provide a coherent framework to better understand the behavior of neural networks across diverse settings.", "sections": [{"title": "1. Introduction", "content": "Interest in neural networks has exploded in the past decade. Capabilities are rapidly improving, and deployment is ever-increasing. Yet, although issues with these technologies now have social repercussions (Bender et al., 2021; Bommasani et al., 2021), many fundamental questions regarding their behavior remain unanswered.\nFor instance, despite extensive research, we still lack a complete understanding of the implicit biases (Neyshabur et al., 2014) of neural networks trained via stochastic optimization. Even basic questions regarding the role of regularization like weight decay (Hanson & Pratt, 1988; Krogh & Hertz, 1991; Zhang et al., 2018a) have only partial answers (Van Laarhoven, 2017; Andriushchenko et al., 2023; Yaras et al., 2023b). Perhaps most vexing, we lack a complete explanation for how neural networks generalize, despite having the capacity to perfectly memorize the training data (Zhang et al., 2021). Such an explanation may allow us to design better algorithms, however a lack of understanding makes the deployment of neural networks vulnerable to uninterpretable errors across fields (Szegedy et al., 2013; Ilyas et al., 2019; Hendrycks et al., 2021; Zou et al., 2023).\nAlthough theoretical explanations have been put forward to make analysis tractable, these studies often rely on special settings like deep linear networks (Arora et al., 2018; 2019) or infinite-width systems (Jacot et al., 2018), and arguments may rely on unsubstantiated or impractical assumptions like near zero initialization. On the empirical side, a growing body of work in interpretability has attempted to reverse-engineer neural networks (Rahaman et al., 2019; Barak et al., 2022; Nanda et al., 2023), but given the difficulty of the tasks, the systems of interest have been very small-scale, and the methodology for analysis quite bespoke and difficult to scale. A third category of work aims at understanding empirical behavior from a higher level (Zhang et al., 2021; Huh et al., 2022; Yu & Wu, 2023), but while these works often study larger-scale systems, they often focus on more abstract objects like the gram matrix (Huh et al., 2022) or NTK (Fort et al., 2020), and thus do not have the granularity and predictive power of the previous two categories.\nTo bridge these gaps, we propose a task-agnostic, unifying perspective of many disparate phenomena in deep learning across many different practical tasks and architectures, including image classification with ConvNets, image generation with UNets, speech recognition with LSTMs and language modeling with Transformers. Through extensive experiments, we examine the dynamics of singular values and singular vectors of weight matrices and the spectral dynamics of weights. We show that such dynamics underlie many distinct phenomena and appear intimately tied to generalization. We are motivated to study these dynamics specifically as optimization is the fundamental process driving deep learning (Nagarajan & Kolter, 2019; Zhang et al., 2021), the matrix parameters form the core identity of any neural network, and the singular value decomposition is a fundamental way to view any matrix. We detail our specific contributions in the following paragraphs.\nAs a test bed for understanding generalization, Power et al. (2022) introduce the \"grokking\" phenomenon, where a small-scale model initially minimizes the training loss but performs poorly on validation data, then with much more training suddenly minimizes the validation loss. In particular, Nanda et al. (2023) showed that in simple modular arithmetic tasks, the specific solution learned by optimization could be reverse-engineered from the weight matrices. Although this description is precise, in Section 3, we notice a task-agnostic view of grokking, observing that the drop in validation loss during grokking coincides with the simultaneous discovery of low-rank solutions across all weight matrices in the network. The connection between rank and generalization might intuitively be through Occam's Razor. Furthermore, echoing existing works (Lyu et al., 2023; Liu et al., 2023), we find that weight decay clearly affects grokking and rank minimization: neither behavior occurs as strongly without it. Having said that, we demonstrate that increasing the training data can partially compensate for the absence of weight decay in terms of generalization and rank minimization, and in all cases, we see a correlation between low-rank matrices and generalization. Thus, we find that examining spectral dynamics provides a task-agnostic view of grokking.\nThough this suggests a connection between rank and gen-"}, {"title": "2. Related Work", "content": "2.1. Grokking\nPower et al. (2022) first noticed a surprising phenomenon they called \"grokking\u201d where models quickly fit the training data on toy tasks, then after a long period of training, very quickly generalize on the validation data. Later, others found that this phenomenon can occur in a relaxed fashion (Thilak et al., 2022) on very simple models and different datasets (Liu et al., 2022; Gromov, 2023; Kumar et al., 2023; Xu et al., 2023) and that weight decay seems critical to cause it (Lyu et al., 2023; Liu et al., 2023; Tan & Huang, 2023). Some posit a transition from the kernel (Jacot et al., 2018) to rich (Atanasov et al., 2023) regime explains grokking (Kumar et al., 2023; Mohamadi et al., 2023). There is also empirical evidence for a connection between double descent and grokking (Davies et al., 2022) the discovery of a sparse solution (Merrill et al., 2023), the simplification of decision boundaries (Humayun et al., 2024) and the leading indicator of loss oscillation (Thilak et al., 2022; Notsawo Jr et al., 2023). None of these works have explicitly examined the connection with rank, which we do in Section 3, and provides a common framework through which to view many of these results.\n2.2. Singular Value Dynamics\nPrior work on deep linear networks (Arora et al., 2019; Milanesi et al., 2021) suggests that rank minimization may describe implicit regularization in deep matrix factorization better than simple matrix norms. See (Arora et al., 2018) (Appendix A) for a detailed argument. However, a critical assumption used in these works is \"balanced initialization.\" This means that for consecutive matrices $W_i$ and $W_{i+1}$ in the product matrix $\\Pi_j W_j$, we have $W_{i+1}^T W_{i+1} = W_i W_i^T$ at initialization. Decomposing these matrices with SVDs and leveraging orthogonality, this simplifies to $V_{i+1} \\Sigma_{i+1}^2 V_{i+1}^T = U_i \\Sigma_i^2 U_i^T$ where $U_i$ and $V_{i+1}$ are orthogonal matrices. Since these are orthogonal decompositions of the same matrix, their diagonals must be equivalent, allowing for the permutation of elements with the same value. This leads to $U_i = V_{i+1} O$ up to signs, where $O$ is a block diagonal permutation matrix that may permute the rows of equivalent diagonal elements. Notably, if all diagonal elements are distinct and $U_i$ and $V_{i+1}$ are square matrices, then $U_i = V_{i+1}$ up to signs.\nUnder the balanced initialization assumption, all product matrices will be aligned. Consequently, the product of the diagonals will evolve in a closed-form manner, with larger singular values growing faster than smaller ones. As shown by (Arora et al., 2019), this translates to rank-minimizing behavior with increasing depth in the matrix products. This formula is also empirically validated for linear matrix factorization problems. Similar results have been derived for tensor products and other structured settings (Saxe et al., 2014; Yaras et al., 2023a). More generally, (Ji & Telgarsky, 2019) show that for deep linear networks with infinite training alignment between layers will happen. In Section 4, we explore how these conclusions and assumptions hold for much larger, practical neural networks that are far from linear.\n2.3. Low-Rank Properties\nAnother line of research focuses on more general low-rank biases. Early work explored norms as an implicit bias (Gunasekar et al., 2017). Theoretical analyses reveal that norms or closed-form functions of weights might be insufficient to explain implicit regularization, but they do not necessarily contradict the possibility of rank minimization (Razin & Cohen, 2020; Vardi & Shamir, 2021). Numerous studies investigate low-rank biases in various matrices, including the Jacobian (Pennington et al., 2018), weight matrices (Le & Jegelka, 2021; Martin & Mahoney, 2020; 2021; Frei et al., 2022; Ongie & Willett, 2022), Gram matrix (Huh et al., 2022), and features (Yu & Wu, 2023; Feng et al., 2022). Additionally, research suggests that dynamics influence the decay of rank (Li et al., 2020; Chen et al., 2023; Wang & Jacot, 2023). Some works establish connections between weight decay and rank minimization in idealized settings (Ziyin et al., 2022; Galanti et al., 2022; Zangrando et al., 2024; Ergen & Pilanci, 2023; Parhi & Nowak, 2023; Shenouda et al., 2023). We are particularly interested in how far these connections extend in practice. In Section 5, we present empirical evidence that sometimes disagrees with, but also expands, arguments from theory and small-scale systems to much larger ones."}, {"title": "3. Grokking and Rank Minimization", "content": "Motivated by theoretical work that proposes connections between rank and generalization (Razin & Cohen, 2020) weight decay and rank (Galanti et al., 2022; Timor et al., 2023; Yaras et al., 2023b; Zangrando et al., 2024), and the importance of weight decay for grokking (Power et al., 2022; Lyu et al., 2023; Liu et al., 2023) in simple settings, we evaluate the potential connection between rank and grokking in neural networks. Examining grokking through the lens of rank (and more generally spectral dynamics) offers a complementary perspective on grokking with other descriptions such as fourier decomposition (Nanda et al., 2023), the simplification of linear decision boundaries (Humayun et al., 2024), the connection to double descent (Davies et al., 2022), and the discovery of a sparse solution (Merrill et al., 2023).\nWe largely follow the setting of Nanda et al. (2023), optimizing a single-layer Transformer for modular addition (details in Appendix A). Inspired by work in the deep linear case (Saxe et al., 2014; Arora et al., 2019; Milanesi et al., 2021; Yaras et al., 2023b), we track the evolution of singular values for individual weight matrices. To gain a high-level overview of all parameter evolutions, we compute the (normalized) effective rank of a matrix $W$ (Roy & Vetterli, 2007) with rank $R$ as\nEffRank(W) := $\\frac{\\sum_{i=1}^{R} \\sigma_i}{\\log \\sum\\sigma_i}$,\nNormEffRank(W) := $\\frac{EffRank(W)}{R}$,\nwhere $\\sigma_i$'s are the singular values of matrix $W$ and EffRank(W) is the entropy of the normalized singular value distribution. As the probability mass concentrates, the effective rank decreases. We plot NormEffRank(W) to compare across layers and time.\nIn addition, inspired by the assumptions of balancedness made by prior work (Arora et al., 2018; 2019), we examine the alignment of consecutive weight matrices in the Transformer. To examine and quantify this alignment between consecutive matrices in a network at training time $t$, i.e.,\n$W_i = \\sum_{j=1}^{R} \\sigma_j (t) u_j(t) v_j(t)^T$,\n$W_{i+1} = \\sum_{k=1}^{R} \\sigma_k (t) u_k(t) v_k(t)^T$,\nwe compute,\nA(t)_{jk} = |\\langle u_j(t), v_k(t) \\rangle|,\nwhere the absolute value is taken to ignore sign flips in the SVD computation. We then plot the diagonal of this"}, {"title": "4. Spectral Dynamics Across Tasks", "content": "Inspired by the results on grokking and prior work on deep linear networks, which studies the evolution of the SVD of the weight matrices (Saxe et al., 2014; Arora et al., 2018; 2019; Milanesi et al., 2021; Yaras et al., 2023a), we apply the same analysis to larger, more practical systems. We show that the trends we saw in the analysis of grokking mostly hold true across networks and tasks at a much larger scale, even though our findings do occasionally deviate from theoretical predictions.\n4.1. Methodology\nOur experiments aim to examine reasonably sized neural networks across a variety of tasks. We select models and tasks that are representative of current applications. Specifically, we focus on:\n* Image classification with CNNs (VGG-16 (Simonyan & Zisserman, 2014)) on CIFAR10 (Krizhevsky, 2009);\n* Image generation through diffusion with UNets (Ronneberger et al., 2015) on MNIST (LeCun, 1998);\n* Speech recognition with LSTMs (Hochreiter & Schmidhuber, 1997b) on LibriSpeech (Panayotov et al., 2015); and\n* Language modeling with Transformers (Vaswani et al., 2017) on Wikitext-103 (Merity et al., 2016).\nTraining hundreds of runs for each of the above experiments is computationally expensive, limiting the scale of models we can explore. We primarily adopt hyperparameters from existing literature, with minor modifications for simplicity. This ensures that any correlations observed are likely a reflection of common practices, not introduced bias on our part. We hope that the broad scope of these experiments will allow for a more general perspective on neural network optimization.\nThe primary evidence in this section comes from computing the SVDs of weight matrices within the models. Consequently, we disregard 1D bias and normalization parameters in our analysis. However, previous research suggests that in some cases these parameters are not crucial for performance (Zhang et al., 2018b; Mohan et al., 2019; Karras et al., 2023). Due to the large number of matrices in these models, we present plots of individual layers' matrix parameters and statistics summarizing behavior across layers for conciseness of presentation. Hundreds of thousands of plots were generated for this study, making it impossible to include them all. Full experimental details, including hyperparameters, are available in Appendix A, where we take hyperparameters from existing settings in the literature.\n4.2. Effective Rank Minimization\nBuilding on theoretical (Saxe et al., 2014; Arora et al., 2019; Milanesi et al., 2021; Boix-Adser\u00e0 et al., 2023; Yaras et al.,"}, {"title": "5. The Effect of Weight Decay", "content": "In light of the previously observed evolution of singular values, we investigate a proposed effect of weight decay. Though weight decay explicitly penalizes the norm of weights, there is evidence that complicates the connection between norm and generalization for neural networks (Razin & Cohen, 2020; Andriushchenko et al., 2023), meaning we do not have a full understanding as to why weight decay may be useful. Alternatively, some theoretical (Boix-Adser\u00e0 et al., 2023; Razin & Cohen, 2020; Yaras et al., 2023a; Timor et al., 2023; Ongie & Willett, 2022; Galanti et al., 2022; Zangrando et al., 2024) and empirical works (Galanti et al., 2022; Boix-Adser\u00e0 et al., 2023) propose a connection with the rank of matrices in constrained settings. Still, a comprehensive connection to larger empirical networks has not yet been demonstrated.\nWe speculate on the intuition of the mechanism in more practical settings. Notice in its simplest form that weight decay asks for arg min$_W$ L(W) + $\\lambda||W||_F^2$, where $||W||_F^2 = \\sum_{i} \\sigma_i^2$ with singular values $\\sigma_i$ of weight matrix W with rank R. We saw that larger singular values of neural networks grow faster (Fig. 3 top row) and that the top singular vectors are much more useful for minimizing task loss than the bottom ones (Fig. 4). Thus, with minor weight decay regularization, one straightforward solution for the network may be to minimize the rank of a given weight matrix while preserving the top singular values to minimize L(W). Timor et al. (2023) argue a similar effect as if all singular values are less than 1, the norm of activations will shrink with depth, so it will be impossible to pass signals with sufficiently deep networks. Thus it is important for a few singular values to be sufficiently large."}, {"title": "6. Spectral Dynamics with Random Labels", "content": "Given the observations connecting generalization and rank thus far, and the enlightening view on the implicit effects of weight decay, we are interested in seeing whether the perspective developed sheds any light on the classic random label memorization experiments of Zhang et al. (2021).\nSimilar to Zhang et al. (2021), we train a simple MLP to fit random or true labels on CIFAR10. Please see Appendix A for the details regarding the experimental setup. Zhang et al. (2021) decay the learning rate to zero, and the random label experiments only converge late in training. Consequently,"}, {"title": "7. Beyond Generalization", "content": "We have seen over the course of many experiments that deep models are biased toward low rank, and that there is a tempting connection between rank minimization and generalization. Still, the lens of spectral dynamics can be applied more broadly. In the following subsections, we explore two phenomena: lottery tickets (Frankle & Carbin, 2018) and linear mode connectivity (Frankle et al., 2020). Beyond shedding further light on neural networks, these phenomena have implications for more efficient inference and storage, as well as understanding the importance of pre-training (Neyshabur et al., 2020). We find that lottery tickets are a sparse approximation of final-checkpoint top singular vectors. The ability to linearly interpolate between faraway checkpoints and improve performance coincides strongly with top singular vector sharing between checkpoints. Such observations may form a foundation for a better understanding compression and model averaging (Wortsman et al., 2022; Ilharco et al., 2022).\n7.1. Top Singular Vectors Become Stable Earlier\nBefore we explore the phenomena, we first make another observation that will be helpful. As top singular values grow disproportionately large, it would be natural that top singular vectors become stable in direction as the gradients remain small. To demonstrate this, for a given matrix in the network $W_i(t) = \\sum_{j=1}^{R} \\sigma_j (t) u_j(t) v_j(t)^T$ at training time"}, {"title": "7.3. Spectral Dynamics and Linear Mode Connectivity", "content": "We come to the final phenomenon that we seek to describe: linear mode connectivity. Linear mode connectivity (LMC) is the property that one can interpolate linearly between two different minima in weight space and every parameter set along that path performs well, which gives the impression that the loss surface of neural networks is somehow convex despite its theoretical nonconvexity. This was first demonstrated in small networks with the same initialization (Nagarajan & Kolter, 2019), then expanded to larger networks and connected to lottery tickets (Frankle et al., 2020; Paul et al., 2022). Entezari et al. (2021) first conjecture that two arbitrary minima show LMC up to permutation, and demonstrate it in simple models. This was expanded to wide models (Ainsworth et al., 2022; Jordan et al., 2022; Qu & Horvath, 2024), and can be proven in various ways (Kuditipudi et al., 2019; Brea et al., 2019; Simsek et al., 2021; Ferbach et al., 2023), but it does not hold for standard models (Qu & Horvath, 2024). LMC has also been exploited for model-averaging and performance gains (Wortsman et al., 2022; Ilharco et al., 2022; Rame et al., 2022). Still despite all of this work, we lack a description for why LMC occurs. In particular: why is there a convex, high dimensional (Yunis et al., 2022) basin that models find shortly in training (Frankle et al., 2020), or after pretraining (Neyshabur et al., 2020; Sadrtdinov et al., 2023)? We do not answer this question in full, but find an interesting view through the singular vectors.\n7.3.1. LINEAR MODE CONNECTIVITY CORRELATES WITH TOP SINGULAR VECTOR AGREEMENT\nAs we saw earlier directional convergence of top singular vectors in Figure 9, it suggests the dynamics of those components are more stable, so we might expect mode-connected solutions to share these components. To examine this, we plot agreement between the singular vectors of the weight matrices at either endpoint of branches:\nW^{(1)}(T) = \\sum_{j} \\sigma_j(T) u_j(T) v_j(T)^T,\nW^{(2)}(T) = \\sum_{k} \\sigma_k(T) u_k(T) v_k(T)^T,\nspawned from the same initialization in training. If the branches are split from an initialization on a trunk trajectory W(t), we call t the split point or epoch. We visualize the diagonal of |\\langle u_j(T) v_j(T)^T, u_k(T) v_k(T)^T \\rangle|_{jk} vs. split epoch, where the absolute value is taken to ignore sign flips in SVD computation.\nTo remind the reader, LMC only occurs after a small amount of training time has passed. Too early and the final models of each branch will show a bump, or barrier, in the loss surface along the linear interpolation (Frankle et al., 2020). To measure this precisely, we use the definition from Neyshabur et al. (2020), which is the maximum deviation from a linear interpolation in the loss, an empirical measure for convexity in this linear direction. When this deviation is 0, we consider the checkpoints to exhibit LMC. Please see Appendix A.9 for details on the calculation. Given evidence in Figure 4 that top components are the most important for prediction,"}, {"title": "8. Discussion", "content": "We provide an empirical perspective to understand deep learning through the lens of SVD dynamics. We first note a tendency toward rank minimization on a small scale in grokking, then expand these findings to practical networks and tasks. In addition we find that weight decay, though it explicitly penalizes norm, implicitly promotes this low-rank bias. We also show through the developed analysis that generalization and memorization differ in the rank and alignment of solutions found by optimization.\nWe go beyond remarks on generalization and show that magnitude pruning for lottery tickets acts similarly to low-rank pruning, and linear mode connectivity coincides with the sharing of top singular vectors between checkpoints. Low rank models have more efficient inference, and finetuning from pretrained models always operates in the LMC regime (Neyshabur et al., 2020), thus this understanding can lead to more efficient inference and compression and possibly stronger optimization practices.\nWhile a comprehensive theory for our results remains elusive, these observations can act as a platform for a deeper understanding of deep learning. Notably, the observed spectral dynamics appear consistent across diverse settings, even without restrictive assumptions like balanced initialization, linearity, or small weight scales. This suggests a common underlying mechanism.\nOn the empirical side, several interesting problems present themselves. Interpretability of neural networks is a growing"}, {"title": "A. Experimental Details", "content": "For all experiments, we use 3 random seeds and average all plots over those 3. This is relatively small, but error bars tend to be very tight, and due to the high volume of runs required for this work we lack the resources to run much more.\nIn order to compute alignment we consider only pairs of layers that directly feed into each other, and ignore the influence of residual connections so as to cut down on the number of comparisons. Specifics on individual architectures are given below.\nA.1. Image Classification with VGG\nWe train a VGG-16 (Simonyan & Zisserman, 2014) on CIFAR-10 (Krizhevsky, 2009) for 164 epochs, following hyperparameters and learning rate schedule in (Frankle et al., 2020), but without data augmentation. For the optimizer we use SGD with batch size 128, initial learning rate 0.1 and momentum of 0.9. We also decay the learning rate 3 times by a factor of 10 at epoch 82, epoch 120, and finally at epoch 160. We also use a minor amount of weight decay with coefficient 0.0001.\nVGG-16 uses ReLU activations and batch normalization (Ioffe & Szegedy, 2015), and includes both convolutional and linear layers. For linear layers we simply compute the SVD of the weight matrix. For convolutional layers, the parameters are typically stored as a 4D tensor of shape $(C_{\\text{out}}, C_{\\text{in}}, h, w)$ for the output channels, input channels, height and width of the filters respectively. As the filters compute a transformation from each position and input channel to an output channel, we compute the SVD of the flattened tensor $(C_{\\text{out}}, C_{\\text{in}} \\cdot hw)$, which maps all inputs to outputs, similar to Praggastis et al. (2022). This is not the SVD of the entire transformation of the feature map to the next feature map, but rather the transformation from a set of adjacent positions to a particular position in the next layer. For the individual SV evolution plot, we use the 12th convolutional layer.\nIn order to compute alignment of bases between consecutive convolutional layers, $V_{i+1}^T U_i$ we need to match the dimensionality between $U_i$ and $V_{i+1}$. For convolutional layers we are presented with a question as to how to handle the spatial dimensions h and w as naively the input dimension of the next layer will be a factor of $h \\cdot w$ larger dimension. We experimented with multiple cases, including aligning at each spatial position individually or averaging over the alignment at all spatial positions, and eventually settled at aligning the output of one layer to the center spatial input of the next layer. That is, for a 3x3 convolution mapping to a following 3x3 convolution, we compute the alignment only for position (1,1) of the next layer. This seemed reasonable to us as on average the edges of the filters showed poorer alignment overall. For the individual alignment plot, we use the alignment between the 11th and 12th convolutional layers at the center spatial position of the 12th convolutional layer.\nA.2. Image Generation with UNets\nWe train a UNet (Ronneberger et al., 2015) diffusion model (Sohl-Dickstein et al., 2015; Ho et al., 2020) on MNIST (LeCun, 1998) generation. We take model design and hyperparameters from (Wang & Vastola, 2022). In particular we use a 4-layer residual UNet and train with AdamW (Loshchilov & Hutter, 2017) with batch size 128, and learning rate of 0.0003 for 100 epochs. This model uses swish (Ramachandran et al., 2017) activations and a combination of linear and convolutional, as well as transposed convolutional layers.\nComputing SVDs and alignment is similar to the image classification case described above, except in the case of the transposed convolutions where an extra transpose of dimensions is needed as parameters are stored with the shape $(C_{\\text{in}}, C_{\\text{out}}, h, w)$. For the individual SV evolution plot, we use the 3rd convolutional layer. For the alignment plot, we use the alignment between the 3rd and 4th convolutional layers at the center spatial position of the 4th convolutional layer.\nA.3. Speech Recognition with LSTMs\nWe train a bidirectional LSTM (Hochreiter & Schmidhuber, 1997a) for automatic speech recognition on LibriSpeech (Panayotov et al., 2015). We tune for a simple and well-performing hyperparameter setting. We use AdamW (Loshchilov & Hutter, 2017) with batch size 32, learning rate 0.0003 and weight decay 0.1 for 50 epochs. We also use a cosine annealing learning rate schedule from 1 to 0 over the entire 50 epochs.\nThe LSTM only has matrix parameters and biases, so it is straightforward to compute SVDs of the matrices. For individual SV evolution plots, we plot the 3rd layer input parameter. In the case of alignment, we make a number of connections: first down depth for the input parameters, then connecting the previous input parameter to the current hidden parameter in both directions, then connecting the previous hidden parameter to the current input parameter. In particular the LSTM parameters"}, {"title": "A.4. Language Modeling with Transformers", "content": "We train a Transformer (Vaswani et al., 2017) language model on Wikitext-103 (Merity et al., 2016). We base hyperparameter choices on the Pythia suite (Biderman et al., 2023), specifically the 160 million parameter configuration with sinusoidal position embeddings, 12 layers, model dimension 768, 12 attention heads per layer, and hidden dimension 768. We use AdamW (Loshchilov & Hutter, 2017) with batch size 256, learning rate 0.0006 and weight decay 0.1. We use a context length of 2048 and clip gradients to a maximum norm of 1. We also use a learning rate schedule with a linear warmup and cosine decay to 10% of the learning rate, like Biderman et al. (2023).\nFor SVDs, for simplicity we take the SVD of the entire $(\\text{3dmodel}, d_{\\text{model}})$ parameter that computes queries, keys and values from the hidden dimension inside the attention layer, without splitting into individual heads. This is reasonable as the splitting is done after the fact internally. We also take the SVD of the output parameters, and linear layers of the MLPs, which are 2 dimensional matrices. For the individual SV evolution plot, we plot the SVs of $W_1$ of the 8th layer MLP\nFor alignment, we consider the alignment of $W_Q$ and $W_K$ matrices, $W_V$ and $W_O$ matrices, computing alignment between heads individually then averaging over all heads. We also consider the alignment between $W_O$ and $W_1$ of the MLP block, between $W_1$ and $W_2$ of the MLP block, and between $W_2$ and the next attention layer. For the individual layer alignment, we plot alignment between $W_1$ and $W_2$ of the 8th layer MLP.\nA.5. Weight Decay Experiments\nAll tasks are trained in exactly the same fashion as mentioned previously, with increasing weight decay in the set {0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0}. For ease of presentation we consider a subset of settings across tasks. In Figure 14 we include trained model performance and pruned model performance to show that, even with high levels of weight decay, models do not entirely break down. More so, the approximation of the pruned model to the full model gets better with higher weight decay.\nA.6. Grokking Experiments\nWe mostly follow the settings and architecture of Nanda et al. (2023), except we use sinusoidal positional encodings instead of learned.\nFor the slingshot case we follow hyperparameter settings in Thilak et al. (2022), Appendix B except with the 1-layer architecture from Nanda et al. (2023) instead of the 2-layer architecture specified. W perform addition modulo 97. The original grokking plot in Thilak et al. (2022) appears much more dramatic as it log-scales the x-axis, which we do not do here for clarity.\nA.7. Random Label Experiments\nWe train a 4-layer MLP on CIFAR10 (Krizhevsky, 2009) with either completely random labels, or the true labels. We use SGD with momentum of 0.9 and constant learning rate"}]}