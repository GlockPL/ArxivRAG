{"title": "Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing", "authors": ["Akash Dhruv", "Anshu Dubey"], "abstract": "The emergence of foundational models and generative artificial intelligence (GenAI) is poised to transform productivity in scientific computing, especially in code development, refactoring, and translating from one programming language to another. However, because the output of GenAI cannot be guaranteed to be correct, manual intervention remains necessary. Some of this intervention can be automated through task-specific tools, alongside additional methodologies for correctness verification and effective prompt development. We explored the application of GenAI in assisting with code translation, language interoperability, and codebase inspection within a legacy Fortran codebase used to simulate particle interactions at the Large Hadron Collider (LHC). In the process, we developed a tool, [Code-Scribe], which combines prompt engineering with user supervision to establish an efficient process for code conversion. In this paper, we demonstrate how [Code-Scribe] assists in converting Fortran code to C++, generating Fortran-C APIs for integrating legacy systems with modern C++ libraries, and providing developer support for code organization and algorithm implementation. We also address the challenges of AI-driven code translation and highlight its benefits for enhancing productivity in scientific computing workflows.", "sections": [{"title": "1 Introduction", "content": "The widespread presence of GPUs in high-performance computing (HPC) platforms and the use of C++ abstractions to simplify GPU programming have spurred significant interest in converting existing Fortran codes to C++. In the Exascale Computing Project (ECP), several scientific applications underwent this transition to adopt Kokkos [14] as their GPU programming model. Most of these conversions were performed manually, taking years of development and translation effort. The advent of generative AI (GenAI) offers substantial potential to reduce the burden of manual code translation.\nTranslating legacy Fortran codebases to C++ has long been a challenging task for software developers. This difficulty arises not only from the nuances of the Fortran language but also from diverse coding practices accumulated over decades of software evolution. Many scientific applications contain code optimized for specific hardware, which complicates direct translation to C++. Additionally, the need for performance optimization across modern HPC platforms introduces another layer of complexity. Bulk translation of entire codebases is seldom a viable option, as non-trivially complex codebases often encounter broken functionality and degraded performance, leading to lengthy debugging processes that can consume significant developer time.\nIn practice, manual incremental translation has been the predominant approach in the community. Typically, developers convert code sections gradually, build Fortran-C interfaces, and run tests to ensure correctness. This controlled approach allows developers to verify the functionality of each section before moving on to the next. However, this process is labor-intensive, requiring extensive knowledge of both Fortran and C++ and an understanding of the interoperability challenges between the two languages. We investigated the use of GenAI to aid in translating MCFM, a Monte Carlo code that simulates particle interactions observed at the Large Hadron Collider (LHC), from Fortran to C++. To this end, we developed a tool, [Code-Scribe], along with a methodology for efficient translation. This paper describes [Code-Scribe] and its application to MCFM and also highlights other legacy Fortran codes under consideration.\nThe paper is organized as follows: Section 2 reviews the literature on code translation and the use of GenAI for developer productivity. Section 3 provides an overview of [Code-Scribe], followed by the translation methodology in Section 4. Section 5 presents results from large language model sensitivity studies, and Section 6 discusses ongoing and future work."}, {"title": "2 Background", "content": "The field of scientific computing has become increasingly reliant on the rapid development and deployment of advanced software tools. As simulations and models drive progress in everything from climate research to high-energy physics, the effective use of large language models (LLMs) in software development is becoming an essential component of the scientific workflow. The application of GenAI in software development, particularly in code translation and refactoring, has gained significant attention in recent years. Numerous studies have explored the potential of language models to automate the translation of legacy programming languages into more modern ones, enhancing interoperability and maintainability. For instance, research by Alon et al. [3] demonstrated that deep learning models could be employed to generate code snippets based on natural language descriptions, thereby bridging the gap between non-technical stakeholders and software development processes.\nAdditionally, recent advancements in retrieval-augmented generation (RAG) have further refined this approach, allowing models to utilize external knowledge bases to enhance their code generation capabilities (Lewis et al. [17]). The RAG framework has been particularly effective in improving the contextual understanding of complex queries, thereby producing more accurate and relevant responses from the model. As generative Al continues to evolve, its integration into scientific computing, especially for translating legacy Fortran code to C++, offers a promising avenue for enhancing developer productivity and driving innovation in high-performance computing environments. While human intervention may still be required to verify correctness of the generated code, the use of AI allows developers to concentrate on higher-level architectural decisions and performance tuning, rather than being slowed down by time-consuming manual tasks.\nRecent advancements in specialized large language models (LLMs), such as CodeLlama [6] and OpenAI's Codex [18], which powers GitHub Copilot [15], have opened new avenues for automating various software development processes. These models excel at parsing, transforming, and generating code across different programming languages, making them particularly valuable in code generation and code-assist tools. The use of these models via their application programming interfaces (APIs) has enhanced application development by facilitating integration and improving productivity, as evidenced by the success of GitHub Copilot.\nFrameworks like LASSI [10] further enhance this automation by streamlining prompt generation, compilation, and execution processes. Originally designed to generate synthetic coding data for training GenAI models, LASSI focuses on producing working code, but the assurance of code correctness remains a significant challenge and an unresolved implementation issue. This highlights the need for ongoing developer oversight, even as automation tools advance.\nWe have developed [Code-Scribe] [11], an AI-assisted tool designed to facilitate the translation of Fortran codebases to C++ as a step towards integrating GenAI technology in scientific code development workflows. We explore its application in code translation for scientific computing, and compare the performance and correctness of different LLMs in this context. Additionally, we highlight how [Code-Scribe] has been applied to various scientific computing software projects, for code translation and code inspection to enhance developer productivity. Finally, we outline future directions for enhancing the tool and integrating it into scientific workflows."}, {"title": "3 Overview of [Code-Scribe]", "content": "In scientific computing, the urgency to translate legacy Fortran codebases to modern languages like C++ is driven by the desire to leverage advanced libraries and hardware optimizations for HPC platforms. While Fortran remains highly performant, its support for platform heterogeneity remains subpar compared to that of C++. Translating legacy codes to C++ makes a much richer ecosystem for performance portability accessible to these codes. However, this translation process is fraught with challenges.\nA primary difficulty in Fortran-to-C++ translation arises from the fundamental differences between the languages. Fortran is renowned for its array operations and implicit type handling, while C++ demands explicit declarations and offers greater control over memory management. Additionally, large-scale scientific codes written in Fortran often evolve over decades, resulting in inconsistent coding styles, varying degrees of optimization, and numerous implicit assumptions about hardware.\nAn alternative method to full-scale rewriting is incremental translation, where code is converted in smaller, manageable sections, creating interfaces (usually Fortran-C layers) between legacy Fortran code and newly written C++ sections. This layered approach preserves functionality as each code segment is tested, facilitating easier debugging by isolating issues in well-defined portions. Figure 1 provides an example of such a workflow where the driver C++ code interfaces with the core codebase written in both Fortran and C++. Interoperability with the Fortran part of the codebase is facilitated by the Fortran-C API. To ensure a smooth transition, the translation process typically begins with converting data structures, which are foundational to the code's organization, before moving on to methods and functionality. As the translation progresses, more of the Fortran codebase is converted to C++ and verified for correctness by running tests.\nDespite its advantages, incremental translation remains labor-intensive. Developers must manually manage Fortran-C layers, handle complex data structures, and ensure that performance optimizations are retained. This is where GenAI can make a significant impact. However, effectively leveraging AI within an application requires a careful understanding of the task at hand, domain insight, and effective prompt development. Through our exploration of this field, we distilled our knowledge into [Code-Scribe], a tool that offers both code translation and software development assistance.\n[Code-Scribe] operates within a systematic framework that uses LLMs to aid developers in managing the complexities of code conversion. It uses chat completion as a prompting technique to generate desired coding output from GenAI models. This interaction-based strategy achieves good results through structured conversations that guide the code generation process. This interaction forms the foundation of [Code-Scribe] 's methodology, creating templates to streamline AI-assisted code conversion."}, {"title": "4 Methodology", "content": "Based on the overview in the previous section and the schematic in Figure 3, we developed a command-line interface for [Code-Scribe] with four commands: (1) Index, (2) Inspect, (3) Draft, and (4) Translate. Each command incorporates elements of RAG, enhancing the effectiveness of the translation process. Detailed documentation for the usage of these commands can be found in the [Code-Scribe] repository [11]; here, we focus on providing an overview of their functionality."}, {"title": "4.1 Index", "content": "This command generates a comprehensive mapping of the source tree by analyzing the project's file hierarchy, dependencies, and code relationships. It creates a YAML file within each subdirectory, capturing critical metadata such as the relative directory location, file names, and associated constructs like modules, subroutines, and functions. This structured index is essential for navigating large legacy codebases and ensures that subsequent translation steps are both efficient and accurate. The design of the Index command embodies RAG principles by enabling efficient retrieval of relevant metadata, which aids in informed querying during later commands. Figure 4 illustrates an example of the YAML index file. The file functions as a dictionary, where the key directory stores the relative path of a subdirectory to the project root defined by the key root. The key files contains a hierarchy of dictionaries with keys corresponding to filenames and constructs. For example: [files][file1.f90] [modules] provides a list of modules in the file file1.f90. During the execution of other [Code-Scribe] commands, YAML files from all directories are loaded and compiled into an inverse dictionary that maps constructs back to their corresponding filenames for efficient querying. For instance, during draft code generation, the inverse dictionary is utilized to determine that subroutineA belongs to file2.f90.\nThe need for the Index command arose from observing that, without specific guidance, LLMs often attempt to fill gaps in context by generating peripheral code, particularly when subroutines or modules appear undefined in the target file. For instance, when prompted to translate a subroutine that references external functions, the LLM would often extrapolate code for these external functions or even provide a main program example to demonstrate functionality. This was especially problematic in large codebases with deeply interconnected files. To mitigate such hallucinations, we developed the Index command to capture a detailed structural map of the source code using YAML files. This layout serves as a reference point for prompts, helping the LLM to focus solely on the intended conversion without producing code for elements already defined elsewhere in the project."}, {"title": "4.2 Inspect", "content": "The Inspect command allows users to interactively query specific details of the source code, using an LLM to answer queries for a list of Fortran files. This feature helps developers understand the structure and behavior of individual code sections, which is particularly useful when working with large or undocumented legacy codebases. Parsing scripts extract module names, functions, and subroutines, which are then queried against the YAML index to provide context for the LLM. The command leverages RAG elements by retrieving context from the YAML index. Interaction with the LLM is achieved through one of the following mechanisms:\n(1) Directly by sending requests to API endpoints, such as for OpenAI models [19].\n(2) Locally using downloaded models via the Transformers library [21], for models like Codellama and Mistral.\n(3) Saving the prompt contents to a JSON file, which can then be copy/pasted into interfaces like ChatGPT for interactive discussions.\nThe Inspect command ensures that the LLM can accurately answer developer queries without over-generating responses that diverge from the target function's scope. It not only aids in code comprehension but also facilitates knowledge generation to design the seed prompt (chat template) shown in Figure 2 for code translation."}, {"title": "4.3 Draft", "content": "The Draft command automates the initial phase of code conversion by identifying patterns within the Fortran source code and applying systematic substitutions to produce a preliminary C++ draft. This step serves as preparation for AI-driven translation by generating additional context alongside the seed prompt.\nThe creation of the Draft command was informed by challenges encountered in maintaining the consistency of data structures and variable types. Without a preliminary draft, LLMs frequently introduced erroneous data types or redefined existing structures, leading to incoherence in generated C++ code. Providing a draft code serves as a controlled data point, helping the LLM to align its output with the expected types and structures. Additionally, comments embedded in the draft help prevent common hallucinations, such as re-declaring functions or subroutines already defined elsewhere, by clearly indicating these constructs in advance.\nThe process converts basic variable types and multidimensional arrays, and uses the construct-query mechanism for YAML files to identify modules, functions, and subroutines used from elsewhere in the source code. It also inserts comments into the draft for LLM to parse. For example, if a Fortran file <filename>.f90 contains a scalar variable declaration real:: funcvar but uses it as a function funcvar (i, j), it may indicate that the variable is defined elsewhere in the code as a function or defined explicitly in the file as a statement function. In such cases, the LLM is given enough context to either reference the function from an appropriate header file or generate a corresponding C++ lambda function. Similarly, when modules are imported via a use statement or subroutines are called with a call statement, the draft includes comments to guide the LLM in correctly handling these constructs. The draft code is saved with to <filename>.scribe file."}, {"title": "4.4 Translate", "content": "The core functionality of [Code-Scribe] lies in its AI-powered translation. This command transforms Fortran source code into C++ while generating the necessary Fortran-C interfaces for smooth integration between legacy and modern codebases. The translation leverages generative AI to interpret Fortran's syntax and semantics. The seed prompt from the chat template is appended with code from the source and draft files and sent to the Al for chat completion. The source code from the Fortran file is supplied within the elements <source>...</source>, and the draft code is enclosed within <draft>...</draft>. The LLM is instructed to produce output for the C++ source and the corresponding Fortran-C interface, enclosing them within <csource>...</csource> and <fsource>...</fsource>. This mechanism is based on the implementation by the Slack team in their recent article [20], allowing for source code extraction and saving to separate files named <filename>.cpp and <filename>_fi.f90.\nInteraction with the LLM is achieved using one of the three mechanisms employed in the Inspect command. Utilizing direct API endpoints or the Transformers library enables automated extraction and creation of C++ and Fortran interface files, while the interactive method of using JSON files with ChatGPT requires manual copy-pasting of relevant source code into its respective file."}, {"title": "5 LLM Sensitivity Studies", "content": "We explored the application of [Code-Scribe] to the following scenarios within scientific computing:\n(1) Converting the MCFM code: MCFM is a parton-level Monte Carlo program that provides predictions for a wide range of processes at hadron colliders [7-9]. The code is primarily written in Fortran and requires complete conversion to C++ to enhance its interoperability with other codes and libraries in high-energy physics. The codebase consists of approximately 450-500 source files, each containing 50-70 lines of code.\n(2) Developing C++ interface for Noah-MP: Noah-MP [16] is a Fortran library that models urban canopy parameterization for atmospheric simulations. Integrating it with the Energy Research and Forecasting (ERF) Model [2] is essential for enhancing the latter's applicability. However, ERF is written in C++. We utilized [Code-Scribe] to create interfaces for this integration.\nThe discussion is dominated by MCFM code conversion because that was not only the motivator for the development of [Code-Scribe] but has seen its most extensive use. As of this writing the entire code has been converted to C++. The conversion process initially averaged 2-3 files per day before the development of the tool. After the tool was implemented, developer productivity increased to 10-12 files per day. Below are a few additional statistics gleaned from this work."}, {"title": "5.1 Model Sensitivity Study", "content": "For this study we focused on evaluating the performance of various GenAI models and provide a quantitative assessment of the developer time required to verify and test the code after LLM conversion. The raw source files used in this analysis are available in [12], and for the rest of this discussion, we will refer to the data repository as <results>.\nConsider the example target Fortran file:\n<results>/model-sensitivity/target.f\nWe utilized five models for the code conversion task: CodeLlama-7B, Mistral-7B, CodeLlama-34B, GPT-40, and GPT-3.5 Turbo, each with distinct architectures, parameter counts, and capabilities. CodeLlama-7B and CodeLlama-34B, based on Meta's LLaMA architecture [6], are optimized for coding tasks, differing primarily in parameter size (7 billion for CodeLlama-7B and 34 billion for CodeLlama-34B). Mistral-7B [1], another open-source model with 7 billion parameters, employs a dense attention mechanism, achieving efficiency comparable to larger models. A key advantage of these models is their capability to run locally using organizational resources; for instance, we executed them on NVIDIA A100 GPUs using the Swing system at Argonne [5].\nGPT-40 and GPT-3.5 Turbo, developed by OpenAI, are general-purpose models accessible via API endpoints, requiring the purchase of API credits. GPT-40 is a multimodal model capable of processing both text and images, and although OpenAI has not disclosed its architecture or parameter count, it is widely speculated to have significantly more parameters than other models. GPT-3.5 Turbo is smaller and optimized for cost-effective, high-speed text generation. While GPT-40 is well-suited for complex multimodal tasks and excels in multi-step reasoning, CodeLlama and Mistral are specialized for coding tasks and do not support multi-step reasoning to the same extent as GPT-40. This distinction is critical when evaluating model performance in specific use cases, such as the MCFM code conversion, where the ability to engage in multi-step reasoning can significantly enhance the translation process.\nFigure 5 provides an overview of the performance of different models in terms of developer time (in minutes) for converting and testing a single Fortran source file in MCFM to its corresponding C++ source and Fortran-C interface. The raw source files for each model's output, along with the reference solution, can be found in the data repository at following locations:\n<results>/model-sensitivity/codellama-7b\n<results>/model-sensitivity/mistral-7b\n<results>/model-sensitivity/gpt-3.5-turbo\n<results>/model-sensitivity/gpt-40\n<results>/model-sensitivity/reference\nAnd the corresponding chat completion template is stored in:\n<results>/model-sensitivity/seed_prompt.toml\nAll models operated under a maximum token limit of 4096, with CodeLlama and Mistral models utilizing a maximum batch size of 8 and default values for temperature and top-p parameters.\nThe results reveal distinct strengths and limitations among the models. CodeLlama-7B did not generate the necessary Fortran interface code and failed to enclose the code within the specified <csource> and <fsource> elements. Additionally, it struggled to separate the main implementation from the extern C interface in the C++ source and could not convert the statement function zab2 to a lambda function. As a result, integrating the generated files with the rest of the codebase took a total of 13.8 minutes for review and 7 minutes for testing.\nIn contrast, Mistral-7B exhibited better structural performance, successfully placing the code in the appropriate elements and separating the main implementation from the extern C interface. However, it hallucinated the Fortran interface by copying code directly from the target. f file and, like CodeLlama-7B, failed to convert the statement function zab2. Additionally, it performed an incomplete conversion for the variable Fcc. The developer time required for Mistral-7B was comparable to CodeLlama-7B, with 12.8 minutes spent in review and 7 minutes in testing.\nGPT-3.5 Turbo demonstrated strong overall performance, meeting the requirements outlined in the chat template by generating the Fortran interface, the extern C interface, and the main implementation. This model notably improved review and testing times but failed to properly convert the statement function zab2 into a lambda function. Instead, it incorrectly transformed zab2 into a multidimensional array, misinterpreting the instructions provided in seed_prompt. toml. Despite this issue, it required only 4.26 minutes in review and 2.3 minutes in testing, reflecting how better adherence to the template reduced overall time spent on manual adjustments and testing iterations.\nGPT-40 slightly outperformed GPT-3.5 Turbo by following the conversion rules for the statement function; however, the conversion of zab2 still resulted in inconsistencies in its usage throughout the C++ source file, requiring manual intervention. The review time for this model was 2.5 minutes, slightly better than GPT-3.5 Turbo, but the testing time remained comparable.\nFigure 6 provides excerpts from files converted by different models and compares them to the reference solution. These findings underscore the varying capabilities of each model in handling the MCFM code conversion, providing insights into which GenAI approaches may be most effective for software development tasks within scientific computing. GPT-40 demonstrated the best overall performance, positioning it as well-suited for the code conversion task. However, the time spent on code review was nearly equivalent to the time required for testing, suggesting inefficiencies that must be managed to reduce developer overhead. To mitigate this, we often batch the translation, review, and testing of multiple files simultaneously, optimizing the overall process and helping manage developer costs more effectively."}, {"title": "5.2 RAG Sensitivity Study", "content": "In this study, we focus on how retrieval-augmented generation (RAG) enhances code translation performance. RAG has previously been shown to improve the accuracy of natural language processing (NLP) tasks, as demonstrated by Lewis et al. [17]. To evaluate its effectiveness in code translation, we applied RAG elements within [Code-Scribe], specifically during the Draft command. In this step, a target.scribe file is created, and RAG is employed by parsing YAML index files and annotating the draft with prompts that provide context to the LLM, particularly about the presence of external functions.\nFigure 7 and 8 present excerpts from two different scribe files, with and without RAG elements, alongside AI-generated C++ code. These files are available in the results repository:\n<results>/rag-sensitivity/with-function-context\n<results>/rag-sensitivity/without-function-context\nThe results demonstrate that when provided with the context of external functions-lnrat, L0, L1, Lsm1-the Al is able to detect their use and avoid declaring them explicitly. Without this context, however, the functions are declared as they appear in the target.f file, found at:\n<results>/rag-sensitivity/target.f\nIn Fortran, external functions can be declared as variables, and the compiler identifies them during linkage. However, in C++, this leads to linkage errors. While the RAG-enhanced draft file annotations improve accuracy, the results are inconsistent, as the LLM occasionally hallucinates despite being explicitly informed of the use of external functions. Our ongoing efforts focus on mitigating these hallucinations by implementing a more robust RAG framework."}, {"title": "6 Ongoing Work", "content": "In our ongoing work, we are applying the code conversion techniques developed for MCFM to design a Fortran-C interoperability layer for Noah-MP and ERF, with the goal of incorporating Urban Canopy parameterization into ERF simulations. Additionally, we are leveraging a RAG-based approach to develop modules for ERF, utilizing information from the existing codebase to seamlessly integrate new physics with the overall system. This approach will introduce a new functionality called Generate into [Code-Scribe], which will assist in writing new modules by drawing on context from the surrounding code.\nTo enhance developer productivity during code conversion, we plan to expand the Inspect command to automatically generate seed_prompt.toml chat templates and to integrate [Code-Scribe] with the LASSI framework [10] to improve the correctness of the generated code."}, {"title": "7 Conclusions", "content": "In this study we presented [Code-Scribe], a GenAI-driven tool designed to facilitate the translation of legacy Fortran code to modern C++ while creating necessary interfaces for seamless integration. Our evaluations demonstrate that [Code-Scribe] reduces the developer time required for code conversion and testing, showcasing its potential to enhance productivity in scientific computing projects.\nWe explored the effectiveness of various Al models for code conversion of the MCFM codebase, revealing distinct strengths and weaknesses across different architectures. While GPT-40 emerged as the most effective model in this context, the performance of GenAI in code conversion also revealed opportunities for optimization, particularly concerning the manual review and testing processes that often accompany such translations.\nIn addition to the MCFM code conversion, our ongoing efforts focus on applying [Code-Scribe] to meet developer requirements across various scientific computing projects, such as Noah-MP and Flash-X, by enhancing existing commands and introducing new functionality.\nAs we continue to refine [Code-Scribe] and explore its applications, our aim is to further streamline the translation process and reduce overhead associated with code review and testing by exploring its integration with the LASSI framework developed at Argonne. We envision [Code-Scribe] as a valuable tool that empowers developers in scientific computing to leverage GenAI effectively."}]}