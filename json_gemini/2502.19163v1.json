{"title": "TestNUC: Enhancing Test-Time Computing Approaches through\nNeighboring Unlabeled Data Consistency", "authors": ["Henry Peng Zou", "Zhengyao Gu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Liancheng Fang", "Yibo Wang", "Yangning Li", "Kay Liu", "Philip S. Yu"], "abstract": "Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications.", "sections": [{"title": "Introduction", "content": "Test-time computing approaches, which leverage additional computational resources during inference to enhance performance, have gained increasing attention in the era of large language models (LLMs). There are two primary strategies for modifying an LLM\u2019s distribution at test time: (1) at the input level: augmenting the prompt with additional tokens (e.g., few-shot in-context learning); or (2) at the output level: sampling multiple candidate answers and aggregating them (e.g., self-consistency, best-of-N). Despite demonstrating promising capabilities, input-augmentation approaches incur a computational cost that scales quadratically with the number of added tokens in the prompt, making them more computationally expensive than output-sampling methods. Meanwhile, output-sampling approaches typically overlook the potential of large amounts of unlabeled data that are often available in real-world settings.\nTo bridge these gaps, we present an initial exploration of how unlabeled data can be efficiently leveraged to enhance test-time computing approaches. We hypothesize that instances with similar embeddings are likely to share the same semantic label, which can provide unsupervised signals for improving inference consistency, particularly for challenging instances. Our pilot experiments across various benchmarks reveal strong semantic label consistency among neighboring instances, and we find that aggregating these neighborhood labels through simple aggregation methods such as majority voting leads to stable and accurate predictions (as shown in Figure 2, 3 in Section 2).\nMotivated by these findings, we propose Test-NUC, a simple yet effective approach that enhances test-time LLM predictions by leveraging neighboring unlabeled data consistency. Concretely, Test-NUC consists of two key steps: \u25cf Neighbor Retrieval, where we identify the top-K nearest unlabeled neighbors of a test sample based on feature similarity; and \u25cf Collaborative Prediction, where the LLM generates predictions for both the test sample and its retrieved neighbors, which are then aggregated to obtain the final answer. The intuition behind TestNUC is that samples in close proximity within the embedding space are likely to share similar labels. By incorporating predictions of nearby unlabeled samples, the LLM can exploit the consistency of local data structures to better contextualize and refine its decision-making, effectively using unlabeled examples as an auxiliary signal to boost test-time performance while reducing noise and uncertainty.\nWe evaluate our approach across diverse tasks, including intent classification, topic mining, domain discovery, and emotion detection, using eight datasets that cover a wide spectrum of granularities, with class sizes ranging from 10 to 150. Our results demonstrate that TestNUC consistently outperforms baseline methods, such as standard prompting and self-consistency, by a large margin across four large language models, showing its effectiveness in leveraging unlabeled data for test-time computation. Moreover, Test-NUC can be seamlessly integrated with existing test-time computing approaches, such as TopK-ICL, best-of-N and self-consistency, significantly boosting their performances (as illustrated in Figure 1). In addition, TestNUC is effective across various embedders of different sizes and scales well with increasing amounts of unlabeled data (as shown in Figure 5), making it applicable to real-world scenarios."}, {"title": "Preliminary Analysis", "content": "Leveraging neighboring examples at inference time has been shown to improve the generalization of language models, mitigate prompting bias, and improve retrieval-augmented generation. Building on these findings, we explore a more focused question: To what extent can semantically similar neighborhood data serve as effective prediction proxies and potentially enhance LLM predictions at test time?\nTo understand this, we introduce neighborhood purity, which measures how often semantically similar examples share the same label. Formally, let \\(D = \\{(x_i, y_i)\\}_{i=1}^{N}\\) be a set of inputs and corresponding ground truth labels, where N is the total number of data points. We denote the K-nearest neighborhood of an input x as \\(\\mathcal{N}_K = \\text{argtopk}\\{S_f(x, x_i) \\mid i = 0, ..., N\\}\\), representing the set of indices corresponding to the most similar instances according to an embedding function f. We refer to x as the anchor of the neighborhood and measure the consistency of its neighborhood with purity \\(\\Phi\\), defined as:\n\\begin{equation}\n\\Phi(\\mathcal{N}) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j \\in \\mathcal{N}_K} \\mathbb{1}(y_i = y_j) \\tag{1}\n\\end{equation}\nIntuitively, purity measures the proportion of instances that share the same label as the anchor.\nWe conduct our preliminary experiments across eight datasets spanning class granularities from 10 to 150. As shown in Figure 2, nearest neighbors frequently belong to the same semantic class as the anchor. In the worst"}, {"title": "Method", "content": "Motivated by our findings in Section 2, we propose TestNUC, a test-time computing strategy that leverages neighboring unlabeled data consistency to enhance LLM predictions. Our approach introduces a complementary dimension to test-time computing by integrating signals from unlabeled data during inference.\nTestNUC consists of two key steps:\n\u2022 Step 1: Neighbor Retrieval. Identify the top-K nearest neighbors of a test sample based on feature similarity.\n\u2022 Step 2: Collaborative Prediction. Prompt the LLM to generate predictions for both the test sample and its K retrieved neighbors. These predictions are combined through a designed aggregation strategy.\nNote that TestNUC is based on LLM predictions instead of the ground truth label. The intuition behind TestNUC is that samples in close proximity within the embedding space are likely to share similar labels. By incorporating predictions on nearby unlabeled samples, the LLM can better contextualize and refine its decision-making. This approach aims to exploit the consistency of local data structures, effectively using unlabeled examples as an auxiliary signal to boost inference-time performance and reduce the noise and uncertainty associated with isolated predictions."}, {"title": "Aggregation Strategy", "content": "The aggregation strategy in Step 2 affects the sensitivity of TestNUC to noise. In this work, we explore three types of aggregation strategies.\nNaive Majority Voting. The naive approach simply selects the most consistent answer across the K unlabeled data predictions.\nWeighted Majority Voting. As demonstrated in our analysis in Section 2, when using a large K, neighborhood purity tends to decline rapidly. This indicates that distant neighbors can introduce significant noise and negatively impact the accuracy of majority voting. To mitigate this issue, we additionally use cosine similarity distance between the test sample and its neighbors as weights for majority voting.\nFiltered Weighted Majority Voting. The quality of LLM\u2019s predictions for neighboring unlabeled data can affect the accuracy of the aggregated results. In this approach, we explore leveraging verbalized confidence to filter out low-quality predictions during majority voting. Specifically, for each unlabeled data, we ask LLM to generate both the prediction and confidence in its predictions and only high confidence predictions are kept for majority voting.\nA complete algorithm for Filtered Weighted Majority Voting is presented in Algorithm 1. The algorithms for the other two voting strategies mentioned above can be obtained by removing the blue- and red-colored code. More complex aggregation strategies can also be explored, such as adding additional distance-based filtering mechanisms or confidence-weighting mechanisms, which we leave for interested researchers to explore."}, {"title": "Experiments", "content": "Tasks and Datasets. We consider eight datasets across diverse tasks with various perspectives and granularities as follows.\n\u2022 Intent Detection. Intent detection aims to discover fine-grained intents in customer utterances.\n\u2022 Topic Mining. We use Reddit and StackExchange from MTEB and ClusterLLM to evaluate models\u2019 ability to categorize discussion topics.\n\u2022 Domain Discovery. For this task, we use MTOP and CLINC(D) to allow evaluations of models\u2019 capability in discovering domain-specific knowledge.\n\u2022 Type Discovery. We use the FewEvent dataset that focuses on extracting event types from the given text and event triggers.\n\u2022 Emotion Recognition. We use GoEmotion, which is a dataset of Reddit comments labeled with fine-grained emotions, such as amusement, fear and gratitude.\nBaselines. We consider three types of baselines:\nStandard Prompting, which prompts the LLM in a standard way to select a label from the provided options to a test sample.\nTest-time computing approaches that operate at the input level by augmenting the given prompt with additional demonstrations to enhance inference performance. Since our proposed method combines decisions based on similar examples, we compare it with two varieties of in-context learning counterparts: TopK-ICL, where the input text of the nearest neighbors of the test example are added to the prompt as context information. TopK-ICL-P, where we additionally append each neighbor\u2019s Standard Prompting prediction result to its text as demonstrations.\nTest-time computing approaches that operate at the output level through multiple candidate answer sampling and aggregation to boost output quality. For this category, we consider three representative approaches: Self-Consistency, Best-of-N and Weighted Best-of-N. Specifically, Best-of-N selects the most confident predictions out of multiple predictions based on the LLM\u2019s own verbalized confidence. Weighted Best-of-N aggregates the decisions by assigning weights based on their respective confidence score."}, {"title": "Main Results", "content": "Comparison with Standard Prompting and Self-Consistency. Table 1 presents the comparison results with Standard Prompting and Self-Consistency across four large language models. It can be observed that TestNUC significantly improves the inference performance of four large lan-"}, {"title": "Influence of Unlabeled Data Size", "content": "Increasing unlabeled data helps boost performance across tasks. Figure 4 reports the linear-scale results on BANKING, CLINC, Reddit, StackEx, FewEvent, and an overall average across eight tasks. In all cases, increasing the unlabeled set yields notable accuracy improvements for GPT-40-mini, Llama-3.1-8B, and Claude-3-Haiku. Adding even a modest number (e.g., 500\u20131k) of unlabeled instances yields substantial accuracy gains, especially on BANKING and Reddit. However, improvements taper off after roughly 8-10k unlabeled samples, suggesting a saturation point where additional unlabeled data provides diminishing returns. The log-scale plots in Figure 5 further highlight these trends, confirming that the utility of unlabeled examples gradually diminishes but still delivers meaningful improvements up to the 10K-15K range. This pattern holds consistently across all tasks, confirming that increasing unlabeled data universally improves"}, {"title": "Aggregation Strategy Comparison", "content": "We find that naive majority voting greatly surpasses standard prompting performance, with advanced strategies further enhancing results. As shown in Table 3, simply aggregating multiple predictions with naive majority voting can already boost average accuracy significantly from 0.613 to 0.670. Introducing distance and confidence weighting further refines these gains from the average to 0.680. Finally, filtering out low-confidence predictions yields the highest performance, although on certain tasks (e.g., GoEmotion), Weighted Majority Voting (Distance & Confidence) can be more effective, suggesting that a carefully tuned confidence threshold may be necessary for each dataset."}, {"title": "Varying Embedding Models", "content": "TestNUC works on different sizes of embedders. The embedders used to generate data embeddings for neighbor retrieval play a crucial role in the success of TestNUC. In this work, we explore diverse embedding models, including public encoders from different companies and embedders of different sizes ranging from 120M to 7B. As shown in Table 4, TestNUC is effective when applied to various embedding models. Not surprisingly, TestNUC achieves significant improvements with larger and more advanced embedders, such as NV-Embed-v2-7B, which records the highest average performance (0.755) and excels across all datasets. Mid-sized embedders like stella-en-400M-v5 (0.738) and gte-Qwen2-1.5B-instruct (0.732) also perform well, demonstrating that TestNUC can effectively leverage diverse embedding architectures. Even smaller models, such as all-MiniLM-L12-v2-120M (0.720), deliver competitive results over standard prompting (0.682), showcasing TestNUC\u2019s robustness across varying model sizes and complexities."}, {"title": "Influence of Neighbor Size", "content": "Influence of the Number of Neighboring Unlabeled Data. Figure 6 shows the results of TestNUC from GPT-40-mini, Llama-3.1-8B, and Claude-3-Haiku as the number of neighbors increases. The results show that even a small set of neighbors can significantly boost performance for all three models, significantly surpassing their zero-neighbor baselines. Additionally, all three models generally benefit from increasing the number of neighbors from 0 to 60, although the gains tend to plateau after approximately 40\u201360 neighbors. Notably, the performance of GPT-40-mini and Llama-3.1-8B slightly decreases when the number of neighbors increases from 60 to 80 on certain datasets, likely due to the introduction of more noisy neighbors. In contrast, Claude-3-Haiku often achieves higher accuracies with relatively larger neighborhood sizes (e.g., 60\u2013100), indicating greater robustness to noise."}, {"title": "Related Work", "content": "Test-time compute improves LLM performance by modifying the prediction distribution during test time. Such modification is usually accompanied with extra computational cost. Instead of decoding greedily, the model may sample multiple decoding paths before aggregating them into a response. Chain of Thought modifies the output distribution through hand-crafted prompts that contain reasoning chains. Self-consistency samples multiple chain-of-thought paths and aggregate the sample with majority voting. observed improved accuracy and robustness by querying the model with semantically equivalent prompts before responding with the majority answer. uses sentence embeddings to retrieve k-nearest-neighbor demonstration for in-context learning. retrieves relevant and diverse demonstrations by training a model that predicts the relevance of a demonstration via contrastive learning. Our work is directly inspired by the KNN method proposed by. Later work has revealed that similarity based demonstration retrieval improves in-context learning because LLMs attend to the most similar demonstration during few-shot prompting. Instead of using similar demonstrations for in-context learning, we explore using them as near neighbors in the fashion of non-parametric prediction."}, {"title": "In-Context Learning", "content": "Apart from Chain-of-Though, many work explore the possibility of using self-generated content by the LLM to aid with reasoning or classification. STaR iteratively add self-generated rationales that are proved correct by a verifier to the exist pool of demonstrations. A significant limitation of STaR is that it relies on knowing the correct answer to the questions the LLM is generating rationale for. Our method simply make predictions for neighboring examples,"}, {"title": "Conclusion", "content": "In this work, we introduced TestNUC, a simple yet effective approach that leverages the consistency of neighboring unlabeled data to enhance test-time predictions in large language models. Extensive experiments across eight datasets and multiple LLMs demonstrate that TestNUC consistently outperforms baselines like standard prompting and self-consistency. It can be seamlessly integrated with existing methods such as TopK-ICL, self-consistency, and best-of-N to yield further gains. These results highlight the practical value of leveraging unlabeled data during inference, which not only boosts label consistency but also offers a scalable path to better generalization in real-world applications where labeled data may be scarce."}, {"title": "Limitation", "content": "Our evaluation of TestNUC is limited to classification tasks and does not include generative tasks. We leave this extension for future work. Due to computational resource constraints and limited budgets, we did not evaluate recent powerful reasoning models such as o3-mini and DeepSeek-R1."}]}