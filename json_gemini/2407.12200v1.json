{"title": "This Probably Looks Exactly Like That: An Invertible Prototypical Network", "authors": ["Zachariah Carmichael", "Timothy Redgrave", "Daniel Gonzalez Cedre", "Walter J. Scheirer"], "abstract": "We combine concept-based neural networks with generative, flow-based classifiers into a novel, intrinsically explainable, exactly invertible approach to supervised learning. Prototypical neural networks, a type of concept-based neural network, represent an exciting way forward in realizing human-comprehensible machine learning without concept annotations, but a human-machine semantic gap continues to haunt current approaches. We find that reliance on indirect interpretation functions for prototypical explanations imposes a severe limit on prototypes' informative power. From this, we posit that invertibly learning prototypes as distributions over the latent space provides more robust, expressive, and interpretable modeling. We propose one such model, called ProtoFlow, by composing a normalizing flow with Gaussian mixture models. ProtoFlow (1) sets a new state-of-the-art in joint generative and predictive modeling and (2) achieves predictive performance comparable to existing prototypical neural networks while enabling richer interpretation.", "sections": [{"title": "1 Introduction", "content": "Concept-based neural networks offer an attractive way of parsing decisions made by complex systems. By providing explanations in terms of higher-level abstractions, these models provide semantic clarity to both experts and non-experts. While there are myriad different interpretations of the word concept in the literature [62], we are interested particularly in prototypical concepts. Prototypes aim to distill traits not directly scrutinizable from the raw data. For instance, the prediction of an image as a bird could be explained by its similarity to a beak prototype [9]. Explainability-by-design guarantees faithful explanations by requiring prototype involvement in decision-making [61]. Such explanations naturally result from the symbolic form of the model [65] rather than post hoc correlational analyses [45], a preferable situation for most users [10, 11, 40].\nExisting prototypical networks have demonstrated a human-machine semantic similarity gap and often learn irrelevant prototypes [35, 40, 70]. These net-"}, {"title": "2 Background", "content": "We cover related work on concept-based neural networks and normalizing flows."}, {"title": "2.1 Concept-Based and Prototypical Neural Networks", "content": "Concept-aware models [62] are designed to produce high-level explanations that are more directly meaningful to human examiners. The semantic quality of these explanations is naturally borne by the model's analytic form [61, 65] rather than uninformed post hoc correlations [45], which may not be meaningful [64]. Fully supervised approaches to concept learning rely on labeled training data that is fully annotated, indicating for example the presence of bone spurs in medical images or the colors of individual body parts in photographs of birds. Notable examples include the concept bottleneck model [43] and the concept embedding model [17]. Such approaches are hindered by the cost and constraints of human labor, restricting their use in unsupervised settings. While weakly supervised tasks, which don't have annotations, may retain class labels, fully unsupervised tasks have neither. This paper will focus on weakly supervised learning with prototypical neural networks [9, 47], the predominant approach in this setting. Though they are related to several other tasks in the literature, including data summarization [29, 61], example-based post hoc explanation [20, 61], and few-shot learning [71], our goal here is to learn prototypes for intrinsically explainable case-based reasoning [9, 47].\nPrototype networks provide local explanations by relating decisions made on input data to discriminative abstractions called prototypes. In Fig. 2, we illustrate how images can be explained with (a) whole-image prototypes and (b) prototypical parts corresponding to specific portions of an image. Decisions are made by a composite mapping \\(X \\rightarrow Z \\rightarrow C\\) that sorts the data \\(x \\in X\\) into mutually exclusive classes \\(c \\in C\\). This first involves learning a transformation \\(a_z\\) of our Euclidean data space \\(X\\) into a latent inner-product space \\(Z\\) whose parameters specify a set of prototypes related to the class labels. A classifier \\(z_c\\) then makes decisions based on a similarity kernel \\(K: Z \\times Z \\rightarrow R\\) imposed on the learned representations.\nCritically, the latent representations \\(z\\) don't exist in the same space as our data points \\(x\\). So, if \\(X\\) is a space of images for example, nothing so far gives us a way to visually understand how images nor prototypes are represented in \\(Z\\). This requires an interpretation transform \\(Z \\rightarrow X\\) going in the other direc-"}, {"title": "2.2 Normalizing Flows", "content": "A normalizing flow is an unsupervised density estimator \\(f : X \\rightarrow Z\\) that invert-ibly transforms between the data and latent spaces. Empirical information from data flows to the latent space through \\(f\\), and density inferences are recovered by the inverse \\(f^{-1} : Z \\rightarrow X\\). Given a data distribution \\(p_x\\) and a latent distribution \\(p_z\\) over \\(X\\) and \\(Z\\) respectively, the impact of changing variables [56] on a random variable \\(x \\sim p_x\\) via \\(f(x) = z\\) is given by the following formula.\n\\[p_x(x) = p_z(f(x)). \\text{det}\\left(\\frac{\\partial f}{\\partial x}\\right)\\]\nThis allows us to formally specify the unknown data distribution \\(p_x\\) as the pushforward of a latent distribution \\(p_z\\), with the inverse transformation \\(f^{-1}\\) responsible for \"pushing\" the latent density \\(p_z\\), which is typically taken to be well-behaved, \"forward\" onto the more unwieldy data distribution \\(p_x\\). With full knowledge and control over \\(p_z\\), we can leverage \\(f^{-1}\\) as a generative model for sampling from \\(p_x\\) implicitly through the transformation \\(f^{-1}(z) \\sim p_x\\) where \\(z \\sim p_z\\). We call this model a \"normalizing flow\" because \\(f\\) \u201cnormalizes\u201d the complicated data distribution by \u201cflowing\u201d information into the latent space [56].\nThe mapping \\(f\\) is typically implemented as a composition of invertible functions learned by neural networks. A neural network can be made invertible by constructing a discrete- or continuous-time flow, such as coupling flows, autoregressive flows, linear flows, and planar flows [42]. The model can then be trained by maximum likelihood estimation as in Eq. (1)."}, {"title": "2.3 Joint Generative and Predictive Modeling", "content": "Invertible neural networks have already seen use for classification tasks [5, 18, 39, 51] partly due to their memory efficiency during training. However, our focus here is on joint generative and predictive modeling. Fetaya"}, {"title": "3 Invertible Prototypical Networks", "content": "We propose an intrinsically interpretable approach to learning that bridges together generative classification with concept-based networks\u2014two key ideas from the explainable AI and generative AI literature. As described in Sec. 2, we aim to invertibly learn latent prototypical distributions. In this section, we first describe the normalizing-flow backbone of our architecture. Then, we detail how prototypical distributions are learned over the latent space. Finally, we discuss our training methodology for ProtoFlow, our proposed architecture for joint predictive and generative modeling. An overview of ProtoFlow is given in Fig. 3.\nWe base our normalizing flow on DenseFlow [28], a state-of-the-art unconditional density estimator [57]. DenseFlow is comprised of invertible Glow-like [41] modules using cross-unit coupling and densely con-"}, {"title": "Experiments and Analysis", "content": "We evaluate ProtoFlow on a variety of image classification datasets used to evaluate prior joint predictive and generative modeling approaches: MNIST [13],"}, {"title": "A Reproducibility", "content": "ProtoFlow is trained on machines with up to four NVIDIA A10 GPUs in parallel and an Intel Xeon Gold 6326 CPU. We employ the following data augmentation scheme to each image during training.\n1. Resize the image's shortest axis to 32 or 64 pixels using bicubic interpolation.\n2. Apply a horizontal reflection randomly with probability 1/2.\n3. Pad the image appropriately by duplicating pixels along the edges.\n4. Randomly perform one or more of the following affine transformations:\na rotation of \\(\\theta^{\\circ}\\) degrees, where \\(\\theta^{\\circ} \\in [-15^{\\circ}, 15^{\\circ}]\\)\na horizontal/vertical translation by \\(t\\%\\) pixels, with \\(t\\% \\in [-4\\%, 4\\%]\\)\na shear transformation (transvection) of \\(a^{\\circ}\\) degrees, with \\(a^{\\circ} \\in [-10^{\\circ}, 10^{\\circ}]\\)\n5. Center-crop the image to 32 \u00d7 32 or 64 \u00d7 64 resolution.\nThe DenseFlow architecture was instantiated with a block configuration of [6, 4, 1], layer middle channels of [48, 48, 48], and a layer configuration of [5, 6, 20]. During training, we use a single Monte Carlo sample in the forward pass of DenseFlow. During testing, we report results with 100 Monte Carlo samples. Furthermore, we use test-time augmentation, using the same augmentation composition as during training. For each sample, we average logits and likelihood estimates across five augmentations. All models were trained for 150 epochs unless oth-erwise specified. For prototypical parts, we use a patch size of 4 x 4 pixels. All other hyperparameters are listed in Tab. 4."}]}