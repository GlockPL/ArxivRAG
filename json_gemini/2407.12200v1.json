{"title": "This Probably Looks Exactly Like That: An Invertible Prototypical Network", "authors": ["Zachariah Carmichael", "Timothy Redgrave", "Daniel Gonzalez Cedre", "Walter J. Scheirer"], "abstract": "We combine concept-based neural networks with generative, flow-based classifiers into a novel, intrinsically explainable, exactly invertible approach to supervised learning. Prototypical neural networks, a type of concept-based neural network, represent an exciting way forward in realizing human-comprehensible machine learning without concept annotations, but a human-machine semantic gap continues to haunt current approaches. We find that reliance on indirect interpretation functions for prototypical explanations imposes a severe limit on prototypes' informative power. From this, we posit that invertibly learning prototypes as distributions over the latent space provides more robust, expressive, and interpretable modeling. We propose one such model, called ProtoFlow, by composing a normalizing flow with Gaussian mixture models. ProtoFlow (1) sets a new state-of-the-art in joint generative and predictive modeling and (2) achieves predictive performance comparable to existing prototypical neural networks while enabling richer interpretation.", "sections": [{"title": "1 Introduction", "content": "Concept-based neural networks offer an attractive way of parsing decisions made by complex systems. By providing explanations in terms of higher-level abstractions, these models provide semantic clarity to both experts and non-experts. While there are myriad different interpretations of the word concept in the literature [62], we are interested particularly in prototypical concepts. Prototypes aim to distill traits not directly scrutinizable from the raw data. For instance, the prediction of an image as a bird could be explained by its similarity to a beak prototype [9]. Explainability-by-design guarantees faithful explanations by requiring prototype involvement in decision-making [61]. Such explanations naturally result from the symbolic form of the model [65] rather than post hoc correlational analyses [45], a preferable situation for most users [10, 11, 40].\nExisting prototypical networks have demonstrated a human-machine semantic similarity gap and often learn irrelevant prototypes [35, 40, 70]. These networks typically learn prototypes as points in latent space, with limited interpretive power in data space [8, 40, 61, 68]. We hypothesize that this focus on points rather than distributions is an underlying issue; given a prototype of a blue jay, could we say the model is representing its blue color, its distinctive shape, the texture of its feathers, or something entirely different? It's impossible to tell from just one point. We propose to instead learn prototypical distributions over latent space with normalizing flows, capable of generating data while providing exact likelihoods [56, 67]. They are also fully invertible, furnishing the latent space with a faithful interpretation back to the data. We leverage this inverse transformation to provide meaningful insight into exactly how the model is learning to represent its training data, in turn allowing an understanding of latent prototypical distributions through their samples' corresponding data-space interpretations. This also removes constraints on learned prototypes [9, 61] and limitations on prototype visualizations [47, 49]. See Fig. 1 for an overview.\nIn this paper, we propose a novel approach to supervised learning that is interpretable by design by bridging together key ideas from the explainable AI and generative AI literature: generative classifiers and concept-based neural networks. We make the following contributions:\n\u2022 We develop an approach to learning interpretable latent prototypical distributions with a joint generative and predictive model.\n\u2022 We propose a diversity loss to reduce prototypical distribution overlap.\n\u2022 We evaluate our method on various image classification datasets and show state-of-the-art performance at joint predictive and generative modeling.\n\u2022 We qualitatively and quantitatively analyze explanations generated by the model. We obtain predictive performance comparable to existing prototypical neural networks while enabling richer interpretation."}, {"title": "2 Background", "content": "We cover related work on concept-based neural networks and normalizing flows."}, {"title": "2.1 Concept-Based and Prototypical Neural Networks", "content": "Concept-aware models [62] are designed to produce high-level explanations that are more directly meaningful to human examiners. The semantic quality of these explanations is naturally borne by the model's analytic form [61, 65] rather than uninformed post hoc correlations [45], which may not be meaningful [64]. Fully supervised approaches to concept learning rely on labeled training data that is fully annotated, indicating for example the presence of bone spurs in medical images or the colors of individual body parts in photographs of birds. Notable examples include the concept bottleneck model [43] and the concept embedding model [17]. Such approaches are hindered by the cost and constraints of human labor, restricting their use in unsupervised settings. While weakly supervised tasks, which don't have annotations, may retain class labels, fully unsupervised tasks have neither. This paper will focus on weakly supervised learning with prototypical neural networks [9, 47], the predominant approach in this setting. Though they are related to several other tasks in the literature, including data summarization [29, 61], example-based post hoc explanation [20, 61], and few-shot learning [71], our goal here is to learn prototypes for intrinsically explainable case-based reasoning [9, 47].\nPrototype networks provide local explanations by relating decisions made on input data to discriminative abstractions called prototypes. In Fig. 2, we illustrate how images can be explained with (a) whole-image prototypes and (b) prototypical parts corresponding to specific portions of an image. Decisions are made by a composite mapping \\(X \\rightarrow Z \\rightarrow C\\) that sorts the data \\(x \\in X\\) into mutually exclusive classes \\(c\\in C\\). This first involves learning a transformation \\(a_z\\) of our Euclidean data space \\(X\\) into a latent inner-product space \\(Z\\) whose parameters specify a set of prototypes related to the class labels. A classifier \\(z_c\\) then makes decisions based on a similarity kernel \\(K: Z\\times Z \\rightarrow R\\) imposed on the learned representations.\nCritically, the latent representations \\(z\\) don't exist in the same space as our data points \\(x\\). So, if \\(X\\) is a space of images for example, nothing so far gives us a way to visually understand how images nor prototypes are represented in \\(Z\\). This requires an interpretation transform \\(Z \\rightarrow X\\) going in the other direction, and several proposals have been offered in the literature. Li et al. proposed an autoencoder framework with \\(f\\) as the encoder and \\(f^{-1}\\) has the decoder [47]. This, however, is only capable of approximate reconstructions, with visualization fidelity susceptible to shifts in the latent prototypical distributions. As an alternative, Chen et al. proposed prototype projection [9]. Under this scheme illustrated in Fig. 1a-learned prototype representations are replaced by the image embeddings of their most similar training images. Although this guarantees an association between certain points in Z and training data from X, the fact that \\(f\\) is typically not injective often results in a human-machine semantic similarity gap [40] when multiple disparate data points are associated with the same latent point. Other approaches choose instead to compute the nearest-neighbor image embeddings to each prototypical point [49, 77]. Point-based approaches such as these have relatively low representation power due to (1) the necessary sparsity of high-dimensional spaces and (2) a dearth of variability measures [78].\nInstead of prototypical points, we propose learning prototypes as distributions over the latent space as shown in Fig. 1b. Moreover, we recover an intrinsic association between \\(Z\\) and \\(X\\) by learning these distributions invertibly with normalizing flows (cf. Sec. 2.2), providing us an implicit mapping \\(f^{-1} : Z \\rightarrow X\\) that interprets and visualizes \\(Z\\) through the eyes of \\(X\\).\nSomewhat similarly, Ma et al. [49] propose learning prototypical balls latent hyperspheres in Z rather than prototypical points. To interpret a given prototype, they visualize every training image whose latent representation lies inside the ball for that prototype. Proto-SegNet [26] models prototypes as the components of Gaussian mixture models (GMMs) learned over the latent space by a prototypical segmentation network; however, their prototype interpretations are limited to indirect visualizations involving nearest neighbors in the data. MGProto [78] again models prototypes as GMMs, this time choosing to visualize prototypes by applying prototype projection to the components' mean points. Peters [60] attempts to extract prototypical points from the latent space of a generative model using a normalizing flow generator; these points unfortunately look like noise, so nearest neighbors are used instead. Unlike prior work, we are able to faithfully visualize the full learned prototypical distributions due to our invertible normalizing flow backbone.\nWhile full-image prototypical neural networks are intrinsically explainable, the part-level explanations of existing prototypical part neural networks are typically considered post hoc [8, 24, 33, 68, 79]. The exceptions are models like PixPNet [8], ProtINN [60], and ProtoBBNet [26], which either impose constraints on the sizes of their receptive fields or process images by patches (using super-pixels or pre-defined grids, for example). See these papers [9, 24, 79] for more details on post hoc part-level explainability in prototype networks."}, {"title": "2.2 Normalizing Flows", "content": "A normalizing flow is an unsupervised density estimator \\(f : X \\rightarrow Z\\) that invertibly transforms between the data and latent spaces. Empirical information from data flows to the latent space through \\(f\\), and density inferences are recovered by the inverse \\(f^{-1} : Z \\rightarrow X\\). Given a data distribution \\(p_x\\) and a latent distribution \\(p_z\\) over \\(X\\) and \\(Z\\) respectively, the impact of changing variables [56] on a random variable \\(x \\sim p_x\\) via \\(f(x) = z\\) is given by the following formula.\n\\[p_x(x) = p_z(f(x)) \\cdot \\text{det}\\left(\\frac{\\partial f}{\\partial x}\\right)\\]\nThis allows us to formally specify the unknown data distribution \\(p_x\\) as the pushforward of a latent distribution \\(p_z\\), with the inverse transformation \\(f^{-1}\\) responsible for \"pushing\" the latent density \\(p_z\\), which is typically taken to be well-behaved, \"forward\" onto the more unwieldy data distribution \\(p_x\\). With full knowledge and control over \\(p_z\\), we can leverage \\(f^{-1}\\) as a generative model for sampling from \\(p_x\\) implicitly through the transformation \\(f^{-1}(z) \\sim p_x\\) where \\(z \\sim p_z\\). We call this model a \"normalizing flow\" because \\(f\\) \u201cnormalizes\u201d the complicated data distribution by \u201cflowing\u201d information into the latent space [56].\nThe mapping \\(f\\) is typically implemented as a composition of invertible functions learned by neural networks. A neural network can be made invertible by constructing a discrete- or continuous-time flow, such as coupling flows, autoregressive flows, linear flows, and planar flows [42]. The model can then be trained by maximum likelihood estimation as in Eq. (1)."}, {"title": "2.3 Joint Generative and Predictive Modeling", "content": "Invertible neural networks have already seen use for classification tasks [5, 18, 39, 51] partly due to their memory efficiency during training. However, our focus here is on joint generative and predictive modeling. Fetaya"}, {"title": "3 Invertible Prototypical Networks", "content": "We propose an intrinsically interpretable approach to learning that bridges together generative classification with concept-based networks-two key ideas from the explainable AI and generative AI literature. As described in Sec. 2, we aim to invertibly learn latent prototypical distributions. In this section, we first describe the normalizing-flow backbone of our architecture. Then, we detail how prototypical distributions are learned over the latent space. Finally, we discuss our training methodology for ProtoFlow, our proposed architecture for joint predictive and generative modeling. An overview of ProtoFlow is given in Fig. 3.\nWe base our normalizing flow on DenseFlow [28], a state-of-the-art unconditional density estimator [57]. DenseFlow is comprised of invertible Glow-like [41] modules using cross-unit coupling and densely connected blocks fused with Nystr\u00f6m self-attention, increasing expressiveness by incrementally augmenting latent vectors with noise. We replace DenseFlow's unconditional latent distribution with conditional distributions as follows.\nWhereas other prototypical neural networks usually learn latent points to represent their prototypes, our prototypes are given by probability distributions learned over the latent space. By leveraging the inverse mapping \\(f^{-1} : Z \\rightarrow X\\), these can be reinterpreted as probability distributions over data, providing a direct, faithful, and accurate visualizations of learned prototypes. For each class \\(c\\in C = \\{1, ..., C'\\}\\), we specify a \\(K\\)-component GMM \\(G_c\\) whose components represent prototypical distributions, so that each class has \\(K\\) associated prototypical distributions. The components are weighted by \\(\\pi_c = (\\pi_{c,1},..., \\pi_{c,K})\\).\n\\[G_c = \\sum_{k=1}^K \\pi_{c,k}G_{c,k} = \\sum_{k=1}^K \\pi_{c,k}\\mathcal{N} (\\mu_{c,k}, \\Sigma_{c,k})\\]\nThe class-conditional likelihood is given below, where is the softmax function.\n\\[p_z(z | y) = \\sum_{k=1}^K \\sigma(\\pi_{y,k}) \\mathcal{N}(z; \\mu_{y,k}, \\Sigma_{y,k})\\]\nAs is often done [25], we constrain \\(G_c\\) by (1) asserting \\(\\Sigma_{c,k}\\) are diagonal, (2) clipping \\(\\Sigma_{c,k}\\) above zero, and (3) enforcing \\(\\sum\\pi_c = 1\\). Applying Bayes' theorem to Eq. (3), we derive the expression below for the data-conditional class likelihood.\n\\[P_x(y | x) = \\frac{\\sum_{k=1}^K \\sigma(\\pi_{y,k}) \\mathcal{N}(f(x); \\mu_{y,k}, \\Sigma_{y,k})}{\\sum_{c=1}^C \\sum_{k=1}^K (\\pi_{c,k}) \\mathcal{N}(f(x); \\mu_{c,k}, \\Sigma_{c,k})}\\]\nRelating this back to our discussion in Sec. 2, we can induce the similarity kernel \\(k\\) at a given point \\(z \\in Z\\) by looking at the mean \\(\\mu_{c,k}\\) of the prototype that maximizes the class-conditional likelihood \\(\\kappa(z, \\mu_{c,k}) = p_z(z | y = C; \\pi_{c,k}, \\mu_{c,k}, \\Sigma_{c,k})\\). The classifier \\(g\\) can be written \\(g(f(z)) = \\text{argmax}_{y\\in c} P_x(y | f(z))\\) using Eq. (4). Finally, and most interestingly, our interpretation function \\(h\\) is simply the inverse transform \\(h = f^{-1}\\) induced by the normalizing flow. This imposes no additional constraints on the learned prototypes, unlike prior work [9, 47, 49, 77].\nWe train our model to maximize the categorical cross entropy, denoted \\(L_{CE}\\), with auxiliary loss terms. Since it is known to improve model robustness, we adapt the proposed consistency regularization loss from [37]. This encourages the model to be invariant to certain perturbations or augmentations of the training data by penalizing the model for predicting two different classes \\(\\hat{y} \\neq y\\) for two perturbations \\(x\\) and \\(\\hat{x}\\) of the same data point \\(x \\in X\\), expressed below.\n\\[L_{CR}(x, \\hat{x}) = -logp_x(\\hat{x} | y) = -log p_z (f(\\hat{x}) | y = \\hat{y}) - log \\text{det} \\left(\\frac{\\partial f}{\\partial x}\\right)\\]\nTo help support diversity and reduce information overlap between prototypes in each class, we penalize the components within each mixture based on their squared Hellinger distance \\(H^2\\). However, high-dimensional spaces' asymptotic sparsity sometimes referred to as the curse of dimensionality-limits the usefulness of \\(H^2\\): as the expected discernibility between points decreases, gradients tend to vanish. To mitigate this, we propose a modified divergence \\(H^2\\) rescaled based on the embedding dimension \\(d = dim(Z)\\). Given two multivariate Gaussians \\(\\mathcal{N}_1 = \\mathcal{N}(\\mu_1, \\Sigma_1)\\) and \\(\\mathcal{N}_2 = \\mathcal{N}(\\mu_2, \\Sigma_2)\\), the modified divergence \\(H^2(\\mathcal{N}_1, \\mathcal{N}_2)\\) between them-which is not a metric-is given below [48].\n\\[1 - \\frac{\\text{det} (\\frac{\\Sigma_1 \\Sigma_2}{\\Sigma_1 + \\Sigma_2})^{1/4d}}{\\text{det} (\\frac{\\Sigma_1 + \\Sigma_2}{2})^{1/2d}} exp \\left( -\\frac{1}{8d} (\\mu_1 - \\mu_2)^T (\\frac{\\Sigma_1}{\\Sigma_2} + \\frac{\\Sigma_2}{\\Sigma_1})^{-1} (\\frac{\\Sigma_1 + \\Sigma_2}{2})^{-1} (\\mu_1 - \\mu_2) \\right)\\]\nThis is bounded within [0,1] \\(\\subseteq \\mathbb{R}\\). The symmetry of Eq. (6) can be leveraged to avoid repetitive computations, yielding the following form for the diversity loss.\n\\[L_{DIV} (G) = \\frac{-2}{C K (K-1)} \\sum_{c=1}^C \\sum_{i=1}^{K-1} \\sum_{j=i}^K H^2 (G_{c;i}, G_{c,j})\\]\nWith hyperparameters \\(\\lambda_{CR}\\) and \\(\\lambda_{DIV}\\), the final training objective is given below.\n\\[L = L_{CE} (p_x(y | x), y) + \\lambda_{CR}L_{CR}(X, \\hat{x}) + \\lambda_{DIV}L_{DIV} (G)\\]\nThe collection of mixtures \\(G\\) can be trained either by stochastic gradient descent (SGD) or expectation maximization (EM). Empirically, however, we find that SGD results in better classification performance than EM. We explore K-means as an initialization strategy of the means of Gaussians. We initialize the K mean parameters of the mixture \\(G_c\\) responsible for modeling \\(p_z(z | y = c)\\) with the K-means clusters of the embeddings \\(\\{f(x) | (x, c) \\in (X,C)\\} \\). During training, we keep an exponential moving average (EMA) of the model parameters to reduce training time. This is a variant of Polyak averaging [63].\nNot all prototypes may end up being useful: they may be redundant, noisy, semantically meaningless, or contribute negligibly to prediction performance. Pruning such prototypes not only reduces not only the number of parameters in the model but also the quantity of information a user must digest to parse an explanation. We propose pruning prototypes based on the weights \\(\\pi_{c,k}\\) learned on their respective mixture components. Prototypes with weights below a minimal threshold \\(\\epsilon\\)-selected using Otsu's method [55] are discarded.\nWe propose a new approach to explaining prototypical parts. The original prototypical part network ProtoPNet [9] enforces a dimensional correspondence between \\(X\\) and \\(Z\\). They find prototypical parts by first computing an element-wise similarity between each embedded image z of shape \\(d\\times l'\\times w'\\) and a given prototypical point \\(p_k\\) that lives in a \\(d\\)-dimensional affine subspace of \\(Z\\), resulting in a similarity map for each prototype. These maps get upsampled to the image dimension \\(l \\times w\\) using bicubic interpolation. The top 5% scoring pixels in the original image are then finally selected as prototypical parts.\nWe take a different approach. Let \\(x \\sim p_x\\) be a random variable distributed according to our data-generating density, and let \\(x \\in \\{x_1,\\dots,x_N\\} \\subseteq X\\) be a query image. We introduce the notation \\([[x]_{i+l}^{j+w}\\) to mean the block of pixels from the image x with coordinates ranging \\(\\{i,\\dots i+l\\}\\) horizontally and \\(\\{j,\\dots j+w\\}\\) vertically. We then compute a heatmap \\(M_{c,k}\\) for each prototype \\(G_{c,k}\\) as follows.\n\\[\\bar{x} := \\frac{1}{N} \\sum \\sum [[x]_{i,j} \\quad \\newline\\newline i \\in \\{1, \\dots, l\\} \\newline \\newline j \\in \\{1, \\dots, w\\} \\text{w}\\]\n\\[\\bar{x} := \\frac{1}{N} \\sum_{i,j} [[x]_{i,j}\\newline\\newline \\bar{x} := \\frac{1}{N} \\sum_{i,j} [[x]_{i,j}\\]\n\\[(M_{c,k})_{i,j} \\leftarrow p_z(f(\\bar{x}) | G_{c,k})\\]\nOn line (9), we begin with an image \\(\\bar{x}\\) computed as the pixel-wise average of every image \\(x\\), in the training data. On line (10), we replace the image patch in \\(\\bar{x}\\) specified by the rectangle with lower-left corner at \\((i, j)\\) and upper-right corner at \\((i+l,j+w)\\) with the corresponding patch of pixels from a given query image \\(x\\), leaving the rest of \\(\\bar{x}\\) as it was. This results in an image whose background is averaged across the entire dataset containing a single patch from a query image. On line (11), we then evaluate \\(p_z(f(\\bar{x}) | G_{c,k})\\) and replace the corresponding patch in \\(M_{c,k}\\) with the prototype-conditional likelihood of this patched image. Like ProtoPNet, our network's receptive field is the full input, so it would be inappropriate to attribute a subset of pixels to a particular prototype. However, we can emphasize the top 5% of pixels to indicate the most influential image parts on a classification decision. Since we assume independence between image patches, this is an approximate, post hoc method in the same vein as ProtoPNet."}, {"title": "4 Experiments and Analysis", "content": "We evaluate ProtoFlow on a variety of image classification datasets used to evaluate prior joint predictive and generative modeling approaches: MNIST [13],"}]}