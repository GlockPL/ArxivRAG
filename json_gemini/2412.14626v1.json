{"title": "Learning to Generate Research Idea with Dynamic Control", "authors": ["Ruochen Li", "Liqiang Jing", "Chi Han", "Jiawei Zhou", "Xinya Du"], "abstract": "The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.", "sections": [{"title": "1 Introduction", "content": "Typically, a well-developed scientific research idea (or hypothesis) consists of a methodology and an experiment plan. The methodology introduces the novel concept or approach, while the experiment plan provides a structured guide for its validation. Formulating such research ideas is fundamental to the research process. Traditional methods, which rely heavily on human intuition and experience, are often time-consuming and prone to biases. In contrast, automated research idea generation systems can swiftly synthesize vast data and insights, uncovering novel connections that might elude human researchers. This capability is demonstrated by a growing body of work employing autonomous LLM-based agents to generate and validate innovative ideas (Baek et al., 2024; Bornstein and Singh, 2024). Despite the notable progress, these efforts primarily rely on pre-trained models without task-specific learning. Such reliance restricts the full exploitation of optimizing the generated content, underscoring the urgent need for further refinement and development.\nIn addition to a coarse overall assessment, the quality of a research idea depends on three critical subdimensions that collectively define its value (Si et al., 2024a): (1) Novelty, which reflects the originality of the idea and how distinct it is from existing concepts; (2) Feasibility, which evaluates how practical it is given current resources and constraints; and (3) Effectiveness, which measures how likely the idea will achieve the intended outcomes. These fine-grained metrics, alongside the overall rating, provide a structured framework for evaluating research ideas and guiding their generation through optimization techniques such as reinforcement learning (RL). For example, Reinforcement Learning from Human Feedback (RLHF) has been explored to benefit LLM training (Ouyang et al., 2022). Despite these advancements, existing approaches cannot tackle the complex interdependence and inherent restrictions among these metrics. One notable challenge identified is to reveal the inevitable innovation-feasibility trade-off (Yang et al., 2024; Si et al., 2024a): highly novel ideas often lack feasibility, while overly feasible ideas tend to limit the scope for groundbreaking discoveries. Optimizing idea generation towards each of the key metrics while achieving a balanced trade-off remains a critical, unresolved question.\nTo address this issue, we propose a novel research ideation framework that dynamically adjusts the emphasis on key metrics to achieve high overall quality through a two-stage approach: SFT and controllable RL. In the SFT stage, the idea generator learns foundational patterns by training on pairs of research papers and corresponding follow-up ideas. In the RL stage, we employ multi-dimensional reward modeling as a real-world assessment approximation (Wu et al., 2023). Reward models, trained on fine-grained feedback from review data, score each metric-novelty, feasibility, and effectiveness\u2013providing detailed guidance for model refinement. To enable precise and adaptive control, we introduce dimensional controllers, trained alongside the RL process, which adjusts the generation style to prioritize specific metric dimensions when necessary. This is complemented at inference time by a sentence-level decoder that dynamically adjusts the weights of controllers, ensuring context-aware emphasis - such as prioritizing novelty in the method section and feasibility in the experiment planning. Together, these mechanisms, guided by feedback signals from the reward models, result in more balanced and high-quality idea generation.\nOur contributions are summarized as follows:\n\u2022 We propose a novel research ideation framework that utilizes fine-tuned LLMs to dynamically control the optimization of the generated ideas towards novelty, feasibility, and effectiveness for better overall quality.\n\u2022 We first introduce dynamic decoding into the RL framework, achieving satisfying performance with a balanced trade-off among different assessment metrics of research ideation.\n\u2022 We train our reward models using collected real-world datasets, enabling research idea scoring in a fine-grained manner.\n\u2022 We conduct a comprehensive evaluation with a human study to demonstrate the effectiveness of our proposed method for optimized, controllable research ideation."}, {"title": "2 Related Work", "content": "NLP for scientific discovery NLP techniques have significantly advanced scientific discovery by enabling researchers to manage extensive literature, identify knowledge gaps, and analyze trends effectively (Raghu and Schmidt, 2020; Hope et al., 2021). Models such as SciBERT (Beltagy et al., 2019) and BioBERT (Lee et al., 2020) pre-trained on scientific materials have enhanced these abilities by improving performance on fundamental tasks. Recent developments in LLMs have extended their utility to creative and generative tasks in scientific research. For example, LLMs have been employed to formulate research questions, generate hypotheses, draft research proposals, and even outline experimental designs (Brown et al., 2020; Zhong et al., 2023; Qi et al., 2023; Yang et al., 2024; Wang et al., 2024a). Several prior works have specifically explored methods to enhance idea generation. Approaches such as iterative novelty boosting (Wang et al., 2024b), multi-agent collaboration (Baek et al., 2024), and multi-module retrieval and revision (Yang et al., 2024) have been proposed to advance ideation capabilities beyond baseline prompting methods. Beyond ideation, another branch of research leverages LLMs for automating experimental workflows. Works like MLAgentBench (Huang et al., 2024) and SciCode (Tian et al.,"}, {"title": "3 Method", "content": "We introduce a scientific idea proposer with multi-dimension feedback, which consists of two stages: supervised fine-tuning stage, and reinforcement learning stage that has three components: reward modeling, multi-dimension reward augmented controllable reinforcement learning, and decoding.\n3.1 Overview\nSuppose we have a training set $D = \\{X_i, Y_i\\}_{i=1}^{N}$, where $X_i$ and $Y_i$ are research paper and idea, respectively. Then we fine-tune the language model $M$ with the training set. Thereafter, we collect a reward training set $D_1 = \\{(X, Y, Y^n, Y^f, Y^e)_1\\}$, where $X$, include the textual content of research paper and research idea, and $Y^n, Y^f, Y^e$ are the labels which show the scores of novelty, feasibility, and effectiveness of research idea. We could utilize this training set to train three reward models as follows,\n\\begin{equation}\\begin{aligned}F_n &= R_n(X,Y|\\Theta_n),\\\\F_f &= R_f(X,Y|\\Theta_f),\\\\F_e &= R_e(X,Y|\\Theta_e)\\end{aligned}\\tag{1}\\end{equation}\nwhere $\\Theta_{n/f/e}$ is the parameters of the reward model $R_{n/f/e}$. $R_{n/f/e}$ denotes reward models that aim to score the novelty, feasibility, and effectiveness of the research idea. $F_{n/f/e}$ is reward values from reward models. Then, we use a set of $N_f$ research papers $\\{P_n\\}_{i=1}^{N_f}$ as input to the language model to generate research ideas, which are assessed with reward models based on three criteria: novelty, feasibility, and effectiveness. Finally, we conduct reinforcement learning on the language"}, {"title": "3.2 Supervised Fine-Tuning", "content": "To make the model training more stable in reinforcement learning (Chen et al., 2024), we also introduce the supervised fin-tuning stage.\nData Collection. To conduct a Supervised Fine-Tuning stage, we first collect papers from the ICLR 2023 and 2024. We selected papers from ICLR as training data due to its prestigious standing as a top-tier conference in the field of machine learning, offering cutting-edge research and high-quality technical discussions. We sample 1,000 instances $P = \\{p\\}$ for training. We utilize the LLaMA with a prompt (detailed in appendix D) to extract the research idea $y$ from the sampled paper $p$ as the golden output. To extract the one corresponding input paper $x$ for each output, we select the one most significant supporting paper from all related works $X_1, X_2..., X_n$ by prompting LLaMA of the abstract and introduction section of $p$, together with the citation counts of $X_1, X_2..., X_n$ within the sampled paper $p$.\nFine-Tuning. Based on the collected training set, we fine-tune the language model $M, (X = x, Y = yX$ is the research paper and $Y$ is the research idea), as follows,\n\\begin{equation}L_{sup} = CE(Y,\\hat{Y})\\tag{3}\\end{equation}\nwhere $CE(\\cdot)$ denotes the cross-entropy loss and $\\hat{Y}$ is the predicted research idea from $M$, formulated as $Y = M(X)$."}, {"title": "3.3 Reward Modeling", "content": "Researchers mainly consider three aspects when they devise a research idea: novelty, feasibility, and effectiveness. Therefore, we train three distinct reward models to score the generated idea in reinforcement learning, each corresponding to one of the quality dimensions."}, {"title": "3.4 Multi-dimension Reward Augmented Controllable Reinforcement Learning", "content": "In this stage, we fine-tune the research idea proposer with controllable steering through reinforce-"}, {"title": "3.5 Decoding", "content": "In this part, we devise two decoding methods for the inference stage."}, {"title": "4 Experiment", "content": "4.1 Dataset and Analysis\nWe collect a dataset of 6,765 usable research papers in total submitted to ICLR and NeurIPS in the years 2023 and 2024, including both accepted and rejected submissions and filtered 5,687 usable data. 4,271 of them are used for training, and 500 are sampled for evaluation. Each paper contains its abstract, methodology, and experiment sections. Additionally, review data from OpenReview provides human ratings for overall quality as well as the review contents and key sub-dimensions - novelty, feasibility, and effectiveness. Paper content is scraped with title from the Semantic Scholar and arXiv APIs and then cleaned up with regular expression to extract corresponding sections. These papers and ratings are used to: 1. Derive ground-truth ideas for supervised fine-tuning. 2. Train reward models for the key dimensions. 3. Optimize idea generation using reinforcement learning with multi-dimensional steering.\nThe dataset is split into the following subsets (following existing RLHF settings (Wu et al., 2023)):\n1. Supervised Fine-Tuning split.: We use 1,000 papers from only ICLR to derive the golden"}, {"title": "4.2 Main Experiments", "content": "Baselines and Setups We establish several baselines to evaluate the effectiveness of different control strategies applied to the LLaMA2-RLHF model. The baselines include T5-SFT, T5-RLHF, and LLaMA2-SFT, representing varying levels of model capacity and reinforcement learning application. These baselines are chosen to compare the impact of applying reinforcement learning fine-tuning (RLHF) and enabling targeted controls for Novelty, Feasibility, and Effective.\n\u2022 T5-SFT: A version of the T5 model trained using SFT on 1,000 examples, without reinforcement learning or control strategies, in which ideas are generated based on the prompt structure, serving as the simplest baseline.\n\u2022 T5-RLHF: RL-based fine-tuned T5 without dimensional controllers to enhance performance while maintaining simplicity in structure.\n\u2022 LLaMA2-SFT: A fine-tuned version of the LLaMA2 model using 1,000 examples of research paper-idea pairs, without any RL or dimensional controllers."}, {"title": "4.3 Main Results and Statistical Analysis", "content": "Table 1 presents the experimental results for Novelty (N), Feasibility (F), Effectiveness (E), and Overall metrics.\nThe baseline models establish foundational performance levels, with T5-SFT and T5-RLHF showing modest improvements in Feasibility and Effectiveness due to reinforcement learning, though their Novelty scores remain limited by the lack of mechanisms to encourage innovation. In contrast, LLaMA2-SFT achieves higher overall scores, benefiting from larger model capacity and superior pre-training, yet its reliance on supervised fine-tuning leaves room for enhancement through reinforcement learning and control strategies.\nAdding targeted controls to LLaMA2-RLHF demonstrates the potential for metric-specific optimizations. For instance, introducing Novelty Control significantly boosts creativity while maintaining balanced practicality and performance, highlighting the feasibility of improving originality without major trade-offs. Similarly, Feasibility Control achieves the highest observed feasibility, albeit with minor reductions in novelty and effectiveness, showcasing its focus on practicality. The Effectiveness Control, on the other hand, enhances impact without compromising the balance across dimensions.\nWhen all controls are combined, Static Decoding provides reliable, balanced performance, but its fixed nature limits adaptability. In contrast, Dynamic Decoding emerges as the most effective approach, leveraging contextual dynamic strategy to balance creativity, practicality, and impact, ultimately producing higher-quality ideas.\nThese results show the importance of rl and dynamic control in tailoring model behavior to complex requirements, while also illustrating trade-offs"}, {"title": "4.4 Human Evaluation", "content": "Domain experts validated the effectiveness of our framework of the generated idea as shown in Table 2 and bar plot in Figure 6, with human scores showing a strong correlation with the automatic scores produced by our reward models. The Correlation Coefficients computed with both Pearson and Spearman between human and reviewing agent scores are shown in table 3."}, {"title": "5 Analysis", "content": "5.1 Novelty and Feasibility Trade-off\nWe learn from (Si et al., 2024a) that increasing novelty will likely reduce the feasibility of an idea. To test this idea, we controlled the weight of the novelty steer in the RLHF + Steer1 setup and observed its impact on both novelty and feasibility scores. The results are shown in Table 4."}, {"title": "5.2 Decoding Strategy Motivation", "content": "Dynamic decoding adapts research ideation outputs to the varying demands of different parts of the idea, as shown in the subdimension analysis with regard to the normalized sentence position of the generated ideas in Figure 8. The observed novelty jump in the 6th sentence illustrates a shift in focus, aligning feasibility with the experiment plan while reducing the emphasis on novelty. By dynamically adjusting decoding weights, this strategy ensures that the generated ideas are coherent, contextually aligned, and balanced across key dimensions."}, {"title": "5.3 Case Study", "content": "Table 5 compares the evolution of ideas generated by models, progressing from SFT to advanced configurations with dynamic control. Baseline models with SFT exhibit moderate feasibility but struggle to achieve a balance between novelty and effectiveness, highlighting their limitations in fostering creative yet practical solutions. With RL fine-tuning, LLaMA2-RLHF demonstrates clear improvements across all metrics, leveraging reward mechanisms to enhance collaboration of fine-"}, {"title": "6 Conclusion", "content": "In this work, we introduced a novel LLM-based framework for research idea generation that optimizes and dynamically balances key metrics\u2014novelty, feasibility, and effectiveness\u2014through a two-stage process combining supervised fine-tuning and controllable reinforcement learning. By leveraging multi-dimensional reward models and integrating the dimensional controller with sentence-level dynamic decoding, our approach effectively navigates the improvement and the inherent trade-offs among these metrics, ensuring context-aware and high-quality idea generation. Comprehensive evaluations, including human studies, highlight the robustness and effectiveness of our method, giving the path for more advanced and controllable systems in automated research ideation. In this work, we intro-"}, {"title": "A PPO", "content": "To optimize our idea proposer, we utilize Proximal Policy Optimization (PPO), an actor-critic RL algorithmwidely used in previous RLHF works. PPO enables the proposer (i.e. the policy model) to be refinedagainst multiple reward models that simulate human feedback, ensuring high-quality idea generation. InPPO, the value model $V_\\theta(s_t)$ estimates the expected cumulative reward for a given state $s_t$, providinga baseline for the advantage function. The proposer is optimized with a PPO clipped surrogate trainingobjective. The advantage $A_t$ at timestep $t$ is estimated by a generalized advantage estimation function(Schulman et al., 2016): $A_t = \\sum_{t'=t}^{T-1}(\\gamma\\lambda)^{t'-t}(r'_t + V_\\theta(s_{t'+1}) - V_\\theta(s_{t'}))$, with $\\gamma$ as a hyperparameterand $\\lambda$ as the discounting factor for rewards. $r_t$ is the reward assigned to $a_t$, which in our case is acquiredusing multiple learned reward models. The value model $V_\\theta(s_t)$ is optimized with an expected squared-error loss with the value target as $V^{targ}(s_t) = \\sum_{t'=t}^{T-1}\\gamma^{t'-t}r_t + \\gamma^{T-t}V^{old} _\\theta(s_T)$, where $V^{old} _\\theta$ is the laggingvalue model. Finally, PPO is trained to optimize both the proposer ($M_\\theta$) and value ($V_\\theta$) models with theirrespective objectives. No reward model is being optimized during PPO training. See Algorithm 1 in theAppendix for more details."}, {"title": "B Definition of Novelty, Feasibility, and Effectiveness", "content": "This appendix provides detailed definitions and scoring guidelines for Novelty, Feasibility, and Effectiveness\u2014the three primary dimensions used to evaluate research ideas.\nNovelty. Novelty evaluates how different a proposed research idea is compared to existing works.Following previous work (), the guidelines for scoring are as follows:\n\u2022 1: Not novel at all  The idea is identical to many existing works.\n\u2022 3: Mostly not novel  Very similar ideas already exist.\n\u2022 5: Somewhat novel  There are differences, but not enough for a standalone paper.\n\u2022 6: Reasonably novel  Notable differences, potentially sufficient for a new paper.\n\u2022 8: Clearly novel  Major differences from all existing ideas.\n\u2022 10: Highly novel  Highly different and creative in a clever, impactful way.\nFeasibility. Feasibility measures how practical it is to execute the proposed idea within 1-2 monthsunder the following assumptions:\n\u2022 Ample access to OpenAI/Anthropic APIs.\n\u2022 Limited GPU computing resources.\nScoring guidelines:\n\u2022 1: Impossible  The idea or experiments are fundamentally flawed.\n\u2022 3: Very challenging  Major flaws or significant resource limitations.\n\u2022 5: Moderately feasible  Possible with careful planning and modifications.\n\u2022 6: Feasible  Achievable with reasonable planning.\n\u2022 8: Highly feasible  Straightforward to implement and run.\n\u2022 10: Easy  Quick to implement without requiring advanced skills."}, {"title": "C Detailed Algorithm for Multi-dimension reward augmented RL", "content": "Algorithm 1 Multi-dimension reward augmented Reninformace Learning\nInput: Initial policy model $M_\\theta init$; initial value model $V_\\theta init$; 3 well-trained reward models $R_{n/f/e}$; taskprompts D; hyperparameters $\\gamma, \\lambda, \\epsilon$\nOutput: Updated policy models $M_\\theta n/f/e$\nInitialize policy model $M_\\theta n/f/e \\gets M_\\theta init$, value model $V_\\theta n/f/e \\gets V_\\theta init$\nfor step = 1, . . . ,M do\nSample a batch $D_b$ from $D$\nSample output sequence $y_n \\sim M_{\\theta n}(\u00b7|x_n)$, $y_f \\sim M_{\\theta f}(\u00b7|x_n)$, $y_e \\sim M_{\\theta e}(\u00b7|x_n)$ for eachprompt $x_n \\in D_b$\nCompute rewards $\\{r^{n/f/e}_t \\}_{t=1}^{|y_{n/f/e}|}$ for each sampled output $y^{n/f/e}$, by running $R_{n/f/e}$\nCompute advantages $\\{A^{n/f/e}_t \\}_{t=1}^{|y_{n/f/e}|}$ and value targets $\\{V_{targ}^{n/f/e}(s_t) \\}_{t=1}^{|y_{n/f/e}|}$ for each $y^{n/f/e}$, with $V_{\\theta}^{n/f/e}$\nfor PPO iteration $t = 1, . . . ,\\mu$ do State Update the policy model by maximizing the PPO clippedsurrogate objective for $M_\\theta n/f/e$:"}, {"title": "D Prompt for Research Idea Extraction", "content": ""}, {"title": "E Prompt for Novelty Score Extraction", "content": "System Prompt: You are a specialized assistant for scientific text evaluation. Your task is to evaluatethe novelty of scientific papers.\nUser Prompt\nBased on the following information about a scientific paper, please evaluate its novelty:\n\u2022 Title: {title}\n\u2022 Abstract: {abstract}\n\u2022 Related Works (top 3 from citations since 2023): {recent_works}\n\u2022 Review Comments: {reviews}\nNovelty Evaluation Prompt\nEvaluate how creative and different the idea is compared to existing works on the topic. Consider allpapers that appeared online prior to July 2024 as existing work. Your evaluation should consider thedegree to which the paper brings new insights and differentiates itself from prior research.\nScoring Criteria\nPlease assign a novelty score on a scale from 1 to 10 based on the following criteria:\nNovelty Definition\nFollowing the definition of the previous work (), we score the novelty of papers based on howdifferent it is from existing works. The guidelines for scoring novelty are:\n\u2022 1: Not novel at all many existing ideas are the same.\n\u2022 3: Mostly not novel very similar ideas exist.\n\u2022 5: Somewhat novel differences exist but not enough for a new paper.\n\u2022 6: Reasonably novel notable differences, could lead to a new paper.\n\u2022 8: Clearly novel major differences from all existing ideas.\n\u2022 10: Very novel highly different and creative in a clever way.\nNovelty Rationale\nAfter assigning a score, provide a short justification for your rating. If the score is below 6, specifysimilar works that closely resemble this paper. The rationale should be at least 2-3 sentences."}, {"title": "F Prompt for Research Idea Generation", "content": "System Prompt: You are an AI assistant specializing in extracting and generating structured researchideas from scientific papers. Your task is to assist researchers in developing concise, clear, andinnovative research ideas based on the provided input.\nUser Instructions: You are tasked with generating a structured research idea that includes:\n\u2022 Method: A concise summary of the methodological approach employed in the study.\n\u2022 Experiment Plan: Key details of the experiment, including dataset preparation, baselineimplementation, and evaluation procedures.\n\u2022 Problem: A clear statement of the research problem or gap the study aims to address.\n\u2022 Related Works: Identify and summarize the top 3 most relevant related works, emphasizinghow the target paper builds upon or differs from them.\nEnsure that the output adheres to the following requirements:\n1. Contextual Relevance: The generated idea must align with the main theme of the providedpaper and incorporate any specified entities or constraints.\n2. Clarity and Structure: The output must be structured, clear, and concise, formatted as follows:"}, {"title": "G Prompt for Automatic Evaluation", "content": "System Prompt: You are an AI reviewer specializing in evaluating the quality of research ideasbased on specific criteria: Novelty, Feasibility, and Effectiveness. Your task is to assesseachcriterion and provide structured feedback for automatic evaluation.\nUser Instructions: For a given research idea, evaluate the following dimensions:\n1. Novelty: Assess how creative and unique the idea is compared to existing works.\n2. Feasibility: Evaluate the practicality of executing the idea within typical resource constraints.\n3. Effectiveness: Judge the potential of the idea to achieve its intended objectives or performanceimprovements.\nScoring Criteria: Provide a score between 1 and 10 for each dimension, adhering to these guidelines:\n{Add detailed definition of 3 Metrics HERE}\nEvaluation Output Requirements: Provide a structured evaluation as follows:"}]}