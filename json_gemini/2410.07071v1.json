{"title": "RETRIEVAL-AUGMENTED DECISION TRANSFORMER: EXTERNAL MEMORY FOR IN-CONTEXT RL", "authors": ["Thomas Schmied", "Fabian Paischer", "Vihang Patil", "Markus Hofmarcher", "Razvan Pascanu", "Sepp Hochreiter"], "abstract": "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.", "sections": [{"title": "1 INTRODUCTION", "content": "In-context Learning (ICL) is the ability of a model to learn new tasks by leveraging a few exemplars in its context [Brown et al., 2020]. Large Language Models (LLMs) exhibit this capability after pre-training on large amounts of data crawled from the web. A similar trend has emerged in the field of RL, where agents are pre-trained on datasets with an increasing number of tasks [Chen et al., 2021; Janner et al., 2021; Reed et al., 2022; Lee et al., 2022; Brohan et al., 2022; 2023]. After training, such an agent is capable of learning new tasks by observing previous trials in its context [Laskin et al., 2022; Liu & Abbeel, 2023; Lee et al., 2023; Raparthy et al., 2023]. Consequently, ICL is a promising direction for generalist agents to acquire new tasks without the need for re-training, fine-tuning, or providing expert-demonstrations.\nExisting methods for in-context RL rely on keeping entire episodes in their context [Laskin et al., 2022; Lee et al., 2023; Kirsch et al., 2023; Raparthy et al., 2023]. Consequently, these methods face challenges in complex environments, as complex environments are usually characterized by long episodes and sparse rewards. Episodes in RL may consist of thousands of interaction steps, and processing them is computationally expensive, especially for network architectures such as the Transformer [Vaswani et al., 2017]. Furthermore, not all information an agent encountered in the past may be necessary to solve the new task. Therefore, we address the question of how to facilitate ICL for environments with long episodes and sparse rewards.\nWe introduce Retrieval-Augmented Decision Transformer (RA-DT), which incorporates an external memory into the Decision Transformer [Chen et al., 2021, DT] architecture (see Figure 1). Our external memory enables efficient storage and retrieval of past experiences, that are relevant for the current situation. We achieve this by leveraging a vector index populated with sub-trajectories, in combination with maximum inner product search; akin to Retrieval-augmented Generation (RAG) in LLMs [Khandelwal et al., 2019; Lewis et al., 2020; Borgeaud et al., 2022]. To encode retrieved sub-trajectories, RA-DT relies on a pre-trained embedding model, which can either be domain-specific,"}, {"title": "2 RELATED WORK", "content": "In-context Learning. ICL is a form of Meta-learning, also referred to as learning-to-learn [Schmidhuber, 1987]. Typically, meta-learning is targeted and learned through a meta-training phase, for example in supervised-learning [Santoro et al., 2016; Mishra et al., 2018; Finn et al., 2017] or in RL [Wang et al., 2016; Duan et al., 2016; Kirsch et al., 2019; Flennerhag et al., 2019]. In contrast, ICL emerges as a result of pre-training on a certain data distribution [Chan et al., 2022]. This ability was first observed in Hochreiter et al. [2001] via LSTMs [Hochreiter & Schmidhuber, 1997] and later re-discovered in LLMs [Brown et al., 2020]. Ortega et al. [2019] found that every memory-based architecture may exhibit such capabilities. Another crucial factor is a training distribution comprising\na vast amount of tasks [Chan et al., 2022; Kirsch et al., 2022]. Recent works combined these proper- ties to induce ICL in RL [Laskin et al., 2022; Lee et al., 2022; Kirsch et al., 2023]. While promising, they require keeping entire episodes in context, which is difficult in environments with long episodes. Raparthy et al. [2023] consider an in-context imitation learning setting given expert demonstrations. In contrast, RA-DT can handle long episodes and does not rely on expert demonstrations.\nRetrieval-augmented Generation. The aim of retrieval-augmentation is to provide a model access to an external memory. This alleviates the need to store the training data in the parameters of a model and allows to condition on new data without re-training. RAG is successfully applied in the realm of LLMs [Khandelwal et al., 2019; Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022; Ram et al., 2023], multi-modal language generation [Hu et al., 2023; Yasunaga et al., 2023; Yang et al., 2023b; Ramos et al., 2022], and for chemical reaction prediction [Seidl et al., 2022]. In RL, the access to an external memory is often referred to as episodic memory [Sprechmann et al., 2018; Blundell et al., 2016; Pritzel et al., 2017]. Goyal et al. [2022] investigate the effect of different data sources in the external memory of an online RL agent. [Humphreys et al., 2022] provide access to millions of expert demonstrations via RAG in the game of Go. In contrast, RA-DT does not rely on expert demonstrations, but leverages RAG to learn new tasks entirely in-context without the need for weight updates. Further, RA-DT does not rely on a pre-trained domain-specific embedding model, as we demonstrate that the embedding model can be entirely domain-agnostic.\nExternal memory in RL. Most prior works have explored the utility of an external memory to cope with partially observable environments [\u00c5str\u00f6m, 1965; Kaelbling et al., 1998], in which the agent must remember past events to approximate the true state of the environment. This is difficult, especially for complex tasks with sparse rewards [Arjona-Medina et al., 2019; Patil et al., 2022; Widrich et al., 2021] and long episodes. To cope with this problem, Neural Turing Machines [Graves et al., 2014], which rely on a neural controller to read from and write to an external memory, were applied to RL [Zaremba & Sutskever, 2015]. Memory networks [Weston et al., 2015] leverage an external memory for reasoning. Wayne et al. [2018] propose a memory architecture with read/write access to learn what information to store based on a world model. In contrast, RA-DT only retrieves pieces of past information similar to the current encountered situation. Hill et al. [2021] propose an attention-based external memory, where queries, keys, and values are represented by different modalities. Similarly, our domain-agnostic embedding model extends the idea of history compression via LLMs [Paischer et al., 2022; 2023] to retrieval, where queries and keys are encoded in the language space, while values comprise raw sub-trajectories."}, {"title": "3 METHOD", "content": null}, {"title": "3.1 BACKGROUND", "content": "Reinforcement Learning. We formulate our problem setting as a Markov Decision Process (MDP) that is represented by a 4-tuple of(S, A, P, R). S and A denote state and action spaces, respectively. At timestept the agent observes state st \u2208 S and issues action at \u2208 A. For each executed action, the agent receives a scalar reward rt, which is given by the reward function R(rt | St, At). P(St+1 | St, at) constitutes a probability distribution over next states st+1 when issuing action at in state st. RL aims at learning a policy \u03c0(at | St) that predicts action at in state st that maximizes rt.\nDecision Transformer. Decision Transformer [Chen et al., 2021, DT] learns a policy from offline data by conditioning on future rewards. This allows rephrasing RL as a sequence modelling problem, where the agent is trained in a supervised manner to map future rewards to actions, often referred to as upside-down RL [Schmidhuber, 2019]. To train the DT, we assume access to a pre-collected dataset D = {Ti | 1 \u2264 i \u2264 N} of N trajectories T\u2081 that are sampled from the environment via a behavioural policy \u03c0\u03b2. Each trajectory \u0442\u2208 D consists of state, action, reward, and return-to-go (RTG) quadruplets T\u2081 = (so, ao, ro, Ro,...,st,at,rt, \u0125\u0442), where T represents the length of trajectory Ti, and Rt = =t rt'. The DT \u03c0\u04e9 is trained to predict the ground truth action at conditioned on sub-trajectories via cross-entropy or mean-squared error loss, depending on the domain:"}, {"title": "3.2 RETRIEVAL-AUGMENTED DECISION TRANSFORMER (RA-DT)", "content": "Processing long sequences with DTs is computationally expensive due to the quadratic complexity of the Transformer architecture. To address this challenge, we introduce RA-DT, which equips the DT with an external memory that relies on a vector index for retrieval. Consequently, RA-DT consists of a parametric and a non-parametric component, reminiscent of complementary learning systems [Mcclelland et al., 1995; Kumaran et al., 2016]. The former is represented by the DT and learns to predict actions conditioned on the future return. The latter is the retrieval component that searches for relevant experiences, similar to Borgeaud et al. [2022] (see Figure 1)."}, {"title": "3.2.1 VECTOR INDEX FOR RETRIEVAL AUGMENTATION", "content": "We aim at augmenting the DT with a vector index (external memory) that allows for retrieval of relevant experiences. To this end, we build our vector index by leveraging an embedding model g: Rar that takes a trajectory 7 and returns a vector of size dr. Given a dataset D of trajectories, we obtain a set of key-value pairs of our vector index by embedding all sub-trajectories Tt-C:t \u2208 D via g(\u00b7) to obtain K \u00d7 V = {(g(Ti,t\u2212C:t), Ti,t\u2212C:t+c) | 1 \u2264 i \u2264 |D|}. Note that values contain sub-trajectories ranging from t \u2013 C to t + C, while keys use sub-trajectories t \u2013 C : t for a fixed C, where t goes over trajectory length in increments of C (see Appendix C.4 for more details). The reason for this choice is that during inference, the model does not have access to future states.\nIn RAG applications for Natural Language Processing (NLP), a common choice for g() is a pre- trained LM. While pre-trained models in NLP are ubiquitous, they are rarely available in RL. A natural choice to instantiate g(\u00b7) is to train a DT on the pre-collected dataset D, as they exhibit a well-separated embedding space after pre-training [Schmied et al., 2023]. Therefore, they are well suited for retrieval since a new task can be matched to similar tasks in the vector index. As a domain-agnostic alternative, we propose to utilize the FrozenHopfield (FH) mechanism Paischer et al. [2022] to map trajectories to the embedding space of a pre-trained LM. This enables instantiating g(\u00b7) with a pre-trained language encoder. The FH mechanism is parameterized by an embedding matrix E\u2208 Rvxdim of a pretrained LM with vocabulary size v and hidden dimension d\u2081m, a random matrix P with entries sampled from N(0, din/dLM), and a scaling factor \u03b2 and performs:"}, {"title": "3.2.2 SEARCHING FOR SIMILAR EXPERIENCES", "content": "Given an input sub-trajectory Tin \u2208 D, we first construct a query q = g(Tin), using our embedding model g() (see Appendix C.4 for details). Then, we use maximum inner product search (MIPS) between q and all keys k \u2208 K and select the corresponding top-l sub-trajectories Tret \u2208 V by:"}, {"title": "3.2.3 REWEIGHTING RETRIEVED EXPERIENCES", "content": "Following Park et al. [2023], we characterize the usefulness of retrieved sub-trajectories in R along two dimensions: relevance and utility. The relevance of a key k \u2208 K is defined by its cosine similarity to the query q. While a retrieved experience may be relevant, it might not be important. Determining the utility of a sequence in general is hard. Thus, we experiment with two heuristics that follow different definitions of utility. The first assigns more utility to sub-trajectories with high return, and is utilized at inference only. The second assigns utility to sub-trajectories that originate from the same task as the query and is used at training only. Then, we reweight a retrieved experience according to:\nNote that we instantiate su(\u00b7,\u00b7) differently depending on whether the agent is in training or inference mode. At training time, a pre-collected set of trajectories that contains multiple tasks is stored in the vector index (Figure 1, left). Trajectories can be obtained from human demonstrations or RL agents. Therefore, we encourage the agent to retrieve sub-trajectories of the same task. During training, we use: Su(Tret, Tin) = 1(t(Tret) = t(Tin)), where t(\u00b7) takes a sub-trajectory and returns its task index.\nDuring inference, we evaluate the ICL capabilities of the agent. Starting from an empty vector index, we store experiences of the agent while it interacts with the environment (see Figure 1, right). Thus, during inference, the agent can only retrieve experiences from the same task. Therefore, we steer the agent to produce high reward behaviour on the new task by reweighting a retrieved sub-trajectory by the total return achieved over the episode it appears in, i.e., Su(Tret, Tin) = \u2211iori. We apply this reweighting to the retrieved experiences in R and select the top-k elements by:"}, {"title": "3.2.4 INCORPORATING RETRIEVED EXPERIENCES", "content": "After reweighting, the set S contains sub- trajectories that are both important and rele- vant for the current input Tin to the DT \u03c0\u03c1. To incorporate the retrieved experiences in the DT, we interleave it with cross-attention lay- ers (CA) after every self-attention (SA) layer. The retrieved sub-trajectories are encoded by separate embedding layers for each token type (state/action/reward/RTG) and then passed to the CA layers. Thus, our RA-DT predicts ac- tions at given input trajectory and retrieved trajectory by:"}, {"title": "4 EXPERIMENTS", "content": "We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section 4.1), Dark Key-Door (Section 4.2), and MazeRunner (Section 4.3) [Laskin et al., 2022; Lee et al., 2022; Grigsby et al., 2023], with increasingly larger grid-sizes, resulting in longer episodes. Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section 4.4) and procedurally-generated video games (Procgen, Section 4.5).\nAcross experiments, we report performances for two variants of RA-DT. The first variant leverages a domain-specific embedding model for retrieval, specifically a DT trained on the same domain. The second variant (RA-DT + Domain-agnostic) makes use of the FH mechanism in combination with BERT [Devlin et al., 2019] as the pre-trained LM. Consequently, this variant of RA-DT does not require any domain-specific pre-training of the embedding model. We compare RA-DT against the vanilla DT and two established in-context RL methods, namely Algorithm Distillation [Laskin et al., 2022, AD] and Decision Pre-trained Transformer [Lee et al., 2023, DPT]. Following, Agarwal et al. [2021] we report the mean across tasks and 95% confidence intervals over 3 seeds. We use a context length equivalent to two episodes (from 200 up to 2000 timesteps) for AD, DPT and DT. For RA-DT, we use a considerably shorter context length of 50 transitions, unless mentioned otherwise. On grid-worlds, we train all methods for 100K steps and evaluate after every 25K steps. Similarly, we train for 200K steps and evaluate after every 50K steps for Meta-World, DMControl and Procgen. All grid-worlds and Procgen exhibit discrete actions and consequently, we train all methods via the cross-entropy loss to predict the next actions. On Meta-World and DMControl, we train all method using the mean-squared error loss to predict continuous actions. Following Laskin et al. [2022] and Lee et al. [2023], our primary evaluation criterion is performance improvement during ICL trials. After training, the agent interacts with the environment for a fixed amount of episodes, each of which is considered a single trial. Upon completion of an ICL trial, the respective episode is stored in the vector index. We provide further training and implementation details in Appendix C."}, {"title": "4.1 DARK-ROOM", "content": "Experiment Setup. Dark-Room is commonly used in prior work on in-context RL [Laskin et al., 2022; Lee et al., 2023]. The agent is located in an empty room, observes only its x-y coordinates, and has to navigate to an invisible goal state (|S| = 2, |A| = 5, see Figure 9). A reward of +1 is obtained in every step the agent is located in the goal state. Because of partial observability, it must leverage memory of previous episodes to find the goal. We conduct experiments on three different grid sizes, namely 10\u00d710, 20\u00d720, and 40\u00d720, and corresponding episode lengths of 100, 200 and 800, respectively. We designate 80 and 20 randomly assigned goals as train and evaluation locations, respectively, as in Lee et al. [2023]. We use Proximal Policy Optimization (PPO) [Schulman et al.,"}, {"title": "4.2 DARK KEY-DOOR", "content": "Experiment Setup. In Dark Key-Door, the agent is located in a room with two invisible objects: a key and a door. The agent has to pick up the invisible key, then navigate to the door. Because of the presence of two key events, the task-space is combinatorial in the number of grid-cells (1002 = 10000 possible tasks for 10 \u00d7 10) and is therefore considered more difficult. A reward of +1 is obtained once for picking up the key and for every step the agent stands on the door grid-cell after it collected the key. We retain the same experiment setup as in Section 4.1 and provide further details in Appendix B.1 (also see Figure 8 for single-task expert scores).\nResults. On 10 \u00d7 10 and 20 \u00d7 20, RA-DT outperforms baselines, with the performance ranking remaining the same as on Dark-Room (see Figure 4). Surprisingly, domain-agnostic RA-DT out- performs its domain-specific counterpart on 40 \u00d7 20, which demonstrates that the domain-agnostic embedding model is a promising alternative. This result indicates that RA-DT can successfully handle environments with more than one key event, even with shorter observed context."}, {"title": "4.3 MAZE-RUNNER", "content": "Experiment Setup. Maze-Runner was introduced by Grigsby et al. [2023] and inspired by Pasukonis et al. [2022]. The agent is located in a procedurally-generated 15 \u00d7 15 maze (see Figure 10), observes"}, {"title": "4.4 META-WORLD & DMCONTROL", "content": "Experiment Setup. Next, we evaluate RA-DT on two multi- task robotics benchmarks, Meta-World [Yu et al., 2020b] and DMControl [Tassa et al., 2018]. States and actions in both benchmarks are multidimensional continuous vectors. While the state and action space in Meta-World remain constant across all tasks (|S| = 39, |A| = 6), they vary considerably in DM- Control (3 \u2264 |S| \u2264 24, 1 \u2264 |A| \u2264 6). Episodes last for 200 and 1000 steps in Meta-World and DMControl, respectively. We leverage the datasets released by Schmied et al. [2023]. For Meta-World, we pre-train a multi-task policy on 45 of the 50 tasks (ML45, 90M transitions in total) and evaluate on the 5 remaining tasks (ML5). Similarly, on DMControl, we pre-train on 11 tasks (DMC11, 11M transitions in total) and evaluate on 5 unseen tasks (DMC5). We provide further details on the environments, datasets, and experiment setup in Appendices B.3 and D.3, and B.4 and D.4 for Meta-World and DMControl, respectively.\nResults. We present the learning curves and corresponding ICL curves for Meta-World and DM- Control in Figure 22 and 23, and Figures 24 and 25 in Appendix D, respectively. In addition, we provide the raw and data-normalized scores in Tables 3 and 4, respectively. On both benchmarks, we find that RA-DT attains considerably higher scores on unseen evaluation tasks, but slightly lower average scores across training tasks compared to DT. However, these performance gains on evaluation tasks are not reflected in improved ICL performance. In fact, we only observe slight in-context improvement on training tasks, but not on holdout tasks for any of the considered methods."}, {"title": "4.5 PROCGEN", "content": "Experiment Setup. Finally, we conduct experiments on Procgen [Cobbe et al., 2020], a benchmark consisting of 16 procedurally-generated video games, designed to test the generalization abilities of RL agents. The procedural generation in Procgen is controlled by setting an environment seed, which results in visually diverse observations for the same underlying task (see starpilot-example in Figure 12). In Procgen, the agent receives image-based inputs (|S| =3\u00d764\u00d764). All 16 tasks share a discrete action space (|A| = 15). Rewards are either dense or sparse depending on the environment.\nWe follow Raparthy et al. [2023] and use 12 tasks for training (PG12) and 4 tasks for evaluation (PG4). First, we generate datasets by training task-specific PPO agents for 25M timesteps on 200 environment seeds per task in easy difficulty. Then, we pre-train a multi-task policy on the PG12 datasets (24M transitions in total, 2M per task). We leverage the procedural generation of Procgen and evaluate all models in three settings: training tasks - seen (PG12-Seen), training tasks - unseen (PG12-Unseen), and evaluation tasks - unseen (PG4). Additional details on the generated datasets and our environment setup are available in Appendices B.5 and D.5."}, {"title": "4.6 ABLATIONS", "content": "To better understand the effect of learning with retrieval, we present a number of ablation studies on essential components in RA-DT conducted on Dark-Room 10 \u00d7 10 (more details in Appendix E).\nRetrieval outperforms sampling of experiences. To investigate the effect of learning with retrieved context, we substitute retrieval with random sampling, either over all tasks, or from the same task (see Figure 6a). We find that training with retrieval outperforms both sampling variants, highlighting the benefit of training with retrieval to improve ICL abilities. We hypothesise this is because retrieval constructs bursty sequences, which was found to be important for ICL [Chan et al., 2022].\nReweighting Experiences. RA-DT reweights a sub-trajectory by its relevance and utility score. By default, we use task-based reweighting during training. In Figure 28, we compare against alternatives, such as reweighting by return. Indeed, we find that task-based reweighting is critical for high performance, because it ensures that retrieved experiences are useful for predicting the next action.\nSensitivity of Reweighting. In addition, we conduct a sensitivity analysis on a used in the reweighting mechanism (see Equation 4) that determines the influence of utility on the retrieval score. In Figure 6b, we find that RA-DT performs well for a range of values for a used during training, but performance declines if no re-weighting is employed (a = 0). We perform the same analysis for a during evaluation in Figure 29.\nEffect of Retrieval Regularization. We evaluate with three retrieval regularization strategies to mitigate the effect of copying the context: deduplication, similarity cut-off, and query dropout. To evaluate their impact on ICL performance, we systematically removed each one from RA-DT (see Figure 30). We found the combination of all three to be effective and add them to our pipeline.\nDifferent LMs for domain-agnostic RA-DT. Finally, we investigate how strongly domain-agnostic RA-DT is influenced by the choice of pre-trained LM for the embedding model. We compare our default choice BERT against other smaller/larger LMs (see Figure 32). We found that BERT performs best and performance decreases with smaller models.\nFor additional ablations on RA-DT (query construction, placement of cross-attention layers, retrieval steps) and on our baselines (effect of K in AD), we refer to Appendix E."}, {"title": "5 DISCUSSION", "content": "In this section, we highlight current challenges of RA-DT and other offline in-context RL methods."}, {"title": "6 CONCLUSION", "content": "ICL is a promising avenue towards more general agents. Existing in-context RL methods keep entire episodes in their context window, which is challenging as RL environments are typically characterized by long episodes and sparse rewards. To address this challenge, we introduce RA-DT, which employs an external memory mechanism to store past experiences. This enables the retrieval of sub-trajectories that are relevant for the current situation. Our RA-DT outperforms baselines on grid-worlds, while using only a fraction of their context length. Furthermore, we found that RA-DT improves average performance on holdout tasks on complex environments (e.g., robotics and video games). However, RA-DT along with other established in-context RL methods struggles to exhibit in-context improvement. Consequently, we illuminate the current limitations of in-context RL methods and discuss future directions. Finally, we release our datasets for Dark-Room, Dark Key-Door, MazeRunner, and Procgen, to facilitate future research on in-context RL.\nFuture Work. Besides the general directions discussed in Section 5, we highlight a number of concrete approaches to extend RA-DT. While we focus on in-context improvement without relying on expert demonstrations, pre-filling the external memory with demonstrations may enable RA-DT to perform more complex tasks. This may be particularly powerful for robotics applications, where expert demonstrations are easy to obtain. Furthermore, end-to-end training of the retrieval component in RA-DT, similar to [Izacard et al., 2022], may result in more precise context retrieval and enhanced down-stream performance. Finally, we envision that modern recurrent architectures [Bulatov et al., 2022; Gu & Dao, 2023; Beck et al., 2024] as policy backbones may benefit RA-DT by maintaining hidden states across many episodes."}]}