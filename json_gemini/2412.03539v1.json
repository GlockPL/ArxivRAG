{"title": "NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model", "authors": ["Xinheng Xie", "Yue Wu", "Cuiyu He"], "abstract": "Understanding adversarial examples is crucial for improving the model's robustness, as they introduce imperceptible perturbations that deceive models. Effective adversarial examples, therefore, offer the potential to train more robust models by removing their singularities. We propose NODE-AdvGAN, a novel approach that treats adversarial generation as a continuous process and employs a Neural Ordinary Differential Equation (NODE) for simulating the dynamics of the generator. By mimicking the iterative nature of traditional gradient-based methods, NODE-AdvGAN generates smoother and more precise perturbations that preserve high perceptual similarity when added to benign images. We also propose a new training strategy, NODE-AdvGAN-T, which enhances transferability in black-box attacks by effectively tuning noise parameters during training. Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more effective adversarial examples that achieve higher attack success rates while preserving better perceptual quality than traditional GAN-based methods.", "sections": [{"title": "Highlights", "content": "\u2022 Introduction of NODE-AdvGAN: The paper introduces NODE-AdvGAN, a model that replaces the original generator in AdvGAN with a dynamic neural network using Neural ODEs to generate adversarial perturbations."}, {"title": null, "content": "\u2022 Dynamic-System Perspective: The process of generating adversarial examples is modeled as a continuous-time dynamic system, allowing for adaptive perturbations that enhance robustness and attack effectiveness."}, {"title": null, "content": "\u2022 Enhanced Transferability: A novel training strategy focuses on maximizing the model's transferability by optimizing the parameters of the perturbation magnitude, thereby enhancing the quality of adversarial images."}, {"title": null, "content": "\u2022 Experimental Validation and Robustness: Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more effective adversarial examples that achieve higher attack success rates while preserving better perceptual quality than traditional GAN-based methods."}, {"title": "1. Introduction", "content": "An adversarial image is a modified version of a benign image that has been intentionally altered to deceive machine learning models such as deep neural networks (DNNs). These modifications are typically imperceptible to the human eye but can cause the target model to make incorrect inferences. These perturbations expose vulnerabilities in machine learning systems and help drive the development of more robust and secure AI applications [1, 2, 3], including face recognition [4, 5], autonomous driving [6, 7], etc. \nThe process of generating adversarial images can be mathematically formulated as\n$$x_{adv} = x + \\delta(x) \\text{ subject to } ||\\delta(x)||_p \\le \\epsilon,$$\nwhere x is the benign image and $x_{adv}$ is its corresponding adversarial image, $\\delta(x)$ is the adversarial perturbation on the benign image x that the adversarial manipulation"}, {"title": null, "content": "In a white-box attack, where one has full access to the target model's architecture, the query of generating such perturbations to fool f can naturally be addressed using a gradient-based method. This method approximately solves the following optimization problem:\n$$\\underset{\\|\\delta(x, f)\\|_p \\le \\epsilon}{\\text{argmax}} J(\\Theta, x + \\delta(x, f), y),$$\nwhere J is the cost functional that maximizes the misclassification likelihood for the adversarial image $x + \\delta(x, f)$, y is the true label of x, and $\\Theta$ is the set of parameters of the model used."}, {"title": "1.1. Our Contribution", "content": "This paper aims to generate more effective adversarial images with stronger attack performance, higher fidelity, and improved transferability. These images can be combined with the original training set as an augmented dataset to enhance model robustness. Our key innovation replaces the generator in the original AdvGAN [13] with a dynamic neural network, specifically a neural ordinary differential equation (NODE) network, forming our NODE-AdvGAN model. Additionally, we introduce a novel training strategy to enhance transferability further in our model, referred to as NODE-AdvGAN-T.\nA dynamic-system perspective. Generating adversarial samples from benign images can be seen as the reverse of image denoising to some extent, where instead of removing noise to recover the clean image, one introduces perturbations to create adversarial images. Inspired by the recent work of image denoising [22], we assume that the procedure of generating adversarial images follows a continuous-time dynamic system:\n$$v(t) = F(t, v, y), \\quad v(0, w) = x, \\quad v(T) = x_{adv},$$\nwhere the function F represents the vector field to be learned. The benefit is three-fold."}, {"title": null, "content": "\u2022 A PDE-informed system: Due to its generality, we observe that F can potentially represent an implicit differential operator involving spatial derivatives, making Eqn. (3) an implicit PDE system. Such a PDE setting may inherit the advantages of traditional PDE-based methods in image-related tasks, including accurately preserving edges and textures. Consequently, this approach is capable of producing adversarial samples with better perceptual similarity."}, {"title": null, "content": "\u2022 Emulating an optimized gradient-based iterative approach, as seen in methods like MI-FGSM/NI-FGSM: Eqn. (3) can be viewed as a continuous model for gradient-based iterative methods. While gradient-based methods are powerful and straightforward, their effectiveness is heavily limited by their rigid dependence on a predefined update formula. In our model, by learning F, we are able to let the system adaptively adjust beyond the constraints of traditional methods. Therefore, we can leverage a more flexible approach, adapting the update strategy dynamically, which enhances the overall robustness and performance of the attack mechanism."}, {"title": null, "content": "\u2022 Smoothness: Continuous in time models like Eqn.(3) usually lead to smoother trajectories in the optimization process [23]. Smooth trajectories allow for finer adjustments to the generated adversarial images. This means that perturbations can be applied gradually rather than in abrupt or random jumps, enabling the designer to have more control over how much the input data is altered. Therefore, the smoothness in the optimization process allows for more precise control over the perturbations, leading to more efficient, less detectable attacks that maintain high accuracy and similarity to the original data.\nLearning the vector field via Neural ODE (NODE). To implement Eqn. (3), we replace the generator G in the original AdvGAN [13] by a NODE network. NODEs have already been previously applied in the domain of adversarial images [24, 25]. However, most of these applications primarily concentrate on defense strategies and research on employing NODEs specifically for generating adversarial images has not yet been conducted. For example, it has been observed that the continuous dynamics modeled by NODEs naturally resist the perturbations generated by adversarial attacks [25]. Finally, we would like to highlight that the NODE model bears a strong resemblance to Deep Residual Networks (ResNet) [11], which is commonly used in imaging applications with considerable efficacy [26, 27, 28, 29].\nA novel training strategy. To improve the efficacy of transfer attacks in adversarial machine learning, we propose a strategic enhancement in training adversarial generative models. This approach incorporates hyperparameter tuning for the noise parameter $E_{train}$, based on the metric of Attack Success Rate (ASR) across various classification models. This is different from the traditional GAN-based methods, where the maximum noise level during the training stage $E_{train}$ and the test stage e are predetermined and set to be the same.\nThe transferability of adversarial attacks refers to the phenomenon where adversarial images generated for one model can also deceive another, even if the second model has a different architecture or was trained on a different dataset [30]. This transferability reveals a shared vulnerability among models when faced with adversarial perturbations, with significant implications for robustness studies [31], attack scalability [32], and security risk assessments [33].\nOur rationale for introducing additional parameter tuning is as follows. To maximize the ASR, a straightforward approach is to apply uniform noise across the entire image, with a magnitude large enough to obscure features detectable by the classifier, thus easily misleading it. Consequently, allowing higher noise levels e often leads to improved ASR in white-box attacks, which is why traditional methods predetermine a maximum tolerable noise level for such attacks.\nHowever, this uniform approach does not perform well for all images due to noise level constraints, limiting robustness in transferability. To achieve greater robustness and transferability, the model must learn to generate noise that selectively targets and masks features detected by the classifier rather than applying noise uniformly. This can be achieved by excluding the trivial solution from the admissible set and tuning the noise parameter e during training, which we denote as $E_{train}$. Experiments indicate that using a smaller $E_{train}$ encourages the model to learn more meaningful, feature-disguising noise. Thus, we propose tuning $E_{train}$ to improve transferability. Results show that this technique significantly enhances the transferability of adversarial images. \nOrganisation. The remaining structure of this paper is outlined as follows. Section 2 reviews the foundational concepts of AdvGAN and NODE. Section 3 elaborates on the architecture of the proposed NODE-AdvGAN, detailing the proposed algorithms with loss functions and the training strategy for NODE-AdvGAN-T. Section 4 presents results of ablation studies and comparison with baseline models on both targeted and untargeted attacks on multiple widely used datasets. Finally, we conclude our study in Section 5."}, {"title": "2. Preliminaries", "content": "2.1. Adversarial Generative Adversarial Network (AdvGAN)\nAs powerful tools in the field of generative modeling, GANs [34] have been extensively applied in various image generation tasks, celebrated for their capacity to create highly realistic and detailed images [35, 36, 37]. The key to the success of GANs lies in their unique adversarial training framework, consisting of a generator and a discriminator. This framework enables GANs to generate high-quality, synthetic images through an adversarial process, where the generator progressively improves its output to fool the discriminator into accepting the images as real.\nAdvGAN [13] adapts the generative adversarial framework specifically for the generation of adversarial examples. In this model, the generator is responsible for producing adversarial examples, while the discriminator's task is to distinguish between these generated examples and real images. Through this process, the generator takes the genuine image as the input and creates noise aiming to mislead the target model and ensures that the noise is subtle enough to remain undetected by the discriminator. Models trained in this manner do not need to access the target classification model's architecture during the testing phase, enabling more efficient generation of adversarial examples. This attribute allows the attack to operate independently of the target model's specifics once the adversarial generator has been trained.\n2.2. NODE\nThe NODE framework models a continuous function that maps from $\\mathbb{R}^d$ to $\\mathbb{R}^d$ by employing an ODE as follows: for any given input $h_0 \\in \\mathbb{R}^d$, the output of the NODE model is represented by h(T) (or, more generally, $h(t)_{t\\in[0,T]}$), where the function h(t) satisfies the differential equation:\n$$\\frac{dh(t)}{dt} = N_{\\Theta}(h(t), t), \\quad h(0) = h_0.$$\nIn this formulation, the vector field $N_{\\Theta}$ is parameterized by a neural network with trainable parameters $\\Theta$. This neural network defines the dynamics of h(t) over time, thus enabling the modeling of complex, continuous transformations of the input data. According to Picard's Theorem, the initial value problem (4) has a unique solution provided that $N_{\\Theta}$ is Lipschitz continuous, which can be ensured by using finite weights and activation functions such as ReLU or Tanh [21].\nIn our model, we apply NODE for the generator. More specifically, we let $h_0$ be the initial clean image x and h(T) as the resulting adversarial example $x_{adv}$."}, {"title": "3. NODE-AdvGAN for adversarial image", "content": "3.1. Problem Statement\nConsider an image sample x, situated within the feature space X, which is a subset of the tensor space $\\mathbb{R}^{C \\times H \\times W}$ (= $\\mathbb{R}^{C} \\otimes \\mathbb{R}^{H} \\otimes \\mathbb{R}^{W}$). Here, C, H, and W represent the number of channels, height, and width of the image, respectively. Each sample x corresponds to a true class label $y \\in Y$. Define f as a classifier that maps input x to a predicted classification label y', expressed as f(x) = y'. Additionally, define $f_i$ as the function within the classifier outputting logits, with $f_i(x)$ representing the logit value for the ith class. Given an instance x, the objective of an adversary is to create adversarial images $x_{adv} \\in \\mathbb{R}^{C \\times H \\times W}$. Define the perturbation vector as $\\delta(x) = x_{adv} - x$, where $x_{adv}$ is the adversarial image and x is the original input, both residing in $\\mathbb{R}^{C \\times H \\times W}$. The parameter $\\epsilon$ represents the maximum allowable strength of the noise, ensuring $||\\delta(x)||_{\\infty} \\le \\epsilon$. This bounds each component of the perturbation $\\delta(x)$ by $\\epsilon$ in absolute value. In an untargeted attack, the goal is to have f($x_{adv}$) != y, where y is the true label. For a targeted attack, the aim is f($x_{adv}$) = $y_{target}$, where $y_{target}$ is the specified target class.\n3.2. Our Proposed NODE-AdvGAN\nSimilar to conventional AdvGAN models, our model consists of a generator G and a discriminator D. When dealing with a target neural network f under a white-box attack, the overall structure of NODE-AdvGAN is presented in Figure 2, where losses mentioned therein can be found in section 3.4.\nWe use the ODE system (4) as the dynamics for generating perturbation noise. The vector field $N_{\\Theta}(h(t), t)$ is modeled through a CNN-based neural network, which consists of 6 CNN layers. We refer to Figure 3 for a graphical representation of the NODE generator. It is worth noting that we have the flexibility to utilize various state-of-the-art image-to-image network models for the vector field $N_{vec}$.\nThe detailed structure of the 6-layer vector field includes two types of layers: Conv + BN + ReLU and Conv. The term 'Conv' refers to a dilated convolutional layer with 3 x 3 filters, 'BN' denotes batch normalization, and 'ReLU' denotes rectified linear units. Layers 1 to 5 follow the Conv + BN + ReLU structure, and the 6th layer is a Conv layer. Each layer's channel count also shown in the Table 9 in Appendix B, with the image size remaining constant throughout. The first layer has c + 1 input channels, where c represents the number of image channels and the additional channel corresponds to the time variable. In the final layer, the number of output channels is set to c. For the initial and final layers, a dilation factor is set to 1, corresponding to standard convolution without dilation. In contrast, a dilation factor of 3 is applied to convolution layers 2 through 5. This design choice ensures that the receptive field size reaches 29, which is nearly the entire height and width (32 \u00d7 32) of the images in the experimental dataset.\nAnother key feature of our model is its flexibility in adjusting its depth by varying the number of time integration steps, N. In our experiments, we set N at values between 2 and 8, striking a balance between training time and model performance. We employ the Euler method for numerical integration and select a final time parameter T = 0.05.\nFor the discriminator, we follow the architectures used in AdvGAN [13], which are similar to those in the image-to-image translation literature [27, 28]. The discriminator consists of a series of convolutional layers with Leaky ReLU activations and BN for stabilization.\n3.3. Training Strategy of NODE-AdvGAN-T\nTo enhance the transfer effectiveness of adversarial attacks, we propose a strategic adjustment in the training process of adversarial generative models, specifically during the adversarial image generation phases of AdvGAN. To achieve that, it is crucial to control the noise intensity to ensure it remains within predefined bounds, denoted by e. The bounds are typically enforced by truncating the perturbation generated by the generative model G as follows:\n$$\\delta(x) = Clip(G(x), -\\epsilon, \\epsilon).$$ Here, the Clip function limits every element in vector G(x) to a specified range [-e, e], mathematically equivalent to:\n$$Clip(G_i(x), -\\epsilon, \\epsilon) = \\min(\\max((G_i(x), -\\epsilon), \\epsilon),$$where $G_i(x)$ represent the $i^{th}$ element of the vector G(x). Traditionally, the noise level, denoted by $E_{train}$, used in the training phase is set equal to e in the testing phase. In our approach, however, we treat $E_{train}$ as a tunable hyperparameter in the training phase, while e in the testing phase remains a fixed, user-defined value. Consequently, $E_{train}$ != e, as detailed in Algorithm 1. Experimental validation suggests that this strategy significantly enhances transferability over traditional methods.\nWe define the model that achieves optimal white-box attack performance when $\\epsilon = \\epsilon_{train}$ as NODE-AdvGAN, and the model that employs the best $E_{train}$ after tuning as NODE-AdvGAN-T. In this paper, we identify the optimal $E_{train}$ by performing hyperparameter tuning ablation experiments specifically on the VGG16 model. While tuning\n3.4. Loss Function\nWe follow the loss definitions from AdvGAN [13], where the loss functions for the generator and discriminator, denoted as $L_G$ and $L_D$, are minimized as $\\min_{G} L_G(G, D)$ and $\\min_{D} L_D(G, D)$ in the training phase.\nMore precisely, the generator aims to minimize the following loss function, which comprises three terms:\n$$L_G(G, D) = L_{CW}(G) + \\alpha L_{LSGAN}(G, D) + \\beta L_{hinge}(G).$$\nThe term $L_{CW}$, designed to deceive the target model f, is based on the Carlini-Wagner (CW) loss [31]. Recall that $f_i(x)$ represents the logits (pre-softmax outputs) of the output of target model f given the input x and the subscript i in $f_i(\\cdot)$ represents the logits for the ith category."}, {"title": null, "content": "\u2022 For targeted attacks, it is defined as:\n$$L_{CW}(G) = \\mathbb{E}_x\\left[ \\max_{i \\neq target}\\left\\{f_i(x + G(x))\\right\\} - f_{target}(x + G(x)), \\kappa \\right],$$\nwhere $\\mathbb{E}_x$ represents the expectation over all data points x i.e. $\\mathbb{E}_x = \\mathbb{E}_{x \\sim data} 1$, the constant $\\kappa \\ge 0$ is a confidence parameter used to control the adversarial strength, and target is the target class index to fool f on $x + G(x)$.\n\u2022 For untargeted attacks, the $L_{CW}$ is expressed as:\n$$L_{CW}(G) = \\mathbb{E}_x \\left[ \\max\\left( \\max \\left\\{ f_{true}(x + G(x)) - f_i (x + G(x)) \\right\\}, \\kappa \\right) \\right].$$\nHere true is the true class index for x.\nTo bound the magnitude of the perturbation noise G(x), a soft hinge/penalty loss on the Euclidean $L_2$ norm is added as:\n$$L_{hinge}(G) = \\mathbb{E}_x \\max(0, ||G(x)||_2 - c),$$\nwhere c \u2265 0 denotes a user-specified hyper-parameter. The model will start penalize the $L_2$ norm of the noise G(x) when its scale exceeds c. This term also helps stabilize the training of the GAN [27].\nThe least squares objective function LSGAN [38]is defined as:\n$$L_{LSGAN}(G, D) = \\frac{1}{2} \\mathbb{E}_x \\left[ (D(x + G(x)) - 1)^2 \\right].$$\nFor the discriminator D(\u00b7), the range of D(\u00b7) \u2208 [0, 1], where a value of D(\u00b7) closer to 1 indicates that the discriminator considers the image to be closer to an original, and vice versa. Minimizing this loss function helps the generator produce adversarial images x + G(x) closer to the original as much as possible and thus makes it more difficult to detect or discern.\nFor the discriminator, training minimizes the following loss function:\n$$L_D(G, D) = \\frac{1}{2} \\mathbb{E}_x \\left[ (D(x) - 1)^2 \\right] + \\frac{1}{2} \\mathbb{E}_x \\left[ D(x + G(x))^2 \\right],$$\nwhere $L_D(D, G)$ uses the least squares objective from LSGAN [38]. Minimizing $L_D$ trains the discriminator to recognize the original image x as authentic and the perturbed adversarial image x + G(x) as altered. The discriminator thus assesses the similarity between adversarial and original images, encouraging the generator to produce more convincing adversarial examples."}, {"title": "4. Experiments", "content": "This section presents the experimental results of our NODE-AdvGAN and NODE-AdvGAN-T models, along with comparisons to gradient-based models and the original GAN model. The datasets and experimental settings are documented in Section 4.1. In Section 4.2, we perform ablation studies and parameter selection using untargeted attacks on the CIFAR-10 dataset. Section 4.3 contains our main experiments, where we test adversarial examples across different datasets and present results for white-box and transferability attacks to evaluate the robustness of the models. Specifically, Section 4.3.1 covers untargeted attacks, while Section 4.3.2 focuses on targeted attacks."}, {"title": "4.1. Environment Setup", "content": "Datasets\nIn our study, we choose the Fashion-MNIST (FMNIST) [39] and CIFAR-10 [40] datasets as benchmark datasets to evaluate our method's performance. The FMNIST dataset comprises single-channel grayscale images structured into ten different classes. It includes a training set of 60,000 images, with 6,000 images per class, and a test set of 10,000 images, with 1,000 images per class. Originally, each image is 1 \u00d7 28 \u00d7 28 pixels in size. In the experiments, images are resized to a resolution of 1 x 32 x 32 pixels. The CIFAR-10 dataset features three-channel color images, similarly divided into ten categories. It comprises a training set of 50,000 images and a test set of 10,000 images, with each category represented by 5,000 and 1,000 images, respectively. Each image in this dataset is 3 \u00d7 32 \u00d7 32 pixels.\nTarget Image Classification Models\nIn selecting target models, we choose three commonly used classification architectures: the Visual Geometry Group Network (VGG) [41], Residual Network (ResNet) [11], and Densely Connected Convolutional Network (DenseNet) [42]. For each architecture, we select two configurations: VGG16 and VGG19, ResNet18 and ResNet34, DenseNet121 and DenseNet169. The classification performance of these architectures on clean samples is shown in Table 1. To improve robustness and accuracy, we apply random cropping with padding, followed by normalization.\nBaseline Methods\nTo validate the effectiveness of our method, we compare it with classical gradient-based methods, including FGSM[43], I-FGSM[44], MI-FGSM[12], and NI-FGSM[4], as mentioned in the introduction section. We also compare with the original AdvGAN[13], which employs a different generator. Since our approach differs from the original AdvGAN only in the generator component, this comparison can also be considered an ablation experiment. For both the FMNIST and CIFAR-10 datasets, we set the user-defined maximal noise intensity \u2208 = 15/255 for all methods. For the iterative methods I-FGSM, MI-FGSM, and NI-FGSM, we set the step size a = 2/255 (see Appendix A) and the number of iterations to 10, with a decay factor \u00b5 = 1 specifically for MI-FGSM and NI-FGSM. For the training of both the original AdvGAN and our method, the datasets undergo the same data augmentation techniques, specifically random cropping with padding, as used in training classification models.\nEvaluation Metrics\nTo evaluate method effectiveness, we use Attack Success Rate (ASR) and measure image similarity with Peak Signal-to-Noise Ratio (PSNR) [45] and Structural Similarity Index (SSIM) [46]. SSIM and PSNR calculations are performed using the piqa Python package (version 1.3.2) [47]. Our goal is to maximize the Attack Success Rate (ASR) while ensuring high values for PSNR and SSIM to maintain image quality.\nOther Experimental Settings\nThe batch size is set to 256 during the training, and the training spans 150 epochs. The initial learning rate is set at 0.002 and is reduced by half every 60 epochs. In terms of loss settings, we set k = 0, c = 0.1, a = 0.01, and \u03b2 = 0.01 in eq. (5); for NODE configurations, we set T = 0.05 and time integration steps N = 5.\nOur experimental setup employs the PyTorch framework [48] for model training. The experiments are executed on a server configured with the Linux-5.4.0 operating system and a Python 3.9 runtime environment. This server boasts 1.4 TiB of memory and utilizes an Intel Xeon(R) Gold 6240 CPU, operating at 2.60GHz with 72 cores. It features an NVIDIA GeForce RTX 3080 Ti GPU with 12GB of dedicated memory.\nTo implement NODEs, we leverage version 0.2.3 of the torchdiffeq package, which is optimized explicitly for GPU computations and facilitates reduced memory usage during backpropagation."}, {"title": "4.2. Parameter Selection and Ablation Experiments", "content": "In this subsection, we conduct ablation studies to evaluate key components of our model: first, the effect of specific values for a and \u03b2 in the loss function; second, the impact of varying the time integration step N; and third, the tuning of noise parameter $E_{train}$. All experiments were performed using white-box untargeted attacks on the CIFAR-10 dataset.\nTraining generators to create adversarial examples requires balancing parameter selection. For instance, with our loss function (see eq. (5)), setting a and \u03b2 values too low could lead to excessively large perturbations, while setting them too high may hinder effective perturbation. Therefore, we test values of 1, 0.1, 0.01, 0.001, and 0.0001 to identify the optimal settings. We use ASR and SSIM as evaluation metrics, with their average as the criterion for the selection of the final parameters.\nUsing NODE-AdvGAN, we conduct white-box attacks on the VGG16 model with the CIFAR-10 dataset, setting a perturbation limit $E_{train}$ = \u2208 of 15/255. The results for ASR, SSIM, and their average are shown in the heatmap in Figure 4. We set a = 0.01, and \u03b2 = 0.01 as the optimal pair.\nWe also examine how adjusting the time step N impacts NODE-AdvGAN's performance, specifically in terms of attack success rate and image similarity as measured by PSNR. We vary N from 2 to 9 to observe performance changes, as shown in Figure 5. This experiment reveals a significant improvement as N increases up to 5, after which the gains become less pronounced. Notably, our model outperforms the original AdvGAN when N is set to 3 or higher. We select N = 5 for the optimal value.\nAfter determining the optimal values for \u03b1, \u03b2, and N, we conduct ablation experiments on the noise parameter to identify the optimal $E_{train}$ for our NODE-AdvGAN-T method, with the goal of maximizing transferability. For this experiment, we set the test noise level to e = 15. We test values of $E_{train}$ ranging from 1 to 15 in increments of 1. The training is performed using VGG16 as the target model. Transferability results are performed on VGG19, ResNet18, and DenseNet169. The transfer ASR results and PSNR values of adversarial examples for various $E_{train}$ settings are presented in the left and right columns, respectively, of Figure 6. We test the original AdvGAN (top row) and XNODE-AdvGAN models (bottom row) and observed that, at approximately $E_{train}$ = 10, both models achieve the highest ASR and substantial PSNR values. For VGG19, however, better ASR and transferability results are observed when $E_{train}$ \u2265 10, likely due to architectural similarities with VGG16, which make transfer behavior similar to a white-box attack. Recognizing that the optimal $E_{train}$ may not always be feasible in real attack scenarios, we standardized $E_{train}$ = 10 across all following NODE-AdvGAN-T implementations. It is worth noting that, ideally, $E_{train}$ should be adjusted according to the target model in white-box attacks."}, {"title": "4.3. Results and Comparisons", "content": "4.3.1. Results on Untargeted Attack\nIn this subsection, we analyze the outcomes of untargeted attacks using our model XNODE-AdvGAN on the FMNIST and CIFAR-10 datasets. We examine the performance for white-box (where training and test models for the classification are the same) and transfer attacks separately. For easier representation, we will refer to a transfer attack in which the test target model is different from the training model as a transfer attack. For the white-box attacks, we employ Vgg16, ResNet34, and DenseNet121 as target models, whereas for the transfer attacks, we employ all models described in Section 4.1.\nIn summary, experiments across both datasets demonstrate that our models generate highly effective adversarial images with robust attack capabilities in transferable and white-box untargeted attacks, applicable to both grayscale and color images. Specifically, our models show significant improvements over all gradient-based noise generation methods across attack scenarios and consistently outperform the original AdvGAN, particularly in transfer attacks, due to optimized noise parameter tuning. In white-box attacks, XNODE-AdvGAN performs similarly to XNODE-AdvGAN-T, but XNODE-AdvGAN-T achieves notably better transfer results.\nWe also compare the computation/time cost on DenseNet121 as the target model on the CIFAR-10 dataset. A run time is measured for generating 10,000 adversarial instances during test time. As observed, our model outperforms other models in speed, except for being slower than AdvGAN. However, it is essential to note that generator-based models require training.\n4.3.2. Results on Targeted Attack\nIn adversarial machine learning, targeted attacks are much more difficult than untargeted attacks. This is mainly because the optimization of targeted attack navigates a more complex and narrow path to alter the model's decision specifically toward the target class. This involves finding a perturbation that not only pushes the input out of its current class but also precisely into the target class, resulting in a more constrained and complex optimization problem.\nGiven the multiple categories, we present experiments using DenseNet121 as the white-box target model on the CIFAR-10 dataset for targeted attacks. Our findings indicate that our method and AdvGAN significantly outperform gradient-based methods in white-box testing. Notably, gradient-based methods consistently struggle to achieve targeted misclassification, likely due to several factors. The primary challenge is the increased difficulty of the optimization problem: stricter constraints narrow the solution space, making the process highly sensitive to initial conditions.\nAnother contributing factor could be the high robustness of the DenseNet121 image classification model, which is trained using techniques like padding and random cropping. These techniques enhanced the model's resilience, making it more challenging for gradient-based targeted attacks to succeed. More specifically, the model's strong robustness allows it to classify images accurately, even with certain perturbation levels. Mathematically, because the model's output remains unchanged within a neighborhood of the input data, the derivative of the cost function around the data is, therefore, nearly zero, causing gradient descent methods to fail.\nCompared to the original AdvGAN, our models NODE-AdvGAN and NODE-AdvGAN-T achieved significantly higher targeted attack success rates of 96.86% and 96.37%, respectively, surpassing AdvGAN's 85.48%. Additionally, NODE-AdvGAN and NODE-AdvGAN-T demonstrated greater similarity to the original images, with an average difference of 2.51 and 1.71, a respective increase of 9.92 and 6.76. This indicates that our methods maintain high attack success rates and introduce less perturbation.\npabilities compared to AdvGAN. Additionally, the NODE-AdvGAN-T method further enhances transfer attack capability beyond that of NODE-AdvGAN."}, {"title": "5. Conclusion", "content": "In this paper, we presented NODE-AdvGAN to improve the quality and transferability of adversarial examples. By leveraging a dynamic-system perspective, we treated the adversarial generation as a continuous process, mimicking the iterative nature of traditional gradient-based methods. This approach allowed us to generate smoother and more precise perturbations while preserving important features of the original images. We also introduced NODE-AdvGAN-T, which enhances transferability by tuning the noise parameters during training.\nOur experiments showed that NODE-AdvGAN and NODE-AdvGAN-T achieve higher attack success rates and better image quality than existing methods. This work demonstrates the effectiveness of combining PDE-inspired dynamic systems with machine learning to address adversarial robustness in AI models.\nFor future work, we could focus on defending against adversarial attacks. One approach would be using our generated adversarial examples to train classifier models, making them more robust against such attacks. We could also explore combining multiple classification models to create a super-classifier with superior accuracy and robustness. Additionally, integrating our approach with other generator structures, such as diffusion models, could further enhance adversarial generation. Finally, we could investigate more efficient NODE architectures or other continuous-time dynamic models to reduce computational costs while maintaining effectiveness."}, {"title": "Appendices", "content": "A. Formula: Gradient-based Methods\nTake a stepsize 0 < a < 1 and a perturbation tolerence 0 < \u0454 \u00ab 1 .\n\u2022 FGSM:\n$$x_{adv"}, "x + \\epsilon \\text{ sign}(\\nabla_x J(\\Theta, x, y)),$$\nwhere function sign : R \u2192 {\u00b11,0}, mapping positive numbers to 1, negative numbers to -1 and zero to 0.\n\u2022 Iterative FGSM (I-FGSM):\n$$x_{n+1} = Clip \\left(x_n + a \\cdot \\text{ sign}(\\nabla_x J(\\Theta, x_n, y) - x, -\\epsilon, \\"]}