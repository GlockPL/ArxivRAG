{"title": "Extracting Finite State Machines from Transformers", "authors": ["Rik Adriaensen", "Jaron Maene"], "abstract": "Fueled by the popularity of the transformer architecture in deep learning, several works have investigated what formal languages a transformer can learn. Nonetheless, existing results remain hard to compare and a fine-grained understanding of the trainability of transformers on regular languages is still lacking. We investigate transformers trained on regular languages from a mechanistic interpretability perspective. Using an extension of the L* algorithm, we extract Moore machines from transformers. We empirically find tighter lower bounds on the trainability of transformers, when a finite number of symbols determine the state. Additionally, our mechanistic insight allows us to characterise the regular languages a one-layer transformer can learn with good length generalisation. However, we also identify failure cases where the determining symbols get misrecognised due to saturation of the attention mechanism.", "sections": [{"title": "1. Introduction", "content": "Transformers (Vaswani et al., 2017) have become a mainstay architecture in natural language processing and deep learning more generally. There has hence been increasing interest in characterising the expressive power of this architecture. Significant effort has been devoted to proving what formal languages can or cannot be expressed by the transformer architecture (Ackerman & Cybenko, 2020; Merrill, 2021; Strobl et al., 2023). The trainability remains less well understood, however. Indeed, the fact that a transformer can express a formal language does not necessarily mean it can also learn it from data.\nAs an example, consider the parity language, which consists of all binary sequences with an odd number of ones. Chiang & Cholak (2022) proved that soft attention transformers can express this language. However, empirical studies found that transformers trained on the parity language fail to generalise to longer sequences (Anil et al., 2022; Del\u00e9tang et al., 2022; Liu et al., 2023). Worryingly, Bhattamishra et al. (2020) even found that transformers fail to learn the parity language for sequences with in-distribution lengths.\nHow can we draw consistent conclusions from these seemingly contradictory results? Strobl et al. (2023) show that different underlying assumptions cause these inconsistencies. Additionally, the specific setup of the training task and evaluation metric are important. We argue that using mechanistic interpretability, by unveiling exactly what the transformers learn, allows us to draw more robust conclusions on trainability, thus preventing future contradictions.\nTo achieve a mechanistic understanding of transformers trained on regular languages we automatically reverse-engineer the finite state machines they learned. This approach, originally proposed for recurrent neural networks (RNN) (Weiss et al., 2018b), relies on the classical L* algorithm (Angluin, 1987) to extract state machines using queries and counterexamples. We further localise the states of finite state machines in the transformer and investigate how faithfully an extracted state machine models the transformer. We observe that when a transformer has effectively generalised on a language, the states of the target finite state machine are represented as directions in the transformer's output layer. Upon identifying these directions, we can precisely determine what occurs in the instances where the transformer fails."}, {"title": "2. Preliminaries", "content": "We will assume basic familiarity with transformers (Vaswani et al., 2017). In this section, we briefly introduce two formalisms commonly used to characterise transformers: finite state machines and Boolean circuits.\nWe write \\(\\sum\\) or \\(\\Gamma\\) for a finite set of symbols, also called an alphabet. The Kleene closure \\(\\sum^*\\) of an alphabet \\(\\sum\\) is the set of all finite sequences using the symbols of \\(\\sum\\). For example, the binary alphabet \\(\\sum = \\{0,1\\}\\) has as closure \\(\\sum^* = \\{\\epsilon,0,1,00, 01, 10, 11, . . .\\}\\). We denote the empty sequence with e. A language L over the alphabet \\(\\sum\\) is a subset of \\(2^*\\)."}, {"title": "2.1. Finite State Machines", "content": "Finite state machines are among the most well-known models to describe formal languages. Although several variants exist, most can be reduced to deterministic finite automatons. For a more thorough introduction, we refer to Hopcroft et al. (2006).\nDefinition 2.1. A deterministic finite automaton (DFA) is a tuple \\((Q, \\sum, \\Gamma, q_0, \\delta)\\), where\n\u2022\n\u2022 Q is a finite set of states.\n\u2022 \\(\\sum\\) is the input alphabet.\n\u2022 \\(q_0 \\in Q\\) is the starting state.\n\u2022 \\(\\delta: Q \\times \\sum \\rightarrow Q\\) is the transition function, mapping the current state and an input symbol to a new state.\n\u2022 \\(F \\subset Q\\) is the set of final states.\nGiven a sequence \\(s \\in \\sum^*\\), a symbol \\(\\sigma \\in \\sum\\) and a state \\(q \\in Q\\), we define the repeated application of the transition function \\(\\hat{\\delta} : Q \\times \\sum^* \\rightarrow Q\\) as \\(\\hat{\\delta}(q, \\sigma.s) = \\delta(\\delta(q, \\sigma), s)\\), with \\(\\hat{\\delta}(q, \\epsilon) = q\\) in the base case. We say that a DFA accepts a sequence s if and only if \\(\\hat{\\delta}(q_0, s) \\in F\\). Each DFA D hence represents a language \\(L(D) = \\{s | \\hat{\\delta}(q_0, s) \\in F\\}\\). A language is regular if it can be represented by a DFA.\nA Moore machine is a DFA without final states and with an output function \\(\\gamma : Q \\rightarrow \\Gamma\\), where \\(\\Gamma\\) is the output alphabet. On each transition, a Moore machine not only takes an input symbol but also produces an output symbol using \\(\\gamma\\). So while DFAs are sequence classification models, Moore machines are sequence-to-sequence models, aligning more closely with transformers. Note that DFAs can be seen as a special case of Moore machines, where \\(\\Gamma = \\{Reject, Accept\\}\\)."}, {"title": "2.2. Circuit Complexity", "content": "A Boolean circuit \\(C_n\\) is a Boolean formula represented as a directed acyclic graph. For a full introduction, we refer to Arora & Barak (2006). A circuit \\(C_n\\) has n input nodes and a single output node with no outgoing edges. Inner nodes represent either the AND, OR, or NOT operations. The NOT nodes have a single incoming edge, while AND and OR nodes can have an arbitrary number of incoming edges. A circuit computes a function \\(C_n: \\{0,1\\}^n \\leftrightarrow \\{0, 1\\}\\), accepting a sequence \\(s \\in \\{0,1\\}^n\\) if \\(C_n(s) = 1\\). The depth of a circuit \\(D(C_n)\\) is the length of the longest path between an input node and the output node. The size of a circuit \\(|C_n|\\) is the number of nodes in \\(C_n\\).\nA circuit family \\(C = \\{C_n\\}_{n \\in \\mathbb{N}}\\) is a set containing a Boolean circuit for each sequence length. The family C induces a function \\(C(s) = C_{|s|}(s)\\) and accepts the language \\(L(C) = \\{w | C(w) = 1\\}\\). The depth and size of C are now functions \\(n \\rightarrow D(C_n)\\) and \\(n \\leftrightarrow |C_n|\\).\nWe next present two relevant classes of languages in terms of Boolean circuits.\nDefinition 2.2. A language is in AC0 if it can be recognised by a family of circuits with constant depth and size polynomial in n.\nDefinition 2.3. The class TC0 is defined the same as AC0, but also allows MAJORITY nodes. These nodes output 1 if and only if at least half of their inputs are 1.\nIt is well-known that \\(AC^0 \\subsetneq TC^0\\) (Vollmer, 1999). Notably, the hierarchy of Boolean complexity classes does not coincide with the Chomsky hierarchy. Indeed, the regular languages span different circuit classes, that are known or conjectured to be different. It has been conjectured that the expressivity of transformers aligns with Boolean complexity, and not the Chomsky hierarchy (Strobl et al., 2023).\nThe class of regular languages in AC0 can be further divided in the star-free and non-star-free languages. The star-free can in turn be subdivided according to their dot-depth.\nDefinition 2.4. (Barrington et al., 1992) The star-free regular languages are those subsets obtained by closing over concatenation, complement and union beginning with the letters of the alphabet and the empty sequence.\nFor example, the language containing no consecutive zeros can be constructed as \\(\\sum^*00\\sum^*\\) and is therefore star-free.\nDefinition 2.5. The dot-depth of a star-free language is defined inductively. Closing the letters of the alphabet and the empty sequence over complement and union yields the languages of dot-depth zero. The dot-depth of a star-free language is the smallest n, such that it can be constructed from the closure of star-free languages of dot-depth n\u22121 under concatenation, using complement and union."}, {"title": "2.3. Example Languages", "content": "We consider the following regular languages defined over the binary alphabet \\(\\sum = \\{0,1\\}\\). These languages are chosen to cover the star-free languages, AC0 and TC0, all known to be expressable by transformers (Merrill & Sabharwal, 2023).\n\u2022 Ones: 1*. Ones includes all sequences containing only ones. Ones is star-free with dot-depth one.\n\u2022 First: 1(0 | 1)*. First contains all sequences that start with a one. First is star-free with dot-depth one.\n\u2022 Depth-bounded Dyck: \\(D_i\\). The depth-bounded Dyck languages contain all sequences where the two symbols are correctly balanced and the maximal nesting depth is bounded. We denote the Dyck language with maximum depth i as \\(D_i\\). For example, \\(D_1 = (01)^*\\) and \\(D_2 = (0(01)^*1)^*\\). The language \\(D_1\\) is star-free with dot-depth i."}, {"title": "3. Trainability of Transformers", "content": "In line with previous work, we distinguish between expressivity and trainability (Strobl et al., 2023). Expressivity asks whether a transformer can represent a language, while trainability asks whether a transformer can learn a language from data. Although we investigate trainability, expressivity is still relevant as it is a prerequisite for trainability.\nAs opposed to expressivity, the study of trainability on transformers has been mostly empirical. Unfortunately, these studies can be hard to compare due to subtle variations in the task. We argue that all tasks can be unified as learning to simulate a Moore machine.\nOur work focuses on the trainability of regular languages and finite state machines. Although it is generally accepted that transformers are not well aligned with the Chomsky hierarchy (Del\u00e9tang et al., 2022), the existing work on expressivity is often still structured along this hierarchy (Bhattamishra et al., 2020; Merrill, 2021). Moreover, this allows us to draw upon the methods and results for RNNs, which are more closely related to regular languages (Giles et al., 1991). Finally, Bricken et al. (2023) recently observed transition functions resembling state machines in transformers trained on natural language, demonstrating the real-world relevance of this formalism.\nNext character prediction Bhattamishra et al. (2020) conducted a large experimental study on the trainability of transformers on different languages in the Chomsky hierarchy. They trained on a multilabel classification task where, given a sequence, the transformer predicts which next symbols are a valid continuation. A symbol \\(\\sigma\\) is a valid continuation of a sequence w if there exists a \\(w' \\in \\sum^*\\) such that \\(\\hat{\\delta}(w.\\sigma.w') \\in F\\). The transformer accepts a sequence when it considers each symbol a valid continuation and predicts the end-of-sequence symbol valid on the final position. We call this setting next character prediction. We can frame this as a Moore machine with the output alphabet \\(\\Gamma = 2^{\\sum \\cup \\{Accept\\}}\\) where \\(\\sum\\) is the language alphabet.\nBhattamishra et al. (2020) presented the transformer only with positive examples, resembling a language modelling task. In their analysis, they hypothesised that transformers only generalise well on star-free languages with dot-depth 1, and not for higher depths."}, {"title": "State prediction", "content": "Liu et al. (2023) investigated transformers learning semiautomata. Semiautomata are a DFA variant without a notion of final or non-final states. Given an input sequence s, the transformer is trained to predict the sequence of states visited by the target semiautomaton. So when \\(\\delta\\) is the transition function, the transformer must predict the generalised transition function \\(\\hat{\\delta}(q_0, s_{1:i})\\) for each position \\(1 \\leq i \\leq |s|\\). This is a multiclass classification task and can be seen as a Moore machine with \\(\\Gamma = Q\\), meaning \\(\\gamma\\) is the identity function.\nLiu et al. (2023) argue that transformers must use shortcut solutions to compute the recurrent dynamics of these automata in their limited number of layers. They proved that any semiautomaton can be simulated up to length T by a transformer with O(log(T)) layers and a sufficiently large embedding, attention and dense layer width. Furthermore, a constant depth solution exists for solvable semiautomata, with sufficiently large embedding and attention width, and a dense layer width linear in T. Experimentally, they found transformers can learn any semiautomaton with near-perfect accuracy, including Parity (a non-star-free language), which is seemingly at odds with the results of Bhattamishra et al. (2020). However, the found solutions did not generalise to longer sequence lengths. The authors did not conduct a full mechanistic analysis, nor claim transformers find the shortcuts they proposed."}, {"title": "Membership prediction", "content": "Predicting membership of a language can also be modelled as a Moore machine. The output function is then binary, \\(\\Gamma = \\{Accept, Reject\\}\\), and the problem reduces to simulating a DFA as \\(\\gamma\\) encodes the final states (Weiss et al., 2018a;b).\nWe refer to the minimal DFA accepting a language as the target DFA of that language. The target Moore machine of a language is then equivalent to the target DFA with the appropriate output function and alphabet.\nThe relationship between different training tasks is now more explicit. Membership prediction is similar to state prediction but with partial observability, as all accepting states (and all non-accepting states) get the same label. For the next character prediction task, the observability depends on the specific machine, in particular, whether the sets of next valid symbols are unique in each of the states."}, {"title": "4. Extracting State Machines", "content": "Assuming that a transformer faithfully implements a state machine, it must be possible to extract said state machine. Weiss et al. (2018b) proposed a method for extracting DFAs from RNNs using the exact learning algorithm L* (Angluin, 1987). We briefly summarise their approach, before specifying our changes.\nThe L* algorithm learns a DFA by asking queries to a teacher oracle. These queries are either 1) membership queries, asking whether the target language accepts a given sequence, or 2) equivalence queries, asking whether a given DFA implements the target language and returning a counterexample should they differ. For a full explanation of the L* algorithm, we refer to Appendix A.\nThe method of Weiss et al. (2018b) boils down to implementing these two queries using an RNN. A membership query is straightforward as the RNNs are trained on membership prediction. Answering equivalence queries is generally intractable, however. For this reason, the hidden activation space is partitioned, such that each partition represents a state and state transitions are defined by a single sample in that partition (Giles et al., 1991). In other words, Weiss et al. (2018b) keep track of two DFAs: the DFA D being extracted by L*, and the DFA D' specified by the partitioning of the activation space.\nTo answer equivalence queries, the states of D and D' are traversed in parallel. Two types of conflicts may occur. The first possibility is that D is different from the target language of the RNN, in which case we can return a counterexample. The second possibility is that the partitioning D' differs from D, in which case the partitioning is refined.\nWeiss et al. (2018b) employ three additional tactics to improve practical performance. 1) They apply a time limit after which the algorithm is stopped and the final proposed DFA is returned. 2) They use an aggressive initial splitting strategy on the first set of conflicting vectors, by splitting along the dimensions where these vectors differ the most. The number of dimensions d to initially split on is a hyper-parameter called the initial split depth. 3) They sometimes supply starting examples of at least one positive and one negative example for the L* learner to get started. These last two additions prevent the algorithm from terminating prematurely on an automaton with a single state."}, {"title": "4.1. Extracting Moore Machines", "content": "With only minor changes, the above method can be adapted to extract Moore machines from transformers instead of DFAs from RNNs.\nThe method of Weiss et al. (2018b) is not specific to RNNs but can be used on any neural acceptor as long as the chosen state vectors are consistent, i.e., they can be partitioned such that all vectors belonging to the same cluster are either rejecting or accepting. In a one-layer transformer, we propose using the final activations of the residual stream after layer normalisation. Layer normalisation projects the activations on a d \u2013 1 dimensional hypersphere of radius \\(\\sqrt{d}\\), with d the dimension of the residual stream (Brody et al., 2023).\nSo at that point, all activation vectors are normalised*. From these activation vectors, the output is computed with a final affine transformation. As such, they are analogous to the state vectors of the RNNs in the original study.\nTo find Moore machines instead of DFAs we use a generalisation of L*. More specifically, we learn a single Moore machine, which is a special case of the Moore machine product considered by Moerman (2019). As opposed to learning DFAs, learning Moore machines allows us to consider transformers with an arbitrary output alphabet."}, {"title": "5. Experiments", "content": "In this section, we analyse the performance of transformers trained on regular languages. Furthermore, we apply our extraction method to analyse the Moore machines learned by transformers on the different settings discussed in Section 3."}, {"title": "5.1. Experiment Setup", "content": "We use the same training method and transformer architecture throughout all experiments to ensure reproducibility and comparability.\nDatasets The training datasets contain 10,000 sequences of length 32 labelled using the task-specific target Moore machine. The validation sets contain 2,000 sequences of length 100. To evaluate the transformers after training we use one test set of length 100 and another of length 1,000, both containing 1,000 sequences. The data is either sampled uniformly at random or contains only positive examples, i.e. examples in the target language. We call the latter positive-only learning and employ it when reproducing the results of Bhattamishra et al. (2020). The positive examples are generated by traversing the target DFA at random after removing the garbage state for efficiency.\nWe often use length ranges as for some languages the sequence classification is identical for sequences of certain fixed lengths. For example, exactly the sequences of even length are part of the language \\(C_2\\). Each example is prepended with a unique beginning-of-sequence symbol, allowing the transformer to learn an output for the starting state.\nArchitecture We use one-layer transformers with soft attention (Vaswani et al., 2017), pre-norm layer normalisation (Nguyen & Salazar, 2019), and rotary positional encodings (Su et al., 2024). The latter two techniques have been demonstrated to enhance the original transformer architecture in terms of training efficiency and length generalisation, respectively. We use a residual stream width of 16, a single-layer MLP layer of width 64 and 4 attention heads. This is a subset of the transformer configurations studied in Bhattamishra et al. (2020). We do not compare different transformer configurations as this has been extensively done before and we instead aim to show transformers can be mechanistically interpreted."}, {"title": "Training", "content": "We use the Adam optimizer (Kingma & Ba, 2015) with a learning rate of \\(3 \\cdot 10^{-4}\\), no learning rate schedule, and a batch size of 32. We employ early stopping on the validation loss, with a patience of 100 epochs, and select the best-performing weights on the validation set. We run each experiment three times with identical hyperparameters and a different seed."}, {"title": "5.2. Positive-Only Character Prediction", "content": "First, we consider the next character prediction setting from Bhattamishra et al. (2020). Remember that in this setting, a sequence is accepted when the transformer considers each symbol valid and predicts the end-of-sequence symbol as valid on the final position. We refer to this evaluation measure as the sequence accuracy. The transformers generalise well on the star-free languages (Ones, First, \\(D_1\\) and \\(D_2\\)). On the non-star-free languages in AC0 (\\(C_2\\) and \\(C_3\\)), the transformers perfectly fit the training set but fail to generalise. On Parity, which lies outside AC0, they do not perfectly learn the training set. However, the evaluation on random data reveals that the transformers do worse or barely outperform the baseline, although the sequence accuracy stays high.\nHow can we explain this? The gradient update of a transformer is calculated based on the predictions made at all positions of the input sequence. Therefore, it is effectively trained on all prefixes of the sequences in the training set."}, {"title": "Extraction", "content": "We now extract a Moore machine from each of the transformers using the method described in Section 4. In accordance with Weiss et al. (2018b), we make use of an initial split depth of 10 and supply a short positive and negative example whenever the algorithm terminates on the trivial one-state machine. Furthermore, the algorithm times out after 30 seconds."}, {"title": "5.3. Positive-Negative State Prediction", "content": "From here on, we analyse transformers trained on randomly sampled data under the state prediction task to avoid learning unintended target languages. We additionally study the languages \\(G_i\\) as we have shown the former are the actual languages transformers manage to generalise well on under the positive-only character prediction setting.\nWe train three transformers on the studied languages, now including \\(G_1\\) and \\(G_2\\), and extract Moore machines from them. Both the training and extraction are performed as before.\nThe transformers learned perfectly on the training set for all languages, except Parity. On \\(C_2\\) and \\(C_3\\) they did not generalise well outside of the training set. At least one transformer had a perfect score even for sequences of length 1000 for Ones, \\(G_1\\) and \\(D_1\\). For First, \\(G_2\\) and \\(D_2\\), there was generalisation but not up to such a long length. The extraction algorithm terminated and extracted the exact target Moore machine for all transformers except for one trained on Parity and two trained on \\(D_2\\), where it timed-out, returning a large machine."}, {"title": "6. State Localisation", "content": "We next attempt to localise how the states of the extracted finite state machines are represented in the final layer of our transformers. For each language, we focus on the model that generalised best.\nAs discussed before, the activations of the final layer all have norm \\(\\sqrt{d}\\) due to the layer normalisation, with d the dimension of the residual stream. Therefore, only the direction of the activation vector matters. Within the activation space, we can find inherently meaningful directions by looking at those directions along which the probability of one of the output labels is maximised. We will refer to them as maximal probability directions. We observe that these directions are arranged opposingly when there are two outputs and in a plane when there are three outputs. This makes sense as, due to the softmax activation, making one output large causes the others to be small. These arrangements maximise the angles between the directions of the different outputs.\nThe computation of the transformer can thus be decomposed in two steps. Firstly, the transformer computes, for each symbol, a fixed-norm activation vector in its final layer. Next, this activation vector is transformed into the output. The directions that maximise the output probabilities span a subspace of the activation space of the final layer. Therefore, the only part of the activation vector that influences the output is the component in that subspace. If an activation vector has a large perpendicular component, the component in the subspace must be small and therefore the confidence of the predictions drops.\nWhen a transformer has learned to generalise well, we assume the transformer has similar activations on input sequences for which the extracted machine is in the same state. We therefore introduce the notion of returning suffixes.\nDefinition 6.1. A returning suffix associated with a state \\(q \\in Q\\) of a Moore machine \\(M = (Q, q_0, \\delta, \\gamma, \\sum, \\Gamma)\\) is a sequence of symbols \\(s_q \\in \\sum^*\\), such that \\(\\hat{\\delta}(q, s_q) = q\\).\nNotice that the empty sequence \\(\\epsilon\\) is a returning sequence for all states. All returning suffixes for a state q up to length L can be found by performing a breadth-first traversal of depth L of the extracted Moore machine and adding all paths returning to q to the set of returning suffixes. For some states, the number of returning suffixes is exponential in L. Therefore, we used a beam search to generate a dataset of returning suffixes. We then look at the transformers activation on \\(p_q.s_q\\), with \\(p_q \\in \\sum^*\\) being a prefix, such that \\(\\hat{\\delta}(q_0, p_q) = q\\). We always choose the shortest such \\(p_q\\). If the transformer has generalised well, we expect to find the activations on the returning suffixes to lie in the same direction.\nOf the languages we study, our transformers managed to generalise well on the languages Ones, First, \\(G_1, G_2, D_1\\) and \\(D_2\\). Indeed, we observe that they have, for each state, a designated direction in their final layer along which they orient the activation vector of a sequence ending in that state. Moreover, these designated directions are well aligned with the direction maximising the correct output probability. The languages \\(D_1\\) and \\(D_2\\) are an exception to the rule, as for these languages, transformers only seem to represent the garbage state well.\nFor the languages \\(C_2\\) and \\(C_3\\), we observe that transformers also can align the activation vectors along such a designated direction, but only for lengths seen in training. For the Parity language, our transformers never seem to align the activation vectors at all.\nFurther analysis is necessary to explain exactly how this alignment comes about. However, observing when a transformer is successful in representing a state and when it is not conveys a great deal of information and allows for a hypothesis on the trainability of one-layer transformers."}, {"title": "7. Characterising Trainability", "content": "Finally, we can propose a coherent characterisation of the trainability of one-layer transformers on regular languages using our mechanistic insights. Concretely, we hypothesise that regular languages are trainable when the state of their minimal DFA is determined by a finite number of symbols. To better understand this, we consider the concept of reset sequences (Maler & Pnueli, 1994). A sequence of symbols is a reset sequence for a DFA if it maps every state to a single state under the generalised transition function.\nDefinition 7.1. We call \\(s \\in \\sum^*\\) a reset sequence for state \\(q_r\\) of DFA \\(D = (Q, F, \\sum, q_0, \\delta)\\) if\n\\[\\forall q \\in Q : \\hat{\\delta}(q, s) = q_r\\]\nFor \\(G_1\\), all sequences of length one are reset sequences. Therefore, the correct state can be determined by looking only at the final symbol. For \\(G_2\\), the sequences 00 and 11 are reset sequences, but 01 and 10 are not. For Ones, a single zero always brings the target DFA to the garbage state and is therefore a reset sequence. Moreover, when the sequence contains a zero anywhere the target DFA is in the garbage state. The occurrence of a zero thus determines the state. Finally, for First, the correct state depends solely on the first symbol.\nFor \\(D_1\\) and \\(D_2\\), only the garbage states have reset sequences. Therefore, the condition in our hypothesis is only met in these garbage states. This is in line with our observation in the previous section that our transformers only represent the garbage state well for these languages.\nThese are thus three slightly different cases where a finite number of symbols determines the automaton's state. Either there are reset sequences, the occurence of a particular symbol anywhere in the sequence determines the state, or the symbol at a certain position determines the state.\nOur results in Appendix D.8 are in line with our hypothesis that one-layer transformers use this property to generalise well on these languages. Through inspection of the attention patterns, we indeed observe that the transformer trained on Ones pays strong attention to zero symbols. For the language First, we observe that the transformer pays strong attention to the symbol at the first position. The transformer trained on \\(G_1\\) pays stronger attention to the current symbol. The transformer trained on \\(G_2\\) pays stronger attention to the current symbol and the two previous symbols, having one head for each of these three positions."}, {"title": "7.1. Attention Saturation", "content": "Although we seem to have a clear understanding of how the transformers represent states, it is still unclear why they do not generalise perfectly. Hahn's lemma provides an explanation.\nTheorem 7.2. (Hahn, 2020) Given a soft attention transformer on an input of n symbols. If we exchange one single input symbol, then the change in the resulting activation at the decoder layer is bounded as O(1) with constants depending on the parameter matrices.\nAs the one-layer transformers always use a finite number of symbols to determine the correct output, they eventually fail when these determining symbols are unable to sufficiently influence the output due to the effect of the unimportant symbols. In other words, the attention mechanism becomes saturated.\nWe verified that this indeed happens in practice. A transformer trained on the language Ones fails on a long sequence 011...11 when the effect of the zero which determines the state becomes too small. A transformer trained on the language First also fails on a sequence 011... 11 as the influence of the first symbol becomes insufficient. A transformer trained on G1, fails on a sequence 00...001 because the effect of the one is not sufficient.\nSomewhat differently, a transformer trained on G2 fails on a sequence 0101 ... 0101 as there are no reset sequences for the transformer to use. Transformers trained on D\u2081 and D2 also quickly fail on a sequence 0101 ... 0101. This again shows that the transformer has difficulty representing the non-garbage states for these two languages.\nThis further validates our hypothesis that one-layer transformers depend on a finite number of symbols to recognise these languages, as they fail when this property no longer holds. Finally, we look at how the activation vector computed by the transformer varies in these failure cases. We find that the activation vectors start to deviate from the direction the correctly classified activation vectors were aligned with, confirming that the wrong output is indeed caused by the inability to detect which state the sequence should be classified as."}, {"title": "8. Conclusion", "content": "We identified a general task setting in which the seemingly contradictory results of previous studies regarding the trainability of transformers can be compared. As illustrated on the positive-only next character prediction task, having a deeper understanding of what is learned during training leads to more robust conclusions on trainability. We showed that the extraction technique proposed by Weiss et al. (2018b) can be used to extract Moore machines from transformers with minimal adaptations. We showed that mechanistic interpretability is a productive approach to empirically investigate and verify the trainability of transformers on formal languages. We successfully localised meaningful directions in the output layer of the transformers along which the transformers orient the activations on sequences that end in the same state. We empirically found that one-layer transformers can find generalisable solutions for the languages Ones, First, G\u2081 and G2 which implies that for languages where a finite number of symbols determine the automaton's state, a stronger lower bound exists than proposed by Liu et al. (2023). Finally, we showed failure cases can be found by reasoning over the structure of the target finite state machine."}, {"title": "9. Future Work", "content": "The languages studied in this paper are anecdotal evidence of tighter lower bounds on the trainability of transformers on some classes of regular languages. A more thorough theoretical characterisation of this lower bound is necessary, as well as further experimental validation of our trainability hypothesis, for example, through ablations and causal interventions.\nSeveral other training tasks could be investigated. Most of these can be framed as simulating a Moore machine with a task-specific output function, such that our method works without adaptations. We are particularly interested in direct membership prediction. This might be more difficult as the output function does not explicitly distinguish between two accepting (or two non-accepting) states, as was the case under the state prediction task. Additionally, other transformer variants could be studied, including transformers with multiple layers.\nIn general, a transformer maps a sequence of input symbols to a probability distribution over sequences of output symbols. If the confidence of the predictions is high, this can be seen as a sequence-to-sequence transformation from the input to the output alphabet, as we have done. This does not hold, however, when the model is less confident. By instead looking at weighted finite automata, it might be possible to model transformers more accurately. This might allow extraction of small machines with high agreement even when the transformers do not perform the original task perfectly.\nThe extraction of state machines from transformers could also be applied as structure learning for neurosymbolic frameworks which rely on weighted grammars (Winters et al., 2022).\nOur work is restricted to regular languages, although the trainability of transformers is not well aligned with the Chomsky hierarchy. Therefore, a mechanistic interpretability approach looking at non-regular formal languages would likely improve our current understanding."}]}