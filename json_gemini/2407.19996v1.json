{"title": "Reproducibility Study of \u201cITI-GEN: Inclusive Text-to-Image Generation\u201d", "authors": ["Daniel Gallo Fern\u00e1ndez", "R\u0103zvan-Andrei Mati\u015fan", "Alejandro Monroy Mu\u00f1oz", "Janusz Partyka"], "abstract": "Text-to-image generative models often present issues regarding fairness with respect to certain sensitive attributes, such as gender or skin tone. This study aims to reproduce the results presented in \u201cITI-GEN: Inclusive Text-to-Image Generation\" by Zhang et al. (2023a), which introduces a model to improve inclusiveness in these kinds of models. We show that most of the claims made by the authors about ITI-GEN hold: it improves the diversity and quality of generated images, it is scalable to different domains, it has plug-and-play capabilities, and it is efficient from a computational point of view. However, ITI-GEN sometimes uses undesired attributes as proxy features and it is unable to disentangle some pairs of (correlated) attributes such as gender and baldness. In addition, when the number of considered attributes increases, the training time grows exponentially and ITI-GEN struggles to generate inclusive images for all elements in the joint distribution. To solve these issues, we propose using Hard Prompt Search with negative prompting, a method that does not require training and that handles negation better than vanilla Hard Prompt Search. Nonetheless, Hard Prompt Search (with or without negative prompting) cannot be used for continuous attributes that are hard to express in natural language, an area where ITI-GEN excels as it is guided by images during training. Finally, we propose combining ITI-GEN and Hard Prompt Search with negative prompting.", "sections": [{"title": "1 Introduction", "content": "Generative AI models that solve text-to-image tasks pose a series of societal risks related to fairness. Some of them come from training data biases, where certain categories are unevenly distributed. As a consequence, the model may ignore some of these categories when it generates images, which leads to societal biases on minority groups.\nIn order to tackle this issue, Zhang et al. (2023a) introduce Inclusive Text-to-Image Generation (ITI-GEN), a method that generates inclusive tokens that can be appended to the text prompts. By concatenating these fair tokens to the text prompts, they are able to generate diverse images with respect to a predefined set of attributes (e.g. gender, race, age). For example, we can add a \u201cwoman\u201d token to the text prompt \u201ca headshot of a person\" to ensure that the person in the generated image is a woman.\nIn this work, we aim to focus on:"}, {"title": "2 Scope of reproducibility", "content": "In the original paper, the authors make the following main claims:\n1) Inclusive and high-quality generation. ITI-GEN improves inclusiveness while preserving image quality using a small number of reference images during training. The authors support this claim by using KL divergence and FID score (Heusel et al., 2017) as metrics.\n2) Scalability to different domains. ITI-GEN can learn inclusive prompts in different scenarios such as human faces and landscapes.\n3) Plug-and-play capabilities. Trained fair tokens can be used with other similar text prompts in a plug-and-play manner. Also, the tokens can be used in other text-to-image generative models such as ControlNet (Zhang et al., 2023b).\n4) Data and computational efficiency. Only a few dozen images per category are required, and training and inference last only a couple of minutes.\n5) Scalability to multiple attributes. ITI-GEN obtains great results when used with different attributes at the same time.\nIn this work, we run experiments to check the authors' statements above. Additionally, we study some failure cases of ITI-GEN and propose other methods that handle negations in natural language."}, {"title": "3 Methodology", "content": "The authors provide an open-source implementation on GitHub\u00b9 that we have used as the starting point. To make our experiments completely reproducible, we design a series of bash scripts for launching them. Finally, since the authors did not provide any code for integrating ITI-GEN with ControlNet (Zhang et al., 2023b), we implement it ourselves to check the compatibility of ITI-GEN with other generative models. All of these are better detailed in Section 3.4."}, {"title": "3.1 Model description", "content": "ITI-GEN is a method that improves inclusiveness in text-to-image generation. It outputs a set of fair tokens that are appended to a text prompt in order to guide the model to generate images in a certain way. It achieves this by using reference images for each attribute category. For example, if we use the prompt \"a headshot of a person\" and provide images of men and women, the model will learn two tokens, one for each gender.\nGiven a set of $M$ attributes where each attribute $m$ has $K_m$ different attribute categories, ITI-GEN learns a set of fair tokens that represent each attribute category. For each combination, the corresponding fair tokens are aggregated and concatenated to the original text prompt $T$ to build a inclusive prompt $P$. We denote the set of all inclusive prompts with $\\mathcal{P}$, and the set of all inclusive prompts that correspond to category $i$ of attribute $m$ with $P_m$.\nThe training procedure involves two losses. The first one is the directional alignment loss, which relates the provided reference images to the inclusive prompts. In order to compare images and text, CLIP (Radford et al., 2021) is used to map them to a common embedding space using its text encoder $E_{text}$ and image encoder $E_{img}$. Following the original code, the ViT-L/14 pre-trained model is used. The directional alignment loss is defined by\n$L_{dir} = \\sum_{m=1}^M \\sum_{1 \\leq i<j \\leq K_m} 1 - <\\Delta_m(i, j), A_m(i, j)>$,\n(1)\nwhere $A_m(i, j)$ is the difference of the average of image embeddings between categories $i$ and $j$ of attribute $m$, and $A_m(i, j)$ is the difference between the average of embeddings of inclusive prompts correspondent to those categories.\nThis loss is replaced by a cosine similarity loss $L_{cos}$ when it is undefined (i.e. when the batches are not diverse enough and do not contain samples for every attribute category). Given an image of a certain attribute category, we want to make it similar to all inclusive prompts that contain that attribute category.\nThe second loss is the semantic consistency loss, which acts as a regularizer by making the original text prompts similar to the inclusive prompts:\n$L_{sem} = \\sum_{m=1}^M \\sum_{1 \\leq i<j \\leq K_m} \\max_{P \\in P_m \\cup P_m} (0, - \\lambda <E_{text}(P), E_{text}(T)>)$,\n(2)\nwhere $T$ is the raw text prompt and $P$ the inclusive text prompt for an attribute category combination including category $i$ or $j$ for attribute $m$ and A is a hyperparameter.\nIn this way, the total loss for a batch is defined by:\n$L_{total} = \\begin{cases} L_{dir} + L_{sem} & \\text{if } L_{dir} \\text{ is defined}, \\\\ L_{cos} + L_{sem} & \\text{otherwise}. \\end{cases}$\n(3)\nWe invite the reader to check the original paper (Zhang et al., 2023a) for a more detailed explanation of the model.\nAt inference, the output of the generative model will reflect the attribute categories of the fair tokens. In this way, sampling over a uniform distribution over all combinations leads to fair generation with respect to the attributes of interest. In addition, the text prompt can be the same that was used for learning (in-domain generation) or different (train-once-for-all).\nThe generative model must be compatible with the encoder (CLIP in this case). Following the original paper, we use Stable Diffusion v1.4 (Rombach et al., 2022) for most of the experiments. We also show compatibility with models using additional conditions like ControlNet (Zhang et al., 2023b) in a plug-and-play manner."}, {"title": "3.2 Datasets", "content": "The authors provide four datasets\u00b2 of reference images to train the model:\n\u2022 CelebA (Liu et al., 2015), a manually-labeled face dataset with 40 binary attributes. For each attribute, there are 200 positive and negative samples (400 in total).\n\u2022 FAIR (Feng et al., 2022), a synthetic face dataset classified into six skin tone levels. There are 703 almost equally distributed images among the six categories.\n\u2022 FairFace (Karkkainen & Joo, 2021), a face dataset that contains annotations for age (9 intervals), gender (male or female) and race (7 categories). For every attribute, there are 200 images per category.\n\u2022 Landscapes HQ (LHQ) (Skorokhodov et al., 2021), a dataset of natural scene images annotated using the tool provided in Wang et al. (2023). There are 11 different attributes, each of them divided into five ordinal categories with 400 samples per category."}, {"title": "3.3 Hyperparameters", "content": "In order to reproduce the results of the original paper as closely as possible, we use the default training hyperparameters from the code provided. In a similar way, for image generation, we use the default hyperparameters from the ControlNet (Zhang et al., 2023b) and Stable Diffusion (Rombach et al., 2022) repositories (for HPS and HPSn). Moreover, we generate images with a batch size of 8, which is the largest power of two that can fit in an NVIDIA A100 with 40 GB of VRAM."}, {"title": "3.4 Experimental setup and code", "content": "The code used to replicate and extend the experiments can be found in our GitHub repository\u00b3. We made two minor changes to the authors' implementation: adding a seed in the training loop of ITI-GEN to make it reproducible and fix a bug in the script for image generation to handle batch sizes larger than 1. We also provide bash scripts to replicate all our experiments easily.\nMoreover, the authors do not provide any code for combining ITI-GEN with ControlNet (Zhang et al., 2023b). Thus, we implement it ourselves on top of the pre-existing ControlNet code4 in order to validate the plug-and-play capabilities of ITI-GEN with other text-to-image generation methods. More specifically,\nAn alternative to ITI-GEN is HPS (Ding et al., 2021), which works by specifying the categories directly in the original prompt. For example, we could consider the prompt \"a headshot of a woman\" to generate a woman. One problem with this approach is that it does not handle negation properly and using a prompt like \"a headshot of a person without glasses\" will actually increase the chances of getting someone with glasses. However, this can be circumvented by using the unconditional_conditioning parameter, which is hard-coded to take in the empty string in the current Stable Diffusion sampler (PLMS). By providing the features that are not wanted we can prevent them from appearing in the generated images. For example, we can pass \"eyeglasses\" to get a headshot of someone without them.\nA denoising step in Stable Diffusion takes an image x, a timestep t and a conditioning vector c. This outputs another image that we will denote by $f(x, t, c)$. The technique involves two conditioning vectors, c and $\\bar{c}$ (the negative prompt), and outputs\n$\\lambda(f(x, t, c) - f(x,t,\\bar{c})) + f(x, t, c)$,\nwhere $\\lambda$ is the scale, that we set to the default 7.5 as we explain later in Section 3.3. We call this Hard Prompt Search with negative prompting and we will refer to it as HPSn throughout the paper."}, {"title": "3.5 Computational requirements", "content": "We perform all experiments on an NVIDIA A100 GPU. Training a single attribute for 30 epochs takes around a minute for CelebA, 3 minutes for LHQ, 4 minutes for FAIR and less than 5 minutes for FairFace. For the four datasets, we use 200 images per category (or all of them if there are less than 200). Generating a batch of 8 images takes around 21 seconds (less than 3 seconds per image). It is also possible to run inference on an Apple M2 chip, although it takes more than 30 seconds per image. In total, our reproducibility study adds up to at most 20 GPU hours.\nAll our experiments were run using the Snellius infrastructure located in the Amsterdam Data Tower. The data center reports a Power Usage Effectiveness (PUE) of 1.19 \u2077. Thus, using the Machine Learning Emissions Calculator (Lacoste et al., 2019), we obtain that our experiments emitted around 6 kg of CO2."}, {"title": "4 Results", "content": "Our reproducibility study reveals that the first four claims mentioned in Section 2 are correct, whereas the last one does not hold when the number of attributes increases. In this section, we first highlight the results reproduced to support or deny the main claims of the authors. Then, we compare HPSn with ITI-GEN for the cases when ITI-GEN struggles to generate accurate images."}, {"title": "4.1 Results reproducing original paper", "content": ""}, {"title": "4.1.1 Inclusive and high-quality generation", "content": "To verify the first claim, we train ITI-GEN on every attribute of the CelebA dataset and generate 104 images (13 batches of size 8) per category. Considering there are 40 binary attributes, that makes a total of 40 \u00d7 2 \u00d7 104 = 8,320 images.\nFor every attribute we have 104 \u00d7 2 = 208 images that are classified using CLIP. Since this classification is not very reliable, we also label the images of selected attributes manually. After that, we compute the KL divergence. In Table 1 we see how HPS struggles with attributes that require negation (e.g., \"a headshot of a person with no eyeglasses\u201d), while ITI-GEN performs well.\nSince the FID score reported in the paper uses around 5,000 images, and we have 8,320, we decide to compute the FID score using the last 63 images of every category, which results in a total of 5,040 images. The results are in Table 2, where we can see that ITI-GEN obtains the best score.\nWe also carry out human evaluations to compare the quality of the images generated by HPS and HPSn with ITI-GEN's. The results show that, in 52.24% of the cases, humans prefer images generated with HPS over ITI-GEN. The same thing happens with HPSn in 56.58% of the cases. This shows a discrepancy with the interpretation of the FID scores, where ITI-GEN shows slightly better quality than the HPS methods. A possible explanation is that the reference datasets for the computation of the FID score and for training ITI-GEN (FFHQ and CelebA respectively) might be similar, which would imply a bias towards ITI-GEN."}, {"title": "4.1.2 Scalability to different domains", "content": "Figure 1 shows that it is possible to apply ITI-GEN to multiple domains, as the second claim holds. For the human faces generated with the \"Skin tone\" attribute, we are able to compute the FID score in the same way as before (using the FFHQ dataset), and obtain 46.177. For the natural scenes generated with the \"Colorful\" attribute, we compute the FID score by comparing the generated images with the training ones, obtaining 62.987. Note that both figures are within a similar range as the ones in Table 2."}, {"title": "4.1.3 Plug-and-play capabilities", "content": "We perform a number of experiments to verify the third claim, demonstrating in the end that it holds. More specifically, we show that inclusive tokens learned with one prompt can be applied to other (similar) prompts without retraining the model, as it is shown in Figure 2. We train the binary \"Age\" and \"Gender\""}, {"title": "4.1.4 Data and computational efficiency", "content": "In order to verify the claim about data efficiency, we generate images for the attributes \u201cColorful\u201d, \u201cAge\u201d, and \"Skin tone\" using different number of reference images per category. As in the original paper, we find ITI-GEN to be robust in the low-data regime. The reader can refer to Appendix D for more information. Regarding computational efficiency, training takes less than 5 minutes and generation takes around 2 seconds per image on our hardware, as we show in Section 3.5."}, {"title": "4.1.5 Scalability to multiple attributes", "content": "Table 3 illustrates how ITI-GEN performs for some combinations of binary attributes. We can observe that while the KL divergence is low when we use two attributes, it increases significantly when we keep adding attributes. Thus, this claim is not entirely correct, as we further investigate this observation qualitatively and quantitatively in the next section, where we also highlight more of ITI-GEN's pitfalls."}, {"title": "4.2 Results beyond original paper", "content": ""}, {"title": "4.2.1 Proxy features", "content": "ITI-GEN might use some undesired features as a proxy while learning the inclusive prompts for certain attributes. This is the case for attributes like \"Bald\", \"Mustache\" and \"Beard\", which seem to use \"Gender\" as proxy. One could argue that this is not important as long as the model is able to generate people with and without the desired attribute. However, it is actually a problem because as we show in Figure 4, some pairs of attributes are strongly coupled and ITI-GEN is not able to generate accurate images for all elements in the joint distribution. HPSn, on the other hand, is able to disentangle the attributes. More examples are shown in Appendix C.\nOur main hypothesis is that the reason for this lies within the reference images. If we inspect them, we see that most of the bald people are men, which may be the reason why the model is using gender as a proxy. To delve into this, we perform an experiment using two variants of the \"Eyeglasses\" dataset (see Table 4), which is diverse and works fine when combined with the \"Gender\" attribute. Then, we test the ability of the model to learn the joint distribution using those reference images."}, {"title": "4.2.2 Handling multiple attributes", "content": "Computational complexity. The training loop iterates through all reference images, which implies a linear complexity with respect to the size of the training set. At the same time, it considers all category pairs within an attribute, which means it is quadratic with respect to that. It also iterates through all elements in the joint distribution, so it is exponential with respect to the number of attributes. This can be problematic when we train ITI-GEN on many attributes. For example, in the case of binary attributes with the same number of reference images, the time complexity is $O(Ne^N)$, as Figure 8 depicts.\nDiversity issues in generated images. Figure 6 illustrates a comparison between ITI-GEN and HPSn. We can observe that ITI-GEN struggles significantly to generate diverse images (the difference between old and young people is almost non-existent, there are many people without eyeglasses, etc.). On the other hand, images generated by HPSn reliably reflect all category combinations."}, {"title": "4.2.3 Combining ITI-Gen with HPSn", "content": "The main motivations for image-guided prompting are handling negations and continuous attributes that are hard to specify with text. As we have shown, negation can be effectively tackled using HPSn, but ITI-GEN"}, {"title": "5 Discussion", "content": "In this work, we conduct several experiments to check the validity of the main claims from the original paper. We find that most of them are correct with some small exceptions.\nFirst, we show ITI-GEN (Zhang et al., 2023a) is able to generate diverse high-quality images, according to the low KL divergence and FID scores we obtained (see Tables 1 and 2). We also highlight that it works on multiple domains, such as human faces and landscapes. Moreover, ITI-GEN has plug-and-play capabilities when learned inclusive tokens are applied to similar text prompts (Figure 2). The fair tokens can also be integrated with different text-to-image generators (i.e. Stable Diffusion (Rombach et al., 2022), ControlNet (Zhang et al., 2023b)), as displayed in Figures 1, 2, 3. In addition, we verify that only a few dozen reference images are required, and that training is computationally efficient (Figure 11).\nOn the other hand, we show that the claim about scalability to multiple attributes does not completely hold. More specifically, our analysis reveals that ITI-GEN needs exponential training time with respect to the number of attributes (Figure 8) and struggles to generate diverse images when we increase the number of attributes (Figure 6). Moreover, we illustrate that ITI-GEN can struggle to disentangle certain attributes (e.g. \"Gender\" and \"Bald\" in Figure 4). To solve these issues, we propose HPSn which does not require training and produces more accurate images. At the same time, it seems to handle negations better than ITI-GEN, especially when we use a large number of attributes (Figure 6, Table 1). However, a limitation of HPSn is that it cannot handle attributes that are difficult to specify with text (e.g. skin tone, colorfulness), whereas ITI-GEN excels on these. As we show in Figure 7, ITI-GEN and HPSn can be combined to generate images that are inclusive with respect to multiple attributes, taking advantage of the strong aspects of each method."}, {"title": "5.1 What was easy", "content": "The paper was well-written and included many examples to make it easier to understand. Additionally, the original code was published on GitHub, with instructions on how to run the training, generation and evaluation scripts."}, {"title": "5.2 What was difficult", "content": "The main difficulty consisted in understanding the code, since there were some differences with the paper. More specifically, when the directional loss is undefined, it is replaced by a cosine similarity loss between the image and text embeddings.\nMoreover, the evaluation script requires a list of classes that is not specified. The authors mention that they had to change the text prompts to tackle the negative prompt issue. They also had to use pre-trained classifiers combined with human evaluations. Thus, it was not possible for us to easily reproduce the KL divergence results."}, {"title": "5.3 Communication with original authors", "content": "We reached out to the authors via email correspondence regarding the quality of the generated images and the missing code for combining ITI-GEN with ControlNet. They replied quickly and cleared up the discrepancies in our results. However, we did not receive from the authors any additional code for integrating ITI-GEN with ControlNet, because they were still working on it. They assured us they would upload the code on their original GitHub repository as soon as possible, so that we can cross-verify our implementations."}, {"title": "6 Ethical and Social Impact", "content": "As ITI-GEN is guided visually, we need a set of reference images. This raises some concerns regarding inclusiveness and privacy.\nEven though the output images are usually inclusive with respect to the attributes of interest, they might not be with respect to others. This can be problematic, because someone aiming to increase diversity could actually be introducing other biases without realizing it. Thus, it is important to ensure that the reference images are not biased, and conduct some sort of quality evaluation on the outputs.\nAnother important aspect is data protection. If ITI-GEN is trained on a publicly available dataset, we need to have permission to do so. Inspired by Carlini et al. (2023), we wonder if an attacker could recover the training images using the inclusive tokens, which would be problematic if ITI-GEN was trained on a private dataset.\nAll in all, we agree with the original authors' that carefully considering potential risks of ITI-GEN is essential to avoid possible negative consequences."}]}