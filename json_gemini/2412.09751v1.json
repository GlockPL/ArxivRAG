{"title": "Al Red-Teaming is a Sociotechnical System. Now What?", "authors": ["TARLETON GILLESPIE", "RYLAND SHAW", "MARY L. GRAY", "JINA SUH"], "abstract": "As generative Al technologies find more and more real-world applications, the importance of testing their performance and safety seems paramount. \"Red-teaming\" has quickly become the primary approach to test AI models-prioritized by AI companies, and enshrined in Al policy and regulation. Members of red teams act as adversaries, probing Al systems to test their safety mechanisms and uncover vulnerabilities. Yet we know too little about this work and its implications. This essay calls for collaboration between computer scientists and social scientists to study the sociotechnical systems surrounding Al technologies, including the work of red-teaming, to avoid repeating the mistakes of the recent past. We highlight the importance of understanding the values and assumptions behind red-teaming, the labor involved, and the psychological impacts on red-teamers.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have rapidly transformed from research projects inside a handful of tech companies and university computer science departments, to the engine behind the global deployment of generative AI to consumers. Whether tapped directly on the web, or embedded in software suites, search engines, and social media platforms, LLMs are everywhere. When a technology jumps this quickly from theoretical plaything to consumer service, many other elements are also settling in around it, without much forethought: interfaces, policies, business models, labor arrangements, infrastructural assurances, complementary technologies, public claims, advertising campaigns, regulations.\nResearchers studying the workings and implications of these technologies, across computer science, engineering, the social sciences, humanities, and law, must gear up just as fast to study not just the core technology, but the sociotechnical system taking shape around it [19]. Many of these decisions, arrangements, and infrastructures may turn out to be as consequential for users and the broader public as the core technology itself. But the boisterous promises and debates that surround a new technology can obscure these other essential elements that make technologies always more than the sum of their engineered parts.\nIn this essay, we hope to call upon computer scientists and social scientists alike to pay closer, critical attention to the phenomenon of \u201cred-teaming.\u201d1 AI Models and their applications typically undergo internal testing before release, and continue to be evaluated during use; red-teaming aims to probe these applications for exploitable vulnerabilities, hallucinations, and bias. It is also a particular kind of labor done by particular sets of people in particular locations, with a rapidly developing set of best practices [10, 24]. Since the commercial launch of ChatGPT, red-teaming has been positioned as a given step in the production and deployment of LLMs. AI providers champion it as proof of their public responsibility, while regulators count on it as essential to preventing AI from inflicting social harms. But the public"}, {"title": "Value Judgments", "content": "Efforts to insert obligations, structures, and benchmarks have had to race to keep up with the rush to commercialize LLMs [21]. Initial red-teaming efforts were being implemented even as design teams were still wondering which harms to even probe for. Al red-teamers have had to develop homegrown taxonomies of harms, and the measurement and benchmarking systems for mitigating them [15, 34]. In public statements, the major AI companies often take as given that generative Al tools will unavoidably produce harmful content [22]. However, it is much rarer for Al designers to discuss how they determine what counts as harmful content, what they should and should not be looking for, and whether their teams are best suited to make those judgments. This prompts the question, \"whose values are being utilized for alignment and evaluation?\" ([9]: p. 14)\nThe development of AI red-teaming echoes the early days of commercial content moderation at social media platforms 8. The parallels are revealing. The categories of concern are strikingly similar: graphic violence, hate speech, harassment, discrimination, sexual content, terrorism, human trafficking, self-harm, child abuse, and misinformation [1, 11]. And when Silicon Valley found itself compelled to manage the gap between what can be generated online and what users should actually see, it too enlisted human labor to serve as that filter.\nSocial media platforms \"discovered\" the need for human moderation labor after being surprised by the kinds of content that could turn up through their services [13, 14, 20]. This awareness often came from user complaints, the technology press calling out the platforms' shortcomings, and stumbling upon it themselves. Al red-teaming has similarly been fueled by user complaints and critical press coverage. Solving these harms has to be done internally and largely in proprietary ways [6, 28], making industry-wide or public-wide discussions about harms and values difficult to develop. This leaves the value judgments to the AI designers themselves. Or as OpenAI explained, echoing so many social media companies before them, \"Our approach is to red-team iteratively, starting with an initial hypothesis of which areas may be the highest risk, testing these areas, and adjusting as we go.\" [1]\nTackling harmful content as an internally, intuitively, and iteratively had profound implications for social media platforms over the past two decades; the same implications could befall the red-teaming of generative Al systems."}, {"title": "Labor Politics", "content": "The tendency to ignore the importance of human labor in Al systems, whether intentional or not, is common [14, 29]. \u03a4\u03bf this point, it is illustrative that 'red-teaming' is often referred to as a verb, eliding the human workforce that constitutes 'red teams.' A sociocultural examination of red-teaming should extend not only to the values and concerns behind the techniques red-teaming deploys, but to the people doing the work and the labor arrangements within which they operate.\nRed-teaming as a method is emerging in various forms: inside and outside companies, salaried and volunteer, with access to the inner workings of the Al system and without. These are bound to change: some forms will fall away, while others will settle in as \"the way things are done.\" But whatever particular labor politics settle into place, there are important questions that arise, ripe for scholarly analysis, about the institutional contexts, material conditions, and economic incentives of this AI-related work.\nWho does this work internally can vary. Sometimes it is ad hoc, performed by design team members, or dedicated red teams within larger companies. These teams may be part of Responsible AI efforts, Trust & Safety divisions, or legal/compliance apparatuses [36]. Those who perform red-teaming for their own companies typically enjoy the job security of full employment. That said, they may not be in a position to refuse a red-teaming request. They command the necessary technical understanding of how models work, but it is not clear that this is sufficient to effective to identify and mitigate AI risks. And they may not be able to raise concerns publicly without breaching corporate norms or legally binding non-disclosure agreements [8].\nEmployee red-teamers typically have little training in any other relevant proficiencies, whether linguistic, sociocultural, historical, legal, or ethical; the incentive structures do not ask for or reward these expertise. Some internal red teams might include someone with sociocultural domain expertise, but they, too, work for the company and may have conflicting incentives [10]. Those windows of opportunity can open, a little. To test GPT-4 ahead of its March 2023 release, OpenAI solicited help from more than 50 experts, though primarily from Trust and Safety and cybersecurity backgrounds [1]. Anthropic and Microsoft encourage their red-teams to consult with experts to test specific types of harms [25]. These partnerships give AI companies access to experts without having to retain them as formal employees, compensating them in clout, API credits, job opportunities, or bragging rights rather than dollars.\nFollowing a pattern ubiquitous in Silicon Valley, there are increasing efforts to shift red-teaming labor from company employees to third-party datawork vendors, often overseas. Early in their model development, researchers at Anthropic"}, {"title": "Wellbeing of Red-Teamers", "content": "Beyond understanding who is doing the AI red-teaming and what is being evaluated, we also need to pay attention to the human cost of doing such work. Scholars and practitioners involved in red-teaming call it \"rather unsavory\nwork\" [4] and \"mentally taxing\" [25]. Like content moderation work, the adversarial probing central to red-teaming requires imagining worst-case scenarios and exposing workers to potentially troubling outputs. This can involve (1) assuming the persona of a potential adversary (e.g., online harasser, sex trafficker, racist, terrorist), (2) strategizing a plan that this adversary may use to compromise the system and executing the strategy, and (3) evaluating the output of the system for potential harms. It may also involve assuming the persona of a benign user with specific intents or contexts (e.g., a user with a history of eating disorder looking for dieting advice) and trying to evoke system vulnerabilities that might be harmful. A red-teamer may have to first research harmful groups to learn their behaviors and bring that knowledge into red-teaming the models for hate speech or deep fakes. They may immerse themselves in child online safety concerns to then evaluate the model's capabilities in aiding child exploitation.\nTo date, there is little empirical research about the psychological impact of AI red-teaming. But red-teamers could benefit from decades of research documenting the psychological impact of content moderation, and similar occupational health concerns experienced by professions that contend with trauma exposure [33]. The outlook for red-teamers will be much brighter if we learn from this history.\nLike content moderators, emergency responders, journalists, or detectives dealing with distressing events, AI red-teamers may experience repeated exposure to disturbing and traumatic content that can lead to symptoms of post-traumatic stress disorder (PTSD) and secondary traumatic stress (STS) [3, 30]. The more successful a red team operation is, the more it reveals potentially harmful content to review. In fact, repeated work-related exposure to traumatic content is among the diagnostic criteria used for PTSD in the DSM5 (Diagnostic and Statistical Manual of Mental Disorders) and has subsequently been used to support the claims in a series of recent lawsuits brought on by content moderators against their employers [27]. Prolonged exposure may include long-lasting mental health symptoms, alterations to their personal belief systems, and increased risks of health issues and substance abuse [33]. These parallels underscore the importance of initiating research to protect red-teamers from the psychological hazards inherent in their work.\nAt the same time, AI red-teaming introduces distinct psychological challenges. A successful Al red-teamer must exhibit a perverse imagination to be effective. Or as one red-teamer put it: \"If there were a red-team motto, it would be: The more sinister your imagination, the better your work\u201d11 Red-teaming involves deliberately engaging in transgressive, uncomfortable, unethical, immoral, or harmful activities, including immersing themselves in scenarios that go against their morals or belief systems-to think like a harasser, or feel like a target of discrimination. This can lead to \"moral injury\" [32], the psychological distress that stems from actions, or the lack thereof, that violate one's moral or ethical code. Those who cannot safely detach their personal identity from their transgressions may experience negative self-perception and guilt. Regularly breaking the rules for the greater good can introduce a potential for a \"loss of self,\" sometimes seen in undercover police profession [18].\nThe potential negative impacts on red-teamers' wellbeing have been acknowledged by those that organize such work. For example, organizers of the DEFCON Generative Red Team event anticipated that models generating unexpected harmful outputs that might be triggering to participants [7]. Anthropic's early red-teaming efforts involved consultations with Trust & Safety professionals to design safety measures for their crowdworkers [11]. Strategies"}, {"title": "Conclusion", "content": "What would more substantive, empirical research on red-teaming provide? Today's siloed, firewalled, market-reactive approach to red-teaming has potential drawbacks for Al's consumers, red-teamers, and AI companies. Each company is rapidly developing its own version of red-teaming, with definitions and workloads varying based on the company's priorities, 'brand, and particular focus. Almost every company working on generative AI today has a red team workforce of some kind. While there is energy being put toward addressing biases and other \"embedded harms,\" many red-teaming efforts are more concerned with ungrounded, existential risks rather than current, tangible concerns. It is even unclear how many issues identified through red team infiltration have been mitigated.\nAn organized empirical research agenda into red-teaming as a sociological object of study could make both AI companies and the public more aware of and intentional about the practices and consequences of red-teaming work."}, {"title": "Regardless of its effectiveness today or its future improvements, red-teaming as a technical approach to AI safety and security has underlying logics and structural conditions that need examination. Lessons from content moderation show that finding and removing \"bad elements\" demands the deliberation and value judgments of teams of people. The specific conditions under which people do this hard work, at an unprecedented global scale and across myriad institutional settings, matters to both red-team workers' occupational health and the integrity of our technical and informational ecosystems.", "content": "We must study how red-teaming judgments are made and by whom if we hope to improve its outcomes as a sociotechnical system. Its compartmentalization and operational opacity can both alienate workers and keep the public from understanding fully what Al systems offer. Empirical research of red-teaming can challenge these arrangements, identify the barriers and incentives at play, and perhaps point the public and AI companies to more sustainable alternatives.\nAnd like most data work, the notion that this labor is only temporary, that it will soon be automated away, is wrong, and (deliberately) distracting from these sociological concerns. It is difficult to believe that we can ever fully automate such peculiarly human judgments, about contentious and shifting topics, under pressure from regulators and the public, whose ethical frameworks can themselves shift. But even if it could be automated in the future, real people are doing this work right now, and with real consequences. Whether we discard this phantasmic notion of full automation or concede that, after twenty years of content moderation, odds are good that data labor will be with us for the foreseeable future, makes little difference. Either way, research today can help identify how to structure this work in ways that are attentive to the well-being and the labor rights of the people doing the work right now.\nIn fact, we may need not just more empirical study of red-teaming, but a coordinated network of scholars studying red-teaming: as a multi-faceted practice, as a component in the institutional and labor arrangements of Silicon Valley, as a global public health concern, and as a hidden value system buried in our newest tools of expression and knowledge. The field of Computer Science has, in the last decade, begun to recognize that information systems are also labor systems and value systems-growing networks like ACM Conference on Fairness, Accountability, and Transparency illustrate this-and it is grappling with the implications of that in ways it had not before. Again, we might learn from the rise of content moderation and the research that attended to it: while many excellent scholars studying content moderation challenged its underlying logics and structural conditions, because of the absence of a coordinated network to deepen, circulate, and affirm those insights, scholars may have missed opportunities to have more substantive impact on these arrangements. It is not too late to pose an empirical and coordinated challenge to red-teaming, and to the many forms of labor and values on which Al systems depend."}]}