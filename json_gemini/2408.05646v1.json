{"title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "authors": ["Utkarsh Saxena", "Gobinda Saha", "Sakshi Choudhary", "Kaushik Roy"], "abstract": "Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance. Code is available at https://github.com/UtkarshSaxena1/EigenAttn.", "sections": [{"title": "1 Introduction", "content": "The recent boom in artificial intelligence applications and their widespread public adoption can be attributed to the human-like capabilities of large language models (LLMs). LLMs have demonstrated remarkable performance across a wide range of natural language processing (NLP) tasks (Imsys.org, 2024). The maximum number of tokens these models can process simultaneously to understand and generate text is referred to as the context/sequence length, and this closely determines their performance limits. Hence, there has been considerable interest in increasing the context length to enhance their capabilities (Zhang et al., 2024; Ding et al., 2024; Achiam et al., 2023). Longer context lengths open up new possibilities, such as summarizing lengthy documents, retrieving information to answer questions about extensive texts, and analyzing code. To make applications enabled by LLMs more accessible, developing techniques to serve these models efficiently is crucial.\nOne standard technique to accelerate LLM inference on GPUs is caching the intermediate attention keys and values through a KV cache to avoid expensive re-computation for every generated token. However, it is observed that at long context lengths, the KV cache becomes the new memory and latency bottleneck (Pope et al., 2022). Furthermore, while batching multiple requests together amortizes the weight access cost of LLMs, it exacerbates the KV cache memory overhead. Consider the weight and KV cache memory footprint for the Llama-2 7B model with a batch size of 16 and a context length of 32k tokens. Here, the weight memory occupies 14 GB, while the KV cache requires a significantly higher memory of 256 GB at 16-bit precision. In essence, increasing the context/sequence length of LLMs has transformed LLM inference into a memory-bound problem. The entire KV cache must be bought on-chip from the off-chip GPU memory for each newly generated token while the computation core stays idle, waiting for data.\nExisting methods to address the KV cache bottleneck can be broadly classified into four distinct categories. First, some approaches focus on reducing the number of KV cache heads in the multi-head attention block through grouped query and multi-query attention (Ainslie et al., 2023; Shazeer, 2019). Second, a few methods aim to alleviate the memory overhead by utilizing a low-precision quantized KV cache (Hooper et al., 2024; Zirui Liu et al., 2024). Third, certain strategies involve evicting KV cache values associated with unimportant tokens, thereby caching only the keys and values of important tokens determined through some metrics (Zhang et al., 2023; Adnan et al., 2024). Finally,"}, {"title": "2 Background", "content": "Multi-Head Attention. A typical LLM consists of L decoder layers, each with two components: multi-head attention (MHA) and the fully connected feed-forward network (FFN). For an input token embedding X \u2208 \u211d^{n\u00d7d}, the MHA block performs attention operations in parallel across h heads. Here, n is the sequence length, and d is the hidden dimensionality of the model. For each attention head i\u2208 {1,2,...h}, X is transformed into key, query, and value matrices as follows:\nQ_i = XW_i^Q, K_i = XW_i^K, V_i = XW_i^V\nHere, W_i^Q \u2208 \u211d^{d\u00d7d_h}, W_i^K \u2208 \u211d^{d\u00d7d_h}, W_i^V \u2208 \u211d^{d\u00d7d_h} are learnable weight matrices with d_h = \\frac{d}{h}. The attention operation A at each head i is computed, and the results from all heads are concatenated to obtain the final output of the MHA block, as shown in Equation 2.\nh_i = A(Q_i, K_i, V_i) = \\text{Softmax}(\\frac{Q_i K_i^T}{\\sqrt{d_h}})V_i,\nMHA(Q, K, V) = [h_1, h_2, \u2026, h_h]W^O\nLLM Inference. The inference consists of the prefill and generation phases. In the prefill phase, the keys K and V values are computed for an input token embedding X \u2208 \u211d^{b\u00d7n\u00d7d} with batch size b (as shown in Equation 1) and cached in memory for the generation phase. The total size of KV cache (in bits) can be derived by 2*b*n*d*h*L*p, where L corresponds to the number of decoder layers in the LLM, and p corresponds to the precision of cached vectors.\nIn the generation phase, the model uses and updates the KV cache to generate the output autoregressively, one token at a time. Note that this phase is memory-bound. For an incoming token embedding x \u2208 \u211d^{b\u00d71\u00d7d}, the key k_i and value v_i computed through Equation 1 are appended to the KV cache:\nK_i \u2190 \\text{Concat}(K_i, k_i), V_i \u2190 \\text{Concat}(V_i, v_i)\nThe query q_i obtained through Equation 1 is used to compute the attention output for each head:\nh_i = \\text{Softmax} (q_i K_i^T / \\sqrt{d_h}) V_i"}, {"title": "3 Related Works", "content": "KV Cache Compression. As mentioned in Section 2, the KV cache size is computed as 2 * b *n*d* h * L * p. KV cache compression methods target these factors to reduce its overall memory footprint. Multi-query attention (Shazeer, 2019) and grouped query attention (Ainslie et al., 2023) reduce the number of attention heads h. Quantization based methods reduce the precision p (Yang et al., 2024; Kang et al., 2024; Zirui Liu et al., 2024; Hooper et al., 2024). Notably, works like KIVI (Zirui Liu et al., 2024) and KV Quant (Hooper et al., 2024) have demonstrated that the precision of K and V matrices can be reduced to as low as 2-bits. Several works attempt to reduce the sequence length n by only caching K and V corresponding to a subset of tokens (Beltagy et al., 2020). Another strategy is to cache K and V according to token importance (Zhang et al., 2023; Adnan et al., 2024; Liu et al., 2024; Niu et al., 2024). H2O (Zhang et al., 2023) finds important tokens by monitoring accumulated attention scores. KeyFormer (Adnan et al., 2024) improves H2O by considering the importance of discarded tokens as well. Recently, (Wu and Tu, 2024) demonstrated that the cached K and V can be reused across the decoder layers, essentially reducing L. In contrast to these techniques, Eigen Attention reduces the embedding dimension d of each cached K and V vector. It is orthogonal to the existing KV cache compression techniques and can be used in conjunction with them.\nLow-Rank Approximation. Various works in literature have leveraged low-rank approximation for performing efficient LLM inference. Recent works (Yu and Wu, 2023; Feng et al., 2022) have shown that while the weight matrices for transformers-based models are not inherently sparse, the activations are. LoRD (Kaushal et al., 2023) leverages this observation to compress the weight matrix of LLMs by representing it as a product of two low-rank matrices. LoSparse (Li et al., 2023) combines pruning and low-rank approximation and expresses the weight matrix as a sum of a low-rank matrix and a sparse matrix. Fisher-weighted SVD (Hsu et al., 2022) proposes to utilize Fisher information to weigh the importance of weights before performing SVD-based low-rank approximation. In this work, we focus on reducing"}, {"title": "4 Methodology", "content": "This section describes Eigen Attention, which achieves KV cache compression by performing the attention operation in a low-rank space determined by a few basis vectors. We first discuss generating these principal basis vectors and then describe our proposed approach detailed in Algorithm 1.\n4.1 Basis Vector Generation\nThe first step towards computing attention in low-rank space is determining the principle basis vectors that span this low-dimensional space. To achieve this, we create representation matrices R^Q, R^K and R^V corresponding to query, key, and value respectively, for each layer l and attention head i. We drop the layer index l for ease of clarity. These representation matrices are obtained by performing a forward pass for n_s samples taken from the calibration dataset, as shown in Equation 4. Note that this calibration dataset is a subset of WikiText (Merity et al., 2016), which is commonly used for pretraining LLMs.\nR^Q = [(Q_1^l)^T, (Q_2^l)^T, ..., (Q_{n_s}^l)^T]\nR^K = [(K_1^l)^T, (K_2^l)^T, ..., (K_{n_s}^l)^T]\nR^V = [(V_1^l)^T, (V_2^l)^T, ..., (V_{n_s}^l)^T]\nHere, R^{Q/K/V} \u2208 \u211d^{(n_s \u22c5 n)\u00d7d_h}. The key and query representation matrices are concatenated as R^{KQ} = [R^K, R^Q] followed by an SVD operation [U^{KQ}, S, V] = \\text{SVD}(R^{KQ}). We obtain a r-rank approximation (R^K), according to the following criteria (Saha et al., 2021),\n||(R^{KQ})_r||_F^2 \u2265 \\text{Eth}||R^{KQ}||_F^2\nEth is the threshold hyperparameter determining the degree of low-rank approximation. We create a unified basis for key and query to aid in low-rank attention, as shown in Section 4.2. The orthogonal basis vectors spanning the low-rank space for key and query are given by U^{KQ} = [u_1, u_2, ..., u_r], which we represent concisely as U^K. Similarly, we generate U^V for the value representation matrix R^V. Please refer to Appendix A.1 for more details on SVD. Note that a unique low-rank basis is obtained for each head in the attention layer, and we keep the rank r the same across heads by taking the maximum rank across heads. Figure 2 (a), (b) show the spectrum of eigenvalue distribution of the keys and values for OPT-30b, with keys having a lower rank than the values. In Figure 2 (c), (d), we plot rank r obtained by keeping eth as 0.9 for different layers of the model. As shown, some layers' dimensions can be reduced to nearly zero.\n4.2 Eigen Attention\nTo understand our proposed low-rank attention, consider the basis vectors U^K, U^V such that (U^{K/V})^T\u22c5U^{K/V}= I due to orthogonality. The attention inputs can be projected to the low-rank space spanned by these vectors as,\nQ = QU^K (U^K)^T\nK = KU^K (U^K)^T\nV = VU^V (U^V)^T\nHere, {Q, K, V} \u2208 \u211d^{n\u00d7d_h} are low rank attention inputs with rank r. Then, we compute Eigen"}, {"title": "Attention as follows:", "content": "A' = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_h}})V\n= \\text{Softmax}(\\frac{QU^K (U^K)^T K^T}{\\sqrt{d_h}})VU^V (U^V)^T\nFor an appropriate rank r, U^{K/V}(U^{K/V})^T \u2248 I (Yu and Wu, 2023). Hence, attention with low-rank inputs (i.e., Eigen Attention) can approximate the full rank attention (A' \u2248 A). Further, the basis vectors U^K and U^V used to compute Eigen Attention can be seamlessly merged with the weight projection matrices of the attention layer:\nW_i^{Q'} = W_i^Q U^K\nW_i^{K'} = W_i^K U^K\nW_i^{V'} = W_i^V U^V\nW^{O'} = (U^V)^T W^O\nHere, {W_i^{Q'}, W_i^{K'}, W_i^{V'}} \u2208 \u211d^{d_h\u00d7r} and W^{O} \u2208 \u211d^{r\u00d7d_h}. This transformation reduces the output dimension of projection matrices, effectively decreasing the embedding dimension of keys, queries, and values. Consequently, both the number of parameters and floating-point operations (FLOPs) in the attention layer are reduced. More importantly, Eigen Attention significantly lowers the KV cache memory footprint (refer Table 1).\n4.3 Rotational Position Embedding\nPositional information can be incorporated in text processed by LLMs in several ways. Different models employ absolute or relative positional embeddings at different levels of model computations. Recent LLM demonstrations employ rotary positional embeddings (RoPE) (Su et al., 2024). ROPE transforms the keys and queries before performing the attention operation as shown below:\nQ^{POS} = Q_i R_i^{POS}; K^{POS} = K_i R_i^{POS}\nLLMs with ROPE are trained with a fixed dimensional R, making them incompatible with any modification to the embedding dimension of the keys or queries. To integrate RoPE with Eigen Attention, we introduce minor modifications. Specifically, we leave the query to be full rank and transform the key back to a high dimension before applying the"}, {"title": "ROPE rotation matrix. The query, key dot product with Eigen Attention is given by,", "content": "Q^{POS} (K^{POS})^T = Q_i R_i R_i^T (U^K)^T(U^K K_i)\nWe store the low-dimensional representation of the key (i.e., U^K K_i) in the KV cache but perform an additional transformation through (U^K)^T before applying RoPE (Figure 6). To mitigate the parameter overhead associated with this additional transformation, we propose to share U^K across all the attention heads. Similar to the standard Eigen Attention, we merge U^K into W_i^K, with the value computation unchanged.\n4.4 Layer-wise Rank Allotment\nEigen Attention introduces layer-wise threshold Eth, which determines the accuracy of low-rank approximation according to Equation 5. We observe that the same Eth across attention layers introduces different errors at the output of the LLM decoder layer. This implies that the layers that incur lower errors due to the low-rank approximation can be further compressed by lowering the eth. To achieve this, we introduce a layer-wise threshold selection methodology based on the normalized output error of each decoder layer. The threshold eth is reduced by a step size \u03f5_s until the decoder layer output"}, {"title": "5 Experiments", "content": "5.1 Setup\nWe evaluate Eigen Attention across three model families: OPT (Zhang et al., 2022), MPT (MosaicML-MPT), and Llama (Touvron et al., 2023; Llama-3), each with distinct position encoding schemes. OPT employs learnable position embeddings, MPT utilizes AliBi (Press et al., 2021), and the Llama model family employs RoPE (Su et al., 2024). We conduct evaluations on both language generation and zero-shot tasks. The language generation tasks include perplexity evaluation on Wikitext-2 (Merity et al., 2016) and C4 (Dodge et al., 2021) datasets. The zero-shot tasks are obtained from lm-eval-harness framework (Gao"}, {"title": "5.2 Results", "content": "We demonstrate the compression benefits achieved by Eigen Attention through our results in Table 2 on various families and sizes of models for a number of language tasks. In particular, we report perplexity (PPL) on Wikitext and C4 datasets and accuracy (Acc) on various zero-shot benchmarks at three KV cache compression ratios: 0.8x, 0.7x, and 0.6x. As expected, increasing the degree of KV cache compression increases the perplexity while reducing accuracy on benchmark tasks. On average, perplexity increases by 0.32, 0.69, and 1.79 while accuracy drops by 1%, 2%, and 3% at 0.8x, 0.7x, and 0.6x KV cache compression, respectively. Within a model family, we find larger models to be"}, {"title": "Eigen Attention followed by Fine-tuning", "content": "We attempt to mitigate the increase in perplexity and the degradation in accuracy observed with the smaller LLMs by fine-tuning. We use LoRA (Hu et al., 2022) to fine-tune these models on the Alpaca dataset (Taori et al., 2023) for 2000 steps. Specifically, we fine-tune the query, key, value, and output projection matrices in the attention layer for MPT, Llama-2, and Llama-3 models and report the results in Table 3. Fine-tuning helps improve the performance of Eigen Attention models, making them perform closer to the baseline. The perplexity gap to baseline reduces from 2.56 (before fine-tuning) to 1.55 after fine-tuning. Similarly, the average zero-shot accuracy gap is reduced from 7% (before fine-tuning) to within 2% of baseline. We find the most improvements in Llama-3-8b, while the least improvements with MPT-7b LLM.\nEigen Attention with Quantization: We compare performance after quantizing key and value in standard and Eigen Attention. Note that the"}, {"title": "Latency Comparisons", "content": "We evaluate the latency"}, {"title": "5.3 Ablation Studies", "content": "We analyze our approach by varying the number of calibration samples used to compute the representation matrix for low-rank approximation and the number of steps used to fine-tune the models.\nNumber of Calibration Samples: Figure 4(a) shows average accuracy (Avg-Acc) on zero-shot benchmarks with 0.6x KV cache for different numbers of calibration samples used to generate the representation matrix for low-rank approximation. Increasing the number of calibration samples enhances the Avg-Acc, as more samples lead to a more generalized set of basis vectors. However, more samples also increase the size of representation matrices (Equation 4), requiring significantly higher GPU memory for performing SVD. Using more than 128 calibration samples leads to out-of-memory errors. We average representations from samples to handle this, thereby reducing representation"}, {"title": "5.4 Layerwise Rank Visualization", "content": "Figure 5 shows the rank r assigned to key and query projection layers by Eigen Attention at 40% KV cache compression. We observe that the initial layers (with the exception of first layer) are compressed more than the later layers. Also, keys are assigned a lower rank and are compressed more than values, which concurs with our eigenvalue spectrum analysis in Figure 2. Specifically, for 40% KV cache compression, keys are compressed by 54% while values are compressed by only 26%."}, {"title": "6 Conclusion", "content": "In this work, we propose Eigen Attention, a novel technique that reduces the memory overhead associated with KV cache in LLMs. Eigen Attention is inspired by the observation that keys, queries, and values can be approximated using a few basis vectors, enabling the possibility of performing the attention operation in a low-rank space with minimal performance loss. To achieve this, we project the attention inputs into low-rank subspaces defined by a set of principal basis vectors computed offline using a calibration dataset in a one-shot manner. These projections are integrated into the weight"}, {"title": "matrices, which are utilized during inference to generate lower dimensional key and value matrices, thereby reducing the KV cache memory footprint.", "content": "Our approach is orthogonal to existing KV cache compression strategies and can be used in synergy. Extensive experiments across a range of LLMs and language tasks demonstrate that Eigen Attention reduces the KV cache memory footprint by 40% with minimal performance loss and achieves up to a 60% improvement in attention operation latency compared to the standard attention baseline. Please refer to Appendix A.6 for important future directions for our work."}, {"title": "Limitations", "content": "Eigen Attention takes a step towards efficiently enabling longer context lengths, thereby opening avenues for enhancing the capabilities of the current state-of-the-art LLMs. While we demonstrate low-rank basis generation using the Wikitext dataset (Merity et al., 2016), we do not extensively study the best dataset for basis generation. Additionally, although our proposed approach has the potential to make LLMs ubiquitous, it does not mitigate the risks of misuse of these models for malicious activities. A strong commitment to user data protection, robust ethical guidelines, and transparency mechanisms is essential to address this issue effectively."}, {"title": "Acknowledgements", "content": "This work was supported by the Center for the Co-Design of Cognitive Systems (COCOSYS), a DARPA sponsored JUMP center of Semiconductor Research Corporation (SRC), National Science Foundation and Intel."}, {"title": "A Appendix", "content": "A.1 SVD for Matrix Approximation\nSingular Value Decomposition (SVD) can be used to obtain a low-rank approximation for any matrix"}, {"title": "Z \u2208 \u211d^{m\u00d7n} by factorizing it into three matrices as Z = U\u03a3V. Here, U \u2208 \u211d^{m\u00d7m} and V\u2208 \u211d^{n\u00d7n} are orthogonal matrices, and \u03a3 is a diagonal matrix", "content": "which contains the sorted singular values. For Z with a rank r < min(m, n), it can be expressed as Z = \u03a3_{i=1}^r \u03c3_i u_i v_i^T, where ui \u2208 U, vi \u2208 V and \u03c3_i \u2208 \u03a3. For a k-rank approximation with k < r, we have Z_k = \u03a3_{i=1}^k \u03c3_i u_i v_i^T such that the top k values from \u03a3 are chosen to represent Zk, a low-rank approximation of Z.\nA.2 Eigen Attention with ROPE\nWe introduce some modifications to Eigen Attention algorithm to handle compatibility with LLMs employing RoPE empedding (Section 4.3). The comparison of Eigen Attention with standard attention in the presence of ROPE is shown in Figure 6."}, {"title": "A.3 Quantization Results", "content": "Section 5.2 shows results for performing quantization with Eigen Attention and standard attention. The data corresponding to Figure 3 is provided in Table 5.\nA.4 Hyperparameters\nWe use the Hugging Face Transformers library (Wolf et al., 2020) to implement Eigen Attention"}, {"title": "A.5 Artifact Licenses", "content": "According to their license, all the LLMs used in this paper fall under acceptable use cases. The licenses for the models are linked for perusal: OPT-30b, OPT-66b, MPT-7b, MPT-30b, Llama-2-7b, Llama-2-13b, Llama-2-70b, Llama-3-8b and, Llama-3-70b.\nA.6 Future Work\nWe describe two key future directions for Eigen Attention: (1) integrating Eigen Attention with efficient LLM serving frameworks like vLLM (Kwon et al., 2023b), which employ additional approximation techniques (e.g., weight quantization (Lin et al., 2024)) to achieve high throughput inference, and (2) finding the best combination of various compression techniques described in Section 3 to achieve extreme KV cache compression."}]}