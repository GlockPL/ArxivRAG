{"title": "One Prompt is not Enough:\nAutomated Construction of a Mixture-of-Expert Prompts", "authors": ["Ruochen Wang", "Sohyun An", "Minhao Cheng", "Tianyi Zhou", "Sung Ju Hwang", "Cho-Jui Hsieh"], "abstract": "Large Language Models (LLMs) exhibit strong\ngeneralization capabilities to novel tasks when\nprompted with language instructions and in-\ncontext demos. Since this ability sensitively de-\npends on the quality of prompts, various meth-\nods have been explored to automate the instruc-\ntion design. While these methods demonstrated\npromising results, they also restricted the searched\nprompt to one instruction. Such simplification sig-\nnificantly limits their capacity, as a single demo-\nfree instruction might not be able to cover the\nentire complex problem space of the targeted task.\nTo alleviate this issue, we adopt the Mixture-of-\nExpert paradigm and divide the problem space\ninto a set of sub-regions; Each sub-region is gov-\nerned by a specialized expert, equipped with both\nan instruction and a set of demos. A two-phase\nprocess is developed to construct the specialized\nexpert for each region: (1) demo assignment: In-\nspired by the theoretical connection between in-\ncontext learning and kernel regression, we group\ndemos into experts based on their semantic simi-\nlarity; (2) instruction assignment: A region-based\njoint search of an instruction per expert comple-\nments the demos assigned to it, yielding a syner-\ngistic effect. The resulting method, codenamed\nMixture-of-Prompts (MoP), achieves an average\nwin rate of 81% against prior arts across several\nmajor benchmarks.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large language models (LLMs)\nhave demonstrated a remarkable ability to solve novel tasks\ndescribed by user instructions (Ouyang et al., 2022; OpenAI,\n2023; Touvron et al., 2023; Peters et al., 2018; Devlin et al.,\n2018; Brown et al., 2020; Wei et al., 2022b). Despite the\nsuccess, there still exists a substantial gap between user in-\ntention and the model's interpretation. Therefore, carefully\ndesigned prompts (a.k.a. Prompt Engineering) become an\nessential ingredient for fully eliciting LLM's superior gen-\neralization ability (Alhoshan et al., 2022; Zhao et al., 2021;\nLiu et al., 2021; Lu et al., 2021; Su et al., 2022; Wang et al.,\n2022a; Wei et al., 2022a; Yao et al., 2023; Schick et al.,\n2023; Kojima et al.). However, it usually requires laborious\nefforts through inefficient trial and error. To reduce human\nefforts, several recent attempts have shown tremendous po-\ntential in utilizing LLMs themselves to design prompts for\nlanguage generation (Zhou et al., 2022; Pryzant et al., 2023;\nChen et al., 2023; Fernando et al., 2023; Yang et al., 2023;"}, {"title": "Mixture of Experts Paradigm.", "content": "Mixture of Experts (Ja-\ncobs et al., 1991; Jordan & Jacobs, 1994) is a classic\nparadigm of longstanding interest within the machine learn-"}, {"title": "3. Preliminaries", "content": "Terminology. We start by introducing key terminologies\nthat will be used throughout the paper. We define a Prompt\nas the entire text preceding the question. We consider the\nsetting where a prompt can be divided into two parts: (1)\nInstruction: a set of natural language sentences describing\nthe task, and (2) Demos: a set of input-output pairs struc-\ntured in a specific way to demonstrate how to solve a task.\nBelow is an example prompt under this definition:\nMathematically, a prompt (P) can be represented as fol-\nlows (Xie et al., 2021):\nPrompt Optimization. Recent efforts have demonstrated\nsignificant potential in automating the prompt engineering\nprocesses. Concretely, given a set of demos sampled from\na task distribution D, analog to the \"training data\" in su-\npervised learning, a prompt optimization aims at finding an\nInstruction (demo-free) that minimizes the empirical risk\n(or maximizes a score):\nAPE - Automatic Prompt Engineering. The most rele-\nvant work to ours is APE (Zhou et al., 2022) - a pioneering\nmethod demonstrating that LLMs can be used to optimize\nprompt. The key idea of APE is to ask an LLM to induce\ncandidate instructions by observing a subset of demos, ran-\ndomly sampled from the entire training dataset, and pick the\nbest one according to their rank on a held-out validation set\n(partitioned from training demos as well). Formally,\nLimitations of APE. While methods like APE demon-\nstrated promising results in designing prompts that surpass\nhuman engineers, they are still constrained to searching\nwithin a single demo-free instruction space. Such a limita-\ntion can hinder the problem-solving potential in NLP tasks,\nwhere the complexity of problems may not be adequately\naddressed by a single demo-free instruction alone."}, {"title": "4. MoP: Mixture-of-Prompts", "content": "4.1. Framework Overview\nMixture-of-Expert for prompt optimization. To address\nthe aforementioned issue of existing automatic prompt engi-\nneering methods, we expand the problem space coverage for\nautomatic prompt engineering by optimizing the Mixture\nof Prompts (MoP). To achieve this, we employ the Mixture\nof Experts (MoE) paradigm (Jacobs et al., 1991; Jordan\n& Jacobs, 1994) to partition the entire problem space into\nC regions, each governed by a specialized expert. Within\nthe MoE framework, prompt optimization (Equation (2))\ntransforms into an expert assignment problem that aims\nto search for the most suitable prompt P* for each expert,\nwith the ultimate goal of optimizing the performance of the\nentire mixture:\nHere, (xc, yc) ~ V refers to the data point assigned to\nexpert c by the employed routing function during inference\ntime (we explain it in more detail later in Section 4.2). No-\ntably, our MoP framework expands the prompt for each"}, {"title": "4.2. Demo Assignment", "content": "In our two-phase search algorithm, we initiate the process by\nassigning training demos to different experts. Since demos\nrepresent local expertise, their assignment defines the design\nof experts in MoE and is entirely up to the constructors.\nWhile there are many options, we propose a clustering-based\ndemo assignment method, derived from a recent theory of\nIn-Context Learning in LLMs.\nLLM learns from demos via Kernel Regression in the\nembedding space. Recently, Han et al. (2023) provides\nthe first theoretical result showing that LLM performs In-\nContext Learning (ICL) from demos as Kernel Regression\nin the embedding space. Formally:\nClustering demos to each expert based on their semantic\nsimilarity. The above analysis motivates a natural way of\nassigning demos to different experts: by clustering them\nbased on semantic similarity. Starting from the kernel model\nin Theorem 4.1, our goal is to divide the demos into C\ngroups {V1, . . ., Vc} such that each group (expert) only\nuses its own demo. In this case, the same sample x\u012f's\nprediction, assuming its in group c, will become"}, {"title": "4.3. Instruction Assignment", "content": "Given the set of demos assigned to each expert, the final\nstep is to identify the best instruction for each cluster, so\nthat the collective performance of the mixture is maximized.\nWe introduce a Region-Based Joint Search (RBJS) algo-\nrithm for solving this objective. RBJS consists of two parts:\ngenerating candidate instructions and identifying the best\none for each expert.\nGenerating candidate instructions to complement the\ndemos. As discussed in Section 4.1, each expert acquires\na different specialty from the local information stored in\ntheir assigned demos. Because of this, they also process dis-\ntinct blind spots in terms of their general task-solving ability.\nTherefore, they might require different instructions to com-\npensate for their special needs. Inspired by this analysis, for\neach expert, we propose to generate candidate instructions"}, {"title": "5. Experiments", "content": "In this section, we experimentally validate MoP, which\njointly searches for the optimal (instruction, demos) pair to\npartition the problem space into homogeneous regions.\n5.1. Experimental Setup\nSettings. We follow the settings in the original APE pa-\nper (Zhou et al., 2022) with the following exceptions. (1)\nOur evaluation is conducted on OpenAI's latest GPT-3.5-\nTurbo-Instruct model \u00b9, a cost-efficient (100\u00d7 cheaper) re-\nplacement for the text-davinci model used in APE. We reran\nAPE on this model. (2) For all our methods, we report the\nmean and standard deviation across 3 runs to account for\nthe randomness in the search phase.\nTasks and Evaluation Metrics. We empirically validate\nthe strength of MoP across three major prompt optimiza-"}, {"title": "5.2. Analysis", "content": "Before delving into the main experiments, we conduct an\nempirical analysis that motivates the development of our\nMoP framework. The results presented here are obtained\nfor the Auto categorization task, with the maximum number\nof experts set to four.\nVisualization of demo clusters. Building upon the the-\noritical connection between ICL and Kernel Regression,\nwe begin by clustering a given set of demos into regions\nbased on their semantic similarity. To achieve this, we first\nmap the given demo sets into the embedding space using\ntext-embedding-ada-002 model 2 as a text encoder (Eo), and\nthen apply the clustering algorithm described in Section 4.2.\nThe illustration indicates that there exist\nunderlying patterns in the data distribution, and demos\nwith semantically similar meanings are grouped closely.\nFor example, for the 'Auto categorization' task shown in\nFigure 2a, demos relevant to country, computer science,\nextinct languages, and apparel are each clustered together.\nBy clustering demos in the embedding space, we can effec-\ntively find semantically similar clusters that help allocate\ntest queries (marked with stars Figure 2a) accurately to the\ncorresponding region and the optimal expert.\nExperts process different specialties. We then verify the\nimpact of demo clusters on performance for each test query.\nIn order to eliminate the impact of instructions on perfor-\nmance, all experts utilize only clustered demos as prompts,\nthereby restricting the output space to demos only. Subse-\nquently, we calculate the Hit Ratio by counting the number\nof correctly answered experts out of the total number of\nexperts (C) for each test input. If test inputs yield Hit Ratios\nwithin the range other than 0/C and C/C', it indicates their\nsensitivity to the assigned expert. As depicted in Figure 2b,\nwe measure the Hit Ratios and observe that 83% of test\ninputs have Hit Ratio values that are neither 0 nor 1. This\nimplies that most test inputs are influenced by the type of\nclustered demos they are assigned; Each expert develops"}, {"title": "5.3. Main Results", "content": "In this section, we conduct a large-scale experiment to com-\npare MoP against six previous SOTAs across three major\nbenchmarks: Instrucion-Induction (Zhou et al., 2022), Super\nNatural Instruction (Wang et al., 2022b), and BIG-Bench-\nHard (Suzgun et al., 2022). Tasks from these benchmarks\ncover a broad spectrum of language understanding scenarios,\nincluding various types of reasoning, knowledge retrieval,\ncoding, math, etc. Due to space limit, we only showcase 19\ntasks here, and include all results in Appendix J. As shown\nin Figure 3, MoP outperforms prior arts (APE + Demos and\nIZ + Demos) by a substantial margin.\nIn addition, we also compute the win rate of every pair of\nmethods. As shown in Figure 4, the win rate of MoP\ndominates all six prior methods, with an average of 81%\nacross three benchmarks. The results not only reveal\nthe strength of our framework but also demonstrate that\nMoP can generalize to a wide range of tasks."}, {"title": "6. Ablation Study", "content": "In this section, we ablate the effect of different modules in\nthe proposed MoP framework. We conduct the experiments\non three tasks: Auto categorization, Mathdataset classifica-\ntion, and Taxonomy animal the task throughout the section.\nAll other settings are identical to the previous section.\n6.1. Robustness of MoP on Out-of-Distribution Data\nTo further assess the robustness of our method under Out-\nOf-Distribution (OOD) settings, we craft OOD datasets\nthat can challenge MoP: Using the same embedding model\nas MoP, we divided the original dataset into two clusters:\none designated for training and the other for testing. This\ndivision ensured that all test data were significantly distant\nfrom the training clusters, providing a rigorous test of MoP's\ndemonstration assignment and routing functions arguably\na more adversarial setup than that faced by APE.\nThe resilience of MoP can be attributed to its strategic seg-\nmentation of the problem space. By dividing the space into\ndistinct regions, each managed by an expert, MoP ensures\nthat even an OOD query is matched to the closest region.\nThis reduces the \"out-of-distribution\" effect for the query\nrelative to the localized data distribution, making the query\neffectively less alien to the selected expert region.\n6.2. Different Number of Demos\nFirstly, we verify the performance of MoP against baselines\nacross different numbers of demos. As shown in Figure 5,\nMoP consistently outperforms the baselines across various\nnumbers of demos, achieving a significant improvement\nof 16.89%, 22.89%, 2.78% compared to APE+Demos and\n21.67%, 19.88%, 3.33% compared to IZ+Demos across all\nthe tasks considered for each number of demos.\n6.3. Different Clustering Algorithm\nWe use K-means-Auto to cluster demos, which automat-\nically decides the best number of experts. The intuition"}, {"title": "6.4. Different Embedding Model", "content": "During demo assignment, we measure the semantic simi-\nlarity of demos using 12 distance in the embedding space.\nWhile our method is agnostic to the specific choice of em-\nbedding models, stronger text encoders perform better in\nidentifying semantically similar demos. Here we examine\nhow the strength of the embedding model affects the per-\nformance of MoP. We examine three commonly used text\nencoders: GPT2-Large, T5, and Ada-002. The results in\nTable 2's 1st group suggests that, while Ada achieves the\nbest result, MoP operates reasonably well with paried with\nGPT2-Large. This shows that the proposed demo assign-\nment method does not rely on a specific embedding model.\n6.5. Different Prompt Assignment Algorithms\nWe further ablate the key elements behind the design of\nour Region-Based Joint Search algorithm. 1). The optimal\ninstruction for each expert might be distinct, therefore their\ngeneration and assignment should be conditioned on the\ndemo clusters (Joint Search); 2). To find instructions that\ncompensate for each expert's demos, we use demos from\nall other experts to generate instructions. 3). The optimal\nprompt for each expert is evaluated only on the text points\nassigned to this expert (Region-based). We designed three\nprompt assignment methods to validate these hypotheses\nrespectively: 1). Independent Search searches for the best\nprompt and assigns demos independently, i.e. the global\nbest prompt will be assigned to all experts; 2). RBJS Same-\nCluster uses each expert's own demos to generate prompts;\n3). Joint Search ranks the best prompt on all validation\ndata. The results in the bottom group of Table 2 confirm the\nclaims: Region-Based Joint Search achieves the best results.\n6.6. Different Routing Algorithms\nThe routing function is crucial to the MoE framework, as it\nis responsible for assigning the test input to the most suitable\nexperts. Following the clustering-based demo assignment,\nour routing function maps a test input to its closest expert in\nembedding space as well. As shown in Table 2, our routing\nfunction significantly outperforms random assignment."}, {"title": "7. Conclusion", "content": "This work introduces the Mixture-of-Prompts (MoP) ap-\nproach to enhance the performance of prompt optimization\nfor Large Language Models. While existing methods search\nfor a single instruction to prompt language models, MoP\noptimizes for a set of experts (prompts), each governing a\nspecialized region of the problem space. This divide-and-\nconquer approach reduces the complexity associated with\nthe task assigned to a single prompt, thereby substantially\nenlarging the problem space coverage. Within MoP frame-\nwork, we further investigate various demo and instruction"}, {"title": "C. Theoretical connection between MoP and MoE", "content": "Our work adapts the MoE framework, traditionally involving distinct models as experts, by defining experts as diverse\nprompts (instructions + demonstrations). We offer the following insights to highlight the duality between this application\nand traditional MoE.\n1. Prompt Optimization can be viewed as model selection: An LLM pre-trained on the next-token prediction task can\nbe seen as a collection of conditional probabilistic models, each defined by a specific prompt. By crafting various\nprompts, LLM users are essentially picking different submodels to perform various tasks. Thus, designing varied"}, {"title": "M. Limitations", "content": "To promote future exploration, we discuss two limitations of the proposed method. First, the K-means-Auto algorithm used\nin the demo assignment does not guarantee the balance of the resulting clusters. When a cluster receives demos that exceed\nthe limit, we randomly discard them to meet the constraint. This operation might be sub-optimal as it does not factor in their\nrelative importance. Future work might explore various data selection methods for trimming the cluster size. Second, MoP\nuses existing instruction generation method (APE), but sometimes APE fails to generate sensible instructions in the first\nplace. However, MoP can be applied to any instruction generation method, and if better instruction generation methods\nemerge in the future, we can also expect improved performance from MoP accordingly. Lastly, the theory that motivates the\nuse of clustering algorithm to assign demos - connecting ICL to kernel regression - cannot explain the demo order sensitivity\nin LLMs. This suggests that future theoretical advancements could help motivate better demo assignment algorithms.\nFinally, while the theory (Han et al., 2023) connecting ICL to kernel regression inspires the use of clustering algorithms for\ndemo assignments, it could not explain the order sensitivity of demos in LLMs; Further theoretical developments could help\ndevelop better demo assignment algorithms."}]}