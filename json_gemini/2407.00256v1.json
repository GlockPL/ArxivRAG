{"title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts", "authors": ["Ruochen Wang", "Sohyun An", "Minhao Cheng", "Tianyi Zhou", "Sung Ju Hwang", "Cho-Jui Hsieh"], "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large language models (LLMs) have demonstrated a remarkable ability to solve novel tasks described by user instructions (Ouyang et al., 2022; OpenAI, 2023; Touvron et al., 2023; Peters et al., 2018; Devlin et al., 2018; Brown et al., 2020; Wei et al., 2022b). Despite the success, there still exists a substantial gap between user intention and the model's interpretation. Therefore, carefully designed prompts (a.k.a. Prompt Engineering) become an essential ingredient for fully eliciting LLM's superior generalization ability (Alhoshan et al., 2022; Zhao et al., 2021; Liu et al., 2021; Lu et al., 2021; Su et al., 2022; Wang et al., 2022a; Wei et al., 2022a; Yao et al., 2023; Schick et al., 2023; Kojima et al.). However, it usually requires laborious efforts through inefficient trial and error. To reduce human efforts, several recent attempts have shown tremendous potential in utilizing LLMs themselves to design prompts for language generation (Zhou et al., 2022; Pryzant et al., 2023; Chen et al., 2023; Fernando et al., 2023; Yang et al., 2023;"}, {"title": "2. Related work", "content": "Prompt optimization for language generation. Aligning pretrained language models with human intentions is a crucial step toward unlocking their potential (Ouyang et al., 2022; Schick et al., 2023; Kojima et al.). An effective line of training-free alignment methods is prompt optimization (PO) (Shin et al., 2020; Zhou et al., 2022). PO originated from in-context learning (ICL) (Dale, 2021), which is mainly concerned with various designs and arrangements of in-context demonstrations (Wei et al., 2022a; Yao et al., 2023). It later evolves into automatic prompt engineering, where various discrete optimization algorithms are utilized to search for the best prompt (Shin et al., 2020; Deng et al., 2022; Zhang et al., 2022). With the emergence large language models (LLMs), there has been a paradigm shift towards leveraging these models for optimizing prompts in a manner akin to human writers (Zhou et al., 2022; Pryzant et al., 2023; Xu et al., 2022; Yang et al., 2023; Chen et al., 2023; Fernando et al., 2023). Our research builds on this recent advancement as these method yields strong results and offers a more interpretable optimization process.\nMixture of Experts Paradigm. Mixture of Experts (Jacobs et al., 1991; Jordan & Jacobs, 1994) is a classic paradigm of longstanding interest within the machine learn"}, {"title": "3. Preliminaries", "content": "Terminology. We start by introducing key terminologies that will be used throughout the paper. We define a Prompt as the entire text preceding the question. We consider the setting where a prompt can be divided into two parts: (1) Instruction: a set of natural language sentences describing the task, and (2) Demos: a set of input-output pairs structured in a specific way to demonstrate how to solve a task.\nMathematically, a prompt (P) can be represented as follows (Xie et al., 2021):\n$P(x) = [I,x_1, y_1, o_{delim},..., x_n, y_n, o_{delim}, x].$ (1)\nHere, $I$ represents an instruction, $\\{(x_i, y_i)\\}_{i=1}^{n}$ represents in-context demos, which is the set of (input, output) pairs, and $o_{delim}$ represents delimiter token.\nPrompt Optimization. Recent efforts have demonstrated significant potential in automating the prompt engineering processes. Concretely, given a set of demos sampled from a task distribution $\\mathcal{D}$, analog to the \"training data\" in supervised learning, a prompt optimization aims at finding an Instruction (demo-free) that minimizes the empirical risk (or maximizes a score):\n$P^*(x) = \\underset{P(x)}{\\operatorname{argmax}} \\mathbb{E}_{(x,y)\\sim \\mathcal{D}} f(P(x), y),$ (2)\nwhere $f(.)$ denotes some task-specific scoring function (Appendix I). After optimization, the best instruction can be used to predict new inputs in the following format: $P^*(x) = [I^*, x]$. Under the framework of Empirical Risk Minimization, one can deduce an underlying assumption that demos (training data) encapsulate all external information about the task.\nAPE - Automatic Prompt Engineering. The most relevant work to ours is APE (Zhou et al., 2022) - a pioneering method demonstrating that LLMs can be used to optimize prompt. The key idea of APE is to ask an LLM to induce candidate instructions by observing a subset of demos, randomly sampled from the entire training dataset, and pick the best one according to their rank on a held-out validation set (partitioned from training demos as well). Formally,\n$\\left\\{I_{i}^{m}\\right\\}_{i=1} \\sim_{I \\sim P\\left(I \\mid \\mathcal{D}_{train}, T(\\mathcal{D}_{train}); \\mathcal{M}_{\\phi}\\right)}.$ (3)\nHere, $\\mathcal{M}_{\\phi}$ denote an LLM, $\\left\\{I_{i}^{m}\\right\\}_{i=1}$ are the candidate instructions, and $T(\\mathcal{D}_{train})$ represents the chosen template format (see Figure 6). Subsequently, the best instruction among the candidate pool is selected based on the validation accuracy:\n$I^* = \\underset{I}{\\operatorname{argmax}} \\mathbb{E}_{(x,y)\\sim \\mathcal{D}_{valid}} f ([I, x],y).$ (4)\nLimitations of APE. While methods like APE demonstrated promising results in designing prompts that surpass human engineers, they are still constrained to searching within a single demo-free instruction space. Such a limitation can hinder the problem-solving potential in NLP tasks, where the complexity of problems may not be adequately addressed by a single demo-free instruction alone."}, {"title": "4. MoP: Mixture-of-Prompts", "content": "4.1. Framework Overview\nMixture-of-Expert for prompt optimization. To address the aforementioned issue of existing automatic prompt engineering methods, we expand the problem space coverage for automatic prompt engineering by optimizing the Mixture of Prompts (MoP). To achieve this, we employ the Mixture of Experts (MoE) paradigm (Jacobs et al., 1991; Jordan & Jacobs, 1994) to partition the entire problem space into C regions, each governed by a specialized expert. Within the MoE framework, prompt optimization (Equation (2)) transforms into an expert assignment problem that aims to search for the most suitable prompt $P^*$ for each expert, with the ultimate goal of optimizing the performance of the entire mixture:\n$P^*(x) = \\underset{\\{P_1(x),...,P_C(x)\\}}{\\operatorname{argmax}} \\sum_{c=1}^{C} \\mathbb{E}_{(x_c,y_c)\\sim \\mathcal{V}_c} f(P_c(x_c), y_c),$ (5)\nwhere $\\mathcal{D} = \\{\\mathcal{V}_c\\}_{c=1}^{C}$.\nHere, $(x_c, y_c) \\sim \\mathcal{V}$ refers to the data point assigned to expert c by the employed routing function during inference time (we explain it in more detail later in Section 4.2). Notably, our MoP framework expands the prompt for each"}, {"title": "4.2. Demo Assignment", "content": "In our two-phase search algorithm, we initiate the process by assigning training demos to different experts. Since demos represent local expertise, their assignment defines the design of experts in MoE and is entirely up to the constructors. While there are many options, we propose a clustering-based demo assignment method, derived from a recent theory of In-Context Learning in LLMs.\nLLM learns from demos via Kernel Regression in the embedding space. Recently, Han et al. (2023) provides the first theoretical result showing that LLM performs In-Context Learning (ICL) from demos as Kernel Regression in the embedding space. Formally:\nTheorem 4.1. Let $\\{(x_i, y_i)\\}_{i=1}^n$ denote the demos used in the prompt; Let $K$ define a kernel function that measures the semantic similarity between two data points, which can be represented as $K(x_i, x_j) = \\phi(x_i)^T\\phi(x_j)$ with some embedding space $\\phi(\\cdot)$. Then the output of LLM, $\\mathcal{P}(y|[S_n, x_{test}])$, converges polynomially to the following Kernel Regression model with probability $1 - \\delta$.\n$y_i = (\\Sigma_j y_i K(x_i, x_j))/((\\Sigma_j K(x_i, x_j))),$ (6)\nTheorem 4.1 (Han et al., 2023) suggests that, when LLM is prompted with a set of demos and a new test query ($x_{test}$), demos that are semantically closer to the test example in embedding space contribute more to its prediction. The same phenomenon has been observed by several empirical studies (Rubin et al., 2021; Han et al., 2023; Liu et al., 2021). This behavior is also intuitive for ICL: the whole purpose of providing demos is for LLMs to leverage and apply their patterns to the test input.\nClustering demos to each expert based on their semantic similarity. The above analysis motivates a natural way of assigning demos to different experts: by clustering them based on semantic similarity. Starting from the kernel model in Theorem 4.1, our goal is to divide the demos into C groups $\\{V_1, ..., V_C\\}$ such that each group (expert) only uses its own demo. In this case, the same sample $x_i$'s prediction, assuming its in group c, will become\n$\\hat{y_i} = (\\Sigma_{j\\in V_c} y_i K(x_i, x_j))/((\\Sigma_{j\\in V_c} K(x_i, x_j))),$ (7)\nand the error $y_i - \\hat{y_i}$ is related to the sum of the kernel entries outside the cluster $\\Sigma_{j \\notin V_c} K(x_i, x_j)$. Therefore, a good demo assignment algorithm will minimize the sum of between-cluster kernel values while keeping the clusters balanced, leading to the following clustering objective:\n$\\underset{\\{V_1,..., V_C\\}}{\\operatorname{min}} \\sum_{c=1}^{C} \\sum_{i \\in V_c} \\sum_{j \\notin V_c} K(x_i, x_j).$ (8)\nBased on the derivation in Appendix F, this is equivalent to the following clustering objective:\n$\\underset{\\{V_1,..., V_C\\}}{\\operatorname{min}} \\sum_{c=1}^{C} \\sum_{i \\in V_c} ||\\phi(x_i) - m_c||^2, m_c = \\frac{1}{|V_c|} \\Sigma_{j \\in V_c} \\phi(x_j).$ (9)"}, {"title": "4.3. Instruction Assignment", "content": "Given the set of demos assigned to each expert, the final step is to identify the best instruction for each cluster, so that the collective performance of the mixture is maximized. We introduce a Region-Based Joint Search (RBJS) algorithm for solving this objective. RBJS consists of two parts: generating candidate instructions and identifying the best one for each expert.\nGenerating candidate instructions to complement the demos. As discussed in Section 4.1, each expert acquires a different specialty from the local information stored in their assigned demos. Because of this, they also process distinct blind spots in terms of their general task-solving ability. Therefore, they might require different instructions to compensate for their special needs. Inspired by this analysis, for each expert, we propose to generate candidate instructions"}, {"title": "5. Experiments", "content": "In this section, we experimentally validate MoP, which jointly searches for the optimal (instruction, demos) pair to partition the problem space into homogeneous regions.\n5.1. Experimental Setup\nSettings. We follow the settings in the original APE paper (Zhou et al., 2022) with the following exceptions. (1) Our evaluation is conducted on OpenAI's latest GPT-3.5-Turbo-Instruct model 1, a cost-efficient (100\u00d7 cheaper) replacement for the text-davinci model used in APE. We reran APE on this model. (2) For all our methods, we report the mean and standard deviation across 3 runs to account for the randomness in the search phase.\nTasks and Evaluation Metrics. We empirically validate the strength of MoP across three major prompt optimiza"}, {"title": "5.2. Analysis", "content": "Before delving into the main experiments, we conduct an empirical analysis that motivates the development of our MoP framework. The results presented here are obtained"}, {"title": "5.3. Main Results", "content": "In this section, we conduct a large-scale experiment to compare MoP against six previous SOTAs across three major benchmarks: Instrucion-Induction (Zhou et al., 2022), Super Natural Instruction (Wang et al., 2022b), and BIG-Bench-Hard (Suzgun et al., 2022). Tasks from these benchmarks cover a broad spectrum of language understanding scenarios, including various types of reasoning, knowledge retrieval, coding, math, etc. Due to space limit, we only showcase 19 tasks here, and include all results in Appendix J. As shown in Figure 3, MoP outperforms prior arts (APE + Demos and IZ + Demos) by a substantial margin.\nIn addition, we also compute the win rate of every pair of methods. As shown in Figure 4, the win rate of MoP dominates all six prior methods, with an average of 81% across three benchmarks. The results not only reveal the strength of our framework but also demonstrate that MoP can generalize to a wide range of tasks."}, {"title": "6. Ablation Study", "content": "In this section, we ablate the effect of different modules in the proposed MoP framework. We conduct the experiments"}, {"title": "6.1. Robustness of MoP on Out-of-Distribution Data", "content": "To further assess the robustness of our method under Out-Of-Distribution (OOD) settings, we craft OOD datasets that can challenge MoP: Using the same embedding model as MoP, we divided the original dataset into two clusters: one designated for training and the other for testing. This division ensured that all test data were significantly distant from the training clusters, providing a rigorous test of MoP's demonstration assignment and routing functions arguably a more adversarial setup than that faced by APE.\nThe results in Table 6.1 reveal several insights. Firstly, all methods exhibited a performance drop on the OOD dataset. This aligns with the principle of empirical risk minimization, where optimization is strictly confined to the information provided by the training data. Secondly, MoP consistently outperforms other baselines.\nThe resilience of MoP can be attributed to its strategic segmentation of the problem space. By dividing the space into distinct regions, each managed by an expert, MoP ensures that even an OOD query is matched to the closest region. This reduces the \"out-of-distribution\" effect for the query relative to the localized data distribution, making the query effectively less alien to the selected expert region."}, {"title": "6.2. Different Number of Demos", "content": "Firstly, we verify the performance of MoP against baselines across different numbers of demos. As shown in Figure 5, MoP consistently outperforms the baselines across various numbers of demos, achieving a significant improvement of 16.89%, 22.89%, 2.78% compared to APE+Demos and 21.67%, 19.88%, 3.33% compared to IZ+Demos across all the tasks considered for each number of demos."}, {"title": "6.3. Different Clustering Algorithm", "content": "We use K-means-Auto to cluster demos, which automatically decides the best number of experts. The intuition"}, {"title": "6.4. Different Embedding Model", "content": "During demo assignment, we measure the semantic similarity of demos using l2 distance in the embedding space. While our method is agnostic to the specific choice of embedding models, stronger text encoders perform better in identifying semantically similar demos. Here we examine how the strength of the embedding model affects the performance of MoP. We examine three commonly used text encoders: GPT2-Large, T5, and Ada-002. The results in Table 2's 1st group suggests that, while Ada achieves the best result, MoP operates reasonably well with paried with GPT2-Large. This shows that the proposed demo assignment method does not rely on a specific embedding model."}, {"title": "6.5. Different Prompt Assignment Algorithms", "content": "We further ablate the key elements behind the design of our Region-Based Joint Search algorithm. 1). The optimal instruction for each expert might be distinct, therefore their generation and assignment should be conditioned on the demo clusters (Joint Search); 2). To find instructions that compensate for each expert's demos, we use demos from all other experts to generate instructions. 3). The optimal prompt for each expert is evaluated only on the text points assigned to this expert (Region-based). We designed three prompt assignment methods to validate these hypotheses respectively: 1). Independent Search searches for the best prompt and assigns demos independently, i.e. the global best prompt will be assigned to all experts; 2). RBJS Same-Cluster uses each expert's own demos to generate prompts; 3). Joint Search ranks the best prompt on all validation data. The results in the bottom group of Table 2 confirm the claims: Region-Based Joint Search achieves the best results."}, {"title": "6.6. Different Routing Algorithms", "content": "The routing function is crucial to the MoE framework, as it is responsible for assigning the test input to the most suitable experts. Following the clustering-based demo assignment, our routing function maps a test input to its closest expert in embedding space as well. As shown in Table 2, our routing function significantly outperforms random assignment."}, {"title": "7. Conclusion", "content": "This work introduces the Mixture-of-Prompts (MoP) approach to enhance the performance of prompt optimization for Large Language Models. While existing methods search for a single instruction to prompt language models, MoP optimizes for a set of experts (prompts), each governing a specialized region of the problem space. This divide-and-conquer approach reduces the complexity associated with the task assigned to a single prompt, thereby substantially enlarging the problem space coverage. Within MoP framework, we further investigate various demo and instruction"}, {"title": "C. Theoretical connection between MoP and MoE", "content": "Our work adapts the MoE framework, traditionally involving distinct models as experts, by defining experts as diverse prompts (instructions + demonstrations). We offer the following insights to highlight the duality between this application and traditional MoE.\n1. Prompt Optimization can be viewed as model selection: An LLM pre-trained on the next-token prediction task can be seen as a collection of conditional probabilistic models, each defined by a specific prompt. By crafting various prompts, LLM users are essentially picking different submodels to perform various tasks. Thus, designing varied"}, {"title": "D. Search cost comparison", "content": "We benchmark the runtime of the search and inference phase of MoP with different baselines. There exist two computational components in our method: between two types of computations: 1). query LLM 2). running the embedding model. Since the first former dominates latter in practice, we will focus on analyzing the complexity w.r.t. LLM queries:\n\u2022 Inference: MoP, operating under the Mixture of Experts (MoE) paradigm, deploys a single prompt akin to deploying a single instruction. This constitutes one key benefit of MoP (MoE) paradigm over prompt ensemble methods, or simply using longer prompts.\n\u2022 Search: The search in MoP involves negligible costs for the demo assignment phase as they do not require LLM querying. For the prompt assignment phase, the complexity is linear w.r.t. the number of experts but is fully parallelizable.\nTable 4 reports the wallclock times comparing MoP with APE and IZ. Our findings include: (1). gs include: (1) All methods exhibit similar inference times, which aligns with our previous analysis. (2). Both APE and MoP have substantially lower search costs compared to IZ, which incurs additional costs due to the local operation of an open-sourced LLM. (3). While MoP's search cost (with 10 experts) approximately triples that of APE, this can be significantly reduced through parallelization."}, {"title": "E. Qualitative analysis of the discovered experts", "content": "We provide an example analysis of the discovered experts, focusing on why MoP are more (less) effective for certain tasks.\nExample success case: An example where MoP significantly outperforms random demos is in the task of Auto-categorization. This task uses training datasets with various categorization questions belonging to different genres, such as Country (e.g., countries with large populations, countries in the UN), Celebrity, Language, and Companies. We found that each identified expert specializes in one or two categories. For instance, one expert handles only celebrity-related queries, enhancing their ability to provide accurate answers. Another case where MoP excels is the Movie recommendation task from BBH. Here, each expert identified by the MoP algorithm focuses on a distinct set of movies. For example, expert 1 focuses on classic adventures in fantasy and whimsical settings, like 'The Wizard of Oz' and 'Raiders of the Lost Ark'; while expert 5 handles movies that involve deep themes and complex stories, such as 'The Matrix' and 'Schindler's List'.\"\nExample failure case: se: An example where MoP exhibits performance similar to random demonstrations is in the task 'Larger Animals'. In this task, MoP performs similarly to APE-Random, indicating that using multiple experts yields no"}, {"title": "M. Limitations", "content": "To promote future exploration, we discuss two limitations of the proposed method. First, the K-means-Auto algorithm used in the demo assignment does not guarantee the balance of the resulting clusters. When a cluster receives demos that exceed the limit, we randomly discard them to meet the constraint. This operation might be sub-optimal as it does not factor in their relative importance. Future work might explore various data selection methods for trimming the cluster size. Second, MoP uses existing instruction generation method (APE), but sometimes APE fails to generate sensible instructions in the first place. However, MoP can be applied to any instruction generation method, and if better instruction generation methods emerge in the future, we can also expect improved performance from MoP accordingly. Lastly, the theory that motivates the use of clustering algorithm to assign demos - connecting ICL to kernel regression - cannot explain the demo order sensitivity in LLMs. This suggests that future theoretical advancements could help motivate better demo assignment algorithms. Finally, while the theory (Han et al., 2023) connecting ICL to kernel regression inspires the use of clustering algorithms for demo assignments, it could not explain the order sensitivity of demos in LLMs; Further theoretical developments could help develop better demo assignment algorithms."}]}