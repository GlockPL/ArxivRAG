{"title": "IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization", "authors": ["Ahmed Frikha", "Nassim Walha", "Krishna Kanth Nakka", "Ricardo Mendes", "Xue Jiang", "Xuebing Zhou"], "abstract": "In this work, we address the problem of text anonymization where the goal is to prevent adversaries from correctly inferring private attributes of the author, while keeping the text utility, i.e., meaning and semantics. We propose IncogniText, a technique that anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. Our empirical evaluation shows a reduction of private attribute leakage by more than 90%. Finally, we demonstrate the maturity of IncogniText for real-world applications by distilling its anonymization capability into a set of LoRA parameters associated with an on-device model.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), e.g., GPT-4 (Achiam et al., 2023), are gradually becoming ubiquitous and part of many applications in different sectors, e.g., healthcare (Liu et al., 2024) and law (Sun, 2023), where they act as assistants to the users. Despite their various benefits (Noy and Zhang, 2023), the power of LLMs can be misused for harmful purposes, e.g., cybersecurity (Xu et al., 2024) and privacy (Neel and Chang, 2023) attacks, and profiling (Brewster). For instance, LLMs were found to be able to predict various private attributes, e.g., age, gender, income, occupation, about the text author (Staab et al., 2023). Hereby, they achieve a performance close to that of humans with internet access, while incurring negligible costs and time. Such private attributes are quasi-identifiers and their combination can substantially increase the likelihood of re-identification (Sweeney, 2000), i.e., revealing the text author identity. This suggests that human-written text data could in some cases be considered as personal data, which is defined as \"any information relating to an identified or identifiable natural person\" in GDPR (European Parliament and Council of the European Union). Hence, human-written text might potentially require further analysis and protection measures to comply with such privacy regulations.\nPrior works proposed word-level approaches to mitigate text privacy leakage (Albanese et al., 2023; Li et al., 2023). However, lexical changes do not change the syntactic features which were found to be sufficient for authorship attribution (Tschuggnall and Specht, 2014). Another line of work leverages differential privacy techniques to re-write the text in a privacy-preserving way (Weggenmann et al., 2022; Igamberdiev and Habernal, 2023), however, with high utility loss. Moreover, while most prior works and current state-of-the-art text anonymization industry solutions succeed in identifying and anonymizing regular separable text portions, e.g., PII, they fail in cases where intricate reasoning involving context and external knowledge is required to prevent privacy leakage (Pil\u00e1n et al., 2022). In light of this and given that most people do not know how to minimize the leakage of their private attributes, methods that effectively mitigate this threat are urgently needed.\nIn this work, we address the text anonymization problem where the goal is to prevent any adversary from correctly inferring private attributes of the text author while keeping the text utility, i.e., meaning and semantics. This problem is a prototype for a practical use case where data can reveal quasi-identifiers about the text author, e.g., online services (ChatGPT) and anonymous social media platforms (Reddit). Our contribution is threefold: First, we propose a novel text anonymization method that substantially increases its protection against attribute inference attacks. Second, we demonstrate the effectiveness of our method by conducting an empirical evaluation with different LLMs and on 2 datasets. Here, we also show that our method achieves higher privacy protection compared to"}, {"title": "2 Method", "content": "We propose IncogniText, an approach to leverage an LLM to protect the original text against attribute inference, while maintaining its utility, i.e., meaning and semantics, hence achieving a better privacy-utility trade-off. Given a specific attribute a, e.g., age, our method protects the original text Xorig against the inference of the author's true value atrue of the private attribute, e.g., age: 30, by re-writing it in a way that misleads a potential privacy attacker into predicting a wrong target value atarget, e.g., age: 45. See Fig. 1 for an illustrative example.\nWe use an anonymization model Manon to re-write the original text Xorig using a target attribute value atarget, the true attribute value atrue, and the template Tanon with anonymization demonstrations, yielding Xanon = Manon (Xorig, Atarget, atrue, Tanon). Hereby, the target value can either be chosen by the user or randomly sampled from a pre-defined set of values for the attribute considered. We additionally inform the anonymizer of the true attribute value atrue to achieve an anonymized text Xanon particularly tailored to hiding that value. The true attribute value could either be read from the text author's device, e.g., local on-device profile or personal knowledge graph, or input by the author. Nevertheless, IncogniText achieves very effective anonymization even without the usage of the true attribute value atrue as demonstrated by our experiments (Section 3).\nTo validate the effectiveness of the anonymized text Xanon against attribute inference, we use a simulated adversary model Madv that tries to predict the author's attribute value atrue. If the prediction is correct, additional rounds of anonymization are conducted with the anonymization model Manon until the adversary model Madv is fooled or a maximum iteration number is reached. This ensures that we perform as few re-writing iterations as necessary, hence maintaining as much utility as possible, i.e., the original text is changed as little as possible. Note that the same model can be used as Manon and Madv with different prompt templates Tanon and Tadv respectively (see Appendix). This might be especially suitable for on-device anonymization cases with limited memory and compute. Note that applying IncogniText to multiple attributes is easily achieved by merging the attribute-specific parts of the anonymization templates. For cases where the text author wants to share a subset or none of the private attributes, they can flexibly choose which attributes to anonymize, if any.\nIn addition to its usage for early stopping of the iterative anonymization, the adversary model Madv can also be used to inform the anonymization by sharing its reasoning X AdvReasoning for the correct prediction of the true attribute value atrue. In this case, the reasoning text X AdvReasoning is fed as an additional input to the anonymization model Manon. This feature was also proposed in the concurrent work (Staab et al., 2024) and we evaluate this variant of IncogniText in our experimental study. We highlight the main differences between this concurrent work and our approach. First, we condition the anonymization model Manon on a target attribute value atarget. We believe that misleading a potential attacker into predicting a wrong private attribute value by inserting new hints is more effective than removing or abstracting hints to the original value present in the original text. Furthermore, we condition the anonymization model Manon of the true attribute value atrue to increase the quality of the anonymization. Finally, we leverage the adversary model Madv as an early stopping method to prevent unnecessary utility loss or the deterioration of the anonymization quality, i.e., further anonymization iterations can in some cases lead to a decrease in privacy as observed in the experiments in (Staab et al., 2024). Our empirical evaluation and ablation study demonstrate the effectiveness of these contributions."}, {"title": "3 Experimental evaluation", "content": "We first evaluate our approach on the dataset of 525 human-verified synthetic conversations proposed by (Staab et al., 2023). The dataset includes 8 different private attributes: age, gender, occupation, education, income level, relationship status, and the country and city where the author was born and currently lives in. We compare to anonymization baseline approaches including the Azure Language Service (ALS) (Aahill, 2023) and the two concurrent works, Dou-SD (Dou et al., 2023) and Feedback-guided Adversarial Anonymization (FgAA) (Staab et al., 2024). We evaluate the privacy of the"}, {"title": "4 Conclusion", "content": "This work tackled the text anonymization problem against private attribute inference. Our approach, IncogniText, anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. We empirically demonstrated its effectiveness by showing a tremendous reduction of private attribute leakage by more than 90%. Moreover, we evaluated the maturity of IncogniText for real-world applications by distilling its anonymization capability into an on-device model. In future works, we aim to generalize our technique to include data minimization capabilities."}, {"title": "5 Limitations", "content": "While our method achieves tremendous reduction of the private attribute attacker accuracy, the attacker might use a stronger attribute inference model, e.g., a model finetuned for this task, than the open-source adversary model we used in our experiments. This is especially true for on-device setting as the adversary model used has to also be on-device and therefore must be small, e.g., Qwen 1.5B in our experiments. Using better models, e.g., GPT-4, for privacy evaluation might also reveal a higher privacy leakage. Nevertheless, we believe IncogniText would still achieve substantially higher protection than the baselines. Finally, conducting the utility evaluation with humans, e.g., with Likert score (Likert, 1932), would yield more insightful results into the willingness of people to use this technique in a real-world application."}, {"title": "A Ablation results", "content": "As mentioned in Section 3, we present the following ablation results on models other than Phi-3-small as anonymizers."}, {"title": "B Preprocessing of the self-disclosure dataset", "content": "As mentioned in Section 3, we use the self disclosure dataset from (Dou et al., 2023) as a starting point. We consider the following attributes: gender, relationship status, age, education, and occupation."}, {"title": "C Finetuning details", "content": "We provide further details to the finetuning data and process. First, we construct the finetuning dataset based on samples from the synthetic conversations in (Staab et al., 2023) that were not included in the officially released set. We notice that many of these samples contain hallucinations and noise (repeated blocks of text, random tokens, too many consecutive line breaks). We filter these samples out. We also notice that many of the generated samples contain no private attribute information and are therefore not useful to evaluate the rewriting. Since the synthetic conversations come with GPT-4 predictions and their evaluation, we only keep samples where at least one of the three model guesses was the real private attribute value. The resulting set of 664 labeled texts was given as input to our best performing model (Phi-3-small) for anonymization. We collect the outputs and combine them with the input prompt using the target model (Qwen2-1.5B) template. The resulting dataset is the one we use for instruction finetuning. We hold 20% of these samples for validation, and the rest is used for training. We use bi-gram ROUGE for evaluation.\nSecond, we use one middle-range GPU for training (takes 3 GPU hours). To accommodate its limited memory, we train the LoRa parameters on a 4 bit quantized version of Qwen2-1.5B. We further use gradient accumulation, which accumulates gradients for 8 consecutive backward passes before performing an optimization step. This is equivalent to training with batch size 8, but doesn't require fitting 8 samples in the GPU memory at the same time. We train for 32 epochs using AdamW as optimizer with learning rate 1e-4. We set LoRa a to 16 and the rank to r = 128."}, {"title": "D Additional results", "content": "We present further results showcasing the differences between anonymization with and without a target attribute value. Figure 2 is a histogram showing that more than 80% of samples are already anonymized in the first iteration using our method, wheras more than half of samples need to go through a second and possibly a third iteration in FgAA."}, {"title": "E Prompt templates", "content": "The following are the prompt templates used for the anonymizer (conditioned on inference, ground truth and target value) and for the adversary. Similar to (Staab et al., 2023), we use a format correction prompt to avoid parsing failures when the model doesn't give the answer in the expected format. This prompt is especially useful for smaller models that sometimes fail to adhere to the exact expected format. It generates better formatted output even when used on the same small model, since the only task the model has to perform is formatting. We also use (Staab et al., 2023)'s model aided evaluation prompt to decide whether the prediciton of the anonymizer is correct, for attributes where exact string matching is too restrictive (Example: 'Bachelors in Computer Science' and 'B.Sc Computer Science'). We also include the LLM-based utility judge template used in (Staab et al., 2024)."}]}