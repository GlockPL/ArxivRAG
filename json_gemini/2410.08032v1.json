{"title": "Strategic Classification with Externalities", "authors": ["Yiling Chen", "Safwan Hossain", "Evi Micha", "Ariel Procaccia"], "abstract": "We propose a new variant of the strategic classification problem: a principal reveals a classifier, and n agents report their (possibly manipulated) features to be classified. Motivated by real-world applications, our model crucially allows the manipulation of one agent to affect another; that is, it explicitly captures inter-agent externalities. The principal-agent interactions are formally modeled as a Stackelberg game, with the resulting agent manipulation dynamics captured as a simultaneous game. We show that under certain assumptions, the pure Nash Equilibrium of this agent manipulation game is unique and can be efficiently computed. Leveraging this result, PAC learning guarantees are established for the learner: informally, we show that it is possible to learn classifiers that minimize loss on the distribution, even when a random number of agents are manipulating their way to a pure Nash Equilibrium. We also comment on the optimization of such classifiers through gradient-based approaches. This work sets the theoretical foundations for a more realistic analysis of classifiers that are robust against multiple strategic actors interacting in a common environment.", "sections": [{"title": "Introduction", "content": "Machine learning algorithms are increasingly deployed in high-stakes decision making, including loan applications, school admissions, hiring, and insurance claims (Bejarano Carbo, 2021; Harwell, 2022; Kumar et al., 2022). Relying on past data as a reference, these algorithms use features of a candidate to determine their merit for the given task. The interactions in these settings, however, often involve strategic agents who may manipulate their features if doing so yields a more favorable outcome. This presents a significant challenge to the algorithm if it is not trained to anticipate such behavior since the training and test distributions no longer match. Correspondingly, a large and growing literature on strategic classification has emerged to understand the dynamics of this behavior and propose strategies for learning in such settings (Hardt et al., 2016).\nBy and large, the existing literature roughly models the interaction as follows: The learner (or principal) deploys a classifier, and an agent with feature \u00e6 observes this and may choose to report a manipulated feature x' to obtain a better outcome. Manipulation is not considered free since it may involve some additional effort or risk. While the classifier may be deployed for any number of agents, agent interactions with the learner are crucially assumed to be independent; in other words, one agent's action has no influence on another. We posit that this ignores a critical aspect"}, {"title": "Our Contributions", "content": "We study the problem of deploying classifiers in strategic multi-agent settings with inter-agent externalities from a theoretical perspective. Consistent with prior literature, the interaction between the learner and the agents is modeled as a Stackelberg game: the learner first commits to a classifier to which agents can respond. The resulting inter-agent interactions are captured as a simultaneous game, and the Stackelberg-Nash Equilibrium is proposed as the solution concept for all interactions.\nWe precisely define these aspects of the multi-agent strategic classification game in Section 2. Learning in this setting is challenging, not least due to the possible multiplicity of equilibrium, their computation, and their dynamics due to a changing classifier. Section 3 motivates a set of structural assumptions on the cost and externality, under which the inter-agent simultaneous game has a unique pure Nash equilibrium that is efficiently computable. Building on this insight, Section 4 comments on the regularity of the Nash equilibrium under changing classifiers, and provides probably approximately correct (PAC) learning guarantees for computing the Stackelberg-Nash"}, {"title": "Related Work", "content": "Our work builds on the growing literature on strategic classification (Hardt et al., 2016; Br\u00fcckner et al., 2012; Br\u00fcckner & Scheffer, 2009). In the basic setting, a learner seeks to release a classifier that accounts for the fact that strategic agents may misreport their true features to maximize their utility, which is determined by the likelihood of a positive outcome and the cost incurred for misreporting.\nRecent papers have studied extensions of the basic strategic classification setting. For example, Levanon & Rosenfeld (2022) introduce a setting where agents' utility functions capture different intentions beyond simply maximizing for the positive outcome. Others aim to account for limited information (Ghalme et al., 2021; Bechavod et al., 2022), unknown utilities (Dong et al., 2018) and causal effects (Miller et al., 2020; Horowitz & Rosenfeld, 2023). These extensions do not handle externalities or multi-agent behaviour in general.\nWhile motivated by a different aspect of strategic classification, the work of Eilat et al. (2023), which studies strategic classification in a graph setting, has conceptual similarities with our work. Node classification on graphs naturally depends on neighboring nodes' features, which strategic agents can exploit. While this implicitly models agent interactions as not wholly independent, there are several fundamental differences from our work. First, agents in their model are interrelated due to the classifier outcome of one agent depending on neighboring nodes; in contrast, we consider the classifier outcome for an agent to only depend on their feature, with the externality explicitly capturing additional risk or cost to agents due to others also interacting in the system. This better aligns with our motivation to capture classification dynamics in competitive high-stakes settings. Furthermore, they consider agents myopically best responding over a sequence of rounds whereas we consider the performance of a classifier at a Nash equilibrium.\nFurther afield, machine learning researchers have investigated the effect of strategic behavior on social welfare (Milli et al., 2019; Haghtalab et al., 2020; Kleinberg & Raghavan, 2020) or on different groups of agents (Hu et al., 2019). However, none of these papers consider the direct impact that manipulation by others can have on each individual agent. The impact of such externalities has, nonetheless, been studied in computational problems like auctions (Agarwal et al., 2020), data markets (Hossain & Chen, 2024), and facility location (Li et al., 2019)."}, {"title": "Model", "content": "Preliminaries: For $k \\in \\mathbb{N}$, let $[k] = \\{1, ...,k\\}$. We consider k strategic agents interacting with a learner who releases a classifier parametrized by weights w. Using one of our running examples, this corresponds to k students applying for admission to a university, which decides to accept or reject using classifier $f_w$. Let $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1,+1\\}$ denote the feature vector and true class of agent i respectively, with $(\u00e6_i, y_i) \\sim D$. Let $X \\in \\mathbb{R}^{k \\times d}$ denote the k agents' feature matrix, and $y \\in \\{-1,+1\\}^k$ the vector of their true class labels, sampled independently from $D$; we use the"}, {"title": "Utility and Externality", "content": "Following the standard strategic classification model, we consider a Stackelberg interaction between the learner and the agents (Hardt et al., 2016). That is, the learner first releases a classifier $f_w$, and thereafter, agents submit their features to be classified. An agent need not be truthful and may instead submit a manipulated feature vector $x'$ to receive a higher score for the positive class. Consistent with the strategic classification literature (Dong et al., 2018; Bechavod et al., 2022), we assume agent utilities are proportional to the score: $f_w(x')g^+(x_i)$, where $g^+(x_i) : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is an agent specific gain for positive classification.\nIn line with prior work, we assume agent features lie within a bounded region (without loss of generality $[0, 1]^d$) and they incur a cost $c(x_i, x')$ in modifying their true feature vector. However, in contrast to these earlier models, we also consider agents' decisions causing externality to others. Conceptually, externality is the impact agents' actions have on one another when interacting within a common system. Formally, the negative externality suffered by agent i due to an agent jis captured by the function $t(x_i, x_i, x_j, x'_j)$. We use $T(x_i, x'_i, X, X') = \\sum_{j\\neq i}t(x_i, x_i, x_j, x'_j)$ to denote the total externality suffered by agent i due to all other participating agents' decision. In summary, the total utility of an agent i in reporting \u00e6 in our multi-agent strategic classification game is:\n$u_i(X, X', f_w) = f_w(x'_i)g^+(x_i) - c(x_i, x'_i) \u2013 T(x_i, x'_i, X, X')$"}, {"title": "Game-Theoretic Model", "content": "Since an agent's total utility depends on others' reports, the interaction between the agents is naturally modeled as a game. Taking inspiration from prior work on strategic regression (Chen et al., 2018; Hossain & Shah, 2021) and related settings (Nakamura, 2015), we model this inter-agent interaction as a simultaneous game. Correspondingly, the best response of agent i given classifier $f_w$ and reports of others is:\n$\\arg \\max_{x'_i\\in[0,1]^d} u_i(X, X', f_w)$ \nFor a fixed classifier $f_w$, the natural solution concept for the agent game is a pure Nash Equi-librium (PNE). Formally, a set of reported values $X^q = [x'_1;...;x'_k]$ is a PNE if for each agent i"}, {"title": "Equilibrium Properties", "content": "Core to our problem is computing the PNE achieved by the sampled agents since it informs the loss suffered by the principal under their chosen classifier. Thus, understanding equilibrium properties such as existence, uniqueness, and computation is crucial to any learning algorithm. However, such properties cannot be precisely stated without a structured model of the agent utilities, which in our game is largely predicated upon the cost and externality. We formally state our assumptions below:\n1. The cost is given by the $l_2$ norm of the manipulation vector: $c(x_i, x'_i) = a||x_i - x'_i||_2^2$\n2. Externalities are pairwise symmetric: $\\forall i, j, t(x_i, x'_i, x_j, x'_j) = t(x_j, x'_j, x_i, x'_i)$.\n3. Total externality faced by any agent i, $T(x_i, x'_i, X, X')$ is smooth and convex in the manip-ulation variables $(x'_1,...,x'_k)$."}, {"title": "Learning and Generalization", "content": "We now tackle the problem of learnability in our setting. Broadly speaking, the learner's goal is to ensure that a classifier trained on a finite dataset to anticipate agents misreporting to an equilibrium, performs well on the broader population distribution which exhibits similar behaviour.\nA unique aspect of our setting is the number of agents participating in a given instance need not be fixed. Taking university admissions as an example, the number of applicants in a given year is not constant but rather a random variable. Our generalization result should accommodate this variability in the number of players in each instance, alongside the equilibrium they reach.\nRecall from Section 2 that $k_{max}$ denotes the largest number of simultaneous participants with $K$, a distribution over $[k_{max}]$, and $D$, the data distribution that we sample from, i.e., $(x_i, y_i) \\sim D$.\nThe learner has access to a training dataset of n instances, where each instance involves a sample $k_i \\sim K$, and then $k_i$ independent samples from the distribution $D$. Observe that the following is an equivalent sampling procedure: sample $k_i \\sim K$, draw $k_{max}$ independent samples from $D$, and then only consider the first $k_i$ elements. We use this latter procedure for the generalization result; formally, we define a joint distribution\n$D = D \\times ... \\times D \\times K$.\n$k_{max}$\nThe learner has access to n samples from this distribution, representing their training set: $S = \\{(X_1,Y_1, k_1), ..., (X_n, y_n, k_n)\\}$, where $X_i \\in \\mathbb{R}^{k_{max} \\times d}$, $Y_i \\in \\mathbb{R}^{k_{max}}$, and $k_i \\in [k_{max}]$. Note that each sample/instance in this dataset can be seen as holding the attributes of $k_i$ individuals.\nFor any chosen classifier $f_w$, the participating agents reach a PNE and the classifier suffers a resulting loss based on this. In the preceding section, we show this equilibrium to be the solution of a convex optimization problem. To provide formal learning guarantees, however, it is important to understand how the outcome of this optimization (i.e. the PNE) behaves as the classifier changes. Indeed, if the PNE drastically changes due to small changes in w, learning can be challenging.\nLet $NE(X, f_w, k) : \\mathbb{R}^{k_{max}\\times d} \\times \\mathbb{R}^d \\times [k_{max}] \\rightarrow \\mathbb{R}^{k_{max}\\times d}$, where the first k rows of the output correspond to the equilibrium solution when k agents are participating. We prove in the following result that the potential function optimum (and thus the PNE strategy) is Lipschitz continuous in w. The theorem statement treats inputs to $\\Phi$ and NE as vectors. This is without loss of generality since our $X \\in \\mathbb{R}^{k_{max}\\times d}$ feature matrix is equivalent to a $x \\in \\mathbb{R}^{dk_{max}}$ vector, with the vector $l_2$ norm $||x||_2$ equivalent to the matrix Frobenius norm $||X||_F$.\nLemma 1. Let $\\Phi(x,x', f_w)$ be the potential function for $k_{max}$ agents ($x,x' \\in \\mathbb{R}^{dk_{max}}$). Then for $\\eta = \\frac{\\gamma}{2}$, where $c = \\min_{x', \\omega}(\\min(\\nabla^2_{xx} \\Phi(x, x', f_w)))$ and $\\gamma = \\max_{x', \\omega} ||\\nabla^2_{x'x} \\Phi(x, x', f_w)|| + 1$, the function $NE(x, f_w,k)$ is $\\eta$-Lipschitz in $\\omega$ (under the $||\\cdot||_2$ norm) for any k.\nThe proof (deferred to the Appendix) is technical and in fact shows the Lipschitz continuity for a general class of parametrized convex optimization problems. Noting that local Lipschitz-ness suffices, we lower bound the difference between an optimal and sub-optimal solution using properties of strict concavity and smoothness of the potential function. The remainder of the proof leverages the continuity of the objective in both x' and w along with the insights above to establish the Lipschitz-ness of the maximizer of $\\Phi$. To relate this to $NE(X, f_w,k)$, we define the optimization objective as summing the potential function over only the first k participants with a trivial function involving the remaining agents.\nNext, for a loss function $l(f_w(x_i), y_i)$ defined on classifying an individual's reported features, the average loss on a sample consisting of k participating strategic agents is given by $L(X, y, f_w, k) =$"}, {"title": "Minimizing Empirial Risk", "content": "While we have shown PAC learning is possible in our multi-agent strategic classification game, a cornerstone of learning is minimizing the empirical risk; that is, finding a classifier that minimizes loss incurred on the training set S. Even for a convex loss function l and linear classifier $f_w$, minimizing this across the samples of S is non-trivial since the NE function is the solution to an optimization problem. In general, there is no closed-form expression for the NE function and we cannot hope for this loss minimization problem to be convex.\nMachine learning nonetheless has a vast literature on optimizing nonconvex loss functions; these, however, are largely gradient-based and require computing the loss gradient with respect to the learning parameter w. This loss is computed with respect to the equilibrium outcome/score of a participating agent i: $f_w(NE(X, f_w)_{i:})$. When $f_w$ is linear, this is simply $(NE(X, f_w)_{i:},w)$. We now show when and how this gradient can be explicitly computed.\nLet $v_i = NE(X, f_w)_{i:}$ and $z_i = f_w(NE(X, f_w)_{i:}) = f(v_i, w)$ for $i \\in [k]$, noting that $z_i \\in \\mathbb{R}$ and $v_i \\in \\mathbb{R}^d$. The loss gradient is given as follows:\n$\\nabla_w L(x, y, f_w) = \\frac{1}{k} \\sum_{i=1}^k \\nabla_w l(f(NE(X, f_w)_{i:}, w), y_i) = \\frac{1}{k} \\sum_{i=1}^k \\nabla_w l(z_i, y_i) = \\frac{1}{k} \\sum_{i=1}^k \\frac{\\partial l}{\\partial z_i} \\nabla_w z_i$\n$= \\frac{1}{k} \\sum_{i=1}^k \\frac{\\partial l}{\\partial z_i} [\\nabla_w f(NE(x, f_w)_{i:}) + \\nabla_w f]$\nWe use the chain rule here, with $J_w$ representing the Jacobian with respect to w. $\\frac{\\partial l}{\\partial z_i}$ is the derivative of the loss with respect to the score, a feature of the loss function. Further, the gradients"}, {"title": "Models of Externality", "content": "The presented results leverage the strict concavity of the potential function to assert that the PNE is unique, efficiently computable, and PAC learning guarantees possible for the learning problem. Section 3 concluded by noting a more general Condition (1') on the cost and externality to en-sure this. This allows us to capture possibly non-convex externality; we give two such examples motivated by the settings we highlighted.\nProportional Externality: Externality in strategic classification can model an increased risk in detection/audit due to others also manipulating. In our admissions example, the risk of the univer-sity (classifier) suspecting something is amiss is much higher when many students wrongfully claim to be top of their class, as opposed to a handful. As such, it is natural that one's externality due to manipulation increases proportional to the extent to which others in the system also manipulate. We express the externality suffered by agent i in such a setting as:\n$t(x_i, x_i, x_j, x'_j) = \\frac{\\beta}{k-1} \\sum_{l=1}^d (x'_{il} - x_{il})^2 (x'_{jl} - x_{jl})^2$\nWe scale by k \u2212 1 since each agent pays a single cost c(.) but suffers externality from k \u2212 1 agents; this allows a (the scale constant for the cost) and \u1e9e to be on the same scale. Next, observe that for individuals who do not manipulate, the total externality they incur is 0. This is consistent with one interpretation of the university admissions setting: even if many claim to be top of their class, those who truly are, have nothing to fear. We note this externality is pairwise symmetric and we show below that it satisfies the updated condition (proof in Appendix).\nProposition 1. Under the cost $c(x,x') = a||x - x'||_2$, the cumulative impact of manipulation under the proportional externality model is smooth and strictly convex for $\\beta < \\alpha$.\nCongestion Externality: In many decision-making scenarios, reported features map to real re-sources and have downstream consequences beyond classification. Externality can thus model an increased cost for manipulated features due to demand from others. Many countries, for example, deploy immigration policies that favour candidates who pledge to settle in under-populated areas (Picot et al., 2023). Naturally, an influx of applicants may report such intentions and initially move to these areas (with many reneging on this soon after and relocating); this can significantly in-crease housing and living costs for new immigrants in these underpopulated communities. In such cases, individuals suffer from others manipulating even if they are being honest. Externalities of this nature can be modelled as follows:\n$t(x_i, x_i, x_j, x'_j) = \\frac{\\beta}{k-1} exp(- (x_{il} - x'_{jl})^2)$"}, {"title": "Discussion", "content": "This paper studies a fundamental question within the strategic classification paradigm: what is the effect of inter-agent externalities on both the agents and the classifier? It is no longer reasonable to assume agents simply best respond to the classifier; rather, the Nash Equilibrium of the induced game becomes the natural solution concept, and we provide a set of conditions whereupon this equilibrium is unique and efficiently computable. The classifier, on the other hand, must learn an optimal model with respect to such an induced equilibrium. We show that this Stackelberg-Nash Equilibrium can be learned in a PAC sense and its loss gradients computable. This paper shows the possibility of deploying loss-minimizing classifiers robust against rich manipulation dynamics.\nA limitation of our work is the structural assumptions on externality. Their pairwise nature gives rise to a potential game, and the convexity assumption ensures this is efficiently computable and satisfies Lipschitz regularity conditions. This leads to an intriguing open question: what learning guarantees, if any, can be given for non-concave potential functions where equilibria may not be unique or well-behaved? Can we precisely specify the necessary conditions (our results give a set of sufficient conditions) on externality to ensure both learnability and robustness? Understanding this is both technically and practically interesting. Another important direction is assuming the learner does not know the cost and externality model and must learn them by observing equilibrium reports over multiple interactions. This closely parallels the literature on online strategic classification (Dong et al., 2018; Chen et al., 2020) and learning in games (Cesa-Bianchi & Lugosi, 2006). Given the high stakes, characterizing the welfare properties of multi-agent strategic interaction is also imperative. This can involve parametrizing the price of anarchy (Roughgarden, 2010) in terms of a given classifier or simultaneously optimizing for welfare alongside robustness. Lastly, engaging with pertinent stakeholders to accurately capture real costs and externality and assess the implications of our model is an important future step. Our work lays the groundwork for such important questions, allowing us to comprehensively understand and mitigate the real challenges of classification in strategic multi-agent settings."}, {"title": "Proof of Lemma 1", "content": "Proof. We begin by proving the Lipschitz continuity of the maximizer of the following generic optimization problem:\n$x^* (w) = \\arg \\max_{x'\\in X} \\Psi(x', w)$\nwhere $\\Psi$ is strictly concave in x' for any w \u2208 \u03a9 and X is convex. We know there is always a unique maximizer, and thus the optimal value $x^*(w)$ is a well-defined function. We also note that our optimization problem satisfies the conditions of Berge's Maximum Theorem (Berge, 1963). Thus, we can immediately conclude that the set-valued correspondence from any w to the set of maximizers is upper-hemicontinuous. Since our maximizer is unique - i.e. the set is a singleton - it suffices to observe that any set-valued map that is a singleton is upper-hemicontinous if and only if the corresponding function is continuous.\nLet $B_{\\epsilon} (w)$ refers to an open ball of radium \u025b centered at w. For Lipschitz-ness, we first note that since \u03a9 is bounded and compact, it suffices to prove this locally. That is, we wish to show that for any w, there exists constants L,\u025b\u025b > 0 such that for all w \u2208 $B_{\\epsilon}(w)$, $||x^*(w) \u2013 x^*(w)||_2 \\le L||w \u2013 w||_2$. Indeed, this means that for any two w',w\" \u2208 \u03a9, we can consider a sequence of distinct point \u03c9\u2081 = \u03c9', \u03c92, . . .,\u1ff3n\u22121, \u1ff3n = \u03c9\" lying on the line \u03b1\u03c9' + (1 \u2212 a)w\" such that $||w_i \u2013 w_{i-1}||_2 \\le \\epsilon_{w-1}$. Then we have the following, which means it suffices to focus on local Lipschitz-ness:\n$||x^* (w\") \u2013 x^* (w')||_2 \\le \\sum_{i=1}^n L||w_i \u2013 w_{i-1}||_2 = L||w\" \u2013 w'||_2$\nWe next prove a property that holds for the maximizer of our problem under any w. Fix a w, and let $x^*$ be the maximizer. Next, consider a Taylor expansion of $\\Psi$ at \u00e6*. One formulation of this presented in Chapter 2 of Wright & Recht (2022) states for some \u03b3\u2208 (0,1), we have (the dependence on w is dropped for now as it is unchanged):\n$\\Psi(x', \u00b7) = \\Psi(x^*, \u00b7) + (x' - x^*)^T\\nabla_{x'} \\Psi(x^*, \u00b7) + \\frac{1}{2} (x' - x^*)^T\\nabla^2_{x'x'} \\Psi(x^* + \\gamma(x' - x^*), \u00b7)(x' - x^*)$\n$\\Psi(x', \u00b7) \\le \\Psi(x^*, \u00b7) + \\frac{1}{2} (x' - x^*)^T\\nabla^2_{x'x'} \\Psi(x^* + \\gamma(x' - x^*), \u00b7)(x' - x^*)$\nwhere the second line follows from Lemma 2.7 in Still (2018), which states that at for any \u00e6' \u2208 X, $\\nabla_{x'}\\Psi(x^*, w) \u00b7 (x' \u2013 x^*) \\le 0$. We also note that $\\nabla^2_{xx} \\Psi(x',\u00b7)$ is the Hessian matrix which is (1) always symmetric and (2) consists of all strictly negative eigenvalues since $\\Psi(\u00e6', \u00b7)$ is always strictly concave. Since any symmetric matrix can be diagonalized as QAQT, where Q is the matrix of orthonormal eigenvectors and the A the eigenvalues, we have that (define $p = x' \u2212 x^*$):\n$(x' \u2013 x^*)^T\\nabla^2\\Psi(x^* + \\gamma(x' - x^*), \u00b7)(x' \u2013 x^*) = p^TQAQp = \\sum_{i=1}^{z} \\lambda_i(v_i p)^2$\nwhere vi is the ith eigenvector. Since we have strictly negative eigenvalues and Q is an orthonor-mal matrix and thus does not affect the norm of vector it matrix multiplies, we have (where the $\\lambda_{min} (\\nabla^2_{xx}\\Psi)$ returns the minimum eigenvalue of the Hessian across w \u2208 \u03a9 and x' \u2208 X):\n$(x' \u2013 x^*)^T\\nabla^2\\Psi(x^* + \\gamma(x' - x^*), \u00b7)(x' \u2013 x^*) \\le \\lambda_{min} (\\nabla^2\\Psi)||QP||^2 = \\lambda_{min} (\\nabla^2\\Psi)||P||^2$\n$\\Psi(x', \u00b7) \\le \\Psi(x^*,\u2022) + \\frac{1}{2} \\lambda_{min} (\\nabla^2\\Psi)||x' \u2013 x^*||^2$"}, {"title": "Proof of Proposition 1", "content": "Proof. We express the cumulative impact of manipulation under the stated conditions as follows:\n$\\alpha \\sum_{i=1}^k \\sum_{l=1}^d (x'_{il} - x_{il})^2 + \\frac{\\beta}{k-1} \\sum_{i=1}^k \\sum_{j>i} \\sum_{l=1}^d(x'_{il} - x_{il})^2 (x'_{jl} - x_{jl})^2$\n$=\\sum_{i=1}^k [\\alpha \\sum_{l=1}^d(x'_{il} - x_{il})^2 + \\frac{\\beta}{k-1} \\sum_{j>i} \\sum_{l=1}^d(x'_{il} - x_{il})^2 (x'_{jl} - x_{jl})^2 ]$\nSince the sum of strictly convex functions is convex, it suffices to show that each inner summand is strictly convex. For a fixed l, we observe that since since there are exactly $\\frac{k(k-1)}{2}$ pairs satisfying j > i, and in each feature $x_i$ appears in exactly $(k-1)$ of these pairs, the inner summand can be written as follows:\n$\\forall l: \\sum_{i=1}^k \\sum_{j>i} [\\alpha \\sum_{l=1}^d(x'_{il} - x_{il})^2 + \\frac{\\beta}{k-1} (x'_{il} - x_{il})^2 (x'_{jl} - x_{jl})^2 + \\frac{\\beta}{k-1} (x'_{jl} - x_{jl})^2]$\nAgain since convexity is preserved in summation, we only need to show strong convexity with respect to the summands, each of whom is a function of two variables ($x'_{il}$ and $x'_{jl}$ since $x_i$ and $x_j$ are constants). For an arbitrary summand index by (i, j), let $u_i = (x'_{il} - x_{il})$ and $u_j = (x'_{jl} - x_{jl})$. Then the Hessian (upon multiplying Equation 10 by k \u2212 1, which does not affect convexity), is:\n$\\nabla^2_{x'_{il} x'_{jl}} = \\begin{bmatrix} 2\\alpha + 2 \\beta u_j^2 & 4 \\beta u_i u_j\\\\4 \\beta u_i u_j & 2\\alpha + 2\\beta u_i^2\\end{bmatrix}$\nThe determinant of this Hessian, when simplified, is given by:\n$\\det(\\nabla^2_{x'_{il} x'_{jl}}) = 4\\alpha^2 + 4\\alpha \\beta u_i^2 + 4 \\alpha \\beta u_j^2 - 12 \\beta^2 u_i^2 u_j^2$\nSince we wish to show the determinant is strictly positive, $\\alpha^2 + \\alpha \\beta u_i^2 + \\alpha \\beta u_j^2 > 3 \\beta^2 u_i^2 u_j^2$. As feature vectors are bounded, $u_i, u_j \\in [-1,1]$, and our condition is $\\alpha > \\beta$, the following holds:\n$\\alpha^2 + \\alpha \\beta u_i^2 + \\alpha \\beta u_j^2 > \\beta^2 + \\beta^2 u_i^2 + \\beta^2 u_j^2 \\geq \\beta^2 u_i^2 u_j^2 + \\beta^2 u_i^2 + \\beta^2 u_j^2 \\geq 3 \\beta^2 u_i^2 u_j^2$\nwhere the second last transition uses the fact that $u_i u_j < u_i^2 + u_j^2$ in the feature vector range."}, {"title": "Proof of Proposition 2", "content": "Proof. We express the cumulative impact of manipulation under the stated conditions as follows:\n$\\alpha \\sum_{i=1}^k \\sum_{l=1}^d (x'_{il} - x_{il})^2 + \\frac{\\beta}{k-1} \\sum_{i=1}^k \\sum_{j>i} \\sum_{l=1}^d exp(-(x_{il} - x_{jl})^2)$\n$=\\sum_{i=1}^k [\\alpha \\sum_{l=1}^d (x'_{il} - x_{il})^2 + \\frac{\\beta}{k-1} \\sum_{j>i} \\sum_{l=1}^d exp(-(x_{il} - x_{jl})^2) ]$\nSince the sum of strictly convex functions is convex, it suffices to show that each inner summand is strictly convex. For a fixed l, we observe that since since there are exactly $\\frac{k(k-1)}{2}$ pairs satisfying"}]}