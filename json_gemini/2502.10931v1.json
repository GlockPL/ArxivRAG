{"title": "D-CIPHER: Dynamic Collaborative Intelligent Agents with Planning and Heterogeneous Execution for Enhanced Reasoning in Offensive Security", "authors": ["Meet Udeshi", "Minghao Shao", "Haoran Xi", "Nanda Rani", "Kimberly Milner", "Venkata Sai Charan Putrevu", "Brendan Dolan-Gavitt", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "abstract": "Large Language Models (LLMs) have been used in cybersecurity in many ways, including their recent use as intelligent agent systems for autonomous security analysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing the automated task-planning abilities of LLM agents across various cybersecurity skill sets. Early attempts to apply LLMs for solving CTF challenges relied on single-agent systems, where feedback was restricted to a single reasoning-action loop. This approach proved inadequate for handling complex CTF tasks. Drawing inspiration from real-world CTF competitions, where teams of experts collaborate, we introduce the D-CIPHER multi-agent LLM framework for collaborative CTF challenge solving. D-CIPHER integrates agents with distinct roles, enabling dynamic feedback loops to enhance reasoning on CTF challenges. It introduces the Planner-Executor agent system, consisting of a Planner agent for overall problem-solving along with multiple heterogeneous Executor agents for individual tasks, facilitating efficient allocation of responsibilities among the LLMs. Additionally, D-CIPHER incorporates an Auto-prompter agent, which improves problem-solving by exploring the challenge environment and generating a highly relevant initial prompt. We evaluate D-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive studies to highlight the impact of our enhancements. Our results demonstrate that the multi-agent D-CIPHER system achieves a significant improvement in challenges solved, setting a state-of-the-art performance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox. D-CIPHER is available at https://github.com/NYU-LLM-CTF/nyuctf_agents as the nyuctf_multiagent package.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable potential in cybersecurity applications such as vulnerability detection [2, 15, 20, 22], bug localization [19, 49], and automated program repair (APR) [5, 40, 41]. Recent advancements in LLM capabilities have led to growing interest in leveraging LLMs to autonomously solve complex cybersecurity tasks. Autonomous agents for offensive security tasks are essential to keep pace with the rapidly expanding cyber threats, as highlighted by the DARPA Cyber Grand Challenge [11] and the AI Cyber Challenge [12]. LLMs demonstrate significant potential for automating offensive security tasks [3, 36]. Capture the Flag (CTF) challenges have gained popularity as a means of evaluating and improving cybersecurity skills across all levels [8, 35]. CTF challenges often include complex tasks that require expertise across diverse domains, including cryptography, digital forensics, and reverse engineering, offering a platform to evaluate an LLM's proficiency in cybersecurity and automated task planning by simulating real-world offensive security scenarios [25, 27, 29, 32, 45]. The evaluation of autonomous LLM agents is most effective with jeopardy-style CTF challenges, which emphasize exploiting standalone software like a binary that can be reverse-engineered, encrypted data that can be decrypted, or a web server with authentication that can be bypassed. Successfully compromising the software results in the discovery or revelation of a unique \"flag\" string, serving as a clear indicator of success. CTF challenge benchmarks [29, 48] advance the autonomous problem-solving abilities of LLM agents.\nCurrent LLM agents designed to solve CTFs operate as single agents tasked with handling the challenge from start to finish. CTFs are complex tasks that require significant amounts of exploration and proper execution of a sequence of tasks to solve the problem and get the flag. Single agent frameworks typically restrict feedback mechanisms to self-reflection within the LLM's context. The agent needs to go through several reasoning-action-observation steps to achieve success, with multiple exploration steps that produce irrelevant outputs and multiple retries to complete a single task. This causes many issues that hinder the agent's problem-solving abilities, such as loss of focus on the broad problem and hallucinations about task details. On the other hand, real-world CTF competitions are generally solved collaboratively by teams [7, 9] of members with diverse expertise to tackle challenges across various domains. All member continuously share insights, provide feedback, and refine strategies through interactive collaboration. Current CTF-solving LLM agents cannot fully capture the collaborative nature of CTF competitions. Inspired by this, we introduce a multi-agent framework that divides responsibilities among multiple LLM agents and facilitates dynamic interactions among the agents to allow collaborative problem-solving. While multi-agent systems have gained prominence across various domains [13, 18, 42], their application to solving CTF challenges remains unexplored. Incorporating the collaborative potential of multi-agent systems can better emulate the team dynamics seen in real-world CTF competitions.\nWe present D-CIPHER, a novel multi-agent framework designed to autonomously solve CTF challenges via collaboration of multiple LLM agents. To overcome the limitations of single agent systems, D-CIPHER introduces two mechanisms to facilitate enhanced interaction and dynamic feedback between LLM agents. The first is the Planner-Executor agent system that involves a Planner agent with the responsibility of solving the CTF challenge end to end, and multiple heterogeneous Executor agents with the responsibility of completing single tasks assigned by the Planner. Dividing responsibilities between planner and executors allows each agent to maintain focus for longer, more complex tasks, and reduces hallucinations. The second is the Auto-prompter agent, tasked with exploring the challenge and generating an initial prompt for the main system. Auto-prompting is a prompt engineering technique to improve LLM performance by generating dynamic task-specific prompts as opposed to human-written hard-coded prompt templates. D-CIPHER incorporates auto-prompting as a separate agent which facilitates environment exploration and produces a highly-relevant initial prompt to kick-start the main system's problem solving."}, {"title": "2 Background", "content": "Text-based LLMs take a text prompt as input from the user, and produce a text output that follows the user prompt. LLMs have a finite length of text tokens that they can process called the context. An alternating sequence of user prompts and LLM outputs makes a conversation and is the basis of chat-based LLM interfaces like ChatGPT. To remove the user from the loop and create autonomous agents, a feedback mechanism is added based on the LLM outputs, so that the LLM can autonomously continue the conversation. Yang et al. [44] introduce iterative feedback prompting where the LLM is tasked with writing a piece of code, and the code's compilation and execution logs are provided as feedback, which the LLM uses to iteratively refine it's output. Recent LLMs support function calling, a way to provide a set of actions to the LLM that it may choose to \"call\" as a function. In this manner, LLM agents can be provided with many \"tools\" such as a command line, web search, file editing, and code execution [37], so that they can autonomously perform various tasks like software development [43], web browsing [47], or solve CTF challenges [1, 29].\nWith access to the command line and file editing tools, LLM agents can autonomously solve many tasks, but they still struggle on complex long-horizon tasks such as CTF challenges that require multiple steps. Plan-and-solve prompting [38] enhances long-term focus of the agent by incorporating a planning phase before iterative execution. This helps agents tackle ambiguous or complex tasks by structured strategies [34]. ReAct (reasoning + action) [46] combines step-by-step reasoning with action, allowing the agent to adjust dynamically through iterative cycles. ReWOO (Reasoning without Observation) [42] separates the reasoning process from tool outputs and observations, allowing it to handle multi-step reasoning tasks efficiently while maintaining focus. The prompting methods in these agents involve static hard-coded templates where environment and task information is filled in. While static prompts provide straightforward guidance, they often fail to adapt to different problems and complex tasks, limiting their effectiveness. Auto-prompting [30, 50, 51] is a technique to allow the LLM itself to generate a highly-relevant prompt. Auto-prompting invokes more factual responses and reduces hallucinations in LLMs. D-CIPHER incorporates auto-prompting as a separate agent that can explore the environment and generate a better prompt."}, {"title": "3 Related Works", "content": "Expanding on single LLM agents, multi-agent LLM systems are a powerful approach to enhance problem-solving by simulating team-based collaboration. Specialized agents, each with distinct objectives, work together to tackle different aspects of complex tasks [14] Multi-agent systems are effective in cybersecurity applications. For instance, Audit-LLM [31] deploys a multi-agent system for insider threat detection by employing agents to decompose tasks, build tools, and use collaborative reasoning to enhance detection accuracy. Liu [21] explores multi-agent systems to enhance incident response in cybersecurity by examining centralized, decentralized, and hybrid team structures to assess how LLM agents can improve decision-making, adaptability, and coordination during cyber-attack scenarios. AutoSafeCoder [24] enhances the security of code generated by LLMs by incorporating a coding agent for code generation, a static analyzer agent that identifies vulnerabilities, and a fuzz testing agent for dynamic testing to detect runtime errors. Division of responsibilities among different agents allows AutoSafeCoder to produce secure, functionally correct code.\nTann et al. [32] evaluate early LLMs such as ChatGPT and Google Bard in solving CTF challenges and answering professional certification questions, showing that LLM responses contain key task information. The InterCode-CTF agent [44] reveals that LLM agents demonstrate basic cybersecurity skills, however they struggle with more complex tasks. The NYU CTF baseline agent [28] integrates external tools into the LLM's function-calling features and demonstrate improved potential of tool-assisted LLMs to solve CTFs, however it exhausts the LLM context length when command output history becomes very long. InterCode-CTF manages this issue by truncating the history to only show the LLM the last few iterations. Even so, LLM agents face issues with longer tasks.\nExcessive tool availability and verbose interfaces can overwhelm agents, leading to inefficiencies. Agents perform better with a focused set of tools with well-defined interfaces [43]. EnIGMA [1] agent incorporates interactive tools and in-context learning techniques to achieve state-of-the-art results. For better context management, EnIGMA also uses an LLM summarizer that summarizes the command outputs for the main agent.\nHackSynth [23], an LLM agent for autonomous penetration testing, shows that iterative planning and feedback summarization stages help the agent finish multiple tasks and improves overall problem solving. Similarly, Cybench [48] introduces a benchmark of 40 CTF challenges augmented with step-by-step tasks, demonstrating better focus of LLM agents on smaller tasks, leading to improved success and alleviating the context length issue. Turtayev et al. [34] expand on InterCode-CTF by implementing plan-and-solve prompting, achieve significant improvement on the InterCode-CTF benchmark. They show that prompting techniques can improve performance even with simple toolsets.\nThese works highlight that LLM agents excel at implementing code and executing commands to accomplish small concrete tasks when provided with dynamic feedback and task-specific toolsets. While these works involved using multiple LLMs with different tasks such as planning and summarizing along-side a main agent, D-CIPHER is the first work to formulate a multi-agent system where there is a bifurcation of responsibilities between agents and meaningful well-defined interactions for dynamic feedback."}, {"title": "4 D-CIPHER Implementation", "content": "The D-CIPHER framework introduces a collaborative multi-agent system of LLM agents with a seperation of responsibilities. Each agent's architecture builds upon the NYU CTF baseline framework [29] with upgraded prompts that describe agent-specific tasks and additional tools to define agent interactions. Function calling features of current LLMs are utilized to prompt for agent actions. The system has three agents: (1) the Planner agent generates the overall plan to solve the CTF challenge, delegating specific tasks to the Executor, and revising the plan based on Executor feedback; (2) the Executor agent handles the execution of the task delegated by the Planner and returns the task execution summary to the Planner; and (3) the Auto-prompter agent generates a prompt for solving the CTF challenge based on the initial environment exploration."}, {"title": "4.1 Prompting and Context Management", "content": "Each agent maintains a conversation history of LLM inputs and outputs. The conversation starts with a system prompt that defines the role of the LLM of that agent, an initial prompt that describes the current task, and an alternating sequence of LLM actions (e.g., a command to run) and observations (e.g., output of the command). Following the ReAct strategy, we tell the LLM to reason about each action, so these reason and action make an LLM message. We utilize the function calling features of current LLMs to produce actions, so we do not define a structured format of our own, but instead rely on the LLM provider's API to parse the actions correctly. At every iteration, the conversation history is sent to the LLM and it generates a message containing the reason and action. Observations from executing the actions are appended to the conversation history. This constitutes a \"round\" of conversation. The LLM agent can continue these rounds of conversation as long as the LLM context is not full. To avoid context length issues, we use two techniques: first, we truncate observations to 25,000 characters; second, we optionally truncate the conversation history to the last 5 action-observation pairs while keeping the reason part intact, similar to [1, 43, 44]. The second technique is only applied to the Executor, as it has been observed to help execute long tasks."}, {"title": "4.2 Tools", "content": "Each agent interacts with the same Linux container environment where the challenge files present. The container's network has access to the challenge server and the internet to install new packages. The agents contain only a basic set of tools to interact with the environment: RunCommand to execute shell commands in the container; CreateFile to create a file or script in the container; Disassemble and Decompile to trigger Ghidra to reverse engineer a binary and obtain its disassembly or decompilation; SubmitFlag to submit a flag and get it checked; and, Giveup to giveup on solving the challenge. Unlike EnIGMA, we do not implement advanced interfaces or interactive tools. The specialized reverse engineer-ing tools offer the agents access to Ghidra which does not provide a direct command line interface. Specialized tools for other categories, like RsaCtfTool for cryptography or nikto for web, are mentioned in the category-specific initial prompt because they can be run from the command line via RunCommand. Apart from the above tools, we introduce special functions that define the interaction between agents: GeneratePrompt for the Auto-prompter to generate a prompt for the Planner; Delegate for the Planner to delegate a task to an Executor; and, FinishTask for the Executor to return a task summary to the Planner."}, {"title": "4.3 Workflow", "content": "The framework first initiates the Auto-prompter agent. The Auto-prompter initially explores the challenge files and interacts with the challenge server or binary if available. After a few exploration turns, it generates a detailed and specific prompt based on its exploration to solve the challenge and calls the GeneratePrompt tool. The framework terminates the Auto-prompter and initiates the Planner with this generated prompt.\nThe Planner is instructed to explore the challenge similar to the Auto-prompter for a few turns. The Planner comes up with the plan and delegates a task to the Executor by calling the Delegate tool. The framework pauses the Planner, and initiates an Executor with this task. The Executor tries to complete the task by running appropriate commands via RunCommand and creating files and scripts via CreateFile. After the Executor finishes or when it cannot proceed any further, it calls FinishTask with task execution and results summary. The framework terminates the Executor, and returns the task summary to the Planner as a result of Delegate call.\nThe Planner continues to revise its plan and delegate further tasks. For each Delegate call, the framework initiates a new Executor with a new conversation history. Each Executor focuses only on it's own task, and the Planner only sees the task summary, allowing for efficient context management of the LLM. This workflow ensures continuous interaction between the Planner and Executors such that they coordinate to enhance collective problem-solving."}, {"title": "4.4 Planner Agent", "content": "The Planner is the central agent in D-CIPHER, responsible for solving the entire CTF challenge. The system prompt defines it's role as a planner in a planner-executor system and instructs to generate a plan and delegate tasks one by one. The initial prompt is set as the prompt generated by the Auto-prompter, however the Planner is not made aware of the Auto-Prompter in the system prompt. Only the Planner is allowed to call the SubmitFlag or Giveup tools to terminate the challenge. The Planner is given access to the RunCommand tool but not the CreateFile, Disassemble, or Decompile tools. This allows the Planner to explore the challenge, but prevents it from trying to solve the challenge by itself.\nThe following conditions stop the Planner and terminate the challenge: SubmitFlag is called with the correct flag; Giveup is called; the Planner conversation has exceeded the maximum number of rounds; or all agents combined have exceeded the maximum cost budget of LLM API calls. If a wrong flag is submitted, a negative response is returned and the Planner may continue.\nEvery prompt to the Planner LLM contains the entire conversation history, with the intermediate plans, delegated tasks, and returned task summaries. Only the task summary returned by the executor is added to the Planner's conversation history. This allows the Planner to view the entire challenge progress to revise the plan and delegate further tasks. If an Executor fails to return a task summary, a warning is returned instead and the Planner can retry the same task with revised instructions."}, {"title": "4.5 Executor Agent", "content": "The system prompt defines the role as an executor in a planner-executor system and instructs to execute the task delegated by the planner. The initial prompt uses a hard-coded template where challenge details and the delegated task instructions are filled in. The template is tailored for each category and recommends usage of some category-specific command line tools.\nThe following conditions stop the Executor and resume the Planner: FinishTask is called with a task summary; the Executor conversation has exceeded the maximum number of rounds; or, all agents combined have exceeded the maximum cost budget of LLM API calls. If the Executor exhausted the maximum number of rounds without finishing the task, it is prompted one last time to call FinishTask. If FinishTask is still not called, a warning message is returned to the Planner.\nEvery prompt to the Executor LLM contains a truncated conversation history that only shows the last few actions and observations. The system and initial prompt with challenge and task details are always sent, along with any intermediate thoughts of the Executor. Truncating to the last few observations prevents the LLM context from filling up and maintains the Executor's focus on the current task. Every Executor instantiation by the framework starts with a fresh conversation history. No part of the previous Executors' conversation is included, except few details which the Planner may include in the task description. This also helps maintain the Executor's focus on the current task, and allows for heterogeneous execution with different tasks."}, {"title": "4.6 Auto-prompter Agent", "content": "The Auto-prompter's system prompt defines it's role as an agent tasked with generating a prompt to solve the CTF challenge. The initial prompt contains details of the challenge files and server. The Auto-prompter is not made aware of the planner-executor system, allowing it to generate a versatile prompt for any solver system.\nThe Auto-prompter halts under the following conditions: when the GeneratePrompt function is called, the conversation exceeds the maximum number of rounds, or the combined cost of LLM API calls by all agents surpasses the allocated budget. Similar to the Executor, if the Auto-prompter runs out of rounds, it is given one final prompt to call GeneratePrompt. If GeneratePrompt is still not invoked, a predefined prompt template is used with the challenge details appropriately filled in."}, {"title": "5 Experiment Setup", "content": "Each run of D-CIPHER attempts to solve one CTF challenge. The following configuration options are set for each run: an overall cost budget of $3, a temperature of 1.0 for each LLM, 5 maximum rounds for the Auto-prompter, 30 maximum rounds for the Planner, 100 maximum rounds for each Executor, and the conversation history of each Executor is truncated to last 5 action-observation pairs."}, {"title": "5.1 Benchmarks", "content": "We evaluate D-CIPHER across three benchmarks: NYU CTF Bench [29], Cybench [48], and HackTheBox [16]. These benchmarks comprise a total of 290 challenges spanning six categories: cryptography (crypto), forensics, binary exploitation (pwn), reverse engineering (rev), web, and miscellaneous (misc), ensuring a well-rounded evaluation of D-CIPHER. We perform our ablation studies on NYU CTF Bench. During framework development and configuration, we use the development set introduced in [1] to select optimal features and design prompts. The development set is an extension of NYU CTF Bench containing 55 extra challenges across the same six categories. By using a separate development set, we prevent overfitting our design to the main benchmarks and avoid biasing the results. We evaluate Cybench in the unguided mode, where we do not utilize the additional subtask information for each challenge. We use the challenge description in the \"hard prompt\" that does not contain extra hints."}, {"title": "5.2 LLM Model Selection", "content": "We test multiple LLMs with D-CIPHER for each of the Planner, Executor, and Auto-prompter agents. For the main experiment, we use the same LLM for all three agents. However, the framework offers freedom to use different LLMs for each agent, and we experiment by combining stronger models for the Planner with weaker models for the Executor. We access the LLMs via their provided APIs: OpenAI API for GPT, Anthropic Inference API for Claude, and Google API for Gemini. For the open-source LLaMa models, we use the Together AI platform [33] via their API. The stronger models and their unique strings are:\n\u2022 Claude 3.5 Sonnet: claude-3-5-sonnet-20241022\n\u2022 GPT 4 Turbo: gpt-4-turbo-2024-04-09\n\u2022 GPT 40: gpt-40-2024-11-20\n\u2022 LLaMa 3.1 405B: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n\u2022 Gemini 1.5 Flash: gemini-1.5-flash\nThe weaker models and their unique strings are:\n\u2022 Claude 3.5 Haiku: claude-3-5-haiku-20241022\n\u2022 GPT 40 Mini: gpt-40-mini-2024-07-18\n\u2022 LLaMa 3.3 70B: meta-llama/Llama-3.3-70B-Instruct-Turbo\n\u2022 Gemini 1.5 Flash 8B: gemini-1.5-flash-8b"}, {"title": "5.3 Evaluation Metrics", "content": "We use the percentage of challenges successfully solved by D-CIPHER (% solved) as the primary metric. A challenge is marked solved when the correct flag is submitted by the Planner. We also mark the challenge as solved if the correct flag is observed in any part of the model conversation. This prevents failure of cases where the flag is found by the Auto-prompter or Executor but they do not pass it to the Planner, because only the Planner can submit a flag via the SubmitFlag tool. It is highly unlikely that this method matches a non-flag string because flags are long, unique strings of specific formats such as \"flag...\". This approach mimics real-world CTFS where participants may submit a flag multiple times and receive instant confirmation. Additionally, we report the average cost of solved challenges ($ cost), computed as the total US dollar cost of all API calls of all agents, averaged across successfully solved challenges of a benchmark. As the cost per token for LLMs deployed on the cloud is indicative of the computational resources required to deploy them, the average cost allows us to estimate computational resources for each solved challenge."}, {"title": "6 Results", "content": "Table 3 compares the performance of D-CIPHER with other LLM agents across multiple LLMs and benchmarks. We run D-CIPHER with five different LLMs, using the same LLM for Planner, Executor, and Auto-prompter in each run. We also rerun the NYU CTF baseline agent with three LLMs to measure the impact of recent updates to the LLM models on NYU CTF Bench. The EnIGMA % solved and $ cost are taken from [1], while the category-wise results on NYU CTF Bench are computed from their provided transcripts. We do not compare with the Cybench baseline agent [48] as EnIGMA is the state-of-the-art on Cybench.\nD-CIPHER with Claude 3.5 Sonnet consistently outperforms the current state-of-the-art EnIGMA, achieving 19.0% over 13.5% on NYU CTF Bench, 22.5% over 20% on Cybench, and 44% over 26% on HackTheBox. D-CIPHER with GPT 40 also outperforms EnIGMA with GPT 40 on NYU CTF Bench, while getting a close result on Cybench and HackTheBox. The rerun results of NYU CTF baseline show that recent LLM models have improved on cybersecurity tasks, getting close to EnIGMA's state-of-the-art performance. Yet, D-CIPHER consistently beats the baseline on NYU CTF Bench in overall % solved and across all categories for both Claude 3.5 Sonnet and GPT 40. These results indicate that D-CIPHER improves capabilities across multiple LLM architectures, and the higher performance stems not only from recent LLM updates but also from it's multi-agent system architecture. Interestingly, D-CIPHER without Auto-prompter with Claude 3.5 Sonnet achieves the highest performance of 22% on NYU CTF Bench. However, performance without the Auto-prompter worsens on GPT 40 and on other benchmarks, while average cost increases, indicating that having the Auto-prompter helps overall.\nD-CIPHER's performance improvement stays consistent across the CTF categories. D-CIPHER outperforms EnIGMA across all categories except pwn, with a notable improvement in crypto, where its performance doubles from 7.7% to 15.4%. Likewise, on rev, and misc, we see 9% to 12% increase. The improvement is due to the enhanced task decomposition and execution ability of the Planner-Executor system. Especially, crypto and rev frequently have long outputs of disassembled binaries or encrypted files that require multiple analysis steps that are effectively decomposed by the Planner and performed by the Executor. D-CIPHER's performance is more balanced across different LLMs, demonstrating that our framework operates well with different reasoning capabilities of the LLMs. While D-CIPHER improves in web over previous results, the performance still lags behind other categories, pointing to a common limitation in how web challenges are addressed."}, {"title": "6.2 Comparison of $ cost", "content": "Table 3 compares average $ cost of solved challenges with EnIGMA across the three benchmarks. Except for Claude 3.5 Sonnet on NYU CTF Bench, D-CIPHER has a lower average cost across all LLMs and benchmarks. With GPT 40 and GPT 4 Turbo, D-CIPHER lowers the cost by 2x to 10\u00d7 across benchmarks while solving more challenges. Despite having multiple agents, a significant cost reduction indicates that bifurcation of responsibilities between agents makes the problem-solving system more efficient in terms of computation costs. GPT 40 is most cost efficient across categories, while Claude 3.5 Sonnet is moderately higher on forensics, pwn, and rev. GPT 4 Turbo is the costliest among the three LLMs, for forensics, pwn and web, while on other categories has a lower cost but also solves less challenges. Among the categories, crypto has higher cost across LLMs as it may require analysis of long encrypted texts, and many iterations for decryption."}, {"title": "6.3 Impact of different configurations", "content": "We run D-CIPHER without the Auto-prompter and without the Planner to observe their impact on the system. Without the Auto-prompter, the hard-coded prompt template is used for the Planner's initial prompt. Without the Planner, a single Executor is run with the prompt generated by the Auto-prompter.\nD-CIPHER without Auto-prompter with Claude 3.5 Sonnet gets a 3% improvement in challenges solved on NYU CTF Bench, but it's performance drops with GPT 40 on NYU CTF Bench and Claude 3.5 Sonnet on Cybench, showing that the Auto-prompter improves performance in most cases. The contrasting result with Claude 3.5 Sonnet on NYU CTF Bench is due to the pwn category, where performance increases by more than 2x, while other categories get matching or lower results. Without the Auto-prompter, average cost increases across LLMs and benchmarks, indicating that the Auto-prompter improves system efficiency without compromising performance in most cases.\nD-CIPHER without Planner sees a 1% to 5% drop in performance on NYU CTF Bench across both LLMs. The performance is consistently lower across all categories. This highlights the benefit of the Planner-Executor system in solving CTF challenges. While the average cost is 2x lower without the Planner, the drop in performance is significant. Whereas, the total cost of a Planner and multiple Executors is only 2x higher than a single Executor, indicating that each individual agent is more efficient."}, {"title": "6.3.2 Combination of stronger and weaker LLMs", "content": "We test the combination of stronger LLMs as Planner with weaker LLMs as Executor. We paired LLMs from the same family, but different capability tiers as indicated by their respective providers. Table 4 shows the performance of the strong-weak combinations on NYU CTF Bench. The results show consistent underperformance when weaker models are substituted for the Executor. For instance, Claude 3.5 Sonnet with Haiku solved only 13.0% of challenges, with a 6.0% drop compared to Claude 3.5 Sonnet with Sonnet. Similarly, GPT-40 and GPT-4 Turbo, when paired with GPT-40-mini, showed reductions of 4% and 1%, respectively. LLaMA 3.1 405B combined with LLAMA 3.3 70B failed to solve any challenges. Notably, Gemini maintained similar performance with the weaker. These results indicate that the D-CIPHER architecture achieves best performance on CTF challenges when models of comparable capabilities are paired."}, {"title": "6.3.3 Impact of temperature", "content": "D-CIPHER with GPT 40 is evaluated under a lower temperature setting of T = 0.95. Decreasing the temperature show consistent drop across crypto, pwn, and rev with no improvements in forensics, web, or misc. Higher temperature is better for creative and generative capabilities, and those capabilities help with problem-solving."}, {"title": "6.4 Analysis", "content": "We analyze the challenge termination (exit) reasons of D-CIPHER on NYU CTF Bench. Exit reasons are of five types: \"Solved\" when the challenge is solved, \u201cGiveup\" when the Planner gives up, \"Max cost\" when the cost budget is exceeded, \"Max rounds\" when the Planner conversation rounds are exhausted, and \"Error\" when the run terminates with an error.\nFor Claude 3.5 Sonnet, max cost is the most dominant exit reason. Comparatively, other LLMs have giveup as the most dominant reason. This indicates that Claude 3.5 Sonnet has less propensity to giveup and continue with the challenge till the cost is exhausted. Max rounds are exhausted for very few challenges, except for LLaMa 3.1 405B which faces problems in function calling and needs many retries.\nWe also see higher errors with LLaMa 3.1 405B that are attributed to it hallucinating function calls, indicating that this model is not capable of operating as an agent and in multi-agent settings. Distribution of exit reasons for GPT 40 and GPT 4 Turbo is similar across categories which shows the holistic capabilities of these models. Claude 3.5 Sonnet sees a high giveup percentage on web challenges, highlighting a gap in capabilities. It's lower performance on web re-iterates that observation."}, {"title": "6.4.2 Total conversation rounds", "content": "We analyze the total conversation rounds, defined as the sum of conversation rounds of each agent in D-CIPHER. shows a histogram of the total conversation rounds by success and failure cases for Claude 3.5 Sonnet and GPT 40 models, with and without Auto-prompter. Successful challenges take lesser rounds than failed challenges. This may indicate that D-CIPHER only solves easier challenges that require lesser rounds, but fails on longer harder challenges. This may also indicate that challenges are only solved when the correct path is found early enough, else the agents stray from the goal for many rounds before giving up. Claude 3.5 Sonnet runs for more rounds compared to GPT 40 for both success and failure cases, re-iterating it's propensity to keep going and not give up. This likely helps it solve challenges that take many rounds. Comparing Claude 3.5 Sonnet with and without Auto-prompter, we see that the Auto-prompter helps solve the challenges faster, increasing efficiency."}, {"title": "6.4.3 Errors and mistakes in LLM outputs", "content": "We inspected the conversation logs of D-CIPHER across all LLMs and document here the interesting errors and mistakes we observed.\nAuto-prompter fails to generate prompt: Often", "functions": "Gemini 1.5 Flash calls non-existing functions like \"decode\u201d and \u201cstrip\u201d", "tools": "We noticed that Gemini 1.5 Flash tries to run an interactive reverse engineering tool \"radare2\". The model first runs \"radare2\" without a script or inputs, and then tries to run internal \u201cradare2\" commands like \"pD\" or \"px\" but via the RunCommand function call. This is the order in which a typical interactive user would write these commands, but the LLM agent does not have an interactive interface. Similar errors have been observed with GPT 4 Turbo also. Advanced interactive toolsets and demonstrations to bring awareness of the agent's interface may"}]}