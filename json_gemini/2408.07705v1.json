{"title": "ENHANCING SUPPLY CHAIN VISIBILITY WITH KNOWLEDGE GRAPHS AND LARGE LANGUAGE MODELS", "authors": ["Sara AlMahri", "Liming Xu", "Alexandra Brintrup"], "abstract": "In today's globalized economy, comprehensive supply chain visibility is crucial for effective risk management. Achieving visibility remains a significant challenge due to limited information sharing among supply chain partners. This paper presents a novel framework leveraging Knowledge Graphs (KGs) and Large Language Models (LLMs) to enhance supply chain visibility without relying on direct stakeholder information sharing. Our zero-shot, LLM-driven approach automates the extraction of supply chain information from diverse public sources and constructs KGs to capture complex interdependencies between supply chain entities. We employ zero-shot prompting for Named Entity Recognition (NER) and Relation Extraction (RE) tasks, eliminating the need for extensive domain-specific training. We validate the framework with a case study on electric vehicle supply chains, focusing on tracking critical minerals for battery manufacturing. Results show significant improvements in supply chain mapping, extending visibility beyond tier-2 suppliers. The framework reveals critical dependencies and alternative sourcing options, enhancing risk management and strategic planning. With high accuracy in NER and RE tasks, it provides an effective tool for understanding complex, multi-tiered supply networks. This research offers a scalable, flexible method for constructing domain-specific supply chain KGs, addressing longstanding challenges in visibility and paving the way for advancements in digital supply chain surveillance.", "sections": [{"title": "1 Introduction", "content": "In an era where globalized and interconnected supply chains form the backbone of industries, achieving comprehensive end-to-end supply chain visibility has become a rising concern for organizations Kalaiarasan et al. (2022). Supply chain visibility is \"the extent to which actors within a supply chain have access to or share information which they consider as key or useful to their operations and which they consider will be of mutual benefit\" Barratt and Oke (2007). Visibility is viewed as not only an operational advantage, but a critical strategic asset for risk management Emrouznejad et al. (2023), insightful decision-making Caridi et al. (2014) and operational efficiency Brusset (2016).\nDespite its recognized importance, achieving full supply chain visibility remains a significant challenge for many organizations Zhang et al. (2011); Caridi et al. (2014). Limited visibility can undermine a company\u00e2\u0102\u0179s ability to proactively respond to supply chain disruptions, such as those caused by global events or geopolitical tensions. For instance, the COVID-19 pandemic starkly exposed how disruptions in distant tiers of the supply chain could ripple through to cause substantial operational and financial impact Ivanov and Dolgui (2020). Restricted visibility hinders a company's ability to ensure customer transparency regarding ethical sourcing and sustainability practices Apeji and Sunmola (2022). Consumers today demand more than just quality products, they require assurance that these products are sourced and manufactured ethically and sustainably. The inability to provide this transparency can damage a company's reputation and competitive standing Mollenkopf et al. (2022)\nThese challenges stem primarily from a fundamental issue: limited information sharing among supply chain partners Kumar and Pugazhendhi (2012); Prajogo and Olhager (2012). This reluctance usually arises from a lack of trust among companies, as they fear that sharing information might expose their vulnerabilities or give competitors a strategic advantage Lotfi et al. (2013); Prajogo and Olhager (2012).\nRevisiting the definition of supply chain visibility proposed by Barratt and Oke (2007), we understand that visibility depends fundamentally on two aspects: information sharing and information access. While challenges in information sharing are well-documented, gaining access to information through other means can also enhance visibility. This observation raises two pivotal questions: 1) Can we access information without relying on stakeholders willingness to share it? If so, 2) How can we effectively leverage this information to create valuable knowledge, thereby enhancing supply chain visibility and supporting better decision-making?\nGiven the demonstrated difficulties in obtaining information directly from supply chain partners, researchers have already explored alternative avenues for information access. The internet has emerged as a rich and diverse source of valuable supply chain data (Wichmann et al. 2018). Specifically, news articles, company websites, social media, and other public sources provide rich insights into supplier-buyer relationships, production capacities, and market trends. However, web-based information presents its own set of challenges, being siloed, fragmented, and unstructured, which impedes the observation of dependencies between such decentralized data points (Wichmann et al. 2020).\nTo address these challenges, researchers have explored the potential of machine learning, particularly Natural Language Processing (NLP) and Deep Learning techniques in of extracting structured information from unstructured text. Studies such as those by Kreimeyer et al. (2017); Li et al. (2022); Shickel et al. (2018); Sheikhalishahi et al. (2019); Wang et al. (2018) have demonstrated significant advancements in this area, especially within the healthcare sector. These studies highlight how NLP can effectively convert unstructured text into structured data, thereby enhancing the ability to extract, understand, and utilize information from unstructured clinical text.\nHowever, only a few researchers in the supply chain field have begun adapting these methods to extract structured information from web sources. By leveraging NLP and deep learning techniques, one can interpret unstructured text and extract critical supply chain information Wichmann et al. (2018, 2020); Kosasih et al. (2022). Specifically, Named Entity Recognition (NER) and Relation Extraction (RE) methods have been employed to automatically identify key elements such as company names, products, and locations, as well as to detect supplier-buyer relationships within textual data. These techniques enable the conversion of unstructured text into structured triplets (source, relation, target), facilitating the extraction of supply chain relationships.\nDespite these advancements, several limitations persist. Handling domain-specific terminology remains a significant challenge, as does the ability to extend these methods to new supply chain contexts. This is mainly due to the complexity of supply chain relationships and the nuances of specific business language, as these approaches often require extensive labeled datasets for training, specially in specialized supply chain domains Kejriwal (2019); Aziz et al. (2021); Brockmann et al. (2022), for example, tracking ethically sourced cotton or critical mineral mining.\nTo address these challenges, we introduce a zero-shot, Large Language Model (LLM)-driven framework that utilizes Knowledge Graphs (KGs) to enhance supply chain visibility. KGs are structured knowledge representations that organize information into interconnected nodes and edges, representing entities and their relationships, respectively Ehrlinger and W\u00f6\u00df (Ehrlinger and W\u00f6\u00df). This structure is particularly effective in complex environments like supply chains, as it clearly captures complex interdependencies and maps various types of relationships between different classes of entities Peng et al. (2023).\nOur framework leverages the extensive pre-training of LLMs, enabling them to perform NER and RE tasks specifically tailored to supply chain contexts. This is achieved through Zero-Shot Learning, a technique where the model applies learned patterns and knowledge without needing task-specific training or examples Kojima et al. (2022).\nEarlier LLM models such as BERT (Bidirectional Encoder Representations from Transformers) Devlin et al. (2019), typically require additional domain-specific data labeling and fine-tuning to perform tasks like NER and RE. This adaptation involves creating and labeling large datasets relevant to the specific domain, followed by retraining the model to understand and process the specialized terminology and relationships within that domain.\nIn contrast, our framework KG-LLM driven framework leverages the generalizability and zero-shot capabilities of more advanced LLMs such as OpenAI's GPT-4 OpenAI et al. (2023). These newer LLMs are pre-trained on vast and diverse datasets, enabling them to generalize across a wide range of tasks and domains without requiring additional task-specific training. Zero-shot prompting allows these models to apply their extensive pre-trained knowledge directly to new tasks, interpreting and extracting information accurately without the need for prior exposure to domain-specific examples Kojima et al. (2022).\nWe further prompt the LLMs to perform entity disambiguation, ensuring the uniqueness of nodes and preventing duplication, thereby preserving the integrity and consistency of the knowledge graph.\nTo demonstrate the practical application of our framework, we present a case study on improving visibility in electric vehicle supply chains. This case study illustrates how our approach can effectively track the origins of critical minerals and raw materials used in electric vehicle battery manufacturing, providing deeper insights into complex, multi-tiered supply chains and achieving levels of visibility previously difficult to attain.\nOur key contributions of this work include:\n1.  Development of a scalable methodology for constructing domain-specific supply chain knowledge graphs from diverse, publicly available data sources.\n2.  Leveraging zero-shot LLMs to extract and contextualize complex supply chain relationships, extending visibility beyond tier-1 and tier-2 suppliers.\n3.  Demonstration of the framework's effectiveness through a case study on electric vehicle manufacturers, revealing critical dependencies and alternative sourcing options for essential materials."}, {"title": "2 Literature Review", "content": null}, {"title": "2.1 Knowledge Graphs in Supply Chain Management", "content": "Knowledge graphs have emerged as a powerful tool for representing and analyzing complex relationships in various domains, including supply chain management (SCM). A knowledge graph is a structured representation of knowledge in the form of entities and relationships, providing a flexible and scalable way to integrate diverse information sources Ji et al. (2020). Unlike supply chain networks or production networks, knowledge graphs offer greater flexibility in representing multi-dimensional relationships and can easily incorporate new information sources. The application of knowledge graphs in SCM has shown promising results. Kosasih et al. (2022) and Deng et al. (2023) demonstrated the use of knowledge graphs for supply chain risk analysis, showcasing their ability to capture complex interdependencies. Huang et al. (2019) proposed a knowledge graph-based approach for supply chain partner recommendation, leveraging the rich relational information encoded in the graph structure. Furthermore, Rolf et al. (2022) proposed using knowledge graphs and human-centric AI for reconfigurable supply chains, highlighting their potential for enhancing supply chain adaptability.\nThese studies underscore the potential of knowledge graphs to address the challenges of data fragmentation and complexity in supply chains. By providing a unified representation of supply chain entities and their relationships, knowledge graphs can offer enhanced visibility and support more sophisticated analysis. However, the construction of comprehensive knowledge graphs is often time-intensive and challenging, particularly when dealing with large-scale supply chains."}, {"title": "2.2 Natural Language Processing for Supply Chain Mapping", "content": "To address the challenges of manual knowledge graph construction, researchers have explored the use of NLP techniques for automated supply chain mapping. Supply chain mapping involves creating a visual representation of the relationships and flows between different entities in a supply chain MacCarthy et al. (2022). Two key NLP techniques, NER and RE, have shown promise in automating this process by extracting relevant information from unstructured text sources.\nNER is a subfield of NLP that focuses on identifying and classifying specific data points, known as named entities, within a text Wang et al. (2023a). These entities can include names of people, organizations, locations, dates, and other significant terms. NER works by using algorithms trained on labeled datasets to detect and categorize these entities, transforming unstructured text into structured data Sun et al. (2018).\nRE, on the other hand, is an NLP technique that identifies and categorizes the connections between entities mentioned in a text Wadhwa et al. (2023a). This process helps in understanding how different entities are related to each other, such as \"works at\", \"located in\" or \"founded by.\"\nBERT, introduced by Devlin et al. (2019), revolutionized the field of NLP with its bidirectional training approach. BERT is a type of LLM designed to understand the context of words in a sentence by looking at the words that come before and after. This model is pre-trained on a large corpus of text and can be fine-tuned for specific tasks, making it highly effective when adapted to the target domain. Building on this work, Liu et al. (2019) proposed ROBERTa, an optimized version of BERT with improved performance across various NLP tasks.\nResearchers have explored the application of different NLP methods in supply chain mapping. For example, Wichmann et al. (2018) proposed using NER and RE methods to generate supply chain maps from online unstructured text sources. Their approach demonstrated the feasibility of automatically extracting supply chain relationships from news articles. Similarly, Yamamoto et al. (2017) developed a method for extracting company relationships from web news articles to analyze industry structure. In another study, Wichmann et al. (2020) explored using deep learning to automatically extract buyer\u00e2\u0102\u015esupplier relations from natural language text.\nWhile these studies demonstrate the potential of machine learning in supply chain mapping, they also highlight certain limitations. These methods often require extensive pre-training and labeled datasets, which can be challenging to obtain in specialized domains like SCM. Moreover, the complexity of supply chain relationships and the nuances of business language can pose challenges for automated extraction methods."}, {"title": "2.3 Large Language Models & Knowledge Graphs", "content": "While BERT is considered an LLM, it is primarily used for understanding and processing text, specifically tailored for tasks like NER and RE through fine-tuning Devlin et al. (2019). The key advantage of newer LLMs lies in their generalizability: they can understand and generate human-like text, perform complex tasks across various domains, and adapt quickly to new tasks with little to no additional training Kojima et al. (2022). The emergence of more advanced LLMs has marked a significant advancement in NLP. LLMs, such as GPT-3 introduced by Brown et al. (2020), have demonstrated remarkable capabilities across various NLP tasks, including Zero-Shot and Few-Shot learning. Zero-shot learning refers to the ability of a model to perform a task it has never been explicitly trained on by leveraging its pre-existing knowledge and patterns learned during its extensive pre-training phase Kojima et al. (2022). This means the model can understand and respond to new types of tasks or questions without having seen any specific examples of them before.\nFew-shot learning, on the other hand, allows the model to adapt to new tasks with only a few examples or instances of the task provided. In this scenario, the model uses a limited number of task-specific examples to understand the new task better, enhancing its performance without requiring a large, labeled dataset for training Brown et al. (2020).\nThese capabilities enable them to efficiently handle tasks in specialized domains like supply chain management. Their ability to perform zero-shot and few-shot learning means they can quickly adapt to domain-specific terminology and relationships without the need for extensive task-specific data labeling and fine-tuning\nRecent research has explored the potential of LLMs in automating knowledge graph construction. Faria et al. (2023) demonstrated the use of GPT-3 for zero-shot knowledge graph completion, achieving competitive performance with minimal task-specific training. Zhu et al. (2023) showed that LLMs can effectively extract triples from unstructured text for knowledge graph construction. Furthermore, several studies highlighted the superior performance of LLMs as they can achieve near state-of-the-art performance with only Few-Shot Prompting, roughly equivalent to existing fully supervised models on both NER and RE tasks Wadhwa et al. (2023b). Additionally, studies report LLM's superior ability in situations where the amount of training data is extremely scarce, as they can perform significantly better than supervised models Wang et al. (2023b).\nWhile LLMs have been applied to various domains, their potential in supply chain management, particularly for enhancing visibility, remains largely unexplored. Li et al. (2023) conducted one of the first studies exploring the use of LLMs for supply chain optimization, demonstrating their potential for bridging the gap between supply chain automation and human comprehension."}, {"title": "2.4 Research Gaps and Opportunities", "content": "Despite notable progress in the use of NLP and knowledge graphs in enhancing supply chain visibility, significant challenges remain:\n\u2022 There is a noticeable lack of flexibility in integrating diverse and heterogeneous data sets, which is crucial for comprehensive supply chain visibility.\n\u2022 Earlier supply chain mapping approaches require extensive labeled data and further fine-tuning to effectively manage supply chain-specific terminology.\n\u2022 The potential for enhancing supply chain visibility by integrating LLMs with knowledge graphs remains largely untapped and requires further exploration."}, {"title": "3 Methodology", "content": "In this section, we address these identified gaps and present our contributions through the following sections. In Section 3: Methodology, we discuss our proposed KG-LLM-driven framework in detail, outlining the steps involved in constructing it to enhance supply chain visibility. Section 4: Experimental Studies presents our evaluation method for the developed framework, validates its performance, and provides results to quantitatively assess the framework's accuracy. Additionally, we include a case study to qualitatively demonstrate the potential of this framework to enhance supply chain visibility. Finally, in Section 5: Conclusion, we summarize our observations, discuss the limitations of our study, and suggest areas for future work.\nIn this section, we begin with an overview of the KG-LLM framework designed to improve supply chain visibility. We then delve into the detailed stages of the framework, covering dataset collection and pre-processing, as well as the construction of the knowledge graph. This includes three key tasks: NER, RE, and entity disambiguation."}, {"title": "3.1 LLM-driven Knowledge Graph Framework for Enhanced Supply Chain Visibility", "content": "As shown in Figure 1, we introduce a KG-LLM driven framework. Our approach addresses the critical challenge of extracting, structuring, and centralizing the vast amount of supply chain information dispersed across the web. While this information is crucial for understanding extended supplier networks, its fragmented state severely limits its utility. To overcome these limitations, we develop a framework that combines the structural advantages of knowledge graphs with the advanced NLP capabilities of LLMs. We employ a zero-shot learning approach, leveraging the contextual understanding and language processing capabilities of state-of-the-art GPT-4 LLM by OpenAI OpenAI et al. (2023). This integration allows for the automated construction of comprehensive, interconnected representations of supply chain ecosystems. As shown in Figure 1, our framework begins with systematically identifying and aggregating data from various unstructured sources such as online web pages, articles, and Wikipedia entries, forming the foundation for extracting the extended supply network. We then develop two main prompts for different stages in the framework: the first facilitates the extraction of entities and relationships, essential for NER and RE, while the second ensures precise distinction of entities during the entity disambiguation stage. Using the LLM, we extract significant data points like company names, locations, and product details in the NER process. The RE process determines the relationships between identified entities, articulating interactions and dependencies critical for constructing the links between nodes in the knowledge graph. Finally, entity disambiguation resolves ambiguities among entities to ensure each node in the knowledge graph is unique, maintaining the graph's integrity and consistency. In the following sections, we discuss each step in more details."}, {"title": "3.2 Data Collection & Pre-Processing", "content": "The initial stage involves systematically identifying and aggregating data from various unstructured web sources. It is crucial to emphasize that the selection of data sources is critical to the integrity and credibility of the extracted insights. We note that when implementing this framework, the sources should be carefully evaluated for relevance, accuracy, and reliability to ensure the quality of the resulting knowledge graph. It is also important to note that this framework provides static information and knowledge. While exploring temporal knowledge graphs is a promising avenue for future research, it is beyond the scope of this study. Our current focus is on establishing a baseline framework for integrating LLMs and knowledge graphs to enhance supply chain visibility."}, {"title": "3.3 Knowledge Graph Construction Process", "content": null}, {"title": "3.3.1 Named Entity Recognition", "content": "Our framework leverages these capabilities through a Zero-Shot Learning (ZSL) approach. ZSL is a machine learning paradigm that enables AI models to recognize and categorize entities or concepts not explicitly encountered during training Kojima et al. (2022). This approach is particularly valuable in our context, as it allows for flexibility in entity extraction without the need for task-specific fine-tuning. To effectively guide the LLM in performing NER tasks using ZSL, we developed a carefully crafted prompt (Prompt 1) that provides contextual descriptions of the target entities. This prompt design is crucial, as it enables the model to recognize and extract entities similar to those described, even if they were not part of its training data.\nFigure 2 illustrates the detailed instructions used to guide the LLM in entity extraction. We divided the instruction into eight distinct tasks, labeled T1 through T8.\n\u2022 Entity Definition (T1-T2): We begin by instructing the model on the specific categories of entities to be recognized. As shown in Figure 2, we provide clear definitions and at least three examples for each entity type. Note that since we provide only descriptions and not specific examples of input text with corresponding correct outputs, our framework employs Zero-Shot prompting rather than Few-Shot.\n\u2022 Output Formatting (T3-T4): To ensure compatibility with subsequent processes in our framework, we provide explicit instructions on data output formatting. This includes specifying a key-value pair structure for entity properties, which enhances readability and consistency. We also include guidelines to avoid formatting errors.\n\u2022 Data Handling Instructions (T5-T8): The LLM is given specific instructions on how to handle various types of data encountered in unstructured texts. While our primary focus is on a limited set of predefined entities, we also direct the model to attach any additional relevant information as properties to these entities. Furthermore, we provide guidelines for node labeling to ensure that the outputs are easily interpretable and well-organized.\nThis methodological approach allows us to leverage the advanced capabilities of GPT-4 in a zero-shot setting, enabling efficient and accurate NER across diverse texts without the need for task-specific training data."}, {"title": "3.4 Relation Extraction", "content": "Similarly, for RE, we provide the LLM with specific instructions as shown in Figure 3.\nWe use the same prompt (Prompt 1) to continue listing the tasks for RE (T9-T16) as follows:\n\u2022 Task Definition (T9): Defines relationships as links between nodes. This basic definition helps the LLM understand the foundational concept of relationships in the context of our data.\n\u2022 Targeted Relationships (T10): Specifies three types of relationships that the LLM must capture. For example, here, we list 'Produces', 'LocatedIn' and 'SuppliesTo'. Each relationship is defined with description to ensure clarity. By providing semantic equivalents, we increase the model's ability to capture relevant variations of the same relationship.\n\u2022 Label Formatting (T11-T16): Ensures consistency and readability in our data. This standardization is crucial for integrating the extracted data into our knowledge graph seamlessly."}, {"title": "3.4.1 Entity Disambiguation", "content": "Entity disambiguation is a critical process in ensuring the accuracy and integrity of the supply chain data within our knowledge graph. Given the diversity and inconsistency in naming conventions across various data sources, it is essential to correctly identify and merge entities that refer to the same underlying node.\nTo effectively address this challenge, we have developed a detailed and structured prompt for our LLM, aimed at disambiguating entities identified in the initial extraction phase. This approach leverages the LLM's expertise in semantics and entity recognition to accurately unify entity representations (See Figure 4).\nThe entity disambiguation process begins by instructing the LLM to assume the role of an expert in semantics and entity identification. As illustrated in Figure 5, we systematically categorize all nodes by type and present each node type list separately to the LLM for disambiguation. The LLM is then prompted to assign unique numerical identifiers to entities that represent distinct nodes, while allocating identical numbers to entities that semantically represent the same node. This approach leverages the LLM's contextual understanding to resolve naming inconsistencies and merge semantically equivalent entities.\nFigure 6 shows some examples of the outcomes of the disambiguation process. We show how various entity types, such as locations and products, are distinctly identified and scored based on their semantic uniqueness. This method enables our framework to maintain a consistent and accurate representation of entities across diverse data sources, crucial for constructing a coherent and reliable supply chain knowledge graph."}, {"title": "4 Experimental Studies", "content": "In this section, we first describe the evaluation methods used to assess our proposed framework. We utilize accuracy as the primary evaluation metric and further analyze the framework\u00e2\u0102\u0179s consistency in generating reliable results. Following this, we present the quantitative results obtained from our framework. Additionally, we conduct a quali-tative evaluation using a case study focused on the electric vehicle supply chain to demonstrate the framework\u00e2\u0102\u0179s effectiveness in enhancing supply chain visibility in domain-specific contexts. Specifically, we examine whether the framework can track critical minerals and mining companies involved in the production of electric vehicle batteries, thereby improving the visibility of Original Equipment Manufacturers (OEMs) into their extended supplier network."}, {"title": "4.1 Evaluation Methods", "content": "In this section, we outline the evaluation methods used to assess our proposed framework's performance. We first describe the dataset developed for the evaluation and then detail the metrics employed to measure the framework's accuracy and consistency."}, {"title": "4.1.1 Evaluation Dataset", "content": "Given the lack of benchmark datasets specifically designed for supply chain data, we developed a tailored evaluation methodology to assess the performance of our framework.\nFor our evaluation, we created a dataset of different Wikipedia pages relevant to electric vehicle manufacturers, comprising of a total of 1,277 sentences. We chose Wikipedia because it is a comprehensive, diverse repository of free-access information, which supports a broad view across industries, technologies, and supply chain relationships. This selection was specifically tailored to align with our subsequent proof of concept case study, which focuses on enhancing visibility in the electric vehicle supply chain, particularly in tracing the sourcing of raw materials for batteries from mines through tier-2 and tier-3 suppliers. Guided by the objectives of our case study, we strategically selected company profiles that span mining companies, battery manufacturers, and electric vehicle producers. This selection was made to maximize the likelihood of capturing comprehensive data on our predefined set of six entity types: Company, Location, Material, Person, Product, and Mine, as well as four key relationship types: locatedIn, suppliesTo, owns, and produces.\nIt is important to clarify that in this evaluation, our main objective is to assess the LLM's ability to accurately extract entities and relationships, represented as triples, from textual content. The focus is on the technical extraction abilities of the LLM, not on the empirical accuracy of the information represented in the triples themselves. Researchers should carefully vet and select data to suit specific applications and contexts.\nFrom the analyzed dataset, our framework successfully identified 735 nodes and 433 relationships, with their distribution detailed in Figure 7."}, {"title": "4.1.2 Evaluation Metrics", "content": "Our evaluation methodology is tailored to address the specific challenges and requirements of supply chain information extraction. We focus on assessing the accuracy of the extracted nodes and relationships, as these form the core of our knowledge graph and directly impact supply chain visibility and decision-making processes. We acknowledge that precision, recall and f-scores, which consider the missed triplets, are also important metrics for comprehensive evaluation. However, the manual identification of missed triplets is resource-intensive and currently not feasible due to the lack of benchmark datasets. Our current evaluation method serves as a preliminary step, demonstrating the framework's ability to extract valuable and accurate information. By focusing on accuracy and ensuring the correctness of the generated triplets, we provide a solid foundation for the practical applicability of our framework. This approach allows us to iteratively improve the system and gradually incorporate more comprehensive evaluation metrics as the field evolves.\nWe measure accuracy across three key tasks as follows:\nNER:\n\u2022 Correct: A node is correctly defined, and its type is accurate (e.g., identifying Tesla as a (Company)).\n\u2022 Incorrect: A node is incorrectly classified (e.g., identifying Tesla as a (Location) instead of a (Company)).\nAccuracy is calculated using:\n$Accuracy_{NER} = \\frac{Number \\space of \\space Correct \\space Nodes \\space Identified}{Total \\space Nodes \\space Identified \\space (Correct + Incorrect)}$\nRE:\n\u2022 Correct: A relationship is accurately defined between two suitable nodes (e.g., Tesla (Company) produces Model 3 (Product)).\n\u2022 Incorrect: A relationship is incorrectly defined between two nodes, or the direction of the relationship is incorrect (e.g., identifying Tesla as supplying to CATL instead of CATL supplying to Tesla).\nAccuracy is defined as:\n$Accuracy_{Re} = \\frac{Number \\space of \\space Correct \\space Relationships \\space Identified}{Total \\space Relationships \\space Identified \\space (Correct + Incorrect)}$\nEntity Disambiguation:\n\u2022 Correct: One or more entities are correctly merged into a unique node, indicating they are similar, or an entity is correctly identified as unique with no similar nodes.\n\u2022 Incorrect: An entity is incorrectly merged with another node that does not represent the same unique entity, or it is not merged when it should have been.\nAccuracy is determined by:\n$Accuracy_{ED} = \\frac{Number \\space of \\space Correctly \\space Disambiguated \\space Nodes}{Total \\space Nodes \\space Disambiguated \\space (Correct + Incorrect)}$\nThe consistency of our LLM-based framework is a critical factor in establishing its reliability for supply chain visibility applications. To assess this consistency, we conducted a comprehensive evaluation across two key dimensions: NER and RE. For each document in our dataset, we performed seven independent runs of the framework, allowing us to quantify variability and assess the robustness of our method across multiple iterations.\nFurthermore, we classified nodes by type (e.g., company, location, material, mine, product) and computed the standard deviation of counts for each unique node type. This granular analysis allows us to pinpoint any inconsistencies in specific entity type recognition. Similarly, for RE, we counted the total relationships identified in each run, calculated the standard deviation of these counts, and further categorized relationships by type (e.g., produces, suppliesTo, locatedIn). The standard deviation for each unique relationship type was also computed, providing insights into the consistency of specific relationship classifications."}, {"title": "4.2 Results", "content": "This section presents a comprehensive analysis of our proposed framework's performance, evaluated using accuracy metrics across three critical dimensions: NER, RE, and entity disambiguation."}, {"title": "4.2.1 Accuracy", "content": "Figure 8 shows the performance of our framework using Zero-Shot Learning across the three key tasks defined.\nNER achieved a high accuracy of 0.95, demonstrating the LLM's robust capability in identifying diverse entity types within unstructured text. This performance can be attributed to the LLM's extensive pre-training on diverse datasets, coupled with our carefully crafted prompts that provide clear supply chain entity definitions and examples.\nRE, while achieving the lowest accuracy among the three tasks at 0.82, it still demonstrates significant capability in a zero-shot setting. This performance is noteworthy given the complexity of identifying semantic relationships without task-specific training. The relatively lower accuracy in RE compared to NER suggests that relationship identification is a more challenging task, potentially due to the nuanced ways relationships can be expressed in natural language. Future work could explore enhancing RE performance through few-shot learning or by providing a set of diverse examples in the prompts.\nEntity Disambiguation recorded the highest accuracy at 0.98, showcasing the LLM's exceptional ability to recognize semantic similarities and differences among entities. The framework successfully reduced the initial 867 nodes to 732 unique entities, merging 135 duplicates. This high performance in disambiguation is crucial for maintaining data integrity and reducing redundancy in the knowledge graph, a key factor in enhancing supply chain visibility.\nWe further analyze the accuracy achieved by node and relationship types to understand the LLM's performance in capturing different entities and relationships within the supply chain. As shown in Figure 9(a), the LLM demonstrates high accuracy in capturing various node types. Particurarly in identifying nodes such as companies, products, and mines, achieving near-perfect accuracy. This performance can be attributed to the LLM's extensive pre-training on diverse datasets, which equips it with a robust understanding of common entity types and their contextual usage in text Kojima et al. (2022); Brown et al. (2020).\nFurthermore, for RE, we observe that the lowest accuracy is recorded for the 'suppliesTo' relationship, with an accuracy of 0.71 9(b). This lower performance can be attributed to the diverse ways in which supply chain relationships are expressed in natural language. The 'suppliesTo' relationship, in particular, can be represented using various terminologies and syntactic structures, making it challenging for the LLM to consistently identify and extract this relationship.\nDespite prompting the LLM to capture different semantic equivalents of the \u2018suppliesTo' relationship, the large inherent variability in language used in supply chain poses a challenge. To address this, future work could explore the use of few-shot learning techniques, where the LLM is provided with explicit examples to cover the majority of different ways this relationship can be represented in the supply chain context. This approach has been shown to improve the performance of LLMs in similar tasks by providing more targeted training data Brown et al. (2020); OpenAI et al. (2023). Additionally, defining and constraining the types of nodes that the 'suppliesTo' relationship should link to could further enhance the model's accuracy. By providing more specific guidelines on the expected relationships between entities, the LLM can better focus its extraction efforts and reduce the likelihood of errors.\nThe framework's performance across all three tasks, particularly in a zero-shot setting, demonstrates the potential of LLMs in extracting structured information from unstructured text when guided by effective prompting. This approach offers significant flexibility, allowing for easy adaptation to new entity types or relationships without the need for retraining."}, {"title": "4.2.2 Consistency", "content": "Figure 10 shows the counts of nodes and relationships extracted over 7 distinct runs. We observe low fluctuations between different trials for both node and RE tasks.\nTo further quantify the consistency of our framework, we calculate the Mean, Standard Deviation, Coefficient of Variation and Range for both tasks across the 7 different trials as shown in Table 1. As observed, the low coefficients of variation for both node count (0.03) and relationship count (0.04) indicate high consistency in the framework's performance. These values suggest that the variability in extraction is minimal relative to the mean, demonstrating the framework's reliability in producing consistent results. The standard deviations of 6.22 and 5.55 for node and relationship counts, respectively, compared to their respective means of 223.86 and 137.43, further support the framework's consistency. The relatively small ranges (17 for nodes and 14 for relationships) also indicate that the framework's performance remains stable across different runs.\nWe further break down the consistency metrics by node type. As shown in Table 2, Location nodes demonstrate the lowest CV of 0.438, among all types, indicating high consistency in identifying geographical entities. Material nodes exhibit the highest CV of 0.960, which may be attributed to the diverse ways materials can be mentioned in text, potentially leading to more variable extraction."}, {"title": "4.3 Case-Study", "content": "To demonstrate the practical benefits and effectiveness of our framework", "categories": "electric vehicle manufacturers", "included": "n\u2022 Electric Vehicle Manufacturers: Toyota", "Suppliers": "Samsung SDI", "Companies": "Zijin Mining, Albemarle Corporation, Tianqi Lithium, Ganfeng Lithium Co Ltd, Norilsk Nickel, Jinchuan Group\nAs this case study aims to provide"}]}