{"title": "RAINBOW PO: A UNIFIED FRAMEWORK FOR COMBIN-ING IMPROVEMENTS IN PREFERENCE OPTIMIZATION", "authors": ["Hanyang Zhao", "Genta Indra Winata", "Anirban Das", "Shi-Xiong Zhang", "David D. Yao", "Wenpin Tang", "Sambit Sahu"], "abstract": "Recently, numerous preference optimization algorithms have been introduced as extensions to the Direct Preference Optimization (DPO) family. While these methods have successfully aligned models with human preferences, there is a lack of understanding regarding the contributions of their additional components. Moreover, fair and consistent comparisons are scarce, making it difficult to discern which components genuinely enhance downstream performance. In this work, we propose RAINBOWPO, a unified framework that demystifies the effectiveness of existing DPO methods by categorizing their key components into seven broad directions. We integrate these components into a single cohesive objective, enhancing the performance of each individual element. Through extensive experiments, we demonstrate that RAINBOWPO outperforms existing DPO variants. Additionally, we provide insights to guide researchers in developing new DPO methods and assist practitioners in their implementations.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020; Ziegler et al., 2019) has significantly contributed to the success of recently released Large Language Models (LLMs) such as InstructGPT (Ouyang et al., 2022), ChatGPT, and GPT4 (Achiam et al., 2023). However, RLHF is a complex and resource intensive process and requires training a reward model. An alternative to RLHF is Direct Preference Optimization (DPO) (Rafailov et al., 2023) that directly optimizes policies from pairwise preferences by minimizing a supervised learning loss objective, which is viewed as the maximum likelihood estimate for the reward model in RLHF. This approach allows DPO and similar other DPO variants to bypass the use of RL, resulting in faster speed of end-to-end training and better resource efficiency, while achieving comparable or superior performance to RLHF in downstream tasks such as summarization (Rafailov et al., 2023).\nDPO and its success during training foundation models like LLama series (Dubey et al., 2024; Touvron et al., 2023), Mistral (Jiang et al., 2023a), has garnered significant research attention in the LLM alignment space (Winata et al., 2024; Wang et al., 2024b), leading to the development of various extensions. These include variants beyond pairwise ranking, such as KTO (Ethayarajh et al., 2023; Song et al., 2024), unified perspectives on loss parameterization, such as IPO (Azar et al., 2024) and GPO (Tang et al., 2024), distribution correction methods like RSO (Liu et al., 2023) and WPO (Zhou et al., 2024), and reference model-free alternatives, such as CPO (Xu et al., 2024), ORPO (Hong et al., 2024), and SimPO (Meng et al., 2024). Each of these DPO variants claims to outperform the original DPO in downstream task evaluations by introducing specific components, or mathematically modifying the loss objective. In the rest of the paper, we will refer to all DPO variants collectively as XPOs for simplicity.\nComparing these XPOS proposed in the literature is not always straightforward due to differences in the base model size and architecture, the alignment datasets, the experimental setup as well as the evaluation metrics. Subsequently, it becomes difficult to assess the effectiveness and choose among different XPO methods given a problem. A brute force comparison across all existing methods is prohibitively expensive and inefficient. Therefore, it is crucial that we study the performance"}, {"title": "2 PRELIMINARIES AND MOTIVATION", "content": "In this section, we first briefly introduce RLHF and DPO as the foundation method, and then discuss on extensions of DPO (XPOs) to understand what are the components proposed in the literature.\nRLHF starts with fine-tuning a pre-trained large language model by supervised learning on high-quality data for some downstream tasks of interest (e.g., dialogue, summarization, etc.), to acquire a model \\(\\pi_{SFT}\\). This step is referred to as the SFT phase. For instance, for training Instruct-GPT (Ouyang et al., 2022), GPT-3 (Brown et al., 2020) is first fine-tuned on the given input prompt distribution. The second stage of RLHF is known as reward modeling, i.e., researchers collect pref-erences \\(D = (x, y_w, y_l)\\) on the generations of fine-tuned model \\(\\pi_{SFT}\\), and learns a reward model \\(r^* (x, y)\\) that could represent the quality or the rating of generation y with respect to prompt x. The final step is policy optimization on \\(\\pi_{SFT} = \\pi_{ref}\\), by maximizing a regularized reward to obtain the optimal policy model \\(\\pi^*\\) through reinforcement learning:\n\\begin{equation}\n\\max_\\theta \\mathbb{E}_{x \\sim D} \\Big[\\mathbb{E}_{y \\sim \\pi_\\theta (y|x)} [r^*(x, y)] - \\beta KL(\\pi_\\theta(\\cdot | x)||\\pi_{ref}(\\cdot | x))\\Big]\n\\end{equation}\nFor ease of reference, we add a table of notations in Table 8 in Appendix C and more detailed description of RLHF in Appendix A.1."}, {"title": "2.1 DIRECT PREFERENCE OPTIMIZATION (DPO)", "content": "One disadvantage of RLHF is that the RL step often requires substantial computational effort (e.g., to carry out PPO). The idea of DPO is to combine the reward model and RL in RLHF into a single objective, bypassing the computation in the RL step. Given the same preference pairs \\(D = (x, y_w, y_l)\\) utilized for reward modeling in RLHF, the DPO objective yields:\n\\begin{equation}\n\\min L_{DPO} (\\pi_\\theta; \\pi_{ref}) := -\\mathbb{E}_{(x,y_w, y_l) \\sim D} \\log \\sigma \\Big( \\beta \\log \\frac{\\pi_\\theta (y_w | x)}{\\pi_{ref} (y_w|x)} - \\beta \\log \\frac{\\pi_\\theta (y_l | x)}{\\pi_{ref} (y_l|x)} \\Big)\n\\end{equation}\nwhere \\(\\sigma(\\cdot)\\) is the sigmoid function and \\(\\beta\\) is a regularization parameter for tuning. DPO thus yields a supervised learning problem, and requires much less computation than the RL based RLHF. The objective in Equation 2 can be understood as maximizing the likelihood difference between the preference pair, making the model more likely to generate the preferred answers than unpreferred."}, {"title": "2.2 MOTIVATION: REVISITING XPOS", "content": "Since DPO is proposed, there is huge interest in developing and improving DPO, leading to nu-merous XPOS. Different XPOS can be motivated by theoretical concerns like relaxing or extending preference distribution assumptions in IPO and Mallows DPO, human aware loss function in KTO, or from practical aspects like reference model-free alternatives, like CPO, ORPO and SimPO. We provide an non-exhaustive list in Table 7 in Appendix C for the ease of comparison.\nDespite different motivations, XPOS all have a main loss objective that they optimize. We thus take the loss objectives as the first class citizen, and mathematically understand the parts that are com-monly adopted or differ in XPOS. Before going into detailed categorization, we want to first argue that, in existing preference optimization literature, there lacks a work in revisiting and examining the DPO variants in their objectives mathematically and comprehensively. As a consequence, some papers may have implicitly proposed some designs for improvement and even didn't highlight it. As"}, {"title": "3 RAINBOWPO: A UNIFIED FRAMEWORK", "content": "We first explain in detail about the components we categorized, after which we propose a generic framework, RainbowPO, to combine these components."}, {"title": "3.1 COMPONENT DESCRIPTIONS", "content": "We first explain in detail about the components we categorized, after which we propose a generic framework, RainbowPO, to combine these components.\nLength Normalization. The literature has noticed a verbosity issue of DPO aligned models. To address this, one promising direction noticed in the literature is to incorporate explicit verbosity penalties, like in R-DPO (Park et al., 2024) and SimPO (Meng et al., 2024):\n\\begin{equation}\nr^{LN}(x, y) = r_\\theta(x, y) - \\alpha|y|, \\text{ and } r_{\\theta}^{LN}(x, y) = \\frac{r_{\\theta}(x, y)}{|y|},\n\\end{equation}"}, {"title": "4 EXPERIMENTS", "content": "To evaluate the performance of the XPOS algorithms, we conducted extensive experiments on train-ing models with various XPOs configurations and compared their instruction-following capabilities.\nExperimental Setup. We choose Llama3-8B-Instruct\u00b2 as our model base to fine tune, mainly because that aligning this widely adopted and flagship instruct model is of great interest to the whole community and meets the standard as a representative setup for alignment. It can also help mitigate the uncertainty from probably not perfectly supervised fine-tuned models.\nFor evaluation metric, we use widely adopted benchmark Alpaca Eval2, which is composed of 805 questions and evaluate the instruction following capability of the model. The win rate is by default annotated by GPT4 through LLM-as-a-judge, and the resulting win rate has a 68.5% consistency ac-cording to official AlpacaEval website. To cross validate the effectiveness of the model and mitigate possible bias of GPT4, we also adopt Llama3-70B instruct as the judge, which is reported to have a 67.5% win rate consistency to humans.\nFor formulating the preference dataset D, we follow the standard RLHF pipeline by directly generat-ing answers from the model (which is thus an on-policy dataset, but the algorithm is still offline) and get AI feedbacks as in SimPO (Meng et al., 2024): we generate 5 answers from Llama3-8B-Instruct for each prompt in UltraFeedback (Cui et al., 2023), rank them with scores evaluated by ArmoRM (Wang et al., 2024a), and choose the best/worst one as winning/losing answer to form the preference pairs. For training, we adopted the popular library Transformer Reinforcement Learning (TRL\u00b3), which already implemented most aforementioned XPOS algorithms and make everything under the same backend and easy to reproduce. If not specified, we train the model with 3 training epochs, which typically yields better performance for each XPOS according to our replication."}, {"title": "4.1 EFFECTIVENESS OF DIFFERENT COMPONENTS", "content": "Individual Components Results. We first study the effectiveness of adding individual compo-nents. We use + to denote that only the component(s) after is added to DPO baseline as in Table 2. From the results, we could notice that some components may not provide firm improvement over the baseline not matter being added individually or combined. For example, for home advantage term, we tune different values under the best performed \u03b2 for DPO, and also always witness a degradation in the performance, see Figure 1a. For link function, we examine the square loss in IPO and also cannot see performance gain over the DPO baseline. Other components (LN, Mixing reference pol-icy, CS) indeed help improve the metric even added individually. Compared to SimPO, using mixing reference policy yields also better results as in Figure 1b. The average win rate gain is reported in the last column."}, {"title": "4.3 LIMITATIONS AND FUTURE WORK", "content": "Broader tasks. In this paper, we focus our evaluation on models trained with LLama3-8B In-struct as the base model. Exploring other models of varying sizes, such as Gemma (Team et al., 2024) or Mistral (Jiang et al., 2023a), could possibly enhance the generalizability of our findings. It will also be beneficial if we could repeat the pipelines and compare the algorithms' performance on other LLM evaluation metrics, like arena-hard or MT-bench, though MT-bench is known to be less tinguishable for RLHF algorithms. Other directions include benchmarking the effectiveness of alignment algorithms on improving other capabilities of LLM other than instruction following, like reasoning Xiong et al. (2024b). However, due to constraints in computing resources and time, we defer this investigation to future work. Nevertheless, we believe that our work provides a unified and comprehensive framework for helping to find the best preference optimization algorithms, and further pushing the boundary of offline RLHF for LLMs.\nIdeas from other XPOS. We were not able to explore other aspects of existing DPO variants in detail, and there might be still promising candidates in further improving the preference of Rain-bowPO. Some methods that propose to update the reference policy dynamically: sDPO (Kim et al., 2024), TR-DPO (Gorbatovski et al., 2024). Additionally, we also recognize the recent literature in pursuing online methods, such as online DPO (Guo et al., 2024) or iterative DPO (Yuan et al., 2024; Xiong et al., 2024a), which provide valuable insights on possibly further improving the down-stream task performance: we will pursue them in future research. Other extensions beyond RLHF include, Nash Learning from human feedback (Munos et al., 2023), and self-play preference opti-mization (Chen et al., 2024b).\nDemystifying observations. We also made some interesting observations in the paper, which we fail to find proper mathematical explanations and may boost further research. For example, the RainbowPO objective could benefit much more than SimPO objective when increasing the training epochs, but the mathematical reasons for such phenomenons are still unknown. In addition, we found some mathematically orthogonal components are actually not empirically independent, for example, RSO can improve DPO, but can not be readily combined with other components like length normalization. It is also interesting to see the some combination of components reach effects \"1+1 > 2\"; it will be interesting to understand the deeper underlying reasons and could potentially lead to better algorithms."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose RAINBOWPO, a comprehensive framework that demystifies and enhances existing DPO methods through the integration of key components into a unified objective. Our findings highlight the effectiveness of length normalization, reference policy mixing, and contex-tual scaling, while also highlighting the promise of warm-up adjustments. However, the selective application of rejection sampling and home advantage is not providing incremental improvements when paired with the other methods. By demonstrating that these enhancements can coexist within a single algorithm to achieve state-of-the-art performance, we pave the way for future research and practical applications. We aim for this work to serve as a foundation for refining DPO methodologies and to inspire further exploration of untested components for integrated agents."}]}