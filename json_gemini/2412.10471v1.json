{"title": "VCA: Video Curious Agent for Long Video Understanding", "authors": ["Zeyuan Yang", "Xueyang Yu", "Delin Chen", "Maohao Shen", "Chuang Gan"], "abstract": "Long video understanding poses unique challenges due to their temporal complexity and low information density. Recent works address this task by sampling numerous frames or incorporating auxiliary tools using LLMs, both of which result in high computational costs. In this work, we introduce a curiosity-driven video agent with self-exploration capability, dubbed as \u201cVCA\". Built upon VLMS, VCA autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences. Instead of directly sampling frames, VCA employs a tree-search structure to explore video segments and collect frames. Rather than relying on external feedback or reward, VCA leverages VLM's self-generated intrinsic reward to guide its exploration, enabling it to capture the most crucial information for reasoning. Experimental results on multiple long video benchmarks demonstrate our approach's superior effectiveness and efficiency.", "sections": [{"title": "1. Introduction", "content": "There is a growing demand for developing long-form video understanding techniques [53, 75] capable of extracting valuable information from extended visual narratives, such as surveillance footage, documentaries, and instructional videos. Analyzing these videos [59, 77], which can range from minutes to hours, requires models that can process multimodal data and perform reasoning over extremely long sequences [5, 36, 66].\nPrevious works commonly utilize two-stream networks to capture spatial and temporal information [20, 21] or introduce 3D operations [9, 63]. Recent approach [37, 81] leverages the long-sequence reasoning capabilities of Large Language Models (LLMs) [1, 70] for long-form video question answering, often by converting videos into densely and uniformly sampled frames [38, 43, 61]. However, this approach is usually inefficient due to the inherent imbalance and varying information density across video segments [67, 85]. Some segments are rich in critical details necessary for answering questions, while others contain only trivial information [72]. Uniform sampling [54, 66], which might focus on redundant frames that increase the computational burden and may overlook frames with essential information, often fails to efficiently utilize the video's dynamic structure for question answering [18, 68].\nTo address this limitation, recent approaches [32, 39,"}, {"title": "2. Related Work", "content": "2.1. Long-context Multi-modal Agents\nLarge multi-modal models (LMMs) [2, 7, 10, 34, 35, 40, 60] have demonstrated promising performance in addressing complex multimodal tasks [22, 50, 54, 88]. However, these LMMs typically lack self-directed exploration capabilities required for handling more tricky tasks [27, 80]. To bridge this gap, multimodal agents [82, 93] are designed to enhance LMMs with the ability to autonomously make decisions and execute tasks within specific environments [16, 79], such as interpreting visual cues and text prompts to make real-time decisions within websites [31, 83].\nA key feature of multimodal agents is their long-context capability [91], i.e., the ability to process and utilize information across extended periods to make informed decisions [29]. An example of this is vision-language model (VLM) agents for video understanding tasks. Despite their advancements, even sophisticated VLM agents struggle to extract crucial information and perform effective reasoning over lengthy inputs [68, 86]. Therefore, efficiently extracting essential information and making informed decisions remains a primary challenge. This work aims to address this challenge by enabling agents to self-explore the environment and self-gather the most relevant information to efficiently solve video-understanding tasks."}, {"title": "2.2. Long Video Understanding", "content": "Early deep learning models for video understanding primarily followed two main approaches [58]. One uses two-stream networks [20, 21, 47, 64] to integrate spatial and motion information, while the second focuses on 3D CNNs [9, 63, 78] for spatial-temporal feature extraction. Vision Transformers [15] then introduce self-attention mechanisms to handle temporal modeling more effectively [3, 8, 17]. Methods with self-supervised pre-training [55, 62, 94] exhibit transferability across different tasks and achieve more robust video understanding performance.\nWith the development of LLM, multimodal LLM [25, 65, 90] shows strong capabilities in video understanding tasks, but efficiently processing information within a prolonged video environment is the key challenge. Recent approaches often use frame sampling [37, 51, 69, 81, 84] and feature extraction [30, 54, 57, 76] to reduce computational costs. Frame sampling selects a subset of frames, which risks missing essential context, especially in lengthy videos where transitions and subtle cues may easily be neglected. Additionally, processing entire video sequences as tokens presents significant memory and computational challenges [12, 48, 70, 87]."}, {"title": "3. Method", "content": "Inspired by human reasoning, we introduce a self-exploration agent that adaptively navigates video environments. Unlike approaches that depend on dense frame sampling or captioning [68], which require VLMs to process numerous frames, our agent analyzes videos iteratively within a limited context window in a process we call self-exploration. This approach allows essential information to be extracted from long videos in a coarse-to-fine manner.\nSpecifically, we propose a segment-based tree-search exploration structure. Given a video v and a user query q, the agent \\u03c0 begins by randomly sampling N frames {fi}^{N}_{i=1} to obtain an initial overview of the content in each video"}, {"title": "3.1. Reward Model", "content": "With the segment-based tree-search exploration, the agent is required to select the most informative video segment for exploration at each step. However, we observe that current VLM-based agents struggle to independently identify optimal exploration paths (see Sec. 4.3). To this end, inspired by the selective attention mechanism in human cognition [14], we incorporate a reward model R to guide the agent's selection process, i.e., the agent will gain a higher reward if it plans to take the action to explore more relevant segment.\nSpecifically, during the exploration of a current selected segment s*, the reward model observes a set of frames {f_{i}^{*}}sampled from s*. These sampled frames {f_{i}^{*}}are then used to form finer sub-segments {s_{[l,l+1]}}^{N+1}_{i=1}, where l and l+1 represent the indices of the beginning and end frame of the sub-segment, respectively. The reward model aims to generate a reward score for each sub-segment {s_{i}}^{N+1}_{i=1} based on its relevance to the user query q. We utilize chain-of-thoughts [73] technique to let the reward model provide a verbal explanation of each segment and then generate a relevant score, i.e., {t_{s_{i}};r_{s_{i}}}. Additionally, to maintain consistency across different exploration steps, we incorporate the reward score history Hr from previous steps alongside the new segments during evaluation. A simplified prompt template for the reward model is presented here, with detailed prompts available in Appendix 7."}, {"title": "3.2. Memory Management", "content": "As exploration proceeds over multiple rounds, the accumulation of frames can lead to significant computational costs for long videos, even when N is small. However, as illustrated in Fig. 2, as humans continuously focus on important video segments, they tend to gradually disregard irrelevant frames, especially from earlier steps, retaining only the most crucial information in their working memory for later analysis. Inspired by this, we introduce a fixed-size memory buffer M that stores only the most relevant frames throughout exploration.\nSpecifically, when the agent \\u03c0 explores a video segment s*, the associated frames {f_{i}^{*}} are added to the memory buffer. If the total number of frames exceeds the buffer limit, irrelevant frames are removed based on relevance scores r_{s_{i}} defined in Sec. 3.1. We discard the frames with the lowest scores and only retain the most relevant ones. While these less relevant frames are removed, their visual information is extracted and encoded as text descriptions {t_{s_{i}}}_{i=1}^{N} within the exploration history H_{r}, allowing the reward model to utilize. This memory management strategy enables the agent to conduct reasoning within a fixed resource limit, regardless of the exploration trajectory's length, thereby preventing excessive memory usage."}, {"title": "3.3. Tree-Search Exploration", "content": "As illustrated in Fig. 2, humans adaptively focus on different video segments during exploration rather than targeting specific frames directly. Inspired by this, our proposed agent VCA treats the video as an unexplored environment, where each segment represents a node on an exploration tree. Given the entire video as the root node, the agent's goal is to efficiently identify an optimal short path leading to a leaf node (segment) containing the most informative content needed to answer the user's query.\nAt each step, the agent \\u03c0 receives both textual and visual guidance: a set of candidate segments S, which includes a collection of segments with associated reward scores, and a memory buffer M containing multiple frames. If the agent has gathered sufficient information to answer the user's query q, it directly generates a response a. Otherwise, it continues exploring by selecting a specific segment s* \\u2208 S to expand the tree based on reward scores.\nHowever, due to the limited capacity of current foundational models, these reward scores are not always accurate (see further analysis in Sec. 5.3). Selecting segments based on reward scores in a purely greedy manner can potentially lead to a suboptimal solution. Instead, we use the reward scores as guidance for agent \\u03c0, allowing it to make the final segment selection independently. In this case, the agent is not forced to select the segment with the highest reward, balancing the exploitation and exploration. More importantly, the candidate set S includes both unexplored coarse segments from prior steps and fine-grained segments from the current step, enabling the agent to backtrack and select earlier segments if it detects that the current exploration path may be suboptimal. Below are the simplified prompts. The full prompt is provided in Appendix 7."}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nBenchmark. We mainly evaluate VCA on two long-term video question-answering benchmarks. EgoSchema [46] is an egocentric video dataset sourced from Ego4D [24], with a duration of 180 seconds. Here we use the validation set of 500 QA pairs. LVBench [67] covers about 1,500 samples for evaluation of reasoning abilities from high-quality videos over 30 minutes in length. We also conduct experiments on other relevant benchmark datasets in Appendix 9."}, {"title": "5. Analysis", "content": "5.1. What's the Exploration Behavior of VCA?\nHere we present a real exploration trajectory from LVBench to illustrate our agent's decision-making process. As shown in Fig. 4, given a user query, the reward model initially evaluates each segment's relevance based on uniformly sampled frames, and the agent selects the fourth segment, which has the highest relevance score. However, upon further investigation of segment 4, both the reward model and the agent find it relatively irrelevant to the query. Rather than continuing with it, the agent redirects its exploration to segment 1 in the last round. After selecting segment 1, the agent chooses to investigate the third segment, despite the second segment having the highest reward score. This behavior reflects VCA 's ability to adaptively adjust its exploration trajectory based on visual context rather than purely following the reward score, resulting in a more robust system.\nWe also examine the limitations of VCA by analyzing its failure cases, which fall into the following scenarios: The most common failure occurs when the agent overlooks subtle clues, resulting in an inaccurate response to the user query even after prolonged exploration, particularly for queries requiring specific visual details. Other failure cases include instances where the agent is misled by inaccurate reward scores or fails to provide the correct answer despite observing key frames. Detailed cases and further discussion are provided in the Appendix 10."}, {"title": "5.2. Can VCA generalize to Open-source Models?", "content": "While VCA demonstrates strong performance using GPT-4o as the base model, we also analyze whether these positive results can be extrapolated to open-source models. We begin by implementing the VCA framework with Qwen2-VL [65] as both the exploration agent and reward model, but observe that it struggles with complex instructions and multi-round multimodal reasoning. To address this, we separate the exploration agent's task across two models: a video assistant (Qwen2-VL-7B) that processes observed frames and encodes visual information in text modality, and an instruction model (Qwen2.5-7B) that selects segments to explore based on the assistant's text outputs. The results presented in Tab. 4 reveal that, compared to directly feeding uniformly sampled frames into Qwen2-VL, our framework achieves higher accuracy on the EgoSchema benchmark while using less than 30% of the frames. Furthermore, it significantly outperforms both VideoAgent and VideoTree with far fewer frames. These findings further underscore the robustness of our VCA framework."}, {"title": "5.3. How Reliable is the Reward Model?", "content": "To further assess the capability of our reward model, we visualize the matching accuracy between segments with the highest reward scores and ground truth time references from LVBench. The results are shown at the bottom of Fig. 5. We observe that segment distances follow an approximately normal distribution with a mean of 1.28, indicating that the segment with the highest reward score often closely aligns with the ground truth segment. This validates the reward model's effectiveness in providing meaningful guidance.\nAdditionally, as discussed in Sec 3.3, we encourage the agent to make independent decisions when selecting segments rather than blindly following reward scores in a purely greedy manner. The benefit of this design choice is demonstrated at the top of Fig. 5, where we present the matching accuracy of the agent's choices. Compared to the greedy approach, the agent's selections result in a sharper distribution of segment distances, with an average distance gap of -0.60, indicating closer alignment with the ground truth. In summary, our reward model assigns reliable relevance scores to segments, while the agent further refines these choices to achieve even more accurate selections."}, {"title": "5.4. Can VCA Benefit from Better Reward?", "content": "Previous analysis demonstrates the effectiveness of the reward model, though a gap remains compared to ground truth guidance. To examine the upper bound of our agent's performance with optimal reward guidance, we substitute the reward scores with ground truth time references. The"}, {"title": "6. Conclusion", "content": "In this work, we propose a curiosity-driven video agent framework for long video understanding tasks with self-exploration capability. VCA achieves state-of-the-art performance and efficiency without relying on auxiliary tools. In Sec. 5.4, we demonstrate VCA's great potential with more accurate reward guidance, suggesting that collecting synthetic data and training a specialized reward model would be a valuable direction for future work."}, {"title": "7. Prompt Design", "content": "In this section, we elaborate the prompt design of our VCA framework. We include the prompt on how the reward model (R) generates relevance scores in Fig. 6 and Fig. 7, and the prompt used for the exploration agent (\\u03c0) in Fig. 8, respectively.\nAs illustrated in in Fig. 6, for the first round, the reward model starts evaluating the relevance of each segment to the given question. For the subsequent rounds of exploration, since the reward model can only access the sampled frames from the selected local segment, we include reward history (Hr) in previous rounds to ensure consistency and alignment in terms of relevance score scale. The detailed template is shown in Fig. 7.\nAs illustrated in in Fig. 8, the Exploration Agent determines the next action based on these relevance scores. If the available information is sufficient, the agent proceeds to provide an answer. Otherwise, it continues to iteratively refine its exploration. Specifically, for the exploration agent, all frames stored in the memory buffer are presented together with the dialog history that is formatted in a conversational style."}, {"title": "8. Implementation Details", "content": "8.1. Dataset\nSystem Prompt\nYou are a helpful assistant. Please answer the following question.\nEgoSchema\nTaking into account all the actions performed by c, what can you deduce about the primary objective and focus within the video content?\nOption 0: C is cooking.\nOption 1: C is doing laundry.\nOption 2: C is cleaning the kitchen.\nOption 3: C is cleaning dishes.\nOption 4: C is cleaning the bathroom.\nPlease directly answer the option number.\nLVBench\nWhat year appears in the opening caption of the video?\n(A) 1636\n(B) 1366\n(C) 1363\n(D) 1633\nPlease select the best answer from the options provided and directly provide the letter representing your choice without giving any explanation.\nMMBench-Video\nWhat is the name of the player who scored the first goal in the video?\nPlease directly reply with your response.\nVideoMME\nWhat is the video mainly about?\nA. Planes invented by the Wright Brothers.\nB. The structural difference between the planes created by Whitehead and planes created by the Wright Brothers.\nC. Who invented the first plane.\nD. How Whitehead and the Wright Brothers cooperated to invent the first motorized flight.\nPlease select the best answer from the options provided and directly provide the letter representing your choice without giving any explanation.\nFor further clarity, we provide detailed prompts of a randomly chosen example for each dataset in Tab. 7. For each dataset, we follow the prompt template provided in their paper or official implementation.\n8.2. Baselines\nIn this section, we elaborate the implementation details of the baselines. For VideoAgent [68] and VideoTree [72], we begin by adhering to their official implementations23.\nHowever, we notice that both methods utilize GPT-4 as the reasoning agent, which is suboptimal compared to GPT-4o, the model we employ as the exploration agent. Therefore,"}, {"title": "9. Experiments", "content": "9.1. Experimental Results\nAs discussed in Sec. 4.1, we present the experimental results on VideoMME and MMBench-Video in Tab. 9 and Tab. 10, respectively. For VideoMME, we evaluate our framework on the long split, with videos averaging over 2,000 seconds in duration. Similarly, to ensure a fair comparison, we re-implement the baselines using GPT-40 as the base VLM. As shown in Tab. 9, our method achieves a significant improvement, outperforming VideoTree by 4.9% with less than 20% of observed frames, and outperforming VideoAgent by 12.1% with about 75% of observed frames.\nSimilar trends are observed in the results for MMBench-Video in Tab. 10. We observe that both VideoAgent and VideoTree perform poorly on this benchmark. We conjecture that this is due to the long split's emphasis on detailed visual clues, while captioning-based methods inher"}, {"title": "9.2. Ablation Study", "content": "In this section, we present the detailed results of the ablation study discussed in Sec. 4.3. The results across different domains of LVBench are summarized in Tab. 8. Overall, the results demostrate that both the reward model and the tree-search exploration mechanism play significant roles in enhancing performance. Interestingly, we observe that the agent without the reward model or tree-search exploration performs better in the Key Information Retrieval (KIR) and Summarization (Sum) domains. This result is unsurprising, as the absence of tree-search exploration prompts the agent to explore nearly twice as many frames. Similarly, without the reward model, the agent lacks focused guidance and instead explores more broadly across the video.\nConversely, these components contribute significantly to performance improvements in tasks like Event Recognition (ER) and Temporal Grounding (TG), where precise and focused exploration is essential. Overall, the reward model and tree-search exploration, each addressing distinct aspects of the exploration process, work together to drive large performance gains across diverse tasks, aligning with our findings in Sec. 4.3."}, {"title": "10. Case Study", "content": "As mentioned in Sec. 5.1, in this section, we investigate the common failure cases of our framework, aiming to provide data points and insights for the future research.\n10.1. Failure Mode: Inability to Detect Subtle Visual Details\n10.2. Failure Mode: Guidance Errors from the Reward Model\n10.3. Failure Mode: Limited Multi-modal Reasoning Abilities\n10.4. Discussion\nThe analysis of failure modes reveals the large potential of our framework. While detecting subtle visual details remains challenging even for human, the error stemming from guidance failures by the reward model and reasoning failures by the"}]}