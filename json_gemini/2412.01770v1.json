{"title": "Robot Learning with Super-Linear Scaling", "authors": ["Marcel Torne", "Arhan Jain", "Jiayi Yuan", "Vidaaranya Macha", "Lars Ankile", "Anthony Simeonov", "Pulkit Agrawal", "Abhishek Gupta"], "abstract": "Scaling robot learning requires data collection pipelines that scale favorably with human effort. In this work, we propose Crowdsourcing and Amortizing Human Effort for Real-to-Sim-to-Real(CASHER), a pipeline for scaling up data collection and learning in simulation where the performance scales superlinearly with human effort. The key idea is to crowdsource digital twins of real-world scenes using 3D re-construction and collect large-scale data in simulation, rather than the real-world. Data collection in simulation is initially driven by RL, bootstrapped with human demonstrations. As the training of a generalist policy progresses across environments, its generalization capabilities can be used to replace human effort with model generated demonstrations. This results in a pipeline where behavioral data is collected in simulation with continually reducing human effort. We show that CASHER demonstrates zero-shot and few-shot scaling laws on three real-world tasks across diverse scenarios. We show that CASHER enables fine-tuning of pre-trained policies to a target scenario using a video scan without any additional human effort.", "sections": [{"title": "I. INTRODUCTION", "content": "Robot learning has the potential to revolutionize decision-making for robots by leveraging data to learn behaviors deployable in unstructured environments, showing generalization and robustness. Critical to the success of robot learning, beyond the algorithms and model architectures, is the training data. As in most machine learning, getting the \"right\" type, quality, and quantity of data holds the key to generalization. Robot learning is still grappling with the question of what the right type of data and how to obtain it at scale. The type of data we can train on is inherently tied to the abundance of this data - good data is both high-quality and abundant. This paper proposes a system for obtaining this diverse, high-quality data at superlinear scale with sublinear human effort.\nUnlike vision and language, data for learning is not available passively there are relatively few robots that are already finding use in the world. This makes applying the same recipes we did in vision and language challenging, necessitating more careful consideration of how and where this data comes from. One option is to rely on teleoperation to collect this data. This approach is inherently limited by human effort, since the cost to collect data scales linearly with human involvement. Recent work [16, 29, 2] has attempted to scale the amount of teleoperation data however the quantity of data collected is still orders of magnitude smaller than the scale at which vision and language models show emergent capabilities.\nSo where might we find data that scales superlinearly with human effort? Simulation offers a potential solution, at face value providing free data up to the limit of computing. However, this hides a significant cost scene, task, and reward creation per domain is non-trivial, and even with scenes generated, behaviors are costly to obtain. This suggests that despite the promise, simulation data isn't quite free of cost, and requires considerable amounts of human efforts for content and behavior creation per environment. While it is possible to generate random environments procedurally, generating thousands of environments randomly is unlikely to cover the distribution of \"natural environments\", and generating behaviors randomly is unlikely to lead to success.\nIn this work, we propose a method to scale up continual data collection, ensuring human effort amortizes sublinearly with the number of environments. Our key idea is to leverage simulation for data scaling without the corresponding increase in content and behavior creation effort. For content scaling, we utilize 3D reconstruction methods, shifting the burden from designers to non-expert users and cheap data collection. For behavior generation, we employ techniques that leverage model generalization to reduce the required human data over time. The insight is that as we go across many simulated environments, models will show some levels of generalization. This generalization can be leveraged to continually reduce the amount of human data needed as new environments are encountered. CASHER creates a data flywheel, where data begets more data through model generalization.\nOur contributions include 1) a novel continual data collection system based on real-to-sim-to-real for training generalist policies, 2) a novel scanned deployment fine-tuning technique for improving the accuracy of a generalist policy on a target environment without additional human demonstrations, 3) a detailed analysis of the scaling laws for zero-shot performance of our generalist policies, 4) evaluation of the few-shot performance of the resulting generalist policies."}, {"title": "II. RELATED WORK", "content": "Large Scale Data Collection for Robotics: Learning from real-world demonstrations has proven effective [6, 44, 28]. To facilitate this, various studies have focused on improving hardware to ease the data collection process for teleoperators [44, 7, 41]. Efforts have also scaled up the volume of data from real-world demonstrations [29, 16, 2], staying nevertheless in the low-data regime. Moreover, real-world data collection is costly, requiring expert supervision and physical robots, which limits scalability. CASHER, instead, trains entirely in simulation, using real-world scans obtained via standard smartphones. Additionally, while traditional teleoperation data collection scales linearly with human effort, CASHER reduces the human effort needed for subsequent learning steps by leveraging the knowledge acquired during training.\nAutonomous Learning: To improve scalability of robot learning and reduce the amount of human demonstrations required, the field has explored autonomous data collection and learning methods. One approach is reinforcement learning (RL) in the real world [21, 19], but the standard RL techniques' need for resets poses scalability issues, as it requires either human supervision or substantial engineering efforts for automating resets. Reset-free reinforcement learning [1, 43, 36, 11] offers a promising alternative, but it still requires occasional human intervention and struggles with high sample complexity for learning more challenging tasks, making it hard to learn in the real-world. Autonomous learning in the real world presents significant challenges that are mitigated in simulation, where resets are manageable and data collection is more abundant. In CASHER, we exploit these advantages of simulation while minimizing the sim-to-real gap through real-to-sim scene transfers. Continual learning also faces challenges, such as catastrophic forgetting, as discussed in prior work [18]. We address this by decoupling the policy used to generate trajectories, which is fine-tuned with RL, from the final generalist policy, which is trained with imitation learning over the entire dataset.\nProcedural and Synthetic Data Generation: Creating realistic environments for robot learning in simulation is a significant challenge. To address this, prior work has proposed using large language models (LLMs) or heuristics to generate scene plans resembling the real world [40, 10, 25], or utilizing real-world scans to replicate actual scenes [9, 5]. Despite reducing human involvement, these methods often produce scenes that are unrealistic in appearance or object distribution, such as failing to accurately simulate real-world clutter. Generating procedurally accurate training environments remains an open challenge. However, extracting digital twins from the real world mitigates this issue, as scans reflect the actual test distribution. Relevant to our work, [42] automates the creation of simulatable environments from real-world scans, which could be integrated into our pipeline to scale up environment crowdsourcing. Once the environments are available, generating valid robot trajectories that solve the task is another challenge. An option becomes procedurally generating the motions using motion planning techniques [12]. However, these techniques require some assumptions beforehand.\nReal-to-Sim-to-Real Transfer for Robotics: Real-to-sim-to-real techniques have proven effective in learning robust policies for specific scenarios with minimal human supervision [38, 39]. However, these policies often fail to generalize to different scenarios, requiring significant human effort for each new environment. In this work, we address this limitation by learning generalist policies through a novel technique that amortizes the number of human demonstrations through training. Other research has tackled various challenges in real-to-sim-to-real, such as enhancing simulator accuracy with real-world interaction data [22, 34, 3], and automatically generating articulations from images [5, 14, 26]. These complementary advancements make simulators more realistic and could reduce"}, {"title": "III. AMORTIZED DATA SCALING FOR LEARNING GENERALIST POLICIES THROUGH REAL-TO-SIM-TO-REAL", "content": "This work presents CASHER, a pipeline for large-scale continual data collection for robotic manipulation. The primary challenge for data scaling in the realm of robotics is the absence of \u201cpassive\u201d, easy-to-collect data from naturally occurring, inadvertent sources, as is common in vision and language. While procedural generation in simulation can provide large amounts of data, the distribution and diversity of the data does not overlap with real-world environments. In this work, we argue that a multi-task, multi-environment real-to-sim-to-real pipeline can enable large-scale data generation, by leveraging model generalization to scale human-effort sublinearly as increasing numbers of environments are encountered. This is opposed to typical human teleoperated data collection that requires considerable expertise, physical infrastructure and suffers from linear scaling in human effort. This approach enables the scaling laws necessary for large scale data collection and training of robotic foundation models, showing non-trivial zero-shot generalization performance as well as cheap and efficient fine-tuning in new environments. CASHER consists of three elements 1) fast, accessible digital twin generation with 3-D reconstruction methods, 2) multi-environment model learning that amortizes the data collection process through autonomous data collection and model generalization, 3) efficient fine-tuning in new environments using 3-D scans, and minimal human demonstrations.\nOur proposed data collection pipeline adopts a real-to-sim-to-real approach, building digital twins of real-world scenes in simulation and collecting behavioral data in these simulations instead of the real world. This method offers several advantages 1) data collection does not require a physical robot setup, and hence can occur in a broader variety of realistic environments 2) it allows for safe, decentralized, and asynchronous data collection 3) digital twins capture the complexities of real-world scenarios more accurately than procedurally generated simulations. These advantages are crucial to the democratization and scalability of data collection as it is scaled up to thousands of non-experts and real environments beyond the lab. We leverage easily accessible mobile software[8, 31] for scene reconstruction from sequences of images to easily crowdsource simulated environments. These environments indicate the geometry, visuals and physics of diverse real-world scenes in simulation but do not have any demonstrations of the desired optimal behavior. We discuss how this can be obtained efficiently in the following section."}, {"title": "B. Amortized Data Collection", "content": "Given the diversity of realistic simulation scenes available through the digital twin pipeline outlined in Section III-A, learning generalizable decision-making policies requires a large training set of visuomotor trajectories demonstrating optimal behavior for each distinct environment. Two natural alternatives for obtaining these trajectories are: 1) human-provided demonstrations and 2) optimal policies trained via reinforcement learning . While tabula-rasa reinforcement learning can provide a robust set of trajectories with extensive state coverage without expensive human intervention, it faces considerable challenges related to exploration and reward design. On the other hand, human demonstrations avoid these issues but are expensive to collect at scale.\nA natural solution is to use sparse-reward reinforcement learning bootstrapped with human demonstrations [38, 13, 33]. This approach balances human effort for data collection and reward specification with state-space coverage. However, scaling it up to hundreds or thousands of scenes becomes tedious, as the required human effort increases linearly with the number of environments. In this work, we learn a generalist multi-environment policy to amortize the cost of human data collection across environments. We demonstrate that the capacity of such a multi-environment model to display non-trivial generalization allows the cost of continual human data collection to decrease as the number of training environments increases.\nThis system, formally stated in the Appendix Algorithm 1, divides the total number of environments into batches of size K. For the first batch of K environments $E_1, E_2, ..., E_K$, we have a multi-environment visuomotor policy $\\pi_G$ randomnly initialized with no generalization capabilities. Thereafter, we initialize it with data from the first K environments, using reinforcement learning bootstrapped with human-provided demonstrations. Demonstration bootstrapped RL produces optimal visuomotor trajectories per environment $D$, that are then distilled into a single perception-based, generalist multi-environment policy $\\pi_G$ with visuomotor policy distillation [4] (Appendix VII-C2).\nWhile human demonstrations are used to bootstrap the data generation and training of the first iteration of the generalist policy $\u03c0_G$ on the first K environments, our key insight is that if $\u03c0_G$ shows non-trivial level of generalization on visuomotor deployment in the next K simulation environments $E_{K+1},..., E_{2K}$, then this policy $\u03c0_G$ can be used to collect simulated demonstrations $T = {T_{K+1,1}, T_{K+1,2},...,T_{2K,N}}$ in place of a human demonstrator. We do so by deploying the visuomotor policy $\u03c0_G(a_t|o_t)$ using perceptual observations $o_t$ such as RGB point clouds, but since we are in the simulation we collect $T$ with paired data of visual observations $o_t$, actions\n\u03c0\u2080 \u2190 max \u03b8,\u03c6 \u03a3 E\u0395\u03951, \u03952,..., \u03b5\u03ba \u03a3 (st,at,rt) \u2208\u03b5\u03af \u03c0\u03b8(at|st) At, clip( \u03c0\u03b8(at|st) `\u03c0\u03b8old (at|st)' 1 - \u20ac, 1 + \u20ac)\u00c2t) +\u03b2 \u03a3 \u0395\u1f30\u20ac{1,2,...,\u03b5\u03ba} (st, Varg) \u0395\u0395 (\u03c0\u03b8old) (V(st) - Varg) 2 +\u03a3 (si,ai) ET log \u03c0\u03b8(ai Si)  (1)\nT can be used to obtain a single robust, state-covering optimal multi-environment policy $\u03c0_{S1}(a_t | S_t)$ for all $E_{K+1},..., E_{2K}$ via demonstration-bootstrapped reinforcement learning. Nevertheless, in some environments, the policy may still perform poorly due to the occasional low-quality demonstrations from $\u03c0_G$. To address this, we define the set of environments where $\u03c0_{S1}$ achieves below $r$ success rate as $F_C {E_K, E_{K+1},..., E_{2K} }$. For these environments $F$, we fall back to querying the human demonstrator for high-quality demonstrations and learn a second state-based policy $\u03c0_{S2}(a_t | s_t)$ using demonstration-bootstrapped reinforcement learning on $F$.\nThe two learned policies $\u03c0_{S1}$ and $\u03c0_{S2}$ can then be used for generating data on {$E_K,E_{K+1},...,E_{2K}$}\\F_and F respectively with these new trajectories being added into $D$. Then, a visuomotor policy can be trained by fitting $D$ on the first 2K environments with supervised learning (see Appendix VII-C2 for implementation details).\nTG \u2190 max \u03b8 E(ot,at)\u223cD [log \u03c0Go(ai|0i)] (2)\nThen the process repeats for the next K environments. As the visuomotor generalist policy $\u03c0_G$ is trained across more environments, it demonstrates increasingly non-trivial generalization, gradually replacing the human demonstrator in more environments. This reduces the amount of human effort required for data collection as training progresses. Importantly, the generalization across environments does not need to achieve perfect success rates but should be sufficient to bootstrap a demonstration-augmented policy learning algorithm (Equation 1). This suggests an interesting scaling law data collection becomes more human-efficient as training progresses, eventually becoming self-sustaining. For a detailed outline of the practical data collection pipeline, refer to Algorithm 1."}, {"title": "C. Fine-tuning of Generalist Policies on Deployment", "content": "The generalist policies $\u03c0_G(a_t | o_t)$ pretrained in Section III-B, show non-trivial generalization across environments but may not achieve optimal performance in any one environment upon zero-shot deployment. However, these generalist policies can serve as a starting point for efficient fine-tuning at test time. In this section, we present an alternative for fine-tuning generalist policies $\u03c0_G(a_t|o_t)$ during deployment. We make the observation that we can follow the same procedure as model-bootstrapped autonomous data collection during training described in Section III-B. Given a scanned digital twin $E_{test}$ of the testing environment in simulation, the pre-trained multi-environment model $\u03c0_G(a_t | o_t)$ shows some non-trivial zero-shot generalization, but may not achieve optimal performance in $E_{test}$. By executing the visuomotor policy $\u03c0_G(a_t|o_t)$ in $E_{test}$, we collect a dataset of only successful trajectories $T_{test}$ consisting of $(o_t, a_t, S_t)$ tuples in simulation, without the need for any external human intervention. This model-generated data can then be used to train a robust, high-coverage state-based policy $\u03c0_S(a_t|s_t)$ using demonstration-bootstrapped reinforcement learning (Eq 1). Finally, for real-world transfer from visual observations this state-based policy $\u03c0_S(a_t | s_t)$ is distilled into a \u201cfine-tuned\u201d visuomotor policy $\u03c0_{Gf}(a_t|O_t)$, by collecting a set of successful rollouts D with $\u03c0_s(a_t|s_t)$ and fine-tuning the previously obtained generalist policy $\u03c0_G(a_t O_t)$"}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "Our experiments are designed to answer the following questions: (a) What are the scaling laws of CASHER? (b) How much can we amortize the quantity of human data needed through learning without a loss in performance? (c) What are the few-shot/scanned fine-tuning capabilities of the learned generalist policies? (d) Do these scaling laws hold across different tasks? (e) Do these generalist policies extrapolate to multi-object environments when trained with single object?\nTo answer these questions, we design two different tasks: placing bowls/mugs/cups in sinks and placing boxes in shelves. We use a single-arm manipulator, the Franka Research 3 arm with 7 DoF and a parallel jaw gripper, see Appendix XII. We crowdsourced environment data collection, obtaining (a maximum of ) 56 and 36 different scenes for the two tasks, respectively. We evaluated the policies across two institutions on 8 and 2 real-world scenes not included in the training set. Further details on the hardware setup and tasks are provided in Appendix VIII and XII."}, {"title": "A. Zero-Shot Scaling Laws Analysis", "content": "In this section, we analyze the zero-shot performance of multiple generalist policies trained with varying amounts of training environments on the task of put a mug/bowl/cup in a sink. For fair comparison, we train these policies using human demonstrations in each environment. In Section IV-B, we compare this baseline to the autonomous data collection system presented in Section III-B."}, {"title": "B. Amortized Human Data Needed Through Continual Data Collection", "content": "In this section, we evaluate the amortization of number of human demonstrations needed as learning progresses across multiple environments. We compare two approaches: our proposed system using continual data collection performed in four sequential batches of 10 environments each, and another baseline providing human demonstrations for each environment individually. The evaluation is conducted in a single real-world kitchen with six different objects for the task of put a bowl/mug/cup in a sink, performing 6 rollouts per object. Figure 4(a) shows that the performance per number of demonstrations significantly increases as the policy starts developing generalization. Specifically, as shown in Figure 4(b), the quantity of human demonstrations needed decreases as the policy improves with each subsequent batch. Although CASHER shifts the burden to compute rather than human effort, Figure 4(c) indicates that the compute required decreases as well when scaling up the system, since the success rate of the generalist policy is higher, the number of trials performed to reach the same number of successful rollout decreases. Finally, we observe that the performance of the continually learned policy is higher than of the policy learned solely from human demonstrations. We hypothesize that this is due to the multimodality in behaviors from the human demonstrations. When the policy autonomously collects the data, behaviors remain closer to those already learned, whereas human-provided demonstrations may introduce more variability, making learning harder."}, {"title": "C. Fine-Tuning of Generalist Policies", "content": "Unsupervised scanned deployment fine-tuning: To evaluate the efficacy of unsupervised fine-tuning through a scan (Section III-C), we select two scenes for the task of placing a mug/cup/bowl in a sink where the policy trained on 36 environments performs poorly (\u2264 20%). We then apply the scanned deployment fine-tuning algorithm as described in Section III-C. As shown in Figure 6, this results in an average performance increase of 55% without any additional human demonstrations.\nFew-shot supervised fine-tuning: We select three environments where the base policy trained on 36 environments performs poorly (<20%). We then collect 10 demonstrations for each environment and apply the few-shot fine-tuning procedure described in Section VII-D2. This fine-tuning improves the performance of the base policy by an average of a 54% in success rate."}, {"title": "D. Analysis of CASHER on More Tasks", "content": "We attempt to solve two additional tasks, putting a box on a cabinet and opening a cabinet. The first is a more complicated manipulation task since it requires more precise grasping to not make the box fall, and the second shows how our proposed method works for articulated objects (see Appendix VII-B where we give more details on how the proposed real-to-sim pipeline can handle articulated objects). In these two tasks, we focus our analysis on few-shot fine-tuning as described in Section III-C. For putting a box on a cabinet, we crowdsourced 36 environments, collected 10 demonstrations for each of three scenes, and reported the performance after fine-tuning with 10 demos. For opening a cabinet, we crowdsource 10 environments, collect 10 demonstrations for a new test environment and report the performance after fine-tuning with these demos. In Figure 5, we show the performance the performance increases with the number of training environments, without reaching a saturation point. Fine-tuning the policy trained on 36 and 10 environments respectively resulted in a significant performance improvement of 36% and 30% compared to the imitation learning baseline, which had a 0% success rate. We expect the performance of the generalist policies to keep improving as we have not reached a saturation point."}, {"title": "V. CONCLUSION", "content": "In this work, we present a system for scaling up robot learning through crowdsourced simulation. We showed that through the learning of visual generalist policies, we are able to scale across environments with decreasing amounts of human effort. The resulting policies are shown to transfer to the real world, enabling both zero-shot and finetuning results.\nLimitations: While with this work we demonstrate superlinear scaling of data with respect to human demonstrations, the burden shifts to compute. And even though we have shown a reduction in compute time with scaling, it still exceeds the time required for collecting real-world demonstrations. Additionally, training in simulation poses challenges, as not all real-world objects can be accurately simulated yet, such as liquids and deformable objects. However, contrary to the human teleoperation efforts, with advancements in compute resources and simulator research, systems like CASHER will benefit from these and further improve scalability. Conclusion: This work presents CASHER, a real-to-sim-to-real system that trains generalist policies with sublinear human effort. This research paves the way for building robotic foundation models in simulation with larger datasets and enhanced robustness."}, {"title": "VII. METHOD DETAILS", "content": "We present the following algorithm to explain the amortized data collection section in CASHER.\nUnlike prior work [38, 5], our goal is not to accurately master a single environment, but rather to train a generalist agent capable of generalizing to new, unseen environments. To obtain a wide distribution of scenes with a variety of layouts, colors, and lighting conditions, We developed our general purpose, an easy-to-use, real-to-sim pipeline that supports the crowdsourcing contribution of 3D scans (See Figure 7 for an overview of the GUI [38]). Digital twins are obtained directly from real-world videos or image sequences using photogrammetry methods such as Gaussian splatting [15] and neural radiance fields [23]. High-fidelity 3D meshes can be scanned in under five minutes using off-the-shelf mobile software such as Polycam [31] and ARCode [8]. In Table I, we show the low average time needed to create a scene with different configurations. This easy-to-use software running on standard, commercial mobile phones enables crowdsourcing of real-world scans from non-experts worldwide with minimal instruction. The crowdsourced scenes demonstrate a natural distribution of clutter, scene layouts, colors, lighting conditions, and positional variations.\nThese real-world scans are then easily transferred into a photo-realistic physics simulator, Issac Sim [27], using an easy to use GUI for scene articulation and curation [38]. This flexible interface accommodates various scene complexities, from static to highly articulated environments. Using the GUI, we also add objects of interest (bowl/mug/cup for putting the object into the sink, box for putting the box in the cabinet) into the scene, and additional sites to mark the position of the sink and cabinet.\nWith the scene and the object rendered inside the simulation, we use teleoperation with a keyboard to collect 10 demonstrations for each articulated environment. There are 14 different discretized actions to choose from, corresponding to two directions in all spatial axes and rotational axes, and open and close the gripper."}, {"title": "IX. IMPLEMENTATION DETAILS", "content": "1) State-based policy: As described in Section III-B, we trained a series of state-based policies with privileged information in simulation. The policy model is a simple Multi-Layer Perceptron (MLP) network, with input as the privileged state in simulation as specified in VIII and outputs a probability distribution of 14 classes, corresponding to the probabilities for each discrete end-effector action. To implement PPO with the BC loss algorithm, we built upon the Stable Baselines 3 repository [32]. The size of the MLP network is a mix of two sizes: two layers of size 256 and 256, and three layers of size 256, 512, and 256."}, {"title": "X. BASELINE COMPARISONS WITH LARGE SCALE REAL WORLD DATA COLLECTION", "content": "We deployed two of the state-of-the-art robot models trained on the Open X-Embodiement dataset [29] consisting of more than 800k real-world trajectories: OpenVLA[17], a vision-language-action model consisting of fine-tuned Llama 2 7B fine-tuned and Octo[37], a pre-trained transformer based diffusion policy.\nWe provided RGB images and language instructions as inputs to the model and evaluated the model zero-shot on the task of moving an object to sink. While neither of the two models successfully completes the task, OpenVLA performs qualitatively better than Octo (see the additional material videos).\nWe then collected 10 demos and fine-tuned the Octo policy with these. However, we observe that the performance for finetuned Octo still yields zero success, with qualitatively better performance than the zero-shot result.\nThese results, presented in Table V, show that zero-shot and few-shot performance on the tasks that we are tackling in this paper is not solved yet. It also shows the need for much more data to be collected in order to solve these tasks in a variety of scenarios. Thereafter providing evidence of the benefits of CASHER and of leveraging data from simulation to scale up the robot data collection."}, {"title": "XI. DETAILED EVALUATION RESULTS", "content": "We conducted experiments involving disturbance and distractors for putting the object into the sink task to study the robustness of the generalist policies. The experiments include multi-object scenarios, dim lighting scenarios, messy kitchen scenarios, and disturbance scenarios."}, {"title": "A. Evaluation on Multi-Object Scenes", "content": "In this section, we study the extrapolation and robustness capabilities of the learned generalist policies by evaluating them on tasks involving multi-object scenes. Specifically, the robot needs to pick and place multiple objects into the sink sequentially, even though it was trained on single objects. We evaluated this by allowing the robot six trials to place the three objects in the kitchen into the sink. As shown in Figure 5, despite not being trained for multi-object, the policy succeeds 80% of the time in placing two objects and 10% in placing all three objects sequentially."}, {"title": "B. Evaluation on Scenes Involving Disturbance and Distractors", "content": "In the dim lighting scenario, there is minimal lighting in the scene, while the robot is only trained in the environment with sufficient lighting. The robot was able to complete the task successfully into the sink for 30% of all trials. See Figure 10 for the experimental setup.\nIn the messy kitchen scenario, dirty dishes and tableware are sitting in the sink, closely mimicking the realistic setting of a household kitchen sink. The robot is only trained in an environment with a clean sink. The robot was able to complete the task successfully into the sink for 30% of all trials. See Figure 10 for the experimental setup.\nIn the human disturbance scenario, the experimenter pushes the object the change its position during the evaluation process. The robot is able to complete the task successfully into the sink for 50% of all trials."}, {"title": "XII. HARDWARE SETUP", "content": "Real-world experiments are run on two different Panda Franka arms. Both of the Panda Franka arms are mounted on mobile tables, and run the same experiments, but they are located in two different institutions and therefore have access to different real-world kitchen settings.\nWe mount two calibrated cameras per setup to obtain depth perception to create an aligned point cloud map for vision-based policies. In particular, we use the two Intel depth Realsense cameras D435i for both setups. See Figure XII for more details on the robot setup."}, {"title": "XIII. CROWDSOURCING", "content": "We source the kitchen scans from both expert and non-expert users. For placing the object-to-sink task, we collected policies on 29 sink scenes, of which 22 were collected through crowdsourcing. For putting the object to cabinet task, we collect policies on 26 cabinet scenes, of which 18 are collected through crowdsourcing."}]}