{"title": "Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative", "authors": ["Zihao Li", "Xiao Lin", "Zhining Liu", "Jiaru Zou", "Ziwei Wu", "Lecheng Zheng", "Dongqi Fu", "Yada Zhu", "Hendrik Hamann", "Hanghang Tong", "Jingrui He"], "abstract": "While many advances in time series models focus exclusively on numerical data, research on multimodal time series, particularly those involving contextual textual information commonly encountered in real-world scenarios, remains in its infancy. Consequently, effectively integrating the text modality remains challenging. In this work, we highlight an intuitive yet significant observation that has been overlooked by existing works: time-series-paired texts exhibit periodic properties that closely mirror those of the original time series. Building on this insight, we propose a novel framework, Texts as Time Series (TaTS), which considers the time-series-paired texts to be auxiliary variables of the time series. TaTS can be plugged into any existing numerical-only time series models and enable them to handle time series data with paired texts effectively. Through extensive experiments on both multimodal time series forecasting and imputation tasks across benchmark datasets with various existing time series models, we demonstrate that TaTS can enhance predictive performance and achieve outperformance without modifying model architectures.", "sections": [{"title": "1. Introduction", "content": "Time series modeling plays an important role in a wide range of real-world applications, including domains of finance (Sezer et al., 2020; Jing et al., 2021; Zhou et al., 2020), healthcare (Zhang et al., 2024b; He et al., 2024), climate (Fu et al., 2024b), and energy systems (Kotzur et al., 2017; Jing et al., 2022; 2024c). While extensive research has focused on approaches that rely solely on the numerical values of time series (Qiu et al., 2025; Lin et al.; Zhou et al., 2021; Wang et al., 2024b; Li et al., 2021), real-world scenarios often involve additional modalities that co-occur with the time series and can provide valuable complementary information (De Baets & Harvey, 2023; Rai et al., 2023; Kyei & Antwi, 2017; Jing et al., 2024a; Zheng et al., 2024b;a).\nIn such real-world scenarios like pandemic policymaking, economic planning, or investment strategies, human experts often incorporate additional sources of information in conjunction with time series data to forecast future trends and make well-informed decisions, as textual information can provide explanations, updates, or external factors that influence the underlying numerical patterns. However, research on effectively leveraging data from other modalities paired with time series remains in its early stages. In this work, we focus on time-series paired with texts at each timestamp, a common data format in real-world applications where textual descriptions are associated with time series at each timestamp in a parallel manner, as illustrated in Figure 3 (left). For instance, during a pandemic, numerical data such as infection rates and hospital admissions are often accompanied by textual information like government announcements and news reports (Cinelli et al., 2020). On the one hand, directly applying numerical-only models while ignoring the accompanying textual information can result in suboptimal performance as it overlooks valuable contextual information that may influence or explain the patterns in the time series. On the other hand, the current state-of-the-art approach (Liu et al., 2024a) treats the textual modality as \"bag-of-texts\", disregarding the unique positional characteristics that time-series-paired texts may inherently possess. Such limitations raise a pivotal research question:\nWhat unique attributes characterize time-series-paired texts, and how can they be systematically integrated to improve time series modeling and predictions?\nIn this paper, we pioneer the exploration of effectively leveraging paired texts to enrich time series analysis. We identify an intriguing phenomenon, which we term \u201cChronological Textual Resonance\u201d: time-series-paired texts exhibit periodic patterns that closely reflect the temporal dynamics of their corresponding numerical time series. Notably, despite variations in words and expressions, the hidden representations of two texts associated with time series points separated by a period demonstrate high similarities, revealing a deeper alignment between textual and numerical modalities. We attribute this phenomenon to the fact that the paired texts, such as expert notes, news articles, social media posts, or event descriptions, are highly sensitive to and inherently evolve in response to the dynamics of the time series itself.\nBuilding on these insights, we propose Texts as Time Series (TaTS), a simple yet effective framework for integrating paired texts to enhance multimodal time series modeling. As previous studies have shown that different variables in a multivariate time series exhibit similar periodicity properties (Zhang & Yan, 2023; Wang et al., 2024c; Yi et al., 2024), our discovery of Chronological Textual Resonance suggests that time-series-paired texts follow a similar pattern. This insight implies that paired texts can be considered as special auxiliary variables to augment the original time series. Motivated by this, TaTS first transforms the paired textual information into a lower-dimensional representation, then combines the original time series with the textual representations as new variables to form an augmented time series. This augmented time series is subsequently fed into existing time series models, allowing them to capture both numerical and textual temporal dynamics. By incorporating paired texts as auxiliary variables and utilizing established time series models to process them, TaTS effectively exploits the intrinsic temporal properties of the texts. Our TaTS framework offers two key benefits: (i) it effectively captures the evolving positional characteristics of texts paired with a time series by incorporating their projected representations into time series models; and (ii) it functions as a plug-in module, maintaining compatibility with existing time series models. Empirically, through extensive experiments over various benchmark datasets and multiple existing time series models, the proposed TaTS achieves state-of-the-art performance on both forecasting and imputation tasks, as shown in Figure 1. In summary, our contributions in this papers are as follows:\n\u2022 We uncover a previously overlooked phenomenon, termed Chronological Textual Resonance (CTR), which demonstrates that time-series-paired texts exhibit periodic patterns closely aligned with their corresponding numerical time series.\n\u2022 Based on this phenomenon, we propose a plug-and-play multimodal time series forecasting framework, Texts as Time Series (TaTS), which transforms text representations into auxiliary variables, seamlessly integrating them into existing time series models to capture the intrinsic temporal properties of paired texts.\n\u2022 Experiments on diverse benchmark datasets and multiple existing time series models demonstrate that TaTS consistently achieves superior performance without requiring modifications to model architectures."}, {"title": "2. Preliminary", "content": "We use calligraphic letters (e.g., \\( \\mathcal{A} \\)) for sets and bold capital letters for matrices (e.g., \\( \\mathbf{A} \\)). For matrix indices, we use \\( A[i, j] \\) to denote the entry in the \\( i^{th} \\) row and the \\( j^{th} \\) column. For a vector \\( v \\), \\( v[i : j] \\) represents the subvector consisting of elements from the \\( i^{th} \\) to the \\( j^{th} \\) position, inclusive. Additionally, \\( A[i, :] \\) returns the \\( i^{th} \\) row in \\( A \\) and \\( A[: i] \\) returns the first \\( i \\) rows of \\( A \\). In this paper, we focus on both time series forecasting and imputation.\nTime Series Forecasting. A time series is denoted as \\( \\mathbf{X} = \\{X_1,X_2, ..., X_N \\} \\in \\mathbb{R}^{T\\times N} \\), where \\( T \\) represents the number of time steps and \\( N \\) denotes the number of variables. \\( x_i \\) is the time series sequence of the \\( i^{th} \\) variable. When \\( N > 1 \\), the time series is referred to as a multivariate time series. Let \\( \\mathbf{X}_{a:b} \\) represent the time slice of the series from timestamp \\( a \\) to \\( b \\), i.e., \\( \\mathbf{X}_{a:b} = \\{x_1[a: b], x_2[a: b],...,x_N[a : b]\\} \\). The task of time series forecasting is to predict the future \\( H \\) steps, which can be formulated as:\n\\[\\mathbf{X}_{T+1:T+H} = \\mathcal{F} (\\mathbf{X}_{1:T}; \\theta_{\\text{forecast}}) \\in \\mathbb{R}^{H\\times N},\\qquad(1)\\]\nwhere \\( \\mathbf{X}_{T+1:T+H} \\) denotes the forecasting results, \\( \\mathcal{F} \\) denotes the mapping function, and \\( \\theta_{\\text{forecast}} \\) denotes the learnable parameters of \\( \\mathcal{F} \\).\nTime Series Imputation. The goal of imputation is to estimate missing values in the observed time series \\( \\mathbf{X} \\), where the missing entries are denoted by a binary mask \\( \\mathbf{M} \\in \\{0,1\\}^{T\\times N} \\). Specifically, \\( \\mathbf{M}_{t,n} = 1 \\) indicates that \\( \\mathbf{X}_{t,n} \\) is observed, while \\( \\mathbf{M}_{t,n} = 0 \\) indicates that \\( \\mathbf{X}_{t,n} \\) is missing. The imputation task can be formulated as:\n\\[\\mathbf{X}^{\\text{Imputed}} = \\mathcal{G} (\\mathbf{X} \\odot \\mathbf{M}, \\mathbf{M}; \\theta_{\\text{impute}}) \\in \\mathbb{R}^{T\\times N},\\qquad(2)\\]\nwhere \\( \\mathbf{X}^{\\text{Imputed}} \\) represents the imputed time series, \\( \\mathcal{G} \\) denotes"}, {"title": "3. Chronological Textual Resonance (CTR)", "content": "The association of time series at each timestamp may imbue time-series-paired texts with unique characteristics that can be effectively harnessed through an appropriate design. In this section, we unveil a phenomenon where paired texts exhibit periodicity closely aligned with that of the time series itself. This phenomenon is prevalent across various real-world domains, such as finance, social sciences, and transportation. To illustrate this phenomenon, we analyze three real-world time series datasets with paired texts (Liu et al., 2024a), including (i) Economy: The time series represents trade data of the United States, while the texts provide descriptions of the general economic conditions of the country. (ii) Social Good: The time series captures the unemployment rate in the United States, and the texts include detailed unemployment reports. (iii) Traffic: The time series reflects monthly travel volume trends from the U.S. Department of Transportation, with corresponding texts derived from traffic volume reports issued by the same department.\nFor each dataset \\( \\mathcal{D} = \\{\\mathbf{X}, \\mathcal{S}\\} \\), we employ the Fourier Transform (Nussbaumer, 1982; Sneddon, 1995) to analyze the frequency components of time series data, allowing us to identify its dominant periodic components, as illustrated by the blue curves in Figure 2. Furthermore, to examine the periodicity of texts, we embed each \\( s_t \\in \\mathcal{S} \\) to obtain the text embedding \\( e_t \\) at timestamp \\( t \\). Then we compute their lag-similarity, defined as \\( d_l = \\sum_t \\text{cos}(e_t, e_{t+L}) \\) where \\( L \\) is the lag and \\( \\text{cos}(\\cdot, \\cdot) \\) represents the cosine similarity. If the text embeddings exhibit a significant periodic pattern, the lag-similarity \\( d_l \\) will also fluctuate periodically as the lag \\( l \\) increases (proof in Proposition A.1). Finally, we identify the major frequencies (those with the largest amplitudes) of the texts by applying FFT to the text lag-similarity, and mark them with red dashed lines, as shown in Figure 2. Detailed process is provided in Appendix A. We find that the major frequencies of the paired texts closely match those of the time series, indicating that the paired texts exhibit periodicity that is strongly aligned with the temporal dynamics of the time series.\nWhy should we leverage CTR? Parallel text provides complementary information and expert knowledge that can significantly enhance the understanding of time series data. To demonstrate the benefits of utilizing periodicity in the text modality, we present an illustrative example in the univariate forecasting task, where the goal is to"}, {"title": "4. Texts as Time Series", "content": "Guided by the above insights, this section introduces our proposed framework, Texts as Time Series (TaTS). TaTS integrates concurrent texts by transforming them into auxiliary variables and seamlessly plugging the text-augmented time series into any existing time series model. Our approach effectively captures the temporal dynamics and interconnections between the time series and concurrent texts by unifying them within a single time series model. An overview of the proposed TaTS is illustrated in Figure 3."}, {"title": "4.1. Concurrent Texts are Secretly Auxiliary Variables", "content": "Previously, we elucidated three key reasons behind the CTR phenomenon, which serve as the core motivation for TaTS:"}, {"title": "4.2. Framework and Training Objectives", "content": "Our proposed TaTS framework builds on the insight that concurrent texts behave similarly to additional variables and, therefore, can be treated in a comparable manner. As shown in Figure 3.\nTransforming Concurrent Texts into Variables. Given the dataset \\( \\mathcal{D} = \\{\\mathbf{X} = \\{\\mathbf{X}_1, \\mathbf{X}_2, ..., \\mathbf{X}_N\\}, \\mathcal{S} = \\{s_1,s_2,...,s_T\\}\\} \\), we first embed the texts using a text encoder \\( \\mathcal{H}_{text} \\) to obtain text embeddings \\( E = \\{e_1,e_2,...,e_T\\} \\in \\mathbb{R}^{d_{text}\\times T} \\). Since the text embedding dimension \\( d_{text} \\) is typically much larger than the number of variables in the time series, we reduce the dimensionality of the text embeddings by applying a Multi-Layer Perceptron (MLP), mapping them into a lower-dimensional space.\n\\[z_t = MLP(e_t; \\theta_{MLP}) \\in \\mathbb{R}^{d_{mapped}},\\qquad(4)\\]\nwhere \\( d_{mapped} \\) is the reduced dimensionality.\nUnifying by Plugging-in a Time Series Model. The resulting mapped embeddings \\( Z = \\{z_1, z_2,...,z_T\\} \\in \\mathbb{R}^{d_{mapped}\\times T} \\) are then treated as auxiliary variables in the time series. Specifically, \\( Z \\) is concatenated with \\( X \\) to form a unified multimodal sequence:\n\\[\\mathbf{U} = [\\mathbf{X}; Z']_{dim=1} \\in \\mathbb{R}^{T\\times(N+d_{mapped})},\\qquad(5)\\]\nThe unified sequence \\( \\mathbf{U} \\) is then passed into an existing time series model for downstream tasks. Here, we formulate the example of forecasting the next \\( H \\) steps of the time series\n\\[\\mathbf{X}_{T+1:T+H} = \\mathcal{F} (\\mathbf{U}_{1:T}; \\theta_{\\text{forecast}}) [: N] \\in \\mathbb{R}^{H\\times N},\\qquad(6)\\]\nwhere \\( \\mathcal{F}(\\cdot; \\theta_{\\text{forecast}}) \\) denotes the time series forecasting model with parameters \\( \\theta_{\\text{forecast}} \\), and \\( [: N] \\) extracts the first \\( N \\) variables corresponding to the original time series.\nFinally, we joint train the time series model \\( \\theta_{\\text{forecast}} \\) as well as the mapping MLP \\( \\theta_{MLP} \\) using the Mean Squared Error (MSE) loss.\n\\[\\mathcal{L}_{\\text{forecast}} (\\mathbf{X}, \\hat{\\mathbf{X}}) = \\frac{1}{H\\cdot N} \\sum_{t=T+1}^{T+H} \\sum_{i=1}^N (X_{t,i} - \\hat{X}_{t,i})^2,\\qquad(7)\\]\nwhere \\( X_{t,i} \\) and \\( \\hat{X}_{t,i} \\) represent the ground truth and predicted values of the \\( i^{th} \\) variable at time step \\( t \\), respectively."}, {"title": "5. Experiment", "content": "In this section, we empirically validate the effectiveness of the proposed TaTS framework through extensive experiments conducted on multiple datasets using a variety of existing time series models. Specifically, we demonstrate that TaTS can be seamlessly integrated with existing time series models to effectively handle time series with paired texts. In particular, we use GPT2 (Radford et al., 2019) encoder to embed the paired texts. We also validate the performance of TaTS with other text encoders across different datasets in section 5.2 and Appendix C.6.\nDatasets. We evaluate our framework on 9 real-world datasets (Liu et al., 2024a), spanning diverse domains such as environmental sciences, social systems, and infrastructure. The datasets have sample frequencies ranging from daily to weekly and monthly. Further details about the datasets can be found in Appendix B.1.\nTime Series Models and Baselines. To demonstrate the compatibility of TaTS with existing time series models, we integrate TaTS with 9 widely used models across different categories, including (i) Transformer-based models: iTransformer (Liu et al., 2024c), PatchTST (Nie et al., 2023), Crossformer (Zhang & Yan, 2023), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021) and Transformer (Vaswani et al., 2017). (ii) Linear models: DLinear (Zeng et al., 2023). (iii) Frequency-based models: FEDformer (Zhou et al., 2022b), FiLM (Zhou et al., 2022a).\nFor each time series model, we compare our TaTS framework against two baselines: (i) Numerical-only uni-modal modeling, which ignores the paired texts and utilizes only the numerical time series with the given time series model. (ii) MM-TSFLib (Liu et al., 2024a), a recently proposed"}, {"title": "5.1. Main Results", "content": "Settings. Regardless of whether numerical-only models, MM-TSFLib, or TaTS are used, all models are trained to minimize the MSE loss as defined in Equation 7. Implementation details are provided in Appendix B.4.\nTable 2 presents the performance results for the time series forecasting task. For each dataset and time series model, we report the average performance across four different prediction lengths, and the full results for each prediction length are provided in Appendix C. For a dataset with relatively few samples, such as Economy, we perform short-term forecasting with prediction lengths of {6, 8, 10, 12}. In contrast, for a dataset with a larger number of samples, such as Environment, we perform long-term forecasting with prediction lengths of {48, 96, 192, 336}. From the results, compared to uni-modal modeling using only numerical values or the state-of-the-art baseline MM-TSFLib, our TaTS consistently achieves the best performance across all datasets. Notably, by plugging in TaTS to various existing time series models, it achieves an average performance improvement of over 5% on 6 out of 9 datasets and delivers a remarkable performance boost of over 30% on the Environment dataset, which contains the largest number of samples among all datasets. The results also demonstrate that TaTS is highly compat"}, {"title": "5.2. Further Analysis", "content": "Hyperparameter Sensitivity. We perform hyperparameter studies to evaluate the impact of (i) the learning rate and (ii) \\( d_{mapped} \\), the dimension to which high-dimensional text embeddings are projected by the MLP, as defined in Equa"}, {"title": "6. Related Work", "content": "Numerical-only Time Series Modeling. Recently, various deep learning models have been developed for time series analysis, which can be broadly categorized into three categories. (1) Patch-based models. PatchTST (Nie et al., 2023) segments time series into subseries-level patches to capture dependencies, while Crossformer (Zhang & Yan, 2023) employs a two-stage attention mechanism to model both cross-time and cross-variable dependencies efficiently. Autoformer (Wu et al., 2021) introduces decomposition blocks to separate seasonal and trend-cyclical components. (2) Global representation models. iTransformer (Liu et al., 2024c) utilizes attention over global series representations to capture multivariate correlations. Informer (Zhou et al., 2021) reduces self-attention complexity using ProbSparse self-attention for improved efficiency. Dlinear (Zeng et al., 2023) demonstrates that simple linear regression in the raw space can perform competitively on MTS tasks. (3) Frequency-aware models. FEDformer (Zhou et al., 2022b) represents series through randomly selected Fourier components, while FiLM (Zhou et al., 2022a) enhances representations with frequency-based layers to reduce noise and accelerate training. In this work, our proposed TaTS is compatible with all of the models listed above.\nTime Series with other data sources. Real-world data often coexists and coevolve with auxiliary information (Li et al., 2023; Ban et al., 2024), such as various modalities (Li et al., 2024c; Fu et al., 2022; Zhu et al., 2024; Zheng et al., 2023; 2019; Li et al., 2024b) and heterogeneous views (Zheng et al., 2024c; 2022; Li et al., 2022; 2024a). In time series analysis, effectively utilizing this auxiliary information remains an emerging and nascent area. In the financial domain, several early works have explored integrating time series with textual data, albeit not in a timestamp-aligned manner, or often leveraging general machine learning models rather than time series-specific architectures. For example, StockNet (Xu & Cohen, 2018) uses a VAE-"}, {"title": "7. Conclusion", "content": "Real-world time series data often comes with textual descriptions, yet prior studies have largely overlooked this modality. We identify Chronological Textual Resonance, where text embeddings exhibit periodic patterns similar to their paired time series. To leverage this insight, we propose a plug-and-play framework that transforms text representations into auxiliary variables, seamlessly integrating them into existing time series models. Extensive experiments demonstrate the state-of-the-art performance of our approach."}, {"title": "Appendix", "content": "Roadmap. In this appendix, we provide a detailed overview of our methodology and experimental setup. Appendix A outlines the complete process of frequency analysis for both time series and paired texts. Appendix B includes details on datasets, hyperparameters, evaluation metrics, and additional implementation specifics. Due to space constraints in the main text, Appendix C presents the full experimental results, including comprehensive forecasting and imputation outcomes, hyperparameter and ablation studies, efficiency evaluations, and visualizations. The table of contents is provided below for reference.\nTable of Contents"}, {"title": "A. Detailed Frequency Analysis Process of Time Series with Paired Texts", "content": "Here, we provide a detailed explanation of the frequency analysis process for both the time series and their paired texts.\nProposition A.1. The computation of lag similarity preserves the original periodicities of the data.\nProof. Let \\( S = \\{s_t\\}_{t=1}^{T} \\) represent the paired texts or data sequence, where each \\( s_t \\) corresponds to a time step \\( t \\). Define the lag similarity at lag \\( k \\) as:\n\\[\\text{LagSim}(k) = \\frac{1}{T-k} \\sum_{t=1}^{T-k} \\text{sim}(s_t, s_{t+k}),\\qquad(8)\\]\nwhere \\( \\text{sim}(\\cdot,\\cdot) \\) is a similarity measure (e.g., cosine similarity).\nNow, consider the periodic component of the data sequence \\( S \\), which can be represented as:\n\\[s_t = A \\text{cos} (\\frac{2\\pi t}{P} + \\phi),\\qquad(9)\\]\nwhere \\( A \\) is the amplitude, \\( P \\) is the period, and \\( \\phi \\) is the phase.\nFor two points separated by lag \\( k \\), the similarity \\( \\text{sim}(s_t, s_{t+k}) \\) depends on the relative difference between their phases:\n\\[s_{t+k} = A \\text{cos} (\\frac{2\\pi (t+k)}{P} + \\phi) = A \\text{cos} (\\frac{2\\pi t}{P} + \\frac{2\\pi k}{P} + \\phi).\\qquad(10)\\]\nThe lag similarity is then computed as:\n\\[\\text{LagSim}(k) = \\frac{1}{T-k} \\sum_{t=1}^{T-k} \\text{sim} \\Big(A \\text{cos} (\\frac{2\\pi t}{P} + \\phi), A \\text{cos} (\\frac{2\\pi t}{P} + \\frac{2\\pi k}{P} + \\phi) \\Big).\\qquad(11)\\]\nSince the cosine function is periodic with period \\( P \\), the similarity \\( \\text{sim}(s_t, s_{t+k}) \\) also inherits this periodicity. Therefore, the overall lag similarity \\( \\text{LagSim}(k) \\) retains the periodicities of the original sequence \\( S \\).\nThus, the computation of lag similarity preserves the original periodicities of the data.\nProposition A.2. The stabilization of a data sequence using first-order differentiation preserves its original periodicities.\nProof. Let \\( S = \\{s_t\\}_{t=1}^{T} \\) represent a data sequence, where \\( s_t \\) is the value at time step \\( t \\). The first-order differentiation of the sequence is defined as:\n\\[\\Delta s_t = s_{t+1} - s_t, \\qquad t = 1, 2, ..., T - 1.\\qquad(12)\\]\nSuppose the sequence \\( S \\) exhibits periodic behavior with period \\( P \\) and can be represented as:\n\\[s_t = A \\text{cos}(\\frac{2\\pi t}{P} + \\phi),\\qquad(13)\\]\nwhere \\( A \\) is the amplitude, \\( P \\) is the period, and \\( \\phi \\) is the phase.\nThe first-order difference of \\( s_t \\) is:\n\\[\\Delta s_t = s_{t+1} - s_t = A \\text{cos} (\\frac{2\\pi (t+1)}{P} + \\phi) - A \\text{cos} (\\frac{2\\pi t}{P} + \\phi)\\qquad(14)\\]\nUsing the trigonometric identity for the difference of cosines:\n\\[\\text{cos}(x + y) - \\text{cos}(x) = -2 \\text{sin}(\\frac{y}{2}) \\text{sin}(x + \\frac{y}{2}),\\qquad(15)\\]"}, {"title": "3.1. Code and Reproducibility", "content": "The code for the experiments is included in the supplementary material, accompanied by a comprehensive README file. We provide detailed commands, scripts, and instructions to facilitate running the code. Additionally, the datasets used in the experiments are provided in the supplementary material as CSV files."}, {"title": "3.2. Hardware and environment", "content": "We conducted all experiments on an Ubuntu 22.04 machine equipped with an Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz, 1.5TB of RAM, and a 32GB NVIDIA V100 GPU. The CUDA version used was 12.4. All algorithms were implemented in Python (version 3.11.11). To run our code, users must install several commonly used libraries, including pandas, scikit-learn, patool, tqdm, sktime, matplotlib, transformers, and others. Detailed installation instructions can be found in the README file within the code directory. We have optimized our code to ensure efficiency. Our tests confirmed that the CPU memory usage remains below 16 GB, while the GPU memory usage is under 20 GB. Additionally, the execution time for a single experiment is less than 10 minutes on our machine."}, {"title": "1. Full Forecasting Performance Comparison Visualization", "content": "To provide a comprehensive comparison of different frameworks for modeling time series with paired texts, we visualize the forecasting performance using radar plots in Figure C.1. Each subfigure corresponds to a dataset, with each axis representing a different time series model. The axes are inverted, where values closer to the center indicate worse performance, and larger areas signify better results. The results demonstrate that TaTS consistently outperforms both baselines across all datasets while maintaining compatibility with various time series models."}, {"title": "2. Full Forecasting Results", "content": "Due to space limitations, we provide the full results of the time series forecasting task on paired time series and text in the appendix. We conduct extensive experiments across 9 datasets using 9 existing time series models, evaluating various prediction lengths as detailed in Table 4. The complete results are presented from Table 6 to Table 14. Overall, TaTS consistently achieves the best performance across all datasets, time series models, and prediction lengths. The averaged results across all prediction lengths are summarized in the main text (Table 2). For better readability, we also visualize the performance of different frameworks using radar plots, as detailed in Appendix 1 and Figure 10."}, {"title": "4. Appendix C.5.1 Hyperparameter Sensitivity", "content": "We evaluate the impact of varying the learning rate in {0.00005, 0.0001, 0.00015, 0.0002, 0.00025, 0.0003} by reporting the mean squared error (MSE) of our TaTS framework across datasets. The results demonstrate that TaTS maintains stable performance across different learning rate choices."}, {"title": "4.7 Full Ablation Study Results Using Different Text Encoders", "content": "We conduct experiments to evaluate the performance of our TaTS with multiple language encoders. Specifically, we evaluation TaTS with BERT, GPT2 and LLaMA2 as the language encoders. The results, presented in Figure 13, demonstrate that TaTS remains robust across different text encoders and consistently outperforms the baselines."}, {"title": "4.4.4 Results and discussion", "content": "We analyze the frequency components of time series data, allowing us to identify its dominant periodic components, as illustrated by the blue curves in Figure 2. Detailed process is provided in the text as the implementation details."}, {"title": "3.4 Full Efficiency Results: Computational Overhead vs. Performance Gain Trade-offs", "content": "We conduct experiments to analyze the efficiency of TaTS, with results presented in Figure 14. Each subfigure visualizes the training time per epoch and the forecasting mean squared error (MSE) for different time series models, represented as transparent colored scatter points. The average performance is computed and marked with cross markers: the green cross represents the average performance of TaTS, while the red and blue crosses indicate the average performance of the baseline models.As TaTS introduces a lightweight MLP and augments the original time series with auxiliary variables projected from paired texts, it incurs a slight computational overhead, with an average increase of ~ 8%. Yet this trade-off results in a ~ 14% average improvement of forecasting MSE."}]}