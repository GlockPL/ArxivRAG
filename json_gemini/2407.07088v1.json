{"title": "Safe and Reliable Training of Learning-Based Aerospace Controllers", "authors": ["Udayan Mandal", "Guy Amir", "Haoze Wu", "Ieva Daukantas", "Fletcher Lee Newell", "Umberto Ravaioli", "Baoluo Meng", "Michael Durling", "Kerianne Hobbs", "Milan Ganai", "Tobey Shim", "Guy Katz", "Clark Barrett"], "abstract": "In recent years, deep reinforcement learning (DRL) approaches have generated highly successful controllers for a myriad of complex domains. However, the opaque nature of these models limits their applicability in aerospace systems and sasfety-critical domains, in which a single mistake can have dire consequences. In this paper, we present novel advancements in both the training and verification of DRL controllers, which can help ensure their safe behavior. We showcase a design-for-verification approach utilizing k-induction and demonstrate its use in verifying liveness properties. In addition, we also give a brief overview of neural Lyapunov Barrier certificates and summarize their capabilities on a case study. Finally, we describe several other novel reachability-based approaches which, despite failing to provide guarantees of interest, could be effective for verification of other DRL systems, and could be of further interest to the community.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep reinforcement learning (DRL) has gained significant popularity in recent years, reaching state-of-the-art performance in various domains. One such domain is aerospace systems, in which DRL models are under consideration for replacing years-old software by learning to efficiently control airborne platforms and spacecraft. However, although they perform well empirically, DRL systems have an opaque decision-making process, making them challenging to reason about. More importantly, this opacity raises critical questions about safety and security (e.g., How can we ensure that the spacecraft will never violate a velocity constraint? Will it always reach its destination?) which are difficult to answer. These reliability concerns are a significant obstacle to deploying DRL controllers in real-world systems, where even a single mistake cannot be tolerated.\nTo cope with this urgent need, a myriad of DRL training techniques have been put forth in recent years to enhance the performance of such systems. However, these current approaches suffer from two main drawbacks: (i) they are usually not geared towards improving safety and reliability (which is key in aerospace systems); and (ii) they are heuristic in nature and do not afford any formal guarantees. At the same time, the formal methods community has been developing methods for formally and rigorously assessing the reliability of DRL systems. However, although such methods are useful for identifying whether a system is safe, they are typically not incorporated into the DRL training process, but are rather used only afterwards.\nIn this work, we begin bridging this gap by proposing a novel design-for-verification approach that can be incorporated during the DRL training process. Our approach both modifies the training loop to be more verification-friendly and also utilizes formal verification (in our case, k-induction), to ensure the correctness of the training. We also report a summary of our recent efforts to use Neural Lyapunov Barrier certificates [26] to generate DRL agents that not only perform well on large batches of data, but also meet rigorous correctness criteria as measured by state-of-the-art verification tools.\nFinally, we introduce additional novel reachability-based approaches for providing safety and liveness guarantees about a DRL system. These approaches are derived from prior work on backward-tube reachability, forward-tube reachability, and abstraction-based reachability methods. Moreover, these ap-proaches all follow a similar paradigm: the reachable space covered by all possible paths from the starting state space is over-approximated using a verification engine, and safety and"}, {"title": "II. PRELIMINARIES AND RELATED WORK", "content": "In this paper, we are interested in obtaining DRL controllers that satisfy safety and liveness properties [2] in discrete-time settings.\nSafety. In a sequence satisfying a safety property, a bad state is never reached. For the set of system states X, let \\tau\\subseteq X^* be the set of potential system trajectories. We say a trajectory \\alpha satisfies safety property $P_1$ if and only if each state in \\alpha satisfies property $P_1$. More formally:\n$\\forall \\alpha: \\alpha \\in \\tau, x \\in \\alpha, x \\models P_1$.\n(1)\nFinite-length trajectories terminating in a \"bad\" state (where $P_1$ does not hold) constitute the set of trajectories in violation of the safety property.\nLiveness. On the other hand, a liveness property indicates a good state is eventually reached. A liveness property $P_2$ is satisfied by trajectory \\alpha if and only if there exists a state x in \\alpha where $P_2$ holds. Defining $\\tau_{\\infty}$ as the set of infinite-length trajectories, we formally specify liveness property $P_2$ as:\n$\\forall \\alpha: \\alpha \\in \\tau_{\\infty}, \\exists x \\in \\alpha, x \\models P_2$.\n(2)\nInfinite-length trajectories which contain no \"good\" states (i.e., no states where $P_2$ holds) constitute the set of trajectories in violation of the liveness property."}, {"title": "B. DNNs, DNN Verification, and Dynamical Systems.", "content": "Deep Learning. Deep neural networks (DNNs) consist of layers of neurons that perform some (usually nonlinear) trans-formation of the input [38]. In this paper, we investigate deep reinforcement learning (DRL), where we train a DNN to obtain a policy, which maps states to actions that control a system [54].\nDNN Verification. Given (i) a trained DNN (e.g., a DRL agent) N; (ii) a pre-condition P on the DNN's inputs, limiting the input assignments; and (iii) a post-condition Q on the DNN's outputs, the goal of DNN verification is to determine whether the property $P(x) \\rightarrow Q(N(x))$ holds for any neural network input x. In many DNN verifiers (a.k.a., verification engines), this task is equivalently reduced to determining the satisfiability of the formula $P(x) \\wedge \\neg Q(N(x))$. If the formula is satisfiable (SAT), then there is an input that satisfies the pre-condition and violates the post-condition, which means the property is violated. On the other hand, if the formula is unsatisfiable (UNSAT), then the property holds. It has been shown [49] that verification of piece-wise-linear DNNs is NP-complete. In re-cent years, the formal methods community has put forth various techniques for verifying and improving DNN reliability [1], [5], [6], [9], [13], [17], [23], [70]. These techniques include SMT-based methods [8], [45], [50], [52], optimization-based methods [15], [30], [55], [68], methods based on abstraction-refinement [10], [22], [31], [32], [58], [59], [65], methods based on shielding [24], [51], [63], and more.\nDiscrete-Time Dynamical Systems. We consider discrete-time dynamical systems, particularly systems whose trajectories satisfy the equation:\n$x_{t+1} = f(x_t, u_t)$,\n(3)\nin which the transition function f takes as inputs the current state $x_t \\in X$ and a control $u_t \\in U$ and produces as output the subsequent state $x_{t+1}$. To control these systems, we employ a policy $\\pi : X \\rightarrow U$ that takes in a state $x \\in X$ and outputs a control action $u = \\pi(x)$. In DRL, the controller $\\pi$ is realized by a trained DNN agent. These learning-based controllers have proven to be effective in many real-world settings including robotics [26], biomedical systems [28], and energy management [44], due to their expressive power and ability to generalize to unseen, complex environments [67]."}, {"title": "C. Control Lyapunov Barrier Functions", "content": "The problem of verifying safety and liveness properties in a dynamical system can be solved by finding a function $V: X \\rightarrow \\mathbb{R}$ with certain properties. Control theory identifies two fundamental types of functions [53].\nLyapunov Functions. Lyapunov functions, a.k.a., Control Lyapunov functions, capture the energy level at a particular state: over time, energy is dissipated along a trajectory until the system attains zero-energy equilibrium [41]. Lyapunov functions can guarantee asymptotic stability, which ensures the system eventually converges to some goal state (thereby"}, {"title": "Barrier Functions.", "content": "Barrier functions [4], a.k.a., Control Bar-rier Functions, guarantee that a system never enters an unsafe region (i.e., a \"bad\" state) in the state space. This is achieved by setting the function value to be above some threshold for unsafe states and then verifying that the system can never transition to a state where the function is above the threshold [3], [12], [72]. Previous work [60], [61], [69], [75] demonstrates how to obtain Barrier functions for various safety-critical tasks such as pedestrian avoidance, neural radiance field-based obstacle navigation [57], and multi-agent control.\nControl Lyapunov Barrier Functions. Often, it is necessary to ensure both safety and liveness properties simultaneously. In such cases, we can employ a Control Lyapunov Barrier Function (CLBF), which integrates the properties and guaran-tees of both Control Lyapunov functions and Control Barrier functions [27]. CLBFs can solve reach-while-avoid tasks [29], which we discuss next.\nReach-while-Avoid Tasks. The goal of Reach-while-Avoid (RWA) tasks is to find a controller $\\pi$ for a dynamical system such that every trajectory {x1,x2...} produced under this con-troller (i) never enters an unsafe (\"bad\") state; and (ii) eventually enters a goal (\"good\") region or state. We can formally define the problem as:\nDefinition 1 (Reach-while-Avoid Task).\nInput: A dynamical system with a set of initial states X1 \u2286 X, a set of goal states XG \u2286 X, and a set of unsafe states XU \u2286 X, where X1 \u2229 XU = \u2205 and XG \u2229 XU = \u2205\nOutput: A controller \u03c0 such that for every trajectory \u03c4 = {x1,x2...} satisfying x1 \u2208 X1:\n1) Reach: \u2203 t\u2208 N. xt \u2208 XG\n2) Avoid: \u2200 t\u2208 N. xt \u2209 XU\nSome solutions for RWA tasks rely on control theoretic principles. The approach in [27] trains Lyapunov and Bar-rier certificates to solve RWA tasks. Hamilton-Jacobi (HJ) reachability-based methods [11]) have also been employed to solve RWA tasks [34], [43], [66]. Safe DRL is closely connected to RWA, with its goal being to maximize cumulative rewards while minimizing costs along a trajectory [14]. It has been solved with both Lyapunov/Barrier methods [20], [73] and HJ reachability methods [35], [74]."}, {"title": "D. Other Verification Approaches", "content": "Reachability Analysis. Reachability analysis methods aim to define and compute the set of final reachable states and then verify that this set (i) does not include any bad states, and (ii) is contained within the goal region. Reachability methods include forward-tube and backward-tube verification [40], which either propagate states forward from the starting set or backward from the goal set. Other related work in reachability analysis includes hybrid system verifiers [46], growing the set of reachable states over a discrete action space [48], approximating reachable states during forward and backward reachability [39], and reformulating the dynamics of a system for easier reachability verification [37].\nBounded Model Checking and k-induction. Bounded model checking uses a symbolic analysis over k copies of a system to check whether a bad state is reachable in k or fewer steps from the starting set of states. k-induction is similar, except that it starts from an arbitrary state and can thus be used to prove that a bad state is never reached. Bounded model checking has been explored in the WhiRL tool [33] using the neural network verifier Marabou [50], [71]. [64] implements another tool for checking adversarial cases and coverage using bounded model checking for artificial neural networks. WhiRL 2.0 [7] adds k-induction capabilities to WhiRL.\nDesign-for-Verification. Design-for-verification broadly en-compasses any method which aims to modify the design and training process to make verification easier. The Trainify frame-work [47] uses a CEGAR-based approach to grow an easily verifiable state space by repeatedly retraining the DRL system. [25] motivates an optimized DRL training approach to reduce the number of safety violations, easing formal verification. This approach was also implemented in Marabou [50], [71]."}, {"title": "III. 2D DOCKING PROBLEM", "content": "We adopt as a motivating case study benchmark the 2D docking problem presented in [62]. The goal is to train a DRL controller to safely navigate a deputy spacecraft to a chief spacecraft within two-dimensional space. The reference frame is defined such that the chief spacecraft is always at the origin (0,0). The state of the deputy spacecraft is x = [x,y,x,y], where (x,y) are the position of the spacecraft and ($\\dot{x}, \\dot{y}$) are the respective directional velocities.\nDynamics\nThe system dynamics are defined according to the linearly-approximate Clohessy-Wiltshire relative orbital motion equa-tions in a non-inertial Hill's reference frame [21], [42]. The control input to the system is u = [Fx, Fy], where Fx and Fy are the thrust forces applied to the deputy spacecraft in the x and y directions. We follow [62], setting the deputy spacecraft mass to m = 12 kg and the mean motion to n = 0.001027 rad/s. The continuous time state dynamics of the system are given by the following differential equations:\n$\\dot{x} = [x, \\dot{y}, \\dot{x}, \\ddot{y}]$\n(4)\n$\\ddot{x} = 2n\\dot{y} + 3n^2x + \\frac{Fx}{m}$\n(5)\n$\\ddot{y} = -2n\\dot{x} + \\ddot{y} + \\frac{Fy}{m}$\n(6)\nIntegration using a discrete time step T yields a closed-form next-state function. Given a state x = [x, y, x, y] and control in-puts u = [Fx, Fy], the spacecraft's next state x' = [x', y', x', \u00ff'] after an elapsed time T is:"}, {"title": "B. Liveness - Docking Region", "content": "The problem as given in [62] defines a docking region which is a circle of radius 0.5 meters centered at the origin. The goal is for the deputy spacecraft to eventually enter this region. To simplify the verification query, it is easier to use linear bounds for the goal region, so we use a square centered at the origin with sides parallel to the axes of length 0.7 meters (note that this square fits inside the docking region of [62]). Formally, our liveness condition is:\n$\\forall \\alpha: \\alpha \\in \\tau_{\\infty}, \\exists t. |\\alpha_{t.x}| \\leq 0.35 \\wedge |\\alpha_{t.y}| \\leq 0.35$,\n(11)\nwhere \\alpha_t is the state at time t in trajectory \\alpha, and $\\alpha_{t.x}$ and $\\alpha_{t.y}$ are the x and y components of $\\alpha_t$.\nSafety - Velocity Threshold\nTo minimize the risk to both spacecraft, a safety constraint is imposed on the magnitude of the velocity of the deputy spacecraft. The constraint depends on the distance from the deputy. Formally, [62] requires the following state invariant:\n$\\sqrt{\\dot{x}^2 + \\dot{y}^2} \\leq 0.2 + 2n\\sqrt{x^2 + y^2}$\n(12)\nWe therefore define the unsafe region to be the negation of (12).\nAgain, we desire to instead use a linear constraint in order to be compatible with our formal tools. We use the Euclidean norm approximation of [16], which approximates the norm by projecting it onto vectors in all different directions and taking the one with the maximum magnitude. We use the two inequalities:\n$max_{i \\in [1, n_{directions}]} (u_1 \\cdot cos(\\frac{2(i-1)\\pi}{n_{directions}}) + u_2 \\cdot sin(\\frac{2(i-1)\\pi}{n_{directions}})) \\leq \\sqrt{u_1^2 + u_2^2}$\n(13)\n$\\frac{1}{cos(\\pi/n_{directions})} max_{i \\in [1, n_{directions}]} (u_1 \\cdot cos(\\frac{2(i-1)\\pi}{n_{directions}}) + u_2 \\cdot sin(\\frac{2(i-1)\\pi}{n_{directions}})) \\geq \\sqrt{u_1^2 + u_2^2}$,\n(14)"}, {"title": "IV. USING k-INDUCTION FOR LIVENESS GUARANTEES", "content": "In this section, we present an approach for scalably verifying a liveness property for the 2D docking problem presented in Section III using k-induction. We describe the conceptual approach, the experimental framework, and the results.\nProving Liveness by k-induction\nIn order to apply k-induction, we must find a way to reduce a liveness property to a k-inductive property. Typically, this is done by finding a ranking function, a function with a well-founded co-domain, which can be shown to always be decreasing by k-induction.\nFor the spacecraft, an obvious choice for a ranking function is the distance from the deputy to the chief. In order to make the function easier to reason about, we use a linear proxy function for the actual distance, namely the Manhattan distance. Unfortunately, it is not the case that this measure always decreases, as the spacecraft may move away from the target.\nThus, we instead propose a property that ensures the space-craft eventually starts moving towards the target. The property is expressed as a logical disjunction: after k steps, either the Manhattan distance decreases or the magnitude of the velocity decreases. Again, we approximate the velocity magnitude by the L\u00b9 norm, the sum of the absolute values of \u017c and \u00fd. Formally, if the current state is (x0,yo, 20, yo) and the future state after k steps is (x', y', 'x', \u00ff'), we must show:\n$(|x'|+|y'])-(|xo|+|yo|)<-\\epsilon \\vee (|\\dot{x'}|+|\\dot{y'}|)-(|\\dot{x0}|+|\\dot{y0}|) < -\\epsilon$,\n(19)\nProposition 1. If property (19) holds (for some k) for every state, then eventually the spacecraft will be moving towards the goal (i.e., the L\u00b9 norm of the position will decrease).\nProof. Suppose that from some starting state, (xo, Yo, Xo, Yo), the spacecraft follows a trajectory that never moves towards the goal in the sense that the L\u00b9 norm never decreases. Let (Xi, Yi, Xi, Yi) be the state after i time steps. This means that for all i, xi + Yi \u2264 |Xi+1| + |Yi+1]. Let Vi = |xi| + |Yi. By (19), we know that for each Vi, there must be some k, such that Vi+k - Vi < -6. Thus, for any n, we can construct a sequence Vjo, Vj1, Vj2,... Vin such that jo = 0 and Vj\u2081 \u2013 Vji+1 > 6. If we"}, {"title": "A. RWA Certificates", "content": "Definition 2. A function V: X \u2192 R is an RWA certificate for the task defined in Definition 1 if there exist some \u03b1 > \u03b2 \u2265 \u03b3 and \u03b5 > 0, such that the following constraints are satisfied.\n$\\forall x \\in X_1. \\quad V(x) \\leq \\beta$\n$\\forall x \\in X \\backslash X_G. \\quad V(x) \\leq \\beta \\rightarrow V(x) - V(f(x, \\pi(x))) \\geq \\epsilon$\n$\\forall x \\in X_U. \\quad V(x) \\geq \\alpha$\nAny tuple of values (\u03b1, \u03b2, \u03b5, \u03b3) for which these conditions hold is called a witness for the certificate. RWA certificates provide the following guarantee.\nLemma 1. If V is an RWA certificate for a dynamical system with witness (\u03b1,\u03b2,\u03b5,\u03b3), then for every trajectory \u03c4 starting from a state $x \\in X \\backslash X_G$ such that V(x) \u2264 \u03b2, \u03c4 will eventually contain a state in XG without ever passing through a state in XU.\nWe use reinforcement learning to jointly train neural networks for both the controller and the corresponding RWA certificate. RWA Training Loss. The training objective for RWA certifi-cates is described below:\n$O_s = C_s \\sum_{i, x_i \\in X_I} ReLU(\\delta_1 + V(x_i) - \\beta)$\n$O_d = C_d \\sum_{i, x_i \\in X \\backslash (X_v \\cup X_G), V(x_i)<\\beta} ReLU(\\delta_2 + \\epsilon + V(x) - V(x_i))$\n$O_u = C_u \\sum_{i, x_i \\in X_U} ReLU(\\delta_3 - V(x_i) + \\alpha)$\n$O = O_s + O_d + O_u$\nEquation (27) penalizes deviations from constraint (24), Equation (28) penalizes deviations from constraint (25), and Equation (29) penalizes deviations from constraint (26). We incorporate parameters $\\delta_1 > 0$, $\\delta_2 > 0$, and $\\delta_3 > 0$, which can"}, {"title": "B. Reachability Analysis Approaches", "content": "In this subsection, we discuss approaches based on reacha-bility analysis. While these approaches were ultimately unsuc-cessful on the case study problem outlined in section III, we still mention them here, as the reasons for their failure may be of interest, and they may be useful on other problems.\nForward-tube and Backward-tube Reachability. Forward-tube and backward-tube reachability attempt to generate a path over abstract state spaces (i.e., sets of states) from the starting state space to the goal state space. At each step along the abstract path, we check that every state in the abstract state set meets any safety guarantees.\nIn forward-tube reachability, a starting set of states X and step size k is defined. Then, a set of states X is constructed such that all states reachable from X in k steps are contained within X. This process is continued, and additional sets of states X+1 are constructed, each with the property that they contain the states reachable from X in k steps. If at some point, the constructed set is a subset of the goal region, then the liveness property is ensured. However, it can be very chal-lenging to find a sequence of sets of states X that eventually lead to a subset of the goal region. This was the case for the spacecraft example.\nOn the other hand, in backward-tube reachability, we start with X set equal to the goal states and define a step size k. Then, a set of states X is constructed such that all states reachable from Xh in k steps are contained within X. Again, this process can be repeated until the set of states includes the initial states. A difficulty with this approach is computing a sufficiently large previous set of states at each step.\nGrid Reachability. Grid reachability is a process which first partitions a bounded subset of the state space into cells, then computes a directed graph where each cell is a vertex, and each directed edge (a,b) denotes that vertex b is reachable from vertex a in k steps, for a specific k, as shown in Fig. 2. The goal is to show that for all paths constructed from cells in the defined initial state space, a goal region reachable. However, to ensure liveness, it is also necessary to show that the graph has"}, {"title": "VI. CONCLUSION", "content": "We have presented methods for verifying safety and liveness properties for DRL systems using k-induction, Neural Lyapunov Barrier Certificates, and reachability analysis. We explore their effectiveness on a 2D spacecraft docking problem posed in previous work. For this problem, we show how a k-induction based approach can be used alongside a design-for-verification training scheme to provide liveness guarantees. We also discuss how Neural Lyapunov Barrier Certificates can be used to provide both liveness and safety guarantees. While reachability analysis ultimately did not provide any formal guarantees, we discuss the approach and its limitations. In future work, we plan to explore scaling these methods to more complex and realistic control systems."}]}