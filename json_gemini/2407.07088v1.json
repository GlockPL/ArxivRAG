{"title": "Safe and Reliable Training of Learning-Based Aerospace Controllers", "authors": ["Udayan Mandal", "Guy Amir", "Haoze Wu", "Ieva Daukantas", "Fletcher Lee Newell", "Umberto Ravaioli", "Baoluo Meng", "Michael Durling", "Kerianne Hobbs", "Milan Ganai", "Tobey Shim", "Guy Katz", "Clark Barrett"], "abstract": "In recent years, deep reinforcement learning (DRL) approaches have generated highly successful controllers for a myriad of complex domains. However, the opaque nature of these models limits their applicability in aerospace systems and sasfety-critical domains, in which a single mistake can have dire consequences. In this paper, we present novel advancements in both the training and verification of DRL controllers, which can help ensure their safe behavior. We showcase a design-for-verification approach utilizing k-induction and demonstrate its use in verifying liveness properties. In addition, we also give a brief overview of neural Lyapunov Barrier certificates and summarize their capabilities on a case study. Finally, we describe several other novel reachability-based approaches which, despite failing to provide guarantees of interest, could be effective for verification of other DRL systems, and could be of further interest to the community.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep reinforcement learning (DRL) has gained significant popularity in recent years, reaching state-of-the-art performance in various domains. One such domain is aerospace systems, in which DRL models are under consideration for replacing years-old software by learning to efficiently control airborne platforms and spacecraft. However, although they perform well empirically, DRL systems have an opaque decision-making process, making them challenging to reason about. More importantly, this opacity raises critical questions about safety and security (e.g., How can we ensure that the spacecraft will never violate a velocity constraint? Will it always reach its destination?) which are difficult to answer. These reliability concerns are a significant obstacle to deploying DRL controllers in real-world systems, where even a single mistake cannot be tolerated.\nTo cope with this urgent need, a myriad of DRL training techniques have been put forth in recent years to enhance the performance of such systems. However, these current approaches suffer from two main drawbacks: (i) they are usually not geared towards improving safety and reliability (which is key in aerospace systems); and (ii) they are heuristic in nature and do not afford any formal guarantees. At the same time, the formal methods community has been developing methods for formally and rigorously assessing the reliability of DRL systems. However, although such methods are useful for identifying whether a system is safe, they are typically not incorporated into the DRL training process, but are rather used only afterwards.\nIn this work, we begin bridging this gap by proposing a novel design-for-verification approach that can be incorporated during the DRL training process. Our approach both modifies the training loop to be more verification-friendly and also utilizes formal verification (in our case, k-induction), to ensure the correctness of the training. We also report a summary of our recent efforts to use Neural Lyapunov Barrier certificates [26] to generate DRL agents that not only perform well on large batches of data, but also meet rigorous correctness criteria as measured by state-of-the-art verification tools.\nFinally, we introduce additional novel reachability-based approaches for providing safety and liveness guarantees about a DRL system. These approaches are derived from prior work on backward-tube reachability, forward-tube reachability, and abstraction-based reachability methods. Moreover, these approaches all follow a similar paradigm: the reachable space covered by all possible paths from the starting state space is over-approximated using a verification engine, and safety and liveness properties are checked over this over-approximated state space.\nTo demonstrate the usefulness of our approaches, we apply them to a benchmark satellite-control model developed in collaboration with industry partners (GE Aerospace Research and the U.S. Air Force). We demonstrate that liveness can be verified using our k-induction approach. Additionally, as a point of comparison, we showcase that the certificate-based approach is indeed able to generate a controller that provably behaves safely. Notably, the problem setting and controller complexity are beyond that acheived in previous work on formally verified controllers.\nThe other reachability-based methods fail on this benchmark. However, we believe that these failed attempts: (i) demonstrate the merits of our successful approaches in handling complex, nontrivial properties; (ii) can be of value to the community, by shedding light on vulnerabilities of alternate methods; and (iii) could be potentially successful when applied over different DRL systems.\nWe view this work as an important step towards the safe and reliable deployment of DRL controllers in real-world systems, especially in the complex domain of avionics. We additionally hope that our work will further motivate additional research in neural network verification, DRL safety, and specifically, their role in the important domain of DRL-controlled aerospace systems.\nThe rest of the paper is organized as follows. In Sec. II, we cover background on deep learning, DRL, and verification, and we also introduce Neural Lyapunov Barrier functions. In Sec. III, we introduce our benchmark problem, a 2D spacecraft docking challenge. We subsequently introduce our k-induction technique in Sec. IV, and we present alternative verification approaches in Sec. V."}, {"title": "II. PRELIMINARIES AND RELATED WORK", "content": "In this paper, we are interested in obtaining DRL controllers that satisfy safety and liveness properties [2] in discrete-time settings.\nIn a sequence satisfying a safety property, a bad state is never reached. For the set of system states $\\mathcal{X}$, let $\\tau \\subseteq \\mathcal{X}^*$ be the set of potential system trajectories. We say a trajectory $\\alpha$ satisfies safety property $P_1$ if and only if each state in $\\alpha$ satisfies property $P_1$. More formally:\n$\\forall \\alpha: \\alpha \\epsilon \\tau. \\forall x \\epsilon \\alpha. x \\models P_1$.\nFinite-length trajectories terminating in a \"bad\" state (where $P_1$ does not hold) constitute the set of trajectories in violation of the safety property.\nOn the other hand, a liveness property indicates a good state is eventually reached. A liveness property $P_2$ is satisfied by trajectory $\\alpha$ if and only if there exists a state $x$ in $\\alpha$ where $P_2$ holds. Defining $\\tau^{\\infty}$ as the set of infinite-length trajectories, we formally specify liveness property $P_2$ as:\n$\\forall \\alpha: \\alpha \\epsilon \\tau^{\\infty}. \\exists x \\epsilon \\alpha. x \\models P_2$.\nInfinite-length trajectories which contain no \"good\" states (i.e., no states where $P_2$ holds) constitute the set of trajectories in violation of the liveness property."}, {"title": "B. DNNs, DNN Verification, and Dynamical Systems.", "content": "Deep neural networks (DNNs) consist of layers of neurons that perform some (usually nonlinear) trans-formation of the input [38]. In this paper, we investigate deep reinforcement learning (DRL), where we train a DNN to obtain a policy, which maps states to actions that control a system [54].\nGiven (i) a trained DNN (e.g., a DRL agent) $N$; (ii) a pre-condition $P$ on the DNN's inputs, limiting the input assignments; and (iii) a post-condition $Q$ on the DNN's outputs, the goal of DNN verification is to determine whether the property $P(x) \\rightarrow Q(N(x))$ holds for any neural network input $x$. In many DNN verifiers (a.k.a., verification engines), this task is equivalently reduced to determining the satisfiability of the formula $P(x) \\wedge \\neg Q(N(x))$. If the formula is satisfiable (SAT), then there is an input that satisfies the pre-condition and violates the post-condition, which means the property is violated. On the other hand, if the formula is unsatisfiable (UNSAT), then the property holds. It has been shown [49] that verification of piece-wise-linear DNNs is NP-complete. In re-cent years, the formal methods community has put forth various techniques for verifying and improving DNN reliability [1], [5], [6], [9], [13], [17], [23], [70]. These techniques include SMT-based methods [8], [45], [50], [52], optimization-based methods [15], [30], [55], [68], methods based on abstraction-refinement [10], [22], [31], [32], [58], [59], [65], methods based on shielding [24], [51], [63], and more.\nWe consider discrete-time dynamical systems, particularly systems whose trajectories satisfy the equation:\n$x_{t+1} = f(x_t,u_t)$,\nin which the transition function $f$ takes as inputs the current state $x_t \\in \\mathcal{X}$ and a control $u_t \\in \\mathcal{U}$ and produces as output the subsequent state $x_{t+1}$. To control these systems, we employ a policy $\\pi:\\mathcal{X} \\rightarrow \\mathcal{U}$ that takes in a state $x \\in \\mathcal{X}$ and outputs a control action $u = \\pi(x)$. In DRL, the controller $\\pi$is realized by a trained DNN agent. These learning-based controllers have proven to be effective in many real-world settings including robotics [26], biomedical systems [28], and energy management [44], due to their expressive power and ability to generalize to unseen, complex environments [67]."}, {"title": "C. Control Lyapunov Barrier Functions", "content": "The problem of verifying safety and liveness properties in a dynamical system can be solved by finding a function $V:\\mathcal{X}\\rightarrow \\mathbb{R}$ with certain properties. Control theory identifies two fundamental types of functions [53].\nLyapunov functions, a.k.a., Control Lyapunov functions, capture the energy level at a particular state: over time, energy is dissipated along a trajectory until the system attains zero-energy equilibrium [41]. Lyapunov functions can guarantee asymptotic stability, which ensures the system eventually converges to some goal state (thereby satisfying a liveness property). Lyapunov functions must be (i) equal to 0 at equilibrium, (ii) strictly positive at all other states; and (iii) monotonically decreasing [18], [19], [36].\nBarrier functions [4], a.k.a., Control Bar-rier Functions, guarantee that a system never enters an unsafe region (i.e., a \"bad\" state) in the state space. This is achieved by setting the function value to be above some threshold for unsafe states and then verifying that the system can never transition to a state where the function is above the threshold [3], [12], [72]. Previous work [60], [61], [69], [75] demonstrates how to obtain Barrier functions for various safety-critical tasks such as pedestrian avoidance, neural radiance field-based obstacle navigation [57], and multi-agent control.\nOften, it is necessary to ensure both safety and liveness properties simultaneously. In such cases, we can employ a Control Lyapunov Barrier Function (CLBF), which integrates the properties and guaran-tees of both Control Lyapunov functions and Control Barrier functions [27]. CLBFs can solve reach-while-avoid tasks [29], which we discuss next.\nThe goal of Reach-while-Avoid (RWA) tasks is to find a controller $\\pi$for a dynamical system such that every trajectory ${x_1,x_2,...}$ produced under this con-troller (i) never enters an unsafe (\"bad\") state; and (ii) eventually enters a goal (\"good\") region or state. We can formally define the problem as:"}, {"title": "Definition 1 (Reach-while-Avoid Task).", "content": "Input: A dynamical system with a set of initial states $\\mathcal{X}_1 \\subseteq \\mathcal{X}$, a set of goal states $\\mathcal{X}_G \\subseteq \\mathcal{X}$, and a set of unsafe states $\\mathcal{X}_U \\subseteq \\mathcal{X}$, where $\\mathcal{X}_1 \\cap \\mathcal{X}_U = \\O$ and $\\mathcal{X}_G \\cap \\mathcal{X}_U = \\O$\nOutput: A controller $\\pi$ such that for every trajectory $\\tau = {x_1,x_2,...}$ satisfying $x_1 \\in \\mathcal{X}_1$:\n1) Reach: $\\exists t \\epsilon \\mathbb{N}. x_t \\in \\mathcal{X}_G$\n2) Avoid: $\\forall t\\epsilon \\mathbb{N}. X_t \\not\\in \\mathcal{X}_U$\nSome solutions for RWA tasks rely on control theoretic principles. The approach in [27] trains Lyapunov and Bar-rier certificates to solve RWA tasks. Hamilton-Jacobi (HJ) reachability-based methods [11]) have also been employed to solve RWA tasks [34], [43], [66]. Safe DRL is closely connected to RWA, with its goal being to maximize cumulative rewards while minimizing costs along a trajectory [14]. It has been solved with both Lyapunov/Barrier methods [20], [73] and HJ reachability methods [35], [74]."}, {"title": "D. Other Verification Approaches", "content": "Reachability analysis methods aim to define and compute the set of final reachable states and then verify that this set (i) does not include any bad states, and (ii) is contained within the goal region. Reachability methods include forward-tube and backward-tube verification [40], which either propagate states forward from the starting set or backward from the goal set. Other related work in reachability analysis includes hybrid system verifiers [46], growing the set of reachable states over a discrete action space [48], approximating reachable states during forward and backward reachability [39], and reformulating the dynamics of a system for easier reachability verification [37].\nuses a symbolic analysis over $k$ copies of a system to check whether a bad state is reachable in $k$ or fewer steps from the starting set of states. k-induction is similar, except that it starts from an arbitrary state and can thus be used to prove that a bad state is never reached. Bounded model checking has been explored in the WhiRL tool [33] using the neural network verifier Marabou [50], [71]. [64] implements another tool for checking adversarial cases and coverage using bounded model checking for artificial neural networks. WhiRL 2.0 [7] adds k-induction capabilities to WhiRL.\nDesign-for-verification broadly en-compasses any method which aims to modify the design and training process to make verification easier. The Trainify frame-work [47] uses a CEGAR-based approach to grow an easily verifiable state space by repeatedly retraining the DRL system. [25] motivates an optimized DRL training approach to reduce the number of safety violations, easing formal verification. This approach was also implemented in Marabou [50], [71]."}, {"title": "III. 2D DOCKING PROBLEM", "content": "We adopt as a motivating case study benchmark the 2D docking problem presented in [62]. The goal is to train a DRL controller to safely navigate a deputy spacecraft to a chief spacecraft within two-dimensional space. The reference frame is defined such that the chief spacecraft is always at the origin (0,0). The state of the deputy spacecraft is $x = [x,y,\\dot{x},\\dot{y}]$, where $(x,y)$ are the position of the spacecraft and $(\\dot{x},\\dot{y})$ are the respective directional velocities."}, {"title": "A. Dynamics", "content": "The system dynamics are defined according to the linearly-approximate Clohessy-Wiltshire relative orbital motion equa-tions in a non-inertial Hill's reference frame [21], [42]. The control input to the system is $u = [F_x, F_y]$, where $F_x$ and $F_y$ are the thrust forces applied to the deputy spacecraft in the $x$ and $y$ directions. We follow [62], setting the deputy spacecraft mass to $m = 12$ kg and the mean motion to $n = 0.001027$ rad/s. The continuous time state dynamics of the system are given by the following differential equations:\n$\\dot{x} = [x, \\dot{y}, \\dot{x}, \\ddot{y}]$\n$\\ddot{x} = 2n\\dot{y} + 3n^2x + \\frac{F_x}{m}$\n$\\ddot{y} = -2n\\dot{x} + \\ddot{y} + \\frac{F_y}{m}$\nIntegration using a discrete time step $T$ yields a closed-form next-state function. Given a state $x = [x, y, \\dot{x}, \\dot{y}]$ and control in-puts $u = [F_x, F_y]$, the spacecraft's next state $x' = [x', y', \\dot{x}', \\ddot{y}']$ after an elapsed time $T$ is:"}, {"title": "B. Liveness", "content": "The problem as given in [62] defines a docking region which is a circle of radius 0.5 meters centered at the origin. The goal is for the deputy spacecraft to eventually enter this region. To simplify the verification query, it is easier to use linear bounds for the goal region, so we use a square centered at the origin with sides parallel to the axes of length 0.7 meters (note that this square fits inside the docking region of [62]). Formally, our liveness condition is:\n$\\forall \\alpha: \\alpha \\epsilon \\tau^{\\infty}. \\exists t. |\\alpha_{t}.x| \\leq 0.35 \\wedge |\\alpha_{t}.y| \\leq 0.35$,\nwhere $\\alpha_t$ is the state at time t in trajectory $\\alpha$, and $\\alpha_{t}.x$ and $\\alpha_{t}.y$ are the x and y components of $\\alpha_t$.\nTo minimize the risk to both spacecraft, a safety constraint is imposed on the magnitude of the velocity of the deputy spacecraft. The constraint depends on the distance from the deputy. Formally, [62] requires the following state invariant:\n$\\sqrt{\\dot{x}^2 + \\dot{y}^2} \\leq 0.2 + 2n\\sqrt{x^2 + y^2}$"}, {"title": "C. Safety Velocity Threshold", "content": "We therefore define the unsafe region to be the negation of (12).\nAgain, we desire to instead use a linear constraint in order to be compatible with our formal tools. We use the Euclidean norm approximation of [16], which approximates the norm by projecting it onto vectors in all different directions and taking the one with the maximum magnitude. We use the two inequalities:\n$\\frac{\\text{max}_{i\\epsilon[1,n \\text{directions}]}(u_1\\cdot \\text{cos}(\\frac{2(i-1)\\pi}{N\\text{directions}})+u_2 \\cdot \\text{sin}(\\frac{2(i-1)\\pi}{N\\text{directions}}))}{cos(\\pi/N\\text{directions})} \\leq \\sqrt{u_1^2 + u_2^2}$\n$\\frac{1}{\\text{cos}(\\pi/N\\text{directions})} \\text{max}_{i\\epsilon[1,n \\text{directions}]}(u_1\\cdot \\text{cos}(\\frac{2(i-1)\\pi}{N\\text{directions}})+u_2 \\cdot \\text{sin}(\\frac{2(i-1)\\pi}{N\\text{directions}})) \\geq \\sqrt{u_1^2 + u_2^2}$,\nwhere $n\\text{ directions}$ is a positive integer. Larger values of $N\\text{directions}$ yield more precise approximations. We can simplify this by noting that:\n$\\sqrt{u_1^2 + u_2^2} = \\sqrt{|u_1|^2 + |u_2|^2}$,\nand then focusing our search only on vectors in the first quadrant. Assuming $n\\text{directions}$ is a multiple of 4, we get:\n$\\text{under}(u_1, u_2) = \\frac{\\text{max}_{i\\epsilon [1,n \\text{ directions}/4 + 1]}(|u_1| \\cdot \\text{cos}(\\frac{2(i-1)\\pi}{N\\text{directions}})+|u_2| \\cdot \\text{sin}(\\frac{2(i-1)\\pi}{N\\text{directions}}))}{cos(\\pi/N\\text{directions})} \\leq \\sqrt{u_1^2 + u_2^2}$\n$\\text{over}(u_1,u_2) = \\frac{1}{\\text{cos}(\\pi/n\\text{directions})} \\text{max}_{i\\epsilon[1,n\\text{directions}/4 + 1]} (u_1\\cdot \\text{cos}(\\frac{2(i-1)\\pi}{n \\text{ directions}})+ u_2\\text{sin}(\\frac{2(i-1)\\pi}{n \\text{ directions}})) \\geq \\sqrt{u_1^2 + u_2^2}$.\nUsing these constraints, we can over-approximate the unsafe region as\n$\\text{over}(\\dot{x}_t, \\dot{y}_t) > 0.2 + 2n \\cdot \\text{under}(x_t, y_t)$.\nThis is a piece-wise linear constraint. Moreover, both the absolute value function and the maximum function can be easily encoded in neural network verification tools such as Marabou. In our experiments, we use $n\\text{directions} = 400$."}, {"title": "D. DNN Setup", "content": "As in [62], we use Ray RLib's Proximal Policy Optimization (PPO) reinforcement learning algorithm to learn the system dynamics, but we make four important alterations to improve downstream verification, part of our design for verification scheme.\nTo improve performance near the docking region, we reduce the docking distance during training from 0.5 meters to 0.25 meters. We also simplify the problem by reducing the initial position of the deputy spacecraft from a radius of 150 meters to only 5 meters. Scaling back up to larger initial positions is part of an ongoing research effort.\nWe limit the observations of the agent to its x and y positions and respective $\\dot{x}$ and $\\dot{y}$ velocities, eliminating the agent's observations of its current speed and the distance-dependent velocity constraint described in Equation 12. This makes it less likely that irregular trajectories will be learned because of observations of the safety constraint. As a result, liveness verification becomes easier.\nWe keep the rewards relating to success or failure, the safety constraint, and delta-v as presented in [62], but we alter the distance change reward to use the $L^1$ norm of the position of the deputy - i.e., the Manhattan distance from the deputy to the chief, rather than the nonlinear $L^2$ norm. This is to match the induction invariant described in Section IV. To account for the new distance metric and previously-described smaller initial distances, we developed a novel reward function for distance change:\n$R^{\\text{new}}_d = 2(e^{-a_1d}-e^{-a_1d_{t-1}})+2(e^{-a_2d}-e^{-a_2d_{t-1}})$, where $d_m = |x_i| + |\\dot{x}_i|$, $A_1 = \\frac{\\ln(2)}{X_i + X_i}$, $A_1 = 2$, and $a_2 = \\frac{\\ln(2)}{0.5}$.\nOur DRL controller should be sufficiently small to keep verification time reasonable and sufficiently large to be able to learn the necessary behavior. We found that reducing the hidden layer widths from 256 neurons to 20 neurons, while maintaining two hidden layers, acheives a good balance between verification time and expressive power. Also, we swap the tanh activation functions for ReLU activation functions since ReLU is supported by most neural network verification tools (such as Marabou)."}, {"title": "IV. USING k-INDUCTION FOR LIVENESS GUARANTEES", "content": "In this section, we present an approach for scalably verifying a liveness property for the 2D docking problem presented in Section III using k-induction. We describe the conceptual approach, the experimental framework, and the results."}, {"title": "A. Proving Liveness by k-induction", "content": "In order to apply k-induction, we must find a way to reduce a liveness property to a k-inductive property. Typically, this is done by finding a ranking function, a function with a well-founded co-domain, which can be shown to always be decreasing by k-induction.\nFor the spacecraft, an obvious choice for a ranking function is the distance from the deputy to the chief. In order to make the function easier to reason about, we use a linear proxy function for the actual distance, namely the Manhattan distance. Unfortunately, it is not the case that this measure always decreases, as the spacecraft may move away from the target.\nThus, we instead propose a property that ensures the space-craft eventually starts moving towards the target. The property is expressed as a logical disjunction: after k steps, either the Manhattan distance decreases or the magnitude of the velocity decreases. Again, we approximate the velocity magnitude by the $L^1$ norm, the sum of the absolute values of $\\dot{x}$ and $\\dot{y}$. Formally, if the current state is $(x_0,y_0, \\dot{x_0}, \\dot{y_0})$ and the future state after k steps is $(x', y', \\dot{x'}, \\dot{y'})$, we must show:\n$(|\\dot{x'}|+|\\dot{y'}|)-(|x_0|+|y_0|) < -\\epsilon \\lor (|\\dot{x'}|+|\\dot{y'}|)-(|x_0|+|y_0|) < -\\epsilon$,\nwhere $\\epsilon$ is some positive value.\nIf property (19) holds (for some $k$) for every state, then eventually the spacecraft will be moving towards the goal (i.e., the $L^1$ norm of the position will decrease).\nSuppose that from some starting state, $(x_0, y_0, \\dot{x_0}, \\dot{y_0})$, the spacecraft follows a trajectory that never moves towards the goal in the sense that the $L^1$ norm never decreases. Let $(x_i, y_i, \\dot{x_i}, \\dot{y_i})$ be the state after i time steps. This means that for all i, $|x_i| + |y_i| \\leq |x_{i+1}| + |y_{i+1}|$. Let $V_i = |x_i| + |y_i|$. By (19), we know that for each $V_i$, there must be some k, such that $V_{i+k} - V_i < -\\epsilon$. Thus, for any n, we can construct a sequence $V_{j0}, V_{j1}, V_{j2},... V_{jn}$ such that $j_0 = 0$ and $V_{j1} - V_{ji+1} > 6$. If we then take n > Vo/\\epsilon, we get that $V_{in} < 0$, which is impossible.\nVerify (19) using Algorithm 1. We gradually increase k until the property holds, a maximum of $k = k_{max}$ is reached, or a timeout is exceeded."}, {"title": "Algorithm 1: Algorithm for k-induction.", "content": "Require: Bounds on state components $x_0, y_0, \\dot{x_0}, \\dot{y_0}$, values\nEnsure: If result = UNSAT, then property (19) holds for all\nVerify the negation of the distilled property:\nreturn result\nInput bounds for the state space can be chosen according to the problem specification. It is also important to note that different $k_{min}$ and $k_{max}$ values can be chosen. In practice, in order to make the verification more tractable, we first split the state space into subregions, then call the algorithm on each subregion. For each subregion of the state space, we explore values of k from $k_{min}$ to $k_{max}$. For each k, a neural network verifier is invoked to check if the negation of the property holds after k steps. There are three possible results of the algorithm.\nIf the negation of the property is satisfiable for each k, the algorithm returns SAT along with a counter-example.\nIf the negation of the property is unsatisfiable for some k, this means that the property holds for that value of k. In this case, the algorithm returns UNSAT together with the value of k for which unsatisfiability was determined. In this case, verification of the region is complete.\nIf a predefined timeout is exceeded, the algorithm termi-nates and a timeout result is returned.\nWe use Marabou for the neural network verification step. We set the following parameters for Marabou: \u201cverbosity=0, timeoutInSeconds=5000, num-Workers=10, tightening Strategy=\"sbt\", solveWithMILP=True\u201d. Marabou also requires a back-end linear programming engine. We use Gurobi 9.5.\nWe start with positional bounds of $|x|, |y| \\in [-25,25]$ and velocity bounds of $\\dot{x}, \\dot{y} \\in [-0.2, 0.2])$. We initially divide these into 25 subregions by focusing on 5\u00d75 regions in the positional space. A subregion is further subdivided if Algorithm 1 times out. We set $k_{min}$ to 1, $k_{max}$ to 20, and use a timeout of 1.4 hours for each loop iteration (i.e., 30 hours if all values of k time out).\nWe end up with 71 subregions. For each subregion, Algorithm 1 returns UNSAT. The minimum returned value for k is 1, the maximum is 12, the average is 5, and the median is 3.\nregions close to the goal region are more difficult: they require more subregions and take longer, whereas regions more distant can sometimes be verified without utilizing addi-tional subregions. The minimum runtime (in seconds) for any subregion is 0.02, the maximum is 4295.86, the average is 193.62, and the median is 1.76.\nAs a sanity check, we validated our results experimentally by running a simulation framework. Starting from randomly sampled points in the state space, we confirmed that the k-inductive property holds on the trajectory starting at each point. These checks also succeeded.\nInitially, we applied our approach to the neural network controller described in [62]. The original network topology (two hidden layers with 256 nodes each) resulted in lengthy verification times. Moreover, for many regions, the verification failed: we discovered counter-examples for all tested values of k.\nshows an example counterexample trajectory from the original neural network. The starting state is $[x = 0.5347935396499356, y = 0.51, \\dot{x} = 0.0006074960859780154, \\dot{y} = 0.00038615766226848813]$. The controller moves steadily away from the goal, and only after many steps turns the spacecraft around to move towards the goal.\nSuch trajectories provided motivation for the design changes mentioned in Section III-D. In particular, the changes to the reward function strongly incentivize the controller to move towards the goal region. shows the trajectory using the verified controller, starting from the same starting state. Note how the spacecraft moves nearly directly towards the goal region.\nThe successful verification of (19) is not sufficient to establish that the deputy eventually reaches the chief. We would need to establish a second property, namely that once the spacecraft is moving towards its goal, it always gets closer (by at least some $\\epsilon$) within k steps. Let $x_i, y_i$ be the position i steps from some starting position $(x_0, y_0)$. This can be formalized with the property:\n$\\exists k. (|x_k|+|y_k|)-(|x_0| + |y_0|) < -\\epsilon$.\nFormally verifying this property is left to future work."}, {"title": "B. An Alternative Approach using Polar Coordinates", "content": "Before moving to the Manhattan distance, we explored an alternative approach using polar coordinates, which allows the $L2$ norm to be used directly in the invariant while maintaining linearity. More specifically, if r is the distance to the origin and $\\theta$ is the angle from the x-axis, then we can write the equivalent of property (19) as:\nr'-r<-$\\epsilon \\lor \\dot{r'} - \\dot{r} < -\\epsilon$.\nNote how much simpler property 21 is compared with prop-erty (19). However, there remain two challenges: training a polar controller and converting the dynamics to polar coordinates. Training a controller for the polar system is not straight-forward; it requires complex parameter changes, for example, adjusting the learning rate, observation vector order, and the length and normalization constants. However, these challenges are ultimately solvable, and we were able to train a network that takes polar coordinate inputs. The output is still $F_x$ and $F_y$, as we did not envision changing the physical spacecraft system.\nThe second challenge proved more difficult. We needed a way to calculuate new values of r and $\\theta$, given current values of r, $\\theta, \\dot{r}$, and $\\dot{\\theta}$, as well as $F_x$ and $F_y$. We did not find closed-form solutions in the literature for the Clohessy-Wiltshire Equations utilizing polar coordinates. We thus converted equations (7) through (10) to polar coordinates using the standard conversion equations:\n$x = r\\text{cos}\\theta$, $y = r\\text{sin}\\theta$, $r = \\sqrt{x^2 + y^2}$, $\\theta = \\text{tan}^{-1}\\frac{y}{x}$\nWe encoded the derivation of the equations directly in Python, which allowed us to confirm in simulation that our polar neural network had behavior similar to that of the original model. However, attempting formal verification with the new dynamics proved difficult. The new dynamics are highly non-linear. We attempted to use the OVERT tool for the purpose of linearizing $\\dot{r}$ and $\\dot{\\theta}$. However, the results were too complex and ultimately unsuccessful. It was at this point that we decided to instead use the $L^1$ norm and revert to standard rectangular coordinates.\nWe report this effort here in order to highlight both the potential benefits and pitfalls of using a different coordinate representation. If the dynamics had been more tractable in polar space, this would have been an attractive direction."}, {"title": "V. ALTERNATE VERIFICATION APPROACHES", "content": "While exploring the k-induction approaches described above, we concurrently explored an alternative approach using Neural Lyapunov Barrier certificates. The results of that effort represent themost complete verification results we have obtained to date and are reported in [56]. Here, for convenience, we review that approach at a high level and present some details not reported there. We also discuss several reachability-based approaches, which we also applied to the 2D docking problem, but which were, ultimately, unsuccessful."}, {"title": "A. RWA Certificates", "content": "A function $V: \\mathcal{X} \\rightarrow \\mathbb{R}$ is an RWA certificate for the task defined in Definition 1 if there exist some $\\alpha > \\beta \\geq \\gamma$ and $\\epsilon > 0$, such that the following constraints are satisfied.\n$\\forall x \\epsilon \\mathcal{X}.\nV(x) \\geq \\gamma$\n$\\forall x \\epsilon \\mathcal{X}_1$.\nV(x) \\leq \\beta$\n$\\forall x \\epsilon \\mathcal{X} \\setminus \\mathcal{X}_G$.\nV(x) \\leq \\beta \\rightarrow V(x) - V(f(x,\\pi(x))) \\geq \\epsilon$\n$\\forall x \\epsilon \\mathcal{X}_U$.\nV(x) \\geq \\alpha$\nAny tuple of values $(\\alpha, \\beta, \\epsilon, \\gamma)$ for which these conditions hold is called a witness for the certificate. RWA certificates provide the following guarantee.\nIf V is an RWA certificate for a dynamical system with witness $(\\alpha, \\beta, \\epsilon, \\gamma)$, then for every trajectory $\\tau$ starting from a state $x \\in \\mathcal{X} \\setminus \\mathcal{X}_G$ such that $V(x) \\leq \\beta$, $\\tau$ will eventually contain a state in $\\mathcal{X}_G$ without ever passing through a state in $\\mathcal{X}_U$.\nWe use reinforcement learning to jointly train neural networks for both the controller and the corresponding RWA certificate. RWA Training Loss. The training objective for RWA certifi-cates is described below:\n$O_s = C_s \\frac{\\sum_{i | x_i \\epsilon X_I} ReLU(\\delta_1 + V(x_i) - \\beta)}{\\sum_{i | x_i \\epsilon X_I} 1}$\n$O_d = C_d \\frac{\\sum_{i|x_i \\epsilon X\\setminus (X_V \\cup X_G), V(x_i) < \\beta} ReLU(\\delta_2 + \\epsilon + V(x) - V(x_i))}{\\sum_{i\\setminus x_i \\epsilon X\\setminus (X_V \\cup X_G), V(x_i) < \\beta} 1}$\n$O_u = C_u \\frac{\\sum_{i | x_i \\epsilon X_U} ReLU(\\delta_3 - V(x_i) + \\alpha)}{\\sum_{i | x_i \\epsilon X_U} 1}$\n$O = O_s + O_d + O_u$\nEquation (27) penalizes deviations from constraint (24), Equation (28) penalizes deviations from constraint (25), and Equation (29) penalizes deviations from constraint (26). We incorporate parameters $\\delta_1 > 0, \\delta_2 > 0$, and $\\delta_3 > 0$, which can be used to tune how strongly the certificate over-approximates adherence to each constraint. Similarly, constants $C_s, C_d, C_u$ can be used to tune the relative weight of the two objectives. The final training objective O in (30) is what the optimizer seeks to minimize, by using stochastic gradient descent (SGD) or other optimization techniques.\nIt is important to note that the RWA training objective does not explicitly penalize deviations from Equation (23). Instead, because V is implemented as a neural network using floating-point arithmetic, it has only a finite number of possible inputs and outputs, so Equation (23) must hold for some $\\gamma$. In practice, we can use Marabou to find $\\gamma$ by doing a linear search for the minimum value of V: we simply set $\\gamma$ to some initial value, say a, then repeatedly check $\\exists x. V(x) < \\gamma$, updating $\\gamma$ with the new value each time the query is satisfiable, and repeat until the query is unsastisfiable.\nWhile $\\mathcal{X}_1$ is typically defined as having both upper and lower bounds on state variables, this is not the case for $\\mathcal{X}_U$, which often has only lower bounds on state variables (this is the case, for example, for the 2D docking problem defined in Section III).\nHowever, during training, we do impose an upper bound on the states sampled from $\\mathcal{X}_U$. Specifically, if the controller operates over n-dimensional states $x = [x_1,x_2,.., x_n]$, we sample points satisfying the following constraints:\n$(x_1 > p_1) \\lor (x_2 > p_2) \\lor ... \\lor (x_n > p_n)$\n$(x_1 < p_1 + \\psi_1) \\land (x_2 < p_2 + \\psi_2) \\land ... \\land (x_n < p_n + \\psi_n)$\nHere, $\\pi_1$ represents the (given) lower bounds on the unsafe region $\\mathcal{X}_U$, and $\\psi_1,..., \\psi_n$ are chosen to be strictly greater than 0.\nA similar issue arises when sampling from $\\mathcal{X} \\setminus \\mathcal{X}_G$. This can often be solved simply by sampling instead from $\\mathcal{X} \\setminus (\\mathcal{X}_G \\cup \\mathcal{X}_U)$, as the lower bounds on variables in $\\mathcal{X}_U$ then create upper bounds for the sampling step.\nFor objective 28, if $x$ lies in $\\mathcal{X}_U$, we replace the actual value of V(x) with $\\alpha$. This is because we learn correct functional behaviors of $\\mathcal{X}_U$ through objective 29 regardless, and thus using the actual value of V(x) would lead to unnecessary training effort and excessive penalties.\nTo improve training, the objective is used to train the certificate V alone for a few iterations, after which training includes both the certificate and the controller. This is done to avoid erratic training of the controller when V has random weights.\nIn order to obtain formal guarantees, we use Marabou to formally verify the constraints in Definition 2. Verification of RWA constraints is generally straightforward, but we have to similarly bound $\\mathcal{X}_U$ and $\\mathcal{X} \\setminus \\mathcal{X}_G$ to verify constraints 26 and 25 respectively. Instead of using $\\mathcal{X} \\setminus \\mathcal{X}_G$ as the input space for 25, we use instead $\\mathcal{X} \\setminus (\\mathcal{X}_G \\cup \\mathcal{X}_U)$, which provides the same guarantees. Moreover, instead of using $\\mathcal{X}_U$ as the input space for 26, we use the bounded space, call it $\\mathcal{X}'_U$, used for data sampling. To ensure this provides the same guarantees, we check that no states beyond the upper bound of $\\mathcal{X}'_U$ are reachable."}, {"title": "B. Reachability Analysis Approaches", "content": "In this subsection, we discuss approaches based on reacha-bility analysis. While these approaches were ultimately unsuc-cessful on the case study problem outlined in section III, we still mention them here, as the reasons for their failure may be of interest, and they may be useful on other problems."}, {"title": "Forward-tube and Backward-tube Reachability.", "content": "Forward-tube and backward-tube reachability attempt to generate a path over abstract state spaces (i.e., sets of states) from the starting state space to the goal state space. At each step along the abstract path, we check that every state in the abstract state set meets any safety guarantees.\nIn forward-tube reachability, a starting set of states $\\mathcal{X}$ and step size k is defined. Then, a set of states $\\mathcal{X}'$ is constructed such that all states reachable from $\\mathcal{X}'$ in k steps are contained within $\\mathcal{X}$. This process is continued, and additional sets of states $\\mathcal{X}^{i+1}$ are constructed, each with the property that they contain the states reachable from $\\mathcal{X}^i$ in k steps. If at some point, the constructed set is a subset of the goal region, then the liveness property is ensured. However, it can be very chal-lenging to find a sequence of sets of states $\\mathcal{X}^i$ that eventually lead to a subset of the goal region. This was the case for the spacecraft example.\nOn the other hand, in backward-tube reachability, we start with $\\mathcal{X}$ set equal to the goal states and define a step size k. Then, a set of states $\\mathcal{X}'$ is constructed such that all states reachable from $\\mathcal{X}'$ in k steps are contained within $\\mathcal{X}$. Again, this process can be repeated until the set of states includes the initial states. A difficulty with this approach is computing a sufficiently large previous set of states at each step."}, {"title": "Grid Reachability.", "content": "Grid reachability is a process which first partitions a bounded subset of the state space into cells, then computes a directed graph where each cell is a vertex, and each directed edge (a,b) denotes that vertex b is reachable from vertex a in k steps, for a specific k. The goal is to show that for all paths constructed from cells in the defined initial state space, a goal region reachable. However, to ensure liveness, it is also necessary to show that the graph has no cycles and that it is not possible to reach any cells beyond the partitioned state space.\nWe applied this technique to the spacecraft example. A challenge is preventing self-cycles in the graph. One strategy for doing this is to construct cells where at least one velocity component never changes sign. It is easy to see that for such cells, the spacecraft cannot remain in the cell forever, so we can ignore self-loops on such cells. For cells containing a velocity sign-change, we use a very narrow velocity range, narrow enough to ensure that the spacecraft leaves the range in k steps. It is also desirable to limit the number of cells reachable from a given cell, to avoid the need to do many reachability checks. This can be ensured by making the cells large enough that it is impossible to cross more than one cell in a single set of k steps."}, {"title": "Algorithm 2: APPLYING GRID REACHABILITY", "content": "1 Let IS be the input space\n2 Let k be the step size\n3 Divide IS into cells $C = C_0, C_1, ..., C_n$\n4 Let vertices $V = C$\n5 Initialize edge set E to be the empty set\n6 i = 0\n7 for i\u2264 n do\n8  Denote set of adjacent cells to $c_i$ as $C_r$\n9  Add $c_i$ to $C_r$ if self-cycles are possible\n10  for $C_r \\epsilon C_r$ do\n11  if $c_r$ is reachable from $c_i$ in k steps then\n12  Add directed edge $(c_i, c_r)$ to E\n13  i = i + 1\n14 Let G := (V, E)\n15 Check for cycles in G\n16 if G is acyclic then\n17  Determine cells $C_s$ with no paths leaving input space\n18  return $C_s$ as cells meeting liveness property\nAnalysis of Grid Reachability. We applied grid reachability to a state space with $x, y \\in [-10,10]$ and $\\dot{x}, \\dot{y} \\in [-1.6, 1.6]$ using Algorithm 2. A binary search was conducted using Marabou to determine cell bounds such that cells could only reach adjacent cells. The step size k was chosen to be 1.\nWe found a variety of cycles of increasing lengths, even as cells were divided further in an attempt to refine the grid abstraction. Moreover, we found that all cells had paths leaving the input space. We showcase one such trajectory of cells with this behavior in Fig. 3. In this trajectory, we see that for the first three steps, the velocity component ranges are negative, thereby guiding the spacecraft towards the goal region, but there is a path from cell 3 to cell 4 that induces a positive velocity component, allowing the path to diverge.\nUltimately, the grid abstraction does not lend itself well to the liveness task because such spurious paths are difficult to rule out. While further refinement of the grid approach is possible and could eventually yield a workable approach, we determined that the complexity and difficulty were too high, and abandoned it in favor of the certificate approach mentioned earlier."}, {"title": "VI. CONCLUSION", "content": "We have presented methods for verifying safety and liveness properties for DRL systems using k-induction, Neural Lyapunov Barrier Certificates, and reachability analysis. We explore their effectiveness on a 2D spacecraft docking problem posed in previous work. For this problem, we show how a k-induction based approach can be used alongside a design-for-verification training scheme to provide liveness guarantees. We also discuss how Neural Lyapunov Barrier Certificates can be used to provide both liveness and safety guarantees. While reachability analysis ultimately did not provide any formal guarantees, we discuss the approach and its limitations. In future work, we plan to explore scaling these methods to more complex and realistic control systems."}]}