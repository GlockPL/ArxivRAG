{"title": "PCAPVision: PCAP-Based High-Velocity and Large-Volume\nNetwork Failure Detection", "authors": ["Lukasz Tulczyjew", "Ihor Biruk", "Murat Bilgic", "Charles Abondo", "Nathanael Weill"], "abstract": "Detecting failures via analysis of Packet Capture (PCAP)\nfiles is crucial for maintaining network reliability and per-\nformance, especially in large-scale telecommunications net-\nworks. Traditional methods, relying on manual inspection\nand rule-based systems, are often too slow and labor-intensive\nto meet the demands of modern networks. In this paper, we\npresent PCAPVision, a novel approach that utilizes computer\nvision and Convolutional Neural Networks (CNNs) to detect\nfailures in PCAP files. By converting PCAP data into images,\nour method leverages the robust pattern recognition capa-\nbilities of CNNs to analyze network traffic efficiently. This\ntransformation process involves encoding packet data into\nstructured images, enabling rapid and accurate failure detec-\ntion. Additionally, we incorporate a continual learning frame-\nwork, leveraging automated annotation for the feedback loop,\nto adapt the model dynamically and ensure sustained per-\nformance over time. Our approach significantly reduces the\ntime required for failure detection. The initial training phase\nuses a Voice Over LTE (VoLTE) dataset, demonstrating the\nmodel's effectiveness and generalizability when using trans-\nfer learning on Mobility Management services. This work\nhighlights the potential of integrating computer vision tech-\nniques in network analysis, offering a scalable and efficient\nsolution for real-time network failure detection.", "sections": [{"title": "1 Introduction", "content": "Detecting failures in PCAP files is a critical task in network\ntroubleshooting and analysis. PCAP files record detailed\npacket-level data, but as networks grow, the volume of data\nbecomes overwhelming. Traditional methods for detection\nrely heavily on manual inspection and rule-based systems,\nwhich are time-consuming and impractical for large-scale\nnetworks where timely failure detection is essential. Fail-\nures, unlike errors, indicate unsuccessful communication\nattempts, such as failed calls, which can significantly im-\npact network performance and user experience and require\nmore in-depth analysis. The primary goal of PCAPVision is\nto quickly identify failed call-flows in PCAP files, allowing\nfurther analysis of only relevant data and avoiding the need\nto store and analyze large volumes of data.\nEfficient failure detection in PCAP files is crucial for main-\ntaining network reliability and performance. Minimizing\nthe Mean Time to Detect (MTTD) and Mean Time to Re-\npair (MTTR) ensures seamless service delivery and customer\nsatisfaction. Traditional parsing methods are accurate, but\noften too slow for real-time network monitoring and trou-\nbleshooting. Rapid filtering and analysis of PCAP files can\nsignificantly reduce downtime and operational costs. Proac-\ntive network management enabled by quick failure detection\nhelps prevent minor issues from becoming major problems,\nespecially in complex modern data centers and distributed\nsystems.\nAdvanced techniques, such as using Convolutional Neural\nNetworks (CNNs) to analyze transposed images of PCAP\nfiles, offer a promising solution. This approach enhances\nspeed and accuracy in detecting failures, identifying subtle\nfailure patterns that traditional methods might miss. Our\napproach leverages computer vision and CNNs to detect\nfailures by transforming PCAP files into images using CNN's pattern recognition capabilities instead of\ntraditional parsing techniques. This\ntransformation involves mapping packet data into structured\nimages, which the CNN can then process to detect failures.\nComputer vision and CNNs excel at pattern recognition,\nmaking them ideal for analyzing PCAP files. This approach\nbypasses the limitations of traditional parsing methods, al-\nlowing for rapid filtering and efficient analysis of large vol-\numes of PCAP data. It enables quicker failure identification,\nenhancing network management efficiency.\nIn a rapidly changing communications network, a static\nmodel has the risk of becoming obsolete very quickly. To\naddress this, our methodology integrates a continual learn-\ning framework to maintain robust performance over time\n. This framework combats catastrophic for-\ngetting through experience replay and adaptive regulariza-\ntion. Experience replay helps the model retain old tasks by\nreusing stored data, while adaptive regularization ensures\nthat new information reinforces rather than overwrites exist-\ning knowledge. Continual learning is crucial for sustaining\nhigh performance amidst rapid technological changes and\nevolving network traffic patterns.\nThis article introduces our approach using computer vi-\nsion to detect failure in PCAP files and continual learning to\nmaintain performance over time. The key contributions of\nour research are:\n\u2022 Efficient PCAP Processing: Enables high-velocity\nfailure detections across large volumes of PCAP files,\nsignificantly reducing storage and network probe costs.\n\u2022 Advanced Pattern Extraction: Utilizes Convolutional\nNeural Networks (CNNs) to identify position-invariant\npatterns within PCAP files, enhancing detection accu-\nracy.\n\u2022 Low Computational Overhead: The model features\na compact architecture with few trainable parameters,\nenabling efficient training and inference using only\nCPU resources, ideal for edge computing deployments.\n\u2022 Continual Learning Architecture: Maintains con-\nsistent, high-quality results by handling data drifts\nthrough a continual learning framework.\n\u2022 Experience Replay Strategy: Incorporates experi-\nence replay to prevent catastrophic forgetting, ensur-\ning reliable performance over time.\n\u2022 Transfer Learning Capabilities: Demonstrates the\nability to transfer learned knowledge to new PCAP\nflows, enabling seamless expansion to additional data\nsets."}, {"title": "2 Background and Related Work", "content": "Traditional methods for analyzing PCAP files have relied\nheavily on manual inspection and rule-based systems. These\ntechniques are labor-intensive, requiring significant exper-\ntise to interpret the complex data structures and are imprac-\ntical at larger scales due to enormous data volumes. Manual\napproaches are thorough but time-consuming, while rule-\nbased systems use predefined criteria to detect anomalies\nand failures in network traffic. However, these systems lack\nflexibility and struggle to adapt to new network behaviors\nor patterns, necessitating constant updates as network tech-\nnologies evolve, which is a maintenance burden. Both meth-\nods have scalability issues, struggling to handle the growing\nvolume of data transmitted across networks. This growing\nvolume of data requires automated tools that can efficiently\nprocess large datasets and detect faults without significant\ndelays, leading to the adoption of advanced techniques like\nmachine learning models that learn from data and improve\nover time without the need for constant rule updates.\nThe application of machine learning (ML) techniques to\nPCAP data is a relatively new field that has gained traction\ndue to its potential to automate and enhance the accuracy of\nnetwork analysis. Several studies have employed supervised\nlearning methods, utilizing labeled datasets to train models\nfor classifying network traffic and identifying malware traffic\n[1]. However, these ML approaches often rely on handcrafted\nfeatures and labeled data, which can be a significant barrier\ndue to the challenge of maintaining the appropriate features\nand labeled dataset over time.\nRecent advancements in network traffic analysis have\nintroduced innovative methods leveraging Large Language\nModels (LLMs). These models use advanced natural language\nprocessing to interpret network traffic data encoded as text.\nBy treating network traffic and logs as a form of language,\nLLMs can identify patterns and failures within PCAP files\nmore flexibly and dynamically than traditional rule-based\nsystems [2]. This LLM-based approach offers several advan-\ntages, including continuous learning and adaptation to new\nnetwork behaviors without manual rule updates. It also pro-\nvides a more nuanced understanding of network data, poten-\ntially identifying subtle indications of failure that traditional\nmethods might overlook. Integrating LLMs into PCAP analy-\nsis represents a significant advancement, providing a scalable,\nadaptable, and efficient tool for managing the complexity\nand volume of network traffic. This study compares our ap-\nproach (PCAPVision) to LLMCap [2] in terms of prediction\nspeed. Even before the rise of LLM-based approaches, sig-\nnificant progress in network analysis and malicious traffic\ndetection was achieved using computer vision techniques,\nsuch as Convolutional Neural Networks (CNNs) or Vision\nTransformers (ViT).\nIn 2017, Wei Wang et al. [3] were among the first to\napply a representation-based learning approach to mali-\ncious traffic classification using CNNs. They divided con-\ntinuous raw traffic into several discrete units like network\nflows/sessions, applied traffic cleaning, and converted these\nunits into greyscale images for deep learning models to ana-\nlyze. Their method created 28x28 pixel images from network\nflows, which were then fed into a simple CNN, achieving\nsignificant accuracy in classifying malware traffic.\nFollowing this, Hussain et al. [4] utilized ResNet, a state-\nof-the-art CNN model, to detect DoS and DDoS attacks by\nconverting network traffic data into RGB images. This ap-\nproach achieved remarkable accuracy in identifying various\nattack patterns, showcasing the potential of CNNs in han-\ndling complex network traffic scenarios. In 2022, Agrafiotis et\nal. [5] advanced the field by incorporating Vision Transform-\ners (ViTs) alongside CNNs to enhance detection accuracy.\nThey used an approach similar to Wei Wang et al., converting\ntraffic data into greyscale images to train both CNNs and\nViTs, which yielded better results than standard Random\nForest models.\nMost recently, Davis et al. [6] proposed a novel approach\nfor malware traffic classification by converting IP header\nfields from network sessions into 50x50 RGB images. Their\nmethod leveraged detailed information within PCAP files to\ncreate visual representations processed by deep CNNs. This\ntechnique outperformed traditional ML methods across mul-\ntiple datasets, achieving higher accuracy and demonstrating\nthe robustness of image-based network traffic analysis.\nThese computer vision-based methods provide a signifi-\ncant improvement over classical feature-based ML approaches,\nas well as traditional rule-based and manual inspection tech-\nniques. They offer a scalable solution for network traffic\nanalysis, capable of processing large volumes of data effi-\nciently and accurately. The ability to automatically learn\nfeatures from raw traffic data without manual feature extrac-\ntion represents a major advancement in the field.\nWhile the cited studies primarily focus on malicious traffic\ndetection, our methodology targets failure detection. Other\napproaches typically preprocess network flows or sessions\nfrom the raw traffic data before converting them into images,\nsegmenting data into standardized units for easier analysis.\nIn contrast, our approach bypasses this preprocessing step,\nallowing direct analysis of raw PCAP files. By avoiding call\nflow extraction, we achieve significantly faster processing\ntimes, to handle large-scale network traffic efficiently and\nimprove real-time network monitoring and failure detection.\nAs previously mentioned, we use a continual learning\nframework to maintain our model's robustness over time.\nContinual learning in neural networks is complex, with var-\nious strategies to mitigate issues such as catastrophic for-\ngetting and overfitting. One approach involves using task-\nspecific components, like context-dependent gating, where\ndifferent model segments are optimized for specific tasks [7].\nAlternatively, gradient descent-based methods selectively\ntrain certain units, often employing task-oriented attention\nmasks to manage model weight allocation [8]. Another ap-\nproach uses evolutionary computation, where agents, learned\nthrough genetic algorithms, determine parameter subsets\nfor each task [9]. Regularization strategies protect learned\nparameters from one task during subsequent task training\nby applying constraints that encourage the model to retain\nrelevant knowledge [10]. The replay method incorporates\nhistorical data from previous tasks into the training set to\nreinforce past learnings [11], although this can be impracti-\ncal in rapidly changing environments with high throughput\nrequirements due to increased training dataset size. Often, a\nsimple regularization-based approach suffices.\nExisting frameworks have shown promise in addressing\ncontinual learning challenges. Parisi et al. [12] reviewed var-\nious strategies to maintain model performance over time,\nemphasizing regularization techniques, dynamic architec-\nture approaches, and experience replay methods. In this ar-\nticle, we present a continual learning framework to solve\noverfitting and catastrophic forgetting."}, {"title": "3 Methodology", "content": "Our methodology leverages computer vision techniques and\na CNN architecture to detect failures in PCAP files. This\nsection details the process of transposing PCAP data into\nimages, the architecture of the CNN used, and the specifics of\nthe training process and dataset employed for failure detec-\ntion. Additionally, we describe the integration of a continual\nlearning framework that adapts the model dynamically to\nnew data, ensuring sustained performance over time."}, {"title": "3.1 Failure Detection", "content": "Convert PCAP files into images. Inspired by Wei Wang et\nal. [3], the process to convert PCAP files into images involves\nseveral transformation steps. First, the PCAP file is opened\nin binary mode to read the data as bytes. These bytes are\nthen encoded into a hexadecimal format for readability. Each\npair of hexadecimal digits is converted into their decimal\nequivalents, resulting in an array of decimal numbers corre-\nsponding to the original byte values from the PCAP file. This\narray is reshaped into the desired image size. If the array\nexceeds the image dimensions, it is truncated; if the array\nis too small, zeros are appended. Each pixel in the image\nrepresents a byte value, ranging from 0 to 255, making it\nsuitable for CNN analysis. In the following, we describe the\nretained architecture (or more details about the sensitivity\nanalysis, see Appendix A).\nConvolutional Neural Network (CNN) architecture.\nThe CNN for analyzing PCAP-derived images is structured\nto handle high-resolution inputs and complex data patterns.\nEach image is fixed at 1600x1600 pixels, representing bytes\nfrom the PCAP file . Reshaping and Padding.\nThe input image is reshaped to 1696x1696 pixels. Reflect\npadding of 48 pixels on both axes preserves the spatial con-\ntext of the image and reduces the effects of boundary ar-\ntifacts during convolution. Normalization and Dropout.\nThe pixel values are normalized to a unit interval [0; 1]\nby dividing by 255, preventing exploding [13] and vanish-\ning gradients [14], thereby improving the stability of the\nmodel during training [15]. Following normalization, a two-\ndimensional dropout with a probability of 0.4 is applied at\nthe outset of the model [16] to enhance model generalization.\nConvolutional Layer. A single convolutional layer with\na kernel size of 64x64 and an 8x8 stride optimizes feature\nextraction. The Exponential Linear Unit (ELU) activation\nfunction allows smooth transitions between the activation\nmaps' negative and positive values, capturing intricate PCAP\ndata patterns. To keep the model computationally efficient,\n4 kernels are used in the first layer. Max Pooling. A two-\ndimensional max pooling with an 8x8 window reduces the\ndimensionality and complexity of the feature maps, lower-\ning the number of trainable parameters in subsequent layers\nand minimizing computational demands. Flattening and\nDense Layers. Reduced feature maps are flattened into a\none-dimensional vector for fully connected processing. The\nfirst fully connected (FC) layer includes 256 units and ELU\nactivation integrates learned features. A dropout layer with\n0.3 probability reduces overfitting. Output Layer. The final\nlayer is a second fully connected layer that outputs through\na single unit with a sigmoid activation function. This layer is\ndesigned for binary classification, determining the presence\nor absence of failures in the PCAP file.\nInitial Training Process and Dataset Used. Our CNN\ntraining process for analyzing PCAP files from VoLTE ser-\nvices is structured to ensure robust performance. We use\nthe Adam optimizer [17] with a learning rate of 0.0005. We\nselected the Binary Cross Entropy [18] loss function as a\ncost function for the binary classification, identifying PCAP\nfiles with at least one failed call-flow (failed PCAP files) or\nonly successful call-flows (successful PCAP files). To prevent\noverfitting, we adopted a conservative approach for training\nduration and monitoring, capping at 500 epochs. An early\nstopping mechanism halts training if no improvement in\nvalidation loss is observed for 16 consecutive epochs, pre-\nventing overfitting, and ensuring that the model generalizes\nwell on unseen data. The dataset used for training and vali-\ndation is derived from VoLTE service communications and\nsegmented into training, validation, and testing subsets. The\ntraining set comprises 768 successful PCAPs and 454 failed\nPCAPs, totaling 1,222 files. A failed PCAP contains at least\none failed call-flow, a successful PCAP cannot include any\nfailure-classified flows. The validation and test sets both con-\ntain 43 successful and 25 failed transmissions, ensuring a\nbalanced mix for model evaluation.\nSubsequent training stages introduce continual learning\nto adapt the model dynamically to new data without forget-\nting previously learned information. This phase also involves\nrefining and selecting hyperparameters for the final model\ntopology, ensuring the network remains efficient and effec-\ntive in real-world scenarios."}, {"title": "3.2 Continual Learning Framework", "content": "Network environments are highly dynamic, with constantly\nchanging traffic patterns and behaviors. This variability can\nimpact model performance, further complicated by vendor-\nspecific rules and constraints. To address these challenges,\nwe employ a continual learning framework that adapts the\nmodel weights over time. Due to the large amount of data\nproduced, retaining all PCAP files is impractical, necessitat-\ning a strategy for ongoing model fine-tuning without using\nthe entire historical dataset.\nOur continual learning framework is composed of several\nparts, with a cyclical process of monitoring, evaluation, fine-\ntuning, and updating the PCAPVision model, all within an\nautomated vendor-premises workflow. Figure 3 illustrates\nour continual learning framework, broken down into four\nprocedures: automatic labeling, data management, daily eval-\nuation and fine-tuning.\nWe use Call-Flow Analyzer (CFA) to automatically label\nnew datasets , P1). CFA processes PCAP files to\nidentify and analyze individual call-flows, detecting failures\nand diagnosing their causes. This automation replaces the\nneed for manual labeling, significantly increasing the num-\nber of annotated PCAP files available for each iteration and\nensuring full process automation.\nThe second procedure corresponds to Data Management\n, P2) collects new PCAP files daily, randomly select-\ning and labeling 5% for the control dataset. Depending on the\nvolume, additional PCAP files are labeled and added to the\ndaily evaluation set which is then included in the training\nset for fine-tuning purposes.\nDaily Evaluation evaluates the champion model \u2013 our\nbest-performing model to date using metrics (F1-score, F2-score, recall, and precision) with a primary\nfocus on the F2-score. This metric prioritizes minimizing\nfalse negatives, ensuring that we capture all failed PCAP\nfiles. If the current day's F2-score (F2t) is lower than the\nprevious day's (F2t\u22121), the fine-tuning procedure is trig-\ngered (P4). All collected metrics are logged to the MLflow\n[19] tracking server for monitoring and evaluation of the\nmodel's performance over time.\nWhen triggered by a drop in the F2-score, the model un-\ndergoes fine-tuning with the latest data (see Figure 3, P4).\nThis involves using the accumulated training data collected\nsince the last instance of stable performance. The data col-\nlection extends back to the point where no metric decay\nwas observed, ensuring coverage of all relevant periods that\nmight explain the performance decline, such as changes in\nnetwork behavior. PCAP files are stored for a maximum of\none month. During fine-tuning, the most up-to-date model,\ntagged as \"champion\" in the MLflow models registry, is se-\nlected. This current-best model serves as the baseline for\nupdates. During fine-tuning, we freeze the convolutional lay-\ners of the model and focus updates solely on the dense layers,\nbased on the hypothesis that the feature extraction compo-\nnent remains valid for the same task category. We optimize\nthe classification layers to enhance the model's responsive-\nness to new examples. We reduce the original learning rate\nby an order of magnitude to prevent large updates during\ngradient backpropagation, aiming for subtle yet effective\nmodel adjustments. Finally, we log the optimal threshold for\nclassifying PCAPs from the newly fine-tuned model. This\nthreshold determines the final class of each PCAP based on\nits probability score. The result is a refined model, deemed\nthe new champion, ready to be deployed as the current best.\nPost fine-tuning, we evaluate the performance of the re-\ntrained model candidate using a control dataset (Figure 3,\nP4), a small, curated set (5%) reserved during evaluations.\nThe purpose of this control dataset is to assess the model's\nsusceptibility to catastrophic forgetting to ensure the model\nretains previously acquired knowledge and to detect if the\nmodel is overfitting. Overfitting may occur if the data char-\nacteristics used for fine-tuning significantly differ from those\nof the broader historical data, which could tailor the model\nparameters to perform well only on the specific, possibly\nanomalous, features of the fine-tuning dataset and lead to\npoor generalization on new, unseen data. After evaluating\nthe new model candidate on the control dataset, the new\nmodel's performance metrics, particularly the F2-score, are\ncompared against the previous champion. If the new model's\nF2-score does not exceed the highest recorded score, the\nmodel is discarded. Only models demonstrating measurable\nprogress are adopted as the new champion.\nWe periodically serialize a fraction of historical data into\na hold-out set for evaluating the model's ability to retain\nknowledge without fully retraining. This approach assumes\nthat periodic inference on this set is sufficient to demonstrate\nthe absence of catastrophic forgetting and overfitting. Fo-\ncusing training on a dataset composed of the most recent\nPCAP examples allows rapid model updates and suitability\nfor high-throughput production environments. This strategy\nbalances the need for model adaptability with operational\ndemands."}, {"title": "4 Results and Discussion", "content": "This section presents the findings from our experiments\non failure detection in PCAP files using PCAPVision, high-\nlighting the continual learning framework, computational\nefficiency, and transfer learning capabilities. Finally, we list\nthe limitations of PCAPVision. In this section, we only con-\nsider the best architecture found. For a complete overview\nof tested architectures, see Deep Learning Architectures in\nAppendix A. We conducted a sensitivity analysis to evaluate\nthe performance of different types of architectures including\nRecurrent Neural Networks (RNN) which provided poor re-\nsults (F2-score: 0.48) compared to the CNN model (F2-score:\n0.89) on the VoLTE dataset."}, {"title": "4.1 Evaluation of the Continual Learning\nFramework in a Lab Environment", "content": "We implemented the continual learning framework in a lab-\noratory setting, where daily VoLTE tests generate 40 to 1,000\nPCAP files per day. Each PCAP file is retained for a period of\none month before becoming inaccessible. This environment\nallowed us to evaluate our continual learning framework\nover a period slightly exceeding one month to determine\nif the framework could autonomously improve its perfor-\nmance over time without any external intervention. The\ninitial model, V1, exhibited moderate performance with F2\nscores between 0.6 and 0.8, trained on our internal dataset\nwithout fine-tuning. On the fourth day, a drop in F2 score\ntriggered fine-tuning, making V2 the new champion as it\nhad no prior control dataset scores. The next day, another\nperformance drop triggered a fine-tuning, resulting in model\nV3 becoming the new champion. However, subsequent mod-\nels failed to surpass V3's performance on the control dataset\nuntil March 23, when V4 was established as the new cham-\npion. A notable period of instability occurred between April\n1 and April 6, characterized by two significant drops in per-\nformance, primarily due to the low number of tests (approxi-\nmately 50 PCAP files) compared to other days (around 300\nPCAP files). The tests conducted during this period were\nsubstantially different from those conducted previously. At-\ntempts to fine-tune the model during this time did not im-\nprove control set performance, and no new champions were\nselected. Later in April, models V5 and V6 were appointed\nas new champions, both demonstrating high stability and\nenhanced performance, with F2 scores exceeding 0.8 on both\nthe control and evaluation datasets. This outcome demon-\nstrates the effectiveness of our continual learning framework\nin achieving convergence towards high performance and sta-\nbility over time."}, {"title": "4.2 Computational Efficiency Analysis of\nPCAPVision Compared to Other Methods", "content": "To illustrate the low computational overhead, we compared\nprocessing times across four systems: PCAPVision, LLM-\nCap [2], CFA, and Subject Matter Experts (SMEs). The task\nwas to detect call-flow failures in PCAP files, excluding the\nidentification of specific failure reasons. While PCAPVision\nfocuses solely on detecting failures, LLMCap and CFA offer\nbroader functionalities that could complement PCAPVision\nfor detailed PCAP file analyses post-failure detection.\nWe used VoLTE PCAP files of various file sizes (mean size\n= 1.03 Mb, std = 0.7) and reported average processing times\nto reflect general computational effort. To compare speed,\nLLMCap and PCAPVision tests were run on an AWS EC2\ng5.4xlarge instance. CFA uses a dedicated Kubernetes cluster\nof 8 cores. We ensured that each system was tested under its\noptimal setup to provide a fair comparison of computational\ntime.\nFor PCAPVision, the inference process involves convert-\ning the binary PCAP file into an image and then using the\nmodel for inference. Our analysis revealed that preprocessing\n(PCAP to image conversion) accounted for 98% of the total\ncomputational time, with the inference process consuming\njust 2%. This highlights the significant computational de-\nmand of the preprocessing step in the PCAPVision approach.\nTable 1 shows that PCAPVision detects failures significantly\nfaster than other automated solutions, achieving speeds one\nto two orders of magnitude greater. Compared to manual\nanalysis, PCAPVision's speed advantage extends to several\norders of magnitude.\nIn high-volume production environments, PCAPVision\ncan initially filter PCAP files to identify potential failures.\nOnce a failure is detected, more detailed analyses can be con-\nducted using LLMCap or CFA. This staged approach allows\nefficient initial screening by multiple instances of PCAPVi-\nsion at the network edge, followed by focused, deeper analy-\nsis only on flagged PCAP files. This optimizes resource use\nand ensures intensive processing is only applied to relevant\ndata."}, {"title": "4.3 Transfer Learning", "content": "In this study, we explored the transfer learning capabilities\nof a pretrained model, initially trained on the VoLTE service\ndataset. We extended the evaluation to the Mobility Man-\nagement dataset to assess its adaptability and generalization\ncapabilities. The Mobility Management dataset includes 674\nsuccessful and 230 failure PCAPs for training, 85 successful\nand 34 failure PCAPs for validation, and 85 successful and\n38 failure PCAPs for testing. We conducted several exper-\niments to assess the model's performance on the Mobility\nManagement dataset (Figure 5). First, as a reference, we eval-\nuated the strategy of assigning the label \"failure\" based on\na prior probability of 0.31 based on the test set, yielding\nan F2-score of 0.18. Next, we applied the pretrained model\ndirectly to the new dataset with no fine-tuning, achieving\nan F2-score of 0.42. Compared to the random strategy, the\nzero-shot performance indicates that the model had learned\nuseful information from the VoLTE dataset, which was par-\ntially transferable to the Mobility Management dataset. We\nthen fine-tuned only the dense layers (prediction part of the\nnetwork) of the pretrained model, resulting in an F2-Score of\n0.56. This approach improved performance compared to zero-\nshot but was not as effective as training the entire model from\nscratch, which achieved an F2-Score of 0.67. This suggests\nthat the feature maps learned from the VoLTE dataset did\nnot generalize well to the new dataset, necessitating retrain-\ning. The most significant improvement was observed when\nwe fine-tuned the entire pretrained model on the Mobility\nManagement dataset, resulting in an F2-score of 0.72. This\nperformance surpassed that of training the full model from\nscratch, indicating a positive transfer of knowledge between\nthe two datasets. Fine-tuning the entire model allowed us to\nleverage the pre-existing knowledge from the VoLTE dataset\nwhile effectively adapting to the new data context."}, {"title": "4.4 Limitations of PCAPVision", "content": "PCAPVision faces two main limitations in its current im-\nplementation. Firstly, its accuracy is closely linked to the\naccuracy of CFA, which automatically labels PCAP files in\nboth training and control datasets. Inaccuracies in CFA's la-\nbeling directly affect PCAPVision's performance. Secondly,\nPCAPVision is currently limited to processing files up to 2.5\nMB, with excess data truncated. To address this, adjustments\nto the input size of the first layer or segmentation of larger\nfiles into smaller chunks using tools like Wireshark's editcap\n[20], can ensure complete analysis without data loss."}, {"title": "5 Conclusions", "content": "This work introduces PCAPVision, an innovative approach\nfor detecting failures in PCAP files using computer vision\nand Convolutional Neural Networks (CNNs). By converting\nPCAP data into images, it harnesses the pattern recogni-\ntion capabilities of CNNs for efficient and accurate network\ntraffic analysis, surpassing traditional parsing methods in\nspeed and precision. A key innovation of PCAPVision is its\ncontinual learning framework, which maintains robust per-\nformance over time by dynamically adapting to new data.\nUsing techniques like experience replay and adaptive regu-\nlarization, the framework mitigates catastrophic forgetting,\nensuring the model remains effective in evolving network\nenvironments. Demonstrating its versatility, PCAPVision\nwas initially trained on a VoLTE dataset and subsequently\nfine-tuned on a Mobility Management dataset, highlight-\ning its transfer learning capabilities across various network\nservices.\nIn high-rate, high-volume production environments, PCAPVi-\nsion can significantly reduce storage costs by filtering out\nnon-failure PCAPs, ensuring that only relevant data are\nstored and analyzed. This efficiency optimizes resource us-\nage and accelerates the failure detection process, facilitating\nrapid identification and remediation of network issues.\nFuture work could explore ensemble weighted methods to\nimprove PCAPVision's accuracy and reliability. By integrat-\ning multiple models specialized in different aspects of net-\nwork traffic analysis, overall performance could be markedly\nimproved. Expanding the application of PCAPVision to other\ndomains of network traffic analysis could broaden its utility\nand impact."}, {"title": "A Deep Learning Architectures", "content": "We evaluated various architectures to detect failure in PCAP\nfiles, aiming to identify the optimal, minimal model structure.\nThis process involved evaluating different configurations,\nincluding the number and types of layers, to identify the most\neffective topology for our specific tasks. Our training dataset\nconsists of 1,222 PCAP files containing VoLTE call-flows (see\nInitial Training Process and Dataset Used subsection 3.1).\nIn this supplementary section, we outline the experiments\nincorporated in the sensitivity analysis. Initially, we explored\na Long Short-Term Memory (LSTM) model [21], where each\nPCAP file is transformed into a one-dimensional signal. The\nlength of the PCAP file dictates the number of time-steps in\nthe LSTM, providing flexibility in handling varying PCAP file\nsizes due to the model's invariance to time-step count. How-\never, training results indicated unsatisfactory convergence,\nsuggesting that the diverse sizes of PCAP files might hinder\noptimization and contribute to training instability. Addition-\nally, we observed issues such as vanishing or exploding gra-\ndients [22], possibly triggered by anomalous byte sequences\ninherent in the PCAP samples. Next, we explored convert-\ning PCAP samples into 2-dimensional images, employing\na convolution-based model architecture. We hypothesized\nthat both the temporal information and contextual details in\nPCAP data could enhance failure prediction capabilities, sup-\nported by the observation that consecutive groups of packets\nprovide insights into the timing and sequence of network\nevents, which are crucial for analyzing network performance\nmetrics and addressing issues related to latency, packet loss,\nand jitter. Our initial model configuration included a single\n2-dimensional convolutional layer, serving as a feature ex-\ntractor, followed by a Flatten layer and two fully connected\n(FC) layers for binary classification. During training, we con-\nducted a quantitative analysis to assess model generalization\nand the extent of overfitting using a hold-out validation set.\nResults indicated overfitting on the training data, prompting\nus to introduce max pooling and a dropout operation prior\nto the FC layers to enhance model generalization. We exper-\nimented with adding mirror padding to the PCAP images to\nprovide additional contextual information for the convolu-\ntional kernels, addressing the challenge of edge pixels in a\nrow-wise converted image, which are often correlated with\ntheir opposite counterparts. Adding mirror padding helped\nmitigate the loss of potentially valuable information across\nimage boundaries. The inclusion of padding improved con-\ntextual awareness, increased the speed of propagation, and\nallowed for more efficient batching. We also placed a dropout\nlayer between the padding and convolution operations to\nfurther reduce overfitting, enhancing the performance of our\nmodel.\nWe employ the Adam optimizer [17] with a learning rate\nof 0.0005 across all models. The loss function used is Binary\nCross Entropy [18], with the decision threshold determined\nby analyzing the precision-recall trade-off on the validation\nset. Given the high impact of false negatives in failure pre-\ndiction tasks, we emphasize recall in the threshold selection\nprocess and adopt the F2-score to prioritize this metric. Table\n2 summarizes our sensitivity analysis and grid search results\nfor determining the optimal model architecture. The final\narchitecture of our model (described in the section Method-\nology 3) has two primary components: the feature extrac-\ntor and the predictor. The feature extractor includes mirror\npadding, a dropout layer, a single convolutional layer with\nfour kernels, followed by max pooling and flattening opera-\ntions. The predictor comprises two blocks, each containing\na dropout layer and a FC layer."}]}