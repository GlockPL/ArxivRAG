{"title": "Evaluation of Deep Audio Representations for Hearables", "authors": ["Fabian Gr\u00f6ger", "Pascal Baumann", "Ludovic Amruthalingam", "Laurent Simon", "Ruksana Giurda", "Simone Lionetti"], "abstract": "Effectively steering hearable devices requires un- derstanding the acoustic environment around the user. In the computational analysis of sound scenes, foundation models have emerged as the state of the art to produce high-performance, robust, multi-purpose audio representations. We introduce and release Deep Evaluation of Audio Representations (DEAR), the first dataset and benchmark to evaluate the efficacy of foundation models in capturing essential acoustic properties for hearables. The dataset includes 1,158 audio tracks, each 30 seconds long, created by spatially mixing proprietary monologues with com- mercial, high-quality recordings of everyday acoustic scenes. Our benchmark encompasses eight tasks that assess the general context, speech sources, and technical acoustic properties of the audio scenes. Through our evaluation of four general-purpose audio representation models, we demonstrate that the BEATS model significantly surpasses its counterparts. This superiority underscores the advantage of models trained on diverse audio col- lections, confirming their applicability to a wide array of auditory tasks, including encoding the environment properties necessary for hearable steering. The DEAR dataset and associated code are available at https://dear-dataset.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "The World Health Organisation (WHO) estimates that al- most half a billion people suffer from disabling hearing loss [1]. This condition has been extensively proven to have a strong negative impact on social interactions and cognitive abilities [2]-[4]. Hearing aids are the most common medi- cal devices used to mitigate hearing difficulties by actively filtering and processing sound for the user. However, many individuals report hearing challenges even without a diagnosis of clinical hearing loss [5]. While these individuals are not typically prescribed hearing aids, they could benefit from fea- tures like active noise cancellation and speech enhancement, which are now available in the latest hearable devices [6]. Independently of whether they are medical devices or not, the final goal of such instruments is to help their users in challenging acoustic situations.\nIn order to apply the correct processing, the devices first need to analyze the surrounding environment. This step can be challenging, especially in complex and dynamic acoustic scenes with concurrent sound sources. Moreover, the dimen- sions, battery lifetime, and usability of the devices themselves set strict requirements on power consumption, available mem- ory, possible number of operations per cycle, and maximum allowed delay. However, the technological leap witnessed in the past years has opened the doors to solutions that used to be considered beyond reach. Whether in a medical device, an off-the-shelf product, or a hearable, artificial intelligence (AI) can provide measurable benefits for users [7]\u2013[10].\nDeep learning (DL) in particular has enabled remarkable results in audio analysis [11], and is making an impact for hearing devices and hearables [12]. A major recent trend is the development of DL models that are not optimized to solve a single task, but learn representations that can be used for diverse purposes. These are generally referred to as foundation models, and are typically trained with self-supervised learning (SSL) on large amounts of unlabeled data. Thanks to their versatility, these neural networks have rapidly gained popu- larity, in particular for their ability to handle diverse data in their training domain and to generalize beyond their training context. Evaluating such models requires testing their capacity to accurately encode diverse properties, which can be human- given labels or characteristics collected together with the data.\nIn the case of audio, this was pioneered by NOSS [13], LeBenchmark [14], and SUPERB [15], which focus on speech tasks, and first extended beyond speech by HARES [16]. The HEAR [17] and LAPE [18] benchmarks followed, addressing the comprehensiveness of evaluation tasks and low-resource environments, respectively. More recently, ARCH [19] was developed to complement and extend HEAR in evaluating general-purpose audio representations. Most closely related our work is the ACE challenge [20], which explicitly focuses on a general audio dataset for RT60 and DRR.\nFoundation models for audio are great candidates for be- coming the basic signal processing engines for hearables. There are however still significant gaps in understanding how well these models would perform for typical hearable steering in real life. A first major shortcoming is that none of the benchmarks mentioned above probes basic acoustic properties such as reverberation or signal-to-noise ratio (SNR), which are crucial for signal processing in hearables. Technical reverberation parameters such as direct-to-reverberant energy ratio (DRR) and reverberation time (RT60) plus SNR are known to heavily affect binaural cues [21], and are thus essential for algorithms involving acoustic source localiza- tion, distance estimation, spatial perception preservation, and speech intelligibility prediction [22]\u2013[24]. Another issue is that most benchmarks are based on well-known datasets, which makes it possible for some models to exploit leaks between training and evaluation data [25].\nWe present and release the first benchmark dataset for the evaluation of audio representations in the context of hearables, called Deep Evaluation of Audio Representations (DEAR). The dataset consists of 1,158 mono tracks of 30 seconds each, obtained by mixing proprietary monologue and dialogue recordings into acoustic scenes from a licensed database. The benchmark includes eight classification and regression tasks covering general context, speech sources, and most impor- tantly acoustic properties of audio scenes. We compare four public self-supervised audio representation models and find that BEATs significantly outperforms all others. In particular, we find that this model encodes technical acoustic properties considerably better than competitors, which gives important clues on how to design general-purpose audio models for hearables in the future. The DEAR dataset and associated code are available at https://dear-dataset.github.io."}, {"title": "II. METHODOLOGY", "content": "The DEAR benchmark is generated by adding speech signals to background sound scenes to ensure full control over the acoustic properties of the final mixture. The background recordings were selected from the HOA-SSR dataset sound scene library (Force Technology, Denmark),\u00b9 which is a cu- rated collection of 150 audiovisual scenes captured using spe- cialized equipment, designed for comprehensive evaluations in audio product development. In particular, we use the 4th-order ambisonics audio, which was recorded using an Eigenmike em32 and encoded in 25-channel AmbiX format at 48 kHz with a bit depth of 24. The category selection has the purpose of capturing typical everyday situations. The speech signals are proprietary anechoic monologues recorded with Lavalier microphones. They span different vocal effort levels, which are elicited by playing pink noise through headphones at different levels. The anechoic speech signals are then convolved with a set of impulse responses to produce sound mixtures with different combinations of speakers, positions, reverberation, and SNRs. Throughout the process, attention was paid to avoid violations of the overall consistency of the generated sound scenes.\nThis procedure resulted in 1,158 audio tracks of 30s length with up to three speech signals, which may be active only at certain times. Since the dataset is meant to explore public models that are mostly trained with mono-channel audio, the scenes are down-mixed to a single reference track at 44.1 kHz sample rate and bit depth of 32. Each track is assigned to either model development or test to ensure fair comparison when using the dataset as a benchmark. These two splits are generated using disjoint sets for all three components\u2014 monologues, background environment recordings, and impulse responses. Monologues are all from distinct speakers, which prevents leakage, and each impulse response is associated with a specific background environment recording."}, {"title": "B. Tasks", "content": "Controlled dataset generation enables accurate labels at the track level, for each speech signal, and running annotations for each second. Each track is assigned a type of environment, as well as a categorization as indoor or outdoor recording. The reverberation of each speech signal as a whole is character- ized by its DRR and its RT60. Furthermore, for each non- overlapping 1s segment, each speech signal is specified as active or not active, and in the active case the corresponding SNR is provided. Finally, a dedicated set of tracks without speech signals contains only sound scenes dominated by noise, categorized as stationary or transient.\nThe suite of evaluation tasks on the DEAR dataset are summarized in the first three horizontal sections of table I and divided into three groups. Each group probes the ability of models to encode certain information, namely acoustic scene context for the first group, speech sources in the foreground for the second group, and acoustic properties of those sources for the third group. For the acoustic scene context, we consider multi-class environment classification of whole tracks into one of five semantic categories: domestic, leisure, nature, professional, and transport, as well as binary classification of whole tracks as indoor or outdoor. We also investigate binary noise classification as either stationary or transient using the corresponding tracks. In this case we select the 30 seconds segment with the highest power to ensure sufficient information content. To probe speech detection and speaker count in a relatively challenging setting, we start from a set of tracks that contain at least one speech source, and based on non-overlapping 1s segments, we classify whether speech is present or not, and regress the number of active speakers. On the segments that have only one speech source active, we regress DRR in dB and RT60 on logarithmic scale, and on segments with only one speech source active, SNR in dB."}, {"title": "C. Models", "content": "We selected publicly available general-purpose audio rep- resentation models, with the intent to cover a wide variety of different pre-training methods, and taking into account performance on typical representation benchmark datasets such as LibriSpeech [28], AudioSet [29], and ESC-50 [30]. This resulted in the selection four models: Wav2Vec2.0 [31], HUBERT [32], WavLM [33], and BEATs [34]. In this section, we first introduce the learning techniques underlying the four models. We then discuss their common base architecture and specify their pre-training datasets.\nWav2Vec2.0 [31] improves upon the original Wav2Vec [35] using a more advanced architecture including a convolutional neural network for extracting latent speech features, followed by a transformer network to capture contextual dependencies. The model uses a contrastive learning objective during pre- training, where it learns to distinguish true speech representa- tions from distractors. HuBERT [32] builds on the concept of masked prediction, similar to BERT [36] in natural language processing, adapted to speech. HuBERT works by learning to predict masked segments of continuous speech data using hidden units derived from clustering the speech features. The model is trained in a two-step process: first, it clusters speech features from a pre-trained model to generate pseudo- labels, and then it predicts these labels for masked portions of the input audio during training. WavLM [33] builds upon the architecture of previous models like Wav2Vec2.0 and HuBERT. The model employs a transformer-based architecture and is trained using a masked speech prediction approach, where parts of the input audio are masked, and the model learns to predict these masked segments. BEATS [34] uses acoustic tokenizers to convert continuous audio signals into discrete tokens, similar to how words are tokenized in text processing. The model uses a transformer-based architecture and learns to predict these discrete tokens from masked input audio segments during training, following a masked language model approach similar to WavLM and HuBERT. Pre-training is done in multiple iterations, and here we use the last iteration.\nThe architecture underlying all four models is a transformer of size \"base\" with 12 encoder layers, 768-dimensional hid- den states, and 8 attention heads, resulting in 90 million parameters. The publicly available models were pre-trained on different datasets, namely Wav2Vec2.0 and HuBERT on Lib- rispeech [28], WavLM on LibriSpeech [28], GigaSpeech [37], and VoxPopuli [38], and BEATs on AudioSet [29]."}, {"title": "D. Evaluation", "content": "Following standard practice in representation learning, mod- els are evaluated on unseen downstream tasks [17]. Specifi- cally, the pre-trained models are used to obtain representations for audio segments, and a linear or k-nearest neighbor (kNN) model is trained to solve each task based on frozen represen- tations only. Note that nearest neighbor evaluation probes the local structure of the feature space and has been shown to correlate well with fine-tuning performance [39]. This frozen evaluation examines how well representations encode a certain dataset attribute, indicating whether inherent properties of the downstream task were learned during pre-training. The tracks reserved for evaluation in the DEAR dataset are held out and used to compute the final performance, and the ones desig- nated for development are further split into non-overlapping training and validation splits. For kNN, the validation split is used to choose the number of neighbors among the values 1, 2, 5, 10, 20, and 50-separately for each model and task. To ease result interpretation, we evaluate both binary and multi- class classification performance using Matthews' correlation coefficient $\\Phi$ and regression performance using the coefficient of determination $R^2$ over all testing tracks or 1s segments. Both $\\Phi$ and $R^2$ take the value of 1 for perfect performance, and they are 0 for random models."}, {"title": "III. RESULTS", "content": "Figure 1 shows the performance of the four public pre- trained representation models described in section II-C on the suite of downstream tasks presented in section II-B, in terms of both kNN and linear performance. Results indicate that BEATs outperforms other models across all tasks, often by a large margin. Remarkably, this also holds in the group related to speech sources, even if the other three models were focused on speech analysis. Although performance on the estimation of acoustic properties has room for improvement, BEATs obtains remarkable results, especially compared to its competitors. This suggests that BEATs has successfully learned to encode SNR and some reverberation characteristics without explicit guidance towards these goals. The success of the model could be attributed both to the label prediction objective from acoustic tokenizers as opposed to a contrastive objective and to the large and varied audio collection it was trained on, namely AudioSet. A recent work suggests that the latter might be particularly important to capture multiple aspects of audio analysis at the same time [19]. Nevertheless, we observe that some tasks such as DRR estimation remain difficult and deserve separate investigation.\nIn the context group, which includes one 5-way and two binary classification tasks, all models demonstrate very good performance, even though BEATs still shows an advantage compared to the others. Further insight can be obtained by looking at the public TUT2017 acoustic scene classification task, where model performance is lower. This suggests that fine-grained classification is more difficult, which is plausible. While all models encode the presence of speech, counting the number of active speakers proves to be challenging in the DEAR dataset. Looking at a similar publicly available task, LibriCount, reveals that models perform significantly better there than on the new benchmark. This could be due to intrinsically more difficult noise backgrounds in DEAR, or to the overlap between the pre-training and downstream data at least for HuBERT, Wav2Vec2.0, and WavLM.\nFor LibriCount, TUT2017, and detecting the presence of speech, the performance under linear evaluation is typically slightly higher than its kNN counterpart. This behavior can not be observed for the other tasks, and there is no clear pattern among the different evaluation types."}, {"title": "IV. CONCLUSION", "content": "In this study, we introduced DEAR, the first dataset and benchmark specifically designed to evaluate the performance of audio foundation models in encoding essential properties for effective steering in hearables its general context, the presence and identification of speech sources, and technical acoustic properties such as reverberation and SNR. Our experi- ments show that out of four compared models, BEATs strongly outperforms all competitors across eight prospective and two retrospective tasks. For reverberation properties, the difference can be as high as 30 percentage points, and BEATs with kNN or linear regression effectively enables a qualitatively higher level of performance. This benchmark sets the stage for the development of foundation models that are applicable to or even specialized for hearables, which will in turn enhance their signal processing capabilities and the overall user experience. Audio foundation models like BEATs are currently still too complex for edge devices, but they allow sharing large parts of the computation for multiple tasks, and further advancements may soon reduce the gap to integration in hearable products."}]}