{"title": "MAGIC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM", "authors": ["Vladimir Yugay", "Theo Gevers", "Martin R. Oswald"], "abstract": "Simultaneous localization and mapping (SLAM) systems with novel view synthesis capabilities are widely used in computer vision, with applications in augmented reality, robotics, and autonomous driving. However, existing approaches are limited to single-agent operation. Recent work has addressed this problem using a distributed neural scene representation. Unfortunately, existing methods are slow, cannot accurately render real-world data, are restricted to two agents, and have limited tracking accuracy. In contrast, we propose a rigidly deformable 3D Gaussian-based scene representation that dramatically speeds up the system. However, improving tracking accuracy and reconstructing a globally consistent map from multiple agents remains challenging due to trajectory drift and discrepancies across agents' observations. Therefore, we propose new tracking and map-merging mechanisms and integrate loop closure in the Gaussian-based SLAM pipeline. We evaluate MAGIC-SLAM on synthetic and real-world datasets and find it more accurate and faster than the state of the art.", "sections": [{"title": "1. Introduction", "content": "Visual Simultaneous Localization and Mapping (SLAM) enables a machine to build a 3D map of its surroundings while determining its location relying solely on its camera input. Imagine an autonomous car navigating a busy city or a robot exploring a cluttered room. For these systems to move safely and make decisions, they need a reliable understanding of their surroundings. Over the years, visual SLAM has advanced [9, 14, 37], learning to handle complex scenes with increasing accuracy. This capability forms the backbone for various systems from self-driving cars to virtual reality glasses, where spatial understanding is crucial for navigation, planning, and interactive experiences.\nRecently, SLAM systems have been enhanced with novel view synthesis (NVS) capabilities [15, 23, 36], allowing them to generate realistic, immersive views of scenes. This allows more detailed scene exploration and supports the creation of high-quality virtual environments. However, these NVS-capable SLAM methods are slow, and tracking accuracy remains limited [15]. These limitations become even more pronounced in a multi-agent setup, with larger scenes and numerous observations.\nEffectively processing information from multiple agents operating simultaneously opens up new possibilities for SLAM systems. First, this naturally accelerates 3D reconstruction by enabling horizontal parallelization; each agent can map different parts of the environment concurrently, leading to faster, more efficient coverage of large areas [5]. Second, agents can collaboratively refine their location estimates by sharing their observations [18]. Finally, the global map constructed from the perspectives of multiple agents is more geometrically consistent and accurate [33].\nThe only solution for multi-agent NVS-capable SLAM hitherto has combined a distributed neural scene representation with a traditional loop closure mechanism [11]. However, this approach faces several issues. Firstly, it fails to reconstruct a coherent map capable of accurate NVS. Neural scene representations inherently lack support for rigid body transformations [20] making it impossible to update and merge the global map efficiently. This in turn leads to poor rendering quality of the novel views. Secondly, localization accuracy remains limited, as neural-based camera pose optimization struggles with the variability and imperfections inherent to real-world data. Finally, the system is prohibitively slow, demands extensive computational resources, and supports only two agents.\nWe present MAGiC-SLAM, a multi-agent NVS-capable SLAM pipeline designed to overcome these limitations. Firstly, MAGIC-SLAM utilizes 3D Gaussians as the scene representation supporting rigid body transformations. Every agent processes the input RGBD sequence in chunks (sub-maps) which can be effectively corrected and merged into a coherent global map. Secondly, we implement a loop closure mechanism to improve trajectory accuracy by leveraging information from all agents. It is further enhanced with a novel loop detection module, based on a foundational vision model, which enables better generalization to unseen environments. Finally, our flexible tracking and mapping modules allow our system to achieve superior speed and easily scale to varying numbers of agents.\nOverall, our contributions can be summarized as follows:\n\u2022 A multi-agent NVS-capable SLAM system for consistent 3D reconstruction supporting an arbitrary number of simultaneously operating agents\n\u2022 A loop closure mechanism for Gaussian maps leveraging a foundational vision model for loop detection\n\u2022 Efficient map optimization and fusion strategies that reduce required disk storage and processing time\n\u2022 A robust Gaussian-based tracking module"}, {"title": "2. Related Work", "content": "Neural SLAM. Neural radiance fields (NeRF) [24] have achieved remarkable results in novel view synthesis [2, 25, 42]. [36] started a whole new line of research [20, 21, 32, 38, 41, 45, 47] by proposing a SLAM system using neural fields to represent the map and optimize the camera poses. Various neural field approaches have provided insights for neural SLAM developments. For instance, NICE-SLAM [47] uses a voxel grid to store neural features, while Vox-Fusion [41] improves the grid with adaptive sizing. Point-SLAM [32] attaches feature embeddings to point clouds on object surfaces, offering more flexibility and the ability to encode concentrated volume density. Co-SLAM [38] employs a hybrid representation combining coordinate encoding and hash grids to achieve smoother reconstruction and faster convergence. A group of methods [6, 30] use neural fields solely for mapping while relying on traditional feature point-based visual odometry for tracking. Loopy-SLAM [20] explores the usage of pose-graph optimization to handle trajectory drift. While these methods have shown success in NVS, they are computationally intensive, slow, and struggle to render real-world environments accurately [15, 43]. Additionally, neural maps inherently lack support for rigid body transformations [20], which greatly limits the efficiency of map correction. In contrast, our scene representation is fast to render and optimize, significantly accelerating our SLAM pipeline. With native support for rigid body transformations, our approach enables faster map updates and seamless map merging.\nGaussian SLAM. Recently, 3D Gaussian Splatting(3DGS) [16] revolutionized novel view synthesis, achieving photorealistic real-time rendering at over 100 FPS without relying on neural networks. Compared to NeRFs, 3DGS is more efficient in terms of memory, and faster to optimize. These factors inspired the line of SLAM systems [12, 13, 15, 23, 40, 43] using 3D Gaussians instead of neural fields for tracking and mapping. [12, 23, 40] estimated camera pose by computing camera gradients from the 3DGS field analytically. Others [15, 43] design a warp-based camera pose optimization. Finally, [13, 29] use existing sparse SLAM systems to speed up tracking.\nWhile addressing many limitations of neural SLAM, the proposed methods overlook global consistency, leading to trajectory error accumulation [34]. In contrast, MAGiC-SLAM integrates a loop closure mechanism within the 3DGS SLAM pipeline to correct accumulated trajectory errors. It effectively generalizes to unknown environments leveraging a foundational vision model for loop detection. Concurrently, Zhu et al. [46] proposed a way to do loop closure in a 3DGS-based SLAM pipeline. However, the method cannot handle multiple agents, uses a standard loop closure detection mechanism, and is less efficient in tracking and mapping.\nMulti-agent Visual SLAM. Despite significant progress in single-agent systems, multi-agent SLAM remains less developed due to the complexity of designing a multi-agent pipeline. Multi-agent SLAM can be categorized into two"}, {"title": "3. Method", "content": "We introduce MAGIC-SLAM, which architecture is shown in Figure 2. Every agent processes an RGB-D stream performing local mapping and tracking. 3D Gaussians are used to represent the agents' sub-maps and to improve tracking accuracy. The foundation vision model-based loop detection module extracts features from RGB images and sends them to the centralized server with the local sub-map data. The server detects the loops based on the image encodings, performs pose graph optimization, and sends the optimized poses back to the agents. At the end of the run, the server fuses the agents' sub-maps into a global Gaussian map. This section introduces per-agent mapping and tracking mechanisms and describes global loop closure and map construction processes."}, {"title": "3.1. Mapping", "content": "Every agent processes a single sub-map of a limited size, represented as a collection of 3D Gaussians. Sub-maps are initialized from the first frame.  Osample points sampled from the lifted to 3D RGBD frame serve as the means for new 3D Gaussians. New Gaussians are added to regions of the active sub-map with low Gaussian density, based on rendered opacity, and are optimized using the loss:\n\\begin{equation}\nL_{mapping} = \\lambda_{color} \\cdot L_{color} + \\lambda_{depth} \\cdot L_{depth} + \\lambda_{reg} \\cdot L_{reg},\n\\end{equation}\nwhere are \\(\\lambda_* \\in \\mathbb{R}\\) are hyperparameters. The color loss \\(L_{color}\\) is is defined as:\n\\begin{equation}\nL_{color} = (1 - \\lambda) \\cdot |\\hat{I} - I|_1 + \\lambda (1 \u2013 SSIM(\\hat{I}, I)),\n\\end{equation}\nwhere \\(\\hat{I}, I\\) are the rendered and input images respectively, and \\(\\lambda \\in \\mathbb{R}\\) is the weighting factor. Depth loss \\(L_{depth}\\) is formulated as:\n\\begin{equation}\nL_{depth} = |\\hat{D} - D|_1,\n\\end{equation}\nwhere \\(\\hat{D}, D\\) are the rendered and input depth maps. The regularization loss \\(L_{reg}\\) is represented as:\n\\begin{equation}\nL_{reg} = |K|^{-1} \\sum_{k \\in K}|s_k - \\bar{s}_k|_1,\n\\end{equation}\nwhere \\(s_k \\in \\mathbb{R}^3\\) is the scale of a 3D Gaussian, \\(\\bar{s}_k\\) is the mean sub-map scale, and \\(|K|\\) is the number of Gaussians in the sub-map. We do not optimize the spherical harmonics of the Gaussians to reduce their memory footprint and improve tracking accuracy [23].\nA new sub-map is created, and the previous one is sent to the server after every @submap frame. Creating new sub-maps in already mapped areas introduces some computational redundancy, but it limits overall computational cost and maintains tracking and mapping speed as the scene expands. Moreover, unlike in [43], only Gaussians with zero rendered opacity in the current camera frustum are dispatched to the server. This approach significantly reduces the disk space required to store the sub-maps and speeds up the map merging process. The supplementary material provides a quantitative evaluation of the strategy."}, {"title": "3.2. Tracking", "content": "Typically GS SLAM systems optimize camera pose either explicitly or implicitly. In the explicit case, the camera pose gradient is derived [22] or approximated [12, 40] analytically. In the implicit case [15, 43], the relative pose between two frames is optimized by warping the GS point clouds and minimizing re-rendering depth and color errors. In both cases, the camera pose is estimated based on the existing map, following a frame-to-model paradigm [14].\nConsidering the advantages and disadvantages of frame-to-frame and frame-to-model tracking paradigms [9, 14, 37] and the convenience of using sub-maps for loop closure, we propose a hybrid implicit tracking approach that combines the strengths of both. Specifically, we initialize the relative pose using a deterministic frame-to-frame dense registration and then refine it through frame-to-model optimization. This approach differs from previous NVS SLAM methods, initializing the relative pose based on a constant speed assumption.\nMoreover, we found implicit tracking to be more accurate than the explicit approach given robust pose initialization. At the same time, explicit tracking methods do not benefit from pose initialization since camera poses are optimized across the whole co-visibility window at every mapping step. We refer to supplementary material for experiments highlighting this phenomenon."}, {"title": "Pose Initialization", "content": "At time t, given the input colored point cloud \\(P_t\\), the goal is to estimate its pose \\(T_{t-1,t} \\in SE(3)\\) relative to the previous input point cloud \\(P_{t-1}\\). The registration process is performed iteratively at several scales, starting from the coarsest, l = 0, down to the finest, l = L. At every scale, both point clouds are voxelized, and the set of correspondences \\(K = \\{(p,q)\\} \\) between \\(P_t\\), \\(P_{t-1}\\) is computed using ICP [3]. For every scale, a loss function:\n\\begin{equation}\nE(T_{t-1,t}) = (1 - \\sigma) \\sum_{(p,q) \\in K} (r_G^{(p,q)}(T_{t-1,t}))^2\n\\end{equation}\n\\begin{equation}\n+ \\sigma \\sum_{(p,q) \\in K} (r_c^{(p,q)}(T_{t-1,t}))^2,\n\\end{equation}\nis optimized where \\(\\sigma \\in [0, 1]\\) is a scalar weight. \\(r_G^{(p,q)}\\) is a geometric residual defined as:\n\\begin{equation}\nr_G^{(p,q)} = ((\\mathbf{T}_{t-1,t}(q) - p) \\cdot n_p)^2,\n\\end{equation}\nwhere \\(n_p\\) is a normal vector of p. The color residual \\(r_c^{(p,q)}\\) is computed as:\n\\begin{equation}\nr_c^{(p,q)} = C_p(f_p(\\mathbf{T}_{t,t-1}(q)) - p) \u2013 C(q),\n\\end{equation}\nwhere \\(f_p\\) is a function projecting a point on the tangent plane of p, \\(C(q)\\) is the intensity of point q, and \\(C_p\\) is a continuous intensity function defined over point cloud \\(P_t\\). The loss is optimized until convergence with the Gauss-Newton method. For more details about the registration mechanism please refer to [28]."}, {"title": "Pose Refinement", "content": "The pose obtained in the initialization step is further refined using re-rendering losses [43] of the scene. To refine the initial pose estimate, we freeze all Gaussian parameters and minimize the loss:\n\\begin{equation}\n\\arg \\min_{T_{t-1,t}} L_{tracking}(\\hat{I}(T_{t-1,t}), \\hat{D}(T_{t-1,t}), I_t, D_t, \\alpha_t),\n\\end{equation}\nwhere \\(\\hat{I}(T_{t-1,t})\\) and \\(\\hat{D}(T_{t-1,t})\\) are the rendered color and depth from the sub-map warped with the relative transformation \\(T_{t-1,t}\\), \\(I_t\\) and \\(D_t\\) are the input color and depth map at frame t.\nWe use soft alpha and color rendering error masking to avoid contaminating the tracking loss with pixels from previously unobserved or poorly reconstructed areas [15, 43]. Soft alpha mask \\(M_{alpha}\\) is a polynomial of the alpha map rendered from the 3D Gaussians. Error boolean mask \\(M_{inlier}\\) discards all the pixels where the color and depth errors are larger than a frame-relative error threshold:\n\\begin{equation}\nL_{tracking} = \\sum M_{inlier} \\cdot M_{alpha} \\cdot (\\lambda_c |\\hat{I}-I|_1 + (1 - \\lambda_c)|\\hat{D}-D|_1).\n\\end{equation}\nThe weighting ensures the optimization is guided by well-reconstructed regions where the accumulated alpha values are close to 1 and rendering quality is high."}, {"title": "3.3. Loop Closure", "content": "Loop closure is the process that detects when a system revisits a previously mapped area and adjusts the map and camera poses to reduce accumulated drift, ensuring global consistency. It includes four key steps: loop detection, loop constraint estimation, pose graph optimization, and integration of optimized poses. Loop detection identifies when a previously mapped location is revisited. Loop constraint estimation calculates the relative pose between frames in the loop. Pose graph optimization then refines all camera poses, minimizing discrepancies between odometry and loop constraints to maintain global consistency. Finally, the optimized poses are integrated into the reconstructed map.\nLoop Detection. Throughout the run, each agent extracts features from the first frame in each sub-map and sends them to a centralized server. The features are stored in a GPU database [8] optimized for similarity search. The"}, {"title": "Pose Graph Optimization", "content": "Each node in a pose graph, \\(T_i \\in SE(3)\\), represents a distinct sub-map. Neighboring sub-maps are connected by odometry edges, derived from the tracker, representing the relative transformations between them. Loop edge constraints \\(T_{st} \\in SE(3)\\) are added between non-adjacent sub-maps and computed using a registration method different from that of the tracker. The error term between two nodes i and j is defined as:\n\\begin{equation}\nc_{ij} = \\frac{1}{2} ||\\log(\\mathbf{T}_{ij}^{-1} (\\mathbf{T}_i^{-1} \\mathbf{T}_j))||,\n\\end{equation}\nwhere \\(T_{ij}\\) is the relative transformation between the nodes i and j, \\(T_i, T_j\\) are the node poses, and log is the logarithmic map from SE(3) to se(3). To get the optimized camera poses we minimize the error term:\n\\begin{equation}\nF(\\mathbf{T}) = \\sum_{(i,j) \\in E} (e(\\mathbf{T}_i, \\mathbf{T}_j)^T \\Omega_{ij} e(\\mathbf{T}_i, \\mathbf{T}_j))\n\\end{equation}\nwhere \\(\\Omega_{ij} \\in \\mathbb{R}^{6\\times 6}\\) is a positive semi-definite information matrix reflecting the uncertainty of the constraint estimate: the higher the confidence of an edge, the larger the weight is applied to the residual. The error terms are linearized using a first-order Taylor expansion, and the loss is minimized with the Gauss-Newton method. For further details, please see [17]."}, {"title": "Pose Update Integration", "content": "The pose graph optimization module provides pose corrections \\({\\bf T}_i^* \\in SE(3)\\) for every sub-map i of every agent. All camera poses \\({\\bf T}_i\\}_{i=1}^N\\) belonging to a sub-map i are corrected as:\n\\begin{equation}\n{\\bf T}_i \\leftarrow {\\bf T}_i^* T_i.\n\\end{equation}\nAll Gaussians \\({\\mathcal{G}(\\mu, \\Sigma, o,)} \\) belonging to sub-map i are updated as well:\n\\begin{equation}\n\\mu \\leftarrow {\\bf T}_R \\mu, \\ \\Sigma \\leftarrow {\\bf T}_R \\Sigma\n\\end{equation}\nwhere \\({\\bf T}_R\\) is a rotation component of \\({\\bf T} \\in SE(3)\\). We do not correct Gaussian colors since we do not optimize spherical harmonics (Subsection 3.1)."}, {"title": "3.4. Global Map Construction", "content": "Once the agents complete processing their data, the server merges the sub-maps from all agents into a unified global map. The map is merged in two stages: coarse and fine. During the coarse stage, the server loads the cached sub-maps and appends them into a single global map. Caching Gaussians that are not visible from the first keyframe of the next sub-map allows for appending the Gaussians without the need for costly intersection check [43]. However, this results in visual artifacts at the edges of the renderings. This happens because some Gaussians with zero opacity for a given view still influence the rendering through the projected 2D densities at the edges. Additionally, the coarse merging of sub-maps might introduce geometric artifacts at their intersections. The fine merging stage addresses these issues by optimizing the Gaussian parameters using color and depth rendering losses for a small number of iterations and pruning Gaussians with zero opacity. The visual effects of the refining step are shown in Fig. 3."}, {"title": "4. Experiments", "content": "We describe our experimental setup and compare MAGiC-SLAM with state-of-the-art baselines. We assess tracking and rendering performance on synthetic and real-world multi-agent datasets and provide ablation studies on key components of our pipeline. Please refer to the supplementary material for the implementation details and hyperparameters for tracking, mapping, and loop closure modules.\nBaselines. To evaluate tracking, following [11] we compare our method to the state-of-the-art multi-agent SLAM"}, {"title": "5. Conclusion", "content": "We present MAGiC-SLAM, a multi-agent SLAM with novel view synthesis capabilities. Thanks to our tracking and loop closure modules, MAGIC-SLAM demonstrates superior tracking accuracy on both synthetic and real-world datasets. Our efficient map-merging strategy allows high-quality rendering in various scenarios. The Gaussian-based scene representation significantly reduces processing time, disk, and VRAM consumption compared to the previous state of the art. Finally, our method can handle a varied number of simultaneously operating agents."}]}