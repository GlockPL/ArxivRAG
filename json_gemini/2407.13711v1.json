{"title": "FSP-LAPLACE: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning", "authors": ["Tristan Cinquin", "Marvin Pf\u00f6rtner", "Vincent Fortuin", "Philipp Hennig", "Robert Bamler"], "abstract": "Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predic-tions of the neural network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we have to recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodic-ity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant, e.g., in many scientific inference tasks. At the same time, it stays competitive for black-box regression and classification tasks where neural networks typically excel.", "sections": [{"title": "1 Introduction", "content": "Neural networks (NN) have become the workhorse for many machine learning tasks, but they do not quantify the uncertainty arising from data scarcity-the epistemic uncertainty. They are therefore incapable of providing a measure of confidence along with their predictions, which is needed for safety-critical applications, decision making, and scientific modeling [1-3]. As a solution, Bayesian neural networks (BNNs) cast training as approximating the Bayesian posterior distribution over the weights (i.e., the distribution of the model parameters given the training data), thus naturally capturing epistemic uncertainty. Various methods exist for approximating the (intractable) posterior, either"}, {"title": "2 Preliminaries: Laplace approximation in weight space", "content": "Consider a dataset D = (X, Y) of inputs $X = (x^{(i)})_{i=1}^{n} \\in \\mathbb{X}^{n}$ and targets $Y = (y^{(i)})_{i=1}^{n} \\in \\mathbb{Y}^{n}$.\nWe consider a model of the data that is parameterized by a neural network $f : \\mathbb{X} \\times \\mathcal{W} \\rightarrow \\mathbb{R}^{d'}$ with weights $w \\in \\mathcal{W} \\subset \\mathbb{R}^{P}$, a likelihood $p(Y | f(X,w)) := \\Pi_{i=1}^{n} p(y^{(i)} | f(x^{(i)}, w))$, and a prior $p(w)$.\nWe seek the Bayesian posterior $p(w | D) \\propto p(Y | f(X,w)) p(w)$. As it is intractable in BNNs, approximate inference methods have been developed. The Laplace approximation [16] approximates the posterior by a Gaussian distribution $q(w \\approx w^{*}) = \\mathcal{N} (w; w^{*}, \\Lambda^{-1})$, whose parameters are obtained by finding a MAP estimate $w^{*} \\in \\arg \\min_{w \\in \\mathcal{W}} R_{ws}(w)$ of the weights, where\n$R_{ws}(w) := - \\log p(w | D) = - \\log p(w) - \\sum_{i=1}^{n} \\log p(y^{(i)} | f(x^{(i)}, w)) + const. \\qquad (2.1)$\nand computing the Hessian of the negative log-posterior $A \\coloneqq H_{w} \\log p(w | D)|_{w=w^{*}} \\in \\mathbb{R}^{P \\times P}$."}, {"title": "The linearized Laplace approximation", "content": "Computing A involves the Hessian of the neural network w.r.t. its weights. This is generally associated with high computational cost. To make the Laplace approximation scalable, it is common to linearize the network around $w^{*}$ before computing A [17],\n$f(x,w) \\approx f(x, w^{*}) + J_{w^{*}}(x)(w - w^{*}) =: f^{lin}(x, w) \\qquad (2.2)$\nwith Jacobian $J_{w^{*}}(x) = D_{w} f(x,w)|_{w=w^{*}}$. Thus, the approximate posterior precision matrix A is\n$\\begin{aligned}\nA &\\approx H_{w} \\log p(w | D)|_{w=w^{*}} = - H_{w} \\log p(w)|_{w=w^{*}} - \\sum_{i=1}^{n} J_{w^{*}}(x^{(i)})^{T} L_{i j} J_{w^{*}}(x^{(i)}),\n\\end{aligned}$\nwhere $L = H_{f} p(y^{(i)} | f)|_{f=f(x^{(i)}, w^{*})}$. The Hessian of the negative log-likelihood (NLL) under the linearized network coincides with the generalized Gauss-Newton matrix (GGN) of the NLL under the non-linearized network. The GGN is equal to the Hessian if the model perfectly fits the data, or if $f(x^{(i)}, \\cdot)$ is linear for all $i = 1, \\dots, n$. Crucially, the GGN is positive-(semi)definite even if $H_{w} \\log p(Y | f(X,w))|_{w=w^{*}}$ is not."}, {"title": "3 FSP-LAPLACE: Laplace approximation under function-space priors", "content": "A conventional Laplace approximation to BNNs requires a prior in weight space, with the issues discussed in Section 1. In the following, we present FSP-LAPLACE, a method for computing Laplace approximations under interpretable GP priors in function space. Section 3.1 introduces an objective function that is a log-density under local linearization. Section 3.1 proposes a scalable algorithm for the linearized Laplace approximation with a function-space prior using matrix-free linear algebra."}, {"title": "3.1 Laplace approximations in function space", "content": "We motivate our Laplace approximation in function space through the lens of MAP estimation in an (infinite-dimensional) function space (Section 3.1) under a GP prior. As Lebesgue densities do not exist in (infinite-dimensional) function spaces, we cannot minimize Equation (2.1) to find the MAP estimate. We address this issue using a generalized notion of MAP estimation, resulting in a minimizer of a regularized objective on the reproducing kernel Hilbert space $\\mathcal{H}_{\\Sigma}$.\nMAP estimation in neural networks under Gaussian process priors The first step of the Laplace approximation is to find a MAP estimate of the neural network weights $w$, i.e., the minimizer of the negative log-density function in Equation (2.1) w.r.t. the Lebesgue measure as a \u201cneutral\u201d reference measure. In our method, we regularize the neural network in function space using a"}, {"title": "3.2 Algorithmic Considerations", "content": "Training with the FSP-LAPLACE objective function. Evaluating the FSP-LAPLACE objective proposed in the previous section requires computing the RKHS norm of the neural network. Unfortunately, this does generally not admit a closed-form expression. Hence, we use the approximation\n$\\begin{aligned}\n|| f(\\cdot, w) - \\mu||_{\\mathcal{H}_{\\Sigma}}^{2} &\\approx (f(\\mathcal{C}, w) - \\mu(\\mathcal{C}))^{T} \\Sigma(\\mathcal{C},\\mathcal{C})^{-1}(f(\\mathcal{C}, w) - \\mu(\\mathcal{C})),\n&=||h_{\\mathcal{C}}(\\cdot,w)||^2\n\\end{aligned}$\nwhere $\\mathcal{C} \\in \\mathbb{X}^{n_\\mathcal{C}}$ is a set of $n_\\mathcal{C}$ context points and $h_{\\mathcal{C}}(x, w) := \\sum(x, \\mathcal{C})\\Sigma(\\mathcal{C},\\mathcal{C})^{-1}(f(\\mathcal{C},w) - \\mu(\\mathcal{C})) \\in \\mathcal{H}_{\\Sigma}$. The function $h_\\mathcal{C}$ is the minimum-norm interpolant of $f (\\cdot, w) - \\mu$ at $\\mathcal{C}$ in $\\mathcal{H}_{\\Sigma}$. Hence, the estimator of the RKHS norm provably underestimates, i.e., $||h_{\\mathcal{C}}(\\cdot, w)|| \\leq || f(\\cdot, w) - \\mu||$.\nDuring training, we need to compute $||h_{\\mathcal{C}}(\\cdot,w)||$, at every optimizer step. Since this involves solving a linear system in $n_\\mathcal{C}$ unknowns and, more importantly, forwarding the $n_\\mathcal{C}$ context points through the neural network, we need to keep $n_\\mathcal{C}$ small for computational efficiency. We find that sampling an i.i.d. set of context points at every training iteration from a distribution $P_\\mathcal{C}$ is an effective strategy for keeping $n_\\mathcal{C}$ low while ensuring that the neural network is appropriately regularized. The resulting training procedure is outlined in Algorithm 2.\nEfficient linear Laplace approximations of the FSP-LAPLACE objective. Once a minimum $w^{*}$ of $R_{FSP}$ is found, we compute a linearized Laplace approximation at $w^{*}$. The Hessian of $R_{FSP}$ is then\n$\\Lambda = \\Sigma_{w^{*}} - \\sum_{i=1}^{n} \\Sigma_{w^{*}} J_{w^{*}}(x^{(i)})^{T} L_{i j} J_{w^{*}}(x^{(i)})$.\nAgain, the RKHS inner products in the entries of $\\Sigma$ do not admit general closed-form expressions. Hence, we use the same strategy as before to estimate $\\Sigma \\approx J_{w^{*}}(\\mathcal{C})^{T} \\Sigma(\\mathcal{C},\\mathcal{C})^{-1} J_{w^{*}}(\\mathcal{C})$ at another set $\\mathcal{C}$ of context points. Unlike above, for a Laplace approximation, it is vital to use a large number of context points to capture the prior beliefs well. Luckily, $\\Sigma$ only needs to be computed once, at the end of training. But for large networks and large numbers of context points, it is still infeasible to compute or even represent $\\Sigma$ in memory. To address this problem, we devise an efficient routine for computing (a square root of) the approximate posterior covariance matrix $\\Lambda^{\\dagger}$, outlined in Algorithm 1. Our method is matrix-free, i.e. it never explicitly constructs any big (i.e.,"}, {"title": "4 Experiments", "content": "We evaluate FSP-LAPLACE on synthetic and real-world data, demonstrating that our method effec-tively incorporates beliefs specified through a GP prior; improves performance on regression tasks for which we have prior knowledge in the form of a kernel (Mauna Loa and ocean current modeling); and shows competitive performance on regression, classification, out-of-distribution detection, and Bayesian optimization tasks compared to baselines.\nBaselines We compare FSP-LAPLACE to a deterministic neural network fitted using maximum a-posteriori estimation with an isotropic Gaussian prior on the weights (MAP) and a neural network for which we additionally compute the standard linearized Laplace approximation (Laplace) [17]. We also compare our method to a Gaussian process (GP) [13], when the size of the dataset allows it, and"}, {"title": "5 Related work", "content": "Laplace approximation in neural networks First introduced by MacKay [34], the Laplace approximation gained strong traction in the Bayesian deep learning community with the introduction of scalable log-posterior Hessian approximations [25, 35], and the so-called linearized Laplace, which solves the underfitting issue observed with standard Laplace [17, 36]. In addition to epistemic uncertainty estimates, the Laplace approximation also provides a method to select prior parameters via marginal likelihood estimation [11, 12]. Recent work has made the linearized Laplace more scalable by restricting inference to a subset of parameters [37], by exploiting its GP formulation [17] to apply methods from the scalable GP literature [38-40], or by directly sampling from the Laplace approximation without explicitly computing the covariance matrix [41]. While these approaches use the GP formulation to make the linearized Laplace more scalable, we are unaware of any method using GP priors to incorporate interpretable prior beliefs within the Laplace approximation.\nBNNs with function-space priors Function-space priors in BNNs, in the context of variational inference, demonstrate improvements in predictive performance compared to weight-space priors [8]. While this idea might seem sound, it turns out that the KL divergence in the VI objective is infinite for most cases of interest due to mismatching supports between the function-space prior and BNN's predictive posterior [14], which therefore requires additional regularization to be well defined [8]. Due to this issue, other work [42] considers generalized VI [10] using the regularized KL divergence [43] or abandons approximating the neural network's posterior and instead uses deterministic neural networks as basis functions for Bayesian linear regression [15] or the mean of a sparse GP [44]. In contrast, our method does not compute a divergence in function space, but only the RKHS norm under the prior's kernel, thus circumventing the issue of mismatching support. Alternatively, rather than directly placing a prior on the function generated by a BNN, work has investigated methods to find weight-space priors whose pushforward approximates a target function-space measure by minimizing a divergence [9, 45], using the Ridgelet transform [46], or changing the BNN's architecture [47].\nRegularizing neural networks in function space Arguing that one ultimately only cares about the output function of the neural network, it has been proposed to regularize neural networks in function space, both showing that norms could be efficiently estimated and that such regularization schemes performed well in practice [48\u201351]. Our method uses the same RKHS norm estimator as Chen et al. [49] (however, under a different kernel) by sampling a new batch of context points at each update step. Similarly, Rudner et al. [51] propose an empirical prior on the weights that regularizes the neural network in function space, which they use for MAP estimation or approximate posterior inference. The objective to fit the empirical prior resembles ours, but unlike our method uses the kernel induced by the last layer of the neural network and includes an additional Gaussian prior on the weights."}, {"title": "6 Conclusion", "content": "We propose a method for performing the Laplace approximation in neural networks using interpretable Gaussian process priors in function space. This addresses the issue that conventional applications"}, {"title": "Appendix", "content": "A Additional theory"}, {"title": "A.1 Assumptions and their applicability", "content": "Assumption A.1. $f \\sim GP (\\mu, \\Sigma)$ is a d'-output Gaussian process with index set X on $(\u03a9, A, P)$ such that\n(i) the paths of f lie (P-almost surely) in a real separable Banach space B of Rd'-valued functions on X with continuous point evaluation maps $\\delta_\u03b1 : B \\rightarrow \\mathbb{R}^{d'}$, and\n(ii) w \u2192 f(\u00b7, w) is a Gaussian random variable with values in $(B, B (B))$.\nWe denote the law of w \u2192 f(\u00b7, w) by $P_B$.\nFor this paper, we focus on $B = C(X)$, with X being a compact metric space. In this case, Assumption A.1(i) can be verified from regularity properties of the prior covariance function \u03a3 [see, e.g., 52]. Moreover, the sufficient criteria from Pf\u00f6rtner et al. [19, Section B.2] show that Assumption A.1(ii) also holds in this case.\nAssumption A.2. The potential $\u03a6_Y : B \u2192 \\mathbb{R}$ is (norm-)continuous and, for each \u03b7 > 0, there is $\u039a(\u03b7) \u2208 \\mathbb{R}$ such that $\u03a6_Y (f) > \u039a(\u03b7) \u2212 \u03b7|| f ||$ for all $f \u2208 B$.\nThis holds if the negative log-likelihood functions $l^{(i)} : \\mathbb{R}^{d'} \u2192 \\mathbb{R}, f^{(i)} \u2192 \u2212 \\log p(y^{(i)} | f^{(i)})$ are continuous and, for all \u03b7 > 0, there is K(\u03b7) \u2208 R such that $l^{(i)} (f^{(i)}) > K(f^{(i)}) + \u03b7|| f^{(i)}||^{2}$ for all $f^{(i)} \u2208 \\mathbb{R}^{d'}$. For instance, this is true for a Gaussian likelihood $l^{(i)} (f^{(i)}) = \\frac{1}{2\\sigma^2}||y^{(i)} \u2212 f^{(i)}||^{2}$.\nAssumption A.3. (i) F is a subset of $\\mathcal{H}_{\\Sigma}$, (ii) F is closed in $B\u2283 \\mathcal{H}$, and (iii) $F \\subset B$ has the Heine-Borel property, i.e., all closed and bounded subsets of F are compact in B.\nAssumption A.3(i) can be verified using a plethora of results from RKHS theory. For instance, for Sobolev kernels like the Mat\u00e9rn family used in the experiments, the RKHS is norm-equivalent to a Sobolev space [see, e.g., 53]. In this case, we only need the NN to be sufficiently often (weakly) differentiable on the interior of its compact domain X. The closure property from Assumption A.3(ii) is more difficult to verify directly. However, if we assume that W is compact and that the map W\u2192 B, w\u2192 f(., w) is continuous, then F is compact (and hence closed) as the image of a compact set under a continuous function. This is a reasonable assumption, since, in practice, the weights of a neural network are represented as machine numbers with a maximal magnitude, meaning that W is always contained in an $l_\u221e$ ball of fixed radius. Incidentally, compactness of F also entails the Heine-Borel property from Assumption A.3(iii). Alternatively, F also has the Heine-Borel property if it is a topological manifold (e.g., a Banach manifold), since it is necessarily finite-dimensional."}, {"title": "A.2 Proofs", "content": "Lemma A.1. Let (X, d) be a metric space, A \u2286 X nonempty, and\n$d(\\cdot, A): X \u2192 \\mathbb{R}_{>0}, x \u2192 \\inf_{\u03b1 \u2208 A} d(x,a)$.\nThen d(\u00b7, A) is 1-Lipschitz.\nProof. For all $x_1, x_2 \u2208 X$ we have\n$\\begin{aligned}\nd(x_2, A) &= \\inf_{\u03b1 \u2208 A} d(x_2, a)\\\\\n&<d(x_2,x_1) + \\inf_{\u03b1 \u2208 A} d(x_1, a)\\\\\n&= d(x_1,x_2) + d(x_1,A)\n\\end{aligned}$\nby the triangle inequality and hence $d(x_2, A) \u2212 d(x_1,A) \u2264 d(x_1,x_2)$. Since this argument is symmetric in $x_1$ and $x_2$, this also shows that\n$\u2212(d(x_2, A) \u2212 d(x_1, A)) = d(x_1,A) \u2212 d(x_2, A) \u2264 d(x_2,x_1) = d(x_1,x_2)$.\nAll in all, we obtain $|d(x_2, A) \u2212 d(x_1,A)| \u2264 d(x_1,x_2)$.\nLemma A.2. Let (X, d) be a metric space and A \u2286 X a closed, nonempty subset with the Heine-Borel property. Then $\\inf_{a\u2208A} d(x, a)$ is attained for all $x \u2208 X$.\nProof. Let x \u2208 X and $r > r_x := \\inf_{a\u2208A} d(x, a)$. Then $A \u2229 B_r(x) \\neq \\emptyset$ as well as $d(x, a) > r$ for all a \u2208 A \\ B_r(x) and thus $\\inf_{a\u2208A} d(x, a) = \\inf_{a\u2208A\u2229B_r(x)} d(x, a)$. Moreover, $A\u2229B_r(x)$ is compact by the Heine-Borel property and d(x,\u00b7) is continuous. Hence, the claim follows from the Weierstrass extreme value theorem.\nProposition 1. Let Assumptions A.1 to A.3 hold. For \u03bb > 0, define $\u03a6_{Y,\u03bb}: B \u2192 \\mathbb{R}, f \u2194 \u03a6_Y (f) + \\frac{1}{\u03bb} d_B(f,\\mathcal{F})$. A function $f^*$ is a weak mode of the posterior measure $P_B(df) \u221d exp(\u2212\u03a6_{Y,\u03bb}(df)) P_B(df)$ if and only if $f^* \u2208 \\mathcal{H}_\u03a3$ minimizes\n$R_{FSP}^\u03bb : \\mathcal{H}_\u03a3 \u2192 \\mathbb{R}, f \u2192 \u03a6_Y (f) + \\frac{1}{\u03bb} d_B(f,\\mathcal{F}) + \\frac{1}{2}||f \u2212 \u00b5||_\u03a3^2$.\nProof. $d_B(\u00b7, \\mathcal{F})$ is (globally) 1-Lipschitz by Lemma A.1 and bounded from below by 0. Hence, $\u03a6_{Y,\u03bb} = \u03a6_Y + \\frac{1}{\u03bb}d_B(\u00b7, \\mathcal{F})$ is continuous and for all \u03b7 > 0, there is K(\u03b7) \u2208 R such that\n$\u03a6_{Y,\u03bb}(f) > K(\u03b7) \u2212 \u03b7|| f || + \\frac{1}{\u03bb} d_B(f, \\mathcal{F}) \u2265 K(\u03b7) \u2212 \u03b7||f||$\nby Assumption A.2. The statement then follows from Theorem 1.1 in Lambley [20].\nProposition 2. Let {\u03bbn}n\u2208N \u2282 R>0 with $\u03bb_n \u2192 0$, and {f_n}n\u2208N \u2282 \\mathcal{H}_\u03a3 such that $f_n$ is a minimizer of $R_{FSP}^\u03bb$. Then {$f_n$}n\u2208N has an $\\mathcal{H}_\u03a3$-weakly convergent subsequence with limit $f^* \u2208 \\mathcal{F}$.\nProof. Analogous to the proof of Theorem 5.2(b) from Lambley [20]."}, {"title": "B Experimental setup", "content": "B.1 Qualitative experiments with synthetic data\nRegression We sample points from the corresponding generative model\n$y_i = \\sin(2\u03c0x_i) + \u03b5 \\text{ with } \u03b5 \\sim \u039d(0, \u03c3_\u03b7^2) \\qquad (B.1)$\nusing $\u03c3_\u03b7$ = 0.1 and draw $x_i \\sim U([-1, -0.5] \\cup [0.5, 1])$. We plot data points as gray circles, functions sampled from the approximate posterior as green lines, the empirical mean function as a red line and its empirical 2-standard-deviation interval around the mean as a green surface. All neural networks have the same two hidden-layer architecture with 50 neurons per layer and hyperbolic"}]}