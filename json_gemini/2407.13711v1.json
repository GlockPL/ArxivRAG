{"title": "FSP-LAPLACE: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning", "authors": ["Tristan Cinquin", "Marvin Pf\u00f6rtner", "Vincent Fortuin", "Philipp Hennig", "Robert Bamler"], "abstract": "Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predic-tions of the neural network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we have to recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodic-ity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant, e.g., in many scientific inference tasks. At the same time, it stays competitive for black-box regression and classification tasks where neural networks typically excel.", "sections": [{"title": "1 Introduction", "content": "Neural networks (NN) have become the workhorse for many machine learning tasks, but they do not quantify the uncertainty arising from data scarcity-the epistemic uncertainty. They are therefore incapable of providing a measure of confidence along with their predictions, which is needed for safety-critical applications, decision making, and scientific modeling [1-3]. As a solution, Bayesian neural networks (BNNs) cast training as approximating the Bayesian posterior distribution over the weights (i.e., the distribution of the model parameters given the training data), thus naturally capturing epistemic uncertainty. Various methods exist for approximating the (intractable) posterior, either"}, {"title": "2 Preliminaries: Laplace approximation in weight space", "content": "Consider a dataset D = (X, Y) of inputs X = (x^(i))^n_{i=1} \u2208 X^n and targets Y = (y^(i))^n_{i=1} \u2208 Y^n. We consider a model of the data that is parameterized by a neural network f : X \u00d7 W \u2192 R^d' with weights w\u2208 W CRP, a likelihood p(Y | f(X,w)) := \u03a0^n_{i=1}P(y^(i) | f(x^(i), w)), and a prior p(w).\nWe seek the Bayesian posterior p(w | D) x p(Y | f(X,w)) p(w). As it is intractable in BNNs, approximate inference methods have been developed. The Laplace approximation [16] approximates the posterior by a Gaussian distribution q(w = w*) = N (w; w*, \u039b^(\u22121)), whose parameters are obtained by finding a MAP estimate w* \u2208 arg min_{w\u2208w} R_{ws}(w) of the weights, where\nR_{ws}(w) := \u2013 log p(w | D) = \u2013 log p(w) \u2013 \u03a3^n_{i=1} log p(y^(i) | f(x^(i), w)) + const. (2.1)\nand computing the Hessian of the negative log-posterior A H_w log p(w | D)|_{w=w*} \u2208 RP\u00d7P."}, {"title": "3 FSP-LAPLACE: Laplace approximation under function-space priors", "content": "A conventional Laplace approximation to BNNs requires a prior in weight space, with the issues discussed in Section 1. In the following, we present FSP-LAPLACE, a method for computing Laplace approximations under interpretable GP priors in function space. Section 3.1 introduces an objective function that is a log-density under local linearization. Section 3.1 proposes a scalable algorithm for the linearized Laplace approximation with a function-space prior using matrix-free linear algebra."}, {"title": "3.1 Laplace approximations in function space", "content": "We motivate our Laplace approximation in function space through the lens of MAP estimation in an (infinite-dimensional) function space (Section 3.1) under a GP prior. As Lebesgue densities do not exist in (infinite-dimensional) function spaces, we cannot minimize Equation (2.1) to find the MAP estimate. We address this issue using a generalized notion of MAP estimation, resulting in a minimizer of a regularized objective on the reproducing kernel Hilbert space \u0397_\u03a3.\nMAP estimation in neural networks under Gaussian process priors The first step of the Laplace approximation is to find a MAP estimate of the neural network weights w, i.e., the minimizer of the negative log-density function in Equation (2.1) w.r.t. the Lebesgue measure as a \u201cneutral\u201d reference measure. In our method, we regularize the neural network in function space using a"}, {"title": "3.2 Algorithmic Considerations", "content": "Training with the FSP-LAPLACE objective function. Evaluating the FSP-LAPLACE objective proposed in the previous section requires computing the RKHS norm of the neural network. Unfortu-nately, this does generally not admit a closed-form expression. Hence, we use the approximation\n|| f(, w) \u2013 \u03bc|| \u2248 (f(C, w) \u2013 \u03bc(C))^TE(C,C)^(\u22121)(f(C, w) \u2013 \u03bc(C)),\n=||h_c(,w)||\nwhere C \u2208 X^nc is a set of nc context points and h_c(x, w) := \u03a3(x, C)\u03a3(C,C)^(\u22121)(f(C,w)\n\u03bc(C)) \u2208 \u0397_\u03a3. The function hc is the minimum-norm interpolant of f (\u00b7, w) \u2013 \u03bc at C in \u0397_\u03a3. Hence, the estimator of the RKHS norm provably underestimates, i.e., ||h_c(, w)|| \u2264 || f(\u00b7, w) \u2013 \u03bc||.\nDuring training, we need to compute ||h_c(\u00b7,w)||, at every optimizer step. Since this involves solving a linear system in nc unknowns and, more importantly, forwarding the ne context points through the neural network, we need to keep ne small for computational efficiency. We find that sampling an i.i.d. set of context points at every training iteration from a distribution Pc is an effective strategy for keeping ne low while ensuring that the neural network is appropriately regularized. The resulting training procedure is outlined in Algorithm 2.\nEfficient linear Laplace approximations of the FSP-LAPLACE objective. Once a minimum w* of RFSP is found, we compute a linearized Laplace approximation at w*. The Hessian of RFSP is then\nA = \u03a3_{w*} \u2013 \u03a3^n_{i=1} \u03a3_{w^*} ((i))^TL J_{w^*} (x^(i)).\nAgain, the RKHS inner products in the entries of \u03a3^w* do not admit general closed-form expressions. Hence, we use the same strategy as before to estimate \u03a3^w* \u2248 J_{w^*}(C)^T\u03a3(C,C)^(\u22121)J_{w^*}(C) at another set C of context points. Unlike above, for a Laplace approximation, it is vital to use a large number of context points to capture the prior beliefs well. Luckily, \u03a3^w* only needs to be computed once, at the end of training. But for large networks and large numbers of context points, it is still infeasible to compute or even represent \u03a3^w* in memory. To address this problem, we devise an efficient routine for computing (a square root of) the approximate posterior covariance matrix A\u2020, outlined in Algorithm 1. Our method is matrix-free, i.e. it never explicitly constructs any big (i.e.,"}, {"title": "4 Experiments", "content": "We evaluate FSP-LAPLACE on synthetic and real-world data, demonstrating that our method effec-tively incorporates beliefs specified through a GP prior; improves performance on regression tasks for which we have prior knowledge in the form of a kernel (Mauna Loa and ocean current modeling); and shows competitive performance on regression, classification, out-of-distribution detection, and Bayesian optimization tasks compared to baselines.\nBaselines We compare FSP-LAPLACE to a deterministic neural network fitted using maximum a-posteriori estimation with an isotropic Gaussian prior on the weights (MAP) and a neural network for which we additionally compute the standard linearized Laplace approximation (Laplace) [17]. We also compare our method to a Gaussian process (GP) [13], when the size of the dataset allows it, and"}, {"title": "4.1 Quantitative evaluation on real-world data", "content": "We now move on to investigate FSP-LAPLACE on two real-world scientific modeling tasks: fore-casting the concentration of CO2 at the Mauna Loa observatory and predicting ocean currents in the Gulf of Mexico. We then assess the performance of our method on standard benchmark regression, classification, out-of-distribution, and Bayesian optimization tasks. When reporting results, we bold the value with the highest mean as well as score whose standard-error bars overlap.\nMauna Loa dataset We consider the task of modeling the monthly average atmospheric CO2 concentration at the Mauna Loa observatory in Hawaii from 1974 to 2024 using data collected from the NORA global monitoring laboratory\u00b3 [27]. This 1-dimensional dataset is very accurately modeled by a combination of multiple kernels proposed in Rasmussen and Williams [13, Section 5.4.3]. We equip FSP-LAPLACE with this informative kernel and with additional periodic features, and compare to a GP with the same prior and to the linearized Laplace with same additional periodic features. Using periodic features (partially) reflects the prior knowledge carried by the kernel. Results are presented in Table 1 and Figure C.8 in Appendix. We find that incorporating prior beliefs both via an informative prior and periodic features in FSP-LAPLACE results in a strong decrease in mean squared error (MSE) over Laplace and GP baselines. In terms of expected log-likelihood, both neural networks under-estimate the likelihood scale, which results in poorer scores than the GP.\nOcean current modeling We further evaluate FSP-LAPLACE's ability to take into account prior knowledge by considering an ocean current modeling task where we are given sparse 2-dimensional observations of ocean drifter buoys velocities, and we are interested in estimating ocean currents further away from the buoys. For this, we consider the GulfDrifters dataset [28] and we follow the setup by Shalashilin [29]. We incorporate known physical properties of ocean currents into the considered models by applying the Helmholtz decomposition to the GP prior's kernel [30] as well as to the neural network. Results are shown in Table 1 and in Figure 2. We find that FSP-LAPLACE strongly improves over both Laplace and GP in terms of expected log-likelihood, and performs competitively in terms of mean squared error (MSE)."}, {"title": "5 Related work", "content": "Laplace approximation in neural networks First introduced by MacKay [34], the Laplace approximation gained strong traction in the Bayesian deep learning community with the introduction of scalable log-posterior Hessian approximations [25, 35], and the so-called linearized Laplace, which solves the underfitting issue observed with standard Laplace [17, 36]. In addition to epistemic uncertainty estimates, the Laplace approximation also provides a method to select prior parameters via marginal likelihood estimation [11, 12]. Recent work has made the linearized Laplace more scalable by restricting inference to a subset of parameters [37], by exploiting its GP formulation [17] to apply methods from the scalable GP literature [38-40], or by directly sampling from the Laplace approximation without explicitly computing the covariance matrix [41]. While these approaches use the GP formulation to make the linearized Laplace more scalable, we are unaware of any method using GP priors to incorporate interpretable prior beliefs within the Laplace approximation.\nBNNs with function-space priors Function-space priors in BNNs, in the context of variational inference, demonstrate improvements in predictive performance compared to weight-space priors [8]. While this idea might seem sound, it turns out that the KL divergence in the VI objective is infinite for most cases of interest due to mismatching supports between the function-space prior and BNN's predictive posterior [14], which therefore requires additional regularization to be well defined [8]. Due to this issue, other work [42] considers generalized VI [10] using the regularized KL divergence [43] or abandons approximating the neural network's posterior and instead uses deterministic neural networks as basis functions for Bayesian linear regression [15] or the mean of a sparse GP [44]. In contrast, our method does not compute a divergence in function space, but only the RKHS norm under the prior's kernel, thus circumventing the issue of mismatching support. Alternatively, rather than directly placing a prior on the function generated by a BNN, work has investigated methods to find weight-space priors whose pushforward approximates a target function-space measure by minimizing a divergence [9, 45], using the Ridgelet transform [46], or changing the BNN's architecture [47].\nRegularizing neural networks in function space Arguing that one ultimately only cares about the output function of the neural network, it has been proposed to regularize neural networks in function space, both showing that norms could be efficiently estimated and that such regularization schemes performed well in practice [48\u201351]. Our method uses the same RKHS norm estimator as Chen et al. [49] (however, under a different kernel) by sampling a new batch of context points at each update step. Similarly, Rudner et al. [51] propose an empirical prior on the weights that regularizes the neural network in function space, which they use for MAP estimation or approximate posterior inference. The objective to fit the empirical prior resembles ours, but unlike our method uses the kernel induced by the last layer of the neural network and includes an additional Gaussian prior on the weights."}, {"title": "6 Conclusion", "content": "We propose a method for performing the Laplace approximation in neural networks using interpretable Gaussian process priors in function space. This addresses the issue that conventional applications"}, {"title": "A Additional theory", "content": "A.1 Assumptions and their applicability\nAssumption A.1. f ~ GP (\u03bc, \u03a3) is a d'-output Gaussian process with index set X on (\u03a9, A, P) such that\n(i) the paths of f lie (P-almost surely) in a real separable Banach space B of Rd'-valued functions on X with continuous point evaluation maps \u03b4\u03b1 : B \u2192 Rd', and\n(ii) w \u2192 f(, w) is a Gaussian random variable with values in (B, B (B)).\nWe denote the law of w \u2192 f(, w) by PB.\nFor this paper, we focus on B = C(X), with X being a compact metric space. In this case, Assumption A.1(i) can be verified from regularity properties of the prior covariance function \u03a3 [see, e.g., 52]. Moreover, the sufficient criteria from Pf\u00f6rtner et al. [19, Section B.2] show that Assumption A.1(ii) also holds in this case.\nAssumption A.2. The potential \u03a6_Y : B \u2192 R is (norm-)continuous and, for each \u03b7 > 0, there is \u039a(\u03b7) \u2208 R such that \u03a6_Y (f) > \u039a(\u03b7) \u2013 \u03b7|| f || for all \u0192 \u2208 B.\nThis holds if the negative log-likelihood functions l^(i) : Rd' \u2192 R, f^(i) \u2192 \u2013 log p(y^(i) | f^(i)) are continuous and, for all \u03b7 > 0, there is K(\u03b7) \u2208 R such that l^(i) (f^(i)) > K(f^(i)) + \u03b7|| f^(i)||2 for all f^(i) \u2208 Rd'. For instance, this is true for a Gaussian likelihood l^(i) (f^(i)) = 1/2\u03c3^2||y^(i) \u2013 f^(i)||2.\nAssumption A.3. (i) Fis a subset of H_\u03a3, (ii) F is closed in B\u2283 H_\u03a3, and (iii) FCB has the Heine-Borel property, i.e., all closed and bounded subsets of F are compact in B.\nAssumption A.3(i) can be verified using a plethora of results from RKHS theory. For instance, for Sobolev kernels like the Mat\u00e9rn family used in the experiments, the RKHS is norm-equivalent to a Sobolev space [see, e.g., 53]. In this case, we only need the NN to be sufficiently often (weakly) differentiable on the interior of its compact domain X. The closure property from Assumption A.3(ii) is more difficult to verify directly. However, if we assume that W is compact and that the map W\u2192 B, w\u2192 f(., w) is continuous, then F is compact (and hence closed) as the image of a compact set under a continuous function. This is a reasonable assumption, since, in practice, the weights of a neural network are represented as machine numbers with a maximal magnitude, meaning that W is always contained in an lo ball of fixed radius. Incidentally, compactness of F also entails the Heine-Borel property from Assumption A.3(iii). Alternatively, F also has the Heine-Borel property if it is a topological manifold (e.g., a Banach manifold), since it is necessarily finite-dimensional."}, {"title": "A.2 Proofs", "content": "Lemma A.1. Let (X, d) be a metric space, A \u2286 X nonempty, and\nd(., A): X \u2192 R>0, x \u2192 inf_{\u03b1\u0395A} d(x,a).\nThen d(, A) is 1-Lipschitz.\nProof. For all x_1, x_2 \u2208 X we have\nd(x_2, A) = inf_{\u03b1\u0395A} d(x_2, a)\n<d(x_2,x_1) + inf_{\u03b1\u0395A} d(x_1, a)\n= d(x_1,x_2) + d(x_1,A)\nby the triangle inequality and hence d(x_2, A) \u2013 d(x_1,A) \u2264 d(x_1,x_2). Since this argument is symmetric in 21 and 22, this also shows that\n-(d(x_2, A) - d(x_1, A)) = d(x_1,A) \u2013 d(x_2, A) \u2264 d(x_2,x_1) = d(x_1,x_2).\nAll in all, we obtain |d(x_2, A) \u2013 d(x_1,A)| \u2264 d(x_1,x_2).\nLemma A.2. Let (X, d) be a metric space and AC X a closed, nonempty subset with the Heine-Borel property. Then inf_{a\u2208a} d(x, a) is attained for all x \u2208 X.\nProof. Let x \u2208 X and r > rx := inf_{a\u2208a} d(x, a). Then A \u2229 B\u2084(x) \u2260 \u00d8 as well as d(x, a) > r for all a \u2208 A \\ B\u2084(x) and thus inf_{a\u2208a} d(x, a) = inf_{a\u2208A\u2229B\u2084(x)} d(x, a). Moreover, A\u2229B\u2084(x) is compact by the Heine-Borel property and d(x,) is continuous. Hence, the claim follows from the Weierstrass extreme value theorem.\nProposition 1. Let Assumptions A.1 to A.3 hold. For > > 0, define \u00de_{Y,\u039b}: B \u2192 R, \u0192 \u2194\n\u0424_Y (f) + 1/\u03bbdB(f,F). A function f* is a weak mode of the posterior measure PB(df) x exp(-\u0424_{Y,\u039b} (df)) P(df) if and only if f* \u2208 H_\u03a3 minimizes\nREsp : H_\u03a3 \u2192 R, \u0192 \u2192 \u0424_{Y\u00b4\u039b} (f) + =||f \u2013 \u03bc||\u00b7\nRFSP:\nProof. dB(, F) is (globally) 1-Lipschitz by Lemma A.1 and bounded from below by 0. Hence, \u03a6_{Y,\u039b} = \u0424_Y + 1/\u03bbd\u33a2(\u00b7, F) is continuous and for all \u03b7 > 0, there is K(\u03b7) \u2208 R such that\n\u03a6_{Y,\u039b}(f) > K(n) \u2013 \u03b7|| f || + dB(f, F) \u2265 K(n) \u2013 \u03b7||f||\nby Assumption A.2. The statement then follows from Theorem 1.1 in Lambley [20].\nProposition 2. Let {Xn}n\u2208N CR>0 with An \u2192 0, and {f}n\u2208N CH_\u03a3 such that f\u0127 is a minimizer of R_FSP^. Then {f}nen has an H_\u03a3-weakly convergent subsequence with limit f* \u2208 F.\nProof. Analogous to the proof of Theorem 5.2(b) from Lambley [20]."}, {"title": "B Experimental setup", "content": "B.1 Qualitative experiments with synthetic data\nRegression We sample points from the corresponding generative model\nyi = sin(2\u03c0xi) + \u03b5 with \u03b5 ~ \u039d (0, \u03c3\u03b7^2) (B.1)\nusing \u03c3\u03b7 = 0.1 and draw xi ~ U([\u22121, \u22120.5] U [0.5, 1]). We plot data points as gray circles, functions sampled from the approximate posterior as green lines, the empirical mean function as a red line and its empirical 2-standard-deviation interval around the mean as a green surface. All neural networks have the same two hidden-layer architecture with 50 neurons per layer and hyperbolic"}, {"title": "B.2 Quantitative experiments with real-world data", "content": "Mauna Loa We consider the Mauna Loa dataset which tracks the monthly average atmospheric CO2 concentration at the Mauna Loa observatory in Hawaii from 1974 to 2024 [27]. We consider the first 70% of the data in chronological order as the train set (from 1974 to 2005) and the last 30% as the test set (from 2009 to 2024). We standardize the features (time) and regression targets (CO2 concentration). We use two hidden-layer neural networks with hyperbolic tangent activations and 50 units each. We augment the input of the neural networks with an additional sine and cosine transformation of the features i.e., we use the feature vectors (ti, sin(2\u03c0ti/T), cos(2\u03c0ti/T)) where ti is the time index and T is the period used in Rasmussen and Williams [13]. For FSP-LAPLACE, we use 100 context points placed on a regular grid and limit Lanczos to run at most 500 iterations. For the Laplace, we use the full generalized Gauss-Newton matrix and find the prior scale by maximizing the marginal likelihood [11].\nOcean current modeling We consider the GulfDrifters dataset [28] and we follow the setup by Shalashilin [29]. We use as training data 20 simulated velocity measurements (red arrows in Figure 2), and consider as testing data 544 average velocity measurements (blue arrows in Figure 2) computed over a regular grid on the [-90.8, -83.8] \u00d7 [24.0, 27.5] longitude-latitude interval. We standardize both features and regression targets. We incorporate physical properties of ocean currents into the models by applying the Helmholtz decomposition to the GP prior's covariance function [30] as well as to the neural network f using the following parameterization\nf(., w) = grad \u03a6(\u00b7, w\u2081) + rot \u03a8(\u00b7, w2) (B.2)\nwhere w = {w1,w2} and \u03a6(\u00b7, w\u2081) and \u03a8(\u00b7, w2) are two hidden-layer fully-connected neural networks with hyperbolic tangent activation functions and 100 hidden units per layer. We use 96 context points placed on a regular grid for the FSP-LAPLACE and limit Lanczos to run at most 500 iterations. For the linearized Laplace, we use the full generalized Gauss-Newton (GGN) and fix the prior scale to op = 1.\nImage classification We consider the MNIST [31] and FashionMNIST [32] image classification datasets. We standardizing the images, fit the models on a random partition of 90% of the provided train splits, keeping the remaining 10% as validation data, and evaluate the models on the test data. We report mean and standard-errors across 5 such random partitions of the train data with different random seeds. We compare our model to the Laplace, MAP, and Sparse GP baselines, as the scale of the datasets forbids exact GP inference. The expected log-likelihood and expected calibration error are estimated by Monte Carlo integration with 10 posterior samples."}, {"title": "B.2.1 Software", "content": "We use the JAX [65] and DM-Haiku [66] Python libraries to implement neural networks. The generalized Gauss-Newton matrices used in the Laplace approximations are computed using the KFAC-JAX library [67]. We implemented the GPs and sparse GPs using the GPyTorch library [61]. We conducted experiments the Bayesian optimization experiments using the BOTorch library [61]."}, {"title": "B.2.2 Hardware", "content": "All models were fit using a single NVIDIA RTX 2080Ti GPU with 11GB of memory."}, {"title": "C Additional experimental results", "content": "C.1 Additional qualitative results\nRegression FSP-LAPLACE successfully adapts to the beliefs specified by different Gaussian process priors in terms of periodicity (Figure C.2), smoothness (Figure 1) and length scale (Figure C.3) without modifying the neural network's architecture or adding features. We also find that our method effectively regularizes the model when the data is very noisy (Figure C.4). Finally, we show that our methods needs more context points to capture the behavior of rougher priors (Figure C.5)."}]}