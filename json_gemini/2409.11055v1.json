{"title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "abstract": "Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets. Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined. This paper evaluates the performance of instruction-tuned LLMs across various quantization methods\u2014GPTQ, AWQ, SmoothQuant, and FP8-on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable performance of recent open LLMs, such as Llama 3.1 released in July 2024, deploying these models in resource-constrained environments remains challenging due to their tens of billions of parameters. Low-bit quantization has emerged as a popular technique to compress LLMs, reducing memory and computational demands during inference.\nQuantization methods for LLMs are broadly categorized into Quantization Aware Training (QAT) and Post-Training Quantization (PTQ) (Zhu et al.,"}, {"title": "2 Evaluation Procedure", "content": "To handle LLMs, which cannot be processed on a single server, and to ensure fast and reliable evaluations, we developed a structured evaluation pipeline based on a multi-node cluster setup. The evaluated LLMs include the Vicuna, Gemma, and Llama families, ranging in size from 2B to 405B. Each model is quantized using GPTQ, AWQ, SmoothQuant, and FP8 methods. The evaluation is conducted using benchmarking tools such as Im-eval (v0.4.4) and MT-Bench (v0.2.36). The multi-node cluster used for evaluation is implemented with vLLM (v0.5.4) and consists of four servers: H100-80Gx8, A100-80Gx4, RTX 6000-48Gx4, and A6000-48Gx4. Additionally, the Huggingface library is integrated into the pipeline to support model hosting and benchmarking. The evaluation is distributed across a multi-cluster environment to ensure a thorough performance assessment.\nIf vLLM cannot be used for processing, we used the Huggingface Accelerate library (v0.33.0) instead, which is slower but shows better comparability."}, {"title": "3 Experimental Setup", "content": "We evaluated the quantized LLMs on widely used datasets and benchmarks to ensure a comprehensive analysis. First, the ARC, HellaSwag, and Winogrande datasets fall under the CommonSenseQA type. These are used to evaluate knowledge that is obvious to humans but difficult for AI to distinguish, as well as basic knowledge at the elementary school level. For ARC, we considered the challenge set only, focusing on the more difficult questions to ensure rigorous evaluation.\nNext, the MMLU, GPQA, MMLU-PRO, BBH, and MuSR datasets are utilized for assessing complex knowledge and language understanding. MMLU contains multiple-choice questions across 57 subjects ranging from STEM fields to the humanities. MMLU-PRO increases the difficulty by expanding the multiple-choice options from four to ten and covers 14 subjects. GPQA comprises PhD-level problems created by experts, making them challenging for the general public to answer. BBH includes 23 subjects and presents problems requiring human-level language understanding, encompassing multi-step arithmetic, algorithmic reasoning, and satire. MuSR consists of three tasks: murder mysteries, object placements, and team allocation. Solving problems in the MuSR dataset requires LLMs to employ Chain-of-Thought (CoT) abilities to analyze lengthy contexts, perform step-by-step reasoning, and integrate the findings.\nIFEval is designed to evaluate whether models precisely follow given instructions, without considering content generation. When provided with an instruction like \"Write in more than 400 words\", the model is expected to fulfill the directive regardless of the content produced. The TruthfulQA dataset is employed to assess the truthfulness of the model's responses, serving as a measure of hallucination tendencies. We also utilized GSM8K and MATH-Lvl-5 for evaluating mathematical reasoning. GSM8K contains elementary-level math problems that require multi-step reasoning. MATH-Lvl-5 comprises competition-level high school problems across seven subjects, where the generated answers must adhere to specific formats, using LaTeX for equations and Asymptote for figures.\nFinally, we assessed free-form conversation quality using MT-Bench. In this evaluation, GPT-4"}, {"title": "3.1 Datasets for Quantized LLMs", "content": "We evaluated the quantized LLMs on widely used datasets and benchmarks to ensure a comprehensive analysis. Table 1 lists the 13 selected datasets. First, the ARC, HellaSwag, and Winogrande datasets fall under the CommonSenseQA type. These are used to evaluate knowledge that is obvious to humans but difficult for AI to distinguish, as well as basic knowledge at the elementary school level. For ARC, we considered the challenge set only, focusing on the more difficult questions to ensure rigorous evaluation.\nNext, the MMLU, GPQA, MMLU-PRO, BBH, and MuSR datasets are utilized for assessing complex knowledge and language understanding. MMLU contains multiple-choice questions across 57 subjects ranging from STEM fields to the humanities. MMLU-PRO increases the difficulty by expanding the multiple-choice options from four to ten and covers 14 subjects. GPQA comprises PhD-level problems created by experts, making them challenging for the general public to answer. BBH includes 23 subjects and presents problems requiring human-level language understanding, encompassing multi-step arithmetic, algorithmic reasoning, and satire. MuSR consists of three tasks: murder mysteries, object placements, and team allocation. Solving problems in the MuSR dataset requires LLMs to employ Chain-of-Thought (CoT) abilities to analyze lengthy contexts, perform step-by-step reasoning, and integrate the findings.\nIFEval is designed to evaluate whether models precisely follow given instructions, without considering content generation. When provided with an instruction like \"Write in more than 400 words\", the model is expected to fulfill the directive regardless of the content produced. The TruthfulQA dataset is employed to assess the truthfulness of the model's responses, serving as a measure of hallucination tendencies. We also utilized GSM8K and MATH-Lvl-5 for evaluating mathematical reasoning. GSM8K contains elementary-level math problems that require multi-step reasoning. MATH-Lvl-5 comprises competition-level high school problems across seven subjects, where the generated answers must adhere to specific formats, using LaTeX for equations and Asymptote for figures.\nFinally, we assessed free-form conversation quality using MT-Bench. In this evaluation, GPT-4"}, {"title": "3.2 Quantization Methods", "content": "To evaluate the performance of quantized models, we applied various quantization techniques, including GPTQ (Frantar et al., 2022), AWQ (Lin et al., 2024), SmoothQuant (Xiao et al., 2023), and the FP8 (Micikevicius et al., 2022). The considered quantization methods fall under the category of Post-Training Quantization (PTQ), with GPTQ and AWQ being weight-only quantization techniques. GPTQ employed layer-wise quantization"}, {"title": "3.3 Models", "content": "Considering that instruction-tuned models are widely used in practice, we applied quantization techniques to the following instruction-tuned open LLM families: Vicuna (Zheng et al., 2023), Gemma (Team et al., 2024), and the Llama families (Dubey et al., 2024), totaling nine models."}, {"title": "4 Experimental Results", "content": "The experimental results are shown in Table 2, Table 3, and Table 4, each representing the performance changes due to quantization across 13 datasets for three different model families of varying sizes. Overall, the results highlight four key takeaways. The specific observations are as follows:\nQuantized LLMs generally outperform small LLMs in most benchmarks, except for hallucination and instruction-following tasks. Across 13 benchmarks, using a larger quantized LLM consistently led to better performance than using smaller models directly. In particular, a 4-bit quantized Llama-2-13B, which effectively reduces its size to 6.5 GB, outperformed an FP16 Llama-2-7B (14 GB) on most benchmarks despite its smaller size. In particular, the 4-bit compressed Llama-2-13B (6.5GB) is 4.66% more accurate on the OpenLLM Leaderboard-v1 dataset and 1.16% more accurate on the OpenLLM Leaderboard-v2 dataset than the FP16 Llama-2-7B (14GB). However, in the TruthfulQA benchmark, which tests for hallucination, the FP16 Llama-2-7B shows better accuracy than the quantized Llama-2-13B. Similarly, for the instruction-following task IFEval, the FP16 Llama-2-7B outperforms the 4bit quantized Llama-2-13B."}, {"title": "5 Related Work", "content": "Quantization for LLMs. There are two main types of quantization methods for LLMs: post-training quantization (PTQ) and quantization-aware training (QAT). Due to the size and training complexity of LLMs, QAT is difficult to apply, and as a result, only limited research has been conducted in this area. Consequently, the majority of quantization research for LLMs has focused on PTQ approaches (Zhu et al., 2023; Wan et al., 2023).\nLLM.int8() (Dettmers et al., 2022) is a posttraining quantization method that uses 8-bit weights and activations to reduce the memory footprint of large models while maintaining performance. GPTQ (Frantar et al., 2022) is a layer-wise quantization that uses inverse Hessian information to reduce the number of bits per weight while maintaining low accuracy loss. AWQ (Lin et al., 2024) proposed that preserving a small portion of important weights is a key part of reducing quantization errors. As part of an activation-aware strategy, AWQ focused on channels with larger activation magnitudes and uses per-channel scaling. SmoothQuant (Xiao et al., 2023) is a method that smooths activation outliers before quantization, improving robustness in large-scale models and enabling more effective 8-bit quantization. Outlier Suppression+ (Wei et al., 2023) reduces the impact of extreme outliers in activations, allowing for more efficient quantization by normalizing problematic values without degrading model accuracy. QLORA (Dettmers et al., 2024) combines low-rank adaptation with quantization to achieve efficient fine-tuning of large models while minimizing computational costs and memory usage.\nHowever, these quantization algorithm works have evaluated only on basic datasets such as perplexity, ARC, and MMLU, which were released 2-3 years ago, and they do not sufficiently take into account the recent advancements in LLMs. Therefore, for a safe application of quantization in LLM services, a more comprehensive performance analysis is necessary.\nEvaluating LLMs. Several studies have explored the effects of model quantization on the"}, {"title": "6 Conclusion", "content": "We evaluated instruction-tuned quantized LLMs across 13 datasets and 6 task types, using models ranging from 7B to 405B and 4 quantization methods, including GPTQ, AWQ, SmoothQuant, and FP8. We found that quantized LLMs generally outperformed smaller models in most tasks, except for hallucination detection and instruction-following. Performance varied by quantization method and precision, with weight-only quanziation performing better in the 405B model. Task difficulty had little impact on accuracy loss, and the MT-Bench evaluation method showed limited ability to distinguish between high-performing LLMs."}, {"title": "A Additional Details", "content": "The dialogue evaluation using MT-Bench is useful when utilizing models designed for chat. As listed in the table 5, when evaluating the quality of free-form text generation, models that are not instruction-tuned for chat tend to produce lower evaluation scores. Therefore, to conduct a meaningful evaluation, MT-Bench should be used with LLMs that have been instruction-tuned for chat."}, {"title": "A.1 MT-Bench", "content": "The dialogue evaluation using MT-Bench is useful when utilizing models designed for chat. As listed in the table 5, when evaluating the quality of free-form text generation, models that are not instruction-tuned for chat tend to produce lower evaluation scores. Therefore, to conduct a meaningful evaluation, MT-Bench should be used with LLMs that have been instruction-tuned for chat."}, {"title": "A.2 Quantization Settings", "content": "The considered quantization methods in this study include GPTQ, AWQ, SmoothQuant, and FP8, with the tools used being AutoGPTQ, AutoAWQ, and Ilmcompressor. Each tool and method follows specific configurations to ensure optimal performance."}, {"title": "A.2.1 AutoGPTQ", "content": "For AutoGPTQ, the model is quantized to 4-bit with a group size of 128, which is the recommended value. Symmetric quantization is applied (sym=True), allowing zero to be precisely represented, which can offer speedups. The activation descent is set to True (desc_act=True), but setting it to False can significantly speed up inference at the cost of slightly reduced perplexity. A 10% damping factor is used (damp_percent=0.1) to further refine the performance."}, {"title": "A.2.2 AutoAWQ", "content": "In AutoAWQ, an asymmetric quantization scheme is employed with a group size of 128, and the model is quantized to 4-bit. This configuration is optimized for maintaining accuracy while reducing computational complexity."}, {"title": "A.2.3 vLLM's Ilmcompressor", "content": "For llmcompressor with GPTQ, only the weights of the linear operators within the transformer blocks are quantized. Symmetric per-channel quantization is applied, where a linear scaling per output dimension maps the INT8 or INT4 representations to floating-point weights. AutoGPTQ is also used for this process, with a 10% damping factor for further precision.\nIn Ilmcompressor with SmoothQuant, symmetric per-channel quantization is again applied, focusing on the weights of the linear operators. This method uses INT8 quantization, and the activations are also quantized using INT8, ensuring consistency across both weights and activations.\nLastly, Ilmcompressor with FP8 utilizes FP8 types, which have two distinct representations typically supported by hardware. FP8 (E4M3) consists of 1 sign bit, 4 exponent bits, and 3 bits of mantissa, capable of storing values up to +/-448. Both weights and activations are quantized per tensor using symmetric quantization, ensuring uniform precision across the model."}]}