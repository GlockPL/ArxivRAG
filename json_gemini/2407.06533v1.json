{"title": "LETS-C: Leveraging Language Embedding for Time Series Classification", "authors": ["Rachneet Kaur", "Zhen Zeng", "Tucker Balch", "Manuela Veloso"], "abstract": "Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a language embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on well-established time series classification benchmark datasets. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging language encoders to embed time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.", "sections": [{"title": "Introduction", "content": "Time series classification has gained significant attention in recent years due to its wide-ranging applications in various domains, such as finance, healthcare, and activity recognition. The increasing availability of time series data has driven the need for efficient and accurate classification methods. Recent advancements in natural language processing (NLP) and large language models (LLMs) have shown strong promises in language modeling, particularly in capturing temporal dependencies within sequential data. Inspired by this success, researchers have explored extending these techniques to the time series domain by fine-tuning pre-trained LLMs, achieving state-of-the-art (SOTA) performance on well-established benchmarks for tasks including classification and forecasting.\nHowever, the use of LLMs for time series classification comes with a significant drawback due to their large model size. These models often have billions of parameters, making them computationally expensive and limiting their usage in resource-constrained environments. At training time, fine-tuning partially frozen pre-trained LLMs also often involves millions of trainable"}, {"title": "Related Work", "content": "In this section, we review the related works in three key areas: time series classification, the application of language models to time series data, and text embeddings.\nTime Series Classification Time series classification has been an active research area for decades."}, {"title": "Methodology", "content": "Given a time series classification dataset D = {(xi, Yi)1}, where x\u2081 is a multivariate time series sample, and yi \u2208 {1, 2, . . ., C'} is the corresponding class label, the goal is to learn a classifier that accurately predicts the class label \u011d for each time series. As illustrated in Figure 1, we propose LETS-C framework that harnesses text embeddings for time series classification tasks. Specifically, we 1) initially preprocess the time series data to standardize it, then 2) subsequently generate text embeddings from the standardized time series, 3) fuse embeddings with the time series data, and finally 4) feed the fused representation to a classification head that consists of CNNs and MLP. The choice of a simple classification head is intentional, we aim to test the hypothesis that the text embeddings of the time series provide sufficiently powerful representations for effective classification.\nPreprocessing To ensure consistent scales across all model inputs, each feature dimension of time series x\u2081 is min-max normalized to the range [0, 1] based on the minimum and maximum feature values of each dimension across the training data.\nText Embedding of Time Series It is crucial to carefully format the preprocessed time series into strings before using text embeddings, as the tokenization of numerical strings can significantly affect the embeddings. Liu & Low (2023) has shown that tokenization impacts a model's arithmetic abilities, with commonly used subword tokenization methods like Byte Pair Encoding (BPE) arbitrarily subdividing numbers, causing similar numbers to appear very differently. To mitigate this, we adopted a digit-space tokenization strategy, as suggested by Gruver et al. (2024), where each digit is spaced, commas are added to separate time steps, and decimal points are omitted for fixed precision.\nFusing Embedding and Time Series Next, we perform an element-wise addition of the embedding to the preprocessed time series, while preserving the maximum length between the two. Specifically, Xi \u2208 Rdxl are added with embeddings e\u00a1 \u2208 Rd\u00d7le. If lx and le do not match, we pad the shorter one with zeros to align their sizes before addition, resulting in a combined representation in Rdxmax(lx,le).\nThe direct addition of embeddings to the time series data is analogous to the use of positional embeddings in NLP, which enriches the sequence representation with additional structural and contextual information. As a result, it enables both time series and text embeddings to contribute to the learning process, thereby enhancing the model's ability to effectively interpret complex temporal patterns (Section 4.3 show alternative fusion approaches are less effective).\nLightweight Classification Head Lastly, we pair the fused embedding and time series with a simple classification head composed of 1D CNNs and an MLP for time series classification. The output from"}, {"title": "Experiments", "content": "We provide a brief introduction to the datasets, baselines, evaluation metrics, and model imple-mentation details in Section 4.1. In Section 4.2, we compare our approach against various base-lines\u2014including classical methods and MLP-, CNN-, RNN-, and Transformer-based techniques, as well as recent SOTA LLM-based methods\u2014highlighting LETS-C's performance and computational costs. Further insights into ablation studies of LETS-C, generalization of our method across different embedding types, the power of the embeddings and its similarities across different time series are explored through additional analyses in Section 4.3."}, {"title": "Experimental Setup", "content": "Datasets and Evaluation Metrics We followed Wu et al. (2022a) and benchmarked on 10 multivariate datasets from the UEA Time Series Classification Archive encompassing a wide range of applications, namely: EthanolConcentration, FaceDetection, Handwriting, Heartbeat, Japanese Vowels, PEMS-SF , SelfRegulationSCP1, SelfRegulationSCP2, SpokenArabicDigits, and UWaveGestureLibrary.\nTo benchmark the classifiers, we used metrics including classification accuracy and AvgWins. Avg-Wins is defined as the average number of times that a method outperforms other methods across benchmarked datasets, with ties also being counted towards this average. Additionally, the methods were analyzed in terms of trainable model parameters.\nBaselines We included 20 baseline models to ensure a comprehensive comparison. We utilize the same baselines outlined by Zhou et al. (2024); Wu et al. (2022a), namely: Classical methods. Note that some of these methods were originally developed for forecasting but were later adapted for classification tasks.\nImplementation Details The experiments were conducted on a Linux machine equipped with 8 NVIDIA T4 Tensor Core GPUs, each with 16GB of memory. We utilized the PyTorch v1.0.0 deep learning platform running on Python 3.6 for all models. All configurations employed the RAdam optimizer, with its default hyperparameters settings (\u03b21, \u03b22) = (0.9, 0.999). For tokenization, we found that maintaining a precision of one decimal place optimizes performance (see Appendix Section F). Further, we explored LETS-C's performance across various embedding types, other than text-embedding-3-large in Section 4.3. Exploratory hyperparameter optimization was conducted, revealing that 1-4 1D convolutional layers and 1-3 linear layers are optimal for the performance of LETS-C across all datasets."}, {"title": "Main Results", "content": "Comparison to State-of-the-art Table 1 presents a comparative analysis of our proposed LETS-C approach against 20 baseline models across 10 benchmark datasets as introduced above. We observe that LETS-C consistently demonstrates robust performance across all datasets, achieving the highest average accuracy of 76.16% and AvgWins of 40%, compared to 20 benchmark models. This includes the most recent SOTA model OneFitsAll (accuracy: 73.97%, AvgWins: 20%) and an older SOTA TimesNet (accuracy: 73.57%, AvgWins: 0%). Notably, LETS-C surpasses OneFitsAll on six out of ten datasets by a significant margin. LETS-C is particularly effective on challenging datasets like PEMS-SF and EthanolConcentration (EC). PEMS-SF is characterized by exceptionally high dimensionality with 963 features, and EC contains an extremely long time series at length of 1751, compared to other datasets in the benchmark. These results showcase the competitive edge of LETS-C against the previous SOTA methods (OneFitsALL), thus establishing a new benchmark for time series classification.\nComputational Cost Analysis Next, we aim to assess how well the LETS-C approach balances performance with computational efficiency, which is crucial for usage in resource-constrained environments. Table 2 provides a detailed analysis of the trainable parameters associated with LETS-C compared to the previous SOTA model, OneFitsAll. Our method achieved higher performance\nwith only 14.48% of the trainable model parameters on average, compared to OneFitsAll. Despite the advantage of OneFitsAll over other leading models like TimesNet and FEDformer on its reduced parameter count, OneFitsAll still requires much more computational cost than our approach, especially for training. Further experiments on training and inference times are detailed in Table 7 in the Appendix. We show that LETS-C offers a lightweight approach to the time series classification task while achieving the SOTA performance."}, {"title": "Additional Analysis", "content": "Ablation Study To empirically assess the advantages of fusing both text embeddings and time series data, as opposed to variants that only leverage either the text embedding or time series itself, we conducted an ablation study."}, {"title": "Conclusion", "content": "In this work, we introduced LETS-C, a novel approach that leverages language embeddings for time series classification. To the best of our knowledge, this is the first work to explore the potential of off-the-shelf text embeddings in time series analysis, specifically for classification tasks. By projecting time series data using text embedding models and utilizing a simple yet effective classification head, LETS-C achieves SOTA performance on a well-established benchmark containing 10 datasets across different domains, surpassing 20 baseline methods. Moreover, LETS-C is significantly more lightweight compared to the previous SOTA method, achieving higher accuracy while using much less trainable parameters. Through comprehensive analysis, we demonstrated the effectiveness of LETS-C along different aspects, including its robustness across various text embedding models, the advantage of text embeddings for time series classification, and the trade-off between accuracy and model size. We believe that the findings and insights from this work will inspire further exploration of language embeddings and their potential applications in time series domain, paving the way for the development of more powerful and efficient methods for various time series tasks."}, {"title": "Limitations and Future Work", "content": "In this work, we adopted the tokenization method proposed by Gruver et al. (2023); however, there may be tokenization strategies better suited specifically for time series data. Future research could investigate these alternatives to optimize performance further (Rajaraman et al., 2024). Additionally, while this study concentrates on using text embeddings exclusively for time series classification, to more comprehensively assess their universality, future research should explore the application of these techniques to a broader spectrum of time series analysis tasks, including forecasting, anomaly detection, and imputation. We expect the societal impact of our work to improve decision-making processes in various sectors, but also practitioners need be careful particularly in sensitive areas like healthcare and personal finance, which raises significant privacy issues. If data is not handled with strict privacy controls, there could be risks of unauthorized access and misuse of personal data."}]}