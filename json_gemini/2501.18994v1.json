{"title": "VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration", "authors": ["Jian-Yu Chen", "Yi-Ru Chen", "Yin-Qiao Chang", "Che-Ming Li", "Jann-Long Chern", "Chih-Wei Huang"], "abstract": "This paper addresses the challenges in learning-based monocular positioning by proposing VKFPos, a novel approach that integrates Absolute Pose Regression (APR) and Relative Pose Regression (RPR) via an Extended Kalman Filter (EKF) within a variational Bayesian inference framework. Our method shows that the essential posterior probability of the monocular positioning problem can be decomposed into APR and RPR components. This decomposition is embedded in the deep learning model by predicting covariances in both APR and RPR branches, allowing them to account for associated uncertainties. These covariances enhance the loss functions and facilitate EKF integration. Experimental evaluations on both indoor and outdoor datasets show that the single-shot APR branch achieves accuracy on par with state-of-the-art methods. Furthermore, for temporal positioning, where consecutive images allow for RPR and EKF integration, VKFPos outperforms temporal APR and model-based integration methods, achieving superior accuracy.", "sections": [{"title": "Introduction", "content": "Visual positioning techniques are increasingly crucial for the advancement of intelligent systems, including applications in augmented/mixed reality, smart factories, education, entertainment, and robotics (Chen et al. 2017; Jinyu et al. 2019). In particular, the concept of monocular positioning (Engel, Sch\u00f6ps, and Cremers 2014; Mur-Artal, Montiel, and Tardos 2015), utilizing a single basic camera, is gradually emerging as an attractive alternative. Advantages like lightweight, cost-effectiveness, and minimal calibration make monocular positioning promising for future applications. However, significant challenges hinder widespread adoption (Yang et al. 2018; Shu et al. 2021), such as managing dynamic objects, adapting to varying illumination, and lacking depth information. These issues significantly impede practical use.\nIn recent years, machine learning approaches have gained attention as a solution to address the limitations of traditional methods in monocular positioning. The introduction of Absolute Pose Regression (APR) methods, pioneered by PoseNet (Kendall, Grimes, and Cipolla 2015), focuses on directly computing the absolute six degrees of freedom (6DoF) pose from a single image using convolutional neural networks (CNNs). Subsequent advances in PoseNet, such as PoseNet16 (Kendall and Cipolla 2016) and PoseNet17 (Kendall and Cipolla 2017), named after the year they were published, extend the original work by considering uncertainty or adding learning weights to the loss function to enhance performance, respectively. As these methods learn rotation in the form of 4-DoF quaternions, which are over-parameterized for rotation, the logarithm of a unit quaternion has been applied to represent the rotation in PoseNet+log q (Brahmbhatt et al. 2018) for better representation of rotation. Moreover, AtLoc (Wang et al. 2020) further improves precision by using an attention (Vaswani et al. 2017) mechanism to focus on more geometrically robust features. These methods, which solely utilize a single image to regress the absolute pose of the camera, are referred to as single-shot positioning in the following content.\nOn the other hand, some approaches leverage multiple images to incorporate temporal information and strengthen the inter-image relationships, thereby enhancing performance. Examples of such methods include VidLoc (Clark et al. 2017), MapNet (Brahmbhatt et al. 2018), and AtLoc+ (Wang et al. 2020), all of which belong to the category of temporal positioning. Methods like ms-Transformer (Shavit, Ferens, and Keller 2021) and DFNet (Chen et al. 2022), while leveraging advanced architectures such as transformers or direct feature matching, still struggle with overfitting and generalization across diverse scenes. These techniques either utilize sequential images as input or design their loss functions to impose constraints on the distances between consecutive predictions. Despite these efforts to improve accuracy, pure APR methods still face certain limitations in terms of accuracy and robustness.\nGiven that camera motion tends to be continuous and smooth, LSTM-KF (Coskun et al. 2017) leverages long short-term memory (LSTM) to learn the current pose by considering all previous observations and states, or (Fran\u00e7ani and Maximo 2023) leverages the Transformer network to understand the video stream, thereby considering trajectory information in the prediction process. However, relying solely on constant velocity or constant acceleration assumptions may not fully capture the dynamics present in the system, potentially leading to inaccuracies in estimation. In contrast, Relative Pose Regression (RPR) (Wang et al. 2017; Li et al. 2018) takes a different approach from APR, focusing on determining the relative motion between successive pairs of images. RPR offers higher accuracy by directly estimating relative poses, but it faces challenges with accumulated errors when converting these to absolute poses due to its dependence on past temporal information. APR, on the other hand, provides drift-free predictions but tends to be less accurate. Hence, integrating both methods is essential to leverage their respective strengths and mitigate their weaknesses.\nIntegration methods can be divided into two main categories: model-based and optimization-based. Model-based aims commonly employ recurrent neural networks (RNNs) to learn the integration strategies of RPR and APR predictions, such as ViPR (Ott et al. 2020). Another common strategy is to employ a shared visual encoder with the expectation that RPR can assist APR, as demonstrated in (Valada, Radwan, and Burgard 2018). Despite the widespread use of RNN, it has become evident that the model-based integration approach lacks stability and does not consistently outperform optimization-based integration methods (Ott et al. 2020). This observation prompts the exploration of optimization-based approaches that may offer greater stability and improved performance compared to the reliance on RNNs.\nThe primary optimization-based methods for integrating predictions from multiple sensors include Pose Graph Optimization (PGO) and the Extended Kalman Filter (EKF). While PGO can achieve higher accuracy by optimizing a greater number of states, EKF is notable for its computational efficiency, making it well-suited for real-time applications where rapid processing is essential (LaViola 2003).\nOne notable work that has applied the EKF in learning-based positioning integration tasks is (Zhou et al. 2020). This study distinguishes itself by optimizing the predicted outcomes of the EKF, APR, and RPR simultaneously, considering posterior, likelihood, and prior probabilities concurrently. While this approach introduces a novel perspective, it deviates from the traditional Bayesian principles applied to the EKF. Such deviation may add complexity and raise concerns regarding its adherence to established Bayesian principles, potentially impacting the validity and reliability of the EKF framework. Another significant work that integrates EKF principles is presented in (Moreau et al. 2022). This study aims to apply EKF to smooth trajectories and eliminate outliers effectively. However, it is important to note that the focus of this work is primarily on trajectory smoothing and outlier removal, without explicitly incorporating RPR in their methodology. The omission of RPR is a critical limitation given its essential role in capturing the relative motion between consecutive image pairs, which is fundamental for accurate positioning tasks.\nIn this study, we propose VKFPos, a novel lightweight monocular positioning approach that integrates APR and RPR using a variational Bayesian extended Kalman filter. The recursive structure of the Kalman filter leverages past state information to enhance current state prediction and trajectory forecasting. A key aspect of VKFPos is its innovative training paradigm for absolute and relative pose estimators, grounded in variational Bayesian inference."}, {"title": "Learning-Based Monocular Positioning with EKF Integration", "content": "In this section, we present the proposed long-term probabilistic trajectory integration approach. The architecture is illustrated in Figure 1, where both the relative pose estimator and the absolute pose estimator predict their respective pose distributions and feed into an EKF to optimize the trajectory.\nThe relative pose estimator aims to predict the relative pose distribution, serving as the control input in the integration system. The absolute pose estimator predicts the absolute pose distribution, which can serve as the single-shot positioning pose and is also utilized in the system correction stage as a measurement. The EKF first performs a prediction step, utilizing the relative pose distribution to estimate the next state. This is followed by a correction step, in which the absolute pose distribution is used to refine the state estimate. The EKF computes the Kalman gain and residual, updating the state and covariance accordingly. After the prediction and correction stages through the EKF, the system outputs a final pose that integrates the entire trajectory using both relative and absolute poses. This final pose is known as temporal positioning. Each component is further detailed in the following sections."}, {"title": "Absolute Pose Estimator", "content": "The absolute pose estimator is responsible for regressing the 6DoF global pose of the camera with its covariance. This prediction is assumed to follow a normal distribution as\n$z^{truth}_t \\in N(z_t, \\Sigma_{z_t}),$ (1)\nwhere $z_t \\in \\mathbb{R}^6 \\simeq se(3)$, containing both global translation $z_x$ and rotation $z_{\\theta}$, and $\\Sigma_{z_t} \\in \\mathbb{R}^{6 \\times 6}$ are the pose predicted mean and covariance at time t, respectively. The Lie algebra $se(3)$ is the tangent space to the Lie group's manifold (Sola, Deray, and Atchuthan 2018), enabling smooth interpolation and compact representation. This allows us to model the covariance associated with each predicted pose, which is crucial for robust and reliable localization. Here, $z^{truth}_t \\in \\mathbb{R}^6 \\simeq se(3)$ is the truth value of the pose, which is the provided ground truth in the datasets.\nAs depicted in the lower part of Figure 2, the absolute pose estimator is initiated from a visual encoder with a single inputted image at time t. After a global average pooling layer reduces the feature dimension to $d_m$ denoted as $F^{apr}_t$, a single non-local style self-attention module (Vaswani et al. 2017) is employed to focus on spatial features as follows,\n$F^{apr}_t = \\alpha(Softmax(QK^T)V) + F^{apr}_t,$ (2)\nwhere Q is a set of queries, K is a set of keys, V is a set of values, \u03b1 is the scaled vectors, all of them are the output of a fully connected network with input $F^{apr}_t$. This architecture is inspired by AtLoc (Wang et al. 2020), which is considered lightweight and accurate.\nDifferentiating from AtLoc, we further predict the covariance information in the final layer. The output layer consists of four fully connected layers with input $F^{apr}_t$ responsible for predicting the mean values, $z_x$ and $z_{\\theta}$ for absolute translation and rotation, as well as their respective covariance matrices, $log(\\Sigma_{zx})$ and $log(\\Sigma_{z\\theta})$. Notably, log covariance matrices are applied to ensure numerical stability (Kendall and Gal 2017). The values will be recovered by taking the exp(.) operation after predicting to prevent values from reaching zero so that the covariance matrix defined as follows can ensure positive definite,\n$\\Sigma_{z_x} = \\begin{bmatrix} \\sigma^2_{zx}I_3 & 0_3\\\\0_3 & \\sigma^2_{z\\theta} I_3 \\end{bmatrix},$ (3)\nWe assert that the covariance matrix of each prediction should be diagonal, where the off-diagonal elements are zero, since each element in the predicted 6DoF pose is independent (Moreau et al. 2022). This independence implies that there are no relationships between two elements."}, {"title": "Relative Pose Estimator", "content": "The relative pose estimator is also tasked with regressing the 6DoF relative motion between consecutive images, along with its covariance matrix, assumed to follow a normal distribution as\n$u^{th}_{t,t-1} \\in N (u_{t,t-1}, \\Sigma_{u_{t,t-1}}),$ (4)\nwhere $u_{t,t-1} \\in \\mathbb{R}^6 \\simeq se(3)$ represents the combined relative translation and rotation between time steps t - 1 and t, and $\\Sigma_{u_{t,t-1}} \\in \\mathbb{R}^{6 \\times 6}$ denotes the covariance matrix associated with $u_{t,t-1}$. Additionally, $u^{th}_{t,t-1} \\in \\mathbb{R}^6 \\simeq se(3)$ represents the relative motion of the ground truth, calculated by $z^{truth}_t$ and $z^{truth}_{t-1}$.\nAs depicted in the upper part of Figure 2, the relative pose estimator begins with a shared visual encoder that receives consecutive images as input, followed by concatenating their output features. Subsequently, a single non-local style self-attention module is applied to learn spatial and temporal information, as Eq. (2). Through the attention network, the model can effectively prioritize feature changes associated with motion.\nSimilarly to the absolute pose estimator, four fully connected layers are employed to predict the mean of the relative motion, $u_x$ and $u_{\\theta}$, representing relative translation and rotation, respectively, along with their respective log covariance matrices, $log(\\Sigma_{ux})$ and $log(\\Sigma_{u\\theta})$. Upon applying the exp(.) operation, the covariance matrix of the relative pose is defined as\n$\\Sigma_{u_x} = \\begin{bmatrix} \\sigma^2_{ux}I_3 & 0_3\\\\0_3 & \\sigma^2_{u\\theta} I_3 \\end{bmatrix},$ (5)\nensuring positive definiteness."}, {"title": "Extended Kalman Filter Integration", "content": "The EKF is a recursive algorithm used to estimate the state of a dynamic system among uncertainty. To successfully apply the EKF, certain assumptions must be met. The first assumption, known as the Markov Chain Assumption, suggests that the current state is conditionally independent of all previous states, given the most recent state. This assumption is inherently satisfied in the positioning task. The second assumption, the Gaussian Noise Assumption, implies that the noise in the system, including both the process model and the measurement model, follows a Gaussian distribution. The previous statement that assumes that every state is Gaussian satisfies the second assumption.\nAs the EKF uses both estimates and knowledge of measurement distributions to find a distribution for the better estimate (Charles 2018), the EKF filter can be expressed as a Bayesian optimization. Through the prior $N(\\hat{x_t}, \\hat{\\Sigma_t})$ obtained from the prediction step and the likelihood $N (z_t, \\Sigma_{zt})$ from the correction step, we can get the optimal distribution $N(x_t, \\Sigma_t)$ according to following Bayes' theorem.\nIn the context of the EKF, the posterior distribution at time t is conditioned on the previous state $x_{t-1}$, the control unit $u_{t,t-1}$, and the measurement $z_t$ can be expressed as $p(x_t|x_{t-1}, u_{t,t-1}, z_t)$, which satisfies the Markov property. This formulation captures the recursive nature of the EKF, where each posterior distribution serves as the prior for the subsequent time step, continuously refining the state estimate based on new measurements and control inputs.\nTheorem 1. If $x_t$ is only conditioned to the measurement $z_t$, the posterior distribution of $x_t$ given $x_{t-1}$, $u_{t,t-1}$, and $z_t$ will be proportional to the product of the measurement likelihood and the transition probability, expressed as follows:\n$p(x_t|x_{t-1}, u_{t,t-1}, z_t)\n\\propto p(z_t|x_t)p(u_{t,t-1}|x_t, x_{t-1}).$ (6)\nProof.\n$p(x_t|x_{t-1}, u_{t,t-1}, z_t)\n=\\frac{p(z_t|u_{t,t-1}, x_{t-1}, x_t)p(x_t, x_{t-1}, u_{t,t-1})}{p(z_t)}$ (7)\n$\\propto \\frac{p(z_t|u_{t,t-1}, x_{t-1}, x_t)p(x_t, x_{t-1}, u_{t,t-1})}{p(z_t)}$ (8)\nAccording to the Bayes' theorem, the Eq. (7) can be factored into Eq. (8), which consists of the measurement likelihood $p(z_t|u_{t,t-1}, x_{t-1}, x_t)$ and the transition probability $p(x_t, x_{t-1}, u_{t,t-1})$ over prior $p(z_t)$. This equation highlights the importance of integrating both the measurement information and the prior state estimate to obtain an updated posterior distribution.\nThe subsequent step is to simplify the calculation by leveraging the conditional independence assumptions inherent in the EKF. Thus, the measurement likelihood can be simplified as follows,\n$P(z_t| u_{t,t-1}, x_{t-1}, x_t) = p(z_t|x_t),$ (9)\nwhich indicates that the measurement is solely dependent on the current state and is independent of control input and the previous state.\nSimilarly, for the prior distribution, the simplification can be expressed as,\n$p(x_t, x_{t-1}, u_{t,t-1}) = p(u_{t,t-1}|x_{t-1},x_t)p(x_{t-1},x_t)\n\\propto p(u_{t,t-1}|x_t, x_{t-1}),$ (10)\nwhich shows the prior is proportional to the control unit according to the current and previous state.\nThus the Eq. (8) can be further simplified as follows,\n$p(x_t|x_{t-1}, u_{t,t-1}, z_t)\n=\\frac{p(z_t|u_{t,t-1}, x_{t-1}, x_t)p(u_{t,t-1}|x_{t-1},x_t)p(x_{t-1},x_t)}{p(z_t)}\n\\propto p(z_t|x_t)p(u_{t,t-1}|x_t, x_{t-1}).$ (11)\nThis final form demonstrates how the EKF integrates information from both the measurement and the state transition to update the posterior distribution of the state estimate. Specifically, in our scheme, the measurement corresponds to the APR, while the transition model reflects the RPR. Thus, the posterior distribution captures the combined influence of the current APR measurement and the dynamics represented by the RPR transition model."}, {"title": "Training the EKF Integrated Model", "content": "APR and RPR Loss Functions\nBenefiting from the inherent flexibility of Bayes' theorem in EKF, we partition the training process into relative and absolute positioning branches. According to Eq. (6), we can effectively optimize both the prior and likelihood components independently. This enables us to formulate the maximized function more effectively.\nHowever, the likelihood $p(z_t|\\hat{x}_t)$ and the transition probability $p(u_{t,t-1}|x_t, x_{t-1})$ still can not be solved analytically. To address this, we aim to approximate them using variational inference with a simpler parameterized distribution $q(z_t|I_t; \\Theta_{APR})$ and $q(u_{t,t-1}|I_t, I_{t-1}; \\Theta_{RPR})$, where $\\Theta_{APR}$ and $\\Theta_{RPR}$ denote the absolute and relative model parameters, respectively. We optimize these parameters by maximizing the evidence lower bound as follows,\n$\\Theta_{APR} = arg \\underset{\\theta}{max} \\ q(z_t|I_t; \\Theta_{APR}),$ (12)\n$\\Theta_{RPR} = arg \\underset{\\theta}{max} \\ q(u_{t,t-1}|I_t, I_{t-1}; \\Theta_{RPR}),$ (13)\nthese density functions serve as the surrogate objective function that balances the trade-off between the complexity of the approximation and the fidelity to the true distribution. To learn the parameters $\\Theta_{APR}$ and $\\Theta_{RPR}$, we take the negative logarithm of Eq. (12) and Eq. (13) as our loss function to minimize and define as follows,\n$L_{APR} = \\frac{1}{2} \\Sigma_{z}^{-1} ||z - z^{truth}||^2 + \\frac{1}{2} log(\\Sigma),$ (14)\n$L_{RPR} = \\frac{1}{2} \\Sigma_{u}^{-1} ||u - u^{truth}||^2 + \\frac{1}{2} log(\\Sigma).$ (15)\nUnlike other approaches that utilize geometric loss (Kendall and Cipolla 2017) to balance the weight between translation and rotation, our model employs this loss function to learn the true distribution of each timestamp, providing a more accurate depiction of the camera's state. By separately minimizing these two loss functions, Eq. (14) and Eq. (15), and integrated with EKF, the posterior of the whole system can be maximized, leading to the optimal system."}, {"title": "EKF Updating and Correction", "content": "Then, we demonstrate how we leverage EKF to enhance the robustness of our predictions. Figure 1 provides a visual representation of how the EKF is seamlessly integrated into our framework, and Figure 3 shows the pose graph, which highlights the operation of the EKF within the overall scheme.\nIn the EKF algorithm, the process model and the measurement model are defined as follows,\n$x_t = f(x_{t-1}, u_{t,t-1}), +N(0, \\Sigma_{u_{t,t-1}})$ (16)\n$z_t = h(x_t) + N(0, \\Sigma_{z_t}),$ (17)\nwhere f(.) is the state transition function for combining the previous state and the relative pose to the new state. h(.) is the measurement function to measure the current state, which is exactly the identity in our approach, and $N(0, \\Sigma_{u_{t,t-1}})$ and $N(0, \\Sigma_{z_t})$ are the white noise, respectively, in the process model and the measurement model.\nAnd for the prediction step, the process involves using the predicted relative pose $u_{t,t-1}$ to estimate the next state $\\hat{x}$ as Eq. (18) and predicting the next covariance $\\hat{\\Sigma}$ from $\\Sigma_{u_{t,t-1}}$ as Eq. (19).\n$\\hat{x} = x_{t-1} \\oplus u_{t,t-1},$ (18)\n$\\hat{\\Sigma_t} = F\\Sigma_{t-1}F^T + \\Sigma_{u_{t-1}},$ (19)\n$F_t := \\frac{\\partial f| }{\\partial x_{t-1},U_{t,t-1}},$ (20)\nwhere $\\oplus$ is the manifold update operation, which is needed since the states are under se(3). Here, $F_t$ is the Jacobian matrix of f(.) at time t.\nMoving to the correction step, the measurement residual $r_t$ and the Kalman gain $K_t$ are sequentially computed through measurement and the state from the prediction step as follows\n$r_t = z_t \\ominus h(\\hat{x}),$ (21)\n$K_t = \\hat{\\Sigma_t}H^T(H_t\\hat{\\Sigma_t} H^T + \\Sigma_{z_t})^{-1},$ (22)\n$H_t := \\frac{\\partial h }{\\partial x}$ (23)\nwhere $\\ominus$ is also the manifold operation representing the inverse operation of $\\oplus$. Here $H_t$ is the Jacobian matrix of h(.).\nTherefore, the corrected state estimate $x_t$ and the updated state covariance $\\Sigma_t$ is obtained as follows,\n$x_t = \\hat{x} \\oplus K_tr_t,$ (24)\n$\\Sigma_t = (I - K_tH_t)\\hat{\\Sigma_t}.$ (25)\nAnd finally, the prediction distribution $N (x_t, \\Sigma_t)$ is well defined and produced."}, {"title": "Experimental Results", "content": "Implementation Details\nWe input pairs of consecutive images into our model, concurrently training both the absolute and relative branches. To maintain consistency, we resize the shorter side length of the input images to 256 pixels and utilize a ResNet34 backbone pretrained on ImageNet for the visual encoder. The images undergo normalization to ensure a zero mean and a standard deviation of 1. We employ a batch size of 64 and set the learning rate to 5 \u00d7 10\u20135 for both branches. A dropout rate of 0.5 is applied to both branches to enhance generalization, and the Adam optimizer (Kingma and Ba 2014) is used for optimization. These hyperparameters are determined through grid search, with the learning rate ranging from 10-4 to 10-6, the batch size varying from 4 to 64. Additionally, we incorporate a weight decay of 5 \u00d7 10-4 to further mitigate overfitting.\nWhen training on the Oxford RobotCar dataset, we further apply random ColorJitter augmentation, setting values of 0.7 for brightness, contrast, and saturation, and 0.5 for hue. This augmentation step is observed to be crucial for improving generalization across various weather and time conditions (Wang et al. 2020). Note that we do not apply random cropping during training, as done in AtLoc, and we also avoid center cropping during inference. This decision is made to maintain consistency with the corresponding ground truth poses, as random cropping could potentially alter the spatial context of the images.\nThe scheme is implemented using PyTorch (Paszke et al. 2019), a widely adopted deep learning framework known for its flexibility, ease of use, and extensive community support. All experiments are conducted on a personal computer equipped with an Intel Core i9-12900K CPU @ 3.2GHz \u00d7 16 and an Nvidia GeForce RTX 3090Ti GPU.\nBaselines\nTo evaluate the effectiveness of our model, we conducted experiments on both indoor and outdoor datasets, specifically the 7-Scenes dataset (Shotton et al. 2013) and the Oxford RobotCar dataset (Maddern et al. 2017). Each dataset presents unique challenges that our model must address.\nGiven our use of ResNet as the visual encoder backbone, we conduct comparisons with algorithms that employ a similar architecture. To ensure credibility, we exclusively select papers with available codes, enabling a fair and relevant benchmarking process. Baseline algorithms are categorized into single-shot methods, which consider only single images, and temporal methods, which utilize sequential images."}, {"title": "Conclusion", "content": "In conclusion, we present a novel integration approach for learning-based monocular positioning through the EKF, effectively leveraging both APR and RPR to address their inherent limitations. Our method is grounded in variational Bayesian inference, providing a solid theoretical foundation. The integration of EKF significantly improves the robustness and accuracy in positioning. Experimental results demonstrate our approach's effectiveness, achieving competitive single-shot accuracy and surpassing existing methods in temporal performance on the 7-Scenes and Oxford RobotCar datasets. The consistent and reliable covariance estimates further validate VKFPos as a valuable advancement in learning-based monocular positioning."}]}