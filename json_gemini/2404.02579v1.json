{"title": "Learning Alternative Ways of Performing a Task", "authors": ["D. Nieves", "MJ. Ram\u00edrez-Quintana", "C. Monserrat", "C. Ferri", "J. Hern\u00e1ndez-Orallo"], "abstract": "A common way of learning to perform a task is to observe how it is carried out by\nexperts. However, it is well known that for most tasks there is no unique way to perform\nthem. This is especially noticeable the more complex the task is because factors such as the\nskill or the know-how of the expert may well affect the way she solves the task. In addition,\nlearning from experts also suffers of having a small set of training examples generally coming\nfrom several experts (since experts are usually a limited and expensive resource), being\nall of them positive examples (i.e. examples that represent successful executions of the\ntask). Traditional machine learning techniques are not useful in such scenarios, as they\nrequire extensive training data. Starting from very few executions of the task presented as\nactivity sequences, we introduce a novel inductive approach for learning multiple models,\nwith each one representing an alternative strategy of performing a task. By an iterative\nprocess based on generalisation and specialisation, we learn the underlying patterns that\ncapture the different styles of performing a task exhibited by the examples. We illustrate\nour approach on two common activity recognition tasks: a surgical skills training task and a\ncooking domain. We evaluate the inferred models with respect to two metrics that measure\nhow well the models represent the examples and capture the different forms of executing a\ntask showed by the examples. We compare our results with the traditional process mining\napproach and show that a small set of meaningful examples is enough to obtain patterns\nthat capture the different strategies that are followed to solve the tasks.", "sections": [{"title": "1. Introduction", "content": "Nowadays, humans learn the execution of a complex task through three steps: study-\ning the description of the task, watching video executions of the task (usually performed\nby experts) or real-life demonstrations of it, and, finally, executing the task under expert\nsupervision several times (Ericsson, 2009). This way of acquiring the skills needed to per-\nform a task is expensive in time and resources. Besides, the lack of continuous supervision\nmay induce mistakes because of the limited experience of the operators or lack of attention\nbecause of the repetitiveness of the task. In this sense, artificial intelligence (AI), and ma-\nchine learning (ML) in particular, is making it possible to help people in their daily lives\nby learning models about their tasks. These models can then be integrated into direct as-\nsistance systems, such as task training learning environments, supervisory contexts in order\nto avoid human mistakes, or machine-human collaboration contexts, where the machine is\nused as an assistant for a complex task. Research in this direction can be found in fields\nsuch as Ambient Intelligence (Camacho et al., 2014), Context-aware systems (Hong et al.,\n2009), Advanced Driving Assistants (\u0160krjanc et al., 2018) or Surveillance systems (Kardas\n& Cicekli, 2017), to name a few.\nHowever, traditional ML techniques require large volumes of data in order to infer models\nof common tasks. In many areas, such as surgery, the access to training examples is very\ncostly, as these are very complex tasks that very few people know how to perform or require\nspecific permissions or instrumental. In these cases, the knowledge acquisition has to be done\nfrom a small set of examples, with examples being a series of steps that an expert carried out\nto complete the task from beginning to end. Besides, this learning process must consider\nthat any process can be performed in several ways; experts can present different styles\nwhen executing the task (in many cases, involuntarily) and can contain noise (interpreted\nas non-essential activities of the task).\nOther approaches (Yang et al., 2017; Blum et al., 2008) can generalise a pattern of\nthe task from a few positive examples. However, they only can learn one general pattern\nfrom the set of provided examples. The same case occurs with methods such as 'Fuzzy\nMining' (G\u00fcnther & van der Aalst, 2007) or the Alpha-algorithm (Alves de Medeiros et al.,\n2004). The previous solutions can lead to incorrect learning of the task and even dangerous\nlearning. Let us illustrate the case with an example. Imagine a scenario where a system has\nto supervise the task of cooking a carrot soup. Table 1 shows the vocabulary of activities\ninvolved in this task. We have two examples of how to cook this dish (expressed as sequences\nof activities): \u201cSABCDFGHIMOPT\u201d and \u201cSABCEFGJKLNOPT\u201d. These examples show two\ndifferent forms of cooking: using a microwave or a kitchen stove. If we want to learn a\ngeneral rule from these two examples, a possible generalisation is the expression \u201cSABC\n{D|E}FG{HIM|JKLN}OPT\", where \u201c|\u201d means a disjunction between two activity groups for\nthe portion of the task delimited by curly brackets. The two given examples are covered\nby the model, but there are other valid sequences of activities according to this model\nthat are not safe. For instance, the sequence \u201cSABCEFGHIMOPT\u201d, where the person takes\na metal pot, put the ingredients in, and, then, put the metal pot into the microwave to\nprepare the soup. We need a learning procedure and a representation language that is able\nto find the right trade-off between an overgeneralisation covering almost everything and\nan overspecialisation that is simply the sequence composed by the disjunction of the two\nexamples \"{SABCDFGHIMOPT|SABCEFGJKLNOPT}\". Additionally, there can be examples\nthat contain noise, for instance the sequence \u201cSABBCDFGHIMOPT\u201d is wrong, as the person\nwashes the carrot twice (activity B) before cutting it. To avoid an overgeneralisation, given\nthat we do not have negative examples, the model should be as close as possible to the\nobserved examples but, at the same time, avoiding the overspecialisation and the noise as,\nfor instance, the pattern \u201cSABC{DFGHIM|EFGJKLN}OPT\u201d.\""}, {"title": "3. A graph-based method for inferring Multiple Models of a Task", "content": "In this section, we present a method to learn the different ways of carrying out a task\nfrom several executions of the task performed by experts. We consider a task execution as a\nsequence of activities that are realised consecutively with a start and an end. Following that\nsequence, the task is successfully completed. Formally, we define $A$ as the set of activities\n(vocabulary) that can be used to accomplish the task, being an execution example a finite\nsequence of these activities $\\delta = (a_1, a_2,..., a_n), a_i \\in A$. We denote the full set of sequences\nprovided to the system as $\\Delta$.\nWorking directly with sequences has several limitations in order to learn and express, in\nan intuitive and simple way, how a task should be realised. Firstly, if the sequence is too\nlong, it becomes extremely difficult to understand the flow of activities that is taking place.\nNote that the main aim is to obtain a model of the task that could be used for training\nand/or supervising non-expert apprentices. Therefore, it is crucial that the learnt model can\nbe easily interpreted by a human. Secondly, the problem of learning from task executions\n(especially when the tasks are complicated to execute because of the skills they require)\nis that the sequences may contain infrequent activities or noise. In our case, noise are\nthose activities that are not really needed to complete the task\u00b2. It may be complicated to\ndetect during the learning process whether the elimination of these noisy activities from the\nsequences leads to invalid models that are not able to complete the task, because the same\nactivity can be essential in one part of the task and unnecessary in another part. Therefore,\nthe use of a graph-based representation language to compactly simplify the sequences of\nactivities as dependency graphs helps in this sense (see Section 3.2). In this case, the\npossible loss of information due to the change of representation is clearly compensated\nbecause dependency graphs greatly facilitate the process of identifying which activities are\nessential to perform a task (and therefore will be part of the models), which activities are\ndispensable because they are non-essential, as well as the formal confirmation that a model\nreally express a correct way to fulfil the task.\nThe general pipeline of our approach is shown in Figure 1. In the first step, each sequence\nof activities is simplify into a dependency graph. The second step is the algorithm that infers\nthe different models."}, {"title": "3.1. Graph-based formulation: notation and definitions", "content": "A dependency graph is a labelled directed graph $G = (V, E)$, where $V$ is a set of labelled\nvertices, and $E \\subseteq V \\times V$ is a set of weighted directed edges, such that each edge has a weight\ngiven by a weight function $w : E \\rightarrow N_{\\geq1}$. The labels of the vertices belong to a finite set\nof labels $L = {l_1,...,l_n}$, where each $l_i, 1 \\leq i \\leq n$, denotes an activity $a_i \\in A$. $V$ contains\ntwo special vertices, $v_S$ and $v_F$, that represent two synthetic activities denoting the starting\nand ending of the task. The two synthetic activities are implicitly placed at the beginning"}, {"title": "3.2. Data formalisation: from activity sequences to dependency graphs", "content": "In this section, we describe how to convert an activity sequence $\\delta = (a_1,..., a_n)$ into a\ndependency graph $G$. Firstly, $V(G)$ is created containing $v_S$, $v_F$ and as many vertices $v_i$ as\ndifferent activities $a_i$ are in $\\delta$ with labels $l_i = a_i$. $E(G)$ is also initialised by containing the\ndirected edges $(S, a_1)$ and $(a_n, F)$ with weights equal to 1. Then, for each pair of consecutive\nactivities $(a_i, a_j) \\in \\delta$, if the edge $e = (a_i, a_j)$ already belongs to $E(G)$, then its weight is\nincreased $w_G(e) = w_G(e) + 1$; otherwise, $e$ is added to $E(G)$ with weight $w_G(e) = 1$. Figure\n2 illustrates this conversion process. On the top, it is shown an activity sequence of length\n19 composed by 8 different activities ($a_1$ to $a_8$), some of them appearing more than once\n(marked in colour). This sequence is expressed as the dependency graph on the bottom of\nthis figure, which is formed by 10 vertices and 10 edges. The labels on the edges indicate\ntheir weights. For instance, the weight of edge ($a_4, a_5$) is 4 because the activity $a_4$ appears\nfollowed by the activity $a_5$ four times in the sequence.\nNote that a dependency graph is a representation more concise than the original activity\nsequence. That means that different sequences can produce the same dependency graph.\nThis can happen when there are activities that are performed more than once along the\nsequence. This could imply losing some information about the order in that the activities\nhave been done when the sequence turns into a dependency graph. For instance, the se-\nquences $\\delta_1 = (a_1, a_2, a_3, a_4, a_2, a_4, a_2, a_4, a_5)$ and $\\delta_2 = (a_1, a_2, a_4, a_2, a_4, a_2, a_3, a_4, a_5)$ generate\nthe same dependency graph. Thus, by looking at the graph we can only say that activity"}, {"title": "3.3. Mining Multiple Models from dependency graphs", "content": "In this section we describe a new method for inducing multiple models from a set of\ndependency graphs $G = {G_1, G_2, ..., G_n}$. A model $M$ is a dependency graph that satisfies\nthe following conditions:\n\u2022 $M$ is valid and, consequently, there exists in $M$ (at least) one walk from $S$ to $F$. In\nterms of the original task, this means that such a walk indicates the activities that are\nneeded to perform the task.\n\u2022 $M$ has to overlap at least one dependency graph in $G$. Newly, in terms of the original\ntask, this means that there exits at least one expert execution that has performed the\nsame activities included in $M$. In other words, $M$ captures one of the ways of solving\nthe task according to the experts.\nAdditionally, the presence of noise in $M$ (non-essential activities for the task) should be\nminimised as much as possible, in order to obtain the desired trade-off between generality\nand specificity.\nThe MMDG algorithm (Algorithm 1) combines a generalisation operator (graph aggre-\ngation) with a refinement operator. After applying both operators, the dependency graphs\nthat overlap (Definition 6) with the induced model are removed. This process is repeated\nusing the remaining graphs until all the dependency graphs overlap with one of the models.\nIn this way, we are able to infer the different styles of solving the task showed by the experts\nin their executions. The MMDG algorithm is inspired by sequential covering strategies of\nrule learning (F\u00fcrnkranz, 1999).\nMore concretely, the MMDG algorithm works as follows. First, the most general graph\nis generated by aggregation (Line 4). This aggregated graph contain all the vertices and\nedges included in the dependency graphs $G_i \\in G$. Clearly, this aggregated graph is too much\ngeneral: (1) it contains not only valid walks but other new walks (not included in any $G_i$)\nby combining partial walks from several $G_i$ (that probably do not represent a correct way\nto execute the task), and (2) it is the noisiest graph of all that can be generated from $G$.\nHence, $G^+$ must be refined below (Line 6). This refinement returns one model $M$ and the\nset of dependency graphs $G$ that overlap with $M$. Then, the set of dependency graphs $G$\nthat overlap with the refined graph $M$ are removed from $G$ (Line 7) and $M$ is added as\na part of the solution (Line 8). Then, the next iteration of the algorithm starts with the\nremaining examples, creating a new aggregated graph and so on. This process is repeated\nuntil complete the solution."}, {"title": "4. Experimental evaluation", "content": "We will analyse the quality of our method through a set of metrics that allow us to\ndetermine how well the models capture the styles of performing a task observed in the ex-\npert executions (expressed as dependency graphs). More concretely, we will consider the\nfollowing two quality measures: fitness and simplicity. Given a set of dependency graphs $G$\nand a set of the models {$M_i$}, the fitness of each model $M_i$ is the number of dependency\ngraphs of $G$ with which $M_i$ overlaps. Note that if some dependency graphs overlap with a\ncertain model is because they share a similar way of solving the task. Hence, the fitness\nof a model is an indicator of how well the model captures such a way to execute the task.\nTo compute the fitness of a solution (the set of inferred models), the models are applied in\nthe order in that are generated. Lastly, the simplicity (or, alternatively, complexity) of a\nmodel is measured in terms of the number of edges and vertices that compose it. Fitness\nand simplicity are well-known quality metrics of process discovery algorithms (Buijs et al.,\n2012). They are related to other state-of-the-art evaluation metrics in data science (Geng\n& Hamilton, 2006): Generality/Coverage and Conciseness measures, respectively. Addi-\ntionally, the generality/coverage relation is employed in concept learning, rule-learning and\ninductive logic programming to determine the examples that are matched by a hypothesis\n(model) (Raedt, 2010).\nTo experimentally evaluate our approach, we implemented a prototype using the pro-\ngramming language R\u2075. The full code and data used for the experiments can be found in"}, {"title": "4.1. Case Study 1: Surgical data", "content": "There are not many public medical datasets specialised in laparoscopic exercises. Among\nthe few available, possibly the most popular and complete is the JHU-ISI Gesture and Skill\nAssessment Working Set (JIGSAWS) dataset (Gao et al., 2014), which has been registered by\nusing the surgery system robotics da Vinci. Originally conceived for developing applications\nfocused on the surgical motion analysis and the automatic skill assessment, this dataset has\nbecome a public benchmark for evaluating the performance of the state-of-the-art methods\nin surgical activity recognition (Ahmidi et al., 2017). Comparing with other human tasks,\nincluding surgery domain, the activities involved in suturing entail more complexity and\ndiversity of movements than other training routines (Cao et al., 1996).\nWe used the suturing executions provided by JIGSAWS dataset to learn the models\nfrom the different ways in which this task can be conducted. A simple suturing routine is\ndivided into three phases. Firstly, the surgeon has to reach the needle and move it at the\ncorresponding dot on the tissue. Then, the main phase of the exercise is performed: the\nsuturing cycles. Figure 5 shows this phase. For each cycle iteration (loop), the surgeon must\npush the needle against the next dot and take it out on the other side of the incision (Frame\n1). The extraction of the needle must be carried out with the other hand (Frame 2). Then,\nthe needle is transferred to the first hand again (Frame 3). Once the suturing cycles are\ncompleted, the last phase consists in laying the needle down at the finishing mark on the\ntissue. Concretely, for the experiments we chose a 4-throw suturing procedure, where the\ndigit denotes the number of loops required to perform the suture.\nIn this context, an activity represents an atomic surgical gesture with a meaningful out-\ncome and it is annotated following a specific activity language of this domain (Table 2).\nHence, each activity transcription includes the name of the gesture, and the start and end\nframes in the video. Each performance contains from 15 to 20 activities per recording. It is\nnecessary to mention that the activities of each task performance are annotated in chrono-\nlogical order of execution and there are not overlapping in time. Therefore, we consider\neach execution as a time-ordered sequence of labelled activities that a surgeon performed\nto completely fulfil the suturing task. Thus, we have preprocessed the transcription files\nto remove the time information and, then we have converted the activity sequences into"}, {"title": "4.1.1. Experimental Setup", "content": "The input dependency graphs have been divided in two groups: the training and the test\nsets. Given that we have few examples, we prepared three exploratory scenarios applying\ndifferent criteria to select the training graphs and analysed the impact of the training data"}, {"title": "4.1.2. Experimental results", "content": "Before reporting the performance obtained by our method in the experimental evaluation,\nwe show in Figure 7 the models learnt in Experiment 1.\nAs can be seen, we can clearly identify the phases of a suturing exercise in models $A_1$\nand $B_1$: the needle reaching, the suture cycle and the needle releasing at the finishing mark.\nAlthough both models are very similar, we detect two important differences: $B_1$ incorporates\nthe activity $G_8$ (i.e., reorienting the needle) before the suturing cycle and the activity $G_9$ (i.e.,\nusing the right hand to tighten the suture) just before releasing the needle. In contrast, in\nthe third iteration, no edges were removed from $G^+$ to generate model $C_1$. As a consequence\n$C_1$ is a large and complex model that captures more specific ways of performing the suturing\nexercise."}, {"title": "4.2. Case Study 2: brownie cooking", "content": "We have applied the MMDG algorithm to another challenging problem in a different\ndomain: cooking. This domain is especially challenging for different reasons: the examples\nhave high variability among them and they do not contain extra information about the\nquality of the execution. The brownie cooking dataset (Spriggs et al., 2009) consists of\n16 executions from different users and, at most, 11 different activities per execution. The\nscarcity of data comes not only from the number but also the length of the examples. Table\n6 shows the vocabulary of the brownie cooking dataset.\nOur algorithm obtains four models. Figure 9 shows the aggregated graph and the first\nthree models. The fourth model (not shown) includes all the variability not contemplated by\nthe three previous models, as has been described previously. With only these 16 examples,\nMMDG can extract three ways of cooking the brownie that allows for the understanding\nand supervising new executions of the task. As can be seen in Figure 9, as the threshold\ndecreases, the variability of the model increases, giving rise to increasingly complex graphs.\nThe model obtained in the first iteration and corresponding to a threshold of 7 (Figure 9.b)\npractically represents a sequence of activities and would correspond to a strict monitoring\nof the recipe. In the second model obtained with a threshold of 4 (Figure 9.c), once the\negg has been introduced into the bowl (which coincides with the first steps of the simplest\nmodel), the model has presented some variations with respect to strict monitoring of the"}, {"title": "5. Discussion", "content": "Given the existence of process mining tools (van der Aalst et al., 2017), a first question\nis how this compare to them. In the case of learning a workflow model, the main goal of\nprocess mining is to model the process that underlies in the event logs, but not to extract a\nset of models with the different forms of the task and the essential activities and transitions\nin them. In principle, they do not assume that there may be noise, non-essential or missing\nactivities in the process logs. Then, the whole processes are modelled. This way of doing\nprocess modelling often leads to spaghetti-like models (G\u00fcnther & van der Aalst, 2007).\nTherefore, the commercial tools like Fluxicon DISCO includes options that allow for the\nsimplification of the obtained model based on the \"popularity\" of the edges. However, the\nfinal result is only one process model and it is not able to extract all the ways of performing a\ntask. In addition, tools like DISCO do not check that the simple model obtained is supported\nby one example at least.\nWe can see some of these issues after applying DISCO to the data for the surgical and\ncooking domains. Figure 11 shows that DISCO only obtains one model from the aggregated\ngraph and it is obtained by collapsing edges and nodes depending on the chosen threshold\n(manually determined). According to the algorithm description, the DISCO model obtained\nfrom the brownie examples has two disjoint paths in the middle of the model. In one path\nthe user must put water (A_16) and oil (A_14) in the bowl but not the brownie bag\n(A_13). In the other path the user must put the brownie bag but not the water and the\noil. These execution options, which are clearly wrong, were not possible with the MMDG\nalgorithm because we check that the execution can be completed correctly following the\nmodel. Besides, we can extract not only one but all the model executions that have a\nminimum number of examples."}, {"title": "6. Conclusions and Future Work", "content": "In this paper, we have presented a new approach to learn the different strategies for\nperforming a task based on a few examples. Our proposal starts from an event log with the\nset of executions of a task in the form of sequences of activities. After converting them into\ndependency graphs, we apply our novel inductive method for learning models from these\ndependency graphs. As a result, we can generate the multiple models with the essence of\nthe task, eliminating the noise (i.e., unnecessary transitions between activities). We have\napplied our approach to two challenging task: suture in minimally invasive surgery and\ncooking a brownie. Having in mind that we only have positive examples, we have evaluated\nthe results with quality dimensions that are usually applied in process mining: fitness and\nsimplicity, which is a common combination (e.g., MML/MDL principles, (Wallace & Dowe,\n1999)) in situations with scarcity of data, where other regularisation terms cannot be used.\nAccording to this balance in performance metrics, the results are quite good, showing that\nour approach was effective in noise reduction independently of the quality of the examples.\nHowever, the procedure is very sensitive to the quality in the training set: increasing the\nquality of the examples during this phase could be translated into a significant reduction\nof the training set size. Thereby, if the examples are rich enough, the algorithm is able to\nlearn good quality models with fewer examples.\nIn order to explore and analyse the MMDG method to other tasks provided by JIGSAWS\ndataset, we have developed a Shiny application\u00ba. On this web application, users can select\na task, apply the learning models and replay the different examples (trials) on the obtained\nmodels. It also possible to watch all the steps performed by the algorithm until obtaining\nthe final models.\nIn the future we will concentrate on extending our approach to also consider short\ndescriptions in natural language of the task (performed by an expert) in order to provide\nthe system with an automatic or semiautomatic verification step after the learning process.\nFinally, we will apply our developments to the automatic learning and supervision system\nof skilled tasks that is one of the most relevant application areas of our method."}]}