{"title": "SC-Bench: A Large-Scale Dataset for Smart Contract Auditing", "authors": ["Shihao Xia", "Mengting He", "Linhai Song", "Yiying Zhang"], "abstract": "There is a huge demand to ensure the compliance of smart contracts listed on blockchain platforms to safety and economic standards. Today, manual efforts in the form of auditing are commonly used to achieve this goal. ML-based automated techniques have the promise to alleviate human efforts and the resulting monetary costs. However, unlike other domains where ML techniques have had huge successes, no systematic ML techniques have been proposed or applied to smart contract auditing. We present SC-Bench, the first dataset for automated smart-contract auditing research. SC-Bench consists of 5,377 real-world smart contracts running on Ethereum, a widely used blockchain platform, and 15,975 violations of standards on Ehereum called ERCs. Out of these violations, 139 are real violations programmers made. The remaining are errors we systematically injected to reflect the violations of different ERC rules. We evaluate SC-Bench using GPT-4 by prompting it with both the contracts and ERC rules. In addition, we manually identify each violated rule and the corresponding code site (i.e., oracle) and prompt GPT-4 with the information asking for a True-or-False question. Our results show that without the oracle, GPT-4 can only detect 0.9% violations, and with the oracle, it detects 22.9% violations. These results show the potential room for improvement in ML-based techniques for smart-contract auditing.", "sections": [{"title": "1 Introduction", "content": "Ethereum [35, 13] is a decentralized, open-source blockchain platform that has become the de facto for running decentralized applications like smart contracts [14, 17]. To govern smart contracts running on Ethereum, a set of formal standards called Request for Comments, or ERCs, have been developed [31]. For example, the ERC20 standard [33] defines a common set of rules for fungible tokens - digital assets that are interchangeable with one another. ERCs play a crucial role in the Ethereum ecosystem by providing a common set of rules and specifications that developers can follow when implementing smart contracts. The violation of an ERC could result in interoperability issues, security vulnerabilities, and financial loss. Additionally, ERC violations could result in the de-listing of tokens from exchanges, as many exchanges have listing requirements of following ERC standards [15].\nDespite the importance of following ERC rules, it is hard for developers to always do so, as they need a comprehensive understanding of all ERC requirements and their contract code. ERC standards include a huge number of rules, and the number keeps growing as standards are developed over time. For just the three ERC standards involved in this work, there are 153 rules. ERC rules are described in different ways, with some being code declarations and others being natural languages. Meanwhile, contract implementations are often complex as well. One contract and its dependent code usually contain hundreds to thousands of lines of source code in multiple files. Some code details may be obscured within intricate caller-callee relationships, while others may involve objects and functionalities written by different programmers. Different rules apply to different parts of a smart contract, and even a small code site could involve multiple rules. These complexities in ERC rules and smart contracts make it extremely hard for programmers to check for ERC violations manually. As a result, ERC rule violations widely exist in real-world smart contracts [8].\nTo detect ERC violations, today's common practice heavily relies on human efforts. Automated, program-analysis-based checkers for ERC rules do exist [9, 24], but they fail to detect complex violations because of many ERC rules' non-structured, natural-language-based definitions. As a result, smart contracts commonly undergo human auditing, provided by specialized services with security experts [6, 28, 25, 3, 20, 1, 8]. Auditing services are not only costly but also slow. For example, we examined the history of 30 smart contracts that were submitted for manual auditing on a platform called Ethereum Commonwealth Security Department. We found that each contract only has an average of 260 lines of code but is audited for ten days with an estimated cost of $500. Clearly, manual auditing is not a scalable approach for smart contract development.\nA promising, scalable approach for automated smart-contract auditing is to leverage large language models (LLMs), both because of LLMs' success in domains like program generation [30] and bug fixing [21, 38] and because of a fair amount of ERC rules' natural-language descriptions. To develop LLM-based techniques (e.g., fine tuning, in-context learning, few-shot learning, etc.) for smart contract auditing, an important step is to construct quality datasets. Unfortunately, no smart contract datasets exist that can be used for ERC rule checking and fixing. Traditional program bug datasets [29, 19] cannot be used for smart-contract auditing, as unlike ERC rules, compiler errors and program runtime errors have well-defined, structured definitions.\nTo drive research and practices in smart-contract auditing and to assist real users with their auditing tasks, we release SC-Bench, the first dataset of real-world smart contracts and their ERC-rule violations. SC-Bench consists of 15975 ERC violations and 5377 real-world smart contracts, collected from etherscan.io [18], polygonscan.com [26], and Ethereum Commonwealth Security Department [8]. 145 violations from 30 contracts are real-world ERC violations we collect and inspect. As real violations are rarely published, we build program analysis techniques and inject 15836 violations into 5347 contracts according to 88 ERC rules.\nWe evaluate SC-Bench using GPT-4 with two methodologies. The first prompts GPT-4 with the contract to be inspected and the full ERC rule sets. Our results show that GPT-4 only detects 29% of real violations and 0.6% of injected violations. To test how GPT-4 could potentially detect violations given oracle information, we manually identify the violated ERC rule(s) and violating code sites for each contract. We then prompt GPT-4 with this precise information and ask it a True-or-False question. GPT-4 successfully detects 42.8% of real violations and 22.8% injected violations. The increase in detecting indicates the potential room for ML-based techniques to improve smart contract automated auditing.\nOverall, SC-Bench goes beyond being a key contribution to Ethereum smart contract auditing. Other software auditing and verification tasks, such as security defenses, can potentially use SC-Bench's contract dataset, its violation dataset, or its methodologies for evaluation. We have released SC-Bench and all our source code at https://github.com/system-pclub/SC-Bench."}, {"title": "2 Background", "content": "This section provides background on Ethereum, smart contracts, ERCs, and ERC violations."}, {"title": "2.1 Ethereum and Smart Contracts", "content": "Ethereum is a blockchain platform where developers can create and deploy smart contracts to build decentralized applications (dApps) [35, 13]. Both Ethereum users and smart contracts have their own unique Ethereum addresses, which allow them to send and receive Ether (the native cryptocurrency of Ethereum) and interact with smart contracts to carry out complex transactions for a variety of purposes. Ethereum has grown into a thriving digital economy ecosystem, with a total market capitalization exceeding $200 billion at the time of writing and more than one million transactions processed daily, amounting to over $4 billion in volume[4, 2]. Smart contracts are central to Ethereum's success,"}, {"title": "2.2 Ethereum Request for Comment (ERC)", "content": "ERCs are technical specifications that define the requirements for implementing smart contracts. Those requirements aim to ensure compatibility across different contracts, applications, and platforms. By standardizing the contract implementations, ERCs help strengthen and promote the growth of the Ethereum ecosystem [16, 15, 32].\nTypically, an ERC begins with a brief explanation of its motivation. For instance, ERC20 [33] aims to establish a standard token interface that can be used by applications such as wallets and decentralized exchanges. After the motivation, an ERC specifies all the necessary public functions and events by outlining their parameters, return values, and any optional attributes for the parameters. It also provides implementation requirements in the form of plain text or code comments for each function or event declaration. For example, besides the requirements for the function API and return value generation, ERC20 includes the following rules for the transferFrom() function (as shown in Figure 1), which mandate emitting a Transfer event, verifying that the message sender has been approved to manage the token owner's tokens (and throwing an exception if not), treating the transfer of zero tokens in the same way as any other amount, and requiring an event to be emitted even when transferring zero tokens."}, {"title": "2.3 ERC Rule Violations", "content": "An ERC rule violation occurs when a smart contract is expected to follow a specific rule, but certain aspects of the contract do not. Figure 1 illustrates an instance of an ERC20 rule violation in a real smart contract, where the transferFrom() function fails to check whether the caller has the necessary authorization to transfer the specified amount of tokens. This verification is required by"}, {"title": "2.4 Today's Auditing Practices", "content": "The common practice for detecting ERC rule violations today relies on manual auditing, often provided by paid services [6, 28, 25, 3, 20, 1, 8]. One such service is the Ethereum Commonwealth Security Department [8], where users submit smart contracts for auditing by filing a GitHub issue. The service then manually audits the submitted contracts and provides feedback through the issue. To reduce the manual workload and associated costs, some automated tools have been developed using static program analysis. For example, Slither offers specific checkers (i.e., slither-check-erc [9]) that verify whether a given contract complies with the corresponding ERC standards for 11 ERCs. However, these tools have limited functionality. They primarily focus on ensuring the presence of required functions and events, confirming that these elements are correctly declared, and verifying that functions trigger the necessary events. Unfortunately, they are unable to check more advanced conditions, such as verifying whether the message caller has enough privilege to transfer tokens. To our knowledge, no machine learning-based automated auditing techniques have been proposed yet. We hope that the release of SC-Bench can foster a new line of research in this regard."}, {"title": "3 SC-Bench", "content": "This section outlines the process of building the dataset and provides some relevant statistics."}, {"title": "3.1 Construction", "content": "We collect real-world smart contracts from Ethereum Commonwealth Security Department [8], etherscan.io [18], and polygonscan.com [26]. As the purpose of SC-Bench is for evaluating automated ERC-rule checking, we include ERC rule violations of these collected contracts in SC-Bench using two approaches. First, we manually inspect a set of real smart contracts and identify all their ERC violations. This process is time-consuming, resulting in a limited number of violations. To address this, we perform automated error injection into a large number of real smart contracts, significantly increasing the number of violations. These two types of violations serve to validate each other and help ensure that the evaluated techniques demonstrate consistent performance."}, {"title": "3.2 Dataset Summary", "content": "Our dataset contains 15,975 ERC rule violations. Among them, 139 are introduced by the real-world programmers, while the remaining 15836 are injected by us. 39 errors made by real-world programmers cannot be replicated using the error-injection methods. These include 28 errors that violate the rule that transferring zero tokens must be treated the same as transferring non-zero tokens and 11 errors that violate the rule that the transfer() function must throw an exception if the sender does not have enough balance.\nThere are 5,377 contracts in our dataset, including 30 originally ERC-violating contracts and 5,347 contracts with injected errors. On average, each contract contains 476.29 lines of code and 2.97 errors. Among these contracts, 5,241 contracts implement ERC20, 110 contracts implement ERC721, and 26 contracts implement ERC1155. ERC20 has the most contracts since it is the most popular ERC.\nFigures 2 and Figure 3 compare errors from two sources on their contract sizes and the average number of errors per contract On average, an originally violating contract contains 260.9 lines of source code, with a standard deviation of 198, whereas a contract with injected errors contains 477.5 lines of code, with a standard deviation of 385. Each originally violating contract contains 4.7 errors on average, and each modified contract contains 2.9 injected errors on average.\nThe security impact of the errors is tied to the methods used for error injection. A total of 3,645 errors arise from failures to emit events, resulting in contract activities going unlogged. These are considered to have a low-security impact. Among the 2230 high-security impact errors, a significant number are caused by missing required checks to verify sufficient privileges for performing specific actions. The lack of these checks can be easily exploited, leading to financial losses (e.g., stolen tokens, as illustrated in Figure 1). These errors are categorized as high-security impact. Additionally, failures to call required functions or incorrect updates to state variables can also have a high-security impact. The remaining 9961 errors cause contracts to behave unpredictably for users, although they may not directly result in financial loss, and are classified as having a medium-security impact. All these errors are injected through API, Value, Call, and Return methods.\nFigure 4 shows the distribution of errors across different security impacts for the two sources. For errors identified through manual inspection, the proportions of high-impact, medium-impact, and low-impact errors are 14.8%, 43.0%, and 42.2%, respectively. For injected errors, the proportions are 51.3%, 24.5%, and 24.2%, respectively."}, {"title": "4 Evaluation", "content": "This section presents our evaluation results of SC-Bench using GPT-4. Our experiments are designed to answer the following research questions: 1) Coverage: How many errors can GPT-4 detect? and 2) Accuracy: How accurate are GPT-4's detection results?"}, {"title": "4.1 Methodology", "content": "We evaluate SC-Bench using GPT-4 via the OpenAI API access. We set its temperature value to zero to ensure that GPT-4's results are deterministic, enabling others to replicate our findings. Below, we detail our evaluation methodology.\nFor each auditing request, we ask GPT whether a contract violates any ERC rules, and if so, which function causes the violation. To instruct GPT on ERC rules, we adopt in-context learning by providing GPT ERC rules in addition to the contract under inspection, as shown in Figure 5. We use two methodologies to provide ERC rules. The first presents an ERC's official document with all rules to GPT. After GPT returns its output, we compare both the GPT-generated violating rules and violating functions to the ground truth. We report when both are correct, when only the violating rule is correct, and when neither is correct.\nThe second method assumes oracle knowledge of which specific ERC rule(s) a contract violates and where the violation happens. So, it directly provides both to GPT and asks GPT a simple True-or-False question. A True result is correct (True Positive), while a False is wrong (False Negative). For this method, we manually select rules and violation code sites based on either the original contract violations or our injected errors.\nPresenting whole ERC rules with the contract serves as a baseline, while hand-picked rules can be viewed as an oracle. Note that we do not provide more advanced methodology such as Chain-of-Thoughts [39], as the focus of this work is to present our collected dataset and demonstrate its potential use. Future research can explore the room for accuracy improvements."}, {"title": "4.2 Experimental Results", "content": "Full-Rule Prompting. As shown in Table 3, using full-rule prompting, GPT-4 successfully detects 139 errors (0.9% of the 15,975 errors), providing both the correct violated rules and the violating functions. For another 7 errors (0.04%), GPT-4 only reports the correct rule but fails to identify the correct location. For the remaining 15,830 errors (99%), GPT-4 does not provide any correct information.\nWe further separate the results between errors identified through manual inspection and those that are injected. For manually inspected errors, GPT-4 correctly reports both the violated rule and the violating function in 29% of the 139 errors. For injected errors, this proportion is 0.6%. The difference is probably due to that the injected errors are more challenging to identify than those introduced by programmers.\nFor errors in different security impacts, we notice GPT-4 has a very high detection rate when pinpointing manually injected errors with a high security impact, with a detection rate to be 80%. For errors in other security impacts, the detection rate ranges from 4% to 20.8%.\nWe then examine whether GPT-4's ability to detect violations varies across different ERCs. We found that the detection rate for errors violating an ERC20 rule (0.9%) is higher than for ERC721 (0%) and ERC1155 (0%). Notably, GPT-4 does not identify any errors in contracts implementing ERC721 and ERC1155. This discrepancy is likely because there are significantly more ERC20 contracts, meaning GPT-4 has probably been trained on a larger dataset of ERC20 contracts.\nWhen using oracle prompting, GPT-4's performance significantly improves, with the detection rate increasing from 0.9% to 22.9%. The improvement varies depending on the source of the errors. For manually inspected errors, the detection rate rises from 29% to 42.7%, while for injected errors, it increases from 0.6% to 22.8%. This boost in detection is mainly due to breaking down a complex task into smaller, more manageable tasks, allowing GPT-4 to focus on each one individually.\nOur results show that simple prompting techniques even with oracle still have a large room for accuracy improvements. A major reason we suspect is the complexity of smart contracts and the original rule description, which prevents GPT-4 from properly detecting violations. Potential ways to improve accuracy include further narrowing down contract sizes (e.g., by slicing code) and improving the understandability of ERC rules (e.g., through specialized prompts)."}, {"title": "5 Discussion and Conclusion", "content": "We present SC-Bench, the first dataset for smart contract auditing. SC-Bench contains 5377 real-world smart contracts and 15975 ERC violations, with two sources of violations: real-world errors and injected errors. Our evaluation of SC-Bench with GPT-4 shows that while ML-based techniques are promising, there is still huge room for improvement.\nNotably, SC-Bench has certain limitations. For example, we only include the three most important ERC standards and only inject one to three errors per contract. Adding more ERC standards and supporting more types of error injection is feasible something we leave for future work. Another limitation is the imbalance of real-world violations and injected errors. As real-world violations are rarely reported, we believe the imbalance cannot be easily improved. One possible approach is to instruct ML models to create real-world-like errors."}]}