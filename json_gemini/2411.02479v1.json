{"title": "Digitizing Touch with an Artificial Multimodal Fingertip", "authors": ["Mike Lambeta", "Tingfan Wu", "Ali Seng\u00fcl", "Victoria Rose Most", "Nolan Black", "Kevin Sawyer", "Romeo Mercado", "Haozhi Qi", "Alexander Sohn", "Byron Taylor", "Norb Tydingco", "Gregg Kammerer", "Dave Stroud", "Jake Khatha", "Kurt Jenkins", "Kyle Most", "Neal Stein", "Ricardo Chavira", "Thomas Craven-Bartle", "Eric Sanchez", "Yitian Ding", "Jitendra Malik", "Roberto Calandra"], "abstract": "Touch is a crucial sensing modality that provides rich information about object properties and interactions with the physical environment. Humans and robots both benefit from using touch to perceive and interact with the surrounding environment (Johansson and Flanagan, 2009; Li et al., 2020; Calandra et al., 2017). However, no existing systems provide rich, multi-modal digital touch-sensing capabilities through a hemispherical compliant embodiment. Here, we describe several conceptual and technological innovations to improve the digitization of touch. These advances are embodied in an artificial finger-shaped sensor with advanced sensing capabilities. Significantly, this fingertip contains high-resolution sensors (\u22488.3 million taxels) that respond to omnidirectional touch, capture multi-modal signals, and use on-device artificial intelligence to process the data in real time. Evaluations show that the artificial fingertip can resolve spatial features as small as 7 um, sense normal and shear forces with a resolution of 1.01 mN and 1.27 mN, respectively, perceive vibrations up to 10 kHz, sense heat, and even sense odor. Furthermore, it embeds an on-device AI neural network accelerator that acts as a peripheral nervous system on a robot and mimics the reflex arc found in humans. These results demonstrate the possibility of digitizing touch with superhuman performance. The implications are profound, and we anticipate potential applications in robotics (industrial, medical, agricultural, and consumer-level), virtual reality and telepresence, prosthetics, and e-commerce. Toward digitizing touch at scale, we open-source a modular platform to facilitate future research on the nature of touch.", "sections": [{"title": "1 Introduction", "content": "While artificial intelligence (AI) has the ability to understand and manipulate language, is AI able to palpably distinguish between a rough surface and a smooth surface? A raw egg and a hard-boiled egg? A soft, pliant surface and a firm, soft or rigid surface? The proverbial full glass and a half-empty glass? In short, how can AI bridge the gap between the digital and physical worlds? Of all our senses, touch arguably is the most critical in how we interact with the world (Johansson and Flanagan, 2009) and how we explore the world (Lederman and Klatzky, 1987). It enables us to measure forces and recognize object properties - shape, weight, density, textures, friction, and elasticity. It also plays an important role both in social relationships (Dunbar, 2010) and in cognitive development (Ardiel and Rankin, 2010). Until now, no solutions have emerged for digitizing touch with the same rich sensorial spectrum that is inherent to the human experience (Klatzky and Lederman, 1992). Toward the advancement of robotic in-hand manipulation, we strive to mimic familiar features of the human hand: Fingers. We postulate that the next generation of touch digitization in robotics will enable intelligent systems to discern significantly higher levels of physical information during environmental interaction.\nDigitizing touch depends on two fundamental features: one temporal in nature and one spatial. Temporal features process information from signals of time variation, whereas spatial features process a discrete multidimensional array of geometrical signals. We combine these methods within a unified platform to significantly improve the capabilities of touch digitization: a modular, finger-shaped, multi-modal tactile"}, {"title": "2 Artificial Fingertip", "content": "To effectively capture the nuances in touch interactions with the world, an artificial fingertip must be sensitive in both temporal and spatial domains. These domains are obtained as modal signals encompassed within the artificial fingertip through visual, audio, vibration, pressure, heat, and gas sensing. Prior vision-based touch sensors using off-the-shelf imaging systems are bound by slow visual capture rates, which reduce the amount of sequential information resulting from frame encoding time and, therefore, limit the temporal nature of non-static touch interactions encountered during manipulation. Increasing the temporal frequency"}, {"title": "2.1 Elastomer Interface", "content": "When the reflective fingertip surface layer is subject to impression stimuli, the surface layer material properties directly relate to the spatial resolving capabilities. A design-of-experiment technique is developed to identify these six material parameters that affect sensor sensitivity to input stimuli: Rgel (gel fingertip radius), Tc (surface reflective coating layer thickness), Tg (surface layer thickness), h (height), Ec (coating Young's modulus), and Eg (fingertip volume Young's modulus). If Tc and Tg are too thick or exhibit low compliance, a low pass filter effect is evident on discrete object edges. Similarly, if the object is rich in spatial information and fractal dimension (Sahli et al., 2020), the fingertip surface will resolve fewer features due to local gradients from material compliance. We avoid specifying any constraints on the coating thickness layer to best capture small input stimuli while maintaining a suitable parameter range for the general size of the fingertip. Depositing and applying this layer onto the fingertip surface could involve manual hand painting, airbrushing, or dip-coating techniques. However, while these techniques produce a touch image, they are far from optimal and result in large coating thickness and inconsistent yield from manufacturing variance. We solve this by developing a new chemical deposition technique for growing a silver thin film directly onto the surface of the fingertip. This technique produces coating thicknesses far smaller than previous methods and thus achieves better sensitivity."}, {"title": "2.2 Optical System", "content": "Common vision-based sensors capture input stimuli at a planar surface, use multiple cameras that are difficult to integrate and process together, or default to common off-the-shelf cameras optimized for human-centered imaging, which results in downgraded optical performance in touch. We refrain from the use of standard image-sensor features such as automatic exposure control, automatic white balance, and automatic focus which are designed for responding to changes in the natural environment as our fingertip chamber is an enclosed and controlled environment. We find that a new approach is required to optimize the capture of the hemispherical surface when modeling an isotropic representation of similar dimensions to a human fingertip. In optimizing for input stimuli from the touch interaction layer, we stipulate that our imaging system should not limit the performance within the finite element method simulation of the material properties. Hence, we determine the optical system requirements to best suit capturing images related to tactile sensing with a CMOS pixel size of 1.1 um. Parameters were chosen for converging spot size to increase spatial resolution, intentionally allowing chromatic aberration, introducing shallow depth of field to allow for defocus proportional to object indentation depth, and removing anti-reflective coatings to enable capture and interpretation of reflections and scattering inside the fingertip. However, such parameters require a non-standard lens. Therefore, we developed a custom solid immersion hyper fisheye lens to tackle the unique environment of visuotactile sensing rather than an off-the-shelf lens catering to general-purpose imaging, thus enabling full control over lens geometry and optical parameters."}, {"title": "2.3 Illumination System", "content": "We describe two metric parameters for the illumination performance within the volume: background uniformity, which measures how evenly the light is distributed, and image-to-background uniformity contrast, which measures how well impressions on the fingertip's surface stand out compared to the background. A common approach is the embodiment of an internal structure, which serves as a hemispherical light pipe and provides fingertip rigidity. However, an internal light pipe structure produces illumination artifacts in the form of glint"}, {"title": "2.4 Multi-modal Sensing", "content": "Multi-modal information capture outperforms prior visuo-tactile techniques in sensitivity to spatial and force measurements, where rapid changes in dynamics or state occur, shown Figure 4B, 6B, 3A,C-E. While visual information provides insight into environmental and object contact, such as textures and surface deformations, this only provides a subset of fingertip-to-object-environment understanding. We further evolve the platform's capabilities to include sensitivity to non-vision-based modalities. For instance, when in contact with the environment, dynamic forces and signals are experienced, such as swiping the fingertip across a surface or the very moment a contact transient or slip occurs. We capture this information through in-fingertip audio microphones and pressure MEMS-based sensors and show the ability to determine the level of liquid inside an opaque bottle (see Figure 3A) and understand the nuances of surface texture between different objects at a much higher frequency (upwards to 10 kHz) than visual capture (240Hz) (see Figure 3B). We further include modalities to understand object state, which is not necessarily a function of contact but can provide priors in understanding touch through heat and smell. Such priors can estimate if an object may be slippery due to the presence of water, soap, or butter and if an action or object may present a danger to human contact due to its temperature."}, {"title": "2.5 On-device Al Processing", "content": "Inspired by the human reflex arc, where quick reactions to input stimuli on the fingertip benefit from the central nervous system instead of a round trip to the brain, we design a similar local processing response on the artificial fingertip. Specifically, we include within the form factor of the fingertip a neural network accelerator to process the sensory reading and allow for direct control, providing actions to a robotic end effector for controlling the phalanges of a robot finger. While this is a new era of on-device fingertip processing, we study two main effects contributing to faster response: latency and jitter. Latency results from the average time required to process a signal of interest, and jitter is the variation in mean time based on system overhead, which may occur due to host processing or bandwidth constraints. Compared to typical methods of using an artificial fingertip with an external host, we achieve a 2x reduction in latency and jitter towards performing an action through local on-device processing."}, {"title": "3 Evaluation", "content": "We evaluate the artificial fingertip, Digit 360, in performance with respect to spatial resolution, shear and normal forces, illumination, and multi-modal ablations. More details and additional experiments can be found in the methods.\nWe first model the fingertip surface as a two-layer stack formed by an external diffusive material adhered to an internal reflective thin film, which is grown onto the non-rigid solid silicone body of the fingertip. We then explore the effects of the non-rigid solid silicone surface mechanical properties, texture, and the degree of controlled light scattering to find an optimal performance metric between background uniformity and image contrast. We show that increasing controlled surface texture scatter from 1-degree scatter to Lambertian scatter results in an increase in background illumination uniformity, thereby increasing image impression contrast. However, with low degrees of scattering, intense hotspot artifacts dominate the background, whereas when the degree of scattering approaches Lambertian scattering, these artifacts decrease along with a decrease in image contrast, which directly results in reduced sensitivity to impression stimuli. With little or no scattering with a polished surface, it is evident that minimal background illumination is present, which motivates the production of shadows created by indentations against the fingertip surface. Furthermore, glint reflections off of produced indentations are minimal to non-existent and do not produce a consistent appearance across the surface. On the contrary, with the traditional method of visual-tactile sensors using Lambertian scattering surfaces, we show that the hemispherical sensing surface acts as an integrating sphere, where shadows cast by direct illumination striking the indentations are wiped out by scattered illumination from other areas and even while imaging may occur on far off-axis angles, their contrast is low. We introduce a controlled degree of scattering in which an optimized uniform background illumination is achieved that lends itself well to contrast between indentations and the surrounding surface; furthermore, all indentations are imaged (see Figure 2).\nWe evaluate the normal force sensitivity and collect tuples of normal forces applied by a micro-indenter and corresponding outputs from the sensors, and then train a deep learning model from this dataset. The trained model (see Figure 2A) can predict the normal forces applied with a median error of 1.01 mN (Region 1). Similarly, to measure the shear force sensitivity, we collect tuples of shear forces applied by a micro-indenter and corresponding outputs from the sensors and then train a deep learning model from this dataset. The model (see Figure 2A) is capable of predicting shear forces applied with a median error of 1.27 mN (Region 1). In contrast to previous sensors that required the presence of explicit markers, this result demonstrates that with a sufficiently high optical resolution, it is possible to directly use the internal texture of the elastomer to measure shear forces.\nTo carry out spatial resolution evaluations, we define the spatial resolution of an artificial fingertip sensor as the minimum feature size that can be resolved with a modulation transfer function (MTF) \u2265 0.5; this is determined by how well the contrast is preserved and quantified by line pairs per millimeter. We first simulate the imaging system from the design, which yields that on-axis contacts are resolvable for features of size \u22656 um for region 1, \u22658 um for region two resolves, and \u226522 um for region 3. We then validate these results by collecting data with a two-pronged micro-indenter depressed onto the fingertip, varying the distance between the two prongs, and observing the taxel intensity line profile; both the visual validation and the inspection of the taxel profile intensity confirmed it is possible to clearly distinguish features as small as \u22657 um for region 1 (see Figure 2C).\nMulti-modal information such as vibrations upwards of 10 kHz, auditory clues, sensitivity to heat, and smells play an essential role in human touch (Johansson and Vallbo, 1979). However, typical vision-based tactile sensors do not contain a broad range of multi-modal capabilities to capture this information or operate lower sensing frequencies, such as 60 Hz. Even with Digit 360's fast camera, which operates at 240 Hz, highly dynamic movements may not be fully captured.\nMulti-modal touch digitization can be characterized by two studies: a) a single modality based characterization and, b) a holistic characterization of all the modalities collectively.\nSingle modality based characterization: We first look at an approach where each modality is characterized by its own respective performance. We evaluate capturing vibrations up to 10 kHz, which can distinguish between different materials with light swiping motions of the finger. Furthermore, we show that these multi-modal"}, {"title": "4 Discussion", "content": "We show design principles that advance the state of artificial fingertip sensing toward digitizing fingertip interactions between the environment and objects. We design an artificial fingertip that is more sensitive in spatial and force sensitivity compared to similar methods with the additional benefit of multi-modal sensing features and a local processing ability. Our results demonstrate the digitization of touch with capabilities that outperform a human fingertip. We believe the richness of touch digitized by our modular research platform Digit 360 opens new promising venues for studying the nature of touch in humans and investigating key questions around the digitization and processing of touch as a sensor modality (Hayward, 2011). Moreover, this technology opens the doors to broader adoption of touch sensors beyond traditional niche research fields: In robotics, to improve sensing and manipulation capabilities with benefits for applications"}, {"title": "5 Methods", "content": "Platform architecture We developed this platform specifically for advancing the state of the art in tactile sensing research. As well, it is a platform based on modular principles to provide an omnidirectional vision-based, multi-modal sensing system, and on-device AI capabilities. We achieved a modular design by isolating each part of the system into a small surface area electronic assembly, as shown in Figure 2. By facilitating the reuse of each sub-system, we present a novel solution to advance tactile sensing research by reducing the complexity and the design and manufacturing iteration cycles. This enables researchers to modify the overall system by adding, removing, and changing sub-systems rather than having to design a stack of new hardware to support changes to system design. Furthermore, this modular system architecture allows for selecting combinations of sub-systems to facilitate the introduction of newer technologies. Feature removal reduces costs when adapting the system to new mechanical form factors. Also, the possibility of replacing the sensing fingertip allows adaptability for different environments and tasks - for example, by using different sensitivities and stiffness in the fingertip material or by using fingertips with markers to introduce more prominent optical-flow features. Based on simulation to achieve optimal spatial and force resolution, we were able to introduce a novel process for the design and manufacturing of the sensing fingertip.\nFingertip sensitivity simulation to input stimuli A 3D Finite Element Method (FEM) model using Comsol Multiphysics was developed for analyzing and characterizing the fingertip material stack-up. The goal was to identify the sensitivity and resolution of the sensor. First, a FEM model was developed to identify the key parameters that presented the most significant change in sensitivity and resolution. Since the fingertip is isotropic and revolves around the origin, only a quarter of the sensor was modeled for faster computation using a multi-layer-based model. The multi-layer model comprises the base gel, polymer, and coating layers.\nWe used a Hysitron TI 980 TriboIndenter for nano-mechanical characterization of the fingertip polymer Young's modulus, E. This system has in-situ high-resolution imaging, dynamic nanoindentation, and a high-precision motion stage with high-resolution force-sensing tips. For characterization, a 30 \u00b5N force was applied with a 10 um probe tip. The corresponding force-displacement curve was measured, yielding Young's modulus of E = 2.86 MPa. Using the experimental E value, the FEM models were updated to correct the simulations. Furthermore, the same force value applied and maximum displacement, Dmax, was measured, resulting in the verification of simulation, Dmax = 2.1 um and experimental Dmax = 2.2 um measurement results, with error <5%. Additionally, multiple measurements were taken across varying samples of the fingertip. An average value for E was measured at E = 2.6\u00b10.74 MPa. Both Emean and Estd were used in detailed analysis for the total E range in the FEM models.\nWe further employed design-of-experiments techniques used to identify critical parameters affecting sensor"}, {"title": "Multimodal action and material classification analysis", "content": "Since each modality is represented as a spatial, temporal, or a combination of the two, we preprocess the dataset items based on their respective modality. In the case of visuotactile inputs, represented as a 3-channel RGB image, we center crop and downsample the hyperfisheye such that the result is a slightly underfilled 160x160 pixel image. For the surface audio signals, we take each window sample and compute the MEL-spectrogram with the following parameters: nfft = 2048 (number of FFT bins), and noverlap = 1024 (overlap stride between each sample), and then scale the resulting RGB spectrogram to 64x64 pixels. The inertial measurement signals are concatenated for each axis and each window sample. For the surface pressure modality, we preprocess the raw signal with two series filters, first a high pass filter with fc = 0.95 Hz (frequency cutoff), and then a low pass of fc = 50 Hz to extract saliant features particular to the perturbations the fingertips are subjected to during dataset collection, for example, sensitive to rapid fingertip adjustments, but filtering out static forces experienced by the grasp of each finger on the object. The other modalities are treated as is without any pre-processing. We construct a model comprising multiple multimodal inputs for RGB and temporal encoders. For the RGB encoder, we use a modified, non-pre-trained ResNet-18 model with the following modifications: batch norm layers changed to group norm layers for training stability, and for the surface audio input, we change the fully connected layer to identity to allow for propagation from and to other blocks. We construct a sequential two-hidden layer deep network for the temporal layers. For training all the modalities, we first encode each modality, and the output of the modality encoder is concatenated to a final MLP, which provides a multi-head output in classifying actions and materials. Finally, we deploy model training with a maximum of 200 epochs with a grid search optimization of hyperparameters for the Adam optimizer learning rate. We present the results in 4, and show the confusion matrix for both finger dependent 24 and finger independent 25 ablations."}, {"title": "Dataset collection for normal and shear-force analysis", "content": "We designed a controllable robot indenter capable of applying a high-precision measured 3-axis force to any spatial position of the sensor. As shown in Figure 18, a tactile sensor is mounted on the robot arm (Mecademic Robotics Meca500) to orient the desired test surface down against a probe with a precision of 5 \u00b5m. The probe with a hemisphere tip of 4 mm diameter is mounted on a force sensor (FUTEK QMA147 3-axis or ME-Systems KD34s single axis) measuring the ground-truth contact force with 1 mN accuracy. The probe and force sensor assembly are then mounted on a hexapod"}, {"title": "Image-to-force regression model analysis", "content": "Contact-force prediction on vision-based tactile sensors such as our system can be achieved using an image-to-force regression model. The model needs to be calibrated from reference data. Once calibrated, evaluating the sensor model as a system for force-sensing performance on a testing dataset is possible. In this section, we share how we collected the dataset training and evaluating the model to benchmark normal and shear-force-sensing performance. We used a modified ResNet50 (He et al., 2016) deep neural network for the image-to-force regression model. The network was initially designed to take an input image of 224\u00d7224\u00d73 and output 1024-way object-classification probabilities. We replaced the classification head with a scalar-output linear layer predicting the force. We used mean-square error as our loss, then optimized with Adam with an initial learning rate search. The raw images from the sensors are 640\u00d7480, which were down-scaled to 224\u00d7224 with 20-pixel spatial jitter to improve spatial invariance. We pooled training data from all three regions to train a single model and obtain the prediction performance (median error) breakdown by regions as described in Figure 2A."}, {"title": "Force resolution factors and effects on shear force detection", "content": "Table 4 and Figure 21 show additional normal-force resolution performance with two kinds of gel surface finish: specular and Lambertian. We consistently saw that Lambertian surface scattering, typically considered preferable for vision-based tactile sensors, is, in fact, outperformed by its specular counterpart. This may come from the enhancement of surface texture contrast due to specular reflection, which helps the imaging system track gel deformation. Figure 19 shows the center-crop of our system's image, where the texture contrast is more evident in the region with strong specular reflections from the LEDs. We obtained clear optical flow (shown in the figure by arrows) within these texture-rich regions, corresponding to fingertip deformation caused by shear and normal force applied"}, {"title": "Internal surface scattering characterization", "content": "Our system's fingertip gel comprises three components, as seen in Figure 8. The outer surface of the base gel has a reflective silver thin-film coating, which is coated with a protective colored diffusive material. To produce an image, the two layers provide scattering of internally incident light from surface interactions to the vision system. We placed the illuminating light-emitting diodes in optical contact with the gel, using an over-molding process. The fingertip gels are initially manufactured with a smooth and polished surface, and through mold texturing, we can control and determine how light is scattered at the interface.\nUsing a Gaussian scatter distribution, we modeled a range of scatter. Our scattering parameter, \u03c3, was chosen to achieve the half-width-half-max angles, a, of the bi-directional scatter distribution (BSDF) function at normal incidence from a = 1\u00b0 to 25\u00b0, along with a Lambertian scattering model. In Figure 12, we show that a surface texture that minimizes scattering produces little background illumination. Neither does it produce large shadows created by surface indentations. Additionally, minimizing scatter produces specular glint reflections, fails to illuminate all indentations equally, and saturates the vision system.\nWith a fully Lambertian scattering model, the hemispherical surface of the gel fingertip acts as an integrating sphere. While Lambertian scattering provides uniform background illumination, the high scattering illumina-tion from nearby interactions reduces the overall indentation contrast. We optimized for high image contrast while maintaining uniform background illumination, better image impressions produced by gel indentations, and minimizing the number of glints that would saturate the image sensor.\nTwo non-uniformity metrics were evaluated over the fingertip hemispherical surface, Std/Mean and (Max - Min)/Mean. We demonstrated that low scatter yields images with a significant variation in the image signal, requiring the camera to handle a high dynamic range. If the image is allowed to saturate, the visual signal resolution of region variations due to the indentations will be lost. Thus, in these cases, the stray light is more likely to cause objectionable artifacts. The contrast in the image caused by spherical indentations is in certain areas high, due to bright glint reflections; but regions with large gradients in the background may make the indentations hard to detect. High scatter gives images with low variation in image signal, and no areas are lost due to saturation. In the image caused by spherical indentations, the contrast is low in certain areas, but the uniform background makes these easier to detect."}, {"title": "Edge Al at the fingertip", "content": "Our system provides communications with the host device over the USB 3.0 standard interface. Three separate streams are provided for data transfer, supporting video, audio, and multi-modal data. These streams collectively output at a maximum rate of 148 MBps and below, depending on the configuration sent to the device. Currently, tactile sensors are used in open-loop control, providing information to the host device for processing and additional actions to manipulators. We propose adding edge AI at the fingertip for three reasons: First, to create a latent representation of the data and reduce the overall bandwidth sent to the host device. Second, to enable fast local decisions for transmitting actions to manipulators. Third, we aim to improve the overall latency of the system while reducing variance in jitter, which is the change in latency. We modeled the tactile fingertip system with a manipulator, where both systems were connected to the host device in a star configuration. This resulted in decisions and actions resulting from tactile"}, {"title": "Timing characterization of Reflex Arc with Edge Al", "content": "Following high-level abstractions of the human reflex arc (Dewey, 1896), we developed a fast reflex-like control loop using Edge AI for local processing. The current paradigm of transferring the sensory input to a central control computer for processing and then sending back the control signals requires high bandwidth while introducing communication latency. In contrast, our paradigm is to process the sensory input inside the fingertip locally using Edge AI. This allows drastic reductions of the required bandwidth while significantly minimizing communication latency and - most importantly- minimizing jitter. We performed an experimental comparison of these two paradigms, shown in Table 3, by measuring the end-to-end latency of the systems using a PCI-e-based precision time measurement tool. We evaluated this experiment on a Linux machine with an Intel i9-11900K, 64GB memory, and an NVIDIA Quadro RTX5000 GPU. First, to ensure granularity in the measurements, each section of the system was isolated, and samples were collected in repeated trials. Second, we verified these results by subjecting the entire system to repeated measurements and comparing the timing results to the sum of the isolated components. This determined the areas that produce deterministic timing results, as well as highlighting the areas that sustained increased latency and jitter. Furthermore, these results indicated areas of performance improvements and design for future tactile sensors. The results for the entire control loop show how the Edge AI paradigm results in a reduction of latency from 4 ms to 1 ms with a desirable smaller variance. It is also noteworthy that, in principle, appropriate Edge-AI processing could be extended further to exploit the sequential nature of the camera FIFO memory to parallelize the data capture with the processing, thus yielding even lower latency. In this case, instead of processing the entire image, select horizontal lines would be sent for processing in the configured region of interest. This is applicable when touch interactions are most"}]}