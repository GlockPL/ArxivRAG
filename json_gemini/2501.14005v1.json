{"title": "Device-aware Optical Adversarial Attack for a Portable Projector-camera System", "authors": ["Ning Jiang", "Yanhong Liu", "Dingheng Zeng", "Yue Feng", "Weihong Deng", "Ying Li"], "abstract": "Deep-learning-based face recognition (FR) systems are susceptible to adversarial examples in both digital and physical domains. Physical attacks present a greater threat to deployed systems as adversaries can easily access the input channel, allowing them to provide malicious inputs to impersonate a victim. This paper addresses the limitations of existing projector-camera-based adversarial light attacks in practical FR setups. By incorporating device-aware adaptations into the digital attack algorithm, such as resolution-aware and color-aware adjustments, we mitigate the degradation from digital to physical domains. Experimental validation showcases the efficacy of our proposed algorithm against real and spoof adversaries, achieving high physical similarity scores in FR models and state-of-the-art commercial systems. On average, there is only a 14% reduction in scores from digital to physical attacks, with high attack success rate in both white- and black-box scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep-learning systems have been shown to be vulnerable to adversarial examples [1], [2], which mislead the models by adding manipulated patterns to the inputs. Face recognition (FR) systems, based on deep neural networks (DNNs), also face the threat of adversarial examples. The security issues are especially important for these systems since they are usually deployed on critical applications such as payment, phone unlocking, access control etc.\nMost work on adversarial examples have focused on digital attacks [3]-[8], where direct accessibility to the input of DNNs is assumed. However, this assumption does not hold for a real FR system, where a photo is taken for a user and sent for inference by a backend model. Efforts have also been made to extend the notion of adversarial attack to physical domain, where the object or physical environment is manipulated so that the captured images can mislead the DNNs [9]\u2013[11]. For example, 2D adversarial patches [12], [13] are attached to physical faces to attack FR systems. A makeup generation method was used to synthesize eye shadow over the orbital region on faces [14]. However, these methods require careful calibration and fabrication of the physical artifacts [15].\nAnother flow of work modulates the light for illuminating objects [16], [17]. While providing enhanced flexibility, these techniques require careful programming of light-emitting devices, which are sensitive to changes in scene lighting and can only achieve non-targeted attacks. Recently, an adversarial light attack was introduced using a projector-camera setup [18], [19], where adversarial patterns are digitally created and projected onto specific objects. One study [18] suggests a global correction method to handle color shifts from projector to camera. However, this approach has limitations as it overlooks spatial variations in optical systems, limiting its applicability. Another study [19] incorporates the projector-camera model into optimizing adversarial attacks by detailed modeling of instruments and environmental factors. Nonetheless, this method requires precise calibration and accurate pixel mapping during modeling.\nIn this paper, we consider a practical setup for adversarial light attacks on FR systems, utilizing a portable projector for easy deployment. We assume that consumer-grade cameras are employed by the FR systems, introducing unique challenges when applying the methodologies outlined in previous works [18], [19] within this context. The accurate color calibration of the projector-camera system proves unattainable due to the low quality of the projected image.\nTo address the challenges in a portable project-camera setup for attacking an FR system, we introduce a series of device-aware adaptations within the adversarial attack algorithm to mitigate losses incurred during the transition from the digital to physical domain. It is important to emphasize that our adaptation scheme seamlessly integrates with any digital attack algorithm. Our main contributions are summarized as follows:\nResolution-aware adaptation. Acknowledging the discrepancy between the actual physical resolution of the projector and that of the digital screen, we propose a patch-based similarity loss mechanism to enforce pixel values within a patch to exhibit similarity, aiming to minimize the loss of adversarial patterns post-projection.\nColor-aware adaptation. Initially, we derive a coarse estimation of the projector's color mapping function, following the methodology outlined in [19]. Subsequently, we dynamically adjust the valid projector color interval, simulate moir\u00e9 noise, and operate in grayscale during attack optimization to mitigate color loss post-projection.\nExperimental validation. We conducted extensive experiments to validate the proposed algorithm, involving real and spoof adversaries made of various materials. Our devised scheme consistently achieves superior physical cosine similarity scores across most scenarios. On average, there is only a 14% decrease in scores from digital to physical attacks."}, {"title": "II. THE PROPOSED METHOD", "content": "As shown in Fig. 1, our work is based on a portable projector-camera setup for attacking a face recognition system. An adversary, projected with digitally generated adversarial mask on its face, aims to prompt the FR application to recognize it as a target identity. We consider this type of impersonation attack, as it presents a more practical threat in real scenarios. As previously mentioned, several challenges exist in this setup that need to be addressed.\nCalibration from digital image to physical face. Achieving precise alignment of pixel positions is essential when projecting the adversarial digital mask onto the adversary's face to maintain adversarial effectiveness. This task is not trivial due to the different positions of the projector and camera, as well as the 2D to 3D transformation involved.\nResolution disparity. Typically, the physical resolution of a portable projector is lower than that of the input signal, resulting in compression of the digital pattern upon projection.\nColor degradation. In the projector-camera configuration, color transformation occurs at both the projector and the camera. Regarding the projector, the nonlinearity of the projector will alter the intensity of the input signal. Following the work in [19], [20], we aim to approximate the radiometric response function of the projector, which determines the projected value for each pixel of the input signal per color channel. However, as shown in Fig. 2, the mapping is significantly overlapped due to pronounced moir\u00e9 noise and the utilization of low-quality optics.\nRegarding the camera, we use a mobile phone camera as a substitute for the one utilized in the FR system. As shown in Fig. 3, significant moir\u00e9 noise can be observed in the captured image, resulting from disparities in the optical characteristics of the projector, the camera's shutter speed and the refresh rate of the projected image."}, {"title": "B. Device-aware Adversarial Pattern Generation", "content": "In this section, we propose a device-aware optical adversarial attack algorithm, as shown in Fig. 1. Instead of utilizing an average representative image generated by applying various transformations to the adversary's face image, as done in [18], we choose to capture a brief video of the adversary with slight camera movements and select k photos from different viewpoints. The faces from the photos are then cropped and aligned. A mask image is manually crafted to constrain the region in the aligned image where the adversarial pattern can be positioned. Throughout the attack, we generate a universal adversarial mask for these multi-view images to account for potential positional discrepancies between the digital and physical faces. In this way, we avoid the intricate calibration of the projector-camera setup and enable a high degree of flexibility in the physical attack procedure.\nLet $x_i$ denote the i-th captured face of the adversary for $1 \\leq i \\leq k$, y denote the face of the target identity, f(x) denote an FR model that extracts a normalized feature representation for the input x, and M denote the binary mask. A universal adversarial mask $x_{adv}$ is generated such that each adversary's face projected with the mask is denoted by $x'_{i}=x_{i}\\odot(1-M)+x_{adv}\\odot M$, where $x_{adv} \\odot (1 - M) = 0$ and $\\odot$ represents the element-wise product. Our goal is to optimize the following:\n$\\underset{x_{adv}}{\\operatorname{argmin}}\\ \\mathcal{L}(x_{adv}, x_i, y), \\ \\text{s.t.}\\ || (x_{adv} - x_i) \\odot M||_1 \\leq \\epsilon$\nfor all $1 \\leq i \\leq k$ and the pixel values in the masked region of $x'_{adv}$ are restricted within the $l_1$-bound of $x_i$. Here, $\\epsilon$ is the perturbation bound. Essentially, $\\mathcal{L}$ can be the sum of multiple cosine similarity functions, each given by $\\mathcal{L}_{loss} = 1 - \\cos(f(x'), f(y))$. $x'$ and $y$ are considered the same identity when $\\mathcal{L}_{loss}$ is above a certain threshold. Alg. 1 outlines our proposed adaptation built upon a baseline algorithm framework.\nBaseline. We consider a representative black-box attack algorithm as the baseline. It incorporates techniques to enhance attack transferability, which include integrating the momentum into the iteration (step 13) and utilizing input transformations such as translation-invariant (TIM, step 12), diverse input method (DIM, step 9, applying transformation $T_i$) and scale-invariant methods (SIM, block of step 5, where $S_j(x)$ denotes the image obtained after scaling each pixel value of x by $\\frac{1}{2^j}$). Additionally, a brightness term $\\gamma$ is sampled from a uniform distribution during each iteration to account for slight environmental illumination variance. We extend the loss of the baseline algorithm with a smoothness loss $\\mathcal{L}_{smooth}$ [21], which aims to enhance adversarial robustness by reducing the variance between adjacent pixels.\nResolution-aware. Taking into account the resolution disparity between the physical resolution of the projector and the digital input, we introduce a patch-based loss constraint to ensure that pixel values within a patch are as similar as possible, thereby minimizing information loss after projection. We divide the adversarial mask $x_{adv}$ into P patches, each of size H \u00d7 W, where the values of H and W are computed to ensure that one patch covers at least one pixel after"}, {"title": "Color-aware.", "content": "To mitigate color degradation in the portable projector-camera setup, we initially operate the input adversarial mask in grayscale (c2g) to minimize the impact of color conversion (step 2). Next, we simulate potential moir\u00e9 patterns during the attack and merge them with the adversarial mask at random locations (step 7). Subsequently, we aim to optimize the attack to operate within a linear relationship interval of the projector color mapping, enhancing alignment between the digital and physical attacks. Based on the rough initial estimation per color channel, we dynamically adjust the linear interval range during the attack to accommodate any inaccuracies in the initial estimation. Taking channel B as an example, if the initially coarsely estimated linear interval for projector color mapping, as depicted in Fig. 2, is denoted by [$b_L, b_h$], we work as follows.\nAt iteration step t, the computed grayscale mask $x_{adv}^{t}$ is first transformed into an RGB color image $x_{adv-rgb}^{t}= g2c(x_{adv}^{t})$. For color channel B, a value v is randomly sampled from a uniform distribution [-\\tau, \\tau] and added to each of $b_L$ and $b_h$."}, {"title": "Experimental Setup", "content": "We experiment with spoof and real adversaries involving ten subjects in total to verify the effectiveness of our proposed device-aware optical adversarial attack method. Real persons are chosen as target identities and are excluded from the adversary set. Three representative FR models, including ArcFace [22], MobileFace [23] and FaceNet [24], are adopted for the evaluation. Each experiment is conducted in an office environment with fixed lighting, using a Huawei Nova9 phone camera and a Lenovo T6S portable projector.\nPreprocessing. A brief video is recorded for the adversary by positioning the camera in front of the face with a slight movement. Five photos are then chosen from the video to cover different viewpoints: top, middle, bottom, left, and right. A real person's image is randomly selected from the target identity set. Each of these photos undergoes face detection using the RetinaFace-based algorithm [25] and is aligned to the size of 896 \u00d7 896, which is chosen to closely match the resolution of the projector input (1920 \u00d7 1080). Note that a resize layer is integrated with the FR model to adjust the input image to the expected shape, while the attack is carried out on the source image of size 896 \u00d7 896. The captured photos of the projected face also undergo the same preprocessing steps.\nHyper-parameters. The number of iterations N and perturbation bound $e$ are set to 50 and 16 respectively. A brightness term $\\gamma$ is sampled from a uniform distribution of [-0.3, 0.3] during each iteration. Based on the physical parameters of the projector used, the patch size for the resolution-aware loss constraint is set to 8 \u00d7 8. The linear intervals for the RGB channels of the projector color mapping are selected as [63, 242], [25, 216], and [25, 204] respectively. The dynamic range per color channel is set to [-5, 5], and the transparency coefficient $\\theta$ is 0.4. The other hyper-parameters related to the baseline algorithm follow the usual settings [4]\u2013[7].\nEvaluation Metrics. The cosine similarity score between each adversarial image and the target face is calculated. In the digital attack scenario, the top score is recorded for the five adversarial faces generated for each adversary. In the physical attack setup, three photos are taken of the adversary's face with a projected mask, captured from different viewpoints around the projector. The top score among the three captured adversarial faces is noted. The difference in similarity scores between the digital and physical attacks is then computed to demonstrate the effectiveness of the proposed method in minimizing the reduction when transitioning from the digital to the physical domain."}, {"title": "B. Results on Spoof Adversaries", "content": "We experiment with five spoof adversaries with various materials, including one plastic head mold, one gypsum head mold, two resin masks, and one silicone mask. In each run, we select one of the three FR models as the substitute model to conduct a white-box attack, while the other two are used as black-box models. A universal adversarial mask is generated for all five photos of each adversary. In the digital attack, each adversary's face, masked with the adversarial pattern, is evaluated with the randomly selected target face. In the physical attack, the adversarial mask is projected onto the adversary's face and then captured by a camera from three viewpoints, which are used to evaluate with the target face.\nTABLE I shows the average cosine similarity score for all five spoof adversaries for each of the FR models. The upper part displays the results for gray-scale based attacks, while the lower part shows the results for color-scale based attacks. The bold and underlined entries indicate the better results between gray and color scale based attacks. It can be observed that in most cases, the generated adversarial masks in gray scale have higher similarity scores and lower reduction loss after projection."}, {"title": "C. Results on Real Adversaries", "content": "We also experiment with five real persons, following the same procedure. As shown in TABLE II, similar results can be observed as with the spoof adversaries that adversarial projections in grayscale outperform those in color. If the top score for a physical projection exceeds the threshold, the attack is deemed successful [18]. We found that the ASR for all the spoof and real physical adversaries achieves nearly 100%."}, {"title": "D. Ablation Experiments", "content": "We conduct ablation experiments with a white plastic head mold. With a randomly selected target face, we capture five photos of the mode and apply the attack algorithm on ArcFace to generate a universal adversarial mask for all source faces. Three photos are taken for the physical adversary with the projected adversarial mask. The top similarity score is recorded for both the digital and physical attacks. As shown in TABLE III, resolution-aware adaption yields the best for the digital attack, while color-aware adaption reduces the loss in color conversion and achieves the highest physical scores in the physical domain."}, {"title": "E. Commercial Systems", "content": "We randomly select ten captured faces projected with the adversarial mask generated from the ArcFace model, with one photo per adversary, to evaluate with three commercial FR systems: Face++ [27], Baidu [28] and Tencent [29]. The mean of the recorded confidence scores are around 70.2%, 80.7% and 45.8% for these three systems respectively."}, {"title": "IV. CONCLUSIONS", "content": "In this paper, we investigate optical adversarial attacks on FR systems using a portable projector-camera setup. To address the loss in adversarial effectiveness resulting from the projection and recapture of digitally generated patterns, we introduce device-aware adaptation during the attack optimization process. Specifically, we enforce similarity constraints on pixel values within patches to mitigate color compression caused by the disparity between physical and digital screen resolutions in portable projectors. Given the challenge of obtaining the precise radiometric response function of the projector, we project pixel values within a dynamically adjusted valid linear interval during each iteration. Our experiments demonstrate that the proposed adaptation algorithm in grayscale yields the most effective results for physical attacks."}]}