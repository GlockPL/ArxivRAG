{"title": "SQ-DM: Accelerating Diffusion Models with Aggressive Quantization and Temporal Sparsity", "authors": ["Zichen Fan", "Steve Dai", "Rangharajan Venkatesan", "Dennis Sylvester", "Brucek Khailany"], "abstract": "Diffusion models have gained significant popularity in image generation tasks. However, generating high-quality content remains notably slow because it requires running model inference over many time steps. To accelerate these models, we propose to aggressively quantize both weights and activations, while simultaneously promoting significant activation sparsity. We further observe that the stated sparsity pattern varies among different channels and evolves across time steps. To support this quantization and sparsity scheme, we present a novel diffusion model accelerator featuring a heterogeneous mixed-precision dense-sparse architecture, channel-last address mapping, and a time-step-aware sparsity detector for efficient handling of the sparsity pattern. Our 4-bit quantization technique demonstrates superior generation quality compared to existing 4-bit methods. Our custom accelerator achieves 6.91\u00d7 speed-up and 51.5% energy reduction compared to traditional dense accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "Diffusion models generate realistic contents by sequentially denoising data from random noise. They have emerged as the cornerstone of generative artificial intelligence (GenAI) for tasks such as image generation [1], video generation [2], and even weather prediction [3]. However, generating high- quality contents is notably slow because it requires repeatedly evaluating a large and complex neural network model over many time steps. Its complexity increases with higher reso- lution images and more advanced models, leading to large computation and memory overheads. Therefore, it is crucial to accelerate diffusion models to enable efficient deployment of real-world applications.\nQuantization and sparsity serve as the two pivotal tech- niques driving dramatic improvements in deep neural network (DNN) performance over the last decade. Quantization enables reducing the precision of the weights and activations of a DNN, typically from 32-bit floating-point to low bitwidth formats like 4-bit integers. Sparsity refers to the practice of pruning less important weights and activations, such as zeroing out the two smallest values for every four adjacent values in a tensor. In isolation or combination, quantization and sparsity can lead to substantial improvements in compute and memory efficiency of DNN execution with smaller model sizes, faster computations, and lower power consumption.\nWhile quantization and sparsity remain the first-order con- siderations when accelerating diffusion models, these tech- niques fail to work out-of-the-box due to the unique charac- teristics of these models. First, quantization error accumulates over time steps. Even small error in a single evaluation (time step) of the model becomes compounded over the many model evaluations required to generate a single sample (one image). Second, activation distributions in diffusion models vary across layers and time steps. This makes it difficult to apply a uniform quantization scheme over the entire generation process. Third, the data values within diffusion models tend to be extremely dense. The observed level of density precludes the effective use of sparse DNN accelerators for executing these models more efficiently.\nIn fact, we observe severe quality degradation when ap- plying existing 4-bit data formats to the diffusion model. As shown in Figure 1, existing 4-bit formats such as INT4 and INT4-VSQ result in obvious and unacceptable image quality degradation. In this paper, we propose novel software- hardware co-design techniques to accelerate diffusion models to achieve state-of-the-art generation quality at significantly reduced hardware costs. Our contributions are as follows:\n\u2022 We are the first to aggressively quantize both weights and activations of diffusion models to 4-bit while promoting significant activation sparsity in these models.\n\u2022 We propose a method to fully exploit the temporal nature of the sparsity during diffusion model sampling to maximize the efficiency of image generation.\n\u2022 We present a new mixed-precision dense-sparse accelerator featuring channel-last addressing and temporal sparsity detection to effectively handle the sparsity pattern.\nThe rest of the paper is structured as follows: Section II pro- vides preliminaries on diffusion models and discusses relevant work in quantizing and sparsifying these models. Section III presents optimizations we propose to improve diffusion model efficiency. Section IV details our accelerator implementation"}, {"title": "II. PRELIMINARIES", "content": "For the purpose of this work, we use Elucidated Diffusion Models, EDM [4] and EDM2 [5], as our baseline diffusion models. EDM is the strongest baseline available today in terms of training and sampling techniques because it is built by ex- tensively exploring the design space for training and sampling for this type of model. More importantly, it is considered the state-of-the-art for its generation speed and quality, and thus very challenging to accelerate while maintaining good quality.\nEDM serves as the backbone model for applications such as text-to-image generation [1] and weather prediction [3], and generalizes to the family of convolution-based diffusion models.\nFigure 2 illustrates the execution process and model ar- chitecture of EDM. Like most convolution-based diffusion models, EDM employs a U-Net based architecture with an encoder to capture features of the noisy input and a decoder to reconstruct the denoised output. As shown in Figure 2, EDM's architecture mainly consists of four types of layers: Skip, Conv+SiLU, Embedding, and Attention. The Skip block manages the skip connections from the encoder to the decoder, enabling information propagation from encoder to decoder and earlier layers to deeper ones. The Embedding linear layer transmits embedding information, including noise scheduling and labels (for conditional generation scenarios). The Attention block, present in specific layers (e.g., enc. 16x16_block_1 in the EDM1 model for CIFAR-10), implements an image self-attention mechanism to capture global context. Typically, dozens to hundreds of time steps (a.k.a. encoder-decoder model evaluations) must be executed to generate an image from random noise, as each time step returns a slightly denoised image from the previous time step.\nDNN quantization converts model weights and activations from high-precision floating-point down to low-precision data formats. Specifically, uniform symmetric quantization of a tensor X is expressed as $\\hat{X} = round(X/s_X)$ where $s_X = max(|x|)/q_{max}$. Here X is the original tensor, $\\hat{X}$ is the quantized tensor, and $s_x$ is the quantization scaling factor for X. $q_{max}$ represents the largest quantized value. The max operator can be taken at different granularity of X, such as over the entire tensor, across each channel, or for each vector [6]. With scaled quantization, data will be stored and computed in the quantized format. Computation results need to be rescaled using the scale factors {sx} back to the model's original dynamic range.\nDifferent techniques have been proposed to quantize dif- fusion model. PTQ4DM [7] and Q-diffusion [8] successfully quantize diffusion models to 8-bit. Several follow-up works [9]\u2013[12] further improve their generation quality. Recently, SVDquant demonstrates 4-bit quantization on diffusion models using activation smoothing and low-rank decomposition [13]. However, it requires high-precision (FP16) low-rank branches"}, {"title": "A. Quantization", "content": "DNN quantization converts model weights and activations from high-precision floating-point down to low-precision data formats. Specifically, uniform symmetric quantization of a tensor X is expressed as $X = round(X/s_X)$ where $s_X = max(|x|)/q_{max}$. Here X is the original tensor, X is the quantized tensor, and sx is the quantization scaling factor for X. qmax represents the largest quantized value. The max operator can be taken at different granularity of X, such as over the entire tensor, across each channel, or for each vector [6]. With scaled quantization, data will be stored and computed in the quantized format. Computation results need to be rescaled using the scale factors {s} back to the model's original dynamic range.\nDifferent techniques have been proposed to quantize dif- fusion model. PTQ4DM [7] and Q-diffusion [8] successfully quantize diffusion models to 8-bit. Several follow-up works [9]\u2013[12] further improve their generation quality. Recently, SVDquant demonstrates 4-bit quantization on diffusion models using activation smoothing and low-rank decomposition [13]. However, it requires high-precision (FP16) low-rank branches"}, {"title": "B. Sparsification", "content": "Sparsity in DNNs refers to the presence of zero-valued weights or activations, which can be exploited to reduce computational complexity and memory usage. In addition to quantization, we can prune less important weights and activa- tions to create a sparse model that requires fewer resources. In fact, current-generation Tensor Cores provide support for structured weight sparsity that exploits a 2:4 (50%) sparsity pattern that enables twice the math throughput of dense matrix multiplications [14].\nPrevious works such as [15] and [16] have demonstrated that implementing structured weight sparsity in diffusion models can achieve the promised nearly 50% reduction in computation. These works leverage sparsity-aware finetuning and report modest degradation in model quality. Orthogonal to these efforts, we focus on activations instead of weights and promote sparsity that is not inherently present in the original model. We are able induce an average activation sparsity of 65% that leads to approximately 52% reduction in computational cost. Activation sparsity can be combined with weight sparsity to enable additional efficiency."}, {"title": "III. DIFFUSION MODEL OPTIMIZATIONS", "content": "To understand the landscape of quantizing diffusion models, we evaluate existing quantization techniques for EDM across various datasets (CIFAR-10 [17], AFHQv2 [18], FFHQ [19] and ImageNet [20]). Following EDM and previous work, we use Fr\u00e9chet Inception Distance (FID) [21] to measure the quality of generated images: lower FID means better image quality. In our experiments, we generate 50,000 images in CIFAR-10, AFHQv2, FFHQ and 10,000 images in ImageNet to calculate the FID scores. Table I shows the results of the models in full-precision (FP32), half-precision (FP16), as well as variants of the most competitive and commonly accepted 8-bit and 4-bit formats. Here we use the same format for both weights and activations uniformly across the entire model.\nFrom Table I, it is evident that MXINT8 and INT4-VSQ, which employ fine-grained per-block scale factors, result in"}, {"title": "A. Mixed-precision Quantization", "content": "The sensitivity to quantization across different layers or computation types is especially notable in these diffusion models, based on our block-wise quantization sensitivity ex- periment in Figure 3. In this experiment, we keep one block in 4-bit precision while all other blocks are set to 8-bit. While we simply use MXINT8 for the 8-bit blocks, we specifically propose our own INT4 format with FP8 scale factors for the 4-bit blocks to improve dynamic range of the representation. The results in Figure 3 indicate that only the first and last few blocks are generally more sensitive to quantization. Con- sequently, it suffices to maintain these quantization-sensitive blocks at higher precision (MXINT8) while using 4-bit for the remaining blocks. The computation and memory cost of the high-precision blocks account for only about 5% of the total cost, allowing the system to retain significant benefits from the 4-bit precision.\nAdditionally, Figure 4 presents a breakdown of the com- putation and memory costs for the four types of blocks in the model. Notably, more than 90% of the total computation cost and 85% of the total memory cost are attributed to the Conv+SiLU block, highlighting its significance in the model's overall performance. Therefore, we emphasize quantizing the Conv+SiLU computation blocks to 4-bit, with other less im- portant blocks in 8-bit. The result of these optimizations are presented in the Ours (MP-only) row of Table II, demon- strating appreciable improvement from those of the baseline data format while attaining 73% and 72% average reduction in both computation and memory cost, respectively. Here we assume a computational equivalence of 1 FP16 multiplication to 2 INT8 multiplications to 4 INT4 multiplications based on computation resources and memory bandwidth [23]."}, {"title": "B. Hardware-efficient Activation Function", "content": "Although our mixed-precision quantization technique sig- nificantly improves the FID score, we identify additional opti- mization opportunities with the non-linear activation functions (SiLU) prevalent in these models. Figure 5 (left) shows the activation data distribution of one Conv + SiLU layer where $SiLU(x) = x/(1 + e^{-x})$ [24]. At the output of the SiLU non- linear function, the data distribution spans from [-0.278, \u221e). The existence of a small negative range necessitates the use of signed data formats (e.g., signed INT4) for activations. Figure 6 (top) illustrates the signed INT4 quantization levels needed when SiLU is used. When the input x is in the range [-1,1], the output of SiLU(x) lies within [-0.269, 0.731]. Based on the quantization formula in Section II-A, only 10 of the 16 levels in signed INT4 can be used, resulting in severe under-utilization of the available bitwidth.\nIn contrast, ReLU is a widely adopted and hardware- efficient non-linear function where the output data distribution ranges from [0,\u221e) [25]. With ReLU in place of SiLU, we can focus our representation on only the positive range and quantize the activations to unsigned INT4 (UINT4) data for- mat to achieve more precise representations. Figure 6 (bottom) depicts the quantization of ReLU activations when x is within [-1,1]. All the available quantization levels of the UINT4 format can be used. As a result, leveraging Conv+ReLU makes the model more quantization-friendly compared to using Conv+SiLU.\nTo adapt the SiLU-based model to use ReLU, we replace the non-linear activation function and then finetune the pre- trained SiLU-based model. The finetuning process takes less than 10% of the total pre-training time [4]. The resulting ReLU-based model achieves similar image quality to the original SiLU-based model. Figure 1 presents example images (targeting AFHQv2 and FFHQ datasets) generated by the SiLU-based models with existing quantization techniques (left three images) and ReLU-based models (the fourth image) with our techniques, as well as their respective performance. We can see that the 4-bit quantized ReLU-based model is clearly superior in quality to other 4-bit quantized models.\nTo derive the ReLU-based model, we finetune the full- precision version of the model followed by post-training quan-"}, {"title": "C. Temporal Per-channel Sparsity", "content": "In addition to enabling aggressive 4-bit quantization, the use of ReLU simultaneously promotes significant activation sparsity in the model, as ReLU clamps all negative values strictly to zero. The average sparsity of the SiLU-based model is around 10%, whereas the ReLU-based model achieves a significantly higher average sparsity of 65% and up to 85% sparse for some layers. However, mid-level random sparsity does not translate well to hardware acceleration; as shown in [26], unstructured sparsity requires at least 87.5% sparsity to yield notable speed-ups on GPUs.\nInterestingly, instead of random sparsity, we observe a temporal per-channel sparsity pattern in ReLU-based diffusion models that can be effectively exploited for further accelera- tion. Figure 7 depicts the sparsity pattern of the activations of a single layer in a ReLU-based EDM (for CIFAR-10 dataset). The values are binarized: zero values are represented in black, and non-zero values in white. Each row corresponds to a channel (32x32 block) of the activation tensor, while each column represents a time step in the diffusion process."}, {"title": "IV. DIFFUSION MODEL ACCELERATOR", "content": "In this section, we propose a novel computation scheme and hardware architecture to accelerate the temporal per-channel sparsity pattern observed in ReLU-based model activations. Figure 8 illustrates the core concept: the activation tensor is categorized into sparse and dense channels. By grouping channels (including both weights and activations) based on the activation sparsity type, the computation can be optimized. Please note that the weights are always dense in this work. Under the proposed scheme, dense channel groups are pro- cessed using the dense processing unit, while sparse channel groups are computed using the sparse processing unit. After that, the partial sums are added together to get the final result. In following sections, we detail the architecture that leverages temporal sparsity to accelerate diffusion model while significantly reducing total system energy consumption."}, {"title": "A. Overall Architecture", "content": "Figure 9 illustrates the overall architecture of our diffusion model accelerator design, adapted from the MAGNet modular accelerator generator [27]. The accelerator comprises of three main components: a controller, a dense/sparse processing ele- ment (D/S PE) array, and an interconnection network between the global buffer and the PEs. The controller manages time step information and orchestrates various PEs through control logic. The architecture features two types of PEs: Dense Processing Elements (DPEs) for dense channel computation and Sparse Processing Elements (SPEs) for sparse channel computation. These PEs are interconnected via configurable routers (R), enabling efficient processing of both dense and sparse data.\nThe D/S PE consists of several key components: a sparsity- aware address generator, weight/input/accumulation buffers, dense/sparse datapaths, and a post-processing unit (PPU) with a sparsity detector. Each PE can be configured to either the dense or sparse datapath, depending on the computation type. Based on the sparsity pattern, either the dense or sparse vector MAC datapath is employed to compute partial sums. Recent works on dense accelerators [28], [29] and sparse accelerators [30]\u2013[37] have shown the effectiveness of tailored architectures for specific data types. In our design, we leverage a MAERI-like architecture [29] for the dense MAC datapath and a SIGMA-like architecture [34] for the sparse MAC datapath, as both architectures are well-suited for handling irregular matrix sizes. SIGMA's flexible distribution and re- duction networks also enable efficient processing of irregular matrix sparsity.\nAfter processing all input channels, the data from the accu- mulation buffer is passed through the PPU. The sparsity-aware address generator maintains channel information, including sparsity type (dense, sparse, etc.) and the corresponding chan- nel index. Utilizing this information, the address generator produces the necessary weight and activation addresses to fetch data from the global buffer. A temporal sparsity detector in the PPU identifies per-channel sparsity in the output. The number of zeros is compared against a predefined threshold to update the sparse channel index information for the next layer in the sparsity-aware address generator."}, {"title": "B. Channel-last Memory Mapping", "content": "To accommodate the non-continuous input channel order required by the sparsity-aware address generator when fetching data from the global buffer, we design a channel-last data mapping strategy, illustrated in Figure 10. For activations, the address mapping follows the sequence of width (W), height (H), and input/output channel (C) being the last, enabling the PE to fetch the entire dense/sparse input/output channel. For sparse channels, only nonzero values and its binary indicator (0 for zero and 1 for non-zero) are stored in the memory, which aligns with the SIGMA sparse accelerator architecture. For weights, the mapping sequence is kernel width (S), kernel height (R), output channel (K), and then input channel (C). This channel-last ordering ensures that the corresponding weights are fetched to align with the activations, optimizing the computation process."}, {"title": "C. Temporal Sparsity Detection", "content": "The temporal sparsity detector calculates the output per- channel sparsity and assigns each channel as either dense or sparse. The sparsity threshold distinguishing dense from sparse channels is determined to balance the execution time between the dense PE and sparse PE. Figure 11 (left) analyzes the sparsity threshold. Based on this analysis, we select 30% as the sparsity threshold. This threshold achieves an average sparsity of 70% for the sparse tensor portion while maintaining a balance between the workloads of the dense and sparse PEs."}, {"title": "D. Hardware Evaluation", "content": "We utilize Stonne [38], an open-source simulation frame- work, to evaluate the latency and energy consumption of the DPE and SPE. Stonne provides end-to-end evaluation of flexible accelerator microarchitectures with sparsity support. The design is simulated under 28nm technology. In these experiments, we assume the architecture includes one DPE and one SPE, each containing 128 multipliers. This architecture is scalable to meet specific latency and power requirements. The baseline for comparison is a purely dense architecture with two DPEs. Figure 12 (top) presents the average speed-up and energy saving across different datasets relative to the baseline. Focusing solely on temporal sparsity in this figure, we achieve an average speed-up of 1.83\u00d7 along with a system energy saving of 51.5%.\nFigure 12 (bottom) illustrates the total speed-up compared to an FP16 SiLU-based diffusion model. Our 4-bit quantization contributes to 3.78\u00d7 speed-up, while our temporal per-channel sparsity adds 1.83\u00d7 speed-up on top of quantization. In combination, our proposed model optimizations and hardware architecture described in this paper achieve a total speed-up of 6.91x."}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose a set of co-designed optimization techniques to aggressively quantize diffusion models to 4-bit while simultaneously promoting significant activation sparsity. By designing a novel heterogeneous dense/sparse accelerator architecture, we achieve a 6.91\u00d7 speed-up compared to an FP16 baseline while demonstrating state-of-the-art image gen- eration quality. By leveraging temporally sparse computations, we save 51.5% in energy consumption compared to traditional dense accelerators. In the future, we plan to extend our techniques to diffusion models targeting video generation [39] and apply our methodology to other generative models. Ex- ploring new DNN models will allow us to further enhance our optimization techniques and expand the applicability of our proposed accelerator design."}]}