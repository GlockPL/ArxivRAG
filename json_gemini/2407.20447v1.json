{"title": "Domain Adaptable Prescriptive AI Agent for Enterprise", "authors": ["Piero Orderique", "Wei Sun", "Kristjan Greenewald"], "abstract": "Despite advancements in causal inference and prescriptive AI, its adoption in enterprise settings remains\nhindered primarily due to its technical complexity. Many users lack the necessary knowledge and appro-\npriate tools to effectively leverage these technologies. This work at the MIT-IBM Watson AI Lab focuses\non developing the proof-of-concept agent, PrecAIse, a domain-adaptable conversational agent equipped\nwith a suite of causal and prescriptive tools to help enterprise users make better business decisions. The\nobjective is to make advanced, novel causal inference and prescriptive tools widely accessible through\nnatural language interactions. The presented Natural Language User Interface (NLUI) enables users\nwith limited expertise in machine learning and data science to harness prescriptive analytics in their\ndecision-making processes without requiring intensive computing resources. We present an agent capable\nof function calling, maintaining faithful, interactive, and dynamic conversations, and supporting new\ndomains.", "sections": [{"title": "Introduction", "content": "There is a growing demand in enterprise for prescriptive AI solutions, which focus on prescribing actions to\noptimize specific objectives. To support the prescriptive AI pipeline, causal analysis and policy learning are\nessential due to their ability to provide actionable insights and guide decision-making processes. Specifically,\ncausal inference allows for leveraging non-experimental datasets to quantify the impact of different actions\nwhile avoiding spurious correlations. Meanwhile, policy learning helps in deriving hyper-segmented strate-\ngies that are interdependent across segments, evolving based on changing market conditions, and ensuring\ncontinuous improvement in decision-making processes.\nWhile powerful, prescriptive AI has faced several barriers to widespread enterprise adoption. Firstly,\ndeveloping and implementing prescriptive AI solutions involves advanced machine learning knowledge in-\ncluding causal analysis and optimization. This complexity can be a barrier for many organizations that lack\nthe necessary technical expertise. Secondly, many end users (such as sales managers, marketers, and oper-\nations leaders), when provided with AI-generated recommendations, lack the technical knowledge required\nto interpret the results and understand how to apply them to their specific business contexts.\nTraditionally, AI recommendations are delivered to users either via (a) the technical team analyzing\ndata and presenting solutions (e.g., Jupyter notebooks, Tableau) along with recommendations, which can be\nchallenging for end users to apply to new use cases without the continuous support from the technical team,\nor (b) purpose-built GUI systems which typically offer only one-way interaction and are passive, not allowing\nfor dynamic two-way interactions. As many tasks involve complex user input (e.g., conditions for conditional\ntreatment effects or specifying policy constraints) and complex outputs (e.g., policies), developing such a\nGUI would require significant investment and likely pose a steep learning curve for users.\nMotivated by these observations, PrecAIse (pronounced as \"precise\", which stands for Prescriptive\nCausal AI Solution for Enterprise), an initiative spearheaded by IBM Research, aims to overcome these\nbarriers by enabling users with limited AI background to leverage cutting-edge causal inference and prescrip-\ntive tools through an intuitive and user-friendly Natural Language User Interface (NLUI). PrecAIse focuses\non leveraging the power of Large Language Models (LLMs) to enable natural-language interaction with\nusers. The vision is to empower an LLM-based agent to utilize these tools through a simple GUI comprising\na text-based input and both text and figure-based output. LLMs naturally lend themselves to high levels"}, {"title": "PrecAlse Overview", "content": "We now briefly describe the backend tools that PrecAIse supports (more details can be found in Sun et al.\n[2024]). We assume we have access to an observational dataset that consists of historical actions A, covariates\nX (may potentially be high dimensional), and outcomes Y.\nCausal Analysis It allows the user to understand and reason about the impact of the considered actions\non the outcome of interest. Specifically, the tools from our prior work Greenewald et al. [2021] are capable\nof:\n1. Estimate the average causal effect of any action A = a1 compared to any other action A = a2. It helps\nto identify the action that is most effective for the entire population.\n2. Estimate the conditional average effect of any action A = a1 compared to any other action A = a2,\nconditioned on certain conditions on X being satisfied. It provides insights on how different actions\nwork on subsets of your data (e.g. age cohorts, geographies).\n3. Identify which covariates are direct causal parents of the outcome Y. This may inform the design of\nnew actions seeking to improve these covariates, or may simply aid the building of intuition about the\nsystem.\nPolicy learning Once we gain insights from the data, it's natural to ask how we can leverage these\ninsights to take better actions. The tools from our prior work Subramanian et al. [2022] focus on learning\na set of optimal policies for a given objective. The policies are presented in a tree form for enhanced\ninterpretability and the framework can handle a wide variety of user-specified constraints (e.g., budget,\nfairness). Specifically,\n1. Learn optimal policies subject to global, inter-segment, and intra-segment constraints.\n2. Allow users to perform \"what if\" analysis by changing specific conditions. Finetune policies and\ncompare different counterfactual outcomes."}, {"title": "PrecAlse Architecture", "content": "The PrecAIse framework is presented in Figure 1 building on the architecture proposed in Sun et al. Sun\net al. [2024]. This work focuses on designing and implementing each module - specifically the function calling\nmodules and conversational modules. This framework incorporates the updated tool wrappers from Table\n1, prompt tuned models specialized in parameter extraction and intent classification introduced in Section\n4.1.2, a new chat model, memory module, and thought injection techniques from Section 4.2, and an updated\nsystem prompt shown in Figure 9.\nFigure 2 shows the logical flow of a user's query in the system. The\nfirst part of the flow sends the incoming query to an intent classifier and several parameter extractors.\nThese classifiers/extractors are prompt tuned for increased accuracy, which means that the original LLM\nweights are frozen and the input query is combined with a light, trained embedding. The intent classifier is\nresponsible for mapping the query to a predefined tool name such as those in Table 1."}, {"title": "Creating a Domain Adaptable Agent", "content": "There are three modules that need to be updated to apply the agent to a new domain - the intent classifier,\ncolumn extractors, and the system prompt. The updated intent classifier must be able to handle domain-\nspecific queries and map them to one of the underlying tools. Similarly, there needs to be new column\nextractors to handle column-specific queries regarding the new dataset that is uploaded. Finally, the system\nprompt needs to take in some general information about the dataset to help guide the user properly.\nWe introduce a fully automated generalization pipeline in Figure 3 that is able to auto-create these two\nmodules from simply the new dataset and some metadata about it. The pipeline works by taking the user"}, {"title": "Methodology", "content": "This section outlines the methods used to improve the current function calling and conversational abilities\nof the agent. Chapter 5 discusses the notable quantitative and qualitative results from these changes."}, {"title": "Improving Function Calling", "content": "The pre-existing prototype of PrecAIse from Sun et al. Sun et al. [2024] used few-shot learning to create an\nintent classifier and several parameter extractors. Each classifier/extractors would be given an instruction\nalong with a set of manually written (query, expected output) samples. For example, PrecAIse used a few-\nshot prompt to extract out the price range in a given query and another few-shot prompt, with its own\nset of examples, to extract the origin and destination pair. If no value was identified in the query, the\nclassifier/extractors would output \"unknown\". An example few-shot prompt for the price-range classifier is\nincluded as Figure 6.\n\n\nThis approach of having each classifier/extractor have its own set of individualized samples proved problem-\natic. Queries such as \"Show me the policy for the BOS-ATL market\" would cause the price range classifier\nto output \"lower_bound: BOS\" and \"upper_bound: ATL\". This happens because in the few-shot examples\nprovided to the price-range classifier, an origin-destination pair was never included; therefore, the model\nlearns to treat anything in the format \"data-data\" as a price range. Furthermore, classifiers in charge of\noutputting numbers would often output whole words other than \"Unknown\".\nA series of prompting techniques were explored to improve classification results. These included restricting\noutputs to one data type and including samples used for one classifier in the example set for other classifiers."}, {"title": "Prompt Tuning", "content": "To achieve a professional level of engagement and usability, we decided to fine-tune models to enhance the\naccuracy of the agent. We opted for parameter efficient fine tuning methods that kept model weights frozen\nin order to keep costs and inference times low. We used prompt tuning Martineau [2023] due to IBM's strong\ninfrastructure support for this method.\nTo prompt tune a model, the creator needs to supply a series of standard hyperparameters (gradient ac-\ncumulation steps, learning rate, number of virtual tokens, etc.) and a training file of (input, output) samples.\nThe training examples are the same as the examples used in few-shot learning. For the hyperparameters,\nthese defaults were chosen:\n\u2022 Gradient accumulation steps: 16\n\u2022 Initialization method: text\n\u2022 Learning rate: 0.3\n\u2022 Number of virtual tokens: 500\nThe hyperparameter that had a noticeable impact on performance and training speed was the initial-\nization method, which sets whether or not to start from a random or an initial text initialization. Starting\nfrom a random initialization means the soft prompt starts from a random embedding before tuning starts.\nUsing text initialization, however, the process starts from the embedding of the passed in text. Since the\nfew-shot classifiers being used already included an instruction string, this instruction was used as the text\ninitialization.\nSystem prompt. The final missing piece is to create the system prompt. This prompt contains useful\ninformation about the dataset and outlines the goals and initiatives of the agent. The template is presented\nin Figure 9. Once this is auto-generated and all the classifiers/extractors have been tuned, the agent is ready\nto be deployed."}, {"title": "Improving Conversational Aspects", "content": "The pre-existing PrecAIse prototype handled conversations by also using a few-shot prompt. This resulted\nin rigid responses and did not have the capacity to follow up for missing information. We were able to signifi-\ncantly improve the agent's conversational abilities, including the capacity to engage in natural conversations,\nfollow up, and present non-hallucinated information. Qualitative results across these dimensions are shown\nin Section 5.2.\nModels. Rather than using a few-shot prompt to create the conversational LLM, we opted for a Sparse\nMixture of Experts model developed by the Mistral AI team Jiang et al. [2023]. More specifically, we used\nthe \"Mixtral 8x7B Instruct\" model, that was fine-tuned on instruction based tasks in order to have more fine-\ngrain control over the model's output. To have it behave as a chat model, the model was simply instructed\nto do so using a system prompt commonly seen in chat based models. The full system prompt used can be\nseen in Figure 9.\nChat Memory. The agent's chat memory was updated to incorporate both semantic and short-term\nmemory. The agent, as before, still stores previously extracted parameters so that it does not have to follow-\nup for this information later on. Now, however, the agent is also capable of remembering recent interactions\nto allow for a more natural feel without incurring significant speed costs.\nA design choice here is to decide how many k recent interactions to store in the context. A higher k leads\nleads to higher inference times as the context passed in to the LLM gets larger, however it allows the model\nto remember more information. Another drawback noticed in practice is that given that the user is usually\nrunning multiple tools one after the other, an agent with high k tends to treat its own memory of previous\nresponses as few-shot examples, leading to the same rigid answer structures seen with using ICL methods.\nFor this reason, k = 2 was used since in practice it seemed to be a good balance of recent interactions without"}, {"title": "Evaluation", "content": "To evaluate the agent's function calling ability, we focus mainly on its capacity of recognizing the user's intent\nand mapping it to one of the predefined functions listed in Table 1. The following is the developed testing\nframework to quantitatively asses the agent's function calling abilities. We show that prompt tuning the\nintent classifier using this framework results in a better model than using ICL methods without introducing\nany additional manual work than is already needed to support few-shot learning.\nManual Training Examples. We map out initial experiments using a manual database of \u2248 50 (query,\ntool) examples as the \"training data\" for ICL and prompt tuning methods. In the the ICL case, these are\nused as the few-shot examples prompted to the model, while in the prompt tuning case, these are used as\ntraining examples to update the weights of the learned soft prompt.\nProgrammatically Generated Training Examples. Due to the need to generalize this pipeline,\nas discussed in Chapter 3, we created templates from the manual training examples and used information\ndirectly from the columns of the database to fill in the blanks. Moreover, doing so allowed us to double the\namount of training examples used for both ICL and prompt tuned models.\nCreating Synthetic Test Data. Finally, the evaluation pipeline needs ample amounts of test data to\nget decent insights to these classifier's performances. While prompting based approaches have been able to\neffectively generate labeled data for intent classification Sahu et al. [2022], these methods focus on generating\ndiverse utterances with a similar intent as the given query. This means that 1) this process would have to be\nrun multiple times since there are multiple queries, 2) out-of-domain queries might slip into the testing set,\nand 3) no other needed labels, i.e. function parameters, are involved, making this process seem less feasible\nin this use case. Even though using these prompting-based approaches with a low temperature has shown\nto improve generation Holtzman et al. [2019], controlling for out of domain utterances would still introduce\nmanual oversight.\nFor this reason, we utilized a work called Parrot, a paraphrase-based utterance augmentation framework\nDamodaran [2021]. Parrot works by preserving the meaning of a sentence while ensuring fluency and diversity,\nthus enabling a nuanced evaluation of the agent's function calling proficiency. Using Parrot on the the \u2248 50\nmanual examples we had created earlier, we generated 238 faithful, synthetic examples for testing."}, {"title": "Evaluating Conversational Aspects", "content": "The conversational aspects of the agent are measured qualitatively by team members to ensure that a\nnoticeable improvement in natural responses is observed.\nChat Models. Crafting a better system prompt as shown in Figure 9 and using an instruction prompted"}, {"title": "UI/UX Improvements", "content": "User Interface (UI) and User Experience (UX) are crucial elements in the design and functionality of PrecAIse\nas it enhances its usability, engagement, and interpretability.\nChat Updates. Changes were made to make these dialog bubbles look more \"modern\" and fix alignment\nissues that sometimes caused the agent's responses to appear in the middle of the chat window. Moreover,\nthe order of the messages were swapped to have the user be right-aligned. This is a more common mental\nmodel used by other applications so this was done for consistency. A comparison of the original and updated\nUI is shown in Figure 18.\nApart from these style changes, we also added some UX changes like auto-scrolling the chat window upon\nnew messages, having the chat input box refocus after sending a message, and adding a \"loading\" symbol\nwhen waiting for a response from the back-end (Figure 19)."}, {"title": "Related Works", "content": "Function calling in the context of LLMs refers to the ability to take a natural language query, recognize\nthe correct function to execute, and extract out the appropriate parameters. This capability is essential for\nconnecting LLMs with external data sources and specialized tools not included in their training data. An\nimportant consideration for enterprise use is to restrict implementations to use open-source models for cost,\ntransparency, and flexibility reasons Sun et al. [2024] to simplify integration.\nTool Augmented Language Models. Although the latest models from OpenAI have been trained\nto detect function intent and output structured JSON OpenAI [2023], these models are proprietary and\nnot freely available. Several studies, however, have explored integrating tools with open-source LLMs. For\ninstance, Toolformer Schick et al. [2023] is a fine-tuned GPT-J model that learns when to use tools in a self-\nsupervised manner. Similarly, Gorilla Patil et al. [2023], a fine-tuned LLaMA-7B model, surpasses GPT-4"}, {"title": "Conversational Aspects", "content": "Functions that make up a usable and interpretable agent includes the capability of engaging in natural\nconversations, providing appropriate follow-ups, and ensuring that the information it presents or requests\ndoes not stem from hallucinated inputs, outputs, or data.\nModels. Different models have been fine-tuned for specific tasks or task groups. Google's Fine-tuned\nLanguage Net (FLAN) Chung et al. [2022] has been instruction-tuned on various tasks like question and\nanswering and various chain-of-thought tasks. Because of this, the model demonstrates considerable profi-\nciency in handling tasks it was not directly trained on. This means that it is possible to use instruction-tuned\nmodels as chat models by simply instructing the model to act like a chat model. However, dedicated chat\nmodels, which are specifically trained to manage conversational dynamics, could potentially offer improved\nperformance. Mixture of Experts (MoE) models can also be a superior alternative due to its faster inference\ntimes compared to similarly sized models Sanseviero et al. [2023].\nChat Memory. Effective memory utilization is crucial for agents to maintain an enhanced contextual\nunderstanding and produce responses that appear more natural to users. Different types of memory include\nshort-term memory (for recent interactions), long-term memory, episodic memory (for significant interac-\ntions), and semantic memory (for general knowledge not tied to specific instances) Kashyap [2023]. Selecting\nthe appropriate type of memory is vital, as each has its own set of advantages in regards to complexity,\nscalability, privacy, and inference time.\nHallucination. Hallucination occurs when a model produces a response that seems plausible but in\nactuality is unfaithful Ji et al. [2022]. Important for decision-making, responses must be anchored in factual\ndata. While techniques like Retrieval Augmented Generation (RAG) Lewis et al. [2020] and Knowledge\nRetrieval Varshney et al. [2023] have been used to mitigate hallucinations, these techniques often introduce\nlatency and maintenance overhead. Employing chain-of-thought prompting has been shown to enhance the\naccuracy of outputs from LLMs Wei et al. [2022], and prompt injection - which is \"the process of overriding\noriginal instructions in the prompt with special user input\" Schulhoff [2024] - can strongly align model\nresponses."}, {"title": "Conclusion", "content": "The culmination of this work is presented in a final demonstration of the agent in Appendix A. While\nresults with the prompt tuned models for intent recognition and parameter extraction to support function\ncalling were not exhaustive, they did serve to give further insights into the capabilities of these smaller"}]}