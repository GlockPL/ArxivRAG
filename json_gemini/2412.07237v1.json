{"title": "ArtFormer: Controllable Generation of Diverse 3D Articulated Objects", "authors": ["Jiayi Su", "Youhe Feng", "Zheng Li", "Jinhua Song", "Yangfan He", "Botao Ren", "Botian Xu"], "abstract": "This paper presents a novel framework for modeling and conditional generation of 3D articulated objects. Troubled by flexibility-quality tradeoffs, existing methods are often limited to using predefined structures or retrieving shapes from static datasets. To address these challenges, we parameterize an articulated object as a tree of tokens and employ a transformer to generate both the object's high-level geometry code and its kinematic relations. Subsequently, each sub-part's geometry is further decoded using a signed-distance-function (SDF) shape prior, facilitating the synthesis of high-quality 3D shapes. Our approach enables the generation of diverse objects with high-quality geometry and varying number of parts. Comprehensive experiments on conditional generation from text descriptions demonstrate the effectiveness and flexibility of our method.", "sections": [{"title": "1. Introduction", "content": "Articulated objects are defined as entities composed of multiple rigid sub-parts connected by various joints which allow the sub-parts to undergo constrained relative motion [29]. Among others, man-made articulated objects constitute most everyday objects around us.\nThe perception [19, 26, 34, 62] and reconstruction [13, 38, 58] of articulated objects have been extensively studied. However, research on generating articulated objects remains limited. On the one hand, to generate a multi-part articulated object, the model must simultaneously produce both the geometry of each sub-part and the kinematic relationships between them. Existing methods find it challenging to generate both modalities with high quality simultaneously. On the other hand, the complexity of articulated objects makes annotating them very costly, resulting in limited datasets for articulated objects.\nMost relevant to this work are NAP [25], CAGE [30],"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Modeling 3D Articulated Objects.", "content": "Articulated objects are a specialized type of 3D object distinguished by their segmented, jointed structure, allowing for flexible movement and positioning of individual sub-parts. Modeling 3D articulated objects, an extension of 3D object modeling, involves the prediction [12, 37, 54], reconstruction, and generation of flexible, jointed structures.\nImplicit neural representations have become a popular option recently [35, 41, 59, 60] due to their GPU memory efficiency and the ability to generate high-quality geometry. A convenient characteristic of implicit representations is that spatial transforms to the shape can be cast as rigid transforms to the input query points, making them a good choice for dealing with the kinematic relations in articulations.\nA-SDF [38] is among the earliest explorers of using SDF to model articulated objects, but did not utilize the aforementioned property. More recently, NAP [25] introduces the first 3D deep generative model for synthesizing articulated objects through a novel articulation tree/graph parameterization and the use of a DDPM [14], enabling masked generation applications. Similarly, CAGE [30] also employs a graph diffusion denoising model but with a primary aim of controllability. SINGAPO [28] further extends controllable generation to single-image conditioning.\nHowever, CAGE and SINGAPO only generate abstractions of sub-parts, which are then used to retrieve similar assets from a dataset. Therefore, they can not produce objects with geometry features that are unseen in the dataset. This limitation is also common to methods that do not use SDF, such as URDFormer [4], which predicts predefined URDF [46] primitives and meshes. A potential reason for such limitation is the difficulty of simultaneously modeling kinematic relations and geometry. Hyper-SDF [9] and Diffusion-SDF [5] propose methods to learn high-quality yet controllable priors of rigid SDFs. This work adopts a shape prior similar to Diffusion-SDF to ensure geometry quality."}, {"title": "2.2. Generating Tree-structured Objects", "content": "Generating tree-structured objects differs from conventional sequential generation, as each node can have multiple successors. Traditional approaches model graph distributions using Variational Autoencoders (VAEs) [24, 53, 63], Generative Adversarial Networks (GANs) [8, 33, 57] and Denoising Diffusion Networks [16, 17, 21, 64, 65]. Di-Gress [56] and FreeGress [39], for example, achieve state-of-the-art generation performance and can handle large, diverse molecular datasets. However, these methods are not tailored for tree-structured graphs and lack autoregressive generation, resulting in unreliable outputs for realistic"}, {"title": "2.3. Learning Representations with Conditional Inputs", "content": "Reed et al. [49] present a neural language model trained from scratch for zero-shot visual recognition, enabling accurate image retrieval using text-based representations. Hubert et al. [18] use autoencoders to extract visual-semantic joint embeddings. State-of-the-art methods [1, 2, 22, 43, 50] use a domain-specific embedding layer to learn implicit representations for multi-modal inputs, employing functions like cross-attention to model the joint distribution. Compared to image and text data, 3D-formatted data is less common, leading to extensive research on reconstructing 3D objects from image or text inputs [2, 3, 10, 15, 58]. Chen et al. [3] utilize learning by association and metric learning techniques to learn representations conditioned on text. Liu et al. [28] reconstruct 3D articulated objects from a single image, leveraging DINOv2 [40] and GPT-4o to extract articulation information."}, {"title": "3. Method", "content": null}, {"title": "3.1. Articulation Parameterization", "content": "Our parameterization process encodes an articulated object into a tree structure highly similar to the format used in URDF [46] and MJCF [55] files. We consider each node (part) as a token that stores the geometry and kinematic relations of the corresponding sub-part of the articulated object. The attributes stored at each node are similar to those stored in the data parameterization of CAGE [30]. Regarding the geometry information, for the i-th node, we identify the following 2 attributes:\n\u2022 Bounding box, $b_i \\in \\mathbb{R}^6$: For an articulated object, each sub-part is assigned an initial position, with its bounding box defining the maximum and minimum coordinates that the sub-part occupies along each axis in this initial state.\n\u2022 Geometry latent code, $z_i \\in \\mathbb{R}^{768}$: We collect the point cloud of the sub-part. The point cloud is then processed through a series of encoders, converting it into a corresponding latent vector with dimension 768 to represent the geometry of the i-th sub-part.\nThe kinematic parameters between i-th node and its parent node are represented by 2 attributes:\n\u2022 Joint axis, $j_i \\in \\mathbb{R}^6$: The joint axis includes an origin point and a unit direction vector. The i-th sub-part is capable of rotating around this axis or translating along it relative to its parent sub-part. The direction vector determines the positive direction for both rotational and translational movements.\n\u2022 Limit, $l_i \\in \\mathbb{R}^4$: The attribute defines the permissible ranges for translational and rotational movements, setting the minimum and maximum extents of both translation and rotation relative to the initial position. If a sub-part is restricted from moving relative to its parent part, both the upper and lower bounds of these ranges are 0.\nFor the i-th node in the tree, we store the aforementioned 4 attributes, as well as the index of its parent node. Consequently, each node is represented by a token of dimension D=6+768+6+4+1 = 785. For all coordinates in each node, we utilize coordinates from the global coordinate system."}, {"title": "3.2. Diverse and Controllable Shape Prior", "content": "As previously mentioned, simultaneous modeling and generating high-quality geometry and accurate kinematic relationships is challenging. Therefore, we first learn a shape prior $p(z)$ of the geometry latent code using a method similar to Diffusion-SDF [5].\nShape Prior. An articulated object consists of multiple sub-parts. Given a sub-part sampled from the dataset, we encode its point cloud with a VAE encoder: $q(z|f)$ where $f = \\Gamma(pcd) \\in \\mathbb{R}^{3\\times256\\times64\\times64}$ is an intermediate tri-plane feature obtained from a PointNet encoder \u0393. A generalizable SDF network \u03a9(f, x) then predicts the part's SDF at query points x \u2208 R\u00b3 from decoded features p(f|z). The training objective is:\n$L(q, p, \\Gamma, \\Omega) =||\\Omega(f, x) \u2013 SDF(x)||_1 + \\beta D_{KL} (q(z|f)||N (0, 1)).$ (1)\nwhere f ~ p(f|qz|f)), SDF(x) is the ground-truth signed-distance at point x and \u03b2 balances the degree of regularization to a Gaussian prior.\nTo subsequently enable guided or conditional generation of object sub-parts geometry, we train a conditional diffusion model e($z_t$, t, $\u0109_g$, $C_s$) on p(z), where the geometry and semantic conditions are given by two corresponding encoders: $c_g$ = $E_g$(z) and $c_s$ = $E_s$(name). Before geometry condition $c_g$ is input into the diffusion model, it is processed into $\u0109_g$ using codebooks. The diffusion model is trained to denoise random latent $z_t$ ~ N(0, I) into a meaningful $z_0$ ~ p(z), following the objective used in [48]:\n$L(\\epsilon) = ||\\epsilon(z_t, t, \\hat{C}_g, C_s) - z_0||_2$. (2)"}, {"title": "3.3. Articulation Transformer", "content": "After the articulated object is parameterized into a tree structure, each node is treated as a token in the classical transformer architecture. i-th token is composed of $[fai, b_i, z_i, j_i, l_i]$, where $fa_i$ is the parent index of i-th node. In addition to the index of the parent node $fa_i$, the remaining attributes are concatenated and subsequently processed through an MLP tokenizer. An overview of the articulation transformer is illustrated in Fig. 3.\nTree Position Embedding. In order for the transformer to recognize the specific position of each token, we proposed a novel position encoding scheme specifically designed for tree structures building upon the works of [52] and [45]. We first calculate the absolute position encoding ai for each i-th node:\n$a_i = GRU (\\{Path_k\\}_{k=R})$. (5)\nIt push the tokens on the path from the root R to the i-th node to a bi-directional GRU [23, 45] to compress the information on its path. We define the position embedding of the i-th node pi to represent the relative position as well:\n$p_i = CAT (\\{a_k\\}_{k=i})$. (6)\nwhere CAT denotes concatenation. We employ truncation"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental Setup", "content": "We train the shape prior on PartNet [36] and PartNet-Mobility [61]. Although PartNet does not provide kinematic information, it still contributes to learning the geometry. The ArtFormer and other baseline models are trained exclusively on PartNet-Mobility. For each articulated object in the dataset, we use Blender to create high-resolution thumbnails and employ ChatGPT-40 to generate corresponding descriptions, which are used for training the baselines. Detailed implementation steps can be found in the supplementary material."}, {"title": "4.2. Baselines", "content": "Previous works, such as NAP [25] and CAGE [30], differ from ours in several key aspects. NAP uses a simple shape prior with hidden dimensions that are not consistent with ours, while CAGE retrieves shapes from a dataset rather than generating them. To enable a fair comparison, we made modifications to these original models. A cross-attention layer, with the same structure as ours, is added to enable them to process text instructions. The compared models are:\n1. NAP-128: The original NAP model, modified to use our shape prior, generating a 128-dimensional shape code consistent with the original work.\n2. NAP-768: Building on NAP-128, we increase the size of the shape code to 768 dimensions to align with our model.\n3. CAGE: The original CAGE model, modified to retrieve outputs based on our shape prior.\n4. Ours: Our proposed model and articulation parameterization, the geometry is generated through shape prior, bypassing the part retrieval.\n5. Ours-PR: Building on our original model. We perform part retrieval after the iterative decoding as CAGE do for fair comparison."}, {"title": "4.3. Metrics", "content": "We adopt the Instantiation Distance (ID) from NAP to evaluate the kinematic relations and geometry. A smaller ID value between two articulated objects indicates greater similarity, and vice versa. The following metrics are defined:\n(1) Minimum Matching Distance (MMD) describes the minimum distance between corresponding matches of generated objects and ground truths. (2) Coverage (COV) represents the ratio of ground truth instances that have a corresponding match in the generated objects, reflecting the similarity between the two distributions. (3) Nearest Neighbor Accuracy (1-NNA) measures the mismatch rate between generated and ground truth objects after performing 1-NN clustering.\nTo examine whether the generated objects are physically plausible, we also propose Part Overlapping Ratio (POR) which assesses the degree of interpenetration between sub-parts. Let E represent the articulated object. We define the interpenetration metric between any two sub-parts P1, P2 \u2208 E as the vIoU (volume Intersection over Union) of their corresponding 3D geometries:\n$I(P_1, P_2) = \\frac{|G_1 \\cap G_2|}{|G_1 \\cup G_2|},$ (12)\nwhere G1 and G2 represent the geometries of the sub-parts, respectively. Given a set of joint states J, we can calculate the mean interpenetration between every pair of parts, denoted as MI(E, J). We uniformly sample Nj = 10 joint states $J_1, J_2, ..., J_{N_j}$ within the limits and define the Part Overlapping Ratio as:\n$POR(E) = \\frac{1}{N_j} \\sum_{i=1}^{N_j} MI(E, J_i).$ (13)\nA human study methodology (HS) is used to assess the alignment between generated objects and their text descriptions, as well as the diversity of the generated objects. P = 20 participants are presented with objects generated"}, {"title": "4.4. Results", "content": "Generation Quality and Diversity. We evaluate the generation quality of baselines, as shown in Tab. 1. Since CAGE cannot directly generate geometry, comparisons are divided into two groups. The first includes NAP-128, NAP-768, and our model, which generate geometry features directly. The second approach consists of CAGE and ours-PR, which retrieve the suitable shape from the dataset to generate objects. Our model outperforms NAP on all metrics in the first group, producing more realistic articulated objects with less part interpenetration. In the second group, while CAGE achieves better MMD and POR, indicating superior object-level reconstruction, our model excels in COV and 1-NNA, capturing the overall distribution and generating more diverse objects. HS results in Tab. 1 suggest that our model produces greater diversity and better aligns with text instructions from an ordinary user's perspective.\nNovel Shape Generation. We conducted an experiment inspired by Diffusion-SDF to demonstrate that our shape prior, guided by an articulation transformer, can generate new geometry shapes that never appear in the dataset. We used our model to produce various objects and dissected them into sub-parts. Then, we calculated the Chamfer Distance between each sub-part and those in the training set and ranked them from nearest to farthest. The results, shown in Fig. 6, indicate that the sub-parts generated by our model are distinct from those in the training set, confirming the model's ability to create novel geometry shapes."}, {"title": "Image Guided Generation.", "content": "In our study, we replaced the original pre-trained text encoder [47] with a pre-trained image encoder [27] to validate the flexibility of our method to support various conditioning modalities. We utilized Blender [6] to render each object in the dataset as input images. The results of our experiment are shown in Fig. 7. Our model is capable of generating high-quality articulated objects from a single image. This outcome further demonstrates its potential to scale to more complex and multi-modality settings."}, {"title": "4.5. Ablations", "content": "In our ablation study (Tab. 2), we evaluated the impact of removing the tree position embedding (TPE) and shape prior (SP). Without the shape prior, the transformer must directly generate sub-part geometry, complicating feature generation, eliminating randomness, and significantly degrading all performance metrics. Removing the tree position embedding causes the model to lose positional information, increasing POR due to sub-part overlap during motion and"}, {"title": "5. Conclusion", "content": "We propose a novel method for modeling and generating 3D articulated objects, addressing limitations in diversity and usability. Representing articulated objects as a tree structure with labeled nodes for rigid parts simplifies ar-"}]}