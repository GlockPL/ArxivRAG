{"title": "An Attentive Graph Agent for Topology-Adaptive Cyber Defence", "authors": ["Ilya Orson Sandoval", "Isaac Symes Thompson", "Vasilios Mavroudis", "Chris Hicks"], "abstract": "As cyber threats grow increasingly sophisticated, reinforcement learning (RL) is emerging as a promising technique to create intelligent and adaptive cyber defense systems. However, most existing autonomous defensive agents have overlooked the inherent graph structure of computer networks subject to cyber attacks, potentially missing critical information and constraining their adaptability. To overcome these limitations, we developed a custom version of the Cyber Operations Research Gym (CybORG) environment, encoding network state as a directed graph with realistic low-level features. We employ a Graph Attention Network (GAT) architecture to process node, edge, and global features, and adapt its output to be compatible with policy gradient methods in RL. Our GAT-based approach offers key advantages over flattened alternatives: policies that demonstrate resilience to certain types of unexpected dynamic network topology changes, reasonable generalisation to networks of varying sizes within the same structural distribution, and interpretable defensive actions grounded in tangible network properties. We demonstrate that GAT defensive policies can be trained using our low-level directed graph observations, even when unexpected connections arise during simulation. Evaluations across networks of different sizes, but consistent subnetwork structure, show our policies achieve comparable performance to policies trained specifically for each network configuration. Our study contributes to the development of robust cyber defence systems that can better adapt to real-world network security challenges.", "sections": [{"title": "1 Introduction", "content": "With cyber attacks becoming more complex and unpredictable, the pursuit of adaptive autonomous defence systems is paramount in cybersecurity research. A promising direction to address this task has recently emerged, leveraging reinforcement learning (RL) to automate the discovery of effective defensive schemes through autonomous policies [1-4]. CybORG (Gym for the Development of Autonomous Cyber Agents) [5, 6] has served as a valuable testbed for early explorations through cyber defence challenges, demonstrating the effectiveness of these approaches.\nHowever, current approaches face a significant limitation: they rely on flattened observations that can be processed by multi-layer perceptron (MLP) based policies, disregarding the inherent graph structure of computer networks or representing it through fixed-size embeddings [7]. Most cyber defence simulation environments resort to this observation restriction to comply with reinforcement learning training suites based on OpenAI Gym [8], including notable simulation environments such as YAWNING TITAN [9], PrimAITE [10], CyberBattleSim [11] and Cyberwheel [12]. We hypothesize that under the absence of explicit structural information, standard RL policies must learn the network topology implicitly through interactions, potentially overlooking critical relationships between hosts in the network. We posit that a more specialised approach that takes advantage of the inherent graph structure of networks could enhance the adaptability of autonomous cyber defence systems, consistent with the growing body of research exploiting structure in RL [13].\nIn this paper, we explore the potential of Graph Attention Networks (GATs) [14] for training robust and adaptable cyber defence policies. We achieve this by developing a graph-based environment with the CAGE Challenge 2 [5] setup using the CybORG simulator, allowing the GAT defensive agent to directly process low-level network information and learn strategies that generalise across dynamic topologies and network sizes within similar structural distributions.\nOur approach offers key advantages over existing methods:\n\u2022 Explicit incorporation of network structure: By using a graph-based representation, our model has a structural inductive bias designed to leverage the relationships between nodes in the network.\n\u2022 Adaptability to varying network sizes and topologies: The GAT-based policy has the capacity to handle graphs of different sizes and structures, enabling a degree of generalisation across network configurations with the same hierarchical structure.\n\u2022 Enhanced interpretability: Using low-level information from the simulator in a custom environment, our approach allows for a mapping of actions to specific network properties.\nThis direction offers a promising avenue for research on graph-based reinforcement learning for cyber defence, potentially contributing to to more robust and adaptive defence systems. Our primary contributions are:\n\u2022 A GAT-based policy architecture adapted for cyber defence tasks that can adapt to varying network layouts and dynamic connections.\n\u2022 A low-level graph encoding scheme built with the CybORG simulator that captures more realistic network features in a reinforcement learning environment.\n\u2022 Evaluation of our approach across environments of varying sizes, demonstrating its scalability and generalisation capabilities.\nThe remainder of this paper is organised as follows: Section 2 discusses related work in the application of GNNs to reinforcement learning and cyber defence. Section 3 presents the building blocks of GATs. Section 4 explains the design of the reinforcement learning environment. Section 5 details our adaptation of a GAT for this cyber defence task. Section 6 presents the results of our experiments. Section 7 discusses our findings on robustness to dynamic connections and policy generalisation, and Section 8 concludes with an overview of our work and directions for future research."}, {"title": "2 Related work", "content": "Reinforcement Learning in Cyber Defence. Works exploring reinforcement learning for au-tonomous cyber defence have routinely used MLP-based policies, most often trained using model-free policy gradient methods such as proximal policy optimisation (PPO) [15]. This includes the highest-scoring submissions to the Cyber Autonomy Gym for Experimentation (CAGE) challenges [5]. The winning submission to CAGE Challenge 1 [1] deployed two PPO-trained agents, specialised in two different behaviour patterns for attacking agents, and used a separately trained bandit-like policy to choose between them. The winning submission of CAGE Challenge 2 relied on expert knowledge of the problem setting to simplify the action space of a defensive policy using heuristics. The dimension of the observation and action spaces in these environments usually depends on the number of nodes in the network. This means that approaches that rely on vanilla MLP function approximators fail to generalise across different network sizes, because they require fixed-size inputs.\nCollyer et al. [7] report favourable results from enhancing the observation space of an agent trained in the YAWNING TITAN environment with the addition of a whole network graph encoding [16] of fixed size. Using this environment, [17] investigated how trained agents performed when the network topology was modified through edge addition, demonstrating that such modifications had a moderate"}, {"title": "Graph Neural Networks in Reinforcement Learning.", "content": "The application of graph neural networks (GNNs) to reinforcement learning has become an active area of research within operations research and robotics. An early application in robotics was NERVENET [21], a GNN-based reinforcement learning policy used for continuous control, where the robot was modelled as a graph of joints and limbs over which the policy had control. In this case, the problem setting was multi-task reinforcement learning, where the policy was trained over many different robot morphologies with different graph structures, and was expected to perform well over all of these tasks at test-time. Derivative improvements over the same strategy are explored in subsequent work within continuous control [22-24]. In operations research, the attention mechanism [25, 26] was adapted to graphs for learning heuristics with reinforcement learning [27], where generalisation was tested in several types of routing problems (see also [28]). Generalisation of GNNs over graph dimensions unseen during training was explored on a similar routing optimisation problem [28]. More recently, a similar strategy was applied to optimal power flow to test generalisation capabilities in power grids [29]."}, {"title": "Graph Neural Networks in Cyber Defence.", "content": "The use of GNNs for cyber defence with reinforce-ment learning is relatively unexplored [30]. GNNs are capable of operating on graph layouts that are different from the ones contained in their training datasets. Understanding the factors that influence their generalisation properties is an active area of research [31\u201333]. This is also a challenging problem in reinforcement learning applications more broadly, where generalisation to unseen conditions is as challenging as partially observable environments [34]. Generalisation to unseen attack strategies and network features, not topology nor size, was tested in CAGE 2 with hierarchical and ensemble reinforcement learning [35], where considerable performance degradation was reported. In a recent review on deep reinforcement learning for autonomous cyber defence [36], the use of GNNs was suggested to incorporate relational inductive biases [37] in defensive agents, and particularly GATs to allow for policies to be deployed on networks of different sizes to those trained on. The use of a size-invariant policy, which resembles a single-layer GNN, was explored for network attack in the penetration-testing environment NASimEmu (Network Attack Simulator-Emulator) [38]. In this case, the motivation was mainly to create a policy that was capable of attacking networks of any configuration. The same authors explored the use of a GNN on the network game SysAdmin [39], which could be seen as a non-adversarial simplistic analogue to a cyber defence environment without specific goals and unlimited horizon. They found that the the agent compares well to a specialised probabilistic planning algorithm, and is able to generalise to variable-sized networks with homogeneous structure [40]. Most relevantly, Nyberg and Johnson [41] explored the defensive generalisation capabilities of the GNN architecture introduced by [38] to changes in network size in the CAGE Challenge 2 environment. In pursuit of enhanced real-world challenges for cyber security, our approach differs from prior work in key aspects. We utilize interpretable low-level simulator data to construct a more realistic and nuanced observation space, making direct numerical comparisons less straightforward. Furthermore, we preserve the original subnetwork structures and the environment's full action space from CAGE 2, avoiding contrived simplifications."}, {"title": "3 Background", "content": "In this section, we briefly introduce the building blocks of graph neural networks and the corre-sponding notation. A graph G is made up of a finite set of nodes X and edges E. The size of the graph is determined by the number of nodes $\\mathbb{N}_r$ and the number of edges $\\mathbb{N}_e$. The nodes are enumerated by indices $u \\in \\{0,1,\\dots,\\mathbb{N}_r-1\\}$, and the edges are denoted by tuples of node indices (u, v). Each node u has features encoded in a vector $x_u$ of dimension $d_r$. Similarly, each edge (u, v) may have an associated vector encoding $e_{uv}$ of dimension $d_e$. For our purposes, we only use homogeneous graphs, where both node and edge encoding dimensions are the same for all nodes and edges. The neighbourhood $\\mathcal{N}(u)$ of a node u is the set of nodes that are connected to it by an edge: $\\mathcal{N}(u) = \\{v \\in \\mathcal{X} \\mid (v, u) \\in \\mathcal{E}\\}$. Equivalently, any two nodes are considered neighbours of each"}, {"title": "3.1 Graph Neural Networks", "content": "other if there is an edge between them. In directed graphs, it is possible as well to define neighbours only by following or reversing the direction of the connecting edge, or to simply ignore the direction distinction as above; this decision is a hyperparameter of each GNN layer.\nEach layer l of a GNN may be decomposed into foundational operations known as message-passing and aggregation. A message $\\phi(\\cdot)$ is calculated for any pair of neighbouring nodes from their corresponding encoding vectors, and potentially the edge encoding vector as well, depending on archi-tecture. These messages are aggregated with an associative operation (e.g., sum/average/min/max) over all the neighbours of a node to then update the node encoding with a function $\\psi(\\cdot)$."}, {"title": "3.1.1 Graph Attention Networks", "content": "A variant of the self-attention mechanism [26] was adapted to GNNs and termed Graph Attention Network (GAT) [14]. This neural network architecture uses as aggregation a normalised sum of neighbourhood messages, where the coefficients are referred to as the attention weights. Here we describe the GAT variant proposed by [42], which allows for the inclusion of global nodes in the attention mechanism, as employed recently in TacticAI [43].\nIn GATs, both the current node embedding and neighbouring messages are weighted by a factor $\\alpha_{uv} \\in \\mathbb{R}$, which is the result of a custom score function $\\eta$ and a softmax reduction of its output over the neighbourhood of a node. The score function $\\eta$ resembles a single MLP layer mixing the origin node ($x_u$), the target node ($x_v$), the embedding of the edge that connects both nodes ($e_{vu}$) and the global node associated with the graph (g),\n$\\eta(u, v) = \\eta(x_u, x_v, e_{vu}, x) = a \\text{ LeakyReLU}(W_u x_u + W_v x_v + W_e e_{vu} + W_g g),$ where,\nwith trainable parameters $a \\in \\mathbb{R}^{d_a}$, $W_u \\in \\mathbb{R}^{d_x \\times d_a}$, $W_v \\in \\mathbb{R}^{d_x \\times d_a}$, $W_e \\in \\mathbb{R}^{d_e \\times d_a}$ and $W_g \\in \\mathbb{R}^{d_g \\times d_a}$, where the dimension $d_a$ is a hyperparameter. Note that the edge and global node embeddings can be optionally excluded from equation 2 if they are not available, making the attention messages versatile for multiple types of graph encodings. The attention weighting is the score function, normalised across all neighbour messages,\n$\\alpha_{uv} = \\frac{\\exp \\left(\\eta(u, v)\\right)}{\\sum_{v \\in \\mathcal{N}(u) \\cup \\{u\\}} \\exp \\left(\\eta(u, v)\\right)},$\nThe aggregation step of each layer is an attention-weighted combination of the node encodings of neighbours:"}, {"title": "4 The CybORG simulator", "content": "The Cyber Operations Research Gym (CybORG) [44] is a computer network simulator developed to facilitate research into the use of reinforcement learning in the autonomous cyber defence (ACD) domain [2, 3, 45]. This is the simulator used to setup the training environment in the CAGE challenges [5]. It is capable of handling concurrent sessions of blue (defender), red (attacker), and green (neutral/user) agents over a network of connected hosts (nodes) in a partially custom layout."}, {"title": "4.1 Red agent", "content": "To simulate realistic cyber-attacker behaviour and generate adversarial network traffic, the envi-ronment incorporates a red agent. Specifically, we employ a red agent of the Meander type as the network attacker. The Meander agent follows a strategy of systematically compromising all hosts within a single subnet before moving on to target the next subnet. The USER 0 (see figure 2) node is set by the environment as the entry point for the attacking red agent, where it starts with admin access cannot be removed by design. At each step, the red agent may take one of the following actions: discovery, exploitation, escalation of privileges and impact (only applicable to the operational server). Given a fixed attack strategy, the network interactions of the red agent are part of the environment's dynamics. The blue agent never has direct access to information about the red agent, and only sees the effects of its actions through standard network observations. The role of the green agent is to simulate benign user interaction with the network, which means that not all detected activity can be attributed to the red agent."}, {"title": "4.2 Rewards and penalties", "content": "We use the same negative reward system used in CAGE 2, shown inTable 1. The optimisation objective for the blue agent is to minimisethe penalty it receives for system disruption within a given episode.Each turn, the blue agent is penalised if a host has been breachedby the red agent, with an increased penalty if the red agent achievesadministrator access to the server hosts. When the red agent hascontrol over the operational server, it may impact it, which incurs thelargest penalty for the blue agent. The blue agent is also penalisedfor restoring a host, simulating the negative effect of disruptions inan operational system."}, {"title": "4.3 Action space", "content": "The action space of the blue (defensive) agent consists of high-level abstractions of the actions typically taken by cyber security professionals. There are 10 actions per node: Analyse, Remove,"}, {"title": "4.4 Observation space", "content": "CAGE 2 offered several observation spaces with different levels of complexity,ranging from the low-level output of the CybORG simulator to a fixed-lengthbit vector compatible with the OpenAI Gym API [8]. This flattened hihg-level observation is generated from a one-to-one correspondence from a tableobservation space, referred to as the BLUETABLE observation and described in the Appendix A. We do not modify the logic of the simulator and only use theCAGE 2 observation spaces as a reference to identify the relevant low-levelinformation. An example of the raw-observation provided by CybORG isshown in Figure 9."}, {"title": "4.4.1 Graph encoding", "content": "To give the CybORG simulator a graph structure, it is required to set theedges between the nodes that compose the scenario. The connections withinthe same subnet are not defined explicitly in the simulator, but we assumethe connections are possible among all members of a subnet based on theattack action space of the red agent. This assumption is the backbone of ourapproach, but this could be detrimental if the empirical simulator's connectionsdeviates from the expected layout; see an extended discussion of this issue inSection 7. Specific hosts in the network configuration are designated to bridgeconnections between subnetworks. The graph layout of the CAGE 2 scenariois depicted in figure 2."}, {"title": "5 Graph neural networks as defensive policies", "content": "We designed our policy to be able to handle a graph representation of the state and to accommodate different numbers of user hosts. As summarised in Section 3.1, most GNNs are neural network architectures that possess both of these properties. Furthermore, given that our version of CybORG provides directed graphs with both edge and global encodings, we leverage GATs for their flexibility to include this information, as detailed in Section 3.1.1. For our use case, the edge features enumerate open connections between hosts while the global feature represents the previous action taken. A GNN control policy takes the directed graph observation at each step and outputs scores per action per host, following the per-node logic described in section 3.1. The raw scores are normalised via a softmax to produce a probability distribution over possible actions (Figure 5).\nNeural networks used as reinforcement learning policies are considerably shallower than those used in other deep learning applications[47, 48]. Furthermore, GNN performance is known to decrease rapidly with increasing layers [49], a phenomenon called \"over-smoothing\". For this reason, we use a small GAT with 2 layers and inter-layer node embeddings of dimension 3."}, {"title": "6 Results", "content": "We demonstrate our approach's potential to maintain consistent performance across networks of different sizes within a fixed sub-network structure, with variations in the number of user hosts. This is a capability that is not achievable with fixed-input methods such as canonical MLP policies. We trained our agent using REINFORCE [27, 50, 51], applying reward normalisation over batches of 1000 episodes sampled from a policy with fixed parameters per batch, episode length of 30 steps, a 0.01 learning rate, and 300 optimiser iterations with ADAM [52]. These training runs were conducted on an Apple M2 Pro with 32 GB of RAM on the CPU, with an average simulation time of 17 episodes per second. All our scenario variants leave the USER 0 intact since this is the entry point for the red agent attack."}, {"title": "7 Discussion", "content": "A fundamental motivation of our approach is to leverage the network topology information in the state representation, which is intrinsically available in multiple network simulators (and real networks). Unfortunately, this information is not typically provided as part of the user interface and hence not designed for direct usage; it is usually considered a technical detail of the simulators. Extracting the network layout reliably from the CybORG environment has proven to be challenging, as unexpected connections appear during the simulation that do not comply with the structure implied by the network configuration file (see figure 8). An empirical account of these connections could could automatically infer network topology, allowing for improved fidelity in similar stochastic network environments by observing the connections throughout fixed scenarios. However, modifying the simulation environment and estimating the correctness of the layout through simulation are beyond the scope of this work. The discrepancy between expected and observed network connections during simulation reveals a limitation of our approach: its reliance on structural patterns from the base graph configuration may become counterproductive when actual network behaviour deviates from these expected patterns. Adaptive methods for uncertain network configurations are an open area of research. GAT-based policies showed the potential to handle unexpected topologies to a certain degree, showcasing resilience to graph changes at runtime; a capability not present in canonical MLP policies. It remains an open challenge to quantify the limits of this behaviour under structural constraints, although [32] has shown that generalisation is not guaranteed under discrepancies of local structure. A further consideration is the expectation of knowing the network layout beforehand, which is reasonable for defensive agents but might hold only partially in realistic scenarios. More broadly, while current openly available cyber defence simulators and emulators operate in idealised settings, our approach lays a promising foundation for future research, with potential extensions to real-world environments as simulation technologies continue to evolve and mature."}, {"title": "8 Conclusions", "content": "For our problem setting, we designed a realistic network interface with low-level features that are representative of information available in real-world networks. Our work based on a custom version of the CybORG simulator, and we believe this technique has the potential to be applied to further envi-ronments of similar complexity and higher realism. Our findings suggest that GATs show promise as defensive policies in graph-based cyber environments. They are capable of processing directed graph representations including low-level realistic features, such as detected open connections and subnet structures. While our policy architecture also demonstrated to operate over unexpected network con-nections generated by the simulator, the quantification of its resilience should be considered in future work under different simulator environments. Our GAT agent offers two main advantages: a degree of"}, {"title": "A Appendix", "content": "The blue agent has access to a low-level full-network observation at the start of each simulation by design. The table representation attempts to provide a simplified and human-friendly report of the real network state by dropping or aggregating lower-level CybORG information. The BLUETABLE construction is based on anomalies detected at each observation, when it differs from the initial baseline observation. In particular, the detected files and processes representing open connections are processed to update two features per host: Activity and Compromised. Activity is flagged as an Exploit if connections to a known malicious remote port are detected, or if there are more than two connections with only one local port open. On the other hand, Activity is marked as Scan if more than two connections and two local open ports are found, or if an anomaly was detected that is not an Exploit. The Compromised features are updated based on both detected anomalies and the previous actions of the blue agent. If the host was restored previously, Compromised is marked as No. If the previous action on the host was a Remove action, and the host is not Compromised, it is marked as Unknown. If the Activity of the host was identified as an Exploit, the Compromised level is assigned as User. If malicious files are detected, the Compromised status is marked as Privileged. An example of a BLUETABLE observation is shown in Table 4, and the equivalence to a one-dimensional vector space is showm in Table 3."}, {"title": "A.1 The Blue Table abstraction", "content": "The blue agent has access to a low-level full-network observation at the start of each simulation by design. The table representation attempts to provide a simplified and human-friendly report of the real network state by dropping or aggregating lower-level CybORG information. The BLUETABLE construction is based on anomalies detected at each observation, when it differs from the initial baseline observation. In particular, the detected files and processes representing open connections are processed to update two features per host: Activity and Compromised. Activity is flagged as an Exploit if connections to a known malicious remote port are detected, or if there are more than two connections with only one local port open. On the other hand, Activity is marked as Scan if more than two connections and two local open ports are found, or if an anomaly was detected that is not an Exploit. The Compromised features are updated based on both detected anomalies and the previous actions of the blue agent. If the host was restored previously, Compromised is marked as No. If the previous action on the host was a Remove action, and the host is not Compromised, it is marked as Unknown. If the Activity of the host was identified as an Exploit, the Compromised level is assigned as User. If malicious files are detected, the Compromised status is marked as Privileged. An example of a BLUETABLE observation is shown in Table 4, and the equivalence to a one-dimensional vector space is showm in Table 3."}]}