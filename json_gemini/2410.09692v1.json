{"title": "ALLORA: ADAPTIVE LEARNING RATE MITIGATES LORA FATAL FLAWS", "authors": ["Hai Huang", "Randall Balestriero"], "abstract": "Low-Rank Adaptation (LoRA) is the bread and butter of Large Language Model (LLM) finetuning. LoRA learns an additive low-rank perturbation, $AB$, of a pretrained matrix parameter $W$ to align the model to a new task or dataset with $W + AB$. We identify three core limitations to LoRA for finetuning-a setting that employs limited amount of data and training steps. First, LoRA employs Dropout to prevent overfitting. We prove that Dropout is only suitable for long training episodes but fails to converge to a reliable regularizer for short training episodes. Second, LoRA's initialization of $B$ at 0 creates a slow training dynamic between $A$ and $B$. That dynamic is also exacerbated by Dropout that further slows the escape from 0 for $B$ which is particularly harmful for short training episodes. Third, the scaling factor multiplying each LoRA additive perturbation creates \"short-sighted\" interactions between the LORA modules of different layers. Motivated by principled analysis of those limitations, we find an elegant solution: a Dropout-free, scaling-free, LoRA with Adaptive Learning rate-coined ALLORA. By scaling the per sample and per parameter gradients with a coefficient inversely proportional to parameters' $l2$ norm, ALLORA alleviates those three limitations. As a by-product, ALLORA removes two hyper-parameters from LoRA: the scaling factor and the dropout rate. Empirical results show that ALLORA admits better accuracy than LORA on various settings, including against recent LoRA variants such as Weight-Decomposed Low-Rank Adaptation (DORA). Ablation studies show our solution is the optimal in a family of weight-dependent / output-dependent approaches on various LLMs including the latest Llama3.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) (Hoffmann et al., 2022; Touvron et al., 2023; Jiang et al., 2023) are Deep Neural Networks (DNNs)-commonly built from Transformer with self-attention-built for sequence processing, e.g., Natural Language Processing (NLP). LLMs have radically changed the way we approach NLP (Chowdhary, 2020) by removing the need for handcrafted feature engineering such as bags of words (Zhang et al., 2010). Instead, current solutions directly operate on the input data-or a lossless compression known as tokens (Shibata et al., 1999). Because we now have access to humongous amount of text data, the standard training pipeline for LLMs take the following form. First, the LLM is pretrained on a large text corpus through next-token prediction. That autoregressive pretext-task enables the LLM to learn the underlying dynamic of the language. Commonly, RLHF is also employed after pretraining to make the model's behavior shift from autoregressive to agentic. Then, the LLM is fine-tuned on a more specific downstream task or dataset. That fine-tuning is user-specific and plays a fundamental role in making LLMs practically useful but relies on much more limited datasets, as we formalize below."}, {"title": "2 RELATED WORKS", "content": "LORA is a type of Parameter Efficient Fine Tuning (PEFT) designed to reduce the cost of finetuning, especially with LLMs. As LLMs typically have large number of parameters-in the scale of billions-one can not afford to finetune all those parameters on a particular downstream task or dataset. Existing PEFT can be divided into three categories, namely Adapter-based Methods, Prompt-based Methods, and LoRA.\nAdapter-based methods (Houlsby et al., 2019; He et al., 2022; Karimi Mahabadi et al., 2021a;b) introduce additional trainable modules, a.k.a. the adapters, into the original backbone whose weights are frozen during the finetuning. In Houlsby et al. (2019), linear modules were added in sequence to the existing layer, while in He et al. (2022), they were added in parallel to the existing layer for the sake of better performance.\nPrompt-based methods (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023) introduce soft tokens as trainable parameters and prepend them to the prompt. This category is the least intrusive as the finetuning can be done by only prompting the LLMs. However, prompt-based methods are in general sensitive to initialization and their overall effectiveness is affected.\nLORA (Hu et al., 2021) uses low-rank matrices to simulate weight changes of the pretrained weights. Since low-rank matrices can be merged back to original weights, LoRA does not incur any additional cost at inference, which is a significant advantage over the other two categories. Many variants were proposed lately. For example, in Zhang et al. (2023), SVD decomposition was employed to determine significance of singular values, and less important ones are pruned. Hyeon-Woo et al. (2022) applies low-rank Hadamard product to federated learning. Qiu et al. (2023) and Liu et al. (2024b) adopt orthogonal factorization and applied to diffusion models. Renduchintala et al. (2023) introduces weight tying and realizes more savings on number of parameters. A unified LoRA family was introduced for Stable diffusion in Yeh et al. (2024). Different combinations of LoRA are chosen for different tasks in Ponti et al. (2022). A scaling vectors is learnt to adjust a pair of frozen random matrices shared across layers in Kopiczko et al. (2023).\nMore recently, Liu et al. (2024a) proposes decomposing the weights into directional and magnitude components to boost accuracy. Hayou et al. (2024a) studies the optimal initialization of the low-rank matrices, and a follow-up work (Hayou et al., 2024b) proposes to apply different learning rate to different low-rank matrices. Superficially this is similar to one of our idea to adapt learning rate, though our idea is inspired by a principled study of dropout (Srivastava et al., 2014).\nMore broadly, Zhao et al. (2024) applies the low-rank concept to compute low-rank gradients directly. Jang et al. (2024) provides a study on the existence and convergence of LoRA solutions. And Zhang & Pilanci (2024) is a study of the potential ill conditioned low-rank matrices."}, {"title": "3 A CRITICAL ANALYSIS OF LORA FOR FINETUNING", "content": "PEFT is the main bridge between large pretrained LLMs and specialized practical use-cases. Hence, PEFT research is extremely active which led to numerous variations of LORA being developed. Our theoretical study builds on the seminal version formalized below, other LORA variants will be compared in our experimental sections 4.4 and 4.5.\nDefinition 1. (Low Rank Adapters (LoRA) from Hu et al. (2021)). For any weight matrix $W\u2208 Rn1xn2$ in the pretrained model, we constrain its update in the finetuning process by representing the latter with a low-rank decomposition $W = W + BA$. Here, only the"}, {"title": "3.1 FIRST FLAW: A STOCHASTIC REGULARIZATION THAT WILL NOT CONVERGE", "content": "LORA's regularization comes from Dropout (Srivastava et al., 2014), i.e., applying a multiplicative random binary mask at the bottleneck of the matrix factorization. The use of Dropout would seem justified since Wager et al. (2013) showed in the linear regime that Dropout acts as a variant of l2 regularization on normalized design matrix (inputs), a result also found in the previous study of Wang & Manning (2013). However, those theoretical results deal with expectations, i.e., infinite training time. Similarly, previous empirical studies of Dropout showed great regularization benefits, but all those studies only considered full training, i.e., long training episodes that ultimately converge to their expectation. We argue that those beneficial findings do not hold during fine-tuning which only employs limited training steps.\nTo understand the impact of Dropout in terms of regularization, we will consider a few variations of models and study the discrepancy between the expected benefit of Dropout with its empirical realisations. We will conduct validation on real dataset and LLMs to support our theory throughout the section, and in particular in section 3.4. Also, we consider here and in section 3.2 a/r to be 1 without loss of generality, none of our results are impacted by that constant scaling factor.\nLinear model with full training. Let's first consider the original setting of a linear model with Dropout applied to its predictions. That is, we consider the following Ordinary Least Squares (OLS) setting $||Y - (XW) V||$, with $Y \u2208 RN\u00d7C, X \u2208 RN\u00d7D, W \u2208 RD\u00d7C$ and the random realization of dropout matrix $V \u2208 {0, 1}N\u00d7C$. We note that such parametrization of Dropout is commonly employed in the literature to ensure that its expectation is equal to 1, and it is also PyTorch's official implementation. We have the following property that has motivated the use of Dropout through more than a decade by now:\n$E [||Y \u2013 (XW) \u00a9 V||}]$\n$= ||Y||} + E [-2Tr (Y (VT \u00a9 (XW)T)) + Tr ((VT \u00a9 (XW)) ((XW) \u00a9V))]$\n$= ||Y||} \u2013 2Tr (Y(XW)T) + Tr ((XW)(XW)) + (1-1) Tr ((XW)(XW)),$\nwhich is solved for $W = p(XTX)-1YX$. We note that we decomposed the loss into the terms that happen in the original (Dropout-free) setting, in blue, and the Dropout induces terms in orange. Comparing that with the usual Tikhonov regularization that produces $W = (X\u00ae X + XI)-1YTX$ we see then whenever the eigenvalues of $XTX$ are all identical to a positive constant c (e.g. when $X$ is whitened), $W = (XX)-1YTX$ hence recovering the dropout solution. Assuming c = 1 without loss of generality, we obtain that applying Dropout with rate p = 1\ud68c is equivalent to applying Tikhonov regularization with rate \u03bb. As a result, we see that if training for long enough, Dropout can efficiently replace explicit forms of regularization such as weight decay. While the above results recovers known theoretical analysis of Dropout-showing its benefits as an implicit regularizer-those derivations only emerge from taking expectation of the loss, i.e., considering infinite training steps. For us, the question thus turns into the following: what are the benefits of Dropout as a LoRA regularizer for very short training regime such as finetuning?\nLORA with Dropout fails to converge. Let's denote the LoRA linear finetuning setting as follows\n$L\u2261 ||Y \u2013 X LORAA,B(W)|| = ||Y \u2013 XW \u2013 ((XA) V)B||$.\n(1)"}, {"title": "3.2 SECOND FLAW: ZERO INITIALIZATION AND UNFAIR REGULARIZATION", "content": "The second flaw we uncover comes from the zero initialization of $B$. As we will see, that is a limitation regardless of employing Dropout or not, but Dropout exacerbates it.\nZero initialization implies imbalanced training dynamics. A peculiarity of LORA compared to most other deep learning framework lies in its asymmetric initialization. While $A$ is initialized with random entries, $B$ is initialized at 0. This initialization is intuitive when looking at the output of LoRA being 0 at first, i.e., one starts from the original model and then moves away from that if needed for the finetuning task. However, this seemingly reasonable initialization creates a strong imbalance in the training dynamic of $A$ and $B$. To see that, we can extend our derivation to obtain the derivative of the expected loss as\n$VAE [C] = \u2212 X (Y \u2013 XW) BT + X\u2122 XA (diag(||B1,2,..., ||B,:||) + \u0432\u0432\u0442),$\n$\u25bdBE [C] = \u2013 A\u00ae XT (Y \u2013 XW)$\n$+AXXA+(diag(||)(XA)1,2,..., diag(|| (XA)1,:||2, ..., || (XA)r,:||2)) \u0412,$\nhence the effective gradient norm for $A$ is 0 during the first few steps because of $B$ being 0, while the gradient norm for $B$ will be high $|| ATX (Y \u2013 XW) ||> 0$-regardless of the Dropout rate employed. While such issue only concerns the first few training steps, it is clear that it will be detrimental in a finetuning regime where the total number of training steps is limited. We note that if $A$ is zero-initialized instead of $B$, our entire argument still holds as the same slow training dynamic appears albeit with respect to $A$ instead of $B$. Our findings support the conclusion of Hayou et al. (2024b) that demonstrated how $A$ and $B$ should receive different learning rates to improve LoRA's performances.\nDropout further slows down the escape from 0. An additional issue arises when using Dropout. In that setting, the escape of $B$ from 0, which is needed for $A$ to also learn and for the LORA module to be effective, will be further slowed down. To better characterize that effect, let's study the close-form regularization impact of Dropout using the expected loss derived in section 3.1. As per the full training regime, we see that Dropout acts as a regularization on $A$ and $B$ based on their alignment with $X$. In fact, taking derivative with"}, {"title": "3.3 THIRD FLAW: RIPPLE EFFECT OF SCALING FACTOR", "content": "The third and last flaw we investigate deals with the scaling factor. While section 3.1 and section 3.2 considered it to be 1 without loss of generality, we now use back the value from definition 1, i.e., using n = as the Scaling Factor.\nThe scaling factor plays an important role to match $||BA||$ to a comparable level with $||W||$. Despite its effectiveness, the scaling factor creates a ripple effect across layers of a LLM and may make finetuning unstable. Hu et al. (2021) discussed the importance of the scaling factor and suggest to tune it carefully to prevent $BA$ from overwhelming $W$. From a different perspective, Houlsby et al. (2019) empirically showed the scale of the initialization of $BA$ can negatively impact validation accuracy. Later, Hayou et al. (2024a) argued that for the best performance, either $A$ or $B$ must be initialized at 0.\nTo illustrate the ripple effect, we adopt a multi-linear model which is a simplified version of the toy model in Hayou et al. (2024b).\n$f(x) = Wifi-1(x), l\u2208 [L]$\n(2)\nwhere $L\u2265 1$ is the number of layers. Applying LoRA at each layer gives\n$f(x) = (W\u03b9 + \u03b7\u0392\u03b9\u0391\u03b9) fi-1(x)$"}, {"title": "4 ALLORA: ESCAPING LORA'S FLAWS FOR FINE-TUNING", "content": "4.1 DECONSTRUCTING LORA\nSection 3 summarized three flaws of LoRA, which we show can be addressed by a single solution: ALLORA.\nFirst we establish the underlying links among dropout, scaling factor, and learning rate. Consider the LoRA finetuning of a single layer as in eq. (2), $f(x) = (W+\u03b7\u0392\u0391)x$. Following Hayou et al. (2024b) and without loss of generality, we can simplify the model by assuming $W = 0$, which is equivalent to defining \u1ef9 = y - Wx, and rewriting the loss function by \u1ef9. Also assuming \u03b7 = 1, we have $f(x) = BAx$. The goal is to minimize loss $L$ whose gradient is $g = af f(x) \u2208 Rn1$ is a column vector. Expanding it per row gives\n$(f(x)) = (BA),:x, i\u2208 [1]$\n(3)\nThe effect of dropping out $(f(x));$ for a given $i$ is equivalent to applying a per-row scaling factor ni = 0 to $(BA),:$. Note that this is true only for $(BA),:$, the effect on $(BA)j,;, j \u2260 i$ is slightly different. Since dnf(x)\ndf (x)\n=, Ni = 0 is implicitly applied to the i-th row of the gradient gi = ((BA));:, which is again a scaling factor applied to the learning rate 1.\nThe observation reveals that both scaling factor and dropout are adaptions on LoRA output f(x), and both have effects on gradient. We are inspired to formalize a general framework that subsumes both, within which we can use a principled approach to systematically discover novel solutions.\nDefinition 2. (Adaptive Learning) Consider a single layer linear model $f(x) = BAx$ with gradient $g(x) = , Let Output Adaptor be a function fo: Rn1 \u2192 Rn1, Gradient"}, {"title": "4.2 ALLORA: LESS HYPER-PARAMETERS AND MORE STABILITY FOR FINETUNING", "content": "Under the Adaptive Learning framework, scaling factor is define by fo = \u03ba : x \u2192 \u03b7x.\n$ff = \u03ba\u00b0 f = nf$\n$[\u1fb7 = \u03ba \u00b0 f = ng$\n(5)\nOne idea to reduce the ripple effect while keeping the positive effect of scaling factor is to force fo = I, while keeping \u011f intact, which is to use a larger learning rate \u03b7. 1. Nonetheless, a fixed learning rate cannot simultaneously achieve both fast escape from 0 and, once away from 0, measured discovery of optimal direction. We think an adaptive learning rate that is inversely proportional to $||(BA)i,:||$ is a good candidate to realize our idea. We use the function 1/\u221a$||(BA)i,:||$ + 1/\u03b7\u00b2 which reaches maximum \u03b7 at $||(BA)\u00bf,:|| = 0$ and then tapers down when $||(BA),:||$ increases (fig. 6).\nFormally, ALLORA is defined by\n$[ f = I \u00b0 f$\n$[\u011fi = 1/\u221a||(BA)i,:|| + 1/72 \u00b7 gi, i \u2208 [n1]$\nwhere n is a hyperparameter. Note that this does not introduce a new hyperparameter. We split learning rate into a constant base learning rate lo and y, and the effective learning rate is nlb.\nOne more implementation detail is the backward pass computes, in addition to the gradients of A and B, also the gradient of the input from layer below, and propagate which back to the layer below. We only modify the gradients of A and B, but not that of the input. This helps further restrict the changes within each layer and reduce ripple effect.\nTo quickly verify our idea, we add probing code to trace the L2 norm of row vectors of BA. As shown in fig. 5 Right, adaptive learning rate escapes from 0 rapidly, the speed matches with LORA with a learning rate that is \u03b7. Then it finds an approriate level and enters measured discovery of optimal directions. LoRA with a learning rate lower than \u03b7 can reach the same level, but at a much slower pace. The experiment is with Snowflake Arctic XS and Rotten Tomatoes.\nNote that ALLORA multiplies different scaling factors to different rows of BA's gradient stochastically (because A is stochastically initialized and BA is stochastically learnt). Intuitively it is a generalization of Dropout which multiplies binary factors (0 or 1) to different rows stochastically. We hypothesize that ALLORA may recover some regularisation effect of Dropout and invite researchers to find a theoretical proof."}, {"title": "4.3 A FAMILY OF ADAPTIVE SOLUTIONS", "content": "We adopt a principled approach to explore other reasonable designs that fall into the adaptive learning framework defined by definition 2."}, {"title": "4.4 EMPIRICAL VALIDATION: PERCEPTION TASKS", "content": "Our first set of experiments gauges the performance of ALLORA on perception tasks. Mainstream LLMs nowadays are mostly pretrained by next token prediction, which is good for generative tasks, but may not be a good fit for perception tasks such as Natural Language Understanding (NLU) and Sentiment Analysis (SA). In fact, we observe subpar accuracy when finetuning popular open-weight models for NLU and SA tasks (see table 6). We hope to show that ALLORA may help boost the accuracy for perception tasks.\nFor our experiments, we pick three midsized LLMs: Qwen2-0.5B, Snowflake-Artic-L, and OpenELM-450M, and four NLU and SA datasets: Bias in Bios, Emotion, Rotten Tomatoes, and Yelp Review. To demonstrate the stability of ALLORA, we run the experiments with various \u03b7\u00b2 \u2208 {1,2,4} with a fixed baseline learning rate l\u2081 = 1e-4. To be fair for LORA, we run LoRA at learning rate l \u2208 {1e \u2013 4, \u221a2e \u2013 4, 2e \u2013 4}, respectively. We finetune for 2 epochs. Each experiment is run 5 times and we report average final accuracy. Table 2 Left shows the accuracy gap between ALLORA and LoRA, where positive numbers indicate ALLORA has better accuracy. The result shows that ALLORA in general admits better accuracy over plain LoRA. Average improvement over all cases is 0.3%.\nIn the experiment, the dropout rate for ALLORA is 0.0 and that for LoRA is 0.05. We also run ALLORA+D, the version of ALLORA with Dropout, also at dropout rate 0.05. Table 2 Right shows that there is no evident difference between ALLORA and ALLORA+D, matching our theoretical result from section 3.1.\nSince n originates from in definition 1, we also run at various LoRA ranks r, and the results show that ALLORA's advantage is consistent across different r (table 1)."}, {"title": "4.5 EMPIRICAL VALIDATION: COMMONSENSE REASONING", "content": "We also compare ALLORA with DoRA (Liu et al. (2024a)), a recent LoRA variant that demonstrated superb performance over a range of PEFT methods. Since DoRA results are universally better than other PEFT methods, we only compare ALLORA to DORA. We run experiments on LLaMA-7B, LLaMA2-7B, and LLaMA3-8B on 8 Commonsense tasks. Following DoRA's setup, for each model, we run both ALLORA and ALLORA+D with LORA rank r\u2208 {16,32} and for 3 epochs. Table 3 shows that for all cases, either ALLORA or ALLORA+D has the best average accuracy. On average, ALLORA and ALLORA+D each boosted accuracy by 0.3% over DORA.\nNote that we run experiments with various n\u00b2 \u2208 {1,2,4} and report the best accuracy, this follows DORA's practices to run with various learning rate l \u2208 {1e \u2013 4, 2e-4} and report the best.\nIn table 3 we also report the number of trainable parameters as a percentage of the number of pretrained parameters. Since ALLORA does not introduce additional trainable parameters, its trainable parameters are slightly lower than that of DORA."}, {"title": "5 ABLATION STUDY", "content": "Using the same setup in section 4.4, we run experiments with ALLORA-OD, the output-dependent variant, and ASF-LORA, the adaptive scaling factor variant. We also run LoRA with comparable fixed scaling factors to form an objective baseline for ASF-LORA."}, {"title": "5.1 ALLORA-OD", "content": "Table 4 Top Left shows the accuracy gap between ALLORA and ALLORA-OD. A positive number indicates that ALLORA has better accuracy. Overall speaking, ALLORA has better accuracy than ALLORA-OD. But the difference is moderate, as average improvement over all cases is 0.4%.\nThe result matches our conjecture that stochastic noise experienced by ALLORA-OD might have dragged down accuracy at early epochs."}, {"title": "5.2 ASF-LORA AND LORA WITH FIXED SCALING FACTOR", "content": "Since a scaling factor on output is implicitly also a scaling factor on gradient, we use the same \u03b7 when comparing between ALLORA and ASF-LORA, i.e., the gradient adaptor f, in ALLORA and the output adaptor fo in ASF-LORA use the same n.\nTable 4 Top Right shows the accuracy gap between ALLORA and ASF-LORA. A positive number indicates that ALLORA has better accuracy. ALLORA has a significant advantage over ASF-LORA as average improvement over all cases is 1.1%. Since we know that ALLORA-OD is only slightly worse than ALLORA, the evidence leans toward that Adaptive Learning Rate is in general a better solution family than Adaptive Scaling Factor.\nWe also run LoRA at comparable fixed scaling factors \u2208 {1, \u221a\u221a2, 2}. The results, as shown in table 4 Bottom, show that"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "This paper identifies three major flaws of LoRA, namely dropout, zero-initialization, and scaling factor. We conducted principled analysis and proved that dropout is not a must-have in the finetuning regime. After uncovering the hidden connection between dropout, scaling factor, and learning rate, we proposed a unified adaptive learning framework to address them all: ALLORA. Empirical results show that ALLORA admits better accuracy than plain LoRA over multiple backbones, datasets, and learning rates; and better accuracy than recent successful LoRA variants such as DoRA. Ablation study shows that ALLORA is the optimal in a family of adaptive methods.\nWe list a few interesting research directions and invite researchers to explore the frontier opened-up by our research:\n\u2022 The adaptive learning framework introduced by this paper is generic and may find broad applications beyond LoRA. Other use cases such as pretraining may not have the constraints that the weight matrix must be initialized at 0. But they may have other types of constraints, which may be solved by adaptive learning with a different adaptor function.\n\u2022 Within the LoRA use case, we only examine one particular adaptor function, there could be other adaptor functions that have superior performance.\n\u2022 We only provide empirical evidence that ALLORA admits better accuracy and hypothesize that ALLORA is implicitly a regularization. Theoretical guarantee is needed, especially for the convoluted case where the base model has multiple layers.\n\u2022 Starting from 0 weights may avoid the lottery ticket hypothesis (Frankle & Carbin (2019)), for good or bad, where adaptive learning rate can be a handy tool."}]}