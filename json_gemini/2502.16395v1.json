{"title": "An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science", "authors": ["Qiuhai Zeng", "Claire Jin", "Xinyue Wang", "Yuhan Zheng", "Qunhua Li"], "abstract": "Large Language Models (LLMs) have demonstrated potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis. We propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows \u2013 the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions. Using this framework, we systematically evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting's potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available\u00b9.", "sections": [{"title": "1 Introduction", "content": "The advancement of large language model (LLM) analytical and quantitative reasoning abilities, such as solving math problems, writing code, and analyzing tabular data has sparked an interest in their potential for data science (Wang et al., 2024; Zhu et al., 2024; Gu et al., 2024). Recent work on LLMs in data science has explored methods to improve their accuracy, from prompting strategies to multi-agent frameworks to fine-tuning models (Liu et al., 2024; Majumder et al., 2024a; Zhu et al., 2024). Various benchmarks have also been developed to assess LLM performance across core data analysis tasks, including table understanding (Yu et al., 2018; Hazoom et al., 2021; Zhao et al., 2022), hypothesis testing (Liu et al., 2024; Zhu et al., 2024), and multi-step workflows (Majumder et al., 2024b). Unlike mathematics, where questions typically have single correct answers, data science is inherently open-ended and exploratory (Yu, 2020). Prior studies have shown significant variation across defensible approaches and outcomes among human expert analysts performing the same task (Breznau et al., 2022; Menkveld et al., 2024; Botvinik-Nezer et al., 2020). Given this variability, ensuring reproducibility in analysis workflows is crucial for independent verification of results using the same dataset and analysis pipeline (Davidson and Freire, 2008; Leek and Peng, 2015; National Academies, 2019). This practice fosters confidence in findings, facilitates knowledge sharing and reuse, and is widely advocated in scientific guidelines (Lee et al., 2022; National Academies, 2018). However, current methods for assessing computational reproducibility are predominantly manual and lack standardized, quantifiable approaches. The rise of LLMs as data science tools introduces new challenges to reproducibility. While LLMs excel at code automation and generating natural language reports (Mayfield et al., 2024; Rasheed et al., 2025), their stochastic outputs and model-specific variations can lead to inconsistencies in analysis results. Consequently, it remains unclear whether analyses generated by one LLM can be reliably reproduced by another LLM or a human analyst. Therefore, rigorously assessing the reproducibility of LLM-generated analyses is imperative to ensure that incorporating these tools into"}, {"title": "2 Related Work", "content": "Recent advancements in LLM code generation have garnered significant attention, exemplified by tools like GitHub Copilot (Chen et al., 2021) and CodeGeeX (Zheng et al., 2023), which leverage code LLMs to streamline software development. These advancements have spurred growing interests in LLM-driven code generation for data tasks (Zhu et al., 2024; Gu et al., 2024), driving progress in automating data manipulation, visualization, and statistical analysis (Hong et al., 2024; Guo et al., 2024b; Li et al., 2024). While prior work examines both general-purpose models (e.g., GPT-4 (OpenAI et al., 2024), Llama 3 (Grattafiori et al., 2024), Claude 3 (Anthropic, 2024)) and code-specialized variants (e.g., Code LLaMA (Rozi\u00e8re et al., 2024), DeepSeek-Coder (Guo et al., 2024a)), empirical evaluations in data analysis code generation remain dominated by studies of general-purpose models, particularly the GPT family (Hong et al., 2024; Gu et al., 2024; Li et al., 2024; Guo et al., 2024b)."}, {"title": "2.1 Methodologies and Frameworks", "content": "Recent work has explored a number of techniques to enhance LLM capabilities for data science tasks. For example, several works (Majumder et al., 2024b; Liu et al., 2024; Zhu et al., 2024) test for existing general-purpose prompting frameworks like chain-of-thought (Wei et al., 2022) and self-reflection (Shinn et al., 2024). Some propose more complex, data-science-specific paradigms \u2013 for example a hierarchical graph modeling framework (Hong et al., 2024) and a multi-agent framework for iterative revisions (Guo et al., 2024b) which are also implemented via prompting. Performance effects of fine-tuning LLMs on data science questions have also been benchmarked (Zhu et al., 2024)."}, {"title": "2.2 LLM-Assisted Data Science", "content": "As LLM-generated data analyses are increasingly integrated into real-world pipelines, recent research has focused on understanding their practical use, particularly how human analysts engage with the LLM assistants. TalkToEBM (Bordt et al., 2024) investigates how LLMs can provide interpretable model summarization and graph explanation, while CliniDSBench (Wang et al., 2024) presents an interactive LLM-based platform to streamline medical data analysis coding. Nascimento et al. (2024) evaluates popular interfaced LLM coding tools as data science programming tools. Nejjar et al. (2024) performs a systematic study of commercially available LLM coding tools used in real-world research for data analysis and visualization, finding that some tools had wrong and misleading analysis results due to difficulty picking up important details."}, {"title": "2.3 Datasets and Benchmarks", "content": "Several benchmarks have been introduced to assess LLM performance on data science and data reasoning tasks. DS-1000 (Lai et al., 2022) and ExeDS (Huang et al., 2022) explore generation of code for building statistical models and making visualizations, while additional benchmarks assess LLM data understanding without extensive code generation (e.g. tabular data understanding) (Zhao et al., 2022; Yu et al., 2018; Hazoom et al., 2021). More recently, DiscoveryBench, StatQA, DAEval, and QRData, (Majumder et al., 2024b; Zhu et al., 2024; Hu et al., 2024; Liu et al., 2024) build on prior works by targeting models' abilities to code multi-step, data-driven workflows using appropriate statistical reasoning and knowledge."}, {"title": "2.4 Reproducibility in LLMS", "content": "Existing research on the reproducibility of LLM outputs focuses on quantifying their consistency as evaluators (Lee et al., 2024) and question-answering tools (Lee and Kim, 2024). Additionally, concerns about the reproducibility of studies investigating LLMs have emerged due to their opacity and experimental design standardization(Vaugrante et al., 2024). However, to the best of our knowledge, no prior work has addressed reproducibility of LLM-generated data analyses. Thus, our work fills a critical gap by ensuring scientific rigor of LLM-assisted data analysis, which is becoming an integral part of modern research."}, {"title": "3 LLM Reproducibility", "content": "Our approach to evaluating and enforcing reproducibility in LLM-generated analyses is inspired by the scientific community's best practices: workflows must include all necessary details (e.g., data processing, model choices, parameters) alongside publicly released code to enable independent replication (National Academies, 2019). To hold LLM-generated analyses these standards, our approach introduces an independent LLM inspector who, given only the workflow and minimal context (e.g., data filenames), attempts to reproduce the results. Successful reproduction confirms the workflow provides sufficient detail for computational reproducibility. This approach is particularly relevant to LLM-assisted data analysis, where human oversight is crucial but detailed code review is time-consuming. Reproducible LLM solutions allow human inspectors to validate workflows directly, bypassing exhaustive code verification."}, {"title": "3.1 Reproducibility Framework", "content": "Our formal reproducibility framework is defined as follows. Let $\\mathcal{D}$ represent a data science task, consisting of input data, contextual information, and a data science question. Let $A$ be an analyst agent that generates analysis solutions according to a probabilistic distribution $f_A$. The function $f_A$ is induced by the model's architecture and learned parameters, from which its statistical reasoning and code generation capabilities emerge. When responding to task $\\mathcal{D}$, analyst $A$ produces a solution tuple $(\\mathcal{W}_A, C_A)$, where\n$\\bullet$ $\\mathcal{W}_A \\sim f_A(\\mathcal{W} | \\mathcal{D})$ denotes the workflow, encapsulating $A$'s reasoning and analysis plan.\n$\\bullet$ $C_A \\sim f_A(C | \\mathcal{D}, \\mathcal{W}_A)$ denotes the corresponding code implementation following $\\mathcal{W}_A$.\nTo assess the reproducibility of $A$'s solution, we introduce an independent inspector agent $I$ that evaluates whether the workflow $\\mathcal{W}_A$ allows $I$ to reproduce the analysis conclusion by generating a"}, {"title": "new code implementation $C_1$ from $\\mathcal{W}_A$, i.e.,", "content": "$C_1 \\sim f_I(C | \\mathcal{W}_A)$,\nwhere $f_I$ is $I$'s probabilistic distribution for generating code implementations given a workflow. We then define the criterion for a reproducible solution as follows.\nDefinition: Given a task $\\mathcal{D}$, workflow $\\mathcal{W}_A$ is reproducible if and only if $C_A \\equiv C_1$, where $\\equiv$ holds if $C_A$ and $C_1$ produce the same result with respect to $\\mathcal{D}$. If $\\mathcal{W}_A$ is reproducible, then its corresponding code implementation $C_A$ is also reproducible. Thus the analysis $(\\mathcal{W}_A, C_A)$ is reproducible. In our framework, $C_A \\equiv C_1$ is instantiated as functional equivalence of code, that is, the outcomes derived from their deterministic code execution, $O_A = O(C_A)$ and $O_I = O(C_1)$, are consistent, even if $C_A$ and $C_1$ differ texturally. This criterion represents the highest level of code similarity, i.e. consistency in program functionality, as discussed in (Zakeri-Nasrabadi et al., 2023)."}, {"title": "3.2 Reproducibility is Sufficient and Complete", "content": "Our reproducibility framework ensures that a workflow encapsulates all necessary information to generate functionally equivalent code while avoiding unnecessary complexity.\nTo see this, first apply our framework to a self-check scenario where the original agent $A$ also serves as the inspector. During inspection, $A$ regenerates code from its original workflow $\\mathcal{W}_A$, producing $C_A \\sim f_A(C | \\mathcal{W}_A)$. If $C_A \\equiv C_A^*$, then probabilistically,\n$f_A(C | \\mathcal{D}, \\mathcal{W}_A) = f_A(C | \\mathcal{W}_A)$,\nwhich implies that, given $\\mathcal{W}_A$, $C$ is conditionally independent of $\\mathcal{D}$, i.e. $C \\perp \\mathcal{D} | \\mathcal{W}_A$. This means $\\mathcal{D}$ contributes no additional information to $\\mathcal{W}_A$ for generating $C$, demonstrating that our framework ensures that $\\mathcal{W}_A$ sufficiently encapsulates all necessary information for reproducing functionally equivalent code within the same agent. To achieve reproducibility, our framework strengthens this sufficiency by introducing an independent inspection agent $I$ with potentially different $f_I$. This imposes a stricter reproducibility requirement:\n$f_A(C | \\mathcal{D}, \\mathcal{W}_A) = f_I(C | \\mathcal{W}_A)$.\nThis condition ensures $\\mathcal{W}_A$ does not rely on implicit assumptions specific to $A$, making reproducibility independent of agent internal knowledge \u2014 a common barrier to successful replication (National Academies, 2019). From a theoretical perspective, our criterion aligns with the principles of sufficiency and completeness in statistics (Casella and Berger, 2024), Sufficiency: $\\mathcal{W}_A$ captures all relevant information from $\\mathcal{D}$ required to generate $C_A$, ensuring the inspector does not need additional task-specific knowledge. Completeness: $\\mathcal{W}_A$ excludes extraneous details that could introduce inconsistencies or ambiguity. By enforcing both sufficiency and completeness, our reproducibility criterion ensures that a workflow is self-contained, independent of agent-specific biases, and suitable for independent verification while avoiding unnecessary complexity."}, {"title": "3.3 Reproduciblity Enhancing Prompts", "content": "To address Q2 and Q4, we propose two prompting strategies to elicit reproducibility in LLM-generated solutions, Reproducibility-of-Thought (RoT) and Reproducibility-Reflexion (RReflexion). (1) RoT builds on Chain-of-Thought (CoT) (Wei et al., 2022) prompting with the explicitly reproducibility-oriented instruction: \u201cMake sure a person can replicate the action input by only looking at the workflow, and the action input reflects every step of the workflow.\u201d By building directly on CoT, RoT will allow us to assess the effect of reproducibility instruction in a controlled manner. (2) RReflexion, inspired by (Shinn et al., 2024), is an iterative strategy utilizing our framework's inspector reproducibility evaluation as an external feedback signal to the CoT analyst. This emulates the situation depicted in Figure 1, where the reproducibility check fails, and the AI analyst is asked to redo or revise the analysis. These techniques are further discussed in Section 4.2."}, {"title": "4 Experiment Design", "content": "To evaluate the performance of LLMs in statistical analysis, we compiled a diverse set of 1,032 question-answer (QA) pairs (details in Table 1) sourced from three data science benchmarks: DiscoveryBench (Majumder et al., 2024b), QRData (Liu et al., 2024), and StatQA (Zhu et al., 2024). (1)"}, {"title": "4.2 LLM Prompting Strategies and Models", "content": "On the aforementioned data, we evaluated the following LLMs in combination with each of the below prompting strategies.\nProprietary LLMs: GPT-40 (GPT-40-2024-11-20) (OpenAI et al., 2024), Claude-3.5-sonnet (v2) (Anthropic, 2024), o3-mini (o3-mini-2025-01-31) (OpenAI, 2025)\nOpen-source LLMs: Llama-3.3-70B (Grattafiori et al., 2024), DeepSeek-R1-70B (Guo et al., 2025)\nPrompting strategies (prompts in Appendix B):\nChain-of-Thought (CoT) follows the approach introduced in Wei et al. (2022), using the line \"let's think step-by-step\" to elicit reasoning.\nReproducibility-of-Thought (RoT), described in Section 3.3, instructs the LLM to incorporate reproducibility in its CoT reasoning while generating the workflow and code. Reproducibility-Reflexion (RReflexion) incorporates the inspector's reproducibility assessment as external feedback for re-analysis via CoT reasoning when the initial analysis was irreproducible.\nReAct, introduced by Yao et al. (2022), iteratively alternates between workflow design, code generation, and code execution to conduct multi-turn data analysis.\nIn all the above prompts, we provide essential context, including detailed dataset descriptions and metadata, and any available relevant domain knowledge. All prompting strategies were equipped with a code execution tool. Maximum number of LLM calls and code executions per sample for each prompting strategy are in Appendix Table 3."}, {"title": "4.3 Evaluation and Performance Analysis", "content": "We assessed performance through accuracy and reproducibility. To compute accuracy, analysis correctness was determined by comparing the LLM analysis conclusions to the benchmark ground truth. For numerical answers, the LLM conclusion agrees with the ground truth if the deviation between LLM conclusion and the ground truth is within a predefined error threshold (Appendix C). As the majority of LLM conclusions and ground truth answers are presented in natural language, we employ an evaluator LLM (GPT-40-2024-11-20) to assess agreement with the ground truth. Reproducibility is evaluated via the procedure detailed in Section 3 with GPT-40-2024-11-20 as the inspector agent. Prompts for the answer agreement evaluation and reproducibility inspection are detailed in Appendix C. To ensure the trustworthiness of LLM accuracy evaluations and reproducibility inspections, a human expert manually verified the LLM assessment for a randomly selected subset of 350 samples (details in Appendix D). For Llama-3.3, GPT-40, Claude-3.5-sonnet, and o3-mini, we used a regex parser to extract the workflow, code, and conclusion from the LLM output. For DeepSeek-R1-70B, the parser extracted the code and conclusion, while all reasoning text preceding the code was treated as the workflow. For CoT and RoT, which generate a single iteration of workflow and code, reproducibility was assessed on that output. For RReflexion and ReAct, which are iterative, only the final workflow and code were evaluated. Solutions with inexecutable code were considered inaccurate and irreproducible. To assess the impact of LLM choice, prompting strategy and task specification (i.e. benchmark"}, {"title": "4.4 Human Data Collection", "content": "Data analysis tasks are inherently exploratory and open-ended, leading to variability in solutions a well-documented phenomenon in prior studies of human analysts (Botvinik-Nezer et al., 2020; Breznau et al., 2022). Thus, to fairly assess the overall performance of LLMs, we establish a human analyst baseline by collecting expert solutions on the DiscoveryBench dataset. Two experienced postgraduate data analysts independently solved half of the tasks following the instructions in Appendix Table 4. Each analyst spent approximately 120 hours developing comprehensive workflows and code solutions. We assessed these solutions using the methodology outlined in Section 4.3."}, {"title": "5 Results", "content": "We present accuracy, reproducibility, and the proportion of executable code for solutions generated by each LLM and prompting strategy combination across the three benchmark datasets in Appendix Tables 5 and 6. Comparing performance across the three benchmark datasets, all LLMs performed best on StatQA (mean accuracy 75.91%), followed by QRData (54.9%), with the lowest performance on DiscoveryBench (28.72%). The differences in accuracy"}, {"title": "5.2 Impact of Prompting Strategies", "content": "Compared with CoT, both RoT and RReflexion resulted in substantial improvements in reproducibility rate (Figure 3 and Appendix E). For example, on DiscoveryBench, switching from CoT to RoT with GPT-40 raised overall reproducibility from 42.68% to 48.54%, while RReflexion pushed it even further to 58.58%. Similarly, for o3-mini on DiscoveryBench, reproducibility increased from 55.23% (CoT) to 60.25% (RoT), peaking at 71.55% with RReflexion. Regression analysis, as described in Section 4.3, confirmed that RReflexion significantly enhanced reproducibility (coefficients in Appendix Table 7). In particular, across all datasets, the RReflexion and o3-mini combination consistently achieved the highest reproducibility rate. The RoT and RReflexion strategies not only improved reproducibility, but also enhanced accuracy across most LLM-dataset combinations. For instance, on QRData, GPT-40 achieved accuracies 48.85% (CoT), 50.89% (RoT), and 51.15% (RReflexion), while o3-mini scored 58.52%, 62.09%, and 59.54%, respectively. Notably, the accuracy gains over CoT were more pronounced on the simpler benchmarks, QRData and StatQA, than on the more complex DiscoveryBench. This suggests that while encouraging consistency between the generated code and the steps in the workflow boosts accuracy, it does not fully compensate for reasoning gaps in more complex tasks. Therefore, enhancing the statistical reasoning that informs the workflow remains crucial for achieving higher accuracy in these cases. Consistent with prior work (Yao et al., 2022), we found that the ReAct prompting strategy often improved accuracy over CoT. This benefit was especially noticeable for Claude-3.5-sonnet and DeepSeek-R1-70B. However, ReAct"}, {"title": "5.3 Reproducibility-Accuracy Relationship", "content": "As briefly discussed in Section 5.2, our results suggest that reproducibility enhances accuracy. Across all datasets and LLMs, we observed that reproducible workflow and code solutions (R = 1) achieved significantly higher accuracy compared to those deemed irreproducible (R = 0) . This trend suggests a positive correlation: when a workflow supports the independent generations of functionally equivalent codes, it is more likely to produce accurate results. The observed improvements of RoT over CoT demonstrate a causal link between explicit reproducibility instructions and improved accuracy. Additionally, a significant (two sample paired t-test p-value of 0.007) 4.15% average accuracy gain in samples reanalyzed by RReflexion after incorporating the inspector reproducibility feedback further underscores reproducibility as a strong proxy for solution quality and accuracy, addressing Q2."}, {"title": "5.4 Performance Across Question Types", "content": "To investigate how different statistical question types affected performance (Q3), we analyzed"}, {"title": "5.5 Human vs LLM Comparison", "content": "Comparing human expert solutions with LLM solutions on DiscoveryBench, we observed a notable performance gap in overall accuracy (Appendix Table 5). Human experts achieved an overall accuracy of 66.53% and a reproducibility rate of 66.53%, albeit at the cost of approximately 240 hours of total effort. In contrast, while LLMs generally lagged behind human experts in overall accuracy (23.01%-35.56%), certain configurations exhibited strong reproducibility. For example, o3-mini with RReflexion achieved a reproducibility rate of 71.6%, surpassing that of the human experts. Moreover, the best LLM performance was often obtained using advanced prompting strategies (i.e., ROT, RReflexion, or ReAct), suggesting that future enhancements in prompt design and reproducibility practices could help narrow the gap between automated and manual approaches."}, {"title": "6 Conclusion", "content": "We propose a novel analyst-inspector framework to automatically evaluate and enforce reproducibility of LLM-generated data analysis solutions. We systematically evaluated open-source and proprietary models across diverse benchmarks, contextualizing their performance against human analysts. Our findings highlight reproducibility as critical to the quality and trustworthiness of LLM-generated analyses, addressing four critical questions that inform their integration into real-world data analysis pipelines. We find that reproducibility directly enhances accuracy, validating this relationship through our RoT prompting strategy, which builds on CoT by prioritizing reproducibility. Through our RReflexion prompting strategy, we show that enabling the analyst agent to integrate the inspector agent's reproducibility feedback into analysis revisions further improves reproducibility and accuracy. Among the five LLMs assessed, o3-mini and Claude-3.5-sonnet produced the most accurate analyses, with o3-mini achieving the highest reproducibility. We identify prompting strategies that optimize accuracy, reproducibility, and code execution rates in different analytical contexts. Moreover, we identify specific statistical tasks and question-answer types where LLMs underperform, highlighting a need for improved analytical and statistical reasoning. Building on these findings, we recommend future research to focus on enhancing the statistical knowledge, code generation reliability, and analytical reasoning abilities of LLMs. Research into agentic systems to evaluate workflow correctness and directly check workflow-code alignment can also provide significant value. Finally, our analyst-inspector framework offers a blueprint for applying reproducibility in other technical domains beyond data science to foster transparency and trust."}, {"title": "7 Acknowledgements", "content": "This work is partially supported by NIH R01 GM109453 to QL."}, {"title": "8 Limitations", "content": "Our study has three primary limitations.\n(1) Despite our extensive evaluation of various prompting strategies, we did not investigate the potential benefits of fine-tuning LLMs.\n(2) Although we employed an independent inspector agent to mitigate inherent biases in LLM evaluations of reproducibility, and manually verified its correctness, this approach cannot guarantee complete elimination of bias.\n(3) While the benchmark datasets we selected cover a broad range of data science tasks, they may not fully capture the complexity and breadth of real-world data analysis scenarios. We also note that data analysis tasks are sometimes open-ended. Our evaluations in this study rely solely on predefined benchmark answers as the ground truth when measuring accuracy. Though we also collected and assessed human expert solutions to these benchmark tasks as a baseline for fair judgement of LLM performance, we acknowledge that evaluating against multiple correct ground truths would better capture the inherent flexibility of data science problems."}, {"title": "9 Ethics Statement", "content": "Existing Benchmark Licenses. Our work builds upon publicly available datasets and models, ensuring compliance with their respective licenses. Below, we detail the licensing terms for the benchmarks we used and our own contributions.\n$\\bullet$ DiscoveryBench (Majumder et al., 2024b): Licensed under ODC-BY, permitting redistribution and modification with proper attribution.\n$\\bullet$ QRData (Liu et al., 2024): Licensed under CC BY-NC 4.0, allowing non-commercial use with attribution.\n$\\bullet$ StatQA (Zhu et al., 2024): Licensed under GPL-3.0, requiring derivative works to adopt the same license. Our use of these datasets adheres to the terms specified by their respective licenses, and we use them strictly for research purposes. Codebase License. We release our code under the MIT License, granting users permission to use, modify, and distribute the code with proper attribution. This choice aligns with our goal of advancing reliability of LLM-generated data analysis and reproducibility of scientific research. Potential Risks. Our evaluation indicates that current LLMs are not yet fully reliable in generating reproducible and accurate data analyses. We suggest that users carefully review any LLM-generated solutions before using them to automate data analysis tasks. Additionally, while reproducible analyses are more likely to be correct, it is possible that both the analyst and inspector agents are incorrect in consistent ways. Thus, our framework is meant for human-in-the-loop systems as detailed in our motivation and in Figure 1."}, {"title": "H Use of AI Assistants", "content": "We used GitHub Copilot for code completion and Anthropic's Claude web interface to assist with debugging. Additionally, we used OpenAI's ChatGPT web interface to refine the clarity and smoothness of our writing. All AI-generated content, whether in code or text, was carefully reviewed, edited, and validated by the authors to ensure accuracy and alignment with our research objectives. No AI-generated content was used without human verification, and all final decisions regarding implementation and manuscript writing remained with the authors."}]}