{"title": "Activation Scaling for Steering and Interpreting Language Models", "authors": ["Niklas Stoehr", "Kevin Du", "V\u00e9steinn Sn\u00e6bjarnarson", "Robert West", "Ryan Cotterell", "Aaron Schein"], "abstract": "Given the prompt \"Rome is in\", can we steer a language model to flip its prediction of an incorrect token \"France\" to a correct token \u201cItaly\u201d by only multiplying a few relevant activation vectors with scalars? We argue that successfully intervening on a model is a prerequisite for interpreting its internal workings. Concretely, we establish a three-term objective: a successful intervention should flip the correct with the wrong token and vice versa (effectiveness), and leave other tokens unaffected (faithfulness), all while being sparse (minimality). Using gradient-based optimization, this objective lets us learn (and later evaluate) a specific kind of efficient and interpretable intervention: activation scaling only modifies the signed magnitude of activation vectors to strengthen, weaken, or reverse the steering directions already encoded in the model. On synthetic tasks, this intervention performs comparably with steering vectors in terms of effectiveness and faithfulness, but is much more minimal allowing us to pinpoint interpretable model components. We evaluate activation scaling from different angles, compare performance on different datasets, and make activation scalars a learnable function of the activation vectors themselves to generalize to varying-length prompts.", "sections": [{"title": "1 Introduction", "content": "Understanding which components of a language model play which roles in which tasks is a core aim of mechanistic interpretability. Given the prompt Rome is in, for instance, one might ask which components of the model most influence it to favor Italy over some incorrect answer token, such as France. In addressing this question, a natural axiom is that a given component can only be understood as influential for a given task if intervening on it meaningfully alters the model's task-specific behavior.\nBuilding on this basic axiom, a growing literature seeks to both generate and test hypotheses about where certain behaviors are localized in a model by employing targeted interventions with methods such as activation patching (Lakretz et al., 2019; Vig et al., 2020; Meng et al., 2022), among others. Studies in this literature often produce a set of attribution scores associated with various locations in the model which represent how much the model's output changed after editing the activation vectors at each location. Although an effective intervention may be necessary to believe a given localization hypothesis, it is not sufficient, as interventions to other model locations may be similarly effective. Indeed, recent work has questioned the relationship between interpretability and intervention on this basis, and has advocated for more rigorous and deliberate methodology for connecting the two (Hase et al., 2023; Wang and Veitch, 2024; Hanna et al., 2024).\nA parallel literature on model steerability also seeks to develop effective interventions, not for the primary purpose of interpretability, but to steer models toward desirable behaviors, like factuality (Li et al., 2023), or away from undesirable behaviors, like toxicity (Ilharco et al., 2023; Turner"}, {"title": "2 Transformer Language Models", "content": "Let \u2211 be an alphabet of tokens, a finite, non-empty set, and let 2* be the set of all strings with tokens drawn from \u03a3. A language model p is a probability distribution over \u2211*. As is current practice, most language models are defined autoregressively. Let v = v1 \u00b7\u00b7\u00b7 21 \u2208 \u03a3* be a string; then the autoregressive factorization of p is given by\n$p(v) = p(EOS | v) \\prod_{i=1}^{I} p(v_i | v_{<i}).$ (1)\nEach local conditional distribution $p(\u00b7 | v_{<i})$ is a distribution over\u2211 = \u2211UEOS, where EOS & \u2211 is the end-of-string token. In the context of an autoregressive language model, we call $v_{<i}$ a prompt.\nLet $x_n \u2208 \u03a3*$ be a prompt of length $I_n$ and $Y_n \u2208 \u03a3$ the next token. A common way to define the local conditional is via the softmax function \u03c3 which maps from $R^{|\u03a3|}$ to the probability simplex $\u2206^{|\u03a3|\u22121}$:\n$p(Y_n | X_n) = \u03c3(f(x_n))_{y_n} = \\frac{exp (f(x_n)_{y_n})}{\u03a3_{y'\u2208\u03a3} exp (f(x_n)_{y'})},$ (2)\nwhere $f(x_n) = E h_{1...I_n}$ defines the logit function $f: \u03a3^* \u2192 R^{|\u03a3|}$ of a language model, where $E \u2208 R^{|\u03a3|\u00d7D}$ is the projection (or unembedding) matrix, and where $h_{1...I_n} \u2208 R^D$ is the activation vector at the final model layer L and last token position $I_n$ of the prompt.\nMost state-of-the-art language models rely on the Transformer architecture (Vaswani et al., 2017) to compute f. Transformers are composed of L lay-"}, {"title": "3 Activation-Level Interventions", "content": "We focus on a class of interventions which modify one or more of the activation vectors in Eq. (3).\nThis level of abstraction is motivated by our desire to interpret larger components of the Transformer. However, we note that the granularity at which we seek to intervene and understand the model is a choice which depends on specific use cases. Our intervention targets a set of layer indices L, token positions I, and sites S. We denote the Cartesian product of the layer indices, token positions and sites, K = L \u00d7 I \u00d7 S, as the intervention points."}, {"title": "3.1 Choosing Intervention Points", "content": null}, {"title": "3.2 Defining an Intervention", "content": "We intervene on the activation vectors of the model f(xn) by specifying an intervention $f_\u03b8(x_n)$ that involves a set of learnable parameters \u03b8 and a hyperparameter $\u03b2\u2208 R$ which controls the strength"}, {"title": "Additive Vectors", "content": "We first consider an intervention based on steering vectors (STEERVEC). Specifically, we define a set of intervention parameters $\u03b8 = {v_{l,i}^{(s)}}_{(l,i,s)\u2208K}$ that associate a vector $v_{l,i}^{(s)}$ with each intervention point. The intervention adds this vector to its corresponding activation vector:\n$h_{l,i}^{(s)} = h_{l,i}^{(s)} + v_{l,i}^{(s)}.$ (4)"}, {"title": "Multiplicative Scalars", "content": "Applying an intervention vector modifies both the direction and magnitude of the activation vector. We instead propose a more parameter-efficient intervention which is restricted to scaling the signed magnitude of each activation vector via a single multiplicative scalar. This approach, which we call activation scalars (ACTIVSCALAR), is given by\n$h_{l,i}^{(s)} = h_{l,i}^{(s)}(1 + \u03b2 \u03b1_{l,i}^{(s)}),$ (5)\nwhere $\u03b8 = {\u03b1_{l,i}^{(s)}}_{(l,i,s)\u2208K}$ are the parameters."}, {"title": "3.3 Learning an Intervention", "content": "What qualities does an interpretable intervention possess? In this article, we focus on interventions that are effective, faithful, and minimal, drawing on analogous concepts established in the Transformer circuits literature (Wang et al., 2023; Bhaskar et al., 2024).\nWe intervene on the model to steer its prediction on a selected task with data points $T = {(x_n, c_n, w_n)}_{n=1}^{N}$. For each data point, the model is prompted by xn to choose between two competing answer tokens $c_n, w_n \u0395 \u03a3$. The answer tokens are selected such that cn and wn always represent correct and wrong continuations of the prompt, re-"}, {"title": "Effectiveness", "content": "A popular objective for learning interventions is the logit difference between candidate tokens: $f(x_n)_{c_n} - f_\u03b8(x_n)_{w_n}$. We extend this objective to allow the sign of \u03b2 to control the sign of the logit difference. Concretely, we want the logit of cn to be larger than that of wn by some margin m \u2265 0 when setting \u03b2 > 0 and smaller for \u03b2 < 0. Define $f_1(x_n)$ to be the intervention with \u03b2 = 1, and $f_{-1}(x_n)$ to be the intervention with \u03b2 = -1. The following objective then encourages learned interventions to yield both $f_1(x_n)_{c_n} > f_1(x_n)_{w_n}$ and $f_{-1}(x_n)_{c_n} < f_{-1}(x_n)_{w_n}$:\n$E_m(\u03b8,T) = \\frac{1}{N} \u03a3_{n=1}^{N} [max (0,f_{-1}(x_n)_{w_n} - f_1(x_n)_{c_n} + m) + max (0, f_1(x_n)_{c_n} - f_{-1}(x_n)_{w_n} + m)].$ (6)"}, {"title": "Faithfulness", "content": "We say an intervention is faithful if it only affects the answer tokens (Wang et al., 2023; Hanna et al., 2024). We promote faithfulness via the following objective\n$F(\u03b8, T) = \\frac{1}{N} \u03a3_{n=1}^{N} D_{KL}(\u03c3(f(x_n)) || \u03c3(f_\u03b8(x_n))) + D_{KL}(\u03c3(f_{-1}(x_n)) || \u03c3(f(x_n))).$ (7)\nwhere DKL is the Kullback\u2013Leibler divergence."}, {"title": "Minimality", "content": "Thirdly, the intervention should be minimal (Wang et al., 2023), which we promote with the following regularizing term\n$M_p(\u03b8) = - \u03bb_M ||vec(\u03b8)||_p$ (8)\nwhich penalizes a (pseudo)norm of the parameters. Here, vec maps the set of parameters \u03b8 to a vector, and the subscript p indicates which (pseudo)norm of the vector is penalized. Setting p = 0 corresponds to l0-regularization, which encourages sparsity directly but is difficult to optimize. We instead take p = 1, which corresponds to l1-regularization, a widely-studied and effective convex relaxation of l0-regularization, which forms the basis of the sparsity-inducing LASSO method (Tibshirani, 1996)."}, {"title": "Gradient-based Parameter Learning", "content": "Putting it all together, we directly optimize for an intervention that is simultaneously effective, faithful, and minimal. Specifically, we choose intervention parameters using gradient-based optimization on the multi-term objective\n$\u03a8(\u03b8,T) = E_m(\u03b8,T) + \u03bb_F F(\u03b8, T) + \u03bb_M M_1(\u03b8).$ (9)\nWe can tune the hyperparameters $\u03bb_F > 0$ and $\u03bb_M \u2265 0$ to control the degree to which the three terms in the objective trade off. The margin m \u2265 0 constitutes a third hyperparameter that controls the strength of the effectiveness term."}, {"title": "3.4 Evaluating an Intervention", "content": "We can evaluate an intervention based on our operationalizations of effectiveness, faithfulness and minimality. To evaluate effectiveness, we set the margin m = 0 to obtain a metric ranging from -\u221e to 0, where E0(\u03b8,T) = 0 indicates that an intervention always successfully flips the answer tokens. The faithfulness objective, which also ranges from -\u221e to 0, can be treated as an evaluation metric without any modification. Finally, to evaluate minimality, we count the number of non-negligible intervention parameters-i.e., those taking values sufficiently far from 0. In practice, we consider absolute values less than 0.01 to be negligible."}, {"title": "4 Experiments", "content": "We fit the two intervention methods, ACTIVSCALAR and STEERVEC, on the three-term objective in Eq. (9) and conduct evaluations for effectiveness, faithfulness and minimality."}, {"title": "4.1 Tasks", "content": "We consider two synthetic tasks presented in Tab. 1. The Country-Capital Conflicts (CCC) task, designed by Du et al. (2024), prompts models to resolve an entity-based knowledge conflict (Longpre et al., 2021) which pits information provided in-context against prior parametric knowledge that models can be assumed to have acquired during training. The Indirect Object Identification (IOI) task, which we adapt slightly from Wang et al. (2023), prompts models to choose which of two tokens is the indirect object in sentences with potentially complex syntactic structure. For both tasks, we select prompts to be of the same length In and ensure the candidate answers are single tokens."}, {"title": "4.2 Quantitative Results", "content": "We do not expect there to be a single solution which is optimal for all three objectives. An optimally effective intervention might not be very faithful, while a highly minimal intervention but not be very effective. We seek to understand the trade-off between the different intervention desiderata by finding Pareto-optimal solutions. To this end, we run a grid search over the hyperparameters AF, A\u2122 and m. We evaluate the learned interventions on the test set and visualize the Pareto frontier in Fig. 2."}, {"title": "Effectiveness versus Faithfulness", "content": "We find that ACTIVSCALAR and STEERVEC perform comparably when it comes to trading off effectiveness and"}, {"title": "Effectiveness versus Minimality", "content": "We find that ACTIVSCALAR is generally more minimal on multiple levels. For CCC and IOI, ACTIVSCALAR accounts for about 85% of the points on the effectiveness-minimality Pareto frontier in Fig. 2. Thus, ACTIVSCALAR learns interventions that are in fact more parsimonious. This is in addition to having far fewer learnable parameters. As a simple illustration, consider learning an intervention on the L = 48 layers of GPT2-XL, for a prompt consisting of In = 19 tokens. If we only intervene on a single site per layer-e.g., residPost, where the activation vector has dimensionality D = 1600\u2014then STEERVEC has 19 \u00d7 48 \u00d7 1600 = 1,459,200 learnable parameters while ACTIVSCALAR has only 48 \u00d7 19 = 912 learnable scalars. ACTIVSCALAR is also more minimal qualitatively, as it is limited to affecting only the signed magnitude of the activation vectors without otherwise affecting their direction."}, {"title": "4.3 Interpretation of Scalars and Vectors", "content": "As shown in Fig. 3, activation scalars highlight task-relevant locations while performing effective interventions at the same time. The norm of activation vectors at the correct in-context tokens is increased, while it is decreased at the incorrect in-context tokens."}, {"title": "Activation Scalars", "content": null}, {"title": "Steering Vectors", "content": "We seek to better understand the properties of the learned steering vectors by analyzing their change in norm and direction in terms of cosine distance before and after training. As visualized in Fig. 4, the change in norm reveals a structured, interpretable pattern, while the directional change appears more arbitrary. In fact, when"}, {"title": "5 Extension to Variable-Length Prompts", "content": "ACTIVSCALAR and STEERVEC, as well as most existing interpretability methods such as activation patching, learn intervention parameters that are tied to specific token positions i in the prompt In. To reuse learned parameters for intervention with a new prompt, the test prompt must match the length In of the training prompt, and ideally match it syntactically. Existing work circumvents the requirement of matched prompts by intervening only on the last token position (Yu et al., 2023; Li et al., 2023; Jin et al., 2024; Stoehr et al., 2024a), or on token positions that can be easily aligned across prompts, like the main verb, or the last token of the subject (Meng et al., 2022; Geva et al., 2023; Ortu et al., 2024; Merullo et al., 2024)."}, {"title": "Dynamic Activation Scalars", "content": "We propose an extension of ACTIVSCALAR which defines each scalar to be a function of the corresponding activation vector. Each function is tied to a particular layer l and site s, but shared across token positions i, and parameterized by a (column) vector $g_l^{(s)} \u2208 R^D$. The set of learnable intervention pa-"}, {"title": "6 Related Work", "content": null}, {"title": "6.1 Repurposing Interpretability Methods", "content": "There exist various methods producing activation-level attribution scores based off a logit difference metric between two answer tokens. Activation patching (ACTIVPATCH; Lakretz et al., 2019; Vig et al., 2020; Meng et al., 2022) swaps activation vectors in one forward pass with activation vectors from another (corrupted) forward pass to study (causal) effects on the logit difference. Attribution patching (ATTRPATCH; Nanda, 2023a; Syed et al., 2023; Kram\u00e1r et al., 2024) represents a gradient-based, efficient approximation of ACTIVPATCH. Direct logit attribution (DLA; Elhage et al., 2021) identifies activation vectors whose directions are correlated with the vectors of the answer tokens in the projection matrix.\nAll three methods produces attribution scores for"}, {"title": "6.2 Other Related Work", "content": null}, {"title": "Transformer Circuits", "content": "Joint interpretability and steerability is also a desired property in circuit discovery that seeks to identify a minimal subgraph responsible for the behavior of the full model when solving a specific task (Wang et al., 2023; Bhaskar et al., 2024). Isolating the subgraph typically requires thresholding the attribution scores associated with model components and then zeroing out (De Cao et al., 2022; Wang et al., 2023; Conmy et al., 2023; Syed et al., 2023) or corrupting them (Geiger et al., 2021; Bhaskar et al., 2024). However, this means that the intervention can be considered discrete, as it cannot smoothly facilitate different intervention strengths and directions. The scaling aspect of the \u03b2 hyperparameters in ACTIVSCALAR and STEERVEC, in turn, is continuous."}, {"title": "Gradient-Based Steering and Interpretability", "content": "This work relies on gradient-based optimization to localize model components that are relevant with respect to a specifically designed objective. This is similar to Subramani et al. (2022); Hernandez et al. (2024) that learn steering vectors to edit activation vectors. Other related work analyzes the direction and magnitude of weight or activation gradients (Du et al., 2023; Stoehr et al., 2024b; Katz et al., 2024) to identify task-relevant components."}, {"title": "Pruning, Masking and Adapters", "content": "Learning activation scalars bears resemblance to work on pruning neural networks (Li et al., 2021), fine-tuning (low-rank) adapters (Houlsby et al., 2019; Hu et al., 2021) and (hard) masking (Louizos et al., 2018; Bondarenko et al., 2023). In this work, however, we do not pursue the typical goals of the pruning literature, which often focuses on reducing the computational cost of inference. Instead, ACTIVSCALAR can be seen as learning a soft mask that strengthens or weakens components for the purpose of obtaining an interpretable map of locations."}, {"title": "7 Conclusion", "content": "We show that scaling the signed magnitude of a few relevant activation vectors is often sufficient to flip a model's prediction between a correct and a wrong answer token. Besides being effective at steering, activation scaling requires many fewer parameters"}, {"title": "Limitations", "content": "STEERVEC, ACTIVSCALAR and DYNSCALAR are controllable via different hyperparameters: the margin m in the effectiveness objective, AF weighing the faithfulness term and A\u2122 weighing the strength of the l\u2081-regularization. There are additional training-related hyperparameters such as the learning rate, the number of epochs, the batch size, the number of data instances, and the standard deviation of the Gaussian noise initialization of the intervention parameters, that have a strong influence on the results. For instance, increasing the noise or the margin, training for more epochs, or weakening the l\u2081-regularization, results in activation scalars that deviate more from zero.\nAccess to many hyperparameters can make a method more difficult to deploy. On the other hand, hyperparameters with well-founded semantic can offer desirable, fine-grained controls. For instance, a larger A\u2122 hyperparameter leads to sparser activation scalars that are easier to interpret, a level of control not offered by many existing methods. Yet, methods like ACTIVPATCH or ATTRPATCH also require finicky hyperparameter choices such as how to corrupt the prompt, e.g., deciding on the standard deviation of the Gaussian noise (Meng et al., 2022) to obtain a second corrupted prompt for patching.\nA limitation of this work is the size of models studied and the small size and synthetic character of the tasks. The two largest models considered in this work are GPT2-XL (1.5 billion parameters) and Pythia-1.4B (1.4 billion parameters). Beyond the required compute, we do not anticipate problems applying activation scaling to larger models. Testing ACTIVSCALAR on more real-world"}, {"title": "Impact Statement", "content": "This work aims to better understand the internal workings of language models. This understanding may serve the post-hoc identification of harmful properties such as hallucination, illicit memorization, and undesired biases. It should ideally help in taking preemptive action to guide the design and training of future models. The required compute to apply activation scaling is predominantly dictated by the size of the studied language models."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Technical Details", "content": "We implement all steering and interpretability methods using TransformerLens (Nanda, 2023b) and hyperparameters are chosen based on a combination of the grid search displayed in Fig. 2 as well as steering and interpretability desiderata specific to each setting. We train gradient-based methods using the Adam optimizer (Kingma and Ba, 2015) for 25 epochs. We typically choose a smaller learning rate of 0.0001 for training STEERVEC and 0.001 for ACTIVSCALAR when training on a single data point. A similarly influential hyperparameter is the initialization of the trainable intervention parameters that we initialize with Gaussian noise N(0, 1e \u2013 5)."}, {"title": "A.2 Multi-headed Attention", "content": "We now describe the multi-headed attention mechanism ATTN (Vaswani et al., 2017) in more detail. Given a D \u00d7 In activation matrix $H^{(4)} = [h_{1,1}^{(4)},..., h_{1,I_n}^{(4)}]$, the multi-headed attention mechanism at layer l computes\n$H^{(1)} = ATTN (LN^{(1)} (H^{(4)})$ (11a)\n$= \\sum_{t=1}^{T} W_{t,l}^{(A)} (LN^{(1)} (H^{(4)}))$ (11b)\n$= \\sum_{t=1}^{T} H_{t,l}^{(2)}$ (11c)\n$= \\sum_{t=1}^{T} H_{t,l}^{(2)}$ (11d)\nEssentially, each individual attention head Atl with head index t \u2208 {1, ...,T} computes an activation matrix $H_{t,l}^{(2)} \u2208 R^{D'\u00d7I_n}$. This per-head matrix is then multiplied with $W_{t,l}^{(2)} \u2208 R^{D\u00d7D'}$ to obtain a per-head output activation matrix $H_{t,l}^{(2)} \u2208 R^{D\u00d7I_n}$. Note that $h_{t,l,i}^{(2)} \u2208 R^{D'}$ and $h_{t,l,i}^{(4)} \u2208 R^{D}$ are column vectors of $H_{t,l}^{(2)}$ and $H_{t,l}^{(4)}$, respectively.\nTo zoom in on individual attention heads, we now omit the head index t and layer index l for notational simplicity. Under this simplified notation, we define A(H) as follows. Each activation (column) vector $h_i \u2208 R^{D}$ of the matrix $H \u2208 R^{D\u00d7I_n}$ is linearly projected to compute query, key and value activation vectors according to\n$h_i^{(q)} = W^{(q)}h_i$ (12a)\n$h_i^{(k)} = W^{(k)}h_i$ (12b)\n$h_i^{(v)} = W^{(v)}h_i$ (12c)\nwhere $W^{(q)}, W^{(k)}, W^{(v)} \u2208 R^{D'\u00d7D}$. The key and value vectors are then used to compute $I_n$ different self-attention distributions \u03bai (Li et al., 2024) over the probability simplex $\u2206^{I_n\u22121}$ following\n$\u03ba_i(j) = \\frac{h_i^{(q)T h_i^{(k)}}}{\\sqrt{D'}}$ (13a)\n$\u03ba_i = \u03c3([\u03ba_i(1),..., \u03ba_i(I_n)]^T)$ (13b)\nwhere \u03bai(j) represents the (unnormalized) attention score that token position i pays to token position j and \u03c3 is the softmax function. Importantly, in autoregressive language modeling, it is common to apply the hard constraint of an attention mask to disallow token positions earlier in the prompt to attend to positions later in the prompt-i.e., j \u2264 i.\nFinally, the self-attention distributions are used to construct a weighted average of the value vectors $h_i^{(v)}$ according to\n$h_i^{(2)} = \\sum_{j=1}^{I_n} \u03ba_i(j) h_i^{(v)}$ (14a)\nIn the main part of this paper, Fig. 7 and Fig. 8 specifically, we refer to $h_{t,l,i}^{(k)}$ as z, $h_{t,l,i}^{(2)}$ as o and $h_{t,l,i}^{(v)}$ as v activation vectors."}]}