{"title": "TEAM: Temporal Adversarial Examples Attack Model against Network Intrusion Detection System Applied to RNN", "authors": ["Ziyi Liu", "Dengpan Ye", "Long Tang", "Yunming Zhang", "Jiacheng Deng"], "abstract": "With the development of artificial intelligence, neural networks play a key role in network intrusion detection systems (NIDS). Despite the tremendous advantages, neural networks are susceptible to adversarial attacks. To improve the reliability of NIDS, many research has been conducted and plenty of solutions have been proposed. However, the existing solutions rarely consider the adversarial attacks against recurrent neural networks (RNN) with time steps, which would greatly affect the application of NIDS in real world. Therefore, we first propose a novel RNN adversarial attack model based on feature reconstruction called Temporal adversarial Examples Attack Model (TEAM), which applied to time series data and reveals the potential connection between adversarial and time steps in RNN. That is, the past adversarial examples within the same time steps can trigger further attacks on current or future original examples. Moreover, TEAM leverages Time Dilation (TD) to effectively mitigates the effect of temporal among adversarial examples within the same time steps. Experimental results show that in most attack categories, TEAM improves the misjudgment rate of NIDS on both black and white boxes, making the misjudgment rate reach more than 96.68%. Meanwhile, the maximum increase in the misjudgment rate of the NIDS for subsequent original samples exceeds 95.57%.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of modern network technology, the means of network intrusion attacks are becoming increasingly complex. The traditional static defense method [1] cannot adapt to the current complex and dynamical security requirements. To achieve active defense, Network Intrusion Detection System (NIDS) [2], [3] has been widely used and achieved good results. Traditional NIDS technology achieves intrusion detection by building a knowledge base in advance and comparing the signatures in the knowledge base with the signatures extracted from the traffic [4]. Hence, NIDS based on traffic signatures can only detect existing attack types and is difficult to detect current complex and changeable network attacks. With the continuous development of Deep Neural Networks (DNN) [5] [6], experts and scholars have discovered that DNN only requires to pre-train the model through a large amount of existing data to detect new intrusion attacks without the need for a priori knowledge base, which brings a new direction for the development of NIDS.\nRecurrent Neural Network (RNN) [7], [8], a kind of DNN, has been widely used in NIDS because it can fully consider the temporal properties of network traffic in the characterization learning [9]. Specifically, network traffic is continuously generated over time and has a temporal relationship with each other. RNN with time steps can also fully consider the impact of the previous moment data in the same time step when learning the traffic at the current moment, which greatly enhances the performance of NIDS. However, almost all DNNs have been proven to be vulnerable to Adversarial Examples (AEs) [10], [11], which brings new challenges to NIDS.\nAt present, a large number of researchers have conducted in-depth research on AEs attacks in NIDS, hoping to improve the defects of NIDS and enhance the robustness of NIDS [12]. Unfortunately, we found that there is a lack of research on adversarial attacks against RNN models with time steps in NIDS, and there are the following problems. Firstly, the RNN model is a temporal model, and traditional AEs do not consider the time characteristics of network traffic. Hence, when traditional AEs are used to attack the RNN model of NIDS, the AEs attack on the RNN model will have a large deviation due to the RNN model's learning of past moment content, resulting in a reduction of AEs attack and transferability [13] success rate; Secondly, in the RNN model, due to differences in network traffic structure, the time distribution of network traffic will be different. Attackers can typically only generate adversarial samples using a small portion of the network traffic dataset to simulate the data distribution. This causes the AEs have a large deviation due to the difference in time distribution when attacking the RNN model, thus reducing the attack success rate and transferability of AEs; Last but not least, because the RNN model learns to characterize part of the traffic content from past moments, the characteristics of AEs from those past moments may influence the Original Examples (OEs) in the current or future moments. This occurs due to the temporal nature of the RNN's learning process, and as a result, it could reduce the detection rate of the NIDS (OEs in this paper specifically refers to the original attack traffic in network traffic).\nTo solve the above problems, we first propose a model called Temporal adversarial Examples Attack Model (TEAM) to reveal the potential connection between adversarial and time steps in RNN. In RNN models with time steps, we observe the impact of the presence of temporal between AEs within the same time step. Due to the weights [14] related to past moment data in the RNN model will vary greatly depending on the data structure. Hence, targeted continuous adversarial attacks constructed by attackers using partial data have certain limitations in attacking RNN models due to the influence between AEs. In our proposed method, TEAM uses the idea of expanding the relevant weights of past moments to make the weight distribution between the attack model and the target model overlap as much as possible, effectively alleviating the above problems and realizing this type of targeted adversarial attacks. Moreover, we also observe for the first time that AEs from the past moment within the same time step have an impact on OEs at the current or future moment. Attackers can use carefully crafted AEs to cause NIDS to misjudge subsequent OEs in the same time step. This new type of attack also provides new ideas for subsequent research on the robustness of NIDS. The specific attack scenario of TEAM is shown in Figure 1\nOur main contributions can be summarized as follows:\n\u2022 We systematically study adversarial attacks against RNN models with time steps in NIDS. To the best of our knowledge, we are the first to reveal potential connection between adversarial and time-stepping in such attacks. That is, past AEs in the same time step can trigger further attacks on current or future normal samples; consecutive AEs in the same time step will affect with each other, affecting the attack success rate of adversarial samples.\n\u2022 We first propose TEAM to implement adversarial attacks against RNN models with time steps in NIDS. Meanwhile, we discovered that carefully designed AEs in an RNN model with time steps can affect the model's judgment of OEs in the same subsequent time step, and propose the concept of the next moment attack.\n\u2022 We first propose a Time Dilation (TD) method for guided AEs generation in TEAM. The TD method adjusts the RNN model's retention of past moment content by expanding the weight of past moments, effectively alleviating the weight distribution differences in the RNN model caused by differences in data structure, thereby further improving the attack success rate of AEs.\n\u2022 We designed a corresponding prototype system and conducted extensive experiments on the NSL-KDD dataset and the CIC-IDS2017 dataset to verify the phenomena we revealed. Experimental results show that in most attack categories, TEAM improves the misjudgment rate of NIDS on both black and white boxes, making the misjudgment rate reach more than 96.68%. Meanwhile, in the RNN model, the next-moment attack of AE on subsequent OE generally increases the misjudgment rate of NIDS on OE, with the highest improvement reaching 95.57%."}, {"title": "II. RELATED WORKS", "content": "Recently, deep learning [15] has been widely applied in NIDS. Due to the excellent representation learning capabilities, deep learning has effectively improved the detection ability of NIDS against new intrusion attacks. Mirza et al. [16] proposed a sequential AutoEncoder framework based on Long Short-Term Memory (LSTM) neural network to implement intrusion detection. Zhou et al. [17] presented an intelligent anomaly detection variational LSTM based on reconstructed feature representation for intrusion detection. Javed et al. [18] designed a new intrusion detection method called CANintel-liIDS for vehicle intrusion attack detection on the CAN bus. Assis et al. [19] proposed a SDN defense system based on the analysis of single IP traffic records, which uses the Gated Recurrent Unit (GRU) to detect DDos and intrusion attacks. Mushtaq et al. [20] designed a hybrid framework including deep AutoEncoders, LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Fu et al. [21] proposed a real-time malicious traffic detection system based on machine learning, which achieves high detection accuracy and high detection throughput by utilizing frequency domain features. Wang et al. [22] introduced GRU into the improved AlexNet to build an intrusion detection model for urban rail transit management systems. It can be seen that the existing NIDS methods are mainly based on RNN-based models.\nAdversarial Attacks for NIDS\nAdversarial attacks have attracted widespread attention from the academic community due to their excellent attack effects on DNN-based NIDS. Yang et al. [23] studied how AEs affect the performance of DNN trained to detect anomalous behavior in black-box models. Alhajjar et al. [24] explored the use of evolutionary computation (particle swarm optimization and genetic algorithms) and generative adversarial networks as tools for adversarial example generation. Clements et al. [25] explored the potential for adversarial entities to compromise such vulnerabilities to disrupt deep learning-based NIDS. Han et al. [12] used adversarial machine learning techniques"}, {"title": "III. MOTIVATION", "content": "We introduce a hypothetical threat scenario, i.e., the NIDS uses an RNN to build the detection model. Note that RNN in this paper refers to the collective name of recurrent neural networks such as the Original Recurrent Neural Network (ORNN) model, the Gated Recurrent Unit (GRU) model, and the LSTM model. The attacker conducts a tentative attack on the NIDS model by accessing the network system and performing a sniffing attack [30], accessing and monitoring the traffic, and then obtains the correct data category, traffic characteristics and approximate time step of the model from NIDS. Moreover, attackers do not know anything else about the model, such as its specific parameters, detection scheme and structure. Since the NIDS model can accurately detect anomalous traffic and process it accordingly, such as discard, purification, and other strategies, attackers want to realize targeted attacks, i.e., to make NIDS recognize abnormal traffic as normal by adversarial means. Meanwhile, due to the data set used in our work is a network traffic feature data set (i.e., the input data of NIDS and the data we generate AE are both \"flow\"). Therefore, our adversarial attacks work is also GFLA [29] based on network traffic feature."}, {"title": "IV. METHODOLOGY", "content": "1) AutoEncoder: AutoEncoder [32] is an unsupervised learning neural network model that is commonly used for data dimensionality reduction, feature learning and data reconstruction. Its basic structure includes two parts: Encoder and decoder. In this paper, we will use AutoEncoder to reconstruct data to generate AEs. The specific implementation formulas of encoder and decoder are as follows:\n$h_e = A_e(W_eX + b_e),$ (1)\nwhere $h_e$ represents the encoded result, $X$ represents the input of the encoder, $b_e$ represents the bias of the encoder layer, $W_e$ represents the weight of the encoder layer and $A_e$ represents the activation function of the encoder layer.\n$X' = A_d(W_d h_e + b_d),$ (2)\nwhere $X'$ represents the decoded result, $b_d$ represents the bias of the decoder layer, $W_d$ represents the weight of the encoder layer and $A_d$ represents the activation function of the decoder layer.\n2) Recurrent Neural Network: Recurrent Neural Network (RNN) [33] is a type of neural network with memory ability. In RNN, neurons can not only receive information at the current moment, but also from past moments. Compared with feedforward neural networks, RNNs are more consistent with the structure of biological neural networks. The existing base RNN models mainly include original RNN, LSTM [34] and GRU [35]. These base RNN models are widely used in tasks such as speech recognition, NIDS and natural language generation."}, {"title": "B. Observations", "content": "Network traffic is a kind of data with time continuity characteristics. Unlike traditional text data that has temporal continuity within a sentence, the temporal continuity of network traffic is reflected in the inter-traffic, such as traffic that users continue to access, continuous Dos [31] traffic attacks, etc. Therefore, there exists a situation in NIDS where time steps are set in the RNN model for continuous detection of traffic data within the time steps.\nThe RNN model is a neural network structure that can efficiently process time series, and it can learn the data of the current moment while considering the influence of the data of the past moments on the current moment. Hence, the judgment of the current moment network traffic of NIDS based on RNN model over a period of time step not only depends on the current input traffic, but also is influenced by the traffic of the past moments.\nHence, we can easily infer that the past moment AEs within the same time step in the RNN model will affect the OEs in the current or future moments as a way to realize the next moment attack; the consecutive AEs within the same time step will affect each other, and we need to design a new mechanism to realize the normal adversarial attack in the RNN model."}, {"title": "B. Temporal Adversarial Examples Attack Model", "content": "To reveal the potential connection between confrontation and time step in RNN-based NIDS, we design a new model called TEAM, which consists of two main parts: AutoEncoder and our proposed Time Dilation RNN (TDRNN). Among them, the AutoEncoder is used to generate AEs suitable for RNN; the TDRNN is a model that has been trained to guide the generation of this AEs. Figure 2 illustrates the pipeline of the TEAM method, where $k_n$ represents the non-functional features of traffic, $k_m$ represents the reconstructed non-functional features, $I_n$ represents the functional features of traffic, $X_{adv-n}^{org}$ represents the original data that needs to be reconstructed into AEs at the same time step, $X_{adv-n}^{no-fun}$ represents the non-functional feature part of $X_{adv-n}^{org}$, $X_{adv-n}^{fun}$ represents the functional feature part of $X_{adv-n}^{org}$, $X_{adv-n}^{adv}$ represents AEs reconstructed by $X_{adv-n}^{fun}$ through AutoEncoder, $X_{adv}^{adv}$ represents network traffic with adversarial effect recomposed by $X_{adv-n}^{no-fun}$ and $X_{adv-n}^{adv}$, $X_{org-n}^{org}$ represents the attack traffic of original data in the same time step. This part of the symbolic content applies to Algorithm 1 below.\n1) Adversarial Attacks on RNN: Adversarial attacks under non-RNN models can be achieved by using part of the data set to restore the feature distribution of the target model, and using similar feature distributions to guide the generation of AEs. However, the RNN model not only needs to judge the input features at the current moment in the process of realizing the detection, but also needs to consider the influence of the features in the past moment on the current moment. Hence, adversarial attacks implemented against RNN models are more complex. Particularly, we find that the AEs generated by the RNN model constructed through partial data sets (PD-RNN, the training model held by attacker) are difficult to effectively apply to the RNN model constructed through all data sets (AD-RNN, the target model that the attacker will attack), i.e., there is an interaction between the AEs in the same time step (the current moment AEs is affected by the AEs of the past moments). This is because different data structures lead to variations in the content retained by the RNN model from past moments, and these variations cause deviations in the influence of previous data on the current moment, resulting in significant differences in the weight distributions between the PD-RNN and AD-RNN models (as shown in Figure 3 (a)).\nTo solve the above problems, we propose an improved RNN model called Time Dilation RNN (TDRNN) to implement RNN adversarial attacks in NIDS. In TDRNN, we adjust the degree of preservation of past moment content in PD-RNN by enlarging the weights used to control past moment data, so that the weight distribution of PD-RNN and the target RNN model overlap as much as possible (as shown in Figure 3 (b) shown), thereby mitigating the influence between AEs within the same time step. Our proposed TD method works on all RNN models. Below we will show the specific formulas of Time Dilation Original RNN (TDORNN), Time Dilation LSTM (TDLSTM) and Time Dilation GRU (TDGRU).\nThe specific implementation formula of TDORNN is as follows:\n$h_t = tanh([x_t . W_{xh}] + [h_{t-1} . (W_{hh} \u00d7 h_n)]),$ (3)\nwhere $x_t$ represents the input at the current moment, $W_{xh}$ represents the weight input to the hidden state, $h_t$ represents the hidden state at the current moment, $h_{t-1}$ represents the hidden state at the previous moment, $W_{hh}$ represents the weight of the hidden state at the previous moment when $h_t$ is updated, and $h_n$ represents the time dilation coefficient of the weight relative to the past moment\nThe specific implementation formula of TDLSTM is as follows:\n$f = \u03c3([x_t . W_{xf}] + [h_{t-1} . (W_{hf} \u00d7 h_n)]),$ (4)\nwhere \u03c3 represents the Sigmoid activation function, $X_t$ represents the input at the current moment, $W_{xf}$ represents the weight input to the forget gate, $h_{t-1}$ represents the hidden state at the previous moment, $W_{hf}$ represents the weight of hidden content in the past moment in the forget gate, and $h_n$ represents the time dilation coefficient of the weight relative to the past moment, and $f$ represents the forget gate, which is used to control whether to forget part of the previous cell state in LSTM."}, {"title": null, "content": "$h_t = \u03c3 * tanh(c_c),$ (9)\nwhere $W_{xf}$ represents the weight input to the output gate, $W_{ho}$ represents the weight of hidden content in the past moment in the output gate, \u03c3 is used to control the hidden state to be output in LSTM, and $h_t$ represents the hidden state at the current moment.\nThe specific implementation formula of TDGRU is as follows:\n$R_t = \u03c3([x_t . W_{xr}] + [h_{t-1} . (W_{hr} \u00d7 h_n)]),$ (10)\n$Z_t = \u03c3([x_t . W_{xz}] + [h_{t-1} . (W_{hz} \u00d7 h_n)]),$ (11)\nwhere \u03c3 represents the Sigmoid activation function, $R_t$ and $Z_t$ represent reset gate and update gate, $x_t$ represents the input at the current moment, $h_t$ represents the hidden state at the current moment, $h_{t-1}$ represents the hidden state at the past moment, $W_{xr}$ and $W_{xz}$ represent the weight assigned by $X_t$ in the reset gate and update gate respectively, $W_{hr}$ and $W_{hz}$ represent the weight assigned by $h_t$ in the reset gate and update gate respectively, and $h_n$ represents the time dilation coefficient of the weight relative to the past moment.\n$h_t = tanh([x_t(W_{xh}\u00d7h_n)]+[(r\u00d7h_{t-1})\u00b7(W_{hh}\u00d7h_n)]),$ (12)\nwhere $h_t$ represents the candidate state, $W_{xh}$ represents the weight assigned to $x_t$ in the candidate state, and $W_{hh}$ represents the weight assigned to $h_{t-1}$ in the candidate state. Note that we also add a TD coefficient to the weight of $x_t$ in $h_t$, which is to balance the ratio of current moment data and past moment data in the candidate set.\n$h_t = Z_t \u00d7 h_t + (1 \u2212 Z_t) \u00d7 h_{t\u22121},$ (13)\nwhere $h_t$ represents the currently hidden state.\nIt is worth noting that in our proposed method, the above $h_n$ is used as the weight time dilation coefficient of the past time in the RNN to participate in the training phase of the model, and is still retained in the attack phase after the training."}, {"title": null, "content": "2) Next Moment Attack in RNN Model: Since RNN can fully consider the time impact between network traffic, we find that AEs can be used in RNN to change the model's judgment of OEs at subsequent moments in the same time step. Specifically, attackers only need to send a small number of consecutive AEs to influence the model's judgment on most traffic in the same time step, which greatly affects the reliability and robustness of the NIDS model. Meanwhile, it also gives attackers greater operating space and lower costs. We use TEAM to implement this type of attack. Specifically, we only need to splice the OEs behind the AEs generated by the AutoEncoder when training the RNN AEs in the same time step. Then, Time Dilation RNN (TDRNN) is used to conduct guided training on the spliced data, where the loss function given for the OE part is as follows.\n$L_{org} = CrossEntropy(x_{org}, Label_{normal}).$ (15)\nWe use this loss function to make the generated past moment AEs influence the current or future OEs as much as possible to achieve the next moment attack. Therefore, by combining with the loss function when generating adversarial samples in the previous section, we can know that the total loss function of TEAM is as follows.\n$L_{CrossEntropy} = L_{adv} + L_{org}.$ (16)\nThe specific training and implementation algorithms for adversarial attacks and the next attack on the RNN model are shown in Algorithm 1. In Algorithm 1, we use $x$ and $y$ to represent the training set and test set in the overall data X, respectively."}, {"title": "V. EXPERIMENTS", "content": "1) Datasets: In this paper, we use NSL-KDD [37] and CIC-IDS2017 [38] datasets as experimental datasets.\nNSL-KDD: As one of the most widely used benchmark datasets in NIDS, the NSL-KDD dataset contains 125,973 training samples and 22,544 test samples, with four different attack types. We refer to the existing NIDS adversarial attack literature, make a detailed division of the functional and non-functional features of the NSL-KDD dataset [27]. In network attacks, attackers can use sniffing attacks to obtain part of the network access traffic. Therefore, we use half of the training set for model training (i.e. PD-RNN) to generate AE, and use the full training set to train the target NIDS (i.e. AD-RNN) to test AE and next-moment attacks.\nCIC-IDS2017: The CIC-IDS2017 dataset is often used in NIDS detection experiments because it reflects the characteristics of modern network traffic. It contains 12 traffics of different attack types. Each traffic in the official CSV file for machine learning consists of 78 features. The entire dataset is collected from Monday to Friday. In this experiment, we conducted a targeted adversarial attack to make NIDS misjudge the attack traffic as normal. Due to all the data collected by the official on Monday is normal data, this experiment excludes the data of that day from the dataset. Therefore, in this experiment, the training set is divided into 1,802,513 and the test set is 339,599. Similar to NSL-KDD, we use half of the training set for model training (i.e. PD-RNN) to generate AE, and use the full training set to train the target NIDS (i.e. AD-RNN) to test AE and the next moment attack. In order to more effectively verify the effectiveness of our proposed method, this half of the CIC-IDS2017 dataset is shuffled, which has a higher randomness of temporal sequence. Meanwhile, to ensure the data balance of the experimental data as much as possible, we merge all Dos attacks in CIC-IDS2017 into one Dos attack. Infiltration and Botnet both involve infiltrating the network, hiding activities, maintaining infection for a long time and performing malicious operations. Therefore, these two are merged into one Infiltration&Botnet attack, and SSH-Patator and FTP-Patator types are merged into Patator attacks. In addition, since there are only 11 traffic in Heartbleed attack types and it is difficult to merge them with other types, we do not consider this attack type in this experiment."}, {"title": null, "content": "2) Compare Models and Evaluation Metrics: To the best of our knowledge, ours is the first work to investigate the potential connection between adversarial and time stepping of RNN models in NIDS. In addition, because the RNN model with time steps will consider the impact of past moment traffic on current moment traffic when characterization learning, other existing work on adversarial attacks against NIDS difficult to be directly adapted to RNN models with time steps. Based on the above reasons, in this paper, we will compare the existing typical adversarial attack methods PGD [41], C&W [42], IDSGAN [27] and J-Attack [28] to verify the adversarial and transferability of our proposed method.\nIn our experiments, we implemented a targeted attack, use Attack Success Rate (ASR) as an evaluation metric, use Mis-judgment Accuracy Rate (MAR) to represent the probability that normal attack traffic in NIDS is misjudged as normal traffic, use MAR1 to represent the probability that NIDS misjudged the OE attack traffic at the first moment after AEs as normal traffic in the original model, use MAR2 to represent the probability that NIDS misjudged the OE attack traffic at the second moment after AEs as normal traffic in the original model. Note that our adversarial attack is a targeted adversarial attack. When the AE of a certain attack category in the test set is identified as normal traffic, we can consider that the ASR of the adversarial attack is the same as the MSR of this attack category by NIDS. Moreover, in the table of experimental results, we bold the optimal data of each item. If the data in a category are all optimal values, we bold the data of our proposed method.\n3) Experimental Model and Details: We conduct adversarial attacks and next time step attacks on three existing baseline RNN models (ORNN, LSTM and GRU) respectively. Meanwhile, we also perform black-box transferability between models. Furthermore, all our attacks are targeted attacks (making NIDS identify abnormal traffic as normal traffic). In the NSL-KDD data set, exceptions are the U2R&R2L attack categories, the functional features and non-functional feature areas of the other attack categories are different. Therefore, different AEs will be generated according to categories for experiments.\nFor the NSL-KDD dataset, the number of network iteration epochs used by TEAM to generate AE is 100, the gradient optimization method is Adam, and the learning rate is 0.001. For the CIC-IDS2017 dataset, the maximum number of network iteration epochs used by TEAM to generate AE is 2000, and we will take the better AE generated in the epoch cycle, the gradient optimization method is Adam, and the learning rate is 0.001. Meanwhile, the intermediate layer of AutoEncoder is 4, and the time step of the RNN model is set to 8. In the same time step, the first 6 samples are AEs and the remaining 2 are OEs."}, {"title": "B. Results and Evaluations", "content": "In this section, we compare the differences between our proposed method and other four typical RNN model methods in terms of AE white-box attack success rate and black-box transferability. Ablation experiments were conducted to verify the effectiveness of our proposed TDRNN method. At the same time, we experimentally verified the concept of next-moment attack. Since our proposed method uses the first 6 OEs for AE in each time step, and the last two OEs are used to verify the concept of attack at the next moment. Therefore, in this experiment, we will take the same number of AEs for comparison and verification.\n1) Adversarial Attacks on RNNs Using the NSL-KDD Dataset: In this section, we first conduct adversarial attacks on RNN-based NIDS on the NSL-KDD dataset to verify the effectiveness of the proposed method. The experimental results are shown in Table III.\nFrom Table III, we can see that in the NSL-KDD dataset, the TEAM method we proposed better guides the generation of AEs, and the attack success rate and transferability of the generated AEs on RNN, LSTM and GRU are significantly better than those of other comparison methods. The reason is the AE generated by the PGD and C&W methods is affected by the difference in the weight distribution due to data differences between the PD-RNN model and the target RNN model, cannot accurately implement adversarial attacks against the target RNN. In the IDSGAN method, due to the generator and discriminator use traditional fully connected neural networks, the PD-RNN model only serves as a query for generating results. Therefore, the AE generated by IDSGAN is difficult to adapt to the temporal of RNN-type models. J-Attack is limited by the scope of feature selection (that is, it can only select perturbation to add targets in non-functional features), and its method of directly using masks to add perturbations is also difficult to effectively deal with the timing differences between RNNs. The TEAM method we proposed not only guides AE generation through the RNN model, but also uses the time dilation method we proposed to minimize the difference in weight distribution between the PD-RNN model and the target RNN model, and then the generated AE can utilize the temporal of the model itself in adversarial attacks as much as possible to achieve effective AE white-box attacks and black-box transferability attacks.\nWe can also see that IDSGAN has extremely low ASR in Dos and Probe [43]. We observed the experimental results and found that the RNN classifier misclassified AE as other attack types. For example, the vast majority of Probe attack AEs are predicted to be Dos attacks by the RNN classifier. This is because in attack types with slightly higher temporal such as Probe, the AE generated by IDSGAN will cross the boundary excessively because the RNN model retains past content, affecting the normal judgment of the RNN model and causing the attack to fail.\nThe adversarial attack effect of the J-Attack method on the NSL-KDD dataset is generally poor, and there is even a situation where the ASR is 0. This is because the adversarial attack of J-Attack is based on feature selection and is implemented by directly adding perturbations to the selected features. However, the NSL-KDD dataset itself has fewer statistical features, and statistical features are usually an important component of NIDS detection under deep learning. The lack of statistical features makes it difficult for J-Attack, a method that directly adds perturbations to selected features, to achieve accurate adversarial attacks. Furthermore, the selection of non-functional feature areas narrows the feature selection range of J-Attack and weakens the adversarial effect of J-Attack. Meanwhile, the information difference between PD-RNN and the target RNN in the past moments also increases the uncertainty of the adversarial effect of J-Attack, making it perform poorly on the NSL-KDD dataset.\nIn addition, in U2R&R2L and Probe attacks, the PGD method uses multi-step iterations of small distance reverse gradients to implement AE attacks. Therefore, most AEs will not have a one-time iteration reverse gradient distance that is too large, causing AEs to approach the decision boundary on the other side of the attack target, making the AEs generated by the PGD method is relatively less affected by the difference weight distribution between PD-RNN and target RNN. Meanwhile, the U2R&R2L attack itself basically does not have temporal, and the Probe attack has low temporal. Therefore, we believe that the AE generated by the PGD method on the U2R&R2L and Probe attack categories is less affected by time interference. This is also the reason why the PGD method generates a part of AE white-box attack success rates and black-box transferability on the U2R&R2L and Probe attack categories and is close to our TEAM method.\nIt is worth noting that in this paper, NIDS misjudges U2R&R2L attacks as normal traffic with a high probability. This is because in network attacks, U2R&R2L attacks are usually hidden in normal traffic, do not have high temporal, and have similar characteristics to normal traffic. Therefore, it is normal for NIDS to have a high probability of misjudging U2R&R2L attacks as normal traffic.\n2) Adversarial Attacks on RNNs Using the CIC-IDS2017 Dataset: In this section, we further verify the effectiveness of our proposed TEAM method on the CIC-IDS2017 dataset. The experimental results are shown in Table IV.\nFrom Table IV, we can see that the proposed TEAM method shows excellent attack effects in all types of attacks compared with the existing four adversarial attack methods on the CIC-IDS2017 dataset, and is only slightly inferior to the IDSGAN method in the LSTM white-box attack of the Patator attack cat-egory. The reason is that the temporal of Patator, Web Attack and Infiltration&Botnet is lower than that of Dos, Portscan and DDos attacks, and is less affected by temporal. Therefore, the C&W, IDSGAN and J-Attack methods are less affected by the temporal difference between PD-RNN and the target RNN in these three types of attacks, and their adversarial attacks have good effects. In addition, we can see that the C&W method has a good adversarial attack effect on LSTM and GRU of Dos, Portscan and DDos, but the effect of adversarial attacks on the original RNN is extremely poor. We believe that this is because the C&W method directly optimizes the decision boundary of the classifier, making the adversarial sample as far away from the classification boundary as possible and close to the decision center of the target category. At the same time, LSTM and GRU can better control the flow of past information through the gating mechanism compared with the original RNN, thereby reducing the impact of past information on current information. Therefore, the C&W method has achieved good results in Dos, Portscan and DDos LSTM and GRU adversarial attacks, but it is very susceptible to the impact of past information on original RNN, resulting in extremely poor results. The PGD method uses multi-step iterations of small distance reverse gradients to implement AE attacks. Therefore, the AE generated by it is usually close to the decision boundary and is easily disturbed by the difference in past information, resulting in slight inferior adversarial attack effects on all attack types. The TEAM we proposed can use the time dilation method to make the weight distribution between PD-RNN and the target RNN closer, thereby achieving better adversarial attacks at any temporal strength. However, it should be admitted that this time dilation method is difficult to make"}, {"title": null, "content": "the weight distribution between PD-RNN and the target RNN completely fit. In the Patator attack type that is inferior affected by temporal", "Method": "In this section, we conduct ablation experiments on the time dilation method in our proposed TEAM method to verify the effec-tiveness of our proposed time dilation method."}]}