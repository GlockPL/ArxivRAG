{"title": "Posture-Informed Muscular Force Learning\nfor Robust Hand Pressure Estimation", "authors": ["Kyungjin Seo", "Junghoon Seo", "Hanseok Jeong", "Sangpil Kim", "Sang Ho Yoon"], "abstract": "We present PiMForce, a novel framework that enhances hand pressure estimation\nby leveraging 3D hand posture information to augment forearm surface electromyo-\ngraphy (sEMG) signals. Our approach utilizes detailed spatial information from\n3D hand poses in conjunction with dynamic muscle activity from sEMG to enable\naccurate and robust whole-hand pressure measurements under diverse hand-object\ninteractions. We also developed a multimodal data collection system that combines\na pressure glove, an sEMG armband, and a markerless finger-tracking module. We\ncreated a comprehensive dataset from 21 participants, capturing synchronized data\nof hand posture, sEMG signals, and exerted hand pressure across various hand\npostures and hand-object interaction scenarios using our collection system. Our\nframework enables precise hand pressure estimation in complex and natural inter-\naction scenarios. Our approach substantially mitigates the limitations of traditional\nSEMG-based or vision-based methods by integrating 3D hand posture information\nwith sEMG signals. Video demos, data, and code are available online.", "sections": [{"title": "1 Introduction", "content": "Hands are a central tool for humans to interact with the surrounding environment. With the ad-\nvancement in hand tracking technology, hand inputs, including position, orientation, gesture, and\nmotion, are increasingly used as a primary means of control, especially for emerging interfaces (e.g.,\naugmented/virtual reality and wearables). Using hands as the main interaction medium offers a high\nlevel of versatility and flexibility to achieve natural and intuitive interactions.\nRecent studies have started to utilize hand pressure information to support hand-based interactions\nsuch as touching [1], grasping [2, 3], and pressing [4]. Researchers also utilized hand pressure to\nprovide effective haptic feedback [5] for a more immersive user experience. Furthermore, precise\nhand pressure measurement becomes essential for real-world applications, including ergonomic\nevaluation [6], hand rehabilitation [7], and prosthetic hand control [8]. To this end, previous works\nfocus on obtaining real-time and accurate hand pressure information with direct measurement\napproaches utilizing gloves [9-11] or load cells [12]. However, these approaches require users to be\nin physical contact by either wearing or holding the device, which hinders natural hand movements\nor reduces user comfort. Thus, the necessity of direct contact limits the users from performing\nhand-based interactions in a natural and unrestricted manner.\nTo this end, non-invasive sensing techniques to estimate exerted hand pressure without embedding\nsensors on the user's hand have been highlighted. These methods include profiling wrist topography\nwith capacitive sensing [13], multiple pressure sensing from the wrist [14], and electromyography"}, {"title": "2 Related Works", "content": "Previous works explored the interaction between the hand and objects by observing the movement\nand rotation of the object over time to estimate the pressure exerted by hands. By determining the\npressure required to produce these observed changes, the model estimated the aggregate pressure\napplied by the hand [18-20]. These approaches enabled the pressure estimation to act upon concealed\nor non-visible areas where direct visibility of the hand in contact with an object is absent. Still,\nprevious approaches had limitations where interacting with immovable objects would not work due\nto the absence of dynamic interaction indicators.\nResearchers also looked into different visual indicators like color changes in fingertips, which\nrepresent fluctuation of blood circulation within the fingertips [21, 22] or compression of skin\ntissues [23, 24]. These physiological behaviors served as indicators of the exerted hand pressure.\nMoreover, examining shadows cast during hand-object interaction provided further insight into the\nspatial relationship and dynamics of force between them [25-27].\nRecent works further advanced the existing visual indicator approach where they use a hand image\ncaptured by a single camera at a distance to estimate the hand pressure [16, 28, 17, 29]. They\nemployed a deep learning model that facilitates the understanding of visual cues to estimate accurate"}, {"title": "2.1 Vision-based Hand Pressure Estimation", "content": null}, {"title": "2.2 Wearable-based Hand Pressure Estimation", "content": "Researchers have used forearm/wrist surface electromyography (sEMG) sensors to acquire finger\nmuscle activation information. Here, the sensor captured a train of neuron impulses propagated\nthrough the arms from the forearm or wrist [30, 31]. Previously, researchers used sEMG sensors to\nestimate various types of hand-related force/pressure, including force/pressure from gripping [32-36]\nand fingertip [4, 37-41]. Recent works also enabled the estimation of hand pressure along with hand\ngesture recognition using sEMG signals [8, 42]. However, previous works only dealt with a limited\nset of discrete hand poses [43]. Moreover, an issue existed with using sEMG signals for complex\nhand interactions where similar muscle activation signal behaviors were observed across different\nhand poses. This could easily confuse the model and lead to false behavior. In this work, we train the\nmodel with 3D hand posture to encode distinctive hand pose information alongside sEMG signals.\nThis integration forms a robust and accurate hand pressure estimation framework for similar muscle\nactivation behaviors but different hand poses. It is worth emphasizing that this study is the first\nto incorporate hand posture information for hand pressure estimation using forearm-worn sEMG.\nPrevious studies primarily focused on estimating pressure at the fingertip or on a single gripping\nforce, but our approach expands this to encompass the whole hand."}, {"title": "2.3 Datasets for Hand Pressure Estimation", "content": "In the computer vision and machine learning community, researchers have formed various types of\nhand-object interaction datasets for hand pressure estimation. These vision-based datasets collected\nrich visual and pose information for hand-object interactions, capturing everything from object\naffordances to whole-body grasps [44, 28, 17, 45-47]. On the other hand, sEMG-based datasets\nhave also been proposed and used to estimate hand pressure for AR/VR or prosthetic robotic arm\ncontrol applications [48, 4]. A highly relevant work is the ActionSense dataset [49], which focuses\non capturing multimodal data of human activities in a kitchen environment using wearable sensors.\nHowever, while ActionSense provides a valuable resource for understanding general kitchen activities,\nour work focuses on the utilization of 3D hand posture in muscular force learning for understanding\nhand pressure estimation. Furthermore, the temporal resolution of EMG data and the spatial resolution\nof hand pressure in ActionSense are substantially lower compared to ours, making it challenging\nto utilize rich sensor input and output effectively. Still, the dataset containing both rich visual and\nphysiological information is missing.\nIn this work, we attempt to set up a new multimodal dataset that contains 3D hand pose information,\nSEMG signals, and ground truth measurement of hand pressure as shown in Table 1. Our work\nprovides a holistic view of whole-hand dynamics during various hand-object interactions. This inte-\ngration enables continuous and comprehensive pressure estimation across the whole palm, addressing\nthe limitations of previous datasets that either infer pressure from visual cues or measure it in isolation."}, {"title": "3 Building Multimodal Dataset: Posture, Electromyography, and Pressure", "content": "To capture multimodal data with various hand-object interactions, we integrated and customized\nexisting hardware, including a pressure glove, an armband with 8-channel sEMG sensors, and a\nmarkerless finger tracking module. Figure 7 in supplementary material showcases our data collection\nsetup to capture real-time and synchronous multimodal data, including 3D hand posture, SEMG\nsignals, and exerted hand pressure. More detailed information about the hardware, defined hand\npostures, data collection protocol, and data processing can be found in Section B."}, {"title": "3.1 Data Collection Setup", "content": "Pressure Glove. To capture pressure exerted from the hand, we developed a customized pressure\nglove using a single 65-node pressure sensing glove (TactileGlove, Pressure Profile Systems) attached\nwith a pressure sensor (RA18DIY, Marveldex) at each fingertip. We added flexible sensors to the\nfingertips to address missed readings when the pressures were exerted on the edge of the fingertip.\nOur pressure glove supports pressure readings up to 55 N/cm\u00b2 with a sampling rate of 40 Hz.\n3D Hand Pose. Recent common approaches to obtaining ground truth 3D hand pose [51] involve\nusing multiple RGB cameras to derive 2D hand poses from each camera, followed by triangulation [52,\n53] or hand template fitting [44, 45, 54]. However, these methods are infeasible when a pressure\ndata glove is worn, as the glove obscures the hand, hindering accurate hand pose estimation from\nRGB images. To acquire accurate 3D hand pose information under this constraint, we employed a\nmagnetic sensing-based markerless finger tracking module (Quantum Mocap Metaglove, Manus).\nThe module provides each finger's 3D position and 3-axis joint angles with a sample rate of 120 Hz\nand less than 5-millisecond latency. We attached the finger-tracking module to the pressure glove to\ncapture exerted hand pressure and 3D hand pose data simultaneously.\n8-Channel SEMG Armband. We used 8 sEMG sensors (Trigno Avanti, Delsys) and installed sensors\ninto a customized armband made with semi-flexible material (TPU 95A) to ensure electrode contact\nfor various sizes of forearms. Our system captures muscle action potentials with a sampling rate of\n2,000 Hz.\nMultimodal Data Synchronization. Before training the multimodal dataset, we employed a linear\ninterpolation approach to synchronize high frame rate readings (sEMG) with low frame rate data\n(3D hand pose). We first joined the data from matched time points based on the collection time of\nthe sEMG data and interpolated missing values of the hand pose data linearly. We applied the same\napproach to the pressure glove, where we synchronized high frame rate sensors (pressure sensors)\nwith the low frame rate sensing glove (TactileGlove). Then, we adopted a nearest-neighbor-based\ninterpolation to synchronize 3D hand pose and sEMG data with hand pressure [4]."}, {"title": "3.2 Data Collection Procedure", "content": "With IRB approval, a total of 21 right-handed participants took part in this study. Of these, 17 were\nmale (81%) and 4 were female (19%). The participants' ages ranged from 20 to 32 years, with a mean\nage of 24.3 years (SD = 3.9). To ensure good quality hand pressure data using our glove, we chose\nparticipants with hand sizes greater than 180 mm. Prior to participation, all participants were provided\nwith a detailed information sheet outlining the purpose, procedures, and potential risks of the study.\nWe obtained written informed consent from each participant, ensuring they understood the nature\nof the data being collected, their right to withdraw at any time, and the measures taken to ensure\ndata privacy. We equipped participants with our multimodal glove and an 8-channel sEMG armband.\nFollowing initial calibration to compensate for each user's hand size, participants performed 22\ndistinctive hand-object interactions for the data collection task (Figure 8 in supplementary material).\nOur hand-object interactions consist of 7 hand-plane interactions, 5 pinch interactions, and 10\ndistinctive hand grasps. We included the same hand-plane and pinch interactions from recent hand\npressure estimation work [4] while adding palm-pressing motion. In terms of hand grasps selection,"}, {"title": "4 Method", "content": "The rationale behind using sEMG to estimate the pressure exerted by the hand lies in the direct\nrelationship between muscle electrical activity and the pressure generated during hand interactions [57-\n59]. When muscles contract for movement, they generate bioelectric signals that can be captured\nby EMG sensors [60]. This indicates that sEMG signals have the potential to estimate the pressure\nexerted by the hand. The ability to decode sEMG signals from forearm muscles generated by finger-level movements will set a robust foundation for understanding complex hand interactions. However,\nthe pressure exerted by the hand cannot be solely represented with muscle activation information.\nThe main reason is that the distribution of hand pressure varies according to different hand postures.\nFor example, similar sEMG patterns may be generated by different hand pressures depending on the\nrelated hand postures or grasps [43, 61]. This highlights the importance of considering hand posture\ninformation along with sEMG signals to estimate the exerted hand pressure precisely. Section C.1 in\nsupplementary material addresses the specific empirical observation for this motivation.\nTo address these issues, we enhance sEMG signals by leveraging 3D hand posture information. By\nintegrating inputs from forearm-worn sEMG sensors with 3D hand pose information derived from an\nRGB image, we observe improvements in the accuracy of hand pressure estimation. Our multimodal\napproach (Figure 2) leverages the strengths of both hand posture and muscle activations, offering\na comprehensive understanding of hand dynamics for whole-hand pressure estimation. Refer to\nSection C for more detailed information about the model architecture, training, and inference."}, {"title": "4.1 Overview", "content": null}, {"title": "4.2 3D Hand Pose and SEMG Feature Extractions", "content": "To verify the validity of our framework, we devise a deep neural network model to effectively utilize\nthe obtained multi-modalities. We represent the model as $f$, where the 3D hand pose is denoted by\n$H$ and the sEMG signal by $E$. The classification and regression targets for pressure are represented\nas $C$ and $P$, respectively. If the model outputs for pressure classification and regression are indicated,\nthey are denoted by $\\hat{C}$ and $\\hat{P}$. The feature extractor for sEMG data is $f_{EMG}$, and for hand pose, it\nis $f_{hand}$. The overall model $f$ comprises $f_{EMG}, f_{hand}$, and a pressure predictor $f_{pred}$ that takes the\nfeatures extracted from hand pose and sEMG to perform pressure classification and regression.\nFeature Extraction from sEMG signals. The sEMG data generally encounters measurement noises,\nincluding powerline noise and electromagnetic artifacts. To mitigate these issues, we utilize the short-time Fourier transform (STFT) to convert sEMG time-domain signals into spectrograms, represented"}, {"title": "Feature Extraction from sEMG signals.", "content": null}, {"title": "as $E\\in \\mathbb{R}^{8\\times 32\\times 64}$, isolating high-frequency noise and facilitating the application of convolutional\nneural network (CNN) models for feature extraction. We employed a 2D encoder-decoder model to\nextract features from the 2D SEMG signals. The encoder-decoder model processes the data and then\nflattens the output, which is subsequently transformed through a fully connected (FC) layer into a\n512-dimensional feature vector.", "content": null}, {"title": "Feature Extraction from 3D Hand Pose.", "content": "The hand model in our study is represented as a kinematic\ntree with 15 joint angles $\\theta \\in \\mathbb{R}^{15\\times 3}$, similar to the pose parameters in MA\u00d1O and its variants [62-\n65]. To handle this skeleton-based representation simply like PoseConv3D [66], we adopt a 3D\nResNet [67] to process 3D heatmap volumes of hand joints, transforming $\\theta$ into 21 3D hand joints\n$J\\in \\mathbb{R}^{21\\times 3}$ through the hand skeleton model's forward kinematics. These 3D hand joints are then\nconverted into 3D heatmap volumes $H \\in [0,1]^{21\\times H\\times W\\times D}$, with H, W, and D all set to 48, for\nprocessing by the 3D ResNet. Unlike the sequential representation of 2D heatmaps in PoseConv3D,\nour approach uses a single timestep 3D joint representation. After processing through the 3D ResNet,\nthe hand pose feature is flattened and transformed into a 512-dimensional vector using an FC layer,\nwith the 3D ResNet34 model being the model of choice for this operation."}, {"title": "Feature Fusion and Estimation.", "content": "The extracted sEMG feature $f_{EMG}(E)$ and hand pose feature\n$f_{hand}(H)$ are concatenated to form a 1024-dimensional joint feature vector. This vector is then passed\nthrough two FC layers, mapping to 256-dimensional features, followed by a 1-D batch normalization\nand ReLU non-linearity. Finally, the last FC layer with Sigmoid activation function maps this to\n$I$-dimensional output, producing $\\hat{C} = f(E, H) = f_{pred}(f_{EMG}(E), f_{hand}(H)) \\in [0, 1]^I$. The predicted\npressure $P$ is defined as $P = 2P_{max} \\cdot ReLU(\\hat{C} - 0.5) \\in [0, P_{max}]$, thus completing the feature fusion\nand pressure prediction."}, {"title": "4.3 Joint Training of Classification and Regression", "content": "The objective function comprises two key components: a classification loss $L_c$ and a regression loss\n$L_r$. The classification loss is designed to accurately identify whether any pressure is exerted by a\nparticular region of the hand (i.e., fingertips or palm areas shown in Figure 5 of Section B.1.1), using\na cross-entropy loss to distinguish between pressure and no-pressure instances for each hand region i:\n$L_c = \\frac{1}{I} \\sum_{i=1}^I C_i \\cdot \\log \\hat{C_i} + (1 - C_i) \\cdot \\log (1 - \\hat{C_i}),$ (1)\nwhere $C_i$ is the ground-truth label for region i, and $\\hat{C_i}$ is the predicted probability of pressure\napplication. Here, $C_i = 1$ indicates the presence of pressure in the i-th region, while $C_i = 0$ indicates\nits absence. In contrast, the regression loss, targets the accurate quantification of pressure levels using\nan $L_2$ loss to minimize the difference between the predicted and actual pressure values:\n$L_r = \\sum_{i=1}^I |P_i - \\hat{P_i}|^2,$ (2)\nwhere $\\hat{P_i}$ represents the model's predicted pressure for region i and $P_i$ is the corresponding actual\npressure. Our dataset contains pressures from 0~20 N. Therefore, our model predicts pressure values\nin [0, 20], organized by $P_{max}$, the maximum of the pressure. To integrate these two aspects into a\nunified training objective, we introduce a balancing hyper-parameter $\\lambda$, resulting in a combined loss\nfunction: $L = L_c + \\lambda \\cdot L_r$. This composite loss enables our model to not only discern the presence\nof pressure but also quantify its magnitude accurately."}, {"title": "4.4 Estimation without the Glove", "content": "For the training phase, we employed data acquired from our data collection system to ensure the\naccurate capture of exerted hand pressure and 3D hand pose data. However, during the inference\nphase, our framework exploits off-the-shelf hand pose detectors [68-70], which extract 3D hand pose\nfrom RGB or RGB+D inputs. These detectors can been chosen for their high accuracy and robustness\nin various conditions, ensuring reliable performance during inference. Thus, users can interact\nwith external objects using their bare hands, without the need for additional hand-worn equipment.\nThis approach ensures our model's practical applicability in real-world scenarios, prioritizing user\nconvenience and natural interaction. By leveraging readily available technology, we make it easier"}, {"title": "5 Experiments", "content": "In Sections 5.2.1 and 5.2.2, where ground truth hand pressure is necessary, data was collected while\nparticipants wore the pressure glove, and hand postures were obtained from the data glove. We\nalso conducted qualitative evaluations (Section 5.2.3 and the demo video) without ground truth,\nwhere data was collected without any gloves, and hand postures were inferred solely from RGB\nimages using an off-the-shelf hand pose detector. For this purpose, we employed the pre-trained\nAttention Collaboration-based Regressor [69], which has demonstrated superior performance with a\nmean per joint position error (MPJPE) of approximately 8mm for reconstructing hand poses from\na single RGB camera. The high accuracy ensures the reliability of our hand posture inferences in\nqualitative assessments. To assess our model's performance, we utilize three metrics: Coefficient of\nDetermination (R2), Normalized Root Mean Squared Error (NRMSE), and classification accuracy.\nThe exact definitions and explanations of evaluation metrics can be found in Section D.2. Refer to\nSection D.3 and D.4 for additional quantitative and qualitative restuls, respectively."}, {"title": "5.1 Comparative Methods", "content": "This study compares the proposed model against several baseline and state-of-the-art methods to\nvalidate its effectiveness in whole-hand pressure estimation. To ensure a fair comparison, we selected\nmethods that quantitatively measure the pressure applied by the hand, rather than solely identifying\nhand contact. Detailed information about the implementation of comparative methods can be found\nin Section D.1. The methods included in the comparison are:\nSEMG Only Model [4]. An sEMG-based approach decodes finger-wise forces in real-time, demon-strating the potential of muscle activation patterns in informing hand activities. This method em-phasizes using electromyography sensors for understanding complex hand dynamics but does notincorporate hand posture information.\n3D Hand Posture Only Model. A variation of our proposed framework that solely utilizes 3D hand\nposture for pressure estimation, omitting the sEMG signal input. This model tests the efficacy of\nhand posture information in isolation.\nSEMG + Hand Angles Model. This model represents a variation of our proposed framework, where\ninstead of utilizing the 3D representation H for hand pose, it employs the angular representation \u03b8 of\nhand joints as the input to the hand pose feature extractor $f_{hand}$. By substituting the 3D hand pose\nwith direct angle measurements of hand joints, this baseline aims to highlight the benefits of using a\n3D representation for hand pose in multimodal sensing.\nPressureVision++ [17]. This vision-based deep learning model estimates hand pressure from a single\nRGB image by identifying visual cues related to hand pressure application, showcasing the use of\nvisual information for pressure estimation without physical FSR sensors.\nPiMForce (Ours). The comprehensive model enhances sEMG signals by leveraging 3D hand\nposture information for continuous and detailed pressure estimation across the whole hand. This\napproach aims to mitigate the limitations of sEMG-based methods by integrating the strengths of\nboth modalities for enhanced pressure prediction accuracy."}, {"title": "5.2 Results", "content": "We analyze the performance of our proposed framework in comparison to these methodologies,\nboth quantitatively and qualitatively. Additionally, we investigate the capabilities of our model to\naccurately estimate hand pressure across a variety of hand postures and parts, providing a thorough\nassessment of its performance. Our framework enhances sEMG signals by leveraging 3D hand posture\ninformation for detailed palm pressure data collection, contrasting with PressureVision++, which\nrelies on visual cues for force estimation. This approach is designed to underscore the distinctive\nbenefits of our multimodal sensing framework in capturing a broad range of hand interactions."}, {"title": "5.2.1 Do hand pose and sEMG signals together improve pressure estimation?", "content": "Table 2 outlines the performance metrics of various comparative models, including the sEMG Only\nModel, the 3D Hand Posture Only Model, the sEMG + Hand Angles model, and our model. PiMForce\nremarkably outperforms the comparative methods, achieving an accuracy of 83.17%, NRMSE of\n6.65%, and an R2 value of 88.86%. This demonstrates the comprehensive capability of our model to\naccurately classify and quantify the pressures exerted by the hand.\nThe integration of 3D hand posture and sEMG information in our framework shows a clear advantage\nover approaches relying on a single data modality, as expected. The sEMG Only Model and the\n3D Hand Posture Only Model show limited pressure estimation performance when compared to\nour integrated approach. Interestingly, the improvement in performance with the sEMG + Hand\nAngles model over the sEMG Only Model is marginal (less than 0.5%p improvements in all metrics).\nThis highlights the importance of incorporating a comprehensive 3D hand posture representation.\nBy embedding comprehensive hand posture knowledge to be used with sEMG data, we develop an\neffective multimodal approach to capture nuanced variations in hand pressure exerted across different\nhand regions and postures.\nCross-user performance assesses how well the model performs on data from individuals not included\nin the training set, which is crucial for real-world applications. As shown in Table 3, our proposed\nmethod combining sEMG signals with 3D hand posture data significantly outperforms the sEMG-only baseline across all\nevaluation metrics in cross-user scenarios. This demonstrates the enhanced\ngeneralizability and effectiveness of our approach in estimating hand pressure among different\nusers. To further demonstrate the performance of our model over time, we present Figure 23 in\nsupplementary material, which illustrates the temporal evolution of both ground truth and predicted\npressure values for all nine hand regions during consecutive TM-Press and Medium Wrap actions."}, {"title": "5.2.2 How does accuracy vary by hand region and posture type?", "content": "We delve into the performance of our model across various hand regions and posture types, utiliz-ing data represented in both Table 4 and Figure 3. This analysis highlights the noticeable impactof incorporating 3D hand pose data, particularly noting a greater improvement in hand palm re-gions (+1.95%p) over fingertips (+1.05%p) compared to the sEMG Only Model. This distinctionemphasizes the crucial role of 3D hand pose for accurate pressure estimation in diverse hand postures.\nOur findings reveal that the model achieves superior pressure estimation in Press and Pinch inter-actions, with classification accuracies surpassing 90% and NRMSE values maintained below 6%.However, it encounters challenges with specific postures such as Palm-Press, which, despite a lowerclassification accuracy of 68.42%, still shows a high regression accuracy of 3.12%. When examiningGrasp postures, our model shows a slight dip in performance relative to Press and Pinch, withNRMSE values ranging between 5~8%. This suggests a moderate pressure estimation capability forthese more complex interactions, yet the model consistently maintains a high $R^2$ range of 0.8 to 0.9across all posture types. This consistent correlation between predicted values and actual pressuremeasurements highlights the model's ability to maintain high accuracy and reliability across a diverse"}, {"title": "5.2.3 Can inference succeed with an off-the-shelf hand pose detector?", "content": "For practical inference applications without a data glove, solely relying on EMG data and incorporat-ing 3D hand pose information obtained through an off-the-shelf hand pose detector, we demonstratethe adaptability of our model in Figure 4a. This comparison with the vision-based method, Pressure-Vision++, showcases our PiMForce's capability to estimate hand pressures robustly during diverseinteractions. During hand-plane interactions, specifically those involving the tip Press type posture,both approaches appear to perform well. However, our analysis reveals vulnerabilities in handlingmore complex Grasp and Pinch motions when using PressureVision++. Furthermore, PressureVision++ requires complete visibility of all fingers within the camera's view due to the high relianceon visual cues for pressure inference. In contrast, our framework effectively utilizes the estimatedhand pose as long as the hand pose information is sufficiently accurate for inference. This capabilityunderscores the practicality of our method, facilitating more natural user interactions with externalobjects without the constraints of direct visibility or glove use. Figure 4b shows demo video footageillustrating our PiMForce's capability to accurately estimate hand pressure while continuously chang-ing hand posture, pressure levels, and the objects being grasped. This demonstrates the flexibility andreliability of our approach in real-world scenarios.\nTo further substantiate our model's effectiveness using an off-the-shelf hand pose detector, weconducted a quantitative comparison with PressureVision++, as presented in Table 5. PiMForcedemonstrates largely better performance across all fingertips during both plane and pinch interactions,indicating superior performance in estimating hand pressures compared to PressureVision++. Thisquantitative evaluation confirms that our framework outperforms existing vision-based methods interms of accuracy and robustness during diverse interactions."}, {"title": "6 Conclusion", "content": "In this paper, we introduce PiMForce, a pioneering framework for hand pressure estimation byintegrating 3D hand posture information with muscle activation signals from forearm-worn sEMG.By embedding 3D hand posture information into a deep neural network, we enable the model toprocess this data alongside sEMG signals, enhancing its capability to learn complex relationship between muscle activations and hand pressure distributions. This novel approach is the first tocombine these modalities, providing a comprehensive analysis of hand dynamics across variousinteractions. We developed a unique multimodal hand data collection system and protocol, capturinga dataset that includes hand pressure, posture, and electromyography signals. Our method notablyimproves upon previous techniques, enabling accurate whole-hand pressure estimation throughdetailed hand posture information. Extensive quantitative and qualitative comparisons demonstratedthe consistent superiority of our framework over existing sEMG-based and vision-based methods."}, {"title": "A Overall Structure of Supplementary Material", "content": "This supplementary material provides added details to complement the main manuscript. Section B\nelaborates on the specifics of the multimodal dataset creation. Section C offers an extended description\nof the methodological framework employed in our study. Section D describes implementation details\nof comparative methods and showcases additional results to further support the findings reported in\nthe main text. Lastly, we note limitations and future works in Section E."}, {"title": "B Detailed Description for the Multimodal Dataset", "content": null}, {"title": "B.1 Data Collection Hardware", "content": null}, {"title": "B.1.1 Pressure Sensor Glove Customization", "content": "To effectively capture comprehensive data on hand pressure and posture, we modified a tactile glove\nequipped with 65 pressure sensors by integrating additional fingertip pressure sensors along with a\nposition tracking module, as shown in Figure 5. We customized the design of the glove to focus on the\nhand pressure measurements across 5 fingertips and 4 palm regions consisting of 16 pressure sensor\nnodes. To determine the pressure in each region, we use the maximum value among the sensors within\nthat region rather than summing or averaging their readings. This approach accounts for variations in\nhand size, ensuring that the pressure measurement is not artificially lowered due to inactive sensors\nthat may not be engaged by all participants."}, {"title": "B.1.2 Pressure Sensor Characteristics", "content": "For fingertip pressure measurements, we utilized a Force-Sensing Resistor (FSR) type pressure\nsensor (RA18DIY, Marveldex), characterized by a force range of 0 ~ 40 N/cm\u00b2 and a thin profile\nof 0.7 mm with an 8 mm diameter sensing area. Figure 6 presents the typical response behavior\nof the FSR, highlighting the force-resistance relationship. In our work, we derived a precise fitting\nmodel for the sensor's output through repetitive calibrations using a push-pull gauge. The calibration\nprocess involved conducting 30 load-unload cycles (ranging from 0 ~ 30 N, incremented by 1 N)\nand meticulously recording the resistance values to ensure accuracy."}, {"title": "B.2 Defined Hand Postures", "content": "For the grasp postures, we carefully selected 10 representative movements from a comprehensive\ngrasp taxonomy presented in recent literature [55]. This taxonomy classifies grasps based on several\nfactors, including opposition type, virtual fingers, grip type, and thumb position, culminating in"}, {"title": "B.3 Data Collection Protocol", "content": "As illustrated in Figure 10, we developed software tools to facilitate user convenience and high-quality data acquisition. These tools provide posture-specific guide images to make it easier forusers to follow along, as well as visual feedback that allows monitoring of EMG, pressure, and handmovements in real time. Additionally, they offer information on data collection and rest times foreach posture, enabling systematic data gathering with time synchronization."}, {"title": "B.4 Data Processing", "content": null}, {"title": "B.4.1 SEMG Signal and Pressure Data", "content": "With a frame length of 1,248 samples, corresponding to 0.624 seconds, we utilized a Short Time\nFourier Transform (STFT) for sEMG signal. This method, leveraging a window length of 256 and a\nhop length of 32 samples, employed a Hamming window function to minimize spectral leakage. To\npreprocess the sEMG signal, we applied the STFT and considered only the signals below 64Hz to\neliminate high-frequency noise, optimizing the signal's relevance for muscle activity analysis [4]. For\nthe pressure data, considering the conventional force that a human can comfortably apply with their\nhand, we propose a maximum inferable force of 20 N, and forces exceeding 20 N were clipped to be\ntreated as 20 N. Additionally, to exclude noise data caused by subtle sensor presses, values below 0.2\nN were processed to be treated as 0."}, {"title": "B.4.2 3D Hand Pose", "content": "For the 3D hand pose data, we use data obtained from the Quantum Mocap Metaglove for the training dataset. The hand skeleton model adopted in this study is as shown in Figure 11. The joints of the fingers from the middle to the ring finger are omitted because they are defined identically to those of the index finger. The raw data obtained through the data glove represents the hand as 20-dimensional angular data, denoting each finger with one abduction angle and three flexion angles [72"}]}