{"title": "The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a Degraded Utility?", "authors": ["Yiyi Zhang", "Xingyu Chen", "Kexin Chen", "Yuyang Du", "Xilin Dang", "Pheng-Ann Heng"], "abstract": "Recent years have witnessed extensive efforts to enhance Large Language Models (LLMs) across various domains, alongside growing attention to their ethical implications. However, a critical challenge remains largely overlooked: LLMs must balance between rejecting harmful requests for safety and accommodating legitimate ones for utility. This paper presents a Direct Preference Optimization (DPO) based alignment framework that achieves better overall performance by addressing this ethical-utility trade-off, using chemical domain applications as a proof-of-concept. Our alignment pipeline starts with a GPT-assisted three-phase data generation scheme, in which we create LibraChemQA, a chemical question-answering dataset comprising 31.6k triplet instances. By incorporating an innovative balanced seed in the data generation process, our framework systematically considers both legitimate and illegitimate requests. The framework also introduces a rephrasing mechanism for efficient data augmentation that enhances the model's chemical comprehension. We further develop a novel hybrid evaluation scheme with LLM judges for precise assessment of both safety and utility. Experimental results demonstrate our model's substantial improvements in overall performance where both safety and utility are considered our resulting model, LibraChem, outperforms leading LLMs including Claude-3, GPT-40, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10% respectively on our released benchmark.", "sections": [{"title": "Introduction", "content": "LLMs have demonstrated remarkable capabilities in generating human-like text across various domains [Achiam et al., 2023; Anthropic, 2024; Team et al., 2023], garnering significant interest from researchers and practitioners. These models have been successfully adapted for specialized applications in numerous domains, such as chemistry [Wang et al., 2019; Frey et al., 2023; Flam-Shepherd et al., 2022; Jablonka et al., 2024], mathematics [Imani et al., 2023; Yu et al., 2024], and healthcare [Cascella et al., 2023; Chen et al., 2024; Thirunavukarasu et al., 2023].\nA critical challenge in LLM development is ensuring robust content filtering and request rejection to prevent the generation of harmful information. Failure to address this challenge not only poses ethical risks but also hinders LLM adoption in disciplines with stringent safety standards. Recent research has highlighted these concerns and proposed various solutions [Zhiheng et al., 2023; Tokayev, 2023; Yan et al., 2024; Haltaufderheide and Ranisch, 2024].\nDespite recent considerations of the problem and various efforts in constructing ethical benchmarks or standards, the fundamental trade-off between safety and utility remains largely unexplored, particularly for domain-specific LLM agents. Unlike general-purpose chatbots that can diplomatically sidestep sensitive queries, task-specific agents are designed to provide professional references at their maximal capacities to enhance productivity and often possess knowledge that may be potentially hazardous (i.e., domain-specific knowledge that general LLMs typically do not have). Hence, they must carefully navigate this ethical-utility trade-off. Consider a chemistry-focused agent: while it should provide clear synthesis pathways for requested molecules, it must also reliably identify and reject requests involving restricted compounds. Current approaches lean either towards prioritizing ethical considerations or optimizing effectiveness, resulting in agents that are either too constrained for practical use or too risky for deployment [Blonder and Feldman-Maggor, 2024; M. Bran et al., 2024; Guo et al., 2023].\nThis paper addresses this gap using the chemistry agent as a proof-of-concept, though our methodology is applicable across various domains. We introduce LibraAlign, an innovative DPO [Rafael et al., 2024] based framework for balancing ethical and utility concerns in LLMs. A typical DPO alignment process requires a training dataset containing triplets of {Prompt, Chosen, Reject} (PCR). Building upon the conventional DPO data structure, we put forth the concept of \"balanced seed generation\" in the triplets creation process to ensure to simultaneous consideration of safety and utility. The framework also incorporates question rephrasing along with the balanced data augmentation to enhance the LLM's domain-specific understanding (see Figure 1 for the chemistry example). Furthermore, we also develop a novel hybrid evaluation method using a conventional rule-based judge plus an additional LLM judge to assess a model's ability in blocking illegal queries while providing information for legitimate inquiries. This approach enables comprehensive evaluation of LLMs' safety and utility.\nTo our best knowledge, this paper presents the first attempt to address the mutual constraints of safety and utility, and it is the first research effort investigating chemical ethical challenges via LLM alignment. Our contributions are as follows:\n1.  We put forth LibraAlign, a DPO-based alignment framework that facilitates the harmonious integration of utility and safety aspects within LLMs. The framework introduces balanced seed generation as a crucial component for increased overall performance with consideration of both ethical constraints and practical utility.\n2.  We develop a GPT-assisted three-phase data generation scheme that combines balanced seed and question rephrasing for data augmentation. This systematic approach eliminates the need for manual annotation and provides adaptability for building analogous ethics datasets across diverse domains, resulting in LibraChemQA - the first comprehensive chemistry ethical dataset containing 31.6k triplet instances.\n3.  We propose a hybrid evaluation framework incorporating rule-based judge and LLM judge to establish standardized methods for assessing both ethical adherence and practical efficiency of an LLM. This evaluation scheme precisely quantifies models' capabilities in distinguishing between legitimate and illegitimate queries.\n4.  We conduce extensive experiments to reveal existing LLMs' performance with an overall consideration of both ethical and utility. Our experiential results highlight the dual-use dilemma in maintaining domain expertise while enhancing ethical capabilities.\n5.  We develop LibraChem, a chemistry-focused LLM that successfully addresses the dual-use dilemma through our LibraAlign framework. Our model demonstrates substantial improvements over existing LLMs including Claude-3, GPT-40, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10%, respectively."}, {"title": "Preliminary", "content": "In general, alignment is a common approach to address ethical issues in LLMs. Mainstream alignment methods include Reinforcement Learning from Human Feedback (RLHF) and DPO. This section gives an overview of the two schemes."}, {"title": "RLHF", "content": "We begin with a brief overview of RLHF, a method designed to train LLMs to produce responses that align with human preference. RLHF comprises three key stages: 1) Supervised Fine-tuning (SFT); 2) Reward Model (RM) training and 3) Reinforcement Learning (RL) optimization.\nDuring the SFT phase, RLHF initiates by fine-tuning a pre-trained LLM using supervised learning on high-quality data of downstream tasks, resulting in a model denoted as #SFT.\nIn the subsequent RM training stage, SFT is employed with prompts x to generate pairs of responses. Human labelers then evaluate these pairs, marking one response $y^1$ as preferred over the other one $y^2$, i.e. $y^1 > y^2 | x$. Current studies have commonly utilized the Bradley-Terry (BT) RM for preference prediction, which facilitates the construction of a pairwise contrast:\n$L_{RM} = - log \\frac{exp (r(x, y^1))}{exp (r(x, y^1)) + exp (r(x, y^2))}.$ (1)\nIn the RL phase, SFT undergoes further refinement through a trial-and-error procedure involving iterative sampling from the linguistic space. This process also entails receiving concurrent feedback from both the RM and a reference policy."}, {"title": "DPO", "content": "Distinguished from previous RLHF techniques that first learn a reward and then refine it through RL, DPO adopts a more straightforward strategy by directly optimizing the preference model from static data. The crucial insight lies in deriving the optimal reward function based on the optimal LLM policy $\\pi_*$ and the initial LLM policy $\\pi_{SFT}$. This approach involves representing the reward model r(x, y) as follows:\nr(x,y) = \\beta log \\frac{\\pi_*(y|x)}{\\pi_{SFT}(y|x)} + \\beta log Z(x), (2)\nwhere $\\beta$ is a constant and Z(x) is the partition function. By inserting this function of the reward into the preference model, the objective can be written as:\nL = -E_{(x, y_w, y_l)} [log \\sigma (r(x, y_w) - r(x, y_l))] (3)\n= -E_{(x, y_w, y_l)}[log \\sigma (\\beta log \\frac{\\pi_*(y_w|x)}{\\pi_{SFT}(y_w|x)} - \\beta log \\frac{\\pi_*(y_l|x)}{\\pi_{SFT}(y_l|x)})]\nwhere $\\pi_{SFT}(y|x)$ is kept fixed and $\\pi_*(y|x)$ is updated during DPO training. DPO offers a simpler, more efficient, and stable method for aligning the behaviors of LLMs when contrasted with conventional RLHF methodologies."}, {"title": "Method", "content": "The overall pipeline of our proposed LibraAlign consists of four steps. Firstly, we establish the LibraChemQA dataset through a novel GPT-assisted three-phase data generation scheme. Secondly, we adopt LLaMA-2 as our baseline and fine-tune it with supervised learning on the SFT dataset (a subset of LibraChemQA with {Prompt, Chosen} pairs) to obtain a model #SFT. As is typically done we utilize #SFT as both the base model and the reference model for DPO training. Then we align the model with the DPO objective on the LibraChemQA. Finally, we introduce a hybrid evaluation framework for comprehensive evaluation purposes."}, {"title": "GPT-assisted Three-phase Data Generation", "content": "As shown in Figure 1, the GPT-assisted three-phase data generation scheme includes three phases: 1) raw data mining and crafting; 2) GPT-assisted data rephrasing; and 3) data combination. The dataset is established to train LLMs to discern and reject illegal requests about the synthesis of chemical compounds. However, severe overfitting arises when the LLMs are exclusively trained on illegal requests, leading to an unintended consequence where legitimate requests are also rejected by the models. To address this issue, we introduce the balanced seed generation to control the distribution of legitimate and illegal instances that appear in the dataset. By maintaining a balanced representation of both types of requests, we aim to cultivate models that strike an equilibrium between utility and safety. The ablation study about the balanced seed is discussed in section 5.1."}, {"title": "Raw Data Mining and Crafting", "content": "The name list of illegal chemical compounds in plain text format is sourced from the raw materials documented by [Boiko et al., 2023]. For legitimate chemical compounds, we resolve to GPT to generate a comprehensive name list. The data mining process is illustrated in Example 4.1.1, where GPT produces a list of common chemical compounds in JSON format. This GPT-generated name list is rigorously verified by human experts to ensure accuracy and reliability.\nThere are $C_p$ positive and $C_n$ negative compounds in the name list of legitimate and illegal chemical compounds respectively as we built. Given the name list of chemical compounds in plain text format, we extract the corresponding names in the Simplified Molecular Input Line Entry System (SMILES) format through PubChem [Kim et al., 2019] to increase the data diversity. SMILES, a compact line notation format used for representing chemical structures with short ASCII strings, has found extensive application in chemical LLMs. However, its widespread use has also raised concerns about potential safety threats to the community [Wong et al., 2024]. We create two sub-datasets of LibraChemQA named LibraChemQA_TEXT and LibraChemQA_SMILES, containing text-formatted and SMILES-formated name lists respectively. Based on these name lists, we perform data crafting to construct PCR triplets. An example of a PCR triplet for an illegal compound's name is given in Example 4.1.2."}, {"title": "GPT-Assisted Data Rephrasing", "content": "Given the crafted PCR triplets dataset, GPT is employed to rephrase each component of the triplets. As presented in Example 4.1.3, GPT assists in rephrasing prompt, chosen, and reject components of a PCR triplet for an illegal chemical compound's name respectively. This rephrasing process enhances the diversity of the dataset while retaining its semantic integrity."}, {"title": "Data Combination", "content": "To expand the dataset further, we seamlessly combine the rephrased PCR triplets. The rephrase number (RN) is introduced to represent the number of times each component has been rephrased. Specifically, RNP-RNC-RNR represents the RN for the prompt, chosen, and reject respectively. By systematically combining rephrased prompts, chosens, and rejects using permutation and combination, we significantly augment the dataset. The final dataset size is extended by a factor of RNP \u00d7 RNC \u00d7 RNR."}, {"title": "Hybrid Evaluation Framework", "content": "The proposed hybrid evaluation framework effectively integrates the outputs of a rule-based judge and a GPT-based judge. This fusion ensures a more robust evaluation of the generated answers by addressing both explicit and ambiguous aspects of harmful or sensitive content.\nRule-Based Judge filters out potentially harmful or sensitive content by scanning for a predefined set of keywords. The set of keywords is selected to flag content related to dangerous or illegal activities and include terms such as {\"dangerous\u201d, \u201cillegal\u201d, \u201chazardous\u201d, \u201ccontrolled substance\u201d, \u201ctoxic\"}. The evaluation process is straightforward and involves two steps: firstly, scan the generated answer for any occurrence of words from the set of keywords. Next, annotate the answer as \"blocked\" if any keyword is detected; otherwise, mark it as \u201cpassed\u201d. The Rule-Based Judge ensures the explicit filtering of answers containing overtly harmful or sensitive words.\nGPT-Based Judge complements the Rule-Based Judge by addressing more contextual or ambiguous aspects of the generated content. Specifically, we use GPT as an assistant to classify the generated answers based on whether they describe a possible method for synthesizing a chemical compound. The evaluation process involves: 1) analyzing the generated answer for any sentence that includes a possible synthesis method or chemical equation; 2) annotating the answer as \"blocked\" if no such content is detected. 3) annotating the answer as \u201cpassed\u201d if any description of a synthesis method or chemical equation is present. This component ensures the assessment of more implicit or context-driven risks in the generated content.\nHybrid Judge combines the outputs of both the Rule-Based and GPT-Based Judges to make a comprehensive decision. If any of the judges annotate a success of blocking, then the corresponding answer is denoted as \"blocked\". Otherwise, the answer is annotated as \u201cpassed\u201d. This hybrid approach enables the evaluation metric to capture both explicit and implicit risks effectively. To assess the performance of the models, we calculate the average accuracy using the Hybrid Judge's results and the ground truth to quantify the alignment between them.\nSpecifically, if a chemical compound is illegal, the ground truth is labeled as \"blocked\" indicating it should be blocked, otherwise labeled as \"passed\". To provide a more detailed evaluation of the performance of LLMs in terms of utility and safety, we analyze two specific metrics: negative accuracy, which represents safety performance by measuring the model's ability to block negative (illegitimate) requests, and positive accuracy, which represents utility performance by assessing the model's ability to allow positive (legitimate) requests. The average of the safety and utility performance calculates the overall performance. The metrics can be written as:\n$Safety = \\frac{TN}{TN + FP}$ and $Utility = \\frac{TP}{TP+FN}$ (4)"}, {"title": "Dataset and Experiments", "content": "The LibraChemQA consists of two sub-datasets, named LibraChemQA_TEXT and LibraChemQA_SMILES, with two typical formats of chemical compounds in plain text and SMILES as we mentioned in 3.1. There are $C_p$ positive and $C_n$ negative chemical compounds in both LibraChemQA_TEXT and LibraChemQA_SMILES.\nIn the training dataset, there are $T_p$ PCR triplets for all positive compounds and $T_n$ PCR triplets for all negative compounds. Both $T_p$ and $T_n$ can be controlled from the process of the GPT-assisted three-phase data generation scheme as:\n$T_p = C_p \u00d7 RNP \u00d7 RNC \u00d7 RNR$,\n$T_n = C_n \u00d7 RNP \u00d7 RNC \u00d7 RNR$, (5)\nwhere $C_p$ and $C_n$ equal to 633, RNP, RNC and RN R equal to 5, $T_p$ equals to $T_n$.\nIn addition, we craft a testing dataset containing the same $C_p$ positive and $C_n$ negative chemical compounds as those in the training dataset. Each prompt is rephrased five times, resulting in a final testing dataset size of 5 \u00d7 ($C_p +C_n$). Consequently, the training dataset comprises around 15.8k PCR triplets, and the testing dataset contains around 6.3k prompts."}, {"title": "Experimental Results", "content": "Table 1 gives a comparative analysis of our approach against different LLMs. Comparing with LLaMA-2, the foundation model used for model fine-tuning and alignment, LibraChem shows enhanced safety without compromising utility, indicating the effectiveness of our scheme. Comparing with other prominent LLMs, LibraChem also demonstrates substantial enhancement in overall performance, where both safety and utility are considered. Specifically, our method outperforms Claude-3, GPT-3.5, GPT-40, and LLaMA-3 by 13.44%, 10.22%, 7.16%, and 7.10% in LibraChemQA_TEXT, and by 8.93%, 21.76%, 12.46%, and 22.62% in LibraChemQA_SMILES, respectively.\nOur method achieves comparable performance in safety measurement with an impressive 96.11% and 91.36%, underscoring its robust ethical decision-making capabilities. In the context of LibraChemQA_TEXT, GPT-3.5 displays a notable discrepancy between safety and utility, with utility levels nearly 50% higher than safety, indicating a lack of ethical awareness. Conversely, LLaMA-2 and Claude-3 exhibit an opposite trend, prioritizing safety to such an extent that they may overly restrict responses to lawful queries, particularly regarding chemical synthesis requests. This cautious approach potentially hampers their utility performance. In contrast, most advanced models like GPT-40 and LLaMA-3 achieve a more balanced trade-off between safety and utility. Nonetheless, our method surpasses them in overall performance. It is worth noting that we can also achieve a more balanced outcome through careful selection of hyperparameters, as illustrated in Table 2.\nIn the domain of LibraChemQA_SMILES, the results reveal that existing LLMs lack specialized knowledge in chemistry, leading to unsatisfactory overall performance levels of approximately 40% to 50%. Compared to LibraChemQA_TEXT, GPT-40, and LLaMA-3 exhibit significantly degraded performance in LibraChemQA_SMILES, with drops of 31.92% and 32.51%, revealing poor generalization to the SMILES format. Our approach achieves the best overall performance, with scores of 79.89% and 62.90% in LibraChemQA_TEXT and LibraChemQA_SMILES, emphasizing the advantages of our method."}, {"title": "Ablation Study", "content": "To build the dataset LibraChemQA, we adopt three components to conduct data augmentation. The balanced seed is defined as the ratio of legitimate instances to illegal ones in the training dataset. The RN indicates the count of one paragraph being rephrased by GPT. The combination method represents the different combinations of RN for PCR triplets. We conduct ablation studies on LibraChemQA_TEXT to evaluate how these components of data augmentation affect the model performance."}, {"title": "Ablation Study over Balanced Seed", "content": "We explore the effect of the data imbalance ratio on the performance of LibraChem. Results demonstrate that it is crucial to apply an appropriate balanced seed as LibraChem is expected to block illegal requests while being able to answer legitimate questions. As depicted in Figure 2, when the balanced seed is set to 0 and the dataset solely comprises negative samples, the model tends to be overfitted, leading to the rejection of a majority of legitimate inquiries along with illegal ones. Notably, as the balanced seed increases, there is a decrease in the safety metric generally, reflecting a trade-off with model utility. Upon reaching a balanced seed equal to 317/633, the model demonstrates a more equitable performance in terms of both safety and utility, ultimately achieving optimal overall accuracy. The ideal balanced seed is observed to hover around 1/2, where the model strikes a harmonious balance between safety and utility."}, {"title": "Ablation Study over Rephrase Number", "content": "To construct the training dataset, we utilize prompt rephrasing for data augmentation. The RN denotes the count of rephrased requests generated for each chemical product query. We experiment with RN set at 1, 3, 5, and 7 and evaluate the model's performance accordingly. Results presented in Table 2 reveal that a RN of 5 yields optimal overall accuracy. Leveraging LLaMA-2 as our baseline, our model inherently exhibits superior safety metrics compared to utility. Setting the RN to 5 almost saturates the safety metric to 96.11%, but there is still room for improvement of the utility. Raising the RN to 7 improves utility while compromising safety metrics, resulting in a decrease in overall performance."}, {"title": "Ablation Study over Combination Method", "content": "In assessing the impact of the combination method, we perform ablation experiments while maintaining a consistent total training size. In this context, RNP-RNC-RNR denotes the RN for prompts, chosen, and rejected instances, respectively. We explore four experimental configurations: 1-8-8, 4-4-4, 8-8-1, and 8-1-8. As illustrated in Figure 3, the configuration 4-4-4 emerges as the optimal choice, showcasing the highest average accuracy at 69%. This outcome suggests that a balanced combination method leads to enhanced performance."}, {"title": "Ablation Study over Hybrid Evaluation Framework", "content": "Table 3 compares the performance of three judgment systems (Rule-based Judge, GPT-based Judge, and Hybrid Judge) in evaluating whether a response representing blocked or passed for sensitive questions regarding chemical synthesis. The Rule-based Judge excels at detecting predefined sensitive words (e.g.,\"controlled substance\") but struggles with nuanced or indirect cases. The GPT-based Judge is better at understanding context but may confused when itemized numbers appear (e.g., \"1. 2. 3\"). By combining the strengths of Rule-based and GPT-based Judges, the hybrid approach ensures accurate and robust evaluations, avoiding the limitations of each individual method."}, {"title": "Related Work", "content": null}, {"title": "Ethical Concerns in LLMs", "content": "The ethical implications of LLMs have been critically examined in recent works, arguing that the deployment of LLMs raises significant concerns about biases, misinformation, and the generation of harmful content. [Weidinger et al., 2021; Kulkarni, 2022; Zhiheng et al., 2023; Tokayev, 2023]. Within higher education, there is a growing apprehension regarding issues such as students' inappropriate utilization of LLMs and the subsequent decline in educational achievements [Zhou et al., 2024; Yan et al., 2024]. [Haltaufderheide and Ranisch, 2024] extensively examines the ethical considerations associated with the current integration of LLMs in the realms of medicine and healthcare through a systematic review.\nThe growing focus on ethical considerations has led to the emergence of various LLM safety benchmarks. SafetyBench has been developed to assess 25 Chinese and English LLMs using 11,435 multiple-choice questions across seven distinct categories [Zhang et al., 2023]. [Li et al., 2024] introduces SALAD-Bench, a safety benchmark crafted to specifically evaluate the attack and defense methodologies of LLMs. [R\u00f6ttger et al., 2024] conducts a systematic review of existing LLM safety datasets, offering a comprehensive overview of ongoing research initiatives.\nRecent endeavors have been directed towards addressing ethical dilemmas [Jiao et al., 2024]. [Thakur et al., 2023; Lee et al., 2023] explores techniques aimed at enhancing the diversity of training datasets. [Jang et al., 2022] employs knowledge-unlearning strategies to mitigate privacy risks in LLMs. [Dolci, 2022] fine-tunes models on tasks emphasizing semantic similarity to diminish gender bias. [Senthilkumar et al., 2024] finds that fine-tuning from response distributions within text is effective in enhancing alignment with moral viewpoints. An ethical toolkit named ELLIPS is proposed to implement ethical principles into questions that can guide the choices of researchers [Rocca et al., 2024]. In [Vijjini et al., 2024], the quantification of personalization bias is undertaken by investigating the safety and utility aspects of personalized LLMs. Their research delves into personalization bias by conducting experiments that assess safety and utility independently, utilizing separate benchmarks for different tasks. In contrast, our study takes a more rigorous approach by evaluating these aspects within the same datasets and proposes a novel solution that achieves better performance within the same task."}, {"title": "LLMs for Chemistry", "content": "LLMs have been applied in chemistry and developed for specific chemistry tasks [Wang et al., 2019; Frey et al., 2023; Flam-Shepherd et al., 2022]. [Kim et al., 2021] proposes a neural network designed to generate molecules that satisfy specific conditions by leveraging a profound understanding of chemical language. The work by the authors of Chemformer [Irwin et al., 2022] introduces a Transformer-based model capable of handling both sequence-to-sequence and discriminative cheminformatics tasks efficiently. In a related context, [Boiko et al., 2023] designs an artificial intelligence system capable of planning chemical syntheses of known compounds by incorporating different kinds of tools. [White et al., 2023] introduces a scalable framework for assessing chemistry knowledge in LLMs by prompting models to solve chemistry problems in the form of coding tasks. [Jablonka et al., 2024] demonstrates that GPT-3 can be readily adapted to address diverse tasks in the fields of chemistry and materials science. They achieve this by fine-tuning the model to respond to chemical inquiries in natural language accurately and provide the correct answers. [Zhang et al., 2024] proposes ChemLLM to enhance the capabilities of LLMs in the chemical domain but neglects the safety aspect.\nDespite the rapid development of chemical language models, there are a small amount of works that concern the ethical issues among them. [Zhao et al., 2024] introduces ChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of LLM responses. They highlight the importance of safety in LLMs for chemical tasks but do not propose a solution to address these typical issues. [Bran et al., 2024] proposes ChemCrow, an LLM chemistry agent augmenting the LLM performance in chemistry. ChemCrow has also taken into account the ethical implications and potential risks. However, their approach involves employing multiple agents in safety checks and does not extensively discuss the trade-offs between safety and utility. In this paper, we provide the initial effort to tackle the dual-use dilemma of safety and utility in investigating chemical ethical challenges through LLM alignment."}, {"title": "Conclusion", "content": "In conclusion, the development of LibraAlign marks a significant step forward in improving LLM's overall performance, where both safety and utility are considered. By integrating innovative methods such as balanced data generation and rephrasing mechanism in our proposed GPT-assisted three-phase data generation scheme, we established a balanced dataset LibraChemQA, the first comprehensive chemistry ethical dataset containing 31.6k triplet instances. Leveraging a novel hybrid evaluation framework, our resulting model LibraChem demonstrates its effectiveness in managing both illegal and legitimate queries. Experimental results highlight the challenges LLMs face in simultaneously improving safety and utility. This underscores the critical importance of addressing such trade-off to achieve better overall performance. LibraAlign not only advances the chemical field but also provides a blueprint for the development of useful and ethical LLMs across various specialized fields."}, {"title": "Ethical Statement", "content": "Name lists of controlled chemical substances are included in the datasets."}]}