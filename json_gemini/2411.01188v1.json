{"title": "Learning Rules Explaining Interactive Theorem Proving Tactic Prediction", "authors": ["Liao Zhang", "David M. Cerna", "Cezary Kaliszyk"], "abstract": "Formally verifying the correctness of mathematical proofs is more accessible than ever, however, the learning curve remains steep for many of the state-of-the-art interactive theorem provers (ITP). Deriving the most appropriate subsequent proof step, and reasoning about it, given the multitude of possibilities, remains a daunting task for novice users. To improve the situation, several investigations have developed machine learning based guidance for tactic selection. Such approaches struggle to learn non-trivial relationships between the chosen tactic and the structure of the proof state and represent them as symbolic expressions.\nTo address these issues we (i) We represent the problem as an Inductive Logic Programming (ILP) task, (ii) Using the ILP representation we enriched the feature space by encoding additional, computationally expensive properties as background knowledge predicates, (iii) We use this enriched feature space to learn rules explaining when a tactic is applicable to a given proof state, (iv) We use the learned rules to filter the output of an existing tactic selection approach and empirically show improvement over the non-filtering approaches.", "sections": [{"title": "Introduction", "content": "Interactive Theorem Provers (ITP), such as Coq [27], Lean [20], and Isabelle [22], are powerful tools that combine human instruction with computer verification to construct formal mathematical proofs, providing a reliable means of certification and ensuring safety in critical applications.\nThese systems operate as follows: the user specifies a goal to prove, the initial proof state. Then the user specifies tactics (an operation transforming a proof state into proof states). Certain tactics close proof states. The proof is complete if there are no remaining open proof states, i.e., the goal has been proved."}, {"title": "Background", "content": "Theorem Proving in Coq Coq is one of the most popular proof assistants and has been widely used for building trustworthy software [18] and verifying the correctness of mathematical proofs [10]. Coq tactics are proof state transformations that provide a high-level combination of underlying logical inferences. To illustrate how theorems are formalized in Coq, we present a simple example in Figure 1. Here, we want to prove the associative property of addition. The natural numbers in Coq are defined by two constructors O and S. O denotes 0, and Sn denotes n + 1. Here, the initial proof state is the same as the statement of the theorem. We first apply induction on n and obtain two cases corresponding to the two constructors. In the first case, n equals 0. After some simplifications, we can prove 0 = 0 by the tactic reflexivity. The second case is a bit more complicated, and we need to apply the induction hypothesis IHn to finish the proof. Figure 1 also presents a concrete example of a proof state. A proof state consists of a goal to prove and several hypotheses. The goal is below the dashed line. IHn, n, m, and pare the names of hypotheses. A proof state is often represented as a sequent  \u0393\u22a2g where \u0393 and g denote the hypotheses and the goal, respectively.\nAdaptations of k-NN to Theorem Proving Several machine learning algorithms have been adapted to theorem proving tasks. In most cases, simpler algorithms adapted to formal reasoning tasks perform better than deep learning based methods in practice. For this reason, modified k-NN (explained in [2]) is the main algorithm for TacticToe and Tactician. Even if evaluations with deep learning or large tree-based classifiers have shown some theoretical improvements, the simpler algorithms training for the particular theories developed by users, give a larger practical advantage to the users. As such, we focus on the modified k-NN in this work. Standard k-NN starts by calculating the distance between a new proof state and all known proof states in the database. The distance is measured by the similarity between the features of the proof states, usually using tree walks in the AST of the proof state [16]. The dependencies of such selected neighbours, with additional scaling by their distances, inclusion of the neighbours themselves, and further modifications exhibit commendable empirical performance [24], and are therefore the default algorithm both in Tactictoe and Tactician."}, {"title": "Background Knowledge", "content": "To utilize ILP, we need to appropriately define the background knowledge. We start this section by encoding the nodes of AST as representation predicates. Then we propose new feature predicates that will allow leveraging the power of ILP. We finally add predicate anonymization, already very useful in automated reasoning systems, to representation and feature predicates."}, {"title": "Representation Predicates", "content": "Every node in the AST of the proof state is converted to a fact. There are two categories of nodes: identifiers of existing objects and constructors of Coq's datatype. A node in the goal is converted to goal_node(name, nat, goal_idx). The argument name refers to the value of the node. A unique natural number is assigned to every proof state to identify it. The argument goal_idx uses a sequence of natural numbers to specify the position of the node in the goal. A node in a hypothesis is converted to a fact hyp_node(name, nat, hyp_name, hyp_idx). Compared to a goal_idx, a hyp_idx starts with the name of a hypothesis so that two hyp_idx from different hypotheses have different prefixes. The goal_node and hyp_node predicates are called representation predicates in this work."}, {"title": "Feature Predicates", "content": "We also define two categories of feature predicates which represent the properties of AST based on the representation predicates.\nPositional Predicates represents the relative relationships between nodes' positions. The predicate goal_left(Goal_idx1, Goal_idx2) and goal_above(Nat, Goal_idx1, Goal_idx2) respectively checks whether the node is left (above) to another node in the goal. They are inspired by the horizontal features and vertical features used in previous works [4,30]. Similarly, we define hyp_left(Hyp_idx1, Hyp_idx2) and hyp_above (Nat, Hyp_idx1, Hyp_idx2). Previous works have confirmed the usefulness of using the occurrence numbers of features in feature characterization, which inspires us to develop the predicate dif (Goal_idx1, Goal_idx2). It denotes that the same node multiply occurs in different positions in the goal.\nEquational Predicates check the equality between two terms. The predicate eq_goal_term(Nat, Goal_idx1, Goal_idx2) checks that the two subterms in the goal are the same. The root nodes of the two subterms are located in the positions Goal_idx1 and Goal_idx2, respectively. It pertains to reflexivity which proves a goal of the equation if the equality holds after some normalization. Thus, it can prove x = x and inspires us to develop eq_goal_term. The predicate eq_goal_hyp_term(Nat, Goal_idx, Hyp_idx) is inspired by a number of tactics that check the equality between the goal and the hypotheses, such as assumption, apply, and auto. For instance, assumption proves a goal if it equals a hypothesis. Assume a proof state H\u2081 : Q x, H\u2082 : P x \u2192 Q x \u22a2 Q x which can be proved by assumption. The predicate eq_goal_hyp_term checks the equality between the goal and Qx in a hypothesis. The predicate is_hyp_root(Nat, Hyp_idx) ensures the node is the root of a hypothesis. Thus, it can show the equality only holds between the goal and H\u2081 instead of H\u2082. With a reason akin to that of is_hyp_root, we define is_goal_root(Nat, Goal_idx). The equality between two terms in different hypotheses is checked by eq_hyp_term(Nat, Hyp_idx1, Hyp_idx2). It is useful for tactics that can apply hypotheses several times, e.g., auto. Assume a proof state H\u2081: P x, H\u2082 : P x \u2192 Q x \u22a2 Q x. First, auto applies H\u2082 to the goal and changes the goal to P x. Then, it applies H\u2081 to prove the new goal. The description of the operation requires to show that H\u2081 equals to the premise of H\u2082."}, {"title": "Anonymous Predicates", "content": "We also substitute identifiers with more abstract descriptions to facilitate the generalization ability of ILP. The substitution is similar to that in ENIGMA anonymous [13]. The predicates that accept original nodes and abstract nodes as their first arguments are called original predicates and anonymous predicates, respectively. We substitute identifiers with their categories, consisting of inductive types, constants, constructors, and variables. Besides the abstract nodes, we also include the original nodes as arguments in goal_node and hyp_node. We need them because when checking the equality, we want to compare the original nodes. Afterward, the anonymous predicates of nodes change to\ngoal_node(anonym_name, nat, goal_idx, origin_name) and\nhyp_node(anonym_name, nat, hyp_name, hyp_idx,origin_name). Some basic identifiers are not substituted, which consist of logic_false, logic_true, and, or, iff, not, eq, bool_true, and bool_false. There are both logic and boolean values of true and false because Coq can represent objects in logic or programs. Concerning the constructors of Coq's datatypes, we only retain four important constructors: rel, prod, lambda, and evar."}, {"title": "Method", "content": "Figure 2 presents an overview of our learning framework. During the training, we first perform orthogonalization, a technique introduced in TacticToe, to clean the dataset. Then, we select examples and apply ILP to generate rules. To make predictions, first, k-NN predicts a sequence of likely helpful tactics. Afterward, the rules are used as a filter to reorder the predictions. The optimization procedure denotes removing some low-quality rules. This is achieved by evaluating the reordered predictions in the validation dataset and removing the low-quality ones. In the next subsections, we describe these parts."}, {"title": "Orthogonalization", "content": "In some cases, different tactics could transform the same proof state in the same way. This raises ambiguity and makes learning difficult. Orthogonalization is used to reduce such ambiguity. In the orthogonalization, we only focus on four very popular tactics in Coq's standard library: assumption, reflexivity, trivial, and auto. We denote the sets of proof states which can be closed by assumption, reflexivity, trivial, and auto as  A S ,  R ,  T , and  A T , respectively. There exist the relations  A S \u2286 T ,  R \u2286 T , and  T \u2286 A T . For each proof state ps to which the tactic t is applied, the above four automation tactics are sequentially tried. If ps can be finished by the automation tactic t', we replace t by t'. If none of the four tactics can finish the proof state, the original t is preserved. The orthogonalization procedure is simpler than in TacticToe, which orthogonalizes all tactics. This is because our current predicates can only capture a part of the usage of tactics. We leave full orthogonalization as future work."}, {"title": "Example Selection", "content": "Choosing appropriate training examples is crucial for learning reasonable rules. For a specific tactic tac, the proof states to which it is applied are regarded as the positive examples. The proof states to which the tactics different from tac are applied are regarded as the negative examples. We experimentally determine the number of positive and negative examples for learning rules. We develop a clustering mechanism to split positive examples into roughly equal-sized clusters. We experimentally evaluate the combinations of different numbers of negative examples and different numbers of positive examples.\nWe choose an implementation of a constrained k-means algorithm [19] to split positive examples into clusters of roughly the same size. The original k-means algorithm [11] can only split examples into a certain number of clusters. In contrast, constrained k-means can also specify the lower bound and the upper bound of the size of the clusters, which is important to give good sizes of training examples for each ILP learning task.\nWe apply k-NN to discover negative examples for each positive example. As this pre-processing step is not theorem-proving specific, we use the general k-NN from the scikit-learn library [23]. We use the same features as Tactician [30]. For each positive example, k-NN calculates the distance between it and every negative example in the training data. Then, we rank the negative examples in an ascending order of distance."}, {"title": "Training and Prediction", "content": "For each tactic, we use Aleph to generate ILP rules for each cluster of positive examples and its associated negative examples. Afterwards, all the rules are merged together, and duplicated rules are removed. Finally, we remove the rules of tactics that are logically subsumed by other rules of the same tactic.\nAlgorithm 1 illustrates the procedures of making predictions. We use the state-of-the-art k-NN in Tactician. The features are the same as those used in Section 4.2. Assume a pair of a proof state and a tactic (ps, tac). To make predictions, first, k-NN preselects a sequence of likely tactics tac\u2081..\u2085\u2080. For each tac\u1d62, we use the learned rules to determine whether to accept it. During the evaluation, the prediction tac\u1d62 is expected (unexpected) if tac\u1d62 is equal (unequal) to tac. If the rules accept (reject) a tactic, the prediction is a declared positive (negative). If the rules reject a tac\u1d62 equal to tac, we regard the prediction made by the rules as a false negative (FN). Based on the expected tactics and acceptances, we also obtain true positives (TPs), true negatives (TNs), and false positives (FPs)."}, {"title": "Rule Optimization", "content": "The idea of rule optimization is to remove some low-quality rules to increase the overall performance of rules. For the evaluation of all rules, we chose the F-1 score as the metric, defined as  \\frac{2TP}{2TP + FP + FN} , because it is a standard metric for evaluating imbalanced data. As an illustration of the imbalance, given a pair of a proof state and a tactic tac, rules make predictions for 50 preselected tactics. However, at most one is the same as tac. If a rule is overly general, which means that the number of FPs introduced by it is much larger than the number of TPs introduced by it, removing it will increase the overall F-1 score.\nAlthough a large number of negative examples prevents generating overly general rules, using them may not produce the best rules for two reasons. First, our background can merely capture a portion of the usage of the tactics; thus, a significantly large number of negative examples cannot produce perfect rules but may produce overly specific rules. Second, some negative examples in our dataset are actually false negatives. A mathematician may be able to choose the next step from a couple of tactics that make different proof transformations. Orthogonalization in Section 4.1 can only partially remove such overlaps between tactics, thereby decreasing the number of false negatives. It is computationally prohibitive to perform full orthogonalization of our data. Observe that, our experiments still show an increase in accuracy in light of the noisy data.\nOur approach allows us to learn many rules explaining a particular tactic. Over the training set, some of these rules capture the usage of the given tactic better than others. Before, moving to testing on unseen data, we prune the learned rules and keep only those that performed well on the validation set.\nTo determine which rules to include, we evaluate the quality of each rule in the validation dataset and remove those with low qualities. The left rules are used for the evaluation on the test dataset. We measure the quality of each rule and remove it if its quality is below a certain threshold. We set different thresholds and choose the threshold leading to the highest F-1 score via experiments. For the metric of the quality of a single rule, we use precision, defined as  \\frac{TP}{TP + FP} . Precision is a good metric because if a rule is too general, its precision will be low and we will be able to remove it to improve the overall F-1 score."}, {"title": "Experiments", "content": "We conducted the experiments on Coq's standard library [7]. We chose Coq's standard library as the benchmark because it is a standard dataset for evaluating machine learning for Coq. Moreover, it comprises well-crafted proofs developed by Coq experts and has been optimized for decades. Coq's standard library consists of 41 theories and 151,678 proof states in total. The code for this paper is available at https://github.com/Zhang-Liao/ilp_coq.\nMost parameters of Aleph were left as default besides three parameters. We set the maximal length of a clause to 1,000, the upper bound of proof depth to 1,000, and the largest number of nodes to be explored during the search to 30,000. We define a cost function similar to the default cost function because, by default, Aleph cannot learn with no negative examples or only one positive example. The user-defined cost function was only used when there were no negative examples or exactly one positive example. We set a timeout of ten minutes for learning.\nWe conducted the experiments in the transfer-theory setting, which means different Coq theories are used for training, validation, and testing. We use this setting because it simulates a practical application scenario of ILP. Mathematicians develop new theories based on the definitions and proven theorems in the developed theories. To be practically beneficial, ILP should also learn rules from training theories, and the learned rules should help make tactic suggestions for theories that do not depend on the training theories. The training theory Structures was chosen for training because it has a balanced distribution of various tactics. To be consistent with the transfer-theory setting, the testing theories should not depend on Structures. From Coq's standard library, we chose all theories which do not depend on Structures for testing including rtauto, FSets, Wellfounded, funind, btauto, nsatz, and MSets. Afterward, from all the theories that do not depend on the testing theories, we randomly chose five theories: PArith, Relations, Bool, Logic, and Lists, merged as the validation dataset."}, {"title": "Parameter Optimization", "content": "In Section 4, we introduced three additional hyper-parameters beyond those already present in Aleph. They are the size of the cluster of positive examples (pos), the number of negative examples of each positive example (neg), and the quality-theshold (qualt) below which the rule should be removed.\nWe evaluated the F-1 scores of different predicate categories with different parameters. There are four predicate categories AF, AR, OF, and OR, respectively denoting the anonymous feature predicates, the anonymous representation predicates, the original feature predicates, and the original representation predicates. AF and OF contain both representation predicates and feature predicates, while AF and AR only contain anonymous predicates. We chose pos between 0 and 32. For neg, we chose it between 0 and 64. For all the combinations of pos and neg, rules were generated. Afterward, the learned rules were evaluated in the validation dataset. Finally, we calculated the F-1 scores with different values of qualt. The range of qualt was set between 0 and 0.30, with intervals of 0.06."}, {"title": "Testing", "content": "According to parameter optimization, we choose the rules with the best parameter and test the performance in the test dataset. Table 2 shows the F-1 scores in the test dataset. Using a background knowledge consisting of AF predicate definitions during training results in rules which perform best during testing. This owes to that AF can learn precise rules to characterize the usage of tactics. In comparison, the rules learned by AR are too general, and the rules learned by OF and OR are too specific. In all theories, except those consisting of only a few proof states (funind has only 14 proof states), training with OF, OR, and AR background knowledge results in rules performing well on the test data F-1 scores. We also evaluated whether the combination of ILP and k-NN can improve the accuracy of k-NN. The algorithm of reordering is explained in Section 4.3. Figure 5 shows the results of the top-k accuracies in different theories. In all the theories, the combination of ILP and k-NN increases the accuracies of k-NN."}, {"title": "Case Studies and Limitations", "content": "To illustrate that we indeed learn precise rules, besides the example of simpl presented in Section 1, we present three more examples in this section. The rule of trivial suits the goal  A \u2192 B = B . First, trivial introduces A as a hypothesis, changing the proof state to H: A \u22a2 B = B. Next, trivial can automatically prove B = B. The rule of auto aligns the proof state of the structure H:B \u22a2 A \u2228 B. The tactic auto decomposes the disjunction, and the goal changes to either proving A or proving B. Then, it proves B with the hypothesis. In contrast, trivial cannot decompose the disjunction. The rule of intuition suits the goal  A \u2192 A which cannot be proved by auto. In comparison, intuition can perform stronger automation than auto and can prove it.\nAlbeit we can learn several reasonable rules, many tactics are difficult to describe. There are several reasons for the difficulties. First, our current work cannot generalize tactics with different arguments. For instance, assume there are two tactics apply H1 and apply H2 where H1 and H2 are names of hypotheses. They are regarded as different tactics but may have the same behavior. Second, the usage of some tactics such as induction is inherently complicated [21]. Third, the same mathematical theorem can proved in various ways which leads to many overlaps between the usage of tactics."}, {"title": "Related Work", "content": "There are several tasks of machine learning for theorem proving. Premise selection is probably the most well-discovered task. It studies the question of how to predict possibly useful lemmas for a given theorem. Quite a lot of classical learning methods [1,8] and neural networks [12] have been applied to premise selection. The most relevant task to our work is learning-based formal theory proving. Researchers have investigated both employing machine learning to learn from human-written proofs [9] and guide some sophisticated software to automatically construct proofs [15]."}, {"title": "Conclusion and Future Work", "content": "We have developed the first application of ILP to interactive theorem proving. For this, we have developed new feature predicates, able to dynamically calculate features based on the representation of AST of the proof state. We proposed a method for using ILP effectively for tactic prediction. We experimentally evaluated the rules learned by ILP and compared them to practically used prediction mechanisms in ITPs. The experiments confirm that the method gives explainable tactic predictions. Our work shows the potential of applications of ILP to improve ITP tactic suggestion methods.\nSeveral improvements are possible. We would like to use our work with stronger ILP systems, such as Popper. However, given that our background knowledge includes predicates with high arity and our method builds large rules with many variables, the underlying ASP (SAT) solver used by Popper struggles with the generation of models. Improvements to our encoding and recent work on improving the performance of Popper can make this research direction viable in the near future. Next, it is interesting to use ILP to capture the relations between arguments of tactics and the objects to which the arguments refer. Finally, we plan to investigate the application of ILP to other ITP tasks [31]."}]}