{"title": "I0T: Embedding Standardization Method Towards Zero Modality Gap", "authors": ["Na Min An", "Eunki Kim", "James Thorne", "Hyunjung Shim"], "abstract": "Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in down-stream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, IOTpost that reduces the modality gap approximately to zero and (2) a trainable method, IOTasync, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our IOT framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, IOTpost can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S). The code is available in https://github.com/xfactlab/I0T.", "sections": [{"title": "1 Introduction", "content": "Utilizing Vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) has been a common practice for performing multimodal tasks (Goel et al., 2022; F\u00fcrst et al., 2022; Li et al., 2023; Zhang et al., 2024; Gao et al., 2024; Sarto et al., 2023; Hu et al., 2023; Lee et al., 2024). Despite these successes, CLIP and its variants (Xu et al., 2021; Zhang et al., 2022b; Goel et al., 2022; Sarto et al., 2023; Zhang et al., 2024) suffer from a significant limitation known as the modality gap; image and text embeddings diverge in the latent space, projected to separate manifolds (Liang et al., 2022; Fahim et al., 2024). This is in contrast to the original image-text contrastive learning (CL) objective, which pulls and pushes the positive and negative pair of image and text embeddings (Radford et al., 2021), deviating from the shared statistical model representing reality (Huh et al., 2024).\nThe undesirable symptom of modality gap is that data within the same modality always have higher semantic similarity than the cross-modal data. Therefore, CLIP cannot draw an accurate semantic relationship for the data pool mixed with different modalities. This problem is especially noticeable when CLIP is extended as an automatic evaluation metric, widely used CLIPScore (CLIP-S) (Hessel et al., 2021; Sarto et al., 2023), which measures the cosine similarity between image and text embeddings. Prior approaches to mitigate the modality gap have focused on shifting (Liang et al., 2022) or training (Fahim et al., 2024; Eslami and de Melo, 2024) the embeddings of the positive pairs closer together. However, they did not attempt to find and attribute explicit factors in the image and text embeddings that lead to the modality gap. In contrast, we find the actual attributing factor of the modality gap; CLIP inadvertently learns the inherent characteristic of each modality (referred to as modality-specific characteristic in this paper), inducing similar activation patterns within the normalized embeddings of all different images (or texts) from each image (or text) encoder. These patterns, characterized by peak activations with distinct negative and positive directions for image and text embeddings (later visualized in Figure 3), significantly contribute to the modality gap. We find that it is crucial to discard not only these peak activations on a specific few dimensions but also existing modality-specific characteristics across all dimensions from each encoder to mitigate the modality gap.\nHere we propose a framework, Zero (0) Modality Gap between Image-Text embedding representations (IOT) that aims to minimize the modality gap towards zero. Correspondingly, it is also crucial to maintain rich semantic embedding representations, even if they become closely aligned. The first stage of IOT is a plug-in-play module that can be implemented with any readily available fine-tuning strategies. The second stage of IOT can be addressed with two proposed approaches. We first develop IOTpost that standardizes the normalized image and text embedding activations independently by subtracting the mean vectors of each modality and renormalizing with Frobenius normalization on the frozen encoders from the first stage.\nIOTpost offers a more explainable image captioning evaluation metric than CLIPScore (Hessel et al., 2021) by assigning a similar range of scores for across different modalities and within the same modality, attributable to the low modality gap property. However, this post-hoc embedding standardization method needs a sufficient amount of data samples with a similar distribution as a test set; hence, we present IOTasync that learns the aligned embeddings with no access to the test distribution. Our main contributions can be summarized as follows:\n\u2022 Achieving both modality gap and downstream performances is challenging; yet, we propose an IOT framework that significantly reduces the gap without hurting performances.\n\u2022 IOTpost and IOTasync significantly reduce the modality gap while enhancing text-to-image retrieval scores by 9.2% and 6.7%.\n\u2022 We are the first to propose an automatic evaluation metric, IOTScore, that can be applied to data across different modalities, overcoming the limitation of CLIPScore that only works within a single modality."}, {"title": "2 Related Works", "content": "2.1 CLIP-Based Models\nVision-language models (VLMs) have addressed multimodal tasks that require a joint understanding of visual and textual data (Liu et al., 2024; Li et al., 2022). Most modern VLMs utilize CLIP-style architectures due to CLIP's exceptional performance in zero-shot downstream tasks using pre-trained image and text encoders (Radford et al., 2021; Jia et al., 2021). However, CLIP alone shows limitations in producing consistent representations (Goel et al., 2022); Hence, CyCLIP (Goel et al., 2022) reduces the similarity of mismatched pairs of image and text (cross-modal cyclic) and the image pairs and the corresponding text pairs (in-modal cyclic). Long-CLIP (Zhang et al., 2024) uses knowledge-preserved enlarged positional embedding, handling up to 248 input tokens, significantly greater than the 77 tokens restricted in CLIP. FLIP (Li et al., 2023) proposes a technique where a significant portion of image patches is randomly masked during training. SoftCLIP (Gao et al., 2024) uses softened target labels derived from fine-grained intra-modal self-similarity.\n2.2 Modality Gap\nThe issue of the modality gap is pervasive in VLMs such as CLIP, caused by embeddings for images and texts occupying disjoint regions in the latent space (Liang et al., 2022; Oh et al., 2024). This gap, by definition, restricts the model from utilizing the entire latent space. The root cause of this modality gap has been debated: Ramasinghe et al., 2024 claims that the intrinsic differences between image and textual data unavoidably result in the modality gap. Liang et al., 2022 attributes the gap to the resulting narrow cone due to the high model hidden dimension. Fahim et al., 2024 suggests that the gap, mainly caused by the contrastive learning objective, could be reduced with additional loss terms for uniformity and stricter cross-modal alignment (Wang and Isola, 2020). In this work, we are interested in removing the actual attributing factor of the gap, in contrast to accepting the modality gap (Ramasinghe et al., 2024) to extend CLIP as an explainable evaluation metric."}, {"title": "3 Preliminary Analyses", "content": "Modality gap was introduced by Liang et al., 2022 and is defined as the centroid distance (CD) between the mean of normalized image embeddings ($x_i \\in \\mathbb{R}^d, i = 1, 2, ..., n$) and mean of normalized text embeddings ($y_i \\in \\mathbb{R}^d, i = 1, 2, ..., n$). Formally, $\\triangle_{CD} := ||\\bar{x} - \\bar{y}||_F$, where $\\bar{x} := \\frac{1}{n} \\Sigma_{i=1}^n x_i$, $\\bar{y} := \\frac{1}{n} \\Sigma_{i=1}^n y_i$, with d and n representing the model's hidden dimension and the data size. Fahim et al., 2024 quantify the gap as the linear separability (LS) of image and text embeddings (Shi et al., 2023). To measure LS, or $\\triangle_{LS}$, we divide the given dataset into training (70%) and test (30%). Then, we train a linear regression model and report 1 - mean squared error of the model separability of image and text embeddings, following the same procedure as Fahim et al., 2024.\n3.1 Severity Levels of Modality Gap\nTo integrate the different definitions of the modality gap, we characterize the relationship between CD, LS, and minimum cosine distance (MCD; $\\triangle_{MCD}$) using piece-wise linear interpolation (Figure 2). We find that if $\\triangle_{CD} < 0.19$, $\\triangle_{LS}$ deviates from 1.0. Also, as $\\triangle_{CD} > 0.63$, MCD increases with a steeper slope than the slope in $\\triangle_{CD} < 0.63$. Thus, our categorization of the modality gap using a relationship of CD, LS, and MCD is as follows:\n\u2022 Severe: $\\triangle_{CD} \\geq 0.63$\n\u2022 Moderate: $0.19 < \\triangle_{CD} < 0.63$\n\u2022 Low: $\\triangle_{CD} < 0.19$\n3.2 Normalized Embedding Activations\nThe attributing factor of the modality gap observed in CLIP can be informed through our analysis of the normalized embedding activations from each image/text encoder. We first investigate distinct peak activations in the normalized image and text embeddings and then theoretically show that these peak activations contribute to the modality gap. As displayed in the first column of Figure 3 (or Figure 6 in Appendix B), a similar pattern of normalized embedding activations is shown across the hidden dimensions for different images and texts with a small standard deviation. Also, we consistently observe negative peak activations at the 93rd dimension for all image samples and positive peak activations at the 134th and 313th dimensions for all text samples with low standard deviation, regardless of semantic representations of each sample per modality. This phenomenon is possibly due to one of the root causes of the modality gap discussed in Related Works. This suggests that each encoder encodes modality-specific characteristics that can contribute to the embedding discrepancy"}, {"title": "3.3 Contribution to Modality Gap", "content": "We now demonstrate how these peak activations in the normalized image and text embeddings prevent the cosine similarity from reaching high values. To illustrate the upper bound of the cosine similarity, suppose there exists one negative peak, p, in normalized image activation (x\u2081 = [x\u2081, x\u2082, ..., xd]) and two positive peaks of q in normalized text embedding activations (y\u2081 = [y\u2081, y\u2082, ..., yd]), and |p| \u226b xi and |q| \u226b yi, in align with our empirical finding (Figure 3). For simplicity, we assume that the other non-peak activations are uniformly distributed. Then, the upper bound of |cos(xi, y\u017c)| converges to \u221a(1-p\u00b2)(1 \u2013 2q\u00b2) as d \u2192 \u221e (proof in the Appendix C). If we set p to be -1/4 , and q to be 0.25 (Long-CLIP activations from Figure 3), the upper bound of |cos(xi, y\u017c)| converges to 0.76. Since this converged value is less than 1, it implies that the existence of peak activations hinders the cosine similarity of text and image embeddings from being close to 1, inducing a modality gap."}, {"title": "4 Methodology", "content": "The IOT framework consists of two stages, the initial stage being a plug-in-play module that can be skipped if the user only wants to tackle the modality gap problem of the models. The second stage of IOT is applied asynchronously after the first stage. The motivation behind these divided stages is maintaining the semantic representations by locking the model parameters in the first stage and mitigating the modality gap in the following stage.\n4.1 The First Stage of IOT\nIn this initial stage, our goal is to enhance the semantic representations of CLIP. Since our work is the first to present a 2-step paradigm that handles both the semantic representations and the modality gap problem, we share our best strategies to improve overall downstream performances on CLIP using a mixture of recently introduced CLIP fine-tuning strategies from several works of literature. We follow the implementation of Long-CLIP (Zhang et al., 2024), but with a key difference; we find that using only long captions for alignment (Long-CLIP-only) on COCO from the ShareGPT4V dataset (Chen et al., 2023) significantly reduces the training time (~1/10) while achieving better performances in downstream tasks (refer to Appendix D for details).\nWe also use a combination of the standard contrastive learning and Cyclic losses (Goel et al., 2022), $L_{CyCLIP} := L_{CLIP} + 0.25L_{I-Cyclic} + 0.25L_{C-Cyclic}$. We fine-tune CLIP for three epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate le-6 and a weight decay le-2. We set a batch size of 128 (64 for each GPU device), and we use the standard contrastive learning loss with the temperature log scale of 4.6052. We apply this procedure to all comparison methods to ensure a comparison procedure. All the details of comparison baselines and evaluation downstream tasks are in Appendix D."}, {"title": "4.2 The Second Stage of IOT", "content": "Post-hoc Method to Reduce Modality Gap To mitigate the modality gap, it is crucial to remove modality-specific characteristics from the embeddings of each encoder. An intuitive approach might involve reducing peak activations through clipping. However, we find that clipping the normalized activations within the range of -0.1 to 0.1 and re-normalized with Frobenius norms still results in severe modality gap (see the second column in Figure 3). We hypothesize that the unclipped activations might still encompass the property that commonly exists across the activations for each image/text encoder linked to the modality gap.\nMotivated by the limitation of this clipping method, we develop an embedding standardization method to remove modality-specific characteristics from the normalized activations across entire dimensions. We standardize the normalized embedding activations ($x_i, y_i \\in \\mathbb{R}^d$) by subtracting the mean vectors ($\\bar{x}, \\bar{y} \\in \\mathbb{R}^d$) for each modality and re-normalize them by dividing by the Frobenius norms : $\\hat{x} = Normalize(x_i - \\bar{x}), \\hat{y} = Normalize(y_i - \\bar{y})$\nWe can clearly observe that our post-hoc method significantly reduces the modality gap, similar to the post-hoc shifting method of Mind-the-Gap (MG) (Liang et al., 2022) ; however, with no outlier peaks and mean activations close to zero across all hidden dimensions, suggesting that our approach can remove the actual cause factor of the modality gap including, especially peak activations, unlike the MG approach (compare peak activations between third and fourth columns in Figure 3).\nLearnable Method to Reduce the Gap Although IOTpost significantly reduces the modality gap, it does not support zero-shot inference for a single sample. To overcome this, we explore a method to automatically reduce the modality gap without relying on post-hoc refinement. The key point of our IOTasync method is to add an independent batch normalization (BN) layers, BNimg and BNtxt for each encoder. This enables the model to learn the means and variances of normalized image and text embedding activations without affecting the semantic encoding capability of the encoder (see Discussion). Through this process, the model iteratively updates the running means and variances of normalized embedding activations for each modality: $\\bar{x}_{t+1} = a \\bar{x}_B + (1 - a)\\bar{x}_t, \\bar{y}_{t+1} = a \\bar{y}_B + (1 - a)\\bar{y}_t$.\n$\\bar{x}_{t+1}$ and $\\bar{y}_{t+1}$ denote the updated running means in training time step t + 1, incorporating the batch mean vectors, $\\bar{x}_B = \\frac{1}{m} \\Sigma_{i=1}^m x_i$ and $\\bar{y}_B = \\frac{1}{m} \\Sigma_{i=1}^m y_i$ with averaging factor a = 0.1, and batch size, m = 64, 128, 256, 512. We use the final updated running mean of normalized image and text embedding activations, $\\bar{x}_{train} := \\bar{x}_T, \\bar{y}_{train} := \\bar{y}_T$ (T: final training step) as the learned modality-specific characteristics of images and texts. Similarly, the final updated running variance of normalized image and text embedding activations are $\\sigma^2_{x_{train}} := \\sigma^2_{x_T}, \\sigma^2_{y_{train}} := \\sigma^2_{y_T}$, which are empirically observed as close to 1.0 across all d dimensions. The final updated image and text semantic representations can be expressed as ($\\epsilon = 1e-05$): $\\hat{x} = Normalize(W_{img} (\\frac{x - \\bar{x}_{train}}{\\sigma^2_{x_{train}} + \\epsilon}) + b_{img})$ and $\\hat{y} = Normalize(W_{txt} (\\frac{y - \\bar{y}_{train}}{\\sigma^2_{y_{train}} + \\epsilon}) + b_{txt})$, where $W_{img}, W_{txt} \\in \\mathbb{R}^d$ and $b_{img}, b_{txt} \\in \\mathbb{R}^d$ indicate the weights and biases of BNimg and BNtxt.\nIt is crucial to consider how we can effectively train the weights/biases of these BN layers of IOTasync. We follow the exact training implementation details as the first stage with $L_{CyCLIP}$ as the loss objective . Then, we freeze the weights of the fine-tuned encoders to preserve semantic representations and train the BN layers asynchronously afterward.\nWhen training these BN layers, here, we propose Multimodal Contrastive Learning of Sentence and Image Embeddings (MCSIE), our re-implementation of MCSE (Zhang et al., 2022a), a learning method using an unsupervised-positive augmentation. Unlike MCSE, dropout (rate: 0.1) is applied to every multi-head attention layer of both the image encoder (ViT-B/32, Dosovitskiy et al., 2020) and the text encoder (Transformer, Vaswani et al., 2017), augmenting relations between all combinations of images/augmented images and texts/augmented texts with $\\Sigma_{x_i \\in \\{I,I_{aug}\\},\\Sigma_{t_i \\in \\{T,T_{aug}\\}} L$, where L indicates a loss function. From our ablation study , we find MCSIE effectively further reduces the modality gap, suggesting that it enables BNs to learn the modality-specific characteristics robustly."}, {"title": "6 Discussion", "content": "6.1 The Relationship between Modality Gap and Downstream Performances\nIOTpost with the lowest modality gap severity level achieves the highest task performance in image-text retrieval (Table 2). However, at the same time, it does not always score the highest performances for classification and correlation tasks. We emphasize that there is no direct causal relationship between the modality gap and downstream performances, similar to ongoing discussions in recent works (Liang et al., 2022; Jiang et al., 2023; Ramasinghe et al., 2024; Schrodi et al., 2024). Specifically, Liang et al., 2022 states that sometimes a \"larger gap\" can help improve zero-shot learning performances, Jiang et al., 2023 empirically shows unguaranteed downstream performances when reducing modality gap. Our work does not claim any relationships between modality gap reduction and performance improvement in line with these studies. Rather, we show that our methods could significantly reduce the modality gap without hurting overall downstream performances.\nWe believe the best of both perspectives can be achieved through the presence of our plug-in-play module of the first stage, which solely focuses on enhancing semantic features in the separation of stage 2, adding single batch normalization layers for each encoder. Although IOTasync does not mitigate the modality gap to near zero due to distribution differences between training and test samples, it still reduces to the moderate level. This suggests we could use a pre-computed embedding average from a subset of the training dataset as another solution when dealing with the modality gap if we also do not have enough resources for training. However, we emphasize that unlike recently introduced learning methods (Wang and Isola, 2020; Eslami and de Melo, 2024; Xia et al., 2024), our learnable approach does not change (but mostly improve) much of the original embeddings with trainable BN layers separately added to pre-trained encoders.\n6.2 Why is Batch Normalization Effective in Reducing the Modality Gap?\nHere, we find that peak activations across a few dimensions for each modality encoder are the main reasons for the large modality gap. However, we also observe that simply clipping these peak activations does not help to reduce the gap. Thus, instead of directly linking the modality-specific characteristics into only peak activations, we aim to remove the aggregated mean/std statistics of normalized embedding activations for each modality. This is possible due to the consistently similar values of means and minuscule standard deviations over data samples per modality and minuscule standard deviations across all hidden dimensions, which can be effectively learned using separate BN layers. In addition, other normalizations, such as LN, do not help reduce the modality gap effectively since the objective of LN is not linked to the modality gap.\nFurthermore, our asynchronous strategies of applying post-hoc and training BN methods on frozen encoders allow the model to significantly reduce the modality gap while preserving semantic representations of embeddings. Thus, although BN layers were conventionally thought of as one of the normalization strategies in the past, we could reinterpret these as effective strategies for mitigating the modality gap."}, {"title": "7 Conclusion", "content": "In this study, we present the IOT framework that can effectively reduce the modality gap between image and text embeddings while preserving the semantic representations. We first introduce a simple post-hoc embedding standardization method of reducing the gap to the close-zero value (I0Tpost) and also provide a novel training strategy using separate batch normalization layers for each modality (IOTasync). I0Ts show effectiveness in both modality gap and downstream performances compared to the other seven CLIP-based models and BLIP with no additional and 10M extra training parameters for IOT post and IOTasync, respectively. We believe this work will guide and inspire future research to address the modality gap further, an area less explored than improving downstream performances."}, {"title": "8 Limitations", "content": "While IOTs demonstrate significant improvements in reducing the modality gap, IOTpost relies on the entire test dataset, which may not be practical for single-sample inference when sufficient data is unavailable. To address this limitation, we introduce IOTasync, which reduces the modality gap without requiring access to the test dataset. However, the modality gap achieved with IOTasync does not entirely reach the near-zero level, similar to existing methods such as CLOOB and Unif-Align. Here, we provide a simple baseline method using the existing BN layers. We leave it as a future study to explore different learning methods and additional modalities (e.g., audio and video) for reducing the modality gap."}, {"title": "9 Ethical Statement", "content": "Misusing our proposed metric, IOT-S, as the reference-free evaluation metric could bring a potential risk. However, we believe this applies to every reference-free metric since IOT-S is built upon the widely used CIIP-S."}]}