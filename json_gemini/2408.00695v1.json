{"title": "Accelerating Full Waveform Inversion By Transfer Learning", "authors": ["Divya Shyam Singh", "Leon Herrmann", "Qing Sun", "Tim B\u00fcrchner", "Felix Dietrich", "Stefan Kollmannsberger"], "abstract": "Full waveform inversion (FWI) is a powerful tool for reconstructing material fields based on sparsely\nmeasured data obtained by wave propagation. For specific problems, discretizing the material field\nwith a neural network (NN) improves the robustness and reconstruction quality of the correspond-\ning optimization problem. We call this method NN-based FWI. Starting from an initial guess, the\nweights of the NN are iteratively updated to fit the simulated wave signals to the sparsely measured\ndata set. For gradient-based optimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\nIn this paper, we introduce a novel transfer learning approach to further improve NN-based FWI.\nThis approach leverages supervised pretraining to provide a better NN weight initialization, lead-\ning to faster convergence of the subsequent optimization problem. Moreover, the inversions yield\nphysically more meaningful local minima. The network is pretrained to predict the unknown ma-\nterial field using the gradient information from the first iteration of conventional FWI. In our\ncomputational experiments on two-dimensional domains, the training data set consists of reference\nsimulations with arbitrarily positioned elliptical voids of different shapes and orientations. We\ncompare the performance of the proposed transfer learning NN-based FWI with three other meth-\nods: conventional FWI, NN-based FWI without pretraining and conventional FWI with an initial\nguess predicted from the pretrained NN. Our results show that transfer learning NN-based FWI\noutperforms the other methods in terms of convergence speed and reconstruction quality.", "sections": [{"title": "1. Introduction", "content": "Civil infrastructure is often deteriorated due to exposure to loads, temperature fluctuations, natural\ndisasters, and potential faults during construction. Assessing the damage present in the infrastruc-\nture requires non-destructive testing (NDT) [1-3]. In recent years, attempts have been made to\nexploit the potential of full waveform inversion (FWI) in NDT. Originating in seismology [4], FWI\ninfers material models of the earth's interior by fitting simulated seismic waves to measured data\nsets from active or passive sources [5, 6]. The approach is adopted in engineering applications [7-\n10] utilizing ultrasonic waves to reveal information about unknown defects such as voids, cracks,\nand inclusions. Extracting detailed information about damage location and severity using FWI is"}, {"title": "2. Methodology", "content": "In the sequel, the scalar wave equation is used. It describes the propagation of the scalar wave field\nu(x, t) in space x and time t in a medium whose spatial distribution of the density is given by p(x).\nThe spatial distribution of the wave speed is denoted as c(x), the external force as f(x,t). With\nthis notation, the 2D scalar wave equation reads\n\\rho(x)\\ddot{u}(x, t) \u2013 \u2207 \u00b7 (\\rho(x)c(x)\u00b2\u2207u(x, t)) = f(x, t), x = [x, y] \u2208 N, t\u2208\u03a4,\nwhere \u03a9 is the 2D spatial domain and T = [0, Tmax] the time interval of interest. Appendix A of [25]\ngives a detailed derivation of this equation. B\u00fcrchner et al. [9] show that using mono-parameter FWI\nto only invert for density leads to better identification of defects with strong contrasts such as voids.\nTherefore, a dimensionless scaling function (x) is introduced to parameterize the scalar wave\nequation and the following optimization problem. Assuming po and co correspond to undamaged\nbackground material, the unknown material field y(x) scales the density, i.e., p(x) = \u03b3(x)\u03c1\u03bf, while\nthe wave speed remains constant, i.e., c(x) = co. This leads to the density-scaled version of the\nscalar wave equation, which is used throughout the paper,\n\u03b3(x)\u03c1\u03bf\u03cb(x, t) \u2013 \u2207\u00b7 (\u03b3(x)poc\u2207u(x,t)) = f(x,t), x = [x, y] \u2208 N, t \u2208T.\nThe initial and boundary conditions are\nu(x, 0) = \u00f9(x, 0) = 0 on \u03a9,\ndu(x, t)\n\u042d\u0445\n= 0 on Ty \u00d7 T,\ndu(x,t)\n\u0434\u0443\n= 0 on \u0393x x T."}, {"title": "2.2. Numerical configuration", "content": "In the paper, all investigations are performed with the same numerical configuration. The setup is\nshown in Figure 1. Considering a 2D domain of 0.1 m \u00d7 0.05 m, a finite differences (FD) solver is\nused to solve the corresponding wave equation (as discussed in detail in Appendix A of [25]). The\ndomain is excited with a two-cycle sine burst at the positions \u00e6. Introducing the central frequency\nkc with the corresponding angular frequency w = 2kc and amplitude Ao (Figure 1b), the external\nforce reads\nf(x, t) =\n{\nAosin(wt) sin\u00b2 (2)(x \u2013 x) for 0 \u2264 t \u2264 2\u03c0\u03b7\u03c2\n0 for 2\u03c0\u03b7\u03c2 < t\nwhere nc 2 is the number of cycles and \u00e6s is the source position of the underlying experiment.\nm\u00b3\nThe undamaged density and wave speed of the material is 2700 and 6000m. The central\nexcitation frequency is 500 kHz. The synthetic reference data is generated with simulations on a\ngrid with 512 \u00d7 256 grid points and 2000 time steps with dt = 3 \u00d7 10-8 s. For the FWI, a different\ngrid configuration has been used to avoid the \u201cinverse crime\u201d [38]. The domain is discretized with\n256 \u00d7 128 grid points, and the time integration is performed with 1000 time steps and a time step"}, {"title": "2.3. Conventional FWI", "content": "Using conventional FWI, the problem at hand is parameterized by a set of coefficients y representing\nthe scaling function y(x) at all finite difference grid points. Introducing a misfit functional L(7)\nthe corresponding optimization problem is described by\n* = argmin L(\u00e2y),\n\u0e0a\u0e34\nwhere * is the optimal solution. The misfit functional sums up the squared error at all N\u00b9 receiver\npositions \u00e6 for all experiments Ns\nNN\nL(y; u\u00ba) =\n1\n2\n\u03a3[\u03a3(\u00fbs (y; x, t) \u2013 u(x, t))2, 8(x \u2013 x)] didt,\n\u03a4\u03a9s=1 r=1\nwhere \u00fbs(7; x, t) denotes a single simulated wave field given a set of coefficients and u(x, t) a\nobserved wave field. Measurements across all experiments are summarized by u\u00b0 = {u}=1.\nN\nFor the computation of the derivative of the cost function with respect to the material coefficients,\nthe adjoint method is used. For a detailed derivation, see e.g. [9]. Given a wave field u, the derivative\nwith respect to one coefficient i is\ndL\ndyi\n=\n\u03a3[-Pos + Pocus,i] dt,\nNs\ns=1\nT\nwhere the subindex i denotes the solution of the corresponding primal and adjoint solution at the\ngrid point i. The derivatives at all grid points form the gradient is \u2207\u2081L(\u0177; u\u00ba) and can readily be\nused in gradient-based optimization."}, {"title": "2.4. NN-based FWI", "content": "In the NN-based FWI, an NN parameterized by e is used to predict the scaling function coefficients,\ni.e., (\u03b8). To this end, consider an NN Ny that predicts the material coefficients at all grid points:\n\u00f4y(0) = N\u2081(\u03b8; k),\nwhere contains all parameters of the NN and k is the input as shown in ??. Therefore, the\noptimization problem becomes\n0* = argmin L((\u03b8); u\u00b0).\n\u03b8\nThe gradient of the cost function with respect to the NN parameters e can be computed with\nthe chain rule\nVoL(y; u\u00b0) = \u2207\u2081L(\u0177; u\u00b0) \u00b7 \u0e2d"}, {"title": "2.4.2. Transfer learning NN-based FWI", "content": "Transfer learning NN-based FWI uses a pretrained U-Net (Figure 3) to parameterize the density\nscaling function y. The knowledge from a training data set is transferred to provide good weight\ninitialization of the NN for the downstream task of FWI.\nPretraining: In [37, 26], the pretraining was carried out using observed wavefields as inputs and\nthe true density distribution as the output. This already improved NN-based FWI. Yet, further\nimprovements are possible by using the adjoint gradient \u2207\u2081L(70; u\u00ba) from the first iteration as"}, {"title": "2.4.3. Conventional FWI with initial guess", "content": "For comparison, the initial guess provided by the pretrained network is used with conventional FWI.\nTherefore, the prediction y from the pretrained network Ny(\u03b8\u00ba; \u2207\u2081L(70; u\u00ba)) is used as starting\nmodel."}, {"title": "2.4.4. Importance of NN initialization", "content": "Weight initialization for NNs is important to achieve fast convergence and good accuracy. Using\nthe weight initialization suggested by Glorot in [44] is common practice. This avoids the vanishing\ngradient problem but does not form an optimal starting point for FWI, as the random weights\nlead to the noisy initialization patterns depicted in the upper left part of Figure 5. On the con-\ntrary, an expected outcome is that the material remains undamaged almost everywhere except for\nlocally confined flaws. An approximation to undamaged material can be provided by additionally\ninitializing the weights of the last layer using a standard normal distribution with a narrow stan-\ndard deviation of 0.01 and setting a rather high bias of, e.g. three. This removes most of the\nnoise from the initialization of the density scaling function and provides a better starting point\nfor NN-based FWI. As a last step, a sigmoid function maps the values to the range between zero\nand one. Furthermore, setting the weight of the last layer causes the method to be less dependent\non the initialization of the weights. Figure 5 shows the effect of the weight initialization on the\nreconstruction of a damage case within 35 iterations. This weight initialization is used for the\ngenerator network for NN-based FWI as well as for the U-Net employed for transfer\nlearning NN-based FWI In the case of transfer learning NN-based FWI, the weights of\nthe last layer are initialized in this way during pretraining, which improves the convergence of the\ntraining process."}, {"title": "3. Results and discussion", "content": "To compare all four methods, i.e., conventional FWI, NN-based FWI , conventional FWI with an initial guess from pretraining , and transfer learning NN-"}, {"title": "3.1. Damage cases", "content": "based FWI , four damage cases are used, see Figure 6. The aim is to quantify the\nmethods with respect to their reconstruction quality and convergence speed. The red color repre-\nsents the region where the density scaling function has a value of 1, and the blue color represents\na void/damage, i.e., a value of 10-5 the scaling function."}, {"title": "3.2. Case 1", "content": "Figure 7 compares the four methods for the first damage case. The test case is similar to but\nnot a sample from the data used for the pretraining of the network described in Section 2.4.2.\nTherefore, it is expected that the pretrained network will perform better than other methods for\nthis case. The first column in Figure 7 shows that conventional FWI recovers the damage within\n10-15 iterations. However, the reconstruction suffers from artifacts, which only reduce slightly with\nincreasing iterations. The second column shows the individual snapshots of NN-based FWI without\npretraining, which recovers the damage within 10 iterations with good accuracy and without any\nartifacts in the reconstruction. The third column shows the results of the corresponding iterations\nfor conventional FWI with an initial guess from the pretrained network. As expected, providing a\ngood initial guess improves the adjoint method's reconstruction and helps recover the damage faster\nthan without pretraining. Finally, the fourth column shows the results from transfer learning NN-\nbased FWI. The reconstruction of the damage is much faster; within 5 iterations, the reconstructed\nshape of the damage is very accurate, leading to the best results for this case."}, {"title": "3.3. Case 2", "content": "Figure 8 shows the comparison of the four methods for a damage case containing a rectangular-\nshaped hole and a circular hole. Note that the training set does not include a square hole; only\nelliptical holes were included . The first column shows conventional FWI, which\nrecovers the shape approximately within 20-25 iterations. However, there are strong artifacts in\nthe area just above the rectangular damage. The NN-based FWI recovers the rectangular-shaped\nhole and the circular hole within 20 iterations with good accuracy. The conventional FWI with\nan initial guess from the pretrained network performs much better than without pretraining and\nachieves a good reconstruction within 20 iterations with less artifacts in the reconstructed image.\nNevertheless, the circular hole is not reconstructed very well. Finally, the transfer learning NN-\nbased FWI performs the best. It achieves an almost perfect reconstruction within 10 iterations\nwithout any artifacts. As compared to the NN-based FWI without pretraining, the reconstructed\nshape is close to the true shape, thus demonstrating that the pretrained network performs very\nwell for cases slightly outside the distribution of data on which it is trained."}, {"title": "3.4. Case 3", "content": "For the third case study , three holes of different sizes need to be reconstructed. This\nrenders the task to be quite different from the training data. The conventional FWI can roughly\nrecover the rightmost damage while failing to recover the damages on the left side within 35\niterations. There are a lot of artifacts in the reconstruction and it is difficult to infer the correct\nshape and location of the damages from the reconstruction. The NN-based FWI can recover one\ndamage shape within 15 iterations but struggles to reconstruct the other two. However, in this\ncase, the reconstruction is better than conventional FWI, although the reconstructed shape of\nthe damages is not accurate. The conventional FWI with an initial guess from the pretrained\nNN, delivers a similar reconstruction of the damage without pretraining. By contrast, the transfer\nlearning NN-based FWI can quickly recover all the locations of the holes with good accuracy within\n20 iterations. It improves the reconstructed shape of the damages in the subsequent iterations. The\ndamaged shape is recovered more accurately than other methods, and the artifacts are negligible.\nTherefore, the transfer learning NN-based FWI performs best among all the methods in this case."}, {"title": "3.5. Case 4: Is transfer learning NN-based FWI always the best?", "content": "In very few cases, the transfer learning NN-based FWI does not perform as well as the other\nmethods. The initial guess provided by the network is too inaccurate in some cases where the\nsought damage is very different from those of the training data. One such example is shown\nin Figure 10. The figure depicts two intersecting rectangular damages with three circular holes with\na decreasing radius located on a straight line. Furthermore, there are two additional circular holes"}, {"title": "3.6. Comparison over 100 test cases", "content": "FWI is carried out for 100 test cases to compare the four methods quantitatively, each undergoing\n35 iterations. The cost function and the mean squared error between the reconstructed and true"}, {"title": "3.7. Can transfer learning FWI recover from bad initial guess?", "content": "When using a transfer learning framework, a potential issue arises if the pretrained network provides\nan incorrect initial guess. This can occur when the data given to the pretrained network is different\nfrom the training data. To this end, case 2 from Figure 8 is revisited. To emulate a bad pretraining,\nan incorrect initial guess is set manually for Case 2 with a subsequent inversion. In\nFigure 12, it is evident that the initial guess in the first iteration differs strongly from the target\ndamage shape. Although the transfer learning FWI required roughly 70 iterations (compared to\nthe 10 iterations with Transfer learning FWI and 25 iterations with NN-based FWI experienced in\nFigure 8), it accurately recovers the damage. Similar results were observed for many cases, including"}, {"title": "3.8. Drawbacks and limitations", "content": "Transfer learning NN-based FWI also suffers from some drawbacks that are common to any\nmachine learning-based method. It requires extra effort to tune all the hyperparameters involved\nin the process, such as the learning rate, the learning rate scheduler, gradient clipping, cost scaling,\nand the number of samples for pretraining. This can be a time-intensive process.\nAs in any other machine learning-based approach, carefully selecting training data is crucial for\npretraining the network. Data that is too similar leads to overfitting, with the consequence that\nthe network will not generalize well for out-of-sample data. However, selecting training data can\nbe challenging, particularly if the type of expected damage is unknown. As the domain gets bigger,\nthe task of generating sensible training data for pretraining, as well as hyperparameter tuning, will\nbecome more challenging. This indicates potential for further improvement in the architecture of\nNN models, which are less sensitive to the choice of hyperparameters."}, {"title": "4. Conclusion", "content": "NN-based FWI [25] uses an NN to parameterize the density scaling function. It relies on wave sig-\nnals emitted at sources and received by the sensors to reconstruct flaws. This paper demonstrates\nthat it is possible to improve the NN-based FWI using transfer learning, as introduced in [37]. The\ntransfer learning is achieved with a U-Net, which is pretrained in a supervised manner - mapping\nthe gradient from the first iteration of conventional FWI to the true density scaling function. The\npretraining alleviates the strong dependency of the NN on the initialization of the NN weights.\nThe combination of the NN ansatz in FWI and transfer learning through supervised pretraining\ngreatly improves the convergence and reconstruction quality over conventional FWI. This includes\nconventional FWI relying on the same initial guess obtained through the pretrained NN.\nIn addition to evaluating the improvements compared to the original transfer-learning-based\nscheme presented [37], the transfer learning NN-based FWI is thoroughly compared to conventional\nFWI both with and without using the same pretrained network to provide a good starting point. In\nall cases, the presented transfer learning NN-based FWI outperforms conventional FWI. It might\nbe argued that appropriate regularization techniques [45] lead to similar results as the NN-based\nFWI for conventional FWI. These can, however, equally be applied to both the NN-based FWI\nand transfer learning FWI. Whether additional regularization boosts the performance of transfer\nlearning FWI and if similar results are obtainable with conventional FWI is the subject of future\nstudy."}, {"title": "A. Neural Network Architectures", "content": ""}, {"title": "A.1. Generator network architecture", "content": "Table 1 shows the detailed architecture of the generator network used for the NN-based FWI\nmethod. The generator network upsamples the input tensor of size 128 \u00d7 8 \u00d7 4 through its layers\nto match the finite differences grid of the domain (1 \u00d7 256 \u00d7 128)."}, {"title": "A.2. U-Net architecture", "content": "Since both the input and the output data are a 2D matrix, a U-Net-based convolutional NN\narchitecture is employed; see table 3. The U-Net architecture consists of a blocked structure\nwhere each block consists of a Convolution layer, a Batch normalization layer, and an activation\nlayer repeated twice, followed by a Max pooling layer. The input and output tensor is 256 \u00d7 128\nin shape. The skip connections are used to overcome the vanishing gradient problem.\nA small parametric study was carried out to find the best parameters for pretraining the U-Net\nand the optimum number of samples and epochs required for pretraining. It's worth noting that\nthe optimum performance for the complete method is the performance of the pretrained network\nfor the downstream task, i.e., carrying out the NN-based FWI. After each pretraining task, the\nnetwork was run for 10 test cases, and the average was compared to the performance of each\npretrained network. It was found that the optimum performance was achieved by continuing the\npretraining further after the validation error had stopped reducing for the pretraining task. A\nsimilar observation was reported in [46]."}, {"title": "A.3. Hyperparameter tuning", "content": "The hyperparameters for each method are tuned separately using 35 iterations to maximize the\nindividual performance (Table 3). A learning rate scheduler with a polynomial decay (\u03b2\u00b7epoch+1)\u00ba\nwas also used in this process with a = \u22120.5 and \u03b2 = 0.2."}, {"title": "A.3.1. Optimal number of epochs", "content": "To find the optimum number of epochs for pretraining, the network was trained for 50, 100, 200,\nand 500 epochs. This was done with three different numbers of samples for pretraining, 50 samples,\n200 samples, and 500 samples, to find out if the change in the number of samples is a factor in the\noptimum number of epochs for pretraining. The hyperparameters used for this parametric study\nare listed in table 4."}, {"title": "A.3.2. Optimal number of samples", "content": "A parametric study was conducted to determine the optimal number of data sets used for pre-\ntraining the U-Net. This study aims to determine the effect of increasing the number of samples\non the performance of the U-Net for NN-based FWI. The number of samples used for pretraining\nthe U-Net is 200, 300, 400, 500 and 800. The hyperparameters used for pretraining the network\nwere kept constant for each case. These hyperparameters are shown in Table 4. Figure 14 depicts\nthe mean squared error at each iteration averaged over 100 test cases. Each curve represents the\ncorresponding amount of samples used for pretraining. Figure 14 demonstrates the clear pattern\nthat more samples improve the performance of transfer learning NN-based FWI. As can be seen,\nthe MSE error is the least for pretraining using 800 samples for the downstream task. Therefore,\nthe network pretrained from 800 samples is used for final comparison with other methods."}]}