{"title": "Not Only the Last-Layer Features for Spurious Correlations: All Layer Deep Feature Reweighting", "authors": ["Humza Wajid Hameed", "Geraldin Nanfack", "Eugene Belilovsky"], "abstract": "Spurious correlations are a major source of errors for machine learning models, in particular when aiming for group-level fairness. It has been recently shown that a powerful approach to combat spurious correlations is to re-train the last layer on a balanced validation dataset, isolating robust features for the predictor. However, key attributes can sometimes be discarded by neural networks towards the last layer. In this work, we thus consider retraining a classifier on a set of features derived from all layers. We utilize a recently proposed feature selection strategy to select unbiased features from all the layers. We observe this approach gives significant improvements in worst-group accuracy on several standard benchmarks.", "sections": [{"title": "1. Introduction", "content": "Thanks to their performance capability, deep learning is increasingly applied across diverse domains including healthcare. However, when trained with empirical risk minimization (ERM), deep learning models may fail to learn stable features, which are those that hold across data distributions collected at different times and places [19]. For instance, it has been observed the tendency of convolutional neural networks (convnets) to often prioritize image texture over more informative features such as shapes, which may be better predictors [5, 7]. This tendency arises from models' ability to exploit any shortcuts or spurious correlation present in training data which may be sufficient to correctly predict training data, but may not hold in unseen test data. As a result, this exposure to learning spurious correlation or any shortcut in data makes them vulnerable to a potential drop in predictive performance.\nAddressing the challenge of learning in the presence of such spurious correlations has motivated several approaches in the literature. It is widely adopted the notion of groups (defined in Sec. 3), in which to correctly classify instances from certain groups, it is expected that models should be robust enough to spurious correlation. Assuming the presence of group annotations in training data, a well-known alternative to ERM is group distributionally robust optimization (group DRO) [18], which minimizes the empirical worst-group risk. There exist methods that do not assume group information during training such as just train twice (JTT) [11], which divides the training time into two phases. The first phase trains the model with ERM, while the second phase continues by up-weighing the loss of misclassified instances of the first phase. Recently, it has been shown that the last-layer representations of ERM-trained models already exhibit both robust and non-robust (to spurious correlation) features [8, 9]. As a remedy, to decrease the impact on non-robust features, deep feature reweighting (DFR) [9] has proven effective, which only retrains the classifier with a balanced-group validation set.\nDFR can be viewed as an instantiation of transfer learning, where there is a desire to exploit robust and generalizing features from the source domain to build a good predictive model on the target domain [21]. Here, the goal of the target domain is a dataset balanced according to groups. In transfer learning literature, more advanced methods exist beyond retraining the classifier. Indeed during supervised learning a network can learn to discard certain robust features present from earlier layers to make the final prediction in the last layer, thus losing potentially useful features. This paper considers a simple yet efficient transfer learning method, called Head2Toe [4]. Unlike last-layer retraining, Head2Toe leverages all layer features, not just the last one, to find a sparse network with the most transferable features. Therefore, we leverage Head2Toe to complement DFR and aim to get the most transferable features while also decreasing the impact of non-robust (to spurious correlation) features.\nOur contributions can be summarized as follows: (i) we show how an efficient transfer learning method (Head2Toe here) can be incorporated in a pipeline of a state-of-the-art method in spurious correlation learning, (ii) we demonstrate that this incorporation can yield better performance on standard evaluation benchmarks."}, {"title": "2. Related Work", "content": "Several approaches have been developed to address the challenge of learning amidst spurious correlations, alongside various methods aimed at efficient transfer learning.\nLearning with Spurious Correlation. Data augmentation techniques appear to be the standard approach to fight against minority groups [1, 2, 20]. For example, [15] introduces a data augmentation technique by generating counterfactual data, which adds or removes object parts responsible for identified spurious patterns. There exist methods that analyze representations throughout the training dynamics to understand how the bias to spurious features arises. For instance, [3] propose a method to reduce model reliance on spurious features by penalizing the gradient in directions of spurious features. Several other works have been done to analyze stochastic gradient descent (SGD) directions, leading to modified versions of SGD or loss functions [13, 14, 17]. Although most of these methods address the spurious correlation problem, it has been recently shown the ability of ERM to competitively learn robust-to-spurious-correlation features [9, 24]. Despite the use of data with labeled spurious features in DFR, it differs from the above works by directly using balanced data where each grouping of class label and spurious feature are equally represented, resulting in a decrease in bias toward spurious or unstable features.\nEfficient Transfer Learning. Transfer learning still has its challenges addressed across various works and there are differing techniques established to improve downstream task performance. [22] presents a method that consists of training affine parameters in batch normalization layers. It explains how training these shift and scale parameters used in the normalization step can have a noticeable impact, especially in scarce data settings. Downstream tasks where there is minimal divergence in distribution from the pre-trained model seldom have difficulty learning on a new dataset when employing finetuning. This assumes that the pre-trained model is otherwise sufficient for the downstream task and improvements can be attained through leveraging batch normalizations alone. [10] explains that these in-distribution tasks struggle most when exposed to data with significant deviations in distribution from the source task. It explains that with a randomly initialized linear head, finetuning results in larger fluctuations in parameter weights during the earlier training steps as the whole network needs to adjust to what it considers an unusual dataset relative to the source task. First keeping the pretrained backbone frozen and training the linear head before unfreezing and finetuning allows the model to better adapt and transfer to out-of-distribution downstream tasks. Here, instead of drawing focus to just one layer, the assumption is that a better initialization of the classifier will lead to improved finetuning on the downstream task. [16] deals with the out-of-distribution transfer learning problem through the use of a weighted loss function. It proposes Automatic Feature Reweighting (AFR), a method that reweighs the loss among groups with fewer examples to push the model to better adjust and adapt for these minority group examples. This method has the additional benefit of not relying on spurious features. This approach leans similar to the DFR method by rebalancing groups to avoid unstable features from dominating the learning process although it does it through a loss function instead of group counts. With these different approaches to efficient transfer learning, the motivation for our method is derived from the use of Head2Toe's unique feature selection process. Unlike the above-mentioned methods, Head2Toe focuses on searching for information found throughout the network instead of relying solely on the penultimate layer. Head2Toe extracts useful intermediate layer features [4] and concatenates"}, {"title": "3. Background", "content": "We consider a classification problem with a training set denoted by $\\mathcal{D}_{T r}=\\{(x_i, y_i)\\}_{i=1}^n$, where $x_i \\in \\mathcal{X}$ is the input and $y_i \\in \\mathcal{Y}$ is its class label. For each data $x_i$, there is a spurious attribute value $a_i \\in \\mathcal{A}$ of $x_i$, where $a_i$ is non-predictive of $y_i$, and $\\mathcal{A}$ denotes the set of all possible spurious attribute values. We denote by a group the pair $g:=(y, a) \\in \\mathcal{Y} \\times \\mathcal{A}:=\\mathcal{G}$. Our goal is to learn a parameterized model $\\lmbda$: $f_\\lmbda: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that will maximize the expected accuracy while avoiding learning spurious features. We consider a neural network model\n\n$f_\\theta(.) = g (h(.)).$ (1)\n\nwhere $g$ denotes its classifier layer and $h$ denotes its feature network. Additionally, we denote by $\\mathcal{D}_{V a l}$ and $\\mathcal{D}_{T e}$ the validation and test sets, respectively. Assuming that we have the information of groups on a sample $\\mathcal{D}$, we denote by $\\mathcal{D}_{R W}$ a balanced subset of $\\mathcal{D}$ in which groups are uniformly distributed, i.e., each group has the same number of examples.\nDeep Feature Reweighting. Deep feature reweighting (DFR) [9] is a method to reduce bias towards spurious features, which are features in the data that are non-predictive of the task yet may be highly correlated with training targets. DFR training involves two phases: initially, the model is trained using empirical risk minimization (ERM) on the training data $\\mathcal{D}_{T r}$, without the information of the spurious attribute. Second, the feature network $h$ is frozen, and the classifier $g$ is trained on a balanced validation set $\\mathcal{D}_{R W}$. The core idea in DFR is that ERM learns both robust and non-robust (to spurious correlation) features, and the second phase enables prioritization of stable features through knowledge of group annotations.\nVal\nHead2Toe. Head2Toe [4] is one of the techniques for efficient transfer learning. It aims to select the most useful features from intermediate layers that better transfer to a target or downstream task. Head2Toe starts by concatenating all feature maps for intermediate layers, then learns a linear head on top of all these feature maps using the group-lasso regularizer [23]. This group-lasso regularizer allows the computation of relevant scores based on each feature's $\\l_2$ norm of weights. Using a threshold $\\tau$, one can select the most useful features that better transfer to the downstream task. It has been shown that this method for transfer learning"}, {"title": "4. Method", "content": "This section describes our method H2T-DFR, which is summarized in Figure 1 and Algorithm 1. It consists of three phases detailed below: (1) unbalanced training (or finetuning) on all the training data (2) balanced data feature selection and finally using the features for (3) balanced data linear classifier training.\nUnbalanced Finetuning. As in the DFR approach our method starts by simple fine-tuning on the entire dataset using ERM on $\\mathcal{D}_{T r}$, denoting this trained backbone as $h_{\\text{pretrained}}$.\nBalanced Feature Selection and Classifier Training. We follow an approach similar to Head2Toe: we concatenate all the features from all layers and perform a group lasso feature selection. Using $h_{\\text{pretrained}}$, we initialize a Head2Toe model which will have the linear layer $g_{H2T}$. It should be noted that $g_{H2T}$ in this stage is a linear classifier layer with inputs from all intermediate features in the network as described in the Head2Toe background section. $g_{H2T}$ is then trained on $\\mathcal{D}_{R W}^{V a l}$ using a group-lasso regularization loss. The regularization allows the model to adjust weights in terms of importance, and the resulting weights can be used to calculate relevance scores $s_i$ [4], where $s_i$ are the $\\l_2$ norms of the final weights of each feature of the $h_{\\text{pretrained}}$ network. The final features selected correspond to the ones with the top $\\tau$ percent scores,"}, {"title": "5. Experiments", "content": "We now discuss our experimental results. We focus our comparison to DFR which has shown to be a powerful technique that can outperform existing methods such as group-DRO [8]. We propose another baseline to compare to DFR and H2T-DFR, specifically Affine-DFR. This approach mimics DFR and does not have an explicit feature selection phase. Similar to DFR, Affine-DFR uses a balanced training phase but adapts only the affine parameters of batch-norm layers.\nExperiments for DFR, Affine-DFR and H2T-DFR were run over 5 seeds and table 1 shows their respective performance on CelebA [12], Waterbirds [18], and HAM10000 [7, 25]. The pre-trained model employed is ResNet-50 [6]. Hyperparameters were selected to reproduce result from the respective papers employed. For spuriousH2T, there is further hyperparameter tuning done starting from the hyperparameters in table 4 as there is added complexities such as feature selection fraction and layer target size to consider along with the different learning rates across the 3 training phases. Hyperparameters were originally set according to papers referenced for baseline results and further tuned through a hyperparameter search similar to the DFR paper [9].\nResults. Our results shown in Tab. 1 are with the use of a balanced dataset for feature selection. We observe improvement in the worst group accuracy for HAM10000 and CelebA, whereas Waterbirds does not show improvement from H2T features, the result being very close to that of the basic DFR. We note however that this dataset is known to be simpler than the others considered. Indeed, for Waterbirds, it was already observed that ImageNet pre-trained models already contain robust features to easily get \u2248 88% of worst-group accuracy by just retraining the classifier [8]. CelebA's worst-group accuracy increased by 2.60% from our method relative to the baseline (DFR). The medical dataset HAM10000 shows a 2.38% increase in worst-group accuracy. With all 3 datasets, the mean across group accuracies did not show a noticeable difference between the 3 methods.\nTo gain insight into the feature selection we illustrate the features selected depending on depth for HAM1000 in figure 2 (but note similar trends are observed in the other datasets). This illustrates that despite not relying solely on the penultimate layer for feature selection, the most crucial information is found towards the end layers of the network. We also compare to a feature selection done on unbalanced data and observe that this tends to select more features from lower layers, which we attribute to the presence of strong spurious features in lower layers that may be selected for the unbalanced problem setting."}, {"title": "6. Conclusion", "content": "This paper studies the problem of learning in the presence of spurious correlation. We propose H2T-DFR, a three-stage method that leverages Head2Toe (an efficient transfer learning method) [4], and incorporate it in the pipeline of DFR [9], a state-of-the-art method to fight against spurious correlation. H2T-DFR selects the most transferable features from all layers before applying DFR. Experiments on standard evaluation benchmarks demonstrate that H2T-DFR improves DFR, showing that efficient transfer learning methods can boost the worst-group predictive performance of robust-to-spurious correlation methods."}, {"title": "A. Appendix", "content": "Here we provide the Algorithm detailing the H2T-DFR method in Algorithm 1. Finally, Tables 2-4 describe the hyperparameters used in our experiments for the different baselines."}]}