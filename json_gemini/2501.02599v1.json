{"title": "Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models", "authors": ["Jalisha Jashim Era", "Bidyarthi Paul", "Tahmid Sattar Aothoi", "Mirazur Rahman Zim", "Faisal Muhammad Shah"], "abstract": "Mathematical word problems (MWPs) involve the task of converting textual descriptions into mathematical equations. This poses a significant challenge in natural language processing, particularly for low-resource languages such as Bengali. This paper addresses this challenge by developing an innovative approach to solving Bengali MWPs using transformer-based models, including Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the \"Patigonit\" dataset was introduced, containing 10,000 Bengali math problems, and these models were fine-tuned to translate the word problems into equations accurately. The evaluation revealed that the mT5 model achieved the highest accuracy of 97.30%, demonstrating the effectiveness of transformer models in this domain. This research marks a significant step forward in Bengali natural language processing, offering valuable methodologies and resources for educational AI tools. By improving math education, it also supports the development of advanced problem-solving skills for Bengali-speaking students.", "sections": [{"title": "I. INTRODUCTION", "content": "The challenge of solving natural language math word problems (MWPs) has intrigued researchers since the inception of artificial intelligence. These problems, often encountered by elementary students, require the conversion of textual descriptions into mathematical equations involving basic arithmetic operations such as addition, subtraction, multiplication, and division. Automating this process using advanced natural language processing (NLP) techniques can significantly enhance educational tools.\nSince the 1960s, researchers have sought to teach computers to solve these kinds of math problems just like humans [5]. With recent technological advancements, we have made significant progress, particularly for simpler problems. Despite advancements in NLP and machine learning, there has been a notable gap in applying these technologies to the Bengali language. Bengali, being a low-resource language, lacks substantial computational resources and datasets to support extensive research and development in NLP applications. Addressing this gap, our study introduces an innovative approach to solving Bengali MWPs using transformer-based models, including the Basic Transformer [2], mT5 [16], BanglaT5 [17], and mBART50 [18]. These models have been trained on a specifically curated dataset, \"PatiGonit\" means \"Arithmetic\" or \"Elementary Mathematics\"."}, {"title": null, "content": "For our study, we employed transformer-based models to identify mathematical equations embedded within the text of Bengali word problems [4]. We fine-tuned key hyperparameters, including learning rate, number of epochs, and batch size, to optimize model performance. Our approach involves predicting the equation from the word problem using deep learning-based natural language processing techniques. Once the equation is predicted, we use an equation solver to obtain the final answer. An example of the type of math word problem handled by our work is shown in Figure 1. The intermediate equations generated by the models are processed by our equation solver to produce the final solution.\nIn summary, our research advances the state-of-the-art in MWP solving for low-resource languages, providing a robust framework for future studies. This work aims to make educational technology more accessible by filling the gap in multilingual and interpretable math word problem solvers. It also supports the development of critical thinking skills and encourages data-driven decision making in education. Our key contributions include:\n\u2022 Creation of the \"PatiGonit\" Dataset: We introduce the \"PatiGonit\" dataset, consisting of 10,000 Bengali Math Word Problems from elementary school. This dataset addresses a significant gap in Bengali computational research, providing a valuable resource"}, {"title": null, "content": "for developing educational tools tailored to Bengali- speaking students.\n\u2022 Application and Fine-tuning of Transformer- based Models: We implement and fine-tune sev- eral transformer-based models, including Basic Trans- former, mT5, BanglaT5, and mBART50, specifically for solving Bengali Math Word Problems. Through a comparative analysis, we determine the effectiveness of these models for Bengali Math Word Problem solving.\n\u2022 Advancement in Bengali Natural Language Pro- cessing (NLP): We advance Bengali NLP by using transformer-based models to accurately translate and process Bengali math word problems into mathemati- cal equations. This approach improves model accuracy and broadens the application of NLP in educational contexts for Bengali learners."}, {"title": "II. RELATED WORKS", "content": "Multilingual models enable the processing and generation of text across multiple languages, enhancing various NLP tasks. Niyarepola et al. [15] employ these models to gen- erate math word problems (MWPs) through a sequence-to- sequence framework with cross-lingual evaluation, adapting datasets like Math23K and MathQA for bilingual assess- ment. Luo et al. [19] introduce MATHWELL, a multilin- gual model fine-tuned to generate math problems for K-8 students, focusing on generating solvable and accurate ques- tions. Both papers highlight how multilingual models can enrich MWP generation, improving mathematical education across languages.\nSeveral studies like Zhang et al. [6] and Wang et al. [7] have explored Seq2Seq models for neural machine trans- lation and solving math word problems (MWPs). In the paper of Zhang et al. [6],the VNMT model employs LSTM networks for both the encoder and decoder without any pretrained models, achieving a BLEU score of 32.07 for Chinese-English translation and 19.58 for English-German translation, and an accuracy of 79.8% on the MAWPS single operation dataset. Wang et al. [7] introduced a Seq2Seq model using a hybrid approach with a GRU encoder and LSTM decoder, achieving 64.7% on the Math23K dataset and 59.5% on the MAWPS dataset. The Saligned model of Chiang and Chen [8], a Seq2Seq neural encoder-decoder architecture, incorporates LSTM and Graph Convolutional Networks as the encoder and a TreeDecoder, achieving over 65% accuracy on the Math23K dataset. Graph2Tree, which also uses an LSTM with Graph Convolutional Networks and a TreeDecoder, addresses poor quantity representations and achieved 77.4% on MAWPS and 69.5% on Math23K.\nRecent research of Liang et al. [9] and Raiyan et al. [10] focuses on improving contextual and semantic understand- ing using transformer-based models. In the study of Liang et al. [9] MWPBert, which utilizes a pretrained BERT as the encoder and a TreeDecoder, achieved 84.7% on the Math23K dataset. DeBERTa of Raiyan et al. [10], combined with an Enhanced Mask Decoder and a Voting Mechanism, reached 91.0% on MAWPS and 79.1% on PARAMAWPS. Hybrid models that combine different approaches have shown significant improvements. An ensemble model by Wang et al. [11] utilizing BiLSTM and LSTM with equation normalization achieved 69.2% accuracy on the Math23K dataset. In the paper Xie and Sun [12] the GTS model, integrating GRU, TreeDecoder, and gated feedforward net- works, achieved 74.3% on Math23K and 83.5% on the MAWPS single operation dataset. WARM has been in- troduced in Chatterjee et al. [13] and it uses a weakly supervised approach with a bidirectional GRU encoder and three fully connected networks as the decoder, achieving 66.9% on All Arith and 56.0% on Math23K.\nUnique methodologies have been proposed in Qin et al. [14] to tackle specific challenges in MWPs. SAUSolver in paper of Qin et al. [14] introduced a Universal Expression Tree (UET) and auxiliary tasks, achieving up to 62.03% on the HMWP dataset."}, {"title": "III. DATASET", "content": "Prior to this study, there were no available datasets com- prising Bengali math word problems, a gap that significantly limited computational research and the development of educational tools for Bengali-speaking students. To address this void, we introduced the Bengali Math Word Problems dataset named \u201cPatiGonit\u201d, comprising 10,000 math word problems. It traditionally refers to the branch of mathemat- ics dealing with basic numerical operations such as addition, subtraction, multiplication, and division. These problems, initially in English within the MAWPS [1] dataset, which encompasses a total of 38,000 problems, underwent the translation process for the conversion into Bengali. Due to resource constraint we could only analyzed 10,000 of these problems. The dataset, typical of elementary school-level math, includes both basic arithmetic and some algebraic challenges, organized into two columns: one for the word problems and the other for the corresponding mathematical equations.\nWe present an analysis of the equations contained within the dataset, categorizing them into simple and complex equations based on the number of mathematical symbols used. Simple equations are defined as those containing only one mathematical operation, while complex equations involve multiple operations.The Table I below summarizes the total count of simple and complex equations, as well as a further breakdown of the simple equations into four cat- egories: addition, subtraction, multiplication, and division."}, {"title": "B. Dataset Annotation", "content": "For our dataset of math word problems, we, the native speakers meticulously reviewed and annotated each problem to ensure they were accurate and suitable for local use. This involved:\n\u2022 Translating the math word problems into Bengali.\n\u2022 Adjusting numbers to fit Bengali formats.\n\u2022 Changing currency symbols to match local standards.\n\u2022 Revising specific terms, especially in mathematical equations, to ensure they were correct in Bengali.\nWe made these adjustments ourselves to ensure the dataset was not only translated accurately but also culturally rel- evant and practical for use in education within Bengali- speaking regions. Figure 2 and Figure 3, shows us a sample for both the simple and complex equation present in our proposed dataset."}, {"title": "D. Train-Test-Validation Splits", "content": "We split the dataset into three parts: 80% for training, 10% for validation, and 10% for testing. This 80:10:10 split offers a well-balanced approach, providing ample data for the model to learn effectively while retaining a sufficient validation set for tuning hyperparameters and a robust test set for evaluating performance on unseen data."}, {"title": "IV. METHODOLOGY", "content": "Our methodology centers on using transformer-based models-MT5, BanglaT5, and mBART50-to translate and process Bengali math word problems into mathematical equations. These models were selected over traditional deep learning architectures like BERT, LSTM, and RNN due to their superior ability to handle complex sequence-to- sequence tasks.\nTransformers effectively manage long-range dependen- cies in text, crucial for solving math word problems. Unlike LSTMs and RNNs, which struggle with longer sequences, transformers process entire sequences in parallel, capturing context more efficiently for accurate equation generation. While BERT excels at understanding text, its encoder-only architecture makes it less suited for generative tasks like equation generation. By fine-tuning pretrained versions of mT5, BanglaT5, and mBART50 on our \"PatiGonit\u201d dataset, we optimized the models to excel at generating accu- rate mathematical equations from Bengali text. mT5 was trained on 101 languages, mBART50 on 50 languages, and BanglaT5 was specifically trained on Bengali. In Figure 6, this is illustrated by providing examples in English, Bengali, French, and Italian, demonstrating the models' multilingual capabilities in understanding and processing text across dif- ferent languages. Figure 6 presents the schematic diagram of the Bengali Math word problem solver.\nTo better understand the differences between the selected transformer models (MT5, BanglaT5, mBART50) and other deep learning architectures (BERT, LSTM, RNN), Table II provides a comparative analysis. This table highlights the variations in architecture, task type suitability, and training corpus, which illustrates why transformer-based models are better suited for our task of translating math word problems into mathematical equations. An overview of the models are given below."}, {"title": "A. Basic Transformer", "content": "The complexity of math word problems poses a signifi- cant challenge in natural language processing, particularly in converting text to mathematical equations. To tackle this, we employed the Basic Transformer model, renowned for its success in sequence-to-sequence tasks. Developed by [2], the transformer architecture processes entire texts simultaneously, enhancing context comprehension, which is crucial for accurately generating corresponding equations from word problems.\nIn the transformer-based model [4], the encoder captures semantic relationships in the input text, converting it into a high-dimensional space, while the decoder generates the output equation, guided by the encoder's output and previ- ous predictions. The attention mechanism ensures accuracy by dynamically focusing on relevant parts of the input text, enhancing the model's performance on our Bengali Math Word Problem dataset. Below are the important functions of transformers taken from [4].\nAttention Function\n$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (1)\n$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$ (2)\nwhere, $head_i = Attention(QW_i^Q, KW_i^K,VW_i^V)$"}, {"title": "B. mT5", "content": "The mT5 is a multilingual variant of the T5 model designed for handling a wide range of natural language processing tasks by framing them as text-to-text problems. It uses an encoder-decoder architecture, which allows it to understand input text and generate corresponding output text, making it highly effective for tasks like translation, summarization, and question answering. One of its key features is its ability to work across 101 languages, as it has been pretrained on a diverse multilingual dataset known as mC4. This makes mT5 particularly relevant for our work, as it can efficiently translate Bengali math word problems into mathematical equations. Additionally, a study [15] demonstrates the potential of mT5 in handling multilingual math word problems, highlighting its capability to gener- ate accurate equations from textual descriptions [16]. Its strength in both understanding and generating text makes it a strong choice for solving word problems in low-resource languages like Bengali."}, {"title": "C. BanglaT5", "content": "BanglaT5 is a language-specific variant of the T5 model, specifically tailored for the Bengali language [17]. Like mT5, it uses an encoder-decoder architecture designed to handle text-to-text tasks, making it highly effective for tasks such as translation, summarization, and question answering. BanglaT5 is pretrained on a large Bengali corpus, allowing it to understand the linguistic details and contextual de- pendencies specific to the Bengali language. This makes it particularly suitable for our work, where the goal is to translate Bengali math word problems into corresponding mathematical equations."}, {"title": "D. mBART-50", "content": "mBART50 is a multilingual sequence-to-sequence trans- former model, part of the mBART family, pretrained on data from 50 languages. Like mT5 and BanglaT5, mBART50 utilizes an encoder-decoder architecture, making it highly effective for tasks that involve both understanding and gen- erating text. Its ability to perform translation, summariza- tion, and other generative tasks across multiple languages"}, {"title": "V. EXPERIMENTS", "content": "Our work was conducted on Google Colab Notebook with Python 3.10.12, PyTorch 2.0.1, a Tesla T4 GPU (15 GB), 12.5 GB of RAM, and 64 GB of disk space."}, {"title": "B. Evaluation Metrics", "content": "We evaluated our models using two metrics: BLEU score and solution accuracy. The BLEU score measures the similarity between predicted and reference outputs, but it focuses on surface-level matching. This means it may not fully reflect correctness if a model generates the right equation with slight variations in token arrangement, potentially resulting in a lower score."}, {"title": "C. Hyperparameter Tuning", "content": "Table IV presents the hyperparameters we applied across all models during training. The selection of hyperparam- eters was based on standard practices and preliminary experimentation to optimize the performance of the model while considering resource constraints. Dropout was fixed at 0.1 to prevent overfitting, as it is commonly effective in transformer models. For batch size, we tested 8 and 16 to balance memory usage and training efficiency, while the number of epochs was varied (5, 10, 15) to capture the effect of extended training. The learning rate was set at 1e-4, a standard value for stable convergence in transformer-based architectures. For mBART50, however, we were constrained to a batch size of 8 with 5 epochs due to resource limitations in Google Colab. Despite this limitation, we maintained consistency in other hyperparameters across all models to ensure a fair comparison."}, {"title": "VI. RESULT ANALYSIS", "content": "The performance analysis of the models as shown in Table III indicates that mBART50 achieved the highest accuracy of 97.20% with a BLEU score of 95.74, us- ing a batch size of 8 and 5 epochs. This demonstrates mBART50's strong capability in handling Bengali math word problems. MT5 also performed exceptionally well, reaching a maximum accuracy of 97.30% and a BLEU score of 94.68 with a batch size of 8 and 15 epochs. BanglaT5 showed competitive performance, with its best accuracy at 95.80% and a BLEU score of 94.06 using a batch size of 8 and 15 epochs. However, when the batch size was increased to 16, BanglaT5's accuracy drastically dropped, indicating sensitivity to hyperparameter settings. The Trans- former model, while showing gradual improvement with more epochs and larger batch sizes, reached a peak accuracy of 77.30%, which is significantly lower than that of the other transformer-based models. This result highlights the superior performance of mBART50, MT5, and BanglaT5 over the basic Transformer model in accurately solving Bengali math word problems.\nmT5 achieved the highest accuracy among all the models, with a peak performance of 97.30% accuracy, and a BLEU score of 94.68 when using a batch size of 8 and 15 epochs closely followed by mBART50 which has the accuracy of 97.20%. This demonstrates mT5's superior capability in translating Bengali math word problems into accurate equa- tions. Its performance remained consistently high across different configurations, indicating its robustness and effec- tiveness in handling the linguistic complexity of Bengali text. These results show that mT5 is exceptionally well- suited for this task, outperforming other models in terms of overall accuracy.\nWe proposed the \"PatiGonit\u201d dataset, which has not been the subject of any prior research so far."}, {"title": "VII. CONCLUSION AND FUTURE WORKS", "content": "This study presents a novel approach to solving Ben- gali math word problems using transformer-based models, specifically mT5, BanglaT5, and mBART50. Through the creation of the \"PatiGonit\" dataset and the fine-tuning of these models, we successfully demonstrated their ability to translate complex Bengali word problems into accu- rate mathematical equations. The results show that mT5 outperformed the other models with the highest accuracy of 97.30%, closely followed by mBART50 and BanglaT5. These findings highlight the effectiveness of transformer models in handling low-resource languages like Bengali, offering significant improvements over traditional deep learning methods. Our work not only advances the field of Bengali natural language processing but also contributes valuable resources and methodologies for future research in educational AI tools. While the research was successful, it faced challenges in translating and culturally adapting math problems into Bengali, which required careful linguistic ad- justments. The study also identified a limitation in handling more complex, multi-step problems, suggesting the need for further dataset expansion to enhance the model's robustness and applicability."}, {"title": "VIII. LIMITATIONS", "content": "Our study faced notable limitations that impacted the scope and effectiveness of the research. Translating English math word problems into Bengali proved challenging due to differences in cultural context, including names and references, which often led to ambiguity or inaccuracies in interpretation. Moreover, the dataset predominantly featured basic arithmetic problems, which constrained the models' ability to tackle more advanced queries involving complex equations with multiple operators. This lack of diversity in problem types limited the models' capacity to demonstrate their full potential in addressing a broader range of mathe-matical challenges."}]}