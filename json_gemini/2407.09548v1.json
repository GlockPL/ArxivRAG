{"title": "Towards Temporal Change Explanations from Bi-Temporal Satellite Images", "authors": ["Ryo Tsujimoto", "Hiroki Ouchi", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "Explaining temporal changes between satellite images taken at different times is important for urban planning and environmental monitoring. However, manual dataset construction for the task is costly, so human-AI collaboration is promissing. Toward the direction, in this paper, we investigate the ability of Large-scale Vision-Language Models (LVLMs) to explain temporal changes between satellite images. While LVLMs are known to generate good image captions, they receive only a single image as input. To deal with a par of satellite images as input, we propose three prompting methods. Through human evaluation, we found the effectiveness of our step-by-step reasoning based prompting.", "sections": [{"title": "Introduction", "content": "Change detection (CD) using two satellite images (SIs) taken at different times (hereafter, bi-temporal SI) plays a crucial role in practical applications, such as analyzing damage caused by disasters (Japan Aerospace Exploration Agency, 2021; The National Aeronautics and Space Administration, 2021). Typical research on CD using bi-temporal SIs focuses on identifying pixel-level changes between images (Yang et al., 2022), detecting and analyzing changes in geographical areas over time (Cheng et al., 2023), and using image differencing and ratioing techniques (Parelius, 2023). Very recently, along with the advance of Large-scale Vision-Language Models (LVLMs), a task of explaning changes between bi-temporal SIs has appeared. An existing dataset for the task is Levir-CC\u00b9 (Liu et al., 2022).2 An example is shown in Figure 1, where the transformation of a dilapidated tree-lined street into a residential area is described. One problematic issue is that explanations in this dataset are often too simple to comprehensively capture changes. Another issue is that each bi-temporal SI pair has multiple captions with conflicting interpretations from different annotators. In Figure 1, there is a conflict; e.g., \u201cFive villas are built\" and \"Four buildings are built.\" This example highlights the need for more comprehensive and consistent explanations. Such explanations are costly for humans to create, so human-AI collaboration is promissing; e.g., LVLMs generate explanations and humans modify them. Here, the cost of human modification depends on the performance of LVLMs; i.e., the better explanations LVLMs generate, the less modification humans do.\nToward the human-LVLMs collaborative dataset construction, this study investigates the ability of LVLMs to explain temporal changes between bi-temporal SIs. LVLMs are known for their ability to integrate visual information seamlessly with descriptive ability inherited from LLMs, allowing for nuanced explanations that align with image data (Xu et al., 2023). In this paper, through human evaluation, we explore the efficacy of the explanations in meeting human expectations and understanding of temporal changes in visual data."}, {"title": "2 Method", "content": "This section describes our three prompting methods. Actual prompt examples are provided in Table 2, 3, and 4 of Appendix A.\nAll-at-Once Prompting The bi-temporal SIs are concatenated side by side and input into the LVLM as a single image, shown in the lefthand in Figure 2. Our intent behind this method is to allow the model to directly compare the two images and identify temporal changes in the single inference step.\nStep-by-Step Prompting LVLMs have a high capability for captioning individual SIs (Hu et al., 2023). The motivation behind this method is to address a potential issue with the All-at-Once prompting, where the model might not fully capture distinctive elements within the images. In Step-by-Step prompting, captions for each SI before and after the temporal change are first generated using the LVLM. Then, these captions are input into an LLM to explain the temporal changes. To ensure more detailed explanations, captions for each SI are constrained to include spatial concepts such as places, spatial entities, and topological relations (Pustejovsky et al., 2015).\nHHybrid Prompting This method integrates both All-at-Once prompting and Step-by-Step prompting. The motivation behind this method is to address the limitations identified in each individual approach. In All-at-Once prompting, the model might overlook detailed elements within the images because of the simultaneous presentation of both images. Conversely, in Step-by-Step prompting, although detailed captions are generated, the model may miss the broader context of the change. To mitigate these issues, captions for each SI are initially generated using the LVLM. Then, the concatenated image of bi-temporal SIs and these captions are input to explain the temporal change, leveraging the strengths of both prompting methods to produce more comprehensive and accurate explanations."}, {"title": "3 Experiments", "content": "3.1 Experimental Setup\nModels In the All-at-Once prompting, we used LLaVA-1.5 and GPT-4V. In the Step-by-Step prompting, we used LLaVA-1.5 as the LVLM to generate each SI caption. For generating the change explanation, we used LLaVA-1.5, GPT-3.5-turbo (Ouyang et al., 2022), and GPT-4-turbo (OpenAI et al., 2023) as the LLMs. In the Hybrid prompting, we used either LLaVA-1.5 or GPT-4V to handle the entire process from the caption generation to the change explanation.\nDataset We used Levir-CC, which includes five explanations for temporal changes of each bi-temporal SI pair. We extracted 100 pairs of bi-temporal SIs from the test data of Levir-CC and evaluated the temporal change explanations generated by each method.\nPrompt constraints To avoid reaching the token limit of LLMs, we instructed that explanations be limited to a single clause.\n3.2 Evaluation Metrics\nAutomatic evaluation We measured the coverage of nouns in the generated explanations compared to the change explanation sentences in Levir-CC. Nouns were extracted from the ground truth captions in Levir-CC using the spaCy library, and the proportion of these nouns present in the generated explanations was defined as \"Coverage.\"\nManual evaluation We measured the Truthfulness and Informativeness (Lin et al., 2022) of the generated captions. Table 5 of Appendix provides examples for each evaluation criterion.\nTruthfulness: This criterion asks whether it is free from false information or not. For instance, an explanation with a truthfulness score of 1 in Table 5 of Appendix includes the incorrect statement \u201cgrass being replaced by a paved road.\"\nInformativeness: This criterion judges whether it describes detailed information or features of the image or not. An example with an Informativeness score of 5 in Table 5 of Appendix describes object positions and relationships, such as \"the transformation of the dirt road into a paved street\" or \"The trees in the left image have been replaced by houses.\"\nWe hired two English-speaking annotators to rate each criterion on a scale from 1 to 5.\n3.3 Experimental Results\nTable 1 shows the results.\nOverall For coverage, LLaVA-1.5 with Hybrid Prompting performed the best, indicating that this method is the most effective in identifying and describing all relevant changes in the images. In terms of Truthfulness and Informativeness, GPT-4V with All-at-Once Prompting was the best, suggesting that this approach provides the most accurate and detailed explanations.\nComparison between prompting methods For LLaVA-1.5, we generated explanations using both All-at-Once Prompting and Step-by-Step Prompting. Step-by-Step Prompting outperformed All-at-Once Prompting across all evaluation metrics. This demonstrates the effectiveness of Step-by-Step Prompting. Additionally, the success of generating the final temporal change explanation using its own inferred information suggests that Chain of Thought Prompting (Wei et al., 2022) is also effective in visual tasks.\n3.4 Discussion\nAnalysis of low scores Figures 3 and 4 show examples of explanations rated 1 for Truthfulness and Informativeness, respectively. In the Truthfulness 1 case, the explanation included changes that did not exist. In the Informativeness 1 case, the explanation lacked specific details about the changes. Many low-scored outputs were from bi-temporal SIs with little to no change. This suggests that the models attempted to explain changes even when there were none, i.e., over-explanation. Therefore, future instructions should include exception handling for cases with no changes.\nRelations between the metrics Sometimes, even if the Coverage score is high, the Informativeness score is low.3 There were many cases where Informativeness was rated low due to errors, while a lot of information is contained. The correlation coefficient between Truthfulness and Informativeness was 0.518, indicating a positive correlation. This suggests that humans are highly sensitive to errors. Even if an explanation contains rich vocabulary, a single mistake can significantly lower its Informativeness rating. This is because errors in the explanation greatly affect the overall trustworthiness and quality evaluation. Additionally, concerning coverage, there were cases where coverage was high despite clear errors and low informativeness in the temporal change explanations. This is likely because the coverage of nouns in the ground truth is calculated, which includes commonly occurring words such as \u201cchange,\u201d \u201cscene,\u201d and \"area.\" Therefore, preprocessing tailored to this task, such as setting stopwords and focusing on nouns directly related to observed changes in SIs, should be applied before calculating coverage to accurately reflect the relevance of the generated explanations to the ground truth.\nNumber of words in outputs The average number of words in explanations generated by the Step-by-Step prompting with GPT-3.5-turbo is 130.46 words. On the other hand, in the case of the Hybrid prompting with GPT-4V, the number is 39.53 words, which is approximately one-third of the former. While the former outperforms the latter in both Truthfulness and Informativeness, the latter outperforms the former in Coverage. As the explanation gets longer, the Coverage score tends to increase due to the larger number of possible covered words, while errors are more likely to be contained, which leads to lower scores of Truthfulness and Informativeness."}, {"title": "4 Conclusion", "content": "In this study, we used LVLMs to generate explanations for the temporal changes observed in bi-temporal SIs. Through the fair comparison using only Llama-1.5, we found the effectiveness of the Step-by-Step prompting. Also, All-at-Once prompting with GPT-4V is effective in terms of Truthfulness and Informativeness. Our analysis on low-scored outputs revealed the over-explanation problem; i.e., models attempt to explain changes even when there are none. One future direction is remedying the over-explanation problem. Another direction is to use the models built in this paper for constructing a comprehensive dataset for the task of explaining temporal changes observed in SIs."}, {"title": "Limitations", "content": "We have not experimented with LVLMs capable of handling multiple images simultaneously. Therefore, our proposed methods are tailored to operate within the constraints of existing LVLM capabilities, potentially constraining their ability to effectively capture intricate temporal changes. Subsequent research should investigate the capabilities of multi-image LLMs to offer more comprehensive explanations of temporal changes and overcome the current limitations in the approach."}]}