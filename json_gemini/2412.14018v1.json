{"title": "SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical Video Generation", "authors": ["Tong Chen", "Shuya Yang", "Junyi Wang", "Long Bai", "Hongliang Ren", "Luping Zhou"], "abstract": "Medical video generation has transformative potential for enhancing surgical understanding and pathology insights through precise and controllable visual representations. However, current models face limitations in controllability and authenticity. To bridge this gap, we propose SurgSora, a motion-controllable surgical video generation framework that uses a single input frame and user-controllable motion cues. SurgSora consists of three key modules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB and depth features from the input frame and integrates them with segmentation cues to capture detailed spatial features of complex anatomical structures; the Decoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D features at multiple scales to enhance temporal understanding and object spatial dynamics; and the Trajectory Controller (TC), which allows users to specify motion directions and estimates sparse optical flow, guiding the video generation process. The fused features are used as conditions for a frozen Stable Diffusion model to produce realistic, temporally coherent surgical videos. Extensive evaluations demonstrate that SurgSora outperforms state-of-the-art methods in controllability and authenticity, showing its potential to advance surgical video generation for medical education, training, and research. See our project page for more results: surgsora.github.io.", "sections": [{"title": "1 Introduction", "content": "Generative artificial intelligence (GAI) has achieved significant success in medical scenarios, including vision-language understanding [36], image restoration [3, 6], data augmentation [37], and medical report generation [20], advancing computer-aided diagnosis and intervention [2,7]. Recently, researchers have explored video generation in endoscopic scenarios [18, 23, 40], where realistic dynamic videos offer high-quality resources to support clinician training, medical education, and AI model development. In particular, the controllability of video content such as the motion of surgical instruments and tissues becomes crucial for endoscopic surgical video generation [39]. Controllable generation enables dynamic and realistic surgical scenarios based on simple instructions, offering valuable applications in medical training. Furthermore, controllable video generation addresses the scarcity of annotated surgical data, reduces labeling costs, and enhances model generalization, accelerating downstream AI model development and deployment.\nGeneral scenarios of controllable video generation using diffusion models have been extensively explored [32], where various control signals\u2014such as motion fields or flow-based deformation modules- -are injected through specific parsers to produce videos with desired features and structures [27, 46]. While these approaches enable sophisticated editing of motion patterns, prior works on medical video generation have primarily focused on achieving visually plausible and temporally coherent outputs through effective spatiotemporal modelling [23,40]. However, the crucial aspect of controllability\u2014specifically for surgical videos remains largely underexplored. Existing methods, such as Surgeon [8], rely on text descriptions to control video generation, but simple textual input often fails to capture the intricate and dynamic details of surgical procedures, limiting the precision of generated content.\nTo address this gap, we focus on controllable surgical video generation, where the primary challenge lies in accurately modeling the motion of surgical instruments and tissues based on intuitive user instructions. Given a single surgical image serving as the first frame, we allow users to specify motion directions through a straightforward process akin to direct clicking. This motion direction information is converted into sparse optical flow, which serves as a directive signal for the generation process. To facilitate controllable generation, we propose a novel framework that employs a dual-branch design to extract object-relevant RGB and depth features from the given first frame. These features are then warped using the optical flow data to represent the spatial information of the objects in subsequent frames. Leveraging our proposed multi-information guidance and decoupled flow mapper, our method effectively integrates targeted motion cues, detailed visual features, and object spatial dynamics, enabling the generation of realistic surgical videos with fine-grained motion and precise controllability. This approach not only fills the existing gap in controllable medical video generation but also opens new possibilities for high-fidelity, instruction-driven simulation of surgical scenarios. Our main contributions are summarized as follows.\nWe present the first work on motion-controllable surgical video generation using a diffusion model. This novel approach allows fine-grained control (both direction and magnitude) over the motion of surgical instruments and tissues, guided by intuitive motion cues provided by simple clicks.\nWe propose the Dual Semantic Injector (DSI), which integrates object-aware RGB-D semantic understanding. The DSI combines appearance (RGB) and"}, {"title": "2 Related Works", "content": "Researchers have explored generating videos from images and associated conditions, such as text descriptions [16] or motion control [1]. Controllability remains one of the most significant challenges in I2V generation research. A series of works explore incorporating multiple prompts (e.g., motion, clicks, text, and reference image) to provide more flexible control during video generation [12, 19, 26, 43]. MOFA-Video realizes controllable I2V with sparse motion hints (e.g., trajectories, facial landmarks) via domain-aware MOFA-Adapters to enable precise and diverse motion control across multiple domains [27]. Pix2Gif introduces explicit motion guidance, enabling users to define dynamic elements in the output, thus creating short, loopable animations [21]. Furthermore, ID-Animator [14] and I2V-Adapter [13] insert lightweight adapters into pretrained text-to-video models, employing cross-frame attention mechanisms to achieve efficient and effective I2V generation. In addition, several methods aim to further improve I2V generation performance and maintain high video quality and fidelity. For instance, PhysGen enhances the quality of generative models by using object dynamics and motion derived from physical properties as control conditions [25]. Meanwhile, ConsistI2V improves visual consistency in I2V generation by addressing temporal and spatial inconsistencies, ensuring high visual fidelity [28]. Although extensive research has been conducted on natural and animated scenes, the adaptation of these approaches to medical scenarios remains relatively unexplored and requires further investigation."}, {"title": "2.2 Medical Video Generation", "content": "Medical video generative models have been widely applied in various scenarios [24], such as ensuring privacy in echocardiogram videos [29], simulating disease progression [5], and editing the Ejection Fraction in ultrasound videos [30]. With advancements in diffusion model families, generalized text-to-video generative models have been explored for controllable generation in diverse medical contexts. For example, Bora, fine-tuned on custom biomedical text-video pairs, can respond to various medical-related text prompts [34]. In the field of endoscopy and surgery, Endora is an unconditional generative model designed as an"}, {"title": "3 Methodology", "content": "Our SurgSora framework, illustrated in Figure 1, comprises three key modules: the Dual Semantic Injector (DSI) introduced in Sec. 3.2, the Decoupled Flow"}, {"title": "3.2 Dual Semantic Injector", "content": "Traditional methodologies primarily rely on RGB images as input to create dynamic visual content. While effective in certain applications, this approach suffers from significant limitations in depth perception and scene understanding.Specifically, relying solely on RGB data complicates accurately capturing spatial relationships between objects, leading to deficiencies in visual coherence and object segmentation in generated videos. To address these challenges, we introduce the Dual Semantic Injector (DSI) module, a dual-branch architecture that enhances object awareness by integrating segmentation features into both the RGB and depth feature branches. Unlike traditional methods that depend solely on RGB images, we estimate and incorporate a depth map to provide crucial geometric cues. These cues improve the understanding of spatial relationships between objects and overall scene structure, making it especially beneficial for complex tasks like surgical video synthesis. Furthermore, to better discriminate between objects, object segmentation is leveraged to refine both RGB and depth features.\nThe segment features $f_{seg}$ are combined with RGB images $I_{RGB}$ and depth images $I_{D}$ by passing through two separate processors $\\varPhi_{RGB}$ and $\\varPhi_{D}$ for feature extraction and fusion, followed by two separate encoders for further encoding. The Dual Semantic Injector can be formulated as:\n$f_{r} = \\begin{cases} E_{RGB}(\\varPhi_{RGB}(I_{RGB},f_{seg})), \\\\ E_{D}(\\varPhi_{D}(I_{D}, f_{seg})). \\end{cases}$ (1)\nRecall that the superscript r indicates different scales of feature maps extracted by the encoders. This design uses a dual encoding method, which synchronizes and harmonizes the enhanced features from RGB and depth channels to optimize the overall representation. The injection of segmentation features enhances the semantic understanding compared with using the original RGB and depth features, significantly improving the discrimination of foreground and background, enhancing depth estimation, and ultimately contributing to more realistic and referenceable video predictions."}, {"title": "3.3 Decoupled Flow Mapper", "content": "Previous works [27,31,46] have demonstrated that the effectiveness of diffusion models can be significantly enhanced by adding additional information encoded into latent spaces. For these reasons, we employ a DFM module that bridges the spatial and sequential information of image and optical features to obtain spatial and temporal features for generating sequential videos. The object-aware RGB and depth features output by the DSI module are spatially transformed by the corresponding resized optical flow, respectively, elaborated as follows.\nLet $f_{r} \\in R^{C\\times H\\times W_{r}}$ denote the output feature maps of the DSI module from either the RGB or the depth branch and $f_{r} (x,y)$ represent the feature at the position (x,y). The optical flow $\\theta \\in O(T-1)\\times 2\\times H\\times W$ is first resized to $\\theta \\in O(T-1)\\times 2\\times H\\times W_{r}$ to match the size of $f_{r}$, and then used to spatially transform $f_{r}$ by applying the displacements (dx, dy) provided in each frame $\\theta_{t}$, in which t \u2208 [0,T) represents the current optical frame. The transformation is defined as:\nx' = x + dx, y' = y + dy. (2)\nHere, dx and dy represent the displacements in the horizontal and vertical directions, respectively. Bilinear interpolation is used to estimate the updated pixel values at the new displacements (dx, dy). The mapping procedure is given by:\nf'(x', y') = Interpolate(f'(x, y)). (3)\nDepth information typically captures geometry and spatial structure, while RGB information focuses on appearance and texture. To effectively leverage these complementary properties, we employ a decoupled-mapping method to independently spatially transform and extract frame features from the depth and RGB streams, and then integrate them via a Multi-Scale Fusion Block.\nMulti-Scale Fusion Block (MSF) fuses the optical-flow-transformed RGB and depth features by concatenating them at different scales and then fusing them by two 3D convolution blocks and an activation block. The fusion process is expressed as:\n$f_{fuse} = SiLU(Conv3d(Conv3d(CONCAT(f_{RGB}^{r}, f_{D}^{r}))))$. (4)\nThe fused feature $f_{fuse}$ is then used to assist a frozen Stable Video Diffusion Model for conditional video generation.\nIn sum, the integration of optical flow information enhances temporal continuity between frames, improving the smoothness and visual coherence of the generated videos. It enables the model to accurately capture scene dynamics and interactions, effectively interpreting complex motion. By fusing information from multiple modalities, the model achieves a more comprehensive understanding of scene depth and structure, maintaining visual authenticity while adapting to subtle changes. This multimodal strategy not only elevates video quality but also enriches detail and realism, ensuring a more accurate and dynamic representation of the scene."}, {"title": "3.4 Trajectory Controller", "content": "Surgical videos require more precision compared with natural videos, and generation with a single image makes it more difficult and non-referencable. Therefore, the Trajectory Controller (TC) is employed to enable custom trajectories as input for conditional motion generation. The pipeline of our TC module is shown in Figure 2, which employs a pre-trained trajectory decoder from [45]. The surgeon inputs the first frame image and then clicks to set trajectories. The trajectories and image will be encoded separately, concatenated together, and then decoded by the trajectory decoder into optical flows as a condition to guide the following generation. By involving the TC module, the quality of the generated video will be more referencable and convenient for generating customized surgical videos."}, {"title": "4 Experiment", "content": "We utilize the publicly available CoPESD dataset [38], which was collected from 20 videos using both conventional endoscopic submucosal dissection (ESD) and the DREAMS system [10], performed on in-vivo porcine models. The videos were recorded at a frame rate of 30 Hz with an original resolution of 1920 \u00d7 1080, which was cropped to 1300 \u00d7 1024. After an expert surgeon provided temporal annotations of ESD activities, video segments corresponding to submucosal dissection were separately extracted. We extract 21-frame video clips from the datasets, then resize them into 256 \u00d7 197 resolution and pad them to 256 \u00d7 256"}, {"title": "5 Limitation", "content": "The accuracy and authenticity of videos are particularly important in the medical field. Moreover, the complexity of medical videos is much higher than that of general video content, containing complex dynamic structures and rich details. SurgSora has been evaluated on specific datasets, but its ability to generalize across surgical scenarios and different types of surgeries still needs to be questioned. Besides that, generating long clips requires the model to sustain narrative coherence and visual consistency, which remains challenging due to accumulating errors and drifts in the generated content over time. The model's current capabilities may not fully account for the dynamic and unpredictable nature of live surgeries, where multiple instruments and varying anatomical structures interact in complex ways. This limitation affects the model's utility in training and planning, where understanding these interactions is crucial. Addressing these limitations requires advancements in computational strategies and more robust adaptability of the model to diverse medical contexts. By overcoming these challenges, medical video generation technology can significantly improve, offering more reliable tools for medical training and procedural planning."}, {"title": "6 Conclusion", "content": "In this study, we propose SurgSora, a customized RGBD-flow-guided conditional diffusion video model. SurgSora incorporates a separate depth branch, the Dual Semantic Injector (DSI), which increases object semantics information for dual features, and the Decoupled Flow Mapper (DFM) to provide a more suitable and richer feature representation for the Stable Video Diffusion model. Quantitative and qualitative experiments demonstrate superior performance in medical video generation and the ability to generate reasonable videos with simple trajectories. SurgSora provides a brand new view on the medical video generation field. Future works will focus on high-quality long medical clip generation and multimodal conditional medical video generation."}]}