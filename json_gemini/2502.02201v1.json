{"title": "Can You Move These Over There? An LLM-based VR Mover for Supporting Object Manipulation", "authors": ["XIANGZHI ERIC WANG", "ZACKARY P. T. SIN", "YE JIA", "DANIEL ARCHER", "WYNONNA H. Y. FONG", "QING LI", "CHEN LI"], "abstract": "In our daily lives, we can naturally convey instructions for the spatial manipulation of objects using words and gestures. Transposing this form of interaction into virtual reality (VR) object manipulation can be beneficial. We propose VR Mover, an LLM-empowered solution that can understand and interpret the user's vocal instruction to support object manipulation. By simply pointing and speaking, the LLM can manipulate objects without structured input. Our user study demonstrates that VR Mover enhances user usability, overall experience and performance on multi-object manipulation, while also reducing workload and arm fatigue. Users prefer the proposed natural interface for broad movements and may complementarily switch to gizmos or virtual hands for finer adjustments. These findings are believed to contribute to design implications for future LLM-based object manipulation interfaces, highlighting the potential for more intuitive and efficient user interactions in VR environments.", "sections": [{"title": "1 Introduction", "content": "In virtual reality (VR), object manipulation refers to the task of moving and manipulating 3D objects. It is a commonly used task in many applications and is generally associated with scene editing, for example, level design in VR game development or customizing a personal space in the metaverse. The most common form of object manipulation requires the user to select an object, specify the transform (position, rotation, or scale), and then confirm the maneuver [72]. Despite its importance in VR, this task has been known to induce arm fatigue. This is caused by the so-called \"gorilla-arm effect\" [28], which will manifest when the user needs to perform mid-air gestures for an extended period. Another issue is the learning curve. Typically, any interface will involve a learning curve for the user, but for object manipulation, the aforementioned three-step procedure, along with modification of an object's position, rotation, and scale, means most hand-based gestural interfaces have a high barrier to entry. These issues are a concern, as arm fatigue and a rigid interface can affect user experience, and can be a challenge for object-manipulation interface research [72]. Another issue affecting object manipulation tasks is multi-object manipulation. There are many situations in which the user needs to move multiple objects together. It seems that there is a lack of discussion on how to effectively handle multi-object manipulation, as most research focuses on improving object manipulation in isolation - one object at a time [72]. For handling multiple objects, the assumption seems to be that the user should first select several objects in tandem and then manipulate them as a group. Although this is a valid strategy, this assumes that all of the grouped objects require exactly the same manipulation, which may often not be the case.\nTo address the issues raised earlier, we take note of how we perform object manipulation tasks in reality. Specifically, we also perform \"scene editing\" in the real world, such as when we move residences or decorate rooms. During these processes, it is possible to instruct others to help move objects. In particular, humans tend to combine speech and gestures in the completion of spatial communication tasks [3]. Thus, we combine the use of commanding statements and gestural pointing to provide others with feedback [30], and we do so naturally, generally without noticeable arm fatigue or difficulty. We can also give unstructured and context-driven instructions that may also involve multiple objects at once. Since this innate spatial communication is effective and intuitive, our goal is to translate it into VR to create the effect of having a virtual mover who can assist with object manipulation tasks.\nInspired by how people communicate spatial manipulation instructions in the real world, we propose utilizing a large language model (LLM) to realize a VR Mover to support the user's object manipulation. Specifically, the LLM is given several APIs (to move, rotate, scale, create, and remove objects), and based on where the user is pointing and their concomitant instructions, the LLM will decide how to perform the object manipulation task. Because the LLM can"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Object Manipulation in Virtual Reality", "content": "Object manipulation in VR has garnered significant interest among researchers. The most intuitive method is hand-based object manipulation. Beginning in 1996, Poupyrev et al. introduced the GO-GO interaction technique, which facilitates the manipulation of objects both in close proximity and at a distance. This method employs a metaphorical extension of the user's arm combined with a non-linear mapping of hand movements to enhance interactive capabilities [52]. Mendes et al. developed a hand-based manipulation technique termed MAIOR, which provides distinct translation and rotation functionalities, thereby achieving the precision of degrees of freedom separation without compromising task completion time [43]. Gloumeau et al. developed PinNPivot, a manipulation technique that utilizes controllers to map hand gestures for virtual hands engaged in manipulation tasks. They compared this technique to other baseline methods, including MAIOR, and found that their approach demonstrated superior performance [25]. However, hand-based manipulation techniques are widely recognized as more likely to induce arm muscle fatigue, which detrimentally affects the user experience [34, 72]. Consequently, many scholars have explored alternative approaches. Some studies have demonstrated that gaze can effectively support object manipulation. Robert proposed that eye movements could function as an input method for computer interactions [32]. Yu et al. developed a technique for 3D object manipulation that integrates gaze for object selection with manual manipulation for object adjustment [72]. Furthermore, Bao et al. introduced methods such as Gaze Position, Guided Interaction, and Gaze Beam Guided Interaction, which not only utilize gaze for object selection but also facilitate object movement [7]. Additionally, several studies have investigated the use of voice commands for object manipulation. For example, Adam S. William et al. conducted an elicitation study"}, {"title": "2.2 Voice-command Interface", "content": "Voice commands are integral to the development of VR interfaces, serving various functions such as navigation [31], design [48], and interactive dialogue [26]. The scope of research in voice-enabled VR interfaces is extensive; however, this study narrows its focus to the specific use of voice commands for interacting within VR settings. Notable contributions in this area include Schroeder et al.'s development of a voice-activated system for VR-based alternator maintenance training [58], and Desolda et al.'s implementation of a voice-driven system to aid in 3D modelling [17]. Aziz et al. introduced innovative voice-controlled techniques-NoSnap, UserSnap, and AutoSnap-for manipulating graphical object dimensions, demonstrating through user evaluations that these methods, particularly AutoSnap, significantly enhance efficiency and accessibility for users with physical impairments in creative applications [5]. Additionally, Friedrich et al. introduced an innovative interaction paradigm that merges voice control with hand gesture recognition for intuitive manipulation of CAD models in VR [21]. Whitlock et al. investigated the efficacy of various interaction modalities - including voice commands, freehand gestures, and handheld devices - for manipulating objects at different distances in augmented reality [68]. Furthermore, Fernandez et al. developed Hands-Free VR, a natural language voice interface for VR that leverages advanced deep learning models for speech-to-text conversion and sophisticated language models for precise text-to-command translation, demonstrating superior efficiency and user preference compared to traditional VR interfaces [19]. Despite these advancements, the integration of voice commands with other control modalities can sometimes create inconsistent user experiences, particularly when switching between interaction types or dealing with complex command structures. These limitations highlight the need for further technical refinement to enhance reliability and user satisfaction in diverse operational settings."}, {"title": "2.3 LLM-based Interface", "content": "LLMs have captivated the global research community due to their demonstrated efficacy across various applications. For instance, LLMs have shown potential in enhancing writing skills [33], aiding programming tasks for novices [36], and developing question-answering capabilities in children [1]. These successful implementations often utilize what is known as prompt engineering. As highlighted by Arora et al., effective prompts typically involve question-answering formats that foster open-ended generation. By feeding LLMs with QA examples, they are able to generate stable responses, thereby facilitating their integration into VR environments. However, the deployment of LLMs is highly task-specific, necessitating tailored configurations for different applications. Consequently, designing an efficient LLM interface for VR remains a challenging endeavor."}, {"title": "2.4 Layout Generation", "content": "Traditional methods for generating layouts through optimization heavily depend on prior knowledge of feasible config-urations, often derived from procedural modelling or predefined scene graphs [40, 53]. These approaches necessitate specialized expertise and exhibit limited adaptability in dynamic settings. Consequently, researchers have explored generative models as a potential solution to these limitations. For instance, Miguel et al. introduced GAUDI, a generative model that facilitates both unconditional and conditional generation of 3D scenes [8]. Handa et al. developed SceneNet, a framework designed to generate annotated 3D scenes, thereby enhancing indoor scene understanding [27]. Additionally, Chen et al. proposed SceneDreamer, a novel generative neural hash grid that parameterizes the latent space based on 3D positions and scene semantics [13]. In the realm of LLMs, new perspectives on text-based layout reasoning have emerged, circumventing the limitations associated with traditional data sets. Feng et al. introduced LayoutGPT, a method that composes in-context visual demonstrations in style sheet language to enhance the visual planning capabilities of LLMs, enabling them to consider layout plans with detailed specifications such as bounding boxes and the orientation of furniture items. Furthermore, Fu et al. developed AnyHome, which utilizes text-based inputs to generate realistic spatial layouts by directing the synthesis of geometry meshes within defined constraints [67]. SceneTeller, another innovative approach, employs an LLM-based pipeline to generate high-quality scenes [49]. These advancements highlight a shift towards more flexible, adaptive layout generation technologies that leverage the power of generative models and language processing. Our work differs from these efforts in that our goal does not rely on a smart agent to generate a scene. Instead, we aim to develop and design an interface that assists users in VR object manipulation."}, {"title": "3 VR Mover: A Supportive Natural User Interface for Object Manipulation", "content": "In this section, we discuss the rationale and cognitive theory behind VR Mover, and how they effect its interaction design."}, {"title": "3.1 Spatial Manipulation in the Real World", "content": "We can visualize how we usually communicate spatial manipulation by imagining that we have movers to help us when moving to a new house. We may do the following:\n(1) point at a chair and ask the mover to move it to a specific location by pointing again.\n(2) verbally ask the mover to move the dining chairs and the table to a general location by pointing\n(3) verbally ask the mover to move a table decoration on top of a dining table.\n(4) gesture in a direction to ask the mover to move an object towards a specific direction.\n(5) use the surroundings to describe where to place the object.\n(6) once the mover has finished moving the object, we may fine-tune the final position ourselves.\nThese are just some of the examples of how we may communicate with others regarding spatial manipulation. Later, we will show how we aim to achieve similar interactions with VR Mover. In the next subsection, we will briefly discuss the cognitive theory on some of the expression and communication we used here."}, {"title": "3.2 Visual Working Memory", "content": "Whether we want to manipulate an object in VR or attempt to communicate spatial manipulation instructions, it is most likely that we have a mental image and then we manipulate it internally first. This human cognitive process is part of our visual working memory (VWM) [62]. VWM is how humans hold, manipulate and interpret information for a variety of everyday tasks [6, 39]. How VWM affects our ability to manipulate and handle memory for object location and movement tasks is of particular interest to us. Two governing principles in VWM can provide insights into object manipulation interfaces.\n\u2022 Chunking: Humans have a tendency to group objects together to facilitate comprehension and communication. In the context of VWM, we have a process called chunking which will encode information as larger perceptual chunks [47, 61]. Generally, humans may chunk objects together based on similar colors, locations, and shapes (e.g. chairs of the same set) [4]. Alternatively, when chunking an environment, larger environments may be organized as nested sub-environments [54]; that is, object and their locations may be chunked together into memories represented as an area (e.g. dining area). Thus, in the context of object manipulation, we may infer that people will follow this tendency to group related objects during a manipulation task. Thus, a convenient method to select or refer to a group of objects is a topic worth exploring for object manipulation.\n\u2022 Coarse-to-fine Processing: Another tendency of human nature is to process visual information in a coarse-to-fine manner [60], which is referred to as the so-called coarse-to-fine theory. Recent works have shown that VWM also follows this coarse-to-fine process where the mental image is constructed gradually [22, 23]. It should be noted, however, that another work has pointed out that the coarse-to-fine process is not the only cognitive pathway in VWM [71]. Still, it has been shown that coarse information takes precedence over detailed information [23]. Thus, based on these VWM works, it can be inferred that object manipulation may also involve a coarse-to-fine process. We should consider a fast coarse placement initially, followed by a fine-tuned adjustment later."}, {"title": "3.3 Interaction Design", "content": "Here, we discuss how the user can interact with the VR Mover, along with its features and benefits. Some of the interaction design aims to mimic the imagined scenarios discussed in subsection 3.1.\n\u2022 Pointing: In the real world, people naturally use pointing as a method to communicate a point of interest or indicate an object of interest [15]. In the case of VR Mover, the user can point at an object and then point at a position to either move the object by saying \"Move the chair [point] to here [point]\" (Figure 2a); or to make the object rotate towards that point by saying \"Make the chair [point] look at here [point]\". It should be noted that our current implementation requires the user to actively quick press A to indicate that they are pointing at something and [point] is indicating when the user has indicated pointing.\n\u2022 Lining: Aside from pointing, we have different ways to gesturally indicate spatial information. Another method is by drawing a line (lining) to indicate direction or a line-of-interest. For example, the user can say \"move the object that"}, {"title": "4 Methodology of VR Mover", "content": "LLMs have shown promise in spatial arrangements, complex task sequences, and as specialized agents [9, 16, 64, 65]. However, many existing models suffer from long response times, ranging from more than 10 seconds to several minutes [16, 24], while interactive VR interfaces crucially rely on real-time responses. Additionally, the lack of domain-specific training data, and the resource-extensive fine-tuning of LLMs for the sake of content quality, further complicate the implementation of object manipulation tasks within VR. To this end, we propose an LLM-empowered interface ready for multi-modal object manipulation in virtual space with real-time responses. VR Mover follows a user-centric design, it is not only training-free to provide stable responses with around a 2-second delay but can also interpret unstructured instructions into structured object manipulation directives.\nStarting from modelling the virtual environment in subsection 4.1, VR Mover operates in a cycle of three main components: a user-centric augmentation module (4.2), LLM processing module (4.3), and scene update module (4.4). The overview of the interface I/O flow is presented in Figure 7. Scene modelling promotes scene understanding, while the user-centric augmentation module continuously collects motion, speech, and action data from the user. The LLM processing module maintains the text-only prompts with different roles, organizes the context data, and sends requests to the cloud LLM service while streamlining the response. The LLM's response consists of prescribed formatted API calls, which are parsed by the scene update module upon arrival."}, {"title": "4.1 Scene Modelling", "content": "To enable the LLM to generate reasonable object(s) placement proposals, it is necessary to effectively convert 3D spatial information into a text-based format while maintaining the important and expressive features of the scene. We model the scene following a previously proposed taxonomy [20], where a scene can be defined as objects placed within a background with spatial and semantic information relations. The scene elements are categorized into environmental objects (static) and manipulatable objects (dynamic and interactive). For example, if the virtual environment in the interface is a large empty room for furniture manipulations, the scene elements should be floor, walls, and windows and manipulatable objects should be the furniture. Both types are 3D models contributing to the scene's spatial and semantic data."}, {"title": "4.2 User-Centric Augmentation Module", "content": "Maintaining the analogy of communicating with the mover during house moving, the process can be broken down into the following steps: the mover listens to the client's instructions, observes their gestures, and interprets the meaning. It is essential to collect accurate data from the user, organized in a user-centric manner. The system primarily prepares data based on: what the user is saying (4.2.1), what the user is looking at (4.2.2), where the user has indicated (4.2.3), and what the user is saying or looking at during these actions (4.2.4). This data will be processed and packed into a JSON format (Figure 9) ready to be sent as a part of the user prompt for each request cycle."}, {"title": "4.2.1 Speech Recognition", "content": "The speech-to-text (STT) service, provided by Microsoft Azure [46], keeps listening to the voice signal captured by the microphone on the VR headset and returns transcriptions with metadata when speech is detected. The return data serves three main purposes: 1. Provide dynamically transcribed text from the user's speech; 2. The timestamp of each spoken word, for future time serializations (subsubsection 4.2.4); 3. Decide whether the user has stopped talking, and if so, send the packed request to the LLM service to generate a response (subsection 4.3). Furthermore, the transcribed text can be used for interjection filtering."}, {"title": "4.2.2 Focus Frames", "content": "Ambiguities can arise if the LLM has no acknowledgment of what the user is looking at during the talking. Due to headset limitations, we track the user's head motion instead of the gaze. However, continually recording the head motion or objects in the view frustum generates a huge amount of data, and is not straightforward for the LLM to decide the object-of-interest. We define focus frames as groups of continuous viewports over a period in which the user is staring at a small range of objects or positions during speech. The system averages the viewport of the focus frames group while accumulating the objects that appear in the view frustum and ranking them based on the screen distance to the center. When the player's current frame head motion changes beyond the threshold, the current group ends, and the current focus frame moves to the next group. A focus frame group with too short a duration will be filtered out. In each focus frame-group, the higher the ranking is, the more likely the user is looking at the related object during that period. One example can be seen at Figure 9."}, {"title": "4.2.3 Processing Gestural Cues", "content": "Except for the point position for the pointing, and the start/end points data for the lining, additional information is recorded alongside these actions to strengthen the connection between the action, time, and environment. For the pointing, along with the position of the intersection point between the interaction ray and any of the initial objects in the scene, the surface normal and hit objects are timestamped. For the lining, despite the hit object, normal, and position of the starting point, there are two endpoints. One is the result of applying the hand movement during the drawing to the start point, presenting the visual endpoint of the line. Considering that inaccuracies might arise from manual input, the second end point is the intersection point when the player stops pressing the button, which also contains the position, normal, and hit object. The start time and duration of a drawing line are stored together with other relevant data. However, ambiguities are introduced when there are multiple pointing or drawing lines with no apparent link to the text provided, even if the actions are presented in chronological order. The next sub-subsection solves this problem with timestamps."}, {"title": "4.2.4 Text-based Time Serialization", "content": "With the transcribed text, focus frames, and the recorded gestural cues, it can still be difficult to understand the user's intentions. The LLM processes the data over a certain period, so timing is regarded as a crucial factor to reduce the ambiguity between actions or viewports. Aside from the pure transcription of the speech (e.g. \"Put the chair here, and I want four pictures along the wall.\"), we also provide a gestural inserted transcribed text (e.g. \"put the chair here [<p>], and [<d0>start] I want four pictures to line [<d0>end] the wall\" by comparing the timestamps of the gestures with the words, where \"po\" and \"do\" are the IDs of the pointing and drawing respectively, and can be referred to the collection of the gestural cues). Moreover, to decipher what was the user talking about when looking at a specific area, the transcription segments are inserted into the focus frames as a JSON field. The changes after the time serialization are highlighted in red in the Figure 9."}, {"title": "4.2.5 Interjection Filtering", "content": "In our pilot study, we found that users tend to express interjections (e.g. \u201cumm\u201d, \u201cOK\u201d) when the VR Mover has completed the request. The user might be interrupted by a response like \"The request is unclear, please provide specific instructions.\" because the interjections will be transcribed and sent to the LLM without being noted. In addition, this sort of redundant communication stretches the context window and also causes a waste of resources. Aside from asking the user to keep silent, we have also filtered frequently occurring interjections such as \u201cumm\u201d, \u201cmmm\u201d, \"OK\u201d, and \u201cgood\u201d. If the transcribed text only consists of interjections, the request will not be sent to the LLM."}, {"title": "4.3 LLM Processing Module", "content": "The overview of this module is shown at Figure 10. The background LLM hosted on Microsoft Azure [46] is responsible for parsing the user's intentions and requests from the user-centric augmentation module (4.2) in a JSON, and returning generated formatted API calls to perform object manipulation automation. To ensure the LLM behaves properly in a highly dynamic situation and provides logical outputs in a limited time, transforming the user's unstructured requests into a set of well-structured API calls to the structured 3D space management, we instruct it via prompt engineering, manage the context, and use a streamlined response with GPT-40, a very fast output model."}, {"title": "4.3.1 Prompt Engineering", "content": "The three types of role-defined prompts serve different purposes. In our system prompt, except for providing its role and principle, we prepared a list of available APIs for the LLM to generate the response, explained the input format to guide the reasoning, and provided the environmental objects and manipulatable prefabs for it to place and manipulate. Upon each request, the outstanding data collected by the user-centric augmentation module are packed into a JSON format and inserted into the context. The assistant prompt is basically the duplication of the exact response of LLM, filtering out the API calls that fail to run.\nHowever, we found it impossible to generate reproducible [45] or stable responses for the current settings, even if we locked the seed, and set the temperature and top k variables as 0. The quality of the response varies each time when starting the system. To address this problem, we manually composed a pair of user and assistant prompts (with a detailed explanation before each API call) right after the system prompt, as a one-shot example, in the context to guide the reasoning and fusion. The contents produced by the LLM then tend to fluctuate within a smaller and more acceptable range."}, {"title": "4.3.2 Context Management", "content": "While VR Mover interface conceptually runs for an infinite time and number of communi-cation rounds, typical LLM models' context length is fixed and limited. To avoid context overflow, a sliding window is applied to the context. The system prompt and the example pair of user and assistant prompts stay at the top, and the latest 5 pairs of user-assistant prompts are appended upon each request. Due to the dynamic length of the user prompt, which can be affected by the existing objects in the scene, number of gesture cues, duration of the speech, etc., there is"}, {"title": "4.3.3 Real-time Response Formatting", "content": "Multiple on-the-shelf local and cloud LLM models, series such as Llama [44], GPT [50], and Claude [2], are available for performing the tasks through our interfaces. Recent work mainly utilize GPT-4, however, as shown in [64], [24], and [16], the response time varied from 10 seconds or more to minutes. This is unacceptable for an interactive interface. We chose GPT-40 [51] hosted by Azure [46], for the sake of response speed and generated content quality. Even if GPT-40 accepts image data, and it is possible to perform visual prompting to enhance the placement of objects, image data takes significantly longer to process. Thus, we have chosen to investigate how text-based structures handle spatial data for the sake of quick response time.\nNevertheless, the response time is also related to the formatting of the response data. While several works [64-66] prefer to stipulate JSON as the response format, it is not straightforward for extraction if not completed, meaning the system cannot execute the response until the entire JSON is generated and returned. We require the LLM to follow a line-wise API function call in the format of \u201c<function name>(<param1>, <param2>, ...); \\n\u201d , and configure the web requests to receive a response in a streaming manner. Thus, once the first line of the response is received, it will be sent to the interpreter in the scene update module (4.4), and the user can see the scene starts updating immediately. On average, the response delay, counted from the request sent to the LLM to the first line of the API call executed (the system starts to reflect the user's input), is around 2 seconds. With this design, almost all users are satisfied with the response speed."}, {"title": "4.4 Scene Update Module", "content": "This module is in charge of parsing the API calls from the LLM response and updating the scene by either amending the existing object(s) within it or adding new object(s). When a new line is received, the module is triggered and starts to extract the method name and corresponding parameters. The API call will be mapped onto the real runtime functions, corresponding to each type of object manipulation, and invoked asynchronously. A successful API call will affect the virtual scene and update the scene states, in turn influencing the next round of communication. The full flowchart of this module is presented at Figure 11.\nDue to the stochastic nature of the LLM [16], the LLM is very likely to generate function calls that cannot be parsed and performed (e.g. not existing function, wrong format, or invalid parameters). However, this rarely happens in practice"}, {"title": "5 User Study", "content": "In this section, the setting of the user study is presented. Its result and subsequent discussion will be discussed later."}, {"title": "5.1 Experimental Techniques", "content": "In the user study, we will compare the following three techniques. We hope to derive insights from how users used and viewed them differently.\n\u2022 Gizmos + Virtual Hand (Control): This is the technique that act as the control. It involves the gizmos (Figure 12a) and virtual hand [42] (Figure 12b). The gizmos can be interacted with via the interactive ray. For the virtual hand, as it can operate even when the user's hand is not next to the object, it is somewhat similar to the implementation of remote hand [72]. The combination of gizmos and virtual hand is picked as it is believed to be a sufficient representation of object manipulation techniques that are commonly used, see previous research [18, 72] and current applications. Multi-object selection to perform synchronized manipulation (e.g. moving all objects in the same way) is possible by first selecting the objects (with quick press A). For brevity, we refer to this interface as the Control technique.\n\u2022 Voice Command: In order to highlight the importance of an LLM that can understand the perspective and unstructured instruction of the user, we have separately developed a voice command variant for the VR Mover. Instead of letting an LLM process the user's speech and decide which API to call to complete the object manipulation request, a set of structured voice commands is used to directly map to the same set of APIs. Its implementation is similar to that of a previous voice-driven locomotion technique where the voice command is implemented via regular expression and mapping [31]. It follows a grammar of  , where the brackets \"()\" indicate optional input. The most important commands are \u201cmove this here\u201d and \u201crotate this here\u201d in which the user can specify the object(s) to manipulate via selecting and the how to manipulate with pointing. It should be noted that this variant interface only includes pointing, but not aligning as the latter is a method to express ambiguous requests (inferring direction, area, and movement). Thus, it is believed that this is no clear way to implement alignment for Voice Command. Regardless, it is not expected to perform well for fine adjustment of object placement. Thus, this Voice Command interface also includes the same gizmos and Virtual Hand from the Control technique (Figure 12c)."}, {"title": "5.2 Experimental Tasks", "content": "Similar to a previous VR object manipulation work [72], our user study includes two tasks. Task 1 is a performance-centric task aimed at evaluating a user's ability to move objects given specific targets. It is further divided into two sub-tasks, Task 1A and 1B. Task 2 is a creative-oriented task that allows the users to freely move objects to embellish the content of a VR room.\n\u2022 (Task 1A) Single Mid-air Object Manipulation: The first task involves the user moving an object from source to target. A semi-transparent version of the manipulatable target is used to indicate the goal. The scene involves a chair placed on the ground and a target in mid-air. The distance between the source object and the goal target is measured by the distances between the eight points of a bounding box. Once the average distance is below a threshold, the object is considered to have reached the target. In this sub-task, we aim to evaluate a technique's ability to handle mid-air manipulation. The chair is around one meter tall and both the object and target are three meters away from the user.\n\u2022 (Task 1B) Multi-object Manipulation: The second task involves moving several manipulable objects to their targets. Similar to the previous task, semi-transparent targets are used to indicate the goals. This sub-task differs from the previous one in that it aims to evaluate a technique's ability in handling multi-object manipulation. However, we did not force the user to use the multi-object capability and they may complete the task one object at a time if they find that to be more suitable for them.\n\u2022 (Task 2) Sandbox Room for Object Placement: In order to see how users utilize the proposed LLM-supported technique in a more realistic setting, this task provides the users with an empty room. Via a gaze-activated prefab menu in VR, the user can use an assortment of prepared 3D models to populate the room. A soft goal of the task is that the user should try to replicate the mini-room provided via the VR UI panel. However, they are encouraged to test the interaction technique as they see fit. It is believed this gives the user an opportunity to test the object manipulation workflow and provide subjective feedback. As there is no specific end goal for Task 2, each user is simply given 7 minutes to freely manipulate objects in the room."}, {"title": "5.3 Participants and Apparatus", "content": "Participants for the user study were recruited through advertisements posted on campus, with all confirming their affiliation as either students or staff of the university. The study involved 24 participants who were randomly assigned to various groups within the experiment. The demographic composition of the study participants included seven males, sixteen females, and one individual of unspecified gender. The mean age of participants was 22.96 years, with a standard deviation of 3.83. Participant ages varied from 18 to 35 years, with a median age of 23 years. Ethical approval for the study was obtained from the institutional review board.\nIn the user study, participants experienced VR through a Meta Quest 3 Head-Mounted Display, while experimenters observed user behavior and recorded the content displayed in the headset using a laptop."}, {"title": "5.4 Measures", "content": "The following measures are used to evaluate the three techniques. Some are objective data collected by the program while some are subjective user feedback. Note that the arrow \u2191 (\u2193) means the higher (lower) the better.\n\u2022 Coarse Manipulation Time (\u2193) is the time it took for the user to move the object to a \u201cnear enough\u201d position (in Task 1). As discussed in a previous paper, the user will spend significant time adjusting the object to fit the target more closely [72]. This measure provides insights into the time required for a user to move an object from the starting position to a location proximate to the target position. An object is considered to have reached to the coarse target when the average distance of the eight points of the bounding box is smaller than 0.3m."}, {"title": "5.5 Procedure", "content": "For each participant, the following procedure was used: (Step 1) The user fills out a consent form and a basic information questionnaire (e.g. Age). (2) Repeat 3a-3e until all three techniques have been tried. (3a) Based on the random order the user is assigned, the technique's overview is presented to the user. (3b) a practice session with an in-VR tutorial to familiarize the user with the interface (3c) Complete Task 1 (3d) Complete Task 2 (3e) The user will fill a SUS, Presence Questionnaire, NASA-TLX, and UEQ-S after the completion of a task with a technique. (4) At this point, the user has completed all the trials and will be given the preference ranking to fill out, in addition to an interview session."}, {"title": "6 Result", "content": "This section reports the results of the user study. When reporting a measure's mean (SD) for each technique, the order of reporting is always Control group, Voice Command and VR Mover. For each measure, the Shapiro-Wilk test is first conducted to check for data normality. If that is the case, typical repeated measures ANOVA and Student's t-tests are used. Otherwise, the Friedman test is performed and the paired test is done with Wilcoxon signed-rank. Bonferroni correction is performed on all post-hoc analyses. Note that we are only interested in comparing VR Mover with Voice Command and the Control group with the post-hoc tests. Effect size is shown with Cohen's d. The threshold for statistical significance was established at a p \u2013 value of 0.05 for all analyses."}, {"title": "6.1 Manipulation Time"}]}