{"title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model", "authors": ["Chunting Zhou", "Lili Yuu", "Arun Babus", "Kushal Tirumalah", "Michihiro Yasunaga", "Leonid Shamish", "Jacob Kahn", "Xuezhe Ma", "Luke Zettlemoyer", "Omer Levy"], "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.", "sections": [{"title": "1 Introduction", "content": "Multi-modal generative models need to be able to perceive, process, and produce both discrete elements (such as text or code) and continuous elements (e.g. image, audio, and video data). While language models trained on the next token prediction objective dominate discrete modalities, diffusion models and their generalizations are the state of the art for generating continuous modalities. Many efforts have been made to combine these approaches, including extending a language model to use a diffusion model as a tool, either explicitly or by grafting a pretrained diffusion model onto the language model . Alternatively, one can quantize the continuous modalities and train a standard language model over discrete tokens , simplifying the model's architecture at the cost of losing information. In this work, we show it is possible to fully integrate both modalities, with no information loss, by training a single model to both predict discrete text tokens and diffuse continuous images.\nWe introduce Transfusion, a recipe for training a model that can seamlessly generate discrete and continuous modalities. We demonstrate Transfusion by pretraining a transformer model on 50% text and 50% image data using a different objective for each modality: next token prediction for text and diffusion for images. The model is exposed to both modalities and loss functions at each training step. Standard embedding layers convert text tokens to vectors, while patchification layers represent"}, {"title": "2 Background", "content": "Transfusion is a single model trained with two objectives: language modeling and diffusion. Each of these objectives represents the state of the art in discrete and continuous data modeling, respectively. This section briefly defines these objectives, as well as background on latent image representations."}, {"title": "2.1 Language Modeling", "content": "Given a sequence of discrete tokens y = y1, ..., yn from a closed vocabulary V, a language model predicts the probability of the sequence P(y). Standard language models decompose P(y) into a product of conditional probabilities  \u041f_{i=1}^{n} P_{o}(y_{i}|y_{<i}). This creates an autoregressive classification task, where the probability distribution of each token yi is predicted conditioned on the prefix of a sequence y<i using a single distribution P_\u03b8 parameterized by \u03b8. The model can be optimized by minimizing the cross-entropy between P_\u03b8 and the empirical distribution of the data, yielding the standard next-token prediction objective, colloquially referred to as LM loss:\nL_{LM} = E_{y}: [-log P_{\u03b8}(y_{i}|y_{<i})]\t\t(1)"}, {"title": "2.2 Diffusion", "content": "Denoising diffusion probabilistic models (a.k.a. DDPM or diffusion models) operate on the principle of learning to reverse a gradual noise-addition process. Unlike language models that typically work with discrete tokens (y), diffusion models operate over continuous vectors (x), making them particularly suited for tasks involving continuous data like images. The diffusion framework involves two processes: a forward process that describes how the original data is turned into noise, and a reverse process of denoising that the model learns to perform.\nForward Process From a mathematical perspective, the forward process defines how the noised data (which serves as the model input) is created. Given a data point x0,  define a Markov chain that gradually adds Gaussian noise over T steps, creating a sequence of increasingly noisy ver-sions x1, x2, ..., xT. Each step of this process is defined by  q(x_t | x_{t-1}) = N(x_t;  \u221a1 \u2013 \u03b2_t x_{t-1},  \u03b2_tI), where \u03b2_t increases over time according to a predefined noise schedule (see below). This process can be reparameterized in a way that allows us to directly sample xt from x0 using a single sample of Gaussian noise \u03f5 ~ N(0, I):\nx_t = \u221a\\bar{\u03b1}_t x_0 + \u221a1 - \\bar{\u03b1}_t \\epsilon  (2)\nHere,  \\bar{\u03b1}_t = \u041f_{s=1}^{t}(1 \u2013 \u03b2_s), providing a useful abstraction over the original Markov chain. In fact, both the training objective and the noise scheduler are eventually expressed (and implemented) in these terms.\nReverse Process The diffusion model is trained to perform the reverse process p_\u03b8(x_{t\u22121}|x_t), learning to denoise the data step by step. There are several ways to do so; in this work, we follow the approach of  and model the Gaussian noise \u03f5 in Equation 2 as a proxy for the cumulative noise at step t. Specifically, a model \u03f5_\u03b8(\u00b7) with parameters \u03b8 is trained to estimate the noise \u03f5 given the noised data xt and timestep t. In practice, the model often conditions on additional contextual information c, such as a caption when generating an image. The parameters of the noise prediction model are thus optimized by minimizing the mean squared error loss:\nL_{DDPM} = E_{x_0,t,\\epsilon} [||\u03f5 \u2013 \u03f5_\u03b8(x_t, t, c) ||^2]  (3)\nNoise Schedule When creating a noised example xt (Equation 2),  \\bar{\u03b1}_t determines the variance of the noise for timestep t. In this work, we adopt the commonly used cosine scheduler , which largely follows \u221a\\bar{\u03b1}_t \u2248 cos(t\u00b7\u03c0/2T) with some adjustments.\nInference Decoding is done iteratively, pealing away some of the noise at each step. Starting with pure Gaussian noise at x_T, the model \u03f5_\u03b8(x_t, t, c) predicts the noise accumulated at timestep t. The predicted noise is then scaled according to the noise schedule, and the proportional amount of predicted noise is removed from xt to produce x_{t-1}. In practice, inference is done over fewer timesteps than training. Classifier-free guidance (CFG) is often used to improve generation by contrasting the prediction of the model conditioned on the context c with the unconditioned prediction, at the cost of doubling the computation."}, {"title": "2.3 Latent Image Representation", "content": "Early diffusion models worked directly in pixel space , but this proved computationally expensive. Variational autoencoders (VAEs)  can save compute by encoding images into a lower-dimensional latent space. Implemented as deep CNNs, modern VAES are trained on a combination of reconstruction and regularization losses, allowing downstream models like latent diffusion models (LDMs) to operate efficiently on compact image patch embeddings; e.g. represent every 8\u00d78 pixel patch as an 8-dimensional vector. For autoregressive language modeling approaches, images must be discretized. Discrete autoencoders, such as vector-quantized VAES (VQ-VAE) , achieve this by introducing a quantization layer (and related regularization losses) that maps continuous latent embeddings to discrete tokens."}, {"title": "3 Transfusion", "content": "Transfusion is a method for training a single unified model to understand and generate both discrete and continuous modalities. Our main innovation is demonstrating that we can use separate losses for different modalities \u2013 language modeling for text, diffusion for images \u2013 over shared data and parameters. Figure 1 illustrates Transfusion.\nData Representation We experiment with data spanning two modalities: discrete text and continuous images. Each text string is tokenized into a sequence of discrete tokens from a fixed vocabulary, where each token is represented as an integer. Each image is encoded as latent patches using a VAE (see \u00a72.3), where each patch is represented as a continuous vector; the patches are sequenced left-to-right top-to-bottom to create a sequence of patch vectors from each image. For mixed-modal examples, we surround each image sequence with special beginning of image (BOI) and end of image (EOI) tokens before inserting it to the text sequence; thus, we arrive at a single sequence potentially containing both discrete elements (integers representing text tokens) and continuous elements (vectors representing image patches).\nModel Architecture The vast majority of the model's parameters belong to a single transformer, which processes every sequence, regardless of modality. The transformer takes a sequence of high-dimensional vectors in Rd as input, and produces similar vectors as output. To convert our data into this space, we use lightweight modality-specific components with unshared parameters. For text, these are the embedding matrices, converting each input integer to vector space and each output vector into a discrete distribution over the vocabulary. For images, we experiment with two alternatives for compressing local windows of k \u00d7 k patch vectors into a single transformer vector"}, {"title": "Transfusion Attention", "content": "Language models typically use causal masking to efficiently compute the loss and gradients over an entire sequence in a single forward-backward pass without leaking information from future tokens. While text is naturally sequential, images are not, and are usually modeled with unrestricted (bidirectional) attention. Transfusion combines both attention patterns by applying causal attention to every element in the sequence, and bidirectional attention within the elements of each individual image. This allows every image patch to attend to every other patch within the same image, but only attend to text or patches of other images that appeared previously in the sequence. We find that enabling intra-image attention significantly boosts model performance (see \u00a74.3). Figure 4 shows an example Transfusion attention mask."}, {"title": "Training Objective", "content": "To train our model, we apply the language modeling objective LLM to pre-dictions of text tokens and the diffusion objective LDDPM to predictions of image patches. LM loss is computed per token, while diffusion loss is computed per image, which may span multiple elements (image patches) in the sequence. Specifically, we add noise \u03f5 to each input latent image x0 according to the diffusion process to produce xt before patchification, and then compute the image-level diffusion loss. We combine the two losses by simply adding the losses computed over each modality with a balancing coefficient \u03bb:\nL_{Transfusion} = L_{LM} + \u03bb \u22c5 L_{DDPM}  (4)\nThis formulation is a specific instantiation of a broader idea: combining a discrete distribution loss with a continuous distribution loss to optimize the same model. We leave further exploration of this space, such as replacing diffusion with flow matching , to future work.\nInference Reflecting the training objective, our decoding algorithm also switches between two modes: LM and diffusion. In LM mode, we follow the standard practice of sampling token by token from the predicted distribution. When we sample a BOI token, the decoding algorithm switches to diffusion mode, where we follow the standard procedure of decoding from diffusion models. Specifically, we append a pure noise xT in the form of n image patches to the input sequence (depending on the desired image size), and denoise over T steps. At each step t, we take the noise prediction and use it to produce xt\u22121, which then overwrites xt in the sequence; i.e. the model always conditions on the last timestep of the noised image and cannot attend to previous timesteps. Once the diffusion process has ended, we append an EOI token to the predicted image, and switch back to LM mode. This algorithm enables the generation of any mixture of text and image modalities."}, {"title": "4 Experiments", "content": "We demonstrate in a series of controlled experiments that Transfusion is a viable, scalable method for training a unified multi-modal model."}, {"title": "4.1 Setup", "content": "Evaluation We evaluate model performance on a collection of standard uni-modal and cross-modal benchmarks (Table 1). For text-to-text, we measure perplexity on 20M held-out tokens from Wikipedia and the C4 corpus , as well as accuracy on the pretraining evaluation suite of Llama 2 . For text-to-image, we use the MS-COCO benchmark , where we generate images on randomly selected 30k prompts from validation set and measure their photo-realism using zero-shot Frechet Inception Distance (FID)"}, {"title": "4.2 Controlled Comparison with Chameleon", "content": "We run a series of controlled experiments to compare Transfusion with Chameleon at different model sizes (N) and token counts (D), using the combination of both as a proxy for FLOPs (6ND). For simplicity and parameter control, the Transfusion variant in these experiments uses simple linear image encoder/decoder with patch size 2\u00d72, as well as bidirectional attention. For each benchmark, we plot all results on a log-metric over log-FLOPs curve and regress linear trendlines. We also estimate relative compute efficiency by measuring the parity FLOP ratio: the ratio between the number of FLOPs required by Transfusion and Chameleon to reach the same level of performance.\nFigure 5 visualizes the scaling trends, and Table 3 shows the results of the largest models in this controlled setting and their estimated parity FLOP ratio. In every benchmark, Transfusion consistently exhibits better scaling laws than Chameleon. While the lines are close to parallel, there is a significant gap in Transfusion's favor. The difference in compute efficiency is particularly striking in image generation, where FID Transfusion achieves parity with Chameleon using 34\u00d7 less compute.\nSurprisingly, text-only benchmarks also reveal better performance with Transfusion, even though both Transfusion and Chameleon model text in the same way. We investigate this phenomenon by ablating the various changes leading up to Transfusion and Chameleon from the original Llama 2 recipe. Table 4 shows that while Transfusion does come at a non-zero cost to text performance, the Chameleon recipe suffers from both the stability modifications made to the architecture and from the introduction of image tokens. Training on quantized image tokens degrades text performance more than diffusion on all three benchmarks. One hypothesis is that this stems from the competition between text and image tokens in the output distribution; alternatively, it is possible that diffusion is more efficient at image generation and requires fewer parameters, allowing Transfusion models to use more capacity than Chameleon to model text. We leave further investigation of this phenomenon to future research."}, {"title": "4.3 Architecture Ablations", "content": "Now that we have established that Transfusion is a viable, scalable approach to multi-modal modeling in a controlled environment, we can explore improvements and extensions that are applicable to Transfusion alone."}, {"title": "4.3.1 Attention Masking", "content": "We first examine the necessity of intra-image bidirectional attention. Table 5 shows that enabling this attention pattern beyond the standard causal attention is advantageous throughout all benchmarks, and using both image encoding/decoding architectures. In particular, we notice a significant improvement in FID when using linear encoding layers (61.3\u219220.3). In the causal-only version of this architecture, there is no flow of information from patches that appear later in the sequence to those before; since U-Net blocks contain bidirectional attention within, independent of the transformer's attention mask, this gap is less pronounced when they are applied."}, {"title": "4.3.2 Patch Size", "content": "Transfusion models can be defined over different sizes of latent pixel patches. Larger patch sizes allow the model to pack more images in each training batch and dramatically reduce inference compute, but may come at a performance cost. Table 6 sheds light on these performance trade-offs. While performance does decrease consistently as each image is represented by fewer patches with linear encoding, models with U-Net encoding benefit from larger patches on tasks involving the image modality. We posit that this is due to the greater amount of total images (and diffusion noise) seen during training. We also observe that text performance deteriorates with larger patches, perhaps because transfusion needs to exert more resources (i.e. parameters) to learn how to process images with fewer patches and thus less inference compute."}, {"title": "4.3.3 Patch Encoding/Decoding Architecture", "content": "Our experiments so far indicate an advantage to using the U-Net up and down blocks instead of a simple linear layer. One possible reason is that the model benefits from the inductive biases of the U-Net architecure; an alternative hypothesis is that this advantage stems from the significant increase in overall model parameters introduced by the U-Net layers. To decouple these two confounders, we scale up the core transformer to 7B parameters, while keeping the amount of U-Net parameters"}, {"title": "4.3.4 Image Noising", "content": "Our experiments order 80% of image-caption pairs with the caption first, and the image conditioning on the caption, following the intuition that image generation may be a more data-hungry task than image understanding. The remaining 20% of the pairs condition the caption on the image. However, these images are noised as part of the diffusion objective. We thus measure the effect of limiting the diffusion noise to a maximum of t = 500 (half of the noise schedule) in the 20% of cases where images appear before their captions. Table 8 shows that noise limiting significantly improves image captioning, as measure by CIDEr, while having a relatively small effect (less than 1%) on other benchmarks."}, {"title": "4.4 Comparison with Image Generation Literature", "content": "Our experiments thus far have covered controlled comparisons with Chameleon and Llama, but we have yet to compare Transfusion's image generation capabilities to those of state-of-the-art image generation models. To that end, we train a 7B parameter model with U-Net encoding/decoding layers (2\u00d72 latent pixel patches) over the equivalent of 2T tokens, comprising of 1T text corpus tokens and 3.5B images and their captions. While the Transfusion variant in \u00a74.2 favored simplicity and experimental control, the design choices and data mixture (\u00a74.1) of this variant lean a bit more towards image generation. Figure 2 and Appendix B showcase generated images from this model.\nWe compare the performance of our model to reported results of other similar scale image generation models, as well as some publicly available text generating models for reference. Table 9 shows that Transfusion achieves similar performance to high-performing image generation models such as DeepFloyd , while surpassing previously published models including SDXL . While Transfusion does lag behind SD 3 , this model leveraged synthetic image captions through backtranslation , which enhances its GenEval performance by 6.5% absolute (0.433\u21920.498) at smaller scale; for simplicity, our experimental setup only included natural data. Finally, we note that our Transfusion model can also generate text, and performs on par with the Llama models, which were trained on the same text data distribution (\u00a74.1)."}, {"title": "4.5 Image Editing", "content": "Our Transfusion models, which have been pretrained on text-text, image-text, and text-image data, perform well across these modality pairings. Can these models extend their capabilities to generate images based on other images? To investigate, we fine-tuned our 7B model (\u00a74.4) using a dataset of only 8k publicly available image editing examples, where each example consists of an input image, an edit prompt, and an output image. This approach, inspired by LIMA , allows us to assess how well the model can generalize to image-to-image generation, a scenario not covered during pretraining.\nManual examination of random examples from the EmuEdit test set , shown in Figure 6 and Appendix 4.5, reveals that our fine-tuned Transfusion model performs image edits as instructed. Despite the limitations of this experiment, the findings suggest that Transfusion models can indeed adapt to and generalize across new modality combinations. We leave further exploration of this promising direction to future research."}, {"title": "5 Related Work", "content": "Most existing multi-modal models are built on the idea of attaching two or more modality-specific architectures together, often pretraining each component separately in advance. State-of-the-art image and video generation models, for instance, use large pretrained text encoders to represent their input prompts in latent space, which can then be used to condition diffusion models. In fact, recent work fuses representations from multiple off-the-shelf encoders to enhance performance . A similar pattern can be observed in the vision language model literature, where typically a pretrained language model is complemented by pretrained modality-specific encoders/decoders via projection layers to/from the pretrained text space. Examples include Flamingo and LLaVA for visual understanding, GILL  for visual generation, and DreamLLM for both visual comprehension and generation. In contrast, Transfusion has one unified architecture learned end-to-end to generate both text and images.\nPrior work on end-to-end multi-modal models includes examples such as Fuyu , which uses image patches as inputs for visual understanding, and Chameleon , which converts each image to a sequence of discretized tokens and then trains over the combined text-image token sequences. However, these approaches are either restricted to input-"}, {"title": "6 Conclusion", "content": "This work explores how to bridge the gap between the state of the art in discrete sequence modeling (next token prediction) and continuous media generation (diffusion). We propose a simple, yet previously unexplored solution: train a single joint model on two objectives, tying each modality to its preferred objective. Our experiments show that Transfusion scales efficiently, incurring little to no parameter sharing cost, while enabling the generation of any modality."}]}