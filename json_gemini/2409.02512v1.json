{"title": "Continual Diffuser (COD): Mastering Continual Offline Reinforcement Learning with Experience Rehearsal", "authors": ["Jifeng Hu", "Li Shen", "Sili Huang", "Zhejian Yang", "Hechang Chen", "Lichao Sun", "Yi Chang", "Dacheng Tao"], "abstract": "Artificial neural networks, especially recent diffusion-based models, have shown remarkable superiority in gaming, control, and QA systems, where the training tasks' datasets are usually static. However, in real-world applications, such as robotic control of reinforcement learning (RL), the tasks are changing, and new tasks arise in a sequential order. This situation poses the new challenge of plasticity-stability trade-off for training an agent who can adapt to task changes and retain acquired knowledge. In view of this, we propose a rehearsal-based continual diffusion model, called Continual Diffuser (CoD), to endow the diffuser with the capabilities of quick adaptation (plasticity) and lasting retention (stability). Specifically, we first construct an offline benchmark that contains 90 tasks from multiple domains. Then, we train the CoD on each task with sequential modeling and conditional generation for making decisions. Next, we preserve a small portion of previous datasets as the rehearsal buffer and replay it to retain the acquired knowledge. Extensive experiments on a series of tasks show CoD can achieve a promising plasticity-stability trade-off and outperform existing diffusion-based methods and other representative baselines on most tasks. Source code is available at here.", "sections": [{"title": "1 Introduction", "content": "Artificial neural networks, such as diffusion models, have made impressive successes in decision-making scenarios, e.g., game playing [45], robotics manipulation [28], and autonomous driving [4]. However, in most situations, a new challenge of difficult adaption to changing data arises when we adopt the general strategy of learning during the training phase and evaluating with fixed neural network weights [12]. Changes are prevalent in real-world applications when performing learning in games, logistics, and control systems. A crucial step towards achieving Artificial General Intelligence (AGI) is mastering the human-like ability to continuously learn and quickly adapt to new scenarios over the duration of their lifetime [8]. Unfortunately, it is usually ineffective for current methods to simply continue learning on new scenarios when new datasets arrive. They will show a dilemma between storing historical knowledge (stability) in their brains and adapting to environmental changes (plasticity) [78].\nRecently, we have noticed that diffusion probabilistic models (DPMs) have emerged as an expressive structure for tackling complex decision-making tasks such as robotics manipulation by formulating deep reinforcement learning (RL) as a sequential modeling problem [18, 67, 26]. Although recent DPMs have shown impressive performance in robotics manipulation, they, however, usually focus on a narrow setting, where the environment is well-defined and remains static all the time [2, 75], just like we introduce above. In contrast, in real-world applications, the environment changes dynamically in chronological order, forming a continuous stream of data encompassing various tasks. In this situation, it is challenging for the agents to contain historical knowledge (stability) in their brains and adapt to environmental changes (plasticity) quickly based on already acquired knowledge [5, 77]. Thus, a natural question arises:\nCan we incorporate DPMs' merit of high expression and concurrently endow DPMs the ability towards better plasticity and stability in continual offline RL?\nFacing the long-standing challenge of plasticity-stability dilemma in continual RL, current studies of continual learning can be roughly classified into three categories. Structure- based methods [70, 79, 59, 43, 41] propose the use of a base model for pertaining and sub- modules for each task so as to store separated knowledge and reduce catastrophic forgetting. Regularization-based methods [81, 80, 31, 29, 46] propose using auxiliary regularization loss such as L2 penalty, KL divergence, and weight importance to contain policy optimization and avoid catastrophic forgetting during training. Rehearsal-based methods [60, 50, 23, 54, 9] are considered simple yet effective in alleviating catastrophic forgetting as rehearsal mimics the memory consolidation mechanism of hippocampus replay inside biological systems. There are many strategies to perform rehearsal. For instance, a typical method is gradient projection [9], which contains the gradients from new data loss as close as to previous tasks, furthest preventing performance decrease.\nAlthough these methods are effective for continual learning, they present limited im- provement in continual offline RL because of extra challenges such as distribution shift and value uncertain estimation. Recently, diffusion-based methods, such as DD and Dif- fuser [2, 26, 67, 25], propose to resolve the above two extra challenges from sequential modeling and have shown impressive results in many offline RL tasks. However, they concentrate solely on training a diffuser that can only solve one task, thus showing limitations in real-world applications where training datasets or tasks usually arrive sequentially. Though recent works, such as MTDIFF [18], consider diffusers as planner or data generators for multi-task RL, the problem setting of their work is orthogonal to ours.\nIn this view, we take one step forward to investigate diffusers with arriving datasets and find that recent state-of-the-art diffusion-based models suffer from catastrophic forgetting when new tasks arrive sequentially (See Section 3.1 for more details.). To address this issue,"}, {"title": "2 Results", "content": "In this section, we will introduce environmental settings and evaluation metrics Section 2.1 and 2.2. Then, in Section 2.3 and 2.4, we first introduce a novel continual offline RL benchmark, including the task description and the corresponding dataset statistics, and introduce various baselines. Finally, in Section 2.5 and 2.6, we report the comparison results, ablation study, and parameters sensitivity analysis."}, {"title": "2.1 Environmental Settings", "content": "Following the same setting as prior works [82, 75], we conduct thorough experiments on Continual World and Gym-MuJoCo benchmarks. In Continual World, we adopt the task setting of CW10 and CW20 where CW20 means two concatenated CW10. All CW tasks are version v1. Besides, we also select Ant-dir for evaluation, which includes 40 tasks, and we arbitrarily select four tasks (tasks-10-15-19-25) for training and evaluation. See Appendix 5.5 for more details."}, {"title": "2.2 Evaluation Metrics", "content": "In order to compare the performance on a series of tasks, we follow previous studies [72, 5] and adopt the totally average success rate $P(p)$ (higher is better), forward transfer $FT$ (higher is better), forgetting $F$ (lower is better), and the total performance $P + FT - F$ (higher is"}, {"title": "2.3 Novel Benchmark for Continual Offline RL", "content": "To take advantage of the potential of diffusion models, we propose a benchmark for continual offline RL (CORL), comprising datasets from 90 tasks, including 88 Continual World tasks and 2 Gym-MuJoCo tasks [72, 64]. For the Gym-MuJoCo domain, there are 42 environmental variants, which are constructed by altering the agent goals. In order to collect the offline datasets, we trained Soft Actor-Critic (SAC) on each task for approximately 1M time steps [17]."}, {"title": "2.4 Baselines", "content": "We compare our method (CoD) with various representative baselines, encompassing structure-based, regularization-based, and rehearsal-based methods. In structure-based methods, we select LoRA [39], PackNet [41], and Multitask. For regularization-based methods, we select L2, EWC [31], MAS [3], and VCL [46] for evaluation. Rehearsal-based baselines include t-DGR [77], DGR [58], CRIL [15], A-GEM [9], and IL [19]. Besides, we also include several diffusion-based methods [25, 2] and Multitask methods, such as MTDIFF [18] for the evaluation."}, {"title": "2.5 Main Results", "content": "To show the effectiveness of our method in reducing catastrophic forgetting, we compare our method with other diffusion-based methods on the Ant-dir tasks ordered by 10-15-19-25. As shown in Table 1 (d) and Figure 2, the results illustrate: 1) Directly applying previous diffusion-based methods into continual offline RL will lead to severe catastrophic forgetting because the scores of Diffuser-w/o rehearsal and DD-w/o rehearsal are far behind CoD. 2) Extending the technique of LoRA into the diffusion model may not always work. The reason lies in that the parameter quantity size is small, which inspires us to construct diffuser foundation models in future work. 3) Rehearsal can bring significant improvements on diffuser as CoD approaches the score of Multitask CoD.\nConsidering that offline datasets prohibit further exploration in the environments, which may hinder the capability of some baselines that are designed for online training. We conduct CW10 and CW20 experiments of these methods under the online continual RL setting. Similarly, we constrain the interaction as 500k time steps for each task and report the comparison results in Figure 3 (a) and Table 1 (a). The results show that our method (CoD) surpasses other baselines by a large margin, which illustrates the superior performance over balancing plasticity and stability. Besides, it is indeed that some methods, such as EWC, are more suitable for online training by comparing the performances in Figure 3 (a) and (b). Additionally, we also report the comparison under mixed-quality datasets CL setting in Table 1 (c). Please refer to Appendix 5.4 for the comparison of model plasticity and generation acceleration details."}, {"title": "2.6 Ablation Study", "content": "To show the effectiveness of experience rehearsal, we conduct an ablation study of CoD in CW and Ant-dir tasks. We compare our method with and without experience rehearsal and find that experience rehearsal indeed brings significant performance gain. For example, CoD achieves 76.82% performance gain compared with CoD-w/o rehearsal. In CW 20 tasks, CoD reaches mean success rate from 20% to 98% when incorporating experience rehearsal. Refer to Table 5 for more results."}, {"title": "Sensitivity of Key Hyperparameters", "content": "In the experiments, we introduce the key hyper-parameters: the rehearsal frequency (v) and rehearsal sample diversity (\u03be). The larger v will aggravate the catastrophic forgetting because the model can access previous samples after a longer training process. A large value of \u00a7 will improve the performance and increase the storage burden, while a small value is more cost-efficient for longer CL tasks but is more challenging to hold the performance. We conduct the sensitivity of the hyperparameters"}, {"title": "3 Discussion", "content": "Previous diffusion-based methods [2, 25, 77], such as DD and Diffuser, are usually proposed to solve a single task, which is not in line with the real-world situation where the task will dynamically change. Thus, it is meaningful but challenging to train a diffuser that can adapt to new tasks (plasticity) while retaining historical knowledge. When we directly extend the original diffusion-based method in continual offline RL, we can imagine that severe catastrophic forgetting will arise in the performance because there are no mechanisms to retain preceding knowledge. As shown in Figure 2, in order to show the catastrophic forgetting, we compare our method and the representative diffusion-based methods on Ant-dir, where we arbitrarily select four tasks, task-10, task-15, task-19, and task-25, to form the CL setting. Diffuser-w/o rehearsal and DD-w/o rehearsal represent the original method Diffuser and DD, respectively. Multitask CoD and MTDIFF are the multitask baselines, which can access all training datasets in any time step, and CoD-RCR represents we use return condition for decision generation during the training stage. CoD-LORA denotes that we train CoD with the technique of low-rank adaptation. IL-rehearsal is the imitation learning with rehearsal. The results show that previous diffusion-based methods exhibit severe catastrophic forgetting when the datasets arrive sequentially, and at the same time, the good performance of CoD illustrates experience rehearsal is effective in reducing catastrophic forgetting."}, {"title": "3.1 Catastrophic Forgetting of Diffuser", "content": "Previous diffusion-based methods [2, 25, 77], such as DD and Diffuser, are usually proposed to solve a single task, which is not in line with the real-world situation where the task will dynamically change. Thus, it is meaningful but challenging to train a diffuser that can adapt to new tasks (plasticity) while retaining historical knowledge. When we directly extend the original diffusion-based method in continual offline RL, we can imagine that severe catastrophic forgetting will arise in the performance because there are no mechanisms to retain preceding knowledge. As shown in Figure 2, in order to show the catastrophic forgetting, we compare our method and the representative diffusion-based methods on Ant-dir, where we arbitrarily select four tasks, task-10, task-15, task-19, and task-25, to form the CL setting. Diffuser-w/o rehearsal and DD-w/o rehearsal represent the original method Diffuser and DD, respectively. Multitask CoD and MTDIFF are the multitask baselines, which can access all training datasets in any time step, and CoD-RCR represents we use return condition for decision generation during the training stage. CoD-LORA denotes that we train CoD with the technique of low-rank adaptation. IL-rehearsal is the imitation learning with rehearsal. The results show that previous diffusion-based methods exhibit severe catastrophic forgetting when the datasets arrive sequentially, and at the same time, the good performance of CoD illustrates experience rehearsal is effective in reducing catastrophic forgetting."}, {"title": "3.2 Reducing Catastrophic Forgetting with Experience Rehearsal", "content": "In Section 2.5, we illustrate the effectiveness of experience rehearsal through the experi- ments on our proposed offline CL benchmark, which contains 90 tasks for evaluation. From the perspective of the CL tasks quantity, we evaluate carious quantity settings, such as 4 tasks for Ant-dir, 4 tasks for CW4, 10 tasks for CW10, and 20 tasks for CW20. From the perspective of classification of traditional CL settings, our experimental settings contain CICORL, TICORL, and DICORL. In the Ant-dir environment, we select 10-15-19-25 task sequence as the CL setting and conduct the experiment compared with other diffusion-based methods. From the results shown in Figure 2, we can see distinct catastrophic forgetting on the recent diffusion-based method, though they show strong performance in other offline RL tasks [26, 18]. To borrow the merits of diffusion models' strong expression on offline RL and equip them with the ability to reduce catastrophic forgetting, we propose to use experience rehearsal to master the CORL. Detailed architecture is shown in Figure 1, and we postpone the method description in Section 4.3.\nApart from the Ant-dir environment, we also report the performance on more complex CL tasks, i.e., CW10 and CW20, in Table 1. Considering that most baselines are trained in online mode in their original papers, we first select the online baselines and compare their mean success rate with our method. The results (Table 1 and Figure 3) show that our method (CoD) surpasses other baselines by a large margin, which illustrates the superior performance over balancing plasticity and stability. Besides, we also compare our method with these baselines trained with offline datasets, where the results show that our method can quickly master these manipulation tasks and remember the acquired knowledge when new tasks arrive, while the baselines (except for Multitask) struggle between plasticity and stability because the performance of these baselines fluctuates among tasks. When the previous tasks appear once again after 5M training steps, the baselines show different levels of catastrophic forgetting because the performance decreases after 5M steps. However, our method still remembers how"}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Continual Offline RL", "content": "In this paper, we focus on the task-incremental setting of task-aware continual learning in the offline RL field where the different tasks come successively for training [82, 68, 59, 57, 1, 69]. Each task is defined as a corresponding Markov Decision Process (MDP) $M = (S, A, P, R, \\gamma)$, where S and A represent the state and action space, respectively, $P : S \\times A \\rightarrow \\triangle(S)$ denotes the Markovian transition probability, $R : S \\times A \\times S \\rightarrow R$ is the reward function, and $\\gamma \\in [0, 1)$ is the discount factor. In order to distinguish different tasks, we use subscript i for task i, such as $M_i, S_i, A_i, P_i, R_i$, and $\\gamma_i$. At each time step t in task i, the agent receives a state $s_{i,t}$ from the environment and produces an action $a_{i,t}$ with a stochastic or deterministic policy $\\pi$. Then a reward $r_{i,t} = r(s_{i,t}, a_{i,t})$ from the environment serves as the feedback to the executed action of the agent. Continual offline RL aims to find an optimal policy that can maximize the discounted return $\\Sigma_{\\pi} [\\Sigma_{t=0}^{\\infty} \\gamma^t r(s_{i,t}, a_{i,t})]$ [75, 63, 71] on all tasks with previously collected dataset ${D_i}_{i\\in\\Icdot}$"}, {"title": "4.2 Conditional Diffusion Probabilistic Models", "content": "In this paper, diffusion-based models are proposed to model the distribution of trajectory $ \\tau$, where each trajectory can be regarded as a data point. Then we can use diffusion models to learn the trajectory distribution $q(\\tau) = \\int q(\\tau_{0:K})d\\tau_{1:K}$ with a predefined forward diffusion process $q(\\tau_k|\\tau_{k-1}) = \\mathcal{N}(\\tau_k; \\sqrt{\\alpha_k}\\tau_{k-1}, (1 - \\alpha_k)I)$ and the trainable reverse process $p_{\\theta}(\\tau_{k-1}|\\tau_{k}) = \\mathcal{N}(\\tau_{k-1}; \\mu_{\\theta}(\\tau_k, k), \\Sigma_k)$, where $k \\in [1, K]$ is the diffusion step, $\\sqrt{\\alpha_k}$ and $\\sqrt{1 - \\alpha_k}$ control the drift and diffusion coefficients, $\\mu_{\\theta}(\\tau_k) =  \\sqrt{\\alpha_k} (\\tau_k + \\epsilon_{\\theta}(\\tau_k, k))$, $\\Sigma_k = \\beta_k I$, and $\\alpha_k + \\beta_k = 1$. $\\epsilon_{\\theta}(\\tau_k, k)$ represents the noising model [61]. According to [20], we can train $\\epsilon_{\\theta}(\\tau_k, k)$ with the below simplified objective\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{k \\sim U(1, 2, ..., K), \\epsilon \\sim \\mathcal{N}(0, I), \\tau^0 \\sim D}[|| \\epsilon - \\epsilon_{\\theta}(\\tau_k, k)||^2],$ where k is the diffusion time step, U is uniform distribution, $\\epsilon$ is multivariant Gaussian noise, $\\tau^0 = \\tau$ is sampled from the replay buffer D, and $\\theta$ is the parameters of model $\\epsilon_{\\theta}$.\nConditions play a vital role in conditional generation because this method makes the outputs of diffusion models controllable. We can also use two conditions methods, classifier- guided and classifier-free, to train diffusion models $p_{\\theta}(\\tau_{k-1}|\\tau_k, C)$ [40]. The classifier-guided"}, {"title": "4.3 Continual Diffuser", "content": "In this section, we introduce the Continual Diffuser (CoD), as shown in Figure 1, which con- tains classifier-free task-conditional training, experience rehearsal, and conditional generation for decision.\nIn RL, we leverage the characteristic of the diffusion model that can capture joint distributions in high-dimensional continual space by formulating the training data from single-step transition to multi-step sequences. Specifically, we have I tasks, and each task $M_i$ consists of N trajectories {1;}N , where the $\\tau_{i,n} = {s_{i,t,n}, a_{i,t,n}}$ will be split into equaling sequences with $T_e$ time steps as the discrepancy of trajectories may occur across tasks. In the following parts, we slightly abuse this notation $\\tau_i$ to represent the sequence data with length $T_e$ sampled from task i 's dataset $D_i$ and $\\hat{\\tau}_i$ to denote the generative sequence. In order to distinguish different tasks, we propose to use environment- related information as the task condition. For example, in the Ant-dir environment, the agent's goal is to maximize its speed in the pre-defined direction, which is given as the goal in the specific tasks. So, we propose to use this information as condition $C_{task}$ to train our model. In each diffusion step k, the task condition $C_{task}$ will pass through a task embedding function to obtain task embedding, which will be fed into the diffusion model jointly with diffusion time step embedding. Apart from the task conditions that are used implicitly in the training, we also need explicit observation conditions. We use the first state $s_{i,t,n}$ of the $T_e$ length sampled sequence $\\tau_{i,n} = {s_{i,t,n}, a_{i,t,n}, s_{i,t+1,n}, a_{i,t+1,n}, ..., s_{i,t+T_e-1,n}, a_{i,t+T_e-1,n}}$ as the conditions. Then at each diffusion generation step, after we obtain the generated sequences ${s_{i,t,n}, a_{i,t,n}, ..., s_{i,t+T_e-1,n}, a_{i,t+T_e-1,n}}_k$, the first observation $\\hat{s}_{i,t,n}$ is directly replaced by $s_{i,t,n}$, i.e., $\\hat{\\tau}_{i,n} = {s_{i,t,n}, a_{i,t,n}, ..., \\hat{s}_{i,t+T_e-1,n}, a_{i,t+T_e-1,n}}_k$.\nFollowing the previous studies of the diffusion model [20, 18], the training and generation for each task i are defined as\n$\\mathcal{L}_i(\\theta) = \\mathbb{E}_{k \\sim U(1, K), \\epsilon \\sim \\mathcal{N}(0, I), \\tau^0 \\sim D_i}[||\\epsilon - \\epsilon_{\\theta}(\\tau, C_{task}, k)||^2],$ (1)\n$\\tau_{k-1} = \\frac{\\sqrt{\\alpha_{k-1}}\\beta_k}{1 - \\alpha_k} \\tau + \\frac{\\sqrt{\\alpha_k(1 - \\alpha_{k-1})}}{1 - \\alpha_k} \\epsilon + \\Sigma_k z$, (2)\nwhere z ~ $\\mathcal{N}(0, I)$, $\\tau_1 = \\frac{\\beta_1}{\\sqrt{\\alpha_1}}\\tau - \\sqrt{1-\\alpha_1}\\epsilon$, $\\Sigma_k = \\frac{1 - \\alpha_{k-1}}{1-\\alpha_k} \\beta_k$, and $\\hat{\\epsilon} = \\epsilon_{\\theta}(\\tau, \\varnothing, k) + \\omega(\\epsilon_{\\theta}(\\tau, C_{task}, k) -  \\epsilon_{\\theta}(\\tau, \\varnothing, k))$"}, {"title": "Architecture", "content": "In this paper, we adopt temporal Unet with one-dimensional convolution blocks as the diffusion model to predict noises. Specifically, temporal Unet contains several down-sampling blocks, a middle block, several up-sampling blocks, a time embedding block, and a task embedding block. We train the time embedding block and task embedding block to generate time and task embeddings that are added to the observation-action sequence\n$\\tau_{i,t:t+T_e-1,n} =  \\begin{pmatrix}  s_{i,t,n} & s_{i,t+1,n} &  & s_{i,t+T_e-1,n} \\\\  a_{i,t,n} & a_{i,t+1,n} & ... & a_{i,t+T_e-1,n}  \\end{pmatrix}$"}, {"title": "4.4 Conclusion", "content": "First of all, to facilitate the development of the continual offline RL community, a continual offline benchmark that contains 90 tasks is constructed based on Continual World and Gym- MuJoCo. Based on our benchmark, we propose Continual Diffuser (CoD), an effective continual offline RL method that possesses the capabilities of plasticity and stability with experience rehearsal. Finally, extensive experiments illustrate the superior plasticity-stability trade-off when compared with representative continual RL baselines."}, {"title": "5 Supplementary Material", "content": ""}, {"title": "5.1 Pseudocode of Continual Diffuser", "content": "The pseudocode for CoD training is shown in Algorithm 1. First of all, we process the datasets of I tasks before training, including splitting the trajectories into equal sequences and normalizing the sequences to facilitate learning. As shown in lines 9 \u2013 24, for each task i, we check the task index in the whole task sequence and sample different samples from the different buffers. For example, for task i, i > 0, we will perform experience rehearsal every v train steps by sampling data from Dj, j \u2208 0, ..., \u0456 \u2014 1, where j is sampled from U(0, i \u2212 1). Then, the networks e\u0473, ftask($), and ftime(6) are updated according to Equation (1) and Equation (3). After training on task i, we preserve a small portion (\u00a7) of the dataset of buffer D; as task i's rehearsal buffer. During the evaluation of multiple tasks (shown in Algorithm 2), we successively generate decisions with CoD and calculate the evaluation metrics."}, {"title": "5.2 Implement Details", "content": "Experiments are carried out on NVIDIA GeForce RTX 3090 GPUs and NVIDIA A10 GPUs. Besides, the CPU type is Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz. Each run of the experiments spanned about 24-72 hours, depending on the algorithm and the length of task sequences.\nIn the implementation, we select the maximum diffusion steps as 200, and the default structure is Unet. Then, in order to speed up the generation efficiency during evaluation, we consider the speed-up technique of DDIM [62] and realize it in our method, thus accomplishing 19.043x acceleration compared to the original generation method. The sequence length is set to 48 in all experiments, where a larger sequence length can capture a more sophisticated distribution of trajectories and may also increase the computation burden. We set the LORA dimension as 64 for each module of down-sampling, middle, and up-sampling blocks, and the percent of LoRA parameters is approximately 12% in our experiments."}, {"title": "5.3 Related Work", "content": "Diffusion models have made big progress in many fields, such as image synthesis and text generation [20, 56, 49, 7, 61, 55]. Recently, a series of works have demonstrated the tremendous potential of diffusion-based models in offline RL tasks such as goal-based planning, composable constraint combination, scalable trajectory generation, and complex skill synthesis [25, 13, 2, 11]. For example, Janner et al. [25] propose to use the value function as the guide during trajectory generation, effectively reducing the effects of out-of-distribution actions and reaching remarkable performance in offline RL tasks. Besides, diffusion models can also be used as policies to model the multimodal distribution from states to actions and as planners to perform long-horizon planning [26, 67, 18, 48]. For instance, Kang et al. [26] use diffusion models as policies to model the distribution from states to actions, while He et al. [18] endow diffusion models with the ability to perform planning and data augmentation with different task-specific prompts.\nContinual learning (CL) aims to solve multi-tasks that come sequentially with explicit boundaries (task-aware CL) or implicit boundaries (task-free CL) and"}]}