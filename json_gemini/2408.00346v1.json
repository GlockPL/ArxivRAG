{"title": "Neural Graph Matching for Video Retrieval in Large-Scale Video-driven E-commerce", "authors": ["Houye Ji", "Ye Tang", "Zhaoxin Chen", "Lixi Deng", "Jun Hu", "Lei Su"], "abstract": "With the rapid development of the short video industry, traditional e-commerce has encountered a new paradigm, video-driven e-commerce, which leverages attractive videos for product showcases and provides both video and item services for users. Benefitting from the dynamic and visualized introduction of items, video-driven e-commerce has shown huge potential in stimulating consumer confidence and promoting sales. In this paper, we focus on the video retrieval task, facing the following challenges: (1) How to handle the heterogeneities among users, items, and videos? (2) How to mine the complementarity between items and videos for better user understanding? In this paper, we first leverage the dual graph to model the co-existing of user-video and user-item interactions in video-driven e-commerce and innovatively reduce user preference understanding to a graph matching problem. To solve it, we further propose a novel bi-level Graph Matching Network (GMN), which mainly consists of node- and preference-level graph matching. Given a user, node-level graph matching aims to match videos and items, while preference-level graph matching aims to match multiple user preferences extracted from both videos and items. Then the proposed GMN can generate and improve user embedding by aggregating matched nodes or preferences from the dual graph in a bi-level manner. Comprehensive experiments show the superiority of the proposed GMN with significant improvements over state-of-the-art approaches (e.g., AUC+1.9% and CTR+7.15%). We have developed it on a well-known video-driven e-commerce platform, serving hundreds of millions of users every day.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of the information explosion, the recommender system has become the most effective way to help users discover what they are interested from enormous data. As two basic Internet applications, e-commerce and content recommendation both provide the recommender service but focus on either item recommendation or content (e.g., video and live) recommendation. Recently, with the thriving of online applications, there is a surge of video-driven e-commerce [8], which integrates video and e-commerce for better services. Benefiting from vivid and attractive video shows, video-driven e-commerce provides a new business paradigm which improves user stickiness and activeness, and has become a new driving force for e-commerce development. For example, TikTok and Kuaishou integrate e-commerce with videos, while Taobao and JD.COM leverage videos to improve the user experience in traditional e-commerce. As shown in Figure 1(a), JD.COM first incentivizes authors to produce videos for related items, and then conducts video recommendations for users. Due to the large-scale video set, industrial recommendation systems usually adopt the classical two-stage architecture: retrieval and ranking [18]. Given a user, the retrieval stage aims to find thousands of candidate videos from the whole video set, then the ranking stage aims to precisely select hundreds of videos from the retrieved candidate videos.\nIn this paper, we focus on the video retrieval stage with the following challenges: (1) How to handle the heterogeneities among users, items, and videos? The characteristics of items (e.g., price, sales, and shop) are markedly different from videos (e.g., resolution, duration, and author) and them aim to stimulate user consumption and attract users stay, respectively. This disparity compels us to treat user-item interactions and user-video interactions with distinct considerations, given the unique characteristics each type of interaction manifests. (2) How to mine the complementarity between items and videos for better user understanding? While at a cursory glance these interactions may seem discrete, they exhibit a strong complementary nature upon closer examination. As exemplified in"}, {"title": "2 RELATED WORK", "content": "Recommendation System (RS), which mainly focuses on user preference understanding and recommends diverse types of candidates (e.g., items and videos) to them, has accompanied diverse paradigms, such as item recommendation [5, 6, 16], video recommendation [8, 13, 15], and business-driven recommendation [9, 10]. Real-world RS usually consists of two stages: retrieval [2, 14, 18] and ranking [1, 8]. YoutubeDNN [2] retrieves videos for online video services, while MIND [14] retrieves items for e-commerce. Unlike them, we study a newly emerging video-driven e-commerce, which models the co-existing of items and videos.\nCross-domain Recommendation (CDR) attempts to learn useful knowledge from the source domain to help the target domain. Hu et al. [7] propose CoNet to perform dual transfer at the unit level. [4, 8, 18, 21] unify both source and target domains as graph-structured data and perform CDR via graph learning. Significantly different from the above works, we we first model video-driven e-commerce as a dual graph and reduce it to a graph matching problem.\nGraph Neural Networks (GNNs) generalize deep learning to graph-structured data, where graph convolution [12] aims to aggregate neighbors information and update node embedding, while graph pooling [11, 19] aims to aggregate all node to update graph embedding. Recently, many works [1, 8, 18] apply GNNs to recommendation scenarios. SURGE [1] reduces user behaviors into graphs and then extracts users' core interests via cluster-aware graph pooling. Furthermore, several works [8, 18, 20] generalize GNN to multiple interactions (or domains). MGFN [20] proposes a multi-graph learning for cross domain video recommendation. Different from the above works, we first propose a novel bi-level graph matching network to align user behaviors and preferences on both videos and items for retrieval."}, {"title": "3 PRELIMINARIES", "content": "Definition 3.1. Dual Graph. The dual graph, termed as G = (GU-V, GU-I), describes the co-existing of user behaviors on both videos and items in video-driven e-commerce. Here N = NU \\cup NV \\cup NI is the union set of User, Video, and Item, respectively. Also, $\\mathcal{E} = \\mathcal{E}_{U-V} \\cup \\mathcal{E}_{U-I}$ is the edge set, where $\\mathcal{E}_{U-V} = (N_U, N_V)$ denotes User-Video interaction and $\\mathcal{E}_{U-I} = (N_U, N_I)$ denotes User-Item interaction.\nExample. Figure 1(b) gives an illustrative example of the dual graph in video-driven e-commerce. Given a user u and the dual graph G, we can obtain his neighbor sets, including $N_{U-V} = \\{v|(u,v) \\in \\mathcal{E}_{U-V}\\}$ from GU-V and $N_{U-I} = \\{i|<u, i> \\in \\mathcal{E}_{U-I}\\}$ from GU-I. In the dual graph, user u\u2081's neighbor sets are $N_{u-1} = \\{i_1, i_2, i_3, i_4\\}$ and $N_{u-V} = \\{v_1, v_2\\}$, respectively. Furthermore, we also extract user-centric sub-graphs for user u from the dual graph, termed as Gu = (GU-V, GU-I). In detail, GU-V contains multi-hops neighbors of user u and himself (i.e., $V_{U-V} = N_{U-V} \\cup u$), as well as induced edges $\\mathcal{E}_{U-V}^{U-V}$. Note that, dual graphs can be widely used in many other related fields, such as the co-existing of user-music and user-movie, or even the co-existing of user-item(old/warm domain) and user-item(new/cold domain).\nFinally, we formulate the video retrieval problem in video-driven e-commerce as follows:\n$\\arg \\max_{v \\in N_V} Pr(v|u; G),$ \t(1)\nwhich aims to retrieve candidate videos v from the whole video set NV for user u based on the dual graph G."}, {"title": "4 THE PROPOSED MODEL", "content": "In this section, we present a novel Graph Matching Network, which aims the to match the users' behaviors as well as preferences via both node- and preference-level graph matching. The overall framework of GMN is shown in Figure 2."}, {"title": "4.1 Embedding Initialization", "content": "In this section, we first initialize the node embeddings of users, videos, and items via feature embeddings, and then update them via GNNs. Taking embedding initialization of video u as an example, we have:\nx0 = ||fx \t(2)\nwhere || denotes the vector concatenation, $x_v^f$, and x denote the initial embedding of video v and the f-th feature embedding of video v, respectively. After that, we further utilize two interaction-specific GNNs (i.e., GNNU-V and GNNU-I) to update corresponding embeddings. Given video v and GU-V, GNNU-V is able to update the video $x^{U-V}_v$ embedding via aggregating its neighbors NU-V, as follows:\nxU-V = GNNU-V (xu|u\u2208 NU-V). \t(3)\nNote that, we focus on bi-level graph matching and simply instantiate GNNU-V as the MeanPooling [9, 10]. Similarly, the embedding of item i via GNNU-I is $x^{U-I}_i$. For user u, we can also get his embeddings on the dual graph $x^{U-V}_u$ and $x^{U-I}_u$."}, {"title": "4.2 Node-level Graph Matching", "content": "Until now, a method way for video retrieval is to fuse user preferences on videos and items i.e., $x^{U-V}_u||x^{U-I}_u$, and then retrieve top videos based on the similarity of their embeddings, suffering from the following weaknesses: (1) In video-driven e-commerce, video production actually revolves around items and aims to promote the item purchase. Although videos and items are different types of nodes, there exists potential relevance between them. (2) It only models each user as a single node embedding while fails to explicitly capture the graph structure surrounding users, which also provides valuable information for understanding user preferences. So, it is necessary to encode the user-centric sub-graph as a whole for video retrieval.\nIn this section, we propose node-level graph matching to mine the relevance between videos and items in the dual graph, and then enhance their connections via aggregating cross-graph neighbors based on the matching scores.\nTo handle large-scale video-driven e-commerce, we first extract user-centric sub-graphs (i.e., Gu-V and Gu\u2212I) for user u, and then actually perform the node-level graph matching GMNL on them, shown as follows:\nRu = GMNL(GU-V,GU-I). \t(4)\nHere $R_u \\in \\mathbb{R}^{|V_u^{U-V}|\\times|V_u^{U-I}|}$ denotes the personalized relevance matrix of user u in node-level. Given a user u, the personalized relevance score $r_{v,i}^u$, between video $v \\in G_u^{U-V}$ and item $i \\in G_u^{U-I}$ is shown as follows:\nr_{v,i}^u = (x^{U-V}_v)^T \\cdot M \\cdot x^{U-I}_i. \t(5)\nHere (.) denotes the transposition operator. M is a learnable metric matrix, harmonizing the embeddings of heterogeneous nodes (video v.s. item). Obviously, the personalized relevance score $r_{v,i}^u$ is dynamically changed with regard to users, videos, and items.\nAfter obtaining the personalized relevance scores $\\{r_{v,i}^u\\}$, we normalize them via the softmax function to get the personalized matching scores $s_{vi}^u$ and $s_{vi}^u$, shown as follows:\ns_{vi}^u = \\text{softmax}_i (r_{vi}^u) = \\frac{\\text{exp}(r_{vi}^u)}{\\sum_{i' \\in V^{U-I}_u} \\text{exp}(r_{vi'}^u)} \t(6)\ns_{vi}^u = \\text{softmax}_v (r_{vi}^u) = \\frac{\\text{exp}(r_{vi}^u)}{\\sum_{v' \\in V^{U-V}_u} \\text{exp}(r_{v'i}^u)} \t(7)\nNote that the above matching scores are asymmetric since they are normalized on different node sets (i.e., VU-I and VU-V). Given a user u, the personalized matching score $s_{vi}^u$ measures the matching level from item i to video v. Intuitively, aggregating relevant items embeddings into video embeddings is able to the alleviate the heterogeneity and sparsity problem, improving user preference understanding and the effectiveness of video retrieval.\nTaking personalized matching scores (e.g., $\\{s_{vi}^u\\}$) as coefficients, we can properly summarize all information $\\sum_{i \\in V_u^{U-I}} s_{vi}^u \\cdot x^{U-I}_i$ which needs to propagate from relevant items $i \\in G_u^{U-I}$ to video v, and then utilize it to enhance the video embedding. The final video"}, {"title": "4.3 Preference-level Graph Matching", "content": "In this section, we introduce a novel method for preference-level graph matching. This method mines the relevance of user preferences for videos and items. Subsequently, this relevance is utilized in cross-graph propagation to refine preference embeddings.\n4.3.1 Extracting User Preferences via Dual Graph Pooling. Specifically, we propose a novel dual graph pooling method (i.e., GPNU-V and GPNU-I) to cluster user-centric sub-graphs (i.e., GU-V and GU-I) as preference graphs, where each node indicates a cluster of similar videos or items and corresponding user preferences.\nGiven a user u and Gu-V, a graph pooling network GPNU-V is able to cluster all video embeddings $\\{h_v^{U-V}|v \\in V_u^{U-V}\\}$ into k\u2081 preferences, and constructs the condensed preference graph ZU-V:\nZU-V = GPNU-V (GU-V), \t(9)\n$\\text{\\{z}_{uk_1}^{U-V},...,z_{uk_1}^{U-V}\\} = GPN^{U-V}(\\text{\\{h}_{v}^{U-V}\\}),$ \t(10)\nwhere $z_{uk_1}^{U-V}$ is the embedding of the k\u2081-th user preference. If we set k\u2081 > 1, then GPNU-V actually extracts multiple types of user preferences. Therefore, we simply instantiate GPNU-V as the MeanPooling [9] Similarly, we use GPNU-I to extract k2 preference embeddings $\\{z_{uk_2}^{U-I},...,z_{uk_2}^{U-I}\\} $ from GU-I.\n4.3.2 Improving Preference Embedding via Preference-level Graph Matching. Intuitively, user preferences on both videos and items also have potential relevance. So we further propose the preference-level graph matching GMPL, which learns the preference-level relevance matrix $P_u \\in \\mathbb{R}^{k_1\\times k_2}$ between preference graphs (i.e., ZU-V and ZU-I), shown as follows:\nPu = GMPL(ZU-V,ZU-I), \t(11)\nwhere each element $p_{k_1,k_2}^u$ indicates the relevance between the k\u2081-th video preference and the k2-th item preference of user u, as follows:\np_{k_1,k_2}^u = (z_{uk_1}^{U-V})^Tz_{uk_2}^{U-I} \t(12)\nThen, we also normalize $p_{k_1,k_2}^u$ on ZU-V and Zu-i via softmax, and get corresponding matching scores $s_{k_1 \\leftarrow k_2}^u$ and $s_{k_1 \\rightarrow k_2}^u$ Then, the embeddings of preference nodes can be enhanced via bi-directional propagation, as follows:\ne_{uk_1}^{U-V} = z_{uk_1}^{U-V} + \\sum_{uk_2 \\in Z^{U-I}} s_{k_1 \\leftarrow k_2}^u z_{uk_2}^{U-I}, \t(13)\ne_{uk_2}^{U-I} = z_{uk_2}^{U-I} + \\sum_{uk_1 \\in Z^{U-V}} s_{k_1 \\rightarrow k_2}^u z_{uk_1}^{U-V} \t(14)\nInteractively performing graph pooling and graph matching on preference graphs for L times, we can progressively compress the original graph into a single node (i.e., k\u2081 = k2 = 1), termed as $e_u^{U-I}$ and $e_u^{U-I}$, representing the overall user preferences."}, {"title": "4.4 Prediction and Optimization", "content": "To comprehensively understand user preferences, we fuse user preferences on both videos and items, and get the final user embedding zu of user u, as follows:\nZu = MLP(eU-V ||eU-1\\xU-V\\xU-1). \t(15)\nSince the feature of video si much simpler and more stable than users, we only perform GMN for user modeling, and the final embedding zo of video v is actually $x_v^{U-V}$. Similar to previous works[1, 8, 17], we do not use a more sophisticated fusion network (e.g., attention-based fusion), so the effectiveness of bi-level graph matching can be clearly verified.\nThe estimated preference $\u0177_{u,v}$ of user u towards the target video v is the inner product of their embeddings:\n\u0177u,v = (zu)T. Zv. \t(16)\nThe overall loss function is as follows:\n$\\mathcal{L} = \\sum_{(u,v,v_{neg}) \\in D} - \\log \\sigma (\\hat{y}_{u,v} - \\hat{y}_{u,v_{neg}}) + \\lambda \\cdot ||\\Theta||_2,$ \t(17)\nwhere $\\Theta$ denotes all trainable model parameters, and \u03bb controls the L2 regularization strength to prevent overfitting. The whole model can be optimized via back-propagation."}, {"title": "4.5 Model Analysis", "content": "Here we give the analysis of the proposed GMN as follows:\nConsidering the characteristics of video-driven e-commerce, we first represent it as the dual graph, and then reduce user preference understanding to a bi-level graph matching problem. So, the proposed GMN focused on matching relevant nodes (e.g., videos and items) and preferences properly in a bi-level manner, rather than designing better GNN architectures to learn nodes and preferences embeddings. Similar to previous works [3, 9, 10], we adopt basic GNNs as backbones, so the effectiveness of bi-level graph matching could be clearly verified.\nWe analyze the time complexity of GMN in both the node- and preference-level graph matching. In the node-level graph matching, if we directly match all nodes between GU-V and GU-I, the computational cost is O(|NV|\u00b7 |NI|), which is unacceptable for real-world scenarios. Fortunately, we actually match user-centric"}, {"title": "5 OFFLINE EVALUATIONS", "content": "5.1 Datasets.\nAlthough video-driven e-commerce has grown rapidly on many platforms, related data is protected by privacy and security policies. Therefore, we sample user logs from a real-world video-driven e-commerce platform, and then extract two large-scale dual graph datasets from different time periods (7-days, and 30-days) for verification. We also use a public dataset MovieLens-1M released by GIFT[8] to mimic video-driven e-commerce, which takes user-movie (warm) as user-item interaction and user-movie (cold) as user-video interaction, respectively. The details of the datasets are shown in Table 1."}, {"title": "5.2 Baselines.", "content": "To validate the effectiveness of GMN, we first select single interaction based methods (i.e., YoutubeDNN[2], MIND[14], and SURGE[1]) as well as multiple interactions based methods (i.e., GIFT[8], MGFN[20], and CCDR[18]). Since MIND and SURGE cannot be directly applied to the dual graph, we also provide corresponding dual versions (i.e., MIND+ and SURGE+) for better performance. The baselines are shown below:\nYoutubeDNN[2]: It is a classical model for large-scale video retrieval, utilizing two-tower architecture to learn the embeddings of users and videos based on their features.\nMIND/MIND+[14]: MIND is a classical retrieval model, which utilizes a dynamic routing mechanism to extract users'"}, {"title": "5.3 Implementation.", "content": "For all models, we random initialize parameters with Xavier initializer and select Adam as the optimizer. For a fair comparison, we use the following hyper-parameters for all models: the dimension of node embedding is 128, the learning rate is 0.0015, the L2 regularizer is 0.01, and the dropout rate is 0.75. For MIND and SURGE, we use the multiple preferences version and set the maximum number of preferences to 5. For YoutubeDNN, we use 3 layer DNN (1024-512-256) for prediction. For offline evaluation, we select AUC, Precision, Recall, and Loss as evaluation metrics. We implement all models with TensorFlow 1.15 and run them on Nvidia A100 Cluster. The code of GMN and the anoymous version of the industrial dataset will be released after acceptance."}, {"title": "5.4 Performance Comparison", "content": "The comparison results are shown in Table 2, where we have the following observations:\nThe proposed GMN consistently performs better than all baselines with significant gaps. Compared to the best performance of baselines, the improvements of GMN on AUC metric is up to 1.2%-1.9% on large-scale video-driven e-commerce, which indicates the effectiveness of both node- and preference-level modeling in the proposed GMN for video retrieval.\nMultiple interactions based methods (i.e., MIND+ and CCDR) outperform single graph methods (i.e., MIND and SURGE). It makes sense because both user-video and user-item interactions are able to provide valuable information for understanding user preferences. By capturing the correlation between two types of interaction, GIFT, MGFN, CCDR, and GMN always show their superiority over concatenate based models (i.e., MIND+ and SURGE+).\nGNN-based models, including SURGE, GIFT, MGFN, CCDR, and GMN, perform better than traditional DNN-based models (i.e., MIND and YoutubeDNN), indicating the effectiveness of graph structure modeling in the recommendation."}, {"title": "5.5 Ablation Study", "content": "In this section, we conduct both model-level and graph-level ablation studies for further verification, shown in Table 3.\nModel-level. We first conduct a model-level ablation study to show how the delicate designs in GMN affect performance. As shown in Table 3, we test two variants of GMN (GMN\\NL and GMN\\PL), which remove node- and preference-level graph matching, respectively. Obviously, GMN performs better than both"}, {"title": "5.6 Case Study", "content": "We further present a case study to show the potential interpretability of GMN. Taking a young boy as an example who is interested in the Apple iPhone and the Casio watch, he has interacted with related items and videos. Figure 3 shows the heatmap on both node- and preference-level relevance matrix, where darker color means higher relevance. Since item i9 and video vs are both related to iPhone, the node-level relevance score $r_{u,j}^u$ is high. A similar phenomenon can be observed at the preference-level (e.g., $p_{1.1}^u$)."}, {"title": "5.7 Parameter Study", "content": "In this section, we investigate the sensitivity of parameters and report the performance of GMN on the offline 7-days dataset with various parameters in Figure 4.\nDimension of Metric Matrix. As shown in Figure 4(a), we test the effect of the dimension of the metric matrix M. Obviously, with the growth of the dimension, the performance of GMN continues to rise. The reason is that, a larger dimension enables the proposed GMN to properly measure the similarity between videos and items.\nNumber of User Preferences. Naturally, most users have diverse preferences, and one preference fails to comprehensively"}, {"title": "6 ONLINE EVALUATIONS", "content": "6.1 Online A/B Testing\nWe conduct A/B testing during 0907-0925 (19 days) for online evaluations, shown in Figure 6. Although MIND+ is not the strongest benchmark in offline evaluation, it is the previously deployed model, affecting hundreds of millions of users every day. Therefore, we use MIND+ as the baseline in online A/B testing. Here, we select the widely used CTR\u00b9 to evaluate online performance. The larger CTR means users are willing to click the exposed videos recommended by our system and indicates better performance.\nAs shown in Figure 6, the proposed GMN consistently shows significant superiority over the previously deployed MIND+ on all days. More specifically, the average CTR of the proposed GMN is 3.74% while MIND+ achieves only 3.49%, resulting in a CTR improvement of up to 7.15%. Following [8], we also use TP992 response time to evaluate the efficiency of our system, shown in Figure 7. Full-day observation shows that the response time of the proposed GMN"}, {"title": "6.2 System Architecture and Online Serving.", "content": "Here we present how to deploy the proposed GMN system for real-world video-driven e-commerce, shown in Figure 5. For offline training, we collect real-world user clicks as positive samples and random sample several videos as negative samples. Then, we sample user-centric sub-graphs and associate them with training samples, as training data. The trained model will be divided into two parts and developed in different platforms: (1) The user part, which is developed to the model center, will generate user embedding based on real-time user behavior. (2) The video part, which is developed to the retrieval center, will retrieve candidate videos based on user embedding."}, {"title": "7 CONCLUSION", "content": "The thriving of e-commerce is accompanied by a newly emerging paradigm, video-driven e-commerce, which utilizes attractive videos to improve user experience and stimulate consumption. In this paper, we propose the dual graph to represent video-driven e-commerce and innovatively reduce the user preference understanding problem to a graph matching problem. To solve it, we further propose a novel Graph Matching Network (GMN), which mainly consists of node- and preference-level graph matching. By properly matching the heterogeneous interactions and preferences,\nthe proposed GMN achieves significant improvements over the state-of-the-art models in both offline and online evaluation (e.g., AUC+1.9% and CTR+7.15%). Currently, the proposed GMN has been developed in a well-known video-driven e-commerce platform, affecting hundreds of millions of users every day."}]}