{"title": "Parameter Tracking in Federated Learning with Adaptive Optimization", "authors": ["Evan Chen", "Jianing Zhang", "Shiqiang Wang", "Chaoyue Liu", "Christopher Brinton"], "abstract": "In Federated Learning (FL), model training performance is strongly impacted by data heterogeneity across clients. Gradient Tracking (GT) has recently emerged as a solution which mitigates this issue by introducing correction terms to local model updates. To date, GT has only been considered under Stochastic Gradient Descent (SGD)-based model training, while modern FL frameworks increasingly employ adaptive optimizers for improved convergence. In this work, we generalize the GT framework to a more flexible Parameter Tracking (PT) paradigm and propose two novel adaptive optimization algorithms, FAdamET and FAdamGT, that integrate PT into Adam-based FL. We provide a rigorous convergence analysis of these algorithms under non-convex settings. Our experimental results demonstrate that both proposed algorithms consistently outperform existing methods when evaluating total communication cost and total computation cost across varying levels of data heterogeneity, showing the effectiveness of correcting first-order information in federated adaptive optimization.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has emerged as a promising paradigm for training Machine Learning (ML) models across distributed devices [10, 17]. Migitating data heterogeneity remains one of the most critical challenges in FL. This non-i.i.d. characteristic of client data often results in slower convergence and suboptimal model performance. While traditional Stochastic Gradient Descent (SGD)-based optimizers are widely employed in FL frameworks, they exhibit limited effectiveness in addressing the adverse effects of heterogeneous data [24]. To overcome these limitations, the concept of Gradient Tracking (GT) has been introduced [5]. GT incorporates correction terms during communication, enabling clients to locally estimate and compensate for the gradient contributions from other clients in the network. This approach helps mitigate the negative impacts of data heterogeneity, thereby improving the convergence behavior and stability of the global model.\nBeyond the widely used SGD optimizer, moment-based optimizers have emerged as prominent local update schemes for ML training [6, 9, 12]. Numerous algorithms have been proposed, and these adaptive optimizers have demonstrated superior empirical performance compared to SGD. Efforts have been made to integrate adaptive optimizers into FL frameworks; however, these approaches often suffer from performance degradation in the presence of data heterogeneity, especially when communication cost is high and cannot be performed frequently. Existing research leveraging GT to address data heterogeneity in FL primarily focuses on SGD optimizers. Consequently, since most adaptive optimization algorithms involves multiplying and dividing gradient-based estimates, how to effectively track first-order gradient information remains a challenging task.\nMotivated by this, we investigate the following questions:\n1. Will adaptive optimization algorithms designed using GT obtain performance advantages across FL systems as their SGD counterparts?\n2. What is the best way to incorporate the concept of GT into adaptive federated optimization?\nIn answering these questions, we extend the concept of GT to a more generalized framework termed Parameter Tracking (PT). Building upon this foundation, we propose two novel algorithms leveraging the Adaptive Moment Estimation (Adam) optimizer. Through rigorous convergence analysis and comprehensive experimental evaluations, we demonstrate that both algorithms effectively stabilize the global learning process. As a result, increasing levels of non-i.i.d. data distributions across clients do not readily lead to a decline in performance, addressing a critical limitation in existing federated optimization techniques.\nOur main contributions are as follows:\n\u2022 We introduce a generalized concept of GT termed Parameter Tracking (PT), where local clients track the discrepancy between their locally collected first-order information and the server's aggregated information (Sec. 4.1). This extension proves beneficial when integrating the Adam optimizer into FL.\n\u2022 Base on the concept of PT, we propose two novel Adam-based algorithms, FAdamET and FAdamGT, by leveraging different interpretations of PT. Both approaches effectively track global information by using control variables that does not require any additional fine-tuning, mitigating model biases caused by various levels of non-i.i.d. data efficiently (Sec. 4.2 and 4.3).\n\u2022 We provide a rigorous theoretical analysis of the convergence rates for both algorithms. This offers insights into their stability and efficiency under heterogeneous data conditions, and better understanding of how PT works differently when applied to different steps of the local update process (Sec. 5.2).\n\u2022 We perform extensive experiments across diverse datasets and multiple FL settings, including image classification tasks using convolution neural networks (CNN) and sequence classification tasks using large language models (LLMs), demonstrating the superior performance of our proposed methods compared to existing baselines (Sec. 6)."}, {"title": "2 Related Works", "content": "Gradient Tracking: Gradient Tracking (GT) methods [3, 5, 13, 21, 26, 27, 29] have emerged as a powerful solution to address data heterogeneity challenges in decentralized optimization algorithms. The core principle of GT lies in tracking gradient information from neighboring nodes during each communication round, ensuring more accurate gradient estimates across the network. Centralized FL algorithms such as SCAFFOLD [11] and Proxskip [20] are both designed base on this concept, and multiple works on serverless FL settings demonstrated superior improvement [1, 2, 8, 18], where communication efficiency is a primary concern. By effectively reducing the synchronization frequency while still guaranteeing convergence to the optimal point, GT has proven to be highly effective in mitigating the adverse effects of heterogeneous data distributions. Furthermore, existing studies have shown that under proper initialization of gradient tracking variables, many standard assumptions on data heterogeneity can be relaxed.\nRecent advancements have also extended GT methods to address hierarchical network structures. SDGT was introduced as the first GT algorithm tailored for semi-decentralized networks [4], bridging the gap"}, {"title": "Adaptive Optimizer", "content": "Adaptive Optimizer: SGD optimizers rely on fixed or decaying learning rates, which often require careful tuning and may struggle with scenarios involving sparse gradients or noisy updates. To address these limitations, adaptive optimizers dynamically adjust learning rates based on the gradient history of individual parameters, enabling more effective navigation of complex optimization landscapes. Among the most prominent adaptive optimizers are AdaGrad [6], and Adam [12]. AdaGrad introduces per-parameter learning rate scaling to handle sparse features effectively. Building on this foundation, Adam combines the benefits of momentum with adaptive learning rates, achieving robust convergence across various learning tasks. Recent advancements have further explored the decoupling of weight decay [19] and the time-varying effects of regularization terms [31], pushing the boundaries of adaptive optimization.\nSeveral approaches have been proposed to integrate adaptive optimizers into FL. [23] introduced FedAdam, where the central server employs an adaptive optimizer to update the global model using aggregated client gradients. Additionally, [30] incorporates adaptive optimization directly on local clients. More recently, [25] presented FedLADA, an FL algorithm in which clients utilize the Adam optimizer for local updates. In FedLADA, the update gradient is computed as a weighted average of local gradients and global gradients. However, besides FedLADA, all of these algorithms aren't designed to deal with data heterogeneity, and hence requires frequent global aggregation for good results. FedLADA although maintained a global gradient estimation, requires a weighted sum operation where an additional hyperparameter has to be fine-tuned based on different data. This causes the performance to vary dramatically base on the chosen weights, which is not required in our algorithms."}, {"title": "3 System Model and Motivation", "content": null}, {"title": "3.1 System Model", "content": "The problem we aim to solve follows the form:\n$\\min_{x \\in \\mathbb{R}^d} f(x) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)$,\nwhere $f_i(x) = \\mathbb{E}_{\\xi_i \\sim \\mathcal{D}_i} f_i(x; \\xi_i)$ is the expectation of the stochastic local function, and $n$ is the total number of clients (typically edge devices) in the system, indexed $i = 1,...,n$. $f_i(x)$ is the local ML loss function computed at client $i$ for model parameters $x \\in \\mathbb{R}^d$, $\\mathcal{D}_i$ is the local data distribution at client $i$, and $\\xi_i$ is an unbiased random sample from $\\mathcal{D}_i$. The server is connected to each device over a star topology, hence allowing the server to have direct communication between any device in the network.\nThe training process operates on two distinct timescales: an outer timescale and an inner timescale. The outer timescale, denoted as $t = 1, 2, . . ., T$, represents global aggregation rounds where the central server updates the global model. The inner timescale, denoted as $k = 1, . . ., K$, represents local training steps performed by each client between global aggregations. We assume a fixed number of $K$ local updates occur between two consecutive global aggregation rounds.\nFor each global iteration $t$, the training procedure can be described in three iterative steps: (i) Client Selection and Initialization: At each global round $t$, the server selects a subset of clients $\\mathcal{S}_t \\subseteq \\{1, ..., n\\}$, where $|\\mathcal{S}_t| = S \\leq n$. The global model is broadcasted to the selected clients to initialize local training. (ii) Local Model Updates: Each selected client performs $K$ local updates using a local optimizer, independently updating their local models based on their respective datasets. (iii) Global Model Aggregation: After completing $K$ local updates, the selected clients send their updated model parameters to the server. The server then aggregates these updates to refine the global model."}, {"title": "3.2 Limitation of Existing Works", "content": "Federated adaptive algorithms such as LocalAdam exhibit instability under data heterogeneity, similar to FedAvg. Specifically, the local update does not remain stable at the optimal solution. Let $m_i = m^* = \\nabla f(x^*) = 0$, $v_i = v^* = \\nabla f(x^*) \\odot \\nabla f(x^*)$, and $x_i = x^*$. A single local update on client $i$ at the optimal point can be expressed as:\n$m_i^+ = \\beta_1 m^* + (1 - \\beta_1) \\nabla f_i(x^*)$,\n$v_i^+ = \\beta_2 v^* + (1 - \\beta_2) \\nabla f_i(x^*) \\odot \\nabla f_i(x^*)$,\n$x_i^+ = x_i - \\eta \\frac{m_i^+}{\\sqrt{v_i^+} + \\epsilon} \\neq x^* $.\nThis discrepancy between $x_i^+$ and $x^*$ implies that LocalAdam only converges to a region around the stationary point, where the radius depends on the degree of data heterogeneity.\nTo aim to address this issue, FedLADA introduces a weighted-average update, where a global averaged term $g_a$ is calculated by the server and broadcasted to all sampled clients. The ideal value of $g_a$ is $g_a = \\nabla f(x^*)$. However, even with the additional weighted-average operation, the ideal update of FedLADA still remains unstable:\n$m_i^+ = \\beta_1 m^* + (1 - \\beta_1) \\nabla f_i(x^*)$,\n$v_i^+ = \\beta_2 v^* + (1 - \\beta_2) \\nabla f_i(x^*) \\odot \\nabla f_i(x^*)$,\n$x_i^+ = x_i - \\eta (\\frac{m_i^+}{\\sqrt{v_i^+} + \\epsilon} + (1-\\alpha)g_a)$.\nThe iterates are still unstable, since for any $\\alpha \\in (0, 1]$, $x_i^+ \\neq x^*$. The equality only holds when $\\alpha = 0$, indicating that no local gradients are utilized during local updates, which is a trivial and impractical solution for mitigating data heterogeneity effectively."}, {"title": "4 Proposed Algorithm", "content": "The usage of PT correction is shown in Figure 1, where we show two possible locations when correction terms can participate into the local update: before the moment estimation and after the moment estimation."}, {"title": "4.1 Model Correction with First-order Information", "content": "GT has significantly improved the performance of SGD methods. We generalize GT to Parameter Tracking, where the core principle remains to locally track first-order update information, but it is not strictly limited to gradient information. This generalization aims to make FL with all kinds of local optimizer more resilient to data heterogeneity.\nAdam optimizer employs multiple variables to track first-order information. With parameter tracking, these variables can be adapted at various stages of the algorithm, enabling the derivation of novel FL algorithms. In this paper, we propose two such algorithms, building upon LocalAdam [25] as a baseline, which replaces the local SGD optimizer in FedAvg with an Adam optimizer.\nCase 1: Estimate Correction with Parameter Tracking. For a given local iteration, the local Adam optimizer performs updates as follows:\n$m_i^+ = \\beta_1 m_i + (1 - \\beta_1)g_i$,\n$v_i^+ = \\beta_2 v_i + (1 - \\beta_2)g_i \\odot g_i$,\n$x_i^+ = x_i - \\eta \\frac{m_i}{\\sqrt{v_i} + \\epsilon}$.\nwhere $m_i$ and $v_i$ represent first and second moment estimates, and $\\epsilon$ ensures numerical stability. In an ideal communication scenario where server synchronization occurs at every local step, the server update can be expressed as:\n$x^+ = x - \\eta \\sum_{i=1}^{n} \\frac{m_i}{\\sqrt{v_i} + \\epsilon}$.\nThe objective of parameter tracking is to correct the discrepancy between the server and local updates via an ideal correction term:\n$z_{ideal} = \\sum_{i=1}^{n} \\frac{m_i}{\\sqrt{v_i} + \\epsilon} - \\frac{m_i}{\\sqrt{v_i} + \\epsilon}$.\nAs shown in Figure 1, the corrected local update now becomes:\n$x_i^+ = x_i - \\eta (\\frac{m_i}{\\sqrt{v_i} + \\epsilon} + z_{ideal}) = x^+$.\nThis adjustment ensures that local updates better approximate globally synchronized updates.\nCase 2: Gradient Correction with Parameter Tracking.\nInstead of emulating LocalAdam with per-iteration communication, we now start from the setting where the server applies Adam updates onto the global model using aggregated gradients from local"}, {"title": "4.2 Estimate Tracking", "content": "Based on Case 1 of Sec. 4.1, we propose Federated Adaptive Moment Estimation with Estimate Tracking (FAdamET), where we incorporate the concept of PT into LocalAdam. As shown in Algorithm 1, during each global iteration $t$, the server samples a set of clients $\\mathcal{S}_t$ with size $S$ for training, and a smaller subset of clients $\\mathcal{Y}_t \\subseteq \\mathcal{S}_t$ with size $Y$ that will perform update on the tracking terms. Further experimental results shows that choosing $Y < S$ can still obtain comparable results while saving total communication.\nDuring the start of each local training interval, the server broadcasts the global model $x^{(t)}$ and the global correction term $y^{(t)}$ to all sampled clients $\\mathcal{S}_t$. Then, for each sampled client $i$ at local iteration $k$, stochastic gradient $g_i^{(t,k)} = f_i(x_i^{(t,k)}, g_i^{(t,k)})$ is computed locally using the local model $x_i^{(t,k)}$. The adaptive local update direction $\\Delta_i^{(t,k)}$ then calculated using $g_i^{(t,k)}$. In this work, we use the Adam optimizer as shown in line 13-16 of Algorithm 1, but it is possible for a more general framework where other adaptive optimizers are considered. Then, the local model performs one update using the PT correction terms and the local update direction:\n$x_i^{(t,k+1)} = x_i^{(t,k)} - \\eta (\\Delta_i^{(t,k)} + y^{(t)} - y_i^{(t)})$.\nAfter $K$ local updates, all clients in $\\mathcal{S}_t$ aggregated its local model to the server to update the global model $x^{(t)}$, and all clients in $\\mathcal{Y}_t$ updates locally its PT correction terms and aggregate them to the server to update the server's correction term $y^{(t)}.$"}, {"title": "4.3 Gradient Tracking", "content": "Based on Case 2 of Sec. 4.1, we propose Federated Adaptive Moment Estimation with Gradient Tracking (FAdamGT), where the PT correction is injected before moment estimation. As shown in Algorithm 1, during each global iteration $t$, the server samples a set of clients $\\mathcal{S}_t$ with size $S$ for training, and a smaller subset of clients $\\mathcal{Y}_t \\subseteq \\mathcal{S}_t$ with size $Y$ that updates on the tracking terms."}, {"title": "5 Convergence Analysis", "content": null}, {"title": "5.1 Analysis Assumptions", "content": "We first establish a few general and commonly employed assumptions that we will consider throughout our analysis.\nAssumption 1 (General Characteristics of Loss Functions). Assumptions applied to loss functions include: 1) The stochastic gradient norm of the loss function $l(\\cdot)$ is bounded by a constant $G$, i.e.,"}, {"title": "5.2 Non-Convex Convergence Behavior", "content": "We now present our main theoretical result, the cumulative average of the global loss gradient can attain sublinear convergence to a stationary point under non-convex problems. The detailed proofs, including of the intermediate lemmas, can be found in Appendix A and B.\nTheorem 1. Under Assumptions 1, 2, and the global and local step size satisfies the following conditions:\n$\\eta_g \\eta_l = \\min\\left(\\frac{(1 - \\beta_1) \\beta_1}{8KL(G + \\epsilon)}, \\frac{1}{8KL}, \\frac{1}{12TL}, \\frac{1}{12TL \\sqrt{T}}\\right)$\nConsider the following conditions for local step size $\\eta_l$:\n$\\eta_l < \\frac{1}{12T^{3/2} L}$,\n$\\eta_l < \\min\\left(\\frac{\\sqrt{G + \\epsilon} + (1 - \\beta_1) \\beta_1 \\sqrt{G + \\epsilon}}{12 \\sqrt{2}(1 - \\beta_1) \\beta_1 KL}, \\frac{1}{12T^{3/2} L}\\right)$\nWhen satisfying Conditions (C.1) and (C.3), for any given global iteration $T > 1$, the iterates of FAdamET can be bounded as:\n$\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}||\\nabla f(x^{(t)})||^2 = O\\left(\\frac{\\mathbb{E}f(x^{(1)}) - f^*}{KL \\sqrt{ST}} + \\frac{Y}{nT} + \\frac{K \\sqrt{Y}}{T^3} + \\frac{K^2}{T^3}\\right)$\nWhen satisfying Conditions (C.1) and (C.2), for any given global iteration $T > 1$, the iterates of FAdamGT can be bounded as:\n$\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}||\\nabla f(x^{(t)})||^2 = O\\left(\\frac{\\mathbb{E}f(x^{(1)}) - f^*}{K \\sqrt{ST}} + \\frac{K}{T} + \\frac{K^2}{T^3}\\right)$\nThe theorems demonstrate that for both algorithms, the global model $x^{(t)}$ converges sublinearly to a stationary point as $T \\rightarrow \\infty$. Both algorithms exhibit a primary convergence term of $O(\\frac{1}{K \\sqrt{ST}})$, which is influenced by the model's initialization. However, while FAdamGT has only two terms that grow with $K$, FAdamET includes three such terms.\nNotably, FAdamGT achieves a superior convergence rate compared to FAdamET. This is attributed to fewer constraints on the upper bound of the local step size $\\eta_l$ and the absence of the $O(\\frac{YK^2}{nT})$ term present in FAdamET, resulting in a tighter convergence guarantee. Detailed proofs in Appendix A and B show that the correction terms in FAdamET introduce additional growing terms, leading to the $O(\\frac{YK^2}{nT})$ component. In contrast, the negative effects of correction terms in FAdamGT cancel out during global aggregation. The $O(\\frac{YK^2}{nT})$ term also implies that the performance of estimate tracking in FAdamET can vary depending on the subset size $Y$ of sampled clients used to update the tracking variables."}, {"title": "6 Experiments", "content": null}, {"title": "6.1 Experimental Setup", "content": "In the baseline comparisons, we consider three widely used datasets: CIFAR-10, CIFAR-100 [14] and TinyImageNet [16]. For all three datasets, we adopt the ResNet-18 model. We set the total number of clients as $n = 100$, the client sampling rate to 10%, and set the number of local iterations $K = 3$. To generate non-i.i.d. data distribution, each dataset is distributed among all clients through a Dirichlet distribution, and the Dirichlet parameter is set to $\\alpha = 0.1$.\nWe compared our algorithm with several FL methods: 1) FedAvg, where local updates are performed using SGD optimizer, 2) SCAFFOLD, where local updates are performed using SGD optimizer with gradient correction, 3) LocalAdam, where the local updates are performed using Adam optimizer and 4) FedLADA, where the local updates are performed using a weighted average of Adam optimizer and gradient estimation. For SGD-based baselines, we set $\\eta_l = 0.1$ and $\\eta_g = 1$. For Adam-based baselines and our methods, we set $\\eta_l = 0.001$ and $\\eta_g = 1$. We set $(\\beta_1, \\beta_2) = (0.9, 0.99)$ and $\\epsilon = 10^{-8}$ for all Adam optimizers, and set weight decay to $10^{-8}$. All mean and standard deviation is based on four random trials.\nFurthermore, we conducted experiments on Large Language Models (LLMs). We tested on a Parameter-Efficient Fine-Tuning (PEFT) algorithm named FedIT [32], where only a limited amount of the LLM's parameters are trained using Low Rank Adaptation (LoRA) modules. The baseline we consider is FedIT, where clients use Adam to perform local updates, we name it FedIT-Adam. We then compare this baseline with our method, where we add estimate tracking and gradient tracking to the Adam optimizer, named FedIT-AdamET and FedIT-AdamGT. We use the GPT-2 model [22], and set the total number of clients as $n = 100$ and the client sampling rate to 10%. We set $(\\beta_1, \\beta_2) = (0.9, 0.99)$ and $\\epsilon = 10^{-8}$ for all Adam optimizers, and set weight decay to $10^{-8}$. We tested on two datasets, 20NewsGroups and the GLUE benchmark [15, 28]. All datasets are distributed among all clients through Dirichlet distribution with parameter set to $\\alpha = 0.1$. The mean and standard deviation is based on four random trials. All learning rates and the target accuracy for each dataset are listed in Appendix C."}, {"title": "6.2 Experiments on CIFAR and TinyImageNet", "content": "Table 1 presents a comparison of the total cost to attain certain accuracy between our proposed methods and existing algorithms. We evaluate two types of cost: 1) The total global iterations, where the system priorities computation efficiency, 2) the total communication operations each client performs, where the system priorities communication efficiency. The sample set size for tracking term aggregation is half the size of the sample set used for model aggregation, denoted as $Y = \\frac{S}{2}$. Our method, FAdamGT, consistently outperforms all baseline algorithms both when we evaluate performance using total global iterations and using total communication per client, demonstrating superior convergence performance and robustness to data heterogeneity through the integration of adaptive optimization and parameter tracking. For FAdamET, while it continues to outperform existing methods when evaluating total global iterations, the performance gap between FAdamET and other baselines diminishes when evaluating total communication per client. Figure 2 demonstrates that both FAdamET and FAdamGT outperforms the baselines in different number of local iterations, showing the consistent performance of our algorithm across multiple communication settings.\nFigure 3 illustrates the performance improvement of our algorithm compared to LocalAdam under varying levels of non-iid data. We vary the Dirichlet parameter $\\alpha$ from 0.1 to 1 to represent different levels of non-i.i.d. When evaluating the total global iterations, the performance gap between LocalAdam and our proposed methods is more pronounced under high data heterogeneity. In contrast, for the more i.i.d. setting, the performance gap between FAdamET and LocalAdam becomes negligible, and the gap between LocalAdam and FAdamGT also narrows. When evaluating the total communication per"}, {"title": "6.3 Experiments on PEFT tasks using LLMs", "content": "In these PEFT tasks, the model weights transmitted between the server and each client account for only 1.9% of the total weights stored locally on the clients. This indicates that the primary bottleneck lies in the computational cost rather than the communication cost. Consequently, we focus on evaluating the performance of each method based on the total number of required global iterations.\nTable 2 presents the performance of parameter tracking when integrated into the FedIT framework. In these experiments, the size of the sample set used for model aggregation is equal to the sample set for tracking term aggregation, i.e., $Y = S$. The local epochs between two consecutive global aggregations is set to one, and the target accuracy for each experiment is detailed in Appendix C.\nThe results demonstrate that while the improvement introduced by ET is less pronounced compared to its impact in CIFAR-100 and TinyImageNet experiments, GT consistently yields significant enhancements over the vanilla version of FedIT. This observation highlights the robustness and adaptability of parameter tracking mechanisms. The substantial improvements achieved by GT emphasize its ability to capture and leverage first-order information effectively during adaptive optimization. This underscores the generalizability of parameter tracking techniques, where a broad family of optimizers can benefit from enhanced convergence rates, showing its potential for advancing large-scale federated learning applications."}, {"title": "7 Conclusion", "content": "In this paper, we introduce Parameter Tracking, a general framework for developing FL algorithms that are robust to data heterogeneity. By incorporating parameter tracking into local adaptive optimizers, we propose two novel algorithms: FAdamET and FAdamGT. Through rigorous theoretical analysis, we demonstrate that both algorithms achieve sublinear convergence to a stationary point and reveal the impact of parameter tracking when applied at different stages of the local update process. Comprehensive numerical evaluations confirm that both methods outperform all baselines, delivering superior training"}, {"title": "A Theoretical Analysis for FAdamET (Theorem 1)", "content": "We first define the following auxilary definitions that will be helpful throughout the proof.\nWe define $c^k$ as the sum of all moving average coefficients to compute the first order moment $m_i^{(t,k)}$:\n$c^{(k,k')} = (1 - \\beta_1) \\beta_1^{k-k'}$\n$c^k = \\sum_{k'=1}^{k} c^{(k,k')} <1$\nWe first define the unbiased version of $m_i^{(t,k)}$ taking expectation on all stochastic gradients $g_i^{(t,k)}$. We define $m_i^{(t,k)}$ as the following:\n$m_i^{(t,k)} = \\sum_{k'=1}^{k} c^{(k,k')} \\nabla f_i(x^{(t,k')})$\nWe define an auxilary variable $\\alpha_i^{t,k}$:\n$\\alpha_i^{t,k} = \\begin{cases} \\frac{m_i^{(t-1,k)}}{\\sqrt{v_i^{t-1,k}} + \\epsilon}, & i \\in \\mathcal{Y}^{t-1} \\\\ \\frac{m_i^{(t-1,k)}}{v_i^{t-1,k}}, & i \\notin \\mathcal{Y}^{t-1} \\end{cases}$\nWe define the tracking variable drift term as:\n$\\Gamma^{(t)} = \\frac{1}{nK} \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{E} ||\\nabla_i^{(t,k)} - \\nabla f_i(x^{(t)})||^2$\nWe define the local update deviation term as:\n$\\Xi^{(t)} = \\frac{1}{nK} \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{E} ||m_i^{(t,k)} - c^k \\nabla f_i(x^{(t)})||^2$\nProof. Given global iteration $t$, the update of the model at the server can be written as:\n$x^{(t+1)} = x^{(t)} + \\eta_g \\frac{1}{S} \\sum_{i \\in S^{(t)}} (x_i^{(t,K+1)} - x_i^{(t)})$\n$= x^{(t)} - \\eta_g \\eta_l \\frac{1}{nS^{(t)}} \\sum_{i \\in S^{(t)}} \\sum_{k=1}^{K} \\frac{m_i^{(t,k)}}{\\sqrt{v_i^{(t,k)}} + \\epsilon}$\nBy injecting Assumption 1, we can get the following inequality:\n$\\mathbb{E}f(x^{(t+1)}) < \\mathbb{E}f(x^{(t)}) - \\eta_g \\eta_l \\mathbb{E} \\langle \\nabla f(x^{(t)}), \\frac{1}{S^{(t)}} \\sum_{i \\in S^{(t)}} \\sum_{k=1}^{K} \\frac{m_i^{(t,k)}}{\\sqrt{v_i^{(t,k)}} + \\epsilon} \\rangle$\n$\\leq - \\frac{\\eta_g \\eta_l L}{2} \\mathbb{E} ||\\frac{1}{S^{(t)}} \\sum_{i \\in S^{(t)}} \\sum_{k=1}^{K} \\frac{m_i^{(t,k)}}{\\sqrt{v_i^{(t,k)}} + \\epsilon} ||^2$"}, {"title": "B Theoretical Analysis for FAdamGT (Theorem 1)", "content": "We first define the expected first order moment $\\mu_i^{(t,k)}$ as the following:\n$\\mu_i^{(t,k)} = \\sum_{k'=1}^{k} c^{(k,k')} (\\nabla f_i(x_i^{(t,k')}) - \\alpha_i^{(t,k')} )+ \\alpha_i^{(t,k')}$,\nWhere $\\alpha_i^{(t,k)}$ is an auxilary variable that tracks the GT terms:\n$\\alpha_i^{(t,k)} = \\begin{cases} \\alpha_i^{(t-k,k)}, & i \\in \\mathcal{Y}^{t-1} \\\\ \\nabla f_i^{(t-k,k)}, & i \\notin \\mathcal{Y}^{t-1} \\end{cases}$\nWe further define the local deviation term $\\Xi^{(t)}$ as:\n$\\Xi^{(t)} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{K} \\sum_{k=1}^{K} || \\sum_{k'=1}^{k} c^{(k,k')}f_i(x^{(t,k')}) - c^k \\nabla f_i(x^{(t)})||^2$\nProof. Given global iteration $t$, the update of the model at the server can be written as:\n$x^{(t+1)} = x^{(t)} + \\eta_g \\frac{1}{S} \\sum_{i \\in S^{(t)}} (x_i^{(t,K+1)} - x_i^{(t)})$\n$= x^{(t)} - \\eta_g \\eta_l \\frac{1}{nS} \\sum_{i \\in S^{(t)}} \\sum_{k=1}^{K} \\frac{m_i^{(t,k)}}{\\sqrt{v_i^{(t,k)}} + \\epsilon}$\nBy injecting Assumption 1, we can get the following inequality:\n$\\mathbb{E}f(x^{(t+1)}) < \\mathbb{E}f(x^{(t)}) - \\eta_g \\eta_l \\mathbb{E} \\langle \\nabla f(x^{(t)}), \\frac{1}{S} \\sum_{i \\in S^{(t)}} \\sum_{k=1}^{K} \\frac{m_i^{(t,k)}}{\\sqrt{v_i^{(t,k)}} + \\epsilon} \\rangle$\n$\\leq - \\frac{\\eta_g \\eta_l L}{2} \\mathbb{E} || \\frac{1}{n} \\sum_{i \\in S^{(t)}} \\sum_{k=1}^{K} \\frac{m_i^{(t,k)}}{\\sqrt{v_i^{(t,k)}} + \\epsilon} ||^2$"}, {"title": "C Learning Rate and Target Accuracy for PEFT tasks", "content": null}]}