{"title": "Multimodal Gender Fairness in Depression Prediction: Insights on Data from the USA & China", "authors": ["Joseph Cameron", "Jiaee Cheong", "Micol Spitale", "Hatice Gunes"], "abstract": "Social agents and robots are increasingly being used in wellbeing settings. However, a key challenge is that these agents and robots typically rely on machine learning (ML) algorithms to detect and analyse an individual's mental wellbeing. The problem of bias and fairness in ML algorithms is becoming an increasingly greater source of concern. In concurrence, existing literature has also indicated that mental health conditions can manifest differently across genders and cultures. We hypothesise that the representation of features (acoustic, textual, and visual) and their inter-modal relations would vary among subjects from different cultures and genders, thus impacting the performance and fairness of various ML models. We present the very first evaluation of multimodal gender fairness in depression manifes- tation by undertaking a study on two different datasets from the USA and China. We undertake thorough statistical and ML experimentation and repeat the experiments for several different algorithms to ensure that the results are not algorithm-dependent. Our findings indicate that though there are differences between both datasets, it is not conclusive whether this is due to the difference in depression manifestation as hypothesised or other external factors such as differences in data collection methodol- ogy. Our findings further motivate a call for a more consistent and culturally aware data collection process in order to address the problem of ML bias in depression detection and to promote the development of fairer agents and robots for wellbeing.", "sections": [{"title": "I. INTRODUCTION", "content": "Mental health disorders (MHDs) are becoming increasingly prevalent [1]. In concurrence, there is a growing body of research which started to explore the use of agents or robotic coaches to support mental wellbeing [2]\u2013[5]. However, a unique challenge is that mental health conditions manifest in diverse ways across individuals. The different manifes- tations and symptoms can be influenced by gender [6]-[8] and countries [9]. Brody et al. show that women aged 20 or over are nearly twice as likely to show signs of depression when compared to men aged 20 or over [10] and Hasin et al. found a similar pattern in their survey of American adults (aged over 18) that targeted an investigation of major depressive disorders [11]. Chang et al. observed that Koreans were far more likely to show symptoms of low energy or difficulties with concentration, whereas Americans were less likely to show concentration difficulty and far more likely to show high levels of work productivity [12]. Following such insights, Juhasz et al. identify that depression can manifest and be displayed in very different ways across different nations and cultures [9]. As a result, many of the western- developed depression diagnostic tools may be biased towards and better at identifying depression within Northern American and European demographic groups [9].\nConcurrently, ML bias is becoming a growing source of concern [13]\u2013[17]. Given the high stakes involved in MHD analysis, it is crucial to investigate and mitigate the ML biases present. Recent research has indicated that ML bias is present in existing MH datasets and models [18]\u2013[20]. However, none of the existing work has investigated the problem of ML bias across different countries and cultures. The gender and cultural differences in depression manifestation, diagnosis, and general symptoms described in [9], [10], [12] clearly communicate that symptoms of depression can significantly differ between different genders and nations or cultures, potentially leading to biased frameworks and datasets for depression diagnosis and manifestation. Thus, we hope to tackle the aforementioned gaps by addressing the following research questions (RQs):\nRQ 1: Are there any differences in depression manifestation across gender and countries? If present, what are the primary sources of difference in depression manifestation? To address this research question, we conduct thorough statistical analysis of features between two datasets collected from different coun- tries, namely China and the USA, and evaluate to what extent the difference in features impacts model performance. RQ 2: How do the results compare between different ML models and modalities? To address this research question, we conduct thorough experimentation using a range of ML models to evaluate how the different models perform across the different datasets and modalities. RQ 3: Are there any differences in ML model performance and fairness across gender and countries? If present, are the differences in performance due to the difference in depression manifestation as hypothesised? To address this research question, we draw on the findings from the previous two RQs and further evaluate some key insights that may have impacted performance and fairness results.\nThe main contributions of this paper are as follows. First, we aim to understand how biases occur with respect to gender and whether they differ based on the location and dataset curation method. We do so by conducting experiments on two mul- timodal depression datasets from the USA and China. Next,"}, {"title": "II. LITERATURE REVIEW", "content": "A. Machine Learning in Depression Prediction\nThere have been recent attempts at using machine learning methods, including both unimodal and multimodal approaches to automatically analyse and predict depression using extracted features [21]\u2013[23]. It is possible to do so via several different data sources such as physiological data [24], [25], motor ac- tivity data [26], [27] or audio-visual sources [28]\u2013[30]. Audio- visual (AV) datasets typically include behavioural signals such as facial affect, body gestures and vocal intensity [28], [31], [32]. In general, multimodal approaches are shown to perform better than unimodal approaches [33]\u2013[35]. Most recently, Zou et al. explored the effects of using unimodal and multimodal variations of the visual, acoustic, and textual features extracted from the CMDC dataset [36]. The visual features Zou et al. identified for extraction from the CMDC dataset were Eye Gazes, Eye Landmarks, Head Poses, Facial Landmarks and Facial Action Units (AU) [37].\nB. Difference in Manifestation of Depression\nExisting literature indicates that depression can manifest in vastly different ways across different cultures, countries and genders [6], [7], [12]. Brody et al. and Hasin et al. demonstrate that adult women are more likely to display signs of depression [10], [11]. Platt et al. investigate this observation further and attribute the increased manifestation of depression in women to the gender pay gap [38]. Across cultural differences in depression symptoms, Chang et al. observed a difference in communicated energy levels and concentration between depressed Koreans and Americans [12]. Further, Iwata and Buka found that Japanese university students reported higher levels of low positive affect compared to American university students [39]. As a result, Japanese students were considered to be more depressed than their American counterparts although this may not necessarily be true [39].\nC. ML Fairness in Mental Wellbeing\nThere is only a handful of studies which have looked into bias in mental well-being prediction [18]\u2013[20], [40]\u2013[43]. Park et al. [42] conducted their experiments on data collected in a clinical setting with a specific focus on post-partum depression. Zanna et al. [19] conducted their experiments on data collected in the wild with a specific focus on anxiety prediction. Park et al. [41] analysed bias across gender in mobile mental health assessment and proposed an algorith- mic impact remover to mitigate unwanted bias. Bailey and Plumbley [18] attempted to mitigate the gender bias present in the DAIC-WOZ dataset using data re-distribution. Cheong et al. evaluated the impact of gender bias on depression prediction with various ML models using various unimodal and multimodal approaches [43].\nHowever, all the existing works have chiefly focused on investigating ML fairness for multiple ML models across gender using a dataset originating from one country. There are no known comparative evaluations of ML fairness for mental health using unimodal and multimodal approaches across different countries or cultures. More specifically, none of the existing works have investigated the problem of ML bias across different countries and/or cultures. The novel contribution of this paper is to present a thorough comparative evaluation of the performance and fairness of various ML models using unimodal and multimodal approaches when trained on the American E-DAIC dataset [44] and the Chinese CMDC dataset [36], thus allowing for the comparison of ML fairness across gender and countries and across multiple modalities."}, {"title": "III. EXPERIMENTAL SETUP", "content": "In this section, we provide the dataset information, and implemented data pre-processing and model training details.\nA. Datasets & Mental Health Measurements\nWe use the Chinese Multimodal Depression Corpus (CMDC) dataset [36] and the American Extended Distress Analysis Interview Corpus (E-DAIC) dataset [44] to address our research questions. The CMDC [36] is a Chinese dataset containing textual, acoustic, and visual data collected from 78 participants. The semi-structured interview scripts contained questions and topics of discussion verified by clinicians. Ground-truth labels were obtained using the Patient Health Questionnaire-9 (PHQ-9) [45]. The E-DAIC [46], [47] is a US-based dataset. E-DAIC contains textual, acoustic and visual data collected from 275 participants. E-DAIC employs the Patient Health Questionnaire-8 (PHQ-8) to obtain the ground-truth label [45]. The only difference between the PHQ- 8 and the PHQ-9 is that the PHQ-8 does not include the ninth item which relates to suicidal thoughts.\nB. Data Pre-Processing\n1) CMDC Dataset: The dataset provided ready to use visual features such as action units (AUs), eye gaze, head pose and facial landmarks extracted using OpenFace (version 2.2.0) [48]. OpenFace is used because it is considered the state-of- the-art open-sourced tool for facial analysis. AUs are used as extracted features to describe the changing characteristics of facial expression in depression since depressed patients may exhibit poor expression ability. Moreover, the occurrence of specific facial movements (e.g. smile, corner of mouth down, etc.) described by specific AUs (e.g., AU12) is directly related to depression. The acoustic features set used is the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) [49]. Another widely used open-source toolkit, openSMILE (version 3.0) [50], was used for acoustic feature extraction. Some examples of extracted acoustic features include pitch, jitters, frequency, and bandwidth of Formant 1, 2, 3 etc. Visual data for questions 2, 8, 10, 11, and 12 for all participants were removed from subsequent ML experimentation as they were missing. In addition, we remove participant MDD 23 as they had no representative textual data. Zou et al. did not mention any of the identified gaps in the CMDC dataset [36].\n2) E-DAIC Dataset: There was no missing data in E-DAIC. For the fairest comparison between CMDC and E-DAIC, we pre-process the data in a similar fashion as done by Zou et al. Across the visual and acoustic features, we therefore concatenate the mean, max, and min features on the response- partitioned video frames to achieve temporal aggregation using the provided extracted features in the E-DAIC dataset. In E- DAIC, the same set of visual features and acoustic features were obtained using the OpenFace [48] and openSMILE [50] toolkits respectively. Readers can refer to [36], [46] for further details. Across the textual modality, participant responses obtained via the provided transcripts were inputs to a pretrained English BERT model [51] which resulted in a 768 feature long textual feature vector per participant.\nC. ML Models and Training Details\n1) ML Models: We investigate a binary classification setup where the models' goal is to predict whether a participant is classified as depressed or not. In addition to the models introduced in [36], we ran our experiments on more models to ensure thorough evaluation. These models are the Support Vector Machine with a linear kernel (SVM Linear), Support Vector Machine with a radial basis function kernel (SVM RBF), Logistic Regression (LogReg), Naive Bayes (NB), K- Nearest Neighbors (KNN), Decision Tree (Dec Tree) and the Multilayer Perceptron (MLP). All models were implemented in Python using the sklearn package. We have selected these basic models in alignment with prior works (e.g., [52], [53], [43]) which employed statistical representations of features for training data as we have done in this work.\n2) Statistical Analysis: We perform one-way multivariate ANOVA (MANOVA) tests across both the visual and acous- tic features for both datasets. We further conduct two-way MANOVA tests to compare how significantly different the means of visual and acoustic features are between participants within both the Healthy Control (HC) versus the Major De- pressive Disorder (MDD) groups and the male versus female groups. All MANOVA analysis was implemented using the 'statsmodels' package in Python [54]. Recent work indicates that analysing interviewees' responses at a question level allows for finer grained understanding and detection of be- havioral characteristics of depression [55]. Thus we conduct the analysis on the CMDC dataset structured according to the questions posed within their semi-structured interview [36].\n3) Training Process: All models were evaluated using stratified 5-fold cross validation. We form stratified folds of equal ratios of depressed and non-depressed participants and also ensured equal ratios of male and female participants. This is to ensure that any observed disparities in model fairness across genders are reflective of the model's behavior rather than artefacts of the sampling process. The folds are consistent across all classification tasks and models. Given that both datasets have acoustic (A), textual (T), and visual (V) features, the models were trained with every possible combination of the feature modalities (A, T, V, A+T, A+V, T+V, A+T+V).\nD. Prediction and Fairness Measures\nWe adopt the evaluation metrics overall accuracy, F1 Score and Area Under the Receiver Operating Characteristics (AU- ROC) used in [36] to evaluate each model's performance. A = 1 denotes the majority group (male) and A = 0 denotes the minority group (female). \u00dd denotes the predicted class. We use Equal Accuracy (EAGender) and Disparate Impact (DIGender) to evaluate each model's fairness in alignment with existing works [43], [56], [57].\n\u2022 Equal Accuracy (EA), can be understood as the accu- racy gap between the majority and the minority group:\n$EAGender = |MAE(\u0176|A = 1) \u2013 MAE(\u0176|A = 0)|$,\nwhere MAE represents the Mean Absolute Error (MAE) of the classification task of each sensitive group.\n\u2022 Disparate Impact (DI), measures the ratio of positive outcome (\u0176 = 1) for the majority and minority groups as follows:\n$DIGender = \\frac{Pr(\u0176 = 1|A = 0)}{Pr(\u0176 = 1|A = 1)}$,\nEA is dependent on predicted and actual outcome whereas DI is mainly dependent on the predicted outcome. The complementary nature of both fairness measures thus provides a better understanding of the bias present."}, {"title": "IV. STATISTICAL ANALYSIS RESULTS", "content": "We evaluate whether there are any differences in depres- sion manifestation across gender and countries. All statistical analysis is conducted at a significance value of 0.05.\nA. Statistical Analysis of Features in the CMDC Dataset\nFor CMDC, our statistical analysis revealed significant dif- ferences between the different features representations across depressed vs. non-depressed classes as well as gender.\n1) One-Way MANOVA (Visual): Looking at Table I, there are statistically significant differences between the HC/MDD groups for the Action Unit (AU) classification features for all questions except 4, 5, 6, and 9. There are statistically significant differences between the HC/MDD groups for the pose features for all questions except 9. Interestingly, there are no statistically significant differences between the HC/MDD groups for the gaze features across all questions. This indicates that participants labelled to have MDD in CMDC indeed exhibit different facial action unit activations and different head orientations (poses) compared to HC participants, how- ever they do not exhibit significantly different gaze directions.\n2) Two-Way MANOVA (Visual): Table I indicates no sta- tistically significant differences between the HC/MDD and Male/Female groups for any of the OpenFace visual feature, indicating that the effect of depression status on the visual features does not vary significantly by gender.\n3) One-Way MANOVA (Acoustic): Table II indicates that participants labelled to have MDD in CMDC indeed exhibit different pitch, amplitude, and spectral sound character- istics compared to HC participants."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we evaluate the performance and fairness across all the different ML models for both datasets.\nA. Evaluating Models on the CMDC Dataset\n1) Performance Evaluation: With reference to Table V, the SVM Linear and Logistic Regression models consistently provide the best results across all performance measures. Both models produce the best accuracy, F1, and AUROC scores for the textual and all multimodal approaches. Most models perform better within a multimodal setting. NB performs the worst across most performance measures. For instance, across the audio modality, NB gave the lowest accuracy, F1, and AUROC scores of 0.86, 0.80 and 0.86 respectively as well as the lowest multimodal (A+T+V) results of 0.91, 0.85 and 0.88 across accuracy, F1 and AUROC as well. This is followed by the KNN and Decision Tree models thus indicating these models may not be suited for the task or dataset at hand.\n2) Fairness Evaluation: Across the fairness measures, most models provide relatively good results. We see from Table V that the SVM Linear, SVM RBF and Logistic Regression mod- els all provide consistently good results across most features and modalities. For instance, both models produce the fairest EAGender score of 0.00 across the textual modality and the fairest DIGender score of 1.00 across the visual, A+V, T+V, and A+T+V modalities. NB consistently produces the least fair outcome across most features and modalities followed by the KNN and Decision Tree models.\nB. Evaluating Models on the E-DAIC Dataset\n1) Performance Evaluation: With reference to Table VI, the best performing model is the MLP which consistently provides the best results for all performance measures across most uni and multimodal settings. For instance, it provides the best accuracy, F1, and AUROC scores across the audio, A+T, A+V, and A+T+V modalities. This is followed by the the Logistic Regression and the SVM Linear models. Similar to the CMDC dataset, NB performs the worst across most performance measures. For instance, across the visual modality, NB gave the lowest accuracy, F1, and AUROC scores of 0.53, 0.53 and results of 0.58 across both F1 score and AUROC. This is followed by KNN. The Decision Tree seems to work better"}, {"title": "VI. DISCUSSION OF FINDINGS", "content": "RQ1: Are there any differences in depression manifestation across gender and countries? Results from Sections IV-A indicate that there are significant differences between the dif- ferent features across depressed vs. non-depressed participants as well as gender for CMDC. This is true for both the visual and audio features. In addition, the spectral acoustic features and the amplitude acoustic features also varied significantly across gender. However, this observation is not as significant for E-DAIC as highlighted in Section IV-B. We see significant differences in depression manifestation across gender in both datasets. However, a distinctive feature is that this observation is stronger for CMDC than for E-DAIC. This hints that there may be statistically significant differences in depression manifestation across the sample population in both datasets.\nRQ 2: How do the results compare between different ML models and modalities across the different datasets? There are differences in ML model performance and fairness. It is interesting to note that for both datasets, certain models such as the SVM Linear and the Logistic Regression models perform much better, and certain models such as NB perform poorly across both datasets. Looking at the gender fairness results on CMDC, multimodal setups generally result in a narrower range of DI scores across gender than the unimodal setups. Using a multimodal setup as compared to a unimodal setup provides the classifiers with more information to learn from, which may have resulted in an improvement across the fairness and performance metrics [43], [58]. However, for E-DAIC, the range of DI scores remain similar across both unimodal and multimodal setups.\nRQ3: Are there any differences in ML model performance and fairness across gender and countries? If present, are the differences in performance due to the differences in depression manifestation as hypothesised? We see that overall, the ML models perform better for CMDC than for E-DAIC. In general, model performance was better across all feature-modality combinations when the classifiers are trained and tested on the CMDC dataset. The change in performance between CMDC and E-DAIC is in agreement with Zou et al.'s benchmark evaluations on both datasets [36]. It is probable that the classifiers found it more difficult to recognise the defining characteristics of depressed people in E-DAIC given the similarity across E-DAIC's features. The contrast between CMDC and E-DAIC could be due to the data elicitation methods employed by the dataset creators. CMDC's features were derived from 12 questions originating from a semi-structured interview that was clinically approved [36], whereas E-DAIC's features were derived from casual recorded conversations [44], [46]. Perhaps the consistency of subject matter ingrained in CMDC's data made a difference. Alternatively, perhaps depression is outwardly more obvious to spot in China than it is in the USA, and it is this cultural difference that is factoring into the contrast observed between each dataset's features and its models' performances. This theory would be in agreement with findings from existing works [9], [12], [39]."}, {"title": "VII. CONCLUSION", "content": "A key takeaway from this study is that there were notable differences between each selected classifier's performance and gender fairness metrics across the CMDC and E-DAIC datasets. However, it is not conclusive if this is due to cultural differences or due to differences in data collection methods. We could not conduct statistical tests between both datasets as the data elicitation methods were too different. This was one of the big limitations of this study as we could not ascertain if the difference in performance across the two countries were due to a difference in depression manifestation or data collection methodology [36], [59]. Future work could involve the creation of a new multimodal depression dataset where data is collected in multiple countries via a single clinically approved protocol (e.g., semi-structured interview). The limited number of participants in CMDC and E-DAIC was another limiting factor. Future work should ensure a reasonable number of samples for the underprivileged group in fairness metric calculation, to experiment with other evaluation methods beyond just stratified k-fold cross validation, and consider using other fairness metrics such as Equal Oppor- tunity and Equalized Odds [60] to ensure more thorough and comprehensive evaluation. Future research can also focus on developing solutions to mitigate bias issues in both datasets. This will enhance our understanding of how to address bias in datasets from different countries. Future work can also investigate other methods [61], [62] or orthogonal forms of fairness [63], [64] to better understand the root cause of bias.\nWe made a conscious effort to repeat the experiments for different algorithms to ensure that the results are not algorithm-dependent. We hope that our work will encourage other researchers to take into consideration fairness and other ethical concerns when developing ML-equipped agents and robots for wellbeing [65]\u2013[69]. Given the high-stakes involved in using such agents and robots for wellbeing coaching [3], [5], [70], we hope our results and findings will encourage more consistent and culturally-aware data collection for depression [71], thus paving the way towards developing fairer ML- equipped agents and social robots for providing wellbeing to all."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "We recognise the sensitive nature of this study and have adopted measures aligned with ethical guidelines. The datasets used have been anonymised by the dataset owners to minimise privacy impact. Our work attempts to avoid any bias against certain groups of people that could result in discrimination. However, our results are limited to the datasets included in this work. Future work should repeat the same analysis on other depression datasets to further validate our findings."}]}