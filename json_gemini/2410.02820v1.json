{"title": "GPT's Judgements Under Uncertainty", "authors": ["Payam Saeedi", "Mahsa Goodarzi"], "abstract": "We investigate whether biases inherent in human cognition, such as loss aversion, framing effects, and conjunction fallacy, manifest in how GPT-40 judges and makes decisions in probabilistic scenarios. By conducting 1350 experiments across nine cognitive biases and analyzing the responses for statistical versus heuristic reasoning, we demonstrate GPT-40's contradicting approach while responding to prompts with similar underlying probability notations. Our findings also reveal mixed performances with the AI demonstrating both human-like heuristic errors and statistically sound decisions, even as it goes through identical iterations of the same prompt.", "sections": [{"title": "1 Introduction", "content": "Cognitive biases and heuristics have been a well-established phenomena of the human mind. Researchers have been advancing the boundaries of science regardless of these biases. They have established the milestone of creating artificial intelligence capable of interacting with humans and offering information. The information they provide is a culmination of our minds and learning of all the text and languages fed into them. With these capabilities also come some of our inefficiencies as human beings. In this research, we aim to examine whether certain biases of the human mind through what we have written and provided to Large Language Models have carried over to their way of presenting information, making choices, or judgements.\nA clear and present pattern with the available literature is that the prior work done to tackle and mitigate cognitive biases inherited by machine learning models, was performed in laboratory and experimental settings. One of the common patterns labeled as hallucination is LLMs tending to change their responses (Goodarzi et al., 2024). We found no cases where the same prompt was repeatedly run to determine whether the machine might return the \"correct\" response from time to time. In this case, one might be the lucky recipient of a correct response from the machine. Given the different types of cognitive heuristics, the machine does not fall for some rooted in statistics. To push the envelope further, we expose production-grade Large Language Models (LLMs) to several categories of cognitive biases via a series of well-established word experiments."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Cognitive Bias, Heuristics, Judgement, and Decision Making", "content": "The human mind is capable of constructing consistent narratives based on incomplete pieces of information available to it. This is possible through a simplification mechanism called heuristics, where we opt to answer a question that requires cognitive effort and elaborate thinking, with a simpler alternative. This gives way to systemic biases of the human mind that can impact judgement and decision making (Ehrlinger et al., 2016; Murata et al., 2015; Tversky and Kahneman, 1974)"}, {"title": "2.2 Generative AI and Cognitive Bias", "content": "Research on cognitive biases in AI systems is scarce. However, authors have investigated the resemblance of human cognitive biases for judgment and decision-making to AI and machine learning because of the training processes (G. Harris, 2020; Lin and Ng, 2023; Mart\u00ednez et al., 2022). While scholars have explored the impact of human cognitive biases on explainable Artificial Intelligence (XAI), scholars have called for an interdisciplinary and socio-technical approach to understanding algorithmic behavior (Bertrand et al., 2022; Irving and Askell, 2019; Mart\u00ednez et al., 2022). Although traditional Machine Learning (ML) algorithms are prone to information accessibility and expected to bypass misjudgment, humans have exceeded ML performance by surpassing their cognitive biases like loss aversion and overconfidence (Blohm et al., 2022).\nWithin Large Language Models (LLMs) and Natural Language Generation (NLG), cognitive biases exhibited presence for tasks such as code generation (Jones and Steinhardt, 2022), sentiment analysis (Abramski et al., 2023), Cognitive Reflection Test (CRT) (Hagendorff and Fabi, 2023), and decision making (Ma et al., 2023). Despite being trained on data on a massive scale and developed using mathematical and computational reasoning to simulate rationality, LLMs act close to humans when approaching cognitive biases (Azaria, 2023). For example, experiments with biases like framing effect, anchoring, availability heuristic, and attribute substitution on Codex, CodeGen, and GPT3 revealed high-impact errors in code generation (Jones and Steinhardt, 2022). Chat GPT has also shown a high tendency to give human-like responses to intuition bias, iterative reasoning, gambler's fallacy, zero cost effect, ultimatum game, endowment effect, and anchoring effect in addition to availability heuristics and framing effect (Azaria, 2023; Ma et al., 2023). However, newer versions of GPT have resembled higher performance in cognitive bias experiments compared to other models (Abramski et al., 2023; Macmillan-Scott and Musolesi, 2024; Singh et al., 2024). Although boosting the size and efficiency of the model increases judgement errors in most models, chat GPT3.5 and GPT4 showcased resistance and performed better in chain-of-thought (system 2) compared to zero-shot (system 1) prompting (Hagendorff et al., 2023).\nCreated by humans, understanding how/if the underlying biases might have transferred onto LLMs through training and interaction feed is crucial (Kudless, 2023). However, it is as critical to distinguish the nature of these biases in the human versus AI context (Gulati et al., 2023). On the other hand, understanding LLM behavior for cognitive tasks requires an experimental exploration similar to the studies of human cognitive behavior (Sartori and Orr\u00f9, 2023). This novel psychological approach demands the treatment of LLMs as participants active in an experiment for uncovering cognitive biases (Sartori and Orr\u00f9, 2023).\nAlthough studies have experimented with multiple output generations for one prompt or variations of the same contextual prompt, they have not reported any quantified measurements regarding the inconsistency of responses (Abramski et al., 2023; Irving and Askell, 2019; Shaki et al., 2023; Macmillan-Scott and Musolesi, 2024). Researchers have labeled incorrect or contradictory text generation as hallucination, while others have argued about the inaccuracy and misleadingness of this term (Hicks et al., 2024). Including repetition in performance evaluation enhances robustness and reproducibility (Zhu et al., 2024)."}, {"title": "3 Experiment Settings", "content": "Understanding how cognitive biases apply to algorithmic behavior, requires rigorous study and collaboration with experts in psychology. A crucial point to consider is that not all cognitive heuristics are applicable to a machine learning or a deep learning model. One instance, is asking ChatGPT to \"recall\" 6 or 12 instances of its own assertive behavior in interaction with colleagues (Schwarz et al., 1991). We went through an extensive list of cognitive biases introduced by Kahneman and Tversky and selected 9 which can be asked in the form of prompts from a machine:\n\u2022 Loss aversion, sunk cost fallacy, prospect theory and framing effect (McDermott et al., 2008; Yechiam, 2019), all of which include introducing two scenarios for loss and gain,\n\u2022 Halo effect, where our impression of a particular field, impacts our judgement in another area,\n\u2022 Conjunction fallacy and bias of resemblance, where the human's fast system of thinking will simplify a question of statistical likelihood by favoring stereotypical similarities over base rates (Moro, 2009), and\n\u2022 Neglect of probability where the human's intuitive system of thinking will ignore probabilistic independence of events in favor of constructing a consistent narrative (Schade et al., 2004).\nWe utilized the OpenAI application programming interface to feed the GPT-40 engine with 10 prompts for 9 biases. The framing effect in particular, required framing the same scenario in 2 different settings.\nThe engine in question was chosen because it is the fastest and most efficient model offered by OpenAI API (Platform) to date. In order to ensure consistency, we used zero-shot chain of thought prompting and assigned the role of a \"human participating in a social experiment\" to the AI (Sartori and Orr\u00f9, 2023). At the end of each prompt, we specifically instructed it to list its reasoning and choose one of the 2 alternatives for each prompt. These responses were labeled:\n\u2022 Elaborate: If the response was based on elaborate, statistical reasoning.\n\u2022 Intuitive: If the response was based on a simplifying heuristic.\nFor instance, the machine was told that an individual was tidy, liked organization, had an eye for details and was also a survivor of domestic violence. It was then asked whether he was more likely to be a librarian or librarian AND an advocate against domestic violence. Statistically, being a member of both sets (Librarian, advocate against domestic violence) is less likely than the probability of being a member of either of them. Yet a heuristic often employed by our minds will drive us to simplify \"likelihood\" to traits and appearances. In this instance identifying the individual as a librarian and NOT as a librarian and a survivor of domestic violence is the elaborate response.\nWe replicated each experiment 150 times and calculated the ratio of elaborate to intuitive responses. Each GPT-40 response was run against 9 lists, containing keywords associated with each cognitive bias. The keywords were extracted from prominent literature resources that either introduced, or made significant contribution to our understanding of these cognitive heuristics (McDermott et al., 2008; Moro, 2009; Schade et al., 2004; Sun et al., 2020; Tversky and Kahneman, 1974; Yechiam, 2019). This was done in order to examine whether GPT-4o recognized or addressed any of the prompts in its responses."}, {"title": "4 Results", "content": "Across a total 1350 experiments, GPT-40 returned 658 elaborate and 692 intuitive responses. The share of each of these responses varied considerably from one experiment to the other.\nWhen it came to conjunction fallacy, which was highlighted as an example above, the machine always provided an elaborate and albeit statistically sound response. It also elaborated on its decision with the reasoning that the intersection of two sets is always smaller than the size of either of them. It recognized the premise as the conjunction fallacy and made note of it in multiple instances.\nIn complete contrast, GPT-40 failed in all but one instance at recognizing the bias of resemblance and relied on the simplifying heuristic of stereotypical similarity rather than base rate probabilities. This is particularly interesting given GPT-40's performance in the second probability neglect experiment. When asked what probability the 10th flip of a coin would have for heads, given that the previous flips were all heads, it very firmly stated that the probability of each event is independent from the last, which demonstrated its grasp of the same probability concepts that it ignored in the case of resemblance. Same elaborate way of thinking persisted as it navigated the first neglect of probability experiment and it managed to answer over 63% of the experiments correctly. In both probability neglect experiments, GPT-40, referenced the word \"probability\" repeatedly. In case of framing effect, the machine provided contradicting responses to the positive and negative framing of the same scenario. Despite its recognition of loss aversion and using the phrase in its responses multiple times, in over 95% of the experiments, the GPT-40 failed to take into account the expected value of gain in a probabilistic scenario of win versus loss based on the outcome of coin flip. Consistent with this result, it often chose the intuitive response in the prospect theory experiment. In 86% of those, GPT-40 leaned towards the risk-free option for gain; however, in the 14% it acknowledged the expected value of a higher reward based on a favorable coin flip outcome; exhibiting knowledge and understanding of statistical techniques used to calculate expected values of returns in decision making methods such as a decision tree. This understanding was on clear display with the sunk cost fallacy, where the GPT-40 not only recognised and referred to the cognitive bias, but managed to answer 82% of the experiments correctly."}, {"title": "5 Discussion", "content": "We provide an examination of how cognitive biases may manifest in large language models, specifically GPT-40. The results indicate a mixed performance in recognizing and appropriately responding to prompts designed to trigger certain biases."}, {"title": "6 Conclusion", "content": "Key findings indicate that GPT-40 consistently navigates conjunction fallacy and certain aspects of probability neglect with statistically robust responses. However, the model frequently falls prey to biases such as the resemblance heuristic and framing effects, similar to human cognitive tendencies. This dichotomy highlights that despite advancements in AI development, these systems can still reflect the cognitive shortcuts that characterize human thought processes.\nOur results suggest that while large language models like GPT-40 can achieve high levels of performance in some areas, they remain susceptible to cognitive biases under specific conditions. This insight is critical for the ongoing development and deployment of AI systems, as it emphasizes the need for continued efforts to understand and mitigate the influence of these biases."}, {"title": "Limitations", "content": "The current body of work has several limitations that should be acknowledged. This study relies on a specific subset of cognitive biases and does not encompass the full spectrum of biases that could potentially affect AI decision-making. Moreover, the experiments were performed on a single AI model, GPT-40, which limits the generalizability of the findings to other models and versions. Additionally, the zero-shot chain of thought prompting technique used in the experiments may not fully represent real-world LLM applications, where context and iterative interactions play significant roles. Furthermore, the categorization of responses into \"elaborate\" and \"intuitive\" is subjective and might not capture the full complexity of the AI's reasoning processes. Future research should address these limitations by including a broader range of biases, multiple AI models, and varied experimental setups to provide a more comprehensive understanding of cognitive biases in AI systems."}]}