{"title": "Understanding Decision Subjects' Engagement with and Perceived Fairness of AI Models When Opportunities of Qualification Improvement Exist", "authors": ["Meric Altug Gemalmaz", "Ming Yin"], "abstract": "We explore how an AI model's decision fairness affects people's engagement with and perceived fairness of the model if they are subject to its decisions, but could repeatedly and strategically respond to these decisions. Two types of strategic responses are considered-people could determine whether to continue interacting with the model, and whether to invest in themselves to improve their chance of future favorable decisions from the model. Via three human-subject experiments, we found that in decision subjects' strategic, repeated interactions with an AI model, the model's decision fairness does not change their willingness to interact with the model or to improve themselves, even when the model exhibits unfairness on salient protected attributes. However, decision subjects still perceive the AI model to be less fair when it systematically biases against their group, especially if the difficulty of improving one's qualification for the favorable decision is larger for the lowly-qualified people.", "sections": [{"title": "Introduction", "content": "The rapid development of Artificial Intelligence (AI) technologies has made it possible to automate decision making in many domains. However, it has been discovered that AI models often acquire pre-existing biases in the dataset used for their training, resulting in the unfair treatment to individuals from different demographic backgrounds (Mesa 2021; Dastin 2018). This increased awareness of fairness issues of AI has led to many recent studies in understanding people's fairness perceptions of and reactions to AI models (Wang, Harper, and Zhu 2020; Gemalmaz and Yin 2022; Harrison et al. 2020). These studies look into the perspectives of different stakeholders, among which a key stakeholder is decision subjects, the people who are actually subject to the AI models' decisions. For example, Wang, Harper, and Zhu (2020) found that when decision subjects only interacted with an Al model once, both the model's unbiased decisions across different groups of subjects and the model's favorable decisions towards subjects' own group resulted in an increase in the perceived fairness of the AI model.\nMeanwhile, recent research on the long-term dynamics and implications of fairness in AI (Liu et al. 2018, 2020; Zhang et al. 2020; Zhang and Liu 2021; Zhang et al. 2019) has drawn the community's attention to the fact that in reality, decision subjects could often interact with an AI model repeatedly over a long term. Moreover, in each interaction, decision subjects may no longer passively accept the AI model's decisions on them as is. Rather, the AI model's decisions on subjects may shape how they actively and strategically respond to the AI model. For instance, one strategic response decision subjects could take in their repeated interactions with the AI model is to decide whether to continue interacting with it and be subject to its decisions (Zhang et al. 2019; Gemalmaz and Yin 2022)\u2014decision subjects have the freedom to quit using an Al model if they wish so. As another example, decision subjects could also respond to AI decisions on them by investing in effort to improve themselves, hoping for an increased chance of receiving the favorable decision in the future (Zhang et al. 2020; Liu et al. 2020)-job applicants could take additional courses about a skill, and loan applicants could explore options to increase their credit scores, both aiming to improve their \"qualification\" for the favorable decision (i.e., getting the job offer or the loan approval). The possibility to improve one's qualification may encourage decision subjects to take a long-term view to think about the future when responding to the AI model at present, and it may indirectly influence their willingness to continue interacting with the AI model.\nAs decision subjects could repeatedly and strategically respond to Al decisions in many real-world scenarios, understanding their reactions to AI models with different fairness properties and what they perceive as \u201cfair\u201d become critical again. Specifically, we ask that when decision subjects can strategically and repeatedly respond to AI decisions:\n\u2022 RQ1: How will the AI model's fairness properties (both across groups and on the subject's group) affect decision subjects' engagement with the model (e.g., willingness to improve themselves and to be subject to AI decisions)?\n\u2022 RQ2: How will the AI model's fairness properties affect decision subjects' perceived fairness of the model?\nPredicting answers to these questions turns out to be very challenging. In terms of the willingness to improve one's qualification for the favorable decision, it is possible that decision subjects decide how much to improve themselves solely based on whether doing so increases their utility, and is not affected by the AI model's decision fairness at all. However, one may also speculate that the AI model's biases against a subject's group could affect their drive to improve themselves. For example, if the AI model consistently"}, {"title": "", "content": "places a subject's group at a disadvantage position in its decisions, individuals in that group might feel they are being treated as \"second-class citizens\u201d. This feeling could diminish their motivation to improve their qualifications. On the other hand, recognizing the bias, they might be even more determined to improve, seeing it as the sole avenue to increase their odds of favorable decisions and level the playing field with people from other groups. Without a clear hypothesis on how the AI model's decision fairness affects decision subjects' willingness to improve their qualification, predicting how their willingness to keep interacting with the AI model or their perceived fairness of the AI model are affected by the AI model's fairness properties also becomes difficult. This is because both retention and fairness perceptions can be highly influenced by the final qualification level that decision subjects could reach.\nTo complicate things further, answers to these questions may also vary across different contexts. For example, one may conjecture that if improving one's chance of getting the favorable decision is particularly difficult for those who really \u201cneed\u201d the improvement (i.e., those with relatively low qualification), the impact of AI fairness on decision subjects may be more salient, as the hope of changing one's fate through efforts and self-improvement is limited. Similarly, it is also possible that the impact of AI fairness on decision subjects is larger if AI exhibits discriminatory behavior on some salient protected social attributes, triggering people's strong emotional attachment to their own group identities. Formally, one may ask that when decision subjects can strategically and repeatedly respond to AI decisions:\n\u2022 RQ3: Do answers to RQ1-RQ2 change if the difficulty for decision subjects to improve their qualification vary with their current qualification level in different ways?\n\u2022 RQ4: Do answers to RQ1-RQ2 change when the AI model's fairness properties is/is not discussed with respect to groups defined by protected social attributes, such as gender?\nTo answer these questions, we conducted three exploratory human-subject studies on Amazon Mechanical Turk (N = 368, 713, 416 for the three studies, respectively). In all three studies, subjects completed a simulated loan application task that was designed to mirror real-world loan application scenarios where the loan decisions are made by an AI model. Subjects were free to decide how many times to apply for a loan from the AI model, and whether to improve their own qualification (i.e., their credit score) before each application. In each study, we created two treatments to reflect that the AI model may or may not show systematic bias towards one group over the other in granting loans. Study 1 was designed to answer RQ1 and RQ2, so the difficulty for subjects to improve their qualification does not vary with their current qualification level, and subject's group identity was randomly assigned. To answer RQ3, we slightly varied the design of Study 2 so that the difficulty of qualification improvement either increased or decreased with the subject's current qualification level. Finally, to answer RQ4, subject's group identity in Study 3 was decided by their self-reported gender rather than a randomly assigned value."}, {"title": "", "content": "Our results show that when decision subjects could repeatedly and strategically respond to the AI model's decisions on them, their engagement with the model\u2014including their willingness to improve their qualification and willingness to keep interacting with the model\u2014are not influenced by the AI model's decision fairness. This holds true both when the difficulty of qualification improvement changes with one's current qualification level in different ways, and when the AI model's fairness is/is not examined with respect to salient protected attributes like gender. However, we find that despite the possibility of strategic responses, decision subjects still perceive the AI model as less fair if the model biases against the subjects' group by placing them at a disadvantaged position in receiving the favorable decision. In other words, when decision subjects can strategically and repeatedly respond to AI decisions, their level of engagement with an AI model does not reflect either the model's group-level decision fairness or their perceived fairness of the model. We conclude by providing possible explanations for our findings and discussing their implications,"}, {"title": "Related Work", "content": "The complexity of the notion of \"AI fairness\" has inspired many studies on understanding whether and when do humans perceive an AI model as \"fair\" in decision making (Yaghini, Krause, and Heidari 2021; Saxena et al. 2019; Harrison et al. 2020; Srivastava, Heidari, and Krause 2019). Of particular relevance to our study are a few recent works on understanding decision subjects' fairness perceptions of an AI model that makes decisions about them. Earlier studies typically focus on one-shot interaction scenarios where decision subjects would only receive a decision from an AI model once. For example, Yurrita et al. (2023) studied the influence of explanations, human oversight, and contestability on fairness perceptions of the decision subjects in a one-shot interaction with the AI model for loan approvals. They found that while explanations and contestability significantly impacted fairness perceptions, human oversight showed minimal effect. In another study involving one-shot interaction with the AI model, decision subjects perceive an AI model as fairer both if the AI model makes a favorable decision on them and if the AI model is not biased towards or against any particular group (Wang, Harper, and Zhu 2020).\nMore recently, there is a line of theoretical works on \u201clong-term fairness\u201d (Liu et al. 2018; Hu and Chen 2018; Zhang et al. 2019; D'Amour et al. 2020; Mouzannar, Ohannessian, and Srebro 2019; Heidari, Nanda, and Gummadi 2019) emphasizing that in the real world, decision subjects often engage in repeated interactions with the AI model, and the dynamics between the AI model's decisions on subjects and subjects' strategic reactions to those decisions could create feedback loops. One domain that is frequently studied in the long-term fairness literature is loan lending-For a loan applicant characterized by a profile x, the bank may use its AI-based loan approval system to make loan lending decisions on the applicant. Moreover, the applicant may strategically respond to these decisions by staying or leaving the system and/or changing their profile x, which may further change the AI model's training data and impact the model's"}, {"title": "", "content": "decision-making policy in the future. This shift from one-shot to long-term, repeated interaction has inspired some empirical studies looking into how do decision subjects react to and perceive AI models in their long-term interactions with AI. For example, it was found that when decision subjects had the choice to leave the AI-based decision system at any time in their repeated interactions, their willingness to stay in the system and their perceived fairness of the system are significantly affected by whether the system is in favor of the subject's own group (Gemalmaz and Yin 2022). Compared to earlier works, in this study, we take into account another key strategic action that decision subjects could take in their repeated interactions with AI. That is, decision subjects could also freely decide whether to improve their qualifications for receiving future favorable decisions from AI. In the real world, these qualification improvement attempts are usually realized through the adjustment of decision subjects' input attributes, possibly as the decision subjects follow the algorithmic recourse plans suggested by the AI model to change their situation towards receiving a more favorable decision (Ustun, Spangher, and Liu 2019).\nWe believe that the addition of qualification improvement in the set of decision subjects' strategic actions brings new perspectives for re-examining the relationship between AI's decision fairness and decision subjects' reactions to and perceptions of it. Indeed, the possibility to improve their qualification for future favorable decisions may shift decision subjects' attention from focusing on the present to thinking about the (long-term) future, which may have complicated implications on how decision subjects would react to the AI model at present. For example, because of their future thinking, decision subjects may change how much risks they are willing to take or how they treat the immediate and future rewards (Thorstad and Wolff 2018; Hershfield 2011). It may also change the ways that decision subjects weigh utility-related considerations (e.g., how many favorable decisions can I get from the AI model in the long run?) and fairness-related considerations (e.g., is the AI model's decision on me fair?) in deciding their engagement with the AI model. It is even possible for decision subjects to change how they define \"fairness\u201d for AI. This is because the possibility of qualification improvement may allow decision subjects to evaluate the AI fairness not only through \u201csocial comparison\u201d (e.g., does AI make similar decisions on my group and other groups? Festinger (1954)) but also through \u201ctemporal self-comparison\u201d (e.g., does AI grant more favorable decisions to me after my qualification is improved? Albert (1977)).\nA few theoretical works have examined the implications of AI fairness in scenarios where decision subjects can respond to AI decisions strategically by improving their qualifications (Zhang et al. 2020; Liu et al. 2020; Mouzannar, Ohannessian, and Srebro 2019). However, none of these works focus on empirically characterizing how decision subjects would respond to AI models with different levels of fairness to determine their qualification transitions. Rather, they make simplified assumptions about subjects' behavior in their theoretical derivations. For example, Liu et al. (2020) assumed that decision subjects decide about their qualification improvement based on a cost-benefit analysis by ex-"}, {"title": "", "content": "amining whether the increase in utility after the qualification is improved is larger than the cost of improvement. Zhang et al. (2020) considered whether decision subjects received a favorable decision from the AI model in one interaction as the key influencing factor for them to decide whether to improve their qualification in the next interaction. Different from these theoretical works, we use experiment with real human subjects to provide empirical insights into whether and how the AI model's decision fairness affects decision subjects' qualification transitions in repeated interactions, and their engagement with/perceived fairness of the AI model in general. Thus, results of our study may help verify or reject assumptions made in previous works."}, {"title": "Study 1", "content": "To understand how an AI model's decision fairness affects decision subjects' repeated and strategic interactions with the AI model, we conducted a series of human-subject experiments\u00b9. In Study 1, we aim to first answer RQ1-RQ2 in an environment where (1) the AI model's fairness properties are not examined with respect to a salient protected attribute, and (2) the difficulty for a decision subject to improve their qualification for the favorable decision does not change with the subject's current qualification level."}, {"title": "Experimental Design", "content": "Tasks. Subjects in our experiment were asked to complete a simulated loan application task, which was carefully designed to mimic the real-world loan application scenario that is often used in theoretical studies of the long-term dynamics of AI fairness (Liu et al. 2018; Zhang et al. 2020; D'Amour et al. 2020) and experimental studies of fairness perceptions of AI (Gemalmaz and Yin 2022; Yurrita et al. 2023). Specifically, each subject was assigned with a randomly-generated persona of a small business owner containing 5 attributes; this persona was used as the subject's loan application \"profile\". We highlight two key attributes in the persona:\n\u2022 Group identity: The subject's group membership, which was set to be either \"red\" or \"blue\". This is to reflect that in Study 1, the AI model's fairness properties across groups are not defined on salient protected attributes.\n\u2022 Initial credit score level: The subject's credit score level at the beginning of the experiment, which was taken from the set {300-350,\u2026\u2026,500\u2013550, \u2026, 700\u2013750}; the subject could later decide to \u201cimprove\u201d their credit score with some cost (see details below).\nIn addition to the above two attributes, the subject's persona also included three other attributes-their number of years of having a credit history, their home ownership status (e.g., rent or own), and the type of small business they ran (e.g., healthcare, construction). For each attribute in a subject's persona, we uniformly randomly sampled a value from the set of all candidate values for that attribute. After getting their assigned profile, the subject was asked to use it to apply for loans from their \"local bank\" to support their"}, {"title": "", "content": "business, and they were explicitly told that the bank utilized an Al model to analyze loan application profiles and make loan approval decisions. The subject was also informed that their credit score level would be used by the AI model as a primary determinant of their \u201cqualification\u201d for the loans-credit scores of 650 or higher were generally considered as \"high\", and higher credit scores were associated with higher chance of getting loans approved.\nIn the experiment, the subject started the experiment with 600 \u201ccoins\u201d in their account. Each loan application cost the subject 50 coins, and the subject would gain 100 coins if the application got approved or nothing otherwise. Each subject was asked to apply for a loan from the bank for at least once to get a sense of the AI model's decision fairness. After that, they could interact with the AI model for at most 9 more rounds. In each round, the subject had the freedom to choose from one of three actions:\n\u2022 Improve and apply: The subject would first attempt to improve their credit score to the next level (e.g., from 600-650 to 650-700) with a cost of 5 coins\u00b2, and then apply for a loan with a cost of 50 coins. The credit score improvement attempt was not guaranteed to be successful (see details below) if successful, subjects would be notified, and the AI model would use the updated credit score to make the loan approval decision.\n\u2022 Apply without improvement: The subject would directly apply for a loan with a cost of 50 coins without attempting to improve their credit score level. This action was provided to subjects to reflect that in reality, whether and when to improve one's qualification is a voluntary (and strategic) decision.\n\u2022 Not apply any more: The subject would not apply for loans any more and would be redirected to the end of the experiment. This action was provided to subjects to reflect that in reality, whether to continue interacting with an AI model is a voluntary (and strategic) decision.\nSince Study 1 concerns an environment where the difficulty for decision subjects to improve their qualification does not change with their current qualification level, we set the \"success rate\u201d for subjects across all credit levels to progress to the next level at a constant value (i.e., 44%). That is, if the subject attempted to improve their qualification in one round, whether they could successfully progress to the next credit level would be stochastically decided by this success rate\u00b3. Once the subject successfully progressed to the next credit level, they would at least maintain that level, and possibly progress to even higher levels if they decided to make additional improvement attempts in future rounds.\nAt the end of each round, if the subject applied for a loan in that round, the AI model's approval or denial decision on"}, {"title": "", "content": "the subject would be revealed to them, without providing explanations on why it makes this decision. To enable subjects to perceive the AI model's decision fairness, a flowchart summary of the AI model's decisions in this round on applicants of both red and blue groups would also be provided to the subject (see Figure A1 in appendix for an example). Figure 1 illustrates the process of the simulated loan application task. We note that the designs of this task reflect a few key characteristics of the real-world repeated interactions between decision subjects and an AI model: (1) participating in the decision making process (thus triggering the usage of the AI model) is costly; (2) receiving a favorable decision from the AI model is rewarding; (3) the improvement of qualification is costly and has uncertainty; and (4) decision subjects have the freedom to respond to the AI model's decisions by deciding whether to improve their qualification and/or whether to continue being subject to the AI model's decision. By assigning each subject with a persona of a small business owner, our experiment reflects a scenario where the real-world decision subjects will often interact with the AI-based decision systems repeatedly, as small business owners often need to apply for loans at different time points to meet their business's various financing needs.\nTreatments. We created two treatments in Study 1:\n\u2022 Fair AI model: In this treatment, the bank's AI model was fair towards subjects in both the red group and the blue group. In particular, as shown in Table 1a, regardless of the subject's group identity, the AI model's loan approval rate always started from 15% for subjects with the lowest credit level (i.e., 300-350), and the approval rate increased by 7% as the subject's credit score went one level up, with a highest possible approval rate of 85% for subjects with the highest credit level (i.e., 800\u2013850).\n\u2022 Unfair AI model: In this treatment, the bank's AI model was unfair and systematically biased against subjects in the blue group. Table 1b shows this model's approval rate for subjects in the red group, while Table 1c shows this model's approval rate for subjects in the blue group. As shown in the tables, for every credit level, the AI model's approval rate for red group subjects with this credit level was always 20% higher than that for blue group subjects4. Same as that in the previous treatment, the AI model's increment in approval rate for each increased credit level was kept at 7% for both red and blue groups.\nSo, if the subject decided to apply for a loan in one round, the AI model's loan approval decision on them would be stochastically decided by the approval rate for their most updated credit level, given the subject's treatment assignment and group identity. As described earlier, at the end of each round where the subject applied for a loan, we also presented to the subject a summary of the AI model's decisions across applicants in different groups. In particular, we told the subject that there are a total of N ~ U[12000, 12200] applicants"}, {"title": "", "content": "who applied for loans from the bank in this round. For each of these N applicants, we randomly generated their persona, and then simulated the AI model's decision on them using the approval rates defined by the subject's assigned treatment. We then visualized the AI model's decisions on all N applicants using two sets of flowcharts. The first set showed the AI model's approve/deny decisions for applicants with-/without \"high\" credit scores (i.e., a score of at least 650). The second set zoomed in to applicants with similar credit scores as the subject and showed the AI model's approve/deny decisions for applicants whose credit level was one level higher than, the same as, or one level lower than the subject (see Figure A1 in appendix)5. Note that within each set, the AI model's decisions on red/blue group applicants were shown in separate flowcharts; this enables subjects to make direct comparisons across the two groups of applicants to determine the AI model's decision fairness.\nProcedure. Our experiment was made available as a Human Intelligence Task (HIT) on Amazon Mechanical Turk (MTurk). Only U.S. workers with a HIT approval rate of at least 95% and a total of at least 1000 approved HITs were eligible for taking this HIT.\nUpon arrival, the subject first created a nickname and selected an avatar for their persona. Next, we provided an interactive tutorial explaining each attribute's meaning on the subject's randomly-assigned persona profile. Additionally, this tutorial explained to subjects what they were asked to do in each round, how to use the interface, and how to interpret the summary information of the AI model's decisions"}, {"title": "", "content": "as displayed in the flowcharts. We then used a quiz of 5 questions to test subjects' understanding of the task procedure and ability to interpret flowcharts. Subjects were only allowed to advance to the actual experiment after correctly answering all 5 questions.\nIn the actual experiment, the subject was first randomly assigned to one of the two treatments. Then, the subject went through the simulated loan application task. Note that throughout the task, the subject's credit level would be updated if the subject's improvement attempt was successful, and the subject's account balance would be updated depending on the loan application outcomes. Once the subject finished the loan application task, they were redirected to our post-experiment survey. The survey included questions about the subject's demographics (e.g., race, age, education) and perceived fairness of the AI model. We also measured a few other characteristics of the subject that we conjectured to influence their behavior in interacting with the AI model, such as their risk attitude. All questions except for the demographics were presented as 5-point Likert scale questions in which the subject needed to indicate their agreement with a set of statements from 1 (strongly disagree) to 5 (strongly agree). The subject could also explain why they considered the AI model they encountered in the experiment as fair or unfair using free-form texts. The complete list of survey questions are provided in Appendix B.\nSubjects were told that their payment in this experiment was composed of a base payment of $2 and a bonus payment that would be decided by their account balance at the end of the experiment. Upon survey completion, we converted the remaining balance in the subject's account to their bonus payment using a 500 coins to $2 ratio. The maximum bonus a subject could earn was $4.40. Subjects spent a median time of 27 minutes on our experiment and received a median payment of $4.30, resulting in an hourly wage of $9.60. We"}, {"title": "", "content": "also included three filtering procedures to filter out potential spammers (see Appendix C for details). A subject's data was only considered valid if they passed all filtering procedures."}, {"title": "Analysis Methods", "content": "We used regression analyses to answer our research questions. Specifically, we used three dependent variables to quantify decision subjects' engagement with an AI model (RQ1) and their fairness perceptions of it (RQ2):\n\u2022 Engagement-Improvement: The number of qualification improvement attempts the subject made in the loan application task; a higher value indicates a higher level of willingness for the subject to improve themselves.\n\u2022 Engagement-Retention: The number of times that the subject applied for a loan; a higher value indicates a higher level of willingness for the subject to continue being subject to the AI model's decisions.\n\u2022 Perceived Fairness: The subject's rating to six statements (e.g., \"The bank's AI system is fair to manage loan applications.\") adapted from Wang, Harper, and Zhu (2020) regarding their perceptions of the AI model's fairness; the higher the rating, the fairer the subject found the AI model to be (the max rating is 30).\nThe independent variable we included in the regression reflects the AI model's \u201cfairness properties,\u201d which was operationalized in two ways. First, to explore how subjects' engagement with and perceived fairness of AI vary with the Al model's decision fairness across groups, we used a binary variable Fair AI to represent if the AI model treats the two groups of loan applicants in a similar way (i.e., it was set to 1 for subjects in the fair AI treatment and 0 otherwise). Second, to understand how subjects' engagement with and perceived fairness of AI are affected by the AI model's decision fairness on the subject's own group, we used two other binary variables in our regressions\u2014Advantaged and Disadvantaged, representing if the subject's group was favored or disfavored by the AI model. That is, Advantaged (Disadvantaged) was set to 1 only for red (blue) group subjects in the unfair AI treatment.\nFinally, to control for the influences on dependent variables beyond those brought up by the independent variables, we considered a few characteristics of the subjects and included them as covariates:\n\u2022 Initial Credit Score: The credit score level that was assigned to the subject at the beginning of the experiment. We conjectured that subjects with higher credit levels make fewer improvement attempts as there is smaller room of improvement for them. However, they might be more willing to interact with the AI model and even perceive it as fairer, because they were more likely to receive loan approval decisions from the AI model.\n\u2022 Fairness Sensitivity: The degree to which the subject values fairness as a core principle, which was measured in the post-experiment survey through soliciting the subject's opinions on a set of statements (e.g., \u201cI would stop using an AI system if it is unfair, even if it tends to be in favor of me\") adapted from Gemalmaz and Yin (2022). We conjectured that subjects' fairness sensitivity may af-\""}, {"title": "", "content": "fect how they react to the AI model's decision fairness.\n\u2022 Empathy: The subject's empathy level, which was measured in the post-experiment survey through soliciting the subject's agreement with a set of statements (e.g., \"I get a strong urge to help when I see someone who is upset\") adapted from Spreng et al. (2009). We conjectured that individuals with higher levels of empathy may exhibit stronger concerns regarding the fairness of an AI model, as they are concerned with the well-being of others even if they are not personally affected by the AI model's bias.\n\u2022 Risk Attitude: The subject's risk attitude, which was measured through soliciting the subject's opinions on a set of statements (e.g., \u201cI like to do frightening things.\") adapted from Kam (2012). We conjectured that subjects who were more risk-seeking might be more willing to take actions to improve their qualification or continue interacting with the AI model. Previous research has also found that people with higher risk-taking tendencies tend to perceive Al systems as fairer compared to those who are less risk-takers (Nakao et al. 2022).\nWe fit our experimental data into regression models to predict subjects' engagement (RQ1)\u2014including their retention and improvement\u2014and fairness perceptions (RQ2). To emphasize the exploratory nature of this study, we followed the interval estimate method (Cumming 2014; Dragicevic 2016) in our analysis. That is, our regression results are interpreted via the estimated coefficient values for the independent variables as well as their 95% bootstrap confidence intervals (R = 1000). When the 95% bootstrap confidence interval of the coefficient for an independent variable does not include zero, we consider the effect of the independent variable to be reliable (Cumming 2014).\""}, {"title": "Experimental Results", "content": "368 subjects participated in our experiment and passed all filtering procedures (see Appendix D for the demographics). In the following, we analyzed the full dataset collected from these 368 subjects to answer our research questions."}, {"title": "RQ1: Impacts of AI's decision fairness on engagement.", "content": "First, we look into that in decision subjects' strategic, repeated interactions with an AI model, how the AI model's decision fairness affects their engagement with the model, including their willingness to improve their qualification and their willingness to be subject to the AI model's decisions (see Figure A2 in Appendix E for the histograms of the number of improvement attempts/loan applications made by subjects in different treatments or different groups).\nRegarding the willingness to improve their qualification, the average number of times that improvement attempts"}, {"title": "", "content": "were made was Mfair = 4.11 (SD = 3.43) and Munfair = 4.11 (SD = 3.26) for subjects in the fair AI and unfair AI treatments, respectively. Model 1 in Table 2 examines whether the AI model's fairness level across groups has any impact on subjects' willingness to improve their qualification. Here, the estimated coefficient for the independent variable \"Fair Al\u201d was not reliably different from zero (\u03b2 = -0.03[-0.73, 0.62]). This suggests that subjects' average level of willingness to improve their qualification is not impacted by the AI model's decision fairness across groups. Moreover, as shown in Table 2 (Model 2), we also find that the AI model's decision fairness on the subject's own group does not impact their willingness to improve, as neither of the coefficients associated with \"Advantaged\" and \"Disadvantaged' are reliably different from zero.\nSimilar observations can also be made for decision subjects' retention. On average, subjects in the fair AI treatment interacted with the AI model for Mfair = 6.23 (SD = 3.93) rounds, while subjects in the unfair AI treatment interacted with the AI model for Munfair = 6.58 (SD = 3.72) rounds. Regression results are shown in the middle panel of Table 2 (Models 3 and 4). We find that once decision subjects can strategically react to the AI model's decisions on them, on average, they are equally willing to keep interacting with the AI model regardless of the model's decision fairness, both across groups and specifically towards their group.\nInterestingly, across Models 1\u20134 in Table 2, we also notice that subjects with higher initial credit score levels consistently made fewer improvement attempts, but interacted with the AI model for more rounds. This is expected and consistent with our conjecture."}, {"title": "RQ2: Impacts of AI's decision fairness on fairness perceptions.", "content": "Subjects in the fair AI treatment reported an average fairness rating of 18.77 (SD = 5.15) for the AI model. Meanwhile, subjects in the unfair AI treatment reported an average fairness rating of 18.02 (SD = 5.40), with those who were placed at the advantaged position (i.e., the red group) reported an average rating of 19.0 (SD = 4.75) and those who were placed at the disadvantaged position (i.e., the blue group) reported an average rating of 17.0 (SD ="}, {"title": "", "content": "5.81). Table 2 (Models 5 and 6) shows the results of the regression analysis. While the AI model's fairness level across different groups still does not appear to reliably influence decision subjects' average level of perceived fairness of the model (Model 5), we do find that when the AI model systematically biases against the subject's group, the subject perceives the model as less fair (i.e., \u03b2 = \u22121.41[\u22122.78, -0.11] for \"Disadvantaged\" in Model 6). Also, consistent with our conjecture, we also observed that subjects who were more risk-seeking or had a higher initial credit score level tended to perceive the AI model as fairer."}, {"title": "Study 2", "content": "In Study 1, we have answered RQ1 and RQ2 in an environment where the difficulty for decision subjects to improve their qualification does not vary with their current qualification levels. In Study 2, we explore the generalizability of our Study 1 results in different environments where the qualification improvement difficulty varies with one's current qualification level and therefore answer RQ3."}, {"title": "Experimental Design", "content": "In Study 2", "exception": "In Study 1"}]}