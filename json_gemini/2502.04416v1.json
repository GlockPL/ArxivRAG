{"title": "CMOE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference", "authors": ["Zehua Pei", "Lancheng Zou", "Hui-Ling Zhen", "Xianzhi Yu", "Wulong Liu", "Sinno Jialin Pan", "Mingxuan Yuan", "Bei Yu"], "abstract": "Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/ JarvisPei/C\u041co\u0415.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have demonstrated exce\u0440- tional proficiency in managing complex tasks and exhibiting emergent capabilities across diverse domains and applications, particularly when scaled to billions of parameters [1]\u2013[4]. While LLMs have attained remarkable success, their expanding computational demands and model sizes have intensified challenges related to practical deployment, especially in environments with limited hardware resources or stringent latency requirements. To mitigate these challenges, mixture- of-experts (MoE) architectures [5]\u2013[8] have emerged as a promising paradigm. Unlike dense LLMs, where all parameters are activated for every input token, MoE models replace monolithic feed-forward networks (FFNs) with sparsely ac- tivated experts: specialized sub-networks that process inputs conditionally via dynamic routing. This design decouples model capacity from computational cost-activating only a subset of experts per token while preserving the model's expressive power.\nRecently, researchers have found that there is high activation sparsity in the hidden neurons of FFNs of dense LLMs, which motivates them to develop sparsity-aware acceleration tech- niques to reduce computational overhead while maintaining model performance [9], [10]. Building upon this insight, a growing body of research has focused on transforming dense LLMs into MoE architectures through strategic reorganization of FFN parameters but not training MoE from scratch [11]\u2013 [13]. The prevailing methodology replaces conventional FFN layers with MoE layers, where neurons are partitioned into multiple expert sub-networks while maintaining the original parameter count. During inference, a routing mechanism selectively activates only a subset of experts per input token, thereby achieving dynamic sparsity without compromising model capacity. However, due to the high sparsity target always required in MoE models, these works often need massive computing resources and billions of training data for continual pre-training on the constructed MoE models.\nTo address the aforementioned limitations, we propose carved MoE, named CMoE, a framework that efficiently carves sparse MoE architectures from dense LLMs through parameter reorganization and training-free structural adaptation. Unlike prior approaches that rebuild MoE models from scratch via resource-intensive pre-training, CMoE strategically \"carves\" experts from the dense model's existing feed-forward network (FFN) neurons while preserving their inherent knowledge. This carving process is both computationally efficient-requiring only minutes of lightweight processing\u2014and sophisticated, as it leverages systematic neuron grouping and analytical router construction to retain performance with minimal fine-tuning. The core innovation lies in CMoE's ability to restructure dense FFNs into MoE layers without re-training the base model. First, we identify neurons that universally encode common knowledge (a.k.a. shared experts) and those that exhibit special- ized, input-dependent activation patterns (a.k.a. routed experts). Shared experts are systematically retained by selecting neurons with the highest activation rates, ensuring they capture broadly applicable features. By formulating routed expert grouping as a balanced linear assignment problem, solved via the Jonker- Volgenant algorithm, CMoE clusters neurons into experts while maintaining parameter balance and activation coherence. Second, we derive the routing mechanism directly from the dense model's activation statistics, bypassing the need for end-to-end router training. This involves constructing a differentiable routing function initialized using representative neurons from each expert cluster, enabling immediate usability while preserving optimization potential.\nIn summary, the key contributions of this paper are:\n\u2022 CMoE: A framework that efficiently carves MoE from dense LLMs by reorganizing FFN neurons into shared/routed experts, eliminating costly pre-training."}, {"title": "II. RELATED WORK", "content": "In contrast to pretraining MoE models from scratch, recent research has investigated the feasibility of constructing MoE architectures by repurposing existing dense LLMs. Current methodologies for deriving MoE models from dense checkpoints generally follow two paradigms: (1) partitioning parameters of FFNs while preserving the original model's total parameter count [10], [14], or (2) expanding the model's overall capacity while retaining activation dimensions com- parable to standard dense models [15], [16]. This work prioritizes the former approach. Notably, MoEBERT [14] introduces an importance-driven strategy to transform FFNS into expert modules by strategically redistributing top-scoring neurons across specialized components. Concurrently, MoEfi- cation [10] leverages the discovery of sparse activation patterns in ReLU-based FFNs within T5 architectures, enabling the decomposition of these layers into distinct expert groups governed by a learned routing mechanism. Based on continual training, the LLaMA-2 7B model is modified as a LLaMA- MoE-3.5B MoE model, where the parameters of the original FFNs are partitioned into multiple experts [11]. After training with 200B tokens, the LLaMA-MoE-3.5B model significantly outperforms dense models that contain similar activation parameters. Furthermore, based on a two-stage post-training strategy, an MoE model is constructed from the LLaMA3 8B model, where both attention and MLP are partitioned into MoE blocks [12]].\nExtensive experiments have shown the effectiveness of constructing an MoE model from a dense model, and many techniques can be utilized to guarantee performance recovery. However, such performance recovery is extremely resource- consuming, which is unfavorable for efficient deployment in industrial applications. Therefore, more lightweight methods are required, such that performance recovery can be done within hours and even training-free.\nNote that model compression such as pruning and quan- tization is another important technique for efficient LLM inference [17]\u2013[20]. Pruning is among the most widely utilized approaches to detect and remove redundant or less significant parameters from models, thereby resulting in a sparser weight matrix and faster inference. ShortGPT [21] has put forward a simple layer-removal approach. This approach is based on block influence, which is determined by the similarity between a layer's input and output. SliceGPT [22] substitutes each weight matrix with a smaller dense matrix, thereby"}, {"title": "III. BACKGROUND", "content": "This study primarily focuses on the LLaMA family [2], [25], which uses SwiGLU [26] as the activation function. However, our analysis and findings can be adapted to most of the FFN structures of existing LLMs, including the ReLU- based FFNs [27].\nAn FFN exists in the tail of each transformer block, which gets the input embedding x \u2208 Rd and then contributes to the output together with the residual connection, i.e. x + F(x). Typically, an FFN is a two-layer fully connected network, i.e. the up projection and down projection layer, with an activation function between them. For LLaMA, the SwiGLU composes another gate projection layer. Given the up projection weight Wup \u2208 Rdxdh, the gate projection weight Wgate \u2208 Rdxdh and the down projection weight Wdown \u2208 Rdn\u00d7d, the process of an FFN is given by:\nF(x) = hW_{down}, \\tag{1}\nh=SwiGLU(xW_{up})=Swish(xW_{gate}) \\odot (xW_{up}),\nwhere Swish(x) = x\u00b7\u03c3(x) is element-wise and \u03c3(\u00b7) is the sigmoid function.\nThe basic MoE architecture is composed of a set of N independent FFNs as experts, {E1, E2, ..., EN}, and a router network G [5]. The output of an MoE-version FFN is then obtained by\nF_{MOE}(x) = \\sum_{i=1}^{N} g_i E_i(x), \\\\\ng_i=\\begin{cases}\n    S_i,& S_i \\in \\text{TopK}(\\{s_j | 1 \\leq j \\leq N\\}, K), \\\\\n    0,              & \\text{otherwise,}\n\\end{cases}, \\\\\nS=[s_1,s_2,\\dots,s_N] = G(x), \\tag{2}\nwhere gi is the score for the i-th expert, s \u2208 RN is the token- to-expert affinity, i.e. the output of G, and TopK(., K) denotes the set comprising K highest scores among the affinity scores calculated for x on all experts."}, {"title": "IV. METHODOLOGY", "content": "CMOE transforms a dense LLM into a sparsely activated MoE architecture through two key phases: efficient expert grouping and training-free router construction, followed by optional lightweight adaptation. As illustrated in Fig. 1, the framework operates as follows: A Neuron Activation Profiling (Section IV-A). Given an FFN layer, CMoE profiles neurons' activation patterns with a small calibration dataset to categorize the neurons into shared experts (high-activation, task-agnostic) and routed experts (sparsely activated, task- specific). B Expert Grouping (Section IV-A). Shared"}, {"title": "A. Shared and Routed Experts Grouping", "content": "CMOE starts with grouping neurons in FFN into experts. Shared experts are expected to process common knowledge instead of specialization. Thus, the key idea is to group neurons that are always activated during FFN inference. For routed expert grouping, as described in previous works [28], the key idea is to cluster the neurons that are always activated simultaneously. CMoE carries forward these ideas but comes up with a detailed and analytical perspective. In this section, we construct N experts of size m = dh/N (assuming dh is a multiple of N, i.e., N | dh), including Ns shared experts and Nr routed experts (Ns + Nr = N).\nWe begin with analyzing the isolated contribution of each neuron to the output of FFN. Consider the i-th element of hi \u2208 Rdh : hi = Swish(x\u00b7wgate,i)\u00b7(x\u00b7wup,i), where wgate,i \u2208 Rd and wup,i \u2208 Rd are the i-th column of Wgate and Wup, respectively. It shows that the value of hi only depends on the input x and the corresponding i-th weights, which implies the independence between neurons. On the other hand, the output of FFN can be written as:\nF(x) = \\sum_{i=1}^{d_h}h_i W_{down,i}, \\tag{3}\nwhere wdown,i \u2208 Rd is the i-th row of Wdown. When we revisit the FFN process, we can regard each hi as the score to be multiplied to the split vector wdown,i, whose product contributes to part of the output F(x). As a finding of some structured pruning research [29], [30], the norm of F(x) is always small due to the residual connection. Such phenomenon implies the high sparsity existed in FFN, and by (3) we relate it to hi, whose value decides how large an isolated neuron contributes to the output. Therefore we make a hypothesis:\narg \\underset{i}{min} |h_iw_{down, i} \\approx arg \\underset{i}{min} |h_i|, \\tag{4}\nwhich is reasonable since when hi is extremely small, the product hiwdown,i will also vanish. It is expected that the hidden state h should be highly sparse, which means hi is often extremely small. To verify it, we hack into the FFN and draw the distribution of h. As demonstrated in fig. 2, the distribution is sharply peaked at 0 and constrained within a small range, indicating that most hi are concentrated near zero and confirming the sparsity. And it exhibits symmetry, suggesting a near-normal distribution centered around the mean. Based on what we discuss and observe above, the hidden state values work well as the basis for judgment of neuron activation, because of its high differentiation and independence across different neurons. Therefore, we propose a new metric, called absolute TopK (ATopK), to determine the activation status of a neuron with index i:\nai = \\begin{cases}\n    1,& h_i \\in \\text{TopK}(\\{|h_j| | 1 \\leq j \\leq d_h\\}, K_a), \\\\\n    0,              & \\text{otherwise,}\n\\end{cases}, \\tag{5}\nwhere we choose neurons with Ka highest absolute value among the hidden state values, and assign their labels in the activation marker a = [a1, a2,\u2026,adh] with 1.\nTo further evaluate the activation information, we make samples in the training set to record their activation markers. Given a batched input tensor X \u2208 Rb\u00d7s\u00d7d, with b the batch size and s the sequence length, we obtain the batched hidden state H \u2208 Rbxsxdh as follows:\nH = Swish(XW_{gate}) \\odot (XW_{up}). \\tag{6}"}, {"title": "B. Training-free Router Construction", "content": "We now present the training-free router network G for CMOE. Unlike previous works, which either built the router from scratch or intuitively used hidden features as initialization, we formulate the router construction as a minimization problem and then develop an algorithm with analysis to construct the router by approximating the optimal solution.\nGiven the same input embedding x, the output of original output of the dense FFN, i.e. F(x) in (1), is equivalent to the sum of the output of all the experts in FMOE:\nF(x) = E_s(x) + \\sum_{i=1}^{N_r}E^r_i(x). \\tag{16}\nThe only difference between (16) and FMOE(X) in (15) is the expert score g, which is obtained from the TopK selection of the output of G. Therefore, to preserve important knowledge captured by the original dense FNN, G can be constructed to enforce FMOE(X) to be close to F(x) by solving the minimization problem (17),.\narg \\underset{G}{min} | F_{MOE}(x; G) - F(x)| \\\\\n= arg \\underset{G}{min} | \\sum_{i \\in S_{de}} (g_i - 1)E_i(x) = arg \\underset{G}{min} | \\sum_{i \\in S_{de}} E_i(x)|,\nwhere Sde = {i: si \u2209TopK({si|1<j<Nr}, Nk)} and |Sde|= Nr - Nk, and the problem becomes constructing the G to minimize the absolute sum of the output of deactivated routed experts. Note that we have made a hypothesis in (4) that the output/sparsity of F(x) is highly related to the norm/sparsity of h, which is the same for the expert outputs. Based on (3) and (4), we reformulate (17) as in (17):\narg \\underset{G}{min} | \\sum_{i \\in S_{de}} E_i(x)| \\\\\n\\underset{G}{by (3)} arg \\underset{G}{min} | \\sum_{i \\in S_{de}}\\sum_{j \\in S_{N_r,i}} h_jw_{down,j}| \\\\\n\\underset{G}{by (4)} = arg \\underset{G}{min} | \\sum_{i \\in S_{de}}( \\sum_{j \\in S_{N_r,i}} |h_j| \\\\\n arg \\underset{G}{min} \\mathbb{E}_n [||h||_1 | i \\in S_{de}]. \\tag{17}\nThe problem becomes constructing G that can minimize the expected hidden states h of deactivated routed experts."}, {"title": "C. Differentiable Routing and Load-balancing", "content": "Though we have constructed a well-designed router in Section IV-B, it is not differentiable since each expert score gi is a constant, as shown in (15), hindering further alignment and performance recovery. Therefore, we introduce a learnable parameter u when computing the expert scores as follows,\ng_i:=\\begin{cases}\n    1 + s'_i \\cdot u_i,& s_i \\in \\text{TopK}(\\{s_j|1 \\leq j \\leq N_r\\}, N_k), \\\\\n    0,              & \\text{otherwise,}\n\\end{cases},\\\\\ns' = \\text{Softmax}(s), u = [u_1, u_2, \\dots, u_{N_r}] \\tag{24}\nwhere the scale u is initialized as zero to avoid perturbation. For MoE models, load-balancing is crucial to guarantee computational efficiency, especially expert parallelism in LLMs serving. As in DeepSeek-V3 [4], we use the auxiliary- loss-free load balancing by introducing a bias term b before the TopK selection:\ng_i=\\begin{cases}\n    1 + s'_i \\cdot u_i, & s_i + b_i \\in \\text{TopK}(\\{s_j|1 \\leq j \\leq N_r\\}, N_k), \\\\\n    0,              & \\text{otherwise,}\n\\end{cases},\\\\\nb = [b_1, b_2, \\dots, b_{N_r}]. \\tag{25}\nHere, b is initialized as zero and updated based on the expert load status during each step of training, as in DeepSeek-V3. The hyper-parameter update speed \u03b3 is used to update the bias term b at the end of each step, i.e. decreasing/increasing the bias term by \u03b3 if the corresponding expert is over-loaded/underloaded."}, {"title": "V. EXPERIMENTS", "content": "CMOE is implemented based on Hugging Face Transform- ers [33] together with Pytorch [34]. The experiments are conducted on one NVIDIA H800 PCIe 80GB graphics card with CUDA Driver 12.6. We randomly select samples from WikiText-2 training dataset [35] as calibration and fine-tuning data. We use only 8 examples with 2,048 sequence length as calibration for calculating the hidden state H in (6). We set Ka = 10 for the activation status record, which sounds counter-intuitive but works best in practice. For lightweight fine-tuning, we run 1 epoch using the Adam optimizer [36] with \u03b21 = 0.9 and \u03b22 = 0.95. We also employ LoRA [37] with a rank of 8 and lora_alpha = 32. We fine-tune all the models including the baselines with 2,048 samples. We set different initial learning rates for the score scale u and other parameters, i.e. 0.001 and 5.95e-5, respectively. We set the bias update speed \u03b3 to 0.001."}, {"title": "A. Main Results", "content": "We compare CMoE with the up-to-date baseline LLaMA- MoE [11], in which the neurons are randomly split, and continual pre-training is carried out with additional router networks. The experiments cover both training-free and lightweight fine-tuning versions. We design these experiments to demonstrate the remarkable post-training performance of CMOE. we demonstrate the results on two different datasets, i.e. WikiText-2 [35] and C4 [38]. The baseline models are chosen as LLaMA-2-7B and LLaMa-3-8B."}, {"title": "B. Ablation Studies", "content": "We conduct ablation studies with LLaMA-2-7B and data randomly selected from WikiText-2 training datasets."}]}