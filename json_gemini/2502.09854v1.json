{"title": "Efficient Multitask Learning in Small Language Models Through\nUpside-Down Reinforcement Learning", "authors": ["Yu-Chen Lin", "Sanat Sharma", "Hari Manikandan", "Jayant Kumar", "Tracy Holloway King", "Jing Zheng"], "abstract": "In this work, we demonstrate that small lan-\nguage models (SLMs), specifically a 100M pa-\nrameter GPT-2 model (Radford et al., 2019),\ncan achieve competitive performance in mul-\ntitask prompt generation tasks while requiring\nonly a fraction of the computational resources\nneeded by large language models (LLMs).\nThrough a novel combination of upside-down\nreinforcement learning and synthetic data distil-\nlation from a powerful LLM, Llama-3 (Dubey\net al., 2024), we train an SLM that achieves\nrelevance scores within 5% of state-of-the-art\nmodels, including Llama-3, Qwen2, and Mis-\ntral, despite being up to 80 times smaller, mak-\ning it highly suitable for resource-constrained\nand real-time applications. This study high-\nlights the potential of SLMs as efficient multi-\ntask learners in multimodal settings, providing\na promising alternative to LLMs for scalable,\nlow-latency deployments.", "sections": [{"title": "1 Introduction", "content": "Large language models have revolutionized var-\nious applications of natural language process-\ning, yet they come with substantial computational\nand memory costs, especially at inference time.\nThese limitations hinder their deployment in high-\nfrequency, real-time applications where efficiency\nis critical. We propose a small language model\n(SLM) framework that can serve as an efficient and\neffective multitask learner for multimodal prompt\ngeneration tasks, utilizing upside-down reinforce-\nment learning to control the generation.\nIn Figure 1, we demonstrate our overall pipeline.\nWe start by generating synthetic data from a large\nlanguage model (LLM) using prompt engineer-\ning and few-shot examples. This synthetic data\nundergoes upside-down reinforcement learning"}, {"title": "2 Related Work", "content": "2.1 LLM Knowledge Distillation\nLLM knowledge distillation has been widely ex-\nplored, particularly for real-time, high frequency\napplications. Proprietary models might not meet\nwith time constraints and often lack flexibility in\nadapting to specific tasks. Supervised fine-tuning\n(SFT) is a common approach to distill knowledge\nfrom a large model to a smaller one. For exam-\nple, Taori et al. (2023); Chiang et al. (2023) distill\nknowledge from GPT-3.5 to Llama, achieving com-\npetitive performance to the teacher model. In our\nwork, we apply SFT to a smaller GPT-2 model,\ntargeting high-frequency, low latency real-world\napplications.\nResearchers have also explored distilling LLM\nknowledge for specific NLP tasks. Similar to our\nfocus on natural language generation, Xu et al.\n(2023b,a); Ramnath et al. (2024); Agarwal et al.\n(2024) apply knowledge distillation across various\ntasks, such as summarization, question-answering,\nand machine translation. In this work, we concen-\ntrate on prompt generation tasks, a critical applica-\ntion that is highly relevant to industry challenges.\n2.2 Reinforcement Learning\nA key challenge in applying LLMs to real-world\nscenarios is aligning them with practical use cases,\nwhich often involve human preferences and satis-\nfaction. This alignment is commonly addressed\nthrough reinforcement learning (RL), where mod-\nels are optimized to maximize a desired reward. For\nexample, Bai et al. (2022); Cui et al. (2024); Lee\net al. (2024); Kim et al. (2023) train reward models\nusing human or AI feedback and apply RL algo-\nrithms like PPO (Schulman et al., 2017) for reward\nalignment. In addition, Rafailov et al. (2024) di-\nrectly optimizes the reward through ranking-based\noptimization, which does not need to train a sep-\narate reward model. In our work, we control the\ngeneration process using upside-down reinforce-\nment learning (UDRL) (Schmidhuber, 2020; Sri-"}, {"title": "3 Methodology", "content": "Our task involves generating prompts for gener-\native models based on multimodal inputs. The\nprocess begins with detecting user intents using an\nin-house model. Once the intents are identified, we\naim to generate prompts accordingly. Rather than\nrelying solely on a large language model (LLM)\nor a multimodal LLM, we adopt a more efficient\napproach.\nSpecifically, our methodology focuses on train-\ning a small language model (SLM) for prompt gen-\neration with intents that emphasizes high efficiency\nand multitask capabilities. We achieve this by lever-\naging synthetic data distillation from a large lan-\nguage model (LLM) combined with upside-down\nreinforcement learning.\n3.1 Synthetic Dataset Distillation\nTo enable the SLM to capture complex task knowl-\nedge from a larger model, we first create a synthetic\ndataset using Llama-3. This involves generating\nhigh-quality, task-specific data that allows the SLM\nto learn from the representations of LLM effec-\ntively. We utilize vLLM (Kwon et al., 2023) to\nparallelize our dataset generation process.\n3.1.1\nDataset Curation Process\n1. Intent and Prompt Pair Generation: We\ncurated a dataset of 52 million intent-prompt\npairs by prompting the LLM with various in-\ntent descriptions and collecting its responses."}, {"title": "3.2 Upside-Down Reinforcement Learning for\nSLM Optimization", "content": "To optimize the SLM's generation quality and con-\ntrol specific attributes like length and relevance,\nwe utilize upside-down reinforcement learning\n(UDRL) (Schmidhuber, 2020; Srivastava et al.,\n2021). This approach allows the model to learn spe-\ncific objectives based on desired outcomes rather\nthan traditional reward structures.\n3.2.1 Reward-Based Prompt Generation\nUpside-down reinforcement learning frames the\nprompt generation task as an optimization problem\nwhere the SLM aims to achieve target specifica-\ntions for each generated output. This process is as\nfollows:\n\u2022 Controlled-Length Generation: The SLM\nis trained to produce prompts within a desired\nlength range (e.g., 10 to 35 words). Tokens\nindicating target lengths are incorporated into\nthe input, guiding the model towards generat-\ning responses that match the specified length\nwith a mean squared error consistently under\ntwo words. More evaluations are in Section 4.\n\u2022 Modality-Agnostic Prompting: We trained\nthe SLM to handle both text-to-image (T2I)\nand text-to-template (T2T) prompts within\nthe same framework. This was achieved by\nadding modality tokens to each training in-\nstance, allowing the model to distinguish be-\ntween generation tasks and tailor its output\naccordingly.\n\u2022 Contextual Relevance and Specificity: By\nassigning relevance scores to generated\nprompts based on a predefined metric (e.g.,"}, {"title": "3.3 Model Architecture and Key Capabilities", "content": "Our SLM is based on nanoGPT\u00b9, a compact variant\nof GPT-2, with 104 million parameters, configured\nwith 12 layers, 12 attention heads, and an embed-\nding dimension of 768. The model architecture and\ntraining setup are designed to maximize efficiency\nand multitask performance.\n3.3.1 Model Specifications\n\u2022 Parameter Efficiency: The SLM contains ap-\nproximately 1/80th the parameters of Llama-3\n8b and similar state-of-the-art LLMs, making\nit computationally efficient and suitable for\ndeployment on standard hardware.\n\u2022 Inference Speed: Our SLM achieves a pro-\ncessing speed of up to 338 tokens per sec-\nond on a single A10G GPU (non-batched,\nnon-quantized, or accelerated), making it suit-\nable for real-time applications. This perfor-\nmance is especially advantageous in resource-\nconstrained environments where inference la-\ntency is a critical factor.\n\u2022 Multitask Learning Capabilities: The SLM\nis trained to handle both T2I and T2T prompts,\nmaking it a versatile tool for multimodal ap-\nplications. Through the integration of task-\nspecific tokens, the model can generate con-\ntextually accurate prompts tailored to the input\ntask, whether it's for text-to-image generation\nor template-based design.\n3.3.2 Training with UDRL\nThe training data are formatted as <|# words of\nthe prompt|> <|intent|> INTENT <|prompt\nfor T2I (IP) or T2T (TP)|> PROMPT. As out-\nlined in Section 3.2, we combine the word count\nand modality tokens to create a single training in-\nstance; refer to Table 1 for examples.\nWe utilize a vocabulary set with legal approval,\nensuring it excludes any offensive, discriminatory,"}, {"title": "4 Experiments and Evaluations", "content": "We focus on both qualitative and quantitative eval-\nuations to judge the quality of our model.\n4.1 Quantitative Evaluation\nThe quantitative evaluation is divided into two pri-\nmary areas: relevance evaluation, which assesses\nhow well the generated prompts align with the spec-\nified intents and modality requirements, and task\nadherence evaluation, which measures the SLM's\naccuracy in meeting specific prompt length require-\nments.\n4.1.1\nRelevance Evaluation\nTo evaluate relevance, we conducted experiments\nto measure how accurately the SLM-generated\nprompts aligned with the specified intents and task\nrequirements. Relevance was assessed using an\nautomatic method with LLM-as-a-judge, where\nGPT-40 (OpenAI et al., 2024) served as the evalu-\nator. Each generated prompt was rated on a scale\nfrom 1 to 10, where higher scores indicate stronger\nalignment with the target intent. We provided sev-\neral examples and criterion to the LLM judge prior\nto the evaluation. Some of these include\n\u2022 Correctness - does the prompt contain gram-\nmatical or semantic errors.\n\u2022 Clarity - is the prompt clear to understand, is\nthe grammar structure sound.\n\u2022 Completeness - does the prompt utilize all of\nthe context (intent) provided.\n\u2022 Usefulness - is the prompt generated useful\nfor the task provided."}, {"title": "4.1.2 Task Adherence Evaluation (Length)", "content": "In addition to relevance, we evaluated the SLM's\nability to meet target length requirements for gen-\nerated prompts. The model was trained to produce\nprompts within specified lengths, typically in the\nrange of 10 to 35 words. Length adherence was\nmeasured by calculating the mean squared error\n(MSE) between the generated prompt length and\nthe target length, and by recording the percentage\nof prompts that fell within an acceptable range (\u00b12\nwords from the target length). The evaluation re-\nsults are in Table 3.\nThe SLM achieves precise control over prompt\nlength, with MSE consistently around 1. These"}, {"title": "4.2 Qualitative Evaluation", "content": "To complement the quantitative results, we con-\nducted a qualitative evaluation with human review-\ners and chose 18 human reviewers with experience\nin writing prompts for image and template gener-\native models. The reviewers assessed the SLM-\ngenerated prompts on two criteria:\n1. Relevance - how closely does the prompt align\nwith the intent.\n2. Correctness - is the prompt clear and easy to\nunderstand, with correct grammar and sound\nstructure.\nEach criterion was rated on a scale from 1 to 3\n(corresponding to not, somewhat, very, e.g., not\nrelevant, somewhat relevant and very relevant re-"}, {"title": "5 Conclusion", "content": "This work highlights the capabilities of small lan-\nguage models (SLMs) as efficient and effective\nmultitask learners. By combining upside-down re-\ninforcement learning with supervised training, we\ndemonstrate that an SLM can achieve competitive\nperformance against much larger models, such as\nLlama-3, with an 80-fold reduction in size on spe-\ncific tasks. Our evaluations reveal that SLMs can\nmaintain high contextual relevance, precision, and\nadherence to tasks, even in resource-constrained\nsettings.\nThe significant improvements in efficiency and\nlow latency underscore its suitability for deploy-\nment in enterprise scenarios where computational\nresources and speed are critical."}]}