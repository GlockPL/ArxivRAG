{"title": "No learning rates needed: Introducing SALSA - Stable Armijo Line Search Adaptation", "authors": ["Philip Kenneweg", "Tristan Kenneweg", "Fabian Fumagalli", "Barbara Hammer"], "abstract": "In recent studies, line search methods have been demonstrated to significantly enhance the performance of conventional stochastic gradient descent techniques across various datasets and architectures, while making an otherwise critical choice of learning rate schedule superfluous. In this paper, we identify problems of current state-of-the-art of line search methods, propose enhancements, and rigorously assess their effectiveness. Furthermore, we evaluate these methods on orders of magnitude larger datasets and more complex data domains than previously done.\nMore specifically, we enhance the Armijo line search method by speeding up its computation and incorporating a momentum term into the Armijo criterion, making it better suited for stochastic mini-batching. Our optimization approach outperforms both the previous Armijo implementation and a tuned learning rate schedule for the Adam and SGD optimizers. Our evaluation covers a diverse range of architectures, such as Transformers, CNNs, and MLPs, as well as data domains, including NLP and image data.\nOur work is publicly available as a Python package, which provides a simple Pytorch optimizer.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of modern machine learning, there are numerous optimization algorithms available [1]. However, determining the most suitable algorithm for a specific problem and finding the appropriate learning rate or learning rate schedule often requires extensive expertise and computational resources. In particular, the prevailing approach involves treating the learning rate as a hyperparameter and training the network repeatedly until the optimal value that yields the best performance is discovered. To simplify and expedite this process, recent research in deep learning [2]\u2013[5] has proposed the reintroduction of line search methods as popular optimization technology, which effectively identify an appropriate learning rate by calculating the loss using different step sizes and comparing its reduction to the gradient of the loss function, hereby they eliminate costly hyperparameter tuning.\nTraditional line search methods necessitate several forward passes per gradient update, making it imperative to explore more efficient alternatives. In the work by Vaswani et al. [2], a Stochastic Line Search (SLS) is integrated with an intelligent re-initialization of the step size, effectively mitigating the requirement of multiple forward passes at each step. This approach was shown in [2], [6] to improve a variety of optimization methods, such as Stochastic Gradient Descent (SGD) on tasks such as matrix factorization as well as image classification for small networks and datasets. In [7] the authors adapt this line search to preconditioned optimizers like Adam [8] further increasing its usability.\nIn this paper we extend upon this work, by introducing a momentum term to the SLS, critically improving its performance and stability. Furthermore, we introduce a limitation on the frequency with which a line search is performed, greatly reducing the computation needed. Additionally, we conduct extensive experiments to evaluate the performance of various optimization methods across different datasets domains and architecture options. Our findings demonstrate that, our improved Stable Armijo Line Search Adaptation algorithm, called SaLSa, consistently outperforms the previously introduced SLS as well as tuned optimizers, with very little computational overhead (about 3% compared to no line search). We observe the SaLSa optimizers have on average an 1.5% advantage on accuracy and a 50% lower average log loss at end of training. Additionally, the stability of the training is improved compared to previously introduced versions of SLS.\nTo make our work easy to reproduce and use, we implement all methods as PyTorch optimizers. The source code is open-source and free software (MIT licensed) and available on github."}, {"title": "II. BACKGROUND", "content": "The stochastic Armijo line search described in [2] and [6] is designed to set a step size for all network parameters wk at iteration k. In this section, we closely follow [6] to formalize a modification of the Armijo criterion to handle the ADAM [8] direction. This is based originally upon [2], [7] More importantly, we introduce an improved Armijo criterion, which mitigates the effect of noise in the mini-batch setting by calculating an exponential moving average smoothing on both sides of the Armijo equation.\nWe adopt the following notation from [6]: The loss function is denoted by f(w). ||\u00b7|| denotes the Euclidean norm and \u2207f denotes the gradient of f. Given the iteration counter k, fk and \u2207fk denote the mini-batch loss and its mini-batch gradient."}, {"title": "A. Armijo Line Search", "content": "The Armijo line search criterion is defined as:\n$$f_k(w_k + \\eta_k d_k) \\leq f_k(w_k) \u2013 c \\cdot \\eta_k ||\\nabla f_k(w_k)||^2$$\nwhere dk is the direction (e.g., dk = \u2212\u2207fk(wk) in case of SGD), c \u2208 (0, 1) is a constant (commonly fixed to be 0.1 [2]). The step size \u03b7\u03ba which satisfies Condition 1 is practically obtained by employing a backtracking procedure, see the pseudo-code in Algorithm 1:"}, {"title": "B. Including preconditioned Optimizers (Adam)", "content": "In case of SGD, the direction de is the negative mini-batch gradient.\n$$d_k = -\\nabla f_k(w_k)$$\nAdam's direction defined in [8] can be written as:\n$$g_k = \\nabla f_k(w_k)$$\n$$m_k = \\beta_1 \\cdot m_{k-1} + (1 - \\beta_1) g_k$$\n$$v_k = \\beta_2 \\cdot v_{k-1} + (1 - \\beta_2) g_k^2$$\n$$\\hat{m}_k = \\frac{m_{k-1}}{(1 - \\beta_1^k)}$$\n$$\\hat{v}_k = \\frac{v_{k-1}}{(1 - \\beta_2^k)}$$\n$$d_k = -\\frac{\\hat{m}_k}{(\\sqrt{\\hat{v}_k} + \\epsilon)}$$\nAdam integrates a momentum-based strategy along with a step-size correction based on gradient variance. In the context of training Transformers, these adjustments have proven to be significant improvements compared to the more straightforward SGD algorithm [9].\nThe Armijo line search criterion from Eq. 1 must be adjusted for the Adam optimizer. We perform this adjustment following [6] based on [2], [7]. To check if the Armijo line search criterion is satisfied in the Adam case, we use the direction de defined in Eq. 2, with momentum \u03b2\u2081 = 0. Note that, the Armijo criterion is only guaranteed to be satisfyable by adjusting the step size nk, if the update direction and the gradient direction are similar enough. However, this condition is not generally met when B1 \u2260 0 in Eq. 2. Additionally, we replace the gradient norm term $$\\frac{\\| \\nabla f_k(w_k) \\|^2}{\\sqrt{\\hat{v}_k} + \\epsilon}$$ as in [7] resulting in Eq. 3.\n$$f_k(w_k + \\eta_k d_k) \\leq f_k(w_k) \u2013 c \\cdot \\eta_k  \\frac{\\| \\nabla f_k(w_k) \\|^2}{\\sqrt{\\hat{v}_k} + \\epsilon}$$\nNote that to perform final weight updates each step we use \u03b2\u2081 \u2260 0."}, {"title": "C. SLS Failure Cases", "content": "As shown in [2], [7] the previously described line search methods perform well on smaller datasets and neural network architectures. However, here we show that these methods have problems to consistently perform during larger scale training.\nThe first of these problems we call \"mini-batch noise\": Eq. 1 and 3 describe criterions which are checked for every mini-batch. This is problematic, since the criteria will be violated subject to inherent noise in the mini-batch data. The phenomenon is amplified by small mini-batch sizes.\nAnother frequently occurring problem is mini-batch gradient approximations \u2207fk(wk) \u2248 0. These are due to computational precision problems, even with float32 precision enabled. In the original implementation by [7], whenever \u2207fk(wk) \u2264 10\u22128 no line search was performed.\nAdditionally, a problem which only occurs on large datasets, can be seen in Figure 1. The step sizes of SLS are very sensitive to initial conditions, the only difference between the runs is the random parameter initialization of the network and the shuffled dataset. This problem only seems to occur on larger datasets and is quite impactful, as some runs do not converge properly, or take very long to even begin converging."}, {"title": "III. METHODS", "content": "To obtain a line search method with better properties in the mini-batch scenario, we propose to extend the Armijo criterion with a momentum term. Below we provide a detailed explanation of the modifications we made, the theoretical basis behind them, and our reasoning. Furthermore, we introduce a method to greatly reduce the computational overhead introduced by a line search."}, {"title": "A. Addressing Mini-batch Noise", "content": "As an extension to Eq. 1 we propose an exponential smoothing (also called momentum term) of all factors which are dependent on the mini batch in this equation. First we rewrite Eq. 1:\n$$f_k(w_k) - f_k(w_k + \\eta_k d_k) \\geq c \\cdot \\eta_k ||\\nabla f_k(w_k)||^2$$\nfk(wk) \u2212 fk(wk + \u03b7\u03badk) denotes the decrease in loss and ||fk(wk)||2 denotes the gradient norm. In order to apply exponential smoothing to both terms we define hk and sk as follows:\n$$h_k = h_{k-1} \\cdot \\beta_3 + (f_k(w_k) \u2013 f_k(w_k + \\eta_k d_k)) \\cdot (1 \u2013 \\beta_3)$$\n$$s_k = s_{k-1} \\cdot \\beta_3 + \\frac{\\|\\nabla f_k(w_k)\\|^2}{\\sqrt{\\hat{v}_k} + \\epsilon} \\cdot (1 \u2013 \\beta_3)$$\nhk represents the smoothed decrease of the loss with the current step size, sk the smoothed gradient norm and \u03b23 \u2208 (0, 1) the smoothing factor used for the exponential moving average. We introduce the Stable Armijo Line Search Adaptation (SaLSa) criterion as:\n$$h_k \\geq c \\eta_k s_k$$\nCombining SaLSa and the Adam optimizer is done by computing sk as follows:\n$$s_k = \\frac{s_{k-1}\\beta_3 +  \\frac{\\|\\nabla f_k(w_k)\\|^2}{\\sqrt{\\hat{v}_k} + \\epsilon} (1 \u2013 \\beta_3)}{\\sqrt{\\hat{v}_k} + \\epsilon}$$\nand computing dk as described in Eq. 2, but with \u03b2\u2081 = 0. We keep the calculation procedure for the step size nk the same as previously described."}, {"title": "B. Intuitive Motivation", "content": "As mentioned in Section III due to the inherent noise in mini-batches we expect some of them to violate the original Armijo line search, even if the step size nk is appropriate for the majority of mini-batches around step k in training.\nLet us assume that all mini-batches are normally distributed with respect to the Armijo criterion, e.g. some mini-batches fulfill the condition with a wide margin, most fulfill it with a small margin and some rare exceptional batches violate the criterion. In this scenario we do not want to decrease the step size by a large amount each time we get an exceptionally bad mini-batch, since the step size is still fitting for most batches and the step size is only increased slowly. The stable Armijo line search adaptation in Equation 6 is implementing exactly this behaviour. Note that the exact distribution is not relevant in this thought experiment.\nIf we analyze Equation 6, we notice that the right hand side is affected by the current step size nk to the same degree as in the original Armijo line search Equation 1. However, the left hand side is substantially less affected since ne is part of the exponential smoothing process. This results in a slower reduction of the step size nk as the criterion is fulfilled more easily than the original criterion by lowering \u03b7\u03ba."}, {"title": "C. Theoretical Analysis", "content": "We extend the convergence Theorem introduced in the original Armijo paper [10] for the full batch setting and the SaLSa criterion with SGD from Eq. 6. We additionally require that every found learning rate yields an improved loss f(wk) \u2212 f(wk + Nkdk) \u2265 0. This condition ensures the convergence for an infinite sequence, which may otherwise not be guaranteed due to the exponential smoothing. In practice, enforcing this condition did not yield a significant difference in the optimization process and is thus not implemented in our experiments."}, {"title": "D. Addressing Computational Costs", "content": "It is unnecessary and computationally expensive to perform a line search for every step during training, as for most steps the step size does not need to be changed. The overall training compute cost increases by roughly 30% when performing a line search every step. To address this, we propose to perform a line search more regularly when a high rate of change of the step size is detected and less regularly otherwise. We realize this by keeping 2 different exponential moving averages of the step size nk which we update after every line search procedure:\n$$\\tilde{\\eta}_k (\\beta) = \\beta \\tilde{\\eta}_{k-1} + (1 \u2013 \\beta) \\cdot \\eta_{k-1}$$\nWe calculate the average rate of change as follows:\n$$r_k = \\frac{\\tilde{\\eta}_k (0.9)}{\\tilde{\\eta}_k (0.99)}$$\nand invert it if rk \u2264 1:\n$$L_k=\\begin{cases}r_k & \\text{ if } r_k \\geq 1\\\\r_k^{-1} & \\text{ otherwise}\\end{cases}$$\nwe set the line search frequency Le to the closest integer of:\n$$L_k=\\frac{1}{r_{k-1}}$$\nand clamp it to the range Lk+1 \u2208 [1, 10]. We perform the line search every Lk+1 steps. This reduces the extra compute needed from roughly 30% to approximately 3% for longer runs."}, {"title": "E. Practical Considerations", "content": "In the original Armijo line search implementation a few outliers dominated the determination of nk as shown in Figure 2. The hyperparameter c was set with this in mind. In our experiments we found good values for c to be in the range c \u2208 [0.3, 0.5]. For all our experiments we used c = 0.3 (compared to c = 0.1 for the original Armijo line search criterion).\nFurthermore, we tuned the hyperparameter \u03b23 \u2208 [0.9, 0.999] from Eq. 5 on a variety of datasets. We found that although performance is robust to the choice of \u1e9e3, a value of \u1e9e3 = 0.99 is the best general choice. Larger \u1e9e3's result in slower adaptation of the step size to the loss landscape, but less susceptibility to noise."}, {"title": "IV. EXPERIMENTAL APPROACH", "content": "In this section, we detail our experimental design to investigate the performance of our proposed optimization method. We utilize datasets, model implementations and weights from the Huggingface library [11], the pytorch datasets library and the nanoGPT [12] github repository."}, {"title": "A. Candidates", "content": "A quick overview of all candidates we are evaluating can be seen below:\n\u2022 SGD with tuned learning rate and learning rate schedule\n\u2022 ADAM with tuned learning rate and learning rate schedule\n\u2022 SGD + SLS, see Section II-A\n\u2022 ADAM + SLS, see Section II-B\n\u2022 SGD + SaLSa, see Section III\n\u2022 ADAM + SaLSa, see Section III\nAs a baseline comparison we evaluate the ADAM and SGD optimizers with a cosine decay with warm starting for 10% of the total training time. For NLP tasks this warm starting and cosine decay is common practice. For the image tasks we compare to a flat learning rate as done in [2].\nWe take the peak learning rate for ADAM on natural language tasks \u03b7 = 2\u00b710\u22125 from the original Bert paper by [13], which presents a good value for a variety of classification tasks, including the Glue [14] tasks upon which we are evaluating. We found the value for the peak learning rate for SGD on the NLP task \u03b7 = 2\u00b710\u22123 using a grid search.\nWe found the value \u03b7 = 1\u00b710\u22123 for image classification for ADAM using a grid search. The same procedure resulted in \u03b7 = 1\u00b710\u22121 for SGD."}, {"title": "B. Datasets and Models", "content": "To evaluate an optimization method it is necessary to perform large scale runs of complex real world datasets and tasks. This is especially important as many optimization methods perform well on small scale or clean theoretical tasks, but fail to perform well on real world data.\nNatural Language Processing - Transformers\nWe consider a common scenario in natural language processing, where a large pre-trained language model (in our case Bert [13]) is fine-tuned on a small to medium sized dataset, similar to previous papers [6]. The Glue dataset, introduced by Wang et al. (2018) [14], comprises a diverse set of widely recognized natural language processing (NLP) classification tasks. This dataset serves as a benchmark for evaluating common NLP capabilities. The versions of the datasets utilized in this study are sourced from tensorflow-datasets 4.0.1.\nTo be specific, of the Glue collection [14], we use the datasets Stanford Sentiment Treebank SST2, Microsoft Research Paraphrase Corpus MRPC, Stanford Question Answering Dataset QNLI, and the Multi-Genre Natural Language Inference Corpus MNLI. These datasets range from 500 400.000 training samples and represent a variety of fine-tuning tasks.\nAs a further evaluation metric for language models, we fine-tune GPT-2 [15] on the Shakespeare dataset as described in [12] implementation.\nImage Classification - Convolutional Neural Networks\nIn image classification common evaluation datasets are CIFAR10 and CIFAR100 [16], both being small scale (50.000 samples, 32x32 resolution). To obtain more reliable results we also compare on ImageNet [17] which consists of roughly 106 samples. We use the ResNet34 [18] architecture without pre-training for small datasets and ResNet50 for ImageNet."}, {"title": "C. Implementation Details", "content": "The following details are the same for all experiments:\nAll models are trained 5 times and the averaged metrics are reported in Tables I and II. The learning curves as well as standard errors are visualized in Figures 5 and 6.\nA Bert [13] model was trained on the NLP dataset with the following hyperparameter choices: Five epochs training time on each dataset. The pooling operation used in the Glue experiments is [CLS]. The maximum sequence length is set to 256 tokens. The batch size used during training is 32.\nFor the image datasets CIFAR10 and CIFAR100 [16] ResNet34 [18] was used. For the ImageNet [17] dataset the ResNet50 [18] architecture was used, a larger architecture was used due to the increased amount of complexity and size of the dataset. The batch size used during training is set to 128 for Cifar10 and Cifar100 and 256 for ImageNet. We applied preprocessing as described in the ResNet paper by [18]. Models were trained on CIFAR10 and CIFAR100 for 100 epochs and on ImageNet for 12 epochs.\nThe computing time for all experiments was roughly 65 days on an A40 GPU. Roughly 15 of these days were used for the NLP tasks and 50 for the image datasets."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we will describe the results of our experiments. We compare the 6 candidates as described in Section IV-A. All metrics reported are average values obtained using 5 training runs. All accuracies presented below are computed based on the validation sets. The losses presented are computed on the training sets and are smoothed using an exponential moving average. The shaded regions surrounding each line denote the standard error for each experiment.\nIn summary, we observe the SaLSa methods having on average an 1.5% advantage on accuracy and a 50% lower average log loss at end of training."}, {"title": "A. Natural Language Processing - Transformer Experiments", "content": "In our NLP experiments, as shown in Figure 5 and in the Appendix for GPT-2 and SST2, we have observed that, on average, ADAM + SaLSa achieves a lower final loss compared to ADAM, ADAM + SLS, and SGD + SLS. However, this improvement in loss does not always translate to a significant difference in the accuracy metric. ADAM + SLS and ADAM + SaLSa perform similarly in terms of accuracy, but both outperform ADAM and SGD + SLS on average. Note that for similar final losses, the convergence rate of SaLSa is generally faster than that of ADAM, as depicted in Figure 5."}, {"title": "B. Image - Convolutional Neural Networks Experiments", "content": "In our image experiments, we have observed that the combination of ADAM + SLS or SGD + SLS yields good results for CIFAR10 and CIFAR100, but performs poorly for ImageNet, as depicted in Figure 6. We attribute this outcome primarily to stability issues. Specifically, ADAM + SLS occasionally produces excessively large step sizes \u03b7, or it diminishes them to unreasonably small values \u03b7 \u2264 10\u221210. On the other hand, our enhanced approaches ADAM + SaLSa and SGD + SaLSa, do not encounter these problems and on average deliver the best performance among all methods."}, {"title": "VI. RELATED WORK", "content": "The refinement of deep neural network optimization has been a focal point of investigation within the realm of machine learning. Various techniques and optimizers have been proposed, including but not limited to SGD [19], Adagrad [20], RADAM [21], ADAMW [22], RMSprop [23] and Adam [8]. However, selecting the most suitable optimizer remains a challenge, and there is no clear consensus on the best according to [1].\nIn a recent study by [1] on the topic of optimization methods, it was observed that while there are various optimizers available, there is no definitive best optimizer. The authors highlight that the introduction of more optimizers does not necessarily lead to improved results, and therefore, alternative approaches should be explored to enhance optimization techniques. One such approach with great potential is automatic step size determination. One of the most common approaches for this are line search methods, which hold promise for enhancing optimization processes [2], [5], [24], [25].\nIn this work we particularly build upon [2]. The Armijo line search method introduced there, offers several important advantages over other optimization techniques: no hyperparameter tuning of the learning rate, faster convergence rates and better generalization. A significant drawback of this method, along with other line search approaches, is that it requires at least an additional forward pass per update step. Consequently, this leads to an increase of approximately 30% in computational resources required per training step.\nRecent work has shown that Transformers are highly sensitive to the choice of learning rate and learning rate schedule schedule during training [21], [26]. To address this issue, various approaches have been proposed, such as RADAM [21] and warm starting. We have showed previously that our methods are able to train Transfomers despite these challenges. Other related work includes [27] which studies the correlation between batch size and learning rate, [28] which shows theoretical and practical results for training using higher order gradients and [9] which investigates why Adam, upon which we focus, is so effective at training the Transformer architecture.\nThe optimization of neural networks continues to be an important area of research, to which, the development of effective and reliable line search methods, which work on sensitive architectures such as transformers or large scale convolutional neural networks, constitutes a significant contribution."}, {"title": "VII. CONCLUSION", "content": "We have introduced SaLSa, an automatic step size selection method and built a general purpose optimizer on top. We have compared its performance against tuned learning rates for larger datasets and architectures than previously done in optimizer evaluations for line search methods. The SaLSa optimizer performance compares favorably in these cases, while requiring no tuning of learning rates, or code overhead, as well as minimal compute overhead. We recommend its use as a first choice for training deep neural networks in these domains and publish the code as a Python package.\nThe source code is open-source and free (MIT licensed) software and available on github"}]}