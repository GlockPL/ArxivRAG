{"title": "Predicting Trust In Autonomous Vehicles: Modeling Young Adult Psychosocial Traits, Risk-Benefit Attitudes, And Driving Factors With Machine Learning", "authors": ["ROBERT KAUFMAN", "EMI LEE", "MANAS SATISH BEDMUTHA", "DAVID KIRSH", "NADIR WEIBEL"], "abstract": "Low trust remains a significant barrier to Autonomous Vehicle (AV) adoption. To design trustworthy AVs, we need to better understand the individual traits, attitudes, and experiences that impact people's trust judgements. We use machine learning to understand the most important factors that contribute to young adult trust based on a comprehensive set of personal factors gathered via survey (n = 1457). Factors ranged from psychosocial and cognitive attributes to driving style, experiences, and perceived AV risks and benefits. Using the explainable AI technique SHAP, we found that perceptions of AV risks and benefits, attitudes toward feasibility and usability, institutional trust, prior experience, and a person's mental model are the most important predictors. Surprisingly, psychosocial and many technology- and driving-specific factors were not strong predictors. Results highlight the importance of individual differences for designing trustworthy AVs for diverse groups and lead to key implications for future design and research.", "sections": [{"title": "1 INTRODUCTION", "content": "Potential benefits of Autonomous Vehicles (AVs) range from reduced traffic, less crashes, and lower environmental impact to improvements in efficiency and risk management [39]. However, widespread adoption of AVs is impeded by a lack of rider trust in AV decisions and the real-world consequences of adoption [55]. AVs are a clear case of a more general lack of public trust in Artificial Intelligence (AI), spanning many domains [12].\nThere is substantial theoretical and experimental evidence supporting that a person's trust in AVs is influenced by their individual traits and experiences [40, 52, 60]. To develop more trustworthy AVs, it is therefore necessary to understand which specific factors influence trust judgments, and by how much. This can empower AV designers to address the unique needs of different individuals.\nMuch prior research has examined personal traits that may predict trust and adoption attitudes toward AVs [15, 23, 28, 37, 63, 67, 78]. However, many of these studies lack comprehensive coverage of relevant characteristics, limiting their utility. By focusing on only a narrow set of traits, potentially critical factors are left unexamined. For instance, several personal factors discussed by theoretical frameworks and trust research in other AI domains have not yet been explored in the context of AVs, including certain driving behaviors, cognitive traits, and institutional attitudes among others [40, 52, 60]. Further, studying traits in isolation or in small sets prevents a clear understanding of the relative importance of different factors in trust development. While various traits - ranging from demographic characteristics to personality, risk assessment, and experience with AVs \u2013 have been associated with trust, without a comprehensive"}, {"title": "2 RELATED WORK", "content": "A concrete result of securing a more comprehensive understanding of how traits impact trust (in particular, which traits are more important) is designing AVs that can meet the specific needs of diverse individuals, as opposed to taking a one-size-fits-all approach to human-AV interaction. For example, explainable AI (XAI) explanations meant to increase trust via transparency for \"black box\" AI (and AV) decisions can provide tailored communications to meet the needs of specific groups [33, 52, 64]. Tailoring communications to individual users has been shown to more effectively address informational needs, helping to calibrate system reliance to appropriate and optimal levels [54, 73, 88, 97]. Knowing who has what needs to address can guide the creation of more accessible and equitable AVs [36], including tailored educational campaigns [81] and inclusive policy guidelines[47]. Without meeting the needs of specific individuals or subpopulations, a future with ubiquitous AV adoption cannot be realized.\nIn the present study, we use survey methods (n = 1457) to measure a broader range of variables than any previous research on this topic, bridging a diverse spectrum of driving and non-driving related research. We include variables ranging from cognitive and psychological traits to cultural values, driving style, and technology attitudes. We also include a wide range of specific risks and benefits in AVs, as these may be important contributors to trust perceptions [9]. Including such a wide range of variables allows us to build a more nuanced and comprehensive understanding of the factors that influence trust than has been previously possible. We are able to capture the complex interactions between variables and assess the relative importance of the factors in our analysis by using machine learning to predict trust in AVs, and then applying SHapley Additive eXplanations (SHAP) [71] to \"peek beneath the hood\" and assess how important different factors were to our models' predictions. Utilizing these advanced analysis methods enhances predictability and builds deeper insights into how a wide range of traits relate to trust judgements.\nOur research also highlights the importance of isolating subpopulations when predicting trust in AVs. Most studies in this area analyze data across general populations [15, 28, 37, 63], which can obscure unique traits and concerns relevant to specific groups. For example, the common finding that age predicts a person's trust in AI may be more of a superficial correlation rather than a true driving factor [1, 7, 24]. Though age may be helpful for predicting trust, age often correlates strongly with underlying variables, such as technical expertise, familiarity with Al systems, and openness to innovation [19, 92]. These underlying variables are far more informative of why people of different ages may trust Al systems differently, giving insight into how to design better systems for them. Subpopulation-focused approaches have proven effective in other areas of computing, such as online interventions for social media, where deeper cognitive traits outperformed broader indicators like age or political affiliation in predicting behavior [48, 53]. We apply similar techniques to this work by focusing on a specific subpopulation - in this case, young adults. By holding broader factors constant, we can identify the more nuanced, informative factors that shape trust in this subpopulation.\nWe sought to prioritize early adopters and the next generation of transportation users, as these groups will likely play a key role in the adoption and shaping of AV design. Young adults, in particular, fit this profile due to their high comfort with new technologies [24] and their potential to influence future transportation trends [14]. By studying young adults, we can better understand their unique perspectives and concerns a critical step towards aligning AV systems with the needs of these early adopters. Our findings not only inform AV design and communication strategies"}, {"title": "2.1 Personal Factors Impacting Trust in AVs", "content": "Individual Differences - Hoff and Bashir's [40] influential analysis of modern human-AI trust research posits that dispositional factors including culture and age, as well as situational and learned factors such as expertise and prior experience, impact how much a person will trust and rely on automated systems like AVs. According to Hoff and Bashir, these factors influence the system in conjunction with the context of use (e.g. driving from location A to location B) and the system's demonstrated performance in the moment. Similarly, Kaufman et al. [52] propose a framework for human-AV interaction based on situational awareness and joint action, emphasizing the role personal traits and contextual characteristics play in guiding interactions and achieving goals. Lee and See's [60] review of trust foundations posits that trust success is shaped by a trustor's predispositions and environment. By all three accounts, personal traits clearly influence trust in the system.\nDemographics \u2013 Prior work examining demographic factors like age have shown that younger individuals tend to adopt autonomous systems earlier than older individuals [1, 45], however, differences in age may be caused by underlying variables, including familiarity with AI, concern over risks, and self-efficacy [19, 78, 92]. Gender differences commonly show that men are more likely to adopt AVs than women [45, 78], however, these effects may also be moderated by underlying variables like risk preferences [78] or vehicle anxiety [41]. Education may also play a role: people with more education may be more willing to adopt autonomous vehicles [44]. Recent work has even shown that people who lean politically liberal in the United States may be more willing to adopt AVs - and see greater benefit to adoption - than those who lean politically conservative, though this finding too may be motivated by underlying driving variables like cultural values and belief systems [75, 78]. Socioeconomic status (SES) is understudied in the realm of AV trust prediction; SES may be important given its negative association with interpersonal trust in other domains [91]. In the present study, we measure all of these demographic variables: age, gender, education, political orientation, and socioeconomic status to see how important they are for our trust models. Our assessment of trust"}, {"title": "2.2 Predicting Trust Using Machine Learning", "content": "Using similar methods as the present study, Ayoub et al. [9] used demographics, knowledge, feelings, and behavioral intentions to predict how a person responded to the question \"how much would you trust an autonomous vehicle?\" They included general ratings of the overall \"risk\" and \"benefit\" of AVs, however, they did not specify which risks or benefits, nor did they gather precise concerns. Nonetheless, they found that the risk and benefit scores were the highest contributors to their XGBoost model. We seek to build upon this effort by incorporating a much larger range of features, a more comprehensive measure of \"trust\" based on modern trust theory, and a more specific analysis focusing on a particular user group of interest (young adults). In addition, we include a wide range of specific risks and benefits based on prior work, allowing us to understand which risks and benefits may be most important.\nIn sum, the integration of machine learning and explainable AI techniques like SHAP for predicting trust in AVs provides a significant opportunity to advance our understanding of trust development in complex sociotechnical systems like autonomous driving [29]. These models not only offer high predictive accuracy but also provide valuable insights for datasets with large numbers of variables not available using other methods. As AV technology continues to evolve, the ability to predict and enhance user trust will be crucial for successful adoption and widespread acceptance of AVs. In the present work, we seek to build on prior research to advance the study of individual differences in trust, addressing knowledge gaps that will allow researchers and designers to build AVs that can meet the needs of diverse groups."}, {"title": "3 METHOD", "content": "In the present study, we use survey methods to assess a wide range of personal factors from a pool of young adults. Then, we build models predicting a person's trust in autonomous vehicles from those factors, using a variety of machine learning techniques. Following, we use SHAP [71] to make the top performing models explainable, giving us measures of feature importance. We add additional nuance to our study with ablation experiments knocking out specific feature groups from the models in order to derive greater insight into how factors contribute to AV trust. Figure 1 describes the overall modeling process. To look at trust differences between the \"high\" and \"low\" groups at the level of each factor, we also perform a more traditional descriptive analysis using Kruskal-Wallis tests as a means to provide comparison to our ML results."}, {"title": "3.1 Participants and Data Collection", "content": "A total of 1457 participants completed the study and passed all quality control requirements. The survey was designed and deployed using Qualtrics [84], taking a median of 17.9 minutes to complete. Recruitment occurred via email lists and via SONA,\\u00b9 an undergraduate study pool where students are granted study credit for participation. As such, the vast majority of participants were enrolled at or affiliated with a large undergraduate university in the United States. The mean age of the study sample was 20.7 (SD = 2.4), with ages capped between 18 and 35 years. The sample was 73.9%\nfemale. Table 1 shows distributions of age and education for our sample. Only participants who passed three attention checks were included. As an additional means of quality control, we screened out participants with low variability in survey responses (e.g. used the same pattern of answers) and those who had unrealistically low completion time. As a result of this quality control process, we are confident that the data used in our analysis is of high quality."}, {"title": "3.2 Survey Design", "content": "3.2.1 Survey Scales and Questionnaires. The survey was designed to be as comprehensive as possible while considering limits on survey duration and participant attention. Table 2 shows factors related to driving-specific attitudes and"}, {"title": "3.3 Model Development", "content": "After deriving an exploratory understanding of the dataset and its features using descriptive statistics and Kruskal-Wallis tests (discussed in Section 4.1), we investigated the potential for machine learning systems to predict users trust in AVs. We discuss our approach to designing such a model below.\nFeature Selection \u2013 Our survey data resulted in a set of features that could be used as predictors for our models predicting user trust. For survey questions that contributed to validated composite scores (like Big 5 personality or CVSCALE culture values), only the composites were kept in the dataset. For measures that provide meaning on their own (for example, each risk/benefit or specific driving behaviors), these were included individually as their own feature. Since this data comes from surveys, we took multiple steps to achieve a concise feature set and mitigate the possibility that some features could be correlated with each other. Following recent work around modeling personality traits [87], we first filtered features using their Variance Inflation Factor (VIF) with respect to the raw trust scores, using a threshold of 10. This helped us detect and reduce multicollinearity. To further reduce multicollinearity, next we ran feature selection through Lasso regression [96] on the raw trust scores. This resulted in a final set of 130 features, which was used for all our machine learning experiments and is henceforth referred to as the full feature set.\nMachine Learning Pipeline - Our machine learning pipeline consisted of two components: (1) feature transfor- mation and (2) classification modeling. For effective modeling, we reduce dimensions through Linear Discriminant Analysis (LDA) [95] on the feature set to attain 10 transformed feature vectors. Since the number of \"high\" and \"low\" people in the dataset was not perfectly balanced, we over-sample the minority in the training data to match the number of majority samples. We used the Simple Minority Oversampling Technique (SMOTE) [18] to achieve this synthetic balanced dataset, a common method used in similar ML analyses.\nNext, we applied several classical machine learning models on the derived feature set in order to find the best performing method. Given our task of binary classification (\\u201clow\\u201d vs. \\u201chigh\\u201d trust), we used models similar to [11] to achieve a balance of interpretability and performance. Specifically, we applied Logistic Regression (LR), Support Vector Machines (SVM) with linear and radial kernels, Decision Tree Classifiers (DTC), Random Forest (RF) and Gradient-Boosted Decision Trees. We used XGBoost implementation [20] for the gradient-boosted trees.\nTraining Experiments We conducted cross-validation experiments to quantify the performance of different models. The pipeline was evaluated across a five-fold split on the overall dataset. For each instance, the data was first normalized using Standard Scaling based on the training data and then fed into the machine learning pipeline. Each type of model was evaluated using balanced accuracy. We also tracked the precision, recall and macro-F1 scores for each iteration given their robustness to imbalance [83]. For each model, we experimented with the various parameters possible for that model type. Final model selection was conducted on the mean balanced accuracy performance across all cross-validation folds."}, {"title": "3.4 Model Investigations", "content": "After training different possible models, our goal was to understand how features contribute to their overall performance. This explainability brings insight into the most important personal factors that predict a person's trust judgement. To achieve this, we used SHapley Additive exPlanations (SHAP) [71] to investigate dimensions indicative of trust. SHAP scores are game-theoretic measures of importance for how each feature contributes to the overall prediction, providing local interpretability of the pipeline. SHAP not only allowed us to understand which features are important, but also how important via a relative importance coefficient. In our training process, we tracked SHAP scores for each feature"}, {"title": "3.5 Deeper Feature Experiments (Systematic Ablation Studies)", "content": "We conducted a series of experiments to derive a deeper understanding of how key features impact our trust models. Given how the risk and benefit dimensions used in this study showed to be the top predictors of trust in our full model (Section 4.2.2), we conducted two follow-up ablation studies to test (1) the accuracy of predicting trust from risk and benefit features only, and (2) the impact of removing risk and benefit features from our full model (leaving only other types of features like psychosocial and driving). For each ablation (systematic elimination) study, we followed a pipeline and training process similar to Section 3.3 and report the metrics averaged across folds. We compare them to the scores found in the larger full feature model; we consider the net decrease in performance to be an indicator of importance."}, {"title": "4 RESULTS", "content": "We present results from our survey of 1457 young adults. First, we show descriptive statistics and Kruskal-Wallis tests comparing high and low trust individuals, generating initial dataset understanding. Next, we introduce the results of our ML analysis, with model explanations provided via SHAP. Finally, we deepen the ML analysis by performing two ablation (systematic elimination) studies, which compare results with and without risk-benefit contributions."}, {"title": "4.1 Descriptive Analysis: Comparison of Factors Using Kruskal-Wallis", "content": "Descriptive statistics and initial comparisons using Kruskal-Wallis tests for the high and low trust groups can be found in Table 5 (factors with large effect size only) and Appendix A (all factors). Kruskal-Wallis results show if individual predictor variable scores differ between people of high trust and people of low trust. Predictably, given the large sample size, nearly all group differences were significant. To give a more functional comparison, we report effect size (Cohen's D) and provide an interpretation based on accepted standards [31].\nWe find 8 variables to have large effect sizes: Overall Risk-Benefit, Ease of Use, Reduce Accidents, Trust in Tech Companies, Increase Fun, AV Feasibility, Improve Efficiency, and Reduce Traffic. Each of these may be important for predicting how much a person trusts AV, and indeed, each of these factors were in the top 20 most important factors for our full feature ML model. However, we note that in the Kruskal-Wallis case, each factor was tested individually, limiting our ability to understand complex patterns and non-linear relationships from the larger, combined set of predictors. Using the more sophisticated and predictive machine learning approaches described next, we get a more nuanced understanding of how each variable contributes to a person's trust assessment."}, {"title": "4.2 Machine Learning Analysis Using The Full Feature Set", "content": "4.2.1 Model Comparisons and Best Performance. We conducted a series of experiments to identify the best performing overall models using the full feature set, optimizing for different sets of parameters. We report the best achieved scores for each model averaged across all five cross-validation folds in Table 6.\nWe find that all models performed well in general, strengthening support for the hypothesis that personal factors can predict trust dispositions for AVs. Overall, Random Forest produced our most predictive model for trust using the full feature set, resulting in 85.8% balanced accuracy. We also note that precision and recall are higher than chance and have similar magnitudes. This implies that the model is intrinsically robust and consistent across both high and low trust groups. The area under ROC curve observed in the best performing model (0.934) shows that our model pipelines and features can create a strong class separation and are indifferent to label imbalance.\n4.2.2 Explaining the Full Feature Model using SHAP. Once the most predictive models were made, we identified the most important features via SHAP. We find that Overall Risk-Benefit, Reduce Accidents, Ease of Use, Increase Fun and Losing Control were the most important factors that influence trust in AVs (Fig. 2). Notably, 12 of the top"}, {"title": "4.3 Ablation Studies: Systematic Isolation and Elimination of Risk-Benefit Factors", "content": "Using our full feature model, risk and benefit-related factors were consistently among the most important for predicting a person's trust level. As such, we conducted two ablation (systematic elimination) studies to derive a deeper understanding of how risk and benefit factors specifically impact trust predictions. We followed a similar approach as we did while building and evaluating the full feature model (Section 4.2). The first ablation study looks at only the risk and benefit factors from Table 4 in isolation (henceforth referred to as the feature subset). The second ablation study is this feature subset's complement, which instead eliminates all risk-benefit factors from the full feature model. Table 7 shows a summary of performance between these two ablation models compared to our full model.\nWe find that the full feature model outperforms both ablation models. The difference between the accuracy of the full feature model (85.8%) and the full feature set without the risk and benefit factors (73.8%) is quite large, dropping 12%. We find much smaller differences between the full feature set and the isolated feature subset (84.6%), losing only 1.2% accuracy. These results imply that risk and benefit factors are very important for the prediction of trust in AVs.\n4.3.1 Feature Subset: Risk-Benefit Features Only. The first ablation study, which we call the feature subset, consisted only of risk and benefit measures. As discussed, the performance of this model was quite high. Investigating the top performing features in this subgroup using SHAP allows us to derive a better understanding of how different risks and benefits impacted our trust prediction."}, {"title": "4.3.2 Full Model Without Risk-Benefit Features", "content": "We also investigated the effect of eliminating the feature subset from the full feature model. This second ablation experiment gives us insight into two additional areas of inquiry. First, if we do not have access to a person's risk-benefit assessment, can we still predict trust from other personal factors? Given the plethora of prior research on dispositional traits and attitudes discussed in Section 2.1, we hypothesized that this answer should be yes. Second, if we can predict trust from other personal factors, what factors are most important?\nTo answer these questions, we ran experiments similar to the development of other models in this study, to find the best possible performance with this group of features. Unlike the full feature model and the feature subset (ablation experiment 1), we found that balanced model accuracy of this second ablation study was quite a bit lower, dropping to 73.8%, a full 12% dip compared to the full feature model. This further emphasizes the importance of risk-benefit features.\nThough 73.8% is not excellent predictive accuracy, from a theoretical lens it is still predictive enough to draw initial insights on how non-risk-benefit features impact a person's judgement of high and low trust. We found that Trust in Tech Companies, Trust in Automakers, AV Feasibility, Know AV Purpose, and Trust in Government Authorities were the strongest predictors (Fig. 6); higher values of each were associated with higher trust (see Fig. 7)."}, {"title": "5 DISCUSSION", "content": "We provide evidence that young adult trust in autonomous vehicles (AVs) can be predicted via machine learning. We used a comprehensive set of personal factors with 130 distinct input features, derived from survey. We compared the predictive power of several machine learning models, all of which performed quite well. Random forest models performed best - 85.8% (Table 6) \\u2013 at categorizing people into high and low trust groups. By applying the explainable AI technique SHAP to our best performing models, our analysis not only shows which factors were most predictive of trust, but also how each feature uniquely contributed to the model's conclusion. In the following sections, we contextualize our findings within the scope of human-AV interaction research and provide insights for how our analysis can be used to build trustworthy vehicles for diverse groups. Many of these insights may generalize to other Al-driven domains or for groups beyond young adults.\nA general principle that resulted from our study is that perceptions of risks and benefits are the most important factors for predicting trust. In our full feature model, 12 of the 20 most important features (based on mean SHAP value) were related to a person's assessment of the risks and benefits of using AVs. Using ablation studies, the importance of risk-benefit perceptions was confirmed: risk and benefit factors alone had similar predictive ability as our full model, whereas a knockout model without risk and benefit factors performed far worse. The importance of risks and benefits has been highlighted in the past [9, 39, 102], however, the present study is the first to assess the relative importance of specific risk and benefit perceptions compared to other types of personal factors for predicting trust.\nWe find a person's overall assessment of the risks and benefits of an AV was the most highly predictive feature for predicting trust level (importance rank 1) in both the full feature model and feature subset. This is unsurprising, as we would assume that the ratio of benefits to risks as discussed at length in cost-benefit analyses and (ir)rational decision-making [97] - would impact a person's decision to trust an AV. Indeed, Ayoub et al. [9] also found that assessment of benefits and risks were the most important factors predicting trust in their model, though they do not specify which risks and benefits these may be. In the present study, we are able to articulate precisely which risk-benefit factors are most important. From an AV design perspective, the importance of overall risk-benefit assessment on trust judgements implies that addressing high priority risks and benefits using strategic design elements can drastically impact a person's trust judgements. In the following sections, we will discuss risks and benefits, respectively.\nRisks associated with system performance and usability failures were the most consequential - Six of the top 20 most important features to our full model were related to risks. These included concerns with: Ease of Use (rank 3), Losing Control (5), Lacking Understanding of AV decision-making (6), System Failures (9), Performance of AVs across varied terrain (16), and Hacking (19). High ratings of these concerns were associated with lower trust.\nThese concerns highlight areas of priority for human-AV interaction design and research. For instance, concerns over ease of use, giving up control, and lack of decision-understanding are fundamental to the development of explainable Al systems for AVs [57, 76], which aim to increase transparency and usability with complex AI-based systems. Other specific concerns with vehicle performance and security \\u2013 such as concern with equipment failures, performance, and hacking - may also be addressed through a mix of UI enhancements and education campaigns (assuming that the systems are, in fact, performing well and secure). These may include visualizations reassuring riders that the vehicle is safe, private, and operating within the bounds of its ability. Importantly, elements should be designed to empower a rider by providing decision agency, not to mislead them by artificially minimizing risks.\nBenefits need to add value beyond what can be achieved by human-controlled driving - Five of the top 20 features were related to benefits. Perceiving that AVs can Reduce Accidents (rank 2), Increase the Fun of driving (4),"}, {"title": "5.1 Implications for Trustworthy Human-AV Interaction Design and Research", "content": "Our results have important implications for the design and study of autonomous vehicles. In this study, we successfully predicted AV trust from the personal factors of young adults. As expected, some features in our model were more important than others. We can use these results as a basis for personalized and adaptable designs based on specific factors of importance, as well as specific insights for trustworthy Explainable AI (XAI) design, communication and education campaign guidelines, and policy suggestions.\nExplainable AI and Transparency - The high importance of concerns around ease of use, giving up control, and lack of understanding around AV decision-making highlights the critical need for continued research and design efforts on AV explainability and explainable AI more generally. From a design perspective, our results echo prior work on the importance of XAI (see Section 2.1), particularly for research that supports clear, real-time explanations for AV decisions. A special focus should be put on XAI explanations that are understandable by riders from diverse communities with a wide range of backgrounds and expertise levels. Failure to satisfy concerns over ease of use, vehicle performance, and safety and privacy may result in refusal to ride with an AV.\nPrioritize Safety and Security Features - One area of focus for XAI in AVs should be on effectively communicating driving performance metrics, particularly when they can reassure a rider that the AV is safe and protected from both physical harm and privacy risks. Explanations for what action an AV is taking and why [57] may give a sense of control and agency back to a driver. These may be effectively paired with visualizations emphasizing that the AV is performing properly and trained well for the driving environment, mitigating concerns over performance adaptability and calibrating trust to an appropriate level. Visualizations and communications should be realistic and not misleading, as under-communicating performance errors could lead to over-reliance.\nCommunicate Unique Benefits Effectively - Given the large number of potential benefits that AVs offer [27] and the importance of believing in these benefits to trust judgements, our research suggests that educational campaigns and mid-drive interactions should strive to highlight benefits to increase trust and adoption. In particular, emphasizing the added value AVs can bring beyond human-controlled driving may be most effective, as the alternative to AV adoption are non-AVs (status quo) which still function quite well in many contexts. Specific benefits which should be prioritized include reducing crashes and traffic, increasing driving efficiency, and increasing fun.\nAnthropomorphizing AV communications may be beneficial, but should be done with caution \u2013 We found that people who conceptualized AV decision-making as closer to human decision-making had higher AV trust. This alludes to the power that humanizing AV communications may have on increasing trust, suggesting similar approaches to AV explanations could be potentially useful. For instance, expert-informed explainable Al has been studied in the past with promising results [10, 51, 82, 90]. This approach should be taken with caution, however, as certain theoretical frameworks such as the Computers Are Social Actors (CASA) paradigm [79] show that people often treat computerized systems the way they would treat other humans, meaning systems which are too humanized may be assumed to have human-like biases and flaws in judgement and decision-making.\nPersonalization and tailoring of AV communications and behavior may be a promising future direction - Key traits found in our current study and in prior work emphasizes that people differ in their underlying traits, attitudes, and experiences, and that these differences impact trust. In the present study, we highlight some of the most important traits that should be looked at for predicting a young adult's trust in AVs. This work can be used as a foundational step towards tailoring AV communications and behaviors to the specific individual who may be riding in the AV. Once"}, {"title": "5.2 Limitations and Future Work", "content": "This study was not without limitations. First, our study leveraged survey methods, which \\u2013 though standard practice \\u2013 may be subject to bias as it is based on self-reported data. To mitigate this concern, we employed a large sample size, used validated questionnaires, and replicated variable selection from prior work whenever possible. Self-report concern may be particularly strong with measurements of trust, as (though quite common) self-reported trust may not be as effective as measuring trust via behavioral reliance [40]. To mitigate this risk, we used a composite trust measure to capture many aspects of a trust judgement. Future work may seek to use behavioral trust measures. We focused on young adults, a majority of whom were enrolled at an undergraduate institution in the United States. As such, results presented in this study may not widely generalize. Future work should replicate on other populations if results in other specific settings are sought. Finally, testing the design implications through actual implementation of the guidelines outlined in Section 5.1 will be necessary to assure their validity."}, {"title": "6 CONCLUSION", "content": "In this study, we predicted young adult trust in autonomous vehicles from a comprehensive set of personal factors. Using machine learning and explainablity techniques, we uncovered which factors were most predictive of trust, and how each feature uniquely contributed to the model's conclusion. We found that perceptions of AV risks and benefits, attitudes toward feasibility and usability, institutional trust, and prior AV experience were the most important trust predictors. A person's mental model also played a role, where those who believed AV decision-making as closer to human decision-making were more trusting. Contrary to previous findings, psychosocial factors \\u2013 including personality, self-esteem, risk perception, and need for control as well as most technology and driving-specific factors, including driving style, driving cognitions, and technology self-efficacy - were not strong predictors of trust. Our results build a new understanding on how personal factors can be used to predict trust in AVs. We conclude our discussion with a set of design, research, and policy implications that can be used to improve future trust and adoption of AVs that can meet the needs of diverse user bases."}]}