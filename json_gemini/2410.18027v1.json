{"title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment", "authors": ["Jiwoo Hong", "Noah Lee", "C\u00e9sar Rodr\u00edguez", "Rodrigo Mart\u00ednez-Casta\u00f1os", "James Thorne"], "abstract": "Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability, along with extensive analyses on off-the-shelf RMs. We release the code\u00b9, model and data\u00b2.", "sections": [{"title": "1 Introduction", "content": "Recent advances in reinforcement learning with human feedback (RLHF) as a large language model (LLM) post-training technique (Christiano et al., 2017; Ziegler et al., 2020) highlight the importance of having high-quality data (Wang et al., 2024f; Dubey et al., 2024) and reward model (RM) (Ethayarajh et al., 2022; Gao et al., 2023; Ji et al., 2023; Wang et al., 2024a,e). Leveraging synthetic data has contributed to building stronger English RMs due to their efficiency and scalability (Cui et al., 2024; Wang et al., 2024b; Zhu et al., 2024). Nevertheless, adopting RMs for non-English languages is heavily understudied. While LLM-as-a-Judge can be used as a generative reward model for multilingual RLHF settings (Son et al., 2024), generative RMs have been shown to underperform traditional RMs (Lambert et al., 2024; Wang et al., 2024b). Meanwhile, Wu et al. (2024) empirically demonstrates the possibilities of cross-lingual transfer in RMs, but the findings were limited to simple tasks and encoder-decoder models.\nIn this paper, we show that RMs trained on English-only datasets (i.e., English RMs) display strong cross-lingual transfer when built on top of multilingual pre-trained language models (MLMs). We first demonstrate the cross-lingual transfer of English RMs by consistently outperforming target language RMs in Multilingual RewardBench. Then, we explain it with two reasons: 1) English preserves representations of the initial MLMs (Section 3.1), and 2) representations of MLMs inherently have a strong understanding of languages (Section 3.2), concluding that RMs should preserve representations of MLMs for generalizability. Additional analysis of off-the-shelf RMs supports our findings by both classifier and generative RMs based on MLMs having strong cross-lingual transfer. Finally, multilingual alignment experiments exhibit the propagation of strong cross-lingual transfer in English RMs to downstream usage, having an average win rate increase of 9.5% across four non-English languages."}, {"title": "2 English as a Lingua Franca in RMs", "content": "We empirically verify the cross-lingual transfer in reward models (RMs) trained with different languages, thereby showing that the English preference data is a lingua franca in reward modeling."}, {"title": "2.1 Background", "content": "Cross-lingual transfer Training multilingual language models (MLMs) at scale has shown to incur cross-lingual transfer in both encoder-only (Devlin et al., 2019; Conneau et al., 2020; Chi et al., 2022) and encoder-decoder (Xue et al., 2021) transformer architectures. Recently, studies revealed the implications of cross-lingual transfer in decoder-only models as well (\u00dcst\u00fcn et al., 2024; Wang et al., 2024c); however, they were limited to generative tasks (Zhang et al., 2024) or downstream alignment-tuning only (Dang et al., 2024).\nReward modeling Reward models are trained as a classifier (Christiano et al., 2017) to return a scalar value $r_{\\theta}(\u00b7)$ with the objective with the Bradley-Terry model (Bradley and Terry, 1952):\n$\\mathcal{L}_{RM} = \\sigma (r_{\\theta}(x, y_w) \u2013 r_{\\theta}(x, y_l))$,\nwith the prompt x and corresponding preferred and dispreferred responses $y_w$ and $y_l$. While crucial in alignment-tuning (Rafailov et al., 2024; Hong et al., 2024; Meng et al., 2024), reward modeling schemes for multilingual usage are still understudied. Motivated by this research opportunity, we study the cross-lingual transfer of English-focused RMs with recent autoregressive models and how it propagates to downstream multilingual alignment."}, {"title": "2.2 Experimental Details", "content": "Dataset We curate a synthetic preference dataset of 86k instances\u00b3 from five representative English preference datasets: SafeRLHF (Dai et al., 2024), WildGuard (Han et al., 2024), HelpSteer2 (Wang et al., 2024e), Offsetbias (Park et al., 2024), and Magpie (Xu et al., 2024b). Using English data, we create four parallel machine-translated versions\u2074, utilizing X-ALMA (Xu et al., 2024a)."}, {"title": "2.3 Results and Analysis", "content": "English RMs show strongest cross-lingual transfer Average reward model accuracy (\"Avg\") in Table 1 shows that English RMs surpass target language RMs in general. Specifically, Llama-3.2-3B gained at least 4.3%, where the cross-lingual generalizability of English RMs is more highlighted than Qwen2.5-3B, which gained at most 4.3%. However, considering that all Qwen-based target language RMs outperform the Llama-based target language RMs, Qwen2.5-3B is shown to be a better model choice for training a language-specific RM.\nReasoning tasks significantly benefit from cross-lingual transfer Generalizability of English RMs is best highlighted in the reasoning tasks (\"Reason\") in Table 1, especially in non-Latin languages. Non-Latin languages, Korean and Chinese, improved significantly in English RMs compared to target language RMs, exceeding 12% and 27% in Chinese, for instance."}, {"title": "3 Analysis on Lingual Tranfer of MLM", "content": "This section provides empirical and theoretical insights on why English is lingua franca in reward modeling, given a multilingual language model (MLM) using two arguments: 1) English acts as a lingua franca in reward modeling because it best preserves the representations of the base model, and 2) representations in MLMs should be preserved since they are inherently effective in language-aware encoding."}, {"title": "3.1 English preserves general representations", "content": "Non-English reward modeling is detrimental to generalizability In general, the generalizability of the downstream model is closely connected to how much the representations are preserved during the fine-tuning (Aghajanyan et al., 2021; Razdaibiedina et al., 2023). We demonstrate this in RMs by ablating over different languages and tasks. We assess the general representation preservation of RMs used in Section 2 by comparing their hidden states against the initial model. To do so, we measure how much the distinct representations are collapsed into similar spaces in Figure 1. In specific, we construct a matrix of the last hidden states $H_{\\theta}(x) \\in \\mathbb{R}^{5 \\times d_{\\text{model}}}$ across five languages using multilingual dataset BeleBele (Bandarkar et al., 2024):\n$H_{\\theta}(x) = \\text{concat}[\\{H_{\\theta}(x_{\\chi_l})\\}_{I \\subset L}] \\in \\mathbb{R}^{|L| \\times d_{\\text{model}}}$,\nwhere $H_{\\theta}(x)$ \u2208 $\\mathbb{R}^{dmodel}$ refers to the last hidden state of the model $\\theta$ for sequence $x_1$ in the language l, but with fixed context. Then, we measure the proportion of the largest singular value in $H_{\\theta}(x)$:\n$f_{\\theta}(x) = \\frac{\\sigma_1}{\\sum_{i=1}^{|L|} \\sigma_i}$, $S = \\text{diag} (\\sigma_1,..., \\sigma_{|L|}),$\nwith S as a singular values of $H_{\\theta}(x)$. Intuitively, having $f_{\\theta}(x)$ close to 1 indicates the hidden states in different languages are homogeneous: i.e., representations are embedded into similar space.\nIn Figure 1, we plot $f_{\\theta}(x)$ with different RMs. English RMs best preserve the representations by staying close to the base instruct model (\"Inst\"). On the other hand, Korean RMs (\"Ko\") tend to deviate the most from the base model, thereby homogenizing the multilingual representations the most. Both observations were more extreme in Llama-3.2-3B.\nGeneral representation preservation is crucial for cross-lingual/task transfer Notably, the proclivity in general representation preservation in Figure 1 aligns with the accuracy in Table 1. Non-English RMs with Llama-3.2-3B tend to introduce stronger representation collapse than Qwen2.5-3B in Figure 1. This aligns with Section 2.3 as Llama-3.2-3B gets more severe degradation using target language RMs, implying the significance of representation preservation in cross-lingual transfer.\nFurthermore, the same tendency holds for cross-task analysis. RewardBench has especially fine-grained divisions under the reasoning category (e.g., Java, Python, Rust, math) compared to other categories. Thus, strong generalization abilities are crucial to achieving decent scores in the reasoning category. Interestingly, English RMs dominate other languages in reasoning despite the fixed data across the languages in Table 1, which strongly supports the significance of representation preservation in cross-task generalization."}, {"title": "3.2 MLM representations are language-aware", "content": "In autoregressive language models (Radford et al., 2019) with tied embeddings (Jiang et al., 2023; Team, 2024a), the logits for next token is:\n$h_t \\cdot E = [||h_t|| \\cdot ||e_i|| \\cos (\\theta_i)]_{i=1}^V,$\nwhere $\\theta_i$ is the angle between $h_t$ and $e_i$. Therefore, the capability of language models in generative tasks is closely related to having good representations (Edunov et al., 2019) that could accurately align with the ideal next token.\nToken embeddings are a good proxy to understand the effectiveness of representations as they imply the imbalance in pre-training corpora (Chung et al., 2024), especially by linguality in this study (Wen-Yi and Mimno, 2023). Thus, we can infer that language models with similar embedding norm distribution across the language will have decoder layers that can return language-aware fine-grained hidden states, which deserve to be preserved for their generalizability.\nMLMs have similar token embedding norm distributions across the language We validate this point by comparing the two models in Section 2 with two monolingual pre-trained language models: OLMO-1B (Groeneveld et al., 2024) and SmolLM-1.7B (Allal et al., 2024). We clarify the lingualities in each model\u2019s pre-training in Appendix C.\nWe collect the disjoint language-specific token embedding norms for each model:\n$e_l = \\{||e_j||\\}_{j \\in A_l}, A_l \\subset V, \\cap A_l = \\varnothing,$\nwhere $A_l$ is the token indices of language l in V. We compare $e_l$ distribution over five languages. In Figure 2, the distribution for English in SmolLM-1.7B and OLMO-1B are distinct from four languages, especially Korean and Chinese, which are non-Latin languages that do not share similar alphabets. However, Qwen2.5-3B and Llama-3.2-3B have similar ranges and distributions across the languages, even in non-Latin languages. Thus, we can infer that Qwen2.5-3B and Llama-3.2-3B, as MLMs, are sufficiently trained on the multilingual corpus to encode information with diverse linguality by having similar embedding norm distributions across the languages (Dagan et al., 2024; Chung et al., 2024). This supports why representation preservation is a crucial condition for generalizable RMs with MLMs, as discussed in Section 3.1."}, {"title": "4 Multilingual Alignment using RM", "content": "In this section, we perform experiments to outline the effects of using the reward models (RMs) from Section 2 and how their cross-lingual transfer can propagate to the actual alignment process."}, {"title": "4.1 Experimental Details", "content": "We sample 10k prompts from the cleaned UltraFeedback dataset (Bartolome et al., 2023; Cui et al., 2024) and translate prompts across target languages. Then, we sample four responses per prompt with Qwen2.5-7B-Instruct (Team, 2024b) and label them with desired RMs. By selecting the responses with the highest and lowest rewards, we prepare pairwise preference data. We train Qwen2.5-7B-Instruct on each language from the newly curated datasets with Direct Preference Optimization (Rafailov et al., 2024, DPO). Refer to Appendix B for the detailed setup.\nEvaluation We evaluate the trained model's language-specific instruction-following capability with Multilingual AlpacaEval, adopted from the instances and evaluation pipeline of AlpacaEval (Li et al., 2023). We report the detailed process and configurations in Appendix D."}, {"title": "4.2 Results and Analysis", "content": "English RM largely improves base models in every language As shown in Figure 3, models aligned with English RM show a notable leap compared to Qwen2.5-7B-Instruct (\u201cBase\u201d), by increasing up to 11.8% point. As the win rate was measured against GPT-4-Turbo, a strong proprietary language model, such enhancements strongly support the validity of using English RMs for multilingual alignment in desired languages.\nExploiting English RMs is a desirable choice in multilingual alignment We emphasize that using high-quality English preference data of better accessibility is a decent choice, considering the efficiency and efficacy in real-world cases. In Figure 3, models aligned with English RM were comparable to or exceeded ones with target language RMs, having at most 1.5% disparity in the worst scenario. Thus, adopting an English RM for multilingual alignment is a cost-efficient yet performant alternative, discarding the need for scaled translations."}, {"title": "5 Cross-lingual Transfer of External RMS", "content": "Along with the controlled comparisons in Section 2, we analyze the cross-lingual transfer in off-the-shelf models on the original RewardBench through Multilingual RewardBench. To ensure diversity in reward modeling schemes, we selected two classifier reward models (RMs), ArmoRM-8B (Wang et al., 2024b) and OffsetBias-8B (Park et al., 2024), alongside two generative RMs, GPT-4o\u2076 and Self-Taught-Llama-70B (Wang et al., 2024d).\nClassifier RMS Two classifier RMS are both trained on top of Llama3-8B-Instruct (Dubey et al., 2024), which are based on multilingual pre-trained language models (MLMs) as discussed in Appendix C. As in Table 1, these RMs also demonstrate strong cross-lingual transfer in four languages, mostly exceeding 70% accuracy across the board in Table 2.\nGenerative reward models Interestingly, we can observe strong cross-lingual transfer in the generative RMs in Table 2, as in the classifier RMs. As discussed in Section 3.2, fine-grained representation learning is a crucial component for having strong downstream generative abilities. While the extent of multilingual pre-training in GPT-4o is not verifiable, GPT-4o has the least decrement in non-English settings. Meantime, Self-Taught-Llama-70B with extensive multilingual pre-training demonstrates the strongest cross-lingual transfer, achieving the best accuracies in all four non-English Multilingual RewardBench."}, {"title": "Conclusion", "content": "We empirically demonstrate English as a lingua franca in reward modeling, given recent multilingual pre-trained language models (MLMs). We explain this with two consecutive arguments. First, English reward models (RMs) best preserve the representations of initial MLMs, while other languages induce representation collapse. Second, MLM representations inherently have a rich understanding of languages and tasks, making them valuable to preserve in downstream tasks. By extending our analysis to the off-the-shelf reward models, we show that using MLMs for reward modeling is crucial for eliciting strong cross-lingual transfer. Through strong cross-lingual transfer in English RMs, we establish a concrete foundation for exploiting English RMs for multilingual alignment."}, {"title": "Limitations", "content": "To extend to more languages and evaluation benchmarks, we have mainly utilized a 3B LLM to train the reward model (RM) with only 86k instances. However, as outlined in Appendix E, the 3B RMs are on par with a state-of-the-art RM, ArmoRM, which was trained with over 550k instances. Future works on the effects of data size and mixture will provide an enhanced understanding of our work.\nAlso, in Section 4, we use the AlpacaEval evaluation setup, which utilizes LLM-generated reference responses and LLM-as-a-Judge to select a winning response. Therefore, while we show a vast increase in post-training alignment, the process relies on the multilinguality of OpenAI models and the evaluation biases of the LLM-based evaluations outlined in Zheng et al., 2023."}]}