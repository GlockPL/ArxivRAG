{"title": "Practical Evaluation of Copula-based Survival Metrics: Beyond the Independent Censoring Assumption", "authors": ["Christian Marius Lillelund", "Shi-ang Qi", "Russell Greiner"], "abstract": "Conventional survival metrics, such as Harrell's concordance index and the Brier Score, rely on the independent censoring assumption for valid inference in the presence of right-censored data. However, when instances are censored for reasons related to the event of interest, this assumption no longer holds, as this kind of dependent censoring biases the marginal survival estimates of popular nonparametric estimators. In this paper, we propose three copula-based metrics to evaluate survival models in the presence of dependent censoring, and design a framework to create realistic, semi-synthetic datasets with dependent censoring to facilitate the evaluation of the metrics. Our empirical analyses in synthetic and semi-synthetic datasets show that our metrics can give error estimates that are closer to the true error, mainly in terms of predictive accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning survival prediction models resembles learning regression models, as both are essentially trying to learn, from a labeled dataset, a model that maps a description of an instance to a real value. However, survival prediction differs as some of the training instances are \u201ccensored\u201d \u2013 that is, we only know a lower bound of the true time. This includes, for example, individuals who are still alive at the end of the study, as well as others who left the study and were \u201clost to follow-up\". The independent censoring assumption states that, within any subgroup of interest, we assume that the patients (i.e., instances) who are censored at time t are representative of all the patients in that subgroup who remained at risk at time t with respect to their survival experience [Emura and Chen, 2018, Ch. 1]. That is, censoring is random within the subgroups. Let $T_E$ be the event distribution and $T_C$ be the censoring distribution, then we have $T_E \\bot T_C$ \u2013 i.e., $T_E$ provides no information about the distribution of $T_C$, and vice versa [Kleinbaum and Klein, 2012, Ch. 1]. If there are features (covariates) X, this assumption becomes the conditional independent censoring assumption, i.e., $T_E \\bot T_C | X$ [Kleinbaum and Klein, 2012, Ch. 1]. Note that many standard survival tools \u2013 e.g., the Kaplan-Meier (KM) [Kaplan and Meier, 1958] and Nelson-Aalen [Nelson, 1969, Aalen, 1978] estimators, and the semiparametric Cox Proportional Hazards (CoxPH) model [Cox, 1972], rely on an assumption of independent censoring for valid inference in the presence of right-censored data [Kleinbaum and Klein, 2012, Ch. 1].\nFigure 1a shows how the independent censoring assumption fails to hold by omitting a confounding covariate \u2013 e.g., \"tumor grade\". Since \"tumor grade\" relates to both $T_E$ (cancer relapse) and $T_C$ (cancer death), the variation in the covariate induces changes in both $T_E$ and $T_C$. That is, we want a model that predicts the time until relapse, but death is censoring. As \u201ctumor grade\u201d is predictive of tumor progression, an advanced (resp., early) stage is linked to shorter (longer) values for both cancer relapse $T_E$ and cancer death $T_C$, and consequently, $T_E$ and $T_C$ are not independent. This shows that the conditional independent censoring assumption might not hold if any confounding covariates are omitted or ignored in a regression model [Emura and Chen, 2018, Ch. 1]. This phenomenon is called residual dependence i.e., dependent censoring not adjusted by the covariates.\nFigure 1b demonstrates how dependent censoring biases the marginal survival estimates from the KM estimator [Kaplan and Meier, 1958]. Consider patient A, censored (due to cancer death) exactly one year after study entry, who likely would have experienced cancer relapse shortly thereafter. However, the KM estimator may overestimate this patient's survival time by assuming that they are like those remaining in the study, who tend to live longer, because it does not consider that these censored patients tend to have shorter event times. Indeed, the estimator relies on a stepwise construction of the survival probability that depends on knowing how many individuals are at risk and the event probabilities at each time point [Bland and Altman, 1998].\nTo address such dependency, Figure 1c presents a copula-based modeling approach. A copula function $C_o(u, v)$ links random variables by specifying their dependence structure [Emura and Chen, 2018, Ch. 1]. Applied here, it captures the dependency between event and censoring distributions ($T_E$ and $T_C$) by sampling the marginals from a (survival) copula. In this example, we see lower tail dependence from the copula \u2013 describing the probability that small (early) values of one variable (e.g., event) are associated with small values of another variable (e.g., censoring).\nIn the absence of features, if any confounder related to the event censors the event time, we refer to this phenomenon as dependent censoring. However, it is typically impossible to verify this in practice because we only observe one outcome (either event or censoring) per instance, but not both [Tsiatis, 1975]. Although we can reduce dependent censoring simply by collecting more data (i.e., features) [Emura and Chen, 2018, Ch. 1], we would still have to evaluate our model under dependent censoring, independent of the learned model, preferably. Popular metrics for evaluating predicted survival probabilities (or risk scores) are Harrell's concordance index (CI) [Harrell Jr et al., 1982], Uno's CI [Uno et al., 2011], the Brier Score (BS) [Brier, 1950], and the mean absolute error (MAE) using one of hinge [Haider et al., 2020], margin [Haider et al., 2020] or pseudo times [Qi et al., 2023a]. Unfortunately, dependent censoring is both common, and also introduces a bias that can affect the estimation of survival probabilities, and none of these metrics account for this dependency. Alternative methods have been proposed to handle dependent censoring, such as the inverse probability of censoring weighting (IPCW) [Robins and Finkelstein, 2000], but this approach still uses the KM estimator. Motivated by these limitations, we introduce a new approach to evaluating survival models under dependent censoring. To our knowledge, no prior work has proposed evaluation metrics that explicitly model the degree of dependent censoring between $T_E$ and $T_C$. Our main contributions are:\n\u2022 We show that conventional survival metrics, such as Harrell's CI, Uno's CI, the IPCW-integrated Brier Score, and the marginal MAE, are biased under dependent censoring, meaning they will give inaccurate estimates of the model's discriminative or predictive performance.\n\u2022 We propose three novel metrics \u2013 CI-Dep, IBS-Dep, and MAE-Dep \u2013 to evaluate survival models in the presence of dependent censoring. These metrics can evaluate the concordance (i.e., ranking), the accuracy of the survival curve, and the accuracy of the predicted time-to-event estimates, respectively, by considering the dependency between the censoring and event distributions.\n\u2022 We design a framework to create realistic, semi-synthetic datasets with various degrees of dependent censoring to determine if the proposed metrics can (1) report an unbiased error closest to the true error, and (2) rank the survival models by the true error.\n\u2022 We provide empirical evidence that our proposed metrics can provide more accurate error measures for survival models, mainly in terms of predictive accuracy, i.e., Brier Score and MAE.\nOur code is publicly available at this GitHub repository."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": ""}, {"title": "2.1 SURVIVAL DATA AND NOTATION", "content": "Many survival analysis models estimate the probability that a specific event will occur for an individual at a time T later than t, for all times $t > 0$. Examples include predicting the time to death in patients with amyotrophic lateral sclerosis (ALS) [Kuan et al., 2023], predicting the time to a fall in older adults [Lillelund et al., 2024a], or evaluating a new experimental treatment for cancer [Zhang et al., 2011]. Most often, the observation for a given instance will consist of a random variable, T, representing the time from a given origin ($t = 0$) to the occurrence of the event, e.g., \u201cdeath\u201d. The distribution of T can be characterized by the probability distribution function $F(t) = Pr(T < t)$ or, equivalently, by the survival function $S(t) = 1 \u2013 F(t) = Pr(T > t)$. Note that $S(t)$ (resp., $F(t)$) correspond to the probabilities of being event-free (resp., having experienced the event by) time t. We can estimate $S(t)$ nonparametrically, e.g., using the Kaplan-Meier [Kaplan and Meier, 1958] estimator (see Appendix H for a detailed derivation)."}, {"title": "Definition 1 The Kaplan-Meier (KM) estimator", "content": "$\\hat{S}(t) = \\prod_{i: t_i \\leq t, d_i=1} (1 - \\frac{1}{n_i}) ,\\quad \\text{for } t\\leq t_{max}$ (2.1)\nwhere $n_i = \\sum_{j=1}^{N} I(t_e \\geq t_i)$ is the number at-risk at time $t_i$; $\\hat{S}(t) = 1$ if no death occurs up to time t; $\\hat{S}(t)$ is undefined for $t > t_{max}$ [Kaplan and Meier, 1958].\nIf features are entered into the model, popular methods include the CoxPH model [Cox, 1972, Katzman et al., 2018, Lillelund et al., 2024b], gradient-boosted regression [Ridgeway, 1999], Random Survival Forest [Ishwaran et al., 2008] and Multi-Task Logistic Regression (MTLR) [Yu et al., 2011]. In this context, we may define our survival dataset D as a set of observations, i.e., we have $D = \\{(x_i, t_i, d_i)\\}_{i=1}^{N}$, where $x \\in R^d$ is a feature (covariate) vector for instance i, $t_i \\in R_{\\geq 0}$ is the time of either censoring or event, depending on which occurred first, and $\\delta_i \\in \\{0, 1\\}$ is the \u201ccensoring bit\u201d: If $\\delta_i = 0$, then $t_i = t_{c,i}$, where $t_{c,i}$ is the time of censoring, otherwise, if $\\delta_i = 1$, then $t_i = t_{e,i}$, where $t_{e,i}$ is the time of the event. Given survival data D, let $f_{T|x}(\\cdot)$ and $F_{T|x}(\\cdot)$ represent the conditional density and cumulative distribution functions, respectively, over the event horizon (i.e., from 0 to $t_{max}$). Given the covariates, we then have the following useful definitions:"}, {"title": "Definition 2 The survival function", "content": "$S_{T|x}(t | X) = Pr(T > t | X) = 1 - F_{T|x}(t | X),$ (2.2)\nrepresents the probability the event occurs at time t [Gareth et al., 2021, Ch. 11]."}, {"title": "Definition 3 The hazard function", "content": "$h_{T|x} (t | X) = \\lim_{\\epsilon \\to 0} \\frac{Pr(T \\in [t, t + \\epsilon) | T \\geq t, X)}{\\epsilon} = \\frac{f_{T|x}(t | X)}{S_{T|x}(t | X)}$ (2.3)\nrepresents the instantaneous event rate at given time t, conditional on surviving t [Gareth et al., 2021, Ch. 11]."}, {"title": "2.2 COPULA-BASED ESTIMATION", "content": "A copula is a function that links two or more random variables by specifying their dependence structure [Emura and Chen, 2018, Ch. 1]. Among the various types of copulas, the Archimedean copula family is the most used, and enables modeling of dependencies in arbitrarily high dimensions using a single parameter, $\\theta$. This family of copulas is defined by a generator function, $\\varphi(t) : [0, 1] \\to [0, \\infty]$, which is continuous, strictly decreasing, and satisfies the boundary conditions $\\varphi(0) = \\infty$ and $\\varphi(1) = 0$ [Emura and Chen, 2018, Ch. 3]. Here, we will focus on two common Archimedean copulas: Clayton [Clayton, 1978] and Frank [Frank, 1979].\nZheng and Klein [1995] generalized the KM estimator under independent censoring to the Copula-Graphic (CG) estimator under dependent censoring, which has become an important tool for analyzing survival data [Emura and Chen, 2016, Emura and Michimae, 2017, Hoora Moradian and Bellavance, 2019]. Rivest and Wells [2001] obtained the first explicit expression of the CG estimator under an assumed copula from the Archimedean family (e.g., Clayton or Frank). Given survival data without covariates, i.e., $(t_i, \\delta_i)$ for $i = 1, ..., n$, where $t_i = min(t_{e,i}, t_{c,i})$, $\\delta_i = I(t_{e,i} \\leq t_{c,i})$ with $I(\\cdot)$ denotes the indicator function, we can estimate the survival function using the CG estimator (see Appendix H for a detailed derivation):"}, {"title": "Definition 4 The Copula-Graphic (CG) estimator", "content": "$\\hat{S}_T(t) = \\prod_{i: t_i \\leq t, d_i=1} (\\frac{\\varphi^{-1}(\\frac{n_i-1}{n})}{\\varphi^{-1}(\\frac{n_i}{n})})^{\\delta_i}$, for $t\\leq t_{max}$ (2.4)\nwhere $n_i = \\sum_{j=1}^{N} I(t_e \\geq t_i)$ is the number at risk at time $t_i$; $\\hat{S}_T(t) = 1$ if no death occurs up to time t; $\\hat{S}_T(t)$ is undefined for $t > t_{max}$ [Emura and Chen, 2018, Ch. 4].\nNote that when $\\varphi(\\cdot)$ is uniform, this reduces to the standard KM estimator. See Appendix D for more details."}, {"title": "2.3 EVALUATION METRICS AND LIMITATIONS", "content": "We now outline the conventional evaluation metrics in survival analysis. The detailed calculation of these metrics is outlined in Appendix E and here we mainly focus on why those metrics are problematic under dependent censoring."}, {"title": "Concordance index", "content": "Concordance index (CI) evaluates the ranking of individuals when their relative ordering is of primary interest. It measures the proportion of concordant pairs - pairs where the predicted ranking of risk scores aligns with the true ordering of event times \u2013 among all comparable pairs [Harrell Jr et al., 1996]. A pair (i, j) is defined as comparable if $\\delta_i = 1$ and $t_i < t_j$, which means that the earlier individual has experienced the event and the observed time of the later individual is greater than the first. Even under the random censoring assumption, it is evident that the comparable pairs selected using the above criteria do not represent all possible pairs based on their true event times:\n$Pr(d_i = 1, t_i < t_j) \\neq Pr(t_{e,i} < t_{e,j})$.\nThis inequality arises because the left-hand term can be decomposed as:\n$Pr(d_i = 1, t_i < t_j)$\n$= Pr(t_{e,i} < t_{c,i}, t_{e,i} < min\\{t_{c,j},t_{e,j}\\})$\n$= Pr(t_{e,i} < t_{c,i}, t_{e,i} < t_{c,j}, t_{e,i} < t_{e,j})$\n$= Pr(t_{e,i} < t_{c,i}) Pr(t_{e,i} < t_{c,j}) Pr(t_{e,i} < t_{e,j}),$\nwhere the factorization follows from the independent censoring assumption. Compared to the right-hand side term, this expression includes additional underlined terms related to the censoring distribution, confirming the findings of Uno et al. [2011].\nTo address this limitation, Uno et al. [2011] introduced a modified version of CI that employs inverse probability of censoring weights (IPCW), ensuring consistency with the true CI. The IPCW approach estimates the probability of being uncensored at each time point and reweights the uncensored instances accordingly."}, {"title": "Integrated Brier Score", "content": "Integrated Brier Score (IBS) evaluates the overall accuracy of predicted survival functions by integrating the Brier Score (BS) [Brier, 1950] over a range of time points. It measures the average squared difference between the predicted survival probabilities and actual survival outcomes over time [Gerds and Schumacher, 2006]. To handle censored instances, IPCW can be used to reweight uncensored instances when computing both BS and IBS [Graf et al., 1999]. Similar to IPCW-CI, IPCW weights are often approximated using the KM estimator, which is biased under dependent censoring [Emura and Chen, 2018, Ch. 1]."}, {"title": "Mean Absolute Error", "content": "Mean Absolute Error (MAE) quantifies the accuracy of point predictions for event times by measuring the absolute difference between the predicted and true event times. For censored instances, Haider et al. [2020] proposed an approach that approximates that subject's event time as the mean of the KM estimator (Equation 2.1), renormalized at that individual's censoring time. This proxy event time, weighted by confidence estimates (also derived from KM), is then used to compute the absolute error. We will refer to this method as MAE-Margin. Given a subject is censored at time $t_i$, its margin time is calculated as:\n$t_{\\text{margin}} (\\hat{S}, t_i) = E_t [C_i | C_i > t_i] = t_i + \\frac{\\int_{t_i}^{\\infty} \\hat{S}(t) dt}{\\hat{S}(t_i)}$ (2.5)\nwhere $\\hat{S}(t)$ is an estimator, e.g., the KM, derived from the training dataset.\nLimitations. All the aforementioned metrics typically rely on KM estimates to approximate either the marginal time-to-event or the marginal time-to-censoring distribution. While KM is advantageous due to its consistency under the independent censoring assumption and its computational simplicity - requiring no interaction with covariates \u2013 it becomes problematic when censoring and event times are dependent [Emura and Chen, 2018, Ch. 1]. In such cases, the KM estimates may be biased and inconsistent [Campigotto and Weller, 2014]. To mitigate these issues, researchers have proposed modifying these metrics by replacing KM with a conditional estimator (e.g., CoxPH) to account for the dependency between covariates and censoring distributions [Gerds and Schumacher, 2006]. However, this approach still has two major limitations: (1) model misspecification: if the conditional estimator is inaccurately specified, the resulting estimates may be unreliable, and (2) uncaptured dependencies: under dependent censoring (i.e., when unobserved confounders exist), even a well-specified conditional estimator may fail to fully account for the dependency if there are important covariates missing in the analysis. These challenges highlight the need for robust estimation methods that can better handle dependent censoring scenarios."}, {"title": "3 THE PROPOSED METRICS", "content": "CI-Dep: We propose a modified version of Harrell's CI [Harrell Jr et al., 1982] that employs inverse probability weighting (i.e., IPCW), but the marginal survival probability for censoring at time t is estimated using the CG estimator under an approximated Archimedean copula, parameterized by $\\theta$. When $\\theta \\approx 0$, it yields the independence copula, which is equivalent to using the KM estimator as in Uno et al. [2011]. Our proposed concordance index thus requires a known copula, that models the dependency between marginals $T_E$ and $T_C$, but does not require interaction with the covariates. Given an estimated risk function $\\hat{\\eta}(\\cdot)$ and corresponding labels, the traditional CI is computed as:\n$CI(D; \\hat{\\eta}(\\cdot)) = \\frac{\\sum_{i,j} \\delta_i\\cdot 1 [t_i < t_j] \\cdot 1[\\hat{\\eta}(x_i) > \\hat{\\eta}(x_j)]}{\\sum_{i,j} \\delta_i\\cdot 1 [t_i < t_j]}$ (3.1)\nNow, employing the IPCW technique used by Uno et al. [2011], but under dependent censoring, let $G_{CG}(t) = Pr(T > t)$ represent the marginal survival probability for censoring at t estimated by the CG estimator. We can then reweight Equation 3.1 using the CG estimator as:\n$CI_{Dep} (D; \\hat{\\eta}(\\cdot)) = \\frac{\\sum_{i,j} \\delta_i\\cdot \\{G_{CG}(t_i)\\}^{-2}\\cdot 1[t_i < t_j] \\cdot 1[\\hat{\\eta}(x_i) > \\hat{\\eta}(x_j)]}{\\sum_{i,j} \\delta_i\\cdot \\{G_{CG}(t_i)\\}^{-2}. 1[t_i < t_j]}$ (3.2)\nwhich denotes the proposed CI-Dep. Here, the weight is given by $1/G_{CG}(t)$.\nIBS-Dep: We propose a modified version of the Brier Score [Graf et al., 1999] under dependent censoring using margin time estimates as the censoring weights. The margin or \"best-guess\u201d time can be interpreted as a conditional expectation of the event time given the event time is greater than the censoring time. Given an instance is censored at time $t_i$, we can calculate its margin time using Equation 2.5, but with the CG estimator instead of the KM, i.e., $t_{\\text{margin}} (t_i) = t_{\\text{margin}} (S_{CG}, t_i)$, where $\\hat{S}_{CG}$ is the marginal survival probability estimated using CG. We then formulate the dependent Brier Score as:\n$BS_{Dep} (D; \\hat{S}(), t^*) = \\frac{1}{N} \\sum_{i=1}^{N} \\{ I[t_i > t^*] - \\hat{S}(t^* | x_i) \\}^2 \\pi_i$ (3.3)\nwhere $\\pi_i = \\begin{cases} \\frac{1}{G_{CG}(t_i)} & \\text{if } \\delta_i = 1 \\\\ t_{\\text{margin}}(t_i) & \\text{elsewhere} \\end{cases}$\nTo calculate the expectation of Equation 3.3 over all times, we simply take the integral from $t = 0$ to $t = t_{max}$\n$IBS_{Dep} (D; \\hat{S}()) = \\int_{0}^{t_{max}} BS_{Dep} (D; \\hat{S}(), t^*) dT$, (3.4)\nwhere $t_{max}$ is usually the maximum observed time.\nMAE-Dep: We propose a modified version of the MAE-Margin [Haider et al., 2020] under dependent censoring. Again, we make use of the CG estimator to calculate margin times for the censored instances (Equation 2.5) and replace the censored event times with their \"best-guess\u201d-estimates. Furthermore, for each censored instance, we adopt the approach from Haider et al. [2020] and assign a confidence weight $w_i = 1 - \\hat{S}_{CG}(t_{c,i})$ to the error based on the margin value estimated using the CG method. For uncensored subjects, we set $w_i = 1$. The dependent MAE is thus:\n$MAE_{Dep} (D; t_i) = \\frac{1}{N} \\sum_{i=1}^{N} w_i |[(1 - \\delta_i) \\cdot t_{\\text{margin}(CG)} (t_i) + \\delta_i t_i] - t_i|$ (3.5)"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "For our empirical analyses, we evaluate the effectiveness of the proposed metrics under dependent censoring in synthetic datasets under a known copula and in semi-synthetic datasets by learning the copula based on the data."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Synthetic datasets. Following the approach of Foomani et al. [2023], we create a synthetic survival dataset from a data generation process (DGP). The event and censoring distributions are specified by linear Weibull marginal distributions. Specifically, we sample $T_e$ and $T_c$ random variables from a known copula given Kendall's $\\tau$; if $\\tau = 0$, the marginals are independent, otherwise if $\\tau > 0$, the marginals are from an Archimedean copula, e.g., Clayton or Frank, known at evaluation time. See Appendix F.1 for more details.\nSemi-synthetic datasets. To evaluate our proposed metrics, we need to know the true metric, which requires precise knowledge of when each instance experiences the event. Due to censoring, event times are not available in real-world datasets for all instances, so we create a realistic, semi-synthetic dataset, where the covariates, event distribution, and censoring distribution align with some real-world datasets. This approach is inspired by Qi et al. [2023a], who proposed a semi-synthetic framework to generate different kinds of censoring distributions. However, in our case, we propose several strategies instead to create dependent censoring. In summary, our approach is as follows (see also the flowchart in Figure 2):\n1. Start with some real-world survival dataset D.\n2. Based on some feature selection strategy (e.g., Top-k features), select the relevant features from the original dataset D based on some technique, e.g., feature importances.\n3. Calculate the censoring distribution of D to generate censor times, $t_{c,i} \\sim G_{CoxPH(D)}(t | x_i)$, where $G_{CoXPH(D)}$ is the feature-dependent censoring distribution estimated by the Cox Proportional Hazard model [Cox, 1972] with Breslow estimator [Breslow, 1975].\n4. Create D' by removing the censored instances from D, leaving just the uncensored ones.\n5. Form D\" by applying some reasonable, but synthetic censoring distribution to D' and restrict the feature set to induce dependent censoring. Here we use the censoring distribution calculated in step 3 and the features calculated in step 2.\nBased on D, we calculate the censoring distributions $G_{CoxPH(1)}$ and apply it to D', so that for each instance, that instance is synthetically censored if the censoring time is earlier than the event time ($t_{c,i} < t_i$), and otherwise is left uncensored. To induce dependent censoring, we consider three different feature selection strategies:\n\u2022 Original, where the original feature set is kept intact. This approach does not introduce additional residual dependence, but conditional independence may still not hold if important features are omitted originally.\n\u2022 Top-k, where we select the k most important features from D based on their predictive power. For this, we train a CoxPH model on D and then use permutation feature importances [Breiman, 2001] to select the k best features. Feature importances are calculated by shuffling feature values and observing how much the model's performance degrades, using Harrell's CI.\n\u2022 Random-%, where we select a random proportion of %-pct features from the original feature set. In a small dataset, randomly selecting 25% of the features can introduce significant residual dependence if key features are excluded. In contrast, in a high-dimensional dataset, this risk is lower as many important features are likely to remain.\nCopula model. To validate our dependent metrics in semi-synthetic datasets, we train two copula models, Clayton and Frank, and two nonlinear Weibull proportional-hazards models for the marginal distributions, $T_e$ and $T_c$, respectively, on D\". The Weibull models use a multilayer perceptron (MLP) as the backbone network architecture with a single hidden layer and the nonlinear ReLU activation function. For each copula model, we train the marginals and the copula $C_o$ jointly using maximum likelihood estimation (MLE) (see Appendix F.3 for details). After training the two candidate models, we choose the model that minimizes the Akaike Information Criterion (AIC), given its loss and number of parameters. This offers a practical, data-driven approach to copula selection [Emura and Chen, 2018, Ch. 4]. We then use the best copula $C_o$ for the evaluation of the dependent metrics.\nDatasets. We apply the evaluation pipeline in Figure 2 to 6 large, real-world datasets: METABRIC [Curtis et al., 2012], 2 from MIMIC-IV (all causes and hospital death) [Johnson et al., 2023], and 3 from SEER (death from brain, liver and stomach cancer) [LAG et al., 2007]. They differ in their number of instances, features, and censoring rates. Appendix F.1 gives further details on the datasets and describes how we preprocessed them.\nSurvival learners. We compare the prediction results using 5 survival learners produced by the CoxPH [Cox, 1972], gradient boosting (GBSA) [Ridgeway, 1999], Random Survival Forest (RSF) Ishwaran et al. [2008], DeepSurv Katzman et al. [2018] and Multi-Task Logistic Regression (MTLR) [Yu et al., 2011]. These learners predict individual survival distributions (ISDs) for each instance i. See Appendix F.2 for further details, including implementation details and hyperparameters.\nGiven the semi-synthetic dataset D\", we train the five survival learners across 5 experiments given a random seed. In each experiment, we follow Sechidis et al. [2011] and first split the dataset into training (70%), validation (10%), and test (20%) sets in a stratified manner wrt the event indicators and event times. This ensures a balanced representation of the time-to-event information and outcome across the data splits. After splitting, we impute missing values using sample mean for real-valued covariates or mode for categorical covariates based on the training set only. We then normalize numerical values and encode categorical features using a one-hot encoding strategy."}, {"title": "4.2 RESULTS AND TAKEAWAYS", "content": "Synthetic data. Figure 3 shows the evaluation bias \u2013 i.e.", "datasets": "METABRIC, MIMIC-IV and SEER. The complete results for all 6 datasets are in G.1. In the small-sized METABRIC dataset, Harrell's gives superior estimates of the true concordance index, and there is a performance degradation in using the dependent variant. We see that CI-Dep degrades the performance compared to Uno's CI, in this case, while IBS-Dep improves on the IPCW version, and MAE-Margin and MAE-Dep give similar results. In the medium-sized MIMIC-IV dataset, Harrell's CI is again better, but our IBS-Dep and MAE-Dep improve on the IBS-IPCW and margin MAE, respectively, in all cases, however, the pseudo MAE is the best (i.e., it has the lowest error). In the large-sized SEER dataset, there are few differences in discriminative performances for the three metrics, but our IBS-Dep improves on IPCW, while our MAE-Dep improves on pseudo and margin. More often than not, Harrell's and Uno's CI (IPCW) give better estimates than our CI-Dep in these datasets, while our IBS-Dep and MAE-Dep give notable improvements to their independent counterparts. See Appendix G.1 for additional results.\nIn general, a (censored) metric should be representative of the true metric, i.e., it should tell us which model makes the best survival predictions \u2013 for example, by identifying the top-3 learners in alignment with the true ranking."}]}