{"title": "Advanced Displacement Magnitude Prediction in Multi-Material Architected Lattice Structure Beams Using Physics Informed Neural Network Architecture", "authors": ["Akshansh Mishra"], "abstract": "This paper proposes an innovative method for predicting deformation in architected lattice structures that combines Physics-Informed Neural Networks (PINNs) with finite element analysis. A thorough study was carried out on FCC-based lattice beams utilizing five different materials (Structural Steel, AA6061, AA7075, Ti6Al4V, and Inconel 718) under varied edge loads (1000-10000 N). The PINN model blends data-driven learning with physics-based limitations via a proprietary loss function, resulting in much higher prediction accuracy than linear regression. PINN outperforms linear regression, achieving greater R\u00b2 (0.7923 vs 0.5686) and lower error metrics (MSE: 0.00017417 vs 0.00036187). Among the materials examined, AA6061 had the highest displacement sensitivity (0.1014 mm at maximum load), while Inconel718 had better structural stability.", "sections": [{"title": "1. Introduction", "content": "In order to understand the physical phenomenon in the history of science differential equations have been formulated to solve the various problems. Differential equations have found applications in various domains like to describe motion, heat flow and other natural processes [1-3]. In the late 17th century, Sir Isaac Newton and Gottfried Wilhelm Leibniz laid the groundwork for calculus independently, allowing differential equations to be formalised. Newton's second law of motion is represented by equation 1.1.\n$F = ma$ (1.1)\nWhere F is the force, m is mass, and a is the acceleration. We can also write acceleration using equation 1.2.\n$a = \\frac{d\u00b2x}{dt2}$ (1.2)"}, {"title": null, "content": "Now if we substitute equation 1.2 in equation 1.1, we obtain a second order differential equation (ODE) shown in equation 1.3. Which describes the dynamics of particle subjected to force field.\n$m\\frac{d\u00b2x}{dt2} = F(x, t)$ (1.3)\nThe use of differential equations advanced rapidly during the 18th and 19th centuries. Leonhard Euler and Joseph-Louis Lagrange generalised Newtonian mechanics by developing the Euler-Lagrange equation [4-5], which determines the stationary points of the action functional in classical mechanics, as depicted in equation 1.4.\n$\\frac{d}{dt}(\\frac{\\partial L}{\\partial \\dot{q}}) - \\frac{\\partial L}{\\partial q} = 0$ (1.4)\nWhere L is the Lagrangian, q represents generalized coordinates, and \u012f represents their time derivatives. The visualisation depicted in Figure 1.1 shows the practical application of the Euler-Lagrange equation using a basic pendulum system, which is a canonical illustration of classical mechanics. Figure 1.1 depicts four important phases of the pendulum's history, each combining phase space dynamics and physical configuration to demonstrate the system's behaviour as defined by the Euler-Lagrange framework. The pendulum begins its motion at 0 = \u03c0/4 with zero initial velocity. The phase space depicts the beginning of the trajectory with \u2202L/8q\u02d9 at its maximum. As the system evolves according to the Euler-Lagrange equation, the time derivative of dL/dq balances with \u2202L/dq, as indicated by the phase space trajectory travelling through points of varied energy. The physical configuration reaches maximum velocity at the equilibrium position."}, {"title": null, "content": "Differential equations become indispensable in both thermodynamics and fluid dynamics. Jean-Baptiste Fourier developed the heat equation, as illustrated in equation 1.5 to model the heat conduction.\n$\\frac{\\partial u}{\\partial t} = a\u00b2\\nabla\u00b2u$ (1.5)\nWhere u is the temperature, a is the thermal diffusivity, and \u2207\u00b2 is the Laplacian operator. Figure 1.2 visualises the heat diffusion process using Fourier's heat equation (du/dt = a\u2207\u00b2u). The graphic depicts four important time snapshots organised horizontally to show the temporal evolution of temperature distribution in a two-dimensional domain. In the initial state (t = 0s), we see a concentrated high-temperature zone (shown in dark red) in the centre of the domain, corresponding to a localised heat source with a temperature of 100\u00b0C against a colder background. As time passes to the early diffusion phase, the visualisation shows how thermal energy begins to move outward from this concentrated hot area using heat conduction principles."}, {"title": null, "content": "These equations, in their various variants, continue to play an important role in understanding and predicting the behaviour of physical systems at all scales, from atomic to cosmic. The ongoing development of analytical and numerical approaches for solving differential equations demonstrates their continued importance in physics.\nThe advancement of machine learning has provided new opportunities for solving differential equations in engineering applications [6-10]. Traditional numerical approaches solved problems using explicit mathematical models, whereas current neural networks can learn complex differential patterns from data. The invention of Physics-Informed Neural Networks (PINNs) is a big step forward, combining neural network learning capabilities with the restrictions of physical laws stated by differential equation.\nThis paper describes a data-driven approach that combines physical concepts and machine learning to anticipate the behavior of architected lattice structures, which are manufactured materials with periodic cellular arrangements tailored for certain mechanical properties [11-16]. This study addresses key structural prediction challenges by combining PINNs and classical mechanical analysis. The methodology makes two key advances: first, it provides more accuracy in displacement predictions than traditional methods, and second, it maintains physical consistency using embedded differential equations. This technique, when applied to five different alloys under differing stresses, illustrates its actual applicability in engineering design optimization."}, {"title": "2. Working mechanism of PINNs based algorithms", "content": ""}, {"title": "2.1. Introduction", "content": "Physics Informed Neural Networks (PINNs) are modernistic computational approach which combines the machine learning techniques with the laws of physics to solve differential equations. PINNs usually depend on large well labelled datasets for incorporating the physical laws into the neural networks by using partial differential equations (PDEs) directly into the training process. Through this coupling, physical restrictions are included as soft penalties in the loss function, allowing PINNs to solve forward and inverse problems even when the input is sparse or noisy."}, {"title": "2.2. Role of Partial Differential Equations in PINNs", "content": "PDEs explain the spatial and temporal variations of physical parameters like temperature, pressure, and velocity. They serve as the basis for numerous scientific and engineering fields, simulating fluid flow, heat conduction, and wave propagation. These equations act as restrictions that the neural network's predictions in PINNs must meet in order for the solutions to be physically consistent. For example, heat equation is a second order PDE given by $\\frac{\\partial u}{\\partial t} = a\u00b2\\nabla\u00b2u$ represents how heat diffuses through a material over time. In the equation $\\frac{\\partial u}{\\partial t}$ represents the temperature field and a represents the thermal diffusivity. PINNs guarantee that the neural network complies with the laws of heat conduction across the domain by incorporating this equation into the loss function."}, {"title": "2.3.\nMathematical formulation of PINNS", "content": "The solution to a PDE is approximated by using PINN by using a neural network u(x, t; 0) where x and t are spatial and temporal variables, and 0 represents the network's parameters. In PINN, loss function consists of two components i.e., a data loss and a physics loss which is minimized during the training process. Data loss component as depicted in equation 2.1 penalizes the difference between the neural network's predictions and observed data points.\n$L_{data} = \\frac{1}{N_d} \\frac{1}{2} \\sum_{i=1}^{N_d} (u(x_i, t_i; \\theta) - U_{data} (x_i, t_i))^2$ (2.1)\nWhere Nd is the number of data points and $u_{data}(x_i, t_i)$ represents the observed values.\nPhysics loss component as depicted in equation 2.2 enforced the physical constraints dictated by the PDE.\n$L_{physics} = \\frac{1}{N_p} \\frac{1}{2} \\sum_{j=1}^{N_p} |\\frac{\\partial u}{\\partial t}(x_j, t_j; \\theta) - a \\nabla\u00b2u(x_j, t_j; \\theta)|^2$ (2.2)"}, {"title": null, "content": "Where Np is the number of collocation points sampled in the domain to compute the residual of PDE.\nThe total loss is computed by summing up the equation 2.1 and 2.2 resulting in equation 2.3.\n$L = L_{data} + L_{physics}$ (2.3)\nThe network learns to approximate the solution u(x, t) that satisfies both the observed data and underlying PDE by minimizing the L."}, {"title": "2.4.\nTraining Process mechanism in PINNS", "content": "The first step in training PINNs is to design a neural network architecture that, given input factors like space and time, produces the dependent variable (like temperature or displacement). To assess the PDE residuals, the necessary derivatives of the network output are calculated using automatic differentiation, a feature of contemporary deep learning frameworks. Then, gradient-based optimization methods like Adam or L-BFGS are used to minimize the loss function. Either extra data points in the loss function or particular neural network parameterizations are used to impose boundary and initial conditions.\nLet's take an example of solving heat equation using PINNs by considering one-dimensional heat equation problem as shown in equation 2.4.\n$\\frac{\\partial u}{\\partial t} = \u03b1 \\frac{\\partial\u00b2u}{\\partial x\u00b2}$, x\u2208 [0,1], t \u2208 [0, t] (2.4)\nThe equation 2.4 is subjected to the boundary conditions as shown in equation 2.5.\nu(x, 0) = u\u2080(x), u(0,t) = u(1,t) = 0 (2.5)\nIn the given formulation, u(x, t) represents the temperature field and a depicts the thermal diffusivity.\nThe neural network is u(x, t; 0) is trained to satisfy two conditions i.e. initial and boundary conditions by including these points in $L_{data}$ and at last satisfying the PDE by minimizing the residual at collocation points in the domain as part of $L_{physics}$. The total loss function of the given formulated problem is given by equation 2.6.\n$L = \\frac{1}{N_d} \\sum_{i=1}^{N_d}(u(x_i, t_i; \\theta) - U_{data} (x_i, t_i))\u00b2 + \\frac{1}{N_p} \\sum_{j=1}^{N_p} |\\frac{\\partial u}{\\partial t}(x_j, t_j; \\theta) - a \\nabla\u00b2u(x_j, t_j; \\theta)|^2$ (2.6)"}, {"title": null, "content": "The total loss function L combines the contributions from the PDE residual, initial conditions and boundary conditions. Physics loss depicted in equation 2.10 enforces the wave equation at collocation points $(x_j, t_j)$ sampled in the domain.\n$L_{physics} = \\frac{1}{N_p} \\sum_{j=1}^{N_p} |\\frac{\\partial\u00b2u}{\\partial t\u00b2}(x_j, t_j; \\theta) \u2013 c\u00b2 \\frac{\\partial\u00b2u}{\\partial x\u00b2}(x_j, t_j; \\theta)|^2$ (2.10)\nInitial condition loss depicted in equation 2.11 penalizes the deviations from the specified initial conditions.\n$L_{init} = \\frac{1}{N_i} \\sum_{k=1}^{N_i} (u(x_k, 0; \\theta) \u2013 u_0 (x_k))\u00b2 + \\frac{1}{N_i} \\sum_{k=1}^{N_i} (\\frac{\\partial u}{\\partial t} (x_k, 0; \\theta) \u2013 v_0 (x_k))\u00b2$ (2.11)\nBoundary conditions loss depicted in equation 2.12 ensures that the displacement is zero at the boundaries.\n$L_{boundary} = \\frac{1}{N_b} \\sum_{m=1}^{N_b} (u(0, t_m; \\theta))\u00b2 + \\frac{1}{N_b} \\sum_{m=1}^{N_b} (u(L, t_m; \\theta))\u00b2$ (2.12)\nThe total loss function is obtained by summing up the equation 2.10,2.11 and 2.12 as depicted in equation 2.13.\n$L = L_{physics} + L_{init} + L_{boundary}$ (2.13)"}, {"title": "3. Materials and Methods", "content": "In the present work, FCC based lattice structure beam has been considered as shown in Figure 3.1. The architected beam structures were modeled with dimensions\n224.90 mm \u00d7 17.30 mm \u00d7 34.60 mm."}, {"title": null, "content": "The dataset was created using a factorial design framework and a methodical process influenced by the ideas of Response Surface Methodology (RSM). The study utilized a diverse selection of alloys, including Structural Steel, AA6061, AA7075, Ti6Al4V, and Inconel 718 were chosen as the categorical factor. Each material was chosen for its distinct mechanical properties and widespread application in structural and aerospace engineering. The applied edge loads were uniformly dispersed within a specified range of 1000 N to 10,000 N for every alloy fixed at the opposite end, guaranteeing a methodical investigation of the input parameter space."}, {"title": "4. Results and Discussions", "content": "Figure 4.1 and Table 4.1 shows the obtained results from the Multiphysics simulation carried out on the 50 samples."}, {"title": null, "content": "The heatmap depicted in Figure 4.2 shows varied displacement patterns for various alloy kinds under different loads. AA6061 has the highest displacement sensitivity, reaching 0.1014 mm at maximum load, followed by AA7075 at 0.0985 mm. In comparison, Inconel718"}, {"title": null, "content": "and Structural Steel exhibit strikingly similar, smaller displacement patterns (approximately\n0.035 mm at maximum load), indicating improved structural stability. Ti6Al4V has a\nmoderate displacement characteristic (0.0666 mm at maximum load), placing it between\naluminum alloys and more rigid materials. Each alloy's linear color transition from dark to\nlight as load increases suggests consistent, predictable material behavior under stress."}, {"title": null, "content": "The plot shown in Figure 4.3 depicts the linear relationship between applied load and displacement for several alloys, with the behavior clearly related to the strength values. AA6061 and AA7075 have the sharpest gradients, suggesting the maximum displacement sensitivity, whereas Inconel718 (1034 MPa) has the least displacement under load. The parallel lines indicate consistent elastic behavior across all materials, with displacement magnitudes inversely proportional to strength values. This linear reaction suggests that deformation will occur predictably within the tested load range."}, {"title": null, "content": "In this study, the PINN developed using python programming predicts the displacement\nmagnitude of materials under applied edge loads by combining physics-based constraints\nwith data-driven learning at 1000 number of epochs. The PINN makes use of a unique loss\nfunction that blends a penalty term based on physics with a data-fitting term (mean squared\nerror). The implemented PINN architecture takes two inputs i.e. alloy strength (x\u2081) and\napplied edge load (x2) and yields the output value as displacement magnitude (y). The data\nwere divided in 80-20 ratio i.e. 80 percent data were used for training purpose and 20 percent\ndata were used for testing purpose. The architecture of the neural network is defined by\nequation 4.1.\n$f(x; \\theta) = \u03c6_l(\u03c6_{l-1}(... \u03c6\u2081(x) ...))$ (4.1)\nWhere x is the input feature vector represented as x = [X\u2081, X\u2082]. Equation 4.2 represents the\ntransformation at the l \u2013 th layer using a ReLU activation function.\n$\u03c6_l(z) = max(0, W_lz + b_l)$ (4.2)"}, {"title": null, "content": "Where W\u2081 and b\u012b are the weights and bias matrices for the l \u2013 th layer and the function\nmax(0,.) applied the ReLU activation function. The neural network architecture uses a\nfeedforward approach to process material deformation characteristics across various layers\nof increasing abstraction. The network begins with an input layer that accepts two features:\nalloy strength and applied load. These inputs are then routed through a network of dense\nhidden layers. The first two hidden layers have 64 neurons each and use ReLU (Rectified\nLinear Unit) activation functions, which add nonlinearity and help the network to learn\ncomplicated patterns in the data. Following this, a third hidden layer with 32 neurons uses\nReLU activation to further compress the characteristics into a more compact\nrepresentation. Finally, the network closes with an output layer made up of a single neuron\nthat generates the projected displacement value.\nThe total loss function depicted in equation 4.3 is given by summing up the two loss\ncomponents i.e. data-driven loss which is the mean squared error (MSE) between the\npredicted displacement \u0177 and the true displacement y as depicted in equation 4.4 and other\ncomponent is physics-based loss which incorporates a physics-based constraint which\nrelates the applied load and alloy strength as represented in equation 4.5.\n$L_{total} = L_{data} + \u03bbL_{physics}$ (4.3)\n$L_{data} = \\frac{1}{N} \\sum_{i=1}^{N}(Vi \u2013 yi)\u00b2$ (4.4)\n$L_{physics} = \\frac{1}{N} \\sum_{i=1}^{N}(yi - P(xi))\u00b2$ (4.5)\nWhere N is the number of datapoints, \u03bb is a hyperparameter controlling the weight of the\nphysics-based loss term, and P(xi) is the physics-based term calculated using equation 4.6.\n$P(x) = \\frac{Applied Load}{Alloy strength+\u2208}$ (4.6)\nWhere P(x) is a simple representation of the physical relationship between stress and strain,\nassuming linear behavior under applied load and E is a small constant for numerical stability\n(E= 1 \u00d7 10\u22127 in the present study). The physics term is further normalized depicted in\nequation 4.7 for the numerical stability and to match the scale of displacement values.\n$P'(x) = \\frac{P(x)-\u03bc\u03c1}{\u03c3\u03c1+E}$ (4.7)\nWhere up is the mean of P(x) across the batch, and op is the standard deviation across the\nbatch. This normalization ensures that the physics-based loss functions at the same scale\nas the data-driven loss."}, {"title": null, "content": "The model is trained using the Adam optimizer with a learning rate of 0.001. A custom training\nstep is constructed using TensorFlow's GradientTape, which simplifies the computation of\ngradients for the loss function in relation to the model's parameters. These gradients are then\nused to update the model weights via the optimizer, ensuring efficient convergence\nthroughout the optimization process."}, {"title": null, "content": "A comparison of performance metrics between the PINN and Linear Regression models\nindicates considerable variations in predicting ability as shown in Figure 4.1. The PINN\nmodel outperforms all important measures. The PINN model has a higher R\u00b2 Score (0.7923)\nthan Linear Regression (0.5686), indicating that it explains more of the dependent variable's\nvariance. This significant difference of about 0.22 points shows that the PINN model reflects\nthe data's underlying patterns more well. In terms of error metrics, the PINN model likewise\nperforms significantly better. The PINN model's Mean Squared Error (MSE) is 0.00017417,"}, {"title": null, "content": "which is less than half of the Linear Regression's MSE of 0.00036187. This lower MSE shows\nthat the PINN model's predictions have lower average squared deviations from the true\nvalues. Similarly, the Mean Absolute Error (MAE) shows the same pattern, with PINN\nobtaining 0.00767965 versus Linear Regression's 0.01624120. This suggests that the PINN\nmodel's predictions depart less from true values in absolute terms, with an MAE about 52%\nlower than the Linear Regression model.\nThe distribution plots shown in Figure 4.6 of prediction errors for the Linear Regression and\nPINN models show major differences in their predictive tendencies. The Linear Regression\nmodel has a more symmetrical, bell-shaped error distribution centered at 0.01 mm,\nindicating a consistent but minor overestimation bias in its predictions. This symmetrical\npattern suggests that the model's mistakes are uniformly distributed on both sides of the\nmean, as is common for linear models dealing with complex interactions. In comparison,\nthe PINN model has a much different error distribution pattern, with a noticeable right-\nskewed shape. Its peak is closer to 0.00 mm, showing more accuracy in most predictions.\nThe PINN distribution has a higher maximum density of around 37.0 than Linear Regression's\n19.0, indicating that a greater proportion of its predictions cluster around true values.\nHowever, the PINN model's distribution has a broader right tail that extends to around 0.04\nmm, showing that while it generally performs better, it may occasionally yield bigger errors\nin specific instances. The error ranges are also different across the two models, with Linear\nRegression covering from -0.02 mm to 0.03 mm in a more uniform spread, whilst the PINN\nmodel's faults range from about -0.01 mm to 0.04 mm. The PINN's sharper peak and\nconcentrated distribution around zero error illustrate its superior predictive performance in\nthe majority of cases, despite the presence of infrequent outliers. These distributional\nproperties are consistent with and complement the preceding performance measures,\ndemonstrating the PINN model's overall improved prediction accuracy over the standard\nLinear Regression technique."}, {"title": null, "content": "A comparison of actual versus predicted displacement plots for PINN and Linear Regression\nmodels, Figure 4.7a) and Figure 4.7b), respectively, indicates large differences in their\npredictive performances. The dashed line in the plots reflects perfect prediction, where\nactual and predicted values are equal, and colors for data points correspond to their\nmagnitude of absolute error, ranging from blue-low error-to red-higher error. The PINN model\ngives a very good prediction accuracy and has most of the points tightly grouped around the\nperfect prediction line over the entire range of displacements between 0.00 and 0.10 mm.\nMost of the predictions are of relatively lower magnitudes of absolute errors represented by\nmostly blue-colored points but for a few at large values of displacements. This consistent\nclustering along the diagonal line indicates that the PINN model has successfully grasped\nthe linearity and nonlinearity of the displacement relationship.\nOn the other side, the Linear Regression model gives more scattered predictions with a lot\nof deviation from the perfect prediction line, particularly for the middle range between 0.04\nand 0.08 mm. The color gradient of the points shows a trend in increasing prediction errors\nwith more points showing lighter blue to red colors, which indicates bigger absolute errors\ncompared to the PINN model. A systematic deviation of points from the diagonal line,\nespecially for the mid-range values, would mean that the Linear Regression model is not able\nto capture the underlying complexity of the relationship between displacements. It can also\nbe seen from the visualization that both models have problems with the extreme values,\nmainly around the biggest displacement measures of 0.10 mm, where the prediction errors\nof both models increase, given by the red points."}, {"title": null, "content": "displacement values. The residuals are the differences between predicted and actual\nvalues; the dashed line at zero represents a perfect prediction, while point colors show\nabsolute magnitude errors ranging from blue (low) to red (high).\nThe residual plot of the Linear Regression model does show a pattern in the residuals that is\nsomewhat troubling: it shows residuals increasing with larger actual values. Indeed, the\nresiduals for smaller displacements\u2014starting from 0.00 to 0.04 mm\u2014stay relatively small\nand close to zero; this can be seen by the blue dots. However, there is an upward trend in\nresiduals as the actual values increase, and the largest residuals (about 0.034) are at the\nhighest level of displacement values (at 0.10 mm). Such a pattern, in the progression from\nblue to red of the color scheme, indicative of underpredicting larger displacement values,\ncould be seen as an implication of a biased Linear Regression model. On the other hand, the\nresidual plot for the PINN model shows more homogeneous and controlled error patterns.\nResiduals are generally smaller in magnitude, with most points clustered closer to the zero\nline and showing predominantly blue coloring, which indicates lower absolute errors. While\nthere is still one noticeable outlier at the highest displacement value of 0.10 mm, the overall\nspread of residuals is more uniform across the range of actual values. The PINN model keeps\nthe prediction accuracy relatively stable for different values of displacement without\nshowing a systematic bias as was seen in the Linear Regression model. It also shows better\nbalance in the residuals distribution above and below zero, which may hint at less biased\npredictions."}, {"title": null, "content": "The 3D surface plots for the PINN shown in Figure 4.9 a) and Linear Regression shown in\nFigure 4.9 b) models show that there is a stark difference on how both models estimate\ndisplacement from alloy strength and applied load . Both of the visualizations map the\npredicted displacement values to a blue to red color scale to reflect the displacement level.\nThe surface plot depicted for the PINN model reveals a more curved three-dimensional\nprofile of the variables. It illustrates a curved shaped surface with different gradients,\nespecially areas with larger applied load and smaller alloys. It appears that the PINN model\nhas succeeded in capturing nuanced relationships between the strength of the alloy used\nand the applied load due to surface features such as gentle waves and a higher degree of\nrounding on the outer surface. These displacement values have an approximate order of 0.00\nand 0.08 mm, and the maximum displacements (depicted in red) are observed at high loads\nand low alloy strength. However, the Linear Regression model gives a significantly more\nstraightforward plane with equal slopes in the entirety of the prediction space. This means\nthat this linear surface represents a direct relationship meaning displacement rises\nuniformly as load goes up and as alloy strength goes down. The displacement range is\nsmaller: 0.01-0.06 mm and the blending from a blue to a red colour is gradual to highly"}, {"title": null, "content": "predictable. This linear behavior is an inherent problem with the model since it can only account for first order effects between the variables."}, {"title": "5. Conclusion", "content": "This study shows that Physics-Informed Neural Networks can accurately anticipate the mechanical behavior of architected lattice structures. The proposed PINN model outperformed traditional linear regression, obtaining 39% higher accuracy in R\u00b2 score and lowering prediction errors by almost 50%. The model successfully reflects the complicated interactions between material properties, applied loads, and consequent deformations, and it excels at handling non-linear behaviors that conventional techniques struggle with."}, {"title": null, "content": "Future research directions could include:\n\u2022 Extending the model to anticipate dynamic loading and fatigue behavior.\n\u2022 Incorporating microstructural characteristics and manufacturing restrictions\n\u2022 Creating real-time optimization tools for lattice structure design\n\u2022 Increasing the material database to include composites and functionally graded materials.\n\u2022 Using multi-objective optimization for both strength and weight considerations\n\u2022 Examining thermal-mechanical coupling effects in lattice structures"}]}