{"title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models", "authors": ["Daniel P. Jeong", "Pranav Mani", "Saurabh Garg", "Zachary C. Lipton", "Michael Oberst"], "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision-language models (VLMs) via continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining (DAPT) improves performance on downstream medical tasks, such as answering medical licensing exam questions. In this paper, we compare ten public \"medical\" LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting and supervised fine-tuning regimes for medical question-answering (QA). For instance, across all tasks and model pairs we consider in the 3-shot setting, medical LLMs only outperform their base models in 22.7% of cases, reach a (statistical) tie in 36.8% of cases, and are significantly worse than their base models in the remaining 40.5% of cases. Our conclusions are based on (i) comparing each medical model head-to-head, directly against the corresponding base model; (ii) optimizing the prompts for each model separately in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. While these basic practices are not consistently adopted in the literature, our ablations show that they substantially impact conclusions. Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs can show performance improvements, but the benefits do not carry over to tasks based on clinical notes. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.", "sections": [{"title": "1 Introduction", "content": "Recent advances in autoregressive large language models (LLMs) and vision-language models (VLMs) have attracted interest from practitioners in medicine, where these models hold great potential to transform various aspects of clinical practice (e.g., medical diagnosis, information retrieval from clinical documents, patient triaging). State-of-the-art performance on various medical benchmarks is typically achieved by massive-scale closed-source models, such as GPT-4, MED-GEMINI, and MED-PALM, often performing on par with humans on medical licensing exams and open-ended consumer health question-answering (QA) tasks. However, the general lack of transparency in these models, high API usage costs, and patient data privacy concerns make their integration into routine clinical workflows challenging.\nTo address such concerns, recent works have proposed cheaper, open-source alternatives through domain-adaptive pretraining (DAPT; Gururangan et al., 2020), where a pretrained open-source general-domain model\u2014such as LLAMA or MISTRAL in the language space, and LLAVA or OPEN-FLAMINGO in the vision-language space\u2014is continually pretrained on biomedical (image-)text corpora from public sources such as PubMed and medical textbooks. While some prior works show that medical models pretrained from scratch only using domain-specific corpora can outperform those trained via DAPT, both in the context of BERT-style encoder-only models and decoder models , the DAPT approach has become common practice, resulting in a trend where the release of a more capable general-domain model is typically followed by the release of its medical counterpart.\nDespite the widespread adoption of medical DAPT, the claimed improvements in performance are worth scrutinizing. While the story is intuitive, more recent base models often already exhibit strong off-the-shelf performance on medical benchmarks without any adaptation. For instance, as of the time of writing, the general-domain LLAMA-3-8B outperforms other medically specialized models such as MEDITRON-70B and BIOMISTRAL-7B on the Open Medical LLM Leaderboard, which evaluates each model on standard medical QA benchmark datasets such as MedQA and MedMCQA. Moreover, given the general lack of transparency about the pretraining corpora used to train the general-domain model in the first place, it is possible that they may already be trained on relevant medical text.\nPerhaps more concerning is the lack of apples-to-apples comparisons in the literature. First, medical models resulting from DAPT are often only compared against other baselines with different architectures and model scale (e.g., CLINICAL-CAMEL-70B vs. GPT-4). Second, even for the same model scale, models are often evaluated under inconsistent evaluation setups (e.g., MEDITRON-70B fine-tuned on MedQA with gradient updates vs. CLINICAL-CAMEL-70B few-shot prompted on MedQA). Third, the common practice of using a single, fixed prompting setup (e.g., prompt format, choice of few-shot examples) for all models under evaluation also warrants concern, as LLM/VLM behavior is extremely sensitive to such"}, {"title": "2 Related Work", "content": "DAPT is a transfer learning approach, where a pretrained model is further pretrained on domain-specific data for better alignment to a target domain of interest (e.g., medicine, law). Several studies show that language models trained via DAPT"}, {"title": "3 Experimental Setup", "content": "To investigate the effectiveness of medical DAPT in improving (i) zero-/few-shot and (ii) supervised fine-tuning performances, we compare ten medical LLMs and two medical VLMs against their general-domain counterparts in pairs , on 22 textual QA datasets (8 based on clinical notes) and 8 visual QA datasets, respectively. The models in each pair are exactly identical in model architecture and scale, and their only difference lies in whether they were additionally pretrained on medical data. We also note that while some of datasets considered contain both closed-ended (i.e., has clear ground-truth answers) and open-ended questions, we focus our evaluations on the former, where an objective, quantitative assessment of medical knowledge and reasoning capabilities is possible. For reproducibility of our results, we open-source the source code used for all of our evaluations described below via our GitHub repository.\nModels. In Table 1, we provide a summary of all of the LLM and VLM pairs that we use for evaluation, along with details about the pretraining corpora used for adaptation to the medical domain. For LLAVA, we use the very first version (v0) that uses VICUNA-V0 as the LLM backbone, as LLAVA-MED was adapted from that particular version. For all models, we use the checkpoints made available via HuggingFace. In all zero-/few-shot prompting experiments, we generate predictions from each model via (i) greedy decoding (i.e., sampling with temperature T=0)"}, {"title": "3.1 Zero-/Few-shot Prompting with Model-Specific Prompt Selection", "content": "In this section, we provide an overview of our approach to assess whether medical DAPT leads to statistically significant improvements in zero-/few-shot medical QA performance. For few-shot prompting, we consider the 3-shot setting to ensure that the input prompt is shorter than the context window sizes for all models evaluated. Meanwhile, we exclude the EHRNoteQA and i2b2 datasets from few-shot prompting evaluations for medical LLMs, as most of the clinical notes in these datasets already occupy the full context window for most models even in the zero-shot setting. For evaluation, we pay special attention to two aspects. First, language models are highly sensitive to the choice of prompting strategy (e.g., prompt format, choice of few-shot examples), where seemingly insignificant changes to the prompt can lead to idiosyncratic model behavior . Second, prior works show that the \"optimal\" choice of prompt format rarely correlates between different models, suggesting that using a single, fixed prompt for all models for comparison can result in misleading conclusions.\nTo ensure a fair comparison that isolates the impact of medical DAPT, we treat the choice of prompt format and few-shot examples as additional hyperparameters when generating predictions, and tailor them to each model independently. We first randomly sample 10 plausible prompt formats from a predefined search space and 10 different sets of few-shot examples from the training set of each dataset. We then search over all pairs of prompt formats (plus one additional manually designed default format) and few-shot examples, and select the best pair out of (10+1) \u00d7 10 = 110 that results in the highest validation exact-match accuracy. Given that a grid search at this scale can be computationally expensive, especially for datasets like MedMCQA that contain 37k validation QA pairs , we randomly subsample 500 validation QA pairs for datasets that have more than 500. Using the vLLM framework for sampling model outputs, this leads to a runtime of around 5-15 minutes per trial, when using 4 NVIDIA A6000 GPUs for the 70B models and 2 GPUs for the others. We then generate the final predictions on the test set using the prompt format and few-shot samples selected for each model. In the zero-shot setting, we only search over the prompt formats.\nTo define the prompt format search space, we follow the approach by Sclar et al. (2024) and construct a context-free grammar of semantically equivalent yet syntactically distinct prompt formats . For the medical models that have a specific prompt format"}, {"title": "3.2 Supervised Fine-tuning", "content": "To assess whether medical DAPT leads to a better initialization of the model parameters for supervised fine-tuning on downstream medical QA tasks, we compare the performances of models in each pair after fine-tuning each on the training split of each QA dataset. Due to computational constraints, we limit our discussions to LLM pairs with 7-8B parameters per model and all VLM pairs (which have 7-9B parameters per model) and focus on parameter-efficient fine-tuning instead of full fine-tuning. For textual QA evaluations, we exclude the MMLU-Medical datasets given that the training sets only include 5 examples per subject. For visual QA evaluations, we exclude the MMMU-Medical datasets for the same reason.\nParameter-efficient fine-tuning. For the MED-FLAMINGO-9B and OPEN-FLAMINGO-9B, we fully fine-tune the gated cross-attention layers, the Perceiver resampler , and the embeddings of the <image> and <|endofchunk|> special tokens, which already only correspond to a subset of all model parameters. For all other LLMs and VLMS, we fine-tune each model with low-rank adapters (LoRA; Hu et al., 2022). We add the low-rank adapters to all of the linear layers (i.e., the attention and the feedforward network layers in each Transformer block), following prior works that demonstrate its effectiveness over adapting a subset of the linear layers . We fine-tune the trainable parameters by minimizing the cross-entropy loss on the output tokens generated conditional on the input context (i.e., the task instruction, image, and question). We train all models for a maximum of 10 epochs and apply early stopping regularization, monitoring the validation loss after every epoch of training and enforcing a patience of 1. To apply the gradient updates, we use the AdamW optimizer with the recommended momentum hyperparameters \u03b2\u2081 = 0.9, \u03b22 = 0.999, and \u03f5 = 10\u207b\u2078 ; and a cosine learning rate schedule with a linear warmup of 0.05 \u00d7 maximum number of training steps. For MED-FLAMINGO-9B and OPEN-FLAMINGO-9B, we perform a grid search over the learning rates and weight decay coefficients and use the validation loss to select the best setting for final evaluation. For all other models, we perform a grid search over the LoRA ranks and learning rates and use the validation loss to select the best setting. We perform all experiments using the DeepSpeed Zero Redundancy Optimizer (ZERO) and PyTorch FSDP frameworks for distributed training."}, {"title": "4 Zero-/Few-Shot Prompting Evaluation Results", "content": "Here, we summarize the main findings from the zero-/few-shot prompting experiments outlined in Section 3.1. Unless specified otherwise, we focus on the greedy decoding results in subsequent discussions and include the results for constrained decoding in Appendix E. Overall, we find that all medical VLMs and the majority of medical LLMs fail to consistently improve over their general-domain counterparts in the zero-/few-shot prompting regime (Section 4.1). Notably, we find that the medical LLMs do not show consistent improvements on either the medical knowledge QA datasets or the clinical note QA datasets in this setting. Moreover, we demonstrate the importance of rigorous experimental design in surfacing this finding-performing pairwise model comparison with a single, fixed prompt optimized only for the medical model, while ignoring statistical uncertainty, can paint a misleadingly optimistic picture of off-the-shelf performance benefits from medical DAPT (Section 4.2)."}, {"title": "4.1 Performance Benefits from Medical DAPT Largely Diminish After Model-Specific Prompt Selection and Statistical Testing", "content": "We first show that all medical VLMs and the majority of medical LLMs fail to consistently outperform their corresponding base models in terms of zero-shot and 3-shot performance, after model-specific prompt selection and statistical testing. We calculate the 95% confidence intervals in relative exact-match accuracy via bootstrapping on the test set, as described in Section 3. Below, we present the results on textual medical knowledge QA datasets, textual clinical note QA datasets, and the visual QA datasets one-by-one."}, {"title": "4.1.1 EVALUATION OF MEDICAL LLMS ON TEXTUAL MEDICAL KNOWLEDGE QA", "content": "In Figure 3, we show the absolute and relative exact-match accuracies achieved by the medical and general-domain LLMs on the textual medical knowledge QA datasets, from zero-shot (Figure 3(a)) and 3-shot prompting (Figure 3(b)), respectively. In Table 2, we also show the win, tie, and loss rates (%) of the medical LLMs, where win rate refers to the proportion of QA datasets where a medical model shows a statistically significant improvement over its base model. For each medical model in Table 2, we boldface the win rate if it wins more than it loses to its general-domain base model, and vice versa. We exclude the results for CLINICAL-CAMEL-70B on both versions of MedQA, and the MED42 models on all textual medical knowledge QA datasets, as they have already been trained on these datasets as part of medical DAPT (see Table 1).\nThe top rows of Figure 3(a) and (b) show that the absolute exact-match accuracies are mostly similar between each model pair across all datasets and model scales, with marginal performance improvements. Going from the zero-shot setting to the 3-shot setting, we observe that both medical and general-domain models generally achieve higher accuracy across all datasets, but the pairwise differences in performance remain similarly small.\nWe also find that more recent general-domain models tend to show stronger performance overall even without any medical adaptation via DAPT. For example, LLAMA-3-8B performs on par with the medically adapted MEDITRON-70B and CLINICAL-CAMEL-70B in both zero-shot and 3-shot settings, despite the substantial difference in model scale."}, {"title": "4.1.2 EVALUATION OF MEDICAL LLMS ON TEXTUAL CLINICAL NOTE QA", "content": "In Figure 4, we show the absolute and relative exact-match accuracies achieved by the medical and general-domain LLMs on the textual clinical note QA datasets, from zero-shot (Figure 4(a)) and 3-shot prompting (Figure 4(b)), respectively. In Table 3, we also show the win, tie, and loss rates (%) of the medical LLMs, where win rate refers to the proportion of QA datasets where a medical model shows a statistically significant improvement over its base model. For each medical model in Table 3, we boldface the win rate if it wins more than it loses to its general-domain base model, and vice versa. As discussed in Section 3.1, we exclude the EHRNoteQA and i2b2 datasets from few-shot prompting evaluations, as most of the clinical notes in these datasets already occupy the full context window for\nmost models even in the zero-shot setting. Additionally, we exclude the zero-shot results on these datasets for MEDITRON-7B (and therefore its base model LLAMA-2-7B), as its small context window size of 2k tokens is insufficient even in the zero-shot setting.\nThe top row of Figure 4(a) shows that in comparison to the zero-shot results on textual medical knowledge QA tasks (Figure 3), the extent of performance improvement achieved by medical LLMs varies more significantly across different model pairs and clinical note QA datasets in the zero-shot setting. In particular, we find that the results vary the most on the four i2b2 datasets, where MED42-v1-70B and CLINICAL-CAMEL-70B tend to show large improvements (\u2248 20%+ in absolute terms), while MEDITRON-70B, BIOMISTRAL-7B, and BIOMEDGPT-LM-7B tend to perform worse than their general-domain counterparts. In part, we note that such variability results from significant differences in the overall syntax, length, and format of the clinical notes provided as context. For the i2b2 datasets, the clinical notes are provided in full with minimal preprocessing, while other datasets often only include short snippets containing just a few sentences extracted from the full note (e.g., MedNLI, CASI & MIMIC-III sense disambiguation datasets) or apply more extensive preprocessing (e.g., EHRNoteQA).\nMeanwhile, we find that even after performing model-specific prompt selection , BIOMEDGPT-LM-7B generally fails to output an appropriate response on these datasets, often refusing to produce an answer or generating irrelevant text completions. In Figure E2, we observe that BIOMEDGPT-LM-7B achieves higher accuracy upon constraining its output vocabulary, albeit with limited improvements over its base model.\nThe bottom row of Figure 4(a) shows that in aggregate, the medical models do not consistently improve over their base models in the zero-shot setting. In fact, the win, tie, and loss rates presented in Table 3 show that only 3 out of 10 medical LLMs\u2014MED42-v1-70B, CLINICAL-CAMEL-70B, and MED42-V2-8B\u2014win more than they lose to their base models in the zero-shot setting. Across all (model pair, textual clinical note QA dataset) combina-"}, {"title": "4.1.3 EVALUATION OF MEDICAL VLMS ON VISUAL MEDICAL QA", "content": "In Figure 5, we show the absolute and relative exact-match accuracies achieved by the medical and general-domain VLMs on the visual QA datasets, from zero-shot (Figure 5(a)) and 3-shot prompting (Figure 5(b)), respectively. In Table 4, we also show the win, tie, and\nloss rates (%) of the medical VLMs, where win rate refers to the proportion of visual QA datasets where a medical model shows a statistically significant improvement over its base model. For each medical model in Table 4, we boldface the win rate if it wins more than it loses to its general-domain base model, and vice versa. Meanwhile, we note that neither LLAVA-MED-V0-7B nor its general-domain base model LLAVA-V0-7B were pretrained to handle inputs with multiple images.\nThe top rows of Figure 5(a) and (b) show that in both zero-shot and 3-shot settings, the absolute exact-match accuracies are mostly similar between each model pair on the VQA-RAD, PathVQA, and SLAKE datasets. On the three datasets, we find that, in absolute terms, the accuracies tend to be higher for all models in the 3-shot vs. the zero-shot setting, but that the pairwise differences remain similarly small in both settings. For MMMU-Medical on the other hand, we see that the extent of performance improvement (or lack thereof) varies significantly across the two model pairs and datasets, and that the absolute exact-match accuracies do not exhibit an increasing trend going from zero-shot to 3-shot prompting. For example, when comparing the absolute accuracies between each VLM pair in the zero-shot setting, LLAVA-MED-7B generally performs significantly worse than LLAVA-V0-7B, while MED-FLAMINGO-9B generally performs significantly better than OPEN-FLAMINGO-9B across all subjects in MMMU-Medical. Meanwhile, the bottom rows of Figure 5(a) and (b) clearly show that the 95% confidence intervals for the MMMU-Medical datasets almost always cross zero, indicating that the observed differences are not statistically significant. Notably, the confidence intervals are much wider on MMMU-Medical than on the other datasets, as the test sets only include 25 QA examples per subject in MMMU-Medical.\nWhen we aggregate the results across all model pairs and visual QA datasets while accounting for such statistical uncertainty, we find that the medical VLMs only achieve win/tie/loss rates of 6.3%/75.0%/18.8% in the zero-shot setting and 6.3%/81.3%/12.5% in the 3-shot setting . In fact, we find that both LLAVA-MED-V0-7B and MED-FLAMINGO-9B are virtually indistinguishable from their base models in terms of performance: LLAVA-MED-V0-7B achieves win/tie/loss rates of 12.5%/62.5%/25.0% in both zero-shot and 3-shot settings, while MED-FLAMINGO-9B achieves win/tie/loss rates of 0%/87.5%/12.5% in the zero-shot setting and 0%/100%/0% in the 3-shot setting.\nWe observe that these conclusions also hold when the model predictions are generated via constrained decoding . Figure E3 shows that the medical VLMs almost always reach a tie with their base models in both zero-shot and 3-shot settings, with the 95% confidence intervals crossing zero. In Table E3, we show that the medical VLMs collectively achieve win/tie/loss rates of 6.3%/87.5%/6.3% in the zero-shot setting and 0%/93.8%/6.3% in the 3-shot setting. In fact, no medical VLM shows improvement over its general-domain counterpart, regardless of the decoding strategy.\nIn summary, these results show that after appropriately accounting for prompt sensitivity and statistical uncertainty, the medical VLMs trained via DAPT show limited improvements in zero-/few-shot prompting performance over their general-domain counterparts, on visual QA tasks spanning various medical imaging modalities (e.g., pathology, radiology)."}, {"title": "4.2 Overlooking Prompt Sensitivity and Statistical Uncertainty May Overestimate the Performance Benefits from Medical DAPT", "content": "Based on our findings presented in Section 4.1, we further investigate whether the conclusions differ if the same prompt is used for each pair of medical and general-domain models. In particular, we consider whether selecting a prompt only for the medical model, following Section 3.1, and using it for the corresponding general-domain model can widen the performance gap between each pair. We also assess whether this gap becomes amplified when models are compared without appropriately accounting for statistical uncertainty, which is often observed in practice. More concretely, we evaluate how the win/tie/loss rates of"}, {"title": "4.2.1 Selecting the \u201cRight\u201d Prompt Independently is Crucial for a Fair Comparison Between Models", "content": "In Figure 6, we show how the win/tie/loss rates change when we optimize the prompt only for the medical model, using Clinical-Camel-70B, OpenBioLLM-8B, and LLaVA-Med-v0-7B as an illustrative example. For Clinical-Camel-70B and OpenBioLLM-8B, we show the 3-shot results on the textual medical knowledge QA datasets (see Section 4.2.3 for discussion of results on textual clinical note QA, which are similar). For LLaVA-Med-v0-7B, we show the 3-shot results on all visual medical QA datasets. We note that the results here are shown with statistical testing, i.e., the win/tie/loss rates are calculated based on the 95% bootstrapping confidence intervals, as described in Section 3.\nIn all three cases, we observe that when we optimize the prompt only for the medical model, the win rates increase significantly and the loss rates decrease significantly. For example, for Clinical-Camel-70B , we find that win rate increases from 0% to 54.5% and that the loss rate decreases from 36.4% to 0%, completely reversing the conclusion about Clinical-Camel-70B. We also observe that such trends exist regardless of model scale: across both 70B-parameter and 7/8B-parameter models, the results are highly sensitive to the choice of prompt format and few-shot examples.\nNotably, we find that such differences in performance can arise as a result of seemingly insignificant differences in the prompt. As an illustrative example, we show the prompt formats optimized for Clinical-Camel-70B and its base model Llama-2-70B on MMLU-Medical (Clinical Knowledge):\nPrompt Formats for Clinical-Camel-70B vs. Llama-2-70B on MMLU Medical (Clinical Knowledge)\nPrompt Format Optimized for Clinical-Camel-70B:\n**Question**= Glycolysis is the name given to the pathway involving the conversion of:\n(A) glycogen to glucose-1-phosphate.\n(B) glycogen or glucose to fructose.\n(C) glycogen or glucose to pyruvate or lactate.\n(D) glycogen or glucose to pyruvate or acetyl CoA.\n**Answer**=\nPrompt Format Optimized for Llama-2-70B:\n### Question: Glycolysis is the name given to the pathway involving the conversion of:\n(A) glycogen to glucose-1-phosphate.\n(B) glycogen or glucose to fructose.\n(C) glycogen or glucose to pyruvate or lactate.\n(D) glycogen or glucose to pyruvate or acetyl CoA.\n### Answer:\nWe find that the main difference in the prompt, modulo the selected few-shot examples, lies on how the question and answer headers are formatted, with the overall semantics of the overall prompt remaining completely intact. We also note that the task instruction"}, {"title": "4.2.2 IGNORING STATISTICAL UNCERTAINTY CAN LEAD TO MISLEADING CONCLUSIONS ABOUT PERFORMANCE IMPROVEMENTS FROM MEDICAL DAPT", "content": "In Figure 7, we show how the win/tie/loss rates change when we remove statistical testing, comparing the performances of each model pair solely based on raw absolute accuracy. As in"}, {"title": "4.2.3 IGNORING PROMPT SENSITIVITY AND STATISTICAL UNCERTAINTY CAN OVERESTIMATE THE PERFORMANCE BENEFITS FROM MEDICAL DAPT", "content": "Based on the findings discussed in Sections 4.2.1-4.2.2, we investigate how the lack of (i) independent prompt selection and (ii) statistical testing can change the broader conclusions"}, {"title": "5 Supervised Fine-Tuning Evaluation Results", "content": "Here, we summarize the main findings from the supervised fine-tuning experiments outlined in Section 3.2, focusing on the 7/8B-parameter medical LLMs and all medical VLMs. Overall, we find that after supervised fine-tuning, medical LLMs do show statistically significant improvements on the textual medical knowledge QA datasets (Section 5.1) but not on the textual clinical note QA datasets (Section 5.2), while medical VLMs do not show consistent improvements on visual QA (Section 5.3)."}, {"title": "5.1 Evaluation of Medical LLMs on Textual Medical Knowledge QA", "content": "In Figure 9(a), we show the absolute and relative exact-match accuracies achieved by the medical and general-domain LLMs on the textual medical knowledge QA datasets, after fine-tuning each model. In Table 5 (left), we also show the win, tie, and loss rates (%) of the medical LLMs on these datasets. We boldface the win rate if it wins more than it loses to its general-domain base model, and vice versa. As discussed in Section 3.2, we do not evaluate on the MMLU-Medical datasets, which only have 5 examples in the training set per subject . We also exclude the results for MED42-V2-8B, since it has already been trained on all medical knowledge QA datasets as part of medical DAPT (see Table 1)."}, {"title": "5.2 Evaluation of Medical LLMs on Textual Clinical Note QA", "content": "In Figure 9(c), we show the absolute and relative exact-match accuracies achieved by the medical and general-domain LLMs on the textual clinical note QA datasets, after fine-tuning each model. In Table 5 (right), we also show the win, tie, and loss rates (%) of the medical LLMs on these datasets. We boldface the win rate if it wins more than it loses to its general-domain base model, and vice versa. As in the zero-/few-shot prompting evaluations in Section 4.1.2, we exclude the results on these datasets for MEDITRON-7B, as its small context window size of 2k tokens in insufficient for handling full-length clinical notes.\nThe top and bottom rows of Figure 9(c) clearly show that across all model pairs and datasets, the fine-tuning performances of the medical LLMs and their general-domain counterpart are almost exactly identical. In Table 5 (right), we find that across all (model pair, QA dataset) combinations, 92.5% of cases result in a statistical tie, with the medical models losing (5%) more than they win (2.5%) against their base models. In fact, we find that 4 out of 5 medical LLMs are completely indistinguishable from their base models in terms of the win/tie/loss rates, with the remaining model (BIOMEDGPT-LM-7B) losing more than it wins. Meanwhile, we observe that while the 7/8B-parameter LLMs do not consistently perform well off-the-shelf on clinical note QA (Section 4.1.2), both medical and general-domain models uniformly show strong performance after being fine-tuned towards each task.\nThese results suggest that additional training on biomedical text data, as commonly done for the medical LLMs that we evaluate (Table 1), is not particularly beneficial for improving downstream supervised fine-tuning performance on QA tasks involving clinical notes."}, {"title": "5.3 Evaluation of Medical VLMs on Visual Medical QA", "content": "In Figure 9(b), we show the absolute and relative exact-match accuracies achieved by the medical and general-domain VLMs on the visual QA datasets, after fine-tuning each model. In Table 6, we also show the win, tie, and loss rates (%) of the medical VLMs on these"}, {"title": "6 Discussion and Conclusion", "content": "In this work, we investigated the effectiveness of DAPT for training medically specialized LLMs and autoregressive VLMs suitable for various medical (visual) QA tasks. To that end, we compared several pairs of state-of-the-art medical LLMS/VLMs to their general-domain counterparts, whose only differences lie in medical DAPT and are exactly identical in model architecture and scale. Our work diverges from prior works by providing a direct apples-to-apples comparison of medical and general-domain models while accounting for LLM/VLM sensitivity to prompting details and assessing the statistical significance of the results. We also ensure that our evaluations are comprehensive, by (i) comparing the models in both the zero-/few-shot prompting and supervised fine-tuning regimes, and (ii) covering both QA tasks focused on assessing medical knowledge and those based on real-world clinical notes.\nIn the zero-/few-shot prompting regime, we found that across both model classes and all model scales, the performance benefits from medical DAPT largely disappear when we (i) tailor the prompt format and choice of few-shot examples to each medical and general-domain model separately; and (ii) account for statistical uncertainty in model comparison . In particular, we found that when we optimize the prompt only for the medical"}, {"title": "7 Limitations", "content": "We discuss our findings with the following caveats. First, there is a vast and growing set of papers on applying medical DAPT to various general-domain base models, and we could not hope to compare all publicly available models here. While we selected the models to cover a wide range of general-domain base models and model scales (7B-70B) and included some of the latest models (e.g., OPENBIOLLM and LLAMA-3), it is always possible that some newly released models do in fact yield better zero-/few-shot or supervised fine-tuning performance on medical QA.\nSecond, we focus in this paper on the narrower task of closed-ended medical QA. In part, this choice reflects the fact that such benchmarks are well-standardized and highly publicized. However, they do not reflect the breadth of possible applications of LLMs and VLMs in medical domains. For instance, Singhal et al. (2023b) show that medical LLMs such as MED-PALM-2 can produce physician-level answers to open-ended consumer health queries; Liu et al. (2022) explore the use of LLMs for generating discharge instructions based on a patient's health records; and Li et al. (2023b) demonstrate the potential of using LLMs to recommend treatments for patients based on patient-doctor conversations. Some would argue that such tasks are a more realistic application of such models in practice, and it is certainly possible that an analysis like ours would find improved performance on such tasks, though we do not investigate these tasks in the present work.\nThird, due to computational constraints, we do not consider full fine-tuning of models and exclude the larger 70B-parameters from evaluation, for evaluations in the supervised fine-tuning regime. Prior works show that parameter-efficient fine-tuning performance (e.g., LoRA) can be worse than that of full fine-tuning, and that the gap in performance varies significantly across different tasks . While"}]}