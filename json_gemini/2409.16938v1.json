{"title": "Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model", "authors": ["Hongliang Zhong", "Can Wang", "Jingbo Zhang", "Jing Liao"], "abstract": "Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object insertion in 3D content represented by Gaussian Splatting. Our approach introduces a multi-view diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable video diffusion model to facilitate view-consistent object inpainting. Within MVInpainter, we incorporate a ControlNet-based conditional injection module to enable controlled and more predictable multi-view generation. After generating the multi-view inpainted results, we further propose a mask-aware 3D reconstruction technique to refine Gaussian Splatting reconstruction from these sparse inpainted views. By leveraging these fabricate techniques, our approach yields diverse results, ensures view-consistent and harmonious insertions, and produces better object quality. Extensive experiments demonstrate that our approach outperforms existing methods.", "sections": [{"title": "1. Introduction", "content": "Recent advances in novel view synthesis, particularly in techniques such as neural radiance fields (NeRFs) [30, 2, 3, 48, 28] and Gaussian Splatting [20, 35, 53, 49], have significantly propelled the progress in 3D reconstruction applications [27, 46, 7]. Consequently, the demand for creating new objects in a 3D content to achieve versatile recreation is rising rapidly. These advancements have opened new possibilities for enhancing the fidelity and usability of reconstructed scenes, benefiting virtual reality, gaming, and digital content creation. However, generating and inserting new objects into a 3D scene remains a challenging task due to the following reasons: 1) Ensuring 3D-consistent generation and placement of objects from different viewpoints. 2) Producing high-quality new objects with fabricate geometry and texture 3) Achieving harmony between the inserted object and the existing scene.\nMotivated by the success of diffusion models [39, 57, 6, 18, 58, 59, 9], various approaches [13, 8, 4] have sought to tackle theses challenges using Score Distillation Sampling (SDS) [34] of diffusion models. The core principle involves optimizing 3D representations by encouraging their rendered images to move toward high-probability density regions conditioned on text, with supervision provided by a pre-trained 2D diffusion model [56]. Building on this principle, new objects can be created by optimizing the 3D representation in alignment with the direction guided by the SDS process. These methods can produce view-consistent and harmonious 3D objects from diverse text prompts without requiring 3D data for training. However, SDS optimization often suffers from high optimization randomness and saturation issues [25, 11], leading to less satisfactory visual quality. In contrast, our method employs a multi-view diffusion model to inpaint view-consistent objects for 3D generation, thereby avoiding the issues associated with SDS optimization and achieving high-quality, realistic results.\nThere are approaches that use image inpainting and 3D reconstruction to insert new objects into a 3D scene without using SDS optimization. Some methods [26, 31] inpaint the target object on a single view, then use the estimated depth to warp this object into 3D space, serving as an initialization [26] or supervised signals [31] for 3D object reconstruction. After inpainting the target object, more intuitive methods [40, 11] utilize a single-view 3D reconstruction model to obtain the target 3D object from its 2D appearance. This generated 3D object is then placed into the original 3D scene according to the depth information. These methods excel at creating realistic and consistent objects but focus solely on harmonization within the inpainting view, failing to achieve consistent results across all viewpoints. In contrast, our multi-view diffusion model ensures harmonious inpainting across multiple viewpoints.\nIn summary, we propose a multi-view diffusion model for generative object insertion. Specifically, given a pre-trained 3D scene representation (we use Gaussian Splatting for its ability in fast and high-quality novel view synthesis), a 3D bounding box (BBox) indicating the target location, and a textual description of the target object, we first use SDS to obtain a coarse model, similar to other SDS-based methods [34, 13]. Next, we derive backgrounds, BBox-level masks, and depth maps from both the original 3D scene and the coarse model. We then design a multi-view diffusion model, MVInpainter, that takes these three sets along with the text prompt as input to produce view-consistent inpainting results"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. 3D Content Editing", "content": "3D content editing [62, 10, 41] has experienced significant advancements, particularly in the modification [52, 60] of geometry and appearance based on textual and spatial guidance. Several recent methods have shown remarkable promise using various inputs such as text prompts [33], sketches [29], and reference images [1, 14, 15], as well as leveraging models like CLIP [43, 45, 17] and diffusion models [33, 16, 39, 57, 6, 18, 58, 59, 9, 51, 54] to provide editing directions. Among these works, diffusion-based methods, in particular, have significantly enhanced the effectiveness of 3D content editing by utilizing the strong capabilities of diffusion models in text-based generation and editing. However, these methods primarily focus on localized edits, altering existing objects in the 3D scene rather than creating new objects from scratch or removing objects. In recent years, there has been a growing body of work focusing on object removal in 3D scenes by exploiting techniques such as differentiable mesh optimization [44], multi-view segmentation guidance [55, 32], and depth priors [23]. These methods typically involve delineating the editing boundaries in different viewpoints based on the shape of the target object. Consequently, they are unable to generate new objects in a scene from scratch without new object-related priors [40]. In contrast, our work targets the creation of 3D-consistent, high-quality, and background-harmonious new objects in 3D scenes from scratch."}, {"title": "2.2. SDS-based Object Insertion", "content": "To facilitate object generation and insertion in 3D scenes, some studies [13, 8, 4] have integrated the classic Score Distillation Sampling (SDS) algorithm [34] into 3D generation. This integration aims to leverage SDS's robust generative capabilities to directly produce viewpoint-consistent 3D content within scenes. Drawing upon prior knowledge from 2D diffusion, SDS optimizes the rendered images of 3D models to align with the desired image distribution conditioned on text. This ensures that the edited 3D model meets the desired outcomes across various viewpoints, enabling the generation and insertion of 3D content from scratch. ReplaceAnything3D [4] enhances this approach by incorporating an improved version of SDS, known as HiFA [61], for generative object insertion, thereby enhancing the realism of editing effects. MVIP-NeRF [8] proposes a multi-view SDS that concurrently supervises surface normals and scene appearance, providing stronger supervision for scene editing and achieving improved results. While SDS-based methods effectively achieve viewpoint-consistent 3D content generation and editing, the inherent stochasticity in their optimization process often results in noticeable quality issues, such as over-smoothness [22, 47] and high saturation levels [25, 11]. In contrast, our method avoids these issues by using SDS optimization solely to provide coarse initial priors rather than the final editing output. We then design a multi-view diffusion model to inpaint view-consistent editing results."}, {"title": "2.3. Combine Image Inpainting and 3D Reconstruction for Object Insertion", "content": "In the quest for high-quality 3D content generation and insertion, several methods [38, 31, 11, 26, 40] have explored combining image inpainting and 3D reconstruction for 3D object insertion. For instance, InFusion [26] starts by inpainting and inserting the object in 2D from a single reference view. It then projects this inpainted 2D appearance back into the 3D space of the original scene and performs a brief reconstruction optimization in the reference viewpoint to produce the final edited scene. On the other hand, after inpainting the object in an image, GaussianEditor [11] first reconstructs the complete shape of the target object from its appearance in this reference image using a single-view 3D reconstruction model [27]. It then predicts the depth and integrates the object back into the original scene. Both approaches are limited by their reliance on single-viewpoint inpainting, which can undermine the seamless integration of the reconstructed object with the environment from other viewpoints. To tackle these challenges, we present the MV-Inpainter model along with a mask-aware 3D reconstruction method, specifically designed to produce 3D-consistent"}, {"title": "3. Generative Object Insertion", "content": "We aim to create new objects within a 3D scene based on a user-provided text prompt that specifies the target object. Additionally, the user can indicate the insertion region interactively. To achieve this, we have proposed a framework comprising three main components, as shown in Fig. 1:\nInputs. The inputs to our method include a pre-trained 3D Gaussian representing a 3D scene, a text prompt $y$ describing the target object for insertion, and a 3D bounding box (BBox) $b$ indicating the insertion region. We illustrate the BBox creation process in Fig. 2. First, we sample a point cloud from $\\Theta$. This point cloud is then visualized using 3D software. Next, we place a cube to represent the BBox. Users can easily translate, rotate, and scale the BBox to define the editing region.\nGenerating a new object from scratch without any shape reference poses a significant challenge in maintaining view consistency. To address this, we propose using SDS optimization [34, 42] to provide a coarse geometry prior.\nSpecifically, we generate a coarse geometry based on the text prompt $y$ within the original 3D Gaussian $\\Theta$. We refer to this optimized 3D Gaussian with the coarse geometry prior as a coarse model $\\Theta'$. It is important to note that, unlike previous works [13, 8, 4], we do not use the SDS optimization results as the final output. Instead, we use them solely as a coarse geometry prior, since SDS optimization can lead to poor visual results, particularly issues with over-smoothness and high saturation.\nMVInpainter for Inpainting. We want the MVInpainter to capture the background from the original scene, the editing region from the BBox, and the geometry prior from the coarse 3D Gaussian model $\\Theta'$, across a range of coherent viewpoints. To achieve this, we start by defining a circular trajectory revolving around the vertical axis of $b$. From this trajectory, we uniformly sample $n$ viewpoints, denoted as $\\{C_{\\text{edit}}^i\\}_{i=1}^n$, where all cameras from these viewpoints are directed towards the center of $b$. For each editing viewpoint, we render the background image $I_{bg}^i$ based on the original Gaussian, generate the 2D editing mask $M^i$ corresponding to the BBox $b$, and produce a depth reference image $D^i$ derived from the coarse Gaussian $\\Theta'$. The MVInpainter takes the image sequences $\\{I_{bg}^i\\}_{i=1}^n$, $\\{M^i\\}_{i=1}^n$, $\\{D^i\\}_{i=1}^n$, and the text prompt $y$ as inputs, and produces view-consistent inpainting results $\\{I'^i\\}_{i=1}^n$ that match the user requirements described by $y$.\nIn our approach, the MVInpainter consists of two branches: a multi-view diffusion module (MVD), inspired by the recent Stable Video Diffusion (SVD) [5], and a ControlNet-based condition injection module, adapted from ControlNet [58].\nAs an image-to-video model, SVD generates a coherent set of image sequences based on an input image, known as the conditioning image. To adapt SVD into our MVD for consistent multi-view generation, we employ a depth-guided"}, {"title": "4. MVInpainter and Mask-aware Reconstruction", "content": "In this section, we first delve deeper into the details of our MVInpainter, which comprises an MVD module and a ControlNet-based condition injection module (Sec. 4.1), as illustrated in Fig. 3. We then explain the technical details of our mask-aware reconstruction method (Sec. 4.2)."}, {"title": "4.1. MVInpainter", "content": "As depicted in the upper part of Fig. 3, our MVD module is built on the SVD framework, which is an image-to-video model fine-tuned from the Stable Diffusion Model. The SVD model takes a conditioning image, $I^c$, as its input. First, $I^c$ is processed by the pre-trained VAE encoder of the SVD, generating an output that is duplicated $n$ times to create a conditioning latent sequence $\\{Z_c^i\\}_{i=1}^n$. These latent representations are then concatenated along the channel dimension with the noisy latent state inputs $\\{Z_{t,i}^{svd}\\}_{i=1}^n$ at time step $t$, forming the input latent sequence $\\{Z_{t,i}^{svd} \\}_{i=1}^n$ for the SVD UNet. Simultaneously, the CLIP embedding of $I^c$, denoted as $E^c$, is extracted and used as the conditioning input to provide semantic guidance. Finally, these inputs are fed into the SVD UNet, where they undergo denoising through the diffusion process.\nWhile SVD is capable of producing coherent video sequences from conditioning images, the trajectory and content of the resulting videos are often unpredictable. The generated frames may not align with the intended viewpoints, and the backgrounds can diverge from the original scene. In contrast, our goal is to generate multi-view inpainting with specific camera trajectories, consistent foreground content, and preserved backgrounds. To accomplish this, we introduce a ControlNet-based condition injection module. This module integrates additional reference information into the SVD, enabling controlled and more predictable multi-view generation.\nAs illustrated in the lower part of Fig. 3, our ControlNet module receives inputs comprising depth maps $\\{D^i\\}_{i=1}^n$, 2D editing masks $\\{M^i\\}_{i=1}^n$, and background images $\\{I_{bg}^i\\}_{i=1}^n$. Following [39], we initially multiply $I_{bg}^i$ by $M^i$, then concatenate the result with $M^i$ and $D^i$ along the channel dimension to create ControlNet Hints $\\{H^i\\}_{i=1}^n$. Subsequently, these hints are input into a trainable CNN downsampler for size adjustment, producing the latent sequence $\\{Z_{ctrl}^i\\}_{i=1}^n$ for the ControlNet UNet. This latent sequence encapsulates reference information, including the rough geometries of the target object, background appearances for each viewpoint, and the editing areas on each viewpoint. In accordance with [58], these inputs undergo processing through a zero-initialized convolution layer before being combined with the latent input from SVD, $\\{Z_{t,i}^{svd}\\}_{i=1}^n$. The combined inputs are then processed by the encoder and middle blocks within ControlNet. These blocks mirror the structure of their counterparts in the pre-trained SVD, with weights initialized by direct replication. The outputs from ControlNet's encoder and middle blocks pass through several zero-initialized convolution layers before being combined with the skip-connection inputs of the corresponding SVD blocks, injecting control signals into the SVD model.\nBy integrating our MVD and ControlNet injection modules, essential references such as the backgrounds of each viewpoint, the editing regions, and the shape prior of the target object are incorporated into the SVD generation process. This combination results in controlled, multi-view outcomes $\\{Z_{t-1,i}^{cn}\\}_{i=1}^n$. After denoising, the final outputs $\\{Z_{0,i}^{cn}\\}_{i=1}^n$ are decoded via the SVD decoder, producing the view-consistent inpainted results that align with the user requirements described by $y$. To ensure the faithful preservation of the scene background in inpainted images, we further employ a video segmentation model [12, 21, 24] to segment the target objects and integrate them with the original background, resulting in the final inpainting results, $\\{I'^i\\}_{i=1}^n$. Subsequently, these images are leveraged to optimize the 3D Gaussian, leading to the edited 3D scene."}, {"title": "4.2. Mask-aware Reconstruction", "content": "Here, we delve into the technical details of our mask-aware reconstruction method. First, for the $u$-th pixel at the $i$-th edited viewpoint, we compute the rendered color using the following formula:\n$I_u^i = \\sum_{j \\in N_p} c_j \\alpha_j \\prod_{k=1}^{j-1} (1 - \\alpha_k).$ (1)\nwhere $N_p$ is a set of sampling points along the camera ray $r$, and $c_j$ and $\\alpha_j$ represent the color and opacity corresponding to sampling point $j$, respectively. Subsequently, we employ the reconstruction loss [20] to finetune the initial 3D Gaussian $\\Theta$:\n$L_{gs}(I', I^{gt}) = (1-\\lambda)\\cdot|I - I^{gt}| + \\lambda\\cdot(1-SSIM(I', I^{gt})),$ (2)\nwhere $I^{gt}$ is the ground truth image corresponding to $I^u$, and $\\lambda$ is a constant weight set to 0.2 following [20]. To ensure that the background remains completely unchanged before and after finetuning, we enhance constraints on the background area by performing reconstruction optimization on the unmasked regions in the training images, in addition to optimizing the edited viewpoints. This helps maintain the integrity of the background:\n$L_{rec}=\\begin{cases}L_{gs}(I', I^{gt}), C \\in \\{C^{edit}\\},\\newline L_{gs}(I' \\cdot (1 - M), I^{gt} \\cdot (1 - M)), C \\in \\{C^{gt}\\}.\\end{cases}$ (3)\nwhere $C^{gt}$ stands for the training set viewpoints of the original Gaussian. We use this reconstruction loss as our final optimization constraint.\nOur mask-aware finetuning strategy significantly improves the reconstruction quality of 3D Gaussians, minimizing floating and noisy artifacts in novel view synthesis. This approach ensures high-quality results even with sparse inpainted views."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "Training Dataset. We utilize the Wild-RGBD dataset [50] to train our MVInpainter. The vanilla dataset comprises about 7,000 collections of single-object-centric scenes spanning 46 common object categories. Each scene in the dataset is captured from around 300 viewpoints along a 360-degree circular trajectory, providing detailed scene appearances and precise object masks at each viewpoint. To improve data quality and ensure its suitability for our generation task, we implement a filtering process that excludes 13 categories with nearly-flat geometry from the original data. Additionally, we carefully select 50 scenes from each of the remaining categories to construct our dataset. In total, our dataset comprises approximately 1,600 scenes, covering a wide range of diverse categories. To create training samples from these selected scenes, we first sample a circular trajectory that matches the range of our inpainting views in each scene. Subsequently, we evenly sample $n$ viewpoints along this trajectory to extract the corresponding observed images and precise masks of the target objects. To eliminate the need for fine-grained masks during inference, we use dilation operations to generate square masks. Besides, we estimate a depth map [37] for each observed image, serving as a reference for inpainting. Ultimately, the observed image sequence, square masks, and depth references of a scene collectively constitute a training data sample that can be used to train our MVInpainter.\nEvaluation Dataset. To comprehensively evaluate the generation capabilities of relevant methods across diverse scenes, we curated facing-forward scenes from the SPIn-NeRF dataset [32] and unbounded scenes from the MipNeRF-360 dataset [3] for our experimental analysis. In total, we use nine scenes (tree, corner, bench, office, bicycle, garden, kitchen, stump, counter) to generate our editing samples.\nBaseline Methods. We compare our model with three state-of-the-art methods, including one SDS-based approach, MVIP-NeRF [8], and two 3D reconstruction-based inpainting methods for object insertion: InFusion [26] and GaussianEditor [11]. We use the officially released code to conduct the comparison experiments.\nMetrics. Following InseRF [40] and InFusion [26], we integrate CLIP Text-Image Similarity (CTIS) to evaluate the consistency between editing results and input texts, and utilize Directional Text-Image Similarity (DTIS) to assess the effectiveness of edits. Here, CTIS computes the cosine similarity between the CLIP [36] embeddings of the text prompt and the rendered images of the edited scene. DTIS measures the similarity of the changing directions between the image and text CLIP embeddings. Additionally, we further employ MUSIQ [19], a non-reference image quality assessment model, to evaluate the authenticity of our edited results [31].\nTraining Details. We train our MVInpainter following a similar optimization process as SVD [5]. Specifically, we synchronize our network parameters with those of the SVD model [5] and conduct 50 epochs of optimization. The training process is performed on four A6000 GPUs, taking approximately 30 hours to complete. For each scene editing task, we initially optimize the 3D Gaussian of the original scene by executing 5000 iterations of SDS optimization to obtain a coarse geometry prior, following the parameter configurations specified in DreamGaussian [42]. Once the coarse geometry and other necessary inputs are prepared, we respectively sample $n=14$ inpainting views on the left and right half of a 120-degree circular trajectory. Subsequently, we leverage the trained MVInpainter to generate the inpainting results for these viewpoints. With the inpainted images, we apply our mask-aware reconstruction strategy, engaging in 30,000 iterations of optimization to achieve the final edited scene. The parameters for this optimization step, including the learning rate, interval for adaptive density control, and densification threshold, are configured in line with the official Gaussian Splatting code [20]. The editing process for a single scene can be completed in approximately one hour on a single GPU."}, {"title": "5.2. Comparisons", "content": "We conduct quantitative and qualitative comparative experiments between our proposed method and the baseline approaches using curated datasets. Our approach outperforms other models in three key areas: maintaining consistency between editing results and textual descriptions, delivering effective edits, and achieving realistic editing effects.\nSpecifically, MVIP-NeRF relies on SDS optimization for object generation. However, in SDS optimization, the update direction at each iteration demonstrates significant randomness [22]. With these diverse update directions applied to the same 3D model, the final results produced by MVIP-NeRF tend to be overly smooth due to the averaging effect, as depicted in Fig. 4. Moreover, SDS optimization requires coupling with a large conditional guidance scale, leading to oversaturated results in content generation. In contrast, our approach involves directly creating 3D-consistent inpainting images from various viewpoints and reconstructing the target object using these images. Such strategy allows us to generate high-quality 3D content without the drawbacks associated with SDS optimization.\nOn the other hand, InFusion performs 2D inpainting on a single view and then projects the inpainted appearance back into 3D based on estimated depth. Nonetheless, since the projected 3D appearance is solely derived from a single-view observation, it can lead to incomplete shapes of the target object. This, in turn, results in fragmented side views of the created content, as depicted in the doll case. Conversely, GaussianEditor first reconstructs the target object from the inpainted image and then places the reconstructed model in 3D to ensure shape completeness. However, due to the side view generation of the reconstructed object does not involve inpainting, it fails to ensure that the created side views of the target object blend seamlessly with the scene, as demonstrated by the black patches on the left side view in the doll case. Additionally, due to the absence of depth information in single-view inpainting, both InFusion and GaussianEditor rely on inaccurate depth estimation for object placement. Consequently, as depicted in the gnome case, InFusion may project parts of the gnome too deeply, leading to floating artifacts. Meanwhile, even after meticulous manual adjustments, GaussianEditor fails to position the gnome accurately, making it floating above the table. In contrast to these methods, our multi-view inpainting approach excels in accurately placing created objects and seamlessly blending their appearance with the scene background across various views, leading to superior editing results."}, {"title": "5.3. Editing Results", "content": "To further illustrate the editing capabilities of our method, we present a wider range of editing results across an increased number of test samples in Fig. 5. It is clear that our method excels in producing results that seamlessly integrate with indoor and outdoor scenes, even under varying lighting conditions. Notably, in cases such as the flower and dog examples, our method exhibits a remarkable ability to preserve view-consistency when creating objects with intricate shapes, underscoring the effectiveness of our approach in achieving high-quality edits across a diverse array of scenes.\nAdditionally, as shown in Fig. 6, our method demonstrates the capability to generate a diverse set of editing outcomes under uniform editing conditions. For instance, in the candlestick case, our method creates candle holders of different sizes, colors, and materials, some lit while others remain unlit. Similarly, in the bear case, the first set of examples features a brighter appearance, while the second set exhibits a darker sheen. Nevertheless, both variations seamlessly blend with the overall scene, appearing highly realistic and showcasing the versatility of our approach.\nFurthermore, as depicted in Fig. 7, the editing results produced by our model maintain a high level of realism and viewpoint consistency across a broad range of observation angles. Even in scenarios involving complex shapes, such as the flower case, our method consistently generates realistic and harmonious 3D results across various viewing angles, whether close or distant, high or low, highlighting the robustness of our approach."}, {"title": "5.4. Ablation Study", "content": "Multi-View Inpainting. To investigate the superiority of using MVInpainter for multi-view inpainting, we carry out a comparative experiment by replacing MVInpainter with a single-view inpainting model [58, 39] and independently creating inpainted content for each viewpoint. Evidently, the single-view inpainting model can not provide constraints on 3D consistency, resulting in divergent objects being generated in different views. For example, in the suitcase case, the single-view inpainting method produces dissimilar suitcases in two separate views, leading to blurry and inconsistent reconstructions. In contrast, our MVInpainter excels at producing view-consistent results across multiple viewpoints. For instance, it generates a consistently colored and shaped suitcase across different views in the suitcase case, enabling view-consistent, high-quality generative object insertion. In addition, we further conduct a comparison to assess the view-consistency of the inpainted images by measuring the reconstruction error between the inpainted images and the reconstructed images. Obviously, both quantitative and qualitative results demonstrate that the content generated by MVInpainter exhibits improved 3D consistency.\nControlNet Module. To validate the necessity of integrating the ControlNet Module into our pipeline, we conducted a comparison by removing the ControlNet Module from our framework and directly utilizing the pretrained SVD model for generation. As shown in the bag case of Fig. 8, only by incorporating the ControlNet module can we generate proper inpainted images that precisely match the camera positions. Specifically, the content produced by single SVD shows minimal changes in appearance between the two viewpoints. However, the actual camera position undergoes a noticeable rotation between the views, leading to significant blurriness in the reconstructed results using this incomplete method. In contrast, our full model produces inpainting content on both viewpoints that align with the camera trajectory, ensuring that the generated content can be effectively reconstructed, thereby facilitating 3D content generation.\nInput Conditions. To validate the necessity of each input for our ControlNet module, we conducted a comparison between the results obtained using all inputs and those achieved using only a subset of inputs. Notably, the inclusion of the editing mask input is crucial for the training of MVInpainter. Therefore, it is not feasible to compare the results obtained without utilizing this input. In the absence of background inputs, it becomes evident that the model struggles to capture accurate background environmental information across varying viewpoints. This limitation leads to the generation of noticeable ghosting artifacts along the boundary of the editing region. For example, in the flower case's second viewpoint, an unnatural extra portion of the flower is synthesized when background inputs are absent. Conversely, our full model adeptly captures the environmental background appearance across diverse viewpoints, enabling accurate differentiation between the foreground and background of the image, thereby facilitating the generation of distinct new content. On the other hand, the lack of the depth condition inputs results in inconsistencies in the generated shape details across viewpoints. For instance, in the flower case, the MVInpainter generates three petals pointing towards the lower left, directly down, and lower right within the red box. However, in the second view of the results without depth input, both the middle and right petals have shifted significantly upwards, pointing towards the lower right and right, respectively. This shift in position between viewpoints introduces significant blurriness in the reconstructed results. In contrast, incorporating the depth reference ensures consistent shapes and positions of elements, enhancing clarity in the reconstructions.\nMask-Aware Reconstrucion. To illustrate the effectiveness of our mask-aware reconstruction technique, we conducted a comparison between the editing results with and without this setting. It is evident that when relying solely on inpainted views for reconstruction, the limited range of viewpoints fails to offer adequate constraints for the scene background. This deficiency leads to visible artifacts at the boundary of the inpainted views' observation range. In contrast, the implementation of our mask-aware reconstruction leverages both the background of the training views and inpainted views to reconstruct edited results. This strategy ensures that the background of the target 3D Gaussian benefits from ample constraints derived from multiple viewpoints, consequently minimizing the occurrence of artifacts. The results demonstrate that, the quality of the background reconstructed through mask-aware reconstruction surpasses the results achieved without employing this technique."}, {"title": "6. Conclusion", "content": "We present a novel approach for 3D generative object insertion. Unlike existing methods that rely on SDS optimization or single-view inpainting with reconstruction, our approach introduces a ControlNet in conjunction with a pre-trained video diffusion model, resulting in the development of a multi-view inpainting diffusion model, MVInpainter. This model ensures consistent inpainting results across various viewpoints, enabling the high-quality generation of 3D-consistent objects that seamlessly integrate into the scene from multiple angles. Additionally, we propose a mask-aware 3D reconstruction technique to improve Gaussian Splatting reconstruction from sparse inpainted views. Extensive experiments demonstrate the effectiveness of our approach in ensuring consistent, realistic, and harmonious insertions, while generating diverse, high-quality 3D content.\nLimitations. While our model excels at generative object insertion, it does have certain limitations. One key challenge is the lack of publicly available sufficient training data, which limits the model's ability to support 360-degree content generation. This limitation could be overcome as more training data becomes available in the future. Another limitation is that the method cannot be directly extended to effective object removal. The main challenge is that the newly patched background often fails to align seamlessly with the original scene, resulting in reconstruction artifacts. Implementing distinct inpainting strategies for void areas and background regions within the editing zone could help address this issue, and we plan to explore this approach in future work."}]}