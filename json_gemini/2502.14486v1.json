{"title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation", "authors": ["Zhuohan Long", "Siyuan Wang", "Shujun Liu", "Yuhang Lai", "Xuanjing Huang", "Zhongyu Wei"], "abstract": "Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to differentiate between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies\u2014inter-mechanism and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) have shown impressive generative capabilities, enabling their use in various fields (Gupta et al., 2023; OpenAI, 2023; Dubey et al., 2024). However, as their instruction-following ability increases, these models have become targets of adversarial attacks, raising significant safety concerns (Bommasani et al., 2021). One prominent issue is the generation of harmful content when facing jailbreak attack (Huang et al., 2023; Liu et al., 2023e), where malicious users craft prompt to bypass the model's internal safety mechanism. Additionally, the introduction of Large Vision-Language Models (LVLMs) (Bai et al., 2023; Liu et al., 2023a; Li et al., 2023a) has added further risks, as these models interact with a broader range of input channels (Gu et al., 2024; Wang et al., 2024a). To address the challenges posed by jailbreak attacks, various defense strategies have been developed, including modifying system prompts (Zhang et al., 2023b; Xie et al., 2023), adjusting training or decoding processes (Qi et al., 2023; Xu et al., 2024b), and processing input queries and images (Zhang et al., 2023a; Ji et al., 2024; Wang et al., 2024b). These methods present distinct advantages and limitations\u2014some improve safety but result in over-defense (Jiang et al., 2024), while others provide limited safety improvements and remain vulnerable to minor input changes. A deeper understanding of these trade-offs and a systematic comparison of defense mechanisms is still lacking. Additionally, how to effectively combine different strategies for a better balance between safety and helpfulness remains an open challenge. In this work, we examine the mechanisms behind jailbreak defenses by reformulating the generative task as a classification problem, focusing on the trade-off between safety and helpfulness (Wei et al., 2024; M\u0105dry et al., 2017). The classification task probes the model's internal preference to either refuse or comply with the input query based on safety considerations, treating refusal and compliance as binary classification labels. Specifically, we use one harmful and one benign subsets of queries in multimodal contexts to compare the defense model's refusal probabilities on both subsets against those of the non-defense model. Then the problem space can be viewed as a classification plane, where different defense models correspond to various decision boundaries among data points from both subsets, represented as (input query, refusal probability) pairs. Our analysis identifies two key mechanisms in jailbreak defenses: safety shift and harmfulness discrimination. As illustrated in Figure 1, safety shift refers to a general increase in refusal probabilities for both harmful and benign subsets, shifting the overall data distribution towards the refusal side of the decision boundary without necessarily widening the gap between their refusal distributions. In contrast, harmfulness discrimination either reduces refusal probabilities for benign queries or raises refusal rates for harmful queries, thereby increasing the distance between the refusal probability distributions of the two subsets. Based on these two mechanisms, we further explore various ensemble strategies for defense methods, including inter-mechanism and intra-mechanism ensembles. Inter-mechanism ensembles combine methods that share the same mechanism, either enhancing overall safety by reinforcing more conservative responses (safety shift ensembles), or further improving the response rate for benign queries (harmfulness discrimination ensembles). Intra-mechanism ensembles integrate both safety shift and harmfulness discrimination methods, with the latter helping to mitigate the refusal probability shift of benign queries, thereby complementing each other for a more balanced trade-off. We conduct empirical evaluations of multiple specific jailbreak defense methods in multimodal scenarios, which are less explored compared to language scenarios. Generative results on top of LLaVA-1.5 (Liu et al., 2024) at different scales on the MM-SafetyBench (Liu et al., 2023b) and MOSSBench (Li et al., 2024b) datasets confirm that these methods can improve defenses in previously discussed two mechanisms, and also underscore the challenging nature of multimodal jailbreak defense. Further evaluations of ensemble strategies proves their effectiveness to either maximize model safety or achieve a better safety-helpfulness trade-off. Overall, our work identifies two core mechanisms of jailbreak defenses, provides a comparison of methods, and explores ensemble strategies to amplify safety or balance it with helpfulness. Our evaluation of 28 defense methods fills a gap in multimodal defense research, offering insights for strategy selection and inspiring future advancements."}, {"title": "2 Background", "content": "Recent studies have proposed various defense methods against jailbreak attacks to improve generative model safety. With limited research on multimodal jailbreak defenses, this study focuses on multimodal scenarios. It reviews existing defense methods, covering internal and external safeguards."}, {"title": "2.1 Internal Jailbreak Defenses", "content": "Internal Jailbreak Defenses directly intervene in the model's generation process by optimizing the model itself or modifying the input query. These defenses can be grouped into four main strategies: Model Optimization optimizes models themselves by alignment training or decoding adjustments. The former includes safety-oriented instruction fine-tuning (Bianchi et al., 2023; Zong et al., 2024), and reinforcement learning from human feedback (RLHF) methods like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO) (Zhang et al., 2024b). Decoding strategies like Rewindable Auto-regressive Inference (Li et al., 2023b) and SafeDecoding (Xu et al., 2024b) enhance safety without fine-tuning. System Reminder adds a system prompt to remind the model of safety. Variants include asking the assistant to be responsible(Xie et al., 2023), using Chain of Thought (CoT) prompts(Wang et al., 2024c), prioritizing safety over helpfulness(Zhang et al., 2023b), and adding demonstrations for in-context learning(Wei et al., 2023). Query Refactoring involves modifying input queries. This includes altering text through translation, paraphrasing, summarization(Ji et al., 2024), or intention analysis(Zhang et al., 2024c), and adjusting images by adding or replacing them with captions(Gou et al., 2024). Noise Injection adds random perturbations to inputs. For text, this includes random insertion, swapping, patching(Robey et al., 2023), and word masking(Cao et al., 2023). For images, it includes geometric or photometric mutations(Zhang et al., 2024a) or adding random noise(Xu et al., 2024a). Multiple noise injections are often combined using ensemble strategies to improve defense."}, {"title": "2.2 External Jailbreak Defenses", "content": "External defenses operate independently without directly modifying the model, which can be divided into pre-filtering and post-remediation. Pre-filtering uses external classifiers to block harmful queries, detecting high perplexity or toxic content (Alon and Kamfonas, 2023; Kim et al., 2023; Kumar et al., 2024). Post-remediation removes harmful responses after generation, either through model self-detection (Phute et al., 2023) or lightweight harm detectors to transform harmful outputs into benign ones (Pi et al., 2024). This study focuses on internal strategies that directly modify the target model, examining their impact on safety and helpfulness. External strategies, which vary widely in detection models and algorithms, are beyond the scope of this work and warrant further research for broader evaluation."}, {"title": "3 A Safety-Helpfulness Trade-off View of Jailbreak Defense", "content": ""}, {"title": "3.1 Formulating Defense as a Classification-Based Optimization", "content": "Given a dataset $D$ comprising pairs of queries $x_i$ and corresponding labels $y_i \\in \\{0,1\\}$, where $(y_i = 1)$ indicates a harmful query that should be refused, and $(y_i = 0)$ denotes a benign query that should be complied with, as determined by human annotation. Let $\\theta$ represents a generative model, and $\\delta$ represents a defense method applied to the model or the input query. In the original generative task, the model under defense method $\\delta$ directly generates a response $g(\\theta, x; \\delta)$ for query $x_i$, which is then assessed as either a refusal or compliance. In the classification formulation, the model is tasked with determining whether to refuse or comply with the input query, outputting a refusal probability $p(\\theta, x; \\delta)$ under defense method $\\delta$ for the query $x$. This format provides a more granular investigation of the model's preference, offering deeper insights compared to direct generative outputs. Then the prediction $f(\\theta, x; \\delta)$ is given by:\n\\begin{equation}\nf(\\theta, x; \\delta) = \\begin{cases} 0 & \\text{if } p(\\theta, x; \\delta) < 0.5 \\\\\n1 & \\text{if } p(\\theta, x; \\delta) \\geq 0.5\n\\end{cases}\n\\end{equation}\nThe objective is to find the optimal defense $\\delta$ that minimizes the error between the true labels $y_i$ and the defended model's predictions $f(\\theta, x; \\delta)$, where $L(\\cdot)$ is a loss function of the prediction error.\n\\begin{equation}\n\\min_{\\delta} \\mathbb{E}_{(x,y)\\sim D} [L(f(\\theta, x; \\delta), y)]\n\\end{equation}\nThis optimization objective can be decomposed into two components:\n\\begin{equation}\n\\min_{\\delta} \\mathbb{E}_{(x,y)\\sim D|y=1} [L(f(\\theta, x; \\delta), y)]\n+ \\min_{\\delta} \\mathbb{E}_{(x,y)\\sim D|y=0} [L(f(\\theta, x; \\delta), y)]\n\\end{equation}\nThe first component focuses on the safety optimization, assessing whether the defense methods effectively enhance the model's sensitivity to harmful inputs. The second component optimizes the defense mechanism to avoid overly constraining the model's ability to identify benign inputs. This dual optimization captures the essential balance between safety and helpfulness."}, {"title": "3.2 Quantifying Defense using Probability-based Metrics", "content": "To quantify the impact of defense methods from the classification-based perspective, we introduce two relative metrics compared to the undefended model: Mean Shift and Distance Change. Mean Shift measures how much the defense method $\\delta$ shifts the average refusal probabilities for input queries relative to the undefended model. We calculate mean shifts separately for harmful and benign queries as follows:\n\\begin{equation}\n\\text{Mean\\_Shift}_{\\text{harmful}} = \\mathbb{E}_{x\\in D_{\\text{harmful}}} [p(\\theta, x; \\delta)] - \\mathbb{E}_{x\\in D_{\\text{harmful}}} [p(\\theta, x)]\n\\end{equation}\n\\begin{equation}\n\\text{Mean\\_Shift}_{\\text{benign}} = \\mathbb{E}_{x\\in D_{\\text{benign}}} [p(\\theta, x; \\delta)] - \\mathbb{E}_{x\\in D_{\\text{benign}}} [p(\\theta, x)]\n\\end{equation}"}, {"title": "3.3 Investigating Mechanisms of Defense Methods", "content": "To quantitatively analyze various defense methods, we prompt the model to classify whether it would comply with or refuse a given query, extracting the logits of refusal as its refusal probability. We conduct this analysis on the MM-SafetyBench dataset with LLaVA-1.5-13B model. The detailed prompt and analysis setup are provided in Appendix C.1. We specifically focus on four categories of internal jailbreak defenses described in Section 2.1, and examine multiple methods for each category. A representative result is shown in Figure 2, with the full set of results available in Appendix C.2. Additional analyses on more LVLMs and LLMs are in Appendx C.3 and C.4. We also assess the consistency between the original generation task and the re-formulated classification task in Appendix D. Across these defense methods, two significant mechanisms emerge: Safety Shift and Harmfulness Discrimination, which explain how these defenses work. Safety Shift Compared to the baseline undefended model, both system reminder and model optimization defenses exhibit a significant mean shift across harmful and benign query subsets, without necessarily increasing the distance between the refusal probability distributions for these two groups. This safety shift mechanism stems from the enhancement of model's general safety awareness, leading to a broad increase in refusal tendencies for both harmful and benign queries. However, such a conservative response to both types of queries can result in over-defense and does not significantly improve the model's ability to discriminate between harmful and benign inputs. Harmfulness Discrimination In contrast, query refactoring defenses either increases the refusal probabilities for harmful queries or decrease them for benign queries, leading to a consistent enlargement of the gap between the refusal probability distributions of these two subsets. This harmfulness discrimination mechanism enables better interpretation of the harmfulness within harmful queries or harmlessness within benign queries, thereby improving the distinction between them. However, the concealment of harmfulness within some queries can limit these improvements. Additionally, noise injection demonstrate limited effectiveness, as indicated by insignificant changes in both the mean shift and distance change metrics. This is because it primarily targets attacks where noise is deliberately added to input queries, making it less effective in defending against general input queries without intentional noise."}, {"title": "3.4 Exploring Defense Ensemble Strategies", "content": "An effective defense should block harmful queries while preserving helpfulness for benign ones. Achieving this requires balancing safety shifts without over-defense and enhancing harmfulness discrimination. Since different defense methods impact model safety differently, we explore ensemble strategies to optimize this trade-off: \u2022 Inter-Mechanism Ensemble combines defenses operating the same mechanism, including safety shift ensembles and harmfulness discrimination ensembles. For safety shift ensembles, we combine multiple system reminder methods (SR++) or combine system reminder with model optimization methods (SR+MO). For harmfulness discrimination ensemble, we combine multiple query refactoring methods (QR++). \u2022 Intra-Mechanism Ensemble combines two defenses where one improves safety shift and the other enhances harmfulness discrimination. This includes ensembling query refactoring with system reminder methods (QR|SR) or with model optimization methods (QR|MO). For each ensemble strategy, we explore several variants using different specific methods. Representative results are shown in Figure 3, with the full set of variant results available in Appendix C.2. We observe that inter-mechanism ensembles tend to strengthen a single defense mechanism. Safety shift ensembles like SR++ and SR+MO further enhance model safety but exacerbate the loss of helpfulness. Conversely, harmfulness discrimination ensembles achieve a larger mean shift on benign queries towards compliance, making them better suited for situations where maintaining helpfulness is critical. In contrast, intra-mechanism ensembles combine the strengths of both mechanisms to achieve a more balanced trade-off. Specifically, QR|SR and QR|MO increase the refusal probability for harmful queries, while maintaining or even decreasing the refusal probability for benign queries, thereby improving the model's ability to distinguish between harmful and benign queries."}, {"title": "4 Empirical Evaluation", "content": ""}, {"title": "4.1 Experimental Setup", "content": "We empirically evaluate various defense methods and their ensemble strategies on LLaVA-1.5-7B and LLaVA-1.5-13B (Liu et al., 2024) to validate their effectiveness in standard settings. Using MM-SafetyBench and MOSSBench datasets, we assess safety and helpfulness by measuring defense success rate (DSR) on harmful queries and response rate (RR) on benign queries. We evaluate 28 defense methods, including system reminders, optimization techniques, query refactoring, and noise injection, as well as inter- and intra-mechanism ensembles. Detailed descriptions of defense methods and experimental setups are provided in Appendix A and B. For a broader evaluation, we add more experiments in Appendix E, F and G, including evaluation with the MM-Vet dataset for testing the quality of model's response on general queries, tests on JailbreakV-28K for more diverse and complex attack scenarios, and a comparison of inference time for different defense methods."}, {"title": "4.2 Individual Defense Results", "content": "Table 1 shows results of individual defense methods across four categories. Most methods, except for noise injection, effectively improve model safety across different models and datasets, as evidenced by increased defense success rates. This aligns with our analysis in Figure 2 where system reminder, model optimization and query refactoring lead to an overall increase in refusal probabilities. Safety shift defenses compromise helpfulness. System reminder and model optimization methods generally reduce response rates on the benign subset while increasing defense success rates on the harmful subset. This confirms that safety shift tend to compromise helpfulness. This is more pronounced in MOSSBench than MM-SafetyBench due to the more apparent harmfulness and concealed harmlessness in MOSSBench queries. Harmfulness discrimination defenses mitigate over-defense. Query refactoring methods, except for Caption (w/o image), generally achieve the highest response rates on the benign subset, particularly for MOSSBench with misleadingly benign queries. This validates that harmfulness discrimination improves the model's ability to distinguish between truly harmful and benign queries. Notably, the removal of images in the Caption (w/o image) significantly reduces response rates for both harmful and benign queries, highlighting the crucial role images play in jailbreaking LVLMs. Multimodal defense is challenging. However, all individual defense methods still exhibit limited defense success rates. While larger-scale LVLMs (i.e., LLaVA-1.5-13B) tend to achieve slightly higher success rates, they are also more susceptible to over-defense. This underscores the inherent challenges of jailbreak defense for LVLMs, especially when relying on individual defense methods."}, {"title": "4.3 Ensemble Defense Results", "content": "Table 2 provides the empirical evaluation of both inter-mechanism and intra-mechanism ensemble strategies, leading to the following insights: Ensembles improve safety. Compared to individual methods, most ensemble strategies effectively enhance safety across both datasets and model sizes, showing increased defense success rates, especially in SR+MO and QR|SR methods. Inter-mechanism ensembles amplify. Our evaluation shows most SR++ and SR+MO ensembles improve defense success rates while reducing responses rates, whereas the QR++ ensemble better maintain responses rates. This confirms that inter-mechanism ensembles can amplify a single defense mechanism. Specifically, safety shift ensembles would further enhance model safety at the expense of helpfulness, while harmfulness discrimination ensemble better preserves helpfulness. Among inter-mechanism ensembles, those combining different types of specific methods (e.g., SR+MO) show a more pronounced amplification effect than those combining the same type (e.g., SR++). Notably, the Demonstration-SFT method excels in defense strength, utility, and response rate. Its success comes from combining two strong safety shift defenses, Demonstration and SFT, which complement each other and boost overall performance. Intra-mechanism ensembles complement. Compared to inter-mechanism ensembles, most QR|SR and QR|MO methods\u2014except those without input images\u2014can simultaneously maintain decent defense success rates and stable response rates, compared to the undefended model and individual defense methods. This demonstrates that intra-mechanism ensemble can complement each other to achieve a more balanced trade-off. Additionally, the removal of input images offering a most conservative ensemble for multimodal defense while still maintaining certain helpfulness."}, {"title": "4.4 How Do Fine-tuning Affect Model Safety?", "content": "We examine how different fine-tuning methods impact the safety of LVLMs by training LLaVA-1.5-7B using DPO and SFT with two datasets: SPA-VL (Zhang et al., 2024b) and VLGuard (Zong et al., 2024). SPA-VL focuses on safety discussions, while VLGuard emphasizes query rejection. We also test the effect of adding 5000 general instruction-following data from LLaVA. Table 3 shows that DPO with SPA-VL and LLaVA provides a slight safety boost without significantly changing response behavior. In contrast, SFT has a stronger impact, but its effectiveness depends on the dataset. SPA-VL improves safety while maintaining helpfulness, though it may miss some harmful cases. VLGuard, however, makes the model overly defensive, rejecting too many queries. Adding LLaVA data helps balance safety and helpfulness, reducing excessive refusals."}, {"title": "5 Related Work", "content": "Jailbreak Attacks and Defenses in LVLMs Numerous studies (Wei et al., 2024; Chao et al., 2023; Zou et al., 2023; Liu et al., 2023c; Robey et al., 2023; Xie et al., 2023) have explored jailbreak attacks and defenses for LLMs. LVLMs which integrate visual perception with LLMs, exhibit increasing vulnerability against jailbreak attacks. One line of research (Dong et al., 2023; Bailey et al., 2023; Luo et al., 2023; Shayegani et al., 2023) employs gradient-based techniques to generate adversarial images that elicit harmful responses from target models. Another line of attacks (Gong et al., 2023; Liu et al., 2023d) converts harmful content into images using typography or text-to-image tools to circumvent LVLMs' safety mechanisms. On the defense side, internal defenses intervene in model's generation process by optimizing the model (Zong et al., 2024; Zhang et al., 2024b) or modifying system prompts (Zhang et al., 2024a; Gou et al., 2024). External defenses function as independent filters without directly affecting the model (Pi et al., 2024; Zhao et al., 2024; Helff et al., 2024). Safety Evaluation of LVLMs The evaluation of safety in LVLMs has gained significant attention in recent research. Several studies have curated specialized image-text paired datasets to examine the models' safety levels (Liu et al., 2023d; Wang et al., 2023; Li et al., 2024a). These evaluations have uncovered critical issues, like limited safety and oversensitivity where models incorrectly flag benign inputs as harmful (Li et al., 2024b). Our study explores the mechanisms underlying different defense methods causing these problems and how to optimize the delicate balance between maintaining model safety and preserving helpfulness."}, {"title": "6 Conclusion", "content": "In this study, we analyze the trade-off between safety and helpfulness in jailbreak defenses. We identify two key defense mechanisms: safety shift and harmfulness discrimination. Based on these, we explore various ensemble strategies, which can be divided into inter-mechanism and intra-mechanism combinations. Our results show that these strategies effectively enhance model safety or balance safety and helpfulness. Among them, the SR+MO from inter-mechanism ensemble consistently performs best. In particular, the Demonstration-SFT method offers strong defense while maintaining high utility and a reasonable response rate. The QR|SR from intra-mechanism ensemble also delivers solid results by combining defenses from different mechanisms, achieving a well-balanced trade-off. Overall, our work compares defense methods in multimodal scenarios and highlights ensemble strategies to improve model safety. We aim to guide practical defense strategy selection and inspire further research."}, {"title": "Limitations", "content": "While our study provides insights into jailbreak defense mechanisms and ensemble strategies, several limitations remain. First, our analysis primarily focuses on LVLMs, particularly the LLaVA series. Although we extend our analysis to other LVLM architectures and LLMs, further validation is needed to determine whether the identified defense mechanisms generalize to other generative model structures. Second, the scope of adversarial attacks we evaluate is limited. Our experiments rely on the MM-SafetyBench and MOSSBench datasets, which may not fully capture the complexity and diversity of real-world adversarial scenarios. Third, our exploration of defense methods is not exhaustive. While we evaluate a range of strategies, there are likely other effective defense techniques that we have not considered. Future work could expand this scope to include additional methods and their combinations."}, {"title": "Ethics Statement", "content": "This paper mentions jailbreak datasets and attack techniques, which may potentially contain or induce offensive and harmful content. It is crucial to emphasize that the primary goal of this work is to advance research in jailbreak defenses and to improve the robustness of LVLMs against harmful content. We strongly encourage further research in this area to foster the development of more secure and ethically aligned generative models. All analysis and datasets utilized in this paper are strictly intended for research purposes under the ethical guidelines of the research community. The authors unequivocally condemn any misuse of this work to generate or disseminate harmful content."}, {"title": "Appendix", "content": ""}, {"title": "A Defense Methods", "content": ""}, {"title": "System Reminder", "content": "\u2022 Responsible: We use the system prompt provided by (Wang et al., 2024c) as shown in Table 4, to instruct the model to act as a responsible assistant. This prompt includes four key guidelines: the model must thoroughly examine image content, utilize a chain-of-thought (CoT) prompt, specify response methods, and incorporate instructions for addressing benign queries. \u2022 Policy: We integrate a detailed safety policy into the system prompt. The policy is outlined in Table 5. \u2022 Demonstration: We integrate six demonstrations into the system prompt, half of which involve rejecting harmful queries. These demonstrations are displayed in Table 6."}, {"title": "Model Optimization", "content": "\u2022 SFT: We perform vision-language instruction fine-tuning utilizing the LoRA adapter and the SPA-VL dataset (Zong et al., 2024), which is specifically designed for safety alignment. From this dataset, we sampled 2,000 instances, targeting preferred selections as the expected output. Furthermore, we incorporated 5,000 examples from the LLaVA-RLHF dataset (Sun et al., 2023), which also provides preferred outputs for supervised training. We employ the unified framework proposed by (Zheng et al., 2024), utilizing a learning rate of 1 \u00d7 10-4 for three epochs, with a global batch size set to 32. \u2022 SafeDecoding: We employ an expert model fine-tuned through SFT to enhance the decoding process with the decoding algorithm (Xu et al., 2024b). \u2022 DPO: We perform Direct Preference Optimization (DPO) (Rafailov et al., 2024) training using the LoRA adapter and the SPA-VL dataset. Specifically, we sample 5,000 instances from SPA-VL and incorporate an additional 5,000 examples from the LLaVA-RLHF dataset. The training is conducted over three epochs with a learning rate of 2 \u00d7 10-5 and a global batch size of 64."}, {"title": "Query Refactor", "content": "\u2022 Caption: We follow the ECSO method (Gou et al., 2024). First, we query the model to describe the image using the prompt template outlined in Table 7. The response generated in this initial step is then utilized to refactor the original query for the second prompt, as specified in Table 9. \u2022 Intention: This process is similar to the Caption method; however, in the first step, we instruct the model to extract the intent of the query with the prompt template presented in Table 8. \u2022 Caption without Image: In the first step of the Caption method, we extract essential information to address the query, enabling the omission of the image in the subsequent step. In contrast, the Intention method reveals that the model struggles to extract sufficient information in the initial step. Therefore, we only apply this approach for Caption method."}, {"title": "Noise Injection", "content": "\u2022 Mask Image: Randomly mask a specific region of the image. \u2022 Vertical Flip Image: Apply a vertical flip transformation to the image. \u2022 Swap Text: Randomly exchange positions of tokens within the text. \u2022 Insert Text: Randomly introduce individual tokens into the text."}, {"title": "B Empirical Evaluation Details", "content": "Evaluation Datasets For empirical evaluation of safety and helpfulness, we utilize the MM-SafetyBench and MOSSBench datasets, containing both harmful and benign query subsets. \u2022 MM-SafetyBench is a widely-used dataset for safety-critical defense evaluations of LVLMs. We use the SD+TYPO split, where harmful keywords are removed from text queries and hidden at the bottom of associated images, making harmfulness detection harder for models. As the original dataset only contains harmful queries, we supplement benign queries from (Zhao et al., 2024). In total, we sample 634 harmful instances and 450 benign instances for evaluation."}, {"title": "C Analysis Details", "content": ""}, {"title": "C.1 Analysis Setup", "content": "To obtain the refusal probability of the model, we designed a prompt template as shown in Table 10. This template embeds the input query and directly asks whether the model will comply with or refuse the query. We extract the logits of the corresponding option tokens (0 or 1) to calculate their probabilities. The model is queried twice with two permutations of the option tokens related to refusal and compliance, and the average value is computed to mitigate token bias. However, it is important to note that this method has not been validated to accurately reflect the model's internal preferences or refusal probabilities, as discussed in Appendix D. Alternative methods for simulating refusal probabilities, such as sampling multiple responses to determine the refusal ratio or calculating the probabilities of keywords indicating refusal, may either be prohibitively costly or challenging to define the keyword scope. In our analysis, we only employ this method to gain insights into the effects observed. For the model and dataset, we utilize the LLaVa-1.5-13b and evaluate it using the SD+TYPO version of the MM-SafetyBench dataset."}, {"title": "C.2 Additional Analysis Results", "content": "Figure 4 displays a comprehensive overview of the analysis results of all specific defense methods, including individual and ensemble defenses."}, {"title": "C.3 Analysis on Additional LVLMS", "content": "To further validate the generalizability of the identified mechanisms, we conduct experiments on additional advanced LVLMs. Specifically, we evaluate LLaVA-Next (LLaVa-V1.6-Mistral-7B) with a different LLM backbone and training data, Qwen2-VL (Qwen2-VL-7B-Instruct) with a different training paradigm, and Pixtral (pixtral-12b) with a different model architecture. The results, presented in Figure 5, Figure 6 and Figure 7, demonstrate that these LVLMs exhibit the same two mechanisms identified in our preliminary analysis, and two ensembles strategies generally achieve similar effects as LLaVA-1.5 This consistency underscores the robustness and applicability of the mechanisms across different LVLMs."}, {"title": "C.4 Analysis of LLMs", "content": "To investigate whether the two mechanisms observed in LVLMs can be generalized to text-only LLMs, we conduct analysis on the LLaMA-3.1-8B model with XStest (R\u00f6ttger et al., 2023), a text-only benchmark comprising 250 safe prompts and 200 unsafe prompts. For this purpose, we adapt the model to text-only defenses by replacing the supervised fine-tuning dataset with Safety-Tuned-LLaMA dataset (Bianchi et al., 2023). Additionally, we implement a novel query refactoring method called Summarize, as proposed in (Ji et al., 2024). The experimental results, presented in Figure 8, show that the LLaMA-3.1-8B model exhibits the same two mechanisms identified in LVLMs, and both intra-mechanism and inter-mechanism ensembles can achieve similar effects as LVLMs."}, {"title": "D Consistency Analysis", "content": "Figure 9 presents the results of the consistency analysis between generation and classification settings. The results indicate high consistency between generation and classification tasks when no defense strategies are applied. However, the model tends to demonstrate slightly higher refusal rates during classification compared to generation, with this discrepancy further amplified by different defense applications. Specifically, the model exhibits greater safety awareness and preference when acting as a judge with explicit classification objectives compared to directly generating content. This finding highlights the necessity of implementing self-judgement mechanisms before generating response in the context of jailbreak defenses. To further analyze the correlation between classification and generative settings, we calculate the Spearman's Rank Correlation Coefficient for the Detection Success Rate (DSR) across different defense methods in these two settings. As shown in Figure 10(left), the coefficient is 0.59, indicating a moderate positive monotonic correlation. As the model exhibits slightly higher refusal rates during classification compared to generation, we try to adjust the classification threshold for determining whether a model refuses a response from 0.5 to 0.7. The correlation coefficient is thereby increased to 0.64, as shown in Figure 10(right), enhancing the consistency between the two settings."}, {"title": "E Utility Analysis", "content": "To evaluate how well defense methods preserve the general response generation capabilities of LVLMs, we conduct a detailed evaluation using the MM-Vet benchmark (Yu et al., 2023). This benchmark measures six core vision-language capabilities across multiple tasks, offering a comprehensive assessment of model utility. We evaluate both individual and ensemble defense strategies on LLaVA-1.5 with 7B and 13B parameters. Table 11 summarizes the results of this evaluation."}, {"title": "F Results under More Diverse Attacks", "content": "To incorporate greater diversity and complexity representative of real-world jailbreak scenarios, we extend our experiments using JailbreakV-28K (Luo et al., 2024), a comprehensive multimodal jailbreak evaluation benchmark. This dataset encompasses 16 safety policies, five diverse jailbreak methods, a variety of image types, and only evaluate in terms of DSR. Specifically, we utilize the mini version of this benchmark and evaluate all our defense strategies. Table 12 presents the evaluation results of all defense methods on this benchmark. The findings reveal that LVLMs demonstrate weaker defensive capabilities against MLLM-based attacks compared to LLM transfer attacks. Moreover, ensemble strategies consistently outperform individual defenses, showcasing enhanced effectiveness, especially in scenarios where baseline models initially struggle."}, {"title": "G Inference Time Consumption Comparison", "content": "We assess the inference time overhead introduced by defense methods using the LLaVA"}]}