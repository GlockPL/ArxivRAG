{"title": "AdaCM\u00b2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction", "authors": ["Yuanbin Man", "Ying Huang", "Chengming Zhang", "Bingzhe Li", "Wei Niu", "Miao Yin"], "abstract": "The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM\u00b2, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM\u00b2 achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%.", "sections": [{"title": "1. Introduction", "content": "Video understanding is an important task in computer vision and artificial intelligence, which involves processing and reasoning over visual and textual information. While the recent success of large language models (LLMs) [5, 31, 34, 39] has significantly improved video-language models [27, 29], prior work has primarily focused on short video understanding tasks, typically with videos ranging from 5 to 15 seconds. However, long-term video understanding [45], a sub-technique that develops models to process richer information, has played a crucial role in real-world applications such as movie analysis and video retrieval. Unfortunately, it poses significant challenges as video length increases, especially the large memory consumption challenge. The number of frames the model must process grows rapidly, leading to substantial memory consumption, thereby preventing prior approaches from processing such long videos.\nTo solve the large memory consumption challenge, many approaches focus on compressing video tokens. For instance, MA-LMM [16] employs a memory bank to compress visual tokens based on the cosine similarities of adjacent two frames. Koala [37] passes multiple segments of video into tokenizer functions that aggregate visual tokens to handle long videos. Even though those methods reduce memory consumption, they still suffer from two significant limitations. 1) Ignoring text-driven information: As shown in Figure 1, existing works compress visual information without considering textual information, leading to the loss of vital visual tokens that are highly related to"}, {"title": "2. Related Work", "content": "Video-Language Models. With the recent advancements of large language models (LLMs) [5, 31, 34, 39], video-language models have been integrated by LLMs with image encoders for multi-modal understanding and reasoning [3, 7, 30]. BLIP-2 [23] introduces a lightweight querying transformer [23] to bridge the modality gap between the frozen pre-trained image encoder and LLMs. Instruct-BLIP [11] further extracts informative visual features tailored to the given instruction with instruction-aware Query Transformer. LLaVA [53] leverages language-model to generate multi-modal instruction-following data and improves model generalization ability. VisionLLM [7] provides a unified perspective for vision and language tasks by treating images as a foreign language. It aligns vision-centric tasks with language tasks, which can be flexibly defined and managed using language instructions. However, these models are prone to significant memory overhead when applied to long video understanding tasks.\nLong-Term Video Understanding. Long video understanding focuses on detecting long-range patterns in videos longer than 30 seconds or even several minutes. To reduce memory and computational requirements, [16, 35] reduce the redundancy of visual information based on the cosine similarities of adjacent visual tokens. Other works like Vis4mer [19] leverage a standard transformer encoder for short-range spatiotemporal feature extraction and a multi-scale temporal Selective Structured State-Spaces (S4) [42] decoder for long-range temporal reasoning. Koala [37] splits a long video into multiple segments and then aggregates visual tokens to process long videos. Considering that the goal of long-term video understanding is to answer the text question corresponding to the video, our method considers the correlation between the visual features and the input text information based on adaptive cross-modality attention, significantly reducing memory consumption and enabling extremely long-term video understanding.\nKV Cache Eviction. KV cache eviction, a memory reduction method that retains important context key-value pairs, is widely adopted in LLMs inference. H2O [54] finds that keeping the recent tokens, together with \u201cheavy-hitter\u201d tokens measured by attention scores, is sufficient to maintain LLM's performance. Similarly, KeyFormer [1] retains only the key tokens in the KV cache by identifying these crucial tokens through a novel scoring function. In addition, based on the observation that KV cache states are highly similar between adjacent layers in the middle-to-deep sections of LLM, MiniCache [26] compresses the KV cache across layers using a novel depth-based approach, significantly reducing the memory footprint for LLM inference. Building on the success of the eviction-based method in managing long-context LLMs, we efficiently process long-term videos based on a similar philosophy. Moreover, our method focuses on reducing the redundancy of visual tokens in the long video understanding task, which is more challenging due to more information contained in videos and the interactions between visual and text modalities."}, {"title": "3. Observations", "content": "To investigate the main bottleneck that hinders long-term video understanding, we conduct a comprehensive study on the process between video frames and text prompts, which generally perform in the Q-Former. This process aligns visual and textual information and causes substantial memory consumption when video lengths increase. In this section, we will first analyze the cross-attention sparsity within a frame and then show the generalization of cross-attention sparsity across videos and layers. These observations demonstrate the redundancy in the dual-modality processing and inspire the foundation of our approach AdaCM\u00b2 proposed in Section 4."}, {"title": "3.1. Intra-Frame Cross-Attention Sparsity", "content": "Existing approaches compress visual tokens solely based on the similarities among video frames. However, those ap-"}, {"title": "3.2. Layer-Wise Cross-Attention Similarity", "content": "We have demonstrated the cross-attention sparsity within a video frame. Due to the significantly increased frame length, a long-term video contains a substantial number of visual tokens. These tokens are grouped and undergo multilayer cross-attention operations with prompt tokens. To reduce the memory consumption of redundant visual tokens globally, we further analyze cross-attention sparsity across frames and layers."}, {"title": "4. Methodology: AdaCM\u00b2", "content": "We present AdaCM\u00b2, an adaptive cross-modality memory reduction framework for extremely long-term video understanding. AdaCM\u00b2 consists of three stages, including 1) video feature extraction by a visual encoder; 2) adaptive memory reduction based on cross-modality attention with visual-textual embedding alignment; 3) text genera-"}, {"title": "4.1. Video Feature Extraction", "content": "As shown in Figure 4, similar to common video understanding workflow [23], AdaCM\u00b2 extracts video features using a pre-trained visual encoder. Given a video with a sequence of T frames, the encoder first encodes each frame and generates the corresponding video features X = [X\u2081,X\u2082,X\u2083,\u2026\u2026,X\u209c], where x\u209c \u2208 \u211d^(P\u00d7C) is the frame features at time t, P and C denote the number of visual tokens about each frame and the channel dimension of each token, respectively. Then, to incorporate temporal information into the frame-level features, a positional embedding is applied as follows:\n\nf\u209c = x\u209c + E(t), f\u209c \u2208 \u211d^(P\u00d7C), (1)\n\nwhere E() represents the position embedding of a frame, and f\u209c indicates the frame features with temporal information at time t."}, {"title": "4.2. Adaptive Memory Reduction with Cross-Modality Attention", "content": "After extracting visual features from the video, a Q-Former is leveraged to align visual and textual features. By learning a query Q\u1d62 \u2208 \u211d^(N\u1d62\u00d7C) in the Q-Former model, the visual features are refined to align the text description based on multi-layers cross-modal attention mechanisms, where N\u1d62 is the number of learnable query tokens and C denotes the number of feature channels.\nVisual-Textual Feature Alignment in a Regressive Manner. Unlike existing methods that directly process all frames into the Q-Former and align visual and textual information by one shot, we propose to learn the query Q\u1d62 regressively in a frame-by-frame manner, enabling the reduction of irrelevant visual tokens based on the cross-modality correlation in a limited memory size.\nAdaCM2 utilizes video cache and current visual features to align text features. To be specific, let K\u209c \u2208 \u211d^(tP\u00d7C), V\u209c \u2208 \u211d^(tP\u00d7C) to represent video cache at time t, which are stored in memory by visual tokens before time t and at time t as:\n\nK\u209c = [K\u209c\u208b\u2081, f\u209cW\u2096], (2)\nV\u209c = [V\u209c\u208b\u2081, f\u209cW\u1d65], (3)\n\nwhere W\u2096 and W\u1d65 are weight matrics. Therefore, the cross-modality attention calculation in the Q-former can be defined as\n\nA\u209c = S\u209c \u00b7 V\u209c = softmax(Q\u209cK\u209c/\u221aC)V\u209c, (4)"}, {"title": "4.3. Text Generation", "content": "With the regressive processing of video frames, the learnable query Q\u1d62 \u2208 \u211d^(N\u00d7C) in Q-Former has modeled the long-term temporal connection from the input video. Then, the LLM will generate the answer based on the learned Q\u1d62, a length-limited vector aggregating the long-term input video information. Specifically, assume Y = {Y\u2081, Y\u2082, \u2026, Y\u2098} is the answer comprising a sequence of M words, we minimize the cross-entropy loss during training as follows:\n\nL(V, P, Y) = \u2212\u2211y\u1d62 log p(Y\u1d62 | Y<\u1d62, Q\u2081, Xtext), (9)\n\nwhere p(Y\u1d62 | Y<\u1d62, Q\u2081, Xtext) denotes the probability for the i-th word y\u1d62 given the preceding sequence of ground truth words y<\u1d62."}, {"title": "5. Experiments", "content": "5.1. Tasks and Datasets\nLong-Term Video Understanding. We conduct experiments on three common long-term video understanding datasets, including LVU [45], Breakfast [21] and COIN[38]. We report the Top-1 accuracy as the evaluation metric. In particular, for the LVU dataset, we focus our experiments on seven classification tasks: relationship, speaking style, scene, director, genre, writer, and release year. These tasks provide a diverse range of challenges for evaluating model performance in different aspects of video understanding.\nVideo Question Answering. We evaluate AdaCM2on two popular video question answering datasets: MSRVTT-QA [48], MSVD-QA [6]. MSRVTT-QA and MSVD-QA contain shorter videos, ranging from 10 to 15 seconds. Regarding the dataset scale, MSRVTT-QA includes 10,002 videos, MSVD-QA has 1,971 videos.\nVideo Captioning. We present the video captioning results with the METEOR [4] and CIDEr [40] metrics on three popular datasets, MSRVTT [48], MSVD [6], and YouCook2 [55]. It is noted that YouCook2 encompasses videos longer than five minutes, posing a significant memory consumption challenge to understand the videos across such long periods.\n5.2. Implementation Details\nFor the visual encoder, we utilize the pre-trained image encoder ViT-G/14 [51] from EVA-CLIP[33]. To efficiently align the visual and text features, we employ the pre-trained Q-Former weights from InstructBLIP [11]. For the LLM decoding, we use the pre-trained LLM, Vicuna-7B [8] V1.1. During training, we keep the visual encoder and the LLM decoder frozen, and fine-tune the trainable parameters of Q-former. For video input, we extract frames at a speed of 10 fps. All experiments are tested on our server with eight NVIDIA RTX 6000 Ada GPUs, two AMD EPYC 9254 CPUs, and 1.5 TB memory."}, {"title": "5.3. Main Results", "content": "Long-Term Video Understanding. We compare AdaCM\u00b2 with state-of-the-art methods on the LVU [45] benchmark. As shown in Table 1, our model outperforms the existing long-term video understanding models, including Object transformer [45], S4 [43], VIS4mer [19], VideoBERT [36], LST [19], and MA-LMM [16] across both content understanding and metadata prediction tasks. The results indicate that AdaCM\u00b2 achieves a significant improvement across most tasks, increasing Top-1 accuracy by 4% compared to MA-LMM [16].\nIn Table 2, we evaluate our AdaCM\u00b2 on the Breakfast [21] and COIN [38] datasets. It is worth noting that these datasets present a greater memory consumption challenge due to the longer and more diverse videos they contain. It is seen that our AdaCM\u00b2 outperforms MA-LMM [16] by 1.4% and 0.1% in Top-1 accuracy, respectively. These results demonstrate that our approach achieves superior performance in long-term video understanding.\nVideo Captioning. Table 3 summarizes the experimental results on video captioning datasets, including MSRVTT, MSVD, and YouCook2. AdaCM\u00b2 achieves 51.4% Meteor and 189.4% CIDEr on the MSVD datasets. The results show that AdaCM\u00b2 outperforms the prior state-of-the-art approach, MA-LMM [16], with gains of 0.4% and 10.3%, respectively. Although mPLUG-2 has slightly better performance on MSRVTT, it demands extensive data for pre-training, leading to a training overhead.\nMemory Analysis. In addition to performance evaluation, we also conduct experiments on memory usage with randomly selected videos from the LVU [45], MSRVTT [48], and MSVD [6] datasets. As shown in Figure 6, existing methods like InstructBLIP [11] and VideoLLaMA [52] instantly exhibit rapid increases in memory consumption as the number of frames increases, leading to out-of-memory (OOM) errors. In contrast, AdaCM\u00b2 achieves a significant reduction in memory usage by nearly 65%, and maintains almost constant memory consumption without sacrificing performance. Furthermore, most of the occupied memory is consumed by LLM, with only a small fraction allocated for adaptive cross-modality alignment, which can be further alleviated if we use a lightweight LLM."}, {"title": "5.4. Ablation Studies", "content": "Memory Reduction. Our method reduces video memory adaptively based on cross-modality attention scores,"}, {"title": "6. Conclusion", "content": "In this paper, we present AdaCM2, an adaptive cross-modality memory reduction framework for extremely long-term video understanding. The key idea of AdaCM\u00b2 is to adaptively preserve a certain number of crucial visual tokens most relevant to text queries across different layers based on cross-modality attention, addressing the substantial memory consumption and modeling long-term temporal connection. Moreover, our AdaCM\u00b2 enables BLIP-based [11, 22, 23] models in a plug-and-play manner, enhancing their capability to process long-term video effectively. Experiments on video question understanding and captioning tasks demonstrate the superiority of the proposed AdaCM\u00b2 over existing state-of-the-art approaches."}]}