{"title": "Self-Supervised Radiograph Anatomical Region Classification \u2013 How Clean Is Your Real-World Data?", "authors": ["Simon Langer", "Jessica Ritter", "Rickmer Braren", "Daniel Rueckert", "Paul Hager"], "abstract": "Modern deep learning-based clinical imaging workflows rely on accurate labels of the examined anatomical region. Knowing the anatomical region is required to select applicable downstream models and to effectively generate cohorts of high quality data for future medical and machine learning research efforts. However, this information may not be available in externally sourced data or generally contain data entry errors. To address this problem, we show the effectiveness of self-supervised methods such as SimCLR and BYOL as well as supervised contrastive deep learning methods in assigning one of 14 anatomical region classes in our in-house dataset of 48,434 skeletal radiographs. We achieve a strong linear evaluation accuracy of 96.6% with a single model and 97.7% using an ensemble approach. Furthermore, only a few labeled instances (1% of the training set) suffice to achieve an accuracy of 92.2%, enabling usage in low-label and thus low-resource scenarios. Our model can be used to correct data entry mistakes: a follow-up analysis of the test set errors of our best-performing single model by an expert radiologist identified 35% incorrect labels and 11% out-of-domain images. When accounted for, the radiograph anatomical region labelling performance increased without and with an ensemble, respectively to a theoretical accuracy of 98.0% and 98.8%.", "sections": [{"title": "1 Introduction", "content": "Radiography is a prominent technique in modern medical diagnostics, capturing approximately 36.4 million images in 2018 in Germany alone [7]. The widespread adoption of PACS (Picture Archiving and Communication Systems) makes the utilization of this vast pool of data for research and diagnostics easier; however, even some of the most basic metadata, such as the captured anatomical region, may be missing in the DICOM (Digital Imaging and Communications in Medicine) files when recorded at external hospitals, or be generally unreliable [9]."}, {"title": "2 Related Work", "content": "As the cost of creating high quality annotations for medical data is very high, methods capable of extracting knowledge without labels have been of major interest in the medical deep learning community.\nSelf-supervised clustering methods such as DeepCluster [3] exploit the powerful prior of local information in CNNs (Convolutional Neural Networks). Alternating between generating features from the CNN and clustering said features using k-means [21], a pre-chosen number of clusters is generated.\nOther self-supervised methods learn to generate image embeddings through a context matching task which computes features that uniquely describe a given image, being able to re-identify it despite heavy augmentations such as crops, rotations and intensity shifts. SimCLR (a Simple framework for Contrastive Learning of visual Representations) [4] is a contrastive method which employs both positive and negative examples for this purpose, while BYOL (Bootstrap Your Own Latent) [8] predicts the feature representation of its target network from an augmented image encoded by the online network. More details on SimCLR and BYOL are available in section 3.3. The resulting encoder can be used as a backbone for downstream image analysis tasks."}, {"title": "3 Dataset and Methodology", "content": null}, {"title": "3.1 Dataset Generation", "content": "14 anatomical regions that frequently appear in clinical practice were defined as our classification targets. A total of 48,434 radiographs with corresponding anatomical region labels from our hospital PACS were exported as DICOM files. The associated anatomical region information was used as our initial, yet noisy (see section 4.4) ground truth labels. Subsequently, we split the dataset into 31,011 (64%)/7,677 (16%)/9,746 (20%) training/validation/test images (see supplementary material for per-class distributions)."}, {"title": "3.2 Augmentation and Image Cleaning", "content": "As we work with unfiltered real-world medical data, we encounter cases such as arbitrarily rotated radiographs which frequently contain a white border around the perimeter of the original capture image. We detect and remove the border, and normalize the rotation using OpenCV [2]. Furthermore, images in the region pelvis/hip and rarely knee can contain a circular gauge for surgery planning. To combat the model relying on this for its prediction (see section 4.2), we add a novel augmentation where we use 6 example gauges extracted from the dataset, and randomly insert zero to two of them at random locations and scales in all images. Moreover, we apply the standard SimCLR augmentations color jitter, random affine and random crop resize (for hyperparameter details and visualizations, see the supplementary material). To preserve fine-grained features of the medical images, the Gaussian blur augmentation was not used [1]."}, {"title": "3.3 Architecture and Pre-training Objectives", "content": "Serving as the backbone of our model architecture (i.e. self-supervised feature extractor), we use PyTorch's [22] standard implementation of a randomly initialized ResNet18 [10] and remove the final fully connected layer, resulting in embeddings of size 512 after global average pooling. An initial comparison with ResNet50 showed no benefit from using a larger model for our task. We perform our pretrainings using the Adam optimizer [16], a weight decay of $10^{-4}$, a learning rate of $3 \\cdot 10^{-4}$ (with cosine annealing [19]), and train for 1000 epochs.\nSimCLR ([4]), a contrastive approach, processes batches of positive and negative pairs: positive pairs are two differently augmented views of the same image (i, j), while negative pairs are from two different source images. The subsequent projection head consists of a 2-layer MLP (multilayer perceptron) with ReLU (Rectified Linear Unit) activations and BatchNorm [12], computing the projections $z_i, z_j$. These are passed to the normalized temperature ($\\tau$) scaled cross entropy, which compares the cosine similarity ($\\operatorname{sim}$) of positive/negative pairs:\n$L_{i,j} = - \\log \\frac{\\exp(\\operatorname{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}^{2N} 1_{[k \\neq i]} \\exp(\\operatorname{sim}(z_i, z_k) / \\tau)}$\nWe set $\\tau = 0.5$ (as suggested in [4]) and train using a batch size of 1024.\nBYOL ([8]) does not rely on negative pairs; instead, it uses an online and target network with separate weights $\\theta$ and $\\xi$. After the projection to $z_\\theta$ and $z_\\xi$, the online network attempts to predict the target projection by computing $q_\\theta(z_\\theta)$. No gradient information is backpropagated through the target network. Prior to computing the mean squared error loss (MSE), both prediction and target are L2 normalized. Finally, the target network is updated as an exponential moving average of the online network's parameters.\nDue to VRAM constraints, we had to reduce our batch size to 896 for BYOL; accordingly, we set $\\tau_{\\text{base}} = 0.9995$ as recommended in [8]."}, {"title": "4 Results and Discussion", "content": "Our first evaluation results summarize both the per anatomical region and overall performances as top-1 accuracy on our test set when training on our entire training set . We achieve excellent accuracy both on average and for the individual classes, with all approaches achieving over 96.5% mean accuracy. Overall, this shows the strength of the features learned through the contrastive pre-training process. Note the slightly lower scores for the spine classes our investigations in section 4.4 tackle the underlying issue: noisy PACS labels."}, {"title": "4.1 Self-supervision excels in a low data regime", "content": "In fig. 2, we plot the effectiveness when training the final linear layer using our pretrained SimCLR/BYOL backbones with varying fractions of the training set SimCLR performs slightly better in very low data scenarios, yielding 92.2% accuracy with labels using just 1% of our training data (i.e. 310 images total across all 14 classes). A comparative baseline approach where we train a randomly initialized ResNet18 fully supervised with cross entropy on these subsets exhibits much steeper drops in performance, achieving just 57.1% accuracy with 1% of the labels."}, {"title": "4.2 Cleaning+augmentation increase attention to anatomical areas", "content": "To judge the impact of our custom data cleaning and gauge augmentation procedures described in section 3.2, we utilize Guided GradCam [23,17] to perform a qualitative comparison on which parts of the image contribute more to the final prediction by following the gradient see fig. 3. In terms of performance, there is very little overall impact on the overall accuracy (\u2264 \u00b10.6%), but both for SimCLR and BYOL, the pelvis/hip class performance, for which we designed the gauge augmentation, increases by 2.3% and 1.4%, respectively."}, {"title": "4.3 Ensembles further increase performance", "content": "In order to further boost the performance of our approach, we build an ensemble from our 3 primary architectures in table 1 by adding their outputs after applying softmax to the logits, and then selecting the class with the highest score this yields an overall accuracy of 97.7% ."}, {"title": "4.4 Detecting and fixing noisy PACS labels", "content": "Since our \"ground-truth\" labels were directly scraped from PACS, they are not perfectly reliable. To analyze their quality, we tasked an expert radiologist with re-evaluating all test failure cases of the SimCLR model (n=328), almost half of which turned out to be either incorrectly labeled (n=116), out-of-domain (i.e. not in the 14 classes) (n=36), or of unusable quality (n=2) (fig. 4). A total of 98 of the 116 incorrectly labeled in-domain images are in fact correctly predicted by our SimCLR model, increasing its theoretical accuracy (when excluding out-of-domain and unusable images) to 98.0%, and the ensemble's to 98.8% . Due to the very high quality of our predictions, we employ them to detect incorrect anatomical region candidates in PACS metadata, which can subsequently be checked or preemptively skipped for future research endeavors, improving the real-life clinical dataset consistency. Since our architecture only fits a single fully connected layer using the noisy PACS labels, it is feasible to apply the inconsistency detection to images which were part of the training/validation set in the future.\nIn our observations, especially the anatomical region annotations on the spine and shoulder vs. clavicle are prone to inconsistencies see both fig. 4 and our T-SNE [20] plot in the supplementary material; this can be explained by the overlap of several anatomical regions within a small examination area."}, {"title": "5 Conclusion", "content": "In this paper, we introduced our ready for real-world application - self-supervised architecture for radiograph anatomical region classification. Due to its blend of flexibility w.r.t. label availability (\u2248 300 labeled instances) and high accuracy (96.6% - 98.8% accuracy), it can be utilized A) to increase the quality of existing metadata collections, B) improve downstream application reliability and research dataset sizes, as well as c) generate PACS metadata in low-resource scenarios from scratch. The training is further complemented by our custom augmentation techniques.\nDemonstrating our method's effectiveness in finding inconsistencies despite learning from the same noisy ground-truth source, an expert radiologist successfully identified over 150 incorrect PACS labels, after analyzing a subset of 328 candidates generated by our method. This research aims at sparking interest in applying such methods to real-world clinical scenarios."}]}