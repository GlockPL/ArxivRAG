{"title": "BEYOND THE SUM: UNLOCKING AI AGENTS POTENTIAL THROUGH MARKET FORCES", "authors": ["Jordi Montes Sanabria", "Pol Alvarez Vecino"], "abstract": "The emergence of Large Language Models has fundamentally transformed the capabilities of AI agents, enabling a new class of autonomous agents capable of interacting with their environment through dynamic code generation and execution. These agents possess the theoretical capacity to operate as independent economic actors within digital markets, offering unprecedented potential for value creation through their distinct advantages in operational continuity, perfect replication, and distributed learning capabilities. However, contemporary digital infrastructure, architected primarily for human interaction, presents significant barriers to their participation.\nThis work presents a systematic analysis of the infrastructure requirements necessary for AI agents to function as autonomous participants in digital markets. We examine four key areas - identity and authorization, service discovery, interfaces, and payment systems - to show how existing infrastructure actively impedes agent participation. We argue that addressing these infrastructure challenges represents more than a technical imperative; it constitutes a fundamental step toward enabling new forms of economic organization. Much as traditional markets enable human intelligence to coordinate complex activities beyond individual capability, markets incorporating AI agents could dramatically enhance economic efficiency through continuous operation, perfect information sharing, and rapid adaptation to changing conditions. The infrastructure challenges identified in this work represent key barriers to realizing this potential.", "sections": [{"title": "1 Introduction", "content": "The field of AI agents has evolved significantly over decades, from early symbolic systems to reactive agents and reinforcement learning approaches [4, 7]. Deep learning dramatically enhanced reinforcement learning capabilities, leading to breakthroughs like AlphaGo Zero [12, 37, 43]. This system mastered Go through pure self-play, without human examples [43]. Yet these systems faced fundamental limitations. They required extensive training for each new task [34, 5] and struggled to transfer knowledge between domains [20, 34].\nThe current iteration of AI agents is powered by large language models. These models serve as the \"brain\" of modern AI agents, providing reasoning and decision-making capabilities that guide the actions of the agent. Unlike traditional AI approaches that require specific training for each task, LLM-based agents can understand and adapt to new situations through their broad knowledge of language and concepts [46, 8]. This flexibility stems from their exposure to diverse human knowledge during training, enabling them to reason about problems in ways similar to human thinking [27, 50].\nTo interact with their environment, these agents use specialized components for perception and action. On the perception side, they can process diverse inputs\u2014like images, text, or structured data\u2014which are then provided to the LLM in a format it can understand, usually text [3]. On the output side, the LLM generates text that guides the agent's actions, whether through code generation to interact with APIs and digital systems, or through specific commands to"}, {"title": "2 The Bridge to Action", "content": "Code generation has been one of the most successful applications of large language models, fundamentally changing how software is written[31][40][15][14]. While current tools like GitHub Copilot[19] focus on assisting human developers, the true potential of code generation lies in enabling autonomous digital operation. Unlike traditional software that executes predefined instructions, AI systems can now write their own code to accomplish tasks, discover and integrate with new services, and adapt their capabilities based on results[47][49]. This shift from assisting humans to independent operation represents a fundamental change in how software interacts with digital systems.\nThe adoption of code generation tools has been remarkably rapid, with millions of developers now relying on AI assistants for their daily programming tasks[18]. These systems excel not just at writing new code, but at understanding and modifying existing codebases - suggesting changes, implementing new features, and adapting code to match"}, {"title": "3 Markets as Coordination Systems", "content": "Markets are decentralized systems that coordinate complex activities through the interactions of independent actors. These actors make local decisions based on their knowledge, responding to price signals that emerge from supply and demand dynamics [23]. Through competition and cooperation, markets efficiently allocate resources and foster innovation without central planning [45].\nIn today's markets, the primary actors are humans - whether operating as individuals, corporations, or other organizational forms. These human actors leverage their local knowledge and expertise to identify opportunities, make decisions, and create value. They compete for resources while simultaneously cooperating through trade and contracts. Market feedback mechanisms help them adjust their strategies based on success or failure.\nMarkets coordinate activity through price signals that emerge from supply and demand dynamics. When entrepreneurs identify opportunities, they direct resources toward potential solutions - with profits signaling successful value creation and losses indicating the need to redirect resources. This feedback loop serves as a distributed computation system, where prices aggregate information about scarcity and value from countless participants [23]. Successful innovations get amplified through investment and imitation, while resources flow away from failed approaches, driving continuous improvement in resource allocation [42].\nThe brilliance of markets lies in their emergent intelligence-how they coordinate vast and intricate activities in a way that is \"smarter\" than any individual participant or centralized system could achieve [41]. This decentralized coordination enables humanity to undertake extraordinary feats. Consider the example of building rockets and sending them to space. No single entity orchestrates every aspect of the process. The materials for the rocket are mined in one part of the world, refined in another, and assembled in facilities that rely on countless other industries\u2014from power generation to computer chip manufacturing. Meanwhile, the workers involved are sheltered, fed, and transported through an interconnected network of businesses that operate independently, all responding to market incentives. This staggering level of complexity is achieved without any central authority dictating every detail.\nSimilarly, cities emerge and function through the spontaneous order of decentralized decision-making [41]. No central planner ensures that every household has sofas, televisions, or cars, yet these items are almost universally present. Homes are built by construction companies that procure materials from various suppliers, while furniture manufactur-ers design and produce items to meet diverse tastes. All of this is accomplished through countless actors responding to localized needs and opportunities, coordinated by market signals rather than direct orders. The result is an intricate,"}, {"title": "4 Infrastructure Challenges", "content": "Imagine an entrepreneur, Sam, who while working at an online retailer notices that their reporting tools aren't effectively tracking how promotional campaigns impact customer lifetime value. Working with the sales team, he builds"}, {"title": "5 Service Discovery", "content": "Success in markets depends critically on information - not just about prices and competition, but about what solutions already exist. No business operates in isolation. From manufacturing to software development, the most successful enterprises build upon existing solutions rather than reinventing every component from scratch. This specialization and reuse of existing capabilities is fundamental to market efficiency and innovation.\nConsider a modern software company building a new service. Rather than implementing their own payment processing, email delivery, authentication system, and cloud infrastructure, they typically assemble these capabilities from existing providers. This allows them to focus their resources and creativity on their core innovation - the unique value they bring to market. The same pattern repeats across industries: automotive manufacturers source components from specialized suppliers, restaurants rely on established food distribution networks, and retailers build on existing logistics infrastructure.\nThis reliance on existing solutions creates a critical challenge: how do market participants discover what's available? Before any economic decision can be made - whether selecting a supplier, choosing a service provider, or identifying market opportunities - participants must first become aware of their options. The efficiency of markets thus depends not just on price signals, but on the mechanisms through which participants discover and evaluate potential solutions. In human markets, these discovery mechanisms have evolved over centuries, from trade fairs to modern digital platforms. As AI agents begin to participate in markets, we must examine whether these mechanisms will serve their needs or whether new approaches are required."}, {"title": "5.1 Current Infrastructure", "content": "Information about available market solutions reaches participants through multiple complementary channels. The most fundamental is experiential discovery - learning through daily life and professional activities. Market participants naturally become aware of solutions through their work, observing what others use, attending industry events, and participating in professional communities. This ambient awareness forms a foundation of market knowledge that shapes future decisions and investigations.\nSocial discovery builds upon this experiential base through professional networks. When faced with a need, partic-ipants often turn first to trusted colleagues and peers, seeking recommendations based on direct experience. These recommendations carry particular weight because they come with context and validation from trusted sources. Com-"}, {"title": "5.2 Limitations for AI Agents", "content": "The discovery methods that serve human participants so well face fundamental limitations when applied to AI agents. Most critically, agents currently lack physical embodiment that enables experiential discovery. They cannot attend conferences, engage in water cooler conversations, or observe solutions in use during their daily activities. This absence of real-world presence cuts them off from the rich stream of ambient information that humans unconsciously process and integrate into their market awareness.\nEven digital discovery channels, despite being technically accessible to agents, present significant challenges. Mar-keting materials and service documentation are optimized for human consumption, leveraging psychological patterns and visual presentation that may not translate meaningfully to machine understanding. Landing pages use persua-sive design, emotional appeals, and carefully crafted imagery - techniques honed over decades to influence human decision-making but largely irrelevant to machine evaluation of capabilities.\nInformation about services is typically fragmented across multiple sources in ways that reflect human discovery pat-terns. While agents can process vast amounts of information quickly, current presentation formats may cause them to miss relevant capabilities or connections. A service's value proposition might be spread across marketing materials, technical documentation, and user testimonials - an organization that follows human information consumption patterns but may not be optimal for machine discovery and evaluation.\nWhile agents can technically access existing discovery channels, doing so is often inefficient and cumbersome. They must parse human-oriented interfaces, extract relevant information from unstructured content, and navigate multiple systems designed around human workflow assumptions. This creates unnecessary friction in the discovery process, reducing the potential speed and efficiency gains that agent-based market participation could offer."}, {"title": "5.3 Future Design Considerations", "content": "The limitations of current discovery mechanisms point toward new infrastructure designs that could better serve AI agents as market participants. Service registries could provide machine-readable descriptions of capabilities, pricing, and integration requirements in standardized formats. These descriptions would embed the complete context needed for evaluation in a single request, allowing agents to assess potential solutions without navigating multiple information layers.\nSeveral approaches could emerge to facilitate discovery between services and agents. Existing indexing infrastructure like search engines and service directories could evolve to better serve agent discovery needs. Rather than focusing on keyword matching and human-readable content ranking, these systems could develop specialized capabilities for understanding and exposing service characteristics, integration requirements, and operational constraints. This might involve new crawling strategies optimized for machine-readable service descriptions, index structures that facilitate capability-based matching, and query interfaces designed around agent decision patterns. Alternative approaches draw inspiration from distributed systems, where gossip protocols efficiently propagate information across networks.\nMachine-friendly discovery mechanisms could also leverage agents' unique information processing capabilities. In-stead of progressive disclosure models designed for human attention spans, these systems could provide comprehen-sive technical specifications and integration requirements upfront. Other possibilities include semantic service descrip-tions, capability-based discovery protocols, or real-time service meshes that dynamically match agent requirements with available solutions. The key lies in designing systems that align with how agents process and evaluate information while maintaining compatibility with existing infrastructure."}, {"title": "6 Identity and Authorization", "content": "Digital services fundamentally rely on knowing who or what is making requests and what they're allowed to do. Every API call, every database query, and every transaction must address two critical questions: \"Who are you?\" (authentication) and \"What are you allowed to do?\" (authorization). While these might seem like simple security concerns, they form the foundation that enables stateful services to operate, track usage, manage resources, and maintain con-sistency across interactions. Authentication establishes identity-proving that an entity is who it claims to be-while authorization determines what authenticated entities can do, controlling their permissions and access rights within the system.\nThe landscape of digital identity has evolved far beyond simple human-to-service interactions. Modern systems must handle complex scenarios where software acts on behalf of other software, requiring sophisticated delegation mecha-nisms and permission models. These interactions must maintain security while providing audit trails for compliance, tracking resource usage for billing, and establishing verifiable chains of trust between different services and systems.\nIn this section, we examine the infrastructure that enables identity and authorization in digital services. We begin by analyzing current methods and understanding why they evolved to their present form. We then explore how these systems, built around human-centric assumptions, present fundamental limitations for AI agents operating at machine speed and scale. Finally, we investigate promising approaches that could better serve the needs of autonomous agents while maintaining security and accountability. These considerations are critical as we move toward digital systems where AI agents become primary participants rather than occasional automated actors."}, {"title": "6.1 Current Infrastructure: Identity", "content": "Today's digital identity infrastructure evolved primarily around human users and their needs, creating a hierarchy of solutions from simple authentication to complex federated systems. At its most basic level, username and password combinations establish digital identity through shared secrets. While this approach remains widespread, it presents significant operational challenges - password reuse, weak choices, and credential theft create security risks that com-plexity requirements and rotation policies only partially mitigate.\nFor larger organizations, federation protocols extend this basic model across organizational boundaries while main-taining local control. Standards like SAML enable centralized identity management with distributed verification an employee can use their corporate credentials across multiple internal systems without creating separate accounts. This creates a natural hierarchy where identities can be established at one level (like an organization) and inherited or delegated to lower levels (like departments or individual services).\nAs the internet grew and services needed to interact with previously unknown parties, Public Key Infrastructure (PKI) and certificate-based systems emerged as a solution for establishing cryptographic proof of identity. Unlike username/password or federated systems which operate in closed environments, PKI enables secure communication between parties that have never interacted before. Certificate authorities act as trusted intermediaries, validating identi-ties and issuing certificates that create verifiable chains of trust. The Automated Certificate Management Environment (ACME) protocol made PKI practical at scale, underpinning critical security infrastructure like HTTPS.\nFor service-to-service communication within controlled environments, API keys provide a more streamlined solution. These long-lived tokens enable programmatic authentication with clear scoping and audit capabilities. Unlike pass-words, API keys can maintain sufficient entropy to resist attacks while supporting automated rotation and revocation. This makes them particularly suitable for automated systems that need to maintain clear records of which keys are used for which operations.\nAuthentication mechanisms have also evolved to account for the physical world and human factors. Two-factor authen-tication adds a second verification layer through physical devices like YubiKeys or phone-based authenticators. These"}, {"title": "6.2 Limitations for AI Agents: Identity", "content": "The identity infrastructure we've described, while robust for human users and traditional services, faces fundamental challenges when applied to AI agents. These limitations stem not just from issues of scale, but from core assumptions about how digital identities behave and interact."}, {"title": "6.3 Future Design Considerations: Identity", "content": "The limitations of current identity systems for AI agents demand new approaches that maintain security and account-ability while operating at machine speed. Several promising directions emerge when we rethink fundamental assump-tions about digital identity.\nPublic key cryptography offers a foundation for addressing the scale challenge. By allowing agents to generate their own key pairs and prove ownership cryptographically, systems can enable autonomous identity creation at machine speed without centralized bottlenecks. Rather than waiting for certificate authorities or federation servers, agents can create verifiable identities instantly while maintaining cryptographic proof of their authenticity.\nThe traditional account creation and login flow also needs reimagining for AI agents. While standards like WebAu-thn and passkeys are eliminating passwords for human users, new protocols could emerge specifically for machine-to-machine account creation. These could allow agents to programmatically establish service relationships through cryptographic attestation rather than traditional registration flows. Instead of filling out forms and verifying emails, agents could prove their capabilities and trustworthiness through cryptographic challenges, enabling instant service onboarding while maintaining security.\nFor ephemeral identities and rapid trust establishment, zero-knowledge systems suggest promising approaches. These systems enable agents to prove properties about themselves - their authorization level, their operational history, their delegation chain - without revealing unnecessary details. Combined with verifiable credentials, an agent could prove it was spawned by a trusted parent or has performed similar operations successfully, enabling a meaningful reputation even for short-lived identities.\nThe challenge of identity hierarchies and dynamic delegation could be addressed through capability-based systems. These allow an agent to delegate subsets of its identity and permissions to child agents while maintaining crypto-graphic proof of the delegation chain. When combined with attribute-based systems, this enables dynamic, context-aware identity verification based on provable properties rather than static relationships. To handle the separation from physical anchors, new trust models could emerge based on observed behavior and computational proof rather than real-world verification. Decentralized reputation systems could track agent behavior across short lifespans, while"}, {"title": "6.4 Current Infrastructure: Authorization", "content": "Once a system knows who is making a request, it must determine what that entity is allowed to do. Authorization systems have evolved to manage these permissions at scale, balancing security with operational efficiency.\nRole-Based Access Control (RBAC) has traditionally been the dominant model for managing permissions in large systems. Rather than assigning permissions directly to users, RBAC groups related rights into roles that map to orga-nizational functions or job titles. This abstraction simplifies permission management - instead of tracking individual permissions across thousands of users, administrators can assign roles like \"admin,\" \"editor,\" or \"viewer.\"\nMore recently, Relationship-Based Access Control (ReBAC) has emerged as a powerful complement to RBAC, partic-ularly for applications with complex social or organizational relationships. ReBAC determines permissions based on how entities relate to each other within the system. For example, in a document management system, users might be able to edit documents owned by their direct reports or view documents from anyone in their department. This model naturally captures real-world permission patterns that are cumbersome to express in traditional RBAC.\nThese authorization decisions are typically enforced through tokens. Stateful approaches use session identifiers stored server-side, often in cookies, allowing for immediate permission revocation but requiring central state management. Stateless approaches using JSON Web Tokens (JWTs) encode the authorization information directly in the token, enabling faster verification but making revocation more challenging. Systems often combine both approaches, using short-lived JWTs with periodic refreshes from a stateful system.\nFor cross-service scenarios, OAuth has become the standard protocol for delegating access rights across organizational boundaries. OAuth enables controlled access sharing without credential exchange - a user can allow one service to access their data on another service without sharing their password. Different OAuth flows serve different use cases: the authorization code flow handles web applications, while the client credentials flow enables service-to-"}, {"title": "6.5 Limitations for AI Agents: Authorization", "content": "Current authorization systems, designed around human organizational structures and workflows, face significant limi-tations when applied to AI agents operating at machine speed and scale.\nThe core challenge stems from how permissions need to be evaluated. Traditional RBAC systems work well when roles change infrequently and map cleanly to organizational hierarchies. However, AI agents might need to adjust their permissions thousands of times per second based on their current task or context. While ReBAC better captures relationship-based permissions, current implementations aren't designed to handle relationships that form and dissolve at machine speed.\nCross-service authorization presents particular challenges. OAuth works well for relatively stable delegation patterns, but AI agents might need to establish and revoke delegated access continuously as they spawn child agents or collab-orate on tasks. The overhead of traditional OAuth flows becomes prohibitive when operating at machine timescales. Additionally, the standard OAuth scopes are too coarse-grained for agents that need precise, task-specific permissions.\nContext-aware access control becomes crucial yet problematic for AI agents. An agent's permissions might need to change based on its current task, the data it's processing, system load, or other environmental factors. Traditional authorization systems aren't designed to incorporate this rich context into real-time permission decisions. While some systems support basic contextual rules, they typically can't handle the complex, dynamic conditions that govern AI agent behavior.\nToken-based authorization systems face their own challenges with AI agents. Stateful tokens require central storage that becomes a bottleneck at machine scale. Stateless tokens like JWTs, while more scalable, make it difficult to revoke permissions quickly when agent behavior or system conditions change. The traditional compromise of short-lived tokens with refresh mechanisms introduces latency that impacts agent operations.\nThese limitations compound each other in practice. An AI agent might need to spawn multiple child agents, each requiring specific permissions based on their task and context while coordinating access across multiple services - all at machine speed. Current authorization systems, optimized for human-scale operations with relatively static permissions, become a fundamental bottleneck in enabling truly autonomous agent interactions."}, {"title": "6.6 Future Design Considerations: Authorization", "content": "The limitations of current authorization systems for AI agents require new approaches that can handle dynamic, context-aware permissions at machine speed while maintaining security. Several promising directions emerge when we rethink how permissions should work in agent-driven systems.\nCapability-based security models offer a foundation for handling fine-grained permissions at scale. Systems like macaroons and biscuits enable unforgeable tokens that can encode complex permissions and delegation rights. These tokens can be attenuated an agent can create restricted versions of its own capabilities for child agents - while maintaining cryptographic proof of the delegation chain. This enables secure permission delegation without requiring central coordination.\nContext-aware authorization could be achieved through real-time policy evaluation engines optimized for machine-speed decisions. Rather than static role assignments, these systems would evaluate permissions based on current conditions, agent behavior, and system state. By encoding authorization logic in verifiable, deterministic rules, agents could even predict whether they would have necessary permissions before attempting operations.\nCross-service authorization could evolve beyond traditional OAuth flows through standardized permission protocols designed for agent-to-agent interactions. These would enable rapid establishment and verification of permissions across service boundaries while maintaining security. Services could publish their permission models in machine-readable formats, allowing agents to automatically discover and request necessary access rights.\nThe infrastructure supporting these approaches needs to handle massive scale without sacrificing security. This sug-gests architectural patterns where permission verification happens as close to the edge as possible, with cryptographi-"}, {"title": "7 Software Interfaces", "content": "Software service consumption has undergone a dramatic evolution over the past decades, transforming from simple command-line interfaces into rich, multi-modal experiences. This evolution reflects both advancing technical capabil-ities and our deepening understanding of human-computer interaction patterns.\nThe earliest software interfaces were purely text-based, requiring users to memorize specific commands and syntax. As graphical user interfaces emerged, they introduced new paradigms built around visual metaphors - windows, icons, menus, and pointers. These interfaces made software more accessible by mapping complex operations to intuitive visual elements that users could manipulate directly.\nThe web browser represents perhaps the most significant shift in how we consume software services. What began as a simple document viewer has evolved into a universal application platform. The browser's combination of standardized technologies, built-in security model, and instant access to services without installation has made it the dominant platform for software delivery. Today, many applications that were once exclusively desktop software have migrated to browser-based versions, from productivity tools to complex enterprise systems.\nWhile the browser dominates, software consumption isn't monolithic. Desktop applications remain important for compute-intensive tasks or deep operating system integration. Mobile applications offer optimized experiences for smaller screens and touch interfaces. Many modern services support multiple consumption patterns - they might offer a web interface for human users, native mobile apps for on-the-go access, and APIs for programmatic integration.\nThis diversity of interfaces reflects a fundamental truth: how we consume software shapes what's possible with it. The interface is not just a wrapper around functionality - it defines the patterns of interaction, sets expectations about response times and data presentation, and ultimately constrains how value can be extracted from the service. As we"}, {"title": "7.1 Current Infrastructure", "content": "Today's software services are primarily consumed through two distinct patterns: user interfaces designed for human interaction, and programmatic interfaces designed for machine-to-machine communication. Each pattern has evolved its own conventions, constraints, and optimization strategies."}, {"title": "7.1.1 User Interface (UI)", "content": "User interfaces dominate human interaction with software services. Whether through web browsers, desktop appli-cations, or mobile apps, these interfaces share common characteristics shaped by human cognitive and perceptual abilities. They present information visually, often breaking complex data into manageable chunks spread across mul-tiple screens or views. They rely on progressive disclosure - showing basic information first with the option to drill deeper - to avoid overwhelming users.\nThe web browser has emerged as the primary platform for delivering user interfaces. Its ubiquity, built-in security model, and ability to update instantly make it ideal for modern service delivery. Web applications use HTML, CSS, and JavaScript to create rich interactive experiences that work across devices. The browser's standardized technologies and APIs provide a consistent foundation for building complex applications that once required native desktop installation.\nModern UI design patterns reflect a deep understanding of human information processing limits. Navigation structures, form layouts, and data visualization techniques are all optimized around human perceptual capabilities and attention spans. Even seemingly simple choices like the number of items displayed in a list or the depth of a menu structure are calibrated to human cognitive load limits."}, {"title": "7.1.2 Application Programming Interface (API)", "content": "Alongside user interfaces, most modern services offer application programming interfaces (APIs) for machine-to-machine communication. These interfaces expose service functionality in ways that other software can consume directly, without human intervention. REST and GraphQL have emerged as dominant paradigms for API design, offering structured ways to request and manipulate data.\nAPIs typically exchange data in formats like JSON or XML that balance human readability with machine parsing. They implement authentication mechanisms, rate limiting, and usage quotas to manage resource consumption. Many services provide software development kits (SDKs) that wrap their APIs in language-specific libraries, making it easier for developers to integrate services into their applications. 11 While programmatic interfaces enable automa-tion, they're still largely designed around human development patterns. API designs prioritize clarity and ease of understanding over machine efficiency. The documentation assumes human readers who can interpret examples and understand the context. Even rate limits and quotas are typically set based on expected human-driven usage patterns rather than machine capabilities.\nThis dual infrastructure - visual interfaces for humans and programmatic interfaces for machines - has served well for traditional software integration needs. However, as AI agents emerge as a new class of software consumers, the assumptions built into both patterns face new challenges."}, {"title": "7.2 Limitations for AI Agents", "content": "Current service consumption patterns, optimized for either human interaction or traditional machine-to-machine com-munication, present several fundamental challenges for AI agents.\nUI-based consumption poses immediate challenges for AI agents. Modern interfaces are built around human visual processing capabilities and cognitive patterns. Elements like buttons, forms, and navigation menus rely on visual recognition and spatial relationships that make perfect sense to humans but require complex interpretation by machines. While browser automation tools can interact with these elements, they must essentially simulate human interaction patterns rather than engaging with the underlying functionality directly. The human-centric design of web interfaces also creates inefficiencies in data access. Information that could be transmitted in a single response is often spread across multiple pages or views to avoid overwhelming human users. What a human experiences as a natural flow clicking through pages of search results or navigating through hierarchical menus becomes a series of forced sequential operations for an AI agent capable of processing far more information in parallel."}, {"title": "7.3 Future Design Considerations", "content": "Creating infrastructure that better supports AI agent consumption of services requires rethinking fundamental assump-tions about how software functionality is exposed and accessed. Several promising directions emerge when we con-sider the unique capabilities and requirements of AI agents.\nAt the protocol level, new standards could emerge specifically for agent-service interaction, optimized for machine-speed operations and parallel processing. These protocols might support dynamic capability negotiation and efficient data formats, moving beyond current REST and GraphQL paradigms. Enhanced RPC frameworks could support more flexible calling patterns while maintaining the performance benefits of traditional RPC, enabling efficient machine-to-machine communication that better matches how AI agents operate.\nServices could expose their capabilities through rich, machine-readable descriptions that go beyond traditional API documentation. These descriptions would enable AI agents to understand not just available endpoints, but the semantic meaning of operations, their prerequisites, and their effects. Service providers could implement AI interfaces that act as intelligent intermediaries, understanding natural language queries and translating them into appropriate internal operations. This approach would allow services to maintain their existing infrastructure while providing a more intuitive interface for AI agents.\nRather than maintaining separate UI and API interfaces, services might adopt unified interfaces that adapt to the consumer's capabilities. These interfaces could adjust their response format and granularity based on whether they're dealing with a human user or an Al agent, eliminating the current inefficiencies of forcing agents to either navigate human UIs or work within constrained API boundaries. These adaptive interfaces would support both traditional consumption patterns and new agent-oriented interactions.\nThe infrastructure could also support dynamic service composition, allowing agents to discover and combine service capabilities at runtime. This would enable agents to create new workflows and applications by combining existing services in novel ways, operating at machine speed and scale. By supporting this kind of dynamic integration, the infrastructure would enable entirely new patterns of service consumption and value creation.\nThese enhancements would fundamentally change how AI agents interact with software services, enabling them to operate more efficiently and create more value through dynamic service integration. However, implementing these changes requires careful consideration of security, stability, and backward compatibility with existing systems. The transition to this new infrastructure will likely be gradual, with services initially offering enhanced capabilities along-side traditional consumption patterns."}, {"title": "8 Payments", "content": "Payment systems were notably absent from the internet's original design. As e-commerce emerged, existing payment networks like Visa and Mastercard were adapted to work online. This adaptation required significant infrastructure changes to secure payment data transmission. The payment card industry developed extensive security standards (PCI"}, {"title": "8.1 Current Infrastructure", "content": "Traditional payment networks form the backbone of online commerce. When a customer makes a credit card pur-chase, their card details are captured through secure browser forms and immediately tokenized - converting sensitive data into secure tokens that can be stored and reused without exposing the actual card numbers. The transaction involves multiple parties: the issuing bank (customer's bank), the acquiring bank (merchant's bank), and the card network (Visa/Mastercard) that connects them. The payment flow differs significantly between one-time payments and subscriptions. While one-time payments require explicit customer action, subscription systems operate on a \"pull\" model where merchants can automatically charge previously stored payment methods. This process involves two distinct phases: real-time authorization where the payment is approved, and settlement which typically occurs days later when funds actually move between banks. For international transactions, this system grows more complex, re-quiring correspondent banking relationships to move money across borders. Traditional bank transfers like ACH and wires still handle significant transaction volume, especially for business payments, though they operate on even slower settlement schedules.\nPayment systems also integrate deeply with business operations beyond just moving money. Merchants must collect billing information for tax purposes, often connecting payment processing with accounting software for automated reconciliation and reporting. While these networks excel at handling larger transactions, they were not designed for micropayments. Instead, services typically implement metering or credit-based systems where users pre-purchase credits or are billed periodically based on accumulated usage.\nModern payment processors have innovated by abstracting this complexity. Rather than requiring merchants to estab-lish their own merchant accounts and banking relationships, processors handle the entire payment stack. They provide unified APIs that support multiple payment methods while managing the underlying complexity of different payment networks. These processors implement sophisticated event systems and webhooks to handle payment state changes, retry logic, and fraud detection. This programmatic approach to payment processing has made it possible for digital services to implement complex payment flows without managing direct relationships with financial institutions.\nDigital payment platforms operate as self-contained systems with their own ledgers. While often presented as \"digi-tal wallets,\" these platforms effectively function as e-money institutions, subject to specific regulatory requirements. Users maintain balances within the platform, with transactions executing as internal ledger updates. This model has evolved furthest in Asian markets, where super-apps like WeChat Pay and Alipay process enormous transaction vol-umes within their ecosystems. These platforms must still interface with traditional banking systems for deposits and withdraws but can operate independently for transfers between users.\nThe cryptocurrency ecosystem has developed a parallel payment infrastructure. While blockchain networks provide the underlying transaction layer, practical implementation requires additional infrastructure. Cryptocurrency exchanges serve as key on and off-ramps, converting between traditional currency and digital assets. Wallet software manages key storage and transaction signing, while stablecoins facilitate faster settlement by avoiding traditional banking rails. However, this infrastructure still struggles with scalability and regulatory compliance, particularly around identity verification and anti-money laundering requirements."}, {"title": "8.2 Limitations for AI Agents", "content": "Current payment infrastructure presents fundamental barriers to AI agents operating as autonomous economic ac-tors. The most immediate challenges stem from identity and verification requirements designed around human actors. Payment systems require extensive Know Your Customer (KYC) and Anti-Money Laundering (AML) verification, including government-issued identification, physical addresses, and phone numbers. These systems often include manual review processes and CAPTCHA mechanisms specifically designed to prevent automated account creation and transactions. Even when verification is possible, it typically requires human intervention, breaking the potential for fully autonomous operation.\nA crucial point is that modern payment infrastructure is deliberately designed to prevent automated participation. Pay-ment processors like Stripe implement multiple layers of anti-automation measures from browser fingerprinting to behavioral analysis - specifically to ensure human involvement in financial transactions. These are not only security features but core design principles of the system. Anti-bot measures are deeply embedded in every layer, from account creation through transaction processing, making the infrastructure inherently hostile to autonomous agent participa-tion.\nTechnical and operational constraints further limit AI agent participation. Typical API rate limits of 100 requests per minute for standard payment endpoints would severely constrain agents operating at machine speed. Authorization flows frequently require human interaction through redirect flows or manual confirmation steps. Fraud prevention"}, {"title": "8.3 Future Design Considerations", "content": "Future payment infrastructure must enable autonomous economic participation while maintaining integration with existing financial systems. This requires both protocol-level innovations and new operational frameworks. At the protocol level, payment systems need standardized interfaces for programmatic execution, condition verification, and real-time settlement. For example, payment APIs could expose endpoints for automated compliance checks and conditional transfers, enabling agents to verify and execute transactions without human intervention. These protocols would need to support atomic operations where complex multi-step transactions either complete entirely or roll back, preventing inconsistent states during agent interactions.\nAn example of this is the L402 protocol which demonstrates potential approaches to machine-friendly payments. L402 extends the original HTTP 402 status code to create a complete protocol for payment-required API access. When a server responds with L402, it includes machine-readable payment terms specifying the price, payment methods, and conditions for access. Clients can then complete payments through supported payment networks before retrying their request with proof of payment. This decoupling of payment negotiation from payment execution allows clients to optimize their payment strategy while giving servers flexibility in how they price and gate access. Such protocols could form the foundation for standardized payment interactions between AI agents and services.\nAuthentication and compliance infrastructure requires significant evolution. Rather than relying on human documenta-tion, systems could implement cryptographic attestation protocols where agents prove their identity and authorization through verifiable credentials. Traditional KYC/AML processes could be augmented with continuous transaction monitoring specifically designed for agent behavior patterns. This would enable payment processors to maintain reg-ulatory compliance while supporting autonomous operation. Such systems might implement risk scoring based on agent transaction history, delegation chains, and behavioral patterns rather than traditional credit metrics.\nThe payment infrastructure must adapt to support machine-scale operations. Payment processors could implement tiered fee structures optimized for high-frequency, low-value transactions. Rather than fixed fees that make micropay-ments impractical, dynamic pricing could adjust based on volume and market conditions. New protocols could enable real-time price discovery and automated negotiation between agents.\nTo support AI agents as economic participants, digital infrastructure needs enhancements that cater to their unique capabilities. Agents would benefit from digital wallets designed for autonomous operation, allowing them to manage funds and transact without human involvement.\nThis infrastructure evolution requires careful coordination between payment processors, financial institutions, and regulatory bodies. Each component must maintain backward compatibility while enabling new capabilities. The goal is to extend existing payment rails to support autonomous economic actors while preserving the security and reliability of current financial systems."}, {"title": "9 Future Work", "content": "Enabling AI agents to participate fully in digital markets represents a transformation too vast for any single organiza-tion to accomplish. The infrastructure challenges outlined in this paper - from authentication to payments - are deeply interconnected and require coordinated evolution across multiple domains.\nRather than seeking perfect solutions immediately, the community should focus first on creating an end-to-end proto-col stack that enables basic machine-to-machine service discovery, integration, and consumption. This initial version might rely on imperfect workarounds and existing infrastructure, but"}]}