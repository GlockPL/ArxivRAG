{"title": "Lessons from Learning to Spin \"Pens\"", "authors": ["Jun Wang", "Ying Yuan", "Haichuan Che", "Haozhi Qi", "Yi Ma", "Jitendra Malik", "Xiaolong Wang"], "abstract": "In-hand manipulation of pen-like objects is an important skill in our daily lives, as many tools such as hammers and screwdrivers are similarly shaped. However, current learning-based methods struggle with this task due to a lack of high-quality demonstrations and the significant gap between simulation and the real world. In this work, we push the boundaries of learning-based in-hand ma- nipulation systems by demonstrating the capability to spin pen-like objects. We first use reinforcement learning to train an oracle policy with privileged informa- tion and generate a high-fidelity trajectory dataset in simulation. This serves two purposes: 1) pre-training a sensorimotor policy in simulation; 2) conducting open- loop trajectory replay in the real world. We then fine-tune the sensorimotor policy using these real-world trajectories to adapt it to the real world dynamics. With less than 50 trajectories, our policy learns to rotate more than ten pen-like objects with different physical properties for multiple revolutions. We present a comprehensive analysis of our design choices and share the lessons learned during development.", "sections": [{"title": "1 Introduction", "content": "Dexterous in-hand manipulation is a foundational skill for various downstream manipulation tasks. For example, one often needs to reorient a tool in hand before using it. Despite decades of active research in this area [1, 2, 3, 4], in-hand manipulation remains a significant challenge. Manipulating pen-like objects, in particular, is considered one of the most challenging and crucial tasks [5, 6]. This capability is highly practical, as many tools, such as hammers and screwdrivers, have similar shapes. Moreover, spinning pen-like objects requires dynamic balancing and sophisticated finger coordination, making it an ideal testbed for advancing dexterous manipulation systems.\nPen spinning has been studied from several perspectives. Classic robotics works demonstrate ro- tating wooden blocks with open-loop force control [1]. With high-speed cameras and advanced hardware, agile pen spinning can also be achieved [6]. However, these methods rely on accurate object models and cannot generalize to unseen objects. On the other hand, learning-based methods hold the promise of being generalizable with large-scale data. They have indeed achieved significant progress either with imitation learning [7, 8, 9] or sim-to-real [3, 10, 11, 12]. However, they have only demonstrated manipulation of regular spherical or cuboid-shaped objects, and none can extend the capability to pen-like objects. We attribute this to two reasons: For imitation learning, current teleoperation systems fail at collecting complex and dynamic demonstrations such as pen spinning; for sim-to-real, bridging the gap for dynamic tasks becomes substantially difficult.\nIn this work, we push the boundaries of learning-based in-hand manipulation systems by demon- strating their capability to spin pen-like objects. Similar to previous approaches [10, 11, 13], we first learn an oracle policy with privileged information using reinforcement learning in simulation. However, when attempting to distill it into a sensorimotor policy, we find the sim-to-real gap too large. While this gap generally exists in previous in-hand manipulation tasks [11, 13], the extreme difficulty of spinning pen-like objects exposes the gap even further. Fine-tuning the policy with real- world trajectories can be one way to mitigate this gap, but it is challenging to collect demonstrations via teleoperation for this dynamic task. Inspired by recent analysis on open-loop controllers [14, 15], we instead collect a high-fidelity trajectory dataset in simulation and use it as an open-loop con- troller on the real robot. The successful trajectories in the real world serve as our high-quality demonstrations. We then bridge the sim-to-real gap by fine-tuning our sensorimotor policy with these real-world trajectories. With simulation pre-training, our sensorimotor policy has the motion prior from diverse data and can adapt to real-world physics with fewer than 50 trajectories.\nWe conduct comprehensive experiments in both simulation and the real world. In simulation, we identify the key factors that enable the oracle policy to learn the challenging pen-spinning task and generate realistic trajectories. We then evaluate different methods of obtaining a deployable policy in the real world. We also conduct ablation experiments showing the importance of pre-training in simulation. We demonstrate that our policy can adapt to real-world physics with fewer than 50 real- world trajectories. To the best of our knowledge, this is the first learning-based system to achieve continuous spinning of pen-like objects in the real world."}, {"title": "2 Related Work", "content": "Classic in-hand manipulation. In-hand manipulation has been studied for decades [2, 16]. Clas- sical methods rely on an accurate model and analytically plan a sequence of motions to control the object. For example, Han and Trinkle [17] manipulate objects using sliding, rolling, and finger gait- ing motions, while Bai and Liu [18] studies the collaboration of fingers and the palm. Open-loop manipulation also shows surprising robustness and dexterous behavior [14, 15]. Sieler and Brock [19] uses linearized feedback-control for in-hand manipulation with a soft hand. State-of-the-art systems in this category include full SO(3) reorientation using a compliance-enabled hand [20] and an accurate pose tracker [21]. However, most methods cannot manipulate pen-like objects due to their complex and dynamic nature. Extrinsic dexterity [22] can also be used to achieve dynamic manipulation, but a precise model is necessary. In contrast, our method uses human priors to build a simulator environment but does not rely on an accurate model during deployment.\nLearning-based dexterous manipulation. Learning-based methods make fewer assumptions and hold the promise of being more generalizable as we acquire more data. Recently, significant progress has been made in this field [3, 4]. The advancement mainly comes from two sources: 1) low- cost and accessible teleoperation systems [7, 8, 23, 24, 25, 26, 27, 28] combined with imitation learning [29, 30]; and 2) reinforcement learning in simulation [31, 32] combined with sim-to-"}, {"title": "3 Learning to Spin Pens", "content": "An overview of our method is shown in Figure 2. Our method consists of three steps. First, we train an oracle policy with privileged information to generate realistic trajectories in simulation. With these trajectories, we pre-train a sensorimotor policy in simulation. We then use these trajectories as"}, {"title": "3.1 Oracle Policy Training", "content": "Obtaining high-quality data for pen spinning is itself a challenging task due to the dynamic and complex movements involved. The current teleoperation system is not suitable due to the non- negligible latency and imperfect retargeting error between the human hand and the robot hand. Alternatively, previous work shows that reinforcement learning can synthesize complex behaviors in simulation [52, 53]. These methods achieve fast and dynamic behavior but may violate real-world physics and hardware constraints. In contrast, we design our approach to generate high-quality trajectories that are realistic enough for use as an open-loop controller in the real world. This is achieved by properly designing the input space, reward function, and initial state distributions.\nObservations. The observation $o_t$ of the oracle policy $f$ is a combination of the following quanti- ties: joint positions $q_t$, previous joint position target $a_{t-1}$, binary tactile signals $c_t$, fingertip posi- tions $p_t$, the pen's current pose and angular velocity $w_t$, and a point cloud of the pen at the current state $x \\in R^{100 \\times 3}$. To obtain fine-grained tactile responses, we augment the sensor arrangement in [12] to include five binary sensors on each fingertip (see Figure 6). The point cloud is obtained by trans- forming points on the original mesh based on the current ground-truth object pose. We encode the point cloud using PointNet [56] as in [33, 57, 58]. We stack three historical states of joint positions and targets as inputs. We also include physical properties such as mass, center of mass, coefficient of friction, and object size in the input [11]. The dimensions of the inputs are detailed in the appendix.\nActions. At each step, the action provided by the policy network $f(o_t)$ is a relative target position. The position command $a_t = \\eta f(o_t) + a_{t-1}$, where $\\eta$ is the action scale, is sent to the robot and it will be converted to torque via a low-level PD controller.\nReward. The goal of the policy is to continuously rotate the pen around the z-axis. Our reward is defined as a combination of rotation reward and a few energy penalty terms. The reward and penalty terms follow [11, 12]. However, stable gaits do not emerge solely from this. Motivated by [52], we propose another reward $r_z$, a penalty regarding the height difference between the highest and the lowest points on the pen, encouraging the robot hand to keep the pen horizontal during rotation.\nIn summary, our reward function is (t omitted for simplicity): $r = r_{rot} + \\gamma_z r_z + \\gamma_{energy} r_{energy}$, where $r_{rot}$ rewards the pen's rotation velocity and $r_{energy}$ penalizes the object's linear velocity, deviation from initial joint positions, mechanical work, and torque applied (see appendix for details).\nInitial state design. Our task fundamentally differs from previous work where the object is placed on the palm [12, 13], a table [10], or fingertip by gravity [11], where there is natural support in those cases. Therefore, using randomly sampled poses does not provide meaningful exploration in our case. We find that a proper design of the initial state distribution is critical for policy training. De- signing initial states for pen rotation is non-trivial because the initial grasp should be stable enough to facilitate learning subsequent steps of motion. Moreover, exploration can be slow if we repeat- edly use the same initial state upon reset. Thus, inspired by human behavior, we manually design multiple patterns of grasping that may occur in the cycle of pen rotation (visualized in Figure 3), and then add noise to generate and filter for a set of stable initial states."}, {"title": "3.2 Sensorimotor Policy Pre-training", "content": "The oracle policy mentioned above can learn smooth and dynamic behavior during simulation train- ing. However, it cannot be deployed because it requires privileged information as input, which is not accessible in the real world. Previous works typically distill the oracle policy into a sensorimotor policy using DAgger [60]. However, we find this approach does not work well for our pen-spinning task. We experimented with either proprioception [11] or adding visuotactile feedback [13, 33]. While the policy with visuotactile feedback can learn reasonable behavior in simulation, the mis- match between simulation and reality is too large for these two modalities. On the other hand, proprioceptive feedback is the most similar and reliable sensing method between simulation and the real world, but the proprioceptive policy cannot converge even in simulation and always drops the object in the first few steps.\nFor this reason, we propose an alternative approach: we roll out the oracle policy $f$ in simulation, in contrast to previous work using DAgger and rolling out the sensorimotor policy [11, 33], and collect a dataset of proprioception and actions $(s_t, a_t)$. This dataset is used to pre-train a proprioceptive policy in simulation. The goal of this step is to expose the sensorimotor policy to diverse training data. Although training with such data cannot enable direct transfer to the real world due to inac- curate dynamics, it can provide a motion prior, allowing the policy to be efficiently fine-tuned with real-world trajectories.\nFollowing [11], our proprioceptive policy takes 30 steps of joint positions $q_{t-29:t}$ and previous joint targets $a_{t-30:t-1}$ as input. We use a temporal transformer similar to the one used in [33] to model sequential features and an MLP for the policy network. Such pre-training allows our proprioceptive policy to experience a wider range of circumstances, preventing overfitting to specific trajectories."}, {"title": "3.3 Fine-tuning Sensorimotor Policy with Oracle Replay", "content": "Due to the large sim-to-real gap of our task, we choose to use real-world trajectories to fine-tune the pre-trained sensorimotor policy to adapt to real-world dynamics. However, obtaining real-world trajectories is challenging. Our key observation is that although the oracle policy cannot be directly distilled and zero-shot transferred to the real world, it does provide motion sequences that are dif- ficult to generate using teleoperation. Inspired by recent work that highlights the effectiveness of open-loop controllers for in-hand manipulation [14, 15], we use the trajectories generated by the oracle policy as an open-loop controller in the real world."}, {"title": "4 Experiments", "content": "In this section, we compare our approach for pen spinning to several baselines in both simulation and the real world. Specifically, we study 1) the critical design choices in obtaining an oracle policy that can be replayed in the real world; 2) various techniques for sim-to-real deployment."}, {"title": "4.1 Experiment Setup", "content": "Hardware setup. We use the Allegro Hand for our hardware experiments.\nThe Allegro Hand has four fingers, each with 4 degrees of freedom. Our neural network outputs the joint position target at 20 Hz, which is sent to a low-level PD controller operating at 333 Hz.\nSimulation setup. We use Isaac Gym [32] for our simulation training. To obtain additional tactile feedback for oracle policy training, we simulate 20 tactile sensors around the fingertips, with 5 on each fingertip. We gather the contact signal from each sensor and binarize the measurement based on a"}, {"title": "4.2 Oracle Policy Training", "content": "The goal of the oracle policy is to generate realistic trajectories that can be used both for pre-training the student policy and serving as an open-loop controller in the real world. We compare several critical factors in achieving this, specifically: 1) without a well-designed initial pose distribution; 2) without privileged information; 3) without $r_z$.\nQ1: How does the initial state distribution help policy training? We study the effect of a well- designed initial state distribution. The results are shown in Figure 4 left. Single Canonical Pose samples states around one canonical hand pose, as used in [11, 33]. In contrast, our method defines multiple canonical hand poses inspired by how humans spin pens and achieves better performance compared to using a single canonical pose. We also emphasize that although the curve for single canonical init does increase over time, the finger gaiting cannot emerge, and this policy cannot escape from the local minima. We visualize the behavior in Figure 5 (c) and find the finger does not break contact with the object and fails to achieve more than one revolution.\nQ2: How does privileged information help policy training? We study the importance of privi- leged information in Figure 4 right. Unlike [11], the oracle policy cannot be trained only with simple object properties such as object position. We find that without tactile feedback or a point cloud, the policy does not achieve good enough performance. The shape of the pen is important as the policy needs to know when to lift the fingers to spin the pen. Privileged information such as the object's physical properties and finger positions is also critical, without which the policy does not converge.\nQ3: How does z-reward help policy training? We study the effect of z-reward $r_z$, shown in Fig- ure 5 (b). Although the trajectories look similar to our approach at first glance, the object gets tilted"}, {"title": "4.3 Sensorimotor Policy Training", "content": "Although the oracle policy achieves great performance in simulation, it cannot be directly deployed in the real world. To address this issue, we use it as an open-loop controller to collect real-world trajectories. We also pre-train a proprioceptive policy in simulation and fine-tune it using this dataset. We compare our method with several alternatives in the real world. The results are shown in Table 1.\nQ4: Is oracle replay a good enough controller? We design our oracle policy so that it achieves decent performance in the real world (Oracle Replay). However, it still performs worse than our method. On Training Objects A/B/C, our method achieves 15%-30% better performance in terms of success rate. On Unseen Objects D/E/F, which are considered out-of-distribution, our method achieves a 10% increase in the radius rotated, despite having a similar success rate. Our method also achieves 15%-30% success rate improvements on objects I/J/K. This result demonstrates that our method generally achieves a longer radius rotated compared to the oracle replay because it is also pre-trained in simulation with more diverse data.\nQ5: Does distillation work for pen spinning? Previous approaches demonstrate promising results by distilling the oracle policy into the sensorimotor policy [11, 33, 13] using DAgger. However, this approach does not work for our dynamic and contact-rich task (Figure 1). First, we try to use segmented depth [33] or two endpoints of the pen, and the visuotactile policy can achieve reasonable performance in simulation. However, the sim-to-real gap is significantly larger compared to previous works. In our real-world deployment, the objects oscillate a lot, making the image distribution far removed from the training one. Secondly, proprioceptive feedback does not have this problem, but using proprioception alone does not achieve good performance in simulation.\nQ6: How do pre-training and fine-tuning contribute to the final performance? Our approach is first pre-trained in simulation and then fine-tuned using real-world data. We study the contribution of each part in Table 2. With only pre-training, the policy has limited effectiveness in the real world. It rarely completes finger gaiting on Objects C and D, and the success rate is also low for the remaining objects. This is mainly because the physics gap between simulation and reality becomes more significant in our task. With only behavior cloning, the approach also does not perform well. It has a 50% lower rotation radius on Training Objects. On out-of-distribution objects, the success rate drops to less than 20%, indicating that its generalization capability is limited. This is because we only have 15 trajectories for each object, and the policy tends to overfit to that data.\nQ7: Can simulation pre-training be replaced by more demonstrations? We also study whether increasing the number of real-world demonstrations can substitute for the advantages gained from pre-training in simulation. The results are shown in Table 3. We find that although the performance of the No Pretrain baseline can be improved with more demonstrations, it gradually saturates when increasing the number of demonstrations from 45 to 75. In addition, the major improvements come from the training objects (A/B/C), while the performance on unseen objects (D/E/F) is still far worse compared to our methods. This indicates that solely relying on real-world trajectories is likely to overfit to certain objects."}, {"title": "4.4 Qualitative Experiments", "content": "In addition to the objects we present in the quantitative study, we also try more different objects for our policy and try to push the limits on objects that are significantly out-of-distribution. Some examples are shown in our Figure 1. We find our model can rotate objects for multiple revolutions with smooth finger gaiting. Videos are shown on our project website."}, {"title": "5 Conclusion and Lessons", "content": "In this paper, we present the first learning-based approach for spinning pen-like objects. Through our extensive experiments, we share the lessons we learned as follows:\n\u2022 Simulation training requires extensive design for exploration, such as the proper design of initial distributions to aid exploration and using privileged information to facilitate policy learning.\n\u2022 Sim-to-Real does not directly work for such contact-rich and highly dynamic tasks. Even when isolating touch and vision, the pure physics sim-to-real gap remains significant and cannot be bridged by extensive domain randomization alone.\n\u2022 Simulation is still useful for exploring skills. The dynamic skill of spinning pens with a robotic hand is nearly impossible to achieve with human teleoperation and imitation learning alone. Re- inforcement learning in simulation is critical for exploring feasible motion.\n\u2022 Only a few real-world trajectories are needed for fine-tuning. Although a proprioceptive policy learned purely in simulation does not work directly in the real world, it can be fine-tuned to adapt to real-world physics using only a few successful trajectories.\nLimitations. We have identified several key bottlenecks of using vision and touch during sim-to-real for this dynamic task. However, we are not stating they should not be used. Humans do not seem to need vision to spin a pen, but touch feedback seems important. In future work, we will explore whether using them can help further improve performance. Currently, the system is only capable of rotating along z-axis, it is also a promising direction to extend it to general multi-axis rotation."}, {"title": "A Implementation Details", "content": "A.1 Training Hyper-parameters\nOur reward function is a combination of $r_{rot}$,$r_z$ and $r_{energy}$. The energy reward consists of $r_{vel}$, $r_{diff}$, $r_{ang}$, $r_{torq}$, and $r_{work}$. Here, $r_{vel}$ penalizes the pen's linear velocity, $r_{diff}$ discourages the hand's pose from deviating much from its initial pose, $r_{ang}$ penalizes the pen's angular velocity above a pre-defined threshold to encourage stable rotation, $r_{torq}$ penalizes large torques, and $r_{work}$ penalizes the work of the controller. We follow the same definition of reward in [33]. We combine the above rewards with weights listed in Table 4.\nWe detail the dimensions of the inputs of our oracle policy in Table 5. We train our oracle policy with PPO, and the training hyper-parameters are shown in Table 7. Specifically, we train with 8192 parallel environments. Each environment gathers # steps data to train in each epoch of PPO. The data is split into # minibatches and optimized with PPO loss. $\\gamma$ and $\\lambda$ are used for computing generalized advantage estimate (GAE) returns. We use the Adam optimizer to train PPO and adopt the gradient clip to stabilize training. We train 500 million agent steps in total, which takes less than one day on a single GPU. We train our student policy with Behavior Cloning, and the training hyper-parameters are shown in Table 6. We collect approximately 50M steps of data in total."}]}