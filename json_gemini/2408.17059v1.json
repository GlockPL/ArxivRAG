{"title": "A Survey of the Self Supervised Learning Mechanisms for Vision Transformers", "authors": ["Asifullah khan", "Anabia Sohail", "Mustansar Fiaz", "Mehdi Hassan", "Tariq Habib Afridi", "Sibghat Ullah Marwat", "Farzeen Munir", "Safdar Ali", "Hannan Naseem", "Muhammad Zaigham Zaheer", "Kamran Ali", "Tangina Sultana", "Ziaurrehman Tanoli", "Naeem Akhter"], "abstract": "Deep supervised learning models require high volume of labeled data to attain sufficiently good results. Although, the practice of gathering and annotating such big data is costly and laborious. Recently, the application of self supervised learning (SSL) in vision tasks has gained significant attention. The intuition behind SSL is to exploit the synchronous relationships within the data as a form of self-supervision, which can be versatile. In the current big data era, most of the data is unlabeled, and the success of SSL thus relies in finding ways to improve this vast amount of unlabeled data available. Thus its better for deep learning algorithms to reduce reliance on human supervision and instead focus on self-supervision based on the inherent relationships within the data. With the advent of ViTs, which have achieved remarkable results in computer vision, it is crucial to explore and understand the various SSL mechanisms employed for training these models specifically in scenarios where there is less label data available. In this survey we thus develop a comprehensive taxonomy of systematically classifying the SSL techniques based upon their representations and pre-training tasks being applied. Additionally, we discuss the motivations behind SSL, review popular pre-training tasks, and highlight the challenges and advancements in this field. Furthermore, we present a comparative analysis of different SSL methods, evaluate their strengths and limitations, and identify potential avenues for future research.", "sections": [{"title": "1 Introduction", "content": "Deep learning based algorithms have exhibited impressive results across various disciplines specially in computer vision (CV) [1] and natural language processing (NLP) [2]. Deep learning based models utilize a pre-training task on large datasets to enhance their performance. This primary step is often utilized as an initialization point and then the model is optimized for any specific use case. Self-supervised learning (SSL) is among the pre-training strategies for deep learning algorithms. Overall, SSL approach is driven by two major motivations. Firstly, a network that has been trained on an extensive data has already learned distinctive patterns that can be applied to subsequent tasks. This also helps to decrease the overfitting issue during training step. Secondly, parameters learned from extensive data provide effective parameter initialization, which enables faster convergence across different applications [3].\nAlthough there is abundance of unlabeled web data in the era of big data, obtaining high-quality labeled data with human annotations can be costly. For instance, data labeling companies like Scale.com [4] charge $6.4 per image for image segmentation labeling. Creating an image segmentation dataset with over 1 million high-quality samples (JFT-300M) could cost up to a million dollars. Usually, this process is time-intensive and inefficient [5]. Moreover, methods of supervised learning"}, {"title": "1.1 Survey Scope and Contributions", "content": "Several interesting surveys related to SSL have been reported. However, these surveys on SSL, serve to particular domains like recommendation systems [13], graphs [14], [15], sequential transfer learning [16], videos [17] and algorithms [5]. Vision Transformers (ViTs) are a prominent area of research in computer vision and have recently demonstrated several breakthroughs. However, the training of ViTs remains challenging. Therefore, this survey, contrary to existing surveys, explores the different SSL methods proposed for ViTs."}, {"title": "1.2 Survey Structure", "content": "The paper begins with an introduction to SSL and a detailed discussion on importance of SSL in ViTs. The reported SSL methods are segmented into 5 classes according to their unique features. The paper discuss the recent advancements and applications of SSL across different computer vision tasks. It also presents a taxonomy for SSL, which classifies them based on how feature learning is applied within their architecture. According to this taxonomy, SSLs are divided into five groups, each representing unique way of making use of input features. Frequently used abbreviations are listed in Table 1. The structure of paper is depicted in Fig 1. Section 1 discuss a systematic understanding of the SSL architecture, highlighting its need in ViTs and outlined the advent of SSL architectures. Proceeding to section 2 which covers the advancements in SSL variants, while section 3 provide a taxonomy of the recent SSL architectures, respectively. Section 4 focuses on the regularization techniques used in SSL, particularly in the area of computer vision, section 5"}, {"title": "2 Evolution of SSL", "content": "In computer vision SSL methods are generally segmented into contrastive, generative, and predictive approaches. Contrastive methods, such as MoCo and SimCLR, aim to learn patterns by contrasting positive and negative samples. Generative methods, such as GANs and VAEs, focus on learning representations by generating samples from the learned representation space. Predictive methods, such as BERT and T5, learn representations by predicting some parts of the input data.\none SSL category is instance discrimination, in which the motto of learning is basically focused on learning patterns by differentiating each sample image from others, and this method is rather challenging for large volume of datasets. Examples are SimCLR, SWAV, BYOL, MOCO [18-21]."}, {"title": "2.1 Evolution of ViTs", "content": "In the domain of computer vision, ViTs is newly arising development motivated by the success of transformers architecture and it is presented in seminal paper \"Attention Is All You Need\" [9]. Transformers has revolutionized the learning process by enabling models to learn patterns from substantial volume of unlabeled data.\nIn computer vision, the traditional approach in image processing tasks is the utilization convolutional neural networks (CNNs). However, the CNNs have limitations in capturing dependencies across long spatial distances and global context, while both are crucial for some vision tasks. ViTs address these limitations via self-attention mechanisms to learn and understand the global context and also dependencies over long spatial distances in the input data.\nThe initial vision transformer architecture ViT, was presented in the paper \"An Image Is Worth 16x16 Words\" [10] Since then, various variants of ViTs are proposed, including DeiT, Swin Transformer, and TNT. These architectures have obtained state-of-the-art performance in different computer vision based tasks, including image classification, object segmentation and object detection."}, {"title": "2.2 Importance of SSL in ViTs", "content": "SSL is especially crucial in the context of ViTs, as it empowers models to learn representations from large amount of unlabeled data. ViTs need vast amounts of data to learn effective representations, and SSL provides a pathway for pretraining on large-scale datasets. By pretraining on large-scale datasets, ViTs can learn more robust."}, {"title": "2.3 Evolution of SSL in Computer Vision", "content": "SSL technique is used for representation learning where a model is designed to learns the underlying relationships between its inputs. The framework of Energy-Based Models (EBMs) can effectively describe this objective, where the aim of assigning high energy to inputs that are incompatible and low energy to those that are compatible. The foundation for many self-supervised learning techniques applied to images was laid by Geoffrey Hinton and others in the 1980s through the development of autoencoders and the proposal of greedy layer-wise pretraining [22], where layers of a deep neural networks are trained sequentially. However, autoencoders were basically used for learning features and reducing dimensionality. They were impactful at that time as they made it possible to train the first \"deep\" networks.\nAn analogous approach was Restricted Boltzmann Machines (RBMs), that allows layer-wise training and then combine those layers to build deep belief nets [23]. Although these techniques were eventually replaced by simpler initialization strategies and extended training procedures, they played a vital part in the development of early deep learning architectures.\nIn 2000s, types of unsupervised learning, including sparse coding and K-means, were employed for self-supervised learning (SSL). These techniques aimed to learn image features without labeled data, laying the groundwork for more complex self-supervised methods. The category of Spatial Context prediction gained attention in SSL for the training of CNNs. In this regard, Pathak et al. (2016) proposed inpainting using context prediction as a pretext task for SSL in images, where the"}, {"title": "2.4 Comparison of SSL with Transfer Learning", "content": "In computer vision, transfer learning (TL) is employed in the development of CNN models to solve a new challenging problem by utilizing the pre-trained model's weight sharing mechanism especially in the presence of data scarcity [39]. Initially, transfer learning was the dominant approach, but recently, SSL has demonstrated promising results across various applications.\nAlthough TL and SSL are not mutually exclusive techniques, they have a few major differences, as discussed in Table. 3. Moreover, Fig 3 shows the TL and SSL workflow. TL and SL have two main steps: (i) training on a source task and (ii) adapting to a specific target task. The aiming of first step is to optimize the network's weight parameters to obtain a better starting point."}, {"title": "3 SSL based Techniques for ViTs", "content": "In computer vision, the success of ViTs is based on large-scale self-supervised pre-training. This paper categorizes the approaches for self-supervised pre-training in ViTs into five groups: Contrastive, Generative, Clustering, knowledge distillation and Hybrid SSL methods. Each methodology's nuances and impact on transformer learning are discussed in detail in subsequent sections."}, {"title": "3.0.1 Contrastive Methods", "content": "Contrastive Learning (CL) stands as a prominent technique in SSL (He et al., n.d.; Chen et al., 2020). Its aim is to capture invariant semantics through pairs of random views, termed Contrastive multi-view coding. CL ensures that global projections of representations are similar for positive samples and dissimilar for negative samples. This section delves into literature reviews on Contrastive Learning applied in transformer frameworks.\nLiterature based Review of Contrastive Learning\nWang et al. (2022) introduced the method SRCL that is semantically-relevant contrastive learning for histopathology image analysis. Unlike traditional contrastive learning, SRCL evaluates similarity among instances to identify additional positive pairs by arranging various positive examples that share related visual concepts. This approach enriches the diversity of positive pairs and yields more informative patterns. They employee a hybrid model named CTransPath, in which they combines a CNN with a multi-scale Swin Transformer architecture, their methodology acts as a unified feature extractor, addressing both local and global features during pretraining, seeking to acquire global feature interpretations tailored for activities within the domain of histopathology images. The efficacy of SRCL-pretrained CTransPath was evaluated across five distinct downstream tasks spanning nine publicly available datasets, such as retrieving patches, classifying patches, performing weakly-supervised classification of whole-slide images, detecting mitosis, and segmenting"}, {"title": "3.0.2 Knowledge Distillation Methods", "content": "Knowledge distillation transfers knowledge from a complex teacher model to a simpler student model without labeled data. This section explores literature on knowledge distillation in SSL applied to ViTs."}, {"title": "3.0.3 Clustering Methods", "content": "Clustering-based SSL utilizes unsupervised clustering algorithms to learn representations from unlabeled data. This technique groups similar data points based on specific criteria, such as feature similarity, and then predicts these clusters or cluster assignments. By encouraging the model to distinguish between different clusters, clustering-based SSL extracts semantically rich features for downstream tasks like classification or segmentation. This approach proves valuable when labeled data is limited or costly to obtain, enabling the model to learn useful representations directly from the unlabeled data.\nClustering-based methods segment the latent space by applying clustering. This clustering can be performed offline, utilizing the entire dataset, as in PCL [47], DeepCluster v2 [19], and SeLa [48], or online, using mini-batches, as in DINO [29] and SwAV [19]. These approaches enforce consistent clustering assignments of positive pairs using cross-entropy loss. However, previous literature have not taken advantage of this beneficial property in semi-supervised scenarios. Our study aims to fill this gap.\nLiterature based Review of Clustering Methods\nSu et al. (2023) introduced a clustering-based SSL framework tailored for ViTs, aiming to enhance performance of tasks related to object detection and segmentation. Their proposed methodology, Feature-Level SSL (FLSL), works at two levels of semantics: intra-view, which deals with individual image, and inter-view which considers over an entire dataset. FLSL utilizes a bi-level feature clustering approach, integrating mean-shift clustering intrinsic to transformers for extracting modes as patterns with a k-means-based SSL technique, ensuring semantic coherence of extracted representations both locally and globally.\nAt the first semantic level, FLSL optimizes intra-cluster affinity using a self-attention layer to encourage semantic representations within clusters. On the second tier, semantic representations are cultivated through non-empty k-means clustering, with positive samples identified using a cross-attention layer. Experimental outcomes illustrate the efficacy of FLSL in dense prediction tasks, achieving significant improvements in object detection and instance-level segmentation on MS-COCO, utilizing Mask R-CNN with ViT-S/16 and ViT-S/8 as backbones, respectively.\nFini et al., (2023) leverage the limited annotations in the semi-supervised setting to enhance the quality of learned representations. The core idea involves substituting cluster centroids with class prototypes learned through supervised methods, guiding unlabeled data to cluster around these prototypes through a self-supervised clustering-based objective. Their approach involves the joint optimization of a supervised loss on labeled data and a self-supervised loss on unlabeled data. For a given an unannotated dataset $K_i = \\{x_1^i, ..., x_n^i\\}$, two versions of each input image, $(x, \\hat{x})$, are created through data augmentation technique. These versions are"}, {"title": "3.0.4 Generative Methods", "content": "Generative approaches, for example Generative Adversarial Networks (GANs) or Autoencoders, provide a promising avenue for pre-training ViTs. These models not only generate realistic images or reconstruct input images but also encode meaningful representations of visual content, facilitating enhanced learning of features in subsequent tasks.\nLiterature based Review of Generative Methods\n123456789- Chen et al. (2020) introduced iGPT, an image-based version of GPT, for self-supervised visual learning. Unlike ViT, which uses patch embedding, iGPT directly resizes and flattens images into lower-resolution sequences, which are then fed into a GPT-2 model for autoregressive pixel prediction. Despite its significant computational cost, iGPT achieves high accuracy on CIFAR-10, surpassing supervised Wide ResNet.\nBao et al. (2022) introduced BEIT, that is Bidirectional Encoder representation from Image Transformers is a self-supervised vision representation model. Instead of pixel-wise generation, they adopted MIM as a task to pretrain ViTs in a self-supervised manner. BEIT, grounded on a BERT-style visual Transformer, reconstructs masked images in the latent space. The approach involves randomly masking parts of image pieces and predicting the visual tokens relevant to these hidden pieces. The main motivation behind this pre-training technique is to restore the base visual tokens utilizing corrupted image patches. The standard visual Transformer serves as the backbone network. Evaluation using thorough fine-tuning for tasks such as image classification and semantic segmentation demonstrated competitive performance without human annotation.\nHe et al., (2021) proposed MAE (Masked Autoencoders), in this methodology parts of input images are randomly masked and subsequently reconstructed the pixels that were masked, enabling optimized training of large models that demonstrate strong generalize across downstream tasks. This method utilizes an asymmetric encoder-decoder setup, where the encoder only looks on the visible parts of image, and a small decoder rebuilds the original image usingthe learned representation and mask tokens. By hiding a large proportion of the input image, e.g., 75%, MAE creates a challenging and useful self-supervised learning task. This scalable method allows large models to train efficiently and effectively, resulting in powerful models that generalize well. For instance, a standard ViT-Huge model achieved the highest accuracy of (87.8%) among techniques using only ImageNet-1K"}, {"title": "3.0.5 Hybrid Methods", "content": "Hybrid methods similar to generative approaches often combine the strengths of different techniques to achieve specific objectives.\nLiterature based Review of Hybrid Methods\nWu et al. (2024) implemented the hybrid SSL pre-training framework Enhanced SSL with Masked Autoencoders (ESLA). This innovative approach integrates both contrastive learning (CL) and MAE frameworks to enhance SSL. ESLA improves model generalization and representation while addressing issues stemming from competing features during data enhancement. This is achieved through a masking mechanism in the CL framework and by blending representations of different views in the latent space.\nDuring training, ESLA conducts data augmentation and masking on input data to create two views. THE online encoder receives EMO and target encoder get EM. The \"mixer\" combines $latent1$ with $latent2$ and restoration masks to generate a \"mixed latent,\" which is then used to compute the reconstruction loss. ESLA's structure comprise of two Interconnected branches known as CL and MAE branch respectively. There are two kinds of encoders in CL branch including online and target encoder that generate distinct intermediate representations (IRs), $latent1$ and $latent2$. To decrease the computational power, an encoder incorporating a masking technique is used to encode two modified variants, referring to the same image. A contrast loss is generated by evaluating the $latent1$ and $latent2$ using predictor and projector. Rather than being trained with gradient descent, the target network uses an exact replica of online network, following it with a delayed using an EMA that is exponential moving average. In MAE branch, a feature map is generated by combining the information with the help of mixer that corresponds to orignal image. This feature map is unmasked and forwarded to the decoder for computing the reconstruction loss. In this context shallow data enhancement occur in pixel space $(D)$ that produces two distinct views of the same image while in the latent space $L$ deep data enhancement is executed that generate the \"mixed latent\", as an aspect of the decoder input.\nQuan et al. (2023) introduced a novel SSL model, based on the global contrast-masked autoencoder (GCMAE), which adeptly extracts both global and local features from pathological images. GCMAE leverages masking image reconstruction and contrastive learning as self-supervised pretext tasks, enabling the encoder to proficiently depict local-global features. This model comprises of an initializer, an encoder, a tile feature extractor, and a global feature extractor facilitating image reconstruction and contrastive learning tasks. In this investigation, an asymmetric encoder-decoder architecture is employed. ViT serves as the backbone for the encoder, pretrained using MAE. Its module related to patches feature extraction is comprised of eight transformer blocks, forming an asymmetric structure in conjunction with an encoder featuring of about 12 transformer blocks (ViT-base). This asymmetry facilitates decoupling of the encoder and decoder, fostering the"}, {"title": "4 Regularization Techniques", "content": "In basic terminology, regularization techniques helps in making model simpler. As Occam's razor principle state, simpler models usually work better, this means keeping things straightforward is key. By using different techniques, we narrow down the model's options to make it more focused. The integration of SSL methods with traditional regularization techniques offers a nuanced approach to improving the generalizability and robustness of computer vision models. This analysis explores how various SSL methods incorporate regularization principles within their frameworks.\nBootstrap Your Own Latent (BYOL) employs a teacher-student framework, where the student network predicts the output of a teacher network on augmented inputs. The teacher network is updated as a moving average of the student's parameters, ensuring learning consistency. This method mirrors the stabilization effect seen in regularization techniques like batch normalization and parameter sharing.\nMomentum Contrast (MoCo) leverages a momentum-based moving average of the query encoder, establishing a large and consistent dictionary. This approach acts as a dynamic regularization mechanism, ensuring stability in the representation of dictionary keys over time. This stability is vital for the extraction of stable and generalizable features, akin to how traditional regularization methods prevent overfitting by maintaining consistency in model parameters.\nSimCLR utilizes a straightforward contrastive learning framework, where its regularization is primarily through aggressive data augmentation techniques. These augmentations include random cropping, resizing, color distortion, and Gaussian blur, enabling the model to learn invariant and robust features. This is similar to how techniques like dropout and data augmentation in traditional settings prevent over-reliance on specific input features.\nDINO (Self-Distillation with No Labels) utilizes self-distillation for regularization, where the student network mimics the teacher network, updated as an exponential moving average of the student's parameters. This method is reminiscent of techniques like early stopping and weight decay, which aim to prevent overfitting by smoothing the learning trajectory.\nBarlow Twins method aims to align the cross-correlation matrix between the outputs of twin networks with the identity matrix. This alignment encourages the model to learn features invariant to input distortions, functioning as a regularization technique by discouraging the learning of redundant features.\nSwAV (Swapping Assignments between Views) regularization is unique, involving clustering approaches and consistency enforcement between cluster assignments for different views of the same image. This approach is akin to enforcing a constraint (regularization) on the feature space to enhance generalizability.\nTiCo (Time-Contrastive Consistent Learning of Visual Representations) focuses on learning temporally consistent representations, acting as a regularization mechanism by ensuring robustness to temporal variations in data. This is similar to entropy regularization, which encourages the model to learn more uncertain, hence generalizable, features.\nCMC (Contrastive Multiview Coding) regularizes learning by enforcing invariance to different views of the same scene. This invariance is a form of"}, {"title": "5 Evaluation Metrics, Benchmarks and loss functions", "content": "In this section, we will discuss the evaluation metrics and benchmarks used to evaluate the performance of SSL (SSL) methods in ViTs."}, {"title": "5.1 Performance Metrics for SSL in ViTs", "content": "The performance of SSL methods in ViTs is typically evaluated on downstream tasks, such as image classification, object detection, and semantic segmentation. The performance metrics used to evaluate these tasks include accuracy, precision, recall, F1 score, and mean average precision (mAP). These metrics provide a quantitative measure of the performance of the SSL method in comparison to other methods."}, {"title": "5.1.1 Accuracy", "content": "Accuracy is a measure that represents the true positives (both true negatives and true positives) of the under observation data. Accuracy and error rate are mutually exclusive and range between 0 to 100. Accuracy is computed by dividing the number of correct predictions by the total prediction number as:\n$Accuracy = \\frac{TruePositives + TrueNegatives}{TruePositives + TrueNegatives + FalsePositives + False Negatives}$"}, {"title": "5.1.2 Precision", "content": "Precision, also known as a measure of quality, quantifies the total number of correct positive predictions. More specifically, it is computed as the number of rightly predicted positive samples divided by the total number of positive examples that were predicted as:"}, {"title": "5.1.3 Recall", "content": "Recall is the total number of data samples that a learning network correctly predicts as belonging to the positive class as:"}, {"title": "5.1.4 F1 Score", "content": "F1 score is computed by taking harmonic mean of Precision and Recall as:\n$F1Score = \\frac{2 * Precision * Recall}{Precision + Recall}$\nIn simple form, it becomes:\n$F1Score = \\frac{True Positives}{TruePositives + \\frac{FalsePositives+FalseNegatives}{2}}$\nThe reason harmonic mean is used instead of geometric mean is to to balance the values for both Precision and Recall. The more the precision and recall scores deviate from each other, the worse the harmonic mean.\nNote that F1 Score is usually preferable when the classes are imabalanced in the dataset which is usually the case in real-world datasets."}, {"title": "5.2 Benchmark Datasets for SSL in ViTs", "content": "Several benchmark datasets have been used to evaluate the performance of SSL methods in ViTs. Some of them are discussed below."}, {"title": "5.2.1 ImageNet", "content": "The ImageNet classification dataset [49] contains roughly 1.2 million images for training, fifty thousand images for validation, and 100000 images for testing. Overall, the dataset is divided into one thousand diverse object classes including tablelamp, goldfish, radiator, screwdriver, and so on."}, {"title": "5.2.2 COCO", "content": "COCO (Common Objects in Context) [50] is a large-scale object detection, key-point detection, segmentation, and captioning dataset. The dataset contains over 330,000 images of which around 200,000 are fully labeled. The dataset is divided into 80 object classes and over 90 generic 'stuff' classes."}, {"title": "5.2.3 CIFAR-10", "content": "The CIFAR-10 a short for Canadian Institute For Advanced Research is a dataset [51] that contains sixty thousand images divided into 10 classes in which every class contains 6000 images. In total, 50000 images are utilized for training purpose while the rest of 10000 images are utilized for testing purpose. The dataset comprises of 10 classes, including horse, airplane, frog, automobile, cat, deer, bird, dog, ship, and truck."}, {"title": "5.2.4 CIFAR-100", "content": "CIFAR-100 dataset [51] is an extension of CIFAR-10. Although the total number of images are same on both datasetss, i.e., sixty thousand, CIFAR-100 is divided into 100 distinct classes with 600 images per class. Similar to CIFAR-10, a total of 50000 images are utilized for training and 10000 images are employed for testing. Every image contains two labels, a coarse label (general category) and a fine label (particular class). Some particular object classes include apple, bee, cattle, chair, rabbit, and wowan, whereas general categories include flowers, household_furniture, medium_mammals, people, etc."}, {"title": "5.3 loss functions", "content": "Machine learning algorithms must include loss functions to gauge how well a model is doing on the job at hand and acts as a feedback signal to adjust the model's parameters during training. By identifying and quantifying the discrepancy between expected output and the actual target labels, they play a critical part in directing the learning process. Over the past few years, contrastive learning has appeared as an effective approach in the realm of unsupervised representation learning, revolutionizing how neural networks can be trained without labeled data. This approach hinges on the idea of learning meaningful representations by increasing the similarity among positive pairs and decreasing the similarity of negative pairs.\nAmong the several contrastive loss functions, the InfoNCE loss (Noise Contrastive Estimation) stands as a common choice. It operates by introducing noise to the input data to create negative pairs, enabling the model to differentiate between real positive pairs and artificially generated negatives. Another widely used loss is the NT-Xent loss [18] (Normalized Temperature-Scaled Cross Entropy), which extends InfoNCE by incorporating a temperature parameter to scale the pairwise similarities. Through a softmax function, it computes the negative log-likelihood of positive pairs, enhancing the overall contrastive learning process. SimCLR loss (Simple Contrastive Learning) represents yet another approach, harnessing multiple augmentations of the"}, {"title": "5.4 Comparison with Supervised Learning Approaches", "content": "SSL, particularly in the context of ViTs, has completely changed the area of computer vision. Large volumes of unlabeled data can be used with these techniques to acquire meaningful representations, reducing the substantial reliance on labeled instances observed in conventional supervised learning. The main benefit of SSL is its capacity to take advantage of the wealth of unlabeled data that is readily available online [18], allowing models to learn from big datasets without the need for explicit annotations. SSL is especially appealing for real-world applications because of its scalability and affordability given that manual data labeling can be resource-intensive, costly, and require significant effort [5].\nThe comparative evaluation of SSL and supervised learning methods using the same downstream tasks and evaluation metrics has shed light on the effectiveness of SSL in ViTs. In situations where the annotated data is limited or challenging to gather, SSL has demonstrated its potential to achieve competitive or even superior performance. This is especially valuable for real-world scenarios where manually annotating data might not be feasible, yet high-performance computer vision models are needed. A comprehensive learning framework known as \"SSL\" employs pretext tasks derived solely from unsupervised data. These pretext tasks are designed to learn the meaningful visual representation in order to complete them effectively [18]. Whereas, in the supervised learning paradigm, each input sample is connected to its matching target or output label, and the system is trained on a labeled dataset. In order for the model to accurately predict outcomes on data that has not yet been"}, {"title": "6 Comparative Analysis of SSL Mechanisms", "content": "In this section, we will provide a comparative analysis of state-of-the-art SSL (SSL) mechanisms employing ViTs. Various SSL methods used different networks as their backbone networks. For a fair comparison, we compare different SSL methods employing ViT-B/16 [10] as their backbone network."}, {"title": "6.1 Performance Comparison", "content": "We compare SOTA SSL methods, in Table 4, where self-supervised pre-training is performed over ImageNet-1K [49] training dataset. Later, fully supervised training is performed to evaluate the feature representations using (i) linear probing and (ii) end-to-end fine-tuning. The inputs are resized to 224x224 crops and report top-1 validation accuracy. From Table 4, we notice that DINO [29], MoCo-V3 [56], MSG-MAE [57], iBOT [58], and EsViT [59] achieve more than 75% top-1 accuracy, in terms of fine-tuning using linear probing. It is notable that the top-performing EsViT [59] employed Swin-B [60] architecture as the backbone network. In contrast, almost all the methods obtain over 82.0% in terms of end-to-end fine-tuning. However, MSG-MAE [57], TEC [61], and dBOT [62] present the top-1 accuracy of 85.3%, 84.7%, and 84.5%, respectively."}, {"title": "6.2 Performance evaluation over downstream tasks", "content": "We further assess the effectiveness of self-supervised methods for downstream tasks such as object detection, semantic segmentation, and instance segmentation in Table 5. We also demonstrate the generalizability capabilities of visual representations learned by self-supervised methods by fine-tuning the pre-trained models on smaller datasets in Table 6."}, {"title": "6.2.1 Object detection and instance segmentation", "content": "We present a comparative analysis of SOTA self-supervised approaches for downstream tasks such as object detection and instance segmentation (as shown in Table 5. For a fair comparison, we compare methods having ViT-B [10] as the backbone architecture. The methods are trained over COCO [50] dataset considering Mask R-CNN [72] as the task head for object detection and instance segmentation and evaluated using average precision $AP_{box}$ and $Ap_{mask}$, respectively. The results in Table 5 demonstrate that TEC [61], dBOT [62], and MSG-MAE [57] outperform other SOTA SSL methods over object detection and obtain $AP_{box}$ scores of 52.3, 52.7, and 52.3 respectively. On the instance segmentation, MSG-MAE [57], \u0422\u0415\u0421 [61], and dBOT [62] show significant promising performance compared to other SSL methods and achieve 48.8, 47.2, and 45.7 $AP_{mask}$ scores, respectively."}, {"title": "6.2.2 Semantic Segmentation Comparison", "content": "In Table 5, we present a performance comparison of SSL methods for semantic segmentation, where UpperNet [73] is trained with ViT-B [10] over ADE20K [74] dataset. From Table 5, it is notable that BootMAE [65], Diet-III [66], Diet-III+AugMask [67], MSG-MAE [57], TEC [61] and dBOT [62] exhibit above 49.0% mIoU. We also observe that top performing Diet-III+AugMask [67], \u0422\u0415\u0421 [61], Diet-III [66], and MSG-MAE [57], demonstate the 50.2%, 49.9%, 49.7%, and 49.7%, respectively."}, {"title": "6.2.3 Transfer Classification Comparison", "content": "To further study the generalizability of the SSL methods, we compare their transfer learning capabilities in terms of accuracy. To do so, the pre-trained methods are fine-tuned over small datasets such as CIFAR-10 [51], CIFAR-100 [51], iNaturalist18 [76] (iNa18), iNaturalist19 [76] (iNa19), Flowers [77], and Cars [78]. The results in Table 6 Deit-III [66] demonstrate outstanding performance over state-of-the-art SSL methods and achieve 99.3 and 92.5 accuracy over CIFAR-10 and CIFAR-100 datasets, respectively. We also note that dBOT [62] illustrates better performance over CIFAR-10, iNa18, and iNa19 datasets, and show accuracy of 99.3, 77.9, and 81.0, respectively. Similarly, iBOT [58] exhibits the best performance over Flowers and Cars datasets while obtaining 98.9 and 94.3 accuracy, respectively."}, {"title": "6.3 Robustness to Perturbations", "content": "We also present a comparative comparison of SSL methods over various robustness datasets including natural adversarial examples [79] (IN-A), objects in different styles and textures (IN-R [80]), controls in rotation, background, and viewpoints (ObjNet [81]), and SI-scores [82] (SI-size, SI-loc, and SI-rot). The results in Table 7 show that Diet-III+AugMask [67] has better robustness across various robustness metrics."}, {"title": "7 Advancements and Open Challenges", "content": "In this section, we will discuss the recent advancements and open challenges in SSL (SSL) for ViTs."}, {"title": "7.1 Data Augmentation as supportive technique", "content": "Data augmentation is a crucial aspect of SSL for ViTs. Data augmentation strategies can assist the SSL models to achieve more robustness and generalizability representations from unlabeled data. Recent advancements in the field including generative models to generate augmented data, incorporating adversarial perturbations, and unsupervised clustering to generate diverse augmentations, to enhance learning capability of the model. However, there is still a need for more effective and productive data augmentation techniques which can help to improve the effectiveness of SSL methods. In supervised and SSL environments in ViTs, data scarcity is one of the challenges for convolutional neural networks (CNN) models"}, {"title": "7.1.1 Traditional Aug"}]}