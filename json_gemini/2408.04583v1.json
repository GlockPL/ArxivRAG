{"title": "Unveiling the Power of Sparse Neural Networks for Feature Selection", "authors": ["Zahra Atashgahi", "Tennison Liu", "Mykola Pechenizkiy", "Raymond Veldhuis", "Decebal Constantin Mocanu", "Mihaela van der Schaar"], "abstract": "Sparse Neural Networks (SNNs) have emerged as powerful tools\nfor efficient feature selection. Leveraging the dynamic sparse train-\ning (DST) algorithms within SNNs has demonstrated promising fea-\nture selection capabilities while drastically reducing computational\noverheads. Despite these advancements, several critical aspects re-\nmain insufficiently explored for feature selection. Questions persist\nregarding the choice of the DST algorithm for network training,\nthe choice of metric for ranking features/neurons, and the compar-\native performance of these methods across diverse datasets when\ncompared to dense networks. This paper addresses these gaps by\npresenting a comprehensive systematic analysis of feature selection\nwith sparse neural networks. Moreover, we introduce a novel met-\nric considering sparse neural network characteristics, which is de-\nsigned to quantify feature importance within the context of SNNs.\nOur findings show that feature selection with SNNs trained with\nDST algorithms can achieve, on average, more than 50% memory\nand 55% FLOPs reduction compared to the dense networks, while\noutperforming them in terms of the quality of the selected features.\nOur code and the supplementary material are available on GitHub\n(https://github.com/zahraatashgahi/Neuron-Attribution).", "sections": [{"title": "Introduction", "content": "With the ever-increasing generation of big data, high-dimensional\ndata has become ubiquitous in various fields such as health care,\nbioinformatics, and social media. Yet, high dimensionality poses\nsubstantial challenges for the analysis and interpretation of data\nsuch as, curse of dimensionality, overfitting, and high memory and\ncomputation demands [29].\nFeature selection emerges as a pivotal approach to address the\nchallenges raised by high-dimensional data. It selects the most\nrelevant attributes of the data for the final learning task [10]. Feature\nselection can reduce the computational, memory, and as a result\nenergy costs, increase interpretability, decrease data collection costs,\nand potentially enhance the model generalization.\nLately, neural networks have emerged as a powerful tool for feature\nselection. Deep learning methods gain increasing attention in various\ntasks due to their intrinsic attributes: automatic feature engineering,\nlearning from data streams, facilitation of multi-source learning\n(multi-modal/multi-view learning), parameters pre-training, and the\nfeasibility of an end-to-end data processing paradigm. This made\nthem attractive for learning feature representations and AutoML\n[2, 43, 14, 8, 20]. One intriguing advantage of neural network feature\nselection compared to most traditional approaches is the ability to\ncapture complex non-linear dependencies among features. Notably,\nneural network-based feature selection has exhibited substantial\nsuccess, often outperforming conventional feature selection methods\nin identifying more relevant features for the final prediction tasks\n[6, 23, 28, 47, 26, 40]. Inspired by Lasso [45], a subset of neural\nnetwork-based feature selection methods introduces sparsity within\nnetworks to perform feature selection. [47] introduces sparsity in the\ninput features of a neural network via stochastic gates. [28] sparsifies\nthe network by selecting the relevant input features for the entire\nnetwork to perform global feature selection.\nHowever, a main challenge with the existing neural network\nfeature selection method is the high computational cost due to their\nlarge over-parameterized networks, particularly when applied to\nhigh-dimensional data [4]. This makes the deployment of such models\ninfeasible in low-resource environments, e.g., mobile phones. In\naddition, in extreme cases, training and deploying over parameterized\ndeep learning models, significantly raise energy consumption in data\ncenters, resulting in high carbon emissions exceeding a human's\nannual carbon footprint [44].\nTherefore, another group of works exploits the characteristics of\na sparse neural network (SNN) [21] trained with dynamic sparse\ntraining (DST) [32] to find the most important attributes of a dataset.\nThis line of works first initiated by [4] and followed by [42, 5] has\ndemonstrated competitive feature selection capabilities, comparable\nto state-of-the-art algorithms, all while maintaining computational\nefficiency. Unlike the methods that regularize weights to achieve\nsparsity, the latter group of works exploits hard sparsity (zero-out\nweights). Moreover, they exploit dynamic sparse training to train the\nnetwork sparsely from scratch to be efficient during the entire training\nprocess. Besides, they deploy sparsity in all layers and not only the\ninitial layer; therefore, they are much more computationally efficient.\nDespite showcasing superior feature selection performance coupled\nwith increased efficiency, certain ambiguities persist concerning fea-\nture selection with sparse neural networks. Is sparsity beneficial for all\ndatasets when performing feature selection? What metric to choose for"}, {"title": "Backgound & Related Work", "content": "Methods to perform feature selection are divided into three main\ncategories: Filter, wrapper, and Embedded methods. Filter Methods\n[16, 19, 10] exploit variable scoring techniques to rank the features.\nThey are model-agnostic and do not rely on the learning algorithm.\nAs the ranking is done before classification, filter methods are prone\nto selecting irrelevant features. Wrapper Methods [49, 27, 30] aim\nto find a subset of the feature that achieves the highest classification\nperformance. To tackle the NP-hard problem of evaluating numer-\nous subsets, they employ search algorithms to find the best subset.\nHowever, due to the multiple rounds of training, these methods are\ncostly in terms of computation. Embedded Methods integrate feature\nselection into the learning process thus being able to select relevant\nfeatures while being costly-efficient. Various techniques are employed\nto perform embedded feature selection including, mutual information\n[7, 36], the SVM classifier [17], and neural networks [39].\nNeural network-based feature selection has gained significant at-\ntention in recent years, both in supervised [31, 28, 47, 46, 26, 12, 37]\nand unsupervised [6, 18, 9, 11] settings. These methods leverage\nthe advantages of neural networks in capturing non-linear dependen-\ncies and performing well on large datasets. However, many existing\nneural network-based feature selection methods suffer from over-\nparameterization, resulting in high computational costs, especially for\nhigh-dimensional datasets. To address these issues, a new category of\nmethods exploits sparse neural networks to perform efficient feature\nselection [4, 42, 5]. Detailed discussion on SNNs for feature selection\ncan be found in Section 2.3.2.\nDepending on label availability, feature selection can be carried out\nin a supervised or unsupervised manner. This study focuses on ad-\ndressing the supervised feature selection challenge. Given a dataset\nX comprising m samples denoted as (x(i), y(i)), where x(i) \u2208 Rd"}, {"title": "2.2 Neuron Attribution", "content": "In this work, we propose a new feature selection metric from neural\nnetworks that is based on neuron attribution. Attribution methods\naim to explain the predictions of a neural network. These methods\nhave also been used to explain the predictions of a model[1]. Many\nworks have regularized network attribution to achieve the desired\noutput and improve the learning of neural networks [38, 15, 34, 25].\nGiven a neural network, an attribution method aims to determine the\ncontribution/relevancy of each input feature to the output of i th\noutput neuron (f\u2212\u00b9) [1]. Usually, the gradient information is used to\nmeasure the neuron attribution. Neuron attribution for the ith neuron\nin the output layer w.r.t. the feature xj can be simply defined as:\n$\\bar{a}^{L-1}_{ij}(x) = \\frac{\\partial f_i(x)}{\\partial x_j}$,\nwhere L is the number of network layers and L - 1 is the index\nof the output layer. The higher the absolute value of $\\bar{a}^{L-1}_{ij}(x)$ is, the\nmore sensitive output neuron i is to the changes in the input feature\nxj for observation x. Other gradient attribution methods can be used\nto measure the neuron attribution in Equation 2 [1]. An example of\nneuron attribution attribution is visualized in Figure 1."}, {"title": "Sparse Neural Networks", "content": "Sparse neural networks are an approach to address the overparam-\neterization of deep neural networks. By introducing sparsity in the\nconnectivity and/or the units of a dense neural network, they aim to\nmatch the performance of dense neural networks while reducing com-\nputational and memory costs [21, 33]. Due to the reduction of learned\nnoise, they can even improve the generalization of the network [21].\nA sparse neural network is represented by f (x, \u03b8s) that has a sparsity\nlevel of P, where P is calculated as 1 - $||\\theta_s|| / ||\\theta||$. Here, \u03b8s denotes a\nsubset of parameters of the dense network parameterized by \u03b8, and\n$||\u03b8s||_0$ and $||\u03b8||_0$ refer to the number of parameters of the sparse and\ndense network respectively."}, {"title": "Dynamic Sparse Training (DST)", "content": "DST comprises a range of techniques to train sparse neural networks\nfrom scratch. The goal of DST methods is to optimize the sparse con-\nnectivity of the network during training, without resorting to dense\nnetwork matrices at any point [33, 32, 48]. To achieve this, DST meth-\nods begin with initializing a random sparse neural network. During\ntraining, DST methods periodically update the sparse connectivity\nof the network by removing a fraction of the parameters \u03b8s and\nadding the same number of parameters to the network to keep the\nsparsity level fixed. In the literature, usually, weight magnitude has\nbeen used as a criterion for dropping the connections. However, there\nexist various approaches for weight regrowth including, random [32],\ngradient [13, 24], and neuron similarity [3]."}, {"title": "DST for Feature Selection", "content": "QuickSelection was the first work to show that sparse neural networks\ntrained with DST can perform feature selection. It proposed the neu-\nron strength [4] in a sparse denoising autoencoder to determine the\nimportance of each input neuron in the input layer of a sparse neural\nnetwork (and the corresponding input feature in the original dataset)\nin the unsupervised learning settings. Neuron strength is computed\nas:\n$S(X_i) = \\sum_{i=0}^{n_h^1-1} |W_{ij}^1|$,\nwhere $n_h^1$ is the number of hidden neurons in the first hidden layer,\nand $W^1$ is the weight matrix of the lth layer.\nLater on, [42], proposed to use a combination of neuron strength\nand gradient of loss to the output neurons in an autoencoder to measure\nthe importance of neurons. [5] proposed to perform input neuron\nupdating in a sparse MLP trained with DST to gradually reduce the\nnumber of input neurons and then use neuron strength to rank the\nfeatures.\nIn this paper, we study the efficacy of SNNs for feature selection\nand propose a new importance metric based on neuron attribution to\nmeasure the importance of input features in neural networks. Table\n1, compares our proposed method with the closest related work that\nexploit sparsity to perform feature selection. Figure 2 shows a toy\nexample of feature selection using sparse neural networks."}, {"title": "Methodology", "content": "The contributions of this research are twofold. Firstly, we provide an\nextensive analysis of the performance of SNNs trained with DST for\nfeature selection. Secondly, we propose a new metric to derive the\nimportance of features in an SNN trained with DST. We elaborated\non the experimental analysis settings and findings in Section 4. In this\nsection, we describe our proposed approach to measure neuron/feature\nimportance based on neuron attribution.\nAs elucidated in Section 2.2, neuron attribution\nserves as a metric to estimate the relevance of an input feature to\na specific output neuron, given an input sample. Consequently, the\nneuron attribution vector for a hidden neuron provides insights into\nwhich set of features is more relevant to derive the output. Looking\nat the contribution vectors of all output neurons, we can select the\nmost relevant input features for each output feature. Thus, neuron\nattribution enables us to rank input features based on their relevance\nto the output features. Features with the highest contributions in\nthe output neurons offer a robust estimate of the output, implicitly\nguiding us toward identifying the optimal feature set, as represented\nin Equation 1. Building on this premise, we introduce the neuron\nattribution feature selection metric in the context of neural networks."}, {"title": "Input Neuron Importance", "content": "The importance of the input neurons is computed based on the\nneuron attribution of the output neurons. We propose to compute the\nimportance score of each input neuron as the following:"}, {"title": "Results", "content": "In this section, we first describe the experimental settings (4.1). Then,\nwe present the results of the feature selection comparison among\nsparse and dense models in Section 4.2, and among standard feature\nselection baselines using sparsity in Section 4.4. Finally, we visualize\nthe neuron importance in Section 4.3. Additionally, we perform an\nanalysis on a synthetic dataset in Appendix C."}, {"title": "Experimental Setup", "content": "In the following, we describe datasets, baselines, and the evaluation\napproach. More experimental details, including the hyperparameter\nsettings and model architecture, can be found in Appendix A."}, {"title": "Datasets", "content": "The datasets, outlined in Table 2, include a diverse collection of\n18 datasets, varying in size and type. This selection allows for a\ncomprehensive analysis of each method across different domains.\nMore than half of these datasets are high-dimensional making them\nchallenging benchmarks for the models."}, {"title": "Baselines", "content": "DST for Feature Selection. The main focus of this paper is to ana-\nlyze how sparse neural networks trained with DST perform in feature\nselection compared to dense networks. Therefore, we consider dense\nand sparse baselines. Secondly, for the sparse models, we want to\nstudy how the DST algorithm for training the sparse neural network\naffects the feature selection performance. We consider two standard\nDST approaches in the literature, SET [32] and RigL [13], that are\nfrequently used in evaluating the DST framework. We describe SET\nand RigL in Appendix A.2. Finally, we want to assess the efficacy of\nthe neuron importance metric. We consider the neuron strength metric\nfrom QuickSelection [4], which is also used as a part of neuron im-\nportance estimation in [42] or directly used in [5] to rank the features\nin a supervised setting; we call this metric as \"QS\" in the experiments.\nWe compare the neuron strength metric with our proposed neuron at-\ntribution metric, called \"Attr\". These metrics combined with the three\nconsidered models, dense network (Dense), SNN trained with SET\n(SET), and SNN trained with RigL (RigL), result in 6 baselines for\nthe experiments: Dense-QS, Dense-Attr, SET-QS, SET-Attr, RigL-QS,\nRigL-Attr. For QuickSelection, as we also do for Attr, to compute the\ninput neuron/feature importance, we sum the importance during all\ntraining epochs. As shown in Appendix E, we observed that looking\nat the history of importance during all training epochs improves the\nresults for these models.\nWe consider three feature selection\nmethods that exploit sparsity to perform feature selection including\nSTG [47], LassoNet [28], and Lasso [45]."}, {"title": "Evaluation", "content": "For evaluating each method, we first perform feature selection to\nderive K most important features (K is a hyperparameter set by the\nuser). Then, we train an SVM classifier on the subset of K features of\nthe original data and report the test accuracy on a hold-out test set."}, {"title": "Feature Selection Comparison", "content": "Settings. In this experiment, we compare the two feature ranking\ncriteria, neuron strength from QuickSelection (QS), and neuron attri-\nbution (Attr). We consider dense and sparse MLPs to perform feature\nselection on. The sparse models are trained with DST; we consider\nSET and RigL as the training algorithms. The comparison results\nare summarized in Tables 3. For ease of comparison, we considered\npairwise comparison. However, comparisons of all baselines referred\nto in Section 4.1.2, are brought in Appendix D. Additionally, we\nanalyze the performance of each method across various K values in\n{25, 50, 75, 100, 200} in terms of average ranking in Appendix B.\nDense vs Sparse. Comparing dense and sparse models, we consider\nDense-Attr and SET-Attr. In Table 3a, we can observe that while on\nhigh-dimensional biological datasets and the noisy dataset (Madelon)\nDense-Attr performs better, on the rest of the datasets (11 out of 18\ndatasets) SET-Attr excels the dense model. Additionally, SET-Attr\nexploits significantly fewer parameters (memory) and FLOPs (com-\nputational costs). As shown in Table 3, on average over all datasets,\nSET achieves 54.2% sparsity and RigL achieves 66.4% sparsity. In\nTable 5, we present the FLOPs (Floating-Point Operations) count for"}, {"title": "Neuron Importance Visualization", "content": "Settings. In this section, we visualize the neuron importance on\nthe MNIST dataset for each method in Table 3. We plot the neuron\nimportance of input features as a 2D heatmap in Figure 4.\nAs we can observe in the first row of Figure 4, the neuron\nstrength metric in QuickSelection is mostly similar for all three mod-\nels, showing that they all can detect the most important features which\nmostly appear in the center of the image as we see in the MNIST\ndataset. The pattern formed by the neuron attribution metric in the\nsecond row of Figure 4, is close to that of neuron strength, where\nimportant features appear in the center of the image. However, the\nmain difference is that neuron attribution reaches a more sparse fea-\nture importance pattern. This shows that neuron attribution focuses\nmostly on the most important features."}, {"title": "Comparsion with Baselines", "content": "Settings. We compare SET-Attr (achieving the highest ranking among\nthe considered methods when K = 100) with three popular feature\nselection methods in the literature that exploit sparsity to perform\nfeature selection: Lasso, STG [47], and LassoNet [28]. We compare"}, {"title": "Analysis of Effects of Sparsity for Feature Selection", "content": "Accuracy vs FLOPs Tradeoff. To summarize the results of Tables\n4 and 5, the tradeoff between accuracy and FLOPs is shown in Fig-\nure 5. As we can observe, SET-Attr has in most cases in order of\nmagnitude lower number of FLOPs than LassoNet while achieving\ncompetitive performance. STG and Dense-Attr fall behind in terms of\nboth accuracy and FLOPs count in most cases considered.\nIn Section 4.2, we optimize the sparsity level, selecting the value\nyielding minimal loss on the validation set. However, our focus\nnow shifts to a comprehensive analysis of the impact of sparsity on\nfeature selection performance, exploring a range of sparsity levels\n(i.e., 0.25, 0.50, 0.75, 0.90, 0.95, 0.98) for each dataset. The results\nare visualized in Figure 6.\nStudying image datasets (Coil-20, ORL, Yale), we observe that\nsparsity levels up to 80% generally enhance or maintain performance,\nwith notable improvement on the Yale dataset. Beyond 80%, the\nfeature selection method with SET (using any of the two importance\nmetrics), remains stable or even exhibits improvement, whereas RigL\nexperiences degradation in the high sparsity regime.\nFor hand-written digits datasets (MNIST, USPS, Gisette), sparsity\nconsistently leads to performance improvement. Notably, the neuron\nattribution metric frequently outperforms neuron strength. At very\nhigh sparsity (98%), feature selection with SET demonstrates\nsignificant performance gains, while RigL's performance either"}, {"title": "Conclusions", "content": "In conclusion, our work contributes significantly to the understand-\ning of feature selection with sparse neural networks within the dy-\nnamic sparse training framework. Through systematic analysis, we\ndemonstrate the efficacy of sparse neural networks in feature selection\nfrom diverse datasets, comparing them against dense neural networks\nand other sparsity-inducing methods. Our findings reveal that SNNs\ntrained with DST algorithms achieve remarkable memory and com-\nputational savings, exceeding 50% and 55% respectively compared\nto dense networks, while maintaining superior feature selection qual-\nity in 13 out of 18 cases. One promising direction to continue this\nresearch is to consider neuron importance metrics to improve the\ntraining of sparse neural networks in the DST framework to guide the\nweight addition process."}, {"title": "Experimental Setup", "content": "The architecture used in the experiments is an MLP with two hidden layers with 1000 and 100 hidden neurons in each layer, respectively. The\nhidden layer activation function is ReLU, and softmax is used for the output layer. We used a learning rate of 0.001 for all datasets. The L2\nregularization term and sparsity level are optimized based on the validation loss in [5e-5, 0.0001, 0.001] and [0.25, 0.5, 0.80, 0.9, 0.95, 0.98],\nrespectively. ( is set to 0.3. All datasets have been scaled to have zero mean and unit variance. The batch size for the datasets with less than\n200 samples is set to 32 and for the rest of the datasets is set to 100. Adam optimizer is used for training the model. The maximum number of\ntraining epochs is 200; if the validation loss does not improve within 50 epochs, the training will end. The datasets are split into train, validation,\nand test sets with a split ratio [65%, 15%, 20%]. The code is implemented in Python and using the PyTorch library [35]. The start of the code is\nGraNet\u00b9 and TANGOS2."}, {"title": "DST Algorithms", "content": "In Section 2.3.1, we explain the dynamic sparse training framework. In the experiments, we consider two dynamic sparse training algorithms:\nSET and RigL. These two algorithms are among the commonly used DST approaches in the literature to study the DST framework.\n\u2022 SET. Sparse Evolutionary Training (SET) [32] is a pioneering approach that introduced the Dynamic Sparse Training framework. This\nmethod starts with initializing a sparse network with the desired sparsity level. Then, at each epoch or every few iterations, it updates the\nconnectivity of the sparse neural network by dropping a fraction ( of weight with the smallest magnitude and adding the same number of\nrandom weights back to the network. This dynamic process serves as a means to continuously refine and update the network's topology, to\nevolve its structure over time.\n\u2022 RigL. The Rigged Lottery (RigL) [13] is another dynamic sparse training that evolves the topology of the sparse neural network by\ndynamically updating the connectivity. However, the difference compared to SET is that RigL adds the new weight based on the gradient\ninformation. It adds the non-existing weights having a large gradient magnitude to the network."}, {"title": "Performance for Various K Values", "content": "While the results of the experiments in Section 4.2 are for when we select K = 100 features, we measure the performance when selecting\n\u039a\u2208 {25, 50, 75, 100, 200}. To summarize the performance of each method when selecting different numbers of K, we compute the average\nranking score. To compute this score, in each experiment (each dataset and value of K), we rank the methods in terms of accuracy and give a\nscore depending on their rank, where the best-performing method receives a score of 6 and the worst-performing method receives 1. Then, we\naverage these scores for all the experiments for each method. The average ranking score for each K value is presented in Figure 7."}, {"title": "Experiment on Synthetic Dataset", "content": "To evaluate the performance of the methods in a controlled environment, we design an experiment on an artificial dataset. We generate an\nartificial dataset with 200 features where only 100 of these features are informative and the rest of the features are noise. The task is a binary\nclassification. We vary the number of samples in {100, 500, 1000, 10000}. We plot the fraction of the relevant features that each method can\nfind (coverage) in Figure 8."}, {"title": "Feature Selection Comparison", "content": "The overall comparison among the methods in shown in Table 6. In addition, more pairwise comparison (as shown in Table 3), are brought in\nTable 7."}, {"title": "Feature Importance Metric Analysis", "content": "In the experiments in the paper, we calculate the neuron importance based on the summation of the importance within all training epochs.\nHowever, we considered two other ways to compute the importance: the summation of importance within last training epoch, and the importance\nin the last iteration. The results of these two approaches are summarized in Tables 8 and 9. As can be seen from this Table, considering the\nimportance within all training epochs, as done in Section 4.2 leads to better results. This shows that the history of the importance, even when\nmeasured at the first epochs is also beneficial to the performance."}]}