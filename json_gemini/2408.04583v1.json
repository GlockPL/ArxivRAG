{"title": "Unveiling the Power of Sparse Neural Networks for Feature Selection", "authors": ["Zahra Atashgahi", "Tennison Liu", "Mykola Pechenizkiy", "Raymond Veldhuis", "Decebal Constantin Mocanu", "Mihaela van der Schaar"], "abstract": "Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient feature selection. Leveraging the dynamic sparse training (DST) algorithms within SNNs has demonstrated promising feature selection capabilities while drastically reducing computational overheads. Despite these advancements, several critical aspects remain insufficiently explored for feature selection. Questions persist regarding the choice of the DST algorithm for network training, the choice of metric for ranking features/neurons, and the comparative performance of these methods across diverse datasets when compared to dense networks. This paper addresses these gaps by presenting a comprehensive systematic analysis of feature selection with sparse neural networks. Moreover, we introduce a novel metric considering sparse neural network characteristics, which is designed to quantify feature importance within the context of SNNs. Our findings show that feature selection with SNNs trained with DST algorithms can achieve, on average, more than 50% memory and 55% FLOPs reduction compared to the dense networks, while outperforming them in terms of the quality of the selected features.", "sections": [{"title": "Introduction", "content": "With the ever-increasing generation of big data, high-dimensional data has become ubiquitous in various fields such as health care, bioinformatics, and social media. Yet, high dimensionality poses substantial challenges for the analysis and interpretation of data such as, curse of dimensionality, overfitting, and high memory and computation demands [29].\nFeature selection emerges as a pivotal approach to address the challenges raised by high-dimensional data. It selects the most relevant attributes of the data for the final learning task [10]. Feature selection can reduce the computational, memory, and as a result energy costs, increase interpretability, decrease data collection costs, and potentially enhance the model generalization.\nLately, neural networks have emerged as a powerful tool for feature selection. Deep learning methods gain increasing attention in various tasks due to their intrinsic attributes: automatic feature engineering, learning from data streams, facilitation of multi-source learning (multi-modal/multi-view learning), parameters pre-training, and the feasibility of an end-to-end data processing paradigm. This made them attractive for learning feature representations and AutoML [2, 43, 14, 8, 20]. One intriguing advantage of neural network feature selection compared to most traditional approaches is the ability to capture complex non-linear dependencies among features. Notably, neural network-based feature selection has exhibited substantial success, often outperforming conventional feature selection methods in identifying more relevant features for the final prediction tasks [6, 23, 28, 47, 26, 40]. Inspired by Lasso [45], a subset of neural network-based feature selection methods introduces sparsity within networks to perform feature selection. [47] introduces sparsity in the input features of a neural network via stochastic gates. [28] sparsifies the network by selecting the relevant input features for the entire network to perform global feature selection.\nHowever, a main challenge with the existing neural network feature selection method is the high computational cost due to their large over-parameterized networks, particularly when applied to high-dimensional data [4]. This makes the deployment of such models infeasible in low-resource environments, e.g., mobile phones. In addition, in extreme cases, training and deploying over parameterized deep learning models, significantly raise energy consumption in data centers, resulting in high carbon emissions exceeding a human's annual carbon footprint [44].\nTherefore, another group of works exploits the characteristics of a sparse neural network (SNN) [21] trained with dynamic sparse training (DST) [32] to find the most important attributes of a dataset. This line of works first initiated by [4] and followed by [42, 5] has demonstrated competitive feature selection capabilities, comparable to state-of-the-art algorithms, all while maintaining computational efficiency. Unlike the methods that regularize weights to achieve sparsity, the latter group of works exploits hard sparsity (zero-out weights). Moreover, they exploit dynamic sparse training to train the network sparsely from scratch to be efficient during the entire training process. Besides, they deploy sparsity in all layers and not only the initial layer; therefore, they are much more computationally efficient.\nDespite showcasing superior feature selection performance coupled with increased efficiency, certain ambiguities persist concerning feature selection with sparse neural networks. Is sparsity beneficial for all datasets when performing feature selection? What metric to choose for"}, {"title": "Backgound & Related Work", "content": "Methods to perform feature selection are divided into three main categories: Filter, wrapper, and Embedded methods. Filter Methods [16, 19, 10] exploit variable scoring techniques to rank the features. They are model-agnostic and do not rely on the learning algorithm. As the ranking is done before classification, filter methods are prone to selecting irrelevant features. Wrapper Methods [49, 27, 30] aim to find a subset of the feature that achieves the highest classification performance. To tackle the NP-hard problem of evaluating numerous subsets, they employ search algorithms to find the best subset. However, due to the multiple rounds of training, these methods are costly in terms of computation. Embedded Methods integrate feature selection into the learning process thus being able to select relevant features while being costly-efficient. Various techniques are employed to perform embedded feature selection including, mutual information [7, 36], the SVM classifier [17], and neural networks [39].\nNeural network-based feature selection has gained significant attention in recent years, both in supervised [31, 28, 47, 46, 26, 12, 37] and unsupervised [6, 18, 9, 11] settings. These methods leverage the advantages of neural networks in capturing non-linear dependencies and performing well on large datasets. However, many existing neural network-based feature selection methods suffer from over-parameterization, resulting in high computational costs, especially for high-dimensional datasets. To address these issues, a new category of methods exploits sparse neural networks to perform efficient feature selection [4, 42, 5]. Detailed discussion on SNNs for feature selection can be found in Section 2.3.2.\nDepending on label availability, feature selection can be carried out in a supervised or unsupervised manner. This study focuses on addressing the supervised feature selection challenge. Given a dataset $X$ comprising m samples denoted as $(x^{(i)}, y^{(i)})$, where $x^{(i)} \\in R^d$"}, {"title": "Neuron Attribution", "content": "In this work, we propose a new feature selection metric from neural networks that is based on neuron attribution. Attribution methods aim to explain the predictions of a neural network. These methods have also been used to explain the predictions of a model[1]. Many works have regularized network attribution to achieve the desired output and improve the learning of neural networks [38, 15, 34, 25].\nGiven a neural network, an attribution method aims to determine the contribution/relevancy of each input feature to the output of i th output neuron $(f^{-1})^{(i)}$[1]. Usually, the gradient information is used to measure the neuron attribution. Neuron attribution for the ith neuron in the output layer w.r.t. the feature $x_j$ can be simply defined as:\n$\\alpha_{ij}^{L-1}(x) = \\frac{\\partial f^{(i)}(x)}{\\partial x_j}$ (2)\nwhere L is the number of network layers and $L - 1$ is the index of the output layer. The higher the absolute value of $\\alpha_{ij}^{L-1}(x)$ is, the more sensitive output neuron i is to the changes in the input feature $x_j$ for observation x. Other gradient attribution methods can be used to measure the neuron attribution in Equation 2 [1]."}, {"title": "Sparse Neural Networks", "content": "Sparse neural networks are an approach to address the overparameterization of deep neural networks. By introducing sparsity in the connectivity and/or the units of a dense neural network, they aim to match the performance of dense neural networks while reducing computational and memory costs [21, 33]. Due to the reduction of learned noise, they can even improve the generalization of the network [21].\nA sparse neural network is represented by $f(x, \\theta_s)$ that has a sparsity level of $P$, where P is calculated as $1 - \\frac{||\\theta_s||_0}{||\\theta||_0}$. Here, $\\theta_s$ denotes a subset of parameters of the dense network parameterized by $\\theta$, and $||\\theta_s||_0$ and $|\\theta||_0$ refer to the number of parameters of the sparse and dense network respectively."}, {"title": "Dynamic Sparse Training (DST)", "content": "DST comprises a range of techniques to train sparse neural networks from scratch. The goal of DST methods is to optimize the sparse connectivity of the network during training, without resorting to dense network matrices at any point [33, 32, 48]. To achieve this, DST methods begin with initializing a random sparse neural network. During training, DST methods periodically update the sparse connectivity of the network by removing a fraction of the parameters $\\Theta_s$ and adding the same number of parameters to the network to keep the sparsity level fixed. In the literature, usually, weight magnitude has been used as a criterion for dropping the connections. However, there exist various approaches for weight regrowth including, random [32], gradient [13, 24], and neuron similarity [3]."}, {"title": "DST for Feature Selection", "content": "QuickSelection was the first work to show that sparse neural networks trained with DST can perform feature selection. It proposed the neuron strength [4] in a sparse denoising autoencoder to determine the importance of each input neuron in the input layer of a sparse neural network (and the corresponding input feature in the original dataset) in the unsupervised learning settings. Neuron strength is computed as:\n$S(X_j) = \\sum_{i=0}^{n_h^1-1}|W_{ji}^1|$, (3)\nwhere $n_h^1$ is the number of hidden neurons in the first hidden layer, and $W^1$ is the weight matrix of the lth layer.\nLater on, [42], proposed to use a combination of neuron strength and gradient of loss to the output neurons in an autoencoder to measure the importance of neurons. [5] proposed to perform input neuron updating in a sparse MLP trained with DST to gradually reduce the number of input neurons and then use neuron strength to rank the features.\nIn this paper, we study the efficacy of SNNs for feature selection and propose a new importance metric based on neuron attribution to measure the importance of input features in neural networks."}, {"title": "Methodology", "content": "The contributions of this research are twofold. Firstly, we provide an extensive analysis of the performance of SNNs trained with DST for feature selection. Secondly, we propose a new metric to derive the importance of features in an SNN trained with DST. We elaborated on the experimental analysis settings and findings in Section 4. In this section, we describe our proposed approach to measure neuron/feature importance based on neuron attribution.\nAs elucidated in Section 2.2, neuron attribution serves as a metric to estimate the relevance of an input feature to a specific output neuron, given an input sample. Consequently, the neuron attribution vector for a hidden neuron provides insights into which set of features is more relevant to derive the output. Looking at the contribution vectors of all output neurons, we can select the most relevant input features for each output feature. Thus, neuron attribution enables us to rank input features based on their relevance to the output features. Features with the highest contributions in the output neurons offer a robust estimate of the output, implicitly guiding us toward identifying the optimal feature set, as represented in Equation 1. Building on this premise, we introduce the neuron attribution feature selection metric in the context of neural networks."}, {"title": "Input Neuron Importance", "content": "The importance of the input neurons is computed based on the neuron attribution of the output neurons. We propose to compute the importance score of each input neuron as the following:\n$S_{itr}(X_j) = \\frac{1}{m}\\sum_{i=0}^{m-1}\\sum_{k=0}^{C-1}|\\alpha_{ji}^{L-1}(x_k)|$, (4)\nwhere $S_{itr}(X_j)$ is the importance of the jth feature at iteration itr, C is the number of output neurons (classes), m is the number of samples, and $\\alpha_{ji}^{L-1}(x_k)$ is the neuron attribution of the jth input neuron for the ith output neuron and sample $x_k$. To compute neuron importance in Equation 4 for each input neuron, we sum the neuron attribution for all output neurons (averaged over all samples); this choice is driven by empirical results. In other words, a feature is important if it is highly relevant for most output features and input samples. For computing the overall importance of the neurons during training, we sum $S_{itr}(X_j)$ over all the training iterations. While attribution methods have been studied only in dense neural networks, in this paper, we study these methods for sparse neural networks."}, {"title": "Results", "content": "In this section, we first describe the experimental settings (4.1). Then, we present the results of the feature selection comparison among sparse and dense models in Section 4.2, and among standard feature selection baselines using sparsity in Section 4.4. Finally, we visualize the neuron importance in Section 4.3. Additionally, we perform an analysis on a synthetic dataset in Appendix C."}, {"title": "Experimental Setup", "content": "In the following, we describe datasets, baselines, and the evaluation approach. More experimental details, including the hyperparameter settings and model architecture, can be found in Appendix A."}, {"title": "Datasets", "content": "The datasets, outlined in Table 2, include a diverse collection of 18 datasets, varying in size and type. This selection allows for a comprehensive analysis of each method across different domains. More than half of these datasets are high-dimensional making them challenging benchmarks for the models."}, {"title": "Baselines", "content": "The main focus of this paper is to analyze how sparse neural networks trained with DST perform in feature selection compared to dense networks. Therefore, we consider dense and sparse baselines. Secondly, for the sparse models, we want to study how the DST algorithm for training the sparse neural network affects the feature selection performance. We consider two standard DST approaches in the literature, SET [32] and RigL [13], that are frequently used in evaluating the DST framework. Finally, we want to assess the efficacy of the neuron importance metric. We consider the neuron strength metric from QuickSelection [4], which is also used as a part of neuron importance estimation in [42] or directly used in [5] to rank the features in a supervised setting; we call this metric as \"QS\" in the experiments.\nWe compare the neuron strength metric with our proposed neuron attribution metric, called \"Attr\". These metrics combined with the three considered models, dense network (Dense), SNN trained with SET (SET), and SNN trained with RigL (RigL), result in 6 baselines for the experiments: Dense-QS, Dense-Attr, SET-QS, SET-Attr, RigL-QS, RigL-Attr. For QuickSelection, as we also do for Attr, to compute the input neuron/feature importance, we sum the importance during all training epochs. For QuickSelection, as we also do for Attr, to compute the input neuron/feature importance, we sum the importance during all training epochs.\nFor evaluating each method, we first perform feature selection to derive K most important features (K is a hyperparameter set by the user). Then, we train an SVM classifier on the subset of K features of the original data and report the test accuracy on a hold-out test set."}, {"title": "Feature Selection Comparison", "content": "In this experiment, we compare the two feature ranking criteria, neuron strength from QuickSelection (QS), and neuron attribution (Attr). We consider dense and sparse MLPs to perform feature selection on. The sparse models are trained with DST; we consider SET and RigL as the training algorithms.The results show that neuron attribution generally outperforms neuron strength, sparse models outperform dense ones, and SET is generally a better choice when training the sparse neural networks for feature selection than RigL. However, selecting an appropriate feature selection metric and training algorithm based on dataset characteristics and domain requirements is of great importance."}, {"title": "Neuron Importance Visualization", "content": "In this section, we visualize the neuron importance on the MNIST dataset for each method We plot the neuron importance of input features as a 2D heatmap"}, {"title": "Comparsion with Baselines", "content": "We compare SET-Attr (achieving the highest ranking among the considered methods when K = 100) with three popular feature selection methods in the literature that exploit sparsity to perform feature selection: Lasso, STG [47], and LassoNet [28]."}, {"title": "Analysis of Effects of Sparsity for Feature Selection", "content": "To summarize the results of Tables 4 and 5, the tradeoff between accuracy and FLOPs is shown in Figure 5. As we can observe, SET-Attr has in most cases in order of magnitude lower number of FLOPs than LassoNet while achieving competitive performance.\nIn Section 4.2, we optimize the sparsity level, selecting the value yielding minimal loss on the validation set. However, our focus now shifts to a comprehensive analysis of the impact of sparsity on feature selection performance, exploring a range of sparsity levels."}, {"title": "Conclusions", "content": "In conclusion, our work contributes significantly to the understanding of feature selection with sparse neural networks within the dynamic sparse training framework. Through systematic analysis, we demonstrate the efficacy of sparse neural networks in feature selection from diverse datasets, comparing them against dense neural networks and other sparsity-inducing methods. Our findings reveal that SNNs trained with DST algorithms achieve remarkable memory and computational savings, exceeding 50% and 55% respectively compared to dense networks, while maintaining superior feature selection quality in 13 out of 18 cases. One promising direction to continue this research is to consider neuron importance metrics to improve the training of sparse neural networks in the DST framework to guide the weight addition process."}, {"title": "Experimental Setup", "content": "The architecture used in the experiments is an MLP with two hidden layers with 1000 and 100 hidden neurons in each layer, respectively. The hidden layer activation function is ReLU, and softmax is used for the output layer. We used a learning rate of 0.001 for all datasets."}, {"title": "DST Algorithms", "content": "In Section 2.3.1, we explain the dynamic sparse training framework. In the experiments, we consider two dynamic sparse training algorithms: SET and RigL. These two algorithms are among the commonly used DST approaches in the literature to study the DST framework.\n\u2022 SET. Sparse Evolutionary Training (SET) [32] is a pioneering approach that introduced the Dynamic Sparse Training framework. This method starts with initializing a sparse network with the desired sparsity level. Then, at each epoch or every few iterations, it updates the connectivity of the sparse neural network by dropping a fraction ( of weight with the smallest magnitude and adding the same number of random weights back to the network. This dynamic process serves as a means to continuously refine and update the network's topology, to evolve its structure over time.\n\u2022 RigL. The Rigged Lottery (RigL) [13] is another dynamic sparse training that evolves the topology of the sparse neural network by dynamically updating the connectivity. However, the difference compared to SET is that RigL adds the new weight based on the gradient information."}, {"title": "Performance for Various K Values", "content": "While the results of the experiments in Section 4.2 are for when we select K = 100 features, we measure the performance when selecting \u039a\u2208 {25, 50, 75, 100, 200}. To summarize the performance of each method when selecting different numbers of K, we compute the average ranking score."}, {"title": "Experiment on Synthetic Dataset", "content": "To evaluate the performance of the methods in a controlled environment, we design an experiment on an artificial dataset. We generate an artificial dataset with 200 features where only 100 of these features are informative and the rest of the features are noise."}]}