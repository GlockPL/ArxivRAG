{"title": "Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, and other SOTA Large Language Models", "authors": ["Ranjan Sapkota", "Shaina Raza", "Manoj Karkee"], "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has recently released its first formal definition of open-source software. This definition, when combined with standard dictionary definitions and the sparse published literature, provide an initial framework to support broader accessibility to AI models such as LLMs, but more work is essential to capture the unique dynamics of openness in AI. In addition, concerns about open-washing, where models claim openness but lack full transparency, has been raised, which limits the reproducibility, bias mitigation, and domain adaptation of these models. In this context, our study critically analyzes SOTA LLMs from the last five years, including ChatGPT, DeepSeek, LLaMA, and others, to assess their adherence to transparency standards and the implications of partial openness. Specifically, we examine transparency and accessibility from two perspectives: open-source vs. open-weight models. Our findings reveal that while some models are labeled as open-source, this does not necessarily mean they are fully open-sourced. Even in the best cases, open-source models often do not report model training data, and code as well as key metrics, such as weight accessibility, and carbon emissions. To the best of our knowledge, this is the first study that systematically examines the transparency and accessibility of over 100 different SoTA LLMs through the dual lens of open-source and open-weight models. The findings open avenues for further research and call for responsible and sustainable AI practices to ensure greater transparency, accountability, and ethical deployment of these models.", "sections": [{"title": "1. Introduction", "content": "Natural Language Processing (NLP) and Large Language Models (LLMs), including multimodal LLMs such as GPT-40, DeepSeek-V2, and Gemini 1.5, have witnessed transformative advancements and significant growth in recent years, as illustrated by the surging global interest from both research and industry, as depicted in Figure 1a\nThese technologies have become integral to systems and solutions across a diverse array of sectors, including healthcare Cascella et al. (2023), finance Li et al. (2023), education Neumann et al. (2024), and entertainmentQiu (2024). Their remarkable capabilities in language understanding and generation have not only revolutionized these industries but have also spurred a new wave of innovation and application development Weldon et al. (2024); Grant et al. (2025). Amidst this rapid expansion, the term \u201copen-source\u201d frequently surfaces within discussions about LLMs Kukreja et al. (2024). However, this descriptor is often misapplied or misunderstood. In many instances, developers may release only the model weights, that is, the trained parameters, without sharing the comprehensive suite of model assets such as model card, training data, code, sustainability factors (e.g., CO2 emissions), or detailed development processes. This gap is also widely discussed in the literature Ramlochan (2023) and in numerous tech blogs, including Walker II (2024), to name a few.\nAlthough proprietary LLMs like OpenAI GPT-series (4/40) Achiam et al. (2023) exhibit strong performance, their closed-source nature limits access to API-based interactions. In contrast, open-weight models like Meta LLama-series Touvron et al. (2023) provide downloadable model weights under non-proprietary licenses, enabling specialized deployments and cost-effective fine-tuning. For instance, Princeton's Llemma leverages Code Llama for advanced mathematical modeling Azerbayev et al. (2023), showcases the flexibility and cost benefits of open-weight models.\nThe distinction between \"open\" and \"closed\" LLMs is evident in their adoption trends. Closed models like GPT-3 followed a linear growth pattern (gray bars, Figure 1d), while open LLMs surged after Meta's Llama release, driving exponential adoption (green and orange bars, Figure 1d). Figure 1e further illustrates how open source models increasingly attract scientific focus compared to the same with proprietary models such as GPT-4.\nDespite this growing interest, the term \"open-source\" has frequently been used interchangeably with \u201copen weights\", leading to confusion in discussions about model accessibility. Many models labeled as open-source provide access only to their trained weights while withholding essential components such as training data, fine-tuning methodologies, and full implementation details. This distinction is critical, as true open-source models enable not just inference but also full transparency and reproducibility in AI research. A recent case highlighting the confusion between open-source and open-weight models is DeepSeek-R1 Guo et al. (2025). Initially surpassing ChatGPT in search interest (Figure 1b), its popularity rapidly declined (Figure 1c), reflecting unmet expectations. While DeepSeek-R1 provides weights and partial code under the MIT license 1, it lacks full open-source transparency, including access to training data and methodologies. This partial openness, common to models like ChatGPT and Google's Gemini, allows broader usage compared to fully closed models, but restricts deeper architectural modifications, evaluation of biases, and further enhancement of the training processes and datasets.\nThis ambiguity in AI terminologies necessitates clearer distinctions between open-source and open-weight models. True open-source Al requires full transparency, including training data and development processes, fostering reproducibility and ethical AI advancements. Defining and broadly adopting clear standards would enhance transparency, set realistic expectations, and promote responsible AI development.\""}, {"title": "1.1. Aim and Objectives", "content": "The goal of this study is to critically examines transparency practices of such \"open-weight\" LLMs, using DeepSeek-R1 and ChatGPT40 as primary examples, to map the distinctions between open-weight and fully open-source models. By doing so, we aim to:\n\u2022 Elucidate the terminological ambiguities surrounding \"open-source\" within the AI domain, specifically distinguishing between truly open-source models and those termed \"open-weight\" which offer limited transparency.\n\u2022 Investigate the implications of partial transparency on the reproducibility, community engagement, and ethical dimensions of AI development, emphasizing how these factors influence the practical deployment and trustworthiness of LLMs.\n\u2022 Propose clearer guidelines and standards to differentiate truly open-source methodologies and models from strategies that merely provide access to pre-trained model weights.\nWith this study, we seek to contribute to an informed advancement of responsible AI, where both technological innovation and collaborative transparency are harmonized. The following sections describe the current landscape of LLMs, the tensions between proprietary and open-weight models, and the broader impacts of these approaches on the AI research community."}, {"title": "2. Methodology", "content": "This study systematically examines the concepts of openness and transparency in the development and dissemination of LLMs. A multi-stage approach is used in this study, beginning with a thorough examination of foundational concepts and progressing through detailed analyses of licensing types and transparency definitions as they relate to AI systems."}, {"title": "2.1. Research Design", "content": "This study adopts a multi-stage research design to evaluate the openness and transparency of SoTA LLMs. As illustrated in Figure 2, the approach integrates established open-source criteria, foundational linguistic definitions of \"transparency\", and an extensive review of scholarly AI literature. A concise mind map (Figure 3) further delineates the core analytical branches, structured into three research questions (RQs) guiding the study. Below, each methodological component is described in detail."}, {"title": "2.2. Criteria for Openness and Transparency", "content": "Open-Source LLMs: An open-source LLM provides unrestricted access to its entire codebase, including the model architecture, training data, and the training processes Ramlochan (2023). Beyond the code and weights, a truly open-source model also discloses key factors such as performance benchmarks, bias mitigation strategies, computational efficiency, and sustainability metrics (e.g., Carbon dioxide emissions, energy consumption). For example, Meta's LLamA aligns with the open-source paradigm by offering detailed insights into its design and implementation.\nThe primary goal with open-source models is to ensurecomplete transparency and flexibility. This openness enables comprehensive understanding, recreation, and reproducibility, even though some usage restrictions may still apply. Such transparency allows the research community to scrutinize, improve, and tailor models for diverse applications. Developing and maintaining such models, however, demands substantial effort and resources, making the open-source approach both a technical and logistical challenge. For example, early models like GPT-1 and GPT-2 were released as open-source projects, providing access to their training data, code and model weights. With subsequent versions like GPT-3, OpenAI shifted to a closed-source approach, restricting access to the model architecture, code, and weights. This trend continued with GPT-4, which also remains proprietary.\nOpen-Weight LLMs: Open-weight LLMs make their pretrained model weights pro (2023), the parameters learned during the pre-training process, publicly available, while the underlying code, training data, or training methodologies may remain proprietary. Open-weight models, while more accessible and easier to deploy than closed-source models, do not provide the same level of insight into the model's inner workings as fully open-source models would. Meta's LLama series is a prime example of an open-weight LLM. Researchers can download the pre-trained weights to fine-tune and deploy the model for various applications. However, while LLama weights are available, the full training pipeline, including the code and data, remains proprietary. This enables a balance between accessibility and intellectual property protection."}, {"title": "2.2.1. Open Source and Licensing Types", "content": "OSI stands for the Open Source Initiative Open Source Initiative (2025). It is a non-profit organization dedicated to promoting and protecting open source software. OSI is best known for its Open Source Definition (OSD), which outlines the criteria that a software license must meet to be considered \"open source.\" These criteria include free redistribution, source code availability, the ability to create derivative works, and non-discrimination, among others. Essentially, OSI serves as a guardian of open source principles, ensuring that software labeled as open source truly adheres to standards that promote collaboration, transparency, and freedom in software development.\nThe primary attributes of the OSI's official definition of open-source Al are illustrated in Figure 4. OSI emphasizes that for an Al system to be truly open source, there must be unrestricted access to its entire structure. This means that key components such as the model weights, source code, and training data-must be accessible under OSI-approved terms. uch access allows any user to use, modify, share, and fully understand the AI system without needing special permissions.\nTransparency refers to the clarity and understandability of the underlying mechanisms that drive AI systems. It is achieved when training data and code are available, enabling stakeholders to replicate and scrutinize the Al's decision-making processes Larsson and Heintz (2020); Felzmann et al. (2020); Von Eschenbach (2021). This openness ensures that AI operations are not only visible but also comprehensible and accountable, thereby enhancing trust and fostering collaboration in AI development and application.\nOpen source software licenses further define the usage, modification, and distribution rights for software Contractor et al. (2022). They are critical for both protecting creators and enabling users to innovate and adapt software to their needs Quintais et al. (2023). For example, the MIT License, highly permissive, allows almost unrestricted use provided the original copyright is included.Similarly, the Apache License 2.0 2 permits broad use including modifications and distributions-with the additional safeguard of patent rights protection Although Creative Commons licenses 3 are primarily designed for creative content, variants such as CC-BY-4.0 can also govern software use by allowing commercial use provided that proper credit is given to the creator. Choosing the right license involves careful consideration of the intended use, attribution requirements, and legal protections, ensuring that software developers can support their objectives while fostering broader collaboration and innovation within the community."}, {"title": "2.2.2. Open Source and Transparency", "content": "Following the OSI guidelines, dictionary definitions further support the concept of open source and transparency. According to Oxford, open source software is described as \u201cUsed to describe software for which the original source code is made available to anyone.\" Cambridge further explains that open source software or information can be \u201cobtained legally and for free from the internet, and can be used, shared or changed without paying or asking for special permission.\u201d Merriam-Webster defines it as \"Having the source code freely available for possible modification and redistribution.\" For transparency, Oxford states it as \"The quality of something, such as glass, that allows you to see through it.\u201d Cambridge calls it \"The characteristic of being easy to see through.\" Merriam-Webster describes transparency as \u201cThe quality or state of being transparent so that bodies lying beyond are seen clearly.\u201d These definitions set a foundational understanding to evaluate the transparency practices in AI systems, as shown in Table 2, which presents a literature review and definitions derived from 10 popular literature defining transparency in AI systems."}, {"title": "2.3. Synthesis of Literature", "content": "The study first identified the requirements outlined by the OSI 7 as the baseline for evaluating AI models. These criteria covers various facets of openness, including licensing provisions, access to source code, free redistribution rights, and the ability to modify or derive new work/models from the original codebase. Building on the OSI standards, the concept of \"transparency\" was clarified through an examination of widely used dictionaries (Oxford, Cambridge, and Merriam-Webster) R\u00f6ttger et al. (2024). Key steps included:\nDatabases and Sources: The selection of databases was aligned with the goal of capturing a breadth of interdisciplinary research that intersects with artificial intelligence. Academic repositories such as ACM Digital Library, IEEE Xplore, Elsevier, Nature, Scopus, ScienceDirect, SpringerLink, Wiley Online Library, MathSciNet and renowned pre-print servers like arXiv were chosen for their extensive coverage of both technical and ethical dimensions pertinent to AI. These platforms are renowned for their consolidation of high-impact and specialized journals, which provide critical insights into both emerging and established research areas within technology and applied sciences.\nOur literature search was further reinforced by prioritizing papers that are highly cited within the academic community. Citation counts, often seen as a proxy for the influence and relevance of a study, were utilized as a key metric in selecting sources. Papers with exceptionally high citation counts (e.g., \u00bf 3000 citations), were specifically targeted. This criterion was instrumental because highly cited papers typically reflect pivotal developments in the field and are often the genesis of new research trajectories or shifts in scientific paradigms. The search terms used were \"Transparency in AI\u201d, \u201cTransparency in LLMs\u201d, \u201cExplainable AI\u201d, \u201cReproducible AI\u201d, \u201cOpen Source AI\", \"Open Source Model\", \"Open Source Software\", \"Fairness in AI\", \"\u201cEthical AI\u201d, \u201cResponsible AI\u201d, \u201cBias in AI\u201d, \"Sustainable AI\", \"Green AI\u201d, \u201cAI Ethics\u201d, \u201cAI Accountability\", \"Interpretable AI\u201d, \u201cAI Robustness\u201d, \u201cAI Reliability\", and \"AI Compliance\".\nTimeframe The literature selected for this study spans publications from 2017 onward\u2014a timeframe strategically chosen to align with the introduction of Transformers. In 2017, Vaswani et al. published Attention is All You Need Vaswani (2017), marking the beginning of a new era in Al by introducing a model architecture based on attention mechanisms. Following this, the launch of GPT-2, T5, BART, and several other language model architectures further advanced the field, shaping the development of modern LLMs. We systematically assessed these models to identify models that exemplify various degrees of openness, including open-source and open-weight practices. In the process of synthesizing these findings, we evaluated a total of 112 LLMs, a sample that represents the diverse and rapidly evolving landscape of language models from 2019 to 2025. These models were analyzed based on a wide array of architectural specifications such as the number of layers, hidden unit sizes, attention head counts, and overall parameter scales as well as openness metrics including licensing type and the public availability of training resources. The model development trend, illustrated in Figure 5, provides a visual representation of the evolution of these models. The figure shows that although the foundational literature for LLMs was established with the advent of Transformers in 2017, the major model breakthroughs and integrated transparency and accessibility features have predominantly materialized from 2019 onward and more post-ChatGPT era (Nov. 2022).\nInclusion Criteria A thorough literature review was conducted to locate transparency within broader discourses in AI development and ethics. This review captured highly cited articles and technical reports, emphasizing themes such as explainable AI, reproducibility, interpretability, and responsible Al governance Raza et al. (2025). By synthesizing these studies, our study addressed both technical (e.g., code-level transparency) and ethical (e.g., data biases) dimensions of openness.\""}, {"title": "2.4. Evaluation Framework and Application", "content": "Findings from the previous stages were synthesized into five key dimensions representing critical facets of open-source and open-weight classifications:\n1. Licensing, Usage, and Redistribution Rights\n2. Code Accessibility and Modification Rights\n3. Training Data Transparency\n4. Community and Support\n5. MMLU Score and Carbon Emissions\n6. Ethical Considerations and Reproducibility\nEach of these dimensions was assessed to determine whether a given model adhered to OSI-like openness or employed more restrictive practices similar to \u201copen-weight\u201d approaches (i.e., sharing only the model parameters). SOTA LLMs were systematically evaluated against these five dimensions as 1) Licensing, usage, and redistribution rights, 2) Training Code and Training Data, 3) Community Support, 4) Open source, and 5) Open Weights. Any evidence of collaborative contributions or transparent reporting of potential biases and vulnerabilities was also documented."}, {"title": "2.5. Research Questions", "content": "The methodology section of this study was structured around a detailed mind map, as depicted in Figure 3. This visual representation, employed to assess the transparency and openness of SoTA multimodal LLMs, organized the analytical framework into three main branches, each corresponding to a specific research question (RQ) as follows:\n1. What drives the classification of LLMs as open weights rather than open source, and what impact do these factors have on their efficiency and scalability in practical applications?\n2. How do current training approaches influence transparency and reproducibility, potentially prompting developers to favor open-weight models?\n3. How does the limited disclosure of training data and methodologies impact both the performance and practical usability of these models, and what future implications arise for developers and end-users?\nThis methodology integrates well-established open-source standards, linguistically and ethically grounded definitions of transparency, and a structured evaluation framework. The outcome is an assessment of whether leading MLLMs adhere to open-source principles or merely present limited transparency through open-weight practices. The subsequent sections detail the findings that emerged from applying this framework, highlighting significant discrepancies and implications for researchers, developers, and broader AI stakeholders."}, {"title": "3. Results", "content": "Drawing on OSI guidelines, dictionary-based definitions of transparency, and scholarly literature, this narrative review reveals that many models marketed or perceived as \"open\" primarily provided open weights (i.e., publicly available trained parameters) rather than full open-source access (i.e., source code, training data, and detailed methodologies). Table 3 outlines these distinctions across leading multimodal LLMs.\nThis comprehensive table (Table 3) compares 112 LLMs released between 2019 and 2025 in terms of release year, training data, and other key features. Early models, such as GPT-2 (Brown et al., 2020) and BERT (Devlin et al., 2019), primarily focused on foundational capabilities including improved text generation, masked language modeling, and next-sentence prediction. These models relied on relatively simple training data and featured basic natural language processing tasks. However, subsequent developments have led to a remarkable progression in both complexity and functionality. Recent models such as DeepSeek-R1 (Guo et al., 2025) and advanced iterations of ChatGPT, introduce enhanced multimodal capabilities, advanced reasoning through mixture-of-experts (MoE) architectures, and efficient scaling strategies. The table demonstrates that models released after 2020 increasingly leverage diverse and massive training datasets, from extensive web corpora to hybrid synthetic-organic data\u2014which significantly boost performance. Moreover, these models exhibit notable improvements in precision, processing speed, and bias mitigation. Although many SoTA models disclose only pre-trained weights, thereby limiting reproducibility, an emerging trend toward greater transparency regarding training methodologies was observed. This evolution reflects an industry-wide shift towards balancing commercial interests with greater accountability and openness in AI research."}, {"title": "3.2. Model-Specific Evaluations", "content": "GPT-4 OpenAI (2023) and ChatGPT Achiam et al. (2023) are proprietary models with limited architectural transparency: their training datasets, fine-tuning protocols, and structural details (e.g., layer configurations, attention mechanisms) remain undisclosed. While GPT-4's technical report outlines high-level capabilitiesGallifant et al. (2024), it omits reproducibility-critical specifics such as pre-training corpus composition, hyperparameters, and energy consumption metrics, reflecting a priority on commercial secrecy, which limits its scientific openness. Similarly, ChatGPT's API-based access restricts users to input-output interactions without exposing model internals Lande and Strashnoy (2023), thus creating a \"black box\" system that lacks transparency and does not allow third-party modifications.\nChatGPT adopt a functional accessibility paradigm, where API endpoints enable task execution (e.g., text generation, reasoning) but do not allow direct weight inspection, retraining, or redistribution Wolfe et al. (2024); Roumeliotis and Tselikas (2023). This approach, therefore, creates a dependency on proprietary infrastructure, which can limit long-term reproducibility and bias mitigation in downstream applications. While the term \"open-weights\" is occasionally used to describe these systems due to their API availability, this is misleading because true open-weight standards such as parameter accessibility (e.g., Llama 2) or training code disclosure (e.g., BLOOM Big-Science Workshop (2022)), are absent, underscoring the competing priorities between commercial control and open scientific collaboration in modern Al ecosystems. The ChatGPT's version's:\n\u2022 GPT-2: Brown et al. (2020) adopts an open-weights model under MIT License, providing full access to its 1.5B parameters and architectural details (48 layers, 1600 hidden size). However, the WebText training dataset (8M web pages) lacks comprehensive documentation of sources and filtering protocols. While permitting commercial use and modification, the absence of detailed pre-processing methodologies limits reproducibility of its zero-shot learning capabilities.\n\u2022 Legacy ChatGPT-3.5:Legacy ChatGPT-3.5 uses proprietary weights with undisclosed architectural details (96 layers, 12288 hidden size). The pre-2021 text/code training data lacks domain distribution metrics and copyright compliance audits. API-only access restricts model introspection or bias mitigation, despite claims of basic translation/text task capabilities Jaech et al. (2024).\n\u2022 Default ChatGPT-3.5: Default ChatGPT-3.5 Jaech et al. (2024) shares Legacy's proprietary architecture but omits fine-tuning protocols for its \"faster, less precise\u201d variant. Training data temporal cutoff (pre-2021) creates recency gaps unaddressed in technical documentation. Restricted API outputs prevent reproducibility of the 69.5% MMLU benchmark results.\n\u2022 GPT-3.5 Turbo: GPT-3.5 Turbo Jaech et al. (2024) employs encrypted weights with undisclosed accuracy optimization techniques. The 16K context window expansion lacks computational efficiency metrics or energy consumption disclosures. Proprietary licensing blocks third-party latency benchmarking despite \"optimized accuracy\" claims.\n\u2022 GPT-40: GPT-40 Hurst et al. (2024) uses multimodal proprietary weights (1.8T parameters) with undisclosed cross-modal fusion logic. Training data (pre-2024 text/image/audio/video) lacks ethical sourcing validations for sensitive content. \"System 2 thinking\" capabilities lack peer-reviewed validation pipelines.\n\u2022 GPT-40 mini: GPT-40 mini Hurst et al. (2024) offers cost-reduced proprietary access (1.2T parameters) with undisclosed pruning methodologies. The pre-2024 training corpus excludes synthetic data ratios and human feedback alignment details. Energy efficiency claims (60% cost reduction) lack independent verification."}, {"title": "3.2.2. DeepSeek", "content": "The DeepSeek-R1 model, a 671-billion-parameter mixture-of-experts (MoE) system built on the DeepSeek-V3 architecture, adopts an open-weights framework under the MIT License, permitting unrestricted access to its neural network parameters for commercial and research use Guo et al. (2025). MoE is an ensemble machine learning technique where multiple specialist models (referred to as \"experts\") are trained to handle different parts of the input space, and a gating model decides which expert to consult for a given input Vasi\u0107 et al. (2022); Masoudnia and Ebrahimpour (2014). This method allows for more scalable and efficient training as well as inference processes, especially in complex models like DeepSeek-R1, by dynamically allocating computational resources to the most relevant experts for specific tasks or data points.\nWhile the DeepSeek-R1 model's weights and high-level architectural details including its MoE design with 37 billion activated parameters per inference and reinforcement learning-augmented reasoning pipelines are publicly disclosed, critical transparency gaps persist. The pre-training dataset composition, comprising a hybrid of synthetic and organic data, remains proprietary, obscuring potential biases and ethical sourcing practices. Similarly, the reinforcement learning from human feedback (RLHF) pipeline lacks detailed documentation of preference model architectures, safety alignment protocols, and fine-tuning hyperparameters, limiting independent reproducibility. These omissions reflect a strategic prioritization of computational efficiency (leveraging 10,000 NVIDIA GPUs for cost-optimized training) over full methodological transparency, positioning the model as open-weights rather than fully open-source.\nThe DeepSeek models:\n\u2022 DeepSeek-R1: DeepSeek-R1's accessibility is defined by its permissive licensing and efficient deployment capabilities, with quantized variants reducing hardware demands for applications like mathematical reasoning and code generation. However, its reliance on undisclosed training data and proprietary infrastructure optimizations creates dependencies on specialized computational resources, restricting independent assessment for safety or performance validation. The model's MoE architecture, which reduces energy consumption by 58% compared to dense equivalents Guo et al. (2025), challenges conventional scaling paradigms, as evidenced by its disruptive impact on GPU market dynamics Bi et al. (2024); Liu et al. (2024a); Zhu et al. (2024); Liu et al. (2024b). This open-weights approach balances innovation dissemination with commercial secrecy, highlighting unresolved tensions between industry competitiveness and scientific reproducibility in large-language-model development. Full open-source classification would necessitate disclosure of training datasets, fine-tuning codebases, and RLHF implementation details currently withheld.\n\u2022 DeepSeek LLM :The DeepSeek LLM uses proprietary weights (67B parameters) with undocumented scaling strategies. Books+Wiki data (up to 2023) lacks multilingual token distributions and fact-checking protocols. Custom licensing restricts commercial deployments despite ", "V2": "DeepSeek LLM V2 employs undisclosed MoE architecture (236B params) with proprietary MLA optimizations. The 128K context window lacks attention sparsity patterns and memory footprint metrics. Training efficiency claims (\"lowered costs\") omit hardware configurations and carbon emission data Liu et al. (2024a).\n\u2022 DeepSeek Coder V2: DeepSeek Coder V2 provides API-only access to its 338-language coding model. Training data excludes vulnerability scanning protocols and license compliance audits. Undisclosed reinforcement learning pipelines hinder safety evaluations of generated code Zhu et al. (2024)."}, {"title": "3.2.3. Miscellaneous Proprietary Models", "content": "Meta's Llama The Llemma language model Azerbayev et al. (2023), developed for mathematical reasoning, provides open weights through its publicly accessible 7B and 34B parameter variants, released under a permissive license alongside the Proof-Pile-2 dataset and training code. These weights enable users to deploy, fine-tune, and study the model's mathematical capabilities, such as chain-of-thought reasoning, Python tool integration, and formal theorem proving. For example, Llemma 34B achieves 25.0% accuracy on the MATH benchmark, outperforming comparable open models like Code Llama (12.2%) and even proprietary models like Minerva (14.1% for 8B). The weights are hosted on Hugging Face, with detailed evaluation scripts and replication code provided, allowing researchers to validate performance metrics like GSM8k (51.5% for Llemma 34B) and SAT (71.9%).\nHowever, Llemma is also categorized as open-weights rather than fully open-source due to incomplete transparency in its development pipeline (Azerbayev et al., 2023). While the Proof-Pile-2 dataset is released 8, it excludes subsets like Lean theorem-proving data and lacks detailed documentation on data-cleaning methodologies. The training code provided is modular but omits critical infrastructure details, such as hyperparameter optimization workflows and cluster-specific configurations (e.g., Tensor parallelism settings for 256 A100 GPUs). This partial disclosure limits reproducibility and prevents independent evaluation of potential biases or training inefficiencies, aligning with broader critiques of open-weight models' inability to fulfill open-source AI's \"four freedoms\" (use, study, modify, share).\nLike Meta's Llama 3-which shares weights but restricts training data and methodology-Llemma's openness prioritizes usability over full transparency. Both models exemplify the open-weight paradigm: they release parameters for inference and fine-tuning but withhold various key elements (e.g., Llama 3's 15T-token dataset; Llemma's cluster-optimized training scripts). For Llemma, this approach balances mathematical innovation with competitive safeguards, as its Proof-Pile-2 dataset represents a significant research asset. However, the MIT License governing Llemma imposes fewer restrictions than Llama 3's proprietary terms, enabling commercial use and redistribution without attribution. The distinction lies in the degree of openness: Llemma provides more components (dataset, code) than Llama 3 but still falls short of open-source standards by omitting infrastructure-level details. This reflects a strategic compromise-enhancing accessibility for mathematical research while retaining control over computationally intensive training processes. Such tradeoffs underscore the AI community's ongoing debate about whether partial transparency suffices for ethical AI development or if full open-source disclosure remains essential for accountability.\nGoogle Gemini:Google Gemini: Google's Gemini model family exemplifies a sophisticated, multimodal approach to artificial intelligence, encompassing the Ultra (1.56 trillion parameters), Pro (137 billion parameters), and Nano (3.2 billion/17.5 billion parameters) variants (Reid et al., 2024; Team et al., 2023; Saab et al., 2024). Operating under an open-weights paradigm, these pretrained model parameters are accessible via APIs yet remain proprietary and unmodifiable, thereby preserving corporate secrecy while enabling limited external deployment. The architectural framework integrates advanced multimodal fusion mechanisms, including cross-modal attention layers and sparsely activated mixture-of-experts (MoE) blocks, and is trained on an expansive corpus of 12.5 trillion text tokens, 3.2 billion images, and 1.1 billion video-audio pairs (Team et al., 2023). Notably, technical documentation highlights innovations such as dynamic token routing for modality-specific computations and TPUv5-optimized distributed training, but omits critical reproducibility details such as the MoE router logic, TPU compiler configurations, and multimodal alignment loss functions. Furthermore, the training dataset comprises web documents (50%), code repositories (18%), and proprietary media (32%), yet lacks granular metadata that could clarify data provenance and ethical sourcing practices. This partial transparency not only restricts independent bias and safety assessments, given that weights are encrypted and inference only, but also delineates Gemini as open-weights rather than fully open-source.\nThe proprietary Google license explicitly prohibits weight modification, redistribution, and competitive commercial use, diverging from open-source frameworks like Apache 2.0. Additionally, essential hyperparameters including Ultra's learning rate schedule (0.00000625), Pro's 4.8-bit quantization thresholds, and Nano's knowledge distillation ratios remain undisclosed, reinforcing reliance on Google's ecosystem. In summary, these design choices reflect a strategy to balance capabilities with safeguards, underscoring an industry trend that prioritizes controlled innovation over transparency.\nMistral AI: Mistral AI's models, including Mistral 7B and Mixtral 8x7B, are classified as open-weights because their model parameters and architectural blueprints are released under the Apache 2.0 license, permitting commercial use, modification, and redistribution (Jiang et al., 2023b). They employ advanced architectures such as grouped-query attention (GQA) and sliding window attention (SWA) with a 4,096-token window to optimize inference efficiency, and Mistral 7B is trained on 2.4 trillion multilingual tokens. Despite this openness, critical reproducibility details remain undisclosed, including the composition of the training dataset, hyperparameter configurations (e.g., learning rate schedules and batch sizes), and reinforcement learning from human feedback (RLHF) pipelines. Additionally, licensing distinctions appear with models like Codestral-22B, which are governed by the Mistral Non-Production License (MNPL) that restricts commercial deployment without explicit agreements, creating tiered accessibility."}, {"title": "Licensing and Openness Spectrum.", "content": "The analyzed models demonstrate a continuum of openness, with Dolly 2.0 representing full open-source implementation (weights, code, data under CC-BY-SA/Apache 2.0), contrasting sharply with proprietary systems like Gemma Team et al. (2024), Jurassic-1 Lieber et al. (2021), and Olympus which provide no public access. Intermediate approaches include Apache 2.0-licensed weights without training data (BERT Devlin et al. (2019), T5 Raffel et al. (2020), Mistral 7B Jiang et al. (2023b)), custom licenses with commercial restrictions (LLaMA 270B Touvron et al. (2023), WuDao 2.0 Jie (2021)), and API-only access models (Gemini 1.5 Reid et al. (2024), Med-Gemini-L 1.0 Saab et al. (2024)). Notably, Grok-1 and GPT-NeoX Black et al. (2022) adopt Apache 2.0 for weights but withhold critical training details, while Switch Transformer Fedus et al. (2022) and CTRL Keskar et al. (2019) share architectures but omit infrastructure specifics. This spectrum reflects industry tensions between collaborative innovation and competitive advantage protection.\nTraining Data Transparency Deficits. Across all surveyed models, only Dolly 2.0 provides complete training dataset documentation. Common omissions include temporal stratification (BERT Devlin et al. (2019), XLNet Yang (2019)), copyright compliance (Codex Chen et al. (2021), WaveCoder-Pro-6.7B Yu et al. (2023)), and ethical sourcing validations (T5 Raffel et al. (2020), Gopher Rae et al. (2021)). Multilingual models like mT5 Xue (2020) and SeaLLM-13b Nguyen et al. (2023) lack low-resource language quality controls, while medical systems (Med-PaLM M Tu et al. (2024)) omit HIPAA compliance proofs. Even open-weight models (ROBERTa Liu (2019), ELECTRA Clark (2020)) typically exclude bias audits and demographic metadata, with notable exceptions in BLOOM's Workshop et al. (2022) partial cultural documentation. Proprietary models (PaLM Chowdhery et al. (2023), GLaM Du et al. (2022)) show near-"}]}