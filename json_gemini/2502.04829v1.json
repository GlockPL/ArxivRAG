{"title": "Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization", "authors": ["Yedidya Kfir", "Elad Sarafian", "Sarit Kraus", "Yoram Louzoun"], "abstract": "Black-box algorithms are designed to optimize functions without relying on their underlying analytical structure or gradient information, making them essential when gradients are inaccessible or difficult to compute. Traditional methods for solving black-box optimization (BBO) problems predominantly rely on non-parametric models and struggle to scale to large input spaces. Conversely, parametric methods that model the function with neural estimators and obtain gradient signals via backpropagation may suffer from significant gradient errors. A recent alternative, Explicit Gradient Learning (EGL), which directly learns the gradient using a first-order Taylor approximation, has demonstrated superior performance over both parametric and non-parametric methods. In this work, we propose two novel gradient learning variants to address the robustness challenges posed by high-dimensional, complex, and highly non-linear problems. Optimistic Gradient Learning (OGL) introduces a bias toward lower regions in the function landscape, while Higher-order Gradient Learning (HGL) incorporates second-order Taylor corrections to improve gradient accuracy. We combine these approaches into the unified OHGL algorithm, achieving state-of-the-art (SOTA) performance on the synthetic COCO suite. Additionally, we demonstrate OHGL's applicability to high-dimensional real-world machine learning (ML) tasks such as adversarial training and code generation. Our results highlight OHGL's ability to generate stronger candidates, offering a valuable tool for ML researchers and practitioners tackling high-dimensional, nonlinear optimization challenges.", "sections": [{"title": "1. Introduction", "content": "Black-box optimization (BBO) is the process of searching for optimal solutions within a system's input domain without access to its internal structure or analytical properties. Unlike gradient-based optimization methods that rely on the calculation of analytical gradients, BBO algorithms query the system solely through input-output pairs, operating agnostically to the underlying function. This feature distinguishes BBO from traditional ML tasks, such as neural network training, where optimization typically involves backpropagation-based gradient computation.\nMany real-world physical systems naturally fit into the BBO framework because their analytical behavior is difficult or impossible to model explicitly. In these cases, BBO algorithms have achieved remarkable success in diverse fields, such as ambulance deployment, robotic motor control, parameter tuning, and signal processing, among others. BBO applications extend beyond physical systems; many ML problems exhibit a black-box nature when the true gradient is absent. Examples include hyperparameter tuning, contextual bandit problems, and large language model training with human feedback, to name a few.\nAs the scale of data continues to grow, the dimensionality of the problem space increases in tandem. This trend is particularly evident in ML, where model architectures, embedding and latent representation sizes, and the number of hyperparameters are continually expanding. High-dimensional optimization challenges traditional BBO algorithms, which often require the number of collected samples at each step, $n_s$, to scale proportionally with the problem size, N. Parametric neural models, however, have shown that reusing past samples can effectively reduce the required sample size such that $n_s << N$. Building on this, we propose a sampling profiler that further reduces the number of samples needed at each step while maintaining performance.\nWhile problem dimensions increase, the cost of evaluating intermediate solutions remains a critical constraint, especially in real-world settings where interaction with the environment is expensive or in ML tasks where larger models counterbalance gains in computational power. Therefore, modern BBO algorithms must not only reduce evaluation steps but also converge more quickly. Achieving this requires algorithms capable of more accurately predicting optimization directions, either through better gradient approximation or momentum-based strategies to handle nonconvexity and noise. In this paper, we propose two key improvements to Explicit Gradient Learning (EGL): (1) Optimistic Gradient Learning (OGL), a weighted gradient estimator that biases toward promising solutions and (2) Higher-Order Gradient Learning (HGL), which incorporates Hessian corrections to yield more accurate gradient approximations.\nWe combine the strengths of OGL and HGL to a unified algorithm termed OHGL which exhibits four key advantages:\n\u2022 Robustness: OHGL consistently outperforms baseline algorithms across a diverse range of benchmark problems, including synthetic test suites and real-world ML applications. Its ability to handle noisy and non-convex environments.\n\u2022 Gradient Precision: By integrating the second-order information via Hessian corrections, OHGL achieves significantly more accurate gradient approximations than standard EGL.\n\u2022 Convergence Rate: OHGL demonstrates faster convergence rate.\n\u2022 Utilizing the sampling profiler and the optimistic approach, OGL is able to solve high-dimensional problems with smaller budget and converge faster than baseline algorithms.\nRelated works: Black-box optimization (BBO) algorithms have a long history, with various approaches developed over the years. Some of the foundational techniques include grid search, coordinate search, simulated annealing, and direct search methods like Generalized Pattern Search and Mesh Adaptive direct search, Gradient-less descent, and ZOO. These approaches iteratively evaluate potential solutions and decide whether to continue in the same direction. However, they resample for every step and don't use the sampled budget from previous iterations, wasting a lot of budget.\nAnother prominent family of BBO algorithms is the genetic algorithm family. This includes methods such as Covariance Matrix Adaptation (CMA) and Particle Swarm Optimization (PSO). These algorithms simulate the process of natural evolution, where a population of solutions evolves through mutation and selection. They are considered state-of-the-art (SOTA) in optimization due to their effectiveness in tackling complex problems. However, they come with significant drawbacks, particularly the need for extensive fine-tuning of parameters like generation size and mutation rates. CMA, for example, struggles in higher-dimensional environments and requires careful adjustment of hyperparameters and guidance to perform optimally. In this work, we propose a simpler method to enhance the performance of CMA, particularly in high-dimensional settings.\nThen there are model-based methods, which attempt to emulate the behavior of the function using a surrogate model. These models provide important analytical information, such as gradients, to guide the optimization process and help find a minimum. Within this class, we can further distinguish two sub-classes. To address the issue of dimensionality, Explicit Gradient Learning (EGL) was proposed by. While many model-based methods focus on learning the function's structure to derive analytical insights (e.g., Indirect Gradient Learning or IGL), EGL directly learns the gradient information. EGL uses Taylor's theorem to estimate the gradient. The authors also emphasize the importance of utilizing a trust region to handle black-box optimization problems. However, EGL has some drawbacks: it often uses the available budget inefficiently, disregarding both the complexity and dimensionality of the environment. Additionally, the datasets created by EGL can be naive, leading to over-fitting or improper network learning. This work tackles these issues by showing the importance of proper algorithm calibration and optimization.\nRecent work also highlights the limitations of common assumptions in BBO algorithms, such as continuity or Gaussian distributions of functions, which can hinder optimization. For instance, OPT-GAN, a generative model, seeks to bypass these assumptions by learning a function's distribution and generating better candidate solutions based on that knowledge.\nThe paper is organized as follows: Section 2 covers the algorithm's theoretical background and mathematical foundations. Sections 3 and 4 present our two enhanced variants of the gradient learning algorithm: OGL and HGL, these are followed by section 5 where we present the full algorithm OHGL. Section 6 provides experimental results on the synthetic COCO test suite and Section 7 highlights 2 real-world high-dimensional applications and potential uses. Finally, section 8 concludes and suggests future research directions. Our code, experiments, and environment setup are available in the supplementary material. XX"}, {"title": "2. Background", "content": "The goal of black-box optimization (BBO) is to minimize a target function f(x) through a series of evaluations over a predefined domain $\\Omega$:\n$\\text{find: } x^* = \\underset{x \\in \\Omega}{\\text{arg min }} f(x)$     (1)\nThe Explicit Gradient Learning method, as proposed by leverages the first-order Taylor's expansion: $f(y) = f(x) + \\nabla f(x)^T (y - x) + R_1(x, y)$. Here, $R_1(x, y) = O(||y - x||^2)$ is a higher-order residual. By minimizing the residual term with a surrogate neural network model, EGL learns the mean-gradient: a smooth approximation of the function's gradient\n$g_{\\varepsilon}^{EGL}(x) = \\underset{g_o:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n}{\\text{arg min }}  \\int_{\\tau \\in B_{\\varepsilon}(0)} (R_{\\varepsilon}^{EGL}(\\tau))^2 d\\tau$   (2)\n$R_{\\varepsilon}^{EGL}(\\tau) = f(x) - f(x + \\tau) + g_o(x)^T\\tau$"}, {"title": "3. Optimistic Gradient Learning with Weighted Gradients", "content": "While local search algorithms like EGL are based on the notion that the gradient descent path is the optimal search path, following the true gradient path may not be the optimal approach as it can lead to inefficient sampling, susceptibility to shallow local minima, and difficulty navigating through narrow ravines. To illustrate, after sampling a batch D of pairs {($x_i, f(x_i)$)}$_{i\\in D}$ around our current solution x, a plausible and optimistic heuristic can be to direct the search path towards low regions in the sampled function landscape regardless of the local curvature around x. In other words, to take a sensible guess and bias the search path towards the lower ($x_i, f(x_i)$) samples. To that end, we define the Optimistic Gradient Learning objective by adding an importance sampling weight to the integral of Eq. 2\n$g_{\\varepsilon}^{OGL}(x) = \\underset{g_o:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n}{\\text{argmin }}  \\int_{\\tau \\in B_{\\varepsilon}(0)} W_f(x,x+\\tau)\\cdot (R_{\\varepsilon}^{EGL}(\\tau))^2 d\\tau$   (3)\nThe importance sampling factor $W_f$ should be chosen s.t. it biases the optimization path towards lower regions regardless of the local curvature around x, i.e. $W(x, y_1) \\geq W(x, y_2)$ for $f(y_1) \\leq f(y_2)$. In our implementation, we design $W_f$ as a softmax function over the sampled batch of exploration points {$y_i$} $\\in$ D around x\n$W_f(x, y_i) = \\frac{e^{-\\min(f(x), f(y_i))}}{\\sum_{y_j\\in D} e^{-f(y_j)}}$   (4)\nNotice that in practice, the theoretical objective in Eq. 3 is replaced by a sampled Monte-Carlo version (see Sec. 5) s.t. the sum of all weights across the sampled batch is smaller than 1. In the following theorem, we show that the controllable accuracy property of EGL, which implies that the mean-gradient converges to the true gradient still holds for our biased version, s.t. when $\\varepsilon \\rightarrow 0, g^{OGL} \\rightarrow \\nabla f(x)$ this guarantees that the convergence properties of the mean-gradient still hold\nTheorem 3.1. (Optimistic Controllable Accuracy) For any differentiable function f with a continuous gradient, there exists $\\kappa_{OGL} > 0$ such that for any $\\varepsilon > 0, g^{OGL}(x)$ satisfies\n$||g^{OGL}(x) - \\nabla f(x)|| \\leq \\kappa_{OGL}\\varepsilon$  for all x $\\in \\Omega$."}, {"title": "4. Gradient Learning with Hessian Corrections", "content": "To learn the mean-gradient, EGL minimizes the first-order Taylor residual (see Sec. 2). Utilizing higher-order approximations has the potential of learning more accurate models for the mean-gradient. Specifically, the second-order Taylor expansion is\n$f(x+\\tau) = f(x)+\\nabla f(x)^T\\tau + \\frac{1}{2}\\tau^T\\nabla^2 f(x)\\tau + R_2(x,x+\\tau)$\nHere $R_2(x, y) = O(||x \u2212 y||^3)$ is the second order residual. Like in EGL, we replace $\\nabla f$ with a surrogate model $g_o$ and minimize the surrogate residual to obtain our Higher-order Gradient Learning (HGL) variant\n$g^{HGL}(x) = \\underset{g_o:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n}{\\text{argmin }}  \\int_{\\tau \\in B_{\\varepsilon}(0)} (R_{\\varepsilon}^{HGL}(\\tau))^2 d\\tau$   (5)\n$R_{\\varepsilon}^{HGL}(\\tau) = f(x) - f(x + \\tau) + g_o(x)^T\\tau + \\frac{1}{2}\\tau^TJ_{g_o}(x)^T\\tau$\nThe new higher-order term $J_{g_o}(x)$ is the Jacobean of $g_o(x)$, evaluated at x which approximates the function's Hessian matrix in the vicinity of our current solution, i.e., $J_{g_o}(x) \\approx \\nabla^2 f(x)$. Next, we show theoretically that as expected, HGL converges faster to the true gradient which amounts to lower gradient error in practice.\nTheorem 4.1. (Improved Controllable Accuracy): For any twice differentiable function f $\\in$ C2, there exists $\\kappa_{HGL} > 0$ such that for any $\\varepsilon > 0$, the second-order mean-gradient $g^{HGL}(x)$ satisfies\n$||g^{HGL}(x) - \\nabla f(x)|| \\leq \\kappa_{HGL}\\varepsilon^2$ for all x $\\in \\Omega$.\nIn other words, in HGL the model error is in an order of magnitude of $\\varepsilon^2$ instead of $\\varepsilon$ in EGL and OGL. In practice, we verified that this property of HGL translates to more accurate gradient models.\nLearning the gradient with the Jacobian corrections introduces a computational challenge as double backpropagation can be expensive. This overhead can hinder the scalability and practical application of the method. A swift remedy is to detach the Jacobian matrix from the competition graph. While this step slightly changes the objective's gradient (i.e. the gradient trough $(R_{\\varepsilon}^{HGL})^2$) it removes the second-order derivative and in practice, we found that it achieves similar results compared to the full backpropagation through the residual $R_{\\varepsilon}^{HGL}$, see Fig. 4(b)."}, {"title": "5. OHGL", "content": "In this section, we combine the optimistic approach from Sec. 3 and the Hessian corrections from Sec. 4. However, unlike previous sections, we will present the practical implementation where integrals are replaced with Monte-Carlo sums of sampled pairs. In this case, the loss function which is used to obtain the Optimistic Higher-order Gradient (OHGL) model is\n$L^{OHGL}(\\theta) = \\sum_{||x_i-x_j||<\\varepsilon} W_f(x_i, x_j)R^{HGL}(x_i-x_j)^2$   (6)\nThe summation is applied over sampled pairs which satisfy $||x_i - x_j|| \\leq \\varepsilon$. As explained in Sec. 4, we detach the Jacobian of $g_o$ from the computational graph to avoid second-order derivatives. Our final algorithm is outlined in Alg. 1 and an extended version including additional technical details is found in the appendix (See Alg. 2)."}, {"title": "6. Experiments in the COCO test suite", "content": "We evaluated the OGL, HGL and OHGL algorithms on the COCO framework and compared them to EGL and other strong baselines: (1) CMA and its trust region variants L-CMA (linear trust region mapping function) and T-CMA (tanh trust region mapping function) and (2) Implicit Gradient Learning (IGL) where we follow the EGL protocol but train a model for the objective function and obtain the gradient estimation by backpropagation as in DDPG. We also adjusted EGL hyper-parameters A.4 and improved the trust region A.5 to reduce the budget usage by our algorithms.\nWe use the following evaluation metrics:\n\u2022 Convergence Rate: Speed of reaching the global optimum.\n\u2022 Success Rate: Percentage of problems solved within a fixed budget.\n\u2022 Robustness: Performance stability across different hyperparameter settings.\nPerformance was normalized against the best-known solutions to minimize bias: $\\text{normalized\\_value = } \\frac{Y-Y_{\\min}}{Y_{\\max}-Y_{\\min}}$. A function was considered solved if the normalized value was below 0.01."}, {"title": "6.1. Success and Convergence Rate", "content": "Figure 4(c) illustrates the success rate of each algorithm relative to the distance from the best point required for solving a function. The results show that both OGL and OHGL consistently outperform all other algorithms. In particular where the error should be small (<0.01), with t-tests yielding p-values approaching 1, indicating strong statistical confidence (see the complete list of t-test p-values in Table 4 in the appendix).\nFigure 4(a) presents the convergence rates for the seven algorithms, where OGL, HGL, and OHGL demonstrate superior convergence compared to the other methods. OHGL, in particular, consistently ranks among the top performers across most metrics, as seen in Table 1, which compares multiple performance indicators. Although OGL achieves better initial results, as it searches longer for the space with the optimal solution, making it a worse fit for problems with a low budget, it ultimately reaches superior results. Additionally, Figure 4(b) shows an ablation test, confirming that the core components of our work: the optimistic approach (OGL) and the Higher-order corrections (HGL) contributed substantially much more to the overall performance than other technical improvements to the algorithm (with respect to vanilla EGL), these technical improvements are denoted as EGL-OPTIMIZED and they amount to better trust-region management and more efficient sampling strategies which mainly helps in faster learning rate at the beginning of the process (as described in Sec. A.4 and A.5 in the appendix)."}, {"title": "6.2. Hyperparameter Tolerance", "content": "We evaluate the robustness of our algorithm to hyperparameter tuning. Our objective is to demonstrate that variations in key hyperparameters have minimal impact on the algorithm's overall performance. We conducted systematic experiments to assess this, modifying several hyperparameters and analyzing their effects. Table 2 (and Table 6 in the appendix) reports the coefficient of variation (CV), defined as $CV = \\frac{\\sigma}{\\mu}$, across different hyperparameter sweeps, highlighting the algorithm's stability under varying conditions.\nOur findings indicate that certain hyperparameters-such as the epsilon factor, shrink factor, and value normalizer learning rate (LR)\u2014exhibit cumulative effects during training. While small variations in these parameters may not have an immediate impact, their influence can accumulate over time, potentially leading to significant performance changes. In A.11.2, we establish the relationship between the step size and the epsilon factor necessary for ensuring progress toward a better optimal solution. When selecting these parameters, this relationship must be considered. Additionally, the shrink factor for the trust region should be chosen relative to the budget, enabling the algorithm to explore the maximum number of sub-problems. This underscores the importance of fine-tuning these parameters for optimal results. Conversely, we found that the structural configuration of the neural network, including the number and size of layers, had minimal effect on performance. This suggests that the algorithm's reliance on Taylor loss enables effective learning even with relatively simple network architectures, implying that increasing model complexity does not necessarily yield substantial improvements."}, {"title": "7. High Dimensional Applications", "content": null}, {"title": "7.1. Adversarial Attacks", "content": "As powerful vision models like ResNet and Vision Transformers (ViT) grow in prominence, adversarial attacks have become a significant concern. These attacks subtly modify inputs, causing models to misclassify them, while the perturbation remains imperceptible to both human vision and other classifiers. Formally, an adversarial attack is defined as:\n$x^* = arg \\underset{x}{ \\text{ min }} d(x, x_a) s.t. f(x) \\neq f(x_a)$  (7)\nWhere f is the classifier and d is some distance metric between elements.\nRecent studies have extended adversarial attacks to domains like AI-text detection and automotive sensors. These attacks prevent tracking and detection, posing risks to both users and pedestrians. Adversarial attacks are classified into black-box and white-box methods. Black-box attacks only require query access, while white-box methods use model gradients to craft perturbations. Despite some black-box methods relying on surrogate models, approaches like generate random samples to approximate gradient estimation, though they are computationally expensive. Other methods use GAN networks to search latent spaces for adversarial examples. Still, they depend on existing GANs and their latent space diversity.\nOur Enhanced Gradient Learning method offers a true black-box approach with precise perturbation control, avoiding gradient back-propagation. OGL directly optimizes perturbations to maintain low distortion while fooling the model, handling high-dimensional spaces with over 30k parameters.\nMethodology: We applied OGL to classifiers trained on MNIST, CIFAR-10, and ImageNet, aiming to minimize (7). To generate adversarial images with minimal distortion, we developed a penalty that jointly minimizes MSE and CE-loss A.9. This approach successfully fooled the model, evading the top 5 classifications.\nResults: We evaluated four different configurations: CMA, OGL, HGL, and a combination of both (CMA+OGL) where the CMA run provides the initial guess for an OGL run. While CMA alone was not able to converge to a satisfying adversarial example, the combined CMA+OGL enjoyed the rapid start of CMA with the robustness of OGL s.t. it was able to find a satisfying adversarial example half the computation time of OGL and OHGL."}, {"title": "7.2. Code generation", "content": "The development of large language models (LLMs) such as Transformers have advanced code generation. Despite these strides, fine-tuning outputs based on parameters measured post-generation remains challenging. Recent algorithms have been developed to generate code tailored for specific tasks using LLMs. For instance, FunSearch generates new code solutions for complex tasks, while Chain of Code incorporates reasoning to detect and correct errors in the output code. Similarly, our method uses black-box optimization to guide code generation for runtime efficiency. Building on, which links LLM expertise to a small parameter set, we fine-tuned the embedding layer to reduce Python code runtime. Using LoRA, we optimized the generated code based on execution time, scaling up to ~ 200k parameters.\nFibonacci: We tested this approach by having the model generate a Fibonacci function. Initially incorrect, optimization guided the model to a correct and efficient solution. Figure 1 illustrates this progression, with the 25th step showing an optimized version.\nLine-Level Efficiency Enhancements: We tested the model's ability to implement small code efficiencies, such as replacing traditional for-loops with list comprehensions. The algorithm optimized the order of four functions-'initialize', 'start', 'activate', and 'stop'\u2014each with eight variants, minimizing overall runtime by optimizing the function order.\nCode Force: For a more complex problem, we used the Count Triplets challenge from Codeforces\u00b2. While the model initially struggled, once it found a correct solution, the algorithm further optimized it for runtime performance A.3.\nDiscussion: Our method demonstrates the ability to generate correct solutions while applying micro-optimizations for efficiency. In simple tasks like Fibonacci, OGL converged on an optimal solution 7, and in more complex problems, it improved the initial solutions. However, in more complex tasks, the LLM may generate code that fails to solve the problem, hindering the optimization of the run-time. To address this, either stronger models or methods focused on optimizing solution correctness are needed. This would ensure that valid solutions are generated first, which can then be further optimized for performance."}, {"title": "8. Conclusion", "content": "In this work, we introduced several key enhancements to the EGL algorithm, resulting in OHGL, a powerful black-box optimization tool capable of addressing various complex problems. We demonstrated improvements such as second-order gradient approximation, optimistic gradients, and trust region enhancements, which collectively led to faster convergence and more robust performance, particularly in high-dimensional and intricate tasks.\nOur experiments showcased OHGL's superiority over state-of-the-art methods, especially in tasks where computational efficiency and precision are paramount, such as adversarial attacks on vision models and optimizing code generation. OHGL's ability to navigate complex optimization spaces while minimizing computational cost demonstrates its utility across various applications.\nIn conclusion, OHGL presents a novel BBO algorithm design, providing a robust framework for solving complex optimization problems. However, the true potential of BBO algorithms lies in their broader applicability, and future research should focus on unlocking new avenues for their use, especially in generative models and other cutting-edge areas."}]}