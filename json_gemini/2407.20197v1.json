[{"title": "Learning Random Numbers to Realize Appendable Memory System for Artificial Intelligence to Acquire New Knowledge after Deployment", "authors": ["Kazunori D Yamada"], "abstract": "In this study, we developed a learning method for constructing a neural network system capable of memorizing\ndata and recalling it without parameter updates. The system we built using this method is called the Appendable\nMemory system. The Appendable Memory system enables an artificial intelligence (AI) to acquire new knowledge\neven after deployment. It consists of two Als: the Memorizer and the Recaller. This system is a key-value store\nbuilt using neural networks. The Memorizer receives data and stores it in the Appendable Memory vector, which is\ndynamically updated when the AI acquires new knowledge. Meanwhile, the Recaller retrieves information from the\nAppendable Memory vector. What we want to teach AI in this study are the operations of memorizing and recalling\ninformation. However, traditional machine learning methods make AI learn features inherent in the learning dataset.\nWe demonstrate that the systems we intend to create cannot be realized by current machine learning methods, that\nis, by merely repeating the input and output learning sequences with AI. Instead, we propose a method to teach\nAl to learn operations, by completely removing the features contained in the learning dataset. Specifically, we\nprobabilized all the data involved in learning. This measure prevented AI from learning the features of the data. The\nlearning method proposed in the study differs from traditional machine learning methods and provides fundamental\napproaches for building an AI system that can store information in a finite memory and recall it at a later date.", "sections": [{"title": "1 Introduction", "content": "In this study, we propose a novel learning method to teach an artificial intelligence (AI) to learn how to operate.\nSpecifically, we trained the AI to memorize information and to recall that information. The operations performed by the\nAl developed with our learning method are independent of the property of the data used for training, allowing the AI to\nmemorize new information and utilize it even after deployment.\nThe development of an AI, which continues to learn even after deployment, is a challenging problem in the field of\nAI research [1]. AI that can continue to learn post deployment must possess the following abilities:\n(1) Memorizing: the ability to retain information that has been supplied thus far in memory data and to add the newly\nsupplied information to existing memory data without compromising the already-memorized information,\n(2) Recalling: the ability to freely extract necessary information from memory data.\nIn this study, the AI system we aim to develop is one that can memorize and recall information without any additional\nlearning processes involving parameter updates after deployment. We intend to teach AI two operations: memorizing and\nrecalling.\nSpecifically, the system we aim to construct in the study is shown in Figure 1. We henceforth refer to this system\nas the Appendable Memory system. The Appendable Memory system consists of a series of two AIs: the Memorizer\nand the Recaller. During the memorization phase, the Memorizer receives multiple pieces of information and generates\nan Appendable Memory, which encapsulates various pieces of information. The Appendable Memory allows for the\naddition of new information. During the recall phase, the Recaller uses this memory vector and a piece of data that\ntriggers the system to restore information from the memory vector. The Memorizer and the Recaller are already trained\nAIs that do not update their parameters when memorizing or recalling information. Instead, the Appendable Memory\nchanges each time the Memorizer adds new information to it.\nNext, we consider actually training this system, but this cannot be achieved with current machine learning methods.\nThis is because machine learning merely teaches AI to elucidate features inherent in learning datasets, not to learn\noperations. In the memorization phase of Figure 1, the input vector is composed of keys and values. Imagine a scenario\nwhere these keys and values have no predictable pattern, meaning they are random. In such a case, if the Recaller is\ntrained repeatedly, it might improve prediction performance only for the training dataset. This is because the Recaller\nonly needs to remember all combinations of keys and values in the dataset. However, if a completely new key is input, the\nRecaller can only return a random value, based on the premise that there is no rule in the combination of keys and values.\nWe demonstrate this by an experiment. Through the experiment, we demonstrate that, while current machine learning\nmethods can extract patterns inherent in data, they cannot teach AI to learn operations, that is, how to remember or recall\ninformation.\nOriginally, machine learning methods fit AI to features inherent in a dataset. However, the system we intend to build\ncannot be realized by the usual machine learning methods. We want the Appendable Memory system not to learn features\nin the dataset. Therefore, to delete predictable patterns in the dataset, we probabilized the data contained in the dataset. By\ngenerating all data randomly at each learning step, namely epoch, we prevented AI from learning features in the training\ndataset. As a result, we succeeded in designing a system that can encode multiple pieces of information independently\ninto a memory vector and restore the information freely.\nIn this article, we will first discuss the limitations of current machine learning methods, then propose a machine\nlearning method using random numbers to realize a memory to which new information can be appended, and finally\ndescribe applications that utilize Appendable Memory to implement a sorting algorithm."}, {"title": "2 Related Work", "content": "The method we propose in this study involves probabilizing the learning data during neural network training. Typi-\ncally, supervised machine learning methods teach AI to learn the features of data in the training dataset. Such AI makes\npredictions based on the features of the training dataset. In contrast, the AI we intend to build is one capable of memo-\nrizing and recalling information, regardless of any properties in the training dataset. Therefore, to teach AI to only learn\nthe operations, we probabilized the dataset for each parameter update during learning, to remove its inherent features.\nThere are several methods that apply stochastic concepts for AI learning. The most basic is using random numbers\nfor parameter initialization. A survey paper [2] introduced various methods that utilize randomness. However, there is no\nresearch and development on probabilizing the dataset itself.\nUsually, stochastic gradient descent (SGD) is used in neural network training [3]. SGD involves randomly shuffling\nand batching the training dataset. This is not the same as randomly generating the training dataset itself, as we do in this\nstudy. SGD only shuffles the training dataset and samples portions of the dataset for each parameter update randomly.\nFor other types of neural networks such as generative adversarial networks (GANs) [4] and diffusion models [5], the\nconcept of randomness is used for the learning process. However, these differ from our method. Generative models like\nGANs and diffusion models utilize random noise to ensure the diversity of the generated data. In contrast, our method\nuses randomness not to teach the AI the features inherent in the dataset, but to teach it operations. Reinforcement learning\ncan be used to make models acquire operations instead of learning patterns in the data, and therefore the underlying\nmechanism of the method we developed might be similar to that of reinforcement learning in this regard."}, {"title": "3 Experiments", "content": "This study proposes an AI system for storing information within a single, modality-unrestricted finite-sized vector and\nretrieving information from it by utilizing a neural network resembling a recurrent neural network (RNN) and an encoder-\ndecoder network, referred to as Memorizer-Recaller. The proposed system can generate memory data, an Appendable"}, {"title": "3.1 Network Architecture", "content": "This study proposes an AI system for storing information within a single", "formula": "n$u_t \u2190 f(k_t"}, {"content": "$p \u2190 \u03c3(w_1u_t + b_1)"}, {"content": "$q \u2190 \u03c3(w_2m_{t-1"}, "content", "$m_t \u2190 \u03c3(w_3(p+q)+b_3),$\\\n      ", "content", "where w denotes the weight parameter, b is the bias parameter, and \u03c3 is the activation function. We used LeakyReLU [6]\nas the activation function. In addition, f is a function that concatenates two vectors, t is an integer from 1 to N, $k_t$ is the\nt-th key, $v_t$ is the t-th value, and $m_t$ is the t-th memory vector. In the Memorizer, the (t-1)-th memory vector is used to\ngenerate the t-th memory vector, and the Memorizer has a recurrent loop in its structure; therefore, the Memorizer is a\ntype of RNN. The variable p in the final formula is derived from the input data, whereas q is derived from the (t \u2212 1)-\nth memory vector. The memory vector includes both previous memory information and new input information. Because\ncalculating the elementwise summation of p and q is essential, these vectors must have identical dimensions. We perform\ncomputations for all N data points during the memorization phase to obtain $m_N$. The second model, that is, the Recaller,\nis represented by the following formula:\n$r \u2190 \u03c3(w_4k_i+b_4),$\\\n      ", "content", "$s \u2190 \u03c3(w_5m_N+b_5),$\\\n      ", "content", "$v_i \u2190 f(r,s),$\\\n      ", "content", "$v_i \u2190 \u03c3(w_6v_i+b_6),$\\\n      ", "content", "$v_i \u2190 \\sigma (w_7v_i+b_7),$\\\n      ", "content", "where \u03c3 is the activation function, and we used the softmax function, because in the experiment the Recaller is used to\nclassify and predict values from 0 to 9. In addition, $k_i$ is any one of the N keys used as the input to the Memorizer. In the\nrecall phase, it attempts to output $\\hat{v_i}$ corresponding to $k_i$.\nFigure 2 illustrates the Memorizer-Recaller network. In the figure, the structure on the left is the Memorizer, and the\none on the right is the Recaller. The Memorizer accepts pairs of key and value vectors as inputs and outputs a memory\nvector. Repeating this calculation for all N input values ultimately generates a memory vector that is expected to contain\nall input information. The Recaller accepts a single key and the memory vector generated by the Memorizer as input and\noutputs the value corresponding to the key.\nThe objective to train the Memorizer-Recaller network is to develop an AI system that can realize the storage and\nretrieval of multiple pieces of information, as shown in Figure 1. In this study, we have not discussed the performance\nenhancement of the system; therefore, we did not include any luxury mechanisms, which are generally used to im-\nprove the performance of neural networks. We designed the system with a simple combination of layers, as illustrated\nin the figure. The source codes used to generate all the networks in this study are available in a GitHub repository,\nhttps://github.com/yamada-kd/Memorizer-recaller.git."]}, {"title": "3.2 Learning Process of the Models", "content": ""}, {"title": "3.2.1 Learning in a Standard Manner", "content": "Firstly", "follows": "n$m_t = M(k_t"}, {"content": "$v_i = R(k_i"}, {"content": "$L(w"}, {"content": "where M represents the Memorizer, R represents the Recaller, I denotes the cross-entropy error function, w signifies all\nweight parameters of the Memorizer-Recaller network, and b denotes all bias parameters of the Memorizer-Recaller\nnetwork. The evaluation of the model was performed using accuracy. The Recaller outputs $\\hat{v_i}$ for each input $k_i$, and it was\nconsidered correct if this value equaled $v_i$. This calculation was repeated 1,024 times, the sample size, and the average\nvalue was taken as the final accuracy. Since this is a 10-class classification problem, the value would be 0.1 if the Re-\ncaller made random predictions. We used the Adam [7] as the parameter optimization method, with all hyperparameters,\nincluding a learning rate of 0.001, set to the default values in TensorFlow 2.2.0.\nThe results of the training are listed in Table 1. For the training, the training and validation datasets were randomly\ngenerated. In other words, there is no discernible rule or pattern that the machine learning model can discover in each\ndataset. Therefore, to derive the correct answer in the validation phase, the Recaller must refer to the memory vector\ngenerated throughout the memorization phase. The training was terminated when the accuracy in the training phase\nreached 0.8. In this case, we stopped the training based on the accuracy on the training dataset. The reason is that this\nmodel exhibited overfitting, where the accuracy on the validation dataset did not improve at all, while only the accuracy\non the training dataset increased. Table 1 displays the number of epochs required and the accuracy of the validation\ndataset. Here, an epoch refers to the unit of time during which the models process all the data, that is, 1,024 sets of\nsampling data in this experiment. As mentioned earlier, each sampling dataset consists of N pairs of keys and values. As\na result of the training, the accuracy in the training phase was 0.8; however, the accuracy in the validation phase was still\nlow; thus, we could not build a sufficiently good model.\nIt was impossible to construct a robust model by the training because the model overfitted the training dataset and\nadapted excessively to it. In other words, this model learned only the characteristics of the training dataset consisting"}, {"title": "3.2.2 Learning with Random Values", "content": "The objective of a Memorizer-Recaller network is to imitate two operations: memorizing input values to the Ap-\npendable Memory vector, and searching for necessary information from the memory and recalling it. In the conventional\nmachine learning training method described in the previous section, a model learns the characteristics of the data and\nattempts to understand the patterns inherent to them. In contrast, this study aims to construct a system that can memorize\nand recall information. The proposed system in the study does not search for patterns inherent to the data but is a type of\nmnemonic system to learn how to memorize information.\nIn this study, we removed the patterns from the learning data to prevent the model from recognizing patterns inherent\nin the data and instead enable it to learn how to operate. Specifically, we probabilized $k_i$, which is the input value to\nthe Memorizer and Recaller, and $v_i$, which is the input value to the Memorizer and teacher data for the Recaller. During\nthe learning process, each of the 16 elements of $k_i$ were randomly generated from a continuous uniform distribution,\nwith parameters ranging from 0 to 9 for each epoch. Only the element of the value vector was generated from a discrete\nuniform distribution with a minimum value of 0 and a maximum value of 9. We used randomly generated data in the\nlearning process, and the aim was to ensure that random data do not have any patterns. The objective of the study was\nto train a Memorizer-Recaller network that could acquire the skills of remembering and retrieving information from\nmemory, rather than simply learning the underlying patterns of data.\nThe results of learning using this method are listed in Table 2. The learning process ended when the validation\naccuracy reached 0.8. In general, the accuracy of the validation dataset is lower than that of the training dataset. If this\nis not the case, abnormal learning settings may be suspected. However, because both the training dataset and validation\ndataset in this learning method are random values, if the learning progresses normally in the training dataset, the accuracy\nin the validation dataset improves simultaneously, and this behavior is normal.\nWe conducted training while varying the volume of data N to be memorized from 2 to 9. Until N reached 8, the\naccuracy reached 0.8 for both the training and validation datasets. In other words, the trained Memorizer-Recaller was\nable to derive the correct answer in a validation dataset by learning from a training dataset where both the training and\nvalidation dataset were randomly generated and thus had no patterns. However, when N was 9, despite training the model\nfor 500,000 epochs, the accuracy did not reach 0.8, indicating that the learning process did not proceed successfully. In\nother words, the Memorizer-Recaller constructed under these learning conditions could memorize at most 8 pieces of\ninformation.\nNext, to demonstrate the performance of the Memorizer-Recaller, we sequentially input 8 sets of key and value data\ninto the Memorizer to generate a memory vector, as illustrated in the upper panel of Figure 3. Subsequently, we conducted\na test using all keys to determine whether the values could be retrieved using the memory vector and Recaller. The"}, {"title": "3.3 Development of Sorting Algorithm", "content": "Next, we explored whether a sorting algorithm can be generated using the Memorizer-Recaller network. As the\nMemorizer-Recaller network can memorize the input information in a memory vector, we expected that it should gener-\nate a sorting algorithm. The rationale behind this hypothesis is as follows: the Memorizer has the capability to sequen-\ntially add information to the Appendable Memory, and the Recaller can retrieve information stored in this memory. By\ndesigning the training data in a specific manner, as described in the following paragraph, we anticipated that it might be\npossible to train the Recaller to retrieve the memorized information in ascending order. Consequently, the Memorizer\ncould potentially store the input values in the Appendable Memory while considering their magnitude. As a result, the\nentire Memorizer-Recaller network could generate a sorting algorithm.\nThe structure of the network used in the experiment is illustrated in Figure 4. Most structures are the same as those\nin Figure 2, but the input data of the Recaller differ from key to query. This query is an integer between 0 and N \u2013 1.\nZero is the prompt for the Recaller to output the smallest number among the N input numbers; one is the prompt for the\nRecaller to output the second smallest number among the N number of inputs; and N \u2013 1 is the prompt for the Recaller\nto output the largest number among the N number of inputs. Additionally, the data structure of the key was changed from\nprevious experiments. In previous experiments, the key consisted of a vector of 16 floating-point numbers. However, in\nthis experiment, since we aim to construct a model that solves the problem of sorting real numbers, the key value was\nset to a single floating-point number. The model sorts these input floating-point numbers. The results of sorting the input\nvalues were obtained by arranging the output results. For example, we used the input value of the Memorizer for N = 5,\nas shown in the left panel of Figure 5. The floating-point numbers are the keys to be sorted, and bold characters are the\nvalues. The keys are randomly generated from a uniform distribution with a minimum value of 0 and a maximum value\nof N. The Recaller-output sorted sequence for the input data is depicted in the right panel of Figure 5 panel. The queries\nto the Recaller from top to bottom are 0, 1, 2, 3, and 4.\nLearning was conducted in the same manner as described in the subheading 3.2.2. It was terminated when the vali-\ndation accuracy reached 0.95. By incrementally increasing the value of N from 2, we checked whether learning could be\ncompleted normally. We observed that this could be achieved within 500,000 epochs up to N = 8, which was consistent\nwith the results presented in the previous section. Although this value of N can vary marginally depending on the stop-\nping condition of learning, discussing whether the value of N is 8, 7, or 9 is meaningless. However, it can be thought that\nthis extent of memory can only be retained by the Memorizer-Recaller network under the current condition of parameter\nsize and architecture design.\nOf the above training on various values of N, we conducted an experiment to prove that the N = 8 model can correctly\nsort input. In the experiment, an output was considered correct only when the order of the elements was perfectly sorted.\nThe test was performed 1,024 times, after which the network could accurately sort in 893 trials. Therefore, the accuracy"}, {"title": "4 Discussion", "content": "The aim of this study was to construct a new learning method, which enables AI to learn how to operate, rather than\njust learning patterns in a training dataset. As an application of the proposed learning method, we developed a system\nin which the AI can memorize information even after the model has been deployed. Usually, AI acquires knowledge\nby updating parameter values during machine learning process. The knowledge of AI is essentially embedded in these\nparameters. On the other hand, the AI that we envision as capable of acquiring knowledge after deployment refers to\nan AI that can acquire information without changing its parameters after the machine learning process that involves\nparameter updates. The proposed Memorizer-Recaller network can store input information in the Appendable Memory\nvector separately and retrieve it using the Recaller. The memory vectors generated by the Memorizer can be saved and\nutilized subsequently. This memory is a dynamic memory to which new information can be appended. In other words,\ninformation can be added to this memory after the machine learning process involving parameter updates, i.e., after\ndeployment. This means that the AI can acquire knowledge after deployment. Furthermore, the Recaller can restore\ninformation from these memory vectors. We attempted to construct a sorting algorithm using this network, which also\nsucceeded in demonstrating that the Memorizer could correctly encode the input information into the memory vector and\nthat the Recaller could appropriately utilize that information.\nThe proposed learning method probabilized the training dataset for each epoch and introduced randomness into the\nlearning process. This prevents a model from learning the inherent patterns of the training dataset. This aspect is a novel\nidea that can be achieved using random numbers in the learning method.\nThe Memorizer-Recaller network was developed to allow AI to store information, and we succeeded in memorizing\n8 pieces of information. The objective of the study is to examine whether we can develop a system capable of acquir-\ning information even after deployment using the learning method we proposed. Therefore, the number is not a primary\nconcern in the study. However, when actually applying this system for a specific purpose, its limited memory capacity\ncould narrow the scope of its application. As mentioned in subheadings 3.1 and 3.2.2, the Memorizer is a type of RNN.\nA simple RNN has a forgetting phenomenon where it cannot retain past information. Similarly, the Memorizer expe-\nriences some form of forgetting, just like a simple RNN. The initially developed simple RNN [9] has been gradually\nimproved, and Long Short-Term Memory (LSTM) [10] was developed as a type of RNN with reduced forgetting. It\nmay be possible to increase the amount of information that the Memorizer can remember by incorporating mechanisms\nsimilar to those introduced in the development of LSTM. For example, as conducted in a study [11], one possible method\nis to randomly generate the structure of the Memorizer and evaluate their performance to select a structure that reduces\nthe forgetting phenomenon. Further investigation will be necessary to improve the memory capacity of the Appendable\nMemory system.\nThe desired operation with the Memorizer can be realized by ANNs. By considering past contextual information,\nANNs can receive sequential values as input and output values. In this study, we performed a similar operation using the\nMemorizer. However, one key distinction between the Memorizer and conventional ANNs is that ANNs cannot append\ninformation to a memory vector.\nCertain dialogue agents can acquire new knowledge after deployment and have the capability to make conversations\nbased on past interactions stored in their memory. For example, large language models (LLMs) currently deployed\ntypically have this capability. LLMs remember previous questions and their own responses, and act as if they remember\npast conversations by using a naive method where they concatenate this information with newly input questions [12].\nTherefore, one of the major challenges in current LLM development is to increase the input sequence length that LLMs\ncan handle [13]. In contrast, the Appendable Memory system developed in this study memorizes past information into a\nsingle vector and recalls the memory from there. As a future application, the Appendable Memory system may be used\nto improve the performance of dialogue agents as shown in Figure 6. The combination of key and value corresponds to\na statement by a particular user, and the memory vector represents the accumulation of these statements stored in the AI\nsystem. The key represents some piece of information, and the value is an explanation of this information. The role of a\nRecaller is to recall the value corresponding to a key based on its memory. If an AI can have this function, it would be\nable to accumulate knowledge from users or even web searches and grow without changing or updating the parameters\nof the neural network after deployment."}, {"title": "5 Conclusion", "content": "This study developed a learning method using randomness of datasets and constructed a system for AI to acquire\nknowledge even after the model has been deployed. The resulting system is called the Appendable Memory system,\nconsisting of the Memorizer-Recaller network, which generates the Appendable Memory. The proposed learning method\nprobabilized the training dataset for each epoch, thereby prohibiting a model from learning patterns inherent in the\ntraining dataset. This differentiates our work from previous learning methods. The advantage of Appendable Memory is\nthat new knowledge is stored in a finite memory vector. This means that the original multiple pieces of information are\nencoded into a single vector of finite size. If the size of the encoded information is smaller than the original information, it\nmeans that compression of information is achieved. Since Appendable Memory is generated by this mechanism, there is\nno need to retain past information in its original size or format. The Memorizer-Recaller network developed in this study\nsuccessfully stored separate input information in the Appendable Memory vector and retrieved it using the Recaller.\nHowever, if we apply our system to real-world tasks, the capacity of the memory vector will be a problem, where\nour system can memorize only up to 8 pieces of information. This poses a significant constraint when applying the\ntechnology to real-world scenarios. Therefore, further investigations are required to improve the performance. Despite\nthe preliminary results of this study, the method to train Memorizer-Recaller is fundamentally different from previously"}]