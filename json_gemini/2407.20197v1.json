{"title": "Learning Random Numbers to Realize Appendable Memory System for Artificial Intelligence to Acquire New Knowledge after Deployment", "authors": ["Kazunori D Yamada"], "abstract": "In this study, we developed a learning method for constructing a neural network system capable of memorizing data and recalling it without parameter updates. The system we built using this method is called the Appendable Memory system. The Appendable Memory system enables an artificial intelligence (AI) to acquire new knowledge even after deployment. It consists of two Als: the Memorizer and the Recaller. This system is a key-value store built using neural networks. The Memorizer receives data and stores it in the Appendable Memory vector, which is dynamically updated when the AI acquires new knowledge. Meanwhile, the Recaller retrieves information from the Appendable Memory vector. What we want to teach AI in this study are the operations of memorizing and recalling information. However, traditional machine learning methods make AI learn features inherent in the learning dataset. We demonstrate that the systems we intend to create cannot be realized by current machine learning methods, that is, by merely repeating the input and output learning sequences with AI. Instead, we propose a method to teach Al to learn operations, by completely removing the features contained in the learning dataset. Specifically, we probabilized all the data involved in learning. This measure prevented AI from learning the features of the data. The learning method proposed in the study differs from traditional machine learning methods and provides fundamental approaches for building an AI system that can store information in a finite memory and recall it at a later date.", "sections": [{"title": "Introduction", "content": "In this study, we propose a novel learning method to teach an artificial intelligence (AI) to learn how to operate. Specifically, we trained the AI to memorize information and to recall that information. The operations performed by the Al developed with our learning method are independent of the property of the data used for training, allowing the AI to memorize new information and utilize it even after deployment.\nThe development of an AI, which continues to learn even after deployment, is a challenging problem in the field of AI research [1]. AI that can continue to learn post deployment must possess the following abilities:\n(1) Memorizing: the ability to retain information that has been supplied thus far in memory data and to add the newly supplied information to existing memory data without compromising the already-memorized information,\n(2) Recalling: the ability to freely extract necessary information from memory data.\nIn this study, the AI system we aim to develop is one that can memorize and recall information without any additional learning processes involving parameter updates after deployment. We intend to teach AI two operations: memorizing and recalling.\nSpecifically, the system we aim to construct in the study is shown in Figure 1. We henceforth refer to this system as the Appendable Memory system. The Appendable Memory system consists of a series of two AIs: the Memorizer and the Recaller. During the memorization phase, the Memorizer receives multiple pieces of information and generates an Appendable Memory, which encapsulates various pieces of information. The Appendable Memory allows for the addition of new information. During the recall phase, the Recaller uses this memory vector and a piece of data that triggers the system to restore information from the memory vector. The Memorizer and the Recaller are already trained AIs that do not update their parameters when memorizing or recalling information. Instead, the Appendable Memory changes each time the Memorizer adds new information to it.\nNext, we consider actually training this system, but this cannot be achieved with current machine learning methods. This is because machine learning merely teaches AI to elucidate features inherent in learning datasets, not to learn operations. In the memorization phase of Figure 1, the input vector is composed of keys and values. Imagine a scenario where these keys and values have no predictable pattern, meaning they are random. In such a case, if the Recaller is trained repeatedly, it might improve prediction performance only for the training dataset. This is because the Recaller only needs to remember all combinations of keys and values in the dataset. However, if a completely new key is input, the Recaller can only return a random value, based on the premise that there is no rule in the combination of keys and values. We demonstrate this by an experiment. Through the experiment, we demonstrate that, while current machine learning methods can extract patterns inherent in data, they cannot teach AI to learn operations, that is, how to remember or recall information.\nOriginally, machine learning methods fit AI to features inherent in a dataset. However, the system we intend to build cannot be realized by the usual machine learning methods. We want the Appendable Memory system not to learn features in the dataset. Therefore, to delete predictable patterns in the dataset, we probabilized the data contained in the dataset. By generating all data randomly at each learning step, namely epoch, we prevented AI from learning features in the training dataset. As a result, we succeeded in designing a system that can encode multiple pieces of information independently into a memory vector and restore the information freely.\nIn this article, we will first discuss the limitations of current machine learning methods, then propose a machine learning method using random numbers to realize a memory to which new information can be appended, and finally describe applications that utilize Appendable Memory to implement a sorting algorithm."}, {"title": "Related Work", "content": "The method we propose in this study involves probabilizing the learning data during neural network training. Typically, supervised machine learning methods teach AI to learn the features of data in the training dataset. Such AI makes predictions based on the features of the training dataset. In contrast, the AI we intend to build is one capable of memorizing and recalling information, regardless of any properties in the training dataset. Therefore, to teach AI to only learn the operations, we probabilized the dataset for each parameter update during learning, to remove its inherent features.\nThere are several methods that apply stochastic concepts for AI learning. The most basic is using random numbers for parameter initialization. A survey paper [2] introduced various methods that utilize randomness. However, there is no research and development on probabilizing the dataset itself.\nUsually, stochastic gradient descent (SGD) is used in neural network training [3]. SGD involves randomly shuffling and batching the training dataset. This is not the same as randomly generating the training dataset itself, as we do in this study. SGD only shuffles the training dataset and samples portions of the dataset for each parameter update randomly.\nFor other types of neural networks such as generative adversarial networks (GANs) [4] and diffusion models [5], the concept of randomness is used for the learning process. However, these differ from our method. Generative models like GANs and diffusion models utilize random noise to ensure the diversity of the generated data. In contrast, our method uses randomness not to teach the AI the features inherent in the dataset, but to teach it operations. Reinforcement learning can be used to make models acquire operations instead of learning patterns in the data, and therefore the underlying mechanism of the method we developed might be similar to that of reinforcement learning in this regard."}, {"title": "Experiments", "content": "This study proposes an AI system for storing information within a single, modality-unrestricted finite-sized vector and retrieving information from it by utilizing a neural network resembling a recurrent neural network (RNN) and an encoder-decoder network, referred to as Memorizer-Recaller. The proposed system can generate memory data, an Appendable"}, {"title": "Network Architecture", "content": "Memory vector, which is updated dynamically when the AI acquires new knowledge. The learning method in this study differs from traditional machine learning methods and provides fundamental approaches for building an AI system that can store information within finite medium and can freely utilize them in the future.\nThe aim of this study is to train the Memorizer-Recaller network such that the Memorizer can store N pairs of vectors into a memory vector, while the Recaller outputs the corresponding value from the memory vector along with one key vector, as shown in Figure 1.\nHere, we presented the formulas of the models. The left arrow (\u2190) is a symbol that assigns the value on its right to the variable on its left. The first model, that is, the Memorizer, is represented by the following formula:\n$u_t \u2190 f(k_t, v_t)$, (3.1)\n$p \u2190 \u03c3(w_1u_t + b_1)$, (3.2)\n$q \u2190 \u03c3(w_2m_{t-1} + b_2)$, (3.3)\n$m_t \u2190 \u03c3(w_3(p + q) + b_3)$, (3.4)\nwhere w denotes the weight parameter, b is the bias parameter, and \u03c3 is the activation function. We used LeakyReLU [6] as the activation function. In addition, f is a function that concatenates two vectors, t is an integer from 1 to N, $k_t$ is the t-th key, $v_t$ is the t-th value, and $m_t$ is the t-th memory vector. In the Memorizer, the (t-1)-th memory vector is used to generate the t-th memory vector, and the Memorizer has a recurrent loop in its structure; therefore, the Memorizer is a type of RNN. The variable p in the final formula is derived from the input data, whereas q is derived from the (t \u2212 1)-th memory vector. The memory vector includes both previous memory information and new input information. Because calculating the elementwise summation of p and q is essential, these vectors must have identical dimensions. We perform computations for all N data points during the memorization phase to obtain $m_N$. The second model, that is, the Recaller, is represented by the following formula:\n$r \u2190 \u03c3(w_4k_i + b_4)$, (3.5)\n$s \u2190 \u03c3(w_5m_N + b_5)$, (3.6)\n$v_i \u2190 f(r, s)$, (3.7)\n$v_i \u2190 \u03c3(w_6v_i + b_6)$, (3.8)\n$\\hat{v_i} \u2190 softmax(w_7V_i + b_7)$, (3.9)\nwhere \u03c3 is the activation function, and we used the softmax function, because in the experiment the Recaller is used to classify and predict values from 0 to 9. In addition, $k_i$ is any one of the N keys used as the input to the Memorizer. In the recall phase, it attempts to output $v_i$ corresponding to $k_i$.\nFigure 2 illustrates the Memorizer-Recaller network. In the figure, the structure on the left is the Memorizer, and the one on the right is the Recaller. The Memorizer accepts pairs of key and value vectors as inputs and outputs a memory vector. Repeating this calculation for all N input values ultimately generates a memory vector that is expected to contain all input information. The Recaller accepts a single key and the memory vector generated by the Memorizer as input and outputs the value corresponding to the key.\nThe objective to train the Memorizer-Recaller network is to develop an AI system that can realize the storage and retrieval of multiple pieces of information, as shown in Figure 1. In this study, we have not discussed the performance enhancement of the system; therefore, we did not include any luxury mechanisms, which are generally used to improve the performance of neural networks. We designed the system with a simple combination of layers, as illustrated in the figure. The source codes used to generate all the networks in this study are available in a GitHub repository, https://github.com/yamada-kd/Memorizer-recaller.git."}, {"title": "Learning Process of the Models", "content": "Firstly, we trained the model in a standard learning manner. Various hyperparameters were used to train the Appendable Memory system in the standard manner and these will be explained before proceeding to explain the training methodology. The size of the intermediate layer of both the Memorizer and the Recaller was set to 256. Similarly, the dimension of the memory vector, which is the output of the Memorizer, was set to 256. Because the Recaller is a predictor that outputs integers from 0 to 9, the dimension of its output vector was set to 10. In addition, the dimension of the key vector was set to 16. Each element of the key vector is a floating-point number arbitrarily generated from a continuous uniform distribution, followed by parameters with a minimum value of 0 and maximum value of 9. The value vector is one-dimensional, and its only element is an integer randomly generated from a discrete uniform distribution with a minimum value of 0 and a maximum value of 9. The volume of data to be input to the Memorizer is N; N is variable for the experiment. For training, 1,024 combinations of these N input data were generated and used in the batch learning"}, {"title": "Learning in a Standard Manner", "content": "method. During the learning process of the Memorizer, the value of t was varied from 1 to N, the memory vector $m_t$ was calculated, and finally, $m_N$ was computed. For $m_0$, random numbers generated from a uniform distribution following the parameters with a minimum value of -1 and a maximum value of 1 were used. Because the softmax function was used in the output layer, the cross-entropy error function was used as the cost function. The parameters were updated when the error between the value and teacher data was calculated after inputting $m_N$ and each of the N keys to the Recaller to output the predicted value. Therefore, the cost function L was computed as follows:\n$m_t \u2190 M(k_t, v_t, m_{t-1})$, (3.10)\n$v_i \u2190 R(k_i, m_N)$, (3.11)\n$L(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} l(v_i, \\hat{v_i})$, (3.12)\nwhere M represents the Memorizer, R represents the Recaller, l denotes the cross-entropy error function, w signifies all weight parameters of the Memorizer-Recaller network, and b denotes all bias parameters of the Memorizer-Recaller network. The evaluation of the model was performed using accuracy. The Recaller outputs $\\hat{v_i}$ for each input $k_i$, and it was considered correct if this value equaled $v_i$. This calculation was repeated 1,024 times, the sample size, and the average value was taken as the final accuracy. Since this is a 10-class classification problem, the value would be 0.1 if the Recaller made random predictions. We used the Adam [7] as the parameter optimization method, with all hyperparameters, including a learning rate of 0.001, set to the default values in TensorFlow 2.2.0.\nThe results of the training are listed in Table 1. For the training, the training and validation datasets were randomly generated. In other words, there is no discernible rule or pattern that the machine learning model can discover in each dataset. Therefore, to derive the correct answer in the validation phase, the Recaller must refer to the memory vector generated throughout the memorization phase. The training was terminated when the accuracy in the training phase reached 0.8. In this case, we stopped the training based on the accuracy on the training dataset. The reason is that this model exhibited overfitting, where the accuracy on the validation dataset did not improve at all, while only the accuracy on the training dataset increased. Table 1 displays the number of epochs required and the accuracy of the validation dataset. Here, an epoch refers to the unit of time during which the models process all the data, that is, 1,024 sets of sampling data in this experiment. As mentioned earlier, each sampling dataset consists of N pairs of keys and values. As a result of the training, the accuracy in the training phase was 0.8; however, the accuracy in the validation phase was still low; thus, we could not build a sufficiently good model.\nIt was impossible to construct a robust model by the training because the model overfitted the training dataset and adapted excessively to it. In other words, this model learned only the characteristics of the training dataset consisting"}, {"title": "Learning with Random Values", "content": "The objective of a Memorizer-Recaller network is to imitate two operations: memorizing input values to the Appendable Memory vector, and searching for necessary information from the memory and recalling it. In the conventional machine learning training method described in the previous section, a model learns the characteristics of the data and attempts to understand the patterns inherent to them. In contrast, this study aims to construct a system that can memorize and recall information. The proposed system in the study does not search for patterns inherent to the data but is a type of mnemonic system to learn how to memorize information.\nIn this study, we removed the patterns from the learning data to prevent the model from recognizing patterns inherent in the data and instead enable it to learn how to operate. Specifically, we probabilized $k_i$, which is the input value to the Memorizer and Recaller, and $v_i$, which is the input value to the Memorizer and teacher data for the Recaller. During the learning process, each of the 16 elements of $k_i$ were randomly generated from a continuous uniform distribution, with parameters ranging from 0 to 9 for each epoch. Only the element of the value vector was generated from a discrete uniform distribution with a minimum value of 0 and a maximum value of 9. We used randomly generated data in the learning process, and the aim was to ensure that random data do not have any patterns. The objective of the study was to train a Memorizer-Recaller network that could acquire the skills of remembering and retrieving information from memory, rather than simply learning the underlying patterns of data.\nThe results of learning using this method are listed in Table 2. The learning process ended when the validation accuracy reached 0.8. In general, the accuracy of the validation dataset is lower than that of the training dataset. If this is not the case, abnormal learning settings may be suspected. However, because both the training dataset and validation dataset in this learning method are random values, if the learning progresses normally in the training dataset, the accuracy in the validation dataset improves simultaneously, and this behavior is normal.\nWe conducted training while varying the volume of data N to be memorized from 2 to 9. Until N reached 8, the accuracy reached 0.8 for both the training and validation datasets. In other words, the trained Memorizer-Recaller was able to derive the correct answer in a validation dataset by learning from a training dataset where both the training and validation dataset were randomly generated and thus had no patterns. However, when N was 9, despite training the model for 500,000 epochs, the accuracy did not reach 0.8, indicating that the learning process did not proceed successfully. In other words, the Memorizer-Recaller constructed under these learning conditions could memorize at most 8 pieces of information.\nNext, to demonstrate the performance of the Memorizer-Recaller, we sequentially input 8 sets of key and value data into the Memorizer to generate a memory vector, as illustrated in the upper panel of Figure 3. Subsequently, we conducted a test using all keys to determine whether the values could be retrieved using the memory vector and Recaller. The"}, {"title": "Development of Sorting Algorithm", "content": "Next, we explored whether a sorting algorithm can be generated using the Memorizer-Recaller network. As the Memorizer-Recaller network can memorize the input information in a memory vector, we expected that it should generate a sorting algorithm. The rationale behind this hypothesis is as follows: the Memorizer has the capability to sequentially add information to the Appendable Memory, and the Recaller can retrieve information stored in this memory. By designing the training data in a specific manner, as described in the following paragraph, we anticipated that it might be possible to train the Recaller to retrieve the memorized information in ascending order. Consequently, the Memorizer could potentially store the input values in the Appendable Memory while considering their magnitude. As a result, the entire Memorizer-Recaller network could generate a sorting algorithm.\nThe structure of the network used in the experiment is illustrated in Figure 4. Most structures are the same as those in Figure 2, but the input data of the Recaller differ from key to query. This query is an integer between 0 and N \u2013 1. Zero is the prompt for the Recaller to output the smallest number among the N input numbers; one is the prompt for the Recaller to output the second smallest number among the N number of inputs; and N \u2013 1 is the prompt for the Recaller to output the largest number among the N number of inputs. Additionally, the data structure of the key was changed from previous experiments. In previous experiments, the key consisted of a vector of 16 floating-point numbers. However, in this experiment, since we aim to construct a model that solves the problem of sorting real numbers, the key value was set to a single floating-point number. The model sorts these input floating-point numbers. The results of sorting the input values were obtained by arranging the output results. For example, we used the input value of the Memorizer for N = 5, as shown in the left panel of Figure 5. The floating-point numbers are the keys to be sorted, and bold characters are the values. The keys are randomly generated from a uniform distribution with a minimum value of 0 and a maximum value of N. The Recaller-output sorted sequence for the input data is depicted in the right panel of Figure 5 panel. The queries to the Recaller from top to bottom are 0, 1, 2, 3, and 4.\nLearning was conducted in the same manner as described in the subheading 3.2.2. It was terminated when the validation accuracy reached 0.95. By incrementally increasing the value of N from 2, we checked whether learning could be completed normally. We observed that this could be achieved within 500,000 epochs up to N = 8, which was consistent with the results presented in the previous section. Although this value of N can vary marginally depending on the stopping condition of learning, discussing whether the value of N is 8, 7, or 9 is meaningless. However, it can be thought that this extent of memory can only be retained by the Memorizer-Recaller network under the current condition of parameter size and architecture design.\nOf the above training on various values of N, we conducted an experiment to prove that the N = 8 model can correctly sort input. In the experiment, an output was considered correct only when the order of the elements was perfectly sorted. The test was performed 1,024 times, after which the network could accurately sort in 893 trials. Therefore, the accuracy"}, {"title": "Discussion", "content": "The aim of this study was to construct a new learning method, which enables AI to learn how to operate, rather than just learning patterns in a training dataset. As an application of the proposed learning method, we developed a system in which the AI can memorize information even after the model has been deployed. Usually, AI acquires knowledge by updating parameter values during machine learning process. The knowledge of AI is essentially embedded in these parameters. On the other hand, the AI that we envision as capable of acquiring knowledge after deployment refers to"}, {"title": "Conclusion", "content": "This study developed a learning method using randomness of datasets and constructed a system for AI to acquire knowledge even after the model has been deployed. The resulting system is called the Appendable Memory system, consisting of the Memorizer-Recaller network, which generates the Appendable Memory. The proposed learning method probabilized the training dataset for each epoch, thereby prohibiting a model from learning patterns inherent in the training dataset. This differentiates our work from previous learning methods. The advantage of Appendable Memory is that new knowledge is stored in a finite memory vector. This means that the original multiple pieces of information are encoded into a single vector of finite size. If the size of the encoded information is smaller than the original information, it means that compression of information is achieved. Since Appendable Memory is generated by this mechanism, there is no need to retain past information in its original size or format. The Memorizer-Recaller network developed in this study successfully stored separate input information in the Appendable Memory vector and retrieved it using the Recaller.\nHowever, if we apply our system to real-world tasks, the capacity of the memory vector will be a problem, where our system can memorize only up to 8 pieces of information. This poses a significant constraint when applying the technology to real-world scenarios. Therefore, further investigations are required to improve the performance. Despite the preliminary results of this study, the method to train Memorizer-Recaller is fundamentally different from previously"}]}