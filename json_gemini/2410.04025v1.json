{"title": "IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback", "authors": ["KEVIN PU", "K. J. KEVIN FENG", "TOVI GROSSMAN", "TOM HOPE", "BHAVANA DALVI MISHRA", "MATT LATZKE", "JONATHAN BRAGG", "JOSEPH CHEE CHANG", "PAO SIANGLIULUE"], "abstract": "Research ideation involves broad exploring and deep refining ideas. Both require deep engagement with literature. Existing tools focus primarily on idea broad generation, yet offer little support for iterative specification, refinement, and evaluation needed to further develop initial ideas. To bridge this gap, we introduce IDEASYNTH, a research idea development system that uses LLMs to provide literature-grounded feedback for articulating research problems, solutions, evaluations, and contributions. IDEASYNTH represents these idea facets as nodes on a canvas, and allow researchers to iteratively refine them by creating and exploring variations and composing them. Our lab study (N = 20) showed that participants, while using IDEASYNTH, explored more alternative ideas and expanded initial ideas with more details compared to a strong LLM-based baseline. Our deployment study (N = 7) demonstrated that participants effectively used IDEASYNTH for real-world research projects at various ideation stages from developing initial ideas to revising framings of mature manuscripts, highlighting the possibilities to adopt IDEASYNTH in researcher's workflows.", "sections": [{"title": "1 INTRODUCTION", "content": "Research ideation often starts with an initial spark of an idea [6]. However, the subsequent process of iteratively developing initial ideas, often high-level and under-specified, into a well-specified and structured research idea grounded by literature is equally important to the success of a research project [14, 22, 44, 46]. Unlike initial research ideation, this process is often multi-faceted with researchers explore and consider many variations of research questions, methods, evaluation criteria, and contributions framing. Through this process, researchers iteratively converge on a specific, well-explored, high-impact, and feasible research idea [22, 46]. However, this process is effortful, because it often involves both understanding of literature to generate variations for different facets [27, 29], and also to compose different idea facets together to form a coherent research idea [6]. Recent work has shown great promise of leveraging Large Language Models (LLMs) to improve the initial stages of broad research ideation [26, 30, 34, 37, 38, 42]. For example, tools that support literature sensemaking and discovery [26], generating new research questions and ideas [30, 42], or critiquing and evaluating ideas [39, 42]. However, these tools focused on supporting the initial idea generation and evaluation phase, and do not account for the further iteration and deeper development phase of research ideation. As a result, the process of focused expansion and distilling initial research ideas into a concrete and specific research idea is left largely unsupported by prior work. In this work, we investigate the user needs and design opportunities to support researchers in the expansion and refinement stages of research ideation. Through a formative study with eight researchers, we identified three common pain points in researchers' workflows. First, researchers struggled with expanding initial ideas to concretely operationalize them into an executable project, which also hindered their ability to evaluate their ideas' novelty and feasibility. Second, researchers felt unsupported in organizing and evaluating multiple versions and iterations of their ideas. Third, researchers who have tried to use off-the-shelf LLMs for research ideation often did not find the feedback helpful because it lacked the level of depth and specificity needed for refining and iterating on ideas. We address these three challenges by introducing IDEASYNTH, a research idea expansion and refinement system that facilitates developing and experimenting with different variations of a research idea. IDEASYNTH features a canvas interface that allows users to externalize variations of different idea facets-such as research problem descriptions, proposed methods, and evaluation approaches-as nodes on the canvas. IDEASYNTH'S LLM-powered idea refinement leverages the context of a user-controlled scientific literature collection to help the user refine individual facets and strengthen connections between facets. Users can also create different idea paths that connect different facets to generate a research brief1 To evaluate IDEASYNTH, we conducted a controlled lab study and a field deployment study where participants used IDEASYNTH to develop research ideas. In the lab study (N = 20), 12 HCI and 8 NLP researchers developed ideas using IDEASYNTH and a strong baseline system that shared a subset of IDEASYNTH'S features. In addition, seven of"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Scientific Research Ideation process Scientific research ideation is a non-linear complex process consisting of multiple scientific tasks and activities [16]. To come up with a new idea, researchers have to identify and prioritize interesting research problems, form ideas to address selected problems, compare the idea with existing literature, integrate new knowledge to make sense of the relevant information space, and develop an evaluation plan [21]. These tasks appear in different phases of a research project and involve different cognitive processes. In the beginning phase of scientific research ideation, researchers mostly engage in problem identification and prioritization. This early phase can involve \"opening processes\" of information-seeking which includes both intentional and serendipitous starting points [16]. Researchers generate and evaluate initial research questions [21]. Like typical ideation process, the initial phase is associated with the divergent stage [35] where researchers can benefit from exploring various research questions before committing to a selected few [30, 31]. The outcome of this phase is mostly a set of prioritized research questions that will be further explored. After researchers decide on an idea to pursue, they start shaping the idea into a research brief, sometimes called a research proposal. In this paper, we define research brief as a description of a research project before any execution (e.g., running an experiment, implementing a system). Inie et al. identifies different types of research ideas: a research question"}, {"title": "2.2 Research ideation support with Al", "content": "2.2.1 Initial Idea Generation. Recent work on ideation informs us of the potential and limitations of leveraging Als for ideation support. For example, prior work suggests that LLMs or humans with access to LLMs can generate more novel ideas [36, 42, 50] and higher quality ideas [17] than humans who do not use LLMs. While the majority of prior work in this space suggests that providing AI support during ideation is beneficial to users [17, 20, 21, 30, 36], a few also pointed to potential risks of incorporating AI in ideation, such as generating less original ideas [47]; less diverse ideas [2, 47]; and more fixation [47]. In sum, prior work points to how carefully design UIs and interactions may be keys to successful integration of Als into the ideation process, and that failure to do so may incur severe adverse effects. More directly related to our work, multiple Al systems and interventions have been tailored to support the initial broad ideation of research ideas where \"opening processes\" happen. CoQuest helps researchers generate research questions with an LLM-based agent [30]. Many prior systems provided users with relevant inspirations instead of generating the ideas directly. For example, helping users find relevant research analogies as inspirations [24]; generating personalized inspirations using LLMs with an evolving knowledge graph [18]. However, most of prior systems only focused on supporting initial broad ideation instead of further developing them into a concrete research brief. For example, CoQuest only supports generating research questions, but does not consider potential solutions nor how to evaluate them to demonstrate impact and contribution. 2.2.2 Idea Convergence, Evaluation, and Refinement. Some systems focus support in the converging stage for evaluating research ideas. Prior work has proposed ways to provide users with feedback for generated ideas. Shaer et al. explores the application of LLMs to the converging phase of brainwriting by using LLMs to help evaluate the ideas based on their novelty, relevance, and insightfulness [39]. Liang et al. and D'Arcy et al. separatedly shows that language models can generate helpful feedback on scientific papers [13, 28]. Lu et al. reports that automated paper evaluation has higher inter-rater agreement than human evaluation [32]. Shen et al. provides AI explanation dialogues to support human-AI scientific writing [41]. These findings suggest the potential for automatically evaluating ideas. However, there are contradicting findings that caution against relying too heavily on automated idea assessment. LLMs still cannot evaluate research ideas consistently [42]. Moreover, evaluating research ideas is a highly subjective task that can be affected by evaluators' preferences and experience. Human experts do not always agree on the novelty of a research idea [42]. Higher inter-rater agreements of LLM ratings [32] do not mean that the generated ratings capture the intrinsic quality of an idea. Moreover, the format of research ideas that get evaluated in prior work are mostly short text description [30] or a writeup on the formal level of a published paper [10, 28]. Informed by these prior work, instead of producing evaluation scores, IDEASYNTH gives actionable suggestions that help users decide on what to do to improve their ideas."}, {"title": "3 FORMATIVE STUDY", "content": "We conducted a formative study with 8 HCI and NLP researchers (5M, 3F) both to understand the challenges in existing ideation workflows and also to identify design opportunities for enhancing research ideation with AI. The study consists of a walk-through of participant's existing artifact of a past or ongoing research project (e.g., research project proposals and running project notes documents), followed by a live ideation session using a Wizard-of-Oz (WoZ) prototype that leveraged an off-the-shelf research support tool. Finally, we conclude the study with a semi-structured post-interview. We recruited Ph.D. students and postdoctoral researchers through our organization's Slack channels and through personal connections. We focused our recruitment on researchers who were actively involved in project ideation and could provide detailed accounts of their workflows. Demographic details of our participants can be found in Appendix A. We selected participants on a first-come, first-serve basis, and kept recruitment open during our study until we reached data saturation. All studies were facilitated by two members of the research team and conducted virtually via Google Meet. Each study was around 60 minutes in length and was divided into 3 parts. To ground the study to real-world research projects, prior to the study, we asked participants to submit a project document-a running artifact with project updates, ideas, links, meeting notes, and more. This artifact was the central focus of Part 1 of the study, where we asked participants about how they sourced and developed their ideas, challenges with ideation, and their use of AI tools in research. In Part 2, participants completed an interactive activity with a WoZ design probe. We leveraged Google Docs\u00b2 and Perplexity AI\u00b3 to power the \u201csystem interface\u201d and \u201cLLM-based intelligent support\" aspects of our WoZ study, respectively. Specifically, Google Docs' real-time collaboration features enabled the study facilitator to populate content to the shared document in real-time from a different computer; and Perplexity AI's ability to search the web and provide functional links for academic papers could help us better understand the design opportunities of a research ideation system connected to an academic database. The probe's interface consisted of two side-by-side Google Docs. One of"}, {"title": "3.1 Challenge 1: Expanding Initial Ideas to Concrete Projects", "content": "While prior works on ideation emphasized on supporting broad ideation and exploring diverse directions [30, 39], participants from our study also point to scenarios where they were already settled on a broad direction but struggle to further develop them into a concrete research project. Further, many said that they often do not lack interesting broad initial research ideas to pursue. However, they pointed to how it is often more challenging to expand their initial ideas due to uncertainty over how to concretely operationalize them. Participants also highlighted difficulties in measuring the novelty of their ideas as they often have broad topics of interests but without specific idea facets (e.g., designs and methods, how to evaluate, specific impacts, and broader contributions) to solidify the project. P2 felt \"initially there's a lack of clarity into what the proposed approach would look like so it's hard to talk about it in a very concrete way. Because I am not sure what the core components of my approach will be. It's hard to even express it in writing.\" Moreover, the process of narrowing down a broad range of thoughts to an operationalized project idea is inherently difficult. P3 expressed that \"narrowing it [initial idea] down to the one operationalizable research hypothesis...is what takes the maximum amount of effort.\" Understanding existing literature deeply is crucial for evaluating ideas but poses significant challenges. As one participant noted, starting a project without a comprehensive literature review makes it hard to confidently discuss existing gaps (P2). The overwhelming volume of related works make it even more difficult to ensure all relevant literature is covered (P4). Researchers often feel insecure about the novelty and impact of their ideas, relying on validation from peers and advisors, who may point out limitations in impact or feasibility (P5)."}, {"title": "3.2 Challenge 2: Exploring Variations and Iterations of Ideas", "content": "When further developing an initial idea into a concrete research project brief, participants reported a common strategy of exploring variations and different framings. However, they faced challenges in evaluating multiple versions and iterations of their ideas. P6 often developed early prototypes of several ideas, setting aside those that do not work for later reconsideration. They often present multiple variations to peers and advisors to get feedback, which allowed them"}, {"title": "3.3 Challenge 3: Receiving Useful Support from LLMs", "content": "Reflecting on interacting with our WoZ prototype during the study and their own experience using LLM-based tools for ideation, participants felt that existing tools do not provide a useful level of specificity in their feedback. For example, P5 felt that the AI often reiterates the same points without adding substantial new insights, failing to clarify concepts or identify specific details needed to refine ideas. P4 proposed that researchers would benefit from AI providing fair and critical feedback, asking research questions, and offering critiques rather than simply affirming ideas. Furthermore, using AI for literature review tends to lead to reiterations of known information without providing the motivational context or the \"why\" behind certain approaches, which is crucial for ideation (P3). Interaction paradigm with AI is another area of concern. While most participants (n = 5) agreed that conversational interaction with Al is useful, typing natural language requests may not always be the preferred interaction method. Instead, participants desired alternative designs where AI support could be more active and in-situ within their artifacts and workspaces but without being disruptive (P2, P5). Additionally, AI suggestions often lack context or history, making it hard for researchers to see and understand the relevance of the suggestions (P7). In terms of agency and ownership, participants highlighted that they prefer using AI to inspire ideas, assist in study design, and spark further development rather than generating final outputs (P3, P5)."}, {"title": "3.4 Design Goals", "content": "Based on the challenges identified by our formative study, we formalize three design goals corresponding to each of the challenges listed above: \u2022 DG1: Better support for further developing an early stage research idea into a concrete research brief through reflection and evaluation. (Challenge 1). We observed that researchers needed support with idea expansion rather than idea generation. We thus aim to provide better tools to make ideas more detailed and concrete so that researchers can operationalize them. \u2022 DG2: Help users explore and evaluate different variations of an idea by analyzing their strengths and weaknesses and visualizing the connections between ideas facets (Challenge 2). Researchers found it difficult to manage idea iterations and fragmented notes while working in linear artifacts (e.g., documents, slides). New, non-linear interfaces may be needed to help researchers visualize and make sense of relationships between various idea components. \u2022 DG3: Enhance the specificity and relevance of LLM support by grounding the response in relevant information in literature and focusing on inspiring users to develop their idea further (Challenge 3). Researchers often found"}, {"title": "4 IDEASYNTH", "content": "Based on our design goals, we present IDEASYNTH (Fig. 2), a research idea development system that helps scholars expand and refine their initial ideas. The system represents idea facets as linked nodes on a canvas, visualizing the logic relationships and forming a tree-like information structure for the user's ideation context (DG2). IDEASYNTH also provides contextualized feedback at the node level and for the overall idea canvas based on the collected papers (DG3), enabling users to iteratively enrich ideas, explore alternatives, and connect facets to create a coherent research brief (DG1). We chose the research brief as the system's output due to its real-world utility in communicating and refining research projects, as it encapsulates core ideation activities such as defining a problem, method, and hypothesis."}, {"title": "4.1 User Scenario and System Walk-through", "content": "In this section, we use a user scenario to walk through various features and design of IDEASYNTH."}, {"title": "4.1.1 Literature-based broad ideation.", "content": "An HCI researcher is brainstorming a new project on using AI to assist with fictional character development in creative writing. She begins with a literature review, which helps refine her research question to: \"How can we use Al to generate interesting fictional characters for writers?\u201d. However, she faces challenges in narrowing the project scope to propose novel contributions and worries about prematurely focusing on a potentially underdeveloped idea."}, {"title": "4.1.2 Decomposing initial idea into idea facet nodes and adding relevant papers.", "content": "The researcher decides to use IDEASYNTH to further develop and refine her research idea. She first created a \"Problem Description and RQ\" node on the canvas interface and entered her research question (Fig.4.1). This node represents a \"facet\" of the idea. Then, using the paper search function, she adds the papers she collected during her literature review into the paper collection tray (Fig.3.12). This way, IDEASYNTH could better understand her mental model and provide more contextualized feedback as she continue to develop her idea. She also uses the \"Recommend papers\" button to find additional relevant papers and expand her literature review (Fig.3.2). To obtain an overview of the collected papers, she generates an AI literature summary and analysis (Fig.3.\u2463\u2464). The system provides a one-paragraph summary and a tailored analysis that draws connections between nodes on the canvas and specific sections of relevant papers. From the summary, she gains a deeper understanding of the challenges around evaluation from a prior work that also focused on AI writing assistants and explored mechanisms of using AI to drive fictional character development. Click on the paper icons in the summary opens the referenced papers in the collection tray for further reading. Beyond showing connections between prior work and the canvas, the literature analysis also provided actionable next step suggestions for expanding the ideas on the canvas. For example, she could drag a suggestion block onto the canvas to generate a chain of nodes that represent a new idea. For now, she decides to first explore and expand upon her initial research question."}, {"title": "4.1.3 Further developing the idea by exploring facet node variations.", "content": "Returning to her initial RQ facet node, she writes down two more detailed research questions in the node: \"How to design a feedback mechanism that helps writers develop fictional characters?\u201d and \"What are the impacts on the writing process and the outcome of the created characters?\u201d (Fig.4.1). She uses \"Get AI Suggestions\" to receive feedback. IDEASYNTH first presents a clarifying question to ask the researcher to specify what aspects of the writing process she would like to focus on, with examples such as initial character creation or continuous character development throughout writing. The suggestions also reference relevant papers with findings on how writers seek feedback. Based on this suggestion, she iterates on her research question to center around \"character creation aligned with a plot narrative vision in early writing\". She also reads a suggested paper in-depth and gains insights on existing writer feedback mechanisms. She then toggles to view another AI suggestion. This time, IDEASYNTH proposes an alternative research question to consider how to balance Al creativity and authorship, a key issue illustrated by a paper she had previously added to the system. The researcher finds the research question inspiring as she has previously not considered the trade-off in authorship. This time, she uses \"Generate Alternatives\" feature to create new nodes that explore variations of this suggested research question. She incorporates the generated node on investigating the author's creative control and authorship and connects it with her first node (Fig.4.2)."}, {"title": "4.1.4 Creating connections between nodes to explore their relations.", "content": "She then uses the downward arrow menu at the bottom of the two \"Problem Description and RQ\" nodes to explore other idea facets (i.e. \"Proposed Design and Solution\", \"Evaluation Method\", and \"Contribution and Impact\"). First, she uses IDEASYNTH to generate multiple \"Proposed Design and Solution\" nodes with the prompt \u201cinteractive AI system for character building with author feedbcak\" (Fig.4.3)."}, {"title": "4.1.5 Generating research briefs by exploring different node compositions.", "content": "Using IDEASYNTH's tools, the researcher refines node content with Al suggestions, deepens her literature understanding through summary and Q&A capabilities, and broadens her research ideas by exploring alternative approaches via distinct idea facets, ultimately resulting in a more well-defined project. After decomposing her initial idea into different facet nodes and generating variations for each, she feels she has sufficiently explored the problem space. She then selects a path of idea facet nodes that encapsulates the research project to generate a research brief on the right-side panel. The brief compiles these facets into a cohesive document, with the corresponding nodes highlighted (Fig.6). The researcher shares the research brief with her collaborators to further discuss this project idea. She can also return to the canvas to develop multiple clusters of ideas, creating multiple research briefs to continue ideation. The canvas space, then, functions as a mind map and preserves the ideation context within this broad topic."}, {"title": "4.2 Node-based Idea Canvas", "content": "IDEASYNTH is built around a node-based canvas, where each node represents a distinct idea facet. We specifically define four types of idea facets: Problem Description and Research Question, Proposed Design and Solution, Evaluation Methods, and Contribution and Impact. These idea facets are chosen as prior works have commonly identified them as critical components that represent the research idea in various formats, such as research paper [9, 23, 33], research proposal [44], or abstract [3], across different scientific domains. We omitted other potential facets such as evaluation results, or limitations as they cannot be concretely described in early research ideas without execution. Users can select the different idea types via a drop-down-menu (Fig.5.1) as they see fit. The node representation decomposes the research idea into a digestable format for iteration and evaluation. Each node on the canvas offers specific functionalities designed to facilitate interaction. Users can freely edit the node title and content (Fig.5.2). They can also link nodes together to form logical connections between them. Each edge is color-coded to represent the strength of the connection (color gradient: red=weak, green=strong) and can be expanded"}, {"title": "4.3 Methods for Generating Literature-Grounded Feedback", "content": "To provide contextualized feedback for the user's ideas like shown in the scenario, IDEASYNTH implements an LLM-powered system that grounds its generation based on the collected papers (DG3). The system allows users to construct a"}, {"title": "4.4 Implementation Details", "content": "IDEASYNTH is implemented as a web application. The backend was implemented in Python using Flask. The frontend used the React framework in Typescript. We used gpt-4-turbo and LangGraph 6 to instantiate an LLM instance with memory of past interactions and generations. All LLM prompts used in the system are included in Appendix I."}, {"title": "5 STUDY 1: COMPARATIVE LAB STUDY", "content": "To evaluate the usability and utility of IDEASYNTH, we first conducted a within-subjects laboratory study where participants developed research ideas using IDEASYNTH and a baseline system. We formulated the following research questions: \u2022 RQ1: How does the node-based canvas interface allow users to explore and connect different idea facets? \u2022 RQ2: How does the externalization of idea facets help the expansion and refinement of initial research ideas? \u2022 RQ3: How do LLM-generated suggestions grounded on collected papers help the user understand the literature space and evaluate their research ideas? \u2022 RQ4: How does the user experience of research ideation using IDEASYNTH compare to using a standard LLM-based tool?"}, {"title": "5.1 Study Design", "content": "5.1.1 Participants. We recruited 20 researchers (11M, 8F, 1 non-binary) who are pursuing or have completed their Ph.D. degrees (i.e., graduate students and postdocs) in Computer Science through posting recruiting messages in Slack channels of several research institutes. We targeted scholars who were actively engaging in research ideation and project development. In total, twenty participants were recruited, of which 12 were in the field of HCI and 8 in the field of NLP. Thirteen participants have 2-5 years of research experience, six have more than 5 years of research experience, and one has less than 1 year of experience."}, {"title": "5.1.2 Conditions.", "content": "The study adopts a within-subjects design with two conditions: IDEASYNTH and a strong baseline. Simulating a common practice reported by participants in the formative study, the baseline system implements a word editor? where participants can develop their research ideas. To further control the task goal, the baseline editor provided an initial writing template containing section headings corresponding to the idea facet categories in IDEASYNTH. The baseline condition also contained a subset of features in IDEASYNTH. Specifically, a literature review panel (Fig.3.left column) with identical paper search and recommendation features to support literature understanding. The baseline system also provided LLM assistance through the same Q&A interface. The user can ask targeted inquiries regarding the collected papers or free-form questions about their research ideas similar to using tools like ChatGPT. The two systems share identical underlying LLM setup and system prompt. The LLM in the baseline is supplemented with the same paper metadata and the writing editor content as context. Additionally, the baseline contains an \"AI Assist Writing\" button in the editor, which will generate a revised research brief based on the user's writing and attached it at the end of the editor. This function uses the same prompt (\u00a7I.6, but with editor content as context) as the \"Generate Research Brief\" function in IDEASYNTH that transforms idea facet nodes into a coherent writing."}, {"title": "5.1.3 Tasks.", "content": "Each participant was asked to develop research ideas based on two provided broad topics. For HCI researchers, we identified a main theme of human-AI interaction and chose two task topics: \"Al-augmented Tool for Creativity\" and \"Human-AI Collaboration in Data Anlaysis\". For NLP researchers, we chose the theme of LLM evaluation, with the two task topics being \"LLM-as-a-Judge\" and \"Biases in LLMs\". The task themes cover a wide range of concepts and specific domains to provide sufficient freedom for researchers in different areas to develop research ideas. We did not let participants ideate on their existing research projects as there might be variance in the stage of ideation between projects and participants."}, {"title": "5.1.4 Procedure.", "content": "We control for the learning and ordering effects by counterbalancing the system condition and task topic order (2 \u00d7 2) independently for the HCI participant group (n = 12) and the NLP participant group (n = 8). The participants were first given a brief introduction on the goal of the study and a task instruction sheet (Appendix E). For each task, they first watched a short tutorial of the assigned system (IDEASYNTH or baseline), then spent 30 minutes to develop their research idea on the assigned task topic. When there were 5 minutes left, participants were reminded to make final changes to their idea and converge on a research brief as the task deliverable. After each task, participant filled out a survey to evaluate the quality of their research idea and record their perception about the use experience and the task workload. At the end of both tasks, we conducted a semi-structured interview where participants reflected on their overall experience and compared the ideation process using both prototypes. The study was conducted remotely using video conferencing software. Each session lasted around 90 minutes, and participants were compensated for $75 USD for their time. The study was approved by our internal review board."}, {"title": "5.2 Measures and Analysis Procedure", "content": "We collected both users' perception and behavioral data to conduct a comprehensive analysis on the usage of IDEASYNTH. Quantitatively, we analyzed the post-task survey responses. The survey consists of 7-point Likert scale questions on the user's perception towards their level of literature understanding, idea refinement, and idea expansion they were able to achieve during the task. Additionally, the survey includes questions from NASA-TLX [19] on perceived workload, trust, and five questions asking the participants self-assess the quality of their final research idea in five"}, {"title": "5.3 Results", "content": "5.3.1 IdeaSYNTH helped users better explore alternative ideas and navigate the idea space (RQ1). Based on the survey response, participants felt they more sufficiently explored and evaluated alternative versions of their research ideas in IDEASYNTH (mean = 5.40, SD = 1.50) than the baseline (mean = 3.65, SD = 1.60, p < 0.01), as shown in Fig 7. H3, H10, H12, and N3 stated that the baseline system, due to its editor-like interface, did not encourage exploring alternative ideas but pushed them to develop their initial idea in depth. However, the lack of exploration could lead to idea fixation (H3). In contrast, fourteen participants mentioned that the support for suggesting and generating alternative nodes in IDEASYNTH led them to explore more ideas. H1, H2, H6, and H11 attributed the enhanced process of exploration to the structured layout with horizontal levels of nodes for each idea facet, visualizing the connections between ideas. N6 thought IDEASYNTH help them \"find the connection between two nodes and this feature can help you discover something you didn't recognize before.\" The ability to evaluate multiple generated idea facets based on collected papers broadened participants' perspectives (H1, H12). H1 claimed that \"since [IDEASYNTH] actually generated new ideas and suggested some papers based on some of my prompts, it actually broadened my understanding on the area.\" Sometimes, the generation of alternative ideas inspired users with new ideas they haven't considered. H8 reflected that through using IDEASYNTH, their research idea \"grew into providing more details or sometimes, an angle that I hadn't thought about.\" H1 described that the node AI suggestions and the generated alternative nodes \u201csomehow challenged my opinion [with questions like] \u2018have you thought of this, have you thought of that' and suddenly I noticed that I didn't touch base on this part, so I should tweak my previous belief a little bit more. And for that part, I think the influence is quite big and also I think it's quite significant for ideation.", "more out of my scope and out of my expectations. So that's good. Not necessarily means that those ideas are good but still it elicits some new thoughts in me. That's also indirectly helping me to find some good directions.": 2}, {"title": "5.3.2 IdeaSynTH helped users expand and iterate on ideas (RQ2).", "content": "Almost all participants (n = 17) reported that they were able to more effectively expand their initial research idea with additional details with IDEASYNTH. This was supported by the survey responses (IDEASYNTH: mean = 6.05, SD = 0.89, baseline: mean = 4.45, SD = 2.04, Fig.8). Many participants (n = 11) attributed this to being able to break down their ideas into multiple facets using the node representation. N6 found the node and canvas interface \u201cmore novel to help automatically concrete the research ideas and evaluation methods.\" H12 claimed that \"when I'm thinking about newer research project, [the node canvas] helps me think through different components.\" Upon connecting facets to compose them, H7 felt more motivated to expand their idea: \"once you have that initial idea to start with, that's when I can see potential for a new node, ... I can imagine each node being a research question and other nodes which are connected to it through edges, sort of motivating [as] that's when I would actually pay attention to the connections.\u201d Comparatively, H3 said that IDEASYNTH \u201cwas more targeted in a sense that they [ideas] were separated into the specific things that I had to include in the research brief so I can iterate on one component much easier,\" whereas the baseline \"just provided general summary that was very general.\"\""}, {"title": "5.3.3 Similar literature search behaviors but IdeaSynth's facet-specific summaries and suggestions can surface unexpected connections to prior work (RQ3).", "content": "In general, participants (n = 12) found the literature review panel in both conditions helpful for exploring related works and gaining an overview of the papers to provide a foundation for research ideation. We did not find a significant difference in users' confidence in understanding the literature in the post-task survey across two conditions. The behavioral logs also showed that participants spent similar percentages of their task duration searching and reading paper information (IDEASYNTH = 9.75%, Baseline = 11.7%), and we did not identify a significant difference in the number of papers collected (IDEASYNTH = 5.60, Baseline = 6.05, p = 0.63). This is perhaps unsurprising as both systems included the same literature search features. However, some participants (n = 7) found IDEASYNTH, with the addition of literature summary and analysis generation at both the canvas-level and node-level, more helpful compared to the baseline. IDEASYNTH summarized the collected papers based on the defined idea facets. H7 found it helpful to seek literature support for ideating specific facets in their ideas: \"I think that's what I need when I'm brainstorming or ideating. I am confident enough in my methods to not need help...but I think even then this is summarizing motivation...that was pretty helpful.\" N4 commented that \u201cthe option to generate a summary of various paper and...being able to see some connection between the two papers is very interesting and not something that I saw before.", "[IDEASYNTH] was much better at not just giving you a summary of papers ... it's sort of connecting the dots for you.\u201d IDEASYNTH additionally presented relevant paper information for individual nodes. H4 commented that \"just being able to explore different types of suggestions was super useful...be able to toggle through and see how it was bringing up some of the literature that I had collected.": 5, "IDEASYNTH": "after you click on the left, it could provide you [with"}]}