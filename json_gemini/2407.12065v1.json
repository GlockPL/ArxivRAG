{"title": "Data selection method for assessment of autonomous vehicles", "authors": ["Linh Trinh", "Ali Anwar", "Siegfried Mercelis"], "abstract": "As the popularity of autonomous vehicles has grown, many standards and regulators, such as ISO, NHTSA, and Euro NCAP, require safety validation to ensure a sufficient level of safety before deploying them in the real world. Manufacturers gather a large amount of public road data for this purpose. However, the majority of these validation activities are done manually by humans. Furthermore, the data used to validate each driving feature may differ. As a result, it is essential to have an efficient data selection method that can be used flexibly and dynamically for verification and validation while also accelerating the validation process. In this paper, we present a data selection method that is practical, flexible, and efficient for assessment of autonomous vehicles. Our idea is to optimize the similarity between the metadata distribution of the selected data and a predefined metadata distribution that is expected for validation. Our experiments on the large dataset BDD100K show that our method can perform data selection tasks efficiently. These results demonstrate that our methods are highly reliable and can be used to select appropriate data for the validation of various safety functions.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous vehicles today are not only confined to the re- search labs but they are becoming more prevalent in the real- world. Recently, many automotive Original Equipment Man- ufacturers (OEMs) launched their commercial autonomous vehicles, such as Tesla, Alphabet, Waymo, and so on [1], [2], [3]. To standardize autonomous vehicles worldwide, self- driving is usually divided into 6 levels, from 0 (zero self- driving) to 5 (full self-driving) [4] where each level consists of a set of Advanced Driver-Assistance System (ADAS) or Automated Driving System (ADS) features such as Adap- tive Cruise Control (ACC), Lane Keeping Assist (LKA), Highway Pilot (HWP), and others [5], [4]. Following many standards such as ISO 26262, 21448, ANSI/UL 4600 [6], [7], [8], [9], ISO 15622 [10], ISO 17361 [11], ISO 11270 [12], ISO 19237 [13], ISO 22839 [14], and national regulators such as the USA NHTSA, EURO NCAP [5], [15], ASEAN NCAP [16], autonomous vehicles must pass certain test cases and scenarios on the public road before being granted the permission to drive in the real world. Data is crucial in the functioning of autonomous vehicles as they heavily depend on vast quantities of it for validation purpose and to successfully implement autonomous vehicles in real-world scenarios. Since self-driving is a safety-critical application, validation requires a diverse set of testing scenarios that are representative of driving. Many OEMs, such as Alphabet, Tesla, and Audi, etc., have collected a large amount of public road data [1] for validating safety of their high- level autonomous vehicles [2]. But collecting data on the public road can generate a huge amount of data on a daily basis. Processing all this data for validation purposes is an extremely exhausting and impractical effort. Hence it is more important to query a subset of the data which is smaller than the original dataset to accelerate validation process. The Society of Automotive Engineers (SAE) has established the Operational Design Domain (ODD) as a framework for determining the scope of validation test cases and scenarios conducted on public roads. ODD then becomes a funda- mental condition when validating an ADAS/ADS system of autonomous vehicles [7], [8], [9], [5]. Several recent works attempted to define scenario identification in terms of corre- sponding ODD to pass validation, such as [2], [1]. However, because public road validation is highly manual, it is heavily reliant on human effort [17]. Moreover, the large amount of data presents a significant challenge for self-driving system validation. Hence, locating and selecting a smaller set that covers the expected ODD is an effective way to reduce human effort while increasing productivity and efficiency. Furthermore, data selection logically implies data reduction. To save storage space, data may need to be discarded after collection or application completion. However, because data collection on public roads is prohibitively expensive, the data should be separated into useful information that can be used later. Mining these examples can be based on a variety of criteria, including unseen conditions, objects or scenes, or examples where self-driving systems underperformed. Hence, a flexible and dynamic data selection approach is required for validation of ADAS/ADS features of au- tonomous vehicle. However, to the best of our knowledge, there is a lack of research or algorithm development pertain- ing to the aforementioned objectives. Recently, several com- mercial services, such as ADaaS\u00b9, Scale Nucleus2, dSPACE IVS3, and many others provide data curation services on data collected on the road. These services, on the other hand, primarily facilitate data querying based on user-defined filters which are relevant to ODD list. However, these commercial services are disclosed, with no explicit method or algorithm described. In this paper, we propose a data selection method for selecting subset of the data to validate multiple self- driving features. Our main idea is to perform the selection using the dataset's metadata. In more detail, we optimize the similarity of selected data's metadata distribution to the"}, {"title": "II. RELATED WORKS", "content": "In reality, self-driving systems are required to validate not only the correctness of their system, but also their safety. Many standards such as ISO 26262, 21448, ANSI/UL 4600 [6], [7], [8], [9], ISO 15622 [10], ISO 17361 [11], ISO 11270 [12], ISO 19237 [13], ISO 22839 [14] or national regulators such as NHTSA [5], Euro NCAP [15], [3], ASEAN NCAP [16] require ADAS/ADS system of autonomous vehicles to pass the validation on the public road for any SAE self- driving levels [4]. These standards and regulations provide the specification of test cases for autonomous vehicle safety validation, which typically consists of certain stages: data collection, event extraction (e.g. scenario, tagging, etc.), data selection for each test case, and verification or validation. In recent years, many OEMs have focused on collecting public road data for validation. For example, Waymo has accu- mulated 5 million miles since 2018, Cruise collected over 770,000 miles in 2020 [18], [19], BMW has been collecting more than 230 PB data from more than 100 vehicles since 2019 [17]. As stated by SAE [4], the ODD refers to the specific set of operating conditions that a particular driving automation system or its feature is designed to function within. This includes various factors such as environmental conditions, geographical limitations, time-of-day restrictions, and specific traffic or roadway characteristics. In simplicity, ODD comprises of a collection of high-level domains as well as details on static tags and dynamic events known as scenarios. Many prior works concentrated on building algorithms for extracting scenarios from captured data, such as clustering-based scenario extraction [20], corner-cases scenario finding [21], traffic scenario extraction [22], tags extraction [23]. In addition to these techniques, self-driving fleets can perform scenarios manually by triggering specified events during test driving [17]. In the following stage, data for each scenario needs to be curated for validation or verification. Data curation is typically accomplished using metadata tags or scenarios that have already been retrieved and saved alongside the acquired data, as defined by ODD [17]. There are two types of data selection: frame-level and scenario-level. The majority of selection processes are performed manually by humans, which is related with the manual validation process [17]. Due to the enormous volume of collected data and the limited availability of validation resources, the output of data selection is typically smaller than the original dataset. Furthermore, with a high number of test cases, the selection operation must run often. A few works describe methods for selecting scenarios for a specific test case, such as [24], [25]. Sadat et al. [26] describe a dataset selection approach that uses infrastructure, traffic participants, and driving maneuvers to measure the level of interest in traffic scenes. The goal of this method is to improve the performance of deep learning models used in self-driving perception tasks. Bogdoll et al. [27] proposed an algorithm for selecting corner case scenarios. Unfortunately, to the best our knowledge, there is a lack of scientific study on data selection to support the validation of autonomous vehicles. Several commercial services, such as the above- mentioned ADaaS, Scale Nucleus, dSPACE IVS, and so on, provide data curation services on data collected on the road for fleet testing or holomogation. These services, on the other hand, primarily facilitate data querying based on user-defined filters such as ODD condition filtering. Moreover, these methods do not provide an explicit algorithm or methodology inside. We then propose a method to better support flexible and dynamic selection for validating autonomous vehicles in the next section."}, {"title": "III. METHOD", "content": "This section describes the details of our method. Our primary idea for scenario selection for autonomous vehicle verification and validation is to use the dataset's metadata. Metadata can be broadly defined as the compilation of information from multiple sources in a dataset. For example, the dataset includes environmental data like weather and road types, as well as dynamic traffic participants like cars, buses, trucks, and pedestrians. Similar to ODD, we consider metadata to be constructed from domains, each of which might contain a set of distinct categories (or tags). Several domains of metadata can coexist independently. For example, data collected on public roads may include weather informa- tion in addition to road information; the weather domain is divided into four categories: wet, sunny, foggy, and cloudy. Similarly, there are five domain object categories: vehicle, bus, truck, pedestrian, and motorcycle. We assume that in the early stages of any data-driven validation task, experts have an expectation of a metadata distribution of interest. For instance, when choosing a subset of data to verify a lane assistant keeping feature, the priority is given to selecting roads with lanes rather than dirt or gravel roads. Therefore, the goal is to ensure that the selected data contains lane- roads. The predetermined expectation of metadata distribu- tion is denoted as E, which $E \\in R^{D \\times C}$ where D, and C denote the number of distinct domains, and the number of categories inside each domain, respectively.\n$S_c(A,E) = 1 - \\frac{||A - E||}{D}$ (1)\n$S_d(A,E) = 1 - \\sum_{i=1}^{D} ( \\frac{||A_i - E_i||}{\\frac{E}{D}} )$ (2)\n$d_i = ( \\frac{||A_i - E_i||}{\\frac{E}{D}} )$ (3)\nwith $\\phi(a)$ is $min(1, a)$ for simplicity. While the domain- based metric indicates how good the selection method is when compared to the average of domains, the category- based metric indicates how good the selection method is when compared to the average of categories. These metrics can be used to measure the quality of selection directly when the selection was done. If both of these metrics are higher then it indicates a better match with expected E. The framework in Figure 1 illustrated the workflow of our data selection. The metadata can be obtained by using the metadata extraction module F, which can be a set of functions such as map API to obtain geographic information, car information (e.g. acceleration, velocity, throttle info) from CAN, weather information query, and so on. The output of metadata extraction F for each sample x\u2081 is the series of metadata mi \u2208 MD associated with this sample xi, where MD denotes all extracted metadata of the entire data D. Each mi contains M interested metadata tags, m1, m2, ..., \u0442\u043c. Metadata mi can be a series of metadata tag by time, with the value associated with each metadata tag m; set to 1 if it occurred and 0 otherwise. Each extracted metadata tag mj by F can be represented as a series in time, for example, a highway tag of data occurred every 5 seconds. To summarize the extracted metadata m\u2081 of each sample xi, we use a transformation function to transform metadata to a ratio list. The ratio of each metadata tag mj can be the duration of this metadata tag over the duration of the data sample x\u2081. All metadata distribution {(m\u2081)|i = 1,.., N} will be inputed into a scoring model G(0;) for calculating the important score of data xi, which is later used for selection decision, where @ is the parameter of model G. The calculated score will be used by a selection function \u03a8\u03c1, which seeks to keep a rate p of data. To simplify, we use the function \u03a8, as the selection function, which selects the top p items from the ranking of D. The ranking can be used to determine the level of significance or interest, as well as diversity and complexity. Thus, we select \u03a8 as the descending rank in the scoring function G. For example, when comparing two data points, x1 and x2, the data point with the higher score is more likely to be retained. Generally, if G(0, \u03a6(mp)) > G(0, \u03a6(mk)), then x\u0127 is more likely to be retained than xk."}, {"title": "IV. EXPERIMENT", "content": "In this section, we run extensive experiments to assess the effectiveness of our proposed method.\nDataset. We use BDD100K [31] video, a very large open self-driving dataset which provided GPS signal for extracting metadata. This dataset contains 100,000 videos collected across the United States of America in the more than 300 days. Each video is about 40 seconds long and 30 frames per second. In this experiment, our goal is to select a subset of videos from the entire video dataset. The videos also include GPS and IMU data captured by cell phones. As the metadata query from OpenStreet map service [32] via GPS, we implement the metadata extraction function FM. We extract the 8 domains and their categories using GPS, as shown in Table II. Because some GPS positions from cell phones cannot be queried in Openstreet map, therefore out of 100,000 videos, 72,197 videos with metadata totaling 2,861,030 seconds were successfully extracted. The extracted metadata are publicly available. The ratio between the duration of all the videos in the category with the total duration represented in all the categories is represented as the Original column in the Table II. Our method. For model training, we use a G model with two layers of 128 hidden layers each. The batch size K is set at 1024. We train our model with the Adam optimizer and a learning rate of 0.01. The model was trained for T = 120 epochs. We set the initialized \u20ac to 0.9 and \u03b7 = 0.85. We set the selection ratio p in a wide range in several experiments. Due to lack of scientific methods and studies on selecting data for validating autonomous vehicles, we chose the work of selecting a diversity and complexity data set based on geographical information [26] (DC).\n$min_\\theta C (G(\\theta; M_D), E)$ (4)\n$arg min_\\theta L (G(\\theta; M_D), E)$ (5)\n$D(m_i, m_j) = D(\\Phi(m_i), \\Phi(m_j)) = \\frac{\\Phi(m_i) . \\Phi(m_j)}{|\\Phi(m_i)||\\Phi(m_j)|}$ (6)\nLimitations. The goal of our selection method is to optimize the representation of the selected dataset to match the predefined expected ratio. The expected ratio must be provided based on the human knowledge depending on the application. A poor predefined expected ratio may result in poor performance on downstream applications."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we present our proposed data selection method for validation of variety of ADAS/ADS features in autonomous vehicles. We introduced our framework for data selection based on the metadata of data set. Furthermore, we proposed an algorithm to train a model which is used to optimize the similarity of metadata distribution of selected data and a predefined metadata distribution that is expected for a validation task. Additionally, we provide two metrics which can be used to evaluate the quality of data selection, and guide the data selection algorithm. In experiment, we used a large self-driving dataset BDD100K which consists of near thousands hours driving. Experiment results of data selection compared with other methods indicates that our method is efficient, highly reliable and can be used for validation of a variety of self-driving safety functions."}]}