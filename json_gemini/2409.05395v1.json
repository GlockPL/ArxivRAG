{"title": "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "authors": ["Georgios Pantazopoulos", "Malvina Nikandrou", "Alessandro Suglia", "Oliver Lemon", "Arash Eshghi"], "abstract": "This study explores replacing Transformers in Visual Language Models (VLMs) with Mamba, a recent structured state space model (SSM) that demonstrates promising performance in sequence modeling. We test models up to 3B parameters under controlled conditions, showing that Mamba-based VLMs outperforms Transformers-based VLMs in captioning, question answering, and reading comprehension. However, we find that Transformers achieve greater performance in visual grounding and the performance gap widens with scale. We explore two hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual encoding on the updates of the hidden states, and 2) the difficulty in performing visual grounding from the perspective of in-context multimodal retrieval. Our results indicate that a task-aware encoding yields minimal performance gains on grounding, however, Transformers significantly outperform Mamba at in-context multimodal retrieval. Overall, Mamba shows promising performance on tasks where the correct output relies on a summary of the image but struggles when retrieval of explicit information from the context is required\u00b9.", "sections": [{"title": "Introduction", "content": "Modern Visual Language Models (VLMs) (Bai et al., 2023a; Li et al., 2024; Alayrac et al., 2022) typically treat patch representations from vision encoders (Radford et al., 2021; Fang et al., 2023; Zhai et al., 2023) as tokens that are mapped to the embedding space of a Transformer-based Large Language Model (LLM). This patch-as-token approach has fostered the development of VLMs that have achieved unprecedented performance on established Vision & Language (VL) on many coarse-grained tasks, for example, image captioning (Lin et al., 2014) or visual question answering (Goyal et al., 2017; Hudson and Manning, 2019). However, fine-grained tasks such as localizing regions within an image (Peng et al., 2023b; Kazemzadeh et al., 2014), or reading text (Sidorov et al., 2020; Mathew et al., 2021) from the image are significantly more challenging for these models. These tasks require the model to grasp nuances within the image beyond summarizing the visual context in a few words as in conventional image captioning.\nA straightforward countermeasure is to scale up the resolution of images, allowing the VLM to \"see greater details\". (Liu et al., 2023b; Karamcheti et al., 2024; McKinzie et al., 2024). On the other hand, increasing the context length requires substantial overhead as Transformer-based VLMs have quadratic complexity with respect to the input. Structured state space models (SSMs) (Gu et al., 2022; Poli et al., 2023) have recently emerged, providing competitive performance against Transformers. Mamba (Gu and Dao, 2023) is a recent SSM that promises computational efficiency as well as performance that surpasses Transformer-based language models of similar size.\nIn this paper, we investigate whether a Mamba LLM is a competitive alternative to a Transformer across established multimodal tasks including both fine-grained and coarse-grained multimodal tasks. The choice of the LLM plays a crucial role for modern VLMs, as recent work (Lauren\u00e7on et al., 2024b) has shown that for a fixed number of total parameters, the quality of the language backbone has a higher impact than that of the vision backbone. More specifically, we train three Mamba-VL variants and compare them against Pythia-VL, a series of equally sized models that follow the established paradigm to train VLMs with a state-of-the-art Transformer-based LLM backbone (Biderman et al., 2023). Notably, the performance of Pythia-VL is comparable with that of existing VLMs, thus establishing it as a robust baseline model. We emphasize that both models are trained on the exact"}, {"title": "Related Work", "content": "Early works showcase the capabilities of LLMs combined with pretrained vision encoders, in VL tasks (Tsimpoukelli et al., 2021). Consequently, current VLMs (Bai et al., 2023b; Dai et al., 2024; Alayrac et al., 2022; Lauren\u00e7on et al., 2024b; Liu et al., 2024a; Chen et al., 2023b) are based on the same foundational formula: a visual expert (Zhai et al., 2023; Fang et al., 2023), a language backbone (Touvron et al., 2023; Jiang et al., 2023; Bai et al., 2023a; Team et al., 2024), and a connector between the two modules. The vast majority of these models are based on highly capable Transformer-based LLMs. In this work, while we do not modify this formula, we investigate the effect of replacing the Transformer LLM with Mamba."}, {"title": "Structured State Space Models", "content": "Structured state space sequence models (S4) are a family of models of sequence models using principles from RNNs, CNNs, and classical state space models that attempt to combat the limitations of Transformers in modeling long sequences (Fu et al., 2023; Poli et al., 2023; Gu et al., 2022; Smith et al., 2023). These models showcase convincing results in modeling long-range dependencies across several synthetic tasks (Tay et al., 2021). Previous research shows, in a controlled study of moderately sized models, that Transformers outperform S4 models in terms of language modeling (Arora et al., 2024). However, Mamba (Gu and Dao, 2023) builds upon previous S4 models by introducing a selective scan operation (Section 3.1) showing competitive performance against Transformers."}, {"title": "Mamba applications", "content": "Inspired by its results in sequence modeling, recent work applies Mamba to computer vision tasks, by introducing inductive biases that better match the domain of image encoding (Zhu et al., 2024a; Huang et al., 2024; Ruan and Xiang, 2024; Liu et al., 2024b). Within NLP, Jamba (Lieber et al., 2024) is a hybrid architecture with interleaved Transformer and Mamba blocks, while MambaByte (Wang et al., 2024), is a language model operating on bytes instead of subwords. To the best of our knowledge, there is not yet a comprehensive study showcasing the effectiveness of Mamba in multimodal settings. Concurrent work has applied Mamba in multimodal tasks (Zhao et al., 2024a; Qiao et al., 2024). However, these studies offer limited insights, because 1) they do not facilitate a fair comparison under controlled conditions, and 2) they do not investigate multimodal tasks that require both high-level and fine-grained information, such as visual grounding."}, {"title": "Transformers vs SSMs", "content": "The development of SSMs and similar RNNs (Katharopoulos et al., 2020; Fu et al., 2023; Peng et al., 2023a; Poli et al., 2023) with competitive performance, has motivated comparisons with Transformers. Recent studies (Park et al., 2024; Grazzi et al., 2024) show that SSMs can match the in-context learning performance of Transformers on certain tasks, but Aky\u00fcrek et al. (2024) demonstrate that Transformers retain an advantage for in-context language learning. Moreover, Merrill et al. (2024) provide theoretical and empirical evidence contrary to previous claims (Gu et al., 2021), showing that SSMs and Transformers have limited expressivity making them unsuitable for state-tracking problems. In terms of the in-context retrieval (e.g., copying) ca-"}, {"title": "VLM Approach", "content": "Figure 1 shows an overview of our model. We built our approach using the standard paradigm for VLMs that combine unimodal experts (Liu et al., 2024a; Alayrac et al., 2022; Dai et al., 2024). More specifically, our model consists of three individual components, a vision encoder, the Vision & Language connector, and the language backbone."}, {"title": "Preliminaries: The Mamba model", "content": "S4 models (Gu et al., 2022) take inspiration from Linear Time-Invariant (LTI) models that map a sequence x(t) \u2208 R \u2192 y(t) \u2208 R through a hidden state h(t) \u2208 RN. The output of an LTI model is computed in a two-stage format:\n\n$h'(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t)$\n\nS4 models first transform the continuous parameters (A, B) with a discretization step with \u039b parameters, into discrete parameters (A, B). Given the discrete parameters \u0100, B the discrete update is defined in recurrent form Equation (2a), or via the convolution form Equation (3a):\n\n$ht = \u0100ht\u22121 + Bxt$\n$Yt = Cht-1$\n\n$K = (CB, C\u0100B,..., C\u0100B)$\n$y = x * K$\n\nHowever, for language modeling S4 models underperform attention-based models (Arora et al., 2023). Gu and Dao (2023) empirically show that the time-independent parameters of an S4 model are not sufficient to select the correct information from their context as it is not straightforward how to reset the hidden state at each timestep. For this purpose, certain parameters of the Mamba model (A, B, C) are allowed to be functions of the input. With this change, hidden states can be updated in a selective fashion over the input \u2013 though due to violation of the convolution view (Equation (3a)), this requires a hardware-aware implementation to compute the hidden states efficiently. For additional implementation details of Mamba please see the original paper (Gu and Dao, 2023)."}, {"title": "Model Architecture", "content": "Figure 1 shows an overview of our model. We built our approach using the standard paradigm for VLMs that combine unimodal experts (Liu et al., 2024a; Alayrac et al., 2022; Dai et al., 2024). More specifically, our model consists of three individual components, a vision encoder, the Vision & Language connector, and the language backbone."}, {"title": "Vision Encoder", "content": "We use EVA-02-L336px/14 (Fang et al., 2023) to obtain high-quality visual representations. While previous work usually adopts CLIP models (Bai et al., 2023b; Liu et al., 2024a), the EVA series outperforms the existing open CLIP models. We also provide results in Appendix C, showcasing a comparison between the two vision encoders using preliminary checkpoints. Furthermore, based on previous work (Karamcheti et al., 2024), we opted for higher resolution images, as it has been shown that it leads to performance gains."}, {"title": "Vision & Language Connector", "content": "We follow LLaVA-1.5 (Liu et al., 2023b) and use a two-layer MLP that projects the visual tokens to the dimensionality expected by the LLM, leaving more sophisticated architectural choices (Dai et al., 2024; Bai et al., 2023b; You et al., 2023) for future work."}, {"title": "Language Backbone", "content": "We use Mamba or Pythia (Biderman et al., 2023) as the language backbone that accepts the visual features from the connector module, and the tokenized text containing the task instruction and any sample text. We select Pythia as the baseline Transformer-based language model because it enables direct comparison as it 1) follows the state-of-the-art Transformer recipe (Su et al., 2024; Dao, 2023), 2) is trained on the same dataset as Mamba (Gao et al., 2020), 3) provides model variants with a similar number of parameters.\nA key difference between the two models is that Mamba does not allocate parameters to model positional information. This inductive bias has been identified by concurrent work (Liu et al., 2024c; Zhu et al., 2024a), applying Mamba to computer vision tasks, since positional embeddings capture the structure of the image. Inspired by Fuyu (Rohan et al., 2023), we overcome this issue by introducing a separator token (\u201c##\u201d) that signals the beginning and the end of the image sequence, as well as an image-newline character (\u201c&&\") that depicts the end of a row of patches.\""}, {"title": "Datasets", "content": "We use a collection of open-source datasets to allow a fully reproducible comparison. For pretraining, we leverage the dataset from Liu et al. (2024a), a subset of 595K captions from Conceptual Captions 3M (Sharma et al., 2018). For instruction tuning, we use a collection of established coarse and fine-grained vision-language tasks (e.g., captioning, visual question answering, and referring expression). Figure 2 shows examples for all tasks in our training and evaluation. We provide details for our dataset, filtering approach, and task instructions in Appendix A. Notably, we pack the examples from the same image and task into one sequence."}, {"title": "Experiments", "content": "Similar to previous work (Liu et al., 2024a; Li et al., 2024) we employ a two-step training regime. First, we perform a warmup stage where we train only the connector component on the pretraining dataset. Next, we unfreeze the language model parameters and train on the instruction-tuning dataset. All models are trained using the same data, in the same order, and with identical training hyperparameters (see Appendix B for further details). Unless stated otherwise, we report the evaluation performance without task-specific fine-tuning. The evaluation"}, {"title": "Experimental Setup", "content": "Similar to previous work (Liu et al., 2024a; Li et al., 2024) we employ a two-step training regime. First, we perform a warmup stage where we train only the connector component on the pretraining dataset. Next, we unfreeze the language model parameters and train on the instruction-tuning dataset. All models are trained using the same data, in the same order, and with identical training hyperparameters (see Appendix B for further details). Unless stated otherwise, we report the evaluation performance without task-specific fine-tuning. The evaluation"}, {"title": "Results", "content": "Table 1 and Table 2 illustrate the comparison between Pythia-VL and Mamba-VL across three model sizes. We provide results for each benchmark individually, along with a summation score as an indication of overall performance for a task group. We observe that Mamba variants match or surpass the performance of models with Pythia as an LLM across all three sizes in"}, {"title": "Pythia vs Mamba", "content": "Table 1 and Table 2 illustrate the comparison between Pythia-VL and Mamba-VL across three model sizes. We provide results for each benchmark individually, along with a summation score as an indication of overall performance for a task group. We observe that Mamba variants match or surpass the performance of models with Pythia as an LLM across all three sizes in"}, {"title": "Finetuning with Higher Resolution", "content": "It is widely known that increasing the image resolution yields benefits in Transformer-based VLMs (Karamcheti et al., 2024; Lauren\u00e7on et al., 2024b). We explore whether the benefits of higher image resolution translate to Mamba given its strong long sequence modeling capabilities (Gu and Dao, 2023). Figure 3 shows the performance of 1.4B models on VQAv2 and RefCOCOg after finetuning on each task with higher-resolution images. As expected, both models benefit from higher-resolution images, and the differences are more evident in RefCOCOg, possibly due to the granularity of the task. Comparing Pythia-VL and Mamba-VL, both models exhibit a"}, {"title": "Why is Grounding Difficult for Mamba?", "content": "We observed that Mamba models are quite effective in multimodal language modeling tasks (e.g., captioning, visual question answering). However, they underperform compared to Transformers of equal capacity in visual grounding tasks. What is the underlying reason for this weakness? We explore two possible explanations using the 1.4B parameter models by 1) examining the effect of task-agnostic visual encoding, and 2) framing visual grounding as an in-context multimodal retrieval task."}, {"title": "Task-agnostic Visual Encoding", "content": "Both Transformer causal models and SSMs operate unidirectionally, i.e. the representation at a given timestep is a function of only the previous and current tokens. However, SSMs enforce a stricter update rule, where the hidden state can only be updated with information from the previous hidden"}, {"title": "Grounding as Multimodal Retrieval", "content": "We can view visual grounding as an in-context multimodal retrieval task. In a standard in-context retrieval task, the model is provided with a context (a text paragraph) and a query (a relevant question), and it needs to extract and copy the part of the input corresponding to the question. Similarly, in a visual grounding task, the model is provided with a series of patch tokens as context and a prompt"}, {"title": "Conclusion", "content": "In this work, we compare Transformer and SSM-based language model backbones for VLMs. We show that Mamba consistently outperforms Transformers in tasks where the output depends on a summary of the visual information. Transformers, on the other hand, maintain the lead in visual grounding tasks, which we link to their ability to learn more accurately and efficiently to retrieve dense information from the context.\nRegardless, Mamba and SSMs, in general, have memory and computational advantages that could be especially critical for tasks that require modeling long sequences, such as high-resolution images, videos, or multimodal documents. Developing hybrid architectures that integrate an attention-like mechanism into SSMs (Dao and Gu, 2024; Waleffe et al., 2024) is therefore an exciting avenue for future work. Such architectures could lead to efficient VLMs that are also able to effectively retrieve relevant information from the context."}, {"title": "Implications of Findings", "content": "In this work, we compare Transformer and SSM-based language model backbones for VLMs. We show that Mamba consistently outperforms Transformers in tasks where the output depends on a summary of the visual information. Transformers, on the other hand, maintain the lead in visual grounding tasks, which we link to their ability to learn more accurately and efficiently to retrieve dense information from the context.\nRegardless, Mamba and SSMs, in general, have memory and computational advantages that could be especially critical for tasks that require modeling long sequences, such as high-resolution images, videos, or multimodal documents. Developing hybrid architectures that integrate an attention-like mechanism into SSMs (Dao and Gu, 2024; Waleffe et al., 2024) is therefore an exciting avenue for future work. Such architectures could lead to efficient VLMs that are also able to effectively retrieve relevant information from the context."}, {"title": "Feature or Bug?", "content": "Additionally, we experiment with the effect of placing the instruction before and after the visual input. While task-aware image encoding provides a marginal performance boost for Mamba on visual grounding, the results fluctuate across other tasks. Ultimately, we want multimodal models that can seamlessly encode different modalities without forcing a strict order on how they are presented to the model. From this perspective, performance differences due to the input structure are a strong signal that the current iteration of VLMs is only partially addressing this issue."}, {"title": "Limitations", "content": "We have not investigated any impact of the data and task distribution. We have not covered any ablations regarding how the examples are packed into sequences. Recent work has shown that this might affect downstream performance in LLMs (Zhao et al., 2024b). Based on our analysis in Section 5.3, and the conclusions from concurrent work (Jelassi et al., 2024; Merrill et al., 2024), we expect that Transformer and Mamba models might behave differently with different packing strategies. However, we want to emphasize that both models are trained on the same data thereby ensuring a fair comparison between them, and also the distribution of the data is heavily skewed towards grounding tasks due to the inclusion of the GRIT dataset."}, {"title": "Data Ablations", "content": "We have not investigated any impact of the data and task distribution. We have not covered any ablations regarding how the examples are packed into sequences. Recent work has shown that this might affect downstream performance in LLMs (Zhao et al., 2024b). Based on our analysis in Section 5.3, and the conclusions from concurrent work (Jelassi et al., 2024; Merrill et al., 2024), we expect that Transformer and Mamba models might behave differently with different packing strategies. However, we want to emphasize that both models are trained on the same data thereby ensuring a fair comparison between them, and also the distribution of the data is heavily skewed towards grounding tasks due to the inclusion of the GRIT dataset."}, {"title": "Ethics statement", "content": "It has been increasingly transparent that the cost of training large-scale models, including VLMs, raises compute barriers (Strubell et al., 2019; Thompson et al., 2020; Luccioni et al., 2024). While patch representations have become the standard approach for encoding images, these representations substantially increase the context window and, consequently, the computational cost of training. To improve efficiency, we have employed sequence packing during training, which results to fewer padding tokens within the batch. Additionally, more sophisticated V&L connectors that downsmple the visual sequence (Alayrac et al., 2022; Dai et al., 2024; Lauren\u00e7on et al., 2024a) can, in principle, accelerate training and inference. We leave comparisons of more efficient V&L connectors in combination with SSMs as future work."}, {"title": "The Cost of Training Large Scale VLMs", "content": "It has been increasingly transparent that the cost of training large-scale models, including VLMs, raises compute barriers (Strubell et al., 2019; Thompson et al., 2020; Luccioni et al., 2024). While patch representations have become the standard approach for encoding images, these representations substantially increase the context window and, consequently, the computational cost of training. To improve efficiency, we have employed sequence packing during training, which results to fewer padding tokens within the batch. Additionally, more sophisticated V&L connectors that downsmple the visual sequence (Alayrac et al., 2022; Dai et al., 2024; Lauren\u00e7on et al., 2024a) can, in principle, accelerate training and inference. We leave comparisons of more efficient V&L connectors in combination with SSMs as future work."}, {"title": "Hallucinations & Reliability", "content": "A widely acknowledged limitation for LLMs and VLMs is the factuality of the generated content (Ji et al., 2023). The impact of this property can vary depending on the downstream task (e.g., answering a question accurately versus creating novel images with text prompts). In this work, we use POPE (Li et al., 2023), a benchmark specifically designed to evaluate object hallucinations in VLMs. However, further investigation is needed to evaluate model hallucinations and improve the reliability of VLMs."}, {"title": "Task-agnostic Visual Encoding", "content": "Both Transformer causal models and SSMs operate unidirectionally, i.e. the representation at a given timestep is a function of only the previous and current tokens. However, SSMs enforce a stricter update rule, where the hidden state can only be updated with information from the previous hidden"}, {"title": "Grounding as Multimodal Retrieval", "content": "We can view visual grounding as an in-context multimodal retrieval task. In a standard in-context retrieval task, the model is provided with a context (a text paragraph) and a query (a relevant question), and it needs to extract and copy the part of the input corresponding to the question. Similarly, in a visual grounding task, the model is provided with a series of patch tokens as context and a prompt"}, {"title": "Synthetic Grounding", "content": "For the task of synthetic grounding, we create sequences of varying lengths (50/100/200). For each sequence, we created in total 1M training examples and evaluated each model on 100k held-out samples. To eliminate any biases regarding the distribution of the targets, we equally distributed the target token evenly within the sequence. For example, for sequences with 100 tokens, 1% of the training examples (1000) have the 1st token as target. All models are trained using a global batch size of 64 for 78K steps. We evaluated every model after 1% of training steps to capture precisely the timestep where each model learns the task."}, {"title": "Prefix Variation", "content": "Additionally, motivated by the improvements of the task-aware encoding on visual grounding, we experiment with a prefix variant of our synthetic task. The key difference is that the query precedes the input sequence, and therefore, Mamba has direct access to the required information from the beginning. We experiment with the same sequence lengths for Pythia and Mamba. Figure 12 illustrates the performance of both models. Compared to the suffix variant (Figure 7) we can see that Mamba learns the task significantly faster. For example, in the suffix version of the task and for sequences of 200 tokens, Mamba is not able to reach 95% accuracy in the training window. On the other hand, in the prefix setting and for the same sequence length, we observe that Mamba learns the task within the first half of the training. Nevertheless, even on this setup, Pythia is more efficient as it learns the task within only 10% of the training steps."}, {"title": "Related to Induction Heads", "content": "Our task is closely related to the Induction Heads (Olsson et al., 2022), which requires models to perform associative recall by retrieving relevant information from the memory. More specifically, if the model has already observed the pattern AB in a sequence of tokens, then it should be able to infer that A is followed by B some time within the same sequence.\nThe results of Mamba (Gu and Dao, 2023) on Induction Heads show that a two-layer model trained on short sequences maintains high performance across varying sequence lengths compared to other SSMs and Transformer recipes. A key difference between this setup and how we framed our syn-"}]}