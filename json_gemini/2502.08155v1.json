{"title": "DGSense: A Domain Generalization Framework for Wireless Sensing", "authors": ["Rui Zhou", "Yu Cheng", "Songlin Li", "Hongwang Zhang", "Chenxu Liu"], "abstract": "Abstract-Wireless sensing is of great benefits to our daily\nlives. By analyzing the impact of human body and movements\non wireless propagation, a variety of sensing tasks can be enabled.\nHowever, wireless signals are sensitive to the surroundings.\nVarious factors, e.g. environments, locations, and individuals,\nmay induce extra impact on wireless propagation. Such a change\ncan be regarded as a domain, in which the data distribution\nshifts. A vast majority of the sensing schemes are learning-\nbased. They are dependent on the training domains, resulting\nin performance degradation in unseen domains. Researchers\nhave proposed various solutions to address this issue. But these\nsolutions leverage either semi-supervised or unsupervised domain\nadaptation techniques. They still require some data in the target\ndomains and do not perform well in unseen domains. In this\npaper, we propose a domain generalization framework DGSense,\nto eliminate the domain dependence problem in wireless sensing.\nThe framework is a general solution working across diverse\nsensing tasks and wireless technologies. Once the sensing model\nis built, it can generalize to unseen domains without any data\nfrom the target domain. To achieve the goal, we first increase the\ndiversity of the training set by a virtual data generator, and then\nextract the domain independent features via episodic training\nbetween the main feature extractor and the domain feature\nextractors. The feature extractors employ a pre-trained Residual\nNetwork (ResNet) with an attention mechanism for spatial\nfeatures, and a 1D Convolutional Neural Network (1DCNN)\nfor temporal features. To demonstrate the effectiveness and\ngenerality of DGSense, we evaluated on WiFi gesture recognition,\nMillimeter Wave (mmWave) activity recognition, and acoustic\nfall detection. All the systems exhibited high generalization\ncapability to unseen domains, including new users, locations, and\nenvironments, free of new data and retraining.", "sections": [{"title": "I. INTRODUCTION", "content": "Human sensing plays an important role in many fields\nof daily life. Conventional approaches to human sensing are\nbased on visions or wearables. Vision-based solutions deploy\ncameras to collect and analyze visual data. They can achieve\nhigh accuracy, but require Line of Sight (LOS) and luminous\nlighting to work properly. In addition, cameras impose privacy\nconcerns, hence they are not appropriate to be deployed in\nprivate spaces. Human sensing based on wearables eliminates\nthe limitations of LOS and lighting and does not impose pri-\nvacy concerns. These solutions exploit the sensors embedded\nin the wearables to realize sensing. However, wearable-based\nsolutions require the users to wear dedicated devices continu-\nously, causing inconvenience and reluctance. To overcome the\nshortcomings of cameras and wearables, human sensing has\nshed light on wireless signals. The ubiquitous availability of\nwireless signals brings new opportunities to human sensing.\nThe past two decades have witnessed extensive wireless sens-\ning techniques, including but not limited to Radio Frequency\n(RF) [1], [2], WiFi [3], [4], Millimeter Wave (mmWave) [5],\n[6], Long Range (LoRa) [7], [8], and acoustic [9], [10]. In\nan environment covered with wireless signals, the presence\nof human bodies alters the propagation of wireless signals,\ncausing them to carry rich user-specific information, which\ncan be exploited to infer human contexts. Wireless sensing\navoids the inconvenience brought by wearables and eliminates\nthe privacy invasion from cameras. A plethora of studies have\nbeen carried out on wireless sensing over the years.\nHowever, wireless signals are sensitive to the surroundings.\nFactors such as environments, locations, and individuals may\ninduce extra impact on wireless propagation and cause the\ndata distributions to shift. Each such a change can be re-\ngarded as a domain. Although the sensing model can achieve\nimpressive performance in the training domains, they often\nsuffer from performance degradation in unseen domains. To\ndeploy wireless sensing in real-world applications, the domain\ndependence problem must be solved. The domain where the\nmodel is trained is called the source domain, whereas the\ndomain where the model is applied/tested is called the target\ndomain. If all the data in the target domain are not previously\nencountered, we call it the unseen domain or new domain.\nExisting solutions to the domain dependence issue typically\nleverage semi-supervised or unsupervised domain adaptation\nmethods, requiring partially labeled or unlabeled data from the\ntarget domain to transfer the sensing model from the source\nto the target domain. However, collecting data from the target\ndomain beforehand is often infeasible in practice. We need a\nsolution that can automatically generalize the sensing model\nto new/unseen domains without any new data and retraining.\nThis is a domain generalization (DG) approach.\nIn this paper, we propose a domain generalization frame-\nwork for wireless sensing-DGSense. It aims to enable gen-\neralization to new/unseen domains without any data from\nthe new domains, meanwhile the framework is general to\ndiverse sensing tasks on diverse wireless technologies. The\nkey points in the framework are: (1) a cross-modal virtual\ndata generator, to increase the diversity of the training set and\nenhance the robustness of the sensing model; (2) a spatial-\ntemporal feature extractor, based on a pre-trained Residual\nNetwork (ResNet) with an attention mechanism for spatial\nfeatures and a 1D Convolutional Neural Network (1DCNN)\nfor temporal features; (3) an episodic training strategy, to\nextract the domain independent features for the purpose of"}, {"title": "II. RELATED WORK", "content": "A. Robust wireless sensing\n1) WiFi sensing: WiFi sensing has been extensively studied\nsince the release of Channel State Information (CSI) tools [11],\n[12]. Many solutions have been proposed to improve the\nrobustness. Exploiting adversarial networks, EI [13] could\nremove environment and user-specific information and learn\ntransferable features of activities. CsiGAN [14] enabled ac-\ntivity recognition adaptive to users based on semi-supervised\nGenerative Adversarial Network (GAN), leveraging a com-\nplement generator to produce diverse fake samples to train a\nrobust discriminator. Kang et al. [15] realized cross domain\ngesture recognition based on CSI Doppler Frequency Shift\n(DFS) through multi-source unsupervised domain adaptation,\nby applying adversarial learning and feature disentanglement\nto remove gesture irrelevant factors. OneFi [16] could recog-\nnize unseen gestures with only one (or few) labeled samples.\nIt utilized virtual gesture generation to reduce the effort in\ndata collection and employed a one-shot learning framework\nbased on transductive fine-tuning to eliminate model retrain-\ning. These methods required some labeled or unlabeled data\nfrom the target domains for model training, while our method\ndid not require any target domain data for model training.\nWidar3.0 [17] achieved cross domain gesture recognition by\nextracting the domain independent feature Body-coordinate\nVelocity Profile (BVP) from CSI DFS. Widar3.0 required one\ntransmitter and at least three receivers to work, while our\nmethod required only one transmitter and one receiver.\n2) mmWave sensing: Due to high frequency and wide\nbandwidth, mmWave can be used to realize highly accurate\nsensing. Researchers have developed various mmWave-based\nsensing applications, mostly exploiting Frequency Modulated\nContinuous Waves (FMCW). S. Wang et al. [18] built a gesture\nrecognition system based on Google's Soli sensor, leveraging\na combination of deep convolutional and recurrent neural\nnetworks. Zhang et al. [19] processed range-Doppler maps\n(RDM) with a zero-filling strategy to boost the range and\nvelocity information of gestures and constructed a 3DCNN\nmodel and a CNN-Long Short Term Memory (LSTM) model\nto reveal the temporal gesture signatures encoded in multiple\nframes. Amin et al. [20] classified daily activities, in particular\ncontiguous motions, using micro-Doppler signatures and range\nmaps. Chen et al. [21] designed a temporal 3DCNN to deal\nwith a series of range-Doppler maps and added a temporal\nattention module to emphasize the sequential relation between\neach frame. These methods focused on in-domain sensing\nwithout considering the cross domain issue. mHomeGes [5]\nproposed concentrated position-Doppler profile (CPDP) to\nrepresent the unique features from different arm joints and\nrecognized continuous arm gestures based on a lightweight\nCNN. mHomeGes could work across various smart-home\nscenarios regardless of the impact of surrounding interference,\nbut domain independence was not under consideration of this\nwork. M-Gesture [22] achieved person-independent gesture\nrecognition by incorporating a pseudo representative model\n(PRM) and a custom-built neural network to depict and\nextract the inherent gesture features. Their experiments showed\nthat M-Gesture required about 20 persons to achieve high\naccuracy for new users and the changes in user orientations\nstill degraded the performance.\n3) Acoustic sensing: Acoustic signals can be used for\nhuman sensing. A variety of applications have been developed\nin recent years [9]. DeepRange [23] synthesized training\ndata and realized acoustic ranging based on deep learning.\nEchoSpot [24] localized a person by periodically emitting\nacoustic chirps, capturing the reflections from the human\nbody and the walls, and normalizing their cross-correlation.\nEarIO [25] extracted the depth features of the face and tracked\nthe facial expressions via cross-correlation between emitted\nand reflected acoustic signals. To adapt to new users, EarIO\nrequired a small amount of data from new users. Lian et\nal. [26] detected falls by emitting continuous waves of fixed\nfrequency and calculating Doppler frequency spectrogram of\nthe reflections. Singular Value Decomposition (SVD) and\nHidden Markov Model (HMM) were applied to classify the\nmotions as falls or non-falls. StruGesture [27] exploited the\nstructure-borne sounds to classify gestures on the back surface\nof mobile phones. It leveraged deep adversarial learning to\nlearn the gesture-specific representation, and required a few\nsamples from the new users to achieve proper recognition for\nnew users."}, {"title": "B. Domain adaptation and generalization", "content": "1) Domain adaptation: Domain adaptation (DA) aims to\nutilize the data in the source domains to solve the learn-\ning problem in the target domains. Some domain adaptation\nmethods have been proposed to tackle the domain dependence\nproblem in wireless sensing. Feature alignment is a common\nmethod of domain adaptation. The features in the source\nand the target domains are mapped to a shared space, in\nwhich the distribution divergence between them is minimized,\nso that the sensing model built in the source domain can\nthen be applied to the target domain. AdapLoc [28] achieved\nadaptive CSI localization by mapping the source and the target\ndomains into a shared space and minimizing the distance\nbetween the fingerprints at the same location and maximizing\nthe distance between the fingerprints at different locations.\nDANGR [29] recognized gestures using CSI based on a\ndeep adaptation network. To shrink the domain discrepancy,\nDANGR adopted Multi-Kernel Maximal Mean Discrepancy\n(MK-MMD). Researchers also proposed synthesizing virtual\nsamples to enlarge the training set and increase the diversity,\nthereby enhancing the robustness. FiDo [30] could localize\ndiverse users with labeled CSI data from one or two users,\nleveraging a data augmenter that introduced data diversity\nusing VAE and a domain adaptive classifier. Zhang et al. [31]\nsynthesized variant activity data through CSI transformation\nto mitigate the impact of activity inconsistency and subject-\nspecific issue for CSI activity recognition. Domain adaptation\ncan also be achieved by adversarial networks. Regarding the\ntarget domain data as fake samples, the domain independent\nfeatures can be extracted through adversarial training. By\nexploiting adversarial networks, EI [13] could remove environ-\nment and user-specific information and achieve environment\nindependent activity recognition based on CSI. CsiGAN [14]\nenabled CSI activity recognition adaptive to users based on\nsemi-supervised GAN. Domain adaptation methods require\nextra efforts to collect data from the target domains. They\nhave to retrain the classifiers each time new target domains\nare added. Therefore, they have difficulties in generalizing to\nnew/unseen domains.\n2) Domain generalization: Domain generalization extends\ndomain adaptation by generalizing to the target domains\nwithout any target domain data and retraining. It focuses\non learning a domain independent sensing model from the\nsource domains and achieving high performance in unseen\ntarget domains [32]. Domain generalization is still at its early\nstage and most researches are on image recognition. H. Li\net al. [33] applied adversarial Autoencoders and MMD to\nalign feature distributions and extract domain independent\nfeatures. D. Li et al. [34] enhanced model robustness to new\ndomains through episodic training, exposing the model to\ndiverse samples. Qiao et al. [35] combined meta-learning and\nWasserstein Autoencoders to generate virtual data, improving\ngeneralization capabilities.\nIn the field of wireless sensing, general domain general-\nization methods for diverse sensing tasks on top of diverse\nwireless signals are still lacking. Previous studies either exploit\ndomain adaptation methods requiring some target domain data,\nor design specific signal processing techniques for specific\nsensing tasks. The main contribution of our work is a general\ndomain generalization framework for wireless sensing tasks."}, {"title": "III. FRAMEWORK OF DOMAIN GENERALIZATION", "content": "A. Problem formulation\nSuppose Ds represents the set of source domains. A domain\nrefers to a different scenario, e.g. an individual, a location, an\nenvironment, etc. Ds can be expressed as:\n$D^{s} = \\{D_{1}, D_{2},\u2026, D_{N}\\}$\nwhere N is the number of source domains. Each source\ndomain contains multiple samples and their labels, which can\nbe expressed as:\n$D_{i} = \\{(x_{ij}, y_{ij})|j = 1,2,\u2026,n_{i}\\}$\nwhere $x_{ij}$ represents sample j in domain i, $y_{ij}$ represents its\nlabel, $n_{i}$ is the number of samples in domain i. Suppose $D^{s}$\ndenotes the training set containing all the training samples,\nexpressed as:\n$D^{s} = \\{(x_{l},y_{l})|l = 1,2,\u2026,n\\}$\nwhere $n = \\sum_{i=1}^{N} n_{i}$ is the number of all the training samples,\n$x_{l}$ represents a sample, and $y_{l}$ represents its label. Suppose\n$D^{t}$ represents the testing set, i.e. the unseen target domain.\nThe domain generalization problem can be described as: we\ntrain the sensing model with the source domain data $D^{s}$ and\ngeneralize it to the unseen target domain $D^{t}$, without any\ntarget domain data participating in the training.\nB. Framework overview\nWe propose a framework of domain generalization for\nwireless sensing-DGSense, to mitigate the impact of domain\ndiversity and achieve domain independent sensing. DGSense\nenables the sensing model trained in the source domains\nto sustain high performance in the unseen target domains.\nIt is a general framework, accommodating diverse sensing\ntasks on top of diverse wireless signals. Fig. 1 depicts the\noverall framework, composed of four main components: data\ncollection, data preprocessing, virtual data generation, domain\nindependent feature extraction and classification. The virtual\ndata generation component augments the training set with\nmore diversity. It generates the virtual data to augment the\nsource domains, allowing the sensing model trained in the\nsource domains to be more robust. The domain independent\nfeature extraction and classification component concentrates\non extracting the sensing-related features whilst mitigating\nthe influence of environmental factors. It consists of a main\nnetwork and multiple domain networks. Each domain network\nis associated with a source domain. Through episodic training\nbetween the main network and the domain networks, the\nmain network acquires the capability of extracting domain\nindependent features. The domain networks are employed only\nin the training phase, while in the testing phase only the main\nnetwork is involved."}, {"title": "C. Data collection and preprocessing", "content": "The framework supports various wireless technologies, such\nas WiFi, mmWave and acoustic. The data collection methods\nrely on the wireless technologies and the preprocessing meth-\nods adapt to the data. The raw WiFi data are time-series of\namplitude and phase, which need to be denoised, calibrated\nand segmented. The raw mmWave data are multi-frame range-\nDoppler maps, which are compressed to reduce the complexity.\nThe raw acoustic data are sound waves, which are denoised\nand transformed to Doppler spectrograms. To reduce the noise\nin time-series data, the moving average algorithm or the\nmedian filter can be applied to smooth the data and eliminate\nthe high-frequency noise. To reduce the noise in images, the\nthreshold filtering algorithm can be applied. After that, the data\nare segmented to obtain the relevant sensing part. For time-\nseries data, the variance threshold algorithm can be leveraged\nto identify the part with the most significant fluctuations. For\nimages, the relevant segments can be located by the Power\nBurst Curve (PBC) method."}, {"title": "D. Virtual data generation", "content": "To increase data diversity and reduce the effort of data\ncollection, we incorporate a virtual data generator to expand\nthe training set. The virtual data generator is based on VAE.\nIn the training phase, we use the source domain data to train\nthe virtual data generator. In the generating phase, the source\ndomain data are input to the encoder to obtain the intermediate\nfeatures, to which the identically distributed noise is added.\nThe intermediate features with noise are passed to the decoder\nto obtain the reconstructions as the virtual data.\n1) Single-modal generation: For samples with a single\nmodality, such as Doppler spectrograms, we construct a single-\nmodal virtual data generator. Its structure is visualized in\nFig. 2(a). The encoder is a CNN and the decoder is a decon-\nvolutional neural network (DCNN). To train the generator, we\ninput the real source domain data into the encoder and derive\nthe reconstructions. To optimize the model parameters, we\nminimize both the reconstruction loss between the input data\nand the reconstructions and the Kullback-Leibler divergence\n(KL-divergence) between the intermediate feature distribution\nand the normal distribution. Assuming the input data is x,\nthe intermediate feature $z_{i}$ is calculated by the encoder as:\n$z_{i} = Encoder(x) = \\mu_{i} + \\sigma_{i} \\varepsilon_{i}$\nwhere $\\mu_{i}$ is the mean value of the encoder output, $\\sigma_{i}$ is the\nstandard deviation of the encoder output, $\\varepsilon_{i} \\sim N(0, I)$ is\na normal distribution, and I represents identity matrix. The\nintermediate feature $z_{i}$ is passed to the decoder to obtain the\nreconstruction $x^{rec}$. The reconstruction loss between x and\n$x^{rec}$ is minimized, along with a regularization term restricting\nthe intermediate feature $z_{i}$ to a normal distribution. The loss\nfunction of the virtual data generator is defined as:\n$min L_{VAE} = \\sum_{i=1}^{n} (MSE(x, x^{rec}) + \\lambda \\cdot KL(N(\\mu_{i}, \\sigma_{i})||N(0, I)))$\nFor generating the virtual data, the real data x is fed to\nthe encoder to extract the intermediate feature $z_{i}$, and the\nnoise with the identical distribution is added to $z_{i}$ in a specific\nproportion. The noised feature $z_{i}$ is then fed to the decoder to\nobtain the virtual data x:\n$z_{i} = Encoder(x)$\n$\\tilde{z_{i}} = w_{1} \\cdot z_{i} + w_{2} \\cdot noise$\n$\\tilde{x} = Decoder(\\tilde{z_{i}})$\nThe labeled virtual data is thus generated in the form of\n($\\tilde{x},y$), where y = y.\n2) Cross-modal generation: When dealing with the sam-\nples containing multiple correlated modalities, such as WiFi\nsamples, where amplitude, phase and spectrogram are corre-\nlated and used concurrently, it is necessary to maintain the\nconsistency among the multiple modalities. When generating\na virtual sample, a real sample is encoded to obtain the\nintermediate feature. The noise with the identical distribution\nis added to the intermediate feature, which is then decoded\nto obtain the virtual sample. As different modalities have\ndifferent data distributions, the noises added to them also have\ndifferent distributions. This will bring external inconsistency\nto these reconstructed modalities. To alleviate this problem,\nwe introduce a cross-modal virtual data generator, leveraging\nthe inter-modal relationships, as depicted in Fig. 2(b). The"}, {"title": "E. Domain independent feature extraction", "content": "To achieve robust sensing, the key is to extract domain\nindependent features. We adopt the episodic training strategy\nintroduced by Li et al. [34] for computer vision. The strategy\nencompasses a main network and multiple domain networks.\nThe training process first trains the domain networks and\nthen trains the main network. Through episodic training, the\nmain network acquires the generalization capability to unseen\ndomain finally.\n1) Network structure: The episodic training strategy con-\nsists of a main network and multiple domain networks, as\ndepicted in Fig. 3(a). The main network is composed of a main\nfeature extractor and a main classifier. Each domain network\nis associated with a source domain, consisting of a domain\nfeature extractor and a domain classifier. All the feature\nextractors share the same structure and all the classifiers share\nthe same structure, tailored to the wireless data. For time-\nseries, we employ 1DCNN as the feature extractor to capture\nthe temporal features. For images, we employ ResNet18 [36]\nwith Convolutional Block Attention Module (CBAM) [37] as\nthe feature extractor to capture the spatial features. All the\nclassifiers are fully-connected neural networks.\n2) Training of the domain network: The domain networks\nare first trained. For each domain i, its dataset Di is input to\nthe corresponding domain network, so that the domain network\nfocuses on feature extraction and classification within domain\ni, as illustrated in Fig. 3(b). During the training of the domain\nnetwork i, the training samples, denoted as $x_{ij}$ \u2208 Di, are input\nto the domain feature extractor. The features are extracted as:\n$f_{ij} = Extractor_{i}(x_{ij})$\nwhere $Extractor_{i}(\\cdot)$ represents the feature extractor of the do-\nmain network i. The features are passed to the corresponding\ndomain classifier to obtain the classification results as:\n$\\hat{y}_{ij} = Classifier_{i}(f_{ij})$"}, {"title": "IV. EVALUATIONS ON WIFI GESTURE RECOGNITION", "content": "A. WiFi signals and preprocessing\nWe exploited amplitude, phase, and Doppler spectrogram\nfrom the CSI data to realize gesture recognition. The amplitude\nand phase depict the received power and phase on each\nsubcarrier, which can be retrieved from certain commodity\nnetwork interface cards with CSI tools [11], [12]. The received\nCSI is a matrix $H = (H_{ij})^{N_{tx} \\times N_{rx}}$, where $N_{tx}$ is the number\nof transmitting antennas, $N_{rx}$ is the number of receiving\nantennas, Hij is the CSI of the channel formed by transmitting\nantenna i and receiving antenna j, containing $N_{s}$ subcarriers,\nexpressed as $H_{ij} = (h_{1},h_{2},\u2026,h_{N_{s}})$. The k-th subcarrier in\nHij can be expressed as $h_{k} = |h_{k}|e^{j\\angle h_{k}}$, where |hk| is the\namplitude and $\\angle h_{k}$ is the phase.\nHuman bodies and movements may alter the amplitude and\nthe phase of the received signals. Human motions may cause\nchanges in the lengths of reflection paths, resulting in Doppler\neffect. We exploit the amplitude and the phase of the subcar-\nriers as well as Doppler spectrograms to recognize gestures.\nThe raw amplitude data are denoised by the median filter, and\nthe raw phase data are sanitized by a linear transformation\nmethod [39]. The Doppler spectrogram is generated through\nantenna selection, bandpass filtering, dimension reduction and\nShort Time Fourier Transform (STFT) on amplitude time-\nseries [40]. The amplitude, the phase and the spectrogram\nare originated from the same sensing medium. They represent\ndifferent but correlated information and depict different aspects\nof the same signal. After preprocessing, they form respective\nsamples for the same gestures, hence we regard them as three\nmodalities and fuse them for gesture recognition.\nB. System design\nThe system is based on the DGSense framework, compris-\ning data acquisition and preprocessing, virtual data generation,\ndomain independent feature extraction and classification. As\nthere are three modalities, we employ the cross-modal virtual\ndata generator to enhance the diversity of the training set. The\nvirtual data generator takes the amplitude as the base modality,\nestablishes the mapping between the amplitude and the other\nmodalities during training, and generates the virtual data of\nthe three modalities from the real amplitude. This ensures\nconsistency between the virtual modalities of the same gesture.\nEpisodic training is applied to extract domain independent\nfeatures. As there are three modalities, we design a composite\nfeature extractor, comprising an amplitude feature extractor, a\nphase feature extractor and a spectrogram feature extractor. As\nillustrated in Fig. 5, the amplitude feature extractor is 1DCNN\nto extract the temporal features, the phase feature extractor and\nthe spectrogram feature extractor adopt ResNet18 to extract\nthe spatial features. The features from the three modalities\nare fused and fed to the classifier for gesture recognition.\nAssuming the sample is denoted as x = (xam, xph, xsp), the\nfeature fusion and classification can be formulated as:\n$f = \\alpha_{1} AmExt(x^{am}) + \\alpha_{2}PhExt(x^{ph}) + \\alpha_{3}SpExt(x^{sP})$\n$\\hat{y} = Classifier(f)$"}, {"title": "C. Evaluations", "content": "We deployed two laptops equipped with Intel 5300 wireless\nadapters, one as the transmitter and the other as the receiver.\nEach transceiver had 3 antennas, forming 9 links. Each link\ncontained 30 subcarrier groups, resulting in 270 dimensions.\nThe sampling frequency was 1000Hz to capture the Doppler\neffect. The gesture data were collected in 4 rooms, as shown\nin Fig. 6, with the sizes of 5m\u00d75m, 6m\u00d78m, 11.4m\u00d76.8m,\nand 7.6m\u00d75.8m. The distance between the transmitter and the\nreceiver was about 2m. During the experiments, 6 volunteers\ntraced the letters {L, O, V, S, W, Z} with hand gestures in\neach room. Each gesture was repeated 20 times, resulting in\na total of 720 gesture samples in each room.\n1) In-domain accuracy: We first evaluated the in-domain\naccuracy of the WiFi gesture recognition system, involving 6\ngestures by 6 volunteers in 4 rooms. Each gesture was repeated\n20 times by each volunteer in each room. We conducted 5-\nfold cross-validation on the dataset and achieved the average\naccuracy of 97.8% for familiar users in seen rooms."}, {"title": "V. EVALUATIONS ON MMWAVE ACTIVITY RECOGNITION", "content": "A. mmWave signals and preprocessing\nWe deploy the TI IWR1443 boost radar board to collect\nmmWave data, which operates in the frequency range of 76-\n81GHz and has a bandwidth of 4GHz for FMCW signals.\nThe mmWave radar contains 3 transmitting antennas and 4\nreceiving antennas. We retrieve the range-Doppler data from\nthe radar to recognize activities. The data are time-series of\nrange-Doppler maps, describing the range and velocity of the\ntarget relative to the radar over time, which can be expressed\nas $RDM = (RDM_{1}, RDM_{2},\u2026, RDM_{t})$. RDMi is the\ni-th frame in the time-series, which is a range-Doppler map\nand can be expressed as a matrix $RDM_{i} = (p_{rv})$, in which r\nrepresents the range index, v represents the velocity index, and\nthe values represent the power of the signal. Different activities\ncause different patterns in the multi-frame range-Doppler\nmaps, which can be used to deduce the activities. To reduce the\ncomputing complexity, we compress the multi-frame range-\nDoppler maps from 3D to 2D, by compressing the velocity\ndimension and using the velocity with the maximal probability,\nas illustrated in Fig. 9. The compressed 2D Doppler map is\nderived as:\n$CDM = (arg \\underset{\\upsilon}{max} RDM_{1},\u2026, arg \\underset{\\upsilon}{max} RDM_{t})$\nIn the compressed Doppler map CDM, one dimension repre-\nsents the time index and the other represents the range index,\nand the values represent the velocities of the target, reflecting\nthe change of velocity over time in terms of distance. We use\ndifferent colors to indicate the velocity values. The red color\nrepresents the positive velocity away from the radar, while the\nblue color represents the negative velocity approaching the\nradar. We take the images of the compressed Doppler maps as\nthe input to the sensing model.\nB. System design\nThe system is based on the DGSense framework, consisting\nof data collection and preprocessing, virtual data genera-\ntion, domain independent feature extraction and classification.\nGiven that the samples take the form of images, we employ\na single-modal virtual data generator. The virtual samples are\ngenerated by adding noise to the intermediate features and\ndecoding them. Feature extraction and classification involves\na main network and several domain networks, exploiting\nepisodic training to extract domain independent activity fea-\ntures. The main network and the domain networks share the\nsame structure, as shown in Fig. 10. The feature extractor\nis based on ResNet18 and enhanced by CBAM within each\nresidual block, so that the feature extractor emphasizes the\nactivity region. The inclusion of CBAM brings together spatial\nattention and channel attention mechanisms [37], enabling\nthe differentiation of activities and enhancing the distinction\nbetween similar activities. The classifier is a 3-layer fully\nconnected neural network."}, {"title": "C. Evaluations", "content": "The experiments were carried out in a laboratory", "activities": "nsquatting down", "accuracy": "We first evaluated the in-domain\naccuracy of the mmWave activity recognition system", "data": "We augmented the training set\nby single-modal virtual data generation. The number of virtual\nsamples equaled the number of real samples. We conducted\nexperiments to verify the quality of the virtual data. We trained\nthe feature extractor and classifier with the real data and tested\non the virtual data", "user": "To evaluate the generalization of the system\non new users", "location": "To evaluate the generalization of the\nmethod at new locations", "models": "We compared our\nmethod with the w/o DG method, the 3DCNN model by\nChen et al. [21", "19": ".", "21": "extracts the\nfeatures from the time-series of range-Doppler maps. The\nCNN-LSTM model [19"}]}