{"title": "Evaluating LLM Reasoning in the Operations Research Domain with ORQA", "authors": ["Mahdi Mostajabdaveh", "Timothy T. Yu", "Samarendra Chandan Bindu Dash", "Rindranirina Ramamonjison", "Jabo Serge Byusa", "Giuseppe Carenini", "Zirui Zhou", "Yong Zhang"], "abstract": "In this paper, we introduce and apply Operations Research Question Answering (ORQA), a new benchmark, to assess the generalization capabilities of Large Language Models (LLMs) in the specialized technical domain of Operations Research (OR). This benchmark is designed to evaluate whether LLMs can emulate the knowledge and reasoning skills of OR experts when given diverse and complex optimization problems. The dataset, crafted by OR experts, presents real-world optimization problems that require multi-step reasoning to build their mathematical models. Our evaluations of various open-source LLMs, such as LLaMA 3.1, DeepSeek, and Mixtral reveal their modest performance, indicating a gap in their aptitude to generalize to specialized technical domains. This work contributes to the ongoing discourse on LLMs' generalization capabilities, providing insights for future research in this area. The dataset and evaluation code are publicly available \u00b9.", "sections": [{"title": "Introduction", "content": "The ability of Large Language Models (LLMs) to follow human instructions and perform diverse tasks has made them an exciting area of investigation. Moreover, the considerable interest in adopting LLMs across various complex technical domains (e.g., medicine (Gao et al. 2023; Zhou et al. 2023)) highlights their potential for significant societal impact. A particularly compelling driver for this adoption is the potential of LLMs to automate many tasks, reducing human intervention and improving productivity. However, as LLMs are becoming integrated into the workflow of various industries, it is important to thoroughly understand their capabilities and limitations (Khatun and Brown 2024; Baktash and Dawodi 2023; Truong et al. 2023). Of particular interest is their ability to reason and perform new challenging tasks across different domains, which are reported critical limitations of LLMs (Arkoudas 2023; Shen and Kejriwal 2023). Our work addresses this need by introducing a new benchmark dataset and applying it to assess these limitations.\nTo evaluate an LLM's ability to generalize to a new domain, we focus on the field of Operations Research (OR). The choice of this domain is deliberate and significant. First, OR is important for making decisions in various industries (Petropoulos et al. 2024). Second, there are many types of optimization problems applied to real-world applications ranging from production scheduling (Mostajabdaveh, Salman, and Tahmasbi 2022) to creating efficient delivery routes for trucks (Vidal, Laporte, and Matl 2020). Third, optimization modeling presents a unique challenge due to the expert-level knowledge and reasoning skills it requires (Hillier and Lieberman 2015), adding a layer of complexity to the automation of this task. Some recent studies also report modest performances of SOTA LLMs such as GPT-4 and Llama2 for optimization model building tasks (AhmadiTeshnizi, Gao, and Udell 2024; Mostajabdaveh et al. 2024). Finally, OR is a niche field with limited publicly available text corpora or optimization model code (Xiao et al. 2023; AhmadiTeshnizi, Gao, and Udell 2024), making it an ideal testbed to assess the generalizability of LLMs, reducing the risk of data contamination.\nTo assess an LLM's knowledge and reasoning skills on unseen, diverse, and complex optimization problems, we propose ORQA (Operations Research Question Answering), a new multi-choice Question Answering (QA) benchmark dataset, crafted and verified by OR experts. Each dataset instance (Figure 1; left) presents a natural language description of an optimization problem along with a question that requires multi-step reasoning (Figure 1; right) to answer correctly.\nOur study is significant for several reasons. First, it contributes to the ongoing dialogue on the generalizability of LLMs. Although many benchmarks claim that LLMs can replicate expert-level knowledge across various technical domains, the actual extent of these models' generalization capabilities remains an open question (Alzahrani et al. 2024). Our benchmark offers a new perspective by focusing on a specialized technical domain that lacks a large-scale, high-quality dataset. To the best of our knowledge, this is the first multi-choice QA dataset in the field of Operations Research. Second, this study has implications for understanding the potential and limitations of LLMs in automating tasks in niche technical fields like OR, where expert knowledge and complex multi-step reasoning are crucial. Optimization modeling requires access to specialized OR experts, which is often impractical for most potential users due to the associated costs.\nWe choose to focus on translating textual problem descriptions into mathematical optimization models rather than directly solving optimization problems. Even specialized AI models struggle with scalability and generalization when solving simple OR problems (Joshi et al. 2022).\nWe also highlight the challenges of constructing a technical dataset like ORQA. Samples of optimization problems are inherently complex and require significant time and effort to create and verify. Moreover, ensuring correctness demands annotators with graduate-level education or extensive experience in OR modeling, making the process expensive, time-consuming, and labor-intensive."}, {"title": "Related Work", "content": "LLMs applied to operations research: Within the field of OR, LLMs are being investigated for their ability to formulate optimization models (Fan et al. 2024). Ramamonjison et al. (2022) proposed using generative models to automate the formulation of OR problems from natural language. Building on this, Ramamonjison et al. (2023) introduced methods for recognizing entities and parsing optimization formulations from text. However, these works primarily utilize a toy dataset of elementary linear programming word problems. More recently, AhmadiTeshnizi, Gao, and Udell (2024) proposed the NLP4LP dataset, which includes 332 instances with structured natural language descriptions (SNOP), parameter data values, optimal value, and optimal solution. However, the majority of the problems are still toys, their description is technical, with mathematical notation, and lacks context from real applications. Xiao et al. (2023) introduced the ComplexOR dataset with only 37 optimization problems. This dataset includes NL descriptions, optimization models, and several input data. While their problem descriptions are context-aware, they often mention the related optimization problem by name (e.g., lot-sizing problem with setup), and only cover a narrow range of application domains. With more than 1.5k instances, our dataset is the largest, offering significant value through its depth, rigor, realism, and diversity.\nAnother disadvantage of existing mentioned datasets is they require the optimization model to be solved for their evaluation. LLMs must generate a model code from the NL description, which is then fed to a solver with data to obtain the optimal value or solution. The evaluation focuses solely on the correctness of the optimal value or solution, presenting two key limitations: (1) it is an end-to-end evaluation that cannot differentiate between minor notation errors and entirely incorrect structures; (2) it cannot distinguish between errors in code generation and errors in model formulation. (AhmadiTeshnizi, Gao, and Udell 2024) show that coding errors account between 21% to 31%. In contrast, ORQA tackles more complex optimization tasks across diverse application scenarios. Our problem descriptions are context-aware, relevant to real applications, and free of OR jargon and mathematical notations. Additionally, ORQA is multiple-choice question answering dataset offers a straightforward evaluation that is independent of model code and does not require solvers.\nMulti-choice question answering: The multi-choice QA task involves receiving a question along with several candidate options and selecting the correct answer. Many such datasets have been made publicly available (Talmor et al., 2019; Mihaylov et al., 2018; Clark et al., 2018). However, the complexity of current reasoning benchmarks has been called into question (Khatun & Brown, 2024; Valmeekam et al., 2023; Sawada et al., 2023), motivating the creation of more challenging benchmarks that surpass basic common-sense reasoning (Kweon et al., 2024; Sawada et al., 2023). While multi-choice QA is a well-studied NLP task, ORQA stands out as a handcrafted, expert-curated dataset in the technical field of OR. This domain is notably underrepresented in current benchmarks and demands deep optimization knowledge. Moreover, tasks in ORQA require identifying optimization model components and their interrelationships, which necessitates multi-step reasoning.\nLLM reasoning capabilities and limitations: Zhou et al. (2024) demonstrated that LLMs can be prompted to reason, leading to improved performance and insights into how decisions are made. Techniques for this purpose include multi-step chained prompts (Yoran et al. 2023; Kojima et al."}, {"title": "Task: Identifying Optimization Model Characteristics", "content": "Task background and motivation\nAn optimization model is a mathematical representation of a decision-making problem. Optimization models are constructed using various components. These components include elements, decision activities, data attributes, calculations, objective criteria, and specifications (S\u00e1nchez et al. 2021). The first and most crucial step in formulating an optimization model is to identify the components of the model and understand their relationships, as any error in this step will result in an incorrect model. ORQA focuses on identifying these components and their relations from the natural language description of the optimization problem by asking questions such as \"What are the decision activities of the optimization problem?\u201d and \u201cWhich data parameters are participating in the objective criterion?\u201d.\nTo illustrate these components and their relationships, we refer to the parking spot assignment example problem described in Figure 2. The figure demonstrates how the extraction of optimization problem components and their relationships can be directly used to formulate the optimization problem mathematically. Unique apartment units and parking spots are the elements of this example problem and their data attributes directly map to sets (i.e., apartment groups, parking spots) and data parameters (i.e., parking need, number of vehicles, etc.). The decision activities are direct actions in the system and define the optimization variables (i.e., assign vehicles to parking spots). These three model components are combined to form the utility/-cost function to be maximized/minimized (i.e., minimize total distance). They also form the specifications that define business rules or system limitations, which lead to optimization constraints.\nTask definition\nWe propose a multi-choice QA task to identify the components of the optimization problem, their attributes, and relationships, from a given natural language problem description. This is a highly complex task requiring multi-step reasoning as these components have multiple layers of interaction and dependencies (Hillier and Lieberman 2015). For example, identifying the objective criteria involves not only recognizing the objective measure and sense, but also determining the specific data attributes and decision activities that influence it. Figure 7 in Appendix illustrates these complex relationships between problem components.\nTask characteristics\nThe task we propose is complex not only due to the heavily mathematical nature of the field of OR, but also the complexity of the optimization models the dataset is built upon. The complexity is directly related to the number of components in the corresponding mathematical model. We describe in Section 4.2 our approach to ensure a standard level of complexity during our dataset creation process.\nAdditionally, the task is difficult due to the underrepresentation of open-sourced OR data during LLM training. The findings from Kandpal et al. (2023) and Mallen et al. (2023) align with our claim that scarcity in optimization modeling-related data would make this task challenging for LLMs. In this regard, we consider optimization modeling as a task that requires long-tail knowledge."}, {"title": "ORQA Dataset", "content": "Dataset overview\nEach dataset instance contains the following (see Figure 1, left):\n1. A CONTEXT describing an optimization problem as a case study using natural language,"}, {"title": "Experiment Setup for Evaluation", "content": "To provide an initial assessment of the difficulty of ORQA and gain insights, we ran a series of experiments to benchmark different LLM models using various prompting strategies.\nBaseline models. Specifically, we run inference on open-source LLMs such as the 11B parameter FLAN-T5 XXL (Chung et al. 2024), Falcon-7B-Instruct (Almazrouei et al. 2023), Mistral model series (Jiang et al., 2023), Mixtral series (Jiang et al. 2024), as well as Llama2, 3 and 3.1 models of different sizes (Touvron et al. 2023; Dubey et al. 2024). We made a deliberate decision not to use closed-source LLMs in the spirit of scientific reproducibility. Because when prompting an LLM through APIs, there is unquantifiable uncertainty in how inputs and outputs are processed. Moreover, the LLMs may change or be deprecated, potentially invalidating our results. Model endpoints are commonly deprecated as newer models are made available 2.\nWe evaluated each model on the 1468 instances of data from the test set for its standard and CoT prompting capabilities in both zero-shot and few-shot settings, as described in Section 3. Table 2 outlines the accuracy performance of the LLMs evaluated using this benchmark. Furthermore, we report the average F1 scores in Table 6 in Appendix. As a preliminary human baseline, a single expert with a related Ph.D. achieved 93% accuracy on a random set of 100 instances without any in-context learning examples.\nPrompting strategies. We evaluate the LLM's ability to reason with different prompting strategies. First, all-at-once (standard) prompting evaluates the LLM's robustness to perform these reasoning steps without explicitly prompting it to reason. Conversely, CoT prompting is implemented as a two-step approach following similar works on multi-choice QA (Wei et al. 2022; Yoran et al. 2023; Kojima et al. 2022). Specifically, the first prompt elicits the LLM to reason \"step-by-step\". Then, the generated reasoning is added to the prompt and given to the LLM to generate the final answer. Both standard prompting and CoT are evaluated for zero-shot and few-shot capabilities, where few-shot prompts are created by randomly sampling instances with the same question type from the validation split. Note that for ICL examples, ground-truth reasonings are excluded in standard prompting but included in CoT.\nEvaluation protocol. Our custom evaluation framework takes inspiration from Robinson and Wingate (2023) and binds each option to a symbol (i.e., A, B, C, D). We further discovered that appending the prompt \"Therefore, among A through D, the answer is (\" to the end reliably guided all tested LLMs to output the required format. We bind each answer to a symbol to avoid punishing models that display the proper reasoning ability to reach the correct solution, but hallucinate or cannot output one of the options exactly. We calculate the accuracy by comparing the exact match between the generated answer and the corresponding ground truth.\nAn example of the different components of the prompt is shown in Figure 4. Standard and CoT prompting strategies are comprised of the same components shown in the figure. The only difference is that zero-shot prompts omit the explicit REASONING step, and CoT prompts would use two different INPUT TAGS between the trigger prompt and the answer-eliciting prompt. As mentioned, CoT is performed through a two-step approach. The first step extracts the REASONING component (Figure 4). The second step appends the REASONING component to the trigger prompt to generate the final answer.\nTo create the reason extraction prompt, we take the same format as the few-shot prompts, but append the TRIGGER PROMPT (e.g., \"Let's think step by step\") after the list of OPTIONS."}, {"title": "Results and Discussion", "content": "We show the potential of ORQA as a useful benchmark for LLMs across multiple dimensions of interest. Namely, model size, model type, prompting strategy, triggering prompt, and question type. Here are some key findings:\nModel size contributes to reasoning performance. Not surprisingly, an LLM's reasoning ability is correlated to the size of an LLM. As shown in Table 2, this is true when comparing models within the same family (e.g., Llama 3.1). However, some models such as Mistral-7B and Flan-T5 perform better than Llama2-13B despite being smaller. This supports the findings from (Jiang et al. 2023) that Mistral 7B outperforms Llama2-13B across their evaluation benchmarks potentially due to Mistral's application of sliding window attention that allows Mistral to better handle the long natural language model descriptions (Jiang et al. 2023).\nCoT generally drops the performance. ICL examples benefit standard but not CoT prompting. Answering ORQA questions requires multi-step reasoning, and CoT prompting has been shown to elicit reasoning in LLMs (Wei et al. 2023). However, our experiments surprisingly indicate decreased performance on ORQA when using CoT (Table 2 and Figure 6). By investigating the generated reasoning, we found that the models often ignored instructions. For example, although the prompt specifies that only one option is correct, the models would attempt to generate reasoning that selects none or multiple options. We also report hallucinations where models would create their own options and select those. Finally, the reasonings were often incorrect, as highlighted in Figures 9 and 10 in Appendix. A promising direction is to explore more advanced CoT prompting techniques, including possible extensions of the faithful CoT reasoning presented by Lyu et al. (2023).\nTrigger prompts impact performance. Prompt ensembling improves CoT performance. Inspired by Li\u00e9vin et al. (2024), we experimented with different trigger prompts for CoT. These experiments were conducted using the following settings: 0-shot with Llama-3.1-70B-Instruct, temperature set to 0.7, and each trigger prompt was run five times. The results are recorded in Table 3. Different trigger prompts can change the performance, with scores ranging from 0.648 to 0.689.\nFair performance on reading comprehension. Poor performance if a question requires OR knowledge and model building knowledge. As shown in Figure 5, the questions requiring only reading comprehension (left-side of the heatmap) resulted in fair performance. However, the questions that require OR and/or model building knowledge (right-side of the heatmap) were too difficult and many LLMs performed poorly. These trends can also be seen in Figure 6 where LLMs performed worse on the more difficult questions. To enable LLMs to deal with such complex questions is a critical venue for future work. One possibility is providing domain-specific knowledge bases or enabling API calls during reasoning, which has been shown to enhance an LLM's ability to perform knowledge-intensive reasoning tasks (Yao et al. 2023). Another option would be to explore architecture alternatives to transformers such as StripedHyena (Poli et al. 2023), that could better deal with the concerning findings of Dziri et al. (2024) which suggest transformer-based LLMs reduce compositional reasoning into linearized subgraph matching. In essence, these findings seem to indicate that transformer-based LLMs are inherently limited in their ability to perform complex multi-step reasoning. Along these lines, ORQA could represent a useful benchmark for testing LLMs based on those novel architectures.\nIn-context learning can reduce the number of reasoning steps and reasoning errors. We analyzed reasoning steps generated by Llama2-13b-chat model in 0-shot and 1-shot CoT settings on our validation set. OR experts manually evaluated a total of 90 instances (45 per setting) and classified the reasoning into four categories: correct reasoning, incorrect logic, insufficient knowledge, and incorrect reading comprehension. Representative examples are provided in Appendix (Figure 10). As shown in Table 4, the number of reasoning steps in the 0-shot setting is significantly higher than in the 1-shot setting. This increases the likelihood of errors in intermediate steps, explaining the lower accuracy in 0-shot reasoning. Interestingly, in 20% of 0-shot tests, the model arrived at the correct answer despite some reasoning errors. Lastly, correct reasoning strongly correlates with finding the correct answer. Please refer to Appendix for our analysis on the relations between reasoning error types and question categories.\n- Length of ICL examples has a greater influence on accuracy than the similarity of question types. We explore the impact of ICL example selection on ORQA by running four experiments on the Llama2-13B-Chat with 1-shot CoT: (1) random ICL selection, (2) same question type, (3) similar prompt length, (4) same question type and similar prompt length. Table 5 presents the results. We found that while both the length of ICL examples and the similarity of question types improved the performance, an example of similar length had more impact."}, {"title": "Conclusion and Future Works", "content": "LLMs have received recognition and popularity through interfaces like ChatGPT. However, as they are being used more in high-stakes fields such as medicine and law, we must understand the reasoning behind these LLM responses. Thus, we developed the Operations Research QA (ORQA) benchmark and utilized it to evaluate the generalization and reasoning abilities of some popular pretrained LLMs within a novel and technically complex domain. We explored different prompting strategies to evaluate perspectives of reasoning using this multi-choice QA benchmark dataset. Our results show that there is still considerable room for improvement across different LLMs with Llama3.1-405B-Instruct achieving the highest accuracy of 0.772 and human expert accuracy (on a subset) of 0.93.\nWe acknowledge that structured reasoning metrics, such as those in ROSCOE (Golovneva et al. 2023), reasoning graph accuracy and similarity in STREET (Ribeiro et al. 2023), and entailment tree comparisons (Dalvi et al. 2021), are promising for automating the evaluation of reasoning steps in ORQA. By making ORQA publicly available, we look forward to seeing it used as a benchmark for LLMs, especially for models trained specifically on the task of QA or OR and related fields. This would provide additional clarity to the difficulty of this benchmark and how general language models perform against domain or task-specific models. We also encourage experts to expand this dataset by introducing not only more OR multi-choice QA problems, but also additional multi-choice QA problems from other technical fields similarly requiring expert-level knowledge and reasoning skills with limited publicly available text corpora."}, {"title": "Maximize", "content": "$Z = \\sum_i$"}, {"title": "Subject", "content": "$\\sum \\epsilon_{i,j} \\leq 1$\n$X_{ij} \\in \\{0,1\\}$ $\\forall i, j$"}]}