{"title": "Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting", "authors": ["Xiangyu Zhao", "Chengqian Ma"], "abstract": "Large Language Models (LLMs) exhibit remarkable proficiency in addressing a diverse array of tasks within the Nat- ural Language Processing (NLP) domain, with various prompt de- sign strategies significantly augmenting their capabilities. However, these prompts, while beneficial, each possess inherent limitations. The primary prompt design methodologies are twofold: The first, ex- emplified by the Chain of Thought (CoT), involves manually craft- ing prompts specific to individual datasets, hence termed Expert- Designed Prompts (EDPs). Once these prompts are established, they are unalterable, and their effectiveness is capped by the ex- pertise of the human designers. When applied to LLMs, the static nature of EDPs results in a uniform approach to both simple and complex problems within the same dataset, leading to the ineffi- cient use of tokens for straightforward issues. The second method involves prompts autonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which provide tailored solutions to specific problems, mitigating the limitations of EDPs. However, LDPs may encounter a decline in performance when tackling com- plex problems due to the potential for error accumulation during the solution planning process. To address these challenges, we have con- ceived a novel Prompt Recursive Search (PRS) framework that leverages the LLM to generate solutions specific to the problem, thereby conserving tokens. The framework incorporates an assess- ment of problem complexity and an adjustable structure, ensuring a reduction in the likelihood of errors. We have substantiated the effi- cacy of PRS framework through extensive experiments using LLMs with different numbers of parameters across a spectrum of datasets in various domains. Compared to the CoT method, the PRS method has increased the accuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22% improvement.", "sections": [{"title": "1 Introduction", "content": "Stem cells differentiate into other types of cells with distinct func- tions upon induction by signaling molecules[2, 20]. A similar pro- cess, as depicted in 2, is proposed by us in the use of Large Language Models (LLMs) to solve problems. Previous work[26, 9, 31, 30, 13] has indicated that a single interaction often fails to adequately ad- dress problems. Instead, a more effective approach is to divide the problem-solving process into multiple steps: first proposing a solu- tion to the problem, and then executing it step by step by the LLM. Each interaction with the LLM involves input to the model and its subsequent output. A single interaction can lead to the refinement, detailing, summarization, or execution of a solution. Each interac- tion corresponds to the realization of a thought within the thoughts that make up the solution. The functionality of a thought evolves from a more basic state to become increasingly specific, much like how a stem cell differentiates into a cell with a particular function. Similarly, an original thought can differentiate in response to various problems, giving rise to thoughts capable of solving different issues. Control over the evolution of thoughts traditionally involves two methods. The first method entrusts the transformation process of thoughts entirely to experienced domain experts. These experts can, based on their understanding of problems within a particular field, plan the problem-solving process in a sequential chain, addressing the problem step by step. This is the contribution of the Chain of Thought (CoT) approach[26]. Prior to this, the practice of present- ing a problem to LLMs and expecting a direct answer placed exces- sive demands on the LLMs, yielding suboptimal results and failing to fully leverage LLM's potential. Given that there is often more than one method to solve a problem, and multiple approaches can yield answers for a specific issue, we can compare the answers derived from various methods to select the most optimal one, thus obtaining the best solution to the problem. Plan-and-Solve (PS) [24] Prompt- ing is a concrete implementation of this line of thinking. Building upon this, the Tree of Thought (ToT) [27] further extends this work by considering a scenario where a thought represents a critical step in the solution. If this thought is flawed, continuing to reason based on it can lead to more severe errors. Therefore, the authors of ToT propose a prompt structure that allows for a retreat from an erro- neous solution path to reconsider and pursue the correct approach. This structure is hierarchical, and the feature is known as the back- tracking function during the tree traversal process. On the foundation of ToT, the Graph of Thought (GoT) [3] structure is even more flexi- ble. By performing operations such as aggregation and refinement on"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 LLM-Derived Prompts", "content": "Large Language Models (LLMs) are an advanced tool in the field of Natural Language Processing (NLP) technology [32, 33]. They are based on deep learning architectures, such as Transformers [22], and have learned complex patterns of language through extensive training on vast amounts of text data. These models are not only capable of understanding the nuances of language but also generating coherent and grammatically correct text, thereby performing well on a variety of language tasks. They can be used in a wide range of applications, including chat-bots[17], recommendation systems[12], content cre- ation aids[5], and educational tools [16, 14]. However, LLMs may also replicate and amplify biases present in their training data, so ethical[4, 7, 1, 14] and bias [28, 15] considerations must be taken into account in their design and use. Notable examples of LLMs include the BERT[6] and GPT[19] models, which have achieved significant success in tasks involving natural language understanding[19, 23] and generation[11, 8]."}, {"title": "2.2 Expert-Designed Prompts", "content": null}, {"title": "2.2.1 Chain of Thought(CoT)", "content": "This technology is designed to enhance the ability of Large Language Models (LLMs) to handle complex issues. It simulates the continu- ous thought process of human problem-solving by embedding a se- ries of logical reasoning steps within the input prompts. These steps progressively guide the model to the solution of the problem, aiding in the processing of tasks that require continuous logic or mathemat- ical computation. By demonstrating examples of problem-solving, the accuracy and logicality of the model's output can be improved, even without specific task training."}, {"title": "2.2.2 Plan-and-Solve (PS) Prompting", "content": "Compared with Chain of Thought (CoT), Plan-and-Solve Prompting significantly improves the performance of Large Language Models (LLMs) in multi-step reasoning tasks by guiding the models to for- mulate plans for solutions and then execute them. It has demonstrated superior performance over Zero-shot-CoT thinking across multiple datasets and is comparable to CoT methods that require manual ex- amples, showcasing the potential to stimulate the reasoning capabil- ities of LLMs without the need for manual examples."}, {"title": "2.2.3 Tree of Thoughts", "content": "The Tree of Thought (ToT) framework aims to enhance the capabili- ties of Large Language Models (LLMs) in problem-solving. It allows the model to explore various reasoning paths and to self-assess its decisions, leading to more deliberate choices. This approach views problem-solving as a search process within a \"thought tree,\" where"}, {"title": "2.2.4 Graph of Thoughts", "content": "Graph of Thoughts (GoT) significantly enhances the performance of Large Language Models (LLMs) in complex task processing by con- ceptualizing the reasoning process as a graph structure composed of nodes, which represent the model's \"thoughts,\" and edges, which de- note the connections between these thoughts. Not only does GoT im- prove the quality of task execution, with a 62% increase in efficiency over existing technologies in sorting tasks, but it also achieves a re- duction in costs by more than 31%. The design of GoT greatly facil- itates the approximation of LLMs' reasoning capabilities to human thought patterns."}, {"title": "2.2.5 Automatic Prompt Engineer(APE)", "content": "The manual design of prompts necessitates a high level of exper- tise from the designer and is constrained by the relatively limited knowledge base and subjective factors of human experts compared to Large Language Models (LLMs). Handcrafted prompts are subject to numerous limitations stemming from human factors, which LLMs are not bound by. Therefore, entrusting the task of prompt design to an LLM can both resolve these constraints and eliminate the incon- venience of manual prompt design, thereby automating the use of LLMs. Initially, the LLM is tasked with generating a set of prompts, which are then not utilized in their entirety. Instead, these prompts are evaluated and the highest-quality ones are selected. Based on the highest-quality prompts, a variety of similar prompts are generated to enhance their diversity."}, {"title": "2.2.6 Intent-based Prompt Calibration(IPC)", "content": "Large Language Models (LLMs) are highly sensitive to the input content, where even seemingly irrelevant random noise can signif- icantly alter the LLM's output. The root cause of this sensitivity is a lack of an accurate and comprehensive understanding of the prob- lem. To ensure that LLMs consider the problem thoroughly, based on the user's requirements and the initial prompt, new prompts and data samples are iteratively generated. The prompts are then optimized us- ing edge data, allowing the LLM to fully contemplate the problem."}, {"title": "2.2.7 Automatic Prompt Optimization", "content": "After the generation of prompts by a LLM, it is often necessary to re- fine them. This refinement can be accomplished through simple rank- ing and selection, known as Approach by APE, or by making mod- ifications to the prompt based on the data provided by the LLM, re- ferred to as IPC. However, when the objective is to identify the most appropriate prompt within a dense space of prompts, gradient descent emerges as a more rational method. The process begins with the LLM generating a response based on an initial prompt. Subsequently, the LLM analyzes the response in conjunction with the prompt to pro- duce Textual Gradients. These Textual Gradients are then integrated to formulate a new prompt. This cycle of generation and refinement is repeated iteratively until the optimal prompt is achieved."}, {"title": "3 Methodology", "content": "Our approach is founded upon the principles of the EDP and the LDP, taking into account that while EDP is adept at addressing straightfor- ward problems with a comprehensive framework designed to encom- pass all scenarios of such issues, it may necessitate superfluous token expenditure when applied to simpler problems. On the other hand, LDP can lead to the accumulation of errors; if an LLM is tasked with designing prompts without constraints or corrections, it must adhere to a process that involves planning a solution and then exe- cuting it. Errors introduced during the planning phase can propagate through to subsequent steps, potentially undermining the problem- solving process. To mitigate the costs associated with manually de- signed prompts, we delegate the responsibility of devising solutions to the LLM itself. The LLM will design tailored solutions for each problem, thereby avoiding unnecessary token usage. Furthermore, to prevent the accumulation of errors during the LLM's prompt de- sign phase, we employ complexity detection methods and procedural planning techniques to constrain the LLM and validate its solution planning. Our method is showed in 1 and 2."}, {"title": "3.1 Evaluating the Complexity of a Problem", "content": "Before devising solutions for a problem, we first assess its complex- ity to facilitate the design of more comprehensive solutions for more complex issues and more straightforward solutions for simpler ones. The complexity level can provide effective guidance for formulating our solutions. In the process of evaluating the complexity of a prob- lem by a Large Language Model (LLM), we consider the characteris- tics of textual descriptions of problems: directly describing the com- plexity using natural language can lead to an inability to distinctly differentiate between problems based on this metric. Therefore, we opt to use numerical values instead of the original complexity de- scriptions. We categorize the complexity of problems into ten levels, represented by ten integers ranging from 1 to 10."}, {"title": "3.2 Planning Solution for a Problem", "content": "Upon determining the complexity of a problem, our focus shifts to designing solutions based on that complexity. In practice, the aspect of a solution that is directly related to complexity is the number of steps involved. Through extensive experimentation, we have corre- lated the complexity levels proposed by the Large Language Model (LLM) for problems with the steps required in their solutions. Our findings indicate a linear relationship: the number of steps to resolve a problem typically ranges between one and five, and the complex- ity level of a problem, when divided by two, often yields the number of steps needed for its resolution. This macroscopic statistical pattern allows us to seamlessly incorporate the initial complexity assessment into the design of solutions, thereby specifying the number of steps required for resolution."}, {"title": "3.3 Recursively Proposing Solutions to Problems", "content": "Following the first two steps, we have established the necessary steps for a solution, which are determined based on the complexity of the problem. This approach, underpinned by our discovered relationship between complexity levels and solution steps, mitigates the accumu- lation of errors during the problem formulation by the LLM. It ad- dresses the shortcomings of the LLM-Derived Prompt (LDP) while fully leveraging its strengths in devising tailored solutions for spe- cific problems, thereby also resolving the issues associated with the Expert-Designed Prompt (EDP)."}, {"title": "3.4 Obtaining a complete solution", "content": "Subsequently, we can employ the identified solution steps to propose resolutions. Recognizing that providing detailed steps for complex problems in a single response demands a high level of proficiency from the LLM, we adopt a recursive strategy for the more intricate steps of the solution. Each complex step is treated as a new problem to be broken down, and a more refined solution is proposed, prompt- ing the LLM to elaborate on the specifics of the solution. This recur- sive process continues until all solutions are simplified to the point where they can be resolved in a single step. By transforming com- plex problems into simpler ones, we prevent the LLM from being overwhelmed by complexity and ensure the provision of accurate so- lutions."}, {"title": "3.5 Obtaining an Answer according to the solution", "content": "After obtaining a solution through the preceding plan, we can then apply this solution to address the problem. Additionally, we can im- pose constraints on the format of the answers outputted by the Large Language Model (LLM) with respect to specific datasets, ensuring the production of coherent and appropriate responses."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Model Selection", "content": "In the process of model selection, considering that Large Language Models (LLMs) with a substantial number of parameters inherently possess higher performance, the advent of prompt design method- ologies has been predominantly aimed at enhancing the capabilities of LLMs with moderate to smaller parameter counts. Moreover, the efficacy of prompt design methods is more pronounced in LLMs with smaller parameter volumes. We can enhance the performance of LLMs by designing prompts, fully tapping into the potential of LLMs; however, LLMs with a larger number of parameters already possess relatively good capabilities, and even with the use of prompt techniques, the improvement in LLM performance may not be very significant. We will demonstrate this point through the results of ab- lation experiments. Consequently, we have elected to utilize the Yi- 34B model, which has a moderately sized parameter volume, along- side the Meta-Llama-3-8B model, which features a smaller parame- ter count.\nYi-34B The Yi-34B model[29] is a middle-scale language model trained on a diverse corpus of 3 terabytes, designed with a bilin- gual focus. It has demonstrated significant potential across various aspects such as language comprehension, common sense reasoning, and reading comprehension, indicative of the emergent capabilities associated with large models[25]. This emergent capability implies that LLMs can exhibit human-like logical thinking, reasoning, and"}, {"title": "4.2 Dataset Introduction", "content": "The BIG-Bench Hard (BBH) is a subset culled from the original BIG-Bench assessment suite, focusing on tasks that pose a challenge to existing language models. BBH comprises 23 tasks and 27 sub- datasets, and when creating the BBH dataset, researchers adhered to specific filtering criteria, including the number of examples in the task, the presence of human rater performance data, the type of task, and the performance of previous models, among others. This dataset is designed to drive improvements in language model performance on complex reasoning tasks and provides a valuable benchmark for future research. This dataset is relatively difficult and covers a wide range of topics, with many of its sub-datasets focusing on assessing the reasoning capabilities of LLMs, making it an ideal tool to test the abilities of PRS."}, {"title": "4.3 Set Up", "content": "We have confirmed the linear relationship between the complexity of a problem and the number of steps required to solve it through exper- iments conducted across multiple datasets. On the Llama3 model and the BBH dataset, this linear relationship is specifically manifested as the number of steps required to solve a problem being half of its com- plexity. Therefore, once we have ascertained the complexity level of"}, {"title": "4.4 Evaluation Metrics.", "content": "After obtaining the answer from the Large Language Model (LLM), it is necessary to compare it with the correct answer to assess its ac- curacy and calculate the rate of correctness. There are two primary methods for comparison: The first method involves the LLM itself comparing the provided answer to the correct one and then making a judgment. This approach is contingent on the performance of the LLM, which carries the potential for error. The second method is ap- plicable only when the correct answers in the dataset adhere to a more uniform and standardized format. In this case, we simply instruct the LLM to pay attention to the format when presenting the answer. Sub- sequently, we can employ regular expressions to extract the answer and use string methods for comparison. This method demands less from the LLM's performance but has a more limited range of appli- cability. The answer format for the problems in the BBH dataset is relatively standardized, therefore we can employ the second method, which is more accurate for making judgments.\nSince the correct answers in the dataset we used have specific formatting requirements, we provided the LLM with the answer to the first sample in the dataset as a template, instructing the LLM to mimic the format of the first answer when responding. The evalua- tion of the LLM's responses begins with the second sample, ensuring that the format of the LLM's answers is maintained while preventing data leakage.\nBefore determining the correctness of the answers, we categorized the types of answers in the dataset, dividing them into single-choice, multiple-choice, numerical, etc., and formulated appropriate regular expressions to match the answers accordingly."}, {"title": "4.5 Result", "content": "We conducted extensive experiments on the BBH dataset, where out of its 27 sub-datasets, only 24 were found to be valuable for explo- ration. The \"dyck_languages\" and \"multistep_arithmetic_two\" tasks, which were discarded, primarily assess the LLM's ability to discern"}, {"title": "4.6 Ablation Experiment", "content": "By applying this framework to the Yi-34B model, we also compared the CoT and PRS methods and found that the average accuracy of PRS increased from 45% with the CoT method to 49%, achieving a 9% improvement. This demonstrates that our method is also applica- ble to LLMs with a larger number of parameters and also validates our viewpoint that the enhancement effect of prompt design methods on LLMs with large parameter volumes is relatively less pronounced."}, {"title": "5 Conclusion", "content": "Traditional prompt design methods can be divided into LDP and EDP, where EDP heavily relies on the subjective experience of hu- man experts. Its immutable characteristics once involved make it waste redundant computational resources when dealing with simple problems. On the other hand, LDP is generated and optimized by LLMs, but due to the lack of effective supervision during the prompt generation process, it is prone to error accumulation. We have in- tegrated the advantages of both by designing a brand-new prompt framework called PRS. It eliminates the high dependence on human expert knowledge through automatic prompt design and saves com- putational resources. By effectively supervising the complexity of the problem, it prevents error accumulation during the reasoning process. We have demonstrated the effectiveness of this framework by com- paring its performance with the CoT method using the Llama3-7B model across multiple domain datasets, as well as with the ablation experiments using the Yi-34 model."}, {"title": "6 Limitations", "content": "By summarizing traditional prompt design techniques, we have categorized traditional prompt design methods into EDP (Expert- Designed Prompts) and LDP (LLM-Derived Prompts). Upon discov- ering the complementary nature of the advantages between the two, inspired by the differentiation phenomenon of stem cells in biolog- ical principles, we have innovatively proposed a new prompt struc- ture: Prompt Recursive Search (PRS). This structure can automati- cally design prompts for problems, which has the advantage of sav- ing computational resources. At the same time, by judging the com- plexity of the problem, it supervises the errors in the prompt design process, thereby ensuring accuracy. Through experiments in multiple domains and with various models, we have demonstrated the effec- tiveness of PRS. However, it is undeniable that our framework still has certain shortcomings, which are as follows:\n1. We mandate that the Large Language Model (LLM) outputs an- swers in a specific format through our prompt, as deviation from this format would render the true-false judgment infeasible. How- ever, the LLM is not always compliant with the prescribed output format, and answers that do not conform to the format could be either erroneous or correct. Our condition for deeming an LLM- provided answer as correct stipulates that not only must the con- tent be accurate, but the format must also be correct. This es- sentially elevates the criteria for acceptable responses. Situations where the content is correct but the format is not are not accounted for in our assessment, leading to an underestimation of our true accuracy rate in the statistics we compile.\n2. Our framework is predicated on a crucial assumption: the com- plexity of a problem is directly proportional to the number of steps required to solve it. We have substantiated this intuitive as- sumption through numerous experiments, yet it is important to recognize that this assumption represents a macroscopic rule and there are a few exceptional cases that do not conform to it. Con- sequently, the number of steps necessary to resolve a problem re- mains a topic worthy of exploration. We thus leave this question for future research endeavors.\n3. Even for the same input, the responses from the Large Language Model (LLM) can vary with each invocation, thus reflecting that the performance of the Prompt Recursive Search (PRS) frame- work has an inherent degree of randomness. The experimental re- sults presented in this paper are the average of three trials, which may still differ from the true capabilities of the method.\n4. Due to the inherently strong capabilities of Large Language Mod- els (LLMs) with a larger number of parameters, applying the Prompt Recursive Search (PRS) method proposed in this paper does not yield a significant enhancement in this kind LLM's abili- ties. In fact, the effectiveness of this framework is contingent upon the LLM's analytical capacity regarding the problem at hand. Con- sequently, LLMs with a larger parameter count are better suited to leverage the full potential of the PRS framework. Our ablation study proved this perspective but did not delve into a detailed in- vestigation. We reserve the exploration of this aspect for future work.\nIn summary, we have proposed a brand-new method for prompt de- sign and have validated it through extensive experiments, providing a starting point for future research."}]}