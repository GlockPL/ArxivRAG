{"title": "Fourier Spectral Physics Informed Neural Network: An Efficient and Low-Memory PINN", "authors": ["Tianchi Yu", "Yiming Qi", "Ivan Oseledets", "Shiyi Chen"], "abstract": "With growing investigations into solving partial differential equations by physics-\ninformed neural networks (PINNs), more accurate and efficient PINNs are required\nto meet the practical demands of scientific computing. One bottleneck of current\nPINNs is computing the high-order derivatives via automatic differentiation which\noften necessitates substantial computing resources. In this paper, we focus on\nremoving the automatic differentiation of the spatial derivatives and propose a\nspectral-based neural network that substitutes the differential operator with a\nmultiplication. Compared to the PINNs, our approach requires lower memory and\nshorter training time. Thanks to the exponential convergence of the spectral basis,\nour approach is more accurate. Moreover, to handle the different situations between\nphysics domain and spectral domain, we provide two strategies to train networks\nby their spectral information. Through a series of comprehensive experiments, We\nvalidate the aforementioned merits of our proposed network.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancements in machine learning and its related theories, integrating mathematical\nmodels with neural networks provides a novel framework for embedding physical laws in scientific\nresearch. The representative methods are the Physics-Informed Neural Networks (PINNs) [1] and\nthe Deep Ritz method [2]. PINNs have garnered significant attention because of their ability of\nsolving partial differential equations (PDEs) without suffering from the curse of dimensionality\n(CoD) [3, 4], thanks to the Monte Carlo method [5], automatic differentiation (AD) [6], and universal\napproximation theorem [7]. Additionally, PINNs demonstrate the merits in handling imperfect\ndata [8], extrapolation [9, 10], and interpolation [11]. These capabilities have propelled PINNs into\na wide applications, including but not limited to fluid dynamics [12], aerodynamics [13], surface\nphysics [14], power systems [15], and heat transfer [16].\nHowever, using AD to compute the loss function (mainly the residual term) during training, is\ninefficient and computational expensive. To address this problem, various approaches have been\nproposed that leverage alternative numerical methods to replace AD. For instance, PhyCRNet [10, 17]\nutilizes the finite difference method (FDM) to replace AD; DTPINN [18] applies the finite difference"}, {"title": "2 Physics-informed neural networks (PINNs)", "content": "We briefly review the physics-informed neural networks (PINNs) [1] in the context of inferring the\nsolutions of PDEs. Generally, we consider PDEs for u taking the form\n$\\begin{aligned}\n\\partial_t u + \\mathcal{N}[u] &= 0, &t \\in [0,T], \\mathbf{x} \\in \\Omega, \\\\\nu(0, \\mathbf{x}) &= g(\\mathbf{x}), &\\mathbf{x} \\in \\Omega, \\\\\n\\mathcal{B}[u] &= 0, &t\\in [0,T], \\mathbf{x} \\in \\partial \\Omega,\n\\end{aligned}$\nwhere $\\mathcal{N}$ is the differential operator, $\\Omega$ is the domain of grid points, and $\\mathcal{B}$ is the boundary operator.\nThe ambition of PINNs is to obtain the unknown solution u to the PDE system Eq. (1), by a neural\nnetwork $u_{\\theta}$, where $\\theta$ denotes the parameters of the neural network. The constructed loss function is:\n$\\mathcal{L}(\\theta) = \\mathcal{L}_{ic}(\\theta) + \\mathcal{L}_{bc}(\\theta) + \\mathcal{L}_{r}(\\theta),$"}, {"title": "2.1 Automatic differentiation (AD)", "content": "AD gives the required derivative of an overall function by combining the derivatives of the constituent\noperations through the chain rule based on evaluation traces. Herein, AD is used to calculate\nthe derivatives respect to the input points in PINNs. However, AD demands both memory and\ncomputation that scale exponentially with the order of derivatives (Fig. 1(a)) although there are\ninvestigations [21, 24, 25] on computing high-order derivatives efficiently by Fa\u00e0 di Bruno's formula.\nAlternatively, replacing the high-order derivatives by simple multiplication, SINNs can reduce both\nmemory and training time (Fig. 1(a) and Table 3)."}, {"title": "3 Fourier Spectral Physics Information Neural Networks (SINNs)", "content": "To implement spectral method into PINNs, the Fourier operator $\\mathcal{F}$ is applied to Eq. (1), converting\nthe solution from physics domain to the frequency domain. Practically, we use $\\mathcal{F}_N$ to represent the\n$N$-th truncated Fourier operator:\n$\\mathcal{F}_N[u] (t, \\mathbf{x}) = \\sum_{k=-N/2}^{N/2-1} \\hat{u} (t, k) e^{ikx},$"}, {"title": "3.1 Physical model for heat transfer", "content": "Ideally, the heat transfer can be described by the heat equation which is investigated widely in\nmathematics as one of the prototypical PDEs. Given $\\Omega \\subset \\mathbb{R}^2$, consider the 2-D heat equation with\nperiodic boundary condition:\n$\\begin{aligned}\n\\partial_t u(t, \\mathbf{x}) &= \\partial_{xx}u(t, \\mathbf{x}) + \\partial_{yy}u(t, \\mathbf{x}), &t\\in [0,T], \\mathbf{x} \\in \\Omega, \\\\\nu(0, \\mathbf{x}) &= g(\\mathbf{x}), &\\mathbf{x} \\in \\Omega.\n\\end{aligned}$\nWhen applying on the loss functionEq. (2), the residual loss $\\mathcal{L}_r(\\theta)$ is explicitly expressed by:\n$\\mathcal{L}_r(\\theta) = \\frac{1}{N_r} \\sum_{i=1}^{N_r} |\\partial_t u_{\\theta}(t^i, \\mathbf{x}^i) - \\partial_{xx} u_{\\theta}(t^i, \\mathbf{x}^i) - \\partial_{yy}u_{\\theta}(t^i, \\mathbf{x}^i)|^2,$"}, {"title": "3.2 Physical model for incompressible flows", "content": "The mathematical model describing incompressible flows is the incompressible NS equation, namely,\n$\\begin{aligned}\n\\nabla \\cdot \\mathbf{u} &= 0, &t \\in [0,T], \\mathbf{x} \\in \\Omega, \\\\\n\\partial_t \\mathbf{u} + \\mathbf{u} \\cdot \\nabla \\mathbf{u} &= -\\nabla p + \\nu \\triangle \\mathbf{u}, &t\\in [0,T], \\mathbf{x} \\in \\Omega, \\\\\n\\mathbf{u}(0, \\mathbf{x}) &= g(\\mathbf{x}), &\\mathbf{x} \\in \\Omega,\n\\end{aligned}$\nwhere $\\nabla = (\\partial_x, \\partial_y)$ is the gradient operator, $\\mathbf{u}(t, \\mathbf{x}) = (u, v)$ is the hydrodynamic velocity, $p(t, \\mathbf{x})$ is\nthe mechanical pressure, $\\triangle = \\nabla \\nabla$ is the Laplace operator, and $\\nu$ is kinematic viscosity. By Fourier\ntransform and appropriate derivation tricks (see Appendix D), the continuity equation Eq. (11a) and\nmomentum equation Eq. (11b) can be expressed in the spectral domain:\n$\\begin{aligned}\n\\mathbf{k}\\cdot \\hat{\\mathbf{u}} &= 0, \\\\\n\\partial_t \\hat{\\mathbf{u}} &= \\Big(-i \\frac{\\mathbf{k}\\mathbf{k}}{\\|\\mathbf{k}\\|^2} \\hat{\\mathcal{N}} - \\nu\\|\\mathbf{k}\\|^2\\Big)\\hat{\\mathbf{u}},\n\\end{aligned}$\nwhere $\\|\\mathbf{k}\\|^2 = \\mathbf{k} \\cdot \\mathbf{k}$ is the inner product for $\\mathbf{k} = (k_x, k_y)$, and $\\hat{\\mathcal{N}}$ is the non-linear term in the\nspectral domain, which has the rotational form $\\hat{\\mathcal{N}} = (\\nabla \\times \\mathbf{u}) \\times \\mathbf{u}$ in the physical space [22]. The\ncontinuity equation Eq. (12a) can be preserved strictly by the projection in our SINNs; however, the\nnon-linear term has the challenge of dealing with the aliasing error, which will be further discussed\nin Appendix B. After solving the aliasing error, the loss function is\n$\\mathcal{L}(\\theta) = \\frac{1}{N_r} \\sum_{i=1}^{N_r} |\\partial_t \\hat{\\mathbf{u}}(\\mathbf{t}_i, \\mathbf{k}) + \\mathcal{F}^{-1} \\Big[\\Big(i \\frac{\\mathbf{k}\\mathbf{k}}{\\|\\mathbf{k}\\|^2} \\hat{\\mathcal{N}}(\\mathbf{t}_i, \\mathbf{k})\\Big) + \\nu\\mathcal{F}^{-1} [\\|\\mathbf{k}\\|^2\\hat{\\mathbf{u}}(\\mathbf{t}_i, \\mathbf{k})]\n\\Big]|^2,$"}, {"title": "3.3 Importance optimisation", "content": "Compared to PINNs, the main divergence is the importance of different input points. Although the\nliterature on sampling method [26, 27, 28] shows that the importance of the input points in physics\ndomain can be dependent on the corresponding residual, generally speaking, every point is equally\nimportant without any prior knowledge. But for SINNs, normally the importance decreases as the\ncorresponding frequency increases. For instance, the energy spectrum in the inertial ranges of 2-D\nturbulence (described by the 2-D NS equation) satisfies the scaling relation [29]:\n$\\sum_{n-\\leq|k|<n+\\Delta}\\|\\hat{u} (t, k) \\|^2 \\sim n^{-3}.$\nSimilar physical analysis can be performed for 1-D problems, and the physical background demon-\nstrates that the Fourier coefficient $\\hat{u}$ decreases rapidly as an increase in frequency due to the effect\nof viscosity. In practical, comprehensive experiments in Fourier Neural Operator [30] show that the\ntruncated Fourier modes can contain most features of the PDEs. Herein, PINNs learn every input\npoint equally, while SINNs are supposed to learn the low-frequency points preferentially. To train the\nnetwork based on the aforementioned divergence, we propose two strategies:\nSampling by importance (SI): Suppose $p(k)$ is the probability density function (PDF) used to\nsample the residual points, then the $p(k)$ in sampling by importance is defined as:\n$p(k) \\propto \\frac{1}{\\|k\\|_1 + \\beta} + \\alpha,$"}, {"title": "3.4 Spectral convergence", "content": "Regardless of the convergence analysis in temporal domain,assume that the capability of MLP is\npowerful enough, $u \\in C^{\\infty}(\\Omega, \\mathbb{R})$ is a smooth function from a subset $\\Omega$ of a Euclidean $\\mathbb{R}$ space to a\nEuclidean space $\\mathbb{R}$, and N is the number of discretized points.\nFirstly, let's review the convergence rate of PINNs. Suppose $u^*$ is the exact solution in the domain $\\Omega$\nand\n$\\begin{aligned}\n\\theta^* &\\triangleq \\mathop{\\arg\\min}\\limits_{\\theta} \\int_\\Omega \\mathcal{L}_r [u^{\\theta}(x)] dx, \\\\\n\\theta^N &\\triangleq \\mathop{\\arg\\min}\\limits_{\\theta} \\sum_{i=1}^N \\mathcal{L}_r [u^{\\theta}(x_i)] .\n\\end{aligned}$\nThen\n$\\begin{aligned}\n\\Vert u^{\\theta^N} - u^*\\Vert_{\\Omega} \\le \\Vert u^{\\theta^N} - u^{\\theta^*}\\Vert_{\\Omega} + \\Vert u^{\\theta^*} - u^*\\Vert_{\\Omega}, \\\\\n\\underbrace{\\text{statistical error}} \\underbrace{\\text{approximation error}}\n\\end{aligned}$\nwhere approximation error depends on the capability of PINNs. As the capability of MLP is powerful\nenough, $\\|u^{\\theta^*} - u^*\\Vert_{\\Omega} < \\Vert u^{\\theta^N} - u^{\\theta^*}\\Vert_{\\Omega}$. Additionally, based on the Monte Carlo method, the\nstatistical error is $O \\left(N^{-1/2}\\right)$ [32], then:\n$\\Vert u^{\\theta^N} - u^*\\Vert_{\\Omega} = O \\left(N^{-1/2}\\right).$\nAs for SINNs, with N discretized points, the truncated $u^*$ is $u_N = \\sum_{k=-\\frac{N}{2}}^{\\frac{N}{2}-1}\\hat{u}^*(k)e^{ikx}$, suppose\n$\\begin{aligned}\n\\hat{\\theta}_N \\triangleq \\mathop{\\arg\\min}\\limits_{\\hat{\\theta}} \\sum_{i=1}^N \\mathcal{L} [\\hat{u}^{\\theta} (k_i)].\n\\end{aligned}$\nThen\n$\\Vert u^{\\hat{\\theta}_N} - u^*\\Vert_{\\Omega} \\le \\Vert u^{\\hat{\\theta}_N} - u_N\\Vert_{\\Omega} + \\Vert u_N - u^*\\Vert_{\\Omega}\n\\underbrace{\\text{spectral error}}$\nAs the capability of MLP is powerful enough, $\\|\\hat{\\theta}_N - u_N\\Vert_{\\Omega} \\le \\sum_{k = -N/2}^{N/2-1} \\|\\hat{u}^*(k) - \\hat{u}^\\theta(k)\\|$ Moreover, as the spectral error is exponential convergence [22], then:\n$\\Vert u^ - u^*\\Vert_{\\Omega} = o(N^{-s}), \\forall s > 0.$\nThus, the convergence rate of SINNs is $o(N^{-s})$ for any $s > 0$ while the convergence rate of PINNs\nis $O(N^{-1/2}).$"}, {"title": "4 Experiments", "content": "To demonstrate the performance of the proposed SINNs, we consider training the linear equations\n(Table 1) using the framework shown in Section 3.1 and training the non-linear equations (Table 2)\nusing the framework shown in Section 3.2. The details of the equations and training are available\nin Appendix C, the details of the relative L2 error metric used in our experiments are described in\nAppendix C.3."}, {"title": "4.1 Different order of derivatives", "content": "To compare the efficiency between our SINNs and PINNs, we consider a specific one-dimensional\n(1-D) hyper-diffusion equation with different order of derivatives:\n$\\begin{aligned}\n\\frac{\\partial u}{\\partial t} + \\epsilon \\frac{\\partial^p u}{\\partial x^p} &= 0, &x\\in [0, 2\\pi], t \\in [0, T], \\\\\nu(0, x) &= \\sum_{k=0}^{N-1} sin(kx),\n\\end{aligned}$\nwhere $p$ is the order of the spacial derivatives. To balance the solution of different order, we set\n$\\epsilon = 0.2^p, T = 0.1$ in our experiments. The results are shown in Fig. 1 and Table 3.\nTraining time One may argue that in Table 3, for the most general derivative term $p = 2$, PINNs\nare more efficient than SINNs. It is because SINNs have the imaginary part thus the output channels\nare double the output channels of PINNs. However, we have a more comprehensive comparison in\nAppendix F, and the conclusion is: if the spacial derivative terms are more than one second-order\nderivative, including one third-order derivative, or one second-order derivative plus one first-order\nderivative, our SINNs are more efficient than PINNs; otherwise, PINNs are more efficient."}, {"title": "4.2 Different spectral structures", "content": "To verify the approximation of SINNs is not only for the low-frequency solutions, experiments on the\ndiffusion equation (Eq. (25)) are implemented with different N:\n$\\begin{aligned}\n\\partial_t u + a \\partial_x u - \\epsilon \\partial_{xx} u &= 0, &x\\in [0, 2\\pi], t \\in [0, T], \\\\\nu(0, x) &= \\sum_{k=0}^{N-1} sin (kx).\n\\end{aligned}$"}, {"title": "4.3 Ablation study of the SI and WL", "content": "SI and WL are different approaches designed for the same object, in this section, we provide an\nablation study of how to choose the strategies of SI and WL, and how to set the corresponding\nhyperparameters. To tune the hyperparameters, one should consider the distribution of the Fourier\ncoefficients: the coefficients concentrate more on the low-frequency, and you should have larger $\\epsilon$ for\nWL or smaller $\\alpha$ and smaller $\\beta$. To select the approach, ideally, WL is better because this approach\ncan learn all coefficients fully. However, as the relationship between the capability of networks and\nthe complexity of PDEs is unknown, using WL requires additional consideration of the size of the\nnetwork. If some low-frequencies are out of the capability, WL makes the network continue focusing\non them because the hard constraint on the loss function makes the rest frequencies be multiplied\nby a small weight. So in practice, SI performs better thanks to its soft constraint on the structure of\nSINNs. We did a series experiments on diffusion equation to show the influence of hyperparameters.\nThe results are depicted in Figs. 4(a) and 4(b) and the details can be found in Appendix G.\nAdditionally, although we think SI and WL have the same effect, thus using both of them isn't\ncost-effective, we did extra experiments that combine SI and WL strategies together to verify how"}, {"title": "5 Conclusion and future work", "content": "In this study, the Spectral-Informed Neural Networks (SINNs) are developed to approximate the\nfunction in the spectral domain. We point out that the difference in learning between the physical\ndomain and the spectral domain is the importance of input points, and therefore specific training\nstrategies are introduced. The chosen Fourier basis helps us compute the spatial derivatives as well\nas train the neural network efficiently and with low memory allocation. Besides, SINNs have more\naccurate predictions due to the exponential accuracy of the spectral method. To provide evidence\nthat SINNs have not only a notable reduction in training time but also significant improvements in\naccuracy, we did a series of experiments on linear and non-linear PDEs.\nLimitations The current SINNs also inherit the disadvantages of spectral methods, for some PDEs\nwith complex geometries or detailed boundaries in more than one space variable would cause spectral\nmethods to collapse, and so would SINNs. On the other hand, functions are often more complex in\nthe spectral domain than in the physical domain, thus those functions that cannot be approximated\nby PINNs always cannot be approximated by SINNs with the same architecture. As for non-linear\nequations, in our experiments, SINNs are usually unstable when dealing with the aliasing error, due\nto the nonnegligible bias between Eq. (13) and the ideal loss that the aliasing error is zero.\nFuture Apart from the positive results shown in this paper, the above limitations remain to be\ninvestigated further in the future. For the inherited disadvantages from spectral methods, in essence,\nthe spectral method is a specific type of collocation methods that rely on selected basis functions\nsatisfying boundary conditions. Similar to the spectral methods, the collocation methods ensure\nthe residual of the target equation approaches zero at the collocation points associated with the\nbasis functions. Therefore, the SINNs could be developed based on the valuable insights of earlier\nstudies [22] on collocation methods. To improve the learning capability of SINNs, we can replace\nthe MLP with the architecture from Neural Operators, such as PINO [33] and physics-informed\nDeepOnets [34]. With the direct derivatives on the spacial, approximating in the spectral domain\ncan make Neural Operators more flexible in dealing with the physics-informed loss. As for the\nnonnegligible bias, with the bias decreases with the number of grids increases, fine enough grids can\nmake the bias negligible.\nFurthermore, during conducting our experiments, we found that SINNs are more sensitive to the\ninitial conditions, so assigning weights [35, 36, 37] in $\\mathcal{L}_r$ and $\\mathcal{L}_{ic}$ may have a dramatic improvement\non SINNs. Some tricks in classical spectral methods can be investigated, for example 1) we can\ninvestigate implementing Compressive Sampling [38, 39] on SINNs to further reduce the training"}, {"title": "A Fa\u00e0 di Bruno's formula", "content": "Given f, g are sufficiently smooth functions, Fa\u00e0 di Bruno's formula states that the n-th derivative of\nf (g(x)) is a sum of products of various orders of derivatives of the component functions:\n$\\frac{d^n}{dx^n} f(g(x)) = (f \\circ g)^{(n)} (x) = \\sum_{\\pi \\in \\Pi} f^{(\\|\\pi|)} (g(x)) \\cdot \\prod_{B \\in \\pi} g^{(\\|B|)} (x),$\nwhere $\\Pi$ is the set of all partitions of the set ${1, . . ., n}$, $|\\pi|$ is the number of blocks in the partition $\\pi$,\nand $|B|$ is the number of elements in the block B. Since Fa\u00e0 di Bruno's formula is a generalization of\nthe chain rule used in first-order, one can directly apply this formula to achieve accurate higher-order\nAD efficiently by avoiding a lot of redundant computations. However, the total number of partitions of\nan n-element set is the Bell number [40]: $\\#{\\Pi} = B_n = \\sum_{k=0}^{n} {n \\choose k} B_k$ that increases exponentially\nwith the order n (Fig. 5)."}, {"title": "B Aliasing error", "content": "The quadratic non-linear term $w(x) = u(x)v(x)$ can be regarded as the convolution sum of the\nFourier expansions of the corresponding two terms in spectral methods, i.e.,\n$w(k) = \\sum_{m+n=k} \\hat{u}(m)\\hat{v}(n),$\nwhere $\\hat{w}$, $\\hat{u}$, and $\\hat{v}$ represent the Fourier coefficients of $u(x)$, $v(x)$, and $w(x)$, respectively. In\npractical computations, when dealing with truncated Fourier series consisting of only N components,\na challenge arises: some frequencies of the summation of the two terms exceed the range of the\ntruncated frequency. This issue stems from the periodic nature of the discrete Fourier transform,\nwhere components outside the frequency range are mapped back into the range as lower or higher\nfrequency components, leading to aliasing errors in the nonlinear terms. For instance, components"}, {"title": "C Details of Experiments", "content": "In the following experiments, we proceed by training the model via stochastic gradient descent\nusing the Adam [43] optimizer with the exponential decay learning rate. The hyperparameters for\nexponential decay are: the initial learning rate is $10^{-3}$, the decay rate is 0.95 and the number of\ntransition steps is 10000. The MLP is equipped with the Sigmoid Linear Unit (SiLU) activations and\nXavier initialization.\nNote that there is no injection of external source terms in our experiments, resulting in a decay of\nthe quantities, including temperature and energy, over time. As time increases, the related functions\ngradually become smoother, and the overall flow field tends to be constant. Herein, to clearly\ndemonstrate the advantages and performance of our SINNs, the temporal domain is restricted to the\ninterval when the flow field undergoes significant changes. Additionally, we did an experiment in a\nlong temporal domain for the 1-D convection-diffusion equation with periodic boundary conditions."}, {"title": "C.1 The experiments on linear equations", "content": ""}, {"title": "C.1.1 1-D problems", "content": "In 1-D problems, we discretize the spatial domain to 100 points and the temporal domain to 100\npoints, thus the total size of the discretization for PINNs is $100 \\times 100$. Thanks to the symmetric of\nreal functions in the spectral domain, the total size of the discretization for SINNs is $51 \\times 100$. The\nMLP we used for both PINNs and SINNs is 10$\\times$ 100: 10 layers and every hidden layer has 100\nneurons. We train both PINNs and SINNs for $5 \\times 10^5$ iterations."}, {"title": "C.1.2 2-D problems", "content": "In 2-D problems, the MLP we used for both PINNs and SINNs is 10$\\times$ 100: 10 layers and every\nhidden layer has 100 neurons. We train both PINNs and SINNs for $10^6$ iterations."}, {"title": "C.1.3 3-D problems", "content": "To compare SINNs and PINNs for 3-D\nlinear equations, a heat equation problem is considered here, which has the form\n$\\begin{aligned}\n\\partial_t u &= \\epsilon (\\partial_{xx} u + \\partial_{yy} u + \\partial_{zz} u), &x\\in [0, 2\\pi]^3, t \\in [0, T], \\\\\nu(0, x) &= \\sum_{k=0}^{N-1} [sin (kx) + sin (ky) + sin (kz)],\n\\end{aligned}$"}, {"title": "C.2 The experiments on non-linear equations", "content": ""}, {"title": "C.2.1 1-D problems", "content": "One of the most important 1-D nonlinear equations is the Burgers\nequation, taking the following form:\n$\\begin{aligned}\n\\partial_t u &= \\nu \\partial_{xx} u - u \\partial_{x}u, &x\\in [0, 2\\pi], t \\in [0, T], \\\\\nu(0, x) &= \\sum_{k=1}^{N} sin(kx)\n\\end{aligned}$"}, {"title": "C.2.2 2-D problems", "content": "The 2-D nonlinear NS equation is the same as\nEq. (11) with $T = 2, \\nu = 2\\pi/100$, and $g(x) = (- cos(x) sin(y), sin(x) cos(y))$ in our experiment.\nThe spatial and temporal domains are discretized to $100 \\times 100$ and 11 points, respectively. The total\nsize for PINNs is $100 \\times 100 \\times 11$, while the total size for SINNs is $51 \\times 100 \\times 11$ since u is real."}, {"title": "C.3 The metrics of the relative error", "content": "The metric we use is the relative L2 error as follows:\n$E = \\sqrt{\\frac{\\sum_{i=1}^N |u_{\\theta}(t^i, x^i) - u_T (t^i, x^i)|^2}{\\sum_{i=1}^N |u_T (t^i, x^i)|^2}},$"}, {"title": "D Some details involved in the spectral form of the incompressible NS\nequations", "content": "This appendix presents the details of the derivation of Eq. (12) and the calculation of non-linear terms\n$\\hat{\\mathcal{N}}$ in spectral space.\nConsidering the periodic boundary conditions, by applying the Fourier transform on both sides of\nEq. (11), the NS equations in the spectral space are expressed as\n$\\begin{aligned}\ni \\mathbf{k} \\cdot \\hat{\\mathbf{u}} &= 0, \\\\\n\\partial_t \\hat{\\mathbf{u}} + \\hat{\\mathcal{N}} &= -i \\mathbf{k}\\hat{p} - \\nu\\|\\mathbf{k}\\|^2\\hat{\\mathbf{u}}.\n\\end{aligned}$\nThe continuity equation reveals that frequency and velocity are orthogonal in spectral space; by\ntaking the frequency dot product on both sides of the momentum equation Eq. (39b), the relationship\nbetween the pressure and the non-linear term can be obtained,\n$\\mathbf{k} \\cdot \\hat{\\mathcal{N}} = -i\\|\\mathbf{k}\\|^2\\hat{p}.$"}, {"title": "E Simplification of weighted residual loss", "content": "Suppose\n$\\hat{F}_i = |\\partial_t \\hat{u}^{\\theta} (t, k^i) + (k_x^i)^2 \\hat{u}^{\\theta} (t, k^i) + (k_y^i)^2 \\hat{u}^{\\theta} (t, k^i) |^2,$\nand $\\mathcal{L} = {\\hat{F}_i}_{i=1}^{N_r}$ is the vectorization of $\\hat{F}_i$ sorted by $\\|k_i\\|_1$, then the Eq. (16) can be simplified to :\n$\\hat{\\mathcal{L}}(\\theta) = \\frac{1}{M}(exp(-W \\mathcal{L}))^T \\mathcal{L}_r,$\nwhere W = ${W_{ij}} \\in \\mathbb{R}^{N_r\\times N_r}$ and\n$W_{ij} = \\begin{cases}\n1 & \\text{if } j < \\#{\\|k\\|_1 \\le \\|k^i\\|_1},\\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nFurthermore, we can decompose $W = W_1 W_2 W_3$ where $W_1 = {w_{ij}^1} \\in \\mathbb{R}^{N_r\\times (M-1)}, W_2 =$\n${w_{ij}^2} \\in \\mathbb{R}^{(M-1)\\times (M-1)}$, and $W_3 = {w_{ij}^3} \\in \\mathbb{R}^{(M-1)\\times N_r}$. For\n$w_{ij}^1 = \\begin{cases}\n1 & \\text{if } \\#{\\|k\\|_1 \\le \\|k^j\\|_1} < i \\le \\#{\\|k\\|_1 \\le \\|k^{i+1}\\|_1},\\\\\n0 & \\text{otherwise}.\n\\end{cases}$\n$w_{ij}^2 = \\begin{cases}\n1 & \\text{if } j \\le i,\\\\\n0 & \\text{otherwise}.\n\\end{cases}$\n$w_{ij}^3 = \\begin{cases}\n1 & \\text{if } \\#{\\|k\\|_1 < \\|k^j\\|_1} < i < \\#{\\|k\\|_1 \\le \\|k^j\\|_1},\\\\\n0 & \\text{otherwise}.\n\\end{cases}$"}]}