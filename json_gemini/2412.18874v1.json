{"title": "IUST_PersonReId: A New Domain in Person Re-Identification Datasets", "authors": ["Alireza Sedighi Moghaddam", "Fatemeh Anvari", "Mohammadjavad Mirshekari Haghighi", "Mohammadali Fakhari", "Mohammad Reza Mohammadi"], "abstract": "Person re-identification (ReID) models often struggle to generalize across diverse cultural contexts, particularly\nin Islamic regions like Iran, where modest clothing styles are prevalent. Existing datasets predominantly feature\nWestern and East Asian fashion, limiting their applicability in these settings. To address this gap, we introduce\nIUST_PersonReId, a dataset designed to reflect the unique challenges of ReID in new cultural environments, empha-\nsizing modest attire and diverse scenarios from Iran, including markets, campuses, and mosques.\nExperiments on IUST_PersonReId with state-of-the-art models, such as Solider and CLIP-ReID, reveal signif-\nicant performance drops compared to benchmarks like Market1501 and MSMT17, highlighting the challenges\nposed by occlusion and limited distinctive features. Sequence-based evaluations show improvements by leverag-\ning temporal context, emphasizing the dataset's potential for advancing culturally sensitive and robust ReID systems.\nIUST_PersonReId offers a critical resource for addressing fairness and bias in ReID research globally. The dataset is\npublicly available at https://computervisioniust.github.io/IUST_PersonReId/.", "sections": [{"title": "1. Introduction", "content": "Person ReIDentification (ReID) is a crucial task in\ncomputer vision, aimed at identifying and matching in-\ndividuals across images captured by different cameras at\nvarious times and locations. The primary motivation for\nReID arises from its vital applications in surveillance,\nsecurity, and urban management. Surveillance cam-\neras, often deployed across vast areas, generate mas-\nsive amounts of footage. Manually sifting through this\ndata to track individuals is impractical. Thus, automated\nReID systems provide a scalable and efficient solution\nfor matching identities across non-overlapping camera\nviews, enabling more effective monitoring and manage-\nment of public spaces.\nThe performance of AI models, heavily depends on\nthe quality and diversity of their training datasets, and\nReID models are no exception. Developing robust mod-\nels requires datasets that capture a wide range of vari-\nations in lighting, viewpoints, quality, and human ap-\npearances. Among these factors, clothing is a signifi-\ncant determinant of human appearance, varying dramat-\nically between different cultures. For instance, women's\nclothing in countries where hijabs are commonly worn,\nsuch as Iran, differs markedly from clothing in other re-\ngions. Incorporating cultural clothing diversity in ReID\ndatasets is essential. These datasets not only improve\nmodel accuracy and reliability but also ensure fairness\nby preventing demographic bias in real-world applica-\ntions [1]. Without sufficient representation, models may\nperform poorly on underrepresented groups, leading to\nbiased and unfair outcomes.\nIn recent years, several person ReID datasets have\nbeen developed, each addressing unique challenges.\nThe CUHK03 [2] dataset captures 1,364 pedestrians\nacross six surveillance cameras, introducing cross-\nview complexities with varying pedestrian movements.\nThe DukeMTMC-reID [3, 4] dataset, derived from\nDukeMTMC, includes 1,852 identities across eight\ncameras, tackling real-world challenges like occlu-\nsions, illumination changes, detection errors, and low-\nresolution imagery. Market-1501 [5] provides over\n32,000 annotated bounding boxes from six cameras,"}, {"title": "2. Related Works", "content": "Person re-identification (ReID) has been a crucial\ntask in computer vision, aiming to identify the same per-\nson across different cameras or time. Over the years,\nnumerous datasets have been proposed to facilitate re-\nsearch in this area. This section provides an overview of\nsome of the most popular and influential ReID datasets.\nEast Asian culture. The CUHK03 dataset [2], created\nby the Chinese University of Hong Kong, consists of\nimages and videos captured on the university's campus\nusing two pairs of cameras. The dataset represents East\nAsian modern clothing culture and includes both manu-\nally labeled and automatically detected bounding boxes,\nwith the latter generated by the DPM algorithm [9]. The\nMarket-1501 dataset [5] is a widely used resource col-\nlected at the Tsinghua University campus supermarket\nwith up to six cameras, capturing East Asian modern\nclothing culture and including detections from the DPM\nalgorithm [9]. An extension of this dataset, MARS [10],\nwas also collected on the Tsinghua University campus\nusing up to six cameras. It encompasses clothing styles\nsame as Market-1501, with detections provided by both\nthe DPM and GMMCP [11] algorithms.\nWestern and European culture. DukeMTMC4ReID\n[12] is another widely recognized dataset, captured by\nup to eight cameras on the Duke University campus. It\nincludes Western modern clothing styles in an academic\nenvironment, with detections facilitated by the Doppia\n[13] algorithm. DukeMTMC-VideoReID [14] is a sub-\nset of the DukeMTMC tracking dataset [12] for person\nre-identification captured by up to eight cameras on the\nDuke University campus. The Airport dataset [15] was\ncreated using videos from six cameras within an indoor\nsurveillance network at a mid-sized airport. It primar-\nily features Western modern clothing styles but also in-\ncludes a variety of international styles due to the diverse\nnature of the airport environment. DeepSportradar-\nReID [16] is a dataset containing 4,869 images of 486\nidentities, each captured by multiple cameras. The RPI-\nfield dataset [17] was created using 12 cameras on the\ncampus of Rensselaer Polytechnic Institute. It captures\nindividuals wearing Western modern clothing in an aca-\ndemic environment, with each identity observed from\n12 different camera views. The SoccerNet-ReID dataset\n[18] was created on soccer fields across six European\n soccer leagues. It focuses on soccer sportswear and in\ntroduces the challenge of distinguishing identities wear-\ning similar clothing, as is typical for players on the same\nteam."}, {"title": "3. Data Collection & Characteristics", "content": "Our dataset was collected with a specific focus on the\nclothing styles prevalent in Islamic countries, particu-\nlarly emphasizing women who wear hijab.\nThe raw videos were gathered from five distinct loca-\ntions: the campus of Iran University of Science & Tech-\nnology, a fruit shop, a hypermarket, a mosque, and the\nArbaeen procession in Iraq, which is one of the largest\ngatherings of Muslims in the world. To ensure that\nthe dataset reflects real-world scenarios, surveillance cam-\nera footage was primarily used during the data collec-\ntion process. However, for the Arbaeen procession,\nhandheld cameras were employed to capture videos of\nthe participants. This approach enabled us to gather\nraw video data that encompasses a wide variety of cul-\ntural representations, which are largely absent in exist-\ning datasets.\nTo enhance the robustness and real-world applicabil-\nity of our dataset, we intentionally designed the data col-\nlection process to include various challenging scenarios\nencountered in surveillance systems. These challenges\nare described below:\nThe dataset includes\nvideos captured from unique and non-standard\ncamera angles typical of surveillance cameras, en-\nsuring coverage of diverse viewpoints."}, {"title": "4. Results", "content": "This section presents the evaluation of our proposed\ndataset for person re-identification (ReID) tasks. We\noutline the dataset preparation process, describe the\ntraining and evaluation methodologies, and provide per-\nformance metrics to assess the effectiveness of the\nmodels and dataset. Detailed results include baseline\nevaluations, sequence-based re-identification, and vari-\nous ablation studies such as cross-dataset performance,\ngender-based analysis, and visibility-based evaluations.\nThe dataset is temporally divided into training and\ntesting subsets, with the first 75% of the annotated video\nfootage from all cameras allocated to the training sub-\nset and the remaining 25% reserved for testing. This\ndivision is based on video duration rather than the num-\nber of images or identities. A separate query subset is\ncreated by sampling identities from the testing set. To\nensure robust evaluation, images corresponding to the\nquery subset are excluded from the gallery during test-\ning. The training set is utilized to fine-tune pre-trained\nReID models, while the test set is dedicated solely to\nevaluating model performance.\nThe Solider [31] model was initialized with weights\npre-trained on LUPerson [7], while the CLIP-ReID [32]\nmodel uses the pre-trained CLIP model [33] without ad-\nditional pre-training. Both models were fine-tuned on\nour dataset with default hyperparameters.\nCLIP-REID is a re-identification method that lever-\nages pre-trained vision-language models like CLIP by\nintroducing a two-stage training strategy [32]. In the\nfirst stage, it generates text descriptions for each iden-\ntity using learnable text tokens optimized to align with\nCLIP's image features. The second stage fine-tunes\nthe image encoder with these descriptions and common\nReID losses, achieving state-of-the-art performance on\nvarious datasets. On the other hand, SOLIDER fo-\ncuses on self-supervised learning by generating pseudo\nsemantic labels for human image regions (e.g., upper\nbody, shoes) and using a semantic classification task to\nlearn rich semantic information [31]. It also introduces\na semantic controller, allowing customizable semantic\nand appearance information, making it versatile for di-\nverse human-centric tasks, including ReID.\nTo ensure consistency with existing research, we used\nstandard evaluation metrics for all experiments in our\nstudy:\nThe Cumu-\nlative Match Characteristic (CMC) curve is a key met-\nric for assessing person re-identification (ReID) perfor-\nmance. It calculates the probability of a correct match\nappearing within the top-kk ranks for a given query. We\nreport the Rank-1, Rank-5, and Rank-10 accuracies as\nstandard measures of model performance [34].\nMean Average Preci-\nsion (mAP) is another essential metric for evaluating\nReID performance. It combines precision and recall by\ncomputing the average precision (AP) for each query\nand then averaging these values across all queries. AP is\nthe area under the precision-recall curve, reflecting how\neffectively the model ranks relevant results. mAP serves\nas a comprehensive metric that evaluates both the accu-\nracy and completeness of retrieval, making it a widely\nused benchmark for comparing ReID models [34].\nA common approach for evaluating models on\ndatasets involves fine-tuning the models on the training\nset and assessing their performance on the test set. This\nsetup provides a baseline for comparison and highlights\nthe dataset's effectiveness in training re-identification\nmodels.\nThe results reveal an average performance drop of ap-\nproximately 30% in mAP for state-of-the-art person re-\nidentification models on the IUST_PersonReId dataset\ncompared to established benchmarks like Market1501\nand MSMT17. This decline demonstrates the chal-\nlenges introduced by the new domain, where individ-\nuals wear modest attire characteristic of Iranian culture."}, {"title": "5. Conclusion", "content": "This paper introduces the IUST_PersonReId dataset,\ndesigned to address the unique challenges of person re-identification in Islamic countries, where modest cloth-ing, particularly hijabs, is prevalent. The dataset cap-tures diverse scenes from Iran and Iraq, with varia-tions in lighting, camera angles, and seasonal clothing.Through a multi-step annotation process involving auto-mated tracking and manual refinement, we ensure high-quality frames for each individual.\nOur experiments with state-of-the-art models, such asSolider and CLIP-ReID, demonstrate a significant per-formance drop when tested on IUST_PersonReId com-pared to traditional benchmarks like Market1501 andMSMT17. This highlights the difficulties posed bymodest attire, especially for women, where occlusionand limited distinctive features impact re-identificationaccuracy. Sequence-based evaluation, which utilizesmultiple images of an individual, shows promising im-provements by leveraging temporal context."}]}