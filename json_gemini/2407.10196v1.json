{"title": "A3S: A General Active Clustering Method with Pairwise Constraints", "authors": ["Xun Deng", "Junlong Liu", "Han Zhong", "Fuli Feng", "Chen Shen", "Xiangnan He", "Jieping Ye", "Zheng Wang"], "abstract": "Active clustering aims to boost the clustering per- formance by integrating human-annotated pair- wise constraints through strategic querying. Con- ventional approaches with semi-supervised clus- tering schemes encounter high query costs when applied to large datasets with numerous classes. To address these limitations, we propose a novel Adaptive Active Aggregation and Splitting (A3S) framework, falling within the cluster-adjustment scheme in active clustering. A3S features strate- gic active clustering adjustment on the initial clus- ter result, which is obtained by an adaptive clus- tering algorithm. In particular, our cluster adjust- ment is inspired by the quantitative analysis of Normalized mutual information gain under the information theory framework and can provably improve the clustering quality. The proposed A3S framework significantly elevates the perfor- mance and scalability of active clustering. In extensive experiments across diverse real-world datasets, A3S achieves desired results with sig- nificantly fewer human queries compared with existing methods.", "sections": [{"title": "1. Introduction", "content": "In the realm of data science, clustering algorithms have emerged as a cornerstone technology within the domain of unsupervised learning (Khanum et al., 2015; Celebi & Ay- din, 2016). By automatically grouping similar data objects based on inherent structures and patterns within datasets, clustering provides an efficacious means to condense and structure complex information and is widely applied in im- age classification (Caron et al., 2020), social network analy- sis (He et al., 2022), etc. However, conventional clustering techniques often rely on static parameter settings and one-off computations, rendering them less adaptable to strange or expanding data environments (See Section 2.3.3). This con- text highlights the growing importance of human-computer collaborative active clustering approaches.\nActive clustering refers to a paradigm that actively selects side information (Anand et al., 2014), in the form of pair- wise constraints, to maximally improve the clustering perfor- mance. Extensive work (Gonz\u00e1lez-Almagro et al., 2023) has explored the combination of the strategic selection of pair- wise constraints and semi-supervised clustering (SSC) (Basu et al., 2002), and attains much lower query complexity com- pared to its semi-supervised counterpart (Bilenko et al., 2004). Despite the practicality, current active clustering methods often suffer from high computational and query costs when the number of classes is large.\nSSC-based active clustering methods primarily assess the uncertainty of all pairwise constraints, and iteratively choose the most uncertain ones for expert queries. This manner, while systematic, faces notable challenges: it potentially relies on the assumption that a small set of initial pairwise constraints will rapidly cover most real classes. This as- sumption becomes increasingly unreliable in scenarios with large sample numbers N and class numbers K. There- fore, some active clustering methods (Van Craenendonck et al., 2017; Shi et al., 2020) shift from SSC to a cluster-adjustment scheme. This scheme involves over-clustering data into k clusters via a specific clustering method (where k is greater than K), and subsequently aggregates the re- sulting small clusters into larger ones based on pairwise constraints. However, it requires a proper cluster number k as an input parameter, which is hard to determine in real applications. Moreover, the human query may be mislead- ing when the selected samples are outliers, as they do not represent the majority sample of a cluster.\nThis work aims to overcome the drawbacks of existing cluster adjustment schemes. We first present a theoretical result that identifies conditions where aggregating two clusters does not reduce the normalized mutual information (NMI) between the resulting clustering and the real clustering. Here, NMI measures the overlap between two clustering results, with larger values indicating better performance (see Definition 2.3). In addition, to guide active human queries, we quantify the impact of merging"}, {"title": "2. Methodology", "content": "2.1. Notation and Definition\nDefinition 2.1 (Active Clustering). We denote the true classes of N samples X = {x\u2081,\u2026\u2026, XN} by Y = {y\u2081,\u2026\u2026, YN}, where y; \u2208 {1,\u2026\u2026, K} and K is the num- ber of classes. Let the ground truth clustering of X as C = {C\u2081,\u2026\u2026, CK}, and the initial clustering result as \u03a9 = {w\u2081,\u2026\u2026, wk}, where X = U\u1d62\u208c\u2081C\u1d62 = U\u1d62\u208c\u2081wi and C\u00a1 \u2229 Cj = \u00d8, wi \u2229 wj = \u00d8, \u2200i, j. Active Clustering strate- gically selects sample pairs (xi, xj), and requires the ora- cle to judge if y\u2081 = y; (two samples are must-linked) or Yi \u2260 yj (two samples are cannot-linked). It then updates the clustering result \u03a9 with the queried pairwise constraints accordingly. Active Clustering aims to utilize the queried constraints to maximally reduce the difference between C and \u03a9, which is measured by the normalized mutual infor- mation (NMI) (Kvalseth, 1987; Vinh et al., 2009).\nDefinition 2.2 (Cluster Adjustment Scheme). We define a cluster adjustment scheme as a label update strategy em- ployed by active clustering algorithms. Specifically, it en- tails the algorithm's process of either locally aggregating small clusters into larger ones or splitting impure clusters into several subclusters, guided by pairwise constraints.\nDefinition 2.3 (NMI). NMI is a measure of how much common information two clustering results share. Given N samples and their two clusterings \u03a9 = {w\u2081,\u2026\u2026, wk} and \u03a9' = {w\u2081,\u2026, w}, we define the NMI value as\n$\\text{n}= \\frac{2I(\\Omega; \\Omega')}{H(\\Omega) + H(\\Omega')} = \\frac{2I(\\zeta; \\zeta')}{H(\\zeta) + H(\\zeta')},\\$\nwhere \u03b6 = (|w\u2081|/N,\u00b7\u00b7\u00b7,|wk|/N) and \u03b6' = (|w\u2081|/N,...,|w|/N) are two distributions induced by \u03a9 and \u03a9', respectively. Here, \u2161(\u00b7;\u00b7) denotes mutual information, and H(\u00b7) denotes entropy.\nWe also introduce the notion of clustering purity (Gonz\u00e1lez- Almagro et al., 2023).\nDefinition 2.4 (Purity). We define the dominant class of a cluster wi as arg maxj |wi \u2229 cj|. We label the sample in a cluster that does not belong to its dominant class or the sample that is a single cluster itself as an outlier. Then, the purity of wi is maxj |wi \u2229 cj|, and the purity of the clustering \\frac{\\sum_i \\max_j|w_i \\cap c_j|}{N}\nActive clustering that adopts the cluster-adjustment scheme does not rely on semi-supervised clustering for updating clustering results. Instead, it emphasizes the use of must-link and cannot-link constraints as indicators to merge or split clusters (Van Craenendonck et al., 2018). This greedy strategy may, however, jeopardize the quality of clustering outcomes for several reasons. For example, the queried samples could be outliers, or the purity of a cluster might not be sufficiently high. In light of this, we introduce a pivotal theorem that offers clear guidance for aggregation actions. This is achieved through an evaluation of the NMI."}, {"title": "2.2. Theoretical Analysis", "content": "Theorem 2.5 (Guarantee for Cluster Aggregation). Denote the clustering of N samples as \u03a9, the ground truth cluster- ing as C, and the NMI value of \u03a9 with respect to C as n\u2081. For any two clusters in \u03a9, say w\u2081 and w\u2082, suppose they have a common dominant class c\u2081 with purities t\u2081 and t\u2082 respec- tively, where t1, t2 \u2208 [p, 1]. By aggregating w\u2081 and w\u2082 into a new cluster W1,2 = W1 U w2, we arrive at a new cluster- ing \u03a9* = {1,2, W3, \u2026\u2026\u2026, wk} with NMI value of n2. This aggregation positively impacts clustering performance (i.e., n2 \u2265 n\u2081) if p \u2265 0.7 and n\u2081 \u2265 2 (1.0586 - min{t1, t2}).\nTheorem 2.5 delineates the conditions for non-deteriorating cluster aggregation, and the detailed proof is in Ap- pendix A.1. Specifically, merging two clusters can achieve provable benefits when their purity is at least 0.7 and preced- ing NMI exceeds 2 (1.0586 - min{t1, t2}). This finding is important as it suggests that the purity requirement for clus- ter aggregation is relatively mild, allowing for the inclusion of a small number of outliers within each cluster without compromising the overall clustering performance.\nWe take a step further by formulating the expected gain in NMI value when we decide to query a sample pair from a cluster pair. This involves aggregating clusters if the query result is \"must-link\" and keeping them separated if the result is \"cannot-link\".\nDefinition 2.6 (Expected NMI Gain). Suppose the dominant class of clusters wi and wj is cm and en respectively, which remain unknown before querying. Let n\u2081 and n\u2082 represent the NMI values of the clustering result before and after aggregating the two clusters. We denote P(cm = Cn) as the probability that the oracle observes a 'must-link' result. Then, we define the expected NMI gain from this query as follows:\n$\\mathbb{E}[\\Delta NMI | w_i, w_j] = P(c_m = c_n) \\cdot (n_2 - n_1),\\quad(1)$$\nIn what follows, we present how to estimate P(Cm = Cn) and (n2 - n1).\nWe use est = 1/0 to signify if ys equals yt or not, and denote the posterior pairwise probability as IP(est = 1). We estimate the pairwise probability following previous probability clustering methods (e.g., Liu et al., 2022), and discuss the estimation details in Appendix B.1. Then we can express the aggregation probability P(Cm = cn) as follows:\n$\\mathbb{P}(c_m = c_n) = \\frac{\\prod_{s \\in w_i, t \\in w_j} \\mathbb{P}(e_{st} = 1)}{\\prod_{s \\in w_i, t \\in w_j} \\mathbb{P}(e_{st} = 1) + \\prod_{s \\in w_i, t \\in w_j} \\mathbb{P}(e_{st} = 0)},\\quad(2)$$\nThe detailed derivation of Eq. (2) is in Appendix A.2.\nMoving forward, we focus on formulating n2 n1. In line with the notations used in Theorem 2.5, we define \u0394h = \u0397(\u03a9) \u2013 H(\u03a9*) and proceed with the following result:\n$\\text{n}_2 - \\text{n}_1 \\approx \\frac{2I(\\Omega^*; C)}{H(\\Omega^*) + H(C)} - \\frac{2I(\\Omega; C)}{H(\\Omega) + H(C)} \\approx - \\frac{2I(\\Omega; C) \\cdot \\Delta h}{(H(\\Omega) + H(C))^2},\\quad(3)$$\nwhere we use the fact that \u2161(\u03a9*; C) \u2248 \u2161(\u03a9, C') when the purity of wi and wj is sufficiently large. Moreover, when the sizes of clusters wi and w; are significantly smaller than the sample size N, the direct calculation gives that \u0394h \u00ab \u0397(\u03a9) + H(C) (refer to Appendix A.3 for verifica- tion). Hence, we have\n$\\mathbb{E}[\\Delta NMI | w_i, w_j] \\propto RHS \\text{ of Eq. (2)} \\cdot \\Delta h.\\quad(4)$$\nwhere RHS denotes the right-hand side."}, {"title": "2.3. Adaptive Active Aggregation and Splitting", "content": "Building on the analysis of cluster aggregation and ex- pected query impact, we detail our Adaptive Active Ag- gregation and Splitting framework, which comprises two distinct stages. First, the Adaptive Clustering stage is intro- duced in Section 2.3.1, where we describe the generation of initial clustering results. Second, in Section 2.3.2, we discuss selecting pairwise constraints and updating cluster- ing during the Active Aggregation and Splitting stage. The A3S workflow is illustrated in Figure 1, and the steps are summarized in Algorithm 1.\n2.3.1. INITIALIZATION VIA ADAPTIVE CLUSTERING\nCurrent active clustering methods often necessitate manu- ally setting initial cluster numbers, a challenging task when the number of classes is unknown. In response, we propose using adaptive clustering methods to determine an appropri- ate cluster number for initialization. An adaptive clustering method organizes data based on local density, revealing the dataset's inherent structure. It handles noise by isolat- ing each noisy sample into a separate cluster (i.e., outlier), avoiding unsuitable data grouping. This approach ensures a more natural and purer clustering outcome. Classic adaptive clustering methods include Probabilistic Clustering (Liu et al., 2022), density-based clustering (Zhang et al., 2021; Khan et al., 2018), among others. Additionally, the qual- ity of adaptive clustering can be significantly enhanced in multi-view clustering scenarios (Liu et al., 2023). In this process, we aim to obtain a suitable reference for the num- ber of clusters (typically larger than real class numbers), rather than the optimal clustering, hence do not require a precise hyper-parameter search. Once the adaptive cluster number is established, we can employ the desired clustering algorithm to produce the initial clustering result.\n2.3.2. ACTIVE AGGREGATION AND SPLITTING\nQuery Strategy. To ensure a high success rate in establish- ing 'must-link' connections among selected cluster pairs, We employ a two-step query strategy. The first step involves filtering out low-quality cluster pairs, focusing on those with aggregation probabilities at the top. In the second step, we utilize Eq. (4) to calculate the expected NMI gain for these cluster pairs. Then we choose the pair with the highest NMI gain. By Theorem 2.5, it is necessary to evaluate the pu- rity of the two chosen clusters and select one representative sample (i.e., it belongs to the dominant class of this cluster) from each cluster to form a sample pair. This pair will then be subjected to queries by oracles. To facilitate this process, we specially designed a purity test.\nPurity Test. For convenience, we employ the sphere struc- ture to depict a cluster, where the centroid sample of a cluster is denoted as jo. The other samples in the cluster are marked as ji,p, indicating that a sphere centered on sample i with a radius of d(i, ji,p) includes p percent of the samples in the cluster. Considering that outlier samples typically reside in the outer regions of a cluster, and samples within impure clusters tend to be more sparsely distributed, we bifurcate the task of purity testing into two consecutive judgments.\nFirst, we evaluate how densely the samples are concentrated within a cluster. This assessment is formalized as the density test for a cluster w, expressed in Eq. (5), where \u03b3 represents a pre-set threshold that determines the level of strictness in this density test, and 1(\u00b7) denotes the indicator function.\n$\\text{DT}(w) = 1(\\frac{\\sum_{i \\in w} w(i)}{|w|} > \\gamma).\\quad(5)$$\n$\\text{w}(i) = \\{j | j \\in w, \\mathbb{P}(e_{ij} = 1) < \\mathbb{P}(e_{iji,0.5} = 1)\\}$\nIf a cluster fails in the density test, we select a sample pair as (jo, jj0,0.7), and require oracles to judge P(w) = 1(Yjo = Yjj0,0.7). It estimates whether the purity of this cluster is higher than 0.7 (i.e., satisfying the conditions in Theorem 2.5). Overall, the purity test is as follows:\n$\\text{PT}(w) = \\text{DT}(w) \\quad \\text{if} \\quad \\text{DT}(w) \\quad \\text{else} \\quad \\mathbb{P}(w).\\quad(6)$$\nClustering Update. In response to different outcomes in the purity test, we adopt the following strategies to update"}, {"title": "3. Experiments", "content": "We organize the experiments as follows: we explain the ex- perimental setup in Section 3.1; we compare A3S with state- of-the-art active clustering methods and present the detailed results in Section 3.2; then we compare the performance of A3S when applied to different clustering algorithms in Section 3.3; lastly, we explore the influence of components in A3S in Section 3.4 to 3.5.\n3.1. Experimental Setup\nDatasets. We sampled six datasets from four real-world image sources for the experiments: Market-1501 (Zheng et al., 2015), which comprises human body images from 1501 individuals. We use two subsets: MK20 (351 images from 20 people) and MK100 (1650 images from 100 peo- ple); Humbi (Yu et al., 2020), a large multiview image dataset focused on human expressions like faces, and we extracted a subset Humbi-Face containing 5600 face im- ages from 100 different people; Handwritten (Dua et al., 2017), a collection containing 2000 samples of handwrit- ten digits from '0' to '9'. We use the Fourier coefficient features in the experiments. (4) MS1M (Guo et al., 2016), a substantial benchmark dataset commonly used in face recog- nition tasks, and we sampled two large subsets: MS1M-10k, MS1M-100k. The details of these datasets are shown in Table 1. Regarding the commonly used benchmarks in pre- vious constrained clustering methods (e.g., UCI datasets (Asuncion & Newman, 2007)), they are not appropriate for our problems due to the very small sample and class sizes. Consequently, we have not evaluated the performance on those benchmarks. We seek to demonstrate that our method is generally workable for different types of data/applications with a wide range of cluster numbers and sample numbers.\nBaselines. To validate the performance of our methods, a set of baselines and state-of-the-art algorithms are compared. Random (Basu et al., 2003) randomly selects pairwise con- straints. FFQS (Basu et al., 2004) uses the farthest-first scheme to acquire diversified samples and pairwise con- straints. NPU (Xiong et al., 2013) uses the classic entropy- based principle to select informative samples to construct pairwise constraints. We use PCKMeans (Basu et al., 2003) as the semi-supervised clustering algorithm for these three methods, because PCKMeans best suits the pairwise con- straints manner, and is suitable for large data sets with sparse high-dimensional data (Cai et al., 2023). URASC (Xiong et al., 2016) aims to iteratively query pairwise constraints that can maximally reduce the uncertainty of spectral clus- tering. COBRA over-cluster a dataset with K-means, then iteratively selects the closest cluster pairs for querying.\nImplementation. For the baseline methods, we maintain the same hyperparameter settings as reported in their origi- nal papers to ensure fairness in the comparison. Note that"}, {"title": "3.2. Comparison with SOTA Active Clustering Methods", "content": "We compare A3S and five baseline methods on four datasets with different numbers of queries in terms of both NMI and ARI. The results are shown in Figure 3. Overall, A3S has higher NMI and ARI values than other methods on these data sets when setting the same number of queries, and A3S requires only a small amount of queries to improve the NMI and ARI values significantly. In addition, we ob- serve that both A3S and COBRA (cluster-based methods) improve steadily with the increase of queries, while Ran- dom, FFQS, NPU and URASC (semi-supervised clustering based methods) show fluctuations on all data sets. This is because genuine supervisory information can sometimes be detrimental to clustering, as it may introduce violations (e.g., (xi, xj) is cannot-linked, but their similarity to xk is both very high). This problem also exists for the latest semi-supervised clustering methods such as PCSKM (Vouros & Vasilaki, 2021). However, it does not mean that this line of work is not applicable. One common advantage of them is that they can ultimately improve the NMI and ARI value to 1.0 if enough queries can be provided (typically less than N \u00d7 log(N)). They are a good choice when the target is to reveal the cluster identity for all samples accurately, or only low-quality features are available and the NMI of the initial clustering is lower than 0.2.\nThe detailed running result of A3S is in Table 2. A3S significantly reduces the fission rate and the entropy ratio to almost 1.0 on all datasets, with a high clustering purity. This validates that A3S can effectively reveal the true clustering structure of data. It's important to highlight that these results are obtained without any prior knowledge about the number of classes or class distribution. Additionally, A3S exhibits robustness to the choice of adaptive cluster number. As detailed in Appendix C, increasing the adaptive number does not significantly alter the number of queries needed to obtain the desired result.\nNext, we delineate the distinctions in the results yielded by A3S and COBRA. Although COBRA quickly improves the NMI and ARI values, it cannot further enhance the clustering even when more queries are invested (all must- link clusters have already been discovered), leading to a performance ceiling dictated by cluster purities. Recent methods like COBRAS (Van Craenendonck et al., 2018) and AQM+MEE (Deng et al., 2023b) also encounter simi- lar issues. In contrast, A3S not only delivers high-quality clusters but also identifies a subset of outlier samples. This approach enables continued augmentation of the NMI and ARI values through strategic querying of these outliers in combination with existing clusters. For instance, A3S and COBRA reach NMI values of 0.93 and 0.90 on MK100 sep- arately, but the overall clustering purity of A3S is 0.9752, far surpassing COBRA that is 0.8509. Further, by querying the outlier samples of A3S with their neighbor clusters until 'must-link' is observed and they are aggregated to the corre- sponding clusters, A3S can reach an NMI of 0.99 with less than 500 more queries."}, {"title": "3.3. A3S for Different Clustering Algorithms", "content": "To assess the compatibility of A3S on different clustering algorithms, we additionally use three classic clustering al- gorithms to generate the initial clustering (adaptive cluster number is provided by FPC): K-means clustering (Choo et al., 2020), Spectral clustering (Von Luxburg, 2007) and Agglomerative Clustering (Murtagh & Legendre, 2014). We test these versions of A3S on MK100 and Humbi-Face, and the results are shown in Figure 4. Besides, we quan- tify the difference between these initial clustering results with their mutual ARI value in Table 3. We have two obser- vations: first, the initial clustering outcomes derived from different algorithms exhibit substantial variability (mutual ARI value is typically lower than 0.6); second, A3S demon-"}, {"title": "3.4. Influence of Estimated Pairwise Probability Quality", "content": "Better pairwise probability can lead to improved cluster- ing performance, but its impact on A3S is unexplored. Multi-view clustering (Yang & Wang, 2018) is the major approach in this domain, and we utilize Handwritten to investigate this aspect. The complete Handwritten dataset comprises four distinct feature types for each sam- ple. Following the setup in Liu et al. (2022), we ini- tially learn the pairwise probabilities using features from each view. Subsequently, pairwise probabilities are aggre- gated across V different views employing the following formula (Liu et al., 2022): P(eij = 1|d\u2081,\u2026\u2026, dv) = $\\frac{\\prod_{m=1}^{V} P(e_{ij}=1|d_m)}{\\prod_{m=1}^{V} P(e_{ij}=1|d_m)+\\prod_{m=1}^{V} P(e_{ij}=0|d_m)},$ where din is the pair- wise distance in the m-th view.\nIn the investigation of A3S's performance with varying views (one/two/four) as illustrated in Figure 5, we observed that employing multi-view clustering slightly improves the initial clustering performance, but significantly boosts A3S's overall effectiveness. This approach leads to quicker con- vergence and nearly perfect NMI and ARI values. The improvement is largely attributed to multi-view aggregated pairwise probabilities, which make outlier samples distin-"}, {"title": "3.5. Performance of A3S on Large datasets.", "content": "We further test A3S on two large datasets MS1M-10k and MS1M-100k, where the initial cluster number is deter- mined by FPC for both A3S and COBRA. The result is in Figure 6. We have two observations: (1) A3S can reach near-optimal ARI value (0.995) with only 3100 queries for MS1M-100k in 8581.86 seconds, which validates that it is scalable to large datasets. As far as we know, this is the largest dataset ever used in active clustering research. (2) COBRA is sensitive to the initial cluster number and can waste too many queries to get a bad clustering result."}, {"title": "3.6. Case Study for A3S", "content": "We present a case study in Figure 2 to demonstrate A3S's mechanism using a two-class dataset. Initially, data points are clustered into seven groups, including two outlier clus- ters. In the first iteration, clusters w6 and w7 are selected for their largest expected impact on the NMI value. During the purity test, we fails the density test, leading to a subsequent query between its central sample (jo) and the margin sam- ple (jj0,0.7) of the sphere that is centered at jo and contains 70% samples of w6. The cannot-link result indicates w6 fails the purity test, so it is split into two sub-clusters using Algorithm 2, which costs 12 queries. In the second iteration, clusters wo and w7 are selected, both passing the purity test. Their central samples are queried, and they are merged into W10 following the must-link result.\nA3S addresses outlier detection through purity tests and subcluster partitioning. However, the purity test can discover most but not all low-quality clusters whose purity is lower than 0.7. As a remedy, we could resort to multi-view features which can significantly improve the clustering purity and reduce the number of outliers, as shown in Section 3.4. We remark that when multi-view"}, {"title": "4. Related Work", "content": "Query Strategy in Active Clustering. Recent Active clus- tering methods have embraced a trend of incorporating sam- ple uncertainty into their query strategies. These methods frequently utilize entropy to quantify uncertainty (Abin, 2016; Xiong et al., 2016; Shi et al., 2020). A common task involves estimating the probability of a sample belonging to different clusters or neighborhoods (Xiong et al., 2013). Additionally, alternative criteria such as maximum expected error reduction (Wang & Davidson, 2010) and maximum expected clustering change (Biswas & Jacobs, 2014) have been proposed to assess the stability of clustering results when perturbing the similarity values between two samples.\nConstraints in Semi-supervised Clustering. When us- ing the must-link and cannot-link constraints to perform SSC, two aspects are usually taken into consideration: the transitive inference of constraints and the combination of constraints to specific clustering algorithms. A few stud- ies (Lutz et al., 2021) address transitive inference with graph- based techniques instead of a brute-force manner. In addi- tion, recent studies that optimize clustering results with con- straints in SSC have explored various approaches, Vouros & Vasilaki (2021) explores Kmeans clustering for high dimen- sional data; Yang et al. (2022) attempts to correctly infer the number of clusters for hierarchical clustering; Ren et al. (2018) tries to utilize prior knowledge to determine a proper cluster number for density-based clustering; Chen & Zhong (2022) develops a graph-based SSC that is robust to noise, but is not suitable for large datasets. However, there remains a relatively unexplored potential in integrating these modern SSC approaches with active clustering, which presents a promising avenue for future research."}, {"title": "5. Conclusion", "content": "This paper studies the cluster-adjustment scheme in ac- tive clustering, offering theoretical guidance for non- deteriorating cluster aggregation and quantifying the impact of human queries and aggregation operations. We then pro- pose A3S, a general framework that does not rely on the dataset prior. Through extensive testing, A3S demonstrates its effectiveness on diverse real-world datasets with varying class numbers and distributions. We will explore the appli- cation of A3S on more complex multi-view datasets and gigantic datasets at the million level in the future."}, {"title": "A. Proofs", "content": "A.1. Proof of Theorem 2.5\nProof of Theorem 2.5. Let p and q denote the sizes of w\u2081 and w\u2082, respectively. We further assume that the class index of these outliers is ij \u2208 {1, 2, \u2026\u2026\u2026, K}, where j \u2208 {1, 2, \u2026\u2026\u2026, (1 \u2212 t\u2081)p + (1 \u2212 t2)q}. For ease of presentation, for any class index i \u2208 {1,2,\u2026\u2026, K}, we use si to denote the class size, i.e., |ci| = si. Without loss generality, we assume that q\u2265 p throughout this proof.\nBy the definition of mutual information, we have\n$\\prod(\\Omega^*; C) = I(\\Omega; C) + \\sum_{\\tau=1}^{K} P(w_{1,2} \\cap c_{\\tau}) log \\frac{P(w_{1,2} \\cap c_{\\tau})}{P(w_{1,2})P(c_{\\tau})} - \\sum_{\\tau=1}^{K} P(w_{1} \\cap c_{\\tau}) log \\frac{P(w_{1} \\cap c_{\\tau})}{P(w_{1})P(c_{\\tau})} - \\sum_{\\tau=1}^{K} P(w_{2} \\cap c_{\\tau}) log \\frac{P(w_{2} \\cap c_{\\tau})}{P(w_{2})P(c_{\\tau})}$\\\n$\\= I(\\Omega; C) + \\sum_{\\tau=1}^{K} \\frac{|w_{1,2} \\cap c_{\\tau}|}{N} log \\frac{N\\cdot |w_{1,2} \\cap c_{\\tau}|}{|w_{1,2}|\\cdot |c_{\\tau}|} - \\sum_{\\tau=1}^{K} \\frac{|w_{1} \\cap c_{\\tau}|}{N} log \\frac{N\\cdot |w_{1} \\cap c_{\\tau}|}{|w_{1}|\\cdot |c_{\\tau}|} - \\sum_{\\tau=1}^{K} \\frac{|w_{2} \\cap c_{\\tau}|}{N} log \\frac{N\\cdot |w_{2} \\cap c_{\\tau}|}{|w_{2}|\\cdot |c_{\\tau}|} \\quad(7)$$\nThen we bound these three terms respectively. For Term (I), we have\n$\\sum_{\\tau=2}^{K} \\frac{|w_{1,2} \\cap c_{\\tau}|}{N} log \\frac{N\\cdot |w_{1,2} \\cap c_{\\tau}|}{|w_{1,2}|\\cdot |c_{\\tau}|} = \\frac{t_1p + t_2q}{N} log \\frac{N(t_1p+t_2q)}{(p+q)s_1} + \\sum_{\\tau=2}^{K} \\frac{|w_{1,2} \\cap c_{i_j}|}{N} log \\frac{N\\cdot |w_{1,2} \\cap c_{i_j}|}{|w_{1,2}||\\cdot |c_{i_j}|} \\quad(8)$$\n$\\sum_{\\tau=2}^{K} \\frac{|w_{1} \\cap c_{\\tau}|}{N} log \\frac{N\\cdot |w_{1} \\cap c_{\\tau}|}{|w_{1}|\\cdot |c_{\\tau}|} = \\frac{t_1p}{N} log \\frac{Nt_1}{s_1} + \\sum_{j=1}^{(1-t_1)p} \\frac{|w_{1} \\cap c_{i_j}|}{N} log \\frac{N\\cdot |w_{1} \\cap c_{i_j}|}{p s_{i_j}} \\quad(9)$$\n$\\sum_{\\tau=2}^{K} \\frac{|w_{2} \\cap c_{\\tau}|}{N} log \\frac{N\\cdot |w_{2} \\cap c_{\\tau}|}{|w_{2}|\\cdot |c_{\\tau}|} = \\frac{t_2q}{N} log \\frac{Nt_2}{s_2} + \\sum_{j=1}^{(1-t_2)q} \\frac{|w_{2} \\cap c_{i_j}|}{N} log \\frac{N\\cdot |w_{2} \\cap c_{i_j}|}{q s_{i_j}} \\quad(10)$$\nPlugging Eq. (8), Eq. (9), and Eq. (10) into Eq. (7), together with the fact that\n$|w_{1,2} \\cap c_{i_j}| \\geq max\\{|w_{1} \\cap c_{i_j}|, |w_{2} \\cap c_{i_j}|\\}, \\forall j \\in \\{1,2,\\ldots,(1 - t_1)p + (1 - t_2)q\\},$"}, {"title": "A.2. Derivation of Eq. (2)", "content": "We obtain that\n$\\mathbb{I}(\\Omega^*; C) > \\mathbb{I}(\\Omega; C) + \\frac{t_1p}{N} log \\frac{t_1p+t_2q}{t_1 (p+q)} + \\frac{t_2q}{N} log \\frac{t_1p+t_2q}{t_2(p+q)} (11)$$\n$ - (\\frac{(1-t_1)"}]}