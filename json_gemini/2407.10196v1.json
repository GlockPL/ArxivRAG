{"title": "A3S: A General Active Clustering Method with Pairwise Constraints", "authors": ["Xun Deng", "Junlong Liu", "Han Zhong", "Fuli Feng", "Chen Shen", "Xiangnan He", "Jieping Ye", "Zheng Wang"], "abstract": "Active clustering aims to boost the clustering per-formance by integrating human-annotated pair-wise constraints through strategic querying. Conventional approaches with semi-supervised clus-tering schemes encounter high query costs when applied to large datasets with numerous classes. To address these limitations, we propose a novel Adaptive Active Aggregation and Splitting (A3S) framework, falling within the cluster-adjustment scheme in active clustering. A3S features strate-gic active clustering adjustment on the initial clus-ter result, which is obtained by an adaptive clus-tering algorithm. In particular, our cluster adjust-ment is inspired by the quantitative analysis of Normalized mutual information gain under the information theory framework and can provably improve the clustering quality. The proposed A3S framework significantly elevates the perfor-mance and scalability of active clustering. In extensive experiments across diverse real-world datasets, A3S achieves desired results with sig-nificantly fewer human queries compared with existing methods.", "sections": [{"title": "1. Introduction", "content": "In the realm of data science, clustering algorithms have emerged as a cornerstone technology within the domain of unsupervised learning (Khanum et al., 2015; Celebi & Ay-din, 2016). By automatically grouping similar data objects based on inherent structures and patterns within datasets, clustering provides an efficacious means to condense and structure complex information and is widely applied in im-age classification (Caron et al., 2020), social network analy-sis (He et al., 2022), etc. However, conventional clustering techniques often rely on static parameter settings and one-off computations, rendering them less adaptable to strange or expanding data environments (See Section 2.3.3). This context highlights the growing importance of human-computer collaborative active clustering approaches.\nActive clustering refers to a paradigm that actively selects side information (Anand et al., 2014), in the form of pair-wise constraints, to maximally improve the clustering perfor-mance. Extensive work (Gonz\u00e1lez-Almagro et al., 2023) has explored the combination of the strategic selection of pair-wise constraints and semi-supervised clustering (SSC) (Basu et al., 2002), and attains much lower query complexity com-pared to its semi-supervised counterpart (Bilenko et al., 2004). Despite the practicality, current active clustering methods often suffer from high computational and query costs when the number of classes is large.\nSSC-based active clustering methods primarily assess the uncertainty of all pairwise constraints, and iteratively choose the most uncertain ones for expert queries. This manner, while systematic, faces notable challenges: it potentially relies on the assumption that a small set of initial pairwise constraints will rapidly cover most real classes. This as-sumption becomes increasingly unreliable in scenarios with large sample numbers N and class numbers K. There-fore, some active clustering methods (Van Craenendonck et al., 2017; Shi et al., 2020) shift from SSC to a cluster-adjustment scheme. This scheme involves over-clustering data into k clusters via a specific clustering method (where k is greater than K), and subsequently aggregates the re-sulting small clusters into larger ones based on pairwise constraints. However, it requires a proper cluster number k as an input parameter, which is hard to determine in real applications. Moreover, the human query may be mislead-ing when the selected samples are outliers, as they do not represent the majority sample of a cluster.\nThis work aims to overcome the drawbacks of existing cluster adjustment schemes. We first present a theoretical result that identifies conditions where aggregating two clusters does not reduce the normalized mutual information (NMI) between the resulting clustering and the real clustering. Here, NMI measures the overlap between two clustering results, with larger values indicating better performance (see Definition 2.3). In addition, to guide active human queries, we quantify the impact of merging"}, {"title": "2. Methodology", "content": "2.1. Notation and Definition\nDefinition 2.1 (Active Clustering). We denote the true classes of N samples $X = {x_1,...,x_N}$ by $Y = {y_1,...,y_N}$, where $y_i \\in {1,......, K}$ and $K$ is the num-ber of classes. Let the ground truth clustering of $X$ as $C = {C_1,......,C_K}$, and the initial clustering result as $\\Omega = {w_1,......,w_k}$, where $X = \\cup_{i=1}^K C_i = \\cup_{i=1}^k w_i$ and $C_i \\cap C_j = \\emptyset$, $w_i \\cap w_j = \\emptyset$, $\\forall i, j$. Active Clustering strate-gically selects sample pairs $(x_i, x_j)$, and requires the ora-cle to judge if $y_i = y_j$ (two samples are must-linked) or $y_i \\neq y_j$ (two samples are cannot-linked). It then updates the clustering result $\\Omega$ with the queried pairwise constraints accordingly. Active Clustering aims to utilize the queried constraints to maximally reduce the difference between $C$ and $\\Omega$, which is measured by the normalized mutual infor-mation (NMI) (Kvalseth, 1987; Vinh et al., 2009).\nDefinition 2.2 (Cluster Adjustment Scheme). We define a cluster adjustment scheme as a label update strategy em-ployed by active clustering algorithms. Specifically, it en-tails the algorithm's process of either locally aggregating small clusters into larger ones or splitting impure clusters into several subclusters, guided by pairwise constraints.\nDefinition 2.3 (NMI). NMI is a measure of how much common information two clustering results share. Given N samples and their two clusterings $\\Omega = {w_1,......,w_k}$ and $\\Omega' = {w'_1,..., w'_{k'}}$, we define the NMI value as\n$\\frac{2I(\\Omega; \\Omega')}{H(\\Omega) + H(\\Omega')}= \\frac{2I(\\zeta; \\zeta')}{H(\\zeta) + H(\\zeta')}$,\nwhere $\\zeta = (|w_1|/N,...,|w_k|/N)$ and $\\zeta' = (|w'_1|/N,...,|w'_{k'}|/N)$ are two distributions induced by $\\Omega$ and $\\Omega'$, respectively. Here, $I(\\cdot;\\cdot)$ denotes mutual information, and $H(\\cdot)$ denotes entropy.\nWe also introduce the notion of clustering purity (Gonz\u00e1lez-Almagro et al., 2023).\nDefinition 2.4 (Purity). We define the dominant class of a cluster $w_i$ as $\\arg \\max_j |w_i \\cap c_j|$. We label the sample in a cluster that does not belong to its dominant class or the sample that is a single cluster itself as an outlier. Then, the purity of $w_i$ is $\\frac{\\max_j |w_i \\cap c_j|}{|w_i|}$, and the purity of the clustering result $\\Omega$ is quantified by $\\frac{\\sum_i \\max_j|w_i \\cap c_j|}{N}$\n2.2. Theoretical Analysis\nActive clustering that adopts the cluster-adjustment scheme does not rely on semi-supervised clustering for updating clustering results. Instead, it emphasizes the use of must-link and cannot-link constraints as indicators to merge or split clusters (Van Craenendonck et al., 2018). This greedy strategy may, however, jeopardize the quality of clustering outcomes for several reasons. For example, the queried samples could be outliers, or the purity of a cluster might not be sufficiently high. In light of this, we introduce a pivotal theorem that offers clear guidance for aggregation actions. This is achieved through an evaluation of the NMI."}, {"title": "2.3. Adaptive Active Aggregation and Splitting", "content": "2.3.1. INITIALIZATION VIA ADAPTIVE CLUSTERING\nCurrent active clustering methods often necessitate manu-ally setting initial cluster numbers, a challenging task when the number of classes is unknown. In response, we propose using adaptive clustering methods to determine an appropri-ate cluster number for initialization. An adaptive clustering method organizes data based on local density, revealing the dataset's inherent structure. It handles noise by isolat-ing each noisy sample into a separate cluster (i.e., outlier), avoiding unsuitable data grouping. This approach ensures a more natural and purer clustering outcome. Classic adaptive clustering methods include Probabilistic Clustering (Liu et al., 2022), density-based clustering (Zhang et al., 2021; Khan et al., 2018), among others. Additionally, the qual-ity of adaptive clustering can be significantly enhanced in multi-view clustering scenarios (Liu et al., 2023). In this process, we aim to obtain a suitable reference for the num-ber of clusters (typically larger than real class numbers), rather than the optimal clustering, hence do not require a precise hyper-parameter search. Once the adaptive cluster number is established, we can employ the desired clustering algorithm to produce the initial clustering result.\n2.3.2. ACTIVE AGGREGATION AND SPLITTING\nQuery Strategy. To ensure a high success rate in establish-ing 'must-link' connections among selected cluster pairs, We employ a two-step query strategy. The first step involves filtering out low-quality cluster pairs, focusing on those with aggregation probabilities at the top. In the second step, we utilize Eq. (4) to calculate the expected NMI gain for these cluster pairs. Then we choose the pair with the highest NMI gain. By Theorem 2.5, it is necessary to evaluate the pu-"}, {"title": "3. Experiments", "content": "We organize the experiments as follows: we explain the ex-perimental setup in Section 3.1; we compare A3S with state-of-the-art active clustering methods and present the detailed results in Section 3.2; then we compare the performance of A3S when applied to different clustering algorithms in Section 3.3; lastly, we explore the influence of components in A3S in Section 3.4 to 3.5.\n3.1. Experimental Setup\nDatasets. We sampled six datasets from four real-world image sources for the experiments: Market-1501 (Zheng et al., 2015), which comprises human body images from 1501 individuals. We use two subsets: MK20 (351 images from 20 people) and MK100 (1650 images from 100 peo-ple); Humbi (Yu et al., 2020), a large multiview image dataset focused on human expressions like faces, and we extracted a subset Humbi-Face containing 5600 face im-ages from 100 different people; Handwritten (Dua et al., 2017), a collection containing 2000 samples of handwrit-ten digits from '0' to '9'. We use the Fourier coefficient features in the experiments. (4) MS1M (Guo et al., 2016), a substantial benchmark dataset commonly used in face recog-nition tasks, and we sampled two large subsets: MS1M-10k, MS1M-100k. The details of these datasets are shown in Table 1. Regarding the commonly used benchmarks in pre-vious constrained clustering methods (e.g., UCI datasets (Asuncion & Newman, 2007)), they are not appropriate for our problems due to the very small sample and class sizes. Consequently, we have not evaluated the performance on those benchmarks. We seek to demonstrate that our method is generally workable for different types of data/applications with a wide range of cluster numbers and sample numbers.\nBaselines. To validate the performance of our methods, a set of baselines and state-of-the-art algorithms are compared. Random (Basu et al., 2003) randomly selects pairwise con-straints. FFQS (Basu et al., 2004) uses the farthest-first scheme to acquire diversified samples and pairwise con-straints. NPU (Xiong et al., 2013) uses the classic entropy-based principle to select informative samples to construct pairwise constraints. We use PCKMeans (Basu et al., 2003) as the semi-supervised clustering algorithm for these three methods, because PCKMeans best suits the pairwise con-straints manner, and is suitable for large data sets with sparse high-dimensional data (Cai et al., 2023). URASC (Xiong et al., 2016) aims to iteratively query pairwise constraints that can maximally reduce the uncertainty of spectral clus-tering. COBRA over-cluster a dataset with K-means, then iteratively selects the closest cluster pairs for querying.\nImplementation. For the baseline methods, we maintain the same hyperparameter settings as reported in their origi-nal papers to ensure fairness in the comparison. Note that"}, {"title": "5. Conclusion", "content": "This paper studies the cluster-adjustment scheme in ac-tive clustering, offering theoretical guidance for non-deteriorating cluster aggregation and quantifying the impact of human queries and aggregation operations. We then pro-pose A3S, a general framework that does not rely on the dataset prior. Through extensive testing, A3S demonstrates its effectiveness on diverse real-world datasets with varying class numbers and distributions. We will explore the appli-cation of A3S on more complex multi-view datasets and gigantic datasets at the million level in the future."}, {"title": "B. Implementation Details of A3S", "content": "B.1. Estimating Pairwise Probability\nFollowing the setup in (Liu et al., 2022), we use isotonic regression to learn a regressor that estimates the pairwise posterior probability $P(e_{ij} = 1|d_{ij})$, where $d_{ij}$ is the Euclidean distance between samples $i$ and $j$. The estimation encompasses three steps:\n(1) Since ground truth labels are unavailable, we employ K-means clustering to generate pseudo labels for the samples. Alternatively, Fast Probabilistic Clustering (FPC) can also be used for this purpose, where similarity values between samples serve as a rudimentary approximation of pairwise probabilities. In practical applications, cosine similarity is particularly well-suited for FPC.\n(2) Next, we generate the training data for isotonic regression. We utilize the $k$-nearest neighbors of each sample to form sample pairs. The independent variable of isotonic regression is the Euclidean distance between two samples in the sample pair. Each pair is labeled as 0 or 1, indicating whether the two samples in each pair share the same pseudo label. The label serves as the dependent variable of isotonic regression.\n(3) Finally, we conduct isotonic regression on the gathered data, learning a function that maps the Euclidean distance between two samples to the pairwise probability, i.e., $P(e_{st} = 1|d_{st})$.\nLiu et al. (2022) also proposed Graph-context-aware refinement to enhance the quality of the posterior probability, but it is not an essential component of A3S. Therefore, we did not include it in our experiments. However, incorporating them would further enhance the performance of A3S, as they can improve the quality of the estimated merging probability between cluster pairs.\nB.2. Calculating Aggregation Probability\nWe derive the aggregation probability based on the condition that the purity of two clusters are 1.0. In practical applications, the assumption that both clusters have a purity of 1.0 might not strictly hold. Specifically, the pairwise probability between major samples and outlier samples from $w_i$ and $w_j$ will degrade the aggregation probability (i.e., making the result biased towards 0). To deal with this issue, we propose a variant of $P(c_m = c_n)$, denoted as $P_{knn}(c_m = c_n)$. This variant considers"}]}