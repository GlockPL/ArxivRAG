{"title": "WANCO: WEAK ADVERSARIAL NETWORKS FOR CONSTRAINED OPTIMIZATION PROBLEMS", "authors": ["GANG BAO", "DONG WANG", "BOYI ZOU"], "abstract": "This paper focuses on integrating the networks and adversarial training into con- strained optimization problems to develop a framework algorithm for constrained optimization prob- lems. For such problems, we first transform them into minimax problems using the augmented Lagrangian method and then use two (or several) deep neural networks(DNNs) to represent the primal and dual variables respectively. The parameters in the neural networks are then trained by an adversarial process. The proposed architecture is relatively insensitive to the scale of values of different constraints when compared to penalty based deep learning methods. Through this type of training, the constraints are imposed better based on the augmented Lagrangian multipliers. Ex- tensive examples for optimization problems with scalar constraints, nonlinear constraints, partial differential equation constraints, and inequality constraints are considered to show the capability and robustness of the proposed method, with applications ranging from Ginzburg-Landau energy minimization problems, partition problems, fluid-solid topology optimization, to obstacle problems.", "sections": [{"title": "1. Introduction", "content": "Constrained optimization problems arise from an intricate and practical set of issues, with the objective of identifying solutions to an optimiza- tion problem while adhering to specific constraints. These problems play a crucial role in various domains such as finance, economics, engineering, operation research, machine learning, and others. For a comprehensive exploration of constrained opti- mization problems, we refer to [7] and the references therein. A wide range of problems in scientific computing can be classified as constrained optimization problems, for ex- ample, inverse scattering problems [4, 2], optical sciences [3], topology optimization problems [6], operation research, optimal control, and obstacle problems.\nIn this paper, we focus on constrained optimization problems related to differen- tial operators that can appear in either the objective functional or the constraints. There are many efficient classical methods for solving constrained optimization prob- lems [7, 22, 46], such as the penalty method, the Lagrange multiplier method, the augmented Lagrangian method, and the interior point method. When solving differ- ential operator related problems with constraints, traditional numerical methods such as the finite difference method, the finite element method, or the spectral method can be applied with careful construction, discretization, and post-processing for satisfying the constraints.\nIn recent years, there has been rapid development in artificial intelligence (AI) for science, with particular attention being drawn to physics-informed machine learn- ing in scientific computing. The universal approximation of continuous functions using neural networks, as proven in [17], has gained significant attention in the past decade, leading to an increasing trend of utilizing deep neural networks to solve dif- ferential operator related problems. A lot of studies have demonstrated the effective- ness of deep learning approaches in solving partial differential equations, optimiza- tion problems with differential equation constraints, or inverse problems, including physics-informed neural network (PINN) [37], deep Galerkin method (DGM) [40], deep Ritz method (DRM) [20], weak adversarial network (WAN) [51, 5], random fea- ture method (RFM) [12], local extreme learning machines (locELM) [18], particle weak-form based neural networks (ParticleWNN) [50] and many other mechanism driven methods [27, 48, 30, 34, 49, 47, 45]. In contrast with traditional methods, deep learning methods are usually mesh-free. Therefore, they are less affected by the curse of dimensionality, making them efficient for computing high-dimensional problems. For a comprehensive overview of using DNNs to solve differential operator related problems, we refer to [31, 26, 16] and references therein.\nThe framework for using DNNs to solve differential operator constrained opti- mization problems involves constructing a problem-specific loss function, followed by solving an optimization problem with boundary conditions and other constraints. One common approach is to utilize penalty methods to transform the constrained optimization problem into an unconstrained optimization problem, as used in PINN, DGM, and DRM. The results are highly dependent on the selection of the weights of different terms in the loss function. In particular, one needs to carefully consider the scale of the values of constraints to guarantee that the training process can minimize each term in the loss function equally. The construction method (e.g., [33, 42]) is another way to deal with specific boundary constraints, while it is only efficient to particular constraints and computational domains.\nExcept for the penalty or construction based methods, one may consider the La- grange method or augmented Lagrangian method(ALM) to transform the constrained optimization problem into a minimax problem. It has been widely used for solving constrained optimization problems involving neural networks and been successfully applied in various fields, such as natural language processing (NLP) [35], classifica- tion [38] and some others [29, 36, 9]. As for problems related to differential operators, neural networks are used to represent the decision variables, and then the problem is discretized, with the introduction of Lagrange multipliers on some fixed grid points. For example, in [25], ALM was used to impose equality constraints in addressing the physical conservation laws of kinetic PDE problems, which resulted in more accurate approximations of the solutions in terms of errors and the conservation laws. In [32], the applications of ALM, penalty method, and adaptive penalty method for inverse design problems were compared under the framework of physics-informed neural net- works (PINNs). The results demonstrated that ALM is relatively more robust and less sensitive to hyperparameters. Enhanced physics-informed neural networks with augmented Lagrangian relaxation method (AL-PINNs) [41] treated the initial and boundary conditions as constraints for the optimization problem of the PDE residual. Various numerical examples showed that AL-PINNs yield a much smaller relative er- ror compared to state-of-the-art adaptive loss-balancing algorithms. The augmented Lagrangian deep learning (ALDL) [24] constructed the Lagrange multipliers in the sense of adversarial network to impose the essential boundary conditions of varia- tional problems, avoiding the discretization of the Lagrange multipliers. However, the papers mentioned above either rely on the ALM form, with the Lagrange multi- pliers defined and updated on a batch of fixed grid points, or only handle boundary conditions as constraints in variational problems. These methods are restricted by the selection of grid points and may be inefficient for high-dimensional problems due to the curse of dimensionality. Additionally, they are difficult to integrate with other improved frameworks, such as adaptive methods that need to adaptively sample the collocation points.\nInspired by the framework of WAN [51, 5], we propose the weak adversarial net- works for constrained optimization (WANCO) to solve constrained optimization prob- lems. WANCO has a wide range of applications and can handle various constraints, including for example, scalar constraints, linear and nonlinear constraints, PDE con- straints, and inequality constraints. It is based on the augmented Lagrangian form of the constrained optimization problems by representing decision variables and La- grange multipliers using individual networks. The problems are then trained through an adversarial process. The proposed WANCO is less sensitive to the weights of dif- ferent terms in the loss function and can be easily integrated with other techniques to improve the performance of the training result. In the numerical examples, we demonstrate the capability and robustness of the parameter setting of WANCO on various constrained optimization problems, including the Ginzburg-Landau energy minimization problems, Dirichlet partition problems in different dimensions, fluid- solid optimization problems, and obstacle problems.\nThe rest of this paper is organized as follows. In Section 2, we introduce the proposed WANCO with its network architectures. In Section 3, we use the mass pre- serving Ginzburg-Landau energy minimization problem as an example to demonstrate the performance of WANCO. In particular, Section 3.1 presents the comparison results to other deep learning based approaches, demonstrating the insensitivity of WANCO to parameters, and Section 3.2 shows comparison results to shed light on the choice of activation functions. In Section 4, we investigate the high dimensional performance of WANCO by considering Dirichlet partition problems up to four dimensions. The PDE constrained optimization problems are considered in Section 5 for fluid-solid optimization problems as the example. In Section 6, WANCO solves the obstacle problems involving inequality constraints. We draw a conclusion and discussion in Section 7."}, {"title": "2. Framework of WANCO", "content": "To introduce the weak adversarial networks for constrained optimization problems (WANCO), we consider a general constrained op- timization problem in the following form:\n```\nmin L(u)\nu\ns.t. C(u) = 0,\n```\nwhere u = u(x) represents the decision variable that needs to be optimized, L denotes the objective functional of u, and C(u) = 0 represents the constraint that can de- scribe various types of constraints, such as scalar constraints, boundary constraints, PDE constraints, and even inequality constraints by introducing slack variables. This problem finds applications in diverse fields, including fluid mechanics, material sci- ence, quantum mechanics, industrial design, and topology optimization.\nFor constrained optimization problems (2.1), we first transform it into a minimax problem with the Lagrange multiplier \u03bb and a penalty term as follows,\n```\nmin max L\u03b2(u, \u03bb) = CoL(u) \u2013 \u27e8\u03bb, C(u)\u27e9 + \u03b2/2 ||C(u)||^2,\n```\nin which Co and \u03b2 are scalar coefficients. Co is a fixed parameter used to avoid the objective functional value being too small in specific problems. \u03b2 gradually increases during the optimization process, similar to that in the augmented Lagrangian method.\nIn WANCO, we propose to use two(or several) individual deep neural networks (similar to WAN and IWAN) to represent u = u(x; \u03b8), and \u03bb = \u03bb(x; \u03b7) with parameters \u03b8 and \u03b7, respectively. Problem (2.2) is then transformed into a minimax problem with respect to \u03b8 and \u03b7,\n```\nmin max L\u03b2(u(x; \u03b8), \u03bb(x; \u03b7)) = CoL(u(x; \u03b8)) \u2013 \u27e8\u03bb(x; \u03b7), C(u(x; \u03b8))\u27e9 + \u03b2/2 ||C(u(x; \u03b8)||^2.\n```\nThe parameters \u03b8 and \u03b7 are then alternatively updated by stochastic gradient descent and stochastic gradient ascent of L\u03b2(u(x; \u03b8), \u03bb(x; \u03b7)), with a gradually increasing value of \u03b2. The algorithmic framework of WANCO is summarized in Algorithm 2.1, and the neural network diagram of WANCO is illustrated in Figure 1.\nIn terms of the network architecture, we represent the decision variables by the residual neural networks (ResNet). The Lagrange multiplier is represented by differ- ent networks based on their properties. For example,\n1. if the constraint is scalar, \u03bb(x; \u03b7) can be simply set as \u03bb(0; \u03b7) with constant input 0 and a shallow neural network,\n2. if the constraint is defined over the entire computational domain (e.g., PDE con- straints), \u03bb(x; \u03b7) is set to be a function in the domain represented by a ResNet,\n3. if the constraint is an inequality, structured neural networks can be used to enforce the non-positivity of \u03bb(x; \u03b7).\nTo be specific, we use the ResNet [23] with Na hidden layers and width Nw, expressed as follows,\n```\nu(x0; \u03b8) = WoutXNa+1 + bout,\nXk+1 = fk(xk) = \u03a6k(Wkxk + bk) + xk, k = Na, ..., 1,\nX1 = Winx + bin.\n```\nHere, xo \u2208 \u03a9 is the input of dimension d, {Win \u2208 R^(Nw\u00d7d),Wk \u2208 R^(Nw\u00d7Nw), Wout \u2208 R^(n\u00d7Nw)} are the weight matrices of the input layer, k-th hidden layer, and output layer, {bin \u2208 R^Nw, bk \u2208 R^Nw, bout \u2208 R^n} are the corresponding bias vectors, and \u03a6k represents the activation function. The parameters Na and Nw are used to define the size of the weight matrices and bias vectors in the neural network, which ulti- mately determine the network's performance and capabilities from many empirical observations and the universal approximation theorem. ResNet effectively mitigates the problems of gradient vanishing and gradient explosion by incorporating a residual connection or skip connection, represented by the +xk term in the second equation of (2.4). As for the activation function \u03a6k, we will use tanh\u00b3 instead of common ac- tivation functions [39] (e.g., sigmoid, tanh, softmax, ReLU, and ReLU\u00b3). A detailed numerical comparison will be provided in Section 3.\nAfter constructing the saddle point problem (2.3), the loss function is opti- mized iteratively by the standard optimizer Adam [28] with the stochastic gradient descent(SGD) and ascent method. When computing different terms of the loss func- tion, we employ several standard DNN techniques, including the Monte Carlo (MC) method for integrals and auto-differentiation for derivatives. To avoid getting stuck in undesirable local minima, we incorporate shuffling of training points during the training process, as shown in [20] for example. To simplify the optimization pro- cess, we design specific networks that automatically satisfy boundary or positivity constraints. In cases where these constraints are not straightforward, they can be im- posed as additional adversarial networks in the WANCO framework. The numerical results presented in Sections 3-6 demonstrate that WANCO is a versatile algorithm"}, {"title": "3. Minimization of the Ginzburg-Landau energy with mass conserva- tion", "content": "In this section, we focus on the empirical study of the sensitivity of the pa- rameters, the choice of activation functions, and the comparison to the penalty based methods and adversarial network with only Lagrange multipliers. We illustrate them through the minimization of the Ginzburg-Landau energy with mass conservation as an example.\nConsider the minimization of the Ginzburg-Landau energy with mass conserva- tion and homogeneous Dirichlet boundary condition in \u03a9 = [0,1] \u00d7 [0, 1] as follows,\n```\nmin \u222b\u03a9 1/2|\u2207u|^2 + 1/4(u^2 - 1)^2 dx\ns.t. \u222b\u03a9 u dx = V, and u = -1 on \u2202\u03a9,\n```\nwhere u : (x,y) \u2192 R is the phase order parameter, and V is a constant used to enforce the integral of u, which is equivalent to the total mass in a two-phase system if u represents rescaled density.\nIn this case, we use ResNet to represent u. Regarding the Lagrange multiplier, we represent \u03bb using a shallow neural network with one hidden layer, which includes a small number of neurons with the activation function \u03a6. We set the input to be 0, and thus \u03bb(x; y) can be viewed as a scalar variable \u03bb(0; \u03b7).\nAs for the boundary conditions of the primal network u(x; \u03b8), we enforce them ex- plicitly at the output layer. Specifically, we impose the following boundary conditions at the output layer:\n```\nu(x0; \u03b8) = (Wout fNa(xNa) + bout) \u00d7 (1 - x1) x (1-x2) x (-1),\n```\nin which x1 and x2 correspond to two coordinates of the input xo and u(x0; \u03b8) = -1 for xo \u2208 \u2202\u03a9. This approach is suitable for relatively regular geometries, such as the unit square we considered in this example. For more general boundary conditions, we can treat them as constraints and use the framework of WANCO to convert them into another adversarial network, as demonstrated in Section 5."}, {"title": "3.1. Sensitivity to parameters and comparison to other deep learning based approaches", "content": "For problem (3.1), we obtain the following minimax problem:\n```\nmin max L^(GL)(\u03b8, \u03b7)\n```\nwhere\n```\nL^(GL)(\u03b8, \u03b7) = C0 \u222b\u03a9 [\u221au(x; \u03b8)2 + 1/4(u(x; \u03b8)^2 \u2212 1)^2] dx\\-\n\u03bb(0; \u03b7) (\u222b\u03a9 u(x; \u03b8)dx \u2212 V) + \u03b2/2 (\u222b\u03a9 u(x; \u03b8)dx \u2212 V)^2.\n```\nWe then solve this problem with the proposed WANCO in contrast to:\n1. DRM combined with penalty method (DRM-P),\n2. DRM combined with adaptive penalty method(DRM-AP), and\n3. DRM combined with Lagrange multipliers.\nFrom the classical analysis of problem (3.1), the zero level set of the solution should be a circle with the enclosed area conserved. In this example, we set \u03f5 = 0.05, V = -0.5, C0 = 400, and \u03b1 = 1.0003. For the primal network u(x; \u03b8), the number of hidden layers is Na = 4 and the width is Nw = 50. To represent the adversarial network, we employ a shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 since the Lagrange multiplier \u03bb is a scalar in this case. The training step is set to N = 5000 with inner iteration steps Nu = N\u03bb = 2. In each iteration, N = 40,000 points are used for training. The initial learning rate is \u03c4\u03b8 = \u03c4\u03b7 = 0.016 and halved at certain training milestones. This learning rate setting is consistent throughout the subsequent numerical examples, and will not be reiterated later. The activation function used here is tanh3(x). Both DRM-P and DRM-AP have the same structure as the primal network mentioned above.\nIn Figure 2, we display the results from WANCO, DRM-P, and DRM-AP for different choices of \u03b2 = 100000, 10000, 1000, together with the convergence of the constraint error, defined as |V-\u222b\u03a9udx|, during the training process in Figure 3. It is worth noting that WANCO employs identical parameters across different \u03b2 values in these results. In contrast, both DRM-P and DRM-AP require individual parameter tuning for each case to obtain satisfactory results (which may still completely fail due to the poor selection of \u03b2). This implies that the proposed WANCO is relatively insensitive to parameter settings. As shown in Figure 2, under three different initial penalty terms \u03b2, the proposed WANCO consistently outperforms DRM-P and DRM-AP. For the first case with \u03b2 = 100000, the profiles of WANCO, DRM-AP, and DRM-P are close to circles, but WANCO and DRM-AP have a clear advantage in the relative error of the constraint. As for the second and third cases with \u03b2 = 10000, 1000, DRM-P fails, and DRM-AP exhibits relatively inaccurate results (the computed results are not circular) due to the unbalanced terms in the loss, while WANCO trains well. This indicates that relying solely on the adaptive penalty method is still sensitive to parameter selection, and comparable results are achieved only when \u03b2 = 100000.\nIn all cases, WANCO can achieve a relative error of the constraint less than 0.005. As shown in Figure 3, compared to DRM-AP, WANCO can significantly reduce the training steps due to the adversarial term in (3.3). To be more specific, due to the Dirichlet boundary condition and the random initialization of network parameters, the output of the primal network in the initial steps closely approximates -1. Con- sequently, the initial integral of u is also close to -1, falling below the constraint of -0.5. With the inclusion of the adversarial term \u03bb, during the early stages of training, the mass of u increases at a faster rate compared to DRM-AP. As the adversarial term \u03bb reaches a sufficiently large value, it becomes dominant, causing the minimization step to shift the mass in the positive direction. This corresponds to the initial steps as shown in Figure 3. Once the error of the constraint becomes positive, \u03bb is reduced. Subsequently, when the third term is dominant again, the constraint error decreases rapidly. This corresponds to the stage of rapid descent in Figure 3, which is more evident in the last two pictures. To conclude, the inclusion of the adversarial term \u03bb allows for an adaptive adjustment of its magnitude, enabling the constraint to be rapidly satisfied from both sides. This is in contrast to DRM-AP, where the mass can only decrease rapidly as the penalty term reaches a threshold value. This further demonstrates that WANCO is relatively insensitive to the setting of hyperparameters, as the adversarial term exhibits a notable adaptive weighting and acceleration effect. Moreover, from the training perspective, in WANCO, half of the training steps are on the optimization of the adversarial network, which is usually constructed by a smaller network. This usually results in faster training compared to other methods.\nWe also consider the Lagrange multiplier formula without penalty terms, and empirical study shows that it is very difficult to achieve convergence in the adversar- ial process. As illustrated in Figure 4, using only the Lagrange multiplier method combined with adversarial networks leads to failure. We observe that the Lagrange multiplier only has a corresponding ascent direction during training, and its effective- ness in imposing the constraint is affected by the step size (learning rate). Therefore, relying solely on the Lagrange multiplier may result in convergence difficulties. How- ever, with the augmented Lagrangian method, we observe that the adversarial term adjusts its scale of values based on the current level of constraint satisfaction to ac- celerate the process of satisfying the constraint, while the penalty term gradually increases to guarantee convergence. As a result, the adversarial process is effectively similar to assigning different weights to the training data, enabling WANCO to impose the constraints more effectively."}, {"title": "3.2. Choice of activation functions", "content": "To illustrate the observations and rea- sons for the utilization of tanh\u00b3 (see the profile in Figure 5) in our examples, we present two numerical simulations to validate the advantages of combining ResNet with the activation function tanh\u00b3 in the proposed WANCO.\nFor the above problem (3.3), we consider: 1. replacing the activation function directly with other activation functions in the model that has been well-tuned for tanh\u00b3, and 2. replacing the activation function with tanh\u00b3 in the model that has been well-tuned for other common activation functions.\nAs shown in Figure 6, when replacing the activation function tanh\u00b3 in the well- tuned neural network with other activation functions, the training results are often poor or even incorrect. However, as shown in Figure 7, when initially utilizing a well- tuned neural network with a different activation function and subsequently changing it to tanh\u00b3, we observe that similar or even better results can be achieved.\nBased on the observations from these two examples, we believe that combining ResNet with tanh\u00b3 exhibits favorable properties and can reduce parameter tuning time for WANCO. This choice is also used and verified in examples for the subsequent problems."}, {"title": "4. Optimal partition problems", "content": "In this section, we consider the optimal par- tition problem, which minimizes the Dirichlet energy with L2 norm preserving in different dimensions and different boundary conditions [10, 19] as follows.\n```\nmin \u222b\u03a9 1/2 \u2211|\u2207ui|^2 + \u2211f(ui, uj) dx\ns.t. \u222b\u03a9 u^2 dx = 1, i = 1, 2, ..., n\n```\nand u satisfies the homogeneous Dirichlet or periodic boundary conditions,\nwhere \u03a9 = [0, 1]^d, d denotes the dimension, n represents the number of phases, u = [u1, u2, ..., un]T denotes the phase variables, with each ui mapping x to R+, \u03f5 is the model parameter related to the thickness of the transition layer, and f takes the form f(u) = \u2211\u2211i,j=1,i\u2260j \u03f5iju^2i u^2j.\nFollowing the augmented Lagrangian method, one can obtain the minimax prob- lem:\n```\nmin max Co 1/2\u222b\u03a9 \u2211|\u2207ui|^2 + \u2211 \u2211 \u03f5iju^2i u^2j dx\n- \u2211 \u03bbi (\u222b\u03a9 u^2i dx - 1) + \u03b2/2\u222b\u03a9 \u2211 (\u222b\u03a9 u^2i dx - 1)^2,\n```\nwhere \u03bb denotes n Lagrange multipliers for n constraints in (4.1), and 1 is the vector in R^n whose component are all ones.\nThen we construct the primal and adversarial networks to represent u = u(x; \u03b8) and \u03bb = \u03bb(x; \u03b7), respectively. Since the L2 norm preserving constraint is independent of x, we construct the adversarial network as \u03bb(0; \u03b7) = [\u03bb1(0; \u03b7), \u03bb2(0; \u03b7), ..., \u03bbn(0; \u03b7)]T using a shallow neural network. Consequently, the problem (4.2) is transformed into the following form:\n```\nmin max L\u03b2(u(x; \u03b8), \u03bb(0; \u03b7))\n= Co \u222b\u03a9 1/2 \u2211 |\u2207u(x; \u03b8)|^2 + \u2211 \u2211 \u03f5iju^2i(x; \u03b8)u^2j(x; \u03b8) dx\n- \u2211 \u03bbi(0; \u03b7) \u222b\u03a9 u^2i(x; \u03b8)dx - 1 + \u03b2/2 \u2211 (\u222b\u03a9 u^2i(x; \u03b8)dx - 1)^2,\n```\nIn the follows, we consider the results with the Dirichlet and periodic boundary conditions separately. The parameter settings and training results for the Dirichlet and periodic boundary conditions are given separately. These results in 2- and 3- dimensional space are consistent with the results presented in [43, 8, 44], indicating the effectiveness and robustness of the proposed method. In addition, the 4-dimensional partition results are displayed to demonstrate the efficiency of the proposed WANCO."}, {"title": "4.1. Dirichlet partition with Dirichlet boundary conditions", "content": "In this sec- tion, we consider the Dirichlet partition problem with Dirichlet boundary conditions in the unit square \u03a9 = [0,1] \u00d7 [0,1] with the L2 norm preserved. The Dirichlet boundary condition is imposed as a strong form at the output layer similar to the first numerical example,\n```\nu(x0; \u03b8) = ReLU(Wout fNa(xNa) + bout) \u00d7 \u03a0_i=1 [(x1i) \u00d7 (1 - x1i)],\n```\nin which xi(i) corresponds to the i-th entry of the input xo \u2208 \u03a9 and ReLU here is used to enforce the non-negativity of u.\nThe parameters for the primal networks are set as follows: \u03f5 = 0.05, Co = 100, \u03b2 = 10000, \u03b1 = 1.0003, Na = 8, and Nw = 120. A shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 is used to represent the adversarial network. The training is performed for N = 20000 steps with inner iteration steps Nu = N\u03bb = 1, and N = 10000 points are used for training in each iteration. The testing results are obtained by evaluating the trained neural network on a uniform 1000 \u00d7 1000 grid.\nIn Figure 8, we display the optimal n-partitions with Dirichlet boundary condi- tions, which are consistent with the results reported in [43, 8], computed with tradi- tional numerical methods. The first and third rows of Figure 8 represent the network output after being well trained. The second and fourth rows represent the plot of n partitions based on the following projection,\n```\n\u03a6(x) = m, m = arg max 1<=k<=n uk(x; \u03b8), for x \u2208 \u03a9.\n```\nIn the traditional method for this optimization problem, one has to carefully handle the boundary condition by employing concave relaxation or Schr\u00f6dinger operator re- laxation and design the optimization scheme to obtain reasonable and regular results. In WANCO, the aforementioned parameters can effectively train the network for all numbers (i.e., n = 2-9) of partitions, which correspond to different dimensions of the network output. This implies the robustness of WANCO to the parameter setting."}, {"title": "4.2. Dirichlet partition with periodic boundary conditions", "content": "In this sec- tion, we consider the Dirichlet partition problem in 2-, 3-, and 4-dimensional flat tori. The periodic boundary conditions on \u03a9 = [0,1]^d are imposed by transforming the input layer as follows [33],\n```\nfin(x0) = f(x(1), x(2), ..., x(d));\n= (cos(2\u03c0x(1)), sin(2\u03c0x(1)), cos(2\u03c0x(2)), sin(2\u03c0x(2)), ..., cos(2\u03c0x(d)), sin(2\u03c0x(d)))T,\n```\nin which x(i) corresponds to the i-th entry of the input x \u2208 \u03a9.\nThe parameters for the primal networks are set as follows: \u03f5 = 0.04, Co = 2500, \u03b2 = 100000, \u03b1 = 1.0003, Na = 3, and the width is increasing from Nw = 50 to Nw = 200 as the dimension or the number of phases increases. A shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 is used to represent the adversarial network. The training is performed for N = 5000 steps with inner iteration steps Nu = N\u03bb = 2. N = 10000, 40000, 60000 points are used for 2-, 3-, and 4-dimensional cases in each iteration, respectively. The sampling points used are fixed as the Hammersley sequence in the corresponding dimension, similar to that mentioned in [47].\nTwo-dimensional flat torus: In Figures 9 and 10, we display the results of n-partition in a 2-dimensional flat torus trained by the proposed WANCO for n = 3 \u2013 9, 11, 12, 15, 16, 18, 20, 23, and 24. Again, the results in Figure 10 are pro- jections using (4.5) based on the trained results in Figure 9. The testing results are obtained by evaluating the trained neural network on a uniform 1000 \u00d7 1000 grid. To our knowledge, even in the 2-dimensional case, there are not too many efficient numerical methods that can compute the n-partition problems for a large n (e.g., n = 24). Existing methods [13, 15] usually require thousands of steps to converge for the problem with a large number of partitions, even in the 2-dimensional case. In all of the previous studies, hexagons are ubiquitous for large values of n. Figures 9 and 10 show that WANCO obtains consistent results for a large range of n. Since the domain has an aspect ratio equal to one, regular hexagons cannot be used to tile the domain, so the hexagons are slightly distorted.\nThree-dimensional flat torus: As for the 3-dimensional flat torus, we consider the situations for n = 4 and 8. The results are based on the evaluation of the trained network on a uniform 100^3 grid. For n = 4, we obtain a partition of the cube consisting of four identical rhombic dodecahedron structures, as displayed in Figure 11. For n = 8, we obtain a partition of the cube that is similar to the Weaire-Phelan structure. Figure 12 shows different views of the first and second type Weaire-Phelan structures, together with different views of a periodic extension of the partition.\nFour-dimensional flat torus: We then apply the proposed WANCO to 4- dimensional partitioning problems. In Figures 13, 14 and 15, the four columns cor- respond to slices perpendicular to the x1-, x2-, x3-, and x4-axes, respectively. The four rows correspond to the slices at xj = 0, 0.25, 0.5, 0.75, respectively. These par- titions are based on testing the trained neural network on a uniform 100^3 grid at corresponding 3-dimensional slices.\nFor n = 2, we obtain a constant extension of the structure along the fourth direction, similar to the Schwarz P-surface, as displayed in Figure 13. Traditional methods usually result in slab partitions for 2 partitions in 3-dimensional flat torus with a random initialization of the partition. However, based on the evidence from the numerical study in [44], the Schwarz P-surface also appears to be a local minimizer for the 3-dimensional case. In this 4-dimensional flat torus, the random initialization represented by the neural network can find different local minimizers or stationary solutions more easily.\nFor n = 4, we obtain a partition structure similar to a constant extension along the fourth direction of the rhombic dodecahedron as displayed in Figure 14, which is a 24-cell honeycomb, which is a tessellation by regular 24-cells."}, {"title": "5. Fluid-solid optimization", "content": "Topology optimization is a technique that aims to find the optimal layout or distribution of materials to maximize the objective within a specified design space while satisfying a set of constraints. Typically", "problem": "n```\nmin CaJa(\u03c6", "networks": "p(x; \u03b70)"}, {"problem": "n```\nmin max L\u03b2(u(x; \u03b8)", "title": "WANCO: WEAK ADVERSARIAL NETWORKS FOR CONSTRAINED OPTIMIZATION PROBLEMS", "authors": ["GANG BAO", "DONG WANG", "BOYI ZOU"], "abstract": "This paper focuses on integrating the networks and adversarial training into con- strained optimization problems to develop a framework algorithm for constrained optimization prob- lems. For such problems, we first transform them into minimax problems using the augmented Lagrangian method and then use two (or several) deep neural networks(DNNs) to represent the primal and dual variables respectively. The parameters in the neural networks are then trained by an adversarial process. The proposed architecture is relatively insensitive to the scale of values of different constraints when compared to penalty based deep learning methods. Through this type of training, the constraints are imposed better based on the augmented Lagrangian multipliers. Ex- tensive examples for optimization problems with scalar constraints, nonlinear constraints, partial differential equation constraints, and inequality constraints are considered to show the capability and robustness of the proposed method, with applications ranging from Ginzburg-Landau energy minimization problems, partition problems, fluid-solid topology optimization, to obstacle problems.", "sections": [{"title": "1. Introduction", "content": "Constrained optimization problems arise from an intricate and practical set of issues, with the objective of identifying solutions to an optimiza- tion problem while adhering to specific constraints. These problems play a crucial role in various domains such as finance, economics, engineering, operation research, machine learning, and others. For a comprehensive exploration of constrained opti- mization problems, we refer to [7] and the references therein. A wide range of problems in scientific computing can be classified as constrained optimization problems, for ex- ample, inverse scattering problems [4, 2], optical sciences [3], topology optimization problems [6], operation research, optimal control, and obstacle problems.\nIn this paper, we focus on constrained optimization problems related to differen- tial operators that can appear in either the objective functional or the constraints. There are many efficient classical methods for solving constrained optimization prob- lems [7, 22, 46], such as the penalty method, the Lagrange multiplier method, the augmented Lagrangian method, and the interior point method. When solving differ- ential operator related problems with constraints, traditional numerical methods such as the finite difference method, the finite element method, or the spectral method can be applied with careful construction, discretization, and post-processing for satisfying the constraints.\nIn recent years, there has been rapid development in artificial intelligence (AI) for science, with particular attention being drawn to physics-informed machine learn- ing in scientific computing. The universal approximation of continuous functions using neural networks, as proven in [17], has gained significant attention in the past decade, leading to an increasing trend of utilizing deep neural networks to solve dif- ferential operator related problems. A lot of studies have demonstrated the effective- ness of deep learning approaches in solving partial differential equations, optimiza- tion problems with differential equation constraints, or inverse problems, including physics-informed neural network (PINN) [37], deep Galerkin method (DGM) [40], deep Ritz method (DRM) [20], weak adversarial network (WAN) [51, 5], random fea- ture method (RFM) [12], local extreme learning machines (locELM) [18], particle weak-form based neural networks (ParticleWNN) [50] and many other mechanism driven methods [27, 48, 30, 34, 49, 47, 45]. In contrast with traditional methods, deep learning methods are usually mesh-free. Therefore, they are less affected by the curse of dimensionality, making them efficient for computing high-dimensional problems. For a comprehensive overview of using DNNs to solve differential operator related problems, we refer to [31, 26, 16] and references therein.\nThe framework for using DNNs to solve differential operator constrained opti- mization problems involves constructing a problem-specific loss function, followed by solving an optimization problem with boundary conditions and other constraints. One common approach is to utilize penalty methods to transform the constrained optimization problem into an unconstrained optimization problem, as used in PINN, DGM, and DRM. The results are highly dependent on the selection of the weights of different terms in the loss function. In particular, one needs to carefully consider the scale of the values of constraints to guarantee that the training process can minimize each term in the loss function equally. The construction method (e.g., [33, 42]) is another way to deal with specific boundary constraints, while it is only efficient to particular constraints and computational domains.\nExcept for the penalty or construction based methods, one may consider the La- grange method or augmented Lagrangian method(ALM) to transform the constrained optimization problem into a minimax problem. It has been widely used for solving constrained optimization problems involving neural networks and been successfully applied in various fields, such as natural language processing (NLP) [35], classifica- tion [38] and some others [29, 36, 9]. As for problems related to differential operators, neural networks are used to represent the decision variables, and then the problem is discretized, with the introduction of Lagrange multipliers on some fixed grid points. For example, in [25], ALM was used to impose equality constraints in addressing the physical conservation laws of kinetic PDE problems, which resulted in more accurate approximations of the solutions in terms of errors and the conservation laws. In [32], the applications of ALM, penalty method, and adaptive penalty method for inverse design problems were compared under the framework of physics-informed neural net- works (PINNs). The results demonstrated that ALM is relatively more robust and less sensitive to hyperparameters. Enhanced physics-informed neural networks with augmented Lagrangian relaxation method (AL-PINNs) [41] treated the initial and boundary conditions as constraints for the optimization problem of the PDE residual. Various numerical examples showed that AL-PINNs yield a much smaller relative er- ror compared to state-of-the-art adaptive loss-balancing algorithms. The augmented Lagrangian deep learning (ALDL) [24] constructed the Lagrange multipliers in the sense of adversarial network to impose the essential boundary conditions of varia- tional problems, avoiding the discretization of the Lagrange multipliers. However, the papers mentioned above either rely on the ALM form, with the Lagrange multi- pliers defined and updated on a batch of fixed grid points, or only handle boundary conditions as constraints in variational problems. These methods are restricted by the selection of grid points and may be inefficient for high-dimensional problems due to the curse of dimensionality. Additionally, they are difficult to integrate with other improved frameworks, such as adaptive methods that need to adaptively sample the collocation points.\nInspired by the framework of WAN [51, 5], we propose the weak adversarial net- works for constrained optimization (WANCO) to solve constrained optimization prob- lems. WANCO has a wide range of applications and can handle various constraints, including for example, scalar constraints, linear and nonlinear constraints, PDE con- straints, and inequality constraints. It is based on the augmented Lagrangian form of the constrained optimization problems by representing decision variables and La- grange multipliers using individual networks. The problems are then trained through an adversarial process. The proposed WANCO is less sensitive to the weights of dif- ferent terms in the loss function and can be easily integrated with other techniques to improve the performance of the training result. In the numerical examples, we demonstrate the capability and robustness of the parameter setting of WANCO on various constrained optimization problems, including the Ginzburg-Landau energy minimization problems, Dirichlet partition problems in different dimensions, fluid- solid optimization problems, and obstacle problems.\nThe rest of this paper is organized as follows. In Section 2, we introduce the proposed WANCO with its network architectures. In Section 3, we use the mass pre- serving Ginzburg-Landau energy minimization problem as an example to demonstrate the performance of WANCO. In particular, Section 3.1 presents the comparison results to other deep learning based approaches, demonstrating the insensitivity of WANCO to parameters, and Section 3.2 shows comparison results to shed light on the choice of activation functions. In Section 4, we investigate the high dimensional performance of WANCO by considering Dirichlet partition problems up to four dimensions. The PDE constrained optimization problems are considered in Section 5 for fluid-solid optimization problems as the example. In Section 6, WANCO solves the obstacle problems involving inequality constraints. We draw a conclusion and discussion in Section 7."}, {"title": "2. Framework of WANCO", "content": "To introduce the weak adversarial networks for constrained optimization problems (WANCO), we consider a general constrained op- timization problem in the following form:\n```\nmin L(u)\nu\ns.t. C(u) = 0,\n```\nwhere u = u(x) represents the decision variable that needs to be optimized, L denotes the objective functional of u, and C(u) = 0 represents the constraint that can de- scribe various types of constraints, such as scalar constraints, boundary constraints, PDE constraints, and even inequality constraints by introducing slack variables. This problem finds applications in diverse fields, including fluid mechanics, material sci- ence, quantum mechanics, industrial design, and topology optimization.\nFor constrained optimization problems (2.1), we first transform it into a minimax problem with the Lagrange multiplier \u03bb and a penalty term as follows,\n```\nmin max L\u03b2(u, \u03bb) = CoL(u) \u2013 \u27e8\u03bb, C(u)\u27e9 + \u03b2/2 ||C(u)||^2,\n```\nin which Co and \u03b2 are scalar coefficients. Co is a fixed parameter used to avoid the objective functional value being too small in specific problems. \u03b2 gradually increases during the optimization process, similar to that in the augmented Lagrangian method.\nIn WANCO, we propose to use two(or several) individual deep neural networks (similar to WAN and IWAN) to represent u = u(x; \u03b8), and \u03bb = \u03bb(x; \u03b7) with parameters \u03b8 and \u03b7, respectively. Problem (2.2) is then transformed into a minimax problem with respect to \u03b8 and \u03b7,\n```\nmin max L\u03b2(u(x; \u03b8), \u03bb(x; \u03b7)) = CoL(u(x; \u03b8)) \u2013 \u27e8\u03bb(x; \u03b7), C(u(x; \u03b8))\u27e9 + \u03b2/2 ||C(u(x; \u03b8)||^2.\n```\nThe parameters \u03b8 and \u03b7 are then alternatively updated by stochastic gradient descent and stochastic gradient ascent of L\u03b2(u(x; \u03b8), \u03bb(x; \u03b7)), with a gradually increasing value of \u03b2. The algorithmic framework of WANCO is summarized in Algorithm 2.1, and the neural network diagram of WANCO is illustrated in Figure 1.\nIn terms of the network architecture, we represent the decision variables by the residual neural networks (ResNet). The Lagrange multiplier is represented by differ- ent networks based on their properties. For example,\n1. if the constraint is scalar, \u03bb(x; \u03b7) can be simply set as \u03bb(0; \u03b7) with constant input 0 and a shallow neural network,\n2. if the constraint is defined over the entire computational domain (e.g., PDE con- straints), \u03bb(x; \u03b7) is set to be a function in the domain represented by a ResNet,\n3. if the constraint is an inequality, structured neural networks can be used to enforce the non-positivity of \u03bb(x; \u03b7).\nTo be specific, we use the ResNet [23] with Na hidden layers and width Nw, expressed as follows,\n```\nu(x0; \u03b8) = WoutXNa+1 + bout,\nXk+1 = fk(xk) = \u03a6k(Wkxk + bk) + xk, k = Na, ..., 1,\nX1 = Winx + bin.\n```\nHere, xo \u2208 \u03a9 is the input of dimension d, {Win \u2208 R^(Nw\u00d7d),Wk \u2208 R^(Nw\u00d7Nw), Wout \u2208 R^(n\u00d7Nw)} are the weight matrices of the input layer, k-th hidden layer, and output layer, {bin \u2208 R^Nw, bk \u2208 R^Nw, bout \u2208 R^n} are the corresponding bias vectors, and \u03a6k represents the activation function. The parameters Na and Nw are used to define the size of the weight matrices and bias vectors in the neural network, which ulti- mately determine the network's performance and capabilities from many empirical observations and the universal approximation theorem. ResNet effectively mitigates the problems of gradient vanishing and gradient explosion by incorporating a residual connection or skip connection, represented by the +xk term in the second equation of (2.4). As for the activation function \u03a6k, we will use tanh\u00b3 instead of common ac- tivation functions [39] (e.g., sigmoid, tanh, softmax, ReLU, and ReLU\u00b3). A detailed numerical comparison will be provided in Section 3.\nAfter constructing the saddle point problem (2.3), the loss function is opti- mized iteratively by the standard optimizer Adam [28] with the stochastic gradient descent(SGD) and ascent method. When computing different terms of the loss func- tion, we employ several standard DNN techniques, including the Monte Carlo (MC) method for integrals and auto-differentiation for derivatives. To avoid getting stuck in undesirable local minima, we incorporate shuffling of training points during the training process, as shown in [20] for example. To simplify the optimization pro- cess, we design specific networks that automatically satisfy boundary or positivity constraints. In cases where these constraints are not straightforward, they can be im- posed as additional adversarial networks in the WANCO framework. The numerical results presented in Sections 3-6 demonstrate that WANCO is a versatile algorithm"}, {"title": "3. Minimization of the Ginzburg-Landau energy with mass conserva- tion", "content": "In this section, we focus on the empirical study of the sensitivity of the pa- rameters, the choice of activation functions, and the comparison to the penalty based methods and adversarial network with only Lagrange multipliers. We illustrate them through the minimization of the Ginzburg-Landau energy with mass conservation as an example.\nConsider the minimization of the Ginzburg-Landau energy with mass conserva- tion and homogeneous Dirichlet boundary condition in \u03a9 = [0,1] \u00d7 [0, 1] as follows,\n```\nmin \u222b\u03a9 1/2|\u2207u|^2 + 1/4(u^2 - 1)^2 dx\ns.t. \u222b\u03a9 u dx = V, and u = -1 on \u2202\u03a9,\n```\nwhere u : (x,y) \u2192 R is the phase order parameter, and V is a constant used to enforce the integral of u, which is equivalent to the total mass in a two-phase system if u represents rescaled density.\nIn this case, we use ResNet to represent u. Regarding the Lagrange multiplier, we represent \u03bb using a shallow neural network with one hidden layer, which includes a small number of neurons with the activation function \u03a6. We set the input to be 0, and thus \u03bb(x; y) can be viewed as a scalar variable \u03bb(0; \u03b7).\nAs for the boundary conditions of the primal network u(x; \u03b8), we enforce them ex- plicitly at the output layer. Specifically, we impose the following boundary conditions at the output layer:\n```\nu(x0; \u03b8) = (Wout fNa(xNa) + bout) \u00d7 (1 - x1) x (1-x2) x (-1),\n```\nin which x1 and x2 correspond to two coordinates of the input xo and u(x0; \u03b8) = -1 for xo \u2208 \u2202\u03a9. This approach is suitable for relatively regular geometries, such as the unit square we considered in this example. For more general boundary conditions, we can treat them as constraints and use the framework of WANCO to convert them into another adversarial network, as demonstrated in Section 5."}, {"title": "3.1. Sensitivity to parameters and comparison to other deep learning based approaches", "content": "For problem (3.1), we obtain the following minimax problem:\n```\nmin max L^(GL)(\u03b8, \u03b7)\n```\nwhere\n```\nL^(GL)(\u03b8, \u03b7) = C0 \u222b\u03a9 [\u221au(x; \u03b8)2 + 1/4(u(x; \u03b8)^2 \u2212 1)^2] dx\\-\n\u03bb(0; \u03b7) (\u222b\u03a9 u(x; \u03b8)dx \u2212 V) + \u03b2/2 (\u222b\u03a9 u(x; \u03b8)dx \u2212 V)^2.\n```\nWe then solve this problem with the proposed WANCO in contrast to:\n1. DRM combined with penalty method (DRM-P),\n2. DRM combined with adaptive penalty method(DRM-AP), and\n3. DRM combined with Lagrange multipliers.\nFrom the classical analysis of problem (3.1), the zero level set of the solution should be a circle with the enclosed area conserved. In this example, we set \u03f5 = 0.05, V = -0.5, C0 = 400, and \u03b1 = 1.0003. For the primal network u(x; \u03b8), the number of hidden layers is Na = 4 and the width is Nw = 50. To represent the adversarial network, we employ a shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 since the Lagrange multiplier \u03bb is a scalar in this case. The training step is set to N = 5000 with inner iteration steps Nu = N\u03bb = 2. In each iteration, N = 40,000 points are used for training. The initial learning rate is \u03c4\u03b8 = \u03c4\u03b7 = 0.016 and halved at certain training milestones. This learning rate setting is consistent throughout the subsequent numerical examples, and will not be reiterated later. The activation function used here is tanh3(x). Both DRM-P and DRM-AP have the same structure as the primal network mentioned above.\nIn Figure 2, we display the results from WANCO, DRM-P, and DRM-AP for different choices of \u03b2 = 100000, 10000, 1000, together with the convergence of the constraint error, defined as |V-\u222b\u03a9udx|, during the training process in Figure 3. It is worth noting that WANCO employs identical parameters across different \u03b2 values in these results. In contrast, both DRM-P and DRM-AP require individual parameter tuning for each case to obtain satisfactory results (which may still completely fail due to the poor selection of \u03b2). This implies that the proposed WANCO is relatively insensitive to parameter settings. As shown in Figure 2, under three different initial penalty terms \u03b2, the proposed WANCO consistently outperforms DRM-P and DRM-AP. For the first case with \u03b2 = 100000, the profiles of WANCO, DRM-AP, and DRM-P are close to circles, but WANCO and DRM-AP have a clear advantage in the relative error of the constraint. As for the second and third cases with \u03b2 = 10000, 1000, DRM-P fails, and DRM-AP exhibits relatively inaccurate results (the computed results are not circular) due to the unbalanced terms in the loss, while WANCO trains well. This indicates that relying solely on the adaptive penalty method is still sensitive to parameter selection, and comparable results are achieved only when \u03b2 = 100000.\nIn all cases, WANCO can achieve a relative error of the constraint less than 0.005. As shown in Figure 3, compared to DRM-AP, WANCO can significantly reduce the training steps due to the adversarial term in (3.3). To be more specific, due to the Dirichlet boundary condition and the random initialization of network parameters, the output of the primal network in the initial steps closely approximates -1. Con- sequently, the initial integral of u is also close to -1, falling below the constraint of -0.5. With the inclusion of the adversarial term \u03bb, during the early stages of training, the mass of u increases at a faster rate compared to DRM-AP. As the adversarial term \u03bb reaches a sufficiently large value, it becomes dominant, causing the minimization step to shift the mass in the positive direction. This corresponds to the initial steps as shown in Figure 3. Once the error of the constraint becomes positive, \u03bb is reduced. Subsequently, when the third term is dominant again, the constraint error decreases rapidly. This corresponds to the stage of rapid descent in Figure 3, which is more evident in the last two pictures. To conclude, the inclusion of the adversarial term \u03bb allows for an adaptive adjustment of its magnitude, enabling the constraint to be rapidly satisfied from both sides. This is in contrast to DRM-AP, where the mass can only decrease rapidly as the penalty term reaches a threshold value. This further demonstrates that WANCO is relatively insensitive to the setting of hyperparameters, as the adversarial term exhibits a notable adaptive weighting and acceleration effect. Moreover, from the training perspective, in WANCO, half of the training steps are on the optimization of the adversarial network, which is usually constructed by a smaller network. This usually results in faster training compared to other methods.\nWe also consider the Lagrange multiplier formula without penalty terms, and empirical study shows that it is very difficult to achieve convergence in the adversar- ial process. As illustrated in Figure 4, using only the Lagrange multiplier method combined with adversarial networks leads to failure. We observe that the Lagrange multiplier only has a corresponding ascent direction during training, and its effective- ness in imposing the constraint is affected by the step size (learning rate). Therefore, relying solely on the Lagrange multiplier may result in convergence difficulties. How- ever, with the augmented Lagrangian method, we observe that the adversarial term adjusts its scale of values based on the current level of constraint satisfaction to ac- celerate the process of satisfying the constraint, while the penalty term gradually increases to guarantee convergence. As a result, the adversarial process is effectively similar to assigning different weights to the training data, enabling WANCO to impose the constraints more effectively."}, {"title": "3.2. Choice of activation functions", "content": "To illustrate the observations and rea- sons for the utilization of tanh\u00b3 (see the profile in Figure 5) in our examples, we present two numerical simulations to validate the advantages of combining ResNet with the activation function tanh\u00b3 in the proposed WANCO.\nFor the above problem (3.3), we consider: 1. replacing the activation function directly with other activation functions in the model that has been well-tuned for tanh\u00b3, and 2. replacing the activation function with tanh\u00b3 in the model that has been well-tuned for other common activation functions.\nAs shown in Figure 6, when replacing the activation function tanh\u00b3 in the well- tuned neural network with other activation functions, the training results are often poor or even incorrect. However, as shown in Figure 7, when initially utilizing a well- tuned neural network with a different activation function and subsequently changing it to tanh\u00b3, we observe that similar or even better results can be achieved.\nBased on the observations from these two examples, we believe that combining ResNet with tanh\u00b3 exhibits favorable properties and can reduce parameter tuning time for WANCO. This choice is also used and verified in examples for the subsequent problems."}, {"title": "4. Optimal partition problems", "content": "In this section, we consider the optimal par- tition problem, which minimizes the Dirichlet energy with L2 norm preserving in different dimensions and different boundary conditions [10, 19] as follows.\n```\nmin \u222b\u03a9 1/2 \u2211|\u2207ui|^2 + \u2211f(ui, uj) dx\ns.t. \u222b\u03a9 u^2 dx = 1, i = 1, 2, ..., n\n```\nand u satisfies the homogeneous Dirichlet or periodic boundary conditions,\nwhere \u03a9 = [0, 1]^d, d denotes the dimension, n represents the number of phases, u = [u1, u2, ..., un]T denotes the phase variables, with each ui mapping x to R+, \u03f5 is the model parameter related to the thickness of the transition layer, and f takes the form f(u) = \u2211\u2211i,j=1,i\u2260j \u03f5iju^2i u^2j.\nFollowing the augmented Lagrangian method, one can obtain the minimax prob- lem:\n```\nmin max Co 1/2\u222b\u03a9 \u2211|\u2207ui|^2 + \u2211 \u2211 \u03f5iju^2i u^2j dx\n- \u2211 \u03bbi (\u222b\u03a9 u^2i dx - 1) + \u03b2/2\u222b\u03a9 \u2211 (\u222b\u03a9 u^2i dx - 1)^2,\n```\nwhere \u03bb denotes n Lagrange multipliers for n constraints in (4.1), and 1 is the vector in R^n whose component are all ones.\nThen we construct the primal and adversarial networks to represent u = u(x; \u03b8) and \u03bb = \u03bb(x; \u03b7), respectively. Since the L2 norm preserving constraint is independent of x, we construct the adversarial network as \u03bb(0; \u03b7) = [\u03bb1(0; \u03b7), \u03bb2(0; \u03b7), ..., \u03bbn(0; \u03b7)]T using a shallow neural network. Consequently, the problem (4.2) is transformed into the following form:\n```\nmin max L\u03b2(u(x; \u03b8), \u03bb(0; \u03b7))\n= Co \u222b\u03a9 1/2 \u2211 |\u2207u(x; \u03b8)|^2 + \u2211 \u2211 \u03f5iju^2i(x; \u03b8)u^2j(x; \u03b8) dx\n- \u2211 \u03bbi(0; \u03b7) \u222b\u03a9 u^2i(x; \u03b8)dx - 1 + \u03b2/2 \u2211 (\u222b\u03a9 u^2i(x; \u03b8)dx - 1)^2,\n```\nIn the follows, we consider the results with the Dirichlet and periodic boundary conditions separately. The parameter settings and training results for the Dirichlet and periodic boundary conditions are given separately. These results in 2- and 3- dimensional space are consistent with the results presented in [43, 8, 44], indicating the effectiveness and robustness of the proposed method. In addition, the 4-dimensional partition results are displayed to demonstrate the efficiency of the proposed WANCO."}, {"title": "4.1. Dirichlet partition with Dirichlet boundary conditions", "content": "In this sec- tion, we consider the Dirichlet partition problem with Dirichlet boundary conditions in the unit square \u03a9 = [0,1] \u00d7 [0,1] with the L2 norm preserved. The Dirichlet boundary condition is imposed as a strong form at the output layer similar to the first numerical example,\n```\nu(x0; \u03b8) = ReLU(Wout fNa(xNa) + bout) \u00d7 \u03a0_i=1 [(x1i) \u00d7 (1 - x1i)],\n```\nin which xi(i) corresponds to the i-th entry of the input xo \u2208 \u03a9 and ReLU here is used to enforce the non-negativity of u.\nThe parameters for the primal networks are set as follows: \u03f5 = 0.05, Co = 100, \u03b2 = 10000, \u03b1 = 1.0003, Na = 8, and Nw = 120. A shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 is used to represent the adversarial network. The training is performed for N = 20000 steps with inner iteration steps Nu = N\u03bb = 1, and N = 10000 points are used for training in each iteration. The testing results are obtained by evaluating the trained neural network on a uniform 1000 \u00d7 1000 grid.\nIn Figure 8, we display the optimal n-partitions with Dirichlet boundary condi- tions, which are consistent with the results reported in [43, 8], computed with tradi- tional numerical methods. The first and third rows of Figure 8 represent the network output after being well trained. The second and fourth rows represent the plot of n partitions based on the following projection,\n```\n\u03a6(x) = m, m = arg max 1<=k<=n uk(x; \u03b8), for x \u2208 \u03a9.\n```\nIn the traditional method for this optimization problem, one has to carefully handle the boundary condition by employing concave relaxation or Schr\u00f6dinger operator re- laxation and design the optimization scheme to obtain reasonable and regular results. In WANCO, the aforementioned parameters can effectively train the network for all numbers (i.e., n = 2-9) of partitions, which correspond to different dimensions of the network output. This implies the robustness of WANCO to the parameter setting."}, {"title": "4.2. Dirichlet partition with periodic boundary conditions", "content": "In this sec- tion, we consider the Dirichlet partition problem in 2-, 3-, and 4-dimensional flat tori. The periodic boundary conditions on \u03a9 = [0,1]^d are imposed by transforming the input layer as follows [33],\n```\nfin(x0) = f(x(1), x(2), ..., x(d));\n= (cos(2\u03c0x(1)), sin(2\u03c0x(1)), cos(2\u03c0x(2)), sin(2\u03c0x(2)), ..., cos(2\u03c0x(d)), sin(2\u03c0x(d)))T,\n```\nin which x(i) corresponds to the i-th entry of the input x \u2208 \u03a9.\nThe parameters for the primal networks are set as follows: \u03f5 = 0.04, Co = 2500, \u03b2 = 100000, \u03b1 = 1.0003, Na = 3, and the width is increasing from Nw = 50 to Nw = 200 as the dimension or the number of phases increases. A shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 is used to represent the adversarial network. The training is performed for N = 5000 steps with inner iteration steps Nu = N\u03bb = 2. N = 10000, 40000, 60000 points are used for 2-, 3-, and 4-dimensional cases in each iteration, respectively. The sampling points used are fixed as the Hammersley sequence in the corresponding dimension, similar to that mentioned in [47].\nTwo-dimensional flat torus: In Figures 9 and 10, we display the results of n-partition in a 2-dimensional flat torus trained by the proposed WANCO for n = 3 \u2013 9, 11, 12, 15, 16, 18, 20, 23, and 24. Again, the results in Figure 10 are pro- jections using (4.5) based on the trained results in Figure 9. The testing results are obtained by evaluating the trained neural network on a uniform 1000 \u00d7 1000 grid. To our knowledge, even in the 2-dimensional case, there are not too many efficient numerical methods that can compute the n-partition problems for a large n (e.g., n = 24). Existing methods [13, 15] usually require thousands of steps to converge for the problem with a large number of partitions, even in the 2-dimensional case. In all of the previous studies, hexagons are ubiquitous for large values of n. Figures 9 and 10 show that WANCO obtains consistent results for a large range of n. Since the domain has an aspect ratio equal to one, regular hexagons cannot be used to tile the domain, so the hexagons are slightly distorted.\nThree-dimensional flat torus: As for the 3-dimensional flat torus, we consider the situations for n = 4 and 8. The results are based on the evaluation of the trained network on a uniform 100^3 grid. For n = 4, we obtain a partition of the cube consisting of four identical rhombic dodecahedron structures, as displayed in Figure 11. For n = 8, we obtain a partition of the cube that is similar to the Weaire-Phelan structure. Figure 12 shows different views of the first and second type Weaire-Phelan structures, together with different views of a periodic extension of the partition.\nFour-dimensional flat torus: We then apply the proposed WANCO to 4- dimensional partitioning problems. In Figures 13, 14 and 15, the four columns cor- respond to slices perpendicular to the x1-, x2-, x3-, and x4-axes, respectively. The four rows correspond to the slices at xj = 0, 0.25, 0.5, 0.75, respectively. These par- titions are based on testing the trained neural network on a uniform 100^3 grid at corresponding 3-dimensional slices.\nFor n = 2, we obtain a constant extension of the structure along the fourth direction, similar to the Schwarz P-surface, as displayed in Figure 13. Traditional methods usually result in slab partitions for 2 partitions in 3-dimensional flat torus with a random initialization of the partition. However, based on the evidence from the numerical study in [44], the Schwarz P-surface also appears to be a local minimizer for the 3-dimensional case. In this 4-dimensional flat torus, the random initialization represented by the neural network can find different local minimizers or stationary solutions more easily.\nFor n = 4, we obtain a partition structure similar to a constant extension along the fourth direction of the rhombic dodecahedron as displayed in Figure 14, which is a 24-cell honeycomb, which is a tessellation by regular 24-cells."}, {"title": "5. Fluid-solid optimization", "content": "Topology optimization is a technique that aims to find the optimal layout or distribution of materials to maximize the objective within a specified design space while satisfying a set of constraints. Typically, these con- straints are usually described by coupled partial differential equations. Such problems can be formulated as interface related optimization problems with PDE constraints. In the follows, we consider the application of WANCO to solve the fluid-solid opti- mization problem as an example.\nTo be specific, following [21, 11"}]}]}