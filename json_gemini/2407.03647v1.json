{"title": "WANCO: WEAK ADVERSARIAL NETWORKS FOR CONSTRAINED OPTIMIZATION PROBLEMS", "authors": ["GANG BAO", "DONG WANG", "BOYI ZOU"], "abstract": "This paper focuses on integrating the networks and adversarial training into constrained optimization problems to develop a framework algorithm for constrained optimization problems. For such problems, we first transform them into minimax problems using the augmented Lagrangian method and then use two (or several) deep neural networks(DNNs) to represent the primal and dual variables respectively. The parameters in the neural networks are then trained by an adversarial process. The proposed architecture is relatively insensitive to the scale of values of different constraints when compared to penalty based deep learning methods. Through this type of training, the constraints are imposed better based on the augmented Lagrangian multipliers. Extensive examples for optimization problems with scalar constraints, nonlinear constraints, partial differential equation constraints, and inequality constraints are considered to show the capability and robustness of the proposed method, with applications ranging from Ginzburg-Landau energy minimization problems, partition problems, fluid-solid topology optimization, to obstacle problems.", "sections": [{"title": "1. Introduction", "content": "Constrained optimization problems arise from an intricate and practical set of issues, with the objective of identifying solutions to an optimization problem while adhering to specific constraints. These problems play a crucial role in various domains such as finance, economics, engineering, operation research, machine learning, and others. For a comprehensive exploration of constrained optimization problems, we refer to [7] and the references therein. A wide range of problems in scientific computing can be classified as constrained optimization problems, for example, inverse scattering problems [4, 2], optical sciences [3], topology optimization problems [6], operation research, optimal control, and obstacle problems.\nIn this paper, we focus on constrained optimization problems related to differential operators that can appear in either the objective functional or the constraints. There are many efficient classical methods for solving constrained optimization problems [7, 22, 46], such as the penalty method, the Lagrange multiplier method, the augmented Lagrangian method, and the interior point method. When solving differential operator related problems with constraints, traditional numerical methods such as the finite difference method, the finite element method, or the spectral method can be applied with careful construction, discretization, and post-processing for satisfying the constraints.\nIn recent years, there has been rapid development in artificial intelligence (AI) for science, with particular attention being drawn to physics-informed machine learning in scientific computing. The universal approximation of continuous functions using neural networks, as proven in [17], has gained significant attention in the past decade, leading to an increasing trend of utilizing deep neural networks to solve differential operator related problems. A lot of studies have demonstrated the effectiveness of deep learning approaches in solving partial differential equations, optimization problems with differential equation constraints, or inverse problems, including physics-informed neural network (PINN) [37], deep Galerkin method (DGM) [40], deep Ritz method (DRM) [20], weak adversarial network (WAN) [51, 5], random feature method (RFM) [12], local extreme learning machines (locELM) [18], particle weak-form based neural networks (ParticleWNN) [50] and many other mechanism driven methods [27, 48, 30, 34, 49, 47, 45]. In contrast with traditional methods, deep learning methods are usually mesh-free. Therefore, they are less affected by the curse of dimensionality, making them efficient for computing high-dimensional problems. For a comprehensive overview of using DNNs to solve differential operator related problems, we refer to [31, 26, 16] and references therein.\nThe framework for using DNNs to solve differential operator constrained optimization problems involves constructing a problem-specific loss function, followed by solving an optimization problem with boundary conditions and other constraints. One common approach is to utilize penalty methods to transform the constrained optimization problem into an unconstrained optimization problem, as used in PINN, DGM, and DRM. The results are highly dependent on the selection of the weights of different terms in the loss function. In particular, one needs to carefully consider the scale of the values of constraints to guarantee that the training process can minimize each term in the loss function equally. The construction method (e.g., [33, 42]) is another way to deal with specific boundary constraints, while it is only efficient to particular constraints and computational domains.\nExcept for the penalty or construction based methods, one may consider the Lagrange method or augmented Lagrangian method(ALM) to transform the constrained optimization problem into a minimax problem. It has been widely used for solving constrained optimization problems involving neural networks and been successfully applied in various fields, such as natural language processing (NLP) [35], classification [38] and some others [29, 36, 9]. As for problems related to differential operators, neural networks are used to represent the decision variables, and then the problem is discretized, with the introduction of Lagrange multipliers on some fixed grid points. For example, in [25], ALM was used to impose equality constraints in addressing the physical conservation laws of kinetic PDE problems, which resulted in more accurate approximations of the solutions in terms of errors and the conservation laws. In [32], the applications of ALM, penalty method, and adaptive penalty method for inverse design problems were compared under the framework of physics-informed neural networks (PINNs). The results demonstrated that ALM is relatively more robust and less sensitive to hyperparameters. Enhanced physics-informed neural networks with augmented Lagrangian relaxation method (AL-PINNs) [41] treated the initial and boundary conditions as constraints for the optimization problem of the PDE residual. Various numerical examples showed that AL-PINNs yield a much smaller relative error compared to state-of-the-art adaptive loss-balancing algorithms. The augmented Lagrangian deep learning (ALDL) [24] constructed the Lagrange multipliers in the sense of adversarial network to impose the essential boundary conditions of variational problems, avoiding the discretization of the Lagrange multipliers. However, the papers mentioned above either rely on the ALM form, with the Lagrange multipliers defined and updated on a batch of fixed grid points, or only handle boundary conditions as constraints in variational problems. These methods are restricted by the selection of grid points and may be inefficient for high-dimensional problems due to the curse of dimensionality. Additionally, they are difficult to integrate with other improved frameworks, such as adaptive methods that need to adaptively sample the collocation points.\nInspired by the framework of WAN [51, 5], we propose the weak adversarial networks for constrained optimization (WANCO) to solve constrained optimization problems. WANCO has a wide range of applications and can handle various constraints, including for example, scalar constraints, linear and nonlinear constraints, PDE constraints, and inequality constraints. It is based on the augmented Lagrangian form of the constrained optimization problems by representing decision variables and Lagrange multipliers using individual networks. The problems are then trained through an adversarial process. The proposed WANCO is less sensitive to the weights of different terms in the loss function and can be easily integrated with other techniques to improve the performance of the training result. In the numerical examples, we demonstrate the capability and robustness of the parameter setting of WANCO on various constrained optimization problems, including the Ginzburg-Landau energy minimization problems, Dirichlet partition problems in different dimensions, fluid-solid optimization problems, and obstacle problems.\nThe rest of this paper is organized as follows. In Section 2, we introduce the proposed WANCO with its network architectures. In Section 3, we use the mass preserving Ginzburg-Landau energy minimization problem as an example to demonstrate the performance of WANCO. In particular, Section 3.1 presents the comparison results to other deep learning based approaches, demonstrating the insensitivity of WANCO to parameters, and Section 3.2 shows comparison results to shed light on the choice of activation functions. In Section 4, we investigate the high dimensional performance of WANCO by considering Dirichlet partition problems up to four dimensions. The PDE constrained optimization problems are considered in Section 5 for fluid-solid optimization problems as the example. In Section 6, WANCO solves the obstacle problems involving inequality constraints. We draw a conclusion and discussion in Section 7."}, {"title": "2. Framework of WANCO", "content": "To introduce the weak adversarial networks for constrained optimization problems (WANCO), we consider a general constrained optimization problem in the following form:\n\nmin L(u)\nu\ns.t. C(u) = 0,\n\nwhere u = u(x) represents the decision variable that needs to be optimized, L denotes the objective functional of u, and C(u) = 0 represents the constraint that can describe various types of constraints, such as scalar constraints, boundary constraints, PDE constraints, and even inequality constraints by introducing slack variables. This problem finds applications in diverse fields, including fluid mechanics, material science, quantum mechanics, industrial design, and topology optimization.\nFor constrained optimization problems (2.1), we first transform it into a minimax problem with the Lagrange multiplier A and a penalty term as follows,\n\nmin max L_{\u03b2}(u, \u03bb) = C_{0}L(u) \u2013 <\u03bb, C(u)> + \\frac{\u03b2}{2} ||C(u)||^{2},\n\nu\u03bb\n\nin which C_{0} and \u03b2 are scalar coefficients. C_{0} is a fixed parameter used to avoid the objective functional value being too small in specific problems. \u03b2 gradually increases during the optimization process, similar to that in the augmented Lagrangian method. In WANCO, we propose to use two(or several) individual deep neural networks (similar to WAN and IWAN) to represent u = u(x; \u03b8), and \u03bb = \u03bb(x; \u03b7) with parameters \u03b8 and \u03b7, respectively. Problem (2.2) is then transformed into a minimax problem with respect to \u03b8 and \u03b7,\n\nmin max L_{\u03b2}(u(x; \u03b8), \u03bb(x; \u03b7)) = C_{0}L(u(x; \u03b8)) \u2013 <\u03bb(x; \u03b7), C(u(x; \u03b8))> + \\frac{\u03b2}{2} ||C(u(x; \u03b8))||^{2}.\n\n\nThe parameters \u03b8 and \u03b7 are then alternatively updated by stochastic gradient descent and stochastic gradient ascent of L_{\u03b2}(u(x; \u03b8), \u03bb(x; \u03b7)), with a gradually increasing value of B. The algorithmic framework of WANCO is summarized in Algorithm 2.1, and the neural network diagram of WANCO is illustrated in Figure 1.\nIn terms of the network architecture, we represent the decision variables by the residual neural networks (ResNet). The Lagrange multiplier is represented by different networks based on their properties. For example,\n1.  if the constraint is scalar, \u03bb(x; \u03b7) can be simply set as \u03bb(0; \u03b7) with constant input 0 and a shallow neural network,\n2.  if the constraint is defined over the entire computational domain (e.g., PDE constraints), \u03bb(\u03b1; \u03b7) is set to be a function in the domain represented by a ResNet,\n3.  if the constraint is an inequality, structured neural networks can be used to enforce the non-positivity of \u03bb(\u03b1; \u03b7).\nTo be specific, we use the ResNet [23] with N_{a} hidden layers and width N_{w}, expressed as follows,\n\nu(x_{0}; \u03b8) = W_{out}x_{N_{a}+1}+b_{out},\nx_{k+1} = f_{k}(x_{k}) = \u03a6_{k}(W_{k}x_{k} + b_{k}) + x_{k}, k = N_{a}, ..., 1,\nx_{1} = W_{in}x + b_{in}.\nHere, x_{0} \u2208\u03a9 is the input of dimension d, {W_{in} \u2208 R^{N_{w}\u00d7d},W_{k} \u2208 R^{N_{w}\u00d7N_{w}}, W_{out} \u2208 R^{n\u00d7N_{w}}} are the weight matrices of the input layer, k-th hidden layer, and output layer, {b_{in} \u2208 R^{N_{w}},b_{k} \u2208 R^{N_{w}},b_{out} \u2208 R^{n}} are the corresponding bias vectors, and \u03a6_{k} represents the activation function. The parameters N_{a} and N_{w} are used to define the size of the weight matrices and bias vectors in the neural network, which ultimately determine the network's performance and capabilities from many empirical observations and the universal approximation theorem. ResNet effectively mitigates the problems of gradient vanishing and gradient explosion by incorporating a residual connection or skip connection, represented by the +x_{k} term in the second equation of (2.4). As for the activation function \u03a6_{k}, we will use tanh^{3} instead of common activation functions [39] (e.g., sigmoid, tanh, softmax, ReLU, and ReLU^{3}). A detailed numerical comparison will be provided in Section 3.\nAfter constructing the saddle point problem (2.3), the loss function is optimized iteratively by the standard optimizer Adam [28] with the stochastic gradient descent(SGD) and ascent method. When computing different terms of the loss function, we employ several standard DNN techniques, including the Monte Carlo (MC) method for integrals and auto-differentiation for derivatives. To avoid getting stuck in undesirable local minima, we incorporate shuffling of training points during the training process, as shown in [20] for example. To simplify the optimization process, we design specific networks that automatically satisfy boundary or positivity constraints. In cases where these constraints are not straightforward, they can be imposed as additional adversarial networks in the WANCO framework. The numerical results presented in Sections 3-6 demonstrate that WANCO is a versatile algorithm"}, {"title": "3. Minimization of the Ginzburg-Landau energy with mass conservation", "content": "In this section, we focus on the empirical study of the sensitivity of the parameters, the choice of activation functions, and the comparison to the penalty based methods and adversarial network with only Lagrange multipliers. We illustrate them through the minimization of the Ginzburg-Landau energy with mass conservation as an example.\nConsider the minimization of the Ginzburg-Landau energy with mass conservation and homogeneous Dirichlet boundary condition in \u03a9 = [0,1] \u00d7 [0, 1] as follows,\nmin \\int_{\u03a9} \\frac{\u03b5}{2} |\u2207u|^{2} + \\frac{1}{4} (u^{2} - 1)^{2} dx\nus.t. \\int_{\u03a9} u dx = V, and u = -1 on \u2202\u03a9,\nwhere u : (x,y) \u2192 R is the phase order parameter, and V is a constant used to enforce the integral of u, which is equivalent to the total mass in a two-phase system if u represents rescaled density.\nIn this case, we use ResNet to represent u. Regarding the Lagrange multiplier, we represent A using a shallow neural network with one hidden layer, which includes a small number of neurons with the activation function \u03a6. We set the input to be 0, and thus \u03bb(x; y) can be viewed as a scalar variable \u03bb(0; \u03b7).\nAs for the boundary conditions of the primal network u(x; 0), we enforce them explicitly at the output layer. Specifically, we impose the following boundary conditions at the output layer:\n\nu(x_{0}; \u03b8) = (W_{out}f_{N_{a}}(x_{N_{a}}) + b_{out}) \u00d7 (1-x_{1})) \u00d7 x_{1} \u00d7 x_{2} \u00d7 (1-x_{2}) \u2013 1,\nin which x_{1} and x_{2} correspond to two coordinates of the input x_{0} and u(x_{0}; \u03b8) = -1 for x_{0} \u2208 \u2202\u03a9. This approach is suitable for relatively regular geometries, such as the unit square we considered in this example. For more general boundary conditions,\nwe can treat them as constraints and use the framework of WANCO to convert them into another adversarial network, as demonstrated in Section 5."}, {"title": "3.1. Sensitivity to parameters and comparison to other deep learning based approaches", "content": "For problem (3.1), we obtain the following minimax problem:\n\nmin max L^{GL}(\u03b8, \u03b7)\n\u03b8,\u03b7\nwhere\nL^{GL}(\u03b8, \u03b7) = C_{0} \\int_{\u03a9} [\\frac{\u03b5}{2}|\u2207u(x;\u03b8)|^{2} + \\frac{1}{4} (u(x;\u03b8)^{2} - 1)^{2}] dx\n- \u03bb(0;\u03b7) (\\int_{\u03a9} u(x;\u03b8)dx - V) + \\frac{\u03b2}{2} (\\int_{\u03a9} u(x;\u03b8)dx - V)^{2}.\n\nWe then solve this problem with the proposed WANCO in contrast to:\n1.  DRM combined with penalty method (DRM-P),\n2.  DRM combined with adaptive penalty method(DRM-AP), and\n3.  DRM combined with Lagrange multipliers.\nFrom the classical analysis of problem (3.1), the zero level set of the solution should be a circle with the enclosed area conserved. In this example, we set \u03b5 = 0.05, V = -0.5, C_{0} = 400, and \u03b1 = 1.0003. For the primal network u(x; \u03b8), the number of hidden layers is N_{a} = 4 and the width is N_{w} = 50. To represent the adversarial network, we employ a shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 since the Lagrange multiplier A is a scalar in this case. The training step is set to N = 5000 with inner iteration steps N_{u} = N_{\u03bb} = 2. In each iteration, N = 40,000 points are used for training. The initial learning rate is \u03c4_{\u03b8} = \u03c4_{\u03b7} = 0.016 and halved at certain training milestones. This learning rate setting is consistent throughout the subsequent numerical examples, and will not be reiterated later. The activation function used here is tanh^{3}(x). Both DRM-P and DRM-AP have the same structure as the primal network mentioned above.\nIn Figure 2, we display the results from WANCO, DRM-P, and DRM-AP for different choices of \u03b2 = 100000, 10000, 1000, together with the convergence of the constraint error, defined as \\frac{V-\\int_{\u03a9}udx}{V}, during the training process in Figure 3. It is worth noting that WANCO employs identical parameters across different \u03b2 values in these results. In contrast, both DRM-P and DRM-AP require individual parameter tuning for each case to obtain satisfactory results (which may still completely fail due to the poor selection of \u03b2). This implies that the proposed WANCO is relatively insensitive to parameter settings. As shown in Figure 2, under three different initial penalty terms \u03b2, the proposed WANCO consistently outperforms DRM-P and DRM-AP. For the first case with \u03b2 = 100000, the profiles of WANCO, DRM-AP, and DRM-P are close to circles, but WANCO and DRM-AP have a clear advantage in the relative error of the constraint. As for the second and third cases with \u03b2 = 10000, 1000, DRM-P fails, and DRM-AP exhibits relatively inaccurate results (the computed results are not circular) due to the unbalanced terms in the loss, while WANCO trains well. This indicates that relying solely on the adaptive penalty method is still sensitive to parameter selection, and comparable results are achieved only when \u03b2 = 100000.\nIn all cases, WANCO can achieve a relative error of the constraint less than 0.005. As shown in Figure 3, compared to DRM-AP, WANCO can significantly reduce the training steps due to the adversarial term in (3.3). To be more specific, due to the Dirichlet boundary condition and the random initialization of network parameters,\nthe output of the primal network in the initial steps closely approximates -1. Consequently, the initial integral of u is also close to -1, falling below the constraint of -0.5. With the inclusion of the adversarial term A, during the early stages of training, the mass of u increases at a faster rate compared to DRM-AP. As the adversarial term A reaches a sufficiently large value, it becomes dominant, causing the minimization step to shift the mass in the positive direction. This corresponds to the initial steps as shown in Figure 3. Once the error of the constraint becomes positive, A is reduced. Subsequently, when the third term is dominant again, the constraint error decreases rapidly. This corresponds to the stage of rapid descent in Figure 3, which is more evident in the last two pictures. To conclude, the inclusion of the adversarial term A allows for an adaptive adjustment of its magnitude, enabling the constraint to be rapidly satisfied from both sides. This is in contrast to DRM-AP, where the mass can only decrease rapidly as the penalty term reaches a threshold value. This further demonstrates that WANCO is relatively insensitive to the setting of hyperparameters, as the adversarial term exhibits a notable adaptive weighting and acceleration effect. Moreover, from the training perspective, in WANCO, half of the training steps are on the optimization of the adversarial network, which is usually constructed by a smaller network. This usually results in faster training compared to other methods.\nWe also consider the Lagrange multiplier formula without penalty terms, and empirical study shows that it is very difficult to achieve convergence in the adversarial process. As illustrated in Figure 4, using only the Lagrange multiplier method combined with adversarial networks leads to failure. We observe that the Lagrange multiplier only has a corresponding ascent direction during training, and its effectiveness in imposing the constraint is affected by the step size (learning rate). Therefore, relying solely on the Lagrange multiplier may result in convergence difficulties. However, with the augmented Lagrangian method, we observe that the adversarial term adjusts its scale of values based on the current level of constraint satisfaction to accelerate the process of satisfying the constraint, while the penalty term gradually increases to guarantee convergence. As a result, the adversarial process is effectively similar to assigning different weights to the training data, enabling WANCO to impose the constraints more effectively."}, {"title": "3.2. Choice of activation functions", "content": "To illustrate the observations and reasons for the utilization of tanh^{3} (see the profile in Figure 5) in our examples, we present two numerical simulations to validate the advantages of combining ResNet with the activation function tanh^{3} in the proposed WANCO.\nFor the above problem (3.3), we consider: 1. replacing the activation function directly with other activation functions in the model that has been well-tuned for tanh^{3}, and 2. replacing the activation function with tanh^{3} in the model that has been well-tuned for other common activation functions.\nAs shown in Figure 6, when replacing the activation function tanh^{3} in the well-tuned neural network with other activation functions, the training results are often poor or even incorrect. However, as shown in Figure 7, when initially utilizing a well-tuned neural network with a different activation function and subsequently changing it to tanh^{3}, we observe that similar or even better results can be achieved.\nBased on the observations from these two examples, we believe that combining ResNet with tanh^{3} exhibits favorable properties and can reduce parameter tuning time for WANCO. This choice is also used and verified in examples for the subsequent problems."}, {"title": "4. Optimal partition problems", "content": "In this section, we consider the optimal partition problem, which minimizes the Dirichlet energy with L^{2} norm preserving in different dimensions and different boundary conditions [10, 19] as follows.\nmin \\int_{\u03a9} \\frac{\u03b5}{2} \u2211_{i=1}^{n} |\u2207u_{i}|^{2} + \u2211_{i=1,j=1,i\u2260j}^{n} f(u_{i}) dx\nus.t. \\int_{\u03a9} u_{i}^{2} dx = 1, i = 1, 2, ..., n,\nand u satisfies the homogeneous Dirichlet\nor periodic boundary conditions,\nwhere \u03a9 = [0, 1]^{d}, d denotes the dimension, n represents the number of phases, u = [u_{1}, u_{2}, ..., u_{n}]' denotes the phase variables, with each u_{i} mapping x to R, \u03b5 is the model parameter related to the thickness of the transition layer, and f takes the form f(u) = \u2211_{i,j=1,i\u2260j}^{n} u_{i}^{2}u_{j}^{2}.\nFollowing the augmented Lagrangian method, one can obtain the minimax problem:\nmin max C_{0} \\int_{\u03a9}\u2211_{i=1}^{n} |\u2207u_{i}|^{2} + \u2211_{i=1,j=1,i\u2260j}^{n} u_{i}^{2}u_{j}^{2} dx\nu\u03bb\n- \u03bb\\cdot (\\int_{\u03a9} u^{2}dx - 1) + \\frac{\u03b2}{2} ||\\int_{\u03a9} u^{2}dx - 1||^{2},\n\nwhere A denotes n Lagrange multipliers for n constraints in (4.1), and 1 is the vector in R^{n} whose component are all ones.\nThen we construct the primal and adversarial networks to represent u = u(x; \u03b8) and A \u03bb(x; \u03b7), respectively. Since the L^{2} norm preserving constraint is independent of x, we construct the adversarial network as A(0; \u03b7) = [\u03bb_{1} (0; \u03b7), \u03bb_{2} (0; \u03b7), ..., \u03bb_{n} (0; \u03b7)]' using a shallow neural network. Consequently, the problem (4.2) is transformed into the following form:\n\nmin max L_{\u03b2}(u(x; \u03b8), \u03bb(0; \u03b7))\n\u03b8 \u03b7\n= C_{0} \\int_{\u03a9} \u2211_{i=1}^{n} |\u2207u_{i}(x; \u03b8)|^{2} + \u2211_{i=1,j=1,i\u2260j}^{n} u_{i}^{2}(x; \u03b8)u_{j}^{2}(x; \u03b8) dx\n- \u03bb(0; \u03b7)\\cdot (\\int_{\u03a9} u^{2}(x; \u03b8)dx - 1) + \\frac{\u03b2}{2} ||\\int_{\u03a9} u^{2}(x; \u03b8)dx - 1||^{2}.\nIn the follows, we consider the results with the Dirichlet and periodic boundary conditions separately. The parameter settings and training results for the Dirichlet and periodic boundary conditions are given separately. These results in 2- and 3-dimensional space are consistent with the results presented in [43, 8, 44], indicating the effectiveness and robustness of the proposed method. In addition, the 4-dimensional partition results are displayed to demonstrate the efficiency of the proposed WANCO."}, {"title": "4.1. Dirichlet partition with Dirichlet boundary conditions", "content": "In this section, we consider the Dirichlet partition problem with Dirichlet boundary conditions in the unit square \u03a9 = [0,1] \u00d7 [0,1] with the L^{2} norm preserved. The Dirichlet boundary condition is imposed as a strong form at the output layer similar to the first numerical example,\n\nu(x_{0}; \u03b8) = ReLU(W_{out}f_{N_{a}}(x_{N_{a}}) + b_{out}) \u00d7 \u220f_{i=1}^{d} [x_{i} \u00d7 (1-x_{i})],\nin which x_{i} corresponds to the i-th entry of the input x_{0} \u2208 \u03a9 and ReLU here is used to enforce the non-negativity of u.\nThe parameters for the primal networks are set as follows: \u03f5 = 0.05, C_{0} = 100, \u03b2 = 10000, \u03b1 = 1.0003, N_{a} = 8, and N_{w} = 120. A shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 is used to represent the adversarial network. The training is performed for N = 20000 steps with inner iteration steps N_{u} = N_{\u03bb} = 1, and N = 10000 points are used for training in each iteration. The testing results are obtained by evaluating the trained neural network on a uniform 1000 \u00d7 1000 grid.\nIn Figure 8, we display the optimal n-partitions with Dirichlet boundary conditions, which are consistent with the results reported in [43, 8], computed with traditional numerical methods. The first and third rows of Figure 8 represent the network output after being well trained. The second and fourth rows represent the plot of n partitions based on the following projection,\n\u03a6(x) = m, m = arg max u_{k} (x; \u03b8), for x \u2208 \u03a9.\n1\u2264k\u2264n\nIn the traditional method for this optimization problem, one has to carefully handle the boundary condition by employing concave relaxation or Schr\u00f6dinger operator relaxation and design the optimization scheme to obtain reasonable and regular results. In WANCO, the aforementioned parameters can effectively train the network for all numbers (i.e., n = 2-9) of partitions, which correspond to different dimensions of the network output. This implies the robustness of WANCO to the parameter setting."}, {"title": "4.2. Dirichlet partition with periodic boundary conditions", "content": "In this section, we consider the Dirichlet partition problem in 2-, 3-, and 4-dimensional flat tori. The periodic boundary conditions on \u03a9 = [0,1]^{d} are imposed by transforming the input layer as follows [33],\nf_{in} (x_{0}) = f((x_{1}), x_{2}, ..., x_{d});\n= (cos(2\u03c0x_{1}), sin(2\u03c0x_{1}), cos(2x_{2}), sin(2x_{2}), ..., cos(2x_{d}), sin(2\u03c0x_{d}))',\nin which x_{i} corresponds to the i-th entry of the input x \u2208 \u03a9.\nThe parameters for the primal networks are set as follows: \u03f5 = 0.04, C_{0} = 2500, \u03b2 = 100000, \u03b1 = 1.0003, N_{a} = 3, and the width is increasing from N_{w} = 50 to N_{w} = 200 as the dimension or the number of phases increases. A shallow neural network \u03bb(0; \u03b7) with 1 hidden layer and width 10 is used to represent the adversarial network. The training is performed for N = 5000 steps with inner iteration steps N_{u} = N_{\u03bb} = 2. N = 10000, 40000, 60000 points are used for 2-, 3-, and 4-dimensional cases in each iteration, respectively. The sampling points used are fixed as the Hammersley sequence in the corresponding dimension, similar to that mentioned in [47].\nTwo-dimensional flat torus: In Figures 9 and 10, we display the results of n-partition in a 2-dimensional flat torus trained by the proposed WANCO for n = 3 \u2013 9, 11, 12, 15, 16, 18, 20, 23, and 24. Again, the results in Figure 10 are projections using (4.5) based on the trained results in Figure 9. The testing results are obtained by evaluating the trained neural network on a uniform 1000 \u00d7 1000 grid. To our knowledge, even in the 2-dimensional case, there are not too many efficient numerical methods that can compute the n-partition problems for a large n (e.g., n = 24). Existing methods [13, 15] usually require thousands of steps to converge for the problem with a large number of partitions, even in the 2-dimensional case. In all of the previous studies, hexagons are ubiquitous for large values of n. Figures 9 and 10 show that WANCO obtains consistent results for a large range of n. Since the domain has an aspect ratio equal to one, regular hexagons cannot be used to tile the domain, so the hexagons are slightly distorted.\nThree-dimensional flat torus: As for the 3-dimensional flat torus, we consider the situations for n = 4 and 8. The results are based on the evaluation of the trained network on a uniform 100^{3} grid. For n = 4, we obtain a partition of the cube consisting of four identical rhombic dodecahedron structures, as displayed in Figure 11. For n = 8, we obtain a partition of the cube that is similar to the Weaire-Phelan structure. Figure 12 shows different views of the first and second type Weaire-Phelan structures, together with different views of a periodic extension of the partition.\nFour-dimensional flat torus: We then apply the proposed WANCO to 4-dimensional partitioning problems. In Figures 13, 14 and 15, the four columns correspond to slices perpendicular to the x_{1}-, x_{2}, x_{3}-, and x_{4}-axes, respectively. The four rows correspond to the slices at x_{j} = 0, 0.25, 0.5, 0.75, respectively. These partitions are based on testing the trained neural network on a uniform 100^{3} grid at corresponding 3-dimensional slices.\nFor n = 2, we obtain a constant extension of the structure along the fourth direction, similar to the Schwarz P-surface, as displayed in Figure 13. Traditional methods usually result in slab partitions for 2 partitions in 3-dimensional flat torus with a random initialization of the partition. However, based on the evidence from the numerical study in [44], the Schwarz P-surface also appears to be a local minimizer for the 3-dimensional case. In this 4-dimensional flat torus, the random initialization represented by the neural network can find different local minimizers or stationary solutions more easily.\nFor n = 4, we obtain a partition structure similar to a constant extension along the fourth direction of the rhombic dodecahedron as displayed in Figure 14, which is a 24-cell honeycomb, which is a tessellation by regular 24-cells."}, {"title": "5. Fluid-solid optimization", "content": "Topology optimization is a technique that aims to find the optimal layout or distribution of materials to maximize the objective within a specified design space while satisfying a set of constraints. Typically", "11": "we consider an optimization model to find the optimal fluid-solid channel inside a designated region with given inflow and outflow boundary conditions", "problem": "nmin C_{a}J_{a}(\u03d5, u) + C_{\u03f5}J_{\u03f5}(\u03d5)\nu,\u03d5\ns.t. {\u2207 \u00b7 u = 0, x \u2208 D,\n\u222b"}]}