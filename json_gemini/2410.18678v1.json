{"title": "Ali-AUG: Innovative Approaches to Labeled Data Augmentation using One-Step Diffusion Model", "authors": ["Ali Hamza", "Aizea Lojo", "Adrian N\u00fa\u00f1ez-Marcos", "Aitziber Atutxa"], "abstract": "This paper introduces Ali-AUG, a novel single-step diffusion model for efficient labeled data augmentation in industrial applications. Our method addresses the challenge of limited labeled data by generating synthetic, labeled images with precise feature insertion. Ali-AUG utilizes a stable diffusion architecture enhanced with skip connections and LoRA modules to efficiently integrate masks and images, ensuring accurate feature placement without affecting unrelated image content. Experimental validation across various industrial datasets demonstrates Ali-AUG's superiority in generating high-quality, defect-enhanced images while maintaining rapid single-step inference. By offering precise control over feature insertion and minimizing required training steps, our technique significantly enhances data augmentation capabilities, providing a powerful tool for improving the performance of deep learning models in scenarios with limited labeled data. Ali-AUG is especially useful for use cases like defective product image generation to train AI-based models to improve their ability to detect defects in manufacturing processes. Using different data preparation strategies, including Classification Accuracy Score (CAS) and Naive Augmentation Score (NAS), we show that Ali-AUG improves model performance by 31% compared to other augmentation methods and by 45% compared to models without data augmentation. Notably, Ali-AUG reduces training time by 32% and supports both paired and unpaired datasets, enhancing flexibility in data preparation.", "sections": [{"title": "1. Introduction", "content": "In industrial applications, cameras, sensors, and other monitoring devices are prevalent for product quality control and inspection. Typically, the data from these cameras and sensors are used to train traditional AI models such as decision trees. However, most of these AI models rely on supervised algorithms, which require human-annotated instances, where each instance is assigned a specific label or class to be learned. The automatic identification of defective products is a common example, where the AI model training dataset must include enough labeled instances categorized as either 'good' or 'defective'. The generation of these datasets presents two significant challenges in the industrial context. First, the scarcity of defective instances results in models struggling to detect defective products accurately. This issue is critical across various fields, such as fault detection in industrial components, early disease identification in medical imaging, and structural health monitoring of infrastructure. Second, personnel and time constraints, especially in small or mid-size enterprises. Generating the necessary datasets presents significant obstacles beyond the scarcity of defective data. The labeling process is labor-intensive and time-consuming, further complicating the task.\nIn recent years, the AI field has shifted from traditional machine learning models to deep learning approaches, which have shown exceptional performance in computer vision (CV) tasks such as image classification, object detection, image segmentation, and medical image categorization. However, despite their success, deep learning models require bigger data amounts than traditional AI models to achieve high accuracy, making them heavily dependent on large, well-labeled datasets. These models effectively learn different features of an image by applying convolution operations, with initial layers focusing on low-level features (e.g., edges, lines) and deeper layers capturing more complex and structured features. Nonetheless, their performance decreases drastically in low-data regimes and often suffers from overfitting, where the model performs well on training data but poorly on unseen test data. This problem is exacerbated, particularly in industry or clinical domains, by the scarcity of large, labeled datasets due to privacy concerns and the time-consuming nature of human labeling tasks [1, 2, 3].\nTo address this issue, the generation of synthetic datasets has recently gained popularity. Models such as Generative Adversarial Networks (GANs) [4, 5] and autoencoders [6, 7] are capable of generating realistic images; however, they still rely on extensive images to generate more [8].\nThis work proposes a novel system for synthetically generating labeled datasets, with the aim of producing images conditioned on specific constraints, such as the presence of a particular defect. Conditioned generation allows us to have better control of the generation process. There are various methods to condition generated images. Initially, Generative Adversarial Networks (GANs) were used for this purpose, but the results were often inaccurate. Later, diffusion models [9] were developed offering improved conditioning capabilities. For instance, we can condition a model by providing it with a textual prompt and/or a mask. A mask is a binary or continuous map that specifies which parts of the image should be altered or remain unchanged, which ensures that the generated image adheres precisely to the shape and details specified by this mask. Although some diffusion models like [10, 11, 12, 13, 14, 15] are capable of conditioning images effectively using masks, they still face limitations related to the extensive training requirements and slow inference times because they incorporate"}, {"title": "1.1. Paired Dataset Example", "content": "In the paired dataset example shown in Table 3, each pair contains the image of an object and its defective version. This allows for direct comparison and assessment of the model's ability to introduce defects while maintaining the integrity of the rest of the image content. The mask guides the specific location of modifications, ensuring that only the designated areas are altered, and the prompt controls the modification type. For instance, a prompt like \"add scratch\" or \"add hole\u201d will introduce these"}, {"title": "1.2. Unpaired Dataset Example", "content": "In the unpaired dataset example shown in Table 4, the input image is a defect-free image that has similar visual characteristics (i.e., form, color, texture), but it is not the same object. This scenario is more common in real-world applications where paired data might not be readily available. By using unpaired data, our approach demonstrates flexibility and robustness in generating defects that adhere to the domain characteristics without relying on identical image pairs."}, {"title": "3. Method", "content": "This section introduces our novel approach for generating synthetic, labeled images using a single-step diffusion model, referred to as Ali-AUG (see Figure 3). Our model is based on Parmar's model [17] (see Figure 2). However, while Parmar's approach focuses on generating an image from a sketch or canny image, our approach is designed to insert specific features into designated regions of an image given an input image. For example, it can add a defect to an object. This method utilizes the provided mask and is guided by a prompt to ensure the desired features are incorporated into specific areas of the input image without affecting the rest of the content, making it particularly useful for industrial applications."}, {"title": "3.1. Ali-AUG for Text-to-Image diffusion.", "content": "We used Stable Diffusion as an example to demonstrate how we could edit images with a large pre-trained diffusion model. Stable Diffusion is composed of three main parts: an encoder, a U-Net [46], and a decoder. The U-Net contains an encoder, a middle block, and a skip-connected decoder. Both the encoder and decoder consist of multiple blocks, with the encoder handling downsampling and the decoder performing upsampling. The model architecture also includes Vision Transformers (ViTs) that provide attention-based processing. In this framework, the text prompts (T) are encoded using the CLIP text encoder [47], and the diffusion timesteps are encoded through a time encoder utilizing positional encoding [16]. The ViTs incorporate cross-attention mechanisms, allowing finer control of the prompt by modifying the internal attention maps of the diffusion model during inference alongside the self-attention mechanisms.\nAli-AUG does not add any overhead to the current model; in fact, it only adds LORA adapters in the encoder, decoder, and U-Net to make it efficient and possible to train even on graphics cards with 24 GB of VRAM. Additionally, skip connections have been added to preserve the input image information, which will be explained in detail below. Stable Diffusion uses a preprocessing method to convert 512 \u00d7 512 pixel-space images into smaller 64 \u00d7 64 latent images. In Ali-AUG, we first convert both the input image and the conditioning image (Mask) from 512 \u00d7 512 into a 64 \u00d7 64 feature space vector [35] to match the size of Stable Diffusion using the same encoder."}, {"title": "3.2. Feature Extraction", "content": "To achieve precise image synthesis, we utilize both the input image and the mask in our encoding process. The image and mask are encoded using a Stable Diffusion architecture enhanced with skip connections and LoRA modules. The encoding process involves several steps:\nImage and Mask Encoding: Both the input image (I) and the mask (M) are encoded using the same encoder to extract relevant features. The encoder compresses the input image spatially while increasing the channel count (512\u00d7512 \u00d7 3 \u2192 64\u00d764\u00d7 4), enabling efficient processing. This step ensures that the essential characteristics of both the image and mask are captured, setting the stage for effective integration in subsequent stages.\n$F_I = E(I), F_M = E(M)$\nwhere E is the encoder that extracts features $F_I$ from the input image and $F_M$ from the mask.\nSkip Connections: To preserve fine details from the input image, intermediate activations are extracted following each downsampling block within the encoder. These activations are added to the corresponding upsampling block of the decoder via a 1x1 zero convolution layer. This method ensures that fine details are retained throughout the image generation process.\n$F = F_M + ZeroConv(F_I)$\nLORA Modules: LoRA modules are employed to integrate mask-conditioned features into the targeted image area. These modules adapt the network weights to the new conditioning, reducing overfitting and fine-tuning time. This method also minimizes the number of parameters that need to be trained, enhancing the efficiency and transferability of the model. LoRA modules are added in the encoder, decoder, and U-Net to ensure effective integration of mask information (see Figure 3)."}, {"title": "3.3. Preserving Input Details", "content": "A key challenge in multi-object and complex scene synthesis is the preservation of fine details. To address this, our approach involves compressing both the mask and the input images using the same encoder. This encoder spatially compresses the images and extracts relevant features, which may otherwise lead to a loss of intricate details. To mitigate this loss, we employ skip connections between the encoder and decoder. Specifically, intermediate activations are extracted from each downsampling block within the encoder during the encoding of the input image. These activations are then processed via a 1x1 zero convolution layer [10] to ensure they retain essential details. These processed activations are subsequently fed into the corresponding upsampling blocks in the decoder. This method ensures that fine-grained details are preserved throughout the image generation process, significantly improving the quality and fidelity of the output [17].\nDropping Input: Dropout is applied to the input image (<30%) during training, allowing the model to learn to generate images using only the mask and the prompt"}, {"title": "3.4. Training", "content": "The training process for Ali-AUG involves several key components to ensure high-quality image synthesis while maintaining efficiency. We utilized various hyperparameters, detailed in Table A.17, to fine-tune the model and ensure optimal performance.\nBasic Augmentation: During training, we applied basic data augmentation techniques such as flipping, rotation, and color jittering to enhance the robustness of our model.\nLoss Functions: To train our model, we utilize a combination of adversarial loss, reconstruction loss, and LPIPS (Learned Perceptual Image Patch Similarity) loss. The total loss $L_{total}$ is given by:\n$L_{total} = \\lambda_{adv}L_{adv} + \\lambda_{rec}L_{rec} + \\lambda_{lpips} L_{lpips}$\nThe generator G aims to minimize this total loss:\n$G^* = arg\\underset{G}{min} L_{total}$\nAdversarial Loss: Ensures that the generated images are realistic. The adversarial loss is defined as:\n$L_{adv} = E_{I,M,T} [log D(I)] + E_{I',M,T} [log(1 \u2013 D(G(I, M,T)))]$\nwhere D is the discriminator network.\nReconstruction Loss: Ensures that the generated image I' closely matches the desired output based on the input image I and mask M:\n$L_{rec} = E_{I,M,T} [||I' - I_{target}||^2]$\nLPIPS Loss: Measures the perceptual similarity between the generated image and the target image:\n$L_{lpips} = E_{I,M,T} [LPIPS(I', I_{target})]$\nWith this setup, we can generate larger datasets using synthetic images, where the provided mask acts as the label, eliminating the need for manual re-labeling. By integrating masks as labels and leveraging advanced architectural elements like zero-convolution layers, our method efficiently generates high-quality synthetic images with precise feature placement. This efficiency allows us to quickly produce large amounts of labeled data, which is crucial for training compact models. For example, we can train lightweight object detection systems like YOLO (You Only Look Once) [48], a state-of-the-art real-time object detection system. These compact models are well-suited for deployment on devices with limited computational capacity, extending the applicability of our approach to resource-constrained industrial environments."}, {"title": "4. Experiments", "content": "We conducted experiments to evaluate the effectiveness of our proposed method and compare it with existing models using the Frechet Inception Distance (FID) metric [49], following the clean-FID's implementation [50]. Additionally, we used two different data preparation strategies, Classification Accuracy Score (CAS) and Naive Augmentation Score (NAS), to further assess the models' performance as mentioned by Ravuri and Vinyals [51]. These strategies are explained in detail in Section 4.2.\nWe used multiple datasets for our experiments: a private industrial dataset of solar panel defects and Wood, Tile, Screw, and Leather from the MVTec AD [52] dataset. These datasets are widely used for defect detection in industrial settings and include images of wood, tiles, screws, and leather surfaces, each with various defects closely resembling those used in real-world industrial applications."}, {"title": "4.1. Metrics", "content": ""}, {"title": "4.1.1. Frechet Inception Distance (FID)", "content": "The FID score measures the distance between the feature distributions of real and generated images, with lower scores indicating better performance."}, {"title": "4.2. Data Preparation Strategies", "content": ""}, {"title": "4.2.1. Classification Accuracy Score (CAS)", "content": "For CAS, all training images are synthetic and generated using our method. The validation is performed on the validation dataset consisting of real images. This strategy helps assess the ability of models trained exclusively on synthetic data to generalize to real-world data."}, {"title": "4.2.2. Naive Augmentation Score (NAS)", "content": "For NAS, the training dataset is a mixture of synthetic and real images, and the validation is performed on real images. This strategy assesses the benefit of combining synthetic and real data during training to improve the model's performance on real-world data."}, {"title": "4.3. Datasets", "content": "We conducted our experiments using both a private industrial dataset and public datasets to demonstrate the model's versatility and reproducibility."}, {"title": "4.3.1. Private Industrial Dataset", "content": "Initially, we conducted experiments on a private solar panel defects dataset, which cannot be publicly released. Due to the limited availability of labeled data specifically, we only had 19 to 50 manually labeled images\u2014we leveraged our model to generate additional synthetic images to supplement the dataset. We created both paired and unpaired datasets for fine-tuning our model:"}, {"title": "4.3.2. Private Industrial Paired Training", "content": "In our private industrial dataset, we have a total of 900 images, of which only 32 are defective. Using traditional models such as YOLO and autoencoders with conventional data augmentation techniques, the models often resulted in a high number of false positives. To address this, we utilized Ali-AUG to generate the CAS and NAS datasets to validate our model's robustness. This approach achieved a FID score of 5.5, as shown in Table 8. Subsequently, we used a YOLO model on these datasets for defect detection, demonstrating improved results compared to traditional data augmentation methods. The detailed results are presented in Table 7."}, {"title": "4.3.4. Public Dataset", "content": "To enable other researchers to validate our approach and methodology, we conducted extensive experiments on public datasets, including Wood, Tile, Screw, and Leather from the MVTec AD dataset [52]. While our implementation is proprietary, we provide comprehensive methodological details and experimental results that should allow researchers to understand and build upon our approach. We followed a similar process of generating synthetic images and creating paired and unpaired datasets for these experiments. The results from these public datasets are presented in Table 8, demonstrating the model's performance and validating our methodology through standard benchmarks and metrics.\nThrough these experiments, we highlight the model's capability to generate high-quality synthetic images, perform efficient data augmentation, and maintain fast training times. The validation results using CAS and NAS metrics further confirm the effectiveness of our approach, providing a robust framework for data augmentation in various industrial applications."}, {"title": "4.3.5. Public Dataset Paired Training", "content": "For paired datasets, the model is fine-tuned to meticulously follow the pattern and details of the original image, embedding the desired feature into the marked area. This setting ensures high fidelity and precise feature insertion. When it is not possible to obtain the dataset with an image before and after modification given a condition"}, {"title": "4.3.6. Public Dataset Unpaired Training", "content": "In scenarios where paired data is unavailable and challenging to create, our model leverages unpaired training techniques. Since we have defect-labeled images, we use these labels as masks and the defective image as the target. For the input, a defect-free image is selected randomly. This approach ensures that the model continues to train quickly while maintaining high-quality results (see dataset example in Table 2). Additional synthetically generated images can be found in Table A.12.\nOnce the dataset was generated and the model trained, we created the CAS and NAS datasets. We then followed the same procedure as with the private solar panel defects dataset, training YOLOv8-seg on these datasets for segmentation instead of object detection. This approach aimed to demonstrate improvements across different scenarios. The results can be seen in Tables 10 and 11."}, {"title": "5. Discussion", "content": ""}, {"title": "5.1. Conclusions", "content": "This study introduces Ali-AUG, an innovative single-step diffusion model that significantly enhances data augmentation capabilities creating labeled images, particularly in industrial applications where labeled data is scarce or expensive. The model demonstrates remarkable efficiency, enabling rapid learning with minimal examples and reducing computational demands through its use of LoRA modules and skip connections. Its versatility across both paired and unpaired training settings, coupled with its ability to generate high-quality images with precise feature insertion, provides a powerful tool for improving the performance of deep learning models in various industrial scenarios. Experimental validation across multiple datasets confirms the model's superiority in generating realistic, defect-enhanced images while maintaining rapid single-step inference. By offering a cost-effective solution to the chronic shortage of labeled defect data and potentially supporting continuous learning paradigms, Ali-AUG represents a significant advancement in the field of synthetic data generation for industrial AI applications."}, {"title": "5.2. Limitation and Future Work", "content": "Despite the advantages of our proposed method, several limitations exist. As with any generative model, the images produced by our approach can sometimes be incorrect or invalid. There are instances where the generated images might not accurately reflect the intended defect or characteristic, leading to potential inaccuracies in the labels. These issues arise from the inherent challenges of conditional image generation and the complexity of faithfully reproducing specific features in varied contexts.\nOur model is designed to aid in accelerating data augmentation and labeling processes, but it is not a replacement for meticulous manual verification. Users should employ this tool to supplement their existing workflows, providing a means to rapidly generate large volumes of synthetic data, which can then be reviewed and corrected as needed. This approach is precious in scenarios where labeled data is scarce or expensive, substantially improving efficiency."}]}