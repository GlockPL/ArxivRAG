{"title": "BOND: Aligning LLMs with Best-of-N Distillation", "authors": ["Pier Giuseppe Sessa", "Robert Dadashi", "L\u00e9onard Hussenot", "Johan Ferret", "Nino Vieillard", "Alexandre Ram\u00e9", "Bobak Shariari", "Sarah Perrin", "Abe Friesen", "Geoffrey Cideron", "Sertan Girgin", "Piotr Stanczyk", "Andrea Michi", "Danila Sinopalnikov", "Sabela Ramos", "Am\u00e9lie H\u00e9liou", "Aliaksei Severyn", "Matt Hoffman", "Nikola Momchev", "Olivier Bachem"], "abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models. Yet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates. In this paper, we propose Best-of-N Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant computational overhead at inference time. Specifically, BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution. We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving anchor for efficiency. We demonstrate the effectiveness of our approach and several design choices through experiments on abstractive summarization and Gemma models. Aligning Gemma policies with BOND outperforms other RLHF algorithms by improving results on several benchmarks.", "sections": [{"title": "1. Introduction", "content": "State-of-the-art large language models (LLMs) such as Gemini (Gemini Team, 2023; Reid et al., 2024) and GPT-4 (OpenAI, 2023) are generally trained in three stages. First, LLMs are pre-trained on large corpora of knowledge using next-token prediction (Radford et al., 2018, 2019). Second, the pre-trained models are fine-tuned to follow instructions via supervised fine-tuning (SFT) (Raffel et al., 2020; Wei et al., 2022). Lastly, reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020) is used to further increase the quality of generations. The RLHF step generally consists of learning a reward model (RM) (Ouyang et al., 2022) on human preferences and then optimizing the LLM to maximize predicted rewards using reinforcement learning algorithms.\nRLHF algorithms and their challenges. Fine-tuning LLMs with reinforcement learning (RL) is challenging (Casper et al., 2023), notably since it can cause forgetting (French, 1992) of pre-trained knowledge, and since loopholes in the RM (Clark and Amodei, 2016; Pan et al., 2022) can cause reward hacking (Askell et al., 2021; Skalse et al., 2022). The standard strategy is to use policy-gradient methods (Williams, 1992) with KL regularization towards the SFT policy. Those RL algorithms seek Pareto-optimal policies with high reward at low KL, to preserve the general capabilities of the original model and tackle the misalignment (Ngo et al., 2022) concerns.\nBest-of-N sampling. In practice, a surprisingly simple inference-time approach is often used to improve the quality of generations: Best-of-N sampling (Stiennon et al., 2020). It consists of drawing N candidate generations from the reference (typically, supervised fine-tuned) model and selecting the one with the highest reward according to the RM. This strategy empirically achieves excellent reward-KL trade-offs (Nakano et al., 2021; Gao et al., 2023; Touvron et al., 2023) but increases the computational cost by a factor of N."}, {"title": "2. Problem Setup", "content": "We consider a LLM based on the transformer (Vaswani et al., 2017) architecture, defining a policy \u03c0(x, \u00b7) by auto-regressively generating token sequences y from the prompt x. Given a pre-trained and typically supervised fine-tuned reference policy tref, we seek to further align it to human preferences. To achieve this, throughout the rest of the paper we assume access to a reward model (RM) which we denote as r(\u00b7), trained to reflect human preferences.\nStandard RLHF. Most RL algorithms optimize a linear combination of the expected reward and a KL"}, {"title": "3. The BOND Approach", "content": "We formulate the BOND approach in two main steps. First, we derive an analytical expression for the Best-of-N distribution (Section 3.1). Second, using the derived expression, we phrase the problem as a distribution matching problem (Section 3.2), i.e., we want to steer the policy closer to the Best-of-N distribution. In Section 3.3, we draw insightful connections between BOND and standard RLHF."}, {"title": "3.1. The Best-of-N distribution", "content": "In this section, we derive the exact analytical distribution of Best-of-N sampling and study its properties. For simplicity, we drop the context x from all notation without loss of generality and assume that the reward r(y) induces a strict ordering on all generations y\u00b9. We can affirm the following main theorem (proof in Appendix A.1).\nTheorem 1. For any generation y, let\n\\(p_{<(y)} = \\mathbb{P}_{y'\\sim \\pi_{ref}} [r(y') < r(y)]\\)\n denote the probability that a random generation y' from aref is strictly worse than y and let\n\\(p_{\\leq(y)} = \\mathbb{P}_{y'\\sim \\pi_{ref}}[r(y') \\leq r(y)],\\)\n the probability that y' is not better than y (thus including the equality case). Then, the probability that y is the output of Best-of-N sampling is given by\n\\[\\pi_{BoN}(y) = \\pi_{ref}(y) \\times p_{\\leq}(y)^{N-1} \\times \\underbrace{\\left(1 + \\sum_{i=2}^{N} \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)^{i-1}}\\right)}_{\\text{(B)}}\\]"}, {"title": "Interpretation.", "content": "Theorem 1 provides an intuitive explanation on the behavior of Best-of-N sampling: it essentially reweights the original sampling distribution ref, by the multiplicative terms (A) and (B).\nThe term (A) corresponds to a penalty exponential in N based on the fraction of generations (for the same prompt) that are worse or equal to the considered generation y. Intuitively, this ensures that we sample exponentially less from bad generations when increasing N.\nThe term (B) is an additional correction factor due to the potential of collisions among generations. Importantly, it is at most linear in N as it is always bounded within [1, N]:\n\\[1 \\leq \\underbrace{1+\\sum_{i=2}^{N} \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)^{i-1}}}_{\\text{(B)}} = \\underbrace{\\sum_{i=1}^{N} \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)^{i-1}}}_{\\text{(B)}} < N\\]\nIt achieves its minimum at 1 for the worst generation y- since we have p<(y-) = 0 by definition. This is not surprising, as we need to sample y exactly N times in a row and which corresponds to Bon(y\u2212) = ref(y_) (note that p<(y-) = \u03c0ref(y-)). In contrast, if the likelihood of individual generations y are low and such generations are good, then p<(y) is almost p\u2264(y) and term (b) is close to N. Intuitively, this corresponds to the case where sampling a generation y multiple times is unlikely. In the extreme case when tref is a continuous distribution, term (B) is constant and equal to N (see Appendix A.2)."}, {"title": "3.2. The BOND objective", "content": "The analytical characterization of the Best-of-N distribution allows us to formulate BOND as a distribu-tion matching problem. That is, we want to solve the objective:\n\\[\\pi_{BOND} = \\underset{\\pi\\in\\Pi}{\\arg \\min} D (\\pi || \\pi_{BON}),\\]\nwhere D( ||) is a divergence metric steering the training policy n towards \u03c0\u0392\u039f\u039d. For this, a toolbox of possible divergences exist in the literature including, e.g., forward and backward KL (Kullback, 1959). Moreover, we can employ existing distribution matching techniques to estimate D from online and offline samples. We defer the choice of suitable divergences and resulting BOND algorithms to Section 4."}, {"title": "3.3. Connection with standard RLHF", "content": "In this section, we draw important connections between the two seemingly different objectives of standard RLHF (Equation (1)) and BOND (Equation (6)).\nIt is well known (see, e.g., Vieillard et al. (2020); Rafailov et al. (2023)) that the policy maximizing the RLHF objective from Equation (1) is:\n\\[\\pi_{RL}(y) \\propto \\pi_{ref}(y) \\exp\\left(\\frac{1}{\\beta_{RL}} r(y)\\right)\\]\nFrom the derived expression of \u03c0\u00dfon in Theorem 1, we see that the Best-of-N sampling distribution coincides with the optimal solution of standard RLHF when using the following specific BOND reward:\n\\[r_{BOND}(y) = \\underbrace{\\log p_{\\leq}(y)}_{\\text{(A)}} + \\underbrace{\\frac{1}{N-1} \\log \\left(\\sum_{i=1}^{N} \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)}\\right)}_{\\text{(B)}}\\]\nand the specific regularization strength \\(\\beta_{BOND} = \\frac{1}{N-1}\\). The term (B) corresponds to the correction factor in Theorem 1, which is bounded in \\([0, \\frac{1}{N-1}]\\), for all generations y. Instead term (A) lies in \\((-\\infty, 0]\\). This provides two interesting insights for Best-of-N sampling:"}, {"title": "4. BOND Challenges and Algorithms", "content": "Implementing the BOND approach induces the three following challenges: (1) how to estimate the reward quantiles, (2) which is the appropriate divergence metric to use, and (3) how to choose the hyperparameter N representing the number of sampled generations in Best-of-N. We discuss and address these challenges in the next three subsections."}, {"title": "4.1. Monte-Carlo quantile estimation", "content": "One key difficulty in estimating the BON distribution is that we need to estimate the quantile\n\\[p_{<(y)} = \\mathbb{P}_{y'\\sim \\pi_{ref}} [r(y') \\leq r(y)],\\]\nof a given generation y. The quantile p<(y) measures the quality of y compared to generations from Tref when conditioned on the same prompt (recall that we have suppressed the conditioning on x in our notation). A very simple but effective quantile estimation method is Monte-Carlo sampling, sampling k generations from aref and obtaining the following empirical estimate:\n\\[\\hat{p}_{<}(y) = \\frac{1}{k} \\sum_{i=1}^{k} [\\{ r(y_{i}) \\leq r(y) \\}].\\]\nWe found this to be a very effective in our experiments, even with a limited number of samples. In principle, though, one could also use alternative approaches, e.g., training a learned quantile model (as we explore in Appendix B.1)."}, {"title": "4.2. Jeffreys divergence as a robust objective", "content": "The choice of the divergence metric used in BOND is of crucial importance: different divergences can steer the policy to very different solutions. Here, we propose the Jeffreys divergence as a robust distribution matching objective.\nThe Jeffreys divergence (Jeffreys, 1946) between two distributions is defined as:\n\\[JB_{Jeffreys}(p || q) := (1 - \\beta) \\cdot KL(q || p) + \\beta \\cdot KL(p || q).\\]"}, {"title": "Estimation of the forward KL.", "content": "The forward KL defined as\n\\[KL(\\pi_{BoN} || \\pi) = \\mathbb{E}_{y\\sim \\pi_{BON}} [\\log \\pi_{BoN}(y) - \\log \\pi(y)]\\]\ncan be estimated directly drawing samples from the BON (i.e., sampling N times from ref and selecting the best one) and can be seen as a supervised fine-tuning loss on the Best-of-N samples:\n\\[\\nabla KL(\\pi_{BoN} || \\pi) = -\\mathbb{E}_{y\\sim \\pi_{BON}}\\nabla \\log \\pi(y).\\]"}, {"title": "Estimation of the backward KL.", "content": "The backward KL defined as\n\\[KL(\\pi || \\pi_{BoN}) = \\mathbb{E}_{y\\sim \\pi} [\\log \\pi(y) - \\log \\pi_{Bon} (y)]\\]\ncan be estimated from the policy samples (note the expectation w.r.t. \u03c0) and their estimated log-likelihood under \u03c0\u24b7ON. In particular, by the analogies drawn in Section 3.3, we show (in Appendix A.3) that its gradient coincides with a policy gradient (e.g., used by REINFORCE (Williams, 1992) in standard RLHF):\n\\[\\nabla_{\\pi}KL(\\pi || \\pi_{BoN}) = -(N - 1) \\mathbb{E}_{y \\sim \\pi} [\\nabla_{\\pi} \\log \\pi(y)(r_{BOND}(y) - \\beta_{BOND} (\\log \\pi(y) - \\log \\pi_{ref}(y)))]\\]\nwith the equivalent reward rbond and regularization \u1e9eBOND defined in Section 3.3. Note that rBOND(y) depends on the true unknown quantile p\u2264 (y) and on the correction factor (B) defined in Equation (8). In practice, we substitute the true quantile by its estimate, while we observed the correction factor does not play a significant role. Thus, we use rbond(y) = p<(y). Moreover, to reduce the resulting variance, we use a policy gradient baseline (Sutton and Barto, 1998) which we compute as the average return for the other generations in the batch.\nThus, the overall JB policy gradient loss."}, {"title": "4.3. Iterative BOND", "content": "Finally, we discuss the choice of the parameter N. In practice, choosing N may be difficult for three main reasons: (1) As in standard RLHF, N plays the role of regularization (see Section 3.3): a large N improves downstream performance, but if N is too large it will eventually cause reward over optimization (Gao et al., 2023). (2) The larger the N the more the estimate of Bon is sensitive to errors in the estimated quantiles (since \u03c0BoN(y) \u221d p<(y)N-1). (3) Estimating the forward KL divergence requires sampling from \u03c0\u0392\u03bf\u039d which is prohibitive for large N.\nTo address the above challenges, we propose the iterative BOND approach. The approach is inspired by the fact that Best-of-N sampling from a Best-of-N distribution, coincides with Best-of-N2 sampling from the original distribution. More generally, by informally defining BoN(\u00b7) as an operator that performs Best-of-N sampling from a base distribution, we have:\n\\[BoN(BoN(BoN(\\pi_{ref}))) = BoN^{M}(\\pi_{ref}).\\]\nThis suggests the key idea behind iterative BOND: if we know how to distill the Best-of-N distribution (i.e., via BOND), then we can apply BOND recursively (say M times), equivalently to distilling a Best-of-NM of the initial distribution tref. This allows fixing a small n (i.e., n = 2) and running BOND (with N = n) in an iterative fashion, as an improvement operator. For this, we can introduce an auxiliary anchor policy anchor initialized as tref. We can then run BOND against ranchor (i.e., we can distill the Best-of-n version of ranchor) and, after a given number of distillation steps, update anchor to be the current training policy \u03c0\u0165."}, {"title": "5. The J-BOND Algorithm", "content": "In this section we present J-BOND, a concrete and practical BOND algorithm motivated by the results discussed in the previous sections. We describe its main components below, and summarize it in the pseudo-code of Algorithm 2.\nJ-BOND follows the template of iterative BOND (Algorithm 1) with n = 2, i.e., it fine-tunes policy \u03c0\u03c4 to iteratively distill the Best-of-2 version of a moving anchor \u03c0 initialized as ref. The name\nJ-BOND stands for Jeffreys divergence BOND because it uses the Jeffreys divergence as distribution matching objective, i.e., it minimizes \\(JB_{effreys} (\\pi || Best-of-2(\\pi^{t}_{anchor}))\\) as defined in Section 4.2.\nMinimal sample complexity. Compared to the BOND algorithms tested in the previous section, J-BOND has a minimal sample complexity: for each prompt in the batch it generates 1 sample from"}, {"title": "Algorithm 2", "content": "The J-BOND algorithm\nInputs: Prompt dataset D, reference policy tref, reward r(\u00b7), \u03b2, \u03b7 \u2208 [0, 1], y \u2265 0.\nInitialize policy and anchor \u03c0\u03bf = \u03c00 = \u03c0ref.\nSample batch of prompts D\u06c1 \u2286 D\nFor each x \u2208 De generate: 1 policy sample y ~ \u03c0\u2081(x) and 2 anchor samples y1, y2 ~ \u03c0anchor (x)\nExtract Best-of-2 sample: yb02 = arg maxy' \u2208 {y1,y,} r(y').\nCompute forward KL gradient: GFW(x, t) = \u2212\u2207\u03c0\u03b5 log \u03c0\u03b5 (x, \u0423\u0432\u043e2)\nCompute rj-BOND(x, y) according to Equation (17).\nCompute return: R(x, y) = rj-BOND (x, y) \u2013 (log n\u2081 (x, y) \u2013 log ranchor (x, y)).\n[Optional] Compute baseline B, e.g. average return of the other generations in the batch.\nCompute backward KL gradient: GBw(x, \u03c0t) = \u2212\u2207\u2081\u2081 log \u03c0t(x, y) \u00b7 (R(x, y) \u2013 B).\nKL regularization gradient: GReg(x, \u03c0t) = \u2212\u2207\u03c0\u03b5 log \u03c0\u2081(x, y) \u00b7 (log n\u2081(x, y) \u2013 log \u03c0anchor (x, y)).\nUpdate policy weights 8t+1 with the overall stochastic gradient:\n\\[\\mathbb{E}_{x\\sim D_{e}}[(1 - \\beta) \\cdot G_{FW}(x, \\pi_{t}) + \\beta \\cdot G_{Bw}(x, \\pi_{t}) + \\gamma \\cdot G_{Reg}(x, \\pi_{t})]\\]\nUpdate anchor weights with EMA: \u03b8+1\n\\[ \\theta_{anchor}^{t+1} \\leftarrow (1 - \\eta) \\cdot \\theta_{anchor}^{t} + \\eta \\cdot \\theta_{t+1}.\\]"}, {"title": "(i)", "content": "We negatively reward y only if it is worse than both the anchor samples, to mimic the concavity of the ideal (and unknown) reward function rBOND = log p\u2264 (\u00b7)."}, {"title": "6. Experiments", "content": "We test J-BOND on relevant use cases with the following main goals. First, we ablate and showcase important aspects of J-BOND: the benefits of the EMA anchor, and the effects of the anchor speed and the additional KL regularization. Then, we compare J-BOND to classical RLHF baselines using REINFORCE, demonstrating its efficacy and better performance.\nSetup. We consider Gemma (2B and 7B) models (Gemma Team, 2024) which we aim to fine-tune into better conversational agents. For this task, we consider a set of conversational prompts D, a reference policy aref previously supervised fine-tuned on similar prompts, and a large previously trained reward model r(\u00b7). We use a batch size 128 and the Adam optimizer (Kingma and Ba, 2015) with learning rate 3e\u00af6 and 100 warm-up steps. For the Jeffreys divergence objective, we set \u1e9e = 0.5."}, {"title": "(ii)", "content": "We choose value \u2013 log(16) to ensure that: \\(\\mathbb{E}_{y_{1,2}\\sim \\pi_{anchor}} [r_{J-BOND}(y)] = \\log p_{\\leq}(y)\\) when p\u2264 (y) = 0.5. The interested reader is referred to Appendix A.4 for a derivation and illustration of this fact.\nIn words, the value \u2013 log(16) calibrates the reward function rj-BOND(\u00b7) so that, in expectation under the 2 anchor samples, it matches with the ideal reward log p\u2264 (\u00b7) for generations y that have median reward (i.e., when p\u2264 (y) = 0.5).\nExponential Moving Average (EMA) anchor. An important component of J-BOND, which refines the vanilla iterative BOND of Section 4.3, is the use of an Exponential Moving Average (EMA) anchor. That is, instead of using a periodically updated anchor, we update the anchor weights \u03b8\nat each fine-tuning step as a moving average of the policy weights \u03b8\u03b5:\n\\[\\theta_{anchor}^{t+1} \\leftarrow (1 - \\eta) \\cdot \\theta_{anchor}^{t} + \\eta \\cdot \\theta_{t+1}.\\]\nConsistently with WARP (Ram\u00e9 et al., 2024), we observed that this weight averaging procedure has a positive effect on training stability by reducing variance, and can improve the overall reward/KL trade-off of J-BOND. We provide an ablation in Section 6.\nAdditional KL regularization. Finally, we further regularize the policy to stay closer to the moving anchor via an extra2 KL regularization term, modulated by a tunable hyperparameter y \u2265 0. The scope is to further stabilize the policy updates, viewing the overall operator as a constrained optimization one:\n\\[\\pi_{t+1} = \\underset{\\pi\\in\\Pi}{\\arg \\min} JB_{effreys}(\\pi || Best-of-2(\\pi_{anchor})) + \\gamma \\cdot KL(\\pi_{\\tau} || \\pi_{anchor}).\\]"}, {"title": "EMA vs. hard anchor updates.", "content": "We ablate the benefits of using an EMA moving anchor (Equation (18)) compared to the periodically updated anchor used in Section 4.3. For this, we run J-BOND with y = 0 and EMA coefficient n = 0.02 on Gemma 7B, and compare it with its variant where the anchor is only updated every 50 steps. In Figure 5, we report the average reward of the policy during training (left plot), the KL from the reference policy ref (middle plot), and the resulting reward/KL trade-off for the two runs. Both runs produce the same reward increase profile (this is not surprising since an EMA with \u03b7 = 0.02 roughly corresponds to an update period of 50 steps) but, crucially, J-BOND with an EMA anchor displays a significantly lower KL increase and, as a result, a better reward/KL trade-off.\nAnchor speed and KL regularization. Here we illustrate the effect of the anchor mixing parameter \u03b7 \u2208 [0, 1] and the benefits of the additional KL regularization parameter y \u2265 0 introduced in Equation (19). For this, we consider Gemma 2B and first run J-BOND with y = 0 and \u03b7 \u2208 {0.01, 0.05, 0.1}. In the left plot of Figure 6 we report the average reward of the policy along training. This illustrates that the larger the mixing parameter n (i.e., the faster the anchor moves), the faster the reward increases, as one could intuitively expect. Second, we fix \u03b7 = 0.05 and run J-BOND with different regularization strengths y \u2208 {0,0.5, 1,2}. We plot the results in the middle and rightmost plots of Figure 6. As expected, the larger the regularization y, the more constrained are the policy updates and thus, the slower the policy moves away from tref (middle plot). Importantly, the right plot shows that such a regularization has a positive effect since it can ultimately improve the reward/KL trade-off.\nComparison with standard RLHF. We compare J-BOND against standard RLHF algorithms that aim at maximizing the KL-regularized objective of Equation (1). To optimize Equation (1), we use"}, {"title": "Scaling post-training and iterated amplification.", "content": "BOND hinges on the idea of investing more resources during training to ensure that computational demands during inference remain low, a factor often overlooked in traditional scaling laws (Hoffmann et al., 2022). Specifically, BOND incorporates the principles of iterated amplification (Christiano et al., 2018; Cotra, 2018), where amplification in this context consists of producing multiple generations, comparing their rewards, and using these to iteratively improve the policy performance. In this regard, BOND is complementary to WARM (Ram\u00e9 et al., 2024) and WARP (Ram\u00e9 et al., 2024), which previously scaled post-training by training multiple reward models and policies, respectively."}, {"title": "8. Conclusion", "content": "We introduce BOND, a novel RLHF method that fine-tunes the policy via online distillation of the Best-of-N sampling distribution. We propose a concrete algorithm, J-BOND, that integrates multiple components to enhance its practicality and efficiency; Monte-Carlo quantile estimation, a combina-tion between forward and backward KL divergence objectives, and an iterative procedure with an exponential moving average anchor. J-BOND improves the KL-reward Pareto front of solutions, and compares favorably against state-of-the-art baselines. We hope this work can help improve alignment of Al systems, making them safer and more reliable."}]}