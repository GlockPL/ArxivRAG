[{"title": "BOND: Aligning LLMs with Best-of-N Distillation", "authors": ["Pier Giuseppe Sessa", "Robert Dadashi", "L\u00e9onard Hussenot", "Johan Ferret", "Nino Vieillard", "Alexandre Ram\u00e9", "Bobak Shariari", "Sarah Perrin", "Abe Friesen", "Geoffrey Cideron", "Sertan Girgin", "Piotr Stanczyk", "Andrea Michi", "Danila Sinopalnikov", "Sabela Ramos", "Am\u00e9lie H\u00e9liou", "Aliaksei Severyn", "Matt Hoffman", "Nikola Momchev", "Olivier Bachem"], "abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models. Yet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates. In this paper, we propose Best-of-N Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant computational overhead at inference time. Specifically, BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution. We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving anchor for efficiency. We demonstrate the effectiveness of our approach and several design choices through experiments on abstractive summarization and Gemma models. Aligning Gemma policies with BOND outperforms other RLHF algorithms by improving results on several benchmarks.", "sections": [{"title": "1. Introduction", "content": "State-of-the-art large language models (LLMs) such as Gemini (Gemini Team, 2023; Reid et al., 2024) and GPT-4 (OpenAI, 2023) are generally trained in three stages. First, LLMs are pre-trained on large corpora of knowledge using next-token prediction (Radford et al., 2018, 2019). Second, the pre-trained models are fine-tuned to follow instructions via supervised fine-tuning (SFT) (Raffel et al., 2020; Wei et al., 2022). Lastly, reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020) is used to further increase the quality of generations. The RLHF step generally consists of learning a reward model (RM) (Ouyang et al., 2022) on human preferences and then optimizing the LLM to maximize predicted rewards using reinforcement learning algorithms.\nRLHF algorithms and their challenges. Fine-tuning LLMs with reinforcement learning (RL) is challenging (Casper et al., 2023), notably since it can cause forgetting (French, 1992) of pre-trained knowledge, and since loopholes in the RM (Clark and Amodei, 2016; Pan et al., 2022) can cause reward hacking (Askell et al., 2021; Skalse et al., 2022). The standard strategy is to use policy-gradient methods (Williams, 1992) with KL regularization towards the SFT policy. Those RL algorithms seek Pareto-optimal policies with high reward at low KL, to preserve the general capabilities of the original model and tackle the misalignment (Ngo et al., 2022) concerns.\nBest-of-N sampling. In practice, a surprisingly simple inference-time approach is often used to improve the quality of generations: Best-of-N sampling (Stiennon et al., 2020). It consists of drawing N candidate generations from the reference (typically, supervised fine-tuned) model and selecting the one with the highest reward according to the RM. This strategy empirically achieves excellent reward-KL trade-offs (Nakano et al., 2021; Gao et al., 2023; Touvron et al., 2023) but increases the computational cost by a factor of N."}, {"title": "2. Problem Setup", "content": "We consider a LLM based on the transformer (Vaswani et al., 2017) architecture, defining a policy $\\pi(x, \\cdot)$ by auto-regressively generating token sequences y from the prompt x. Given a pre-trained and typically supervised fine-tuned reference policy $\\pi_{ref}$, we seek to further align it to human preferences. To achieve this, throughout the rest of the paper we assume access to a reward model (RM) which we denote as $r(\\cdot)$, trained to reflect human preferences.\nStandard RLHF. Most RL algorithms optimize a linear combination of the expected reward and a KL"}, {"title": "BOND: Aligning LLMs with Best-of-N Distillation", "content": "divergence between the current and reference policy:\n$\\pi_{RL} = \\underset{\\pi}{argmax}\\  \\mathbb{E}_{\\pi}[r(y)] - \\beta_{RL} \\cdot KL(\\pi \\|\\| \\pi_{ref}),$ \nwith regularization strength $\\beta_{RL} \\geq 0$. This KL regularization forces the policy to remain close to its initialization $\\pi_{ref}$ (Geist et al., 2019; Lazaridou et al., 2020), reducing forgetting (French, 1992) and reward hacking (Skalse et al., 2022). Equation (1) is usually optimized with online algorithms, as they perform better than their offline counterparts (Tang et al., 2024). Moreover, simple methods have demonstrated the best results, e.g., REINFORCE (Williams, 1992) with a sampled baseline for variance reduction (Li et al., 2023; Ahmadian et al., 2024) outperform PPO (Schulman et al., 2017).\nBest-of-N. A complementary alignment strategy is Best-of-N, which is an inference-time strategy that involves sampling multiple times from $\\pi_{ref}$ and selecting the generation with highest reward according to the RM r. In contrast to RLHF strategies, Best-of-N does not fine-tune the weights of the LLM, but instead modifies the inference procedure. Best-of-N was empirically shown to be efficient (Touvron et al., 2023) when looking at reward/KL trade-offs, and comes with theoretical guarantees (Qiping Yang et al., 2024) in terms of Pareto-optimality. Unfortunately, Best-of-N comes at a significantly higher inference cost which increases linearly with N, since producing N generations is (in general) N times more costly than sampling a single one.\nMotivated by the above considerations, we propose a novel alignment method which we name BOND for Best-of-N Distillation. The goal of BOND is to distill the Best-of-N strategy into the policy. This allows the policy to reach the strong performance of Best-of-N sampling, while requiring only a single sample at inference time. We outline our overall approach in the next section."}, {"title": "3. The BOND Approach", "content": "We formulate the BOND approach in two main steps. First, we derive an analytical expression for the Best-of-N distribution (Section 3.1). Second, using the derived expression, we phrase the problem as a distribution matching problem (Section 3.2), i.e., we want to steer the policy closer to the Best-of-N distribution. In Section 3.3, we draw insightful connections between BOND and standard RLHF."}, {"title": "3.1. The Best-of-N distribution", "content": "In this section, we derive the exact analytical distribution of Best-of-N sampling and study its properties. For simplicity, we drop the context x from all notation without loss of generality and assume that the reward r(y) induces a strict ordering on all generations y\u00b9. We can affirm the following main theorem (proof in Appendix A.1).\nTheorem 1. For any generation y, let\n$p_{<}(y) = P_{y'\\sim \\pi_{ref}} [r(y') < r(y)]$ \ndenote the probability that a random generation y' from $\\pi_{ref}$ is strictly worse than y and let\n$p_{\\leq}(y) = P_{y'\\sim \\pi_{ref}}[r(y') \\leq r(y)],$ \nthe probability that y' is not better than y (thus including the equality case). Then, the probability that y is the output of Best-of-N sampling is given by\n$\\pi_{BoN}(y) = \\pi_{ref}(y) \\times p_{\\leq}(y)^{N-1} \\times \\sum_{i=1}^{N}  \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)}$"}, {"title": "BOND: Aligning LLMs with Best-of-N Distillation", "content": "Interpretation. Theorem 1 provides an intuitive explanation on the behavior of Best-of-N sampling: it essentially reweights the original sampling distribution $\\pi_{ref}$, by the multiplicative terms (A) and (B).\nThe term (A) corresponds to a penalty exponential in N based on the fraction of generations (for the same prompt) that are worse or equal to the considered generation y. Intuitively, this ensures that we sample exponentially less from bad generations when increasing N.\nThe term (B) is an additional correction factor due to the potential of collisions among generations. Importantly, it is at most linear in N as it is always bounded within [1, N]:\n$1 \\leq 1+\\sum_{i=2}^{N}  \\frac{p_{<}(y)}{p_{\\leq}(y)}^{i-1} =  \\sum_{i=1}^{N}  \\frac{p_{<}(y)}{p_{\\leq}(y)}^{i-1} < N$\nIt achieves its minimum at 1 for the worst generation y- since we have $p_{<}(y^{-}) = 0$ by definition. This is not surprising, as we need to sample y exactly N times in a row and which corresponds to $\\pi_{BoN}(y^{-}) = \\pi_{ref}(y^{-})$ (note that $p_{<}(y^{-}) = \\pi_{ref}(y^{-})$). In contrast, if the likelihood of individual generations y are low and such generations are good, then $p_{<}(y)$ is almost $p_{\\leq}(y)$ and term (b) is close to N. Intuitively, this corresponds to the case where sampling a generation y multiple times is unlikely. In the extreme case when $\\pi_{ref}$ is a continuous distribution, term (B) is constant and equal to N (see Appendix A.2)."}, {"title": "3.2. The BOND objective", "content": "The analytical characterization of the Best-of-N distribution allows us to formulate BOND as a distribution matching problem. That is, we want to solve the objective:\n$\\pi_{BOND} = \\underset{\\pi \\in \\Pi}{arg\\  min}\\  D(\\pi \\|\\| \\pi_{BON}),$\nwhere $D(\\cdot\\|\\|\\cdot)$ is a divergence metric steering the training policy $\\pi$ towards $\\pi_{BON}$. For this, a toolbox of possible divergences exist in the literature including, e.g., forward and backward KL (Kullback, 1959). Moreover, we can employ existing distribution matching techniques to estimate D from online and offline samples. We defer the choice of suitable divergences and resulting BOND algorithms to Section 4."}, {"title": "3.3. Connection with standard RLHF", "content": "In this section, we draw important connections between the two seemingly different objectives of standard RLHF (Equation (1)) and BOND (Equation (6)).\nIt is well known (see, e.g., Vieillard et al. (2020); Rafailov et al. (2023)) that the policy maximizing the RLHF objective from Equation (1) is:\n$\\pi_{RL}(y) \\propto \\pi_{ref}(y) \\exp \\left(\\frac{r(y)}{\\beta_{RL}}\\right)$\nFrom the derived expression of $\\pi_{Bon}$ in Theorem 1, we see that the Best-of-N sampling distribution coincides with the optimal solution of standard RLHF when using the following specific BOND reward:\n$r_{BOND}(y) = log\\ p_{\\leq}(y) + \\frac{1}{N-1} \\sum_{i=1}^{N-1} log\\frac{p_{<}(y)}{p_{\\leq}(y)},$\nand the specific regularization strength $\\beta_{BOND} = \\frac{1}{N-1}$. The term (B) corresponds to the correction factor in Theorem 1, which is bounded in $[-1, 1]$ for all generations y. Instead term (A) lies in $(-\\infty, 0]$. This provides two interesting insights for Best-of-N sampling:"}, {"title": "4. BOND Challenges and Algorithms", "content": "Implementing the BOND approach induces the three following challenges: (1) how to estimate the reward quantiles, (2) which is the appropriate divergence metric to use, and (3) how to choose the hyperparameter N representing the number of sampled generations in Best-of-N. We discuss and address these challenges in the next three subsections."}, {"title": "4.1. Monte-Carlo quantile estimation", "content": "One key difficulty in estimating the $\\pi_{BON}$ distribution is that we need to estimate the quantile\n$p_{<}(y) = P_{y'\\sim \\pi_{ref}} [r(y') \\leq r(y)],$\nof a given generation y. The quantile $p_{<}(y)$ measures the quality of y compared to generations from $\\pi_{ref}$ when conditioned on the same prompt (recall that we have suppressed the conditioning on x in our notation). A very simple but effective quantile estimation method is Monte-Carlo sampling, sampling k generations from $\\pi_{ref}$ and obtaining the following empirical estimate:\n$\\hat{p}_{<}(y) = \\frac{1}{k} \\sum_{i=1}^{k} {\\mathbb{I}\\left{ r(y_{i}) \\leq r(y) \\right} }$.\nWe found this to be a very effective in our experiments, even with a limited number of samples. In principle, though, one could also use alternative approaches, e.g., training a learned quantile model (as we explore in Appendix B.1)."}, {"title": "4.2. Jeffreys divergence as a robust objective", "content": "The choice of the divergence metric used in BOND is of crucial importance: different divergences can steer the policy to very different solutions. Here, we propose the Jeffreys divergence as a robust distribution matching objective.\nThe Jeffreys divergence (Jeffreys, 1946) between two distributions is defined as:\n$JB_{Jeffreys}(p \\|\\| q) := (1 - \\beta) \\cdot KL(q \\|\\| p) + \\beta \\cdot KL(p \\|\\| q) $."}, {"title": "4.3. Iterative BOND", "content": "Finally, we discuss the choice of the parameter N. In practice, choosing N may be difficult for three main reasons: (1) As in standard RLHF, N plays the role of regularization (see Section 3.3): a large N improves downstream performance, but if N is too large it will eventually cause reward over optimization (Gao et al., 2023). (2) The larger the N the more the estimate of $\\pi_{Bon}$ is sensitive to errors in the estimated quantiles (since $\\pi_{BoN}(y) \\propto p_{<}(y)^{N-1}$). (3) Estimating the forward KL divergence requires sampling from $\\pi_{BON}$ which is prohibitive for large N.\nTo address the above challenges, we propose the iterative BOND approach. The approach is inspired by the fact that Best-of-N sampling from a Best-of-N distribution, coincides with Best-of-N2 sampling from the original distribution. More generally, by informally defining BoN(\u00b7) as an operator that performs Best-of-N sampling from a base distribution, we have:\n$BoN(BoN(... BoN(\\pi_{ref}))) = BoN^{M}(\\pi_{ref})$.\nThis suggests the key idea behind iterative BOND: if we know how to distill the Best-of-N distribution (i.e., via BOND), then we can apply BOND recursively (say M times), equivalently to distilling a Best-of-NM of the initial distribution $\\pi_{ref}$. This allows fixing a small n (i.e., n = 2) and running BOND (with N = n) in an iterative fashion, as an improvement operator. For this, we can introduce an auxiliary anchor policy $\\pi_{anchor}$ initialized as $\\pi_{ref}$. We can then run BOND against $\\pi_{anchor}$ (i.e., we can distill the Best-of-n version of $\\pi_{anchor}$) and, after a given number of distillation steps, update $\\pi_{anchor}$ to be the current training policy $\\pi_{t}$."}, {"title": "5. The J-BOND Algorithm", "content": "In this section we present J-BOND, a concrete and practical BOND algorithm motivated by the results discussed in the previous sections. We describe its main components below, and summarize it in the pseudo-code of Algorithm 2.\nJ-BOND follows the template of iterative BOND (Algorithm 1) with n = 2, i.e., it fine-tunes policy $\\pi_{t}$ to iteratively distill the Best-of-2 version of a moving anchor $\\pi_{anchor}$ initialized as $\\pi_{ref}$. The name J-BOND stands for Jeffreys divergence BOND because it uses the Jeffreys divergence as distribution matching objective, i.e., it minimizes $JB_{Jeffreys} (\\pi \\|\\| Best-of-2(\\pi_{anchor}))$ as defined in Section 4.2.\nMinimal sample complexity. Compared to the BOND algorithms tested in the previous section, J-BOND has a minimal sample complexity: for each prompt in the batch it generates 1 sample from"}, {"title": "BOND: Aligning LLMs with Best-of-N Distillation", "content": "(ii) We choose value \u2013 log(16) to ensure that: $E_{y_{1},y_{2} \\sim \\pi_{anchor}} [r_{J-BOND}(y)] = log\\ p_{\\leq}(y)$ when $p_{\\leq}(y) = 0.5$. The interested reader is referred to Appendix A.4 for a derivation and illustration of this fact.\nExponential Moving Average (EMA) anchor. An important component of J-BOND, which refines the vanilla iterative BOND of Section 4.3, is the use of an Exponential Moving Average (EMA) anchor. That is, instead of using a periodically updated anchor, we update the anchor weights $\\theta_{anchor}$ at each fine-tuning step as a moving average of the policy weights $\\theta_{t}$:\n$\\theta_{anchor}^{t+1} \\leftarrow (1 - \\eta) \\cdot \\theta_{anchor} + \\eta \\cdot \\theta_{t+1}.$\nConsistently with WARP (Ram\u00e9 et al., 2024), we observed that this weight averaging procedure has a positive effect on training stability by reducing variance, and can improve the overall reward/KL trade-off of J-BOND. We provide an ablation in Section 6.\nAdditional KL regularization. Finally, we further regularize the policy to stay closer to the moving anchor via an extra2 KL regularization term, modulated by a tunable hyperparameter $\\gamma \\geq 0$. The scope is to further stabilize the policy updates, viewing the overall operator as a constrained optimization one:\n$\\pi_{t+1} = arg\\  min_{\\pi \\in \\Pi}\\  JB_{Jeffreys} (\\pi \\|\\| Best-of-2(\\pi_{anchor})) + \\gamma \\cdot KL(\\pi_{t} \\|\\| \\pi_{anchor}).$"}, {"title": "6. Experiments", "content": "We test J-BOND on relevant use cases with the following main goals. First, we ablate and showcase important aspects of J-BOND: the benefits of the EMA anchor, and the effects of the anchor speed and the additional KL regularization. Then, we compare J-BOND to classical RLHF baselines using REINFORCE, demonstrating its efficacy and better performance.\nSetup. We consider Gemma (2B and 7B) models (Gemma Team, 2024) which we aim to fine-tune into better conversational agents. For this task, we consider a set of conversational prompts D, a reference policy $\\pi_{ref}$ previously supervised fine-tuned on similar prompts, and a large previously trained reward model r(\u00b7). We use a batch size 128 and the Adam optimizer (Kingma and Ba, 2015) with learning rate 3e\u00af6 and 100 warm-up steps. For the Jeffreys divergence objective, we set $\\beta = 0.5$."}, {"title": "7. Related Work", "content": "Best-of-N was introduced in Stiennon et al. (2020) as a straightforward but costly inference method to optimize language generation against a given reward function. Further works established and refined an analytical form for the KL divergence against the reference (i.e., Bol) policy (Hilton, 2023; Beirami et al., 2024), provided an estimator for the average Best-of-N reward (Nakano et al., 2021), made theoretical connections with KL-constrained RL (Yang et al., 2024) and provided scaling laws for Best-of-N alignment (Gao et al., 2023).\nMatching Best-of-N for improved alignment is a strategy that was studied in different flavors in the literature. Dong et al. (2023) and Touvron et al. (2023) propose to fine-tune LLMs in a supervised fashion on Best-of-N data actually applying forward KL minimization. Concurrently to ours, Gui et al. (2024) proposes to mimic the Best-of-N policy by applying a combination of supervised fine-tuning on best responses and direct preference optimization on best-and-worst response pairs. The latter is similar to a common strategy in online preference optimization methods: Guo et al. (2024) use pairwise AI feedback on online generations to obtain online preferences that are then optimized, while Calandriello et al. (2024) use a dedicated preference reward model instead. Concurrently and closest to our work, Amini et al. (2024) also apply distribution matching in order to get the benefits of Best-of-N sampling with amortized cost. While their formalization is identical, we opt for a different divergence (i.e., Jeffreys) than the one they use (i.e., only backward KL), and propose an iterative procedure with dynamic anchor, which we show critical for optimal results. Best-of-N can also be used for self-improvement in reward modeling, as evidenced in Pace et al. (2024).\nUsing a contrastive advantage is an option of J-BOND studied in prior works as well, which replaced a value estimate by the average Monte Carlo return of other samples. This was applied in the context of REINFORCE (Kool et al., 2019; Pinto et al., 2023), for online RLHF (Ahmadian et al., 2024), offline RLHF (Flet-Berliac et al., 2024) and preference optimization (Wu et al., 2024).\nExponential moving average (EMA) of policy as reference in regularization, which we use in J-BOND, is an increasingly popular option. While most alignment approaches use a static anchor, dy-namic anchors bring the benefit of improving the flexibility of the policy space being explored (Munos et al., 2023; Gorbatovski et al., 2024; Ram\u00e9 et al., 2024), with the caveat that too slow updates limit optimization and too fast updates hinder stability."}, {"title": "8. Conclusion", "content": "We introduce BOND, a novel RLHF method that fine-tunes the policy via online distillation of the Best-of-N sampling distribution. We propose a concrete algorithm, J-BOND, that integrates multiple components to enhance its practicality and efficiency; Monte-Carlo quantile estimation, a combina-tion between forward and backward KL divergence objectives, and an iterative procedure with an exponential moving average anchor. J-BOND improves the KL-reward Pareto front of solutions, and compares favorably against state-of-the-art baselines. We hope this work can help improve alignment of AI systems, making them safer and more reliable."}, {"title": "A. Supporting results and derivations", "content": "A.1. Proof of Theorem 1\nConsider N random generations $y_{1}, y_{2}, ..., y_{N}$ from $\\pi_{ref}$ and an arbitrary generation y among them. Let $A_{i}(y)$ denote the event that y is the best sample (i.e., r(y) $\\geq$ r($y_{i}$) for all i) and that i is the lowest index for which $y_{i}$ = y. It is trivial to see that the the events {$A_{i}(y)$}$_{i=1,2,...,N}$ are disjoint and that their union corresponds to y being selected by Best-of-N sampling.\nThe event $A_{i}(y)$ occurs if and only if three conditions are met: r($y_{j}$) < r(y) for all j < i, $y_{i}$ = y, and r($y_{j}$) $\\leq$ r(y) for all j < i. This allows us to derive the likelihood of the event $A_{i}(y)$:\n$P[A_{i}(y)] = \\prod_{j=1}^{i-1} P[r(y_{j}) < r(y)] \\times \\pi_{ref}(y) \\times \\prod_{j=i+1}^{N} P[r(y_{j}) \\leq r(y)]$\n= $p_{<}(y)^{i-1} \\times \\pi_{ref}(y) \\times p_{\\leq}(y)^{N-i-1}$.\nThe likelihood that Best-of-N sampling selects the generation y is then given by\n$\\pi_{BoN}(y) = \\sum_{i=1}^{N} P[A_{i}(y)]$\n= $\\sum_{i=1}^{N} [p_{<}(y)^{i-1} \\times \\pi_{ref}(y) \\times p_{\\leq}(y)^{N-i}]$\n= $\\pi_{ref}(y) \\times \\sum_{i=1}^{N} [p_{<}(y)^{i-1} \\times p_{\\leq}(y)^{N-i}]$\n= $\\pi_{ref}(y) \\times p_{\\leq}(y)^{N-1} \\times \\sum_{i=1}^{N}  \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)}$\nA.2. Link to the continuous case\nA noteworthy observation is that we can relate the Best-of-N expression to the case of a continuous distribution, in which case the term (B) in Equation (4) is constant and equal to N (which is intuitively natural as $p_{<}(y)$ and $p_{\\leq}(y)$ have the same value in this case).\nIndeed, recall that the probability for a sequence y to be drawn from the Best-of-N distribution is\n$\\pi_{BoN}(y) = \\pi_{ref}(y) \\times p_{\\leq}(y)^{N-1} \\times \\sum_{i=1}^{N}  \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)}$\nHere, y is a discrete variable, as it lives in $[[1;T]]^{L}$ where T is the number of tokens and L is the maximum length of a sequence.\nNow, we show why Equation (20) matches the classic formula for the max of N continuous variables. Formally, let X be a real valued random variable with density $f_{X}$ and a cumulative distribution function $F_{X}$. Taking $Y_{1}, ..., Y_{N}$ i.i.d. variables with the same density, define $X_{N}$ = max{$Y_{1}, ..., Y_{N}$} as the maximum over the N variables. Then, we have that\n$F_{X_{N}} (y) = P(Y_{1} \\leq y, ... Y_{N} \\leq y) = F_{X}(y)^{N}$"}, {"title": "BOND: Aligning LLMs with Best-of-N Distillation", "content": "and thus\n$f_{X_{N"}]}, {"as": "n$\\nabla_{\\pi"}, {"into": "n$E_{y \\sim \\pi} [\\nabla_{\\pi} log \\pi(y)(log \\pi(y) - log \\pi_{bon}(y))]$\n= $E_{y \\sim \\pi} [\\nabla_{\\pi} log \\pi(y) log {\\pi(y) - log \\pi_{ref} (y) - (N-1) log p_{\\leq} (y) - log \\sum_{i=1}^{N}  \\frac{p_{<}(y)^{i-1}}{p_{\\leq}(y)"}, {}]