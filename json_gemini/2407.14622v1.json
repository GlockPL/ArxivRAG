[{"title": "Recall that rJ-BOND(\u00b7) is meant to approximate the the true reward function rbond (y) = log p\u2264 (\u00b7) which", "authors": ["Pier Giuseppe Sessa", "Robert Dadashi", "L\u00e9onard Hussenot", "Johan Ferret", "Nino Vieillard", "Alexandre Ram\u00e9", "Bobak Shariari", "Sarah Perrin", "Abe Friesen", "Geoffrey Cideron", "Sertan Girgin", "Piotr Stanczyk", "Andrea Michi", "Danila Sinopalnikov", "Sabela Ramos", "Am\u00e9lie H\u00e9liou", "Aliaksei Severyn", "Matt Hoffman", "Nikola Momchev", "Olivier Bachem"], "abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-\nthe-art large language models. Yet, a surprisingly simple and strong inference-time strategy is Best-of-N\nsampling that selects the best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant\ncomputational overhead at inference time. Specifically, BOND is a distribution matching algorithm that\nforces the distribution of generations from the policy to get closer to the Best-of-N distribution. We\nuse the Jeffreys divergence (a linear combination of forward and backward KL) to balance between\nmode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving\nanchor for efficiency. We demonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning Gemma policies with\nBOND outperforms other RLHF algorithms by improving results on several benchmarks.", "sections": [{"title": "1. Introduction", "content": "State-of-the-art large language models (LLMs) such as Gemini (Gemini Team, 2023; Reid et al., 2024)\nand GPT-4 (OpenAI, 2023) are generally trained in three stages. First, LLMs are pre-trained on\nlarge corpora of knowledge using next-token prediction (Radford et al., 2018, 2019). Second, the\npre-trained models are fine-tuned to follow instructions via supervised fine-tuning (SFT) (Raffel et al.,\n2020; Wei et al., 2022). Lastly, reinforcement learning from human feedback (RLHF) (Christiano\net al., 2017; Ziegler et al., 2019; Stiennon et al., 2020) is used to further increase the quality of\ngenerations. The RLHF step generally consists of learning a reward model (RM) (Ouyang et al.,\n2022) on human preferences and then optimizing the LLM to maximize predicted rewards using\nreinforcement learning algorithms.\nRLHF algorithms and their challenges. Fine-tuning LLMs with reinforcement learning (RL) is\nchallenging (Casper et al., 2023), notably since it can cause forgetting (French, 1992) of pre-trained\nknowledge, and since loopholes in the RM (Clark and Amodei, 2016; Pan et al., 2022) can cause\nreward hacking (Askell et al., 2021; Skalse et al., 2022). The standard strategy is to use policy-gradient\nmethods (Williams, 1992) with KL regularization towards the SFT policy. Those RL algorithms seek\nPareto-optimal policies with high reward at low KL, to preserve the general capabilities of the original\nmodel and tackle the misalignment (Ngo et al., 2022) concerns.\nBest-of-N sampling. In practice, a surprisingly simple inference-time approach is often used to\nimprove the quality of generations: Best-of-N sampling (Stiennon et al., 2020). It consists of drawing\nN candidate generations from the reference (typically, supervised fine-tuned) model and selecting\nthe one with the highest reward according to the RM. This strategy empirically achieves excellent\nreward-KL trade-offs (Nakano et al., 2021; Gao et al., 2023; Touvron et al., 2023) but increases the\ncomputational cost by a factor of N."}, {"title": "2. Problem Setup", "content": "We consider a LLM based on the transformer (Vaswani et al., 2017) architecture, defining a policy\n$\\pi(x, \\cdot)$ by auto-regressively generating token sequences y from the prompt x. Given a pre-trained and\ntypically supervised fine-tuned reference policy $$\\pi_{\\text{ref}}$$, we seek to further align it to human preferences.\nTo achieve this, throughout the rest of the paper we assume access to a reward model (RM) which we\ndenote as r(\u00b7), trained to reflect human preferences.\nStandard RLHF. Most RL algorithms optimize a linear combination of the expected reward and a KL"}, {"title": "3. The BOND Approach", "content": "We formulate the BOND approach in two main steps. First, we derive an analytical expression for the\nBest-of-N distribution (Section 3.1). Second, using the derived expression, we phrase the problem as\na distribution matching problem (Section 3.2), i.e., we want to steer the policy closer to the Best-of-N\ndistribution. In Section 3.3, we draw insightful connections between BOND and standard RLHF."}, {"title": "3.1. The Best-of-N distribution", "content": "In this section, we derive the exact analytical distribution of Best-of-N sampling and study its properties.\nFor simplicity, we drop the context x from all notation without loss of generality and assume that\nthe reward r(y) induces a strict ordering on all generations y\u00b9. We can affirm the following main\ntheorem (proof in Appendix A.1).\nTheorem 1. For any generation y, let\n$p_{<(y)} = \\mathbb{P}_{y'\\sim\\pi_{\\text{ref}}} [r(y') < r(y)]$   (2)\ndenote the probability that a random generation y' from $\\pi_{\\text{ref}}$ is strictly worse than y and let\n$p_{\\le (y)} = \\mathbb{P}_{y'\\sim\\pi_{\\text{ref}}}[r(y') \\le r(y)],$   (3)\nthe probability that y' is not better than y (thus including the equality case). Then, the probability that\ny is the output of Best-of-N sampling is given by\n$\\pi_{\\text{BoN}}(y) = \\pi_{\\text{ref}}(y) \\times p_{\\le}(y)^{N-1} \\times \\left( 1 + \\sum_{i=1}^{N} \\frac{p_{<}(y)^{i-1}}{p_{\\le}(y)} \\right).$   (4)"}, {"title": "Interpretation.", "content": "Theorem 1 provides an intuitive explanation on the behavior of Best-of-N sampling:\nit essentially reweights the original sampling distribution $\\pi_{\\text{ref}}$, by the multiplicative terms (A) and (B).\nThe term (A) corresponds to a penalty exponential in N based on the fraction of generations (for the\nsame prompt) that are worse or equal to the considered generation y. Intuitively, this ensures that\nwe sample exponentially less from bad generations when increasing N.\nThe term (B) is an additional correction factor due to the potential of collisions among generations.\nImportantly, it is at most linear in N as it is always bounded within [1, N]:\n$1 \\le 1+\\sum_{i=2}^{N} \\frac{p_{<}(y)}{p_{\\le}(y)} \\left(\\frac{p_{<}(y)}{p_{\\le}(y)}\\right)^{i-1}  \\le \\sum_{i=1}^{N} 1 \\le N$   (5)"}, {"title": "3.2. The BOND objective", "content": "The analytical characterization of the Best-of-N distribution allows us to formulate BOND as a distribu-\ntion matching problem. That is, we want to solve the objective:\n$\\pi_{\\text{BOND}} = \\arg \\min_{\\pi \\in \\Pi} D \\left(\\pi || \\pi_{\\text{BoN}}\\right),$   (6)\nwhere $D(\\cdot || \\cdot)$ is a divergence metric steering the training policy $\\pi$ towards $\\pi_{\\text{BoN}}$. For this, a toolbox of\npossible divergences exist in the literature including, e.g., forward and backward KL (Kullback, 1959).\nMoreover, we can employ existing distribution matching techniques to estimate D from online and\noffline samples. We defer the choice of suitable divergences and resulting BOND algorithms to Section 4."}, {"title": "3.3. Connection with standard RLHF", "content": "In this section, we draw important connections between the two seemingly different objectives of\nstandard RLHF (Equation (1)) and BOND (Equation (6)).\nIt is well known (see, e.g., Vieillard et al. (2020); Rafailov et al. (2023)) that the policy maximizing\nthe RLHF objective from Equation (1) is:\n$\\pi_{RL}(y) \\propto \\pi_{\\text{ref}}(y) \\exp\\left(\\frac{r(y)}{\\beta_{RL}}\\right).$   (7)\nFrom the derived expression of $\\pi_{\\text{BoN}}$ in Theorem 1, we see that the Best-of-N sampling distribution\ncoincides with the optimal solution of standard RLHF when using the following specific BOND reward:\nr_{\\text{BOND}}(y) = \\log p_{\\le}(y) + \\frac{1}{N-1} \\sum_{i=1}^{N-1} \\log \\left(1 + \\frac{p_{<}(y)}{p_{\\le}(y)}\\right) ,   (8)\nand the specific regularization strength $\\beta_{\\text{BOND}} = \\frac{1}{N-1}$. The term (B) corresponds to the correction\nfactor in Theorem 1, which is bounded in [0, $\\log N$] for all generations y. Instead term (A) lies in\n(-\u221e, 0]. This provides two interesting insights for Best-of-N sampling:"}, {"title": "4. BOND Challenges and Algorithms", "content": "Implementing the BOND approach induces the three following challenges: (1) how to estimate the\nreward quantiles, (2) which is the appropriate divergence metric to use, and (3) how to choose the\nhyperparameter N representing the number of sampled generations in Best-of-N. We discuss and\naddress these challenges in the next three subsections."}, {"title": "4.1. Monte-Carlo quantile estimation", "content": "One key difficulty in estimating the $\\pi_{\\text{BoN}}$ distribution is that we need to estimate the quantile\n$p_{\\le}(y) = \\mathbb{P}_{y'\\sim\\pi_{\\text{ref}}} [r(y') \\le r(y)],$   (9)\nof a given generation y. The quantile $p_{\\le}(y)$ measures the quality of y compared to generations from\n$\\pi_{\\text{ref}}$ when conditioned on the same prompt (recall that we have suppressed the conditioning on x\nin our notation). A very simple but effective quantile estimation method is Monte-Carlo sampling,\nsampling k generations from $\\pi_{\\text{ref}}$ and obtaining the following empirical estimate:\n$\\hat{p}_{\\le}(y) = \\frac{1}{k} \\sum_{i=1}^{k} \\mathbb{I}{r(y^i) \\le r(y)}.$   (10)\nWe found this to be a very effective in our experiments, even with a limited number of samples. In\nprinciple, though, one could also use alternative approaches, e.g., training a learned quantile model\n(as we explore in Appendix B.1)."}, {"title": "4.2. Jeffreys divergence as a robust objective", "content": "The choice of the divergence metric used in BOND is of crucial importance: different divergences\ncan steer the policy to very different solutions. Here, we propose the Jeffreys divergence as a robust\ndistribution matching objective.\nThe Jeffreys divergence (Jeffreys, 1946) between two distributions is defined as:\n$JB_{\\text{Jeffreys}} (p || q) := (1 - \\beta)\\cdot KL(q || p) +\\beta \\cdot KL(p || q) .$   (11)"}, {"title": "4.3. Iterative BOND", "content": "Finally, we discuss the choice of the parameter N. In practice, choosing N may be difficult for three\nmain reasons: (1) As in standard RLHF, N plays the role of regularization (see Section 3.3): a large\nN improves downstream performance, but if N is too large it will eventually cause reward over\noptimization (Gao et al., 2023). (2) The larger the N the more the estimate of Bon is sensitive\nto errors in the estimated quantiles (since $\\pi_{BoN}(y) \\propto p_{<}(y)^{N-1}$). (3) Estimating the forward KL\ndivergence requires sampling from $\\pi_{\\text{BoN}}$ which is prohibitive for large N.\nTo address the above challenges, we propose the iterative BOND approach. The approach is inspired by\nthe fact that Best-of-N sampling from a Best-of-N distribution, coincides with Best-of-$N^2$ sampling from\nthe original distribution. More generally, by informally defining BoN(\u00b7) as an operator that performs\nBest-of-N sampling from a base distribution, we have:\n$\\text{BoN}(\\text{BoN}(\\text{BoN}(\\pi_{\\text{ref}}))) = \\text{BoN}^{M}(\\pi_{\\text{ref}}).$   (16)\nThis suggests the key idea behind iterative BOND: if we know how to distill the Best-of-N distribution\n(i.e., via BOND), then we can apply BOND recursively (say M times), equivalently to distilling a Best-\nof-$N^M$ of the initial distribution $\\pi_{\\text{ref}}$. This allows fixing a small n (i.e., n = 2) and running BOND (with\nN = n) in an iterative fashion, as an improvement operator. For this, we can introduce an auxiliary\nanchor policy $\\pi^{\\text{anchor}}$ initialized as $\\pi_{\\text{ref}}$. We can then run BOND against $\\pi^{\\text{anchor}}$ (i.e., we can distill the\nBest-of-n version of $\\pi^{\\text{anchor}}$) and, after a given number of distillation steps, update $\\pi^{\\text{anchor}}$ to be the\ncurrent training policy $\\pi_t$. The overall approach is depicted in Figure 3 and summarized in Algorithm 1."}, {"title": "5. The J-BOND Algorithm", "content": "In this section we present J-BOND, a concrete and practical BOND algorithm motivated by the results\ndiscussed in the previous sections. We describe its main components below, and summarize it in the\npseudo-code of Algorithm 2.\nJ-BOND follows the template of iterative BOND (Algorithm 1) with n = 2, i.e., it fine-tunes policy $\\pi_t$\nto iteratively distill the Best-of-2 version of a moving anchor $\\pi_{\\text{anchor}}$ initialized as $\\pi_{\\text{ref}}$. The name\nJ-BOND stands for Jeffreys divergence BOND because it uses the Jeffreys divergence as distribution\nmatching objective, i.e., it minimizes $JB_{\\text{Jeffreys}} (\\pi || \\text{Best-of-2}(\\pi^{\\text{anchor}}))$ as defined in Section 4.2.\nMinimal sample complexity. Compared to the BOND algorithms tested in the previous section,\nJ-BOND has a minimal sample complexity: for each prompt in the batch it generates 1 sample from"}, {"title": "Algorithm 2", "content": "The J-BOND algorithm\nInputs: Prompt dataset D, reference policy $\\pi_{\\text{ref}}$, reward r(\u00b7), $\\beta, \\eta \\in [0, 1]$, $\\gamma \\ge 0$.\nInitialize policy and anchor $\\pi_0 = \\pi_{\\text{anchor}_0} = \\pi_{\\text{ref}}$.\nfor t = 0, . . . do\nSample batch of prompts $D_t \\subseteq D$\nFor each $x \\in D_t$: generate: 1 policy sample $y \\sim \\pi_t(x)$ and 2 anchor samples $y_1, y_2 \\sim \\pi_{\\text{anchor}}(x)$\n*/ Forward KL\nExtract Best-of-2 sample: $y_{\\text{Bo2}} = \\arg \\max_{y' \\in \\{y_1, y_2\\}} r(y').$\nCompute forward KL gradient: $G_{\\text{FW}}(x, \\pi_t) = -\\nabla_{\\pi_t} \\log \\pi_t(x, y_{\\text{Bo2}})$\n*/ Backward KL\nCompute $r_{\\text{J-BOND}}(x, y)$ according to Equation (17).\nCompute return: $R(x, y) = r_{\\text{J-BOND}}(x, y) - (\\log \\pi_t(x, y) - \\log \\pi_{\\text{anchor}}(x, y)).$\nCompute backward KL gradient: $G_{\\text{BW}}(x, \\pi_t) = -\\nabla_{\\pi_t} \\log \\pi_t(x, y) \\cdot (R(x, y) - B).$\n*/ Additional KL regularization\nKL regularization gradient: $G_{\\text{Reg}}(x, \\pi_t) = -\\nabla_{\\pi_t} \\log \\pi_t(x, y) \\cdot (\\log \\pi_t(x, y) - \\log \\pi_{\\text{anchor}}(x, y)).$\n*/ Overall policy update: Jeffreys divergence + KL regularization\nUpdate policy weights $\\theta_{t+1}$ with the overall stochastic gradient:\n$E_{x \\sim D_t}[(1 - \\beta)\\cdot G_{\\text{FW}}(x, \\pi_t) + \\beta \\cdot G_{\\text{BW}}(x, \\pi_t) + \\gamma \\cdot G_{\\text{Reg}}(x, \\pi_t)]$\n*/ Update moving anchor\nUpdate anchor weights with EMA: $\\theta_{\\text{anchor}_{t+1}} \\leftarrow (1 - \\eta) \\cdot \\theta_{\\text{anchor}_t} + \\eta \\cdot \\theta_{t+1}.$\nthe policy $\\pi_t$ and 2 samples from the anchor $\\pi_{\\text{anchor}}$. While more anchor samples are generally useful\nfor a better divergence estimation (in Section 4 we used 16 MC samples), autoregressive sampling is\nthe main bottleneck of online RLHF and we have thus opted for a practical approach working with a\nsmall number of samples.\nCrude divergence estimate based on 2 anchor samples. The policy and anchor samples are used to\nobtain a crude estimate of the forward and backward KL components of $JB_{\\text{Jeffreys}} (\\pi || \\text{Best-of-2}(\\pi_{\\text{anchor}}))$\nas described next.\nWe can minimize the forward KL as described in Section 4.2, by doing supervised fine-tuning on the\nbest of the 2 anchor samples. To minimize the backward KL, we utilize the policy gradient-style loss of\nEquation (15) replacing rBOND(y) with a different reward which we denote as $r_{\\text{J-BOND}}(y)$. The reason\nfor this is that when only 2 anchor samples are available, the reward $r_{\\text{BOND}}(y) = \\log p_{\\le}(y)$ would be\nquite uninformative due to $p_{\\le}(y)$ being a very noisy MC estimate. Let y be the policy sample and\n$\\y_1, y_2\\$ be the corresponding anchor samples, we instead define $r_{\\text{J-BOND}}(y)$ as\nr_{\\text{J-BOND}}(y) = \\begin{cases}\n-\\log(16) & \\text{if } r(y) < \\min\\{r(y_1), r(y_2)\\} \\\\\n0 & \\text{otherwise}\\end{cases}   (17)\nThat is, generation y receives a negative reward of \u2013 log(16) if it has worse reward than both anchor\nsamples, while receives 0 reward otherwise. The above definition is motivated by the following two\nmain reasons:\n(i) We negatively reward y only if it is worse than both the anchor samples, to mimic the concavity\nof the ideal (and unknown) reward function $r_{\\text{BOND}} = \\log p_{\\le}(\u00b7).$"}, {"title": "Experiments", "content": "We test J-BOND on relevant use cases with the following main goals. First, we ablate and showcase\nimportant aspects of J-BOND: the benefits of the EMA anchor, and the effects of the anchor speed\nand the additional KL regularization. Then, we compare J-BOND to classical RLHF baselines using\nREINFORCE, demonstrating its efficacy and better performance.\nSetup. We consider Gemma (2B and 7B) models (Gemma Team, 2024) which we aim to fine-tune\ninto better conversational agents. For this task, we consider a set of conversational prompts D, a\nreference policy $\\pi_{\\text{ref}}$ previously supervised fine-tuned on similar prompts, and a large previously\ntrained reward model r(\u00b7). We use a batch size 128 and the Adam optimizer (Kingma and Ba, 2015)\nwith learning rate 3e\u00af6 and 100 warm-up steps. For the Jeffreys divergence objective, we set $\\beta$ = 0.5."}, {"title": "7. Related Work", "content": "Best-of-N was introduced in Stiennon et al. (2020) as a straightforward but costly inference method\nto optimize language generation against a given reward function. Further works established and\nrefined an analytical form for the KL divergence against the reference (i.e., BoI) policy (Hilton, 2023;\nBeirami et al., 2024), provided an estimator for the average Best-of-N reward (Nakano et al., 2021),\nmade theoretical connections with KL-constrained RL (Yang et al., 2024) and provided scaling laws\nfor Best-of-N alignment (Gao et al., 2023).\nMatching Best-of-N for improved alignment is a strategy that was studied in different flavors in the\nliterature. Dong et al. (2023) and Touvron et al. (2023) propose to fine-tune LLMs in a supervised\nfashion on Best-of-N data actually applying forward KL minimization. Concurrently to ours, Gui et al.\n(2024) proposes to mimic the Best-of-N policy by applying a combination of supervised fine-tuning\non best responses and direct preference optimization on best-and-worst response pairs. The latter\nis similar to a common strategy in online preference optimization methods: Guo et al. (2024) use\npairwise AI feedback on online generations to obtain online preferences that are then optimized,\nwhile Calandriello et al. (2024) use a dedicated preference reward model instead. Concurrently and\nclosest to our work, Amini et al. (2024) also apply distribution matching in order to get the benefits of\nBest-of-N sampling with amortized cost. While their formalization is identical, we opt for a different\ndivergence (i.e., Jeffreys) than the one they use (i.e., only backward KL), and propose an iterative\nprocedure with dynamic anchor, which we show critical for optimal results. Best-of-N can also be used\nfor self-improvement in reward modeling, as evidenced in Pace et al. (2024).\nUsing a contrastive advantage is an option of J-BOND studied in prior works as well, which replaced\na value estimate by the average Monte Carlo return of other samples. This was applied in the context\nof REINFORCE (Kool et al., 2019; Pinto et al., 2023), for online RLHF (Ahmadian et al., 2024), offline\nRLHF (Flet-Berliac et al., 2024) and preference optimization (Wu et al., 2024).\nExponential moving average (EMA) of policy as reference in regularization, which we use in\nJ-BOND, is an increasingly popular option. While most alignment approaches use a static anchor, dy-\nnamic anchors bring the benefit of improving the flexibility of the policy space being explored (Munos\net al., 2023; Gorbatovski et al., 2024; Ram\u00e9 et al., 2024), with the caveat that too slow updates limit\noptimization and too fast updates hinder stability."}, {"title": "8. Conclusion", "content": "We introduce BOND, a novel RLHF method that fine-tunes the policy via online distillation of the\nBest-of-N sampling distribution. We propose a concrete algorithm, J-BOND, that integrates multiple\ncomponents to enhance its practicality and efficiency; Monte-Carlo quantile estimation, a combina-\ntion between forward and backward KL divergence objectives, and an iterative procedure with an\nexponential moving average anchor. J-BOND improves the KL-reward Pareto front of solutions, and\ncompares favorably against state-of-the-art baselines. We hope this work can help improve alignment\nof Al systems, making them safer and more reliable."}, {"title": "A. Supporting results and derivations", "content": "A.1. Proof of Theorem 1\nConsider N random generations $y_1, y_2, ..., y_n$ from $\\pi_{ref}$ and an arbitrary generation y among them.\nLet $A_i(y)$ denote the event that y is the best sample (i.e., r(y) \u2265 r($y_i$) for all i) and that i is the lowest\nindex for which $y_i$ = y. It is trivial to see that the the events $\\{A_i(y)\\}_{i=1,2,...,n}$ are disjoint and that their\nunion corresponds to y being selected by Best-of-N sampling.\nThe event $A_i(y)$ occurs if and only if three conditions are met: r($y_j$) < r(y) for all j < i, $y_i$ = y, and\nr($y_j$) < r(y) for all j < i. This allows us to derive the likelihood of the event $A_i(y)$:\n$\\mathbb{P}[A_i(y)] = \\left[\\prod_{j=1}^{i-1} \\mathbb{P}[r(y_j) < r(y)] \\right] \\times \\pi_{ref}(y) \\times \\left[\\prod_{j=i+1}^{N} \\mathbb{P}[r(y_j) \\le r(y)] \\right]$\n= $p_{<}(y)^{i-1} \\times \\pi_{ref}(y) \\times p_{\\le}(y)^{N-i-1}$.\nThe likelihood that Best-of-N sampling selects the generation y is then given by\n$\\pi_{BoN}(y) = \\sum_{i=1}^{N} \\mathbb{P}[A_i(y)]$\n= $\\sum_{i=1}^{N} [p_{<}(y)^{i-1} \\times \\pi_{ref}(y) \\times p_{\\le}(y)^{N-i}]$\n= $\\pi_{ref}(y) \\times \\sum_{i=1}^{N} [p_{<}(y)^{i-1} \\times p_{\\le}(y)^{N-i}]$\n= $\\pi_{ref}(y) \\times p_{<}(y)^{N-1} \\times \\sum_{i=1}^{N} \\left[  \\frac{p_{<}(y)}{p_{\\le}(y)} \\right]^{i-1}$.\n$\\square$\nA.2. Link to the continuous case\nA noteworthy observation is that we can relate the Best-of-N expression to the case of a continuous\ndistribution, in which case the term (B) in Equation (4) is constant and equal to N (which is intuitively\nnatural as $p_{<}(y)$ and $p_{\\le}(y)$ have the same value in this case).\nIndeed, recall that the probability for a sequence y to be drawn from the Best-of-N distribution is\n$\\pi_{BoN}(y) = \\pi_{ref}(y) \\times p_{\\le}(y)^{N-1} \\times \\sum_{i=1}^{N} \\left[  \\frac{p_{<}(y)}{p_{\\le}(y)} \\right]^{i-1}$.  (20)\nHere, y is a discrete variable, as it lives in $[[1;T]]^L$ where T is the number of tokens and L is the\nmaximum length of a sequence.\nNow, we show why Equation (20) matches the classic formula for the max of N continuous variables.\nFormally, let X be a real valued random variable with density $f_X$ and a cumulative distribution function\n$F_X$. Taking $Y_1, ...Y_N$ i.i.d. variables with the same density, define $X_N$ = $max\\{Y_1, ...Y_N\\}$ as the maximum\nover the N variables. Then, we have that\n$F_{X_N}(y) = \\mathbb{P}(Y_1 \\le y, ... Y_N \\le y) = F_X(y)^N$,  (21)"}, {"title": "and thus", "content": "$f_{X_N"}, "y) = f_X(y)F_X(y)^{N-1}N$. (22)\nIn Equation (22), we recognize the Best-of-N formula in the case where the correction factor (B) is N.\nFor the term (A), $F_X(y)$ plays the role of $p_{\\le}(y)$, as by definition $F_X(y) = \\mathbb{P}(X \\le y)$. Finally, $f_X(y)$ is\nthe density of X, which is analogous to the probability $\\pi_{ref}(y)$ in the discrete case.\nA.3. Backward KL and policy gradient equivalence\nWe formally show the analogy between the gradient of the backward KL divergence of Equation (14)\nand the standard (e.g., REINFORCE (Williams, 1992)) policy gradient of a KL-regularized RLHF\nproblem with equivalent reward $r_{Bond}$ and regularization $\\beta_{BOND}$.\nThe exact backward KL gradient can be derived as:\n$\\nabla_{\\pi} KL(\\pi || \\pi_{BoN}) = \\nabla_{\\pi} \\mathbb{E}_{y \\sim \\pi} [log\\pi(y) - log\\pi_{BoN}(y)]$\n= $\\nabla_{\\pi} \\sum_{y} \\pi(y) (log\\pi(y) - log\\pi_{BoN}(y))$\n= $\\sum_{y} \\nabla_{\\pi} \\pi(y) (log\\pi(y) - log\\pi_{BoN}(y)) + \\pi(y) \\nabla_{\\pi} log\\pi(y)$\n= $\\sum_{y} \\pi(y) \\nabla_{\\pi} log \\pi(y) (log\\pi(y) - log\\pi_{BoN}(y)) + \\pi(y) \\nabla_{\\pi} log\\pi(y)$\n= $\\mathbb{E}_{y \\sim \\pi}[\\nabla_{\\pi} log \\pi(y)(log\\pi(y) - log \\pi_{Bon}(y)) + \\nabla_{\\pi} log\\pi(y)]$\n= $\\mathbb{E}_{y \\sim \\pi}[\\nabla_{\\pi} log \\pi(y)(log\\pi(y) - log \\pi_{Bon}(y))]$.\nAbove, we have used the product rule of gradient, the rule $\\nabla_{\\pi}\\pi(y) = \\pi(y)\\nabla_{\\pi} log \\pi(y)$ and the fact\nthat $\\mathbb{E}_{y \\sim \\pi}\\nabla_{\\pi} log \\pi(y) = 0$.\nEquivalence with Policy Gradient RL. As anticipated, one can verify that descending the above gradient\nis equivalent \u2013 up to a constant scaling \u2013 to running the RL policy gradient REINFORCE algorithm on\nthe RL objective of Equation 1 with r = BOND and BRL = BBOND. Indeed, we can use the expression for\n$\\pi_{BON}$ to break down the above gradient into:\n$\\mathbb{E}_{y \\sim \\pi}[\\nabla_{\\pi} log \\pi(y)(log\\pi(y) - log \\pi_{BoN}(y))]$\n= $\\mathbb{E}_{y \\sim \\pi} \\left[\\nabla_{\\pi} log \\pi(y) \\left(log \\pi(y) - log \\pi_{ref}(y) - (N-1) log p_{\\le}(y) - log \\sum_{i=1}^{N} \\left[  \\frac{p_{<}(y)}{p_{\\le}(y)} \\right]^{i-1} \\right)\\right]$\n= $\\mathbb{E}_{y \\sim \\pi} \\left[\\nabla_{\\pi} log \\pi(y) \\left(log \\frac{\\pi(y)}{\\pi_{ref}(y)} - \\beta_{Bond}r_{Bond}(y) \\right)\\right]$\n= -(N - 1) $\\mathbb{E}_{y \\sim \\pi}[\\nabla_{\\pi} log \\pi(y)(r_{BOND}(y) - \\beta_{BOND}(log \\pi(y) - log \\pi_{ref}(y)))].$\ngradient used by REINFORCE\nA.4.```json\n\"content\":", "Derivation of J-BOND reward\nHere we provide a theoretical explanation behind the design of the J-BOND reward function discussed\nin Section 5:\nr_{\\text{J-BOND}}(y) = \\begin{cases}\n-\\log(16) & \\text{if } r(y) \\le \\min\\{r(y_1),r(y_2)\\} \\\\\n0 & \\text{otherwise}\\end{cases}"], "content": "is unknown since p<(\u00b7) in general requires knowing the reward distribution of anchor (in J-BOND, we\nonly take 2 samples y1, y2 ~ anchor).\nAs mentioned in Section 5, we designed rJ-BOND(\u00b7) to assign a negative reward only if sample y is\nworse than both the anchor samples, to mimic the concavity of the log quantile log p<(\u00b7). In practice,\nwe did not observe gains when rewarding also the intermediate case. The particular choice of value\n- log(16) is motivated by the following main reason.\nWe want that, when sample y has median reward compared to the anchor rewards' distribution (i.e.,\np<(y) = 0.5), then \u2013 in expectation \u2013 rj-BOND (y) coincides with the true reward rbond(y) = log p\u2264(\u00b7) =\nlog(0.5). For this purpose, let us consider the parametrized function:\nJ-BOND (y) = \\begin{cases}\n\\alpha & \\text{if } r(y) < min\\{r(y_1), r(y_2)\\} \\\\\n0 & \\text{otherwise}\\end{cases}\nNote that the stochasticity of rf-BOND (y) is due to the 2 random anchor samples y1, y2 and its expecta-\ntion can be computed as:\n$\\mathbb{E}_{y_1, y_2 \\sim \\pi_{\\text{anchor}}} [r_{\\text{J-BOND}}(y)] = \\alpha \\cdot \\mathbb{P}[\\{r(y_1) > r(y)\\} \\cap \\{r(y_2) > r(y)\\}] + 0 \\cdot \\mathbb{P}[\\text{``otherwise''}]$  (23)\n= $\\alpha \\cdot (1 - p_{\\le} (y))^2, $  (24)\nwhere we have used the definition of p\u2264 (y) = Py'~[r(y') \u2264 r(y)]. Using the expression above,\nwe can find the \u03b1 for which Ey\u2081\u2082anchor [J-BOND (y)] = BOND(y) when p\u2264 (y) = 0.5:\n$\\alpha \\cdot (1 - 0.5)^2 = log(0.5) \\rightarrow \\alpha = -log(16)$.\nWe illustrate this in Figure 8, where we plot the expected rj-BOND(y) reward and the true reward\nBOND(y) as a function of p<(\u00b7)."}, {"title": "B. Additional Experiments", "content": "B.1. Learned quantile models\nMonte-Carlo quantile estimation (Section 4.1) approximates the reward quantiles by sampling multiple\ntimes from the reference policy tref, for each observed context. While we found it to be very simple"}, {"title": "and effective, it may require many sample for an accurate quantile estimation and, in addition, it", "content": "does not exploit any information about the given context. For instance, assuming we have a good\nquantile estimation for context x and are presented a new context x'. MC quantile estimates treat x'\nindependently from x, although they may have very similar reward quantiles.\nMotivated by this, in this section we explore an alternative approach that aims at learning a context-\ndependent quantile estimator $\\hat{p}_{\\le \\theta}(\\cdot)$, parametrized by parameter \u03b8. The idea is to view quantile\n$p_{\\le}(y)$ as the parameter of a Binomial random variable Z where Z = ${\\mathbb{I}\\{r(y_i) \\le r(y)\\} }$ for $y_i \\sim \\pi_{ref}$. Under\nsuch a view, we can interpret $\\hat{p}_{\\le \\theta}(\\cdot)$ as the output of a binary classifier and train it via maximum\nlikelihood estimation using the standard binary cross-entropy loss (Cover, 1999):\n$\\mathcal{L}(\\theta, x, y) = - \\mathbb{E}_{y' \\sim \\pi_{ref}(x)} [\\log \\hat{p}_{\\le \\theta}(x, y)]_{\\{r(x, y') \\le r(x, y)\\}} + [\\log (1 - \\hat{p}_{\\le \\theta}(x, y))]_{\\{r(x, y') > r(x, y)\\}}]$.  (25)\nWe test such an approach in the abtractive summarization task considered in Section 4. We parametrize\n$\\hat{p}_{\\le \\theta}(\\cdot)$ with a LLM initialized as $\\pi_{ref}$ and fine-tuned using the loss of Equation (25). Simultaneously,\nthe policy $\\pi_t$ is fine-tuned using BOND and utilizing $\\hat{p}_{\\le \\theta, t}(\\cdot)$ as quantile estimator at each step. Notably,\nwe approximate the expectation in Equation (25) via a single sample from $\\pi_{ref}$ for each prompt in the\nbatch. In Figure 9 we report the backward and forward KL divergences between the training policy\nand the $BO_N$ distribution as well as the average log quantiles, and compare them to the ones obtained\nwhen running BOND with MC quantile estimation. Note that in both cases, $\\pi_{BO_N}$ is approximated by\n32 MC samples during evaluation. When using the learned quantile model $\\hat{p}_{\\le \\theta}(\\cdot)$, we observe BOND\nachieves very comparable KL divergences and log quantiles compared to using MC quantile estimaton.\nThis illustrates that the use of learned quantiles is valid and promising, potentially offering interesting\ncomputational advantages in situations where, e.g., $\\hat{p}_{\\le \\theta, t}(\\cdot)$ can be re-used or learned offline with a\nfixed sample budget.\nFinally, we remark that the explored approach is quite naive, and alternative learned quantile models\ncan definitely be derived, e.g., further enforcing ordering in the predicted quantiles, using quantile\nregression (Dabney et al., 2017), or assuming a pre-specified (e.g., Gaussian) rewards' distributions.\nB.2. Additional plots for BOND with Jeffreys divergence objective\nWe provide additional experiments that complement the ablation of Section 4.2 when running BOND\nwith a Jeffreys divergence objective $JB_{\\text{Jeffreys}}$ for different values of $\\beta$. In particular, in Figure 10 we\nshow BOND results when using N = 4 (top plots) and N = 16 (bottom plots). Results are consistent\nwith what discussed in Section 4.2: When using $\\beta$ = 0.5, BOND minimizes both backward and forward\nKL divergences from $\\pi_{BoN}$ compared to when solely one of them is used as BOND objective. Moreover,\nit optimizes the log quantiles similarly to $\\beta$ = 1."}]