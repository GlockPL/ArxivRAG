{"title": "InfoDisent: Explainability of Image Classification Models by Information Disentanglement", "authors": ["\u0141ukasz Struski", "Jacek Tabor"], "abstract": "Understanding the decisions made by image classification networks is a critical area of research in deep learning. This task is traditionally divided into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc methods, such as GradCam, aim to interpret the decisions of pre-trained models by identifying regions of the image where the network focuses its attention. However, these methods provide only a high-level overview, making it difficult to fully understand the network's decision-making process. Conversely, intrinsic methods, like prototypical parts models, offer a more detailed understanding of network predictions but are constrained by specific architectures, training methods, and datasets.\nIn this paper, we introduce InfoDisent, a hybrid model that combines the advantages of both approaches. By utilizing an information bottleneck, InfoDisent disentangles the information in the final layer of a pre-trained deep network, enabling the breakdown of classification decisions into basic, understandable atomic components. Unlike standard prototypical parts approaches, InfoDisent can interpret the decisions of pre-trained classification networks and be used for making classification decisions, similar to intrinsic models. We validate the effectiveness of InfoDisent on benchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford Dogs for both convolutional and transformer backbones.", "sections": [{"title": "Introduction and related works", "content": "Machine learning techniques, such as deep neural networks, have become essential tools in various applications including classification, image generation, speech recognition, and natural language processing. These techniques have achieved remarkable effectiveness, often rivaling human capabilities and sometimes surpassing them. A significant recent development in deep learning is the focus on model interpretability, giving rise to a subfield of artificial intelligence known as eXplainable AI (XAI) (Xu et al. 2019).\nInterpreting and understanding model learning is crucial in critical applications such as medicine, image recognition, autonomous driving, etc (Struski et al. 2024; Khan et al. 2001; Bojarski et al. 2017; Samek et al. 2021; Nauta et al. 2023b; Patr\u00edcio, Neves, and Teixeira 2023).\nHistorically, the world of XAI is divided into two disjoint categories: post-hoc interpretability (Ribeiro, Singh, and Guestrin 2016; Lundberg and Lee 2017; Selvaraju et al. 2017), where we analyze the pre-trained model to explain its predictions, and inherently explained models (Chen et al. 2019; B\u00f6hle, Fritz, and Schiele 2022), where the aim lies in building networks which decisions are easy to interpret. Both of the above approaches have their advantages and disadvantages.\nPost-hoc methods In the post-hoc methods, we interpret existing pre-trained network architectures. The commonly used methods like SHAP (Shapley 1951; Lundberg and Lee 2017), LIME (Ribeiro, Singh, and Guestrin 2016), LRP (Bach et al. 2015) or Grad-CAM (Selvaraju et al. 2017) provide in practice only feature importance which can be visualized as a saliency map that shows on which part of the"}, {"title": "Inherently explained models", "content": "While post-hoc methods are easy to implement due to their non-intrusive nature, they often produce biased and unreliable explanations (Adebayo et al. 2018). To address this, recent research has increasingly focused on designing self-explainable models that make the decision process directly visible (Brendel and Bethge 2019; Alvarez Melis and Jaakkola 2018). Many of these interpretable solutions utilize attention mechanisms (Liu et al. 2021; Zheng et al. 2019) or exploit the activation space, such as with adversarial autoencoders (Guidotti et al. 2020).\nAmong the most recent approaches, ProtoPNet (Chen et al. 2019) has significantly influenced the development of self-explainable models. It learns class-specific prototypes, similar to concepts, with a fixed number per class. The model classifies inputs by calculating responses from each class's prototypes and summarizing these responses through a fully connected layer, providing explanations as a weighted sum of all prototypes. This method inspired the development of several other self-explainable models (Donnelly, Barnett, and Chen 2022; Rymarczyk et al. 2021, 2022; Wang et al. 2021; Nauta et al. 2023a; Donnelly, Barnett, and Chen 2022; Wang et al. 2021). Typically, in the prototypical parts model, the final decision of a given class is decomposed into the appearance of a few selected prototypes, which are similar to some strongly localized parts of some chosen images from the dataset. We see that in this approach the final class decision is split into a set of simpler and interpretable compounds."}, {"title": "InfoDisent", "content": "Thus in our opinion, one of the most important problems in XAI is whether it is possible to construct Post-hoc methods that would provide similar functionality to inherently interpretable ones. In particular, similarly to prototypical models, we would like to be able to split the final decisions into simpler interpretable components. Additionally, the ability to explain global predictions (what are the reasons for choosing a given class in general) is extremely desirable.\nIn this paper, we propose a hybrid model InfoDisent, whose aim is to take the best from both worlds and provide a partial positive answer to the above problem. The motivation of InfoDisent was to add the functionality of prototypical models to pre-trained architectures (we test InfoDisent both on convolutional and transformer architectures). To do so InfoDisent disentangles the information from the feature layer into separate prototype channels (similarly to the prototype-based model PIP-Net (Nauta et al. 2023a)). This is achieved by applying the information bottleneck, where we encourage the sparsity of the activation of a given prototype channel. Consequently, InfoDisent constructs interpretations of feature space of pre-trained models, which allow the decomposition of the final decision into simpler atomic components. Consequently, we can explain the decision behind a given class by looking at the most active prototype channels, see Figure 1.\nSummarizing, the contributions of the paper are the following:\n\u2022 we propose InfoDisent, a hybrid model devoted to explainability, which lies in the border between Post-hoc and intrinsic models,\n\u2022 InfoDisent has similar applicability to prototypical parts models, but constructs interpretations for feature space of pre-trained networks,\n\u2022 the final class decision of InfoDisent can be traced back to easily interpretable prototypical channels."}, {"title": "InfoDisent: architecture", "content": "Our motivation comes from the general methodology of prototypical models, which can be described in the following way:\n\u2022 trace the final class decision to the co-occurrence in the image of some prototypes,\n\u2022 each prototype can be visualized by some prototypes from the training dataset,\n\u2022 prototypes should be strongly localized parts of the image.\nWe claim that even for pre-trained models it is possible to disentangle the channels in the feature space in such a way that they will become sufficiently informative to satisfy the above points. In InfoDisent we disentangle the channels in feature space by applying an orthogonal map in the pixel space, and consequently do not change the inner-lying distance and scalar product.\nSince we disentangle the channels in the feature space, we restrict our attention to images in the feature space and consider only the classification head. Thus consider the data set of images in the feature space, which consists of images of possibly different resolutions but the same number of channels d.\nDefault classification head in deep networks To establish notation, let us first describe the typical classification head. In the case of a classification task with k classes, we apply the following operations for the image I in the feature space:\n1. $I \\rightarrow v_I = \\text{avg\\_pool\\_over\\_channels}(I) \\in \\mathbb{R}^d$\n2. $v_I \\rightarrow w_I = Av_I$, where A is a matrix of dimensions d x k\n3. $w_I \\rightarrow p_I = \\text{softmax}(w_I)$."}, {"title": "The sparse pooling features mechanism", "content": "Most interpretable models involve retraining certain parts of the CNN (Chen et al. 2019; Rymarczyk et al. 2022), whereas others, like PIP-Net (Nauta et al. 2023a), retrain the entire CNN. In contrast, our method utilizes a pre-trained CNN or transformer without further modification during training. We first employ a trainable orthogonal transformation U on pixel space to enable the disentanglement of hidden features from feature maps\u00b2. To enforce the disentanglement we follow by an introduced sparse analogue of average pooling over channel K given by\n$K \\rightarrow \\text{mx\\_pool}(K) = \\text{max}(\\text{ReLU}(K))-\\text{max}(\\text{ReLU}(-K))$.\nObserve that to compute mx_pool(K) we need to know only the highest positive and negative pixel values of K, contrary to avg-pool, where all the pixel values are needed. Subsequently, we identify sparse representations (superpixels) within the channels that contribute positively or negatively to predictions. This enables us to generate heatmaps akin to Grad-CAM (Selvaraju et al. 2017) without necessitating a backward model step, as shown in Figure 5a. Importantly, unlike Grad-CAM, our technique supports the visualization of negative heatmaps, resembling the LRP (Bach et al. 2015) method that requires a backward pass in a neural network. Our method operates solely during the forward step."}, {"title": "Classification head in InfoDisent", "content": "Finally, the classification head in InfoDisent is given by:\n1. $I = (I_{rs})_{rs} \\rightarrow J = (UI_{rs})$, where $U : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is an orthogonal matrix and $I_{rs}$ denotes the pixel value of I with coordinates r and s,\n2. $J \\rightarrow v_J = \\text{mx\\_pool\\_over\\_channels}(J) \\in \\mathbb{R}^d$, where for a given channel K we have\n$\\text{mx\\_pool}(K) = \\text{max}(\\text{ReLU}(K)) - \\text{max}(\\text{ReLU}(-K))$,\n3. $v_J \\rightarrow w_J = Av_J$, where A is a matrix with nonnegative coefficients of dimensions d x k\n4. $w_J \\rightarrow p_I = \\text{softmax}(w_J)$\nInfoDisent model Thus InfoDisent consists of two main components: the frozen CNN or transformer Backbone, and InfoDisent classification head, as illustrated in Figure 2.\nThe first component, the CNN or transformer Backbone, is a frozen classical pre-trained network up to the final feature maps layer. Importantly, this part of the network and its weights remain unaltered during the training of InfoDisent, ensuring that the learned feature representations are preserved.\nThe second component involves pooling important features from the final feature maps produced by the Frozen Backbone. This pooling mechanism encourages the network to use information bottlenecks to build maximally informative channels independent of each other. This leads to constructing prototypical channels, which can be easily interpreted. The final element is the fully connected layer, which is distinguished by its positive weight values. This restriction on positivity, except biases handled as in conventional linear layers, ensures that the information about the positive or negative contributions of selected features from the previous part of the model is preserved. This constraint is beneficial for the interpretability of the model's predictions, as it clarifies the contribution of each feature to the final output. Observe, that contrary to some Post-hoc methods InfoDisent need training of the model on the whole dataset."}, {"title": "Understanding the classification decisions", "content": "Prototypes in InfoDisent The crucial consequence of InfoDisent that is disentangles the channels making them interpretable. Thus, similarly to PiPNet (Nauta et al. 2023a), we identify channels as prototypes. To illustrate a given prototype channel, we present five images from the training dataset on which the activation of the channel is the greatest, see Figure 3, where we present consecutive prototypes for the pre-trained ResNet-50 model. This follows from the fact that for better interpretability of model decisions, it is beneficial for humans to be presented from 4 to 9 concepts (Rymarczyk et al. 2022). Formally, similarly as in prototypical models, as the prototypical part, we understand the part of the image corresponding to pixels in feature space with maximal activation, marked by the yellow box in Figure 3. Observe that the presented prototypes seem consistent with each other, and could be well interpreted.\nUnderstanding the Decision for a Given Image by Prototypes Now that we can understand and visualize prototypes, there appears to be a question of how to visualize the prototypes crucial for the decision of the model on the given image. To do this we chose 5 prototypical channels which are most important for the prediction\u00b3 in Figure 4. For each channel, we identified 5 images from the training dataset that exhibit the strongest activation values for that channel, as depicted by the red spots in Figure 5a. In simpler terms, we selected the top 5 images based on the highest activation values, or arg-top5, for each channel.\nThe model's proposed prototypes are easy to interpret. Moreover, unlike current state-of-the-art prototype methods, our model excels at interpreting images from the ImageNet dataset. More examples from various datasets and models are presented in the Supplementary Materials."}, {"title": "Heatmaps", "content": "Our approach, which relies on representation channels, enables us to easily generate heatmaps similar to those produced by the Grad-CAM method, see Figure 5a. To do this we accumulate the activations of all prototypes (both positive and negative ones) over all channels. Since in InfoDisent we use information bottleneck, we obtain more localized results than other standard approaches.\nObserve that, unlike Grad-CAM, our heatmaps also illustrate negative activations, akin to the Layer-Wise Relevance Propagation (LRP) method. While LRP can effectively high-light both positive and negative contributions to the model's"}, {"title": "Understanding the Decision Behind Class", "content": "We examine the model's decision-making process on a per-class basis by utilizing prototypes. To identify prototypes for a given class, we focus on key channels that are prominently activated across all test set images belonging to that class. These key channels are selected based on their consistent presence and strong activation in images of the same class. Once we have identified these crucial channels, we visualize the prototypes as previously described, providing a clear representation of what the model deems important for that particular class.\nFigure 6 illustrates the prototypes for selected classes from the ImageNet dataset. Each prototype captures essential features such as material types, structural elements, or specific textures that are characteristic of the class."}, {"title": "Experiments and Results", "content": "This section outlines the experiments conducted to compare our approach with current state-of-the-art methods, highlighting the most significant results. Our experiments utilized a variety of datasets, including well-established bench-"}, {"title": "Classification Performance", "content": "To compare our approach, we selected several state-of-the-art, interpretable models based on the same CNN architectures as our approach. We categorized the models into a few groups based on their CNN architecture \u2013 ResNet-34/50 (He et al. 2016), DenseNet-121 (Huang et al. 2017), and ConvNeXt-Tiny (Liu et al. 2022b) \u2013 and the experiments we conducted on these datasets. Specifically, we performed two experiments: the first used cropped images for training and testing, and the second used full images.\nIn the first experiment, we utilized two key datasets: CUB-200-2011 and Stanford Cars, which are frequently employed in prototype model evaluations. We trained both the"}, {"title": "Multi-dimensional analysis", "content": "In the last experiment, we utilized the FunnyBirds (Hesse, Schaub-Meyer, and Roth 2023) dataset to evaluate our approach. The FunnyBirds dataset, along with our novel automatic evaluation protocols, supports semantically meaningful image interventions, such"}, {"title": "Conclusions and limitations", "content": "In this paper, we introduced InfoDisent, an innovative model that integrates the strengths of both Post-hoc and inherently interpretable methods. InfoDisent offers a dual approach to interpretation by providing both local explanations (per image) and global insights (per class), thereby addressing key limitations of existing models. It builds upon the post-hoc interpretation of pre-trained networks while incorporating elements from prototypical methods, allowing for a deeper understanding of the feature space.\nIt should be observed that InfoDisent has some important limitations. To construct the model one needs to train the InfoDisent head on the whole dataset. The decisions of InfoDisent are formally not reduced to a small number of prototypes as in prototypical parts models. Finally, we provide interpretations to frozen feature space with InfoDisent classification head and not to the full original model."}, {"title": "Architectural Adjustments for Training", "content": "In this section, we will provide a detailed explanation of the training process for the InfoDisent network. Recall that the first component of the network is the CNN or Transformer Backbone, a pre-trained classical model that is frozen up to the final feature mapping layer. During InfoDisent training, this backbone, along with its weights, remains unchanged, preserving the learned feature representations.\nThe second component of the network involves combining essential features from the final feature mapping produced by the Frozen Backbone. This combination mechanism encourages the network to leverage information bottlenecks, thereby constructing maximally informative and independent channels. These channels, known as prototype channels, are designed to be easily interpretable. The final component is a fully connected layer characterized by positive weight values.\nTo maximize the extraction of information from these channels, we introduce an information bottleneck within our model architecture. This is achieved by applying the $\\text{arg max}$ operation to individual channels, see Figure 9. While the $\\text{arg max}$ function can be used to extract a sparse representation from feature maps, our goal is to enable the model to learn to select the most important values. To achieve this, we require a differentiable $\\text{arg max}$ function. The ideal solution for this is the Gumbel-Softmax estimator (Jang, Gu, and Poole 2016). Given $x = (x_1,...,x_D) \\in \\mathbb{R}^D$ and $\\tau\\in (0,0)$,\\nGumbel-Softmax(x, t) = (y\u2081, \u2026\u2026\u2026, $y_D) \\in \\mathbb{R}^D$, where\n$y_i = \\frac{\\text{exp} ((x_i + n_i)/\\tau)}{\\sum_{d=1}^{D} \\text{exp} ((x_d + n_d)/\\tau)}$,\nand $n_d$ for $d \\in \\{1, . . ., D\\}$ are samples taken from the standard Gumbel distribution.\nThe Gumbel-Softmax distribution serves as an interpolation between continuous categorical densities and discrete one-hot encoded categorical distributions, with the discrete form being approached as the temperature $\\tau$ decreases within the range of [0.1, 0.5]. In our experiments, we initialized $\\tau$ at 1 and progressively reduced it to 0.2. Finally, at the end of the training, we applied a hard softmax.\nFollowing the extraction of key features using the sparse operation - specifically, the $\\text{arg max}$ operation via the Gumbel-Softmax trick - we preserve the original structure of the network's output, maintaining the classical form of the convolutional network's output, as shown in Figure 9. During the subsequent aggregation of positive and negative features, we utilize an average pooling operation to consolidate the information. This approach ensures that the pooled features capture a balanced representation of the activations, contributing to a robust final output."}, {"title": "Details of the experiments performed", "content": "Datasets In our experiments, we leveraged several diverse datasets to evaluate performance. The first dataset is the Caltech-UCSD Birds-200-2011 (CUB-200-2011)(Wah et al. 2011), which contains 11,788 images meticulously labeled across 200 bird species, divided into 200 subcategories. Of these, 5,994 images are allocated for training, while 5,794"}, {"title": "Training Details", "content": "We train the architectures using stochastic gradient descent (SGD) with standard categorical cross-entropy loss. The momentum, damping, and weight decay are set to 0.9, 0.9, and 0.001, respectively. For the baseline networks, the initial learning rates are 0.1, 0.05, and 0.01, which are reduced by a factor of 0.1 when the validation loss converges. In our approach, we train only the last two segments of the network, thus we use lower learning rates of 0.001 and 0.0001, utilizing the 'ReduceLROnPlateau' (Al-Kababji, Bensaali, and Dakua 2022) mechanism that reduces the learning rate when the cost function stops improving. All numerical experiments were conducted using NVIDIA RTX 4090 and NVIDIA A100 40 GB graphics cards.\nFor cropped images, we follow previous studies (Chen et al. 2019) by applying on-the-fly data augmentations (e.g., random rotation, skew, shear, and left-right flip) on the cropped CUB and cropped Cars datasets using the provided bounding boxes. We also validate our method on the full (uncropped) CUB and Dogs datasets, employing the same online data augmentation techniques (e.g., random affine transformation and left-right flip). For the FunnyBirds dataset, we adhered to the detailed instructions provided in the framework's documentation, which can be found at https://github.com/visinf/funnybirds. For training various CNN and transformer models on the ImageNet dataset, we utilized the augmentation techniques described at https://github.com/pytorch/vision/tree/main/references/classification."}, {"title": "Ablation study", "content": "In this part, we present additional results from a series of analyses investigating the significance of the number of channels on model predictions across various datasets and models. The results of these analyses are illustrated in Figures 10 to 13. Recall, that InfoDisent model organizes information into channels with sparse representations, which are later utilized in the model's prediction process. We specifically examine how the number of channels influences the model's predictions by determining the minimum number of channels required to account for at least 95% of the information used in the model's predictions. Formally, if\n$\\text{logits} = \\sum_{i=0}^{N} a_{ki}v_i + b_k$,\nwhere N is the total number of channels, k represents the image class, and $a_{ki}, V_i, b_k \\in \\mathbb{R}$, then for each image from"}, {"title": "Explaining Classification Decision for a Given Image by Prototypes", "content": "InfoDisent employs prototypes, similar to the approach used in PiPNet (Nauta et al. 2023a), to explain individual decisions made on an image. Below, we describe the process for identifying prototypes for a given input image. An average pooling operation makes the aggregation of information from individual channels. The outcome of this operation is a scalar for each channel K, computed as\nmx_pool(K) = max(ReLU(K)) \u2013 max(ReLU(-K))\n(as explained in the main paper). This dense representation, formed by aggregating all the channels, is then processed by a linear layer that outputs logits, see Figure 9. The logits can be represented in a format similar to the output of a convolutional layer, as illustrated by V+ and V_ in Figure 9."}]}