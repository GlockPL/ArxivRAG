{"title": "Impact of Adversarial Attacks on Deep Learning Model Explainability", "authors": ["Gazi Nazia Nur", "Mohammad Ahnaf Sadat"], "abstract": "In this paper, we investigate the impact of adversarial attacks on the explainability of deep learning models, which are commonly criticized for their black-box nature despite their capacity for autonomous feature extraction. This black-box nature can affect the perceived trustworthiness of these models. To address this, explainability techniques such as GradCAM, SmoothGrad, and LIME have been developed to clarify model decision-making processes. Our research focuses on the robustness of these explanations when models are subjected to adversarial attacks, specifically those involving subtle image perturbations that are imperceptible to humans but can significantly mislead models. For this, we utilize attack methods like the Fast Gradient Sign Method (FGSM) and the Basic Iterative Method (BIM) and observe their effects on model accuracy and explanations. The results reveal a substantial decline in model accuracy, with accuracies dropping from 89.94% to 58.73% and 45.50% under FGSM and BIM attacks, respectively. Despite these declines in accuracy, the explanation of the models measured by metrics such as Intersection over Union (IoU) and Root Mean Square Error (RMSE) shows negligible changes, suggesting that these metrics may not be sensitive enough to detect the presence of adversarial perturbations.", "sections": [{"title": "Introduction", "content": "Computational perception capabilities have impacted our daily lives and industries in a revolutionary way. These capabilities include but are not limited to image recognition, speech recognition, augmented reality, sentiment analysis, and natural language processing. In this paper, however, our focus is solely on image recognition.\n\nThe influence of image recognition, powered by computer vision, is profoundly evident in manufacturing. Automated inspection systems significantly enhance product quality by identifying defects in components. Similarly, automated object recognition systems modernize inventory management, reducing manual labor and boosting profitability and productivity. Furthermore, the retail sector also benefits from advancements in customer experience, notably through implementing self-checkout systems.\n\nBefore the emergence of deep learning, computer vision predominantly utilized conventional techniques and algorithms. These methods frequently depended on handcrafted features and rules-based systems for image analysis. Common practices included feature extraction techniques like edge detection to discern important image patterns and structures. These approaches faced difficulties with real-world data's complexity and diversity, relying on handcrafted features and having a restricted ability to learn from vast datasets. Despite such hurdles, these approaches are still applied in the industrial automation, surveillance, and medical imaging sectors.\n\nAlthough our goal in this paper is not to compare deep learning models with hand-crafted algorithms, we still present how hand-crafted algorithms excel with their explainability to demonstrate what explainability means. As an example of explainability, let us consider the problem of receipt classification in the MATLAB image processing onramp (refer to Figures 1"}, {"title": "Related Work", "content": "At least two streams of literature are relevant to the problem we are investigating: Explainable Artificial Intelligence (XAI) and adversarial attacks. Below, we discuss related works in each of these fields.\n\nResearch in XAI aims to enhance the understandability and interpretability of deep learning models, thereby increasing trust in artificial intelligence systems (Hassija et al., 2024). This field examines the extent to which inputs affect changes in outputs. In this paper, we concentrate on post hoc explanation methods. These methods explain how deep learning models decisions after they have been trained (Lopardo et al., 2024). Many of these post hoc explanation methods are gradient-based approaches. A simple example of gradient-based explanation involves analyzing the gradient of the model's output with respect to its input (Simonyan et al., 2014). Noteworthy gradient-based techniques include Gradient-weighted Class Activation Mapping (Grad-CAM)\n\nOn the other hand, research on adversarial attacks takes advantage of the \"black-box\" nature of deep learning models to exploit their vulnerabilities. These attacks involve creating inputs that are visually indistinguishable from humans but can deceive deep learning models such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) and the Basic Iterative Method (BIM) (Kurakin et al., 2018). Moreover, adversarial attacks can compromise model explanations. For instance, Ghorbani et al. (2019) showed that small, random perturbations to input data can significantly disrupt feature importance methods, such as saliency maps and DeepLIFT. Building on this, numerous studies have examined the impact of adversarial attacks on model explanations. These investigations employ various attack strategies, including adding perturbations to inputs (Kindermans et al., 2019; Dombrowski et al., 2019), altering model weights (Heo et al., 2019;\nDimanov et al., 2020), and manipulating both training data and models to carry out more complex backdoor attacks (Viering et al., 2019; Noppel et al., 2023).\n\nDespite significant research on adversarial attacks and explainable AI techniques, there remains a gap in the literature regarding a comprehensive benchmark for evaluating adversarial attacks specifically targeting explanation methods. Liu et al. (2021) developed the XAI-BENCH library, a benchmark focused on explanation techniques such as LIME and SHAP. OpenXAI, a benchmark\naimed at post hoc explanation methods, was introduced by Agarwal et al. (2022). Yuan et al. (2019)\nprovided a detailed project covering a wide range of adversarial attacks and their defense\nmechanisms, detailing the operating principles of advanced attacks and defenses. Baniecki and\nBiecek (2024) conducted a literature review on a broad spectrum of studies addressing adversarial\nattacks on explanation models, including various defense strategies. However, to the best of our\nknowledge, there is still no established benchmark for adversarial attacks on model explanations.\nOur goal is to fill this gap by compiling the results and establishing a benchmark."}, {"title": "Research Methodology", "content": "In this paper, our objective is to systematically compare the explanations of original images with those of adversarial images and to tabulate how these explanations alter when faced with adversarial manipulation. We use the ground truth explanation as a baseline control. First, we evaluate the explanations generated for original images against this control. Next, we perform a similar analysis for adversarial images, evaluating their explanations in relation to the control.\nFinally, we compare the results obtained from these two evaluations. Figure 3 outlines the steps involved in the methodology employed."}, {"title": "Select data and generate ground truth of explanation:", "content": "In this paper, we require image datasets accompanied by their corresponding ground truth explanations. In this context, by ground truth explanations, we imply an annotated part of the image responsible for its classification into a specific category. These annotations can be created by humans or through alternative methods. Please refer to Figure 4 to see sample images of the ground truth of the explanation.\n\nIn this study, we work with a specific subset of the ImageNet dataset (Deng et al., 2009), which\nencompasses 1,000 classes. Our chosen subset, sourced from Na (n.d.), consists of 5 images per\nclass from the ImageNet validation dataset. We have selected images from classes 0 through 40\nand 80 through 120 for our analysis, intentionally excluding classes 41 through 79. This exclusion\nwas due to the presence of snake and lizard images in these classes, which we found to be\nemotionally depressing to handle manually.\n\nNow, we are required to generate the ground truth of explanations of these images. We have used\nthe Segment Anything Model (Kirillov et al., 2023) to develop the ground truth. SAM is itself a\ndeep learning model designed for image segmentation. It combines convolutional neural networks\nwith the attention mechanism (Vaswani et al., 2017) to identify and delineate boundaries of various\nobjects within an image. SAM is capable of segmenting almost all types of objects within a picture."}, {"title": "Select an image classification deep learning model:", "content": "Next, we need to choose a deep learning model capable of classifying (image classification) the datasets. To achieve this, we have the option to develop a model from the ground up or leverage\npre-trained models available within the TensorFlow (Abadi et al., 2016) library.\n\nIn our study, we utilized the pre-trained EfficientNetV2B0 (Tan & Le, 2021) model from the\nTensorFlow library, as described by Tan and Le in 2021. This model achieves a Top-1 accuracy\nof 78.7% (Keras, n.d.). on the ImageNet validation dataset. Top-1 accuracy refers to the model\ncorrectly identifying the true class as its top prediction. EfficientNetV2B0 incorporates several\nadvanced features, such as depth-wise convolution and skip connections. For a detailed\nunderstanding of the model, we refer to Tan & Le (2021). The model comprises a total of 7.2\nmillion parameters, with 7.13 million being trainable, striking a balance between computational\ndemands and efficiency. Moreover, we found that explainable AI techniques were more effective\nwith this model compared to other pre-trained models. All of these led us to choose this model for\nthis study."}, {"title": "Evaluations of original image's explanation:", "content": "Get predictions of the original images using deep learning model:\n\nOnce the deep learning model and dataset have been selected, we can proceed to classify\nthe images into their respective categories using the chosen deep learning model\n(with/without defense).\n\nGet explanations of the original images using an explanation technique:\n\nIn the deep learning literature, there are quite a few techniques to explain why an image is\nclassified into a particular class. Of these techniques, we employ GradCAM (Selvaraju et\nal., 2016), SmoothGrad (Smilkov et al., 2017), and LIME (Ribeiro et al., 2016) separately.\nWe expect these explanation techniques to produce images similar to the ground truth\nshown in Figure 4. Now, we explain the key concepts behind GradCAM, SmoothGrad, and\nLIME.\n\nGradCAM: GradCAM is a gradient-based explanation technique developed by Selvaraju\net al. (2016). This technique is designed to explain the output of CNN models. CNN models\ncan capture spatial information from images, and GradCAM utilizes this feature to identify\nwhich pixels are most influential in determining the model's output. GradCAM computes\nthe gradient of the target class score (the output before activation) with respect to the\nfeature maps of a convolutional layer. This expresses how each pixel affects the class score.\nSubsequently, Global Average Pooling (GAP) is applied to these gradients to generate a\nset of weights for each feature map. These weights are then used to create a weighted\ncombination of the feature maps, producing a heatmap. When this heatmap is overlaid on"}, {"title": "Evaluate comparison metrics between explanations and the ground truth of the explanations:", "content": "Next, we compare the generated explanations with the ground truth explanations. Various\nmetrics can be utilized for this comparison; however, for this study, we use root mean\nsquare error (RMSE) and intersection over union (IoU). These two metrics are defined mathematically as follows:\n\nIf Y and \u0176 represents the ground truth and prediction explanation of an image, then RMSE\nand IoU can be described as\n\n$RMSE = \\sqrt{\\frac{1}{n^2} \\sum_{i} \\sum_{j}(Y_{ij} \u2013 \\hat{Y}_{ij})^2}$ \n$IoU = \\frac{pixels(Y \u2229 \\hat{Y})}{pixels(Y \u222a \\hat{Y})}$"}, {"title": "Evaluations of adversarial images\u2019s explanation:", "content": "Generate adversarial images using an adversarial generator on the data and deep learning model:\n\nIn the deep learning literature, there are two primary categories of adversarial attacks:\ntargeted and non-targeted. In a targeted attack, the goal is to manipulate a model into classifying an adversarial image as a specific, incorrect label chosen by the attacker.\nConversely, a non-targeted attack aims to cause the model to incorrectly classify the adversarial image as any label other than the true one. We use the non-targeted attack methodology to generate the adversarial images.\n\nWe can find a wide range of methods for generating non-targeted adversarial images. However, we use the FGSM (Goodfellow et al., 2014) and the BIM (Kurakin et al., 2018)\nfor generating adversarial images in this study. The key concepts of these adversarial generation methods are described below:\n\nFGSM: Initially, it was believed that the vulnerability of deep learning models to adversarial attacks stemmed from the deep learning model's nonlinearity and tendency to\noverfit. However, Goodfellow et al. (2014) suggested that the real issue lies in their inherent linearity and capacity for generalization. This insight led to the development of the Fast Gradient Sign Method (FGSM), a simple yet effective technique applicable to both\ntargeted and non-targeted attacks. FGSM works by introducing slight perturbations in the\noriginal image, aligned with the gradient of the loss function relative to the model's output.\nThe underlying equation for this is as below:\n\n$X_{adv} = x + \u2208 sign(\u2207_xJ(\u03b8, x, y))$\n\nIn this equation, $X_{adv}$ denotes the adversarial image generated by adding a small perturbation to the original sample x. The model parameter is expressed by @ and the\ncorrect label of the original image is y. The term $\u2207_xJ(\u03b8,x,y)$ captures the gradient of the loss function with respect to x, and e is a small value ensuring the adversarial image\nremains visually indistinguishable to humans.\n\nBIM: This method is built upon FGSM. Kurakin et al., (2018) introduced two iterative methods to enhance attack success rates. Among these, the Basic Iterative Method applies\nthe FGSM multiple times with pixel values clipped to ensure they remain within a specific"}, {"title": "Compare the explanation results between original and adversarial images:", "content": "Finally, we compare original and adversarial images based on the metrics obtained from evaluating explainable AI techniques against the ground truth. By doing so, we get an idea of the adversarial impact on explainable techniques.\n\nWe designed the experiment to include a baseline control measure, addressing the potential inaccuracies in prediction explanations from original images. By employing baseline control, we\nensure a more scientific approach in comparing explanations between original and adversarial images, thus establishing a more rigorous evaluation framework. The results are systematically tabulated to identify any discernible patterns."}, {"title": "Experimental Platform", "content": "Dataset: We utilized a subset of the ImageNet validation dataset (Deng et al., 2009), specifically classes 1-40 and 80-120, as sourced from Na (n.d.). Each of these classes contains 5 images, though we did not use all five images from each class. Instead, we only selected images from these classes for which the ground truth of the explanation was accurately generated by SAM.\n\nDeep learning model: We have used Pre-trained EfficientNetV2B0 (Tan & Le, 2021). This model achieves a Top-1 accuracy of 78.7% (Keras, n.d.). on the ImageNet validation dataset. Top-1 accuracy refers to the model correctly identifying the true class as its top prediction. The model comprises a total of 7.2 million parameters, with 7.13 million being trainable, striking a balance between computational demands and efficiency. The accuracy of the deep learning image classification model on our subset of original images is 89.94%.\n\nComparison metric: We have used RMSE and IoU to compare the generated masks with the ground truth.\n\nSoftware and hardware: We conducted our experiments using Python in the Google Colab Pro environment, equipped with a high RAM (25 GB) configuration and a T4 GPU."}, {"title": "Results and Discussion", "content": "We present our results step by step. Initially, we display sample images alongside their ground truth generated by SAM. Following this, we will show the masks created by various explanation techniques. Subsequently, we demonstrate how adversarial images affect model performance while remaining indistinguishable from human eyes. We then present the masks derived from these adversarial images. Afterward, we compare these masks with the ground truth. Finally, we compile and tabulate the results of these comparisons."}, {"title": "Conclusion", "content": "In this paper, we started the study by selecting a dataset and generating a ground truth explanation\nusing the SAM. Next, we employed GradCAM, SmoothGrad, and LIME techniques to find deep-\nlearning model explanations for the original images. We then created adversarial images using the\nFGSM and BIM methods and obtained explanations for these adversarially generated images as\nwell. The final step involved calculating the IoU and RMSE to compare these explanations against\nthe ground truth.\n\nThe primary aim of this paper was to explore how the explanations of a deep learning model for\nan image classification task are changed in the presence of adversarial attacks and to assess the\neffectiveness of different explanation techniques under adversarial attacks. We observed a\nsignificant drop in the test accuracy of the deep learning model, from 89.94% to 58.73% following\nan FGSM attack, and to 45.50% after a BIM attack. However, we noted no significant changes in\nthe IoU and RMSE metrics for the explanations after the attacks, suggesting that these explanation\nmethods with the combination of IoU and RMSE metrics are not effective in discerning adversarial\ninfluences.\n\nRegarding limitations, we acknowledge that we did not incorporate a comprehensive range of\nadversarial generation methods, metrics, and explanations to establish a complete benchmark.\nAdditionally, the analysis was conducted on a very small subset of data, which restricts the\ngeneralizability of the findings. For future directions, these limitations can be addressed by\nincluding a broader variety of adversarial attack methods, metrics, and explanation techniques, as\nwell as by expanding the dataset used for analysis. Furthermore, benchmarking the impact of\nimplementing various adversarial defense mechanisms presents another promising direction.\n\nThe necessary programming codes, along with images, can be found in the following repository:\nhttps://github.com/ahnafsadat/Explainable_AI_evaluation"}]}