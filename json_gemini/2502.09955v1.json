{"title": "Diverse Inference and Verification for Advanced Reasoning", "authors": ["Iddo Drori", "Gaston Longhitano", "Mao Mao", "Seunghwan Hyun", "Yuke Zhang", "Sungjun Park", "Zachary Meeks", "Xin-Yu Zhang", "Ben Segev", "Howard Yong", "Nakul Verma", "Avi Shporer", "Alon Amit", "Madeleine Udell"], "abstract": "Reasoning LLMs such as OpenAI 01, 03 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.", "sections": [{"title": "1. Introduction", "content": "Reasoning LLMs such as OpenAI 01 (OpenAI, 2024) and o3 (OpenAI, 2025b), as well as DeepSeek R1 (Guo et al., 2025), have led to impressive performance in mathematics, coding, and problem solving. Despite this progress, a single large model or method may struggle with challenging tasks. To address this, diversity, of models and methods for inference, has emerged as a mechanism to increase performance by using complementary strengths.\nWe demonstrate the advantages of diverse inference on three representative and challenging benchmarks:\n\u2022 International Mathematical Olympiad (IMO, 2024) combinatorics problems: We increase the accuracy from 33.3% to 77.8% correct answers.\n\u2022 Abstraction and Reasoning Corpus (ARC) (Chollet, 2019): We solve 80% of puzzles that 948 humans collectively could not solve, and 26.5% of puzzles that 03 high compute could not solve.\n\u2022 Humanity's Last Exam (HLE) (Phan et al., 2025): We increase accuracy from 8% to 37% on this set of questions across mathematics, humanities, social sciences, and others.\nThree key methodological contributions drive these results:\n1. Diverse inference. We aggregate multiple models, methods, and agents at test time rather than relying on a single model or method. Any single correct solution is validated automatically for the verifiable tasks of IMO combinatorics and ARC puzzles. Specifically:\n\u2022 IMO: Using eight different methods (LEAP, Z3, RTO, BON, SC, MoA, MCTS, PV) significantly increases accuracy. We autoformalize English into Lean, enabling perfect verification.\n\u2022 ARC: Synthesized code solutions are verified on training examples as unit tests.\n\u2022 HLE: Using best-of-N as an imperfect verifer, increases the solve rate with increased samples.\n2. Test-time simulations and reinforcement learning. We generate additional problem-specific information at inference time:"}, {"title": "2. Methods", "content": "2.1. Reasoning LLMs\nA foundation model \\(\\pi\\) with pre-trained parameters \\(\\theta\\) defines a conditional distribution:\n\n\\( P_{\\theta}(y | x), \\)\n\nwhere x is a prompt and y is a response. A reasoning model is trained to generate a (hidden) rationale also known as chain-of-thought (CoT) z, so that the joint generation is given by:\n\n\\( P_{\\theta}(z,y|x) = p_{\\theta}(z|x) p_{\\theta}(y | z, x). \\)\n\nModel training consists of two phases: (i) Supervised fine-tuning (SFT): from \\(\\pi\\) to \\(\\pi_{SFT}\\); and (ii) Reinforcement learning (RL): from \\(\\pi_{SFT}\\) to \\(\\pi_{RL}\\).\nSupervised fine-tuning (SFT). Samples are generated using \\(\\pi_{\\theta}\\) in Eq. 1 and stored in a dataset \\( D = \\{(x^i, y^i)\\}_{i=1,...,n} \\). A supervised fine-tuning loss is derived by taking the negative log likelihood of Eq. 1 on the dataset:\n\n\\( L(\\theta) = - \\sum_{(x^i, y^i) \\in D} \\log p_{\\theta}(y^i | x^i). \\)\n\nSimilarly, for a reasoning model, samples are generated using \\(\\pi_{\\theta}\\) in Eq. 2 and stored in a dataset \\( D = \\{(x^i, z^i, y^i)\\}_{i=1,...,n} \\). A supervised fine-tuning loss is derived by taking the negative log likelihood of Eq. 2 on the dataset:\n\n\\( L(\\theta) = - \\sum_{(x^i, z^i, yi) \\in D} \\left[\\log p_{\\theta}(z^i | x^i) + \\log p_{\\theta}(y^i | x^i, z^i)\\right]. \\)\n\nReinforcement learning. For tasks such as solving math problems or generating code, we define a reward function R(x, y) that is checked automatically, by verifying an answer or proof or by running unit tests. We then optimize:\n\n\\( \\underset{\\theta}{\\text{maximum}} \\mathbb{E}_{x\\sim D, y\\sim \\pi_{\\theta}} [R(x, y)]. \\)\n\nThis is a classical RL objective without the need for a learned preference model.\nMore generally, given a foundation model we define a reward:\n\n\\( r(x, y) = f(\\pi_{RM}(x, y)), \\)\n\nwhere y is the resulting output, and f is a function measuring the quality of that output result. For example, using policy gradient, we update \\(\\theta\\) by:\n\n\\( \\nabla_{\\theta} L_{RL} = - \\mathbb{E}_{\\hat{y} \\sim \\pi_{\\theta}(\\cdot |x)} \\left[ r(x, \\hat{y}) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\hat{y} | x)\\right]. \\)"}, {"title": "2.2. Diverse Models and Methods", "content": "We ablate multiple models and methods (Sharma, 2024) at test time on the IMO, ARC, and HLE. The models are described in Appendix R. Each method is described next:\n\u2022 Zero-shot: The problem, as-is, given to the LLM.\n\u2022 Best of N sampling: Generates n candidate responses \\( Y = \\{y_1, y_2,...,y_n\\}, y^\\circ \\sim p(y | x) \\) and selects the best one according to a criterion \\( y^* = \\text{arg}\\underset{y_i \\in Y}{\\text{max}} C(y) \\). Given a verifier and a chain of thought, we perform rejection sampling, by sampling different chains of thought \\( z_n \\sim p(z | x) \\), their responses \\( y^n \\sim p(y | x,z_n) \\) and keeping those responses \\( y^n \\) that are verified.\n\u2022 MCTS (Xie et al., 2024): Typically used to explore the solution space by constructing a search tree. The state transition is \\( s_{t+1} = T(s_t, a_t) \\), a node value is estimated by \\( V(s) = \\frac{N(s)}{N(s)} \\sum_{i=1}^{N(s)} R_i \\), where N(s) is the number of times node s has been visited and \\( R_i \\) is the reward from simulation i. In our context, we perform rejection sampling from an intermediate step in the chain of thought by Monte-Carlo roll outs.\n\u2022 Self-consistency (Wang et al., 2022): Instead of relying on a single response, self-consistency evaluates multiple outputs \\( y^n \\) for the same input x and selects the most common or majority vote response \\( y^* = \\text{Majority Vote}(\\{y^n\\}) \\). This approach enhances the reliability and accuracy of predictions, reducing variability and improving the overall quality of the model's output, however often saturates given sufficient samples."}, {"title": "2.3. Aggregating Diverse Models and Methods", "content": "We aggregate the results of diverse models and methods whose solutions may be perfectly verified as correct by a maximum. Let T = {\\( t_1, t_2,...,t_t \\)} be the set of N IMO problems or ARC puzzles and K the number of models M = {\\( M_1, M_2,..., M_K \\)}, where each \\( M_k \\) attempts to solve each \\( t_i \\in T \\). The indicator is defined\n\\(1, \\) if \\(M_k\\) correctly solves \\(t_i\\),\nby \\( \\mathbb{1}(M_k \\text{ solves } t_i) = \\\n\\(0, \\) otherwise.\nSince we can verify the correctness of each individual solution, for each problem \\( t_i \\), there exists a ground truth validation mechanism indicating whether \\( M_k \\)'s proposed solution is correct. We combine the outputs of all models by taking the logical maximum, i.e., logical OR, over their correctness indicators: \\(\\mathbb{1} \\text{ (any model solves } t_i \\) = \\underset{k \\in \\{1,...,K\\}}{\\text{max}} \\mathbb{1}(M_k \\text{ solves } t_i) \\). Problem \\( t_i \\) is considered solved if and only if at least one method in M solves it. We define the success rate, or accuracy, of the aggregated system across the set T of N problems as:\n\\( \\frac{1}{N}\\sum_{i=1}^N \\underset{k \\in \\{1,...,K\\}}{\\text{max}} \\mathbb{1}(M_k \\text{ solves } t_i) \\). Since a problem is counted as solved if any one of the K models solves it, this aggregation is the best-case scenario. If all models make different systematic errors, this approach substantially improves coverage of solvable problems relative to individual models. If any model's solution is correct for a particular problem, that problem is marked as solved in the aggregated result, giving the maximum performance across diverse models."}, {"title": "2.4. Test-Time Simulations and Reinforcement Learning", "content": "IMO Figure 1 is a high-level architecture of our approach for solving IMO combinatorics problems. See Appendices F-I for details. Our pipeline consists of three components: encoding, simulation and deep reinforcement learning, and decoding. During the encoding phase, we find the answer by formulating the problem into a state space, action space, and rewards (S, A, R). Then, we prompt an LLM to transform the problem into a game environment. We represent the problem as Python code in Gymnasium with an agent and policy. We use simulation and deep reinforcement learning to find an optimal policy. We repeat this process, generating multiple games per problem with different dimensions, generating data and videos of multiple episodes per game. In the decoding phase, we extract data and frames and augment them by transformations. We use LLMs to compose textual representations of each sequence's images and policy explanations in the form of descriptions. Finally, we use this information, along with the problem statement, answer, books and guides as described in Appendices M and N, to auto-formalize a proof by in-context learning. We curated a dataset of all previous IMO ShortList combinatorics problems between 2006-2023 and used a subset for in-context learning of autoformalization. The result is automatically verified in the Lean environment, as shown in Appendix J, and refined to generate a complete and correct proof as shown in Appendix K. Our approach handles combinatorics problems that may be formulated as a game with a state space, action space, and rewards.\nAutoformalization in Lean. In addition to answering and solving in English, we perform cyclic auto-formalization using in-context learning. Given a problem we translate it into formal Lean by in-context example pairs from previous years IMO problems and their corresponding Lean theorems. We auto-verify the Lean code, remove comments, translate the Lean code back to English, and have the LLM compare the original and back-translated problems, verifying that they are mathematically equivalent. Appendix J shows autoformalization examples. The significance of a robust and reliable back-and-forth translation between English and Lean is that it allows for automatic verification of problem statement and proofs. We also verify proofs by an expert Mathematician. Formally, we convert \\(X_{EN}\\) into a Lean formal proof using few-shot learning. Specifically, let \\(\\Phi_{E\\to L}: \\{\\text{English text}\\} \\to \\{\\text{Lean code}\\} \\) be a translation function by M (with in-context examples of English-Lean pairs). We generate \\(X_{Lean} = \\Phi_{E\\to L}(X_{EN}) \\), which is then compiled in Lean. Let Compile(\\(X_{Lean}\\)) be a boolean function indicating if the proof compiles successfully in the Lean environment. To validate that the final Lean theorem matches the original solution, we remove comments or annotations from \\(X_{Lean}\\) to avoid using the original English text that may appear as documentation and get \\(X'_{Lean}\\). We then apply the inverse translation function \\(\\Phi_{L\\to E}: \\{\\text{Lean code}\\} \\to \\{\\text{English text}\\} \\) to obtain a back-translated theorem \\( X'_{EN} = \\Phi_{L\\to E}(X'_{Lean}) \\). Finally, we compare \\( X_{EN} \\) to \\( X_{EN} \\) to confirm that they are mathematically equivalent using an LLM. Formally, we require: Equivalent(\\(X_{EN}, X'_{EN}\\)) = true, where Equivalent(\\( \\cdot,\\cdot\\)) is a function that verifies the theorems, definitions, and logical inferences in both texts align. If the equivalence holds, we have a Mathematician evaluate the theorem in Lean and English, to check if pipeline successfully generated and verified the answer or proof. Our approach is able to prove the 2024 IMO combinatorics problems no previous model or method was able to solve by itself or using existing agentic frameworks. Why does our approach work? One reason is"}, {"title": "2.5. Meta Learning", "content": "We experiment with meta-learning using LLMs to modify agent graph hyper-parameters, prompts and code, and the agent graph topology, adding or removing nodes and edges. The input is an agent graph, and the output are traces of runs on the graph variants, described in Appendices I, O, and V. Our implementation is based on Gentrace (Gentrace, 2025) and LLMs. We representing the pipelines (agent graphs) in a structured format. Execute them and log a detailed trace of intermediate steps. We use an LLM to propose pipeline revisions based on the pipeline representation, trace, and result, and an LLM to correct the revised pipeline."}, {"title": "2.6. HLE", "content": "While math and coding have automatic 0/1 verifiers, other problems, such as many HLE questions, do not. Therefore, we cannot aggregate answers to non-math and non-coding questions by a maximum. In practice, we find that best-of-N rejection sampling with large N works well on the HLE questions. We compute the consensus among answers of different models or methods as the average agreement between them \\( c = \\sum_{i=1}^n \\mathbb{1}(y_k=y_i) \\) and the diversity as 1 \u2013 c."}, {"title": "2.7. Avoiding Data Contamination", "content": "We use best practices to avoid data contamination when evaluating closed and open-weight foundation models. The knowledge cutoff date of the models is before the availability of the evaluated problems, models do not have Internet access and are used with fresh API calls so that solutions are not inadvertently reused from chat memory, and the evaluation does not leak information about solutions. We test OpenAI models using OptiLLM (Sharma, 2024), which consists of multiple methods, prompts, and default parameters that are available online. We test closed and open-weight models. IMOs 2020-2023 were before OpenAI's models were trained and therefore we cannot evaluate them or build our mapping based on these IMO's without contamination. The IMO shortlist problems and solutions are released after the following year's IMO, so 2023 IMO shortlist problems and solutions are released after July 2024, which is after the cutoff dates of the LLMs and may be safely used for testing, except for problem 6 which was selected for IMO 2024 and is therefore excluded. We go beyond problem-solving by generating new problems and solving them, and verifying that the answers and proofs are correct and complete."}, {"title": "2.8. Generating New IMO Problems and Solutions", "content": "OpenAI Deep Research (OpenAI, 2025a) has advanced reasoning capabilities. However it has Internet access, including access to existing IMO solutions, and therefore it is not used to solve existing problems or synthesize data used for solving existing problems. However, we use Deep Research to generate new problems for future use, and in addition to previous IMO problems, these generated problems will serve as part of our training data toward the 2025 IMO. Appendix Y illustrates our approach for generating new problems and their solutions for training toward future IMO's."}, {"title": "2.9. IMO Human Evaluation", "content": "Our IMO answers, their formal theorems in Lean, and proofs are evaluated by an expert Mathematician with math Olympiad evaluation experience. The problems, answers, and solutions appear in Appendix B along with the official IMO problems and solutions as released by the IMO committee (Thomas et al., 2024)."}, {"title": "3. Results", "content": "3.1. IMO\nWe perform extensive evaluations on IMO combinatorics problems using different methods and models. We test all combinatorics problems from non-contaminated exams. Figure 3 reports for each method and model if the answer is correct by \\(\\checkmark\\), and X otherwise. Running times, in brackets, are in seconds. Similar tables for all 2024 IMO, USAMO, and 2023 IMO ShortList problems appear in Appendices C, D, and E. AG denotes our IMO agent graph detailed in Appendices F-N. Zero-shot o1 answers 1/9 problems correctly. The best method using 03-mini high answers 3/9 problems correctly, whereas A diverse set of 8 methods using 03-mini high answers correctly 7/9 (77.77%) of the problems, with automatic verification. Similarly, the best method using o1 answers 3/9 problems correctly, whereas the diverse set of 8 methods using o1 answers correctly 6/9 (66.66%) of the problems, with automatic verification.\nOur approach proves the fifth combinatorics problem (Turbo the Snail) out of six problems in the 2024 IMO, tipping performance to a gold medal level as shown in Figure 3. The knowledge cutoff date of the foundation models we use is before the 2024 IMO and before the release of the IMO 2023 shortlist, and we do not use Internet access. Our approach is strict, beginning with the problems in plain English as it is given to IMO contestants. Deepmind's AlphaProof and AlphaGeometry 2 solve four out of six problems in the 2024 IMO for 28 points which is at the level of a silver medal (Castelvecchi, 2024; Google DeepMind, 2024a) given the formal problem in Lean (Google DeepMind, 2024b). We do not give partial credit and consider the solution correct only"}, {"title": "3.2. ARC", "content": "We perform an extensive evaluation of 16 models and methods on 400 ARC evaluation puzzles as illustrated in Figures 4 and 5, and described in Appendices P, Q, and R. Diversity is the maximum verifiable aggregation of 16 models and methods at inference time. We find that:\n1. Without 03, diversity of 16 models and methods increases performance from the blue dotted line (53%) to the orange dotted line (69.5%).\n2. With o3, diversity of 16 models and methods increases performance from the purple dotted line (91.5%) to the red dotted line (93.75%).\n3. Diversity of 16 models and methods solves 80% of the puzzles on which 948 humans collectively fail on. These 5/400 puzzles are between the dotted green line (98.8%) and black line (100%).\n4. Diversity of 16 models and methods solves 26.5% of the puzzles on which o3 with high-compute fails on. These 34/400 puzzles are between the dotted purple line (91.5%) and black line (100%).\nAppendices P and Q show the detailed evaluation of each of the 16 models and methods on each of these puzzles, and Appendix R shows the detailed evaluation of each of the 16 models and methods on each of the 400 evaluation puzzles."}, {"title": "3.3. HLE", "content": "We run our experiments on a random sample of 100 questions due to the costs of compute. Accuracy of different models and methods is shown in Table 1. The accuracy of best-of-N rejection sampling with N = 3 using 03-mini high on these 100 randomly sampled questions is 37% over all categories and 33.3% on Math questions, and using o1 is 21% over all categories and 29.6% on Math, as shown in Figures 6 and 7, and described in detail in Appendices T and U. The accuracy of best-of-N with N = 16 on 10 random questions is 40% using o1 and 50% using o3-mini high. Questions, answers, and evaluation details appear in Appendix S."}, {"title": "3.4. Limitations", "content": "IMO. A correct solution consists of both a correct answer and a correct and complete proof. Simple frameworks using LLMs such as OptiLLM may correctly answer problems but fail to correctly prove them. Not all problems have answers, and there are problems that require only proofs. Formulating correct, complete and verifiable proofs is non-trivial. Appendix L provides examples of combinatorics problems that require finding an invariant or involve very high dimensional spaces that our approach does not handle. In general, proving upper bounds may be harder than proving lower bounds. For example, when proving a lower bound, we show that we can achieve a high score by simulation and deep reinforcement learning, which is relatively easy, whereas when proving an upper bound, we show that we cannot achieve a better score, which may be more difficult. Combinatorics problems may involve extremely large dimensions and solutions, where it is difficult to generalize from small to large examples by induction. Our use of meta-learning across multiple instances allows us to generalize. Combinatorics problems may be classified into different types of problems, such as enumerative combinatorics, which involves counting the number of ways patterns or structures are formed (for example, permutations or partitions); graph theory, which deals with combinatorial properties of graphs (such as paths, cycles, coloring, or flow); combinatorial optimization, where the goal is optimizing a combinatorial structure by criteria (for example TSP, knapsack, or scheduling problems); and others. We handle problems that may be modeled using a game with state, action space, and rewards. We would like to test our approach in real test-time conditions during the 2025 IMO.\nHLE. The main limitation for evaluating our approach for answering HLE questions is the cost of inference which is currently above a Dollar per question per method with N = 1. Best-of-N rejection sampling multiplies this cost by 2N and is prohibitive for large N on a large sample. We therefore perform HLE evaluation on a random sample of 100 questions."}, {"title": "4. Conclusions", "content": "This work shows that combining diverse inference methods with perfect verifiers tackles advanced reasoning tasks such as IMO combinatorics problems and ARC puzzles. In contrast, using an imperfect verifier, best-of-N rejection sampling, on the HLE shows good performance but at significant inference costs.\nIn Mathematics there is often a wide gap between the capability of the average human, expert Mathematician, and best Mathematician. The average human cannot solve, or finds it challenging to solve a single IMO problem, an expert Mathematician may find it challenging to solve half of the problems, whereas the best human problem solvers or Mathematicians can solve all of the problems. On unseen Mathematical Olympiad combinatorics, the best single model or method answers a third of the problems correctly, whereas the aggregate of diverse models and methods answer two thirds of the problems. The correct proof of the 2024 IMO combinatorics problem tips AI's overall performance from Silver to Gold medal level, placing it on par with around the top fifty worldwide each year, among tens of thousands of participants in national and international competitions."}, {"title": "Impact Statement", "content": "This work accelerate progress in AI for Mathematics and visual reasoning tasks. Responsibly deployed, these methods may benefit education, research, and industry by improving Mathematics accessibility, supporting formal verification, and enhancing STEM education."}]}