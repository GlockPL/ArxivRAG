{"title": "Realtime Generation of Streamliners with Large Language Models", "authors": ["Florentina Voboril", "Vaidyanathan Peruvemba Ramaswamy", "Stefan Szeider"], "abstract": "This paper presents the novel method StreamLLM for generating streamliners in con-\nstraint programming using Large Language Models (LLMs). Streamliners are constraints\nthat narrow the search space, enhancing the speed and feasibility of solving complex prob-\nlems. Traditionally, streamliners were crafted manually or generated through systematically\ncombined atomic constraints with high-effort offline testing. Our approach uses LLMs to\npropose effective streamliners. Our system StreamLLM generates streamlines for problems\nspecified in the MiniZinc constraint programming language and integrates feedback to the\nLLM with quick empirical tests. Our rigorous empirical evaluation involving ten problems\nwith several hundreds of test instances shows robust results that are highly encouraging,\nshowcasing the transforming power of LLMs in the domain of constraint programming.", "sections": [{"title": "1 Introduction", "content": "Streamliners are certain constraints added to a constraint model to reduce the search space,\nthereby improving the feasibility and speed of finding solutions to complex constraint satisfaction\nproblems. By incorporating domain-specific knowledge, streamliners can guide the constraint\nsolver, allowing it to bypass less promising areas of the search space. Gomes and Sellmann\n(2004a) introduced streamliners to speed up the constrained-based search for hard combinatorial\ndesign problems. Today, streamliners are a standard tool for speeding up constrained-based\nsearch. Streamliners are closely related to implied/redundant constraints, symmetry-breaking\nconstraints, and dominance-breaking constraints; however, adding a streamliner may even cause\nthe constraint model to become inconsistent.\nOriginally, streamliners were hand-crafted by researchers who used their theoretical insight\nto analyze the constrained model. However, progress has also been made on the automated gen-\neration of streamliners (Spracklen et al. 2023) by systematically trying the effect of some atomic\nconstraints, such as imposing specific constraints on integer and function domains, like enforcing\nodd or even values, monotonicity, and properties like commutativity, as well as facilitating spe-\ncific attributes in binary relations. These atomic restrictions are tested on thousands of problem\ninstances, and those that show a good streamlining effect are systematically combined.\nThis paper proposes a different approach to automated streamliner generation that utilizes\nthe creative power of Large Language Models (LLMs) to generates streamliners, provides a proto-\ntype implementation StreamLLM, and rigorously tests it on ten different constraint satisfaction\nproblems using at least 245 CPU days. Our approach leverages the capabilities of LLMs to\ninfer potentially effective streamliners, similar to an experienced researcher's intuitive grasp of"}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Constraint Programming", "content": "Constraint Programming (CP) is a methodology for solving combinatorial problems specified\nby means of declarative constraints. Please refer to Rossi, Van Beek, and Walsh (2006) for a\ncomprehensive discourse. Examples include scheduling, routing, planning etc. These problems\ncan be of two types decision problems require a yes/no answer, while optimization problems\nrequire you to find a solution which minimizes or maximizes a given objective function. In this\npaper, we focus only on decision problems."}, {"title": "2.2 Streamliners", "content": "Streamlining constraints or streamliners, introduced by Gomes and Sellmann (2004a), are con-\nstraints that are added to constraint programming models with the goal of narrowing the focus\nto a small but highly promising segment of the search space. Thus, they have the potential\nto significantly reduce the search effort for hard combinatorial problems. For example, Gomes\nand Sellmann (2004a) used constraints for \u201cLatin squareness\u201d as a streamliner for finding Diag-\nonally Ordered Magic Squares. Streamliners have been successfully applied in a diverse range of\nsettings.\nWe note that streamlining constraints are not required to be sound. This means that adding\nthe streamlining constraint may make a satisfiable instance of a constraint model unsatisfiable.\nAs a consequence, an unsatisfiable instance in the streamlined model does not imply that the\ninstance is unsatisfiable in the original model.\nOther constraints that are similar to streamliners implied (or redundant) constraints, symmetry-\nbreaking constraints, and dominance-breaking constraints. Implied constraints do not change the\nset of feasible solutions (Frisch, Jefferson, and Miguel 2004; Frisch, Miguel, and Walsh 2001;\nCharnley, Colton, and Miguel 2006; Colton and Miguel 2001; Frisch, Miguel, and Walsh 2003).\nSymmetry-breaking constraints eliminate certain solutions within each equivalence class while\nensuring that at least one solution from each class remains (Flener et al. 2001; Frisch et al. 2006,\n2007, 2009; Itzhakov and Codish 2022; Fichte, Hecher, and Szeider 2020). Dominance breaking\nconstraints are applicable in the context of optimization problems (possibly formulated as a deci-\nsion problem, with the objective value explicitly stated in the model), as they disallow solutions\nthat are known to be suboptimal, and possibly some optimal solutions as well, so long as at least\none optimal solution remains (Prestwich and Beck 2004; Chu and Stuckey 2015; Lee and Zheng\n2022; Gomes and Sellmann 2004b). In contrast to streamliners, such constraints are guaranteed\nto be sound.\nFinding a useful streamliner manually by a human expert is a time-consuming process. There-\nfore it is appealing to generate streamliners automatically, a direction explored successfully in\nprevious work (Wetter, Akg\u00fcn, and Miguel 2015; Spracklen, Akg\u00fcn, and Miguel 2018; Spracklen\net al. 2019, 2023). A common point between all these previous approaches is that they treat the\nstreamliner generation as a high-effort, offline task, taking up to 4 CPU days for a single problem.\nThe streamliner is built up systematically from elementary constraints to reduce the domain of\na decision variable and tested on a large set of automatically generated instances.\nSimilar to other recent studies (Spracklen et al. 2023) we will use the term streamliner in a\nwider sense to accommodate also redundant, symmetry-breaking constraints. This is particularly\nuseful in our context, since for automatically constructed constraints, it is difficult to determine\nwhich type the new constraint is, and, from a pragmatic point of view, the distinction does not\nmatter."}, {"title": "2.3 Large Language Models (LLMs)", "content": "Large Language Models (LLMs) are advanced AI systems based on the transformer models\n(Vaswani et al. 2017) and trained on vast data sets to produce human-like text and source code\nin response to instruction prompts (Minaee et al. 2024). These models, trained on vast amounts\nof text data, can produce human-like text across various domains and styles (Brown et al. 2020).\nRecent advancements have expanded their capabilities beyond traditional language tasks, includ-\ning generating, analyzing, and debugging code across multiple programming languages (Chen\net al. 2021; Xu et al. 2022; Wu, Barrett, and Narodytska 2023; Pei et al. 2023). In addition to\ncode-related tasks, LLMs have shown promise in mathematical reasoning and problem-solving\n(Lample et al. 2022; Romera-Paredes et al. 2024); they can process and generate mathematical\nexpressions, solve equations, and even assist with proofs. However, it is essential to note that\nwhile LLMs can produce seemingly correct mathematical output, their responses should be care-\nfully verified. The models' performance on these tasks varies, and they may sometimes generate\nplausible-looking but incorrect solutions (Polu et al. 2023). Despite these limitations, the poten-\ntial applications of LLMs in fields such as computer science, mathematics, and engineering are\nsubstantial and continue to expand as the technology evolves."}, {"title": "3 Realtime Generation of Streamliners with LLMS", "content": "As outlined in the introduction, we propose a novel approach to the automatic generation of\nstreamliners and implement it in the prototype system StreamLLM. In contrast to the exist-\ning state-of-the-art approaches that focus on a high-effort streamliner generation that builds a\nstreamliner systematically from elementary steps and takes several CPU days (Spracklen et al.\n2023), we aim at the realtime generation of streamliners utilizing the power of LLMs.\nWe envision a use-case scenario where a user wants to solve hard instances of a constraint\nsatisfaction problem with a model provided in the MiniZinc language. Assume the hard instances\nrequire about an hour to be solved with the unstreamlined model. Instead of just starting the\nsolver, the user provides a few small instances of the problem that are solvable within a few\nseconds. StreamLLM will then run several queries to an LLM and run tests on the small instances\nto finally suggest streamliners. The user then runs streamlined versions of the original constraint\nmodel on the hard instance until a solution is found. This has the potential to drastically reduce\nthe running time of the original model and more than compensate for the time needed to find\nthe streamliners. However, the streamlined models are not guaranteed to find a solution. Thus,\nit also makes sense to run the original and the streamlined models it parallel."}, {"title": "3.1 StreamLLM", "content": "Our system utilizes several queries (prompts) to the LLM that results in candidate streamliners\nand tests the candidate streamliners on a small set of ntrain easy training instances that can be\nsolved in less that ttrain seconds. Rigorous testing on several problems indicates that a relatively\nsmall number of test instances suffices (about 15) and even small and easy test instances (solv-\nable in under 10 seconds with the unstreamlined model) provide a good indication of how well\nstreamliners will work on large and hard instances; we will lay out this experiments in more detail\nin Sections 4.5 and 4.6. We test various strategies for this type of streamliner generation, that\nwe divide into static and adaptive categories. The general strategy can be found in Algorithm 1."}, {"title": "3.2 Prompt Engineering", "content": "Prompt1, given in Listing 1, is the prompt we use for most of our experiments. In the beginning,\nthe task is shortly summarized. Then, following the Chain of Thought technique (Wei et al.\n2022), the task is split into single steps that are explicitly explained. In the end, there are some"}, {"title": "4 Experiments", "content": "All instances, MiniZinc models, and the Python implementation of StreamLLM including the\nprompts are available jon Zenodo\u00b3."}, {"title": "4.1 Setup", "content": "We use MiniZinc version 2.8.3 and the Chuffed 0.13.1 solver. As LLMs we use GPT-40 (gpt-\n40-2024-05-13) and Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) and access them via the\nopenai 1.29.0 and anthropic 0.25.8 packages in Python 3.11.5. We evaluate the running times of\nthe test instances on compute nodes with 2.40GHz, 10-core 2x Intel Xeon E5-2640 v4."}, {"title": "4.4 k-Best Selection", "content": "As already explained in Section 2.2, there is no guarantee that every satisfiable instance in the\noriginal model is also satisfiable in a streamlined model. It also might happen that one streamliner\nworks well on the training instances but is impractical for the large test instances. In order to get\nmore robustness, we decided to not only rely on one streamliner but to return k streamliners that\nwork best on the training instances. In the experiments, we run the original model and k = 3\nstreamlined models in parallel and stop as soon as any of the k + 1 models has found a solution."}, {"title": "4.5 Experiment la", "content": "The objective of this experiment is to determine a suitable maximal running time ttrain of the\nunstreamlined model for the training instances. We want to keep ttrain as low as possible so\nthat the evaluation can be done quickly but still provides a good predictions for the streamliner\nperformance on the significantly larger test instances.\nWe run the experiment with the problems BIBD, BH, and SG. For each problem, we let the\nLLM suggest ten streamliners. Then, we pick those three streamliners that perform best on the\ntraining instances, and among the three, we pick the one that performs best on the test instances.\nWe use five sets of training instances with an upper bound of ttrain \u2208 {10,20,30,60} seconds,\nrespectively. The experiments show that ttrain has no influence on the one streamliner that was\npicked in the end, hence we settled on the upper bound ttrain = 10 for the following experiments.\nFor reducing the influence of I/O operations, we require training instances to take at least 1\nsecond to be solved."}, {"title": "4.6 Experiment 1b", "content": "The goal of this experiment is to decide on the number ntrain of training instances. More training\ninstances promises better results on the test instances but makes the evaluation process longer,\nwhich we want to avoid for realtime streamliner generation. Hence we aim at a fair compromise.\nWe conduct the experiment with the same three problems as in Experiment la. For each\nproblem, we generate ten streamliners. For each ntrain \u2208 {3, 5, 7, 10, 20, 50}, we randomly pick\nnTrain training instances. We determine the combination of three out of the ten streamliners that\nperform best on the training instances and compute the time this combination saves on the test\ninstances, normalized by the time saved by the virtually best combination of three streamliners."}, {"title": "4.7 Experiment 1c", "content": "MiniZinc is a complex tool that relies on several heuristics, some of which rely on randomness.\nHence it is possible to run the same model using MiniZinc with different random seeds and obtain\ndifferent results. The goal of this experiment is to determine the extent of this variance resulting\npurely from randomness, so that, when we evaluate the generated streamliners, we can distinguish\nthe contribution of the streamliner itself from random variation.\nIn this experiment, we run four identical copies of the original model in parallel and measure\nthe difference in running times between the first copy and quickest among the other three copies.\nAggregating this metric across all test instances of all ten problems, we observe a 6.38% reduction\non average. Hence, we set 6.38% as the minimum improvement threshold to count a streamlined\nversion as significantly better."}, {"title": "4.8 Experiment 2", "content": "This is the main experiment where we evaluate the StreamLLM approach. We run StreamLLM\non all ten problems with PROMPT1 (Listing 1), comparing the static and adaptive approach\nand the two considered LLMs. We conducted the experiment twice to fathom the influence of\nrandomness on the results. Figure 3 reports the average time reduction of both runs, the vertical\nblack bars indicate the difference between the two runs. The bars representing configurations\nare sorted by time reduction. Table 1 shows the average percentage of significantly improved\ninstances on the two runs.\nOverall, StreamLLM achieves very encouraging results. For some problems, the reduction in"}, {"title": "4.9 Experiment 3", "content": "So far we have focused on PROMPT1 (Listing 1), next we compare it with the following prompt\nvariants on problems SG, CS, CC, and OA, using the overall-best combination from Experiment 2,\nstatic GPT.\nPrompt2 differs from PROMPT1 in the last few steps where the feedback for the adaptive ap-\nproach are omitted.\nPrompt3 is even shorter and just requests five additional streamliners in the JSON output for-\nmat.\nPrompt4 differs from PROMPT1 in that implied, symmetry-breaking, or dominance-breaking\nconstraints are not mentioned explicitly.\nWe exhibit the results in Figure 5. It is interesting to see that PROMPT3, which is very short\nand imprecise, is the only prompt that produces better results than PROMPT1. It is also inter-\nesting, that PROMPT2 performs worse than PROMPT1. The only difference between those two is\nthat PROMPT1 contains more steps, which are only relevant for the adaptive approach, but here\nwe use the static approach. PROMPT4, which does not mention implied, symmetry-breaking, or\ndominance-breaking constraints explicitly perform worse than PROMPT1."}, {"title": "5 A Closer Look at Some Generated Streamliners", "content": "It is interesting to see whether LLM streamliners can compete with additional constraints sug-\ngested by human experts. For some of the considered MiniZinc models, symmetry-breaking\nconstraints were available, two for the SG problem, for instance. When comparing the running\ntime of this model with the original model, a time reduction of 32.1% is reached, which is signif-\nicantly less than the reduction StreamLLM achieves. StreamLLM also returned one of the two\nconstraints in some runs. n that sense, one could argue that our approach can beat the addi-\ntional constraints suggested by human experts while no human expertise or time to deal with the\nproblem manually is required.\nIn the following list, we exhibit a representative subset of some well-performing streamlining\nconstraints produced by StreamLLM. For the sake of conciseness, we state the streamliners in\nmathematical terms.\n1. $\\Sigma_{c \\in C} s_c = |V|$, (PIP)\n2. $\\forall v: color(v) = v \\mod |C| + 1$, (PIP)\n3. $\\forall 2 < v \\leq |V| - 1: color(v) \\leq color(v + 1)$, (HC)\n4. $\\forall c_i, c_j \\in C: i < j \\Rightarrow s_{c_i} > s_{c_j}$ (HC)\n5. $\\forall c \\in C: s_c \\leq \\lceil \\frac{|V|}{|C|} \\rceil$, (OB)\n6. $\\forall f \\in F: (1 < j < n \\Rightarrow conf\\_req(conf\\_at(j), f) = \\Sigma_{k \\in K} conf\\_req(k, f) \\cdot n_k$, (CS)\n7. $\\forall 1 < i < n - 1: conf\\_at(i) < conf\\_at(i + 1) + 1$, (CS)\n8. $\\forall 1 \\leq s \\leq 13: ( (\\Sigma_{1 \\leq j \\leq 3} layout(s, j)) \\geq 2 ) $, (BH)\n9. $x(26) = 26$, (BH)\n10. $\\forall 1 < i < 17: y(layout(i, 1)) \\leq 30$, (BH)\nIn the above list, streamliners 1 and 6 are implied constraints, while streamliner 5 is an implied\nconstraint for only certain values of imbalance m, and streamliner 4 is a symmetry-breaking\nconstraint. The remaining streamliners are neither implied nor symmetry-breaking constraints.\nThe above list hints at the possibility of the LLMs having a good grasp of the problem we are\ntrying to solve. We see a mix of implied constraints and streamlining constraints. Several of\nthe suggested streamliners seem non-trivial, while others seem trivial, but they still significantly\nspeed up the solving process."}, {"title": "6 Conclusion and Future Work", "content": "We introduced StreamLLM, a novel approach that utilizes the power of LLMs for automatic\nstreamliner generation for constraint satisfaction problems. Originally, streamliners were hand-\ncrafted or generated through systematic testing of atomic constraints. Our approach utilizes\nLLMs' creativity and analytical capabilities to automatically develop effective streamliners that\nmimic a deep understanding of the problem. By quickly testing the proposed streamliners on\nsmall problem instances, we can reduce the chance of errors or unsatisfiable instances on the\nstreamliners we select. Our rigorous experiments exhibit significant improvements in solving\ntimes. In some cases, our approach could achieve running time reductions of 99% compared to\nthe unstreamlined solving time. Even in a realtime scenario, StreamLLM achieves remarkable\nresults. However, it can make sense to use our approach even for instances that only take about\n30 minutes to solve. We could save more than 80 percent of the solving time for some larger\ninstances. This percentage gets higher for even larger instances.\nStreamLLM's capability is not tied to a particular LLM, prompt, or filtering strategy, although\nGPT-40 with our static approach and PROMPT1 (or PROMPT3 as tested on a subset of problems)\nshows the best overall performance.\nAs Experiments 2 and 3 show, the obfuscated problems do not work significantly worse than\nthe unobfuscated problems, indicating that StreamLLM might not depend on memorization and\nrecognizes some deeper structure. Indeed, by taking a closer look at some of the best-performing\nstreamliners proposed by StreamLLM, we see a mix of implied constraints and streamlining\nconstraints, several of which seem non-trivial and could be proposed by a human expert, others\nlook rather puzzling but still work efficient for speeding up the search.\nIt is worth pointing out how well we can scale from training instances to test instances; based\non Experiments la and 1b, we can restrict the streamliner generation phase to only 15 training\ninstances solvable in at most 10 seconds, whereas in other related areas, scaling has been identified\nas one of the main challenges (Bengio, Lodi, and Prouvost 2021).\nFor future work, we plan to extend StreamLLM to generate combinations of streamliners; we\nexpect the combination of the strength of multiple streamliners to lead to even greater improve-\nments. By investigating the influence of the temperature for LLM calls, one could find the best\nbalance between creativity and predictability of the results.\nMoving from realtime to offline scenarios allows more training instances with longer running\ntimes and the generation of more streamliners. Using an evolutionary procedure based on pairing\na pre-trained LLM with a systematic evaluator, as demonstrated impressively by Romera-Paredes\net al. (2024), seems promising to try for streamliner generation. Further avenues for further work\nare to fine-tune LLMs for streamliner generation and to test whether small language models\n(SLMS) could offer a balance between performance and computational efficiency."}]}