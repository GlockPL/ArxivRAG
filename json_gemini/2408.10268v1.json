{"title": "Realtime Generation of Streamliners with Large Language Models", "authors": ["Florentina Voboril", "Vaidyanathan Peruvemba Ramaswamy", "Stefan Szeider"], "abstract": "This paper presents the novel method StreamLLM for generating streamliners in constraint programming using Large Language Models (LLMs). Streamliners are constraints that narrow the search space, enhancing the speed and feasibility of solving complex problems. Traditionally, streamliners were crafted manually or generated through systematically combined atomic constraints with high-effort offline testing. Our approach uses LLMs to propose effective streamliners. Our system StreamLLM generates streamlines for problems specified in the MiniZinc constraint programming language and integrates feedback to the LLM with quick empirical tests. Our rigorous empirical evaluation involving ten problems with several hundreds of test instances shows robust results that are highly encouraging, showcasing the transforming power of LLMs in the domain of constraint programming.", "sections": [{"title": "Introduction", "content": "Streamliners are certain constraints added to a constraint model to reduce the search space, thereby improving the feasibility and speed of finding solutions to complex constraint satisfaction problems. By incorporating domain-specific knowledge, streamliners can guide the constraint solver, allowing it to bypass less promising areas of the search space. Gomes and Sellmann (2004a) introduced streamliners to speed up the constrained-based search for hard combinatorial design problems. Today, streamliners are a standard tool for speeding up constrained-based search. Streamliners are closely related to implied/redundant constraints, symmetry-breaking constraints, and dominance-breaking constraints; however, adding a streamliner may even cause the constraint model to become inconsistent.\nOriginally, streamliners were hand-crafted by researchers who used their theoretical insight to analyze the constrained model. However, progress has also been made on the automated generation of streamliners (Spracklen et al. 2023) by systematically trying the effect of some atomic constraints, such as imposing specific constraints on integer and function domains, like enforcing odd or even values, monotonicity, and properties like commutativity, as well as facilitating specific attributes in binary relations. These atomic restrictions are tested on thousands of problem instances, and those that show a good streamlining effect are systematically combined.\nThis paper proposes a different approach to automated streamliner generation that utilizes the creative power of Large Language Models (LLMs) to generates streamliners, provides a prototype implementation StreamLLM, and rigorously tests it on ten different constraint satisfaction problems using at least 245 CPU days\u00b9. Our approach leverages the capabilities of LLMs to infer potentially effective streamliners, similar to an experienced researcher's intuitive grasp of"}, {"title": "Preliminaries", "content": ""}, {"title": "Constraint Programming", "content": "Constraint Programming (CP) is a methodology for solving combinatorial problems specified by means of declarative constraints. Please refer to Rossi, Van Beek, and Walsh (2006) for a comprehensive discourse. Examples include scheduling, routing, planning etc. These problems can be of two types - decision problems require a yes/no answer, while optimization problems require you to find a solution which minimizes or maximizes a given objective function. In this paper, we focus only on decision problems."}, {"title": "Streamliners", "content": "Streamlining constraints or streamliners, introduced by Gomes and Sellmann (2004a), are constraints that are added to constraint programming models with the goal of narrowing the focus to a small but highly promising segment of the search space. Thus, they have the potential to significantly reduce the search effort for hard combinatorial problems. For example, Gomes and Sellmann (2004a) used constraints for \u201cLatin squareness\u201d as a streamliner for finding Diagonally Ordered Magic Squares. Streamliners have been successfully applied in a diverse range of settings\u00b2.\nWe note that streamlining constraints are not required to be sound. This means that adding the streamlining constraint may make a satisfiable instance of a constraint model unsatisfiable. As a consequence, an unsatisfiable instance in the streamlined model does not imply that the instance is unsatisfiable in the original model.\nOther constraints that are similar to streamliners implied (or redundant) constraints, symmetry-breaking constraints, and dominance-breaking constraints. Implied constraints do not change the set of feasible solutions (Frisch, Jefferson, and Miguel 2004; Frisch, Miguel, and Walsh 2001; Charnley, Colton, and Miguel 2006; Colton and Miguel 2001; Frisch, Miguel, and Walsh 2003). Symmetry-breaking constraints eliminate certain solutions within each equivalence class while ensuring that at least one solution from each class remains (Flener et al. 2001; Frisch et al. 2006, 2007, 2009; Itzhakov and Codish 2022; Fichte, Hecher, and Szeider 2020). Dominance breaking constraints are applicable in the context of optimization problems (possibly formulated as a decision problem, with the objective value explicitly stated in the model), as they disallow solutions that are known to be suboptimal, and possibly some optimal solutions as well, so long as at least one optimal solution remains (Prestwich and Beck 2004; Chu and Stuckey 2015; Lee and Zheng 2022; Gomes and Sellmann 2004b). In contrast to streamliners, such constraints are guaranteed to be sound.\nFinding a useful streamliner manually by a human expert is a time-consuming process. Therefore it is appealing to generate streamliners automatically, a direction explored successfully in previous work (Wetter, Akg\u00fcn, and Miguel 2015; Spracklen, Akg\u00fcn, and Miguel 2018; Spracklen et al. 2019, 2023). A common point between all these previous approaches is that they treat the streamliner generation as a high-effort, offline task, taking up to 4 CPU days for a single problem. The streamliner is built up systematically from elementary constraints to reduce the domain of a decision variable and tested on a large set of automatically generated instances.\nSimilar to other recent studies (Spracklen et al. 2023) we will use the term streamliner in a wider sense to accommodate also redundant, symmetry-breaking constraints. This is particularly useful in our context, since for automatically constructed constraints, it is difficult to determine which type the new constraint is, and, from a pragmatic point of view, the distinction does not matter."}, {"title": "Large Language Models (LLMs)", "content": "Large Language Models (LLMs) are advanced AI systems based on the transformer models (Vaswani et al. 2017) and trained on vast data sets to produce human-like text and source code in response to instruction prompts (Minaee et al. 2024). These models, trained on vast amounts of text data, can produce human-like text across various domains and styles (Brown et al. 2020). Recent advancements have expanded their capabilities beyond traditional language tasks, including generating, analyzing, and debugging code across multiple programming languages (Chen et al. 2021; Xu et al. 2022; Wu, Barrett, and Narodytska 2023; Pei et al. 2023). In addition to code-related tasks, LLMs have shown promise in mathematical reasoning and problem-solving (Lample et al. 2022; Romera-Paredes et al. 2024); they can process and generate mathematical expressions, solve equations, and even assist with proofs. However, it is essential to note that while LLMs can produce seemingly correct mathematical output, their responses should be carefully verified. The models' performance on these tasks varies, and they may sometimes generate plausible-looking but incorrect solutions (Polu et al. 2023). Despite these limitations, the potential applications of LLMs in fields such as computer science, mathematics, and engineering are substantial and continue to expand as the technology evolves."}, {"title": "Realtime Generation of Streamliners with LLMS", "content": "As outlined in the introduction, we propose a novel approach to the automatic generation of streamliners and implement it in the prototype system StreamLLM. In contrast to the existing state-of-the-art approaches that focus on a high-effort streamliner generation that builds a streamliner systematically from elementary steps and takes several CPU days (Spracklen et al. 2023), we aim at the realtime generation of streamliners utilizing the power of LLMs.\nWe envision a use-case scenario where a user wants to solve hard instances of a constraint satisfaction problem with a model provided in the MiniZinc language. Assume the hard instances require about an hour to be solved with the unstreamlined model. Instead of just starting the solver, the user provides a few small instances of the problem that are solvable within a few seconds. StreamLLM will then run several queries to an LLM and run tests on the small instances to finally suggest streamliners. The user then runs streamlined versions of the original constraint model on the hard instance until a solution is found. This has the potential to drastically reduce the running time of the original model and more than compensate for the time needed to find the streamliners. However, the streamlined models are not guaranteed to find a solution. Thus, it also makes sense to run the original and the streamlined models it parallel."}, {"title": "StreamLLM", "content": "Our system utilizes several queries (prompts) to the LLM that results in candidate streamliners and tests the candidate streamliners on a small set of $n_{train}$ easy training instances that can be solved in less that $t_{train}$ seconds. Rigorous testing on several problems indicates that a relatively small number of test instances suffices (about 15) and even small and easy test instances (solvable in under 10 seconds with the unstreamlined model) provide a good indication of how well streamliners will work on large and hard instances; we will lay out this experiments in more detail in Sections 4.5 and 4.6. We test various strategies for this type of streamliner generation, that we divide into static and adaptive categories. The general strategy can be found in Algorithm 1."}, {"title": "Prompt Engineering", "content": "Prompt1, given in Listing 1, is the prompt we use for most of our experiments. In the beginning, the task is shortly summarized. Then, following the Chain of Thought technique (Wei et al. 2022), the task is split into single steps that are explicitly explained. In the end, there are some compliance rules to ensure the quality and the correct format of the response. Especially the correct JSON output format is important for the automated tests on the training instances. We decided on this prompt because, in some preliminary experiments, it showed good results. In Section 4.9 we will consider variants of PROMPT1."}, {"title": "Experiments", "content": "All instances, MiniZinc models, and the Python implementation of StreamLLM including the prompts are available jon Zenodo\u00b3."}, {"title": "Setup", "content": "We use MiniZinc version 2.8.3 and the Chuffed 0.13.1 solver. As LLMs we use GPT-40 (gpt-40-2024-05-13) and Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) and access them via the openai 1.29.0 and anthropic 0.25.8 packages in Python 3.11.5. We evaluate the running times of the test instances on compute nodes with 2.40GHz, 10-core 2x Intel Xeon E5-2640 v4."}, {"title": "Our Benchmark Problems", "content": "Although our method can be applied to optimization problems, similar to previous work (Spracklen et al. 2023), we focus solely on decision problems. This way we can evaluate streamliners only on the basis of their running times. For optimization problems, one needs to consider solution quality. We constructed a benchmark set of 10 constraint satisfaction problems to test the streamliners LLM-generated streamliners. This includes four problems which were already considered in previous work by (Spracklen et al. 2023), namely Balanced Incomplete Block Design (BIBD), Car Sequencing (CS), Social Golfers (SG) and Vessel Loading (VL), all of which are from CSPLib\u2074. In addition to that, we included two other well-known problems, Black Hole (BH), which is also from CSPLib and Carpet Cutting (CC) which is from MiniZinc Challenge 2021\u2075. Please refer to the respective sources for the problem descriptions and references. The main criterion for selecting these six problems was that the MiniZinc model must be readily available. Further filtering was done based on the ease of generation of instances of a desired difficulty. Note that, if present, we stripped any redundant constraints, symmetry-breaking constraints, and excessive documentation from the MiniZinc models of these problems.\nFurther, we added the well-known NP-hard problem Hypergraph Coloring (HC) problem (described below), for which, to the best of our knowledge, no constraint model is available. The main reason for including this problem is to see whether our approach relies on memorization (see, e.g., Lee et al. 2023; McCoy et al. 2023).\nHypergraph Coloring (HC): Given integers $c$ and $m$, a set $V$ of vertices, and a set $E$ of hyperedges, where each hyperedge is a subset of $V$, find a coloring of vertices using at most $c$ colors such that no hyperedge is monochromatic, i.e., is only incident to vertices of the same color. Further, the imbalance, i.e., difference in sizes of the largest and the smallest color class, must not exceed $m$.\nAdditionally, we also include the Pizza Ingredient Partitioning (PIP) problem which is a rewriting of the HC model without using any technical hypergraph-related terminology and using 'pizza' terminology instead (even in the comments). For instance, colors, vertices, and hyperedges are called classes, toppings and pizzas respectively. This, rather unusual rewording, ensures that the LLM has never seen this problem before.\nAnd finally, the Obfuscated A (OA) and Obfuscated B (OB) problems are obtained by obfuscating the CS and HC problems respectively. In order to obfuscate a problem, we rename all variables, which are sensibly-named in the original version (e.g., n_cars, n_options) to meaningless names (e.g., id21, id9) in the obfuscated version."}, {"title": "Instance Generation", "content": "Since we are working with a broad range of problems, working with instances of similar/uniform difficulty ensures the generality of our findings and prevents bias from creeping in due to any one particular problem. Hence we construct our own data set by generating instances of desired difficulties. For each problem, we create 15 training instances that take less than 10 seconds and at least 50 test instances that take between 10 minutes and 120 minutes to solve. The 10-minute lower limit is because we want our approach to be comparable in a realtime setting. The 2-hour upper limit is due to limited availability of computational power.\nWe generated satisfiable instances of the desired difficulties for all the above problems. Since SG, BH, and BIBD all have relatively simple input specifications, i.e., a small number of integer parameters, we generated instances exhaustively and tested their difficulty. For CC, CS, HC, VL we generated instances using the AutoIG pipeline developed by Dang et al. (2022). This involved coming up with a parameterized Essence specification describing the input space of each problem. We then fix a difficulty window in the form of lower and upper bounds for the MiniZinc running time and then use the automatic algorithm configuration tool irace to find an optimal configuration of those parameters such that the constraint solving tool Conjure can find an instance which is very likely to fall in the desired difficulty window.\nA detailed visualization of the distribution of the original running times (or in other words, difficulties) of the generated instances is shown in Figure 1. The total number of instances for each problem is stated in the legend."}, {"title": "k-Best Selection", "content": "As already explained in Section 2.2, there is no guarantee that every satisfiable instance in the original model is also satisfiable in a streamlined model. It also might happen that one streamliner works well on the training instances but is impractical for the large test instances. In order to get more robustness, we decided to not only rely on one streamliner but to return $k$ streamliners that work best on the training instances. In the experiments, we run the original model and $k = 3$ streamlined models in parallel and stop as soon as any of the $k + 1$ models has found a solution."}, {"title": "Experiment la", "content": "The objective of this experiment is to determine a suitable maximal running time $t_{train}$ of the unstreamlined model for the training instances. We want to keep $t_{train}$ as low as possible so that the evaluation can be done quickly but still provides a good predictions for the streamliner performance on the significantly larger test instances.\nWe run the experiment with the problems BIBD, BH, and SG. For each problem, we let the LLM suggest ten streamliners. Then, we pick those three streamliners that perform best on the training instances, and among the three, we pick the one that performs best on the test instances. We use five sets of training instances with an upper bound of $t_{train} \\in \\{10,20,30,60\\}$ seconds, respectively. The experiments show that $t_{train}$ has no influence on the one streamliner that was picked in the end, hence we settled on the upper bound $t_{train} = 10$ for the following experiments. For reducing the influence of I/O operations, we require training instances to take at least 1 second to be solved."}, {"title": "Experiment 1b", "content": "The goal of this experiment is to decide on the number $n_{train}$ of training instances. More training instances promises better results on the test instances but makes the evaluation process longer, which we want to avoid for realtime streamliner generation. Hence we aim at a fair compromise.\nWe conduct the experiment with the same three problems as in Experiment la. For each problem, we generate ten streamliners. For each $n_{train} \\in \\{3, 5, 7, 10, 20, 50\\}$, we randomly pick $n_{train}$ training instances. We determine the combination of three out of the ten streamliners that perform best on the training instances and compute the time this combination saves on the test instances, normalized by the time saved by the virtually best combination of three streamliners."}, {"title": "Experiment 1c", "content": "MiniZinc is a complex tool that relies on several heuristics, some of which rely on randomness. Hence it is possible to run the same model using MiniZinc with different random seeds and obtain different results. The goal of this experiment is to determine the extent of this variance resulting purely from randomness, so that, when we evaluate the generated streamliners, we can distinguish the contribution of the streamliner itself from random variation.\nIn this experiment, we run four identical copies of the original model in parallel and measure the difference in running times between the first copy and quickest among the other three copies. Aggregating this metric across all test instances of all ten problems, we observe a 6.38% reduction on average. Hence, we set 6.38% as the minimum improvement threshold to count a streamlined version as significantly better."}, {"title": "Experiment 2", "content": "This is the main experiment where we evaluate the StreamLLM approach. We run StreamLLM on all ten problems with PROMPT1 (Listing 1), comparing the static and adaptive approach and the two considered LLMs. We conducted the experiment twice to fathom the influence of randomness on the results. Figure 3 reports the average time reduction of both runs, the vertical black bars indicate the difference between the two runs. The bars representing configurations are sorted by time reduction. Table 1 shows the average percentage of significantly improved instances on the two runs.\nOverall, StreamLLM achieves very encouraging results. For some problems, the reduction in running times is drastic, even as much as 99% in some cases, where the streamlined models could solve most of the test instances in less than 1 second. For a few of the problems, most noticeably the BIBD problem, the reduction is only moderate. This is not so surprising considering that the model for BIBD already includes symmetry-breaking constraints and so it turns out to be more challenging to find significant time savings. It is worth noting that BIBD also sees very minor improvement in the approach by Spracklen et al. (2023).\nThere is no clear winner of the two considered LLMs or of the adaptive and static approach: For Claude, the adaptive approach works better, and for GPT, the static approach works better. Overall, static GPT achieves the best results.\nWe have interesting results on the influence of obfuscation. Here, the two considered LLMs differ in some cases. Claude works better on the unobfuscated CS problem than on its obfuscated version OA, for GPT there is not such a big difference although it shows a bigger variance on the obfuscated problem.\nFor the three versions of the hypergraph problem HC, including PIP (pizza formulation) and OB (obfuscated), the PIP model could achieve (unexpected to us) the best results. Claude performs a bit weaker at the HC problem, and adaptive GPT performs weak on OB. This findings suggest that LLMs prefer an easy and understandable language rather than mathematical terminology.\nBecause of the very different setup of previous work on automated streamliner generation (high-effort online versus realtime), we cannot make a direct head-to-head comparison. However, as the problems BIBD, CS, SG, and VL where also considered by Spracklen et al. (2023), we give, for reference, the reported speedup factors for these problems: 1.71, 6.77, 2.53, and 2.34, which translates into time reductions of 42%, 85%, 60%, and 57%, respectively.\nOur analysis so far compared the time reduction of streamlined over unstreamlined models, but did not consider the time StreamLLM spends on streamliner generation. To revisit the realtime scenario as sketched in the introduction, Figure 4 shows the percent of saved time when counting the time for streamliner generation to the running time of the streamlined model\u2076. We see a positive net saving in all the cases with a running time above 20 minutes. As expected, the saved time for instances that take less than 20 minutes is relatively poor since more than half of the time is spent with the streamliner generation. For larger instances, however, the generation time matters less, and StreamLLM shows a remarkably strong performance on most of the problems, with the exception of BIBD."}, {"title": "Experiment 3", "content": "So far we have focused on PROMPT1 (Listing 1), next we compare it with the following prompt variants on problems SG, CS, CC, and OA, using the overall-best combination from Experiment 2, static GPT.\nPrompt2 differs from PROMPT1 in the last few steps where the feedback for the adaptive approach are omitted.\nPrompt3 is even shorter and just requests five additional streamliners in the JSON output format.\nPrompt4 differs from PROMPT1 in that implied, symmetry-breaking, or dominance-breaking constraints are not mentioned explicitly.\nWe exhibit the results in Figure 5. It is interesting to see that PROMPT3, which is very short and imprecise, is the only prompt that produces better results than PROMPT1. It is also interesting, that PROMPT2 performs worse than PROMPT1. The only difference between those two is that PROMPT1 contains more steps, which are only relevant for the adaptive approach, but here we use the static approach. PROMPT4, which does not mention implied, symmetry-breaking, or dominance-breaking constraints explicitly perform worse than PROMPT1."}, {"title": "A Closer Look at Some Generated Streamliners", "content": "It is interesting to see whether LLM streamliners can compete with additional constraints suggested by human experts. For some of the considered MiniZinc models, symmetry-breaking constraints were available, two for the SG problem, for instance. When comparing the running time of this model with the original model, a time reduction of 32.1% is reached, which is significantly less than the reduction StreamLLM achieves. StreamLLM also returned one of the two constraints in some runs. n that sense, one could argue that our approach can beat the additional constraints suggested by human experts while no human expertise or time to deal with the problem manually is required.\nIn the following list, we exhibit a representative subset of some well-performing streamlining constraints produced by StreamLLM. For the sake of conciseness, we state the streamliners in mathematical terms\u2077.\n1.  $\\Sigma_{c\\in C} s_c = |V|$, (PIP)\n2.  $\\forall v color(v) = v \\mod |C| + 1$, (PIP)\n3.  $\\forall 2<v\\leq|V|-1 color(v) \\leq color(v + 1)$, (HC)\n4.  $\\forall c_i,c_j \\in C : i < j \\Rightarrow s_{c_i} > s_{c_j}$ (HC)\n5.  $\\forall C \\in C s_c \\leq \\lceil \\frac{|V|}{|C|} \\rceil$, (OB)\n6.  $\\forall f \\in F : (1<j<n conf\\_req(conf\\_at(j), f) = \\Sigma_{k \\in K} conf\\_req(k, f) n_k$, (CS)\n7.  $\\forall 1 \\leq i \\leq n-1 conf\\_at(i) < conf\\_at(i + 1) + 1$, (CS)\n8.  $\\forall 1\\leq r\\leq 13 ((\\Sigma_{i\\leq i\\leq r} x(s + r)) \\geq 2)$, (BH)\n9.  $x(26) = 26$, (BH)\n10. $\\forall 1 \\leq i \\leq 17 y(layout(i, 1)) \\leq 30$, (BH)\nIn the above list, streamliners 1 and 6 are implied constraints, while streamliner 5 is an implied constraint for only certain values of imbalance m, and streamliner 4 is a symmetry-breaking constraint. The remaining streamliners are neither implied nor symmetry-breaking constraints. The above list hints at the possibility of the LLMs having a good grasp of the problem we are trying to solve. We see a mix of implied constraints and streamlining constraints. Several of the suggested streamliners seem non-trivial, while others seem trivial, but they still significantly speed up the solving process."}, {"title": "Conclusion and Future Work", "content": "We introduced StreamLLM, a novel approach that utilizes the power of LLMs for automatic streamliner generation for constraint satisfaction problems. Originally, streamliners were hand-crafted or generated through systematic testing of atomic constraints. Our approach utilizes LLMs' creativity and analytical capabilities to automatically develop effective streamliners that mimic a deep understanding of the problem. By quickly testing the proposed streamliners on small problem instances, we can reduce the chance of errors or unsatisfiable instances on the streamliners we select. Our rigorous experiments exhibit significant improvements in solving times. In some cases, our approach could achieve running time reductions of 99% compared to the unstreamlined solving time. Even in a realtime scenario, StreamLLM achieves remarkable results. However, it can make sense to use our approach even for instances that only take about 30 minutes to solve. We could save more than 80 percent of the solving time for some larger instances. This percentage gets higher for even larger instances.\nStreamLLM's capability is not tied to a particular LLM, prompt, or filtering strategy, although GPT-40 with our static approach and PROMPT1 (or PROMPT3 as tested on a subset of problems) shows the best overall performance.\nAs Experiments 2 and 3 show, the obfuscated problems do not work significantly worse than the unobfuscated problems, indicating that StreamLLM might not depend on memorization and recognizes some deeper structure. Indeed, by taking a closer look at some of the best-performing streamliners proposed by StreamLLM, we see a mix of implied constraints and streamlining constraints, several of which seem non-trivial and could be proposed by a human expert, others look rather puzzling but still work efficient for speeding up the search.\nIt is worth pointing out how well we can scale from training instances to test instances; based on Experiments la and 1b, we can restrict the streamliner generation phase to only 15 training instances solvable in at most 10 seconds, whereas in other related areas, scaling has been identified as one of the main challenges (Bengio, Lodi, and Prouvost 2021).\nFor future work, we plan to extend StreamLLM to generate combinations of streamliners; we expect the combination of the strength of multiple streamliners to lead to even greater improvements. By investigating the influence of the temperature for LLM calls, one could find the best balance between creativity and predictability of the results.\nMoving from realtime to offline scenarios allows more training instances with longer running times and the generation of more streamliners. Using an evolutionary procedure based on pairing a pre-trained LLM with a systematic evaluator, as demonstrated impressively by Romera-Paredes et al. (2024), seems promising to try for streamliner generation. Further avenues for further work are to fine-tune LLMs for streamliner generation and to test whether small language models (SLMS) could offer a balance between performance and computational efficiency."}]}