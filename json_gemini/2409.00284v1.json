{"title": "Reframing Data Value for Large Language Models Through the Lens of Plausability", "authors": ["Mohamad Rida Rammal", "Ruida Zhou", "Suhas Diggavi"], "abstract": "Data valuation seeks to answer the important question, \"How much is this data worth?\" Existing data valuation methods have largely focused on discriminative models, primarily examining data value through the lens of its utility in training. However, with the push for ever-larger language models, relying on valuation methods that require training becomes increasingly expensive and dependent on specific techniques. We propose an alternative perspective on the data value problem for language models, centering around the plausibility of the data. We posit that data holds lesser value if it can be plausibly generated by the model itself. Starting from some intuitive criteria that align with our notions of valuable data, we develop a novel value function that is computationally tractable and derived from first principles with provable properties. We conduct a theoretical analysis of our value function and evaluate it across multiple scenarios and datasets.", "sections": [{"title": "1 Introduction", "content": "An ongoing challenge in language modeling is obtaining high-quality data to evaluate and improve model performance. However, the proliferation of large language models (LLMs) has raised concerns about the use of copyrighted material without proper authorization from the original creators. As data owners restrict access to data that was once publicly available, a natural question arises: How can we determine the inherent value of a dataset to a learning model? To explore this, we consider a hypothetical scenario involving two individuals: Alice and Bob. Alice owns a language model, while Bob possesses a dataset that Alice might want to acquire (or purchase) from him. We aim to assess the value of this dataset to Alice. Note that this differs from the standard way the data valuation problem is posed, where the goal is to fairly allocate credit for the validation performance of a learning algorithm among the training data.\nOne approach to determining acquisition value involves training the model. This approach typically re- quires training the model (often multiple times) with the dataset and assessing its impact on performance [7, 9, 8, 10, 21, 23]. However, training large language models can be prohibitively expensive and time- consuming, and the results may vary significantly depending on the training method. A dataset might be considered valuable with one algorithm but less so with another due to variability in outcomes and the specific task on which the model is evaluated. In this work, we propose an intuitive methodology that quantifies the value of data for language models through statistical means rather than through optimization or training.\nThe main intuition behind our approach arises from replacing the challenging question, \"what makes data valuable?\" with the closely related but more manageable \"What data is not worth acquiring?\" For a generative model, such as a large language model, there is a natural answer to the latter question: data that can be generated by the model itself. Referring back to the earlier scenario, if Bob's dataset was generated by Alice's"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Setup", "content": "We consider language models that take as input a sequence of tokens x1:n = (x1,...,xn) drawn from some vocabulary V. The models then output a probability distribution over the next token in the sequence p(\u00b7|x1:n). State-of-the-art language models typically have a maximum context length L, in which case the probability of the current tokens depends only on the previous L tokens. A standard value for L is somewhere between 512 and 4096 tokens.\nWe consider a dataset D = {d1,...,dx} composed of multiple datapoints. Each datapoint consists of a sequence of tokens of possibly variable length. Our goal is to find a measure of value for the dataset D assuming that our knowledge is represented through a language model, i.e., the transition distribution p it determines. In other words, we would like to find a value function V : D \u2192 V(D) which maps the dataset to a non-negative number and satisfies a number of desirable properties."}, {"title": "2.2 Properties of a Desirable Value Function", "content": "We argue that a good value function V should possess several key properties and that it should align with some of our intuitive notions about valuable data. Specifically, we are looking for a function which satisfies the following criteria:\nAdditivity: Given a dataset D = {d1,...,dx}, the value function V of the dataset should be additive in terms of the data points, i.e., V(D) = \u2211i=1V({di}). Assuming V({d}) \u2265 0 for all d, this would immediately imply that if D\u2081 and D2 are two datasets such that D1 \u2286 D2, then V(D1) \u2264 V(D2).\nBaseline: We require a quantifiable criterion for determining when a data value reaches zero, establishing a baseline against which we can make comparisons. Additionally, as previously outlined, we would like this to be a statistical property rather than one based on optimization or training.\nEfficient: V should be computationally feasible even for large vocabularies and context sizes. Moreover, V should prioritize sample efficiency, enabling evaluation for sequences of tokens regardless of length without the necessity for excessively lengthy datapoints."}, {"title": "2.3 Overview of the Solution", "content": "As previously mentioned, our approach stems from a shift in perspective: instead of focusing on what makes data valuable, we pivot towards identifying the data that does not carry value. In the context of a generative model, data generated internally holds no value or relevance to the model's owner. This then serves as the foundation of our approach. By initially focusing on identifying data generated by the model, we establish a starting point for developing a value function. To identify data generated by the model, we make use Rosenblatt's probability integral transform, see (3).\nIn particular, we are given a sequence of tokens X1,..., In, where each token maps to a unique integer in the set {1,...,|V|}. To prepare for Rosenblatt's transformation, which requires continuous variables, we construct a sequence of continuous variables 21,..., In using the transformation Xi = Xi - ui, where ui is a standard uniform random variable independent of xi. Our language model provides a sequence of probability mass functions over the next token p(\u00b7|x\u22121), one for each token in the input. We can transform these discrete probability functions provided by the model into continuous representations given by\n\nF(x | x\u00af) = \u2211_{j=1}^{[x]-1} p (j | x\u00af) + (x \u2212 [x]) p ([x]|x\u00af) (1)\n\nFinally, we evaluate the continuous yi's at their corresponding continuous distribution functions to obtain the new sequence zi = F (yi | x\u22121). We prove that if the sequence of tokens were generated by the language model, then the zi's must necessarily be independent and identically distributed U random variables (see"}, {"title": "2.4 Alternatives", "content": "In this section, we explore two other potential candidates we have considered for our value function, namely one centered on compression and the other on identity testing. We explain why we chose our method over these alternatives and discuss the insights gleaned from them that steered us towards our proposed approach.\nCompression: In information theory, prediction is often viewed from the lens of compression. If you can compress some data down to a small size, then it likely does not contain much new information beyond what you already knew (and vice versa). In particular, arithmetic coding is a technique which attains near-optimal compression rates when coupled with an accurate model (distribution) of the data. Large language models, coupled with arithmetic coding, have been shown to excel at compressing text data[4, 18]. The compression approach provides a concrete measure of data value: compress the dataset with arithmetic coding and use the compressed size (in bits or nats) as the measure.\nLet X1:n = (X1,X2...,xn) be a token sequence generated by a distribution q over Vn. It can be shown that any compression scheme corresponds to a distribution r over Vn, where the number of nats used to compress x1:n is 1/r(x1:n). A standard result asserts that the expected compressed data size L per input token is bounded below by H(q) + KL(q||r), where H is the entropy rate and KL(q||r) is the Kullback-Leibler divergence between q and r [3].\nTherefore, defining the value function as V(D) = L-H(q) ensures non-negativity, additivity, and a well-defined baseline since V(\u00b7) = 0 when r = q. However, computing H(q), the true entropy rate of the token sequence Xn, is non-trivial for large alphabet sizes. In particular, the min-max regret analysis of universal data compression shows that the estimation gap goes down as O(|V|L/n) [17, 22], where the token sequence is generated by an (unknown) L-order Markov chain; which means that for sources with large memory L and token al- phabet size, we need an extremely long sequence to obtain a good estimate, something that is often not feasible.\nIdentity Testing: Identity testing, also known as goodness-of-fit, deals with the following hypothesis testing problem: Given an explicit description of a probability distribution p, samples from an unknown distribution q, and bound \u20ac > 0, distinguish with high probability between hypothesis p = q and hypothesis ||q - P||1 > E whenever q satisfies one of these conditions[1, 5, 2, 19]. Identity testing presents a potential method of answering the question 'Is this data valuable to me?' where the threshold for \"valuableness\" is dictated by the constant \u20ac.\nHowever, the sample complexity required to solve the identity testing problem severely diminishes the practical utility of this method, especially in the context of large language models where p can be well approximated by a high-order Markov chain. In this setting, identity testing requires datasets containing most of the states in the chain, a demand that far exceeds the feasibility of any realistic dataset. Specifically, the requisite number of samples needed to solve the identity testing problem scales as \u03a9 (VL/\u20ac\u00b2) [20]. Nevertheless, we leverage some insights from distribution testing to propose our own approach for measuring value."}, {"title": "3 Value function and its properties", "content": "The hardness of the identity testing problem for complicated sources, e.g., natural languages, originates from the dependency among the samples and the ever-changing next token probabilities as the context grows. To this end, we propose to use Rosenblatt's transformation [15] which decouples the dependency among tokens and transforms the changing probabilities into the fixed uniform distribution. Rosenblatt's transformation serves as an extension to the probability integral transform (PIT), originally designed for univariate i.i.d. samples. It can be viewed as iteratively applying the PIT using the chain rule in probability (see Appendix A.1).\nTheorem 3.1 (Rosenblatt's transformation [15]). Let X = (X1, X2, ..., Xa) be a continuous d-dimensional random vector with a joint density function\n\nf(x1,...,xa) = f1(x1) f2(x2 | X1) \u00b7\u00b7\u00b7 fa(xa | X1, ...,Xd\u22121). (2)\nLet Fi(\u00b7|x1,...,xi\u22121) be the conditional cumulative distribution function corresponding to the conditional density function fi(\u00b7|x1,..., Xi\u22121). The random variables Z1,..., Za given by Rosenblatt's transformation\n\nZ\u2081 = F1(X1),\nZi = Fi(Xi | X1,..., Xi\u22121), i = 2,...,d, (3)\n\nare independent and identically distributed following standard uniform distribution U.\nTheorem 3.1 establishes that Rosenblatt's transformation provides an effective and unified approach for testing the plausibility of data. For example, given a random sequence X = (X1, ..., Xa) and a reference distribution Fref, we can obtain Z = (Z1,..., Za) by Rosenblatt's transformation of X according to Fref as in Equations (3). Once X follows the reference distribution Fref, the random vector Z will necessarily follow a uniform distribution over the d-dimensional hypercube [0, 1]d; otherwise, it will not. We thus have an equivalent statement for the identity hypothesis testing:\n\n{\nH0: X~ Fref\nH1: X\u2241 Fref \u21d4 [H0: Z ~ Unif([0,1]d)\n(H1: otherwise (4)\n\nNote that the testing on the right-hand side can be further decomposed into two terms: first, testing the closeness, which can be measured by any f-divergence, of the averaged marginal distributions of Z1,..., Zd to standard uniform distribution U. Second, testing the independence among Z1,..., Zd. Both of these are simpler and more standard tests in statistics. Motivated by this, we propose the Uniform-Marginal and Independence (UMI) value function that encapsulates both tests."}, {"title": "3.1 The Components of the Value Function", "content": "From Discrete to Continuous. Rosenblatt's transfor- mation in Eq (3) can only be applied to continuous ran- dom variables. However, given that the token vocab- ulary and probability mass functions generated by the language model are discrete, we adapt by transforming both into continuous forms. Since disparate discrete out- comes are mapped to disjoint intervals on the real line, our choice of transformation will have no effect on what the value function is trying to capture (see Theorem 3.2).\nLet X be a random variable taking values in the discrete set [k] = {1,...,k} with a probability mass function p. We con-"}, {"title": "3.2 The UMI Value Function", "content": "Given a sequence of tokens (x1,...,xn), we first calculate a sequence of probability mass functions p(\u00b7|x1:i\u22121) using our language model. We compute the continuous versions of the tokens 21,..., \u00cen by taking Xi ~"}, {"title": "3.3 Analysis", "content": "We relate the f-divergence between the marginal distribution of z and uniform distribution with the f- divergence between the data distribution and the language model."}, {"title": "3.3.1 IID Case", "content": "Let P and Q be two probability distribution over the finite set of tokens V, and let Fp and Fp (resp. Fq and Fq) be the cdf corresponding to P (Q) and its continuous counterpart, respectively. Suppose X, is a r.v. distributed according to Fq and X, according to Fq. Let Z = Fp(Xq) be the Rosenblatt's transformation of Xq by a possibly mismatched CDF F, and let G be its CDF. Note that G = U if P = Q. We show below the equivalence of their f-divergences, which indicates the measure under the marginal distribution of Z can indeed quantitatively capture the distance between the distributions in the token space.\nTheorem 3.2. Let P, Q and G be as described above, then for any f-divergence, we have\n\nDf(Q||P) = Df(G||U). (8)\n\nProof. The proof uses two properties of f-divergences which we state and prove.\nLemma 3.1. Let X and Y be any two random variables. Let Px,y = PxPy\\x and Qx,y = QxQY\\x be any two joint distributions, then\n\nDf (PX,Y || Qx,y) > Df (Px || Qx). (9)"}, {"title": "4 Experiments", "content": "Model and datasets. We now study the behavior of our value function on real-world large language models and datasets. In particular, we employ LLaMA2-7B as our language model, configured with a maximum context length of L = 512. We use \u20ac = 0.05 and a = 0.1 as the hyperparameters of the UMI function, and evaluate it over four distinct categories of data: (1) data generated by the model, (2) data generated by the same model using different parameters and sampling methods, (3) tokens and (ascii) characters generated uniformly at random, and (4) new data previously unseen by the model. While we intended to also evaluate our value function on data from the model's training set, the lack of publicized training data prevents us from confidently asserting which data was included. However, we can evaluate the model's performance on texts it has not encountered before, such as publicly available articles written after the model's publication.\nSampling Methods. In our analysis, we focused on multinomial (standard) sampling, where the next token is chosen according to the probability distribution provided by the model. However, practitioners use many other sampling methods to generate text from a language model. Thankfully, this does not change the"}, {"title": "5 Related Works", "content": "Over the past several years, there has been a significant line of research in data valuation, primarily focusing on discriminitive models from a training and optimization perspective. In [7], the 'Data Shapely' is introduced for the purpose of data valuation, where the value of a datapoint is the impact on the performance of the model when that datapoint is removed from the training set. Similarly, [9] defines the consistency score (C-score) of a datapoint as the expected of a held-out datapoint given training sets sampled from a data distribution. However, to estimate the data shapely and C-score of datapoints, the model needs to be trained multiple times. To reduce computational costs, [8] introduces more efficient algorithms for computing the data shapely when the learning algorithm satisfies certain assumptions, and suggests estimating the data shapely"}, {"title": "6 Conclusion", "content": "In this work, we introduced the theoretically-grounded UMI value function, based on Rosenblatt's transfor- mation. We analyzed this function theoretically and proved that it exactly matches the distribution distance between the model and the data in i.i.d setting. Moreover, it lower bounds the distribution distance in the Markovian setting. We evaluated our function on real-world datasets, and have showed its effectiveness in assigning a low value to data generated by the model under various situation. There are several open questions, including whether it is possible to integrate the semantic information of the dataset into our measure, since this is not currently incorporated by our proposed value function."}, {"title": "Appendix A Additional Proofs", "content": ""}, {"title": "A.1 Univariate Probability Integral Transform", "content": "We include the following theorem and its proof for completeness.\nTheorem. Suppose X is a real-valued random variable with a continuous distribution (cdf) Fx, then the random variable defined as Y := Fx(X) is uniformly distributed on the range (0,1).\nProof. We define the generalized inverse of the cdf Fx as\n\nFx\u00b9(y) = inf{x : Fx(x) \u2265 y}.\n\nWhen Fx is strictly increasing, the generalized inverse coincides with the standard definition of the inverse i.e. Fx\u00b9(y) = x \u2192 Fx(x) = y. However, if Fx is constant on some interval, then the standard definition fails. The generalized inverse takes care of this by assigning to Fx\u00b9(y) a single value. Now, let 0 < y < 1, then\n\nPr(Y < y) = Pr(Fx(X) \u2264 y)\n= Pr(Fx\u00b9Fx(X) \u2264 Fx\u00b9(y))\n= Pr(X < Fx\u00b9(y))\n= Fx (Fx\u00b9(y))\n= y. (20)\n(21)\n(22)\n(23)\n(24)\n\nThe justification behind the second line second line is Fx\u00b9 is always strictly increasing. The justification behind the third line is a bit tricky. If Fx is strictly increasing, then Fx\u00b9Fx(x) = x. However, if Fx is constant on some interval I = [a, b], then Fx\u00b9Fx(x) \u2260 x for all x \u2208 I \\ {a}. But since P(X \u2264 x) = P(X \u2264 a) for all x \u2208 I, this ends up making no difference."}, {"title": "A.2 Proof of Transformation Change", "content": "Lemma A.1. Let x \u2208 [k], u \u2208 (0,1), y = x \u2212 u, p(\u00b7 | xi\u00af\u00b9) a probability mass function over [k], and F(x|x-1) be defined as in 6, then\n\nF(y | x-1) = up(x | x\u2081\u00af\u00b9) + F(x | xi\u22121). (25)\n\nProof. Using the definition of F(y; | x-1) and the fact that [y] = x \u2212 1 and [y] = x, we have\n\nF(y | x\u00af\u00b9) = \u2211_{j=1}^{[y]}p(j | xi\u00af\u00b9) + (y - [y])p([y] | x\u22121)\n= F(x \u2212 1 | x\u00a1\u00af\u00b9) + up(x | x\u2081\u22121). (26)\n(27)\n\nThis proves our assertion."}, {"title": "A.3 Multivariate PIT", "content": "To extend this theorem to the case of multivariate random variables, a straightforward application of the multivariate cumulative distribution function will not work. To see why, consider the continuous random vector X = (X1, X2). Let Fi be the marginal distribution function of X\u00bf for i = 1,2, and F the joint distribution function. Let x1,x2 be any two real numbers, then\n\nF(x1, x2) = Pr(X1 \u2264 X1, X2 \u2264 x2),\n= Pr(F(X\u2081) \u2264 x1, F(X2) \u2264 F(x2)),\n= C(F(x1), F(x2)) (28)\n(29)\n(30)"}, {"title": "Appendix B Statistical Tests", "content": ""}, {"title": "B.1 Testing Uniformity", "content": ""}, {"title": "B.1.1 Kolmogorov-Smirnov", "content": "Given n observations U1,..., Un, we find first compute the empirical cumulative distribution function\n\nFn(X) = \u2211_{1-2}^{n} 1_{Ui\u2264x} (31)\n\nThe Kolmogorov-Smirnov statistic for a given cumulative distribution function F(x) is then\n\nDn = sup|Fn(x) \u2013 F(x)| (32)\nX\n\nThe Wiener process Wt is the characterised by the following properties:\n\u2022 Wo = 0 almost surely\n\u2022 W has independent increments: for every t > 0, Wt+u - Wt, u \u2265 0 are independent of the past values Ws, s <t\n\u2022 W has Gaussian increments: Wt+u - Wt is normally distributed with mean 0 and variance u i.e. Wt+u - Wt ~ N(0, u).\n\u2022 W has almost surely continuous paths: Wt is almost surely continuous in t.\nDefine the Brownian bridge as the stochastic process whose probability distribution is the conditional probability distribution of a standard weiner process subject to W\u2081 = 0, so that the process is pinned to the same value at both t = 0 and t = 1 i.e. Bt := (Wt | W\u2081 = 0), t\u2208 [0, 1] or in other words:\n\nBt = Wt-tW1 (33)\n\nDefine the random variable K = supt\u2208[0,1]|B(t)|. This random variable has what is called the Kolmogorov distribution:\n\nPr(K \u2264 x) = 1-2 \u2211_{(-1)}^{infty} (-1)^{k-1}e^{-2k^{2}x^{2}} = \u221a(2\u03c0) \u2211^{\u03c0} (e{-2(2k-1)^{2\u03c0}/8x^{2}} (34)\n\nTheorem B.1. Under null hypothesis that the sample comes from the hypothesized continuous distribution F(x),\n\n\u221a(n)DnK (35)\n\nThe goodness-of-fit test or the Kolmogorov-Smirnov test can be constructed by using the critical values of the Kolmogorov distribution. This test is asymptotically valid when n\u2192\u221e. It rejects the null hypothesis at level a if\n\n\u221a(n)Dn > \u039a\u03b1, (36)\n\nwhere Ka is found from Pr(K < K\u2084) = 1 \u2212 \u03b1."}, {"title": "B.1.2 Chi-Squared Test", "content": "Let [P1,...,Pk] be a discrete distribution over the categories {1, ..., k}. Let Y\u2081,..., Yn ben samples from a discrete distribution taking values over {1, ...,k}. Let (01, O2, ..., On) be the count number of samples from a finite set of given categories. They satisfy \u03a3i=1 Oi = n.\nThe null hypothesis is that the count numbers are sampled from a multinomial distribution Multinomial(n; P1, ..., Pk). That is, the underlying data is sampled IID from a categorical distribution Categorical(p1, ..., pn) over the given categories. The Pearson's chi-squared test statistic is defined as\n\nV := \u2211_{1}^{\u03c0} (37)\n\nTheorem B.2. Under the null hypothesis V ~ x\u00b2-1 i.e. a chi-squared distribution with v = k 1 degrees of freedom.\nIt is important to note that the chi-squared test is designed for discrete random variables. Let d be some positive integer, and define the new sequence Y\u2081,..., Yn as\n\nY; = [dU;] (38)\n\nThis is a sequence of integers that purports to be independently and uniformly distributed between 0 and d-1. The number d is chosen for convenience; for example, we might have d = 64 = 26 on a binary computer, so that Yn represents the six most significant bits of the binary representation of Un. The value of d should be large enough so that the test is meaningful, but not so large that the test becomes impracticably difficult to carry out."}, {"title": "B.2 Independence Testing", "content": ""}, {"title": "B.2.1 Poker test (Partition test)", "content": "The \"classical\" poker test considers n groups of five successive integers, {Y5j, Y5j+1,..., Y5j+4} for 0 \u2264 j < n, and observes which of the following seven patterns is matched by each (orderless) quintuple:\n\u2022 All different: abcde\n\u2022 One pair: aabcd\n\u2022 Two pairs: aabbc\n\u2022 Three of a kind: aaabc\n\u2022 Full house: aaabb\n\u2022 Four of a kind: aaaab\n\u2022 Five of a kind: aaaaa\nA chi-square test is based on the number of quintuples in each category. A simpler version of this test which is almost just as good is to count a number of distinct values in the set of five. In which case,\n\u2022 5 values = all different;\n\u2022 4 values = one pair;\n\u2022 3 values = two pairs, or three of a kind;\n\u2022 2 values = full house, or four of a kind;\n\u2022 1 value = five of a kind."}, {"title": "B.2.2 Permutation Test", "content": "Divide the input sequence into n groups of t elements each i.e. Vj = max(Utj, Utj+1,...,Utj+t\u22121) for 0 \u2264 j < n. The elements in each group can have t! possible relative orderings; the number of times each ordering appears is counted, and a chi-square test is applied with k = t! and with probability 1/t! for each ordering.\nIn order to perform this test, we need a method of indexing permutations. The set {0,1,2} can be permuted 3! = 6 ways. Those 6 permutations and their lexicographic ranks are:\n\u2022 0 \u2192 (0, 1, 2)\n\u2022 1 \u2192 (0,2,1)\n\u2022 2 \u2192 (1,0,2)\n\u2022 3 \u2192 (1,2,0)\n\u2022 4 \u2192 (2, 0, 1)\n\u2022 5 \u2192 (2,1,0)\nCalculating sequential indexes for permutations is done by computing the Lehmer code of the permutation, and then converting that Lehmer code to a base-10 number. Like that base-10 number, a Lehmer code is just a sequence of digits; However, each digit has a different base. Technically, it's a mixed-radix numeral system known as a factorial number system.\nFirst, let o be a permutation of n numbers {0, ..., n \u2212 1}. For example, if n = 3, we can represent oas\n\n\u03c3 =( 0 1 2\n1 2 0 ) (40)\n\nThis means that o places in the Oth position what used to be in the 1st position, in the 1st position what used to be in the 2nd position, and in the 2nd position what used to be in the Oth position. So \u03c3((0, 1, 2)) = (1, 2, 0) and \u03c3((2,0,1)) = (0,1,2). A more compact notation of the above is \u03c3 = (\u03c3\u03bf,...,n\u22121) which just include the second row of the above representation. The Lehmer code of a permutation L(\u03c3) = (L(\u03c3)\u03bf, . . ., L(\u03c3)n\u22121) is given by:\n\nL(\u03c3)\u2081 = |{j > \u03af: \u03c3; < \u03c3\u03af}| (41)\n\nIn other words, L(\u03c3); counts the number of terms to the right of \u03c3\u03b5 in (\u03c3\u03bf, ..., \u03c3\u03b7\u22121) which are smaller than it, and the number is between 0 and n \u2212 i \u2013 1; A pair of indices (i, j) with i < j and oi > 0; is called an inversion of \u03c3, and L(\u03c3); counts the number of inversions (i, j) with i fixed and varying j. For example, L((1,2,0)) = (1,1,0). Moreover, we can easily convert the Lehmer code into the permutation index. The index of the permutation is then \u2211i=0->^(L(\u03c3)i(n \u2212 i \u2212 1)! e.g. the index of (1,2,0) is 1(2!) + 1(1!) + 0(0!) = 3.\nWe can find the Lehmer code in linear time. Let b = 000b be a binary number with n bits and let \u03c3 = (\u03c3\u03bf,..., \u03c3\u03b7\u22121) be our permutation, then for i = 0, . . ., n \u2212 1, we\n1. We flip bit the oith bit of b\n2. Right-Shift b by n \u2212 \u03c3\u03b5 to get b'.\n3. Count the number of 1s in b' and subtract it from \u03c3\u03b5 to get the Lehmer code L(\u03c3)\u03af.\nProof of correctness: Say we are at position i in the loop. Until now bit j of b is 1 iff j is to the left of \u03c3\u03af. We you shift b by n \u2013 \u03c3\u03b5 positions to the right, and what would be left is the first \u03c3\u03b5 bits of b corresponding to the numbers 0,..., \u03c3i \u2013 1."}, {"title": "B.2.3 Serial Correlation Test", "content": "We may also compute the following statistic:\n\nC =(n (UoU1 + U1U2 + ... Un-2Un\u22121 + Un-1Uo) \u2013 (Uo + U1 + . . . + Un\u22121)\u00b2)/(n (U3 + U\u00b2 + ... + U2\u22121) \u2212 (Uo + U1 + . . . + Un\u22121)\u00b2) (42)\n\nThis is the \"serial correlation coefficient,\" a measure of the extent to which Uj+1 depends on Uj. Correlation coefficients appear frequently in statistical work. If we have n quantities Uo, U1,...,Un\u22121 and n others Vo, V1,..., Vn-1, coefficient between them is defined to be:\n\nC = (n\u2211(UjV;) \u2013 (\u2211U;)(\u2211V;))/(\u221a(\u03b7 \u03a3\u03c5 \u2013 (\u03a3\u03c5\u2081)2) (n\u2211V\u00b2 \u2013 (\u2211V;)2)) (43)\n\nWhen C is zero or very small, it indicates that the quantities U; and V; are uncorrelated (a weaker notion than independence), whereas a value of \u00b11 indicates total linear dependence. Therefore it is desirable to have C close to zero. In actual fact, since UoU1 is not completely independent of U1U2, the serial correlation coefficient is not expected to be exactly zero.\nIn other words, we are finding the correlation between (Uo, U1,..., Un\u22121) and (U1,...,Un-1, Uo). We can also compute the correlation coefficient between (Uo,U1,...,Un\u22121) and any cyclically shifted sequence (Uq, ..., Un\u22121, Uo, ..., Uq\u22121). A naive approach to computing the cyclic correlations takes O(n\u00b2). However, one can use the Fast Fourier Transform to find all these correlations in O(nlogn)."}, {"title": "B.2.4 Serial Test", "content": "More generally, we want pairs of successive numbers to be uniformly distributed in an independent manner. We can also use the chi-squared test to test for independence (somewhat).\nTo carry out the serial test, we simply count the number of times that the pair (Y2j, Y2j+1) = (q, r) occurs, for 0 <= j < n; these counts are to be made for each pair of integers (q,r) with 0 \u2264 q, r < d, and the chi-square test is applied to these k = d\u00b2 categories with probability 1/d\u00b2 in each category.\nAs with the equidistribution test, d may be any convenient number, but it will be somewhat smaller than the values suggested above since a valid chi-square test should have n large compared to k. A common rule of thumb is to take n large enough so that each of the expected values npi \u2265 5 i.e. the expected number of occurrences of each categorical outcomes is five or more; preferably, however, take n much larger than this, to get a more powerful test. Hence, here we need n \u2265 5d\u00b2."}, {"title": "B.2.5 Gap Test", "content": "Another test is used to examine the length of \"gaps\" between occurrences of U; in a certain range. Let a and \u1e9e be two real numbers with 0 < a < \u03b2 < 1, and suppose Uj \u2208 [\u03b1, \u03b2), then we want to consider the lengths of consecutive subsequences Uj, Uj+1, ..., Uj+r_in which Uj+r lies between a and \u1e9e but the other U's do not. This subsequence of r + 1 numbers represents a gap of length r.\nThe classical gap test considers the sequence U1, ..., Un to be cyclic sequence with Un+j identified with Uj. If m of the numbers U1, ..., Un fall into the range [\u03b1, \u03b2), there are n gaps in the cyclic sequence. We have the following theorem:\nTheorem B.3. Lett be some positive integer. Let Gr be the counts of the gaps of length r for 0 \u2264 r \u2264 t -1, and Gt the count of gaps of length > t. Define p = \u03b2 - a, and let\n\nPr = p(1-p)"}]}