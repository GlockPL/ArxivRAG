{"title": "CNN-BASED EXPLANATION ENSEMBLING FOR DATASET, REPRESENTATION AND EXPLANATIONS EVALUATION", "authors": ["Weronika Hryniewska-Guzik", "Luca Longo", "Przemys\u0142aw Biecek"], "abstract": "Explainable Artificial Intelligence has gained significant attention due to the widespread use of com-\nplex deep learning models in high-stake domains such as medicine, finance, and autonomous cars.\nHowever, different explanations often present different aspects of the model's behavior. In this\nresearch manuscript, we explore the potential of ensembling explanations generated by deep classifi-\ncation models using convolutional model. Through experimentation and analysis, we aim to investi-\ngate the implications of combining explanations to uncover a more coherent and reliable patterns\nof the model's behavior, leading to the possibility of evaluating the representation learned by the model.\nWith our method, we can uncover problems of under-representation of images in a certain class.\nMoreover, we discuss other side benefits like features' reduction by replacing the original image with\nits explanations resulting in the removal of some sensitive information. Through the use of care-\nfully selected evaluation metrics from the Quantus library, we demonstrated the method's superior\nperformance in terms of Localisation and Faithfulness, compared to individual explanations.", "sections": [{"title": "1 Introduction", "content": "Deep learning models, despite their unprecedented success [1, 2], lack full transparency and interpretability in their\ndecision-making processes [3, 4]. This has led to growing concerns about the use of \"black box\" models and the need\nfor explanations to better understand their inferential process [5]. Using examples of specific cases from a dataset,\ngenerated explanations might reveal which elements are most important in a model's prediction [6, 7, 8].\nCurrently, explanations generated for a trained deep learning models often are presented as individual insights that need\nto be investigated separately and then compared [9]. Each explanation provides a limited view of the model's decision,\nas it tends to focuse on specific aspects, making it challenging for a human to obtain a comprehensive understanding.\nThis approach hinders the ability to discern the reasons behind a model's predictions.\nThere has been an emerging trend in explanation ensembling, which is derived from model ensembling, which involves\ncombining multiple predictive models to reduce variation of predictions which often leads to higher overall performance.\nExamples of such predictive techniques are random forests [10] and gradient boosting [11]. This tendency shows that it\nis plausible that individual explanations possess unique pieces of information that, when combined, might form a more\ncomprehensive and accurate understanding of a model's inferential process. [12, 13]"}, {"title": "2 Related works", "content": "Explanations for deep learning models have gained considerable attention in the field of computer vision, particularly\nin the context of post-hoc analysis. Various approaches have been proposed to generate meaningful explanations\nfor deep learning models [14]. Perturbation-based methods, such as Lime [7], Occlusion [31], and Shapley Value\nSampling [32], involve modifying an input image to evaluate the impact on a model's output and identify significant\nregions or features. On the other hand, gradient-based methods, including Integrated Gradients [6], DeepLift [33],\nGuided GradCAM [34], GradientShap [8], or Guided Backpropagation [35], utilize gradients to highlight the salient\nregions of an input image that contribute most to the model's inferential process. Additionally, Noise Tunnel [36] is\nan approach that introduces noise into the input image and analyzes its impact on the model's output.\nThe ensemble of explanations has not yet received significant attention in the field of Explainable Artificial\nIntelligence (XAI). Zou et al. [12] proposed an approach for XAI ensembling using a Kernel Ridge regression [37]\nto combine the normalized Grad-CAM++ technique [34] and the normalized positive SHAP values. In an alternative\nmethod for XAI ensembling based on metrics (consistency, stability, and area under the loss) [13], the authors\ncalculated the weighted importance of three different metrics and created a combined vector of explanations by\ntaking a weighted sum of ensemble scores and their corresponding original explanations. Another study explored\nthe use of mean to combine explanations generated by multiple explanation methods [38], aiming to provide a more\nnoise-resistant ensembled explanation. Additionally, Bhatt et al. [39] proposed a XAI ensembling method based\non optimization around the points of interest, minimizing explanation sensitivity and bridging the gap between AI\ndecision-making system and human interpretability, particularly in the context of medical applications. These research\nworks contribute to the growing body of research on XAI ensembling, offering various techniques and perspectives"}, {"title": "3 The concept of CNN-based ensembled explanations", "content": "In our proposed methodology, presented in Figure 1 and Algorithm 1, we introduce a novel approach for ensembling\nexplanations to enhance the interpretability of classification models. The process begins by generating explanations\nusing established techniques for the trained classification model. Consider a set of p explanation methods. Let Eij\nrepresent the explanation created by j-th method for the i-th image (Ii) in the dataset. For each image, we aim\nto generate a set of explanations:\nEi = {Ei1, Ei2, ..., Eip}. (1)\nTo ensemble these explanations, we employ two strategies depending on the availability of ground truth masks. Let\nMi denote the pixel-wise annotated mask corresponding to the i-th image. In the first strategy, we utilize pixel-wise\nannotated masks for a selected class of images, which serve as a reference for generating ensembles. Alternatively,\nin cases where annotated masks are not readily available, a domain specialist can manually annotate a small subset\nof representative images.\nOnce the explanations are generated, our objective is to train a CNN-based explanation ensembling model (XAI\nEnsembler), shown in Figure 2, to predict masks given explanations. The training procedure involves minimizing\nthe segmentation loss Lseg as follows:\nLseg = \\sum_{i=1}^{n} SoftDice(M, XAI\\_Ensembler(Ei)), (2)"}, {"title": "3.1 Experimental setup for training", "content": "Our experiments employed a segmentation model based on CE-Net [56], which has shown promising performance\nin various image analysis tasks [57]. The dataset used for training and evaluation was ImageNet-S50 [58]. We split\nthe data 80/20 for training and testing. The input images were resized to 224x224 pixels. During training, we utilized\na learning rate of 0.0002, and the learning rate was halved when no improvement was observed, with a minimum\ninterval of 20 epochs. The training process was stopped if the learning rate dropped below 10-9, or after 200 epochs.\nTo optimize the segmentation model, we employed the Soft dice loss [55] and utilized the Adam optimizer [59].\nThe Soft dice loss has been proven effective in handling classes with lesser spatial representation in an image, and is\ncommonly used in medical image segmentation tasks [60].\nTo enhance the robustness of the model and minimize overfitting, we applied various image transformations, which are\nstandard for ImageNet. These included a center crop to focus on the central region of the image, Imagenet normalization\nto standardize pixel values, random adjustments of hue and saturation values, random shifts, scaling, and rotations,\nrandom flips to introduce horizontal and vertical symmetry variations, as well as random rotations by 90\u00b0.\nThe code written using PyTorch library is available on Github: https://github.com/Hryniewska/\nCNNbasedXAlensembling."}, {"title": "3.2 Ablation studies", "content": "The objective of our experiments is to select the best ensemble model's architecture. We created three different model's\narchitectures, presented in Figure 2. The first and the second architecture used many encoders. Each encoder takes one\nexplanation method as an input; therefore the number of encoders is linearly dependent on the number of explanations\nthat are going to be ensembled.\nIn the first architecture, we used concatenation as the method for combining inputs from multiple encoders. Concate-\nnation involves stacking encoders' outputs along a new channel dimension. This type of stacking is useful when you\nwant to retain all the information from each input and allow the network to learn how to use each input separately. It is\nparticularly suitable for problems where the inputs represent different aspects of the problem. It is worth noting that\nthe dimensionality of the input space increases when using concatenation, potentially leading to a large number of\nmodel parameters. This architecture's total number of parameters is 17 675 256 multiplied by the number of encoders\nsquared.\nThe second approach employed summation as the combining method. It entails an element-wise addition of the\nencoder outputs, channel by channel. Summation is used when the inputs are expected to be complementary, and it\nemphasizes their joint influence on the output. This method helps keep the model's number of parameters low (17 675\n256 parameters) and can be beneficial when the inputs are additive.\nThe last architecture consists of a single encoder that takes explanations in channels, resulting in the number of channels\nbeing a product of three and the number of explanations for ensembling. The Captum library [14], employed for\ngenerating explanations, provides output with three channels for each explanation. The output values fall within the\nrange of [-1, 1] or [0, 1], depending on the explanation method. Additional channels in convolutional layers are\nappropriate when we need to leverage the spatial relationships in the inputs while allowing the network to learn features\nfrom each input independently. It is commonly used in tasks like multi-modal image processing, where different inputs\n(RGB image, depth map, infrared image) are processed jointly to improve feature extraction [61]. The number of\nparameters is equal to 17 675 256.\nTypically, the choice among these methods depends on empirical experimentation and the specific requirements of a ma-\nchine learning or deep learning task. It is important to consider the unique characteristics of the data and the problem\nto determine which input combination method is most appropriate.\nFor this reason, we trained all three architectures using the same training procedure, as described in Section 3.1.\nThe proposed model architectures were trained on several dozen models with the same configurations encompassing\nvarious explained deep learning classification models, explanation methods, and specific classes. These configurations\nincluded explained deep learning classification models, such as SqueezeNet1.1 MobileNet v2, VGG16, DenseNet121,\nResNet50, and EfficientNet B0. An effort was made to select diverse classes, e.g. objects that are large or small,\npopular or rare, always look the same or always differently. The target classes comprised digital watch, dining table,\ngibbon (monkey), grand piano, kuvasz (dog), ladybird, purse, umbrella, beach wagon, water bottle, and water tower."}, {"title": "3.3 Method evaluation", "content": "The decision to employ the Quantus library for the evaluation of our CNN-based XAI explanation ensembling model is\nrooted in its capability to offer a comprehensive and systematic comparison of various XAI methods. Quantus [45]\nprovides a library for evaluating the performance of XAI techniques, ensuring a thorough examination of key aspects.\nThe chosen metrics for this evaluation were thoughtfully selected to optimize the meta-consistency score [62]. It is\npivotal as it indicates the stability and reliability of an XAI method across different contexts and datasets. Prioritizing\nmetrics contributing to a higher meta-consistency score ensures that our method not only explains individual instances\neffectively but also maintains consistency across diverse scenarios.\nThe radar plot in Figure 3 visually represents the comparative performance of our CNN-based XAI ensembling method\nagainst individual non-ensembled XAI methods. The inclusion of key evaluation metrics-Faithfulness, Robustness,\nLocalisation, Complexity, and Randomisation\u2014offers a holistic view of a method's capabilities. The subsequent\nparagraphs delve into detailed explanations of each of these five metrics."}, {"title": "4 Metrics for representation, dataset and explanation evaluation", "content": "While post-hoc explainability methods provide insights into individual images, they lack a holistic view. These methods\nfail to comprehensively assess factors like dataset representativeness, learned representations, and explanation quality.\nFurthermore, relying solely on ImageNet accuracy can be misleading. Even visualization techniques like t-SNE fail\nin quantitative assessment of data representation quality.\nRepresentation-oriented ensembling performance metric\nTo address these challenges, a novel metric is proposed for quantitatively assessing data representation quality through\nCNN-based XAI ensembling. The objective of explanations is to reveal the elements within an image that guide\nthe model's inferential process. Evaluating these explanations becomes challenging when multiple methods generate\ndiverse interpretations [67]. This is where CNN-based XAI ensembling could prove valuable. The model endeavors\nto learn to segment the object by exclusively considering explanations and the ground-truth mask of the classified object.\nSuccess in learning indicates that explanations might effectively represent the classified object, minimizing the impact\nof noise for an accurate interpretation."}, {"title": "4.1 Representation evaluation", "content": "High accuracy on datasets like ImageNet may not be enough for thorough model evaluation [23]. The experiment focuses\non adding an interpretability layer to enable a model's explanation, allowing a deeper understanding of the learned\nrepresentations beyond accuracy metrics. This aligns with the perspective that assessing model performance requires\nconsidering not only predictive accuracy but also interpretability and the meaningfulness of the learned representations.\nThis experiment aims to determine which classification model from SqueezeNet1.1 MobileNet v2, VGG16,\nDenseNet121, ResNet50, and EfficientNet B0 learned the best representation. To achieve this, we trained XAI\nensembling models using explanations generated by mentioned models. We decided to average the results for eleven\nclasses and three different sets of explanations, listed in legends to Figures 5b and 5c, respectively. The results are\nshown in Figure 5a. Moreover, in Figure 5a a performance comparison of various architectures on ImageNet dataset is\npresented, illustrating their top-1 accuracy."}, {"title": "4.2 Dataset evaluation", "content": "In this experiment, our objective was to investigate whether the difficulty of an image class from the ImageNet dataset\ncould be evaluated through explanation ensembling. We trained CNN-based explanation ensembling on eleven various\nimage classes using 5-fold cross-validation, explanations generated by ResNet50, and using three sets of explanation\nmethods. Cross-validation is important especially for dataset evaluation, as it reduces the impact of splitting the dataset\ninto training and test data. The averaged across cross-validations performance metrics are shown in Figure 5b.\nBy analyzing the performance metrics, we can gain insights into the difficulty of each image class. Firstly, identifying\nclasses with the lowest ens(fl) and ens(IoU) highlights challenging classes, such as \"dining table\" and \"grand piano\".\nAddressing the difficulties in recognizing these classes may be crucial for applications like training models for object\nrecognition in autonomous vacuum cleaners. Particularly, in some cases, it is important to ensure fairness by preventing\nsubjects such as \"monkeys\" from being misidentified as \"humans\". Secondly, it is important to examine the cases where"}, {"title": "4.3 Explanations evaluation", "content": "In this section, we present the experimental results of our study, which aimed to answer the research question of how well\nthe explanations succeeded in extracting relevant features. We also determine the influence of each input explanation on\nthe final result.\nThe experimental setting is the same as in the previous subsection. To understand the significance of the number of XAI\nmethods incorporated in the CNN-based explanation ensembling, we visualised the results in Figure 5c. The measure\nexh(iou) determines how much of the important features are left in the picture. Our analysis revealed that the results\nof segmenting an original image and applying XAI ensembling on seven different explanations obtained almost the same\nresults.\nThese findings indicate that, based on the selected performance metric, seven XAI methods have the potential to deliver\nthe same predictive performance as the original images. The original image contains many more features than its\nexplanations. Demonstrating that explanations can serve as substitutes for original images, this approach holds promise\nfor facilitating reduction of redundant information or the anonymization process of images. Anonymization involves\ntransforming data into a format that does not disclose individuals' identities. In this context, one possible application\ncould be modifying existing images [71], or alternatively, employing synthetic data generation as an alternative method\nfor data anonymization.\nSeveral studies report that no single explanability method performs best in all metrics evaluating the quality of an ex-\nplanation [72, 73, 74]. However, it is hard to clearly state how explanability methods should be compared among\nthemselves. It is possible to choose a single metric, but according to each metric, the result may be different. Faced\nwith this, comes the problem of evaluating the validity of metrics for evaluating explanations among themselves.\nIn our solution, we do not have such a problem. Our explanation ensembling solution allows, after training the XAI\nEnsembler, to disable, in other words set the attribution values to zero, individual input explanations. Disabling one\ncomponent explanation can measure its validity in the aggregated explanation. For this reason, we check the impact\nof different explanability methods on the ensembling result."}, {"title": "5 Conclusions and future works", "content": "The novel aspect of our CNN-based explanation ensembling lies in using explanations to evaluate the learned representa-\ntion. Our approach provides a unique way to assess the difficulty of each class and the quality of trained models. It aids\nin the selection of a pretrained backbone for tasks like detection, providing quantitative evaluation and dataset analysis.\nNoteworthy is the possibility of ensembling explanations from trained vision transformers, as their explanations are\nalso commonly presented as heatmaps [75].\nWe gain insights into where the model excels and where it struggles by analyzing the performance metrics of the gen-\nerated explanations for many class. Through the study on the elements that should or should not be present in the\nensembled explanation, our method helps to predict the possibility of bias, which is particularly crucial for sensitive\nattributes, and suggests strategies to address imbalances in the dataset. This valuable information can guide further\nimprovements in dataset preparation and model training. Our approach finds unnecessary information in data, which\nmay help in designing algorithms that enhance anonymity.\nMorever, our CNN-based explanation ensembling presents a novel approach to address the challenge of generating\ncomprehensive explanations, in which we do not have to worry about the importance of each of the aggregated\nexplanation methods. Through the use of carefully selected evaluation metrics from the Quantus library, we demonstrated\nthe method's superior performance in terms of Localisation and Faithfulness, compared to individual XAI methods.\nWhile we have highlighted the key strengths of our approach, it is essential to acknowledge its limitations. The perfor-\nmance of our method is highly dependent on the quality and diversity of the training data, as limited or biased training\ndata can result in suboptimal explanations and potentially undermine a model's reliability. Furthermore, ensembling\nexplanations using CNNs can be computationally intensive, which may limit its applicability in resource-constrained\nenvironments. Interpretability remains an ongoing challenge, as even the best explanations might not fully capture\nthe complex inferential processes of deep learning models. It is crucial to recognize these limitations to ensure\nthe responsible and informed use of the CNN-based explanation ensembling.\nIn light of these limitations, future research can explore several directions. First of all, we see a growing area of universal,\nrobust feature extractors, such as DINOv2 [76]. We believe that the CNN-based explanation ensembling might be\navailable as pretrained backbone and work on many different classes. Additionally, an idea of the trainable XAI\nEnsembler can be generalised to other modalities. The code for our CNN-based explanation ensembling is available at\nhttps://github.com/Hryniewska/CNNbasedXAIensembling."}]}