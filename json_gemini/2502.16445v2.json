{"title": "ITERATIVE FLOW MATCHING PATH CORRECTION AND\nGRADUAL REFINEMENT FOR ENHANCED GENERATIVE\nMODELING", "authors": ["ELDAD HABER", "SHADAB AHAMED", "MD. SHAHRIAR RAHIM SIDDIQUI", "NILOUFAR\nZAKARIAEI", "AND MOSHE ELIASOF"], "abstract": "Generative models for image generation are now commonly used for a wide variety of\napplications, ranging from guided image generation for entertainment to solving inverse problems.\nNonetheless, training a generator is a non-trivial feat that requires fine-tuning and can lead to\nso-called hallucinations, that is, the generation of images that are unrealistic. In this work, we explore\nimage generation using flow matching. We explain and demonstrate why flow matching can generate\nhallucinations, and propose an iterative process to improve the generation process. Our iterative\nprocess can be integrated into virtually any generative modeling technique, thereby enhancing the\nperformance and robustness of image synthesis systems.", "sections": [{"title": "1. Introduction", "content": "Generative AI can be thought of as a point cloud matching\nproblem. Given data points xo that are sampled from a distribution \\(\\pi_0(x)\\) and final\npoints xy that are sampled from a distribution \\(\\pi_\\tau(x)\\), our goal is to find a function\nthat transforms points from \\(\\pi_0(x)\\) to \\(\\pi_\\tau(x)\\) [37, 19, 42, 40]. Such a transformation is\noften modeled as a stochastic or deterministic process that maps points from the source\ndistribution \\(\\pi_0(x)\\) to the target distribution \\(\\pi_\\tau(x)\\) while preserving key statistical\nproperties [1]. One approach to constructing such a transformation is through transport\ntheory, where we seek a transport map T such that \\(x_\\tau = T(x_0)\\) and the induced\npushforward distribution \\(T_\\#\\pi_0\\) closely matches \\(\\pi_\\tau\\) [32].\nWhile it is straightforward to obtain such maps in low dimensions (see e.g. [5,\n17, 2, 29, 10]), solving the problem for high dimension is difficult. A widely used\nalternative to finding such maps is to parameterize the transformation through a\nso-called generative model, such as normalizing flows, generative adversarial networks\n(GANs), variational autoencoders or flow matching and diffusion models. Each of\nthese models approach the problem differently:\n\\begin{itemize}\n    \\item Normalizing flows [33, 35]: These models construct an invertible trans-\n    formation \\(f_\\theta\\) such that if \\(x_0 \\sim \\pi_0(x)\\), then \\(x_\\tau = f_\\theta(x_0)\\) follows \\(\\pi_\\tau(x)\\). The\n    transformation is learned by maximizing the likelihood of training data while\n    ensuring that the Jacobian determinant of \\(f_\\theta\\) remains tractable.\n    \\item Generative adversarial networks (GANs) [7, 16, 4, 3]: GANs learn to\n    generate samples from \\(\\pi_\\tau(x)\\) by training a generator \\(G_\\theta\\) that maps noise \\(z\\sim\\)\n    \\(\\pi_0(z)\\) to the data space. A discriminator network \\(D_\\phi\\) is used to distinguish real\n    samples from generated ones, and both networks are trained in an adversarial\n    manner to improve the quality of the generated data.\n    \\item Variational autoencoders (VAEs) [22, 23]: VAEs model the data distri-\n    bution by learning a probabilistic latent space representation. They introduce\n    an encoder network that maps data to a latent distribution and a decoder\n    network that reconstructs data from latent variables. The training objec-\n    tive consists of maximizing a variational lower bound on the data likelihood\n\\end{itemize}"}, {"title": "2. Flow Matching.", "content": "We now describe the concept of flow\nmatching (FM)", "as": "n\n\n\nwhere x\\(_\\tau\\) are sampled from \\(\\pi_\\tau(x)\\), x\\(_0\\) is sampled from \\(\\pi_0(x)\\) and time \\(t\\in [0,T", "1": ".", "velocity\" or flow v is defined as\n\n\nNote that the velocity field v is defined at points x\\(_t\\), that is, on all the trajectories\nthat lead any point in \\(\\pi_0\\) to any point in \\(\\pi_\\tau\\). The trajectories collide at times t = 0\nand t = 1, however, in principle, they can collide or pass very close to each other even\nat other times.\nThe main idea behind flow matching is to approximate (learn) a velocity field\nv(x\\(_t\\)) by a function v\\(_\\theta\\)(x,t), parameterized by learnable weights \\(\\theta\\), by solving the\nstochastic optimization problem,\n\n\n\nIn the deterministic framework, which we adopt here for simplicity, given the learned\nvelocity model v\\(_\\theta\\), one uses it at inference and integrates the ordinary differential\nequation (ODE),\n\n\nto obtain samples from the target distribution \\(\\pi_\\tau(x)\\).\nAt the core of flow matching stands\nthe solution of the optimization problem given by Equation 2.3. The problem has a very\nintuitive form. At time t, we are given some points x\\(_t\\) and a corresponding velocity\nv(x\\(_t\\)). Our goal is to interpolate the flow field to every point x in space. This enables\nthe integration of the ODE (Equation 2.4) for all times. In this paper, we consider\ntwo types of approximations for the flow field. In the first, v\\(_\\theta\\)(x, t) is parameterized\nby a neural network. Solving the optimization problem (Equation 2.3) is therefore\ndone by training the network to predict v(x\\(_t\\)). This approach is particularly useful in\nhigh dimensions.\nFor problems in low dimensions, a simpler approach can be utilized. We use a\nradial basis function (RBF) approximation to generate an interpolant for the velocity\nfield v(x, t) given the data v(x\\(_t\\)) (see [6": "for details). To this end, we approximately\nsolve the linear system,\n\n\n\nusing the Conjugate Gradient method, where \\(\\Phi(x_t, x_t)\\) is the kernel matrix. We\nuse a Gaussian exponential kernel with a tunable smoothing parameter that is a\nhyperparameter. Given the coefficients \\(\\Theta_t\\), we then evaluate v(x, t) by,\n\n\n\nwhere \\(\\beta\\) is a hyperparameter to avoid over-fitting and resolve inconsistencies in the\ndata. Putting it all together, the ODE in Equation 2.4 can be written as,"}]}