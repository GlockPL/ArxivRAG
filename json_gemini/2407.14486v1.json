{"title": "Explainable Post hoc Portfolio Management Financial Policy of a Deep Reinforcement Learning agent", "authors": ["Alejandra de la Rica Escudero", "Eduardo C. Garrido-Merch\u00e1n", "Mar\u00eda Coronado-Vaca"], "abstract": "Financial portfolio management investment policies computed quantitatively by modern portfolio theory techniques like the Markowitz model rely on a set on assumptions that are not supported by data in high volatility markets such as the technological sector or cryptocurrencies. Hence, quantitative researchers are looking for alternative models to tackle this problem. Concretely, portfolio management is a problem that has been successfully addressed recently by Deep Reinforcement Learning (DRL) approaches. In particular, DRL algorithms train an agent by estimating the distribution of the expected reward of every action performed by an agent given any financial state in a simulator, also called gymnasium. However, these methods rely on Deep Neural Networks model to represent such a distribution, that although they are universal approximator models, capable of representing the previous distribution over time, they cannot explain its behaviour, given by a set of parameters that are not interpretable. Critically, financial investors policies require predictions to be interpretable, to assess whether they follow a reasonable behaviour, so DRL agents are not suited to follow a particular policy or explain their actions. In this work, driven by the motivation of making DRL explainable, we developed a novel Explainable Deep Reinforcement Learning (XDRL) approach for portfolio management, integrating the Proximal Policy Optimization (PPO) deep reinforcement learning algorithm with the model agnostic explainable machine learning techniques of feature importance, SHAP and LIME techniques to enhance transparency in prediction time. By executing our methodology, we can interpret in prediction time the actions of the agent to assess whether they follow the requisites of an investment policy or to assess the risk of following the agent suggestions. To the best of our knowledge, our proposed approach is the first explainable post hoc portfolio management financial policy of a DRL agent. We empirically illustrate our methodology by successfully identifying key features influencing investment decisions, which demonstrate the ability to explain the agent actions in prediction time.", "sections": [{"title": "1 Introduction", "content": "Financial portfolio management is a critical task in the investment domain [1], traditionally guided by modern portfolio theory techniques such as the Markowitz model [2]. These models, which optimize the trade-off between risk and return, are grounded in a set of assumptions about market behavior [3]. However, for example, in high-volatility markets like the technological sector or cryptocurrencies, these assumptions often fail to hold true [4]. This discrepancy has led quantitative researchers to seek alternative methods that can better handle the dynamic and unpredictable nature of such markets [5].\n\nThe application of Deep Reinforcement Learning (DRL) [6] to portfolio management has gained popularity in recent years. Concretely, DRL algorithms train agents to maximize expected returns by learning optimal actions through interaction with a simulated environment [7], also known as gymnasium [8]. These agents use Deep Neural Networks models (DNNs) [9] to approximate the distribution of expected rewards for different actions in varying financial states. Despite their capability as universal function approximators [10], DNNs suffer as they lack interpretability [11]. This means a challenge in financial contexts, where decision-making transparency is crucial for investors to trust and adopt automated strategies [12].\n\nThe need for explainability in financial decision-making drives the development of Explainable Artificial Intelligence (XAI) techniques [13,14,15]. In the context of DRL, incorporating XAI methods enhance the high performance of DRL agents and the necessity for transparent, interpretable investment policies. But, as we will show in section 2 (sate of the art), despite the growing literature that analyzes the application of DRL to portfolio management, the literature on the explainability of DRL algorithms applied to portfolio management is very scarce and underdeveloped, with only four recent studies [16,17,18,19], to the best of our knowledge. Moreover, these four published DRL explainability methods in portfolio management, only offer explanations of the model in training time, not being able to monitor the predictions done by the agent in the trading time.\n\nDriven by this motivation, and to respond to this research gap in the literature, in our work, we introduce a novel Explainable Deep Reinforcement Learning (XDRL) framework for portfolio management. Concretely, our approach combines the popular Proximal Policy Optimization (PPO) algorithm [20], a state-of-the-art DRL technique, with model-agnostic XAI methods such as feature importance [21], SHAP (SHapley Additive exPlanations) [22], and LIME (Local Interpretable Model-agnostic Explanations) [23]. By doing so, we obtain interpretability of DRL trained agents. The three explainability techniques can be implemented independently or jointly, being able to explain the DRL agent predictions in trading time, being able to track throughout the time whether the policy is acting as it is expected or not, what is an advantage with respect to the rest of four published DRL explainability methods just mentioned. Our working hypothesis is that the predictions of DRL agents can be explained in a post-hoc"}, {"title": "2 State of the art", "content": "The application of DRL in financial portfolio management is gaining popularity in the recent years, mainly due to the rise of computing power and architectures that enables a reasonable estimation of the rewards distribution of the actions with respect to the states given by the training process of the agents with respect to financial data [24]. But despite the growing literature that analyzes the application of DRL to portfolio management, the literature on the explainability of DRL algorithms applied to portfolio management is very scarce and underdeveloped, with only four recent studies [16,17,18,19], to the best of our knowledge. In this section, we show a detailed state-of-the-art of DRL and XDRL applied to financial portfolio management to show the research gap in the literature to which our work responds.\n\nMultiple DRL algorithms have been proposed recently, which has motivated their application in our area of interest. Liu et al. [25] classify the state-of-art DRL algorithms into three categories: 1) value-based algorithms: those based on Deep Q-Networks (DQN) [26]; 2) policy-based algorithms: directly update the parameters of a policy through policy gradient (PG) [27]; and 3) Actor-Critic based algorithms, such as Advantage Actor Critic (A2C) [26], Proximal Policy Optimization (PPO) [20], Deep Deterministic Policy Gradient (DDPG) [28], Soft Actor-Critic (SAC) [29], or Twin Delayed Deep Deterministic Policy Gradient (TD3) [30]. And all of them have been used to maximize portfolio returns while minimizing risk; for example, [31] apply DDPG for stock trading; [32] apply"}, {"title": "3 Methodology", "content": "We now introduce the methodological details of our proposed approach to post-hoc explainable deep reinforcement learning applied to financial portfolio management. First, we will explain the fundamentals of deep reinforcement learning applied to finance, then, we will illustrate the explainable artificial techniques"}, {"title": "3.1 Fundamentals of Deep Reinforcement Learning applied to financial portfolio management", "content": "We will first introduce objections to our methodology for portfolio management and arguments that answer to those objections. Then, we will describe the fundamentals of deep reinforcement learning and how we can apply these algorithms to financial portfolio management.\n\nAlthough DRL has potential for portfolio management due to its competence in capturing nonlinear features, low prior assumptions, and high similarities with human investing, there are characteristics worth paying attention to as pointed out by Liang et al. [33]: First, a financial market is both highly volatile and non-stationary, totally different to games or robot control [53,54] which are the main sectors where DRL has been experimented. Second, traditional Reinforcement Learning (RL) aims to maximize rewards over an infinite period, while portfolio management focuses on maximizing returns within a finite time. Third, in finance, it's crucial to test strategies on separate data sets to evaluate their performance, unlike in games or robotics. Lastly, the stock market has an explicit expression for portfolio value; therefore, approximating the value function is useless and can even deteriorate the agent's performance.\n\nHowever, deep neural networks are able to approximate any function given enough data and a particular architecture of the network, being universal approximator functions. Regarding maximizing returns within a finite time, we can tune the DRL algorithm via the y hyperparameter to consider high future rewards. Concretely,  \u03b3 \u2208 [0,1] controls the focus of the agent in immediate or far rewards as a function of time where a value near to one focus on maximizing returns on a long time period, being y the same as a discount rate for all DRL algorithms. Next, we can assume that an immediate future behaviour of the stock market is explained technically and by past information, being also DRL suited in this scenario. The agent can be retrained in a constant fashion after its predictions happen in the real-time scenario with the new information. Lastly, although Markowitz model assumes that portfolios are only a function of expected reward and risk, if those assumptions, like normal distributed returns, are not met, then, the function explaining the optimal portfolio is a black-box of an enormous set of features like technical indicators, fundamental ratios or social networks, that DRL algorithms can handle due to neural scaling laws. In this work, we assume that the market can be perfectly explained by technical data, focusing hence only in this kind of data. However, any source of data can also be integrated into the space state of the agent, even multimodal data, so this assumption is not an issue in real-case scenarios. To sum up, we consider that DRL can be successfully applied to the financial portfolio management problem, and now explain the fundamental concepts of this methodology."}, {"title": "3.2 Explainable artificial intelligence techniques", "content": "As the purpose of our work is to enhance the DRL framework by integrating explainability of financial features to make the decision-making process of the DRL model transparent and understandable, we briefly describe in this section some of the techniques that we have integrated in the DRL framework to make it interpretable for financial experts.\n\nHuman decision makers use predictions made by ML models in their process, but the usability of those predictions is limited if the human is unable to justify and understand their trust in said predictions. Explanation is a way of obtaining such understanding, by selecting \"what\" must be communicated and \"how\" that information is presented. Moreover,according to [?] \u03a7\u0391\u0399 is necessary for regulatory issues, and thus, there is also a legal component to be considered. The EU General Data Protection Regulation (GDPR) aims to ensure the 'right to explanation' concerning automated decision-making models.\n\nInterpretability also helps developers understand and improve the model performance. It can aid in troubleshooting and debugging, as well as in detecting potential biases in the AI system and provide insights into how the system would react under different circumstances. This understanding can also provide insights"}, {"title": "4 Experiments", "content": "In this section, we will describe the different experiments that we have performed to show the usefulness of our explainable deep reinforcement learning methodology. For reproducibility and transparency, we have uploaded all the code of these experiments in the following Github repository https://github.com/aleedelarica/XDRL-for-finance.\n\nFor our portfolio management experiment we have considered a training period from 2015 to 2017 and a trading period from 2017 to 2018. We have considered the OHCL information of five different technological assets, having a total of 20 features in the state space of the agent. We have considered a PPO DRL algorithm to learn the weights of the deep neural network with default hyperparameters and 100 epochs across all the financial data downloaded from Yahoo finance. Having all that information and performing the training period of the neural network, we interpret the predictions of the deep neural network in the trading period using feature importance, SHAP and LIME methods as we will describe in this section.\n\nWe begin with experiments that show how the feature importance can be extracted for the predictions of the agent during the trading period. In order to do so, we configure a portfolio of several technological assets and measure the importance of their OHCL features in the trading period, information that we illustrate in Figure 2. We will then illustrate how can SHAP and LIME methods offer interpretability and explainability of the actions performed by the agent in such a scenario.\n\nFigure 3 shows the average importance across all features for each stock. Apple (AAPL) remains the most significant, followed by Visa (V), Alibaba (BABA), Adobe (ADBE), and Sony (SNE). This means the DRL agent in the model prioritizes Apple's data in order to make the investing decisions, as it is consistently seen in the previous experiment. Most critically, we can see how, for every asset, it is not clear which of the OHCL is the most important one, as it varies in every asset. Consequently, it is wise to use them all to make the agent more robust to different portfolios.\n\nHowever, as we believe that it is important to know whether the OCHL information is important, we have made a plot of the average importance of each technical indicator for all the assets of the portfolio during all the trading period, obtaining the results plotted on Figure 4, where we can clearly see that although close and open are the most critical indicators, the extreme results of the day, high and low, are features that have as well importance in the predictions of the DRL agent.\n\nTo explain the model's predictions with SHAP, we use SHAP's force plot. Our model is a multi-output model (6 actions, one for each asset and 1 for the cash risk-free asset), consequently, SHAP can generate one force plot for each asset weight allocation through all the samples. We can observe the force plot for Apple's weight allocation in Figure 5.\n\nThe SHAP values represent the contribution of each feature to the model's output for the specific sample. Positive SHAP values (red) push the prediction"}, {"title": "5 Conclusions and Further Work", "content": "In this work, we successfully used SHAP, LIME and feature importance to explain the decisions made by our DRL model in portfolio management tasks. These methods provided clear and detailed insights into why the model made specific investment choices, affirming our hypothesis that it is possible to explain DRL predictions in portfolio management with explainability techniques. Also, our prediction explainability analysis with SHAP and LIME revealed that the"}]}