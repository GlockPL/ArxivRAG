{"title": "A Reverse Mamba Attention Network for Pathological Liver Segmentation", "authors": ["Jun Zeng", "Ulas Bagci", "Debesh Jha"], "abstract": "We present RMA-Mamba, a novel architecture that advances the capabilities of vision state space models through a specialized reverse mamba attention module (RMA). The key innovation lies in RMA-Mamba's ability to capture long-range dependencies while maintaining precise local feature representation through its hierarchical processing pipeline. By integrating Vision Mamba (VMamba)'s efficient sequence modeling with RMA's targeted feature refinement, our architecture achieves superior feature learning across multiple scales. This dual-mechanism approach enables robust handling of complex morphological patterns while maintaining computational efficiency. We demonstrate RMA-Mamba's effectiveness in the challenging domain of pathological liver segmentation (from both CT and MRI), where traditional segmentation approaches often fail due to tissue variations. When evaluated on a newly introduced cirrhotic liver dataset (CirrMRI600+) of T2-weighted MRI scans, RMA-Mamba achieves the state-of-the-art performance with a Dice coefficient of 92.08%, mean IoU of 87.36%, and recall of 92.96%. The architecture's generalizability is further validated on the cancerous liver segmentation from CT scans (LiTS: Liver Tumor Segmentation dataset), yielding a Dice score of 92.9% and mIoU of 88.99%. The source code of the proposed RMA-Mamba is available at https://github.com/JunZengz/RMAMamba.", "sections": [{"title": "1 Introduction", "content": "Technical motivation. The Transformer architecture [37] significantly advanced sequence modeling by employing self-attention mechanisms to capture long-range dependencies without relying on recurrence. Despite their success, Trans-formers face limitations in handling long sequences due to the quadratic com-plexity of the self-attention operation with respect to sequence length. To ad-dress this challenge, recent studies have explored the integration of State Space Models (SSMs) into Transformer architectures, leading to the development of"}, {"title": "2 Related Work", "content": "2.1 State Space Model\nSSMs have emerged as powerful architectures for sequence modeling [29,6,12], with each iteration addressing key computational and performance challenges.\nThe Linear State Space Layer (LSSL) [11] pioneered the use of HIPPO (High-order Polynomial Projection Operators) matrices and structured matrices for capturing long-range dependencies. However, its computational complexity lim-ited practical applications for extended sequences.\nS4 [10] marked a significant advancement by introducing novel parameter-ization techniques that transform HIPPO matrices into diagonal plus low-rank"}, {"title": "2.2 Liver segmentation", "content": "Deep learning has transformed medical image segmentation, initially with UNet [32], later with nnUnet [47], establishing a foundational encoder-decoder architec-ture that efficiently combines high-resolution features with contextual informa-tion through skip connections. These architectures have spawned numerous in-novations in liver segmentation [24,21,23,38,7], each addressing specific aspects of the segmentation challenge.\nAttention mechanisms have emerged as a key enhancement to the basic UNet architecture. AttentionUNet++ [24] introduced attention gates with dense con-nections, while Jin et al. developed a dual-branch approach combining attention residual learning with feature enhancement through a sophisticated trunk-and-mask architecture. SAR-UNet [38] further refined this approach by incorporat-ing squeeze-and-excitation blocks for adaptive feature enhancement and atrous spatial pyramid pooling for multi-scale context capture, while addressing gra-dient flow through residual connections.\nRecent architectures have focused on multi-scale feature learning and bound-ary refinement. Xie et al. [43] proposed a multi-scale context extraction module with external attention and boundary correction mechanisms. Liu et al. [26] in-troduced a hybrid attention approach combining global dependencies with lo-cal feature focus. Moving beyond traditional convolutional architectures, Jha et al. [19] leveraged transformer-based vision models with residual connections, demonstrating superior computational efficiency and segmentation accuracy."}, {"title": "3 Method", "content": "In this section, we first provide an overview of the proposed RMA-Mamba ar-chitecture. Next, we discuss the specifics of the encoder used in this study. Fol-lowing that, we present a detailed explanation of the VSS and RMA modules. Finally, we introduce the loss function in detail."}, {"title": "3.1 Overview of the architecture", "content": "shows the overview of the proposed RMA-Mamba architecture. As illustrated, RMA-Mamba consists of three key components: the VMamba en-coder, additional VSS blocks, and the RMA module. The synergy of these com-ponents significantly enhances the model's learning capability and comprehen-sive segmentation performance. In the encoding process, the pre-trained VMamba backbone extracts four different levels of features \\(2^{\\frac{4H}{2^{i+1}}} \times 2^{\\frac{4W}{2^{i+1}}} \times C_i\\) from the in-put liver image, where \\(C_i \\in \\{96,192,384, 768\\}\\) and \\(i \\in \\{1,2,3,4\\}\\). Additional VSS blocks are then applied to refine the semantic representation of these fea-tures, Where N denotes the number of VSS blocks. Finally, the refined features are passed through the RMA to produce the final segmentation result."}, {"title": "3.2 Encoder", "content": "The proposed architecture adopts VMamba as the encoder backbone, capital-izing on its efficient inference capabilities and high throughput characteristics. VMamba [28], built upon the selective SSM paradigm, offers linear computational complexity while maintaining strong scalability for vision tasks. The encoder's architecture progressively processes input images through multiple stages:\nA stem module that transforms the input image into initial feature maps of dimension \\(\\frac{H}{2^{i+1}} \\times \\frac{W}{2^{i+1}}\\),\nFour sequential processing stages, where all but the first incorporate VSS blocks preceded by down-sampling layers,\nMulti-scale feature representations generated at each stage, crucial for com-prehensive visual understanding.\nVMamba offers three model variants (Tiny, Small, and Base) to balance com-putational requirements with performance. Our empirical analysis led us to implement VMamba-Tiny and VMamba-Small variants, as they achieve opti-mal performance-efficiency trade-offs for pathological liver segmentation. The larger VMamba-Base variant, despite its increased parameter count, did not demonstrate commensurate performance gains to justify its computational over-head."}, {"title": "3.3 VSS block", "content": "While traditional visual data processing relies heavily on spatial relationships, unidirectional scanning mechanisms often fail to capture comprehensive depen-dencies across image regions. Although Transformers [37] successfully address this limitation through self-attention mechanisms that capture long-range de-pendencies, they present computational challenges at scale.\nThe 2D-Selective-Scan module (SS2D) [9] offers an elegant solution by im-plementing four distinct scanning routes across feature maps, effectively cap-turing global receptive fields while maintaining computational efficiency. This innovation evolved from replacing Mamba's S6 module with SS2D in the vanilla VSS block design. VMamba further refined this approach by adopting Transformer-like architectural principles, resulting in streamlined VSS blocks that combine SS2D with Feed-Forward Network (FFN) layers in a single processing branch. Our network implements a multi-stage feature refinement process:"}, {"title": "3.4 Reverse Mamba Attention Module", "content": "The reverse attention mechanism has demonstrated remarkable success in salient object detection [2,45], particularly in boundary delineation and feature refine-ment. This success was notably exemplified in PraNet [5], where reverse atten-tion effectively enhanced boundary detection and tissue differentiation, leading to improved segmentation accuracy through targeted error correction. Build-ing on this foundation, RTA-Former [25] advanced the concept by introduc-ing transformer-based reverse attention, demonstrating superior edge segmen-tation capabilities through sophisticated feature extraction. These successes in boundary refinement and feature enhancement inspired our novel approach: leveraging the computational efficiency of VSS blocks within a reverse attention framework for pathological liver.\nOur proposed RMA module represents a significant advancement in this trajectory, specifically designed to address the unique challenges of pathologi-cal liver segmentation. By combining the selective scanning capabilities of VSS blocks with reverse attention principles, RMA enables precise tissue boundary delineation while maintaining computational efficiency.\nAs illustrated in Figure 1, RMA obtains complementary information from the adjacent upper stage and integrates it into the previous segmentation map by an addition operation. First, the initial binary segmentation map \\(P_{i+1}\\) and shallow feature \\(f_i\\) are processed by the reverse operation to obtain the attention map. Next, we perform element-wise multiplication to obtain reverse attention features \\(R_i\\). This process is formulated as follows:\n\\[ R_i = (P_{i+1}) \\cdot \\delta(f_i), \\]\nwhere \\(\\overline{\\delta}\\) denotes a reverse operation, where the input is subtracted from the matrix E, with all elements equal to 1. \\(\\delta(\\cdot)\\) denotes processing by a VSS block, and \\(\\cdot\\) denotes element-wise multiplication. To further supplement the infor-mation, we reintroduce \\(f_i\\) and perform an element-wise addition, followed by two convolutional layers to generate the feature map \\(p_2\\). Finally, we perform an addition operation between p and \\(p_2\\) to obtain the final output."}, {"title": "3.5 Loss function", "content": "In medical image segmentation tasks with deep learning, Binary Cross-Entropy and Dice losses are commonly employed to improve the precision of delineations."}, {"title": "4 Experiments and Results", "content": "4.1 Datasets\nCirrMRI600+ dataset: The CirrMRI600+[14] dataset is a single-center, multi-vendor, multiplanar, and multiphase dataset specifically designed for cirrhotic"}, {"title": "4.2 Implementation", "content": "In our development environment, we implemented RMAMamba using Ubuntu 20.04, Python 3.8, PyTorch 1.13, and CUDA 11.7. All experiments were con-ducted using the PyTorch framework on a single NVIDIA RTX A10 GPU with"}, {"title": "4.3 Evaluation metrics", "content": "For a comprehensive comparison, we utilized six common evaluation metrics: dice coefficient (Dice), mean Intersection over Union (mIoU), recall, precision, F2 score, and Hausdorff Distance (HD). Dice and IoU are reliable metrics for as-sessing the similarity between predicted segmentation masks and ground truth values. These two metrics are crucial for measuring overlap and consistency be-tween the two masks and evaluating segmentation accuracy. Precision reflects the proportion of true positive samples among all the predicted positive sam-ples. Recall measures the model's ability to identify the targeted liver tissues correctly. HD demonstrates the discrepancy between the segmentation and ac-tual boundaries, with lower values indicating smaller differences."}, {"title": "4.4 Benchmarking", "content": "We compared our architectures, RMAMamba-S and RMAMamba-T, with the state-of-the-art deep segmentation methods (U-Net [32], ResUNet++ [16], DoubleU-Net [15], ColonSegNet [13], NanoNet [17], UNext [36]), Transformer-based methods (TransNetR [20], TransResUNet [35], PVTFormer [19]), and SSM-based segmentation methods (VM-UNet [33], VM-UNetV2 [44], UltraLight VM-UNet [42], Mamba-UNet [40]) on the CirrMRI600+ and LiTS datasets. To ensure a fair comparison, all models were trained and evaluated with identical dataset partitioning and hyperparameter settings.\nResults on CirrMRI600+ dataset: Our extensive experimentation on the patho-logical liver segmentation benchmark dataset demonstrates RMA-Mamba's su-perior performance (Table 1). RMA-Mamba-T outperforms existing CNN and Transformer-based architectures while showing marked improvement over re-cent SSM-based approaches like VM-UNet and Mamba-UNet. The larger vari-ant, RMAMamba-S, achieves state-of-the-art performance with a Dice score of 92.08%, mIoU of 87.36%, and Hausdorff Distance of 3.39 mm.\nWhile Transformer-based methods (TransResUnet: 92.58%, PVTFormer: 92.48%) and CNN-based approaches (Double-UNet: 92.37%) demonstrate competitive performance, they incur substantial computational overhead and longer train-ing times. RMA-Mamba achieves comparable or superior results while main-taining computational efficiency and faster training convergence. Qualitative analysis (Figure 2) further validates our method's effectiveness, particularly in challenging cases. The visualization results, especially evident in rows three and five, demonstrate RMA-Mamba's superior boundary delineation and enhanced discriminative capability at tissue interfaces.\nResults on LiTS dataset: Table 2 presents the quantitative results of various state-of-the-art models on the LiTS dataset. We can observe that RMA-Mamba-T achieves exceptional segmentation performance, with a Dice coefficient of 92.94%, mIoU of 88.99%, recall of 92.34%, precision of 95.59%, F2 score of 92.44%, and a HD score of 2.83 mm. RMA-Mamba-T achieved the highest Dice, surpassing"}, {"title": "4.5 Ablation study", "content": "Table 3 summarizes the ablation study of the RMA-Mamba on the CirrMRI600+ dataset. As mentioned in Section 3, RMA-Mamba consists of a VMamba back-bone, additional VSS blocks beyond the backbone, and RMA modules, where N represents the number of additional VSS blocks. In Table 3, RA denotes the tra-ditional convolution-based reverse attention mechanism, originally introduced in PraNet [5]. The first group (#1) and the second group (#2) represent exper-imental setups using Vmamba-Tiny and Vmamba-Small as the network back-bones, respectively, without the RMA module and additional VSS blocks. The fifth group (#5) and the sixth group (#6) correspond to the RMA-Mamba-T and RMA-Mamba-S architectures, respectively.\nThe third group (#3) and the fourth group (#4) represent experimental set-tings where RMA-Mamba-S removed the additional VSS block and the RMA module, respectively. Results from #1, #2, and #4 indicate that removing the RMA module leads to a significant drop in model performance, underscoring the crucial role of the RMA module in enhancing segmentation quality. Ad-ditionally, in #3, removing the additional VSS block from RMA-Mamba-S also leads to a decline across multiple metrics.\nTable 3 illustrates that both RMA-Mamba-S (#6) and RMA-Mamba-T (#5) demonstrate excellent performance on the CirrMRI600+ dataset. Therefore, we selected #5 as RMA-Mamba-T and #6 as RMA-Mamba-S based on their comple-mentary strengths: RMA-Mamba-T delivers the highest performance despite a larger parameter count, while RMA-Mamba-S maintains strong segmentation accuracy with fewer parameters. Depending on the specific requirements of dif-ferent tasks, we can flexibly choose between RMA-Mamba-S and RMA-Mamba-T."}, {"title": "5 Conclusion", "content": "In this paper, we introduced RMA-Mamba, a novel architecture that leverages state space models for pathological liver segmentation from MRI and CT scans. Our approach combines VMamba's efficient hierarchical feature extraction ca-pabilities with a specially designed Reverse Mamba Attention module to cap-ture both fine-grained tissue details and global anatomical context. Extensive experimentation on the CirrMRI600+ and LiTS datasets demonstrates RMA-Mamba's superior performance to other state of the art methods while maintain-ing computational efficiency. RMA-Mamba not only advances the state-of-the-"}]}