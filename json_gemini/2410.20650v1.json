{"title": "NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks", "authors": ["Yongchang Hao", "Yanshuai Cao", "Lili Mou"], "abstract": "The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.", "sections": [{"title": "1 Introduction", "content": "Deep learning with neural networks has become the backbone of numerous artificial intelligence applications. The search for better-performing networks is a longstanding topic in deep learning. Without modifying the design, scaling up the number of parameters (e.g., number of hidden dimensions or layers) has been demonstrated as an effective practice to boost the performance of neural networks of the same kind (Kaplan et al., 2020). This idea has been successfully applied to text, image, audio, and multi-modal tasks with a wide range of model architectures (Yu et al., 2022; Radford et al., 2019; Brown et al., 2020). Recently, the number of parameters in the state-of-the-art models has become more than 100 billion or even a trillion parameters. For example, one of the state-of-the-art language models in 2020, GPT-3, has 175B parameters (Brown et al., 2020), growing by nearly 100 times compared with the largest Transformer architecture in the 2017 paper (Vaswani et al., 2017).\nDespite the growth in the model size, the hardware capacity is not keeping up with the pace: the largest on-device memory of GPUs was 32GB in 2017, and is 80GB to this date in 2024, growing by only 2.5 times. The available hardware supply poses a limitation on the trainable model size, bottlenecking the scaling capacity. Although this problem can be alleviated by using more GPUs and sharding the model in multiple devices (Rajbhandari et al., 2019), such a practice introduces more communication overheads among GPUs, making large-scale distributed training less efficient. Therefore, saving the total memory usage is critical in scaling up neural networks.\nThe peak memory usage is dominated by three relatively independent parts: the optimizer, the saved activations for back-propagation, and the model itself. For the optimizer, there are already memory-efficient optimizers achieving a sublinear space complexity (Shazeer and Stern, 2018; Hao et al., 2024); for the activations, the memory can be saved by enabling activation checkpointing (Chen et al., 2016), which saves the storage by recomputing the forward activations during the back-propagation. For the model parameters, there has not been an effective method to save the memory while preserving the ability to train the model. Recently, Dettmers et al. (2023) proposed the quantized low-rank adaptation (QLoRA), which freezes the parameters using a 4-bit data type for the backbone pre-trained model. While significantly saving the mem-"}, {"title": "2 Our Approach", "content": "The Shannon entropy (Shannon, 1948) is used to measure the \u201cstochasticity\u201d of a random variable with the following definition:\n$H(X) := E [-log_2 p(X)]$\n$X\\sim p(X)$\n(1)\nfor a random variable X with probability p. A lower entropy indicates a less stochasticity of a random variable. In fact, the entropy equals the minimum number of bits required, in expectation, to represent a random variable, therefore corresponding to data compressibility. For the non-concentrating random variable with all possible values sharing an equal probability, the entropy of which reaches the maximum value $log_2 n$, where n is all possible values X can take. On the other hand, for highly-concentrating (e.g., fully deterministic) random variables, the entropy can be as low as 0.", "eq": "$H(X) := E [-log_2 p(X)]$\n$X\\sim p(X)$"}, {"title": "2.1 Low-Entropy Nature of Neural Network Parameters", "content": "We argue that the parameters in neural network tend to have low entropy. First, parameters are typically initialized with Gaussian distribution for matrices (Glorot and Bengio, 2010; He et al., 2015). This encourages all weights to be centered around zero, effectively reducing the entropy (or randomness). In addition, regularization is also applied for better generalization ability. For example, the weight decay technique reduces the magnitudes of weights at every update iteration. Similarly in Bayesian inference, prior distributions (e.g., Gaussian and Laplace distributions) are often applied, imposing a zero-concentrated preference over the parameters.Even without explicit regularization, stochastic gradient descent (SGD) or its variants are shown to have the implicit regularization effect on neural networks, meaning the model parameters are implicitly encouraged to have smaller magnitudes during training (Soudry et al., 2018; Vardi and Shamir, 2021). All the above effects and techniques lead to the following observation:\nObservation 2.1 Assuming neural network parameters are i.i.d. random variables, the entropy of the distribution is likely to be low.\nSpecifically, each parameter is represented is represented by three components: the sign bit, the exponent bits, and the mantissa bits in the IEEE 754 standard (IEEE, 2019). Therefore, we conduct a fine-grained analysis and investigate the distribution of each component of a floating-point number in neural networks.\nAs shown in Figure 1, the sign bit has a high entropy as it is evenly distributed; hence, it is not compressible. For the exponent bits, there is a clear pattern that they demonstrate a low-entropy nature, carrying only less than 3 bits of information with 8 bits of capacity. For the mantissa bits, they store nearly 7-bit information with 7-bit capacity. In fact, we shown in Appendix A that this is common in deep learning.\nThis phenomenon suggests that by simply compressing the exponents, we are able to recover the overall optimal compression ratio. In this example, an ideal compression algorithm is able to achieve a ratio as"}, {"title": "2.2 Lossless NeuZip: Compressing Exponents for Training", "content": "Compressed representation. Based on observation, we see that the number of bits per exponent is largely inflated compared with the information entropy. However, previous research demonstrates that the dynamic range provided by the 8-bit exponents are critical for neural networks (Kalamkar et al., 2019). We therefore propose to compress the exponent bits in a lossless manner based on the entropy. This practice mainly has three benefits: (1) it increases the throughput of compression as only part of the bits are processed by the compression; (2) it reduces the burden of maintaining the statistics of a large set of symbols (e.g., 256 symbols for 8-bit exponents versus 65,536 symbols for 16-bit representations), enabling a great efficiency of compression algorithms; (3) most importantly, it recovers most of the compressibility as shown in Figure 1.\nMulti-layer neural networks. The compression alone does not save any memory for maintaining a single array. This is because, either compression or decompression, requires at least one buffer of the same size as the uncompressed array. In the scope of neural networks, the whole model is prohibitively large and it is infeasible to duplicate the memory. In NeuZip, however, we exploit the multi-layer structure of modern neural networks to avoid creating a large buffer. Without loss of generality, we focus on the linear function as a common building block in neural networks at layer l:\n$x_l = W_l x_{l-1} + b_l,$\n(2)\nwhere $W_l \\in \\mathbb{R}^{m \\times n}$ is the weight matrix, $b_l \\in \\mathbb{R}^{m}$ is the bias vector of layer l, and $x_l$ is the input of layer 1.\nWe propose to modify the compressed forward pass in the following form\n$W \\leftarrow decompress(c_l)$\n$x_l \\leftarrow W x_{l-1} + b_l,$\n(3)\n(4)\nwhere $c_l$ is the compressed storage of the matrix $W_l$. In this way, we only need to store $c_l$ for each layer, enjoying low-memory usage. During each forward pass, weight matrices stay in the compressed form until the original data is needed, in which case it is decompressed into a temporary space $\\hat{W}$ for computation. As a result, the entire network is never fully decompressed at any point in time, making the overall forward pass memory efficient. The per-layer procedure is shown in Figure 2.\nNote that although we alter the forward pass, the back-propagation for each linear layer is fully unaffected. This is because\n$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial x_l} \\frac{\\partial x_l}{\\partial W} = (\\nabla_{x_l} L) x_{l-1}^T.$\n(5)\nTherefore, we are able to obtain the gradient as long as the activations are saved. Similarly, we can also propagate the gradient of inputs with\n$\\frac{\\partial L}{\\partial x_{l-1}} = \\frac{\\partial L}{\\partial x_{l}} \\frac{\\partial x_{l}}{\\partial x_{l-1}} = (\\nabla_{x_l} L) W_l,$\n(6)"}, {"title": "2.3 Lossy NeuZip: Additionally Truncating Mantissa for Inference", "content": "In the algorithm above, we show that the training of neural networks can be completely unaffected by lossless compression. On the other hand, inference is known to be less sensitive to precision loss compared with training (Dettmers et al., 2022; Dettmers and Zettlemoyer, 2023). This enables further memory reduction of NeuZip by reducing the precision. In our study, we conduct a pilot experiment that perturbs each weight with a noise proportional to the weight magnitude. We observe that with a small noise ratio there is little or no effect on the overall performance (Appendix C). Motivated by this, we propose a variant of NeuZip that compresses mantissa in a lossy way during inference.\nIn its core, we simply round and truncate the mantissa to fewer bits. Specifically, we assume the original floating-point number f has an exponent e and mantissa m. After rounding, the mantissa is denoted by $\\hat{m}$ and the resulting floating-point number is denoted by $\\hat{f}$.\nThe rounding introduces an error expressed as:\n$|f - \\hat{f}| = |\\frac{2^{e-127} m}{2^7} - \\frac{2^{e-127} \\hat{m}}{2^7}| = 2^{e-134} . |m - \\hat{m}|$\n(7)\nwhere e 127 interprets the exponent bits e as an integer, which can be either positive, negative, or 0. In the fraction m/27, m is the significand (an unsigned integer) and 7 is the precision. It is straightforward to see that the relative error is given by\n$\\frac{|f-\\hat{f}|}{|f|} = \\frac{2^{e-134} . |m - \\hat{m}|}{2^{e-134} . |m|} = |\\frac{m-\\hat{m}}{m}|$\n(8)\nSuppose our rounding keeps k most significant bits in m, the earliest point where $\\hat{m}$ could differ from the original number m is at the (k + 1)th bit. This means that the maximum possible relative change introduced"}, {"title": "3 Experiments", "content": "We empirically verify the effectiveness of NeuZip across different model architectures and datasets. Given the success of large language models, we mainly consider Transformer-based models for our experiments. We choose two designs of Transformer, decoder-only and encoder-decoder models, to show the generality of our method. All experiments are conducted on RTX A6000 GPUs where the uncompressed data type is BFloat16."}, {"title": "3.1 Lossless NeuZip for Pre-Training", "content": "Settings. We choose decoder-only models to evaluate our method on the pre-training task. We select 3 models with different sizes to study the scaling effect, including GPT-Neo 2.7B (Black et al., 2021), Llama-3 8B (Dubey et al., 2024), and LLama-2 13B (Touvron et al., 2023). For fair comparison, all competing methods are initialized with the same random weights.\nFor the task, we consider language modeling, which requires the model to predict the next token given the context. We use the Wikitext-2 dataset (Merity et al., 2016), where each data sample is a fixed-length sequence from an article on Wikipedia. We set the length to 1024 following the common practice (Radford et al., 2019).\nFor each experiment, we report the loss (negative log-likelihood) on unseen samples. To study memory saving, we report the peak memory usage for each run during the training process. The numbers are shown in gibibyte (GiB, 10243 bytes). We also report the speed by the number of iterations per second to demonstrate the time-efficiency of each method.\nWe apply the vanilla SGD update to all runs for efficiency. The activation checkpointing technique (Chen et al., 2016) is enabled by default. It is worth noting that pre-training these large models to the optimal performance is extremely expensive (Rajbhandari et al., 2019). Given that our NeuZip training method is lossless, we only train the models for 1 epoch to showcase its effectiveness. We use the same hyper-parameters for all runs.\nResults. We present the results in Table 1. We first test the vanilla training method, where only the activation checkpointing is applied (shown in Figure 2b). As shown, the vanilla training requires the highest amount of memory because it stores the uncompressed weights and gradients for all layers.\nWe also test the LOMO technique (Lv et al., 2023), which promptly updates the weights in a layer-by-layer fashion (shown in Figure 2c). This allows LOMO to reuse a buffer to store the gradients for each layer. As a result, LOMO approximately reduces the peak memory usage by the size of a model.\nFinally, we apply our NeuZip on top of LOMO (shown in Figure 2d). For all models, NeuZip additionally reduces more than 20% percentage of memory compared with LOMO, accounting for a total memory reduction of more than 50%. Notably, NeuZip reduces the peak memory of training a Llama-2 13B model to less than 20GB, enabling training a 13B model on consumer-grade GPUs without any precision loss."}, {"title": "3.2 Lossless NeuZip for Fine-Tuning", "content": "Settings. A benefit of using lossless compression comes from retaining the pre-trained weight without any information loss. We conduct a fine-tuning experiment with encoder-decoder models to test the performance of our NeuZip on broader architectures. In particular, we choose three T5 models: T5 1B, T5 3B, and T5 11B (Raffel et al., 2020), where the pre-trained parameters are used for initialization.\nThe T5 models are pre-trained on the C4 dataset (Lin et al., 2020), which is filtered to contain natural language only. To avoid data leaks from pre-training, we choose a non-natural language generation dataset for fine-tuning. Specifically, we use a public SQL generation dataset (Zhong et al., 2017; Yu et al., 2018) as the test bed. For each sample, the model is required to generate the SQL command from a human question. For example, the question could be \u201cCREATE TABLE head (age INTEGER). How many heads of the departments are older than 56 ?\". The model is expected to generate \u201cSELECT COUNT(*) FROM head WHERE age > 56\". We feed the question and response into the encoder and decoder, respectively. The objective is to minimize the cross-entropy loss on the response.\nSimilar to the pre-training experiments, we also sweep the learning rate from 10\u20133 to 3 \u00d7 10\u20131 for each run. After fine-tuning, we generate with the model on the validation set with greedy decoding. The generated SQL commands are then compared with the ground truths by SacreBLEU (Post, 2018), a metric that evaluates the similarity between corpora based on precision scores.\nResults. The results are reported in Table 2. All baselines in the pre-training experiment (i.e., the vanilla training, LOMO, and NeuZip) are included in this table. Similar to the results in Section 3.1, they achieve the same BLEU scores for each model. Specifically, our NeuZip is able to train a 11B model within 24GB.\nFor fine-tuning, it is possible to apply other memory-efficient training techniques. For example, QLoRA (Dettmers et al., 2023) compresses the pre-trained model by using low-precision data types and train the LoRA modules only (Hu et al., 2022). In our comparison experiment, we choose the widely used quantization data types for QLoRA, including INT8 (Dettmers et al., 2022), FP4, and NF4 (Dettmers et al., 2023). We apply the LoRA modules (Hu et al., 2022) on all linear layers, where every LoRA rank is set to"}, {"title": "3.3 Lossy Compression for Inference", "content": "As mentioned in Section 2.3, the inference process is less sensitive in precision loss, which provides an opportunity for compressing mantissa in a lossy fashion during inference. We evaluate the performance of our lossy NeuZip in such scenarios."}, {"title": "3.4 In-Depth Analyses", "content": "The memory-performance trade-off. In Section 3.3, we observe that the performance is generally decreased with less memory usage. We analyze this trade-off of our NeuZip as well as quantization methods in Figure 3. Note that the optimal methods are the ones on the Pareto frontier (Pareto, 2014), i.e., the more bottom-left, the better. In addition to measuring the perplexity, we also include a preliminary study by evaluating the end-to-end performance on the MMLU dataset (Hendrycks et al., 2020) in Appendix E.\nAs shown, three out of four NeuZip variants are on the Pareto frontier, with the remaining one staying fairly close to the frontier. On the other hand, there is only one quantization technique that lies on the"}, {"title": "4 Related Work", "content": "Model compression. Previous work has explored different techniques to reduce the memory usage of neural networks, including knowledge distillation (Hinton et al., 2015) and pruning (Kwon et al., 2022). Most related to our work is the quantization technique, which represents each parameter with fewer bits; common approaches include k-means-based quantization (Han et al., 2016), linear quantization (Han et al., 2016), and mixed precision quantization (Dettmers et al., 2022, 2023). When training data are available, one may incorporate the quantization into the training process to improve performance (Xiao et al., 2023; Frantar et al., 2023). In this paper, our NeuZip compression is a zero-shot method, and therefore, our experiments consider the widely used zero-shot quantization methods (Dettmers et al., 2022, 2023) for fair comparison. We leave the utilization of additional data of NeuZip to future work.\nMemory-efficient optimizers. The optimizer also occupies a considerable amount of memory during training (Rajbhandari et al., 2019). To address this, memory-efficient optimizers (Shazeer and Stern, 2018; Zhao et al., 2024; Hao et al., 2024) are developed to reduce the memory footprint of training. Our NeuZip is orthogonal to these optimization techniques, as it can be seamlessly combined with any of these methods for further memory saving. In particular, the lossless NeuZip is expected to have exactly the same results with less memory.\nParameter-efficient training. Another line of research saves memory by training a subset of parameters (Houlsby et al., 2019; Zaken et al., 2022) so the optimizer only stores information about a small set of trainable parameters. One notable example is the low-rank adaptation (LoRA (Hu et al., 2022)). However, such a practice restricts the optimization space of parameters, and thus usually leads to significant performance degradation. Moreover, low-rank methods are unsuitable for pre-training.\nIt is important to mention that memory-efficient optimizers and parameter-efficient training cannot reduce the memory cost during inference. By contrast, our NeuZip is suitable for both training and inference."}, {"title": "5 Conclusion", "content": "Summary. In this work, we present NeuZip, a novel compression scheme for neural networks that achieves memory-efficient training and inference. By analyzing the floating-point structures, we propose to compress the exponent in a lossless way and to compress the mantissa in a lossy way. The lossless variant of our NeuZip may be applied to both training and inference, while yielding exactly the same result as the uncompressed model. The lossy NeuZip provides additional memory saving for inference, achieving superior memory-performance trade-off.\nLimitations and future work. Due to the hardware constraint, the largest model that we consider in this paper is 70B. We would like to verify our NeuZip on even larger models like GPT-3 (Brown et al., 2020) with more capable hardware. Another limitation of NeuZip is that the throughput is lower than the vanilla model. However, it has a comparable speed with highly optimized quantization methods while achieving significantly better performance. By using NeuZip, we expect to create opportunities for researchers with academic budgets to explore and study large models."}, {"title": "A Inspecting the Entropy on More Models", "content": "Random initialization. When training from scratch, the parameters are randomly initialized. To verify the compressibility in this case, we check the parameter entropy of a randomly initialized model with the same architecture as Llama-3. The initialization methods follow the standard procedure provided in the Hugging Face library. The results show a similar pattern to what the released Llama model has, suggesting the compressibility with NeuZip occurs even with random weights."}, {"title": "B The Algorithm for Training with Lossless NeuZip", "content": "In this section, we describe the forward-backward procedure of NeuZip. First, we compress all the linear layers in the original model and store the compressed information on-device. During the training iterations, we decompress the compressed weights in a layer-by-layer manner for the forward pass. For the backward pass, the input is recalculated again following the forward pass like activation checkpointing (Chen et al., 2016). A linear operation calculates the gradients for both the weight matrix and input. To do so, we need to decompress the weight again, which is used to calculate the gradient of input. After the gradient is calculated, we directly update the weight without storing it similar to LOMO (Lv et al., 2023)."}, {"title": "C The Tolerance of Random Perturbation", "content": "In this experiment, we would like to explore the sensitivity of neural network weights to random perturbations. For each parameter, we have two types of magnitudes: absolute and relative magnitudes. The former one represents the actual numerical error, whereas the second one is calculated based on the original value. For example, when the original value is \u20131.5, an absolute magnitude of 0.125 means the perturbed range is [\u22121.5 \u2013 0.125, -1.5 + 0.125]. On the other hand, a relative magnitude of 0.125 means the perturbed range is [-1.5 * (1 + 0.125), \u20131.5 * (1 \u2013 0.125]. We conduct such a experiment with the perturbation grid in Figure 7. For each cell, we choose the maximum error between relative error and absolute value for perturbing. A random value is sampled from the perturbed range uniformly as the perturbation. The weight value is then set to the random sample.\nAs shown in Figure 7, we see a clear pattern that the model tends to tolerate the relative change rather than the absolute change."}, {"title": "D The Storage for Lossless and Lossy Compression", "content": "In this section, we describe the underlying storage layout for NeuZip in Figure 8.\nEssentially, each BFloat16 number is first split into an exponent and a signed mantissa. We group all the exponents in the matrix and perform the lossless compression. The signed mantissa is optionally truncated, depending on the required precision. The signed mantissa is then stored separately in memory."}, {"title": "E Evaluating on MMLU", "content": "We provide the results on MMLU (Hendrycks et al., 2020) in Figure 9. Here, the theoretical optimal point should be at the top left corner."}]}