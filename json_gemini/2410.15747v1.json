{"title": "GIG: Graph Data Imputation With Graph Differential Dependencies", "authors": ["Jiang Hua", "Michael Bewong", "Selasi Kwashie", "MD Geaur Rahman", "Junwei Hu", "Xi Guo", "Zaiwen Feng"], "abstract": "Data imputation addresses the challenge of imputing missing values in database instances, ensuring consistency with the overall semantics of the dataset. Although several heuristics which rely on statistical methods, and ad-hoc rules have been proposed. These do not generalise well and often lack data context. Consequently, they also lack explainability. The existing techniques also mostly focus on the relational data context making them unsuitable for wider application contexts such as in graph data. In this paper, we propose a graph data imputation approach called GIG which relies on graph differential dependencies (GDDs). GIG, learns the GDDs from a given knowledge graph, and uses these rules to train a transformer model which then predicts the value of missing data within the graph. By leveraging GDDs, GIG incorporates semantic knowledge into the data imputation process making it more reliable and explainable. Experimental results on seven real-world datasets highlight GIG's effectiveness compared to existing state-of-the-art approaches.", "sections": [{"title": "1 Introduction", "content": "In the era of big data, addressing the challenge of missing value imputation has become critical [1]. In general, datasets with incomplete information pose challenges for deriving reliable knowledge [2]. Consequently, considerable efforts have been directed towards the data imputation problem, viewed as a fundamental task in data cleaning [3]. The complexity of identifying the optimal values for imputing missing data arises from the necessity to evaluate all potential combinations within the value distribution. Numerous approaches in the literature operate under the premise that a missing value can be imputed with another value from the same population, with the overarching goal of maintaining the overall integrity of the data [4]. Furthermore, heuristics have been extensively"}, {"title": "2 RELATED WORK", "content": "In the last decade, various solutions have been proposed for data imputation, including the application of linear regression methods. Specifically, the linear regression model in [7] addresses imputation of numerical missing values to tackle the data sparsity problem, where the number of complete tuples may be insufficient for precise imputation. Further, REMIAN proposed in [8] employs a multivariate regression model (MRL) to handle real-time missing value imputation in error-prone environments, dynamically adapting parameters and incrementally updating them with new data. The work in [9] utilizes regression models differently for imputing missing values. Rather than directly inferring missing values, the authors suggest predicting distances between missing and complete values and then imputing values based on these distances. Other techniques such as [10] use regression models to analyse missing values.\nIn [11], Samad proposes a hybrid framework that combines ensemble learning and deep neural networks (DNN) with the Multiple Imputation using Chained Equations (MICE) approach to improve imputation and classification accuracy for missing values in tabular data [11]. They introduce cluster labels from training data to enhance imputation accuracy and minimize variability in missing value estimates. The paper demonstrates superior performance of their methods"}, {"title": "3 PRELIMINARIES", "content": "In this section we introduce the concept of GDDs and show how GDDs can be used for graph data imputation.\nA GDD \\( \\sigma \\) is a pair \\( (Q[z], \\Phi_x \\rightarrow \\Phi_y) \\), where: \\( Q[z] \\) is a graph pattern called the scope, \\( \\Phi_x \\rightarrow \\Phi_y \\) is called the dependency, \\( \\Phi_x \\) and \\( \\Phi_y \\) are two (possibly empty) sets of distance constraints on the pattern variables \\( z \\). A distance constraint in \\( \\Phi_x \\) and \\( \\Phi_y \\) on \\( z \\) is one of the following [21]:\n\\( d_A(x.A, c) \\lt t_a \\); \\( d_{A_iA_j}(x.A_i, x'.A_j) \\le t_{A_iA_j} \\);\n\\( d_=(x.eid, x'.eid) = 0 \\);\n\\( d_=(x.rela, x'.rela) = 0 \\);\n\\( d_=(x.eid, C_e) = 0 \\);\n\\( d_=(x.rela, C_r) = 0 \\);\nwhere \\( x, x' \\in z \\), \\( A, A_i, A_j \\) are attributes in \\( A \\), \\( c \\) is a value of \\( A \\), \\( d_{A_iA_j}(x.A_i, x'.A_j) \\) (or \\( d_A(x,x') \\) if \\( A_i = A_j \\)) is a user specified distance function for values of \\( (A_i, A_j) \\), \\( t_{A_iA_j} \\) is a threshold for \\( (A_i, A_j) \\), \\( d_=(,) \\) are functions on \\( eid \\) and relations and they return 0 or 1. \\( d_=(x.eid, C_e) = 0 \\) if the \\( eid \\) value of \\( x \\) is \\( C_e \\), \\( d_=(x.eid, x'.eid) = 0 \\) if both \\( x \\) and \\( x' \\) have the same \\( eid \\) value, \\( d_=(x.rela, C_r) = 0 \\) if \\( x \\) has a relation named \\( rela \\) and ended with the profile/node \\( C_r \\), \\( d_=(x.rela, x'.rela) = 0 \\) if both \\( x \\) and \\( x' \\) have the relation named \\( rela \\) and ended with the same profile/node.\nThe user-specified distance function \\( d_{A_iA_j}(x. A_i, x'.A_j) \\) depends on the types of \\( A_i \\) and \\( A_j \\). It can take the form of an arithmetic operation involving interval values, an edit distance calculation for string values, or the distance computation between two categorical values within a taxonomy, among other possibilities. These functions accommodate the wildcard value '*' to represent any domain, and in such cases, they return a 0 distance.\nWe call \\( \\Phi_x \\) and \\( \\Phi_y \\) the LHS and the RHS functions of the dependency respectively. In this work we rely on the GDDMiner proposed in [22] to mine the GDD rules from graph data."}, {"title": "4 GIG ALGORITHM", "content": "GIG relies on a transformer architecture and graph differential dependencies (GDDs) for data imputation. Given a knowledge graph G, GIG proceeds in three steps namely (1) rule mining and selection; (2) transformer training; and (3) missing value imputation. These steps are summarised in Figure 1 and explained as follows."}, {"title": "4.1 Rule Mining and Selection", "content": "The GDD mining algorithm proposed in [22] is employed to extract GDD rules from the knowledge graph G as shown in Figure 1, Part 1. For example \\( d(y.name, \\hat{y}.name) \\le 1 \\rightarrow d(y.genre, \\hat{y}.genre) = 0 \\) is one such rule that implies any two entities y and \\( \\hat{y} \\) with a similar name will belong to the same genre.\nGDD rules are composed of a left-hand side (LHS) and a right-hand side (RHS). The LHS serves as the input to the transformer encoder, while the RHS serves as the input to the transformer decoder (cf. Figure 1, Part 2 (a)). When dealing with multiple GDD rules, where two or more rules exhibit distinct left-hand sides (LHS) but share the same right-hand side (RHS), we opt to consolidate the LHS to enhance the predictive outcomes of the transformer model. Similarly, in cases where multiple rules feature the same LHS but different RHS, we amalgamate the RHS. For example, a rule set \\( X \\rightarrow Y_1, X \\rightarrow Y_2, X \\rightarrow Y_3 \\), will be merged to \\( X \\rightarrow Y_1, Y_2, Y_3 \\). This adaptation is implemented to consolidate the rules and streamline the learning process of the transformer model.\nIn the case of GDD rules, we employ masking vectors for the storage of attribute and type details. Both the left-hand side (LHS) and right-hand side (RHS) of GDD rules encompass attribute information, and the utilization of masking vectors for this purpose is both succinct and efficient. For instance, consider a tuple h1 from Table 1. Let us assume we have a GDD ruler : x.name \\( \\rightarrow \\) y.genre, in this case, we can construct a mask for the attributes, represented by (0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0), where a value of 1 indicates that the attribute is utilized. This approach enables us to preserve a substantial amount of information related to GDD rules."}, {"title": "4.2 Transformer Training", "content": "The transformer consists of an encoder and a decoder (cf. Figure 1, Part 2), where the LHS of the rules serves as the input to the encoder, and the RHS as the input to the decoder. Thus, for a GDD rule, (Q, X \\( \\rightarrow \\) Y), the transformer's encoder learns the features of X and the decoder learns the features of Y. In this way, the transformer learns the relationship between the LHS and RHS of the rules.\nEncoder. Given a rule (Q, X \\( \\rightarrow \\) Y), X contains one or more distinct attribute values, which are all considered as a single unit during the training phase. For example, the LHS X := d(y.name,y'.name) \\( \\le \\) 1, will be treated as a literal {y.name, y'.name, \\( \\le \\) 1}. By treating (y.name, y'.name) and \\( \\le \\) 1 as unified literal within the encoding framework, we enable the transformer to infer their interconnectedness. We have observed, empirically, that this approach is more robust against the potential of variations within the attribute values which can deteriorate model performance.\nDecoder. The decoder uses a similar approach to the encoder. When handling the RHS Y := d(y.gender, y'.gender) = 0, by treating {y.gender, y'.gender, = 0} as unified literal.\nTraining. Once the inputs to the encoder (LHS) and decoder (RHS) are derived from the rules, the transfomer model learns GDD rules by minimising the loss function i.e. Kullback-Leibler Divergence (KLDivLoss). KLDivLoss measures the divergence between the predicted distribution and the target distribution. Lower values of the KLDivLoss indicate better alignment between input and output, which is desirable during training. The final result after training serves as the predictive model."}, {"title": "4.3 Imputing missing value", "content": "Imputing missing value. Missing values can also be expressed as rules for example X \\( \\rightarrow \\) Y implies the LHS values X are known and the values of Y are to be determined. GIG uses the LHS in the trained transformer predictive model to predict an RHS value for imputing Y, facilitating the identification of all viable candidates for the imputation.\nLet us consider a scenario where we are interested in predicting the y.name value of record h2 in the pseudo-relational table Table 1. The first step is to identify all GDD rules that has y.name in its RHS. That is any rule that has a '1' in its positional mask at index 3 is considered. For example, the rule x.name \\( \\rightarrow \\) y.name which has the positional mask (0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0) will be considered since it has a value '1' in the index position 3. Intuitively, the rule means any two nodes of type x that share the same name, will also share the same name on a node type \u0443.\nNext, based on the rules identified to be relevant, we can identify the LHS value which should serve as input to the transformer model. In this case EA becomes the input since according to the rule, the LHS should consists of x.name, and in h2 x.name is EA. The transformer then makes a prediction of what the"}, {"title": "5 Experiment", "content": "In this section we compared our proposed method GIG with known relational data based techniques such as Derand [9] and Holoclean [12]. Figure 2 are the results. In Figure 2 we observe that GIG performs competitively with Holoclean and Derand. In particular, across all datasets, GIG has a higher precision rate than other techniques due to the rule guided transformer approach. We do notice, however that with the exception of Restaurant dataset, the rule guided approach also leads to a comparatively lower recall on Adult, CSD and Ncvoter datasets. Yet still, GIG is highly competitive and in most cases has a better performance in F1 across all the datasets."}, {"title": "5.3 Effectiveness on Graph Datasets:", "content": "We assess the effectiveness of GIG for imputing missing values in graph data. We evaluate GIG's performance on the three graph datasets Entity Resolution, Graph Data Science, and Womens WorldCup 2019. Figure 3 is the result of that experiment. In the figure, we observe that the results for Entity Resolution were the best, as its data distribution yields more exemplar points that enables the"}, {"title": "6 Conclusion", "content": "In this paper, we introduced GIG, a data imputation algorithm that leverages graph differential dependencies for missing value imputation. GIG relies on a rule-guided transformer architecture and has three main steps involving GDD rule mining, transformer training and missing value imputation. Evaluation results from seven datasets and in comparison with existing state-of-the-art demonstrate the competitiveness and suitability of GIG for both relational and graph data imputation. In our future work, we aim to enhance GIG by considering different types of rules such as approximate graph entity rules which may improve the overall recall of GIG."}]}