{"title": "Convolutional Neural Networks for Predictive Modeling of Lung Disease", "authors": ["Yingbin Liang", "Xiqing Liu", "Haohao Xia", "Yiru Cang", "Zitao Zheng", "Yuanfang Yang"], "abstract": "In this paper, Pro-HRnet-CNN, an innovative model combining HRNet and void-convolution techniques, is proposed for disease prediction under lung imaging. Through the experimental comparison on the authoritative LIDC-IDRI dataset, we found that compared with the traditional ResNet-50, Pro-HRnet-CNN showed better performance in the feature extraction and recognition of small-size nodules, significantly improving the detection accuracy. Particularly within the domain of detecting smaller targets, the model has exhibited a remarkable enhancement in accuracy, thereby pioneering an innovative avenue for the early identification and prognostication of pulmonary conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of medicine, precision medicine and personalized treatment are gradually becoming the mainstream trend, and the early diagnosis and prediction of lung diseases is a key link in this trend. Respiratory ailments, encompassing conditions like carcinoma of the lung, chronic obstructive pulmonary disease (COPD), and pneumonic infections, not only boast a considerable prevalence rate but also profoundly influence the standard of living and longevity of those afflicted [1]. Traditional diagnostic methods rely on the experience of doctors and various imaging examinations, such as X-rays, CT scans, etc., but in complex cases, these methods often have certain limitations, such as the subjectivity of diagnosis, sensitivity, and lack of specificity.\nOver recent years, the swift advancement in artificial intelligence, notably the leaps in deep learning, has ushered in novel prospects for the prognosis and predictive analysis of lung diseases [2]. Serving as potent tools for pattern recognition, deep neural networks are capable of autonomously extracting features from voluminous datasets, devoid of manual intervention[3-5]. This self-learning capability renders them exceptionally promising in tackling intricate image recognition challenges, particularly in medical diagnostics[6]. Especially in lung image analysis, deep neural networks can identify subtle structural changes that are difficult to detect with traditional methods, thus improving the accuracy of disease detection[7]. However, developing an efficient and precise lung disease prediction model leveraging deep neural networks is far from straightforward. First, the construction of high-quality lung image data sets is the foundation, which requires a sufficient number and diversity of samples, as well as accurate labeling information. Secondly, the design of the model should fully consider the characteristics of lung imaging to ensure the effectiveness and robustness of the model. Ultimately, the model's training and validation phases necessitate robust methodological underpinning to ascertain its generalizability and predictive precision.\nThe study aims to develop a robust and precise model for predicting lung diseases by leveraging deep neural network technology and analyzing lung image characteristics. This research will encompass the creation of the dataset, the design of the model, optimization techniques during training, and the verification of results. The ultimate goal is to innovate new methods for the early diagnosis and prediction of lung diseases. By doing so, we seek to enhance the accuracy of clinical decision support, provide earlier treatment opportunities for patients, and ultimately advance the entire healthcare system towards a future marked by greater precision and efficiency."}, {"title": "II.CORRELATIONAL RESEARCH", "content": "Recently, the integration of artificial intelligence with image analysis techniques has significantly advanced the field of intelligent diagnostic capabilities in medical imaging, leading to substantial technological breakthroughs[8-10]. Researchers globally are dedicated to enhancing the performance of intelligent diagnostic systems, creating computer-aided diagnostic tools that combine machine learning and deep learning to improve the recognition and classification of lung images[11]. Despite these advancements, the inherent complexity and morphological variations in lung imaging continue to pose significant challenges. These challenges result in notable deficiencies in the sensitivity and classification accuracy of current detection technologies, which must be overcome to achieve optimal performance[12].\nLassen et al. [13] used regional growth techniques to identify pulmonary nodules, and the initial seed points needed to be manually determined. However, the varied characteristics of pulmonary nodules and their resemblance to surrounding tissue pose significant challenges, constraining the precision of this approach. Farahani FV et al. [14] integrated multi-layer perceptrons, K-nearest neighbors and SVM classifiers, and determined the categories of pulmonary nodules by voting mechanism through CT image segmentation and shape feature analysis. Javaid et al. [15] used the intensity threshold and K-means algorithm to first segment and then refine to realize the preliminary detection of pulmonary nodules. Roth et al. [16] used 2D CNN to detect lung nodules and reduce misjudgment through image preprocessing and feature extraction. In recent years, the emergence of more efficient 2D CNN models has promoted the development of related fields.\nYan et al. [17] proposed an innovative cancer detection scheme that integrates graph convolutional neural networks (GCNs) and advanced image analysis directly applied to lung cancer detection. Similar to how GCNs enhance prognosis accuracy in gastric and colon cancers by analyzing spatial relationships in tumors, this method could improve the detection and predictive modeling of lung cancer. Adapting this technique to lung cancer could enhance tumor detection and staging, ultimately improving treatment strategies and patient outcomes. This highlights the potential of convolutional neural networks in advancing lung disease management. Setio et al. [18] focused on the multi-angle analysis of CT images. They used two-dimensional slices of lung nodules in different directions (including axial, coronal, sagittal and six diagonal angles) to build a multi-view 2D CNN model to capture the spatial distribution characteristics of lung nodules and further optimize the classification effect. Liu et al. [19] presented a comprehensive study on feature extraction using CNNS, emphasizing the model's capability to identify intricate patterns in medical images. Lin et al. [20] focused on constructing disease prediction models using advanced deep learning technologies, demonstrating how AI can be leveraged to predict medical conditions by analyzing complex datasets. Hu et al. [21] investigated multi-scale image fusion systems in intelligent medical image analysis, enhancing image analysis accuracy through multi-scale techniques, a concept that parallels the void-convolution technique used in our Pro-HRnet-CNN model. Sun et al. [22] explored the optimization of Natural Language Processing (NLP) models using multimodal deep learning approaches, with principles of multimodal learning and model optimization pertinent to improving the performance of CNNs in medical image analysis. Xiao et al. [23] examined the use of attention mechanisms in deep learning models for mining medical textual data, providing valuable insights into improving model accuracy and efficiency. Additionally, Skourt et al. [24] employed the U-net framework to analyze CT images, attaining precise delineation of lung nodules. The model combined downsampling encoding and upsampling decoding to extract and recover features effectively, and obtained a Dyess similarity coefficient of 0.9502 on the LIDC-IDRI dataset.\nThese studies collectively contribute to the foundational knowledge required to develop sophisticated deep learning models for medical diagnostics. Our proposed Pro-HRnet-CNN model builds on these advancements, integrating HRNet and void-convolution techniques to enhance the detection accuracy of small pulmonary nodules, thereby pushing the boundaries of early lung disease diagnosis and prediction."}, {"title": "III.METHOD", "content": "The core components of convolutional neural network include convolutional layer, activation unit, downsampling module and fully connected layer. The convolution layer realizes the high-level abstraction of image features by performing convolution operations. The downsampling function can be realized by convolutional processing or pooling technology with step size greater than 1. Studies have confirmed that the use of large-step volume instead of traditional pooling can significantly optimize the overall performance of the network[25-27]. However, with the image undergoing multiple downsampling, the original details tend to be reduced. In this case, the hollow convolution technique can partially alleviate this problem and maintain the feature integrity. In void convolution, as shown in formula (1), the void rate 1 is incorporated into the properties of the convolution filter, and its function is to intersperse voids on the basis of standard convolution, so as to expand the perceptual field of view of the feature map. A detailed void-convolution structure is shown in Figure 1.\n$\\(F *_{1} k)(p) = \\sum_{s+lt=p} F(s)k(t)$                                                                          (1)\nIn Figure 1, the blue boxes indicate three types of 3x3 convolution nuclei with different cavity rates: standard (a), nuclei with cavity rate l=2 (b), and more advanced (c). From the 3x3 perception domain (a) to 7x7 (b) to 15x15 (c), the void convolution amplifies the perception domain step by step, far exceeding the 7x7 limit of three successive standard 3x3 convolution layers. This strategy can greatly expand the perception field without increasing parameters and reduce the information loss of small targets while maintaining high resolution."}, {"title": "2. High resolution deep neural networks", "content": "In the field of image analysis, especially in the field of small scale target recognition, it is very important to maintain high definition feature representation. Based on this need, the researchers designed a deep learning framework specifically optimized for high-resolution performance, High resolution Neural networks (HRNet), which is particularly suitable for complex visual tasks such as human pose resolution and scene semantic understanding. As shown in Figure 2, HRNet's innovative architecture ensures efficient flow of information at multiple scales, resulting in superior performance in small target detection."}, {"title": "3. Evaluation index", "content": "In this study, we adopted Average Precision (AP), a commonly used evaluation criterion in object detection and instance segmentation, to measure model performance. In the context of target detection, IOU serves as a prevalent quantitative metric, adeptly assessing the overlap between the predicted bounding box and the actual target area, thereby providing an efficacious gauge for the precision of object detection algorithms, and thus reflect the accuracy of detection results. The specific calculation formula of the intersection ratio is shown in expression (2).\n$\\IoU = \\frac{Area_{PRED} \\cap Area_{GT}}{Area_{PRED} \\cup Area_{GT}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (2)\nThe Intersection over Union (IoU) metric is calculated by determining the ratio of the intersecting area between the predicted bounding box (the detection frame) and the true bounding box (the actual frame), to the combined area encompassed by both boxes when they are merged together. The higher the value, the greater the overlap degree of the two frames, the better the detection performance. In the average accuracy (AP) evaluation process, an IoU threshold is usually set in advance to judge whether the detection frame classification is correct or not.\nBy iterating over all the bounding boxes predicted by the model, we can calculate Precision and Recall based on the given equation, such as formula (3) and formula (4).\n$\\Precision=\\frac{TP}{TP + FP}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (3)\n$\\Recall=\\frac{TP}{TP + FN}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (4)\nThe accuracy rate reflects the proportion of actual positive cases in the current predicted detection box; The recall rate measures the proportion of correctly detected boxes to all actual positive boxes. Each time a detection box is evaluated, a set of corresponding Precision and Recall values is generated. Plot these pairs of values to form an accuracy rate-recall curve, or P-R curve. According to this curve, we can calculate the average accuracy of the model according to formula (5).\n$\\AP = \\int_{0}^{1}Precision (Recall)d(Recall)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (5)\nThe AP is the area under the P-R curve. Each category is calculated independently, and the AP average of all categories is the mAP. Because there is only one kind of label in the data set, AP is directly used as the index. The AP calculation in instance segmentation only needs to change the detection frame and true frame to prediction and true mask respectively, and the other steps are the same as the target detection."}, {"title": "4. Loss function", "content": "Conventional GAN architectures employ stochastic noise as input to produce outputs, without considering the correlation between the quality of the input imagery and the resultant output labels. In the field of image segmentation, the generative adversarial network generates segmentation labels by parsing the input image, ensuring that the output directly responds to the input pixels, forming a clear function mapping. The objective function for the complete task is specified in Equation (6).\n$\\L(p, t) = AL_{cls} (p, p^*) + p^*L_{reg} (t_i, t_i^*) $(6)\nHere a is used to coordinate the classification and regression losses and takes a value of 0.5. Where, p refers to the prediction probability, and p* is the truth label of the Anchor box. An anchor box is deemed a positive sample, denoted as p* = 1, if its Intersection over Union (IoU) with the actual boundary exceeds 0.5. Conversely, an IoU less than 0.2 classifies it as a negative sample, represented as p* =0.\nIn the classification task, we choose weighted binary cross entropy as the loss function, and its specific mathematical expression is shown in formula (7). Here, the symbol w represents the weight factor used to balance the different classes of loss contributions.\n$\\L_{cls}(p,p^*) = -w[p^*logp + (1 \u2212 p^*) log(1-p)] $ (7)\nThe loss of position regression is calculated using the smooth L1 loss function specified in equation (8) to accurately quantify the position deviation between the predicted box and the real box.\n$\\L_{reg}(t_i, t_i^*) = \\sum_{i\\in\\{x,y,z,d\\}} smootl_{l1}(t_i - t_i^*) $ (8)\n$\\smootl_{l1} (t_i - t_i^*) = \\begin{cases} 0.5(t_i - t_i^*)^2, & \\text{if } |t_i - t_i^*| < 1 \\\\ |t_i-t_i^*|-0.5, & \\text{else} \\end{cases}$ (9)"}, {"title": "B. Improved Pro-HRnet-CNN model", "content": "This paper innovatively proposes the Pro-HRnet-CNN network structure, as shown in Figure 3. In the HRNet architecture, a rich feature pyramid is built by fusing feature maps at different levels. This design ensures that the final output features not only contain fine high-resolution details, but also incorporate rich semantic information extracted by the deep network, which is particularly important when dealing with fine structures such as small target nodules. Next, the candidate region (anchor box) generated by the RPN module is obtained by convolution operation, and then the class probability and position adjustment parameters of each anchor box are calculated by the two-layer fully connected network. After NMS (non-maximum suppression) screening, the candidate boxes that perform well are fed into the Rol Align module for precise cropping. Then, the multilevel cascade detection mechanism uses a series of detectors with increasing IoU thresholds to perform detailed classification and positioning correction of the candidate regions, and finally maps the detection results back to the original image space to achieve accurate positioning. This improved Pro-HRnet-CNN network structure not only ensures the capture of deep semantic information, but also effectively retains the key position clues of small-scale targets, and significantly improves the detection performance of small target nodules."}, {"title": "IV.APPLICATION OF MODEL", "content": "This study selected as the experimental dataset the LIDC-IDRI dataset, a lung CT image database supported by the National Cancer Institute of the United States, containing more than 1000 chest CT scans, each of which was labeled by four physicians with lung nodule boundaries, malignancy, and type. This dataset covers a diverse set of cases and is suitable for the development and testing of lung nodule detection algorithms. Open to researchers for non-commercial research, it is a key resource for lung disease imaging analysis and contributes significantly to improving lung nodule recognition technology.\nIn the experimental design, we adopted a dataset consisting of 6002 chest CT images, among which 4733 images were selected for constructing the training set through random sampling, and the remaining 1269 images were assigned to the test set, ensuring that the ratio of training to test data was close to 4:1, and there was no crossover between the two sets. All images are derived from the DICOM compliant LIDC-IDRI database. At the beginning of the experiment, we used the inherent Rescale Slope and Intercept attributes in DICOM file to convert the original pixel value of CT image into CT value (in HU) according to formula (10), and realized effective conversion and standardized processing of image value.\n$\\OutputUnits = m*SV + b$ (10)\nIn formula (10), variable \"SV\" represents the stored value of each pixel in the medical image in DICOM format, \"m\" corresponds to the Rescale Slope attribute, and \"b\" is the value of the Intercept attribute. After calculation, \"OutputUnits\" reflects the corresponding HU values of each position in the converted CT image. This transformation ensures consistency of image data and accuracy of clinical interpretation."}, {"title": "B. Experimental environment and parameters", "content": "(1) The hardware platform used in the experiment is a high-performance server equipped with Intel(R) Core(TM) i9-9900X CPU with a main frequency of 3.50GHz, supplemented by 64GB memory and equipped with dual NVIDIA GeForce GTX 2080Ti Gpus. Each GPU has 11GB of video memory, ensuring efficient computing power for complex models.\n(2) In terms of software environment, the operating system is a stable version of Ubuntu 16.04 LTS, The integrated development environment (IDE) selected is PyCharm, with Python 3.6 serving as the coding language. For the deep learning framework, both TensorFlow and PyTorch are employed to facilitate swift algorithm refinement and enhancement. Such hardware and software configuration provides powerful technical support for the experiment, ensuring the smooth progress of the experiment and the reliability of the results."}, {"title": "2. Experimental parameters", "content": "Considering the limitations of GPU memory, we split the complete CT image into 96x96x96 three-dimensional cube fragments to better fit the data processing capacity of the GPU, and set the Batch size of each batch to 8. In order to avoid overfitting of the model, data enhancement strategies were implemented for 3D image fragments (positive samples) containing nodules: first, random scaling according to [0.75,1.25] intervals; The second is to perform horizontal flip and random Angle rotation. The model optimization was carried out using the Stochastic Gradient Descent (SGD) technique, with the overall training cycle being fixed at 30 epochs.\nThroughout the training procedure, we noted that with fewer than 30 epochs, despite the rapid decline in the loss values for both the training and validation datasets, there remains potential for enhanced model efficacy. After 30 epochs, loss reduction slowed down, additional training time cost increased, but performance improvement was limited. Based on this, we decided to stop training at 30 epochs.\nDuring testing, we streamlined the process by initially discarding nodules under 3mm, then applying Non-Maximum Suppression (NMS) to manage overlapping candidate regions, simplifying the final results. To mitigate the risk of overfitting, our methodology encompassed augmenting the data, executing 10-fold cross-validation, and instituting a dropout protocol with a 0.5 dropout rate throughout training. These interventions were collectively geared toward fortifying the model's generalization prowess."}, {"title": "C. Experimental results and analysis", "content": "In order to gauge the capability of the newly proposed Pro-HRnet-CNN model in this chapter for accurately detecting small pulmonary nodules in medical images, we used AP under a specific IoU threshold as the main performance measure. At the same time, in order to make a comprehensive comparison, the RetinaNet model widely recognized by the industry and the Fully Convolutional One-Stage Object Detection (FCOS) model are placed in the same data set environment for control tests, the precise comparative outcomes are elaborated upon in Table 1. In this way, the difference of detection accuracy and efficiency of each model is visually demonstrated."}, {"title": "V.CONCLUSIONS", "content": "This study demonstrates the innovative capabilities of the Pro-HRnet-CNN model, which combines HRNet with void convolution techniques to markedly enhance the detection of small pulmonary nodules in CT images. Our findings, supported by rigorous testing on the LIDC-IDRI dataset, clearly show that Pro-HRnet-CNN outperforms the traditional ResNet-50 model, particularly in detecting smaller nodules with higher accuracy. This enhancement is not only a technological advance but also a significant step toward more precise and early diagnosis of lung diseases. The potential implications of the Pro-HRnet-CNN model extend far beyond the realm of lung disease. Its application could revolutionize the fields of oncology, neurology, and cardiology, where early detection of small anomalies can drastically alter prognosis and treatment outcomes. For instance, in oncology, the ability to accurately detect small tumors at an early stage could lead to more effective intervention strategies, potentially increasing survival rates. Similarly, in cardiology, early detection of minute changes in heart tissue could aid in preventing severe cardiac events. Additionally, the integration of this model into real-time imaging systems in clinical settings could provide immediate and accurate diagnostic support, thus facilitating faster decision-making in emergency and critical care scenarios. This could significantly reduce diagnostic errors and improve patient outcomes. By advancing this technology, we move closer to a healthcare paradigm where precision medicine is not just a concept but a practical reality, enhancing patient care across multiple specialties and making early, accurate diagnosis accessible to all."}]}