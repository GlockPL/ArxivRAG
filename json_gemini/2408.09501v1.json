{"title": "Beyond Local Views: Global State Inference with Diffusion Models\nfor Cooperative Multi-Agent Reinforcement Learning", "authors": ["Zhiwei Xu", "Hangyu Mao", "Nianmin Zhang", "Xin Xin", "Pengjie Ren", "Dapeng Li", "Bin Zhang", "Guoliang Fan", "Zhumin Chen", "Changwei Wang", "Jiangjin Yin"], "abstract": "In partially observable multi-agent systems, agents typically\nonly have access to local observations. This severely hinders\ntheir ability to make precise decisions, particularly during de-\ncentralized execution. To alleviate this problem and inspired\nby image outpainting, we propose State Inference with Dif-\nfusion Models (SIDIFF), which uses diffusion models to re-\nconstruct the original global state based solely on local obser-\nvations. SIDIFF consists of a state generator and a state ex-\ntractor, which allow agents to choose suitable actions by con-\nsidering both the reconstructed global state and local obser-\nvations. In addition, SIDIFF can be effortlessly incorporated\ninto current multi-agent reinforcement learning algorithms to\nimprove their performance. Finally, we evaluated SIDIFF on\ndifferent experimental platforms, including Multi-Agent Bat-\ntle City (MABC), a novel and flexible multi-agent reinforce-\nment learning environment we developed. SIDIFF achieved\ndesirable results and outperformed other popular algorithms.", "sections": [{"title": "Introduction", "content": "As cooperative multi-agent reinforcement learning (MARL)\nadvances, many proposed methods are being success-\nfully applied to a variety of practical problems, such\nas online ride-hailing platforms (Li et al. 2019), drone\nswarm management (Wang et al. 2023), and energy system\nscheduling (Zhang et al. 2023). While some methods\nrely on explicit communication mechanisms (Das et al.\n2019; Li et al. 2024; Jiang and Lu 2018), most algorithms\nfollow the centralized training with decentralized execution\n(CTDE) framework (Lowe et al. 2017). Agents can share\ninformation or access the global state in the training phase\nof MARL algorithms that adhere to the CTDE paradigm.\nDuring the decentralized decision-making phase, agents\nmust select appropriate actions based solely on their\nindividual local observations. This limitation is especially\npronounced in partially observable Markov decision pro-\ncesses (POMDPs) (Spaan 2012), where the absence of\nglobal state awareness during execution can impede the\nagents' ability to make optimal choices.\nTo address the challenges of partial observability, some\nprevalent methods (Hausknecht and Stone 2015; Wang, Ev-\nerett, and How 2020) involve integrating recurrent neural\nnetworks (RNNs) (Yu et al. 2019) within reinforcement\nlearning models. By incorporating historical information\ninto their decision-making process, agents can mitigate is-\nsues arising from incomplete information. However, these\napproaches completely disregard the utilization of any ad-\nditional information about global state. An alternative ap-\nproach (Xu et al. 2022a; Jiang et al. 2020) concentrates on\nforecasting a low-dimensional, abstract representation of the\nglobal state. This method employs representation learning\ntechniques, such as contrastive learning (Chen et al. 2020),\nto distill global state information. This paradigm avoids the\ncomplexities involved in directly reconstructing the global\nstate. However, since the processes of representation learn-\ning and reinforcement learning are relatively independent,\nthere is no guarantee that the derived additional information\nwill enhance the reinforcement learning process. In sum-\nmary, neither of the methods above effectively leverages the\nglobal state during the decentralized execution phase.\nDecision-making based on reconstructed global states has\na biological foundation. In practical scenarios, humans fre-\nquently make decisions with incomplete information by in-\nferring the broader context from local cues (Simon 1955;\nFriston 2010). Thoughtful and rational decisions can only\nbe made when both the inferred global state and the immedi-\nate local observations are considered. This principle applies\nequally to cooperative multi-agent systems. Suppose each\nagent can infer the current global state in a distributed man-\nner. In that case, it not only improves the decision-making\nprocess but also empowers the agent to select useful in-\nformation autonomously. This approach ensures that global\nstate information is utilized comprehensively and efficiently.\nHowever, reconstructing the global state from local obser-\nvations within the original state space remains a formidable\nchallenge. Some prior studies (Huang et al. 2020; Han,\nDoya, and Tani 2020; Hausknecht and Stone 2015; Igl et al.\n2018) have explored traditional gener-\native models, such as Generative Adversarial Networks\n(GANs) (Goodfellow et al. 2022) and Variational Autoen-\ncoders (VAEs) (Kingma and Welling 2014), for reconstruct-\ning the global state. Nevertheless, they have typically been\ntailored to simple single-agent tasks or multi-agent scenar-\nios with low-dimensional state spaces. Furthermore, other\nworks employing particle filter methods to build world mod-\nels have encountered similar drawbacks.\nTo overcome this challenge, we proposed the State\nInference with DIFFusion Models (SIDIFF) framework.\nDiffusion models can extend the boundaries of an existing\nimage, a technique known as image outpainting (Bertalmio\net al. 2000), which has been successfully applied in well-\nknown computer vision projects like DALL-E (Ramesh\net al. 2021). Inspired by the diffusion model, we use it as a\nstate generator for agents to infer the original global state in\na distributed manner based on local observations. We then\nintroduce the Vision Transformer (ViT) (Dosovitskiy et al.\n2021) architecture as a state extractor to extract information\nfrom the reconstructed global state effectively. With access\nto comprehensive global state information, agents can\nimprove training efficiency in partially observable online\ncooperative reinforcement learning tasks. Therefore, the\nagent can reconstruct the global state in the original state\nspace, and then refines this reconstructed state to yield more\nefficient global information, as illustrated in Figure 1. As a\nresult, during the decentralized execution phase, the agent in\nthe SIDIFF framework can select appropriate actions lever-\naging both global insights and local observations. SIDIFF\nhas achieved good performance in recent popular partially\nobservable benchmarks, as well as in the new environment\nwe proposed, Multi-Agent Battle City (MABC). To the\nbest of our knowledge, SIDIFF is the first framework that\nuses diffusion models for reconstructing the global state in\nonline multi-agent reinforcement learning tasks."}, {"title": "Related Work", "content": "Partially Observable Problems\nIn both single-agent and multi-agent scenarios, the partially\nobservable Markov decision process (POMDP) (Cassandra\n1998) requires agents to make decisions based on incom-\nplete information. Due to the possibility of a lack of crit-\nical information, agents are frequently limited to selecting\nlocally optimal actions. Therefore, bridging this information\ngap is an important research topic in reinforcement learn-\ning. The most popular idea is to employ recurrent neural net-\nworks (RNNs) to integrate local observation data over time,\nthereby endowing agents with a form of long-term mem-\nory. As a seminal work in this field, DRQN (Hausknecht\nand Stone 2015) has influenced various MARL methods,\nincluding R-MADDPG (Wang, Everett, and How 2020),\nQMIX (Rashid et al. 2018), and MAPPO (Yu et al. 2022).\nAnother popular technique is belief tracking with particle\nfilters (Ma et al. 2020b,a), which typically requires a pre-\ndefined model. Some research (Huang et al. 2020; Han,\nDoya, and Tani 2020; Hausknecht and Stone 2015; Igl et al.\n2018) has focused on generating latent state representa-\ntions through variational inference. Variational autoencoders\n(VAE) (Kingma and Welling 2014), as generative models,\ncan compress complex observations to a compact latent\nspace by maximizing the evidence lower bound. However,\nthese methods focus primarily on the issue of inaccessible\nglobal state during training, often overlooking the potential\nutility of states. Communication (Foerster et al. 2016; Peng\net al. 2017) emerges as a distinct method to mitigate the need\nfor global information. Agents in communication methods\nexchange information, which is either manually specified or\nlearned. However, this approach also incurs high communi-\ncation costs. Additionally, some studies (Zhang et al. 2021;\nXu et al. 2022b; P\u00e1sztor, Krause, and Bogunovic 2023)\nmodel opponents or the environment as auxiliary tasks to\npredict the behavior of other entities. However, these meth-\nods are unsuitable for complex environments and may be\nconstrained by issues such as compounding errors.\nDiffusion Models for Reinforcement Learning\nDiffusion models (Ho, Jain, and Abbeel 2020), renowned\nfor their capacity to generate diverse data and capture mul-\ntimodal distributions, are increasingly being integrated into\nreinforcement learning to improve performance and sample\nefficiency. The most mainstream method is to fit the dynam-\nics of the environment with diffusion models, which serve\nas a planner (Brehmer et al. 2023; Janner et al. 2022; Liang\net al. 2023). Guided by objectives such as expected return-\nto-go, diffusion models can generate trajectories that align\nwith both the given directives and the environmental dy-\nnamics. Moreover, just as diffusion models are used for data\naugmentation in computer vision, they also hold promise\nas data synthesizers in offline reinforcement learning (Chen\net al. 2023; Lu et al. 2023). Some studies (Ada, \u00d6ztop, and\nUgur 2024; Chi et al. 2023; Hansen-Estruch et al. 2023) have\nemployed diffusion models directly as policies, addressing\nchallenges such as over-conservatism and limited adaptabil-\nity to diverse datasets in offline settings. Furthermore, a few\nworks also applied diffusion models in fields such as quality\ndiversity reinforcement learning and multi-agent coopera-\ntion tasks. However, almost all of the above-mentioned stud-\nies combined diffusion models and reinforcement learning\nalgorithms in offline settings or within single-agent tasks,\nprimarily to mitigate common offline issues. In contrast, the\nSIDIFF framework we propose is intended for online multi-\nagent tasks, explicitly targeting the challenges of partial ob-\nservability. Regardless of the background or concerns, SID-\nIFF is entirely orthogonal to previous work."}, {"title": "Preliminaries", "content": "Dec-POMDPs\nThe fully cooperative multi-agent decision-making prob-\nlem can be modeled as a decentralized partially observable\nMarkov decision process (Dec-POMDP). Within this prob-\nlem, there are n agents denoted by the set A = {1, ..., n}.\ns \u2208 S is the global state of the environment. The observa-\ntion function, denoted by O : A\u00d7S \u2192 Z, assigns dis-\ntinct local observations to each agent based on the current\nglobal state s. At each time step, each agent a selects an\naction u \u2208 U based on its local observation z = O(a, s).\n$\\mathbf{u} \\in \\mathbf{U} = \\mathbf{U}^n$ represents the joint action of all agents. In\nthe Dec-POMDP problem, all agents share a reward func-\ntion r(s, u): S\u00d7U \u2192 R, which ensures full coopera-\ntion between agents when making decisions. $p(s' | s,\\mathbf{u}) :$\nS\u00d7U \u00d7 S \u2192 [0, 1] denotes the transition function of the\nsystem. The goal of all agents is to maximize the discounted\nreturn $\\sum_{i=0}^{\\infty} \\gamma^i r_{t+i}$, where \u03b3 is the discount factor.\nDenoising Diffusion Probabilistic Models\nThe diffusion model draws inspiration from nonequilibrium\nthermodynamics (Sohl-Dickstein et al. 2015). It encom-\npasses two key processes: the forward and the reverse pro-\ncesses. Noise is gradually introduced to the original data x\nduring the forward process. Conversely, the reverse process\nsystematically removes this noise, thereby restoring the data\nto its initial state. First, sample $x_0 \\sim q(x_0)$ from the actual\ndata distribution. The forward process sequentially intro-\nduces Gaussian noise to the data according to the predefined\nvariance parameter $\\beta_{1:K}$. Specifically, the $x_k$ obtained in the\nk-th iteration of the forward process is derived as follows:\n$q(x_k | x_{k-1}) = \\mathcal{N}(x_k; \\sqrt{1 - \\beta_k}x_{k-1}, \\beta_k\\mathbf{I})$.\nWhen deriving $x_k$ during the forward process, it can\nbe directly computed from the initial data $x_0$ without the\nnecessity of sequentially calculating the intermediate steps\n$x_{1:k-1} = \\{x_1, x_2, ..., x_{k-1}\\}$. The specific calculation\nformula can be written as follows:\n$x_k = \\sqrt{\\bar{\\alpha}_k}x_0 + \\sqrt{1 - \\bar{\\alpha}_k}\\epsilon(x_k, k)$,\nwhere $\\bar{\\alpha}_k = \\prod_{i=1}^{k} \\alpha_i$ and $\\epsilon(x_k, k) \\sim \\mathcal{N}(0, \\mathbf{I})$, with\n$\\alpha_k = 1 - \\beta_k$. In the reverse process, the diffusion model\nis trained to learn a conditional distribution that iteratively\ndenoises and restores the original data. The formula to\nrecover $x_{k-1}$ from $x_k$ is as follows:\n$p(x_{k-1} | x_k) = \\mathcal{N}(x_{k-1}; \\mu(x_k, k), \\sigma(x_k, k)).$ (1)\nAnd $\\mu$ and $\\sigma^2$ can be given as follows:\n$\\mu (x_k, k) = \\frac{1}{\\sqrt{\\alpha_k}} (1 - \\frac{\\beta_k}{1 - \\bar{\\alpha}_k}) x_k + \\frac{\\sqrt{\\alpha_k} - \\alpha_k}{\\sqrt{1 - \\bar{\\alpha}_k}} \\epsilon_{\\theta} (x_k, k)),$\n$\\sigma^2 (x_k, k) = \\frac{\\beta_k (1 - \\bar{\\alpha}_{k-1})}{1 - \\bar{\\alpha}_k},$\nwhere $\\sigma^2$ is a constant value, and $\\mu$ is a function dependent\non $\\epsilon_{\\theta}(x_k, k)$. We employ a neural network to estimate\n$\\epsilon(x_k, k)$, denoted as $\\epsilon_{\\theta}(x_k, k)$, where \u03b8 represents the\nparameters of the neural network. The diffusion model is\ntrained by minimizing the following loss function:\n$\\mathcal{L} := \\mathbb{E}_{k, x_0} [||\\epsilon_k - \\epsilon_{\\theta} (\\sqrt{\\bar{\\alpha}_k}x_k + (\\sqrt{1 - \\bar{\\alpha}_k}) \\epsilon_k, k) ||^2] .$\n$\\mathbb{E}_k$ is the actual noise sampled during the forward process."}, {"title": "Methodology", "content": "The core idea behind SIDIFF is to empower the agent to de-\nduce the global state based on local observations during de-\ncentralized execution, leveraging this inferred information\nto refine their decision-making processes. In this section, we\ndelve into the architecture of SIDIFF, which is composed of\ntwo key components: the State Generator and the State Ex-\ntractor. For ease of exposition, when variables in this paper\nare subscripted with two indices, the first index refers to the\nspecific time step within an episode, while the second in-\ndicates the iteration of the diffusion process. For instance,\n$s_{t,k}$ represents the state s generated by the diffusion model\nat time step t in the k-th iteration of the diffusion process.\nState Generator\nFirst, we introduce the state generator module, which mod-\nels data distribution using the diffusion model. The state\ngenerator is intended to infer the global state automatically,\nsimilar to image outpainting tasks. The state generator is\npatterned after the U-Net architecture (Ronneberger, Fis-\ncher, and Brox 2015), as depicted in Figure 2. It comprises\na series of one-dimensional convolutional residual blocks.\nThe structure of the state generator is divided into two sym-\nmetrical segments: an encoder and a decoder. The encoder\nextracts contextual state information, while the decoder re-\nconstructs the original state. Moreover, the decoder employs\nskip-connection techniques to merge feature maps from the\nencoder with those in the decoder, facilitating the integration\nof multi-scale features. Similar to the traditional diffusion\nmodel, the state generator takes the reconstructing state as\ninput and outputs the predicted noise \u03f5. Further details will\nbe given below.\nThe state generator needs to reconstruct the global state\nbased on local observations of individual agents. However,\nin partially observable problems, agents may encounter the\nchallenge of having identical local observations, even in fun-\ndamentally distinct global states. Therefore, to ensure the\nuniqueness of these conditions used to generate the state\nas much as possible, we integrate time-sensitive data into\ndecoders. SIDIFF involves the current diffusion iteration k,\nalong with unique identifiers a for each agent, and the trajec-\ntory history \u03c4. These elements are employed as conditions\nto predict the noise. The embeddings $e_k$, $e_a$, and $e_t$ are the\nfeatures derived from the affine transformation of the afore-\nmentioned conditions. Assuming $f_l$ represents the output\nfrom the preceding layer within a particular decoder block,\nand $f_e$ denotes the skip-connected feature from the corre-\nsponding encoder block, the final input for the decoder block\nwould be written in the form:\n$\\text{CONCAT}((e_\\tau + e_A), (f_L + e_k), f_e)$.\nThe trajectory history of an agent is essential for the state\ngenerator to reconstruct the global state. In addition, $e_k$\nplays a pivotal role in the training and sampling phases of\nthe diffusion model. Like other classic works, we incorpo-\nrate it as a bias term. Finally, these conditional variables\nare concatenated with the output of the symmetric encoder\nblock to form the input of the decoder block. Referring to\nclassifier-free diffusion models (Ho and Salimans 2022), we\njointly train the unconditional and conditional models sim-\nply by randomly setting the information of these agents to\nthe unconditional class identifier \u00d8 with some probability\n$\\beta \\sim Bern(p)$. In practical implementation, the uncondi-\ntional class identifier is typically substituted with a vector\nof zeros.\nThe training and sampling regimen of the state generator\nclosely resembles that of traditional diffusion models. We\nset the diffusion process duration to $K \\in \\mathbb{N}^+$. The goal of\ntraining is for the state generator to combine the iteration\nk, the local observation of the agent, and the ground-truth\nglobal state to predict the noise $\\epsilon_k$ introduced in the for-\nward process, rather than directly reconstructing the original\nglobal state as the VAE does. The loss function for training\nthe state generator is as follows:\n$\\mathcal{L}_{SG}(\\theta) = \\mathbb{E}_{k, s_t, o \\in D, \\beta \\sim Bern(p)} [||\\epsilon_k | \\epsilon_k - \\epsilon_\\theta (s_{t,k}, (1 - \\beta)c + \\beta \\varnothing, k) ||^2]$,\nwhere \u03b8 denotes the network parameters of the state genera-\ntor, and $c = \\{\\alpha, \\tau\\}$ is the conditional dependencies required\nfor generating the global state. Even in a fully distributed\nframework, agents can acquire these conditions. In the in-\nference phase, each agent initializes $\\hat{s}_{t,K} \\sim \\mathcal{N}(0, \\mathbf{I})$ and\nthen removes the noise \u03f5 predicted by the state generator to\nobtain the denoised state. Precisely, the inferred global state\nat the k-th iteration of the reverse process can be calculated:\n$\\hat{s}_{t,k-1} = \\frac{1}{\\sqrt{\\alpha_k}} (s_{t,k} - \\frac{1 - \\alpha_k}{\\sqrt{1 - \\bar{\\alpha}_k}} \\epsilon_\\theta (s_{t,k}, c, k)) + \\sigma_k \\mathcal{z}$,\nwhere z \u223c N(0, I). This iterative procedure continues until\nthe original global state $\\hat{s}_{t,0}$ is restored. The incorporation of\nthe state generator module endows the agent with the ability\nto conduct distributed inference of the original global state.\nState Extractor\nOn the one hand, the global state encompasses critical in-\nformation that agents may depend on for decision-making,\nwhich is frequently unavailable through local observations\nalone. On the other hand, the global state contains a large\namount of redundant information irrelevant to the decision-\nmaking of individual agents. Directly incorporating the\nglobal state into the agent network may place an undue\nburden on individual agents, potentially reducing learning\nefficiency. This phenomenon has been well-documented in\nsome prior research (Li et al. 2024; Guan et al. 2022). Con-\nsequently, extracting decision-relevant information from the\ninferred global state $\\hat{s}$ is also an important contribution of\nSIDIFF. In the following, we explain how to extract crucial\ndecision-relevant information from the original global state.\nThe global state is defined as containing all information\nin a multi-agent system. However, for an individual agent,\nonly a small portion of the global state is actually useful\nfor decision-making. Therefore, it is imperative to distill the\nglobal state into a meaningful abstraction, a task that typi-\ncally demands expert knowledge. Drawing inspiration from\nthe Vision Transformer (ViT) in computer vision, we first\ndivide the global state into a series of fixed-size 1D patches\n$P = \\{p_i\\}, i \\in \\{1, . . ., N\\}$. These vectors are then arranged\nin sequence to form an ordered sequence. Simultaneously,\nposition embeddings are integrated to infuse positional in-\nformation into the patch vectors, preserving the intrinsic se-\nmantics of the original global state. These vectors serve as\nthe input to the Transformer model. By utilizing the multi-\nhead attention mechanism (Vaswani et al. 2017), we derive\nthe abstracted feature embedding $s_g$ for the global state. It\ncan be computed by the following equation:\n$s_g = \\text{CONCAT} (\\text{Softmax}( \\frac{(PW_Q)^i (PW_K)^T}{\\sqrt{d}} ) PW_V)$.\nwhere $j \\in \\{1, . ., H\\}$. d is the scaling factor equal to the\ndimension of each patch, and H is the number of attention\nheads. Finally, $s_g$ will be concatenated with the agent's local\nobservation and fed into the agent network.\nAs illustrated in Figure 2, the state extractor converts the\noriginal global state into information that is more conducive\nto the agent's decision-making and feeds it into the agent's\nmodel. Unlike other studies that replace the global state with\nlow-dimensional embeddings, the state extractor is directly\noptimized end-to-end by the reinforcement learning process\nrather than multiple loss functions or multi-stage optimiza-\ntion. Therefore, when making decisions, the agent actively\nselects information from the global state conducive to its\nown decision-making, rather than passively absorbing infor-\nmation that other relatively independent modules have pro-\ncessed.\nCentralized Training with Decentralized Execution\nAlthough diffusion models can generate more complex data\ncompared to other generative models, it is achieved by\nbreaking down the entire generation process into multiple\nsteps, which incurs a higher cost in terms of wall-clock\ntime. SIDIFF is an online multi-agent reinforcement learn-\ning framework, in contrast to previous research, which was\nprimarily conducted offline and relied solely on predeter-\nmined datasets with no real-time environmental interaction.\nTherefore, to reduce the additional time cost introduced by\nthe denoising process of the diffusion model, SIDIFF uti-\nlizes the ground-truth state s during the training phase. Only\nduring the decentralized decision-making phase does the\nagent use the state generator to produce $\\hat{s}$ to replace the true\nglobal state s. Thus, the multi-step reverse process only im-\npacts the evaluation phase, accounting for a small portion of\nthe algorithm training process. This approach ensures that\nthe time cost is not significantly increased while also main-\ntaining the stability of the algorithm training. Moreover, in\nour implementation, we only update the state generator prior\nto evaluation, in accordance with Eq. (2), which also reduces\ntraining time. Last but not least, more advanced diffusion\nmodel methods (Bao et al. 2022; Song et al. 2023) can be\napplied in the future to further enhance the efficiency of SID-\nIFF during the decentralized execution phase.\nSIDIFF can be applied to various multi-agent reinforce-\nment learning algorithms that adhere to the CTDE paradigm,\nincluding value decomposition and policy-based methods.\nThe most significant modification in the SIDIFF-augmented\nvariants of these algorithms is the introduction of $s_g$, which\ncontains important information about the global state. Note\nthat $s_g$ is generated by the state generator and subsequently\nabstracted by the state extractor. For value decomposition\nmethods with SIDIFF, the value function for agent a is de-\nfined by $Q_a (\\tau_a, s_g)$. And we can obtain the joint value func-\ntion $Q_{tot}$:\n$Q_{tot} = f_m (Q_a, a \\in A)$,\nwhere the function $f_m$ represents the mixing network,\nwhich can take different forms depending on the specific\nvalue decomposition method. For policy-based methods like\nMAPPO, the Actor network is denoted as $\\pi_{\\eta}(\\tau_n, s_g)$ in their\nSIDIFF-augmented versions. It can be seamlessly integrated\ninto the policy gradient loss function of the original algo-\nrithm. The details for the variants with SIDIFF of these two\ndifferent algorithms are presented in Appendix A."}, {"title": "Experiment", "content": "To validate whether SIDIFF can be applied to various al-\ngorithms and enhance performance, we have obtained ex-\ntensive experimental results on three different experimental\nplatforms: SMAC (Samvelyan et al. 2019), VMAS (Bettini\net al. 2022), and MABC. Both SMAC and VMAS are well-\nknown environments in the field of multi-agent systems,\nwith partial observability and a diverse set of scenarios for\nevaluating cooperation between agents. Multi-Agent Bat-\ntle City (MABC) is a novel experimental platform we have\nproposed, in which agents only have access to local obser-\nvations and scenarios can be flexibly customized. We con-\nducted a case study in a specific MABC scenario to demon-\nstrate the importance of the global state in decision-making.\nFinally, through a series of ablation experiments and visu-\nalization, we show that both the state generator and state\nextractor in SIDIFF are indispensable. Detailed information\nabout the testbeds can be found in Appendix B.\nStarCraft Multi-Agent Challenge\nSMAC is a cooperative multi-agent reinforcement learning\nenvironment based on the real-time strategy game StarCraft\nII. It includes a variety of unique scenarios in which agents\nselect actions based on local observations within their vi-\nsual field. The goal of all scenarios is to command allied\nunits to eliminate enemy units, which are controlled by built-\nin heuristic algorithms. We applied SIDIFF to the value\ndecomposition method QMIX and compared it with other"}, {"title": "Conclusion and Discussion", "content": "In this paper, we propose SIDIFF to address the issue of\nagents being unable to access the global state during the\ndecentralized execution phase in Dec-POMDP tasks. The\nstate generator in SIDIFF, a diffusion model, reconstructs\nthe global state in a distributed manner based on the agents'\ntrajectory history. The state extractor then processes the re-\nconstructed global state, removing redundant information\nand extracting decision-relevant information for the agents.\nThese two components work together to provide SIDIFF\nwith superior performance in a variety of cooperative multi-\nagent environments. In addition, we introduce Multi-Agent\nBattle City (MABC), a novel multi-agent experimental envi-\nronment that allows flexible customization of scenarios. We\nbelieve that MABC is sufficiently simple yet distinctive to\nincentivize more contributions from the MARL community.\nFuture work will involve exploring the application of SID-\nIFF in multi-task multi-agent scenarios. Moreover, replacing\nthe current traditional diffusion model with faster generative\ndiffusion models to improve inference efficiency is a promis-\ning research direction."}, {"title": "A Implementation Details", "content": "A.1 Algorithmic Description\nThe algorithms for different methods with SIDIFF are summarized in Algorithm 1 and Algorithm 2. The code for SIDIFF can\nbe found in the supplementary material.\nA.2 Implementation Details and Hyperparameters\nIn practical implementation, to ensure algorithm training stability, we typically update the state generator only before evaluation.\nThis approach ensures the effectiveness of the state generator in inferring states during evaluation and also reduces training\ntime. When optimizing the state generator, we typically use the trajectories from the most recent L episodes as the training\ndataset to avoid significant differences between the agent's current policy and the historical transition data. By default, L = 5.\nFurthermore, the historical trajectories, which serve as the basis for reconstructing the global state, usually include observations\nfrom the past four steps and the current step.\nThe abstracted feature embedding $s_g$ output by the state extractor is eventually combined with the agent's local observations\nand fed into either the agent network (in value decomposition methods) or the actor network (in policy gradient methods).\nFurthermore, to mitigate the compounding error caused by minor deviations in the reconstructed state $\\hat{s}$, we feed $s_g$ into a fully\nconnected layer following the RNN layer."}, {"title": "B Environment Details", "content": "B.1 StarCraft Multi-Agent Challenge (SMAC)\nThe StarCraft Multi-Agent Challenge (SMAC) is a benchmark environment designed to test and develop MARL algorithms\nwithin the complex setting of the real-time strategy game StarCraft II. SMAC focuses on micromanagement tasks", "range": "relative x, relative y,orientation, and remaining cooldown\ntime for missile firing. Additionally, agents can obtain their own absolute position, orientation, and cooldown time.\nThe global state, which is only available to agents during centralized training, contains information about all units on the map.\nAll features, both in the state and in the observations of individual agents, are normalized by their maximum values. The sight\nrange of all agents can be adjusted according to the needs. Moreover, MABC provides both raw pixel data and low-dimensional\nfeature information for states and observations.\nIn MABC, all agents (tanks) share the same action space. The discrete set of actions that agents are allowed to take consists\nof move [left, right, up, down"}]}