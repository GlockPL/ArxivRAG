{"title": "SAFECHAIN: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities", "authors": ["Fengqing Jiang", "Zhangchen Xu", "Yuetai Li", "Luyao Niu", "Zhen Xiang", "Bo Li", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SAFECHAIN, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMS with SAFECHAIN, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.", "sections": [{"title": "1 Introduction", "content": "With the rapid evolution of Large Language Models (LLMs), significant efforts have been made to improve model capabilities, particularly in complex reasoning tasks such as mathematics and coding. Emerging large reasoning models (LRMs), such as OpenAI's o1 (Jaech et al., 2024) and DeepSeek-R1 series models, (Guo et al., 2025) are trained to learn reasoning by \"thinking\" through long chain-of-thought. LRMs follow a structured thought process, generating long reasoning traces with multiple intermediate steps when answering complex questions even without advanced prompting strategies. These models have demonstrated remarkable performance on complex tasks and are increasingly integrated into daily tasks such as assisting coding development and scientific discovery (Chan et al., 2024; Chen et al., 2025).\nAs LRM gain broader attention, evaluating their safety is crucial as long reasoning traces do not inherently guarantee safe responses (Qi et al., 2024). Unsafe responses from reasoning models raise ethical concerns and lead to severe consequences, such as creating bugs and vulnerabilities in codebases and spreading misinformation that biases students' understandings. At present, however, safety of LRMs remains less studied.\nCompared with regular LLMs, LRMs differ in two key ways: (1) their responses intrinsically include chains of thought alongside a final answer, (2) their outputs tend to be significantly longer. While an LRM's final answer may appear safe\u2014e.g., by refusing to comply with a harmful prompt\u2014its intermediate reasoning can still contain harmful or policy-violating content, as the eample shown in Figure 1. This makes it crucial to adopt new safety evaluation methods that inspect both the chain of thought (CoT) and the final answer. Moreover, the sheer length of LRMs' outputs makes manual evaluation prohibitively expensive at scale, yet the effectiveness of existing automated evaluators on long reasoning traces is largely unknown. Finally, developing approaches that improve LRMs' safety without degrading their strong performance on complex reasoning tasks is an equally urgent goal."}, {"title": "2 Preliminary: LRMs with Long CoT", "content": "We denote the sequence of tokens representing an instruction as $x$. The token sequence representing a response generated by an auto-regressive model is denoted as $y$. For LRMs, the response $Y = Y_{COT} \\oplus Y_{ans}$ comprises two parts: the reasoning trace $Y_{COT} \\subseteq y$ constitutes the CoT and the final answer $Y_{ans} \\subseteq y$. Here $\\oplus$ represents concatenation. The reasoning trace allows models to branch out and explore other paths to generate final answer, or revert to a previous checkpoint to correct errors. An illustrative example is shown in Figure 1. Depending on the developer, the reasoning trace $Y_{COT}$ is not necessarily visible to users. For example, OpenAI's o-series models do not reveal the reasoning thoughts whereas DeepSeek-R1 displays the intermediate steps as part of responses."}, {"title": "3 Safety Evaluation of LRMS", "content": "This section presents a comprehensive safety evaluation of reasoning models."}, {"title": "3.1 Pilot Study of Safety Evaluators for LRMS", "content": "Our goal in this study is to find safety evaluators to effectively flag unsafe responses with long CoT generated by reasoning models.\nEvaluators. We consider four safety evaluators in this study: Llama-Guard, Refusal String Matching (RS-Match) (Zou et al., 2023), OpenAI Moderation API (OpenAIMod) (Kivlichan et al., 2024), and fine-tuned LLM Judge from HarmBench (Mazeika et al., 2024).\nModels and Dataset. We consider six reasoning models, including R1-7B/8B, Gemini-Thinking, Sky-T1, QwQ, and Skywork-01. We prompt these reasoning models with queries from StrongReject small (with 60 instructions) using temperature $t = 0.6$. This leads to 360 query-response pairs. We label each query-response as safe, unsafe, or unsure, and eliminate the samples labeled as unsure due to their ambiguity in safety evaluation. This leaves us an evaluation dataset containing 272 samples labeled with either safe or unsafe.\nMetrics. We assess the effectiveness of evaluators using three metrics: accuracy (ACC), F-1 Score (F-1) and Pearson correlation coefficient (PCC) (Cohen et al., 2009) to human annotations.\nLlama-Guard is the best evaluator. We summarize the effectiveness of all evaluators based on ACC, F-1, and PCC in Table 2. Our results show that Llama-Guard consistently outperforms others across all metrics. This implies that Llama-Guard is robust and can serve as the safety evaluator for reasoning models. In our following study, we use Llama-Guard as our safety evaluator."}, {"title": "3.2 Experimental Setup", "content": "Models and Configurations. We consider a broad range of LRMs for safety evaluation, including both open- and closed-source models. These reasoning models include DeepSeek-R1 series, Skywork-o1, QwQ, Sky-T1, Gemini-Thinking, and Kimi-k1.5. Detailed information of evaluated reasoning models is presented in Table 1. We do not include OpenAI's o-series because they do not disclose the reasoning traces to users. For each model, we consider two sets of generation configurations as in Table 3: Greedy sampling with temperature $t = 0$ and Non-deterministic (Non-Det) sampling using various temperature/top-p/top-k setups.\nMetrics. We employ multiple metrics to evaluate the safety of reasoning models. Motivated by Guo et al. (2025), we define the following metrics:\n$Safe@1 = \\frac{1}{K} \\sum_{i=1}^{K} S_i$\n$ConsSafe@K = \\frac{1}{K} \\sum_{i=1}^{K}  1{ s_i \\geq \\frac{K}{2} },$\n$Safe@K = \\frac{1}{K}  1{ s_i = 1},$\nwhere $s_i$ is a binary indicator showing whether response $y_i$ for $i \\in {1,\u2026, K}$ to a query $x$ is safe or not. Specifically, Safe@1 evaluates the percentage of safe ones among $K$ generated responses. Safe@K is a binary indicator, where Safe@K=1 if all K responses are safe and Safe@K = 0 otherwise."}, {"title": "3.3 Experimental Results", "content": "Finding 1: Safety performance of SOTA LRMs should be improved.\nTable 4 summarizes the safety performance of SOTA LRMs evaluated using Safe@1, Safe@K, and ConsSafe@K under all configurations. We observe that no model exhibit strong safety performance on both StrongReject and WildeJailbreak datasets. This implies that LRMs should be better aligned for safety.\nFinding 2: Safety performance improves as model scales.\nIn Table 4, we evaluate the safety of LRMs based on Safe@1, Safe@K, and ConsSafe@K under all configurations. We observe that within the same model family, the models become safer as their sizes scale (from DeepSeek-R1-1.5B to R1).\nFinding 3: Unsafe responses from LRMs are likely to be longer than safe ones.\nAn interesting observation is that some unsafe responses have extremely long lengths. To investigate the patterns exhibited by safe and unsafe responses, we collect the responses to prompts in StrongReject and WildJailbreak. We show the histogram of safe and unsafe responses based on response length in terms of number of tokens in Fig. 2. We note that unsafe responses tend to use more tokens and hence are longer than safe ones.\nFinding 4: Learning long CoT does not necessarily enhance safety.\nWe investigate how long CoT affects safety of LRMs by comparing the safety of R1-70B with its pre-trained model Llama-3.3-70B-Instruct, as well as the corresponding base model Llama-3.1-70B2. Note that R1-70B is fine-tuned using long CoT from Llama-3.3-70B-Instruct. We evaluate both models using StrongReject and WildJailbreak. For each dataset, we use each model to create a set of instruction-response pairs. We then filter the pairs from both models whose responses are flagged by safety evaluator, leading to 350 curated pairs. Inspired by LLM-as-a-Judge framework for pairwise assessment of LLMs (Zheng et al., 2023; Lin et al., 2025), we adapt the score-based LLM safety judge (Souly et al., 2024) into a pairwise evaluation format. The full evaluation prompt is in Figure 5 in Appendix B. We select gpt-4o-2024-11-20 as the LLM judge, as it demonstrates top performance on the Chatbot Arena leaderboard (Zheng et al., 2023).\nOur results are summarized in Figure 3. We make two observations. First, R1-70B outperforms Llama-Base, where R1-70B generates safe responses to 76.6% queries. We hypothesize that the this is because R1-70B uses Llama-3-Instruct as the base model, which has undergone a thorough safety fine-tuning. Our second observation is that the safety performance degrades after fine-tuning with long CoT (R1-70B vs Llama-3-Instruct). Particularly, Llama-3-Instruct wins 45.7% in terms of generating safe responses. This implies that fine-tuning with long CoT does not necessarily enhance safety performance.\nFinding 5: Temperature affects safety.\nIn Fig. 4, we present the safety of LRMs under different decoding configurations. As temperature increases, the safety performance of LRMs degrades. For example, Safe@K of R1-7B drops from 30% to less than 20% as temperature increases to 1.2. However, the values of p for top-p decoding and k for top-k decoding do not impact the safety significantly."}, {"title": "4 Safety of LRMs' Thought and Answer", "content": "In this section, we perform a granular safety analysis on responses by LRMs. We focus on the DeepSeek R1-series models, which provide clear segmentation tags of reasoning trace and answer."}, {"title": "4.1 Fine-Grained Safety Analysis of LRMS", "content": "We collect the responses from R1-1.5B to R1 on StrongReject and WildJailbreak, and decompose the responses into thought ($y_{CoT}$) and answer ($y_{ans}$) pieces. Then the decomposed thoughts and answers are evaluated by Llama-Guard respectively. The results are shown in Table 5. We make the following observations. First, safe thought may not always lead to safe answers. Responses whose thoughts and answers are safe only account for 41.1% of the examples. Second, unsafe thought of LRMs is likely to lead to unsafe answers. In some occasions, unsafe thought may still generate safe answers due to the reflection and error correction capabilities of LRMs."}, {"title": "4.2 Thinking Affects Answer Safety", "content": "Based on our analysis in Section 4.1, we investigate how thought process affects safety. We design three decoding strategies to control the length of thought process, with detailed examples in Table 8 of Appendix:\n* ZeroThink: Motivated by Jiang et al. (2024a), we enforce the response prefix to be an empty thought segment, i.e., \"<think></think>\". This forces the model to generate responses without applying any thought.\n* LessThink: We enforce the model to start its response with a short thought process, i.e., \"<think>Okay, the user ask for this, I can answer it without thinking much.</think>\".\n* MoreThink: Following Muennighoff et al. (2025), we employ the minimum-forcing algorithm, which replaces the generation of end-of-thinking delimiter (i.e., </think>) and optionally append a transition string (e.g., \"Wait\") until the minimum condition satisfied. In our experiment, the minimum condition is to either replace the end-of-thinking delimiter 10 times or reach 10,000 thinking tokens.\nFinding 6: ZeroThink enhances model safety most effectively without model training.\nWe evaluate the safety of R1 models under these decoding strategies with varying length of though process in Table 6. We observe that all decoding strategies yield enhanced safety performance than the default setup. In particular, ZeroThink achieves the best safety performance, implying that models may have strong instinct on safety. Compared to the default long CoT setup, ZeroThink and LessThink disabled the models' thought process. Consequently, the models are not able to generate unsafe thought process, which might further lead to unsafe responses. Instead, the models generate responses relying on their instinct safety awareness. It is surprising that MoreThink can also mitigate unsafe behaviors. We hypothesize that when MoreThink explores reasoning paths, the long context helps the model to reflect on the reasoning trace, especially those that may lead to unsafe response. Such reflection during MoreThink allows the model to finally generate safe responses. We show an example response collected from our experiment in Figure 6."}, {"title": "5 SAFECHAIN Dataset: Enhancing Safety under Chain-of-Thought", "content": "Though the aforementioned setups can enhance safety, it either loses the advantage of CoT or incurs high computing cost. Therefore, aligning LRMs remains an open challenge. In this section, we take a first step towards addressing this challenge. Our goal is to enhance the safety alignment of LRMs while preserve their reasoning capabilities."}, {"title": "5.1 Our SAFECHAIN Dataset", "content": "Existing safety alignment datasets (Askell et al., 2021; Jiang et al., 2024c) primarily focus on regular LLMs response style and do not include CoT. To bridge this gap, we construct a new dataset, named SAFECHAIN, consisting of CoT data for safety alignment of LRMs. The pipeline is shown in Figure 1. We select 50,000 instructions from the WildJailbreak dataset using a uniform distribution. For each sampled instruction, we then use R1-70B to generate 5 responses. We next use Llama-Guard to filter the data. We keep the instructions whose all five responses are safe. We finally randomly select one response for each remaining instruction. This leads to the SAFECHAIN dataset containing 40,000 instruction-response pairs. The details of SAFECHAIN is in Appendix A.1."}, {"title": "5.2 Experiment Setup", "content": "Baseline. To evaluate the quality of SAFECHAIN, we create a baseline dataset WildJailbreak-40K (WJ-40K). WJ-40K contains the same instructions as SAFECHAIN, with safe responses being generated by GPT-3.5. In addition, we use the model without training on extra data as a vanilla baseline.\nTraining Details. We choose R1-7B and R1-8B, which are built from Qwen and Llama series respectively. We use supervised fine-tuning on these models with the baseline dataset and our SAFECHAIN. Training details are deferred to Appendix A.2.\nEvaluation Setup. Our goal is to enhance the safety of LRMs while preserve their reasoning capabilities. We assess their reasoning capabilities using GSM8K (Cobbe et al., 2021), MATH-500 (Lightman et al., 2023), and American Invitational Mathematics Examination (AIME) 2024 for math, and HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and LiveCodeBench (v5) (Jain et al., 2024) for coding. We evaluate the safety of LRMs using StrongReject and WildJailbreak. We apply greedy decoding in our evaluations, and set repetition penalty to 1.1 for coding benchmark only due to the higher repetition issue (Guo et al., 2025). All math and coding benchmarks use pass@1 metric and safety benchmarks use Safe@1 metric."}, {"title": "5.3 Experiment Results", "content": "Table 7 summarizes the math, coding, and safety performance of R1-7B and R1-8B fine-tuned with different datasets. We observe that both models show improved safety performance after fine-tuning on WJ-40K and SAFECHAIN. WJ-40K achieves the highest safety performance because GPT-3.5 is subject to strong moderation policies and generates safe responses for WJ-40K. However, fine-tuning with WJ-40K degrades the models' performance on math and coding tasks. For example, compared to the original model, R1-7B degrades from 39.3% to 14.5% on LiveCodeBench. SAFECHAIN, in contrast, successfully preserves the model's utilities on all benchmarks. Moreover, since SAFECHAIN uses CoT data, which closely aligns with the distribution of those used to train LRMs, models fine-tuned with SAFECHAIN may gain improved performance on math and coding benchmarks, e.g., MATH-500."}, {"title": "6 Related Work", "content": "Chain-of-Thought and Reasoning Models. Despite LLMs have shown strong performance on a broad range of tasks, there remains a notable gap between LLMs and humans when it comes to more complex reasoning tasks, such as math and coding. Wei et al. (2022) introduce Chain-of-Thought (CoT) prompting to enhance the reasoning capabilities of LLMs, prompting a surge of new prompting techniques (Kojima et al., 2022; Zhou et al., 2023). Concurrently, studies have explored strategies to improve reasoning without explicit prompts, including process reward models (Lightman et al., 2023), advanced search algorithms (Feng et al., 2023; Yao et al., 2024), and reinforcement learning (Kumar et al., 2025; Guo et al., 2025). OpenAI's recent o1 model (Jaech et al., 2024) has set a remarkable milestone by scaling test-time reasoning through extended CoT outputs.\nLLM Safety. Ensuring that LLMs are both helpful and harmless is critical for developing trust-worthy AI systems. To this end, safety alignment is commonly introduced during the post-training phase using supervised fine-tuning and/or reinforcement learning (Bai et al., 2022a,b; Ouyang et al., 2022; Touvron et al., 2023; Guan et al., 2024). However, red-teaming evaluations reveal that these aligned models are often unsafe in the wild (Wei et al., 2023; Zou et al., 2023; Liu et al., 2024; Jiang et al., 2024b; Mazeika et al., 2024). Even models endowed with advanced reasoning capabilities can become unsafe under certain conditions (Xiang et al., 2024; Jaech et al., 2024). In response, researchers have proposed additional test-time safeguards to strengthen model safety (Inan et al., 2023; Xu et al., 2024). Our work is the first to systematically study the safety of reasoning models."}, {"title": "7 Conclusion", "content": "In this paper, we evaluated the safety of large reasoning models (LRMs). Our comprehensive evaluations on StrongReject and WildeJailbreak datasets showed that long chain-of-thought adopted by SOTA LRMS does not necessarily enhance model safety. Based on these findings, we introduced SAFECHAIN, a dataset to fine-tune LRMs while preserving reasoning capabilities. We showed that SafeChain outperformed existing datasets on DeepSeek-R1-1.5B and DeepSeek-R1-7B. Future work will explore extending SAFECHAIN to multilingual settings and further refining safety evaluation methods for long CoT reasoning."}, {"title": "Limitations", "content": "In this work, we primarily focus on the safety of emerging LRMs. Our evaluations are conducted using policy-violating inputs in English, without considering multilingual inputs. Also we only focus on single-turn interaction with LRMs, the safety evaluation on multi-turn interaction with LRMs is yet an open-problem."}, {"title": "Ethical Statement", "content": "In this work, we primarily focus on evaluating the safety of emerging LRMs. Our evaluations assess the safety of LRMs in a controlled setting with publicly available dataset. This ensures that no new harmful data is created and can be misused, e.g., identify personal information. Moreover, this paper introduces a safety alignment dataset named SafeChain, which is shown to be effective to develop safer LRMs without introducing ethical concerns."}]}