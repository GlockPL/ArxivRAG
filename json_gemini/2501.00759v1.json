{"title": "Enhancing Transformers for Generalizable First-Order Logical Entailment", "authors": ["Tianshi Zheng", "Jiazheng Wang", "Zihao Wang", "Jiaxin Bai", "Hang Yin", "Zheye Deng", "Yangqiu Song", "Jianxin Li"], "abstract": "Transformers, as a fundamental deep learning architecture, have demonstrated remarkable capabilities in reasoning. This paper investigates the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and explores ways to improve it. The first-order reasoning capability of transformers is assessed through their ability to perform first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) the unseen knowledge and query settings discussed in the task of knowledge graph query answering, enabling a characterization of fine-grained generalizability. Results on our comprehensive dataset show that transformers outperform previous methods specifically designed for this task and provide detailed empirical evidence on the impact of input query syntax, token embedding, and transformer architectures on the reasoning capability of transformers. Interestingly, our findings reveal a mismatch between positional encoding and other design choices in transformer architectures employed in prior practices. This discovery motivates us to propose a more sophisticated, logic-aware architecture, TEGA, to enhance the capability for generalizable first-order logical entailment in transformers.", "sections": [{"title": "Introduction", "content": "As a fundamental architecture in deep learning, transformers possess strong reasoning capabilities on various tasks, including arithmetic reasoning (Saxton et al., 2019; Hendrycks et al., 2021), symbolic reasoning for first-order logic rules (Dehghani et al., 2019; Lample and Charton, 2019), set-theoretic operations (Barrett et al., 2018), and theorem proving (Polu and Sutskever, 2020). Besides, transformers have also demonstrated proficiency in logical reasoning over natural language (Han et al., 2022; Tian et al., 2021). To distinguish whether transformers conduct the reasoning rather than fitting the data distribution, recent studies in natural language reasoning further measure the capabilities of transformers for out-of-demonstration samples (Saparov et al., 2023). However, their discussions are limited in two aspects: (1) they only concern the reasoning ability of transformers with in-context knowledge, and (2) they fail to elicit the connection of out-of-demonstration samples with two distribution shifts (Moreno-Torres et al., 2012) for the study of out-of-distribution generalization.\nIn this paper, we take a step further to understand the generalizable reasoning capability of transformers. What sets us apart from (Saparov et al., 2023) is that (1) our investigation covers the transformer reasoning with parameterized knowledge, which suits many scenarios when the related knowledge is not explicit for users and only questions are given, and (2) we further realize two distribution shifts (Moreno-Torres et al., 2012) in our first order reasoning tasks, which boils down to the process of first-order logical entailment, where we verify whether a first-order sentence (conclusion) is the logical consequence of a set of known first-order sentences (premises) or not (Marker, 2002).\nSpecifically, we study the first-order logical entailment with Knowledge Graphs (KG), leading to the widely discussed task of knowledge graph query answering. In our setting, knowledge in KGs is parameterized in models and forms implicit premises. (\u00a72) The process of logical entailment occurs when identifying the answer to a logical query. Therefore, the two distribution shifts - concept shift and covariant shift are naturally credited to the unobserved knowledge and the unseen query types in our experimental settings. (\u00a73) For better evaluation, we build our own benchmark with fifty-five types of logical queries"}, {"title": "Preliminaries", "content": "This section briefly introduces First-Order (FO) logical entailment and its restricted application in knowledge graph query answering. By revealing the connection, we show that the key ability to address complex queries is the first-order logical entailment. For simplicity, our introduction only restricts to a finite entity set & and a finite binary relation set R. No functions are mentioned. Detailed presentation of first-order logic can be found in model theory literature (Marker, 2002)."}, {"title": "First-order Logical Entailment", "content": "Definition 1 (First-order sentence). The set of first-order formulae is the minimal set F such that\n1. p(s,o) \u2208 F, where p is the relation, s, o can be either an entity in E or a variable.\n2. If s \u2208 F, then \u00acs \u2208 F; If s,t \u2208 F, then s \u2227 t \u2208 F and s \u2228 t \u2208 F.\n3. If s(x) \u2208 F and x is a variable that is not quantified, then \u2200x.s(x) \u2208 F and \u2203x.s(x) \u2208 F.\nWe say a variable x in a formula is quantified if there is a quantifier (\u2203x or \u2200x). Otherwise, we say a variable is free. A sentence is a formula without free variables.\nNext, we introduce first-order logical entailment, the central concept of first-order reasoning. In general, the entailment is a process of verifying the conlusion (one sentence to verify) given the knowledge (a set of given premises). Notably, knowledge, conclusion and verification are the three key components of the definition. For first-order logical entailment, the knowledge and conclusion are restricted as first-order sentences, and the verification process is subject to the first-order logical calculus. A set of FO sentences {pi, i = 1, ..., n} entails a FO conclusion s is denoted as:\n{pi, i = 1, ..., n} |= S  (1)\nAfter verification, its truth value suggests whether s is the logical consequence of the premises."}, {"title": "Knowledge Graph Query Answering", "content": "Knowledge graph query answering is an important application of FO logical entailment. Its importance comes from both the importance of knowledge graphs and the queries in database systems. Here, we connect the task of KG query answering and FO entailment from the aspects of knowledge, conclusion, and verification.\nKnowledge. Given an entity set & and relation set R, a knowledge graph is defined as a set of triplets G = {(hi,ri,ti)}, where hi, ti \u2208 E are entities and ri \u2208 R are relations. Each triple (h, r, t) in KG can be regarded as the simplest form of first-order sentence r(h, t). Thus, the knowledge graph G provided the premises.\nAnswers to the query as multiple conclusions to verify. Existing research for KG query answering usually focuses on the restricted families of FO queries, specifically, the Existential First-Order (EFO) queries. An EFO query is represented in Disjunctive Normal Form (DNF) as a disjunction of one or multiple conjunctions:\nq[V?] = V?.\u2203V1, V2, ..., Vn : C1 \u2228 C2 \u2228 ... \u2228 Cm (2)\nIn equation 2, V? is the free variable, V1, V2, ..., Vn are existential variables, while C1, C2, ..., Cm are"}, {"title": "Distribution Shifts in KG Query Answering", "content": "This section introduces the formulation of distribution shifts in KG query answering tasks. Such tasks are connected to first-order reasoning due to their close connection with logical entailment. By clearly presenting their connection with distribution shifts studied in statistical ML community (Moreno-Torres et al., 2012), they are then able to measure the generalizable first-order reasoning capability of deep learning models, in particular transformers, with parameterized knowledge.\nProblem formulation. A dataset for KG query answering is denoted as a set of samples {(xi, Yi)} drawn from the joint distribution P(X, Y). Here, X represents the random variable corresponding to an EFO query, with xi as individual samples, and Y represents the random variable for the answer set, with yi as individual samples.\nThis paper studies transformers, where the input X is tokenized and fed into the transformer architecture, and the output Y is predicted with a classifier, where all entities in the KG are ranked by embedding similarities. In both the training and testing phases, the knowledge graph G is not explicitly accessed by the model. In this way, the necessary condition for transformers to correctly answer the queries is that the knowledge in KG is materialized into their parameters. To examine whether transformers possess the generalizable first-order reasoning capability, we consider two classic types of out-of-distribution shifts: concept shift and covariant shift. They are connected to unobserved knowledge and unseen query types."}, {"title": "Two Types of Distribution Shifts", "content": "Out-Of-Distribution (OOD) generalization is one of the key topics in machine learning research (Vapnik, 1991; Quionero-Candela et al., 2009). It considers the shifts in the joint distribution P(X, Y) from the training to the testing phases. Equation 5 decomposed the distribution shift by applying conditional probability formula, introducing two basic types of OOD shifts: concept shift refers to the changes in P(Y|X), while covariate shift refers to the changes in P(X).\nPtrain (Y|X)Ptrain (X) \u2260 Ptest (Y|X) Ptest (X) (5)\nAs illustrated in Fig. 1, we studied distribution shifts in both knowledge and query type dimensions."}, {"title": "Concept Shift by Unobserved Knowledge", "content": "In our KG query answering setting, we construct an observed knowledge graph Go as a subset of the full knowledge graph G. In the training stage, the models only access the answers from the observed graph, denoted as A(Go, q). While in the testing stage, they are evaluated on answers from the full graph, denoted as A(G, q). This setup introduces a concept shift in the conditional distribution P(Y|X) between training and testing because the change of knowledge causes different results of entailment, see Equation (3). Therefore, the set difference Aood = A(G, q) \u2013 A(Go, q) contains the entities that can only be derived when the model generalizes from knowledge Go to G. By measuring the performances of models on Aood, we are able to access how a model generalizes under the concept shift caused by unobserved knowledge. This is also termed knowledge generalization or knowledge inference in the literature (Sun et al., 2021)."}, {"title": "Evaluation Benchmark", "content": "To better investigate the performance in generalizable FO reasoning, we build a new benchmark of KG query answering for our experiment. We first introduce the dataset construction process and evaluation settings, and then present our results of transformers in comparison to previous methods."}, {"title": "Dataset Construction", "content": "Table 1 compares the feature and statistics of query types selected in our benchmark with previous benchmarks in KG query answering, namely GQE"}, {"title": "Evaluation Settings", "content": "In accordance with the two types of distribution shifts discussed above, we evaluate the performance of query answering models across the two corresponding dimensions using the mean reciprocal rank (MRR) metric. In the knowledge dimension, we employ the notation ID (K) and OOD (K) to represent the performance on the answer set"}, {"title": "General Benchmarking Results", "content": "We evaluate transformers against five existing methods for KG query answering, with details provided in Appendix B. Meanwhile, we compare three positional encoding settings in transformers: Absolute PE is the default sinusoidal positional encoding introduced in the original transformer (Vaswani et al., 2023). Relative PE (Shaw et al., 2018) applied a learnable matrix based on the relative position of tokens. Disentangled PE (He et al., 2021) uses separate vectors to encode the content and position of each token, decoupling the contextual and positional information during self-attention."}, {"title": "Transformer Architectures and First-order Reasoning Capability", "content": "In this section, we further investigate the dependencies between transformers' performances and multiple design choices throughout the modeling process. Figure 2 depicts the three-stage pipeline of deriving the answer set A (output Y) of query q (input X) using transformers. The impact of query syntax, token embeddings, and transformer architecture are presented in Section 5.1, Section 5.2, and Section 5.3, respectively, and summarized in Section 5.4."}, {"title": "Study on Query Syntax", "content": "Logical queries with the same semantics can be represented in different syntaxes with different formal languages. To investigate the impact of difference in formal languages, we conducted experiments on two formal languages: Lisp-like Syntax (Wang et al., 2021) represents query with fully parenthesized nested formula; EFO Syntax (Yin et al., 2023b) represents query with all one-hop atomics connected with conjunctions and disjunctions in parallel. As the Lisp-like syntax has a limited representation scope, we used a subset of our benchmark that includes 13 seen query types and 12 unseen query types that are supported by both languages. The experimental results on the impact of formal language are presented in Table 3. In transformers with absolute PE, there is no difference between the results of the two formal languages. However, with relative PE, the EFO syntax achieves much better generalizability in the query type domain. This phenomenon can be explained by the differences in the structural features of the two languages: with a parallel structure, the distances (i.e., relative positions) between tokens with the same logical relationship are more consistent than those in a nested structure. As a result, relative PE can better learn the logical relationship between tokens and better generalize to unseen query types.\nFurthermore, even for the same query in EFO"}, {"title": "Study on Token Embeddings", "content": "Some methods in KG query answering (Arakelyan et al., 2021; Wang et al., 2023b; Xu et al., 2023) utilized pre-trained KG embeddings from link predictors in their query encoding process, while other methods (Ren and Leskovec, 2020; Zhang et al., 2021; Bai et al., 2023b) randomly initialize the embedding matrix and jointly learn the KG embedding and model parameters from the training process. It's still unclear that to what extent can pre-trained KG semantics can assist the performance of query answering models. To rigorously assess"}, {"title": "Study on Transformer Architecture", "content": "Some existing literature (Kotnis et al., 2021; Liu et al., 2022; Xu et al., 2023) has explored the use and design of transformers in KG query answering, in which several inductive biases in transformer architecture were proposed. However, as all these methods focused on different aspects and were evaluated on different benchmarks, a clearer investigation is needed. We implemented two designs from previous works on top of transformers: Adjacency Matrices Masking: an inductive bias proposed in kgTransformer (Liu et al., 2022), which aims to enhance reasoning performance by limiting the range of self-attention to one-hop neighboring nodes in the query graph. Directed Distance Encoding: another inductive bias, introduced in Query2Triple (Xu et al., 2023), that facilitates information aggregation by injecting a directed distance message into the self-attention mechanism. The experimental results on existing transformer designs are presented in Table 6. Both inductive biases effectively improve generalizability in the query type dimension when using absolute positional encoding. In contrast, under relative positional encoding, these improvements are not observed, and the use of adjacency matrices masking even results in a sig-"}, {"title": "Summary of Empirical Findings", "content": "In terms of query syntax, we discovered that employing formal language with parallel structure can significantly enhance OOD generalizability in the query type dimension for transformer with relative PE. Moreover, we observed that transformers with relative PE can robustly handle queries with different permutations. For token embeddings, frozen pre-trained KGEs from stronger link predictors (e.g. ComplEx) outperform learned embeddings, whereas weaker ones (e.g. TransE) do not. Regarding transformer architecture, we found that the inductive biases proposed in previous approaches are only effective under absolute PE, but fail to improve upon transformer with relative PE, which we have shown to be the preferred setting. This mismatch between transformer design and positional encoding settings motivated us to design a methodology that effectively boosts the performance and generalizability of transformers with relative PE in first-order logical entailment."}, {"title": "Transformer Encoder with Guided Attention", "content": "One of the main challenges in KG query answering is to perform multi-hop logical reasoning following the one-hop atomics and the logical operations"}, {"title": "Logic-aware Relative Positional Encoding", "content": "Our analysis so far shows that RPE is the best positional encoding for transformers in our task for its superior performance and generalizability in both knowledge and query type dimensions. However, it still has limited expressiveness in the logical relationship between tokens, as the tokens with the same relative distance can have different logical relationships among queries. To fill this gap, we propose LogiRPE, a logic-aware mechanism to enhance self-attention. Specifically, for a sequence input sequence embeddings xi \u2208 Rd, i = 1, ..., n, the enhanced self-attention is computed by\nzi = \u2211 aij (xjWV + \u03b2), (6)\nlij = exp eil , (7)\nwhere WQ, WK, WV \u2208 Rd\u00d7d are the weight matrices for query, key, and value while BY, BE Rd are two logical bias terms introduced by LogiRPE to capture the missing logical relations.\nThe key feature of logical bias terms is that they should be able to discriminate different types of tokens. Let the type of the i-th token be ti from a finite type set of size t. In our study, we consider six types of tokens: parenthesis[(,)],entity[s,e,f], relation[r], conjunction[^], disjunction[\u2228], and negation[!]. Then the bias vector \u03b2, (l is either K or V) is indexed from an trainable embedding bank\nBij = w\u00b2 [ti, tj, i - j]], l\u2208 {K, V}. (8)\nLogiRPE may also be applicable to broader tasks where tokens with logical semantics can be labeled."}, {"title": "Free-variable Pooling", "content": "Most existing transformer-based approaches (Bai et al., 2023b; Xu et al., 2023) utilize the last layer hidden states of the first token(s) as the final query encoding, akin to the use of the [CLS] token in the original BERT paper (Devlin et al., 2019). However, we contend that the reasoning capabilities inherent in self-attention mechanisms should effectively aggregate the query information into free variables that semantically represent the answer sets of queries. Consequently, we adopt Free-Variable Pooling, wherein the final query encoding is derived through max pooling across the last-layer hidden states of these free variables."}, {"title": "Result and Ablation", "content": "The experimental results of TEGA and Transformer baselines are presented in Table 7. In all three datasets, TEGA substantially improved the performance and generalizability of knowledge and query type dimensions over baselines. According to our ablation study results in Table 8, both inductive biases proposed in TEGA are effective on their own. At the same time, a stronger improvement in entailment performance can be achieved when applied together. Moreover, we provide an example of attention visualization in Appendix D, demonstrat-"}, {"title": "Related Work", "content": "Transformers in Logical Reasoning Transformers have exhibited remarkable performance in various forms of logical reasoning. In natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018), transformers analyze the logical relationship between a premise and a hypothesis with deductive reasoning over statements. Furthermore, transformers have been employed for inductive reasoning within rule-based systems (Clark et al., 2020). Additionally, transformers have been applied to perform abductive reasoning in both natural language (Bhagavatula et al., 2020) and formal language (Bai et al., 2024) settings. Meanwhile, various techniques have been proposed to improve the logical reasoning capability of transformers for language understanding (Chen, 2023; Pan et al., 2023; Yuan et al., 2020). Though many inductive biases and empirical findings from natural language reasoning scenarios are not transferable to our setting due to the different nature of the task, some design choices may result in similar effects. For instance, the application of relative PE can improve the generalizability in both query answering and other natural language tasks (Ontanon et al., 2022).\nKGQA with Parameterized Knowledge Prior to this work, extensive research had been done on KG query answering with parameterized knowledge. Regarding modeling approaches, iterative neural query encoders (Ren and Leskovec, 2020; Zhang et al., 2021; Chen et al., 2022) design representations for entity sets and execute logical operators iteratively, following the computational graph. In addition, neural-symbolic methods (Arakelyan et al., 2021; Zhu et al., 2022; Bai et al., 2023c; Yin et al., 2023b) incorporate link predictors and search over the symbolic space. In terms of the knowledge domain, existing benchmarks (Ren et al., 2020; Ren and Leskovec, 2020; Yin et al., 2023a) primarily utilize knowledge graphs containing general factual knowledge, while some studies have focused on commonsense knowledge (Fang et al., 2024) and eventuality knowledge (Bai et al., 2023a). Research has also shown that such queries can be extended to natural language settings with template or LLM-based approaches (Zheng et al., 2024a,b; Zong et al., 2024), enabling broader applications."}, {"title": "Conclusion", "content": "In this paper, we studied the first-order logical reasoning capability of transformers with parameterized knowledge under an out-of-distribution generalization setting. We established connections between distribution shifts and logical entailment and comprehensively evaluated transformers on a new knowledge graph query answering benchmark. Detailed results provided empirical evidence of optimal settings throughout the entire pipeline, highlighting the importance of formal language and relative positional encoding. Our results also further motivate the LogiRPE, a logic-aware and more effective self-attention component for first-order reasoning. All our findings give rise to the TEGA, a novel architecture that substantially improves performance and generalizability."}, {"title": "Limitations", "content": "Our study focused on the transformer encoder architecture, which is intuitively more suitable for our task as it generates outputs without the dependency on and interference from the auto-regressive mask found in transformer decoders. Therefore, we did not compare decoder-only and encoder-decoder architectures in our experiments. Additionally, we limited the scope of logical queries to EFO queries with a single free variable, leaving queries with multiple free variables for future exploration."}, {"title": "Ethics Statement", "content": "The experiments were conducted on publicly available knowledge graphs, eliminating any data privacy concerns. However, it should be noticed that most approaches for generalizable logical entailment are susceptible to adversarial attacks (Dai et al., 2018; Z\u00fcgner et al., 2019) and data poisoning (You et al., 2023) on knowledge graphs, which may result in unintended outcomes in applications."}, {"title": "Dataset Statistics", "content": "In this section, we introduce the statistics and query types of our benchmark dataset. Details of the query statistics for our dataset are presented in Table 9. For each knowledge graph, we sampled all one-hop projection (1p) queries from the training graph and sampled twice as many for the other 22 in-distribution query types. For validation and testing queries, we set the quantities to 8000, 5000, and 4000, respectively, following the convention established in KG literature (Ren et al., 2020; Ren and Leskovec, 2020; Bai et al., 2023b)."}, {"title": "Query Reversion", "content": "The logical formula of five query types before and after reversion is provided in Table 13."}, {"title": "Attention Visualization", "content": "Fig. 3 provides an example of how attention guidance in TEGA can affect the reasoning process in self-attention. In 2in queries [(r1(s1,f))&(!(r2(s2,f)))], the negation operator [!] is applied to calculate the complement of the entity set represented by the second atomic formula [(r2(s2, f))]. Benefited from the attention guidance from inductive biases, TEGA can effectively aggregate the information from the second atomic formula [r2/s2/f] to the negation operator with self-attention. While in baseline (Transformer + Relative PE), the information flow between tokens are scattered and less effective (highlighted in red). As a result, TEGA outperforms Transformer + Relative PE by 21-26% in 2in queries over three knowledge graphs."}, {"title": "Knowledge Graph Statistics", "content": "Detailed statistics of the knowledge graphs selected are presented in Table 14."}, {"title": "Query Graph Definition", "content": "We provide query graph for each query types in our dataset. Query graphs can be utilized for graph-augmented methods, and have been applied in the two inductive biases discussed in Sec. 5.3. Here we provide the definition:\nFor each atomic formula or its negation \u03b1 = r(h,t) or \u00acr(h,t) in a conjunctive formula c, we have {(h,r), (r,t)} \u2208 Geor {(h, r), (r, n), (n,t)} \u2208 Gc, where n denotes the negation node in the conjunctive query graph Ge. By our definition of query, all conjunctive query graphs have exactly one node as a free variable. For queries that represent the disjunction of multiple conjunctive formulas, we replace all free variable nodes in every conjunctive query graph with a single union node u, and connect it to a final free variable node f, with {(u, f)} \u2208 Gd, where Gd is the disjunctive query graph."}, {"title": "Evaluation Details", "content": "For each knowledge graphs, we separate their edges into training, validation, and testing edges with a ratio of approximately 8:1:1, as shown in Table 14. We construct three graphs, training graph Gtrain, validation graph Gvalid, and testing graph Gtest with training edges, training+validation edges, and training+validation+testing edges, respectively.\nWe adopt the mean reciprocal rank (MRR) as our evaluation metric, following the calculation presented in Equation 9 and 10. It is important to note that for all testing queries, the answers in A(Gvalid, q) - A(Gtrain, q) are excluded from Aood to ensure fairness. Consequently, we define Aid = A(Gtrain, q) and Aood = A(Gtest, q) \u2013 A(Gvalid, q) for our evaluation of ID (K) and OOD (K), respectively. In each table regarding experimental results, the scores below ID (Q) (i.e., either MRR of ID (K) or OOD (K)) represent the average scores among all seen query types. While the scores below OOD"}]}