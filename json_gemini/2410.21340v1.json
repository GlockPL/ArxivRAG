{"title": "META-LEARNING FOR SPEEDING UP LARGE MODEL\nINFERENCE IN DECENTRALIZED ENVIRONMENTS", "authors": ["Yuzhe Yang", "Yipeng Du", "Ahmad Farhan", "Claudio Angione", "Yue Zhao", "Harry Yang", "Fielding Johnston", "James Buban", "Patrick Colangelo"], "abstract": "The deployment of large-scale models, such as large language models (LLMs)\nand sophisticated image generation systems, incurs substantial costs due to their\ncomputational demands. To mitigate these costs and address challenges related to\nscalability and data security, there is a growing shift towards decentralized systems\nfor deploying such models. In these decentralized environments, efficient infer-\nence acceleration becomes crucial to manage computational resources effectively\nand enhance system responsiveness. In this work, we address the challenge of\nselecting optimal acceleration methods in decentralized systems by introducing a\nmeta-learning-based framework. This framework automates the selection process\nby learning from historical performance data of various acceleration techniques\nacross different tasks. Unlike traditional methods that rely on random selection\nor expert intuition, our approach systematically identifies the best acceleration\nstrategies based on the specific characteristics of each task. We demonstrate that\nour meta-learning framework not only streamlines the decision-making process\nbut also consistently outperforms conventional methods in terms of efficiency and\nperformance. Our results highlight the potential of meta-learning to revolutionize\ninference acceleration in decentralized AI systems, offering a path towards more\ndemocratic and economically feasible artificial intelligence solutions.", "sections": [{"title": "INTRODUCTION", "content": "The advancement of large-scale models such as large language models (LLMs) and sophisticated\nimage generation systems has dramatically increased computational demands, necessitating signifi-\ncant innovation in deployment architectures (Brown et al., 2020; Ramesh et al., 2022). Traditional\ncentralized systems, while powerful, encounter critical limitations in terms of scalability, data secu-\nrity, and operational costs (Li et al., 2022). These limitations have spurred interest in decentralized\narchitectures that distribute computational tasks across multiple nodes to enhance efficiency, reduce\nlatency, and improve data privacy (Belotti et al., 2019).\nDecentralized systems, by their nature, facilitate a more robust approach to managing data and com-\nputational resources, offering an attractive solution for deploying computationally intensive models\n(Xu et al., 2019). Such architectures are crucial for supporting real-time processing and high avail-\nability across varied geographical locations without compromising on performance and security\n(Wang et al., 2019)."}, {"title": "NESA'S SYSTEM OVERVIEW", "content": "To enable AI democratization, Nesa proposed a model-agnostic hybrid sharding approach named\nBSNS, layered onto a hardware-based and software-based privacy co-optimization protocol Angione et al. (2024). The idea is to distribute the computational load of an inference request across"}, {"title": null, "content": "S:(Ai)i\u2208{1,...,p}"}, {"title": null, "content": "where S represents the sequence of consumers, with each A\u00bf denoting the i-th shard managed by the\ni-th consumer (Fig. 1a). This setup reduces latency and bandwidth needed for real-time inference.\nImportantly, our routing mechanism is dynamically updated based on each consumer's features,\nincluding hardware-based and reputation-based metrics."}, {"title": "Challenges of Fast Inference in Decentralized Systems", "content": "However, managing inference accel-\neration within these decentralized systems presents a unique set of challenges, primarily due to\nthe diverse and often constrained computational resources available at different nodes (Deng et al.,\n2020). To address these challenges effectively, there is a pressing need for adaptive strategies that\ncan dynamically select the most appropriate inference acceleration methods based on the specific\ncharacteristics of the task and the underlying system constraints (He et al., 2020)."}, {"title": "Our Solution", "content": "In response, we extend the BSNS architecture by introducing a novel meta-learning\nframework, MetaInf, designed to optimize the selection of inference acceleration methods within\ndecentralized systems. This framework leverages historical performance data to train a meta-learner\ncapable of predicting the most effective acceleration strategies under various operational scenar-\nios, thereby enhancing the computational efficiency of large model deployments and making high-\nperformance AI more accessible and cost-effective (Chen et al., 2019). MetaInf streamlines the\ninference process and shows substantial improvements over traditional methods that rely on either\nrandom selection or solely expert-driven strategies (Wang et al., 2021)."}, {"title": "Contributions", "content": "Our work makes several contributions to the field of AI model deployment:\n1. We present the first comprehensive framework for fast inference that effectively addresses the\nunique challenges of large model deployments in decentralized systems.\n2. We introduce a novel meta-learning-based approach for selecting inference acceleration methods,\noptimizing computational resources across heterogeneous environments.\n3. We demonstrate the effectiveness of our framework through extensive experiments, showing sig-\nnificant improvements in efficiency and cost-effectiveness over traditional inference acceleration\nmethods.\nThe following sections will first discuss the fast inference approaches in \u00a72, the challenge of extend-\ning to decentralized systems (\u00a73), and our meta-learning-based selection framework (\u00a74)"}, {"title": "A CLOSER LOOK AT FAST INFERENCE", "content": "Fast inference has become a crucial factor in the deployment of large models and complex AI sys-\ntems (Brown et al., 2020). In many real-world scenarios, models need to process user queries and\nproduce results in a fraction of a second (Rajbhandari et al., 2020). This requirement poses chal-\nlenges in both centralized and decentralized environments, particularly as model sizes and the de-\nmand for real-time processing grow. In this section, we first analyze state-of-the-art fast inference\nmethods in centralized systems, benchmarking their performance across a variety of tasks, models,\nand hardware settings."}, {"title": "LANGUAGE MODELS", "content": "Autoregressive (large) language models have seen significant advancements in fast inference (Kwon\net al., 2023; Leviathan et al., 2023). Many approaches are aimed at reducing latency and maxi-\nmizing throughput without compromising model accuracy. We identify and summarize several key\ntechniques, evaluating their efficacy and trade-offs.\nLarge Batch Size. One of the most straightforward methods for optimizing inference is batching,\nwhere multiple requests are grouped into a single batch and processed simultaneously. GPUs, as\nhighly parallel processors, operate more efficiently when handling large batches, which leads to a\nsignificant increase in throughput. The main advantage of large batch sizes is the improved GPU\nutilization, especially when multiple inference requests are made at once. This method has been"}, {"title": "TEXT-TO-IMAGE MODELS", "content": "In addition to autoregressive models such as LLMs, serving text-to-image (T2I) generation mod-\nels (e.g., diffusion models like Stable Diffusion (Rombach et al., 2022)) presents its own unique\nchallenges for optimizing inference speed. Improving latency and generation speed is essential\nto enable real-time interaction, fast response, and seamless user experiences (Dhariwal & Nichol,\n2021). Particularly in resource-constrained or real-time environments, it becomes critical to identify\nand implement key techniques for accelerating T2I model inference.\nDistilled Models. Neural network distillation offers a more efficient alternative to full-sized mod-\nels by reducing the number of residual and attention blocks (Hinton et al., 2015). This distillation\nprocess results in a smaller, faster model that maintains image quality while significantly reducing\nmemory usage and inference time. For example, a distilled version of Stable Diffusion can achieve\nup to a ~1.5X speedup, and when paired with a distilled autoencoder, this performance boost can\nreach up to ~1.6X compared to baseline models (Meng et al., 2023; Ho et al., 2020).\nReduced Precision. Utilizing reduced numerical precision formats, such as bfloat16, can\nsignificantly improve inference latency without impacting the overall quality of image generation\n(Ansel et al., 2024). Modern GPUs feature specialized cores that are optimized for lower-precision\ncomputations, allowing them to process operations faster than with higher precision formats like\nfloat32 or even float16. The robustness of bfloat 16 compared to float16 makes it par-\nticularly suitable for scenarios where quantization is applied, as it helps maintain model accuracy\ndespite the reduced precision (Gupta et al., 2015).\nScaled Dot Product Attention. Scaled Dot Product Attention (SDPA) optimizes the computa-\ntionally intensive attention-related operations that are central to transformer-based models like the\nUNet and Variational Autoencoder (VAE) components used in diffusion models (Ansel et al., 2024).\nBy implementing SDPA, PyTorch 2.0 introduces more efficient kernels that reduce the overhead of\nattention calculations, speeding up inference while maintaining accuracy (Ansel et al., 2024). This\noptimization addresses a common bottleneck in high-dimensional image generation tasks, particu-\nlarly when processing complex attention maps.\nCombined Projections for Attention Operations. In transformer-based architectures, including\nthose used in T2I models, the self-attention mechanism typically projects the input into three sep-\narate subspaces (queries, keys, and values) using distinct matrices. By combining these three pro-\njections into a single matrix and performing the operation simultaneously, the model can benefit\nfrom more efficient matrix multiplications (Child et al., 2019). This larger matrix multiplication al-\nlows for better utilization of GPU hardware, particularly in scenarios that benefit from quantization,\nenhancing both speed and memory efficiency.\nDynamic Quantization. Dynamic INT8 quantization reduces the precision of specific layers in\nthe UNet and VAE models, leading to faster inference, particularly for larger matrix multiplications.\nAlthough quantization can introduce overhead due to data type conversions, selectively applying\nit to layers where it delivers the most benefit, such as pointwise convolution layers converted into\nlinear layers, can optimize the overall performance without a significant loss in image quality (Nagel\net al., 2021; Han et al., 2016)."}, {"title": "IMPLEMENTATION", "content": "We implement and integrate the aforementioned methods into your system, and carry out a compre-\nhensive study to understand the performance of different fast inference methods in heterogeneous\nsetup (Figure 2).\nExperimental Setup. We conducted our experiments using the ShareGPT dataset (Kwon et al.,\n2023) for throughput testing. NVIDIA L4 GPUs were used for testing. Tensor parallelism was\nvaried across different number of GPU configurations. The models tested include Meta-Llama-3.1-8B-Instruct, Microsoft Phi-2, and Mistral-7B-v0.1, with all outputs limited to 10 tokens. We"}, {"title": "FAST INFERENCE IN HETEROGENEOUS SYSTEMS", "content": "Building on the state-of-the-art techniques evaluated for improving inference speed in centralized\narchitectures, we extend our analysis to explore how these methods can be adapted to decentralized\nsystems. In distributed settings, achieving both scalability and efficiency presents unique challenges\ndue to heterogeneous hardware configurations and variable system conditions.\nConceptualization. Effective AI deployment in decentralized systems requires careful considera-\ntion of heterogeneous hardware resources, network latency, and dynamic load balancing (Borzunov\net al., 2024). Unlike in centralized architectures, where inference tasks are managed by a single\nor a small number of powerful machines, decentralized settings must accommodate various hard-\nware setups. This includes devices with differing computational power, memory constraints, and\ncommunication overheads. The challenge of distributing workloads across this diverse infrastruc-\nture is non-trivial and requires dynamic scheduling based on the available hardware, current system\nthroughput, and overall network status.\nObservations from the Motivating Example. As illustrated in Figure 2, our experiments eval-\nuated the effectiveness of applying fast inference techniques, including continuous batching and\nprefix caching, under varying conditions. In environments with consistent hardware capabilities,\nthese methods demonstrated significant improvements in inference speed. However, when varying\nthe batch sizes, the performance gains became less predictable. Specifically, for smaller batch sizes,\ncombining all techniques (\u201cAll\u201d) yielded the best results. In contrast, with larger batch sizes, the\nperformance benefits diminished, and single-method strategies were more effective. Although this\nis a simplified setup, it highlights a key observation: in heterogeneous systems, performance varies\nbased on the distribution strategy and the real-time adaptation of workload scheduling."}, {"title": "META-LEARNING FOR INFERENCE ACCELERATION OPTIMIZATION", "content": "Interpretation. Our results highlight the importance of flexibility in distributed AI systems and\nthe need for adaptive frameworks to handle the inherent variability of decentralized environments.\nInspired by these observations, we propose a learnable meta-scheduler, which dynamically predicts\nthe optimal methods and hardware configurations to utilize, based on the input data and current\nsystem status. The details of this adaptive meta-scheduler are discussed further in Section 4, where\nwe outline its architecture and implementation for fast inference across distributed systems."}, {"title": "PROBLEM STATEMENT AND FRAMEWORK OVERVIEW", "content": "Given a new model, dataset, and hardware environment for distributed inference, the goal is to\nselect the best acceleration method from a heterogeneous set of techniques such as quantization,\nmodel compression, and parallelism, without requiring extensive empirical evaluations. This selec-\ntion process must account for different hardware platforms' unique computational constraints and\ncapabilities.\nIn this work, we leverage meta-learning to transfer acceleration performance information from prior\nexperiences to the new inference task. Meta-learning, often referred to as \"learning to learn,\" is\na technique where an algorithm learns from a collection of historical or meta tasks and uses this\nexperience to perform well on new, unseen tasks. The rationale is that an acceleration method that\nperformed well in a similar historical context is likely to outperform other methods on a new dataset,\nmodel configuration, and hardware setup. This approach is particularly useful when immediate\nevaluation is infeasible or costly due to computational constraints or the need for rapid deployment.\nThe proposed meta-learner, MetaInf, is designed to optimize inference acceleration across diverse\ncomputing environments by learning from historical data. It incorporates several key components:\n\u2022 A detailed database of historical inference tasks, Dtrain = {D1,..., Dn}, provides a broad learn-\ning base from which MetaInf can draw insights. Each task in this database is defined by a unique\ninput to the machine learning models, e.g., prompts for LLMs."}, {"title": null, "content": "f: Edata, Emodel, Ehardware \u2192 Pi,j,k, i\u2208 {1,...,n}, j\u2208 \u2208 {1,...,m}, k\u2208 \u2208 {1,...,h} (2)\nThe embeddings for the datasets Dtrain, methods M, and hardware configurations H are generated\nto capture the essential characteristics that influence the efficiency and efficacy of the acceleration\nmethods under various computational and environmental constraints. These embeddings, referred\nto as Edata, Emodel, and Ehardware respectively, are derived from a comprehensive analysis of each\ndataset's, method's, and hardware's features.\nTo train the meta-learner that predicts the best-performing acceleration method for new, unseen\ntasks, we employ a regression-based meta-predictor. Inputs to this predictor are the embeddings of\nthe datasets, models, and hardware, which encapsulate critical performance-influencing factors:\nDataset Embedding (Edata): Generated by encoding characteristics such as data volume, complex-\nity, distribution, and typical processing requirements crucial for selecting an appropriate acceleration\nmethod.\nMethod Embedding (Emodel): Created by encoding features such as the method's impact on com-\nputational overhead, energy efficiency, and potential accuracy trade-offs, including method-specific\nparameters like degree of quantization, levels of compression, or the architecture required for paral-\nlel execution.\nHardware Embedding (Ehardware): Encodes the specific hardware capabilities and limitations, such\nas processor type, memory availability, and power consumption, which can significantly affect the\nperformance of acceleration methods.\nThe meta-predictor, denoted as f, is trained using a dataset of historical performance metrics. This\ndataset comprises various combinations of dataset characteristics, method efficacies, and hardware\nsettings, represented by the embeddings. We employ a model such as XGBoost (Chen & Guestrin,\n2016) due to its effectiveness in handling diverse and high-dimensional data, as well as its capabil-\nity for feature importance evaluation, which is vital for understanding which characteristics most\nsignificantly impact performance.\nObjective: The primary goal during this training phase is to establish a reliable mapping from\n(Edata, Emodel, Ehardware) to Pi,j,k, where Pi,j,k might include metrics like runtime efficiency, energy"}, {"title": "DATA, MODEL, AND HARDWARE EMBEDDINGS", "content": "Data, model, and hardware embeddings serve as inputs to the meta-learner f, offering a compact,\nstandardized representation of each component's characteristics within the meta-learning process.\nRather than using raw, high-dimensional data directly, these embeddings aim to capture the essential\nproperties that influence the performance of various acceleration methods under different scenarios.\nData Embedding (Edata): We generate embeddings by analyzing the datasets' intrinsic properties\nsuch as size, diversity, feature distribution, and complexity. These embeddings help in adapting the\nacceleration methods effectively to the nature of the data. For generic datasets, this might include\nstatistical metrics like mean, variance, skewness, and kurtosis of the features.\nModel Embedding (Emodel): For the models, which range from large language models to complex\nimage generation networks, embeddings are crafted to reflect characteristics such as model archi-\ntecture, parameter count, and computational requirements. This is achieved through methods like\none-hot encoding of model types or more nuanced embeddings that capture model behavior, such as\nthose generated from model summaries or performance profiles.\nHardware Embedding (Ehardware): Given the diversity in hardware configurations, from high-end\nGPUs like NVIDIA's A100 to setups with no GPU support, it is crucial to encode the hardware's\ncomputational capabilities, memory limits, and energy consumption profiles. These embeddings are\ncrucial for optimizing the selection of acceleration methods that are compatible with the available\nhardware and capable of delivering enhanced performance.\nEmbedding Generation Techniques:\n\u2022 Classical Meta-Features: Traditional meta-features include basic statistical descriptions of\ndatasets and simplistic encodings of model architectures. These are useful for establishing base-\nline comparisons and understanding the broad impact of different dataset and model configurations\non performance.\n\u2022 Language Model-Based Embeddings: Advanced embeddings are generated using natural language\nprocessing techniques. By feeding descriptive text about datasets, models, and hardware into pre-\ntrained language models, we can obtain rich, contextual embeddings that reflect deeper insights\ninto each component's characteristics. For example, dataset descriptions or model architecture\nsummaries can be processed by models such as BERT or GPT to produce embeddings that capture\nnuanced interactions that might affect performance."}, {"title": "DATA EMBEDDING VIA LANGUAGE MODELS", "content": "To generate data embeddings, we describe each\ndataset in natural language, detailing its key attributes such as size, type, feature distribu-\ntion, and any specific challenges it presents. For example, a description for a generic dataset\nmight read: \"This dataset comprises 10,000 instances, each with 20\nfeatures, ranging from numerical to categorical types, intended\nfor regression tasks. The data variability is high, making model\nfitting challenging.\" Such descriptions are processed by LLMs to create embeddings\nthat reflect the dataset's complexity and suitability for different learning tasks."}, {"title": "MODEL EMBEDDING VIA LANGUAGE MODELS", "content": "Model embeddings are generated by describing\nthe model's architecture, usage, and any specific configurations or optimizations. For instance:\n\"The model is a convolutional neural network designed for image\nclassification, featuring three convolutional layers followed by\ntwo fully connected layers. It is optimized for GPU execution and\nhas been previously used in real-time applications.\" This description\nhelps encode the model's operational characteristics and computational demands."}, {"title": "HARDWARE EMBEDDING VIA LANGUAGE MODELS", "content": "Hardware embeddings are created by detailing the\nspecifications and performance characteristics of the hardware setup. For a mixed hardware envi-\nronment, the description might be: \"The primary machine features an NVIDIA\nA100 GPU, optimized for high-throughput computing tasks. The\nother machines are CPU-only, intended for lighter, non-parallel\ncomputational tasks.\" These descriptions allow LLMs to generate embeddings that reflect\nthe hardware's capabilities and limitations.\nTechnique Implementation: We utilize state-of-the-art LLMs such as BERT or GPT-3 to pro-\ncess these textual descriptions. The embeddings generated by these models are then used in our\nmeta-learning framework to predict the performance of different acceleration methods across varied\ndatasets, models, and hardware environments.\nPractical Example: For a practical implementation, consider a dataset described as: \"Dataset\nconsists of one million images, each labeled with one of 100\ncategories. Images vary significantly in background noise and\nlighting conditions.\" An LLM processes this description to produce an embedding that\nthe meta-learner uses to evaluate the suitability of various acceleration methods for this specific\ndataset under different hardware constraints.\nThis approach, leveraging the power of LLMs for embedding generation, offers a scalable, adapt-\nable, and computationally efficient alternative to traditional meta-feature extraction. It allows the\nmeta-learner to make informed decisions based on comprehensive and nuanced understandings of\nthe data, models, and hardware environments involved.\nWe detail the experiments and results of this new embedding approach in \u00a7??, demonstrating its ef-\nfectiveness in improving the accuracy and robustness of our meta-learning model selection process."}, {"title": "ONLINE MODEL SELECTION", "content": "In the online model selection process, we generate embeddings for the new dataset, model, and\nhardware environment denoted as Dnew. We then utilize the pretrained meta-performance predictor\nf to estimate the performance of various acceleration methods based on these embeddings, selecting\nthe method predicted to offer the best performance within the given cost constraints."}, {"title": null, "content": "M\u2217 := arg maxMjEM,Cj,kbPnew,j,k, where Pnew,j,k = f (Edata, Emodel, Ehardware) (3)"}, {"title": null, "content": "Cj,k represents the cost of running method Mj on hardware Hk and must not exceed the budget b.\nIt is calculated as the product of the cost associated with Hk and the expected runtime of Mj on Hk.\nThus, for each new task defined by Dnew = {xtrain test, new, new, Hnew}, embeddings for the dataset (Edata),\nthe applicable model (Emodel), and the hardware environment (Ehardware) are computed. The trained\nmeta-predictor f is then used to evaluate the performance of each available acceleration method\nunder the specific hardware settings of Hnew, ensuring that the total cost remains within the budget.\nThis selection process is essentially zero-shot with respect to the new dataset, requiring no retraining\nof the network or additional empirical testing. It relies wholly on the synthetic insights generated\nthrough the meta-learning framework, ensuring rapid deployment and optimization of inference\ntasks under varied computational environments and cost constraints.\nThe selection algorithm uses a maximization strategy to pick the acceleration method that is pre-\ndicted to enhance performance the most, based on the calculated embeddings and within the opera-\ntional budget. The process aims to determine the optimal method that balances speed, accuracy, and\nresource efficiency, tailored to the specific demands of the new environment."}, {"title": "CONCLUSION", "content": "In this paper, we introduced a novel meta-learning framework designed to optimize inference accel-\neration within decentralized systems. By addressing the unique challenges associated with deploy-\ning large-scale models such as LLMs and image generation systems, our framework significantly\nenhances the adaptability and effectiveness of acceleration techniques. Specifically, it allows for the\ndynamic selection of methods tailored to both the characteristics of the task and the nuances of the\nsystem architecture. Our results demonstrate that this approach not only surpasses traditional accel-\neration methods in efficiency and performance but also offers a scalable and cost-effective solution\nfor the deployment of AI models across diverse and distributed environments.\nFurthermore, the successful application of our framework highlights its potential to facilitate rapid,\nefficient inference processes, thereby supporting the broader adoption and democratization of ad-\nvanced AI technologies in decentralized settings. Looking forward, this work sets a solid founda-\ntion for future research focused on refining and expanding the capabilities of AI systems to meet the\nincreasing demands of real-world applications. We believe that the methodologies developed here\nwill inspire further innovations in the field, particularly in enhancing the operational efficiency of\nlarge model infrastructures in decentralized networks."}]}