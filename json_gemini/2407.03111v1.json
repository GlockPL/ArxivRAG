{"title": "Compressed Latent Replays for Lightweight Continual Learning on Spiking Neural Networks", "authors": ["Alberto Dequino", "Alessio Carpegna", "Davide Nadalini", "Alessandro Savino", "Luca Benini", "Stefano Di Carlo", "Francesco Conti"], "abstract": "Rehearsal-based Continual Learning (CL) has been intensely investigated in Deep Neural Networks (DNNs). However, its application in Spiking Neural Networks (SNNs) has not been explored in depth. In this paper we introduce the first memory-efficient implementation of Latent Replay (LR)-based CL for SNNs, designed to seamlessly integrate with resource-constrained devices. LRs combine new samples with latent representations of previously learned data, to mitigate forgetting. Experiments on the Heidelberg SHD dataset with Sample and Class-Incremental tasks reach a Top-1 accuracy of 92.5% and 92%, respectively, without forgetting the previously learned information. Furthermore, we minimize the LRs' requirements by applying a time-domain compression, reducing by two orders of magnitude their memory requirement, with respect to a na\u00efve rehearsal setup, with a maximum accuracy drop of 4%. On a Multi-Class-Incremental task, our SNN learns 10 new classes from an initial set of 10, reaching a Top-1 accuracy of 78.4% on the full test set.", "sections": [{"title": "I. INTRODUCTION", "content": "The current landscape of edge AI is dominated by Artificial Neural Networks (ANNs) driven by high-end server models, like Transformers and Convolutional Neural Networks (CNNs) [1]. Concurrently, the recent advancements in hardware and software for edge Internet of Things (IoT) devices have enabled the integration of Artificial Intelligence (AI) on low-power sensor nodes. However, deploying complex ANNs for edge inference involves imposing constraints like quantization and pruning [14] to accommodate small IoT devices, like ultra-low-power microcontrollers. Moreover, edge AI models are susceptible to errors, once deployed, due to shifts in data distribution between training and operational environments [2]. Also, an increasing number of applications require adapting A\u0399 algorithms to individual users while maintaining privacy and minimizing internet connectivity.\nContinual Learning (CL) i.e., the ability to continually learn from evolving environments, without forgetting previ-ously acquired knowledge emerges as a novel paradigm to address these challenges. Rehearsal-based methods [17], [20], the most accurate CL solutions, mitigate forgetting by continually training the learner on a mix of new data and a stored set of samples from previously learned tasks, albeit at the expense of additional on-device storage. Rehearsal-free methods [5], [15] rely on tailored modifications to network architecture or learning strategy to ensure model resilience to forgetting, without saving samples on-device, but with a poten-tial trade-off in accuracy. CL at the edge, especially Rehearsal-based methods, can be resource-intensive for various ANN models, as CNNs, demanding substantial on-device storage for complex learning data.\nIn this context, Spiking Neural Networks (SNNs) emerge as a promising energy-efficient paradigm, exhibiting high accuracy and efficiency in processing time series [11]. SNNS closely emulate biological neurons' behavior, communicating with spikes that can be efficiently stored as 1-bit data in digital architectures. This simplified data encoding creates new opportunities for developing CL solutions. While online learning has been explored in both hardware and software SNNs implementations [7], CL strategies in SNNs are only partially investigated in Rehearsal-free methods [12], [19], [21]. To the best of our knowledge, only Proietti, et al. [18] addressed Rehearsal-based CL in SNNs. However, their work shows limited accuracy and does not optimize memory storage, which is vital for edge devices. This work makes the following contributions:\nA cutting-edge, memory efficient implementation of Rehearsal-based CL for SNNs, designed to seamlessly integrate with resource-constrained devices. We enable CL on SNNs by exploiting a Rehearsal-based algorithm i.e., Latent Replay (LR) proving to achieve State-of-the-Art classification accuracy on CNNs [17].\nA novel approach to reduce the rehearsal memory, based on the robustness of information encoding in SNNs to precision reduction, which allows us to apply a lossy compression on the time axis.\nWe validate our method targeting a keyword spotting application, using Recurrent SNN, on two common CL setups: Sample-Incremental and Class-Incremental CL.\nFinally, to highlight the proposed approach's robustness, we test it in an extensive Multi-Class-Incremental CL routine, learning 10 new classes from an initial set of 10 pre-learned ones.\nIn the Sample-Incremental setup, we achieve a Top-1 ac-curacy of 92.46% on the Spiking Heidelberg Dataset (SHD) test set, requiring 6.4 MB for the LRs. This occurs while incorporating a new scenario, for which the accuracy is improved by 23.64%, without forgetting the previously learned ones. In the Class-Incremental setup, a Top-1 accuracy of 92% was attained, requiring 3.2 MB, when learning a new class with an accuracy of 92.50%, accompanied by a loss of up to 3.5% on the old classes. When jointly applying compression and selecting the most performing LR index, we reduce by"}, {"title": "II. RELATED WORK", "content": "CL on ANNs involves two main approaches: Rehearsal-based and Rehearsal-free methods. In Rehearsal-based methods, mit-igating catastrophic forgetting involves training the learner on a mix of newly acquired data and samples from previously learned tasks. To increase learned classes, iCaRL [20] utilizes a set of representative old class examples as rehearsal data, chosen to maintain a balanced set of classes. To improve the efficiency of Rehearsal-based methods, Pellegrini et al. [17] propose storing rehearsal data as Latent Replays, i.e., activations produced as the output of one of the hidden layers of the learner, specifically a feed-forward CNN. Only the last layers are retrained, while the earliest backbone's weights are frozen. Results, compared to iCaRL, demonstrate three orders of magnitude faster execution, with an almost 70% improvement in Top-1 accuracy on a Class-Incremental setup using the CORe50 dataset. In Rehearsal-free methods, address-ing forgetting involves modifying the learner's architecture or customizing the training procedure [5], [15]. However, their accuracy lags behind Rehearsal-based approaches. We adopt the Rehearsal-based approach using LRs, given its demonstrated effectiveness on CNNs.\nContinual Learning in SNNs has primarily focused on Rehearsal-free methods. Skatchkovsky et al. [21] propose a Bayesian continual learning framework, providing well-calibrated uncertainty quantification estimates for new data. In contrast, the SpikeDyn framework [19] introduces unsu-pervised continual learning on a model search algorithm, supporting adaptive hyperparameters. However, this approach performs poorly, achieving 90% accuracy on average for a new class in the MNIST dataset while only maintaining 55% accu-racy on the old classes. Drawing inspiration from human brain neurons, Han et al. [12] propose Self-Organized Regulation networks, capable of learning new tasks by self-organizing the neural connections of a fixed architecture. Additionally, they simulate irreversible damage to SNNs structures by pruning the models. Results on the CIFAR100 and Mini-ImageNet datasets, using DNN-inspired Convolutional and Recurrent networks, demonstrate an average accuracy of around 80% and 60%, respectively, for all learned classes. To the best of our knowledge, only the work of Proietti, et al., [18] targets Rehearsal-based CL on SNNs. In their approach, a Convolu-tional SNN model is trained to learn in a Class-Incremental and Task-free manner on the MNIST dataset, learning multiple binary classification tasks in sequence. However, their explo-ration doesn't involve any memory optimization technique, storing raw data for the rehearsal phase. Additionally, they report a top-1 accuracy of 51% after learning sequentially the"}, {"title": "III. BACKGROUND", "content": "SNNs are a category of neural networks inspired by the behav-ior of biological brain segments. Unlike other ANNs models, SNNs convey information through sequences of spikes, with their representation adapting to the execution domain. In the digital domain, considered in this study, spikes are encoded as binary values (one for a spike, zero otherwise) associated with discrete timesteps. The information conveyed by spikes is encoded in their sequence, such as their instantaneous rate, arrival time, or more complex patterns. Following Eshraghian, et al., [6] and aiming at the classification of signals with dense time-domain information (e.g., sound samples), we use a Fully Connected Recurrent SNN architecture with L layers, incorporating intra-layer side connections akin to those found in [4] and [23]. The chosen model employs a Synaptic Conductance-based Second Order Leaky Integrate and Fire (LIF) neuron model [3]. Optimally training SNN models poses a challenge, due to the non-differentiability of the spike function. Similar to Recurrent Neural Networks (RNNs), state-of-the-art back-propagation techniques can be applied to SNNs using Surrogate Gradients (SGs) [16], implemented through Back-Propagation Through Time (BPTT) [22]. In this study, SNNs undergo training via BPTT, utilizing a fast-sigmoid SG to approximate the step function characteristic of Recurrent Neurons.\nIn this work, we target CL, a machine learning paradigm that enables models to continuously acquire knowledge from a data stream without erasing prior learning. Let us con-sider a dataset D on which the model is pre-trained on $K_{class} < N_{class}$ classes and/or $K_{scene} < N_{scene}$ potential scenarios. A scenario here represents a specific data subset containing all $N_{class}$ classes, exemplifying a distinct data variation (e.g., a single speaker in a keyword-spotting dataset). Let T be a test set comprising samples from $N_{class}$ classes and $N_{scene}$ scenarios. In the CL paradigm, the pre-trained model continually incorporates and learns new labeled samples $\\epsilon D$, enhancing its predictive accuracy on T. CL algorithms require specific techniques to prevent models from forgetting previously learned data, a challenge known as Catastrophic Forgetting [13], which has recently been demonstrated to exist also in SNNs [9], [10]."}, {"title": "IV. LATENT REPLAY-BASED CONTINUAL LEARNING IN SNNS", "content": "In this paper, we draw inspiration from the work of Pellegrini et al. [17] on LR, initially designed for CNNs. Several crucial steps must be addressed to adapt this paradigm for SNNs. First, rehearsal data need to encapsulate the temporal evolution"}, {"title": "A. Latent Replays in SNNs", "content": "Algorithm 1 outlines the proposed Latent Replay-based train-ing for an SNN with L layers. The neural network undergoes pre-training over $E_{pre}$ epochs using BPTT on an initial train-ing set $TR_{pre}$ with an \u03b7 learning rate (lines 4-8). As illustrated in Fig. 1-b), the network is then split into two sections: $(SNN_f)$, comprising the first K layers denoted as frozen layers, and $(SNN_l)$, encompassing layers from the $L - K + 1$-th to the L-th, marked as learning layers (line 11). The latent replays (Fig. 1-c), denoted as $LR$, constitute a collection of the output activations of the $K^{th}$ layer when exposed to a subset $TR_{replay}$ of the pre-training set. This collection must be stored for later use during CL training (line 12), necessitating onboard storage. Since input data are trains of spikes (i.e., binary values) distributed over $T_s$ time-steps, the stored LRs are in turn sequences of $T_s$ single-bit activations.\nWhen training the network on new data, belonging to a new scenario or class, only the learning layers, are trained: the frozen layers simply propagate the input spikes sequences in the forward direction, processing them through the function learned in the pre-training phase (line 16). The learning layers are then trained for $E_{cl}$ epochs, using the output activations of the $K^{th}$ layer, blended with the stored LRs to keep memory of the learned data (line 17)."}, {"title": "B. Optimization of LR memory", "content": "To keep memory of previously learned information, an ad-ditional storage is required for the LRs. The problem be-"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "We evaluated our methodology using the SHDs dataset [4], which comprises 10,420 spiking trains of 100 timesteps each."}, {"title": "B. Weights initialization", "content": "When pre-training an SNNs for a Class-Incremental setup, neurons associated with unlearned classes are trained to be inactive. Therefore, when adding a new class to the pre-trained classifier, the yielded accuracy is poor, reaching a maximum of 57%. This issue can be addressed by re-initializing the neuron weights devoted to detect the new class. When performing weight re-initialization, the obtained accuracy is strictly linked to the re-initialization strategy. Notably, random, constant, and Xavier-Glorot initializations [8] proved inefficient for proficient learning of the new class. Instead, we adopted a normal random distribution with mean and variance aligned with those of the classifier's weights trained on the old classes. This approach enables our model to achieve a remarkable 92.9% accuracy on the new class. Further details are discussed in Section V-D. Unlike the Class-Incremental scenario, the Sample-Incremental scenario does not require any initializa-tion, as the number of classes does not change between samples."}, {"title": "C. Sample-Incremental CL", "content": "Our model undergoes pre-training on 11 out of 12 scenarios (speakers) to simulate user personalization in keyword spot-ting tasks. The 12th scenario is introduced using a Sample-Incremental CL approach. To benchmark our technique against methodologies like [18], we conduct experiments in three setups: (i) a na\u00efve incremental setup, where the 12th speaker is learned without rehearsal; (ii) a na\u00efve rehearsal setup, mixing the 12th speaker's input activations with 2,560 samples from the training step; (iii) the proposed LR setup, integrating 2,560 LRs stored at the output of the 2nd-to-last layer. Fig. 3-a)"}, {"title": "D. Class-Incremental CL", "content": "In this scenario, we simulate the introduction of a new keyword by pre-training our model on 19 out of 20 classes and adding the 20th class subsequently. Similar to the Sample-Incremental setup, we compare (i) a na\u00efve incremental, (ii) a na\u00efve rehearsal, and (iii) the proposed LR setup. Fig. 4-a) measures forgetting, while 4-b) shows Top-1 accuracy on the new class. In terms of forgetting, LRs show no accuracy drop, outperforming both na\u00efve rehearsal (~ 3% drop) and na\u00efve incremental (complete forgetting). The new class is"}, {"title": "E. Classification Accuracy vs Number of LRs", "content": "Fig. 5 comprehensively analyzes the impact of various hy-perparameters on accuracy, examining (a) the number of LRs and (b) LR indices across different epochs. In the Sample-Incremental setup, increasing LRs positively influences Top-1 accuracy. The primary effect is on the maximum accuracy, rather than the convergence slope, which remains relatively constant across all cases. An initial forgetting can be observed with 640 and 1280 past samples, becoming more and more accentuated with decreasing numbers of LRs. However, in all the cases the network is able to retrieve the past information, bringing the accuracy back to its initial value, or exceeding it. The choice of LRs index impacts maximum accuracy as well: figure 5 shows the superiority of LRs (indexes 1, 2 and 3) with respect to na\u00efve reharsal (index 0). The highest accuracy of 92.5% is achieved with 5,120 LRs at layer index 1, as layer 0 retains knowledge of old data while extending towards old and new samples.\nIncreasing LRs has similar effects for the Class-Incremental CL setup. While varying the layer index, we observe a peak accuracy at LRs index 2, making Top-1 accuracy reach 93% with 4,864 LRs. This indicates the capability of the final layers to learn the new class, exploiting the higher-level features learned by previous layers. An interesting effect can be noted at index 1, where the curve is non-monotonic: in this case, a larger number of LRs delays the convergence of the model, preventing it from reaching an acceptable accuracy within the 50 epochs."}, {"title": "F. Compressed LRs", "content": "We now explore the memory-accuracy trade-off of our LR compression algorithm. The choice of the LRs' layer index and the time compression factor $C_r$ is based on the trade-off between accuracy and memory requirements."}, {"title": "H. Multi-Class-Incremental Setup", "content": "We now provide a more complex testing scenario, i.e., a keyword spotting language shift in which our model learns to classify 10 classes of digits in German, starting from 10 pre-learned English-spoken digits. The pre-trained knowledge is stored as 2560 LRs, 256 per-class. After each new class is learned, 256 LRs are added to the memory. Our model is trained for 50 epochs per class, with $C_r = 2$.\nFig. 6 shows the Top-1 accuracy over the 20-class SHD test set. The pre-training accuracy is of 48.5%, corresponding to an accuracy of 97% on the first 10 classes only. While learning with CL, a monotonic growth is assessed, learning each successive class with 88.2% average accuracy. For each new class learned, we observe an average forgetting of 2.2% on all the previous classes. In the end, we observe a final accuracy of 78.4% on the full test set."}]}