{"title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference", "authors": ["Hailin Zhang", "Xiaodong Ji", "Yilin Chen", "Fangcheng Fu", "Xupeng Miao", "Xiaonan Nie", "Weipeng Chen", "Bin Cui"], "abstract": "As the field of Large Language Models (LLMs) continues to evolve,\nthe context length in inference is steadily growing. Key-Value Cache\n(KVCache), a crucial component in LLM inference, has now become\nthe primary memory bottleneck due to limited GPU memory. Cur-\nrent methods selectively determine suitable keys and values for\nself-attention computation in LLMs to address the issue. However,\nthey either fall short in maintaining model quality or result in high\nserving latency. Drawing inspiration from advanced embedding\nretrieval techniques used in the database community, we consider\nthe storage and searching of KVCache as a typical embedding re-\ntrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality\nwhile ensuring low serving latency. During the prefilling phase, we\napply PQ to tokens' keys for each LLM layer and head. During the\nautoregressive decoding phase, for each newly generated token, we\nfirst identify important tokens through Maximum Inner-Product\nSearch (MIPS) using PQ codes and centroids, then fetch the corre-\nsponding key-value pairs for self-attention computation. Through\nmeticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both\nphases. Extensive experiments show that PQCache achieves both\neffectiveness and efficiency. It maintains model quality even when\nonly 1/5 of the tokens are involved in attention, while attaining\nacceptable system latency.", "sections": [{"title": "1 INTRODUCTION", "content": "With the emergence of ChatGPT [40], Large Language Models\n(LLMs) have captured the attention of researchers and engineers as\npromising candidates for Artificial General Intelligence (AGI). LLMs\nexhibit exceptional performance in the \u201cnext token prediction\u201d task,\nwhere they take a sequence of tokens as input (also called prompt)\nand generate subsequent tokens autoregressively during inference.\nConstructed with transformer layers, the fundamental mechanism\nof LLMs is the self-attention module. For each token, this module\ncomputes \"query\u201d, \u201ckey\u201d, and \u201cvalue\u201d representations. Each token's\nquery interacts with the previous tokens' keys (including itself) to\nderive attention weights, which are then used for weighted sum-\nmation of the previous tokens' values. Figure 2 illustrates a typical\nself-attention module within a transformer layer.\nTo accommodate increasingly lengthy prompts, the maximum\ninput length of LLMs has expanded significantly, from 2K-4K [50,\n52] to 32K [25, 51], 128K [15, 40], or even millions of tokens [2, 9, 31].\nAs illustrated in Figure 2, the process of LLM inference involves\ntwo phases: prefilling and decoding. During prefilling, LLMs handle\nthe lengthy input and compute keys and values for all input tokens.\nDuring decoding, LLMs generate the next new token and produce\nits key and value. To avoid redundant computations, the keys and\nvalues of preceding tokens are commonly cached in the Key-Value\nCache (KVCache), and fetched for subsequent tokens' attention\ncomputation. However, as prompts grow in length, the memory\nconsumption of KVCache has far exceeded the memory capacity\nof each individual GPU, even for 7B and 13B LLMs in Figure 1(a).\nThis poses a formidable challenge for modern LLM inference.\nRecognizing that specific tokens significantly influence genera-\ntion, i.e. their attention weights are much larger than others [33, 63],\nnumerous methods selectively incorporate these tokens within at-\ntention mechanisms while excluding others. This approach aims\nto address the memory challenge posed by KVCache and is com-\nmonly referred to as selective attention [37]. Related methods can\nbe classified into two categories: KVCache dropping [33, 58, 63]\nand KVCache offloading [46, 57]. However, these methods either\nrely on improper assumptions or introduce notable latency dur-\ning inference, failing to obtain both effectiveness and efficiency.\nKVCache dropping methods discard unnecessary key-value pairs,\nbased on the assumption that unimportant tokens have no rele-\nvance for subsequent generation. Nevertheless, as shown in an\nattention score example in Figure 1(b), many tokens with lower\naverage attention weights can still contribute to later generated to-\nkens. Prior research [10, 29] also highlights the drawback of direct\ndropping. KVCache offloading methods, including InfLLM [57] and\nSPARQ [46], store the KVCache on CPU, and fetch relevant key-\nvalue pairs for each newly generated token according to easy-to-\ncompute proxy scores. InfLLM organizes the KVCache into blocks,"}, {"title": "2 PRELIMINARY", "content": "In this section, we introduce fundamental concepts related to LLM,\nPQ, and the memory hierarchy."}, {"title": "2.1 Large Language Model Inference", "content": "An overview of LLM inference is depicted in Figure 2. An LLM\ncomprises a stack of transformer layers, along with a vocabulary\nembedding for input and a token classifier for output. The self-\nattention module, which is a crucial component of a transformer\nlayer, facilitates interaction and information aggregation among\ndifferent tokens. Multi-Head Attention (MHA) and Grouped-Query\nAttention (GQA) [3] are the primary variants of the self-attention\nmodule. Following the notations in Table 1, the attention module\nreceives an input of shape $(n, s, d)$. In MHA, the input is separately\nprojected and transposed for query, key, value, resulting in the\nsame shape of $(n, h, s, d_h)$, where it usually holds that $d = h * d_h$.\nDifferent heads are expected to capture different semantic infor-\nmation. The attention mechanism multiplies the queries and keys,\napplies a lower-triangular mask to restrict queries to preceding\nkeys only, and performs softmax to obtain the attention scores of\nshape $(n, h, s, s)$. The attention scores are then used to weighted-\nsum the values, yielding an output of shape $(n, h, s, d_h)$, which is\nlater reshaped into $(n, s, d)$. To alleviate memory and computation\nburden, GQA employs a smaller number of heads $h_{kv}$ for keys and\nvalues, resulting in their shape being $(n, h_{kv}, s, d_h)$. In this setup,\neach key-value pair corresponds to multiple queries.\nDuring LLM inference, each execution of the model generates a\nnew token, following an autoregressive manner. The first traversal\nand the subsequent traversals of the LLM are referred to as \"prefill-\ning\" and \"decoding\" separately, as shown in Figure 2. During the\nprefilling phase, the self-attention module computes the queries,\nkeys, and values for all input tokens, and stores the key-value pairs\nas KVCache for later usage. During the autoregressive decoding\nphase, the attention module only computes the query, key, value for\nthe last generated token. It leverages previous keys and values from\nthe KVCache, and computes an attention score of shape $(n, h, 1, s)$.\nConcurrently, the newly generated key and value are added to the\nKVCache. Consequently, the memory consumption of KVCache\nscales linearly with the sequence length, which leads to a memory\nbottleneck in scenarios involving long-context LLM inference."}, {"title": "2.2 Product Quantization", "content": "PQ [24] was proposed to facilitate efficient Approximate Nearest\nNeighbor Search (ANNS), retrieving relevant embeddings from\na large pool of candidates given a query embedding. MIPS is a\nspecial case of ANNS that uses inner product as similarity. As\nshown in Figure 3, PQ divides each candidate embedding into $m$\npartitions, essentially decomposing the original embedding space\ninto $m$ separate sub-spaces. Each sub-space undergoes K-Means\nclustering to group the sub-embeddings, yielding $2^b$ centroids. Each\nembedding is assigned $m$ codes, each having $b$ bits, corresponding\nto the centroids. These compact PQ codes enable the reconstruction\nof approximate embeddings with reduced memory requirements.\nDuring ANNS, the query embedding computes similarity with the\ncentroids and aggregates the similarity using PQ codes, bypassing\nthe need for full similarity calculations with every embedding.\nPQ has a profound impact on ANNS, with its principles inte-\ngrated into various efficient ANNS methods [6, 23, 27]. PQ has\nseveral variants, including Optimized PQ [18], Residual Quantiza-\ntion [36], and SCaNN [19]. While PQ was initially designed for\nANNS, its variants are also applied in various learning tasks [30,\n53, 61] to achieve effective compression and efficient computation."}, {"title": "2.3 GPU-CPU Memory Hierarchy", "content": "Modern deep learning tasks heavily rely on GPUs for executing\ncompute-intensive operations. The GPU-CPU structure forms a\ntypical memory hierarchy: the more expensive GPU memory of-\nfers faster memory I/O speeds for computation, while the CPU\nmemory, connected via PCIe or NVLink, provides lower bandwidth.\nAs model parameters increase and the demand for intermediate\nresults storage (such as KVCache) grows, CPUs are often employed\nto share the memory load. Numerous research studies in machine\nlearning systems propose offloading certain model parameters or\nactivations to the CPU memory [39, 42, 45, 47], thereby enhancing\nthe overall performance of GPU-centric deep learning tasks. The\nprimary challenge in this context is to effectively schedule memory\nI/O (or say GPU-CPU communication) in conjunction with GPU\ncomputation to efficiently hide the associated overhead."}, {"title": "3 PQCACHE", "content": "In this section, we introduce PQCache, a novel system-algorithm co-\ndesigned method to enable effective and efficient long context LLM\ninference with large-scale KVCache. Figure 4 provides an overview"}, {"title": "3.1 Overview", "content": "We design PQCache to reserve all the KVCache in CPU, and se-\nlectively fetch relevant key-value pairs for self-attention compu-\ntation. In long context inference scenario, the entire KVCache is\ntoo large for both attention computation and I/O communication\nwithin the memory hierarchy. Therefore, a common technique is\nto only perform attention on a subset of the key-value pairs, a\nprocess known as \"selective attention\u201d. According to previous re-\nsearch [1, 17, 33, 44, 46, 55, 57, 59, 63], attention score is a proper\nmetric to measure the importance or relevance of previous tokens.\nAs shown in Figure 5, we plot the attention score distributions at\nseveral randomly-selected positions on an example from the XSUM\ndataset [38]. The attention scores generally follow powerlaw distri-\nbutions, indicating that a small part of tokens are more important\nthan most other tokens. Therefore, we can only include those to-\nkens with large scores for self-attention computation. Following\nprior works [20, 58, 63], we also include initial tokens and the most\nrecent tokens (called local tokens) in attention computation."}, {"title": "3.2 Complexity Analysis", "content": "During the prefilling phase, we do not modify the attention com-\nputation, so the complexity remains the same for both time and\nmemory. The additional K-Means clustering process has an average\ncomplexity of $O(s \\cdot m \\cdot 2^b \\cdot T)$, where $T$ is the number of K-Means\niterations. We leverage the idle CPU resources to perform K-Means,\nwhich is detailed in Section 3.3. During the decoding phase, the\noriginal attention time complextiy is $O(sd + d^2)$. In PQCache,\nwe first conduct multiplication on PQ centroid with a time com-\nplexity of $O(sm + d^2)$, then compute the attention with a time\ncomplexity of $O(k \\cdot d)$. The memory complexity is $O(s \\cdot m + 2^b. d)$,\ncontaining PQ centroids and PQ codes. Considering that $m \\ll d$,"}, {"title": "3.3 Prefilling Phase", "content": "At the prefilling phase, on obtaining the input tokens' keys and\nvalues in each layer, they can attend to attention computation\nand be offloaded to CPU simultaneously. Given that the attention\ncomputation time scales quadratically with sequence length, while\nthe communication time scales linearly, the communication can be\nfully overlapped in long context scenarios, as shown in Figure 7.\nThe K-Means clustering is of great complexity according to Sec-\ntion 3.2. To enable overhead-agnostic inference, we aim to fully\nutilize the idle CPU resources for clustering. However, as shown in\nFigure 7, the clustering process on CPU, including PQ construction\nfor all heads in each layer, consumes more time than one-layer\ntransformer computation on GPU at the prefilling stage. This is be-\ncause the computational capability of GPUs has grown rapidly over\nthe past decades, whereas CPUs are not specifically designed for\ncomputationally intensive tasks. To address the issue, we propose\nan adaptive K-Means clustering process which limits the number of"}, {"title": "3.4 Decoding Phase", "content": "At the decoding phase, the constructed PQ structure needs to be\nutilized by the attention module in each layer. While the preceding\ncomputation is underway, the PQ centroids and codes of the current\nlayer can be pre-fetched in parallel. Since the PQ structure consumes\nnegligible memory according to Section 3.2, its communication can\ndirectly overlap with decoding phase computation.\nThroughout the entire inference, the only communication that\ncannot be overlapped is the retrieval of the top-k relevant tokens,\nbecause it is dependent on preceding PQ approximation computa-\ntion. Inspired by previous research [33, 57, 63], there are certain\npivotal tokens that are consistently important during the inference\nprocess. Therefore, we maintain an GPU cache for these tokens. Fol-\nlowing LM-Infinite [20] and StreamingLLM [58], we first preserve\ninitial tokens and local tokens in the cache. For the remaining to-\nkens, we maintain a block-level cache, wherein the cache structure\nresides on CPU and the storage is allocated on GPU. We construct\ntoken-blocks to reduce cache overhead, and cache the blocks on\nGPU. During each decoding step, we identify the blocks containing\ntop-kcache hit tokens, and use them to fetch tokens while updating\nthe cache structure. We employ asynchronous updates to avoid\nadditional overhead. Experimental results in Section 4.3.4 illustrate\nthe cache hit-rate, which helps reduce overall communication."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments and compare PQCache with\nexisting methods. We experimentally show that PQCache achieves\nboth effectiveness and efficiency."}, {"title": "4.1 Experimental Setup", "content": "We conduct experiments using two representative\nopen-source LLMs: LLaMA-2-7B-Chat [52] and Mistral-7B-Instruct\nv0.2 [25]. The former employs MHA and supports 4K context length,"}, {"title": "4.2 Model Performance", "content": "The LongBench results of the methods on two\nLLMs are presented in Table 2 and 3. LongBench uses different\nmetrics for each dataset and calculates an average score to measure\noverall performance. We consider including 1/5 and 1/10 of the input\ntokens in selective attention, respectively, with an extra communi-\ncation that equals to 1/128 of the KVCache memory: for PQCache,\nwe use $m = 2$ and $b = 6$, which satisfies $2 \\times 6/16/128 < 1/128$; for\nSPARQ, we use $r = 1$ considering $d_h = 128$; for InfLLM, we use\n1 representative token from every 128 tokens. H2O(C) is allowed\nto attend to more tokens as introduced in Section 4.1.3. Excluding\nOracle, the best results for each setting, are highlighted in bold.\nOn average, models without compression (denoted as Full) can\nachieve the best results, since there is nearly no information loss\u00b9.\nWe investigate how the\namount of extra communication impacts the model performance\non the HotPotQA dataset"}, {"title": "4.3 Efficiency", "content": "In PQCache, K-Means clustering occurs concur-\nrently with GPU computation. While it doesn't affect the first token\ngeneration, subsequent tokens depend on clustering results. To\nassess system optimization, we use Time To 2nd Token (TT2T),\nconsidering query entry to LLM output time and KVCache man-\nagement overhead. As shown in Figure 10(a), with overlapping and\nadaptive clustering, PQCache can achieve the lowest TT2T. All\nbaseline methods have significant overhead. Since H2O collects\nattention scores during prefilling, it cannot utilize FlashAttention\nfor acceleration and encounters OOM when dealing with lengthy\ninput. SPARQ has no prefilling overhead, but its decoding process\nis slow (see Section 4.3.2). InfLLM incurs time overhead due to the\nsetup required for block-level KVCache management.\nTime Per Output Token (TPOT) measures the\ntime of each decoding step. We compare the TPOT of H2O, SPARQ,\nInfLLM, and PQCache in Figure 10(b). Here we use 1/5 number of\ntokens in selective attention, and a 4096-token GPU cache. SPARQ\nexhibits the highest latency due to its sequential computation and\ncommunication, with the communication scaling linearly with the\ninput sequence length. All the other methods exhibit per-token\nlatency faster than the human reading speed, which is around"}, {"title": "5 RELATED WORK", "content": "To eliminate the impact of\nmemory-intensive KVCache, a group of methods include only es-\nsential tokens for attention computation during LLM inference.\nOne way is to discard unnecessary tokens. LM-Infinite [20] and\nStreaming-LLM [58] only preserve the initial tokens and the most re-\ncent tokens. H2O [63] and Scissorhands [33] utilize attention scores\nto identify important tokens. Their following works [1, 17, 44, 55]\nhave explored adaptive token selection and additional metrics for\nbetter model accuracy. The LLMLingua series [26, 41] leverage an\nauxiliary small model to tell which tokens are necessary. Since\ntoken-level compression evicts the tokens in a greedy manner, the\ninformation loss in subsequent decoding phase may lead to model\ndegradation. Another way is to fetch relevant tokens on demand\nduring the decoding phase. SPARQ [46] and InfLLM [57] offload\nKVCache to CPU, and selectively fetch relevant key-value pairs for\neach attention computation. PQCache also falls under this category\nof methods, demonstrating effective and efficient LLM inference in\ncomparison to existing techniques.\nQuantization can be directly applied\non the entire KVCache [11, 21, 34] - a straight-forward approach\nwith promising model quality. Other compression techniques can\nalso be employed to address the residuals introduced by quanti-\nzation [29]. It is worth noting that quantization is orthogonal to\ntoken importance, and recent research has explored applying both\ntechniques [59].\nAnother way to address the KVCache\nmemory challenge is to meticulously schedule the KVCache within\nmemory hierarchy. FlexGen [47] employs linear programming to\nschedule the communication, searching for efficient patterns to\nstore and access tensors. AttentionScore [16] maintains a hierarchi-\ncal KV caching system, allowing efficient reuse of KVCache across\nmulti-turn conversations. Another related research topic is KV-\nCache streaming for LLM serving [32, 49], which involves handling\nmultiple requests within more levels of memory hierarchy.\nEmbedding management is a com-\nmon research focus within the database and data management\ndomains, including embedding compression [48, 60, 62], embed-\nding retrieval [22, 54], and key-value storage [8, 43]. Our work\nprovides a potential direction for integrating classic embedding\nmanagement into the LLM ecology."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed PQCache, a system-algorithm co-designed\nmethod for effective and efficient long context LLM inference. We\nincorporated the embedding retrieval technique PQ to reduce both\nmemory and computation burden, and leveraged PQ codes and cen-\ntroids to facilitate efficient MIPS for important tokens used in the\nattention module. Through meticulous overlapping and caching,\nwe managed to minimize overhead to a negligible level. We evalu-\nated PQCache on extensive experiments, and show that PQCache\neffectively maintains model quality with only 1/5 of the tokens\ninvolved in attention, while achieving acceptable system latency."}]}