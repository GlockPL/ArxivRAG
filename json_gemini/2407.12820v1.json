{"title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference", "authors": ["Hailin Zhang", "Xiaodong Ji", "Yilin Chen", "Fangcheng Fu", "Xupeng Miao", "Xiaonan Nie", "Weipeng Chen", "Bin Cui"], "abstract": "As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), a crucial component in LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques used in the database community, we consider the storage and searching of KVCache as a typical embedding retrieval problem. We propose PQCache, which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, for each newly generated token, we first identify important tokens through Maximum Inner-Product Search (MIPS) using PQ codes and centroids, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments show that PQCache achieves both effectiveness and efficiency. It maintains model quality even when only 1/5 of the tokens are involved in attention, while attaining acceptable system latency.", "sections": [{"title": "1 INTRODUCTION", "content": "With the emergence of ChatGPT [40], Large Language Models (LLMs) have captured the attention of researchers and engineers as promising candidates for Artificial General Intelligence (AGI). LLMs exhibit exceptional performance in the \u201cnext token prediction\u201d task, where they take a sequence of tokens as input (also called prompt) and generate subsequent tokens autoregressively during inference. Constructed with transformer layers, the fundamental mechanism of LLMs is the self-attention module. For each token, this module computes \"query\u201d, \u201ckey\u201d, and \u201cvalue\u201d representations. Each token's query interacts with the previous tokens' keys (including itself) to derive attention weights, which are then used for weighted sum-mation of the previous tokens' values. Figure 2 illustrates a typical self-attention module within a transformer layer.\nTo accommodate increasingly lengthy prompts, the maximum input length of LLMs has expanded significantly, from 2K-4K [50, 52] to 32K [25, 51], 128K [15, 40], or even millions of tokens [2, 9, 31]. As illustrated in Figure 2, the process of LLM inference involves two phases: prefilling and decoding. During prefilling, LLMs handle"}, {"title": "2 PRELIMINARY", "content": "In this section, we introduce fundamental concepts related to LLM, PQ, and the memory hierarchy."}, {"title": "2.1 Large Language Model Inference", "content": "An overview of LLM inference is depicted in Figure 2. An LLM comprises a stack of transformer layers, along with a vocabulary embedding for input and a token classifier for output. The self-attention module, which is a crucial component of a transformer layer, facilitates interaction and information aggregation among different tokens. Multi-Head Attention (MHA) and Grouped-Query Attention (GQA) [3] are the primary variants of the self-attention module. Following the notations in Table 1, the attention module receives an input of shape (n, s, d). In MHA, the input is separately projected and transposed for query, key, value, resulting in the same shape of (n, h, s, dh), where it usually holds that d = h * dh. Different heads are expected to capture different semantic infor-mation. The attention mechanism multiplies the queries and keys, applies a lower-triangular mask to restrict queries to preceding keys only, and performs softmax to obtain the attention scores of shape (n, h, s, s). The attention scores are then used to weighted-sum the values, yielding an output of shape (n, h, s, d\u2081), which is later reshaped into (n, s, d). To alleviate memory and computation burden, GQA employs a smaller number of heads hko for keys and values, resulting in their shape being (n, hkv, s, dh). In this setup, each key-value pair corresponds to multiple queries.\nDuring LLM inference, each execution of the model generates a new token, following an autoregressive manner. The first traversal and the subsequent traversals of the LLM are referred to as \"prefill-ing\" and \"decoding\" separately, as shown in Figure 2. During the prefilling phase, the self-attention module computes the queries, keys, and values for all input tokens, and stores the key-value pairs as KVCache for later usage. During the autoregressive decoding phase, the attention module only computes the query, key, value for the last generated token. It leverages previous keys and values from the KVCache, and computes an attention score of shape (n, h, 1, s). Concurrently, the newly generated key and value are added to the KVCache. Consequently, the memory consumption of KVCache scales linearly with the sequence length, which leads to a memory bottleneck in scenarios involving long-context LLM inference."}, {"title": "2.2 Product Quantization", "content": "PQ [24] was proposed to facilitate efficient Approximate Nearest Neighbor Search (ANNS), retrieving relevant embeddings from a large pool of candidates given a query embedding. MIPS is a special case of ANNS that uses inner product as similarity. As shown in Figure 3, PQ divides each candidate embedding into m partitions, essentially decomposing the original embedding space into m separate sub-spaces. Each sub-space undergoes K-Means clustering to group the sub-embeddings, yielding 2b centroids. Each embedding is assigned m codes, each having b bits, corresponding to the centroids. These compact PQ codes enable the reconstruction of approximate embeddings with reduced memory requirements. During ANNS, the query embedding computes similarity with the centroids and aggregates the similarity using PQ codes, bypassing the need for full similarity calculations with every embedding.\nPQ has a profound impact on ANNS, with its principles inte-grated into various efficient ANNS methods [6, 23, 27]. PQ has several variants, including Optimized PQ [18], Residual Quantiza-tion [36], and SCaNN [19]. While PQ was initially designed for ANNS, its variants are also applied in various learning tasks [30, 53, 61] to achieve effective compression and efficient computation."}, {"title": "2.3 GPU-CPU Memory Hierarchy", "content": "Modern deep learning tasks heavily rely on GPUs for executing compute-intensive operations. The GPU-CPU structure forms a typical memory hierarchy: the more expensive GPU memory of-fers faster memory I/O speeds for computation, while the CPU memory, connected via PCIe or NVLink, provides lower bandwidth. As model parameters increase and the demand for intermediate results storage (such as KVCache) grows, CPUs are often employed to share the memory load. Numerous research studies in machine learning systems propose offloading certain model parameters or activations to the CPU memory [39, 42, 45, 47], thereby enhancing the overall performance of GPU-centric deep learning tasks. The primary challenge in this context is to effectively schedule memory I/O (or say GPU-CPU communication) in conjunction with GPU computation to efficiently hide the associated overhead."}, {"title": "3 PQCACHE", "content": "In this section, we introduce PQCache, a novel system-algorithm co-designed method to enable effective and efficient long context LLM inference. Figure 4 provides an overview"}, {"title": "3.1 Overview", "content": "We design PQCache to reserve all the KVCache in CPU, and se-lectively fetch relevant key-value pairs for self-attention compu-tation. In long context inference scenario, the entire KVCache is too large for both attention computation and I/O communication within the memory hierarchy. Therefore, a common technique is to only perform attention on a subset of the key-value pairs, a process known as \"selective attention\u201d. According to previous re-search [1, 17, 33, 44, 46, 55, 57, 59, 63], attention score is a proper metric to measure the importance or relevance of previous tokens. As shown in Figure 5, we plot the attention score distributions at several randomly-selected positions on an example from the XSUM dataset [38]. The attention scores generally follow powerlaw distri-butions, indicating that a small part of tokens are more important than most other tokens. Therefore, we can only include those to-kens with large scores for self-attention computation. Following prior works [20, 58, 63], we also include initial tokens and the most recent tokens (called local tokens) in attention computation.\nAs detailed in Section 2.1, attention scores are calculated using a softmax function applied to the product of the current query and preceding keys. The procedure of identifying the top-k keys with the highest scores fundamentally constitutes a Maximum Inner Product Search (MIPS) operation. Therefore, we try to leverage em-bedding retrieval techniques to enable effective selective attention and address the KVCache memory issue. Based on the observations above, we design PQCache, which offloads all the KVCache to CPU, and fetch only relevant tokens' key-values pairs during the decod-ing phase. Calculating exact attention scores of all previous tokens involves costly I/O communication, which is unacceptable in long context LLM inference. Inspired by Approximate Nearest Neighbor Search (ANNS) [6, 23, 27], we leverage the light-weight Product Quantization (PQ) method [24], which compress the vectors by partitioning and K-Means clustering. Though there are other ANNS methods (e.g. graph-based methods [13, 23, 35]) that can achieve better recall performance, they suffer from a computationally ex-pensive construction process which may hinder LLM inference.\nIn PQCache, we construct PQ at the prefilling phase and utilize PQ at the decoding phase. At the prefilling phase, we need to calcu-late all the input tokens' keys and values for the self-attention mod-ule. After obtaining the keys, which have the shape of (n, hkv, s, dh), we can construct PQ for each sample and each head. Concretely, for a tensor of shape (s, dh), we further split the dimension dh into m sub-spaces with dimension dm. For partitioned vectors (m, s, dm), where dm = dh/m, we conduct K-Means clustering separately for each group and get the centroids of shape (m, 2b, dm) and PQ codes of shape (s, m). Each PQ code, which indicates the cluster that the vector belongs to, only consumes b bits to store.\nAt the decoding phase, we first perform matrix multiplication between the query and the PQ centroids, then aggregate the results for all the tokens according to PQ codes. We can determine the top-k relevant tokens using the approximate scores (before softmax). After fetching the approximate top-k key-value pairs from CPU, the self-attention computation continues with retrieved tokens. Unlike normal embedding retrieval tasks, in LLM inference, newly generated keys and values are added into the KVCache. These tokens are first regarded as local tokens and reserved in GPU. When they are evicted from the sliding window of local tokens, they are assigned PQ codes based on their nearest centroids."}, {"title": "3.2 Complexity Analysis", "content": "During the prefilling phase, we do not modify the attention com-putation, so the complexity remains the same for both time and memory. The additional K-Means clustering process has an average complexity of O(s \u00b7 m \u00b7 2b \u00b7 T), where T is the number of K-Means iterations. We leverage the idle CPU resources to perform K-Means, which is detailed in Section 3.3. During the decoding phase, the original attention time complextiy is O(sd + d\u00b2). In PQCache, we first conduct multiplication on PQ centroid with a time com-plexity of O(sm + d\u00b2), then compute the attention with a time complexity of O(k \u00b7 d). The memory complexity is O(s \u00b7 m + 2b. d), containing PQ centroids and PQ codes. Considering that m \u00ab d,"}, {"title": "3.3 Prefilling Phase", "content": "At the prefilling phase, on obtaining the input tokens' keys and values in each layer, they can attend to attention computation and be offloaded to CPU simultaneously. Given that the attention computation time scales quadratically with sequence length, while the communication time scales linearly, the communication can be fully overlapped in long context scenarios, as shown in Figure 7.\nThe K-Means clustering is of great complexity according to Sec-tion 3.2. To enable overhead-agnostic inference, we aim to fully utilize the idle CPU resources for clustering. However, as shown in Figure 7, the clustering process on CPU, including PQ construction for all heads in each layer, consumes more time than one-layer transformer computation on GPU at the prefilling stage. This is be-cause the computational capability of GPUs has grown rapidly over the past decades, whereas CPUs are not specifically designed for computationally intensive tasks. To address the issue, we propose an adaptive K-Means clustering process which limits the number of"}, {"title": "3.4 Decoding Phase", "content": "At the decoding phase, the constructed PQ structure needs to be utilized by the attention module in each layer. While the preceding computation is underway, the PQ centroids and codes of the current layer can be pre-fetched in parallel. Since the PQ structure consumes negligible memory according to Section 3.2, its communication can directly overlap with decoding phase computation.\nThroughout the entire inference, the only communication that cannot be overlapped is the retrieval of the top-k relevant tokens, because it is dependent on preceding PQ approximation computa-tion. Inspired by previous research [33, 57, 63], there are certain pivotal tokens that are consistently important during the inference process. Therefore, we maintain an GPU cache for these tokens. Fol-lowing LM-Infinite [20] and StreamingLLM [58], we first preserve initial tokens and local tokens in the cache. For the remaining to-kens, we maintain a block-level cache, wherein the cache structure resides on CPU and the storage is allocated on GPU. We construct token-blocks to reduce cache overhead, and cache the blocks on GPU. During each decoding step, we identify the blocks containing top-kcache hit tokens, and use them to fetch tokens while updating the cache structure. We employ asynchronous updates to avoid additional overhead. Experimental results in Section 4.3.4 illustrate the cache hit-rate, which helps reduce overall communication."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments and compare PQCache with existing methods. We experimentally show that PQCache achieves both effectiveness and efficiency."}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Models. We conduct experiments using two representative open-source LLMs: LLaMA-2-7B-Chat [52] and Mistral-7B-Instruct-v0.2 [25]. The former employs MHA and supports 4K context length,"}, {"title": "4.2 Model Performance", "content": "4.2.1 LongBench. The LongBench results of the methods on two LLMs are presented in Table 2 and 3. LongBench uses different metrics for each dataset and calculates an average score to measure overall performance. We consider including 1/5 and 1/10 of the input tokens in selective attention, respectively, with an extra communi-cation that equals to 1/128 of the KVCache memory: for PQCache, we use m = 2 and b = 6, which satisfies 2 \u00d7 6/16/128 < 1/128; for SPARQ, we use r = 1 considering dh = 128; for InfLLM, we use 1 representative token from every 128 tokens. H2O(C) is allowed to attend to more tokens as introduced in Section 4.1.3. Excluding Oracle, the best results for each setting, are highlighted in bold.\nOn average, models without compression (denoted as Full) can achieve the best results, since there is nearly no information loss\u00b9.\nPQCache outperforms the major baselines (i.e., H2O(C), InfLLM, and SPARQ) on most of the datasets. Although PQCache achieves slightly lower scores in a handful of cases, it exhibits substantial improvements on average. Concretely, PQCache achieves +3.88 and +6.21 improvements on Mistral-7B, and achieves +1.61 and +1.60 improvements on LLaMa2-7B, respectively. Note that the offloading-based counterparts, InfLLM and SPARQ, have the worst performance on average due to the limited additional communi-cation for minimized latency. In contrast, PQCache performs well under the same constraint, validating the strength of our work.\nOracle is an ideal approach with the exact top-k tokens for se-lective attention, which gives excellent performance in most cases. However, we observe that PQCache even beats Oracle and achieves the same score as the uncompressed counterpart in the \"1/5#Tokens\" cases. This suggests that clustering may help PQCache uncover in-trinsic structures within the KVCache latent space, thus leading to promising results. Furthermore, although it is usually expected that the performance should drop when there are fewer tokens, there are exceptions where the opposite happens. This could be because not all tokens are useful for generating new ones, so getting rid of unnecessary ones might enhance inference."}, {"title": "4.3 Efficiency", "content": "4.3.1 Prefilling. In PQCache, K-Means clustering occurs concur-rently with GPU computation. While it doesn't affect the first token generation, subsequent tokens depend on clustering results. To assess system optimization, we use Time To 2nd Token (TT2T), considering query entry to LLM output time and KVCache man-agement overhead. As shown in Figure 10(a), with overlapping and adaptive clustering, PQCache can achieve the lowest TT2T. All baseline methods have significant overhead. Since H2O collects attention scores during prefilling, it cannot utilize FlashAttention for acceleration and encounters OOM when dealing with lengthy input. SPARQ has no prefilling overhead, but its decoding process is slow (see Section 4.3.2). InfLLM incurs time overhead due to the setup required for block-level KVCache management."}, {"title": "5 RELATED WORK", "content": "Selective Attention for KVCache. To eliminate the impact of memory-intensive KVCache, a group of methods include only es-sential tokens for attention computation during LLM inference. One way is to discard unnecessary tokens. LM-Infinite [20] and Streaming-LLM [58] only preserve the initial tokens and the most re-cent tokens. H2O [63] and Scissorhands [33] utilize attention scores to identify important tokens. Their following works [1, 17, 44, 55] have explored adaptive token selection and additional metrics for better model accuracy. The LLMLingua series [26, 41] leverage an auxiliary small model to tell which tokens are necessary. Since token-level compression evicts the tokens in a greedy manner, the information loss in subsequent decoding phase may lead to model degradation. Another way is to fetch relevant tokens on demand during the decoding phase. SPARQ [46] and InfLLM [57] offload KVCache to CPU, and selectively fetch relevant key-value pairs for each attention computation. PQCache also falls under this category of methods, demonstrating effective and efficient LLM inference in comparison to existing techniques.\nKVCache Quantization. Quantization can be directly applied on the entire KVCache [11, 21, 34] - a straight-forward approach with promising model quality. Other compression techniques can also be employed to address the residuals introduced by quanti-zation [29]. It is worth noting that quantization is orthogonal to token importance, and recent research has explored applying both techniques [59].\nKVCache Scheduling. Another way to address the KVCache memory challenge is to meticulously schedule the KVCache within memory hierarchy. FlexGen [47] employs linear programming to schedule the communication, searching for efficient patterns to store and access tensors. AttentionScore [16] maintains a hierarchi-cal KV caching system, allowing efficient reuse of KVCache across multi-turn conversations. Another related research topic is KV-Cache streaming for LLM serving [32, 49], which involves handling multiple requests within more levels of memory hierarchy.\nEmbedding Management. Embedding management is a com-mon research focus within the database and data management domains, including embedding compression [48, 60, 62], embed-ding retrieval [22, 54], and key-value storage [8, 43]. Our work provides a potential direction for integrating classic embedding management into the LLM ecology."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed PQCache, a system-algorithm co-designed method for effective and efficient long context LLM inference. We incorporated the embedding retrieval technique PQ to reduce both memory and computation burden, and leveraged PQ codes and cen-troids to facilitate efficient MIPS for important tokens used in the attention module. Through meticulous overlapping and caching, we managed to minimize overhead to a negligible level. We evalu-ated PQCache on extensive experiments, and show that PQCache effectively maintains model quality with only 1/5 of the tokens involved in attention, while achieving acceptable system latency."}]}