{"title": "On the role of Artificial Intelligence methods in modern force-controlled manufacturing robotic tasks", "authors": ["Vincenzo Petrone", "Enrico Ferrentino", "Pasquale Chiacchio"], "abstract": "This position paper explores the integration of Artificial Intelligence (AI) into force-controlled robotic tasks within the scope of advanced manufacturing, a cornerstone of Industry 4.0. AI's role in enhancing robotic manipulators - key drivers in the Fourth Industrial Revolution \u2013 is rapidly leading to significant innovations in smart manufacturing. The objective of this article is to frame these innovations in practical force-controlled applications - e.g. deburring, polishing, and assembly tasks like peg-in-hole (PiH) \u2013 highlighting their ne-cessity for maintaining high-quality production standards. By reporting on recent AI-based methodologies, this article contrasts them and identifies current challenges to be addressed in future research. The analysis concludes with a perspective on future research directions, emphasizing the need for common performance metrics to validate AI techniques, integration of various enhancements for performance optimization, and the importance of validating them in relevant scenarios. These future directions aim to provide consistency with already adopted approaches, so as to be compatible with manufacturing standards, increasing the relevance of AI-driven methods in both academic and industrial contexts.", "sections": [{"title": "1 INTRODUCTION", "content": "Manufacturing processes are nowadays experiencing the peak of their technological evolution, spurred by rapid advances in industrialization methods currently developing in the ongoing Fourth Industrial Revolution (Xu et al., 2018). These emerging innovations are defining the next generation of industries, leading to the so-called Industry 4.0 (Yang and Gu, 2021). \nOne of the pillars of the revolution is Artificial In-telligence (AI), whose application has seen tremen-dous advancements and increasing popularity in re-cent years. Robotic manipulators, which were already one of the core drivers of the Third Industrial Revolu-tion, are now amongst the technologies that are ben-efiting the most from AI (Bai et al., 2020). Merging these two powerful technologies is leading to the rise of advanced manufacturing (also termed smart manu-facturing), which constitutes the foundation of Indus-try 4.0 (Yang and Gu, 2021).\nThis position paper presents a discussion on a pe-culiar sector of robotic tasks, namely force-controlled applications. Such tasks are of fundamental practi-cal importance in manufacturing, since they deal with manipulators exerting forces on the working environ-ment, with the objective of, e.g., refining a workpiece or manipulating and assembling objects.\nPopular force-controlled tasks encompass, for in-stance, deburring (Lloyd et al., 2024), polishing (Iskandar et al., 2023), and assembly (Luo et al., 2019). On the one hand, the first two require exert-ing a specific normal force on the working surface, in order to remove burrs and production inaccuracies of a workpiece, or to smooth and finish the surface itself. On the other hand, the latter consists in the assembly of two or more objects together: the most renowed example in this context is peg-in-hole (PiH) (S\u00f8rensen et al., 2016). Although different in terms of requirements, all of the aforementioned tasks ne-cessitate to control, directly or indirectly, the forces exchanged between the manipulator and the working environment."}, {"title": "2 MOTIVATION", "content": "2.1 Preliminaries\nForce control is an essential requirement in a vast number of applications of utmost importance in a broad spectrum of real-world contexts, ranging from industrial (Lloyd et al., 2024) to medical (Tang et al., 2023b) scenarios. For this reason, force control has been one of the major interests in robotics research in the last decades.\nWith the fundamental objectives of ensuring safety and preserving environment integrity, indirect force control methods, e.g. impedance (Hogan, 1985) and admittance (Newman, 1992) controllers, have been proposed to regulate the manipulator behavior, generalizing pure motion control to interaction sce-narios and providing robots with compliant charac-teristics with respect to the environment they are in contact with.\nUsually, an impedance control law (Figure 1a) can be formulated as\n$f_c = K_px+K_dx,$\n(1)\nwhere $K_p$ and $K_d$ are stiffness and damping param-eters, $x = x_d - x$ is the task-space error between the setpoint $x_d$ and the actual end-effector (EE) position $x$, and $f_e$ is a Cartesian wrench to control via a torque-based controller, which also compensates for the ma-nipulator's internal dynamics, in order to impose a compliant interaction at the EE. On the other hand, admittance control (Figure 1b) takes the form\n$X_c = M_a^{-1}(f_e - K_dx \u2013 K_px)$,\n(2)\nwhere $M_a$ is the mass matrix of virtual mass-spring-damper dynamics EE wrenches $f_e$ (usually measured with a force/torque sensor, mounted at the manipu-lator's flange) are input to. The resulting task-space acceleration $X_c$ is usually commanded to a low-level motion controller.\nAs an additional performance requirement, indus-trial tasks usually demand a desired force to be ac-curately exerted on the working surface: in this case, direct force control (DFC) strategies can be employed to track a reference force (Khatib, 1987). Typical ex-amples of force-related tasks are illustrated in Fig-ure 2, i.e. workpiece deburring (Figure 2a) and sur-face polishing (Figure 2b). Typically, this is imple-mented with a PI loop as\n$X_c = K_{pf} + K_I \\int f dt,$\n(3)\nwhere $K_p$ and $K_I$ are proportional and integral con-trol parameters, respectively, and $f= f_d - f_e$ is the wrench error between the desired wrench $f_d$ and the actual one (Figure 1c).\n2.2 AI-based methods\nOne of the most challenging issues force control al-gorithms have to face is the inaccuracy and unpre-dictability of the environment geometry and dynam-ics. Most of advanced techniques are based on the as-sumption that the environment force follows a simple linear spring model in the form\n$f_e = K_e(x-x_r)$,\n(4)\nwith $K_e$ and $x_r$ being the (unknown) environment stiff-ness and rest position, respectively. However, this assumption does not always hold, especially at high"}, {"title": "3 CHALLENGES", "content": "3.1 Problem definition\nIn literature, PiH is considered as a benchmark for force control strategies applied to contact-rich assem-bly tasks (Jiang et al., 2020). As displayed in Fig-ure 3, it consists of different phases: first, the robot searches for the hole location where the held peg has to be inserted, typically with translational movements (Figure 3a). When the peg engages the hole, the EE rotates to align the former against the latter's walls (Figure 3b). Lastly, the insertion phase actually places the peg into the hole slot (Figure 3c).\nFor this task, inherent difficulties arise, caused by sub-millimetric tolerance between peg's and hole's dimensions, inaccuracies in estimating the hole's ex-act location, and complex forces and torques to be managed at the EE. In recent years, RL has been the most popular approach with which these challenges have been faced (Ji et al., 2024), as it usually does not rely on a specific model, instead trying to optimize motion policies according to a tailored reward func-tion, which is constantly updated as data are collected during the task execution.\n3.2 Stability and safety\nThe first practical objective RL usually struggles to accomplish is guaranteed asymptotic stability, i.e. it does not usually provide a theoretical proof for-mally guaranteeing the RL policy to actually con-verge towards the objective. In (Khader et al., 2021), this paramount problem is analyzed, thus devising RL policies with guaranteed stability. To achieve this feature, a variable impedance controller \u2013 orig-"}, {"title": "3.3 Optimization strategies", "content": "Although PiH is one of the most common assembly tasks, there is still no consolidated methodology to approach it. Indeed, even considering the spectrum of RL-based techniques, various diverse strategies have"}, {"title": "3.4 Reward formulation", "content": "As mentioned in Section 3.1, formulating a significant reward is fundamental for a RL approach to succeed. Indeed, the reward function R should coherently ex-press the goal of the task, so as to drive the optimiza-tion algorithm to yield the optimal policy.\nIn RL-based PiH, the most popular rewards are Euclidean distance to goal position $x_g$, in the form\n$R_g = -||x_g-x||^2,$\n(5)\nand the time to complete the task, expressed as\n$R_r = 1-\\frac{k}{T},$\n(6)\nwhere $k \u2208 N$ is the current time step and $T \u2208 N$ a pa-rameter denoting the maximum number of steps the task should be completed in.\nAdditionally, some penalties may be added for the sake of safety, e.g., a penalty on the norm of the action a:\n$R_a = -||a||^2,$\n(7)\nor a safety penalty to avoid the manipulator to exert excessive forces, i.e.,\n$R_f = \\begin{cases}\n    -P, & \\text{if } ||f_e|| > f_{th}\\\\\n    0, & \\text{otherwise}\n  \\end{cases},$\n(8)\nwhere $f_{th} \u2208 R+$ is the safety threshold and $P \u2208 R+$ is the penalty.\nA unique reward is used by (Johannink et al., 2019), in which the RL action $u_r$ is added as a resid-ual to that of a low-level controller: in particular, with the objective of inserting a peg in a hole between two fixed blocks (see Figure 6), the reward includes a con-tribution to describe how much the \"left\" and \"right\" blocks are tilting from their upright position:\n$R_h = \u2212\u03b1_\u03b8 (|\u03b8_l| + |\u03b8_r|) \u2013 \u03b1_\u03a6 (|\u03a6_l| + |\u03a6_r|)$,\n(9)\nwhere $\u03b8$ and $\u03a6$ represent the hole blocks' tilting an-gles, and $\u03b1_\u03b8, \u03b1_\u03a6 \u2208 R+$ are hyperparameters.\nAs evident, the reward does not always follow a particular formulation: researchers typically tend to design them according to certain safety, accuracy or time requirements. In the next section, we will report what the reference works choose as reward, possibly linearly combining them in the form (e.g. summing (5) and (7))\n$R = \u03b1_g R_g+\u03b1_a R_a,$\n(10)\nwith $\u03b1_g, \u03b1_a \u2208 R+$ being two weights."}, {"title": "3.5 Force-tracking tasks", "content": "Although PiH does not require force tracking, simi-lar strategies and learning frameworks have been ap-plied to pursue this specific requirement as well, as"}, {"title": "3.6 Summary", "content": "In Table 1 we summarize the major differences in the PiH approaches discussed in the previous sec-tions, in terms of (i) reward; (ii) explicit or implicit stability; (iii) RL algorithm\u00b2; (iv) RL policy action; (v) unification of the hole-searching and hole-inser-tion phases; (vi) simulator. Simulators are paramount in rapidly training RL policies, but their fidelity w.r.t. crucial factors such as system dynamics and real-"}, {"title": "4 FUTURE DIRECTIONS", "content": "In the light of the discussions done in Section 3, we now state our position on the topic of empowering manufacturing processes with AI methods. We deem that future research on this subject should concen-trate on consolidating the novel technologies that are rapidly emerging in the latest years, both in force-tracking applications and in contact-rich assembly tasks.\nTo this aim, we suggest to devise common meth-ods to formally compare RL-based techniques, both among them and against standard approaches, defin-ing quantitative metrics according to which existing and novel methodologies shall be validated. For in-stance, possible performance metrics in PiH might be (i) success rate; (ii) amount of exerted forces on the workpiece; (iii) execution time. In this sense, it is required to define a specific reward shape (see Sec-tion 3.4), and to formally state performance require-"}, {"title": "5 CONCLUSIONS", "content": "This paper reported recent advancements on AI methodologies applied to manufacturing robotic tasks. The rationale behind employing these tech-nologies is two-fold. First, in the context of Indus-try 4.0, they can further optimize manufacturing pro-cesses, increasing the production quality, efficiency and throughput. Moreover, they can compensate for inherent limits of classical model-based control meth-ods, as usually happens in challenging force-related and contact-rich applications.\nWe analyzed issues and objectives these meth-ods are demanded to undertake when applied in real-world industrial scenarios, and analyzed the differ-ences in recent relevant research works on this topic, both on methodological and implementation-related aspects. In conclusion, we claimed our position on possible directions future research should pursue, in order to accommodate for specific performance requirements, and proposed suggestions researchers shall potentially follow to increase the relevance of future works on both academic and practical level."}]}