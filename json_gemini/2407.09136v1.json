{"title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "authors": ["Nico Daheim", "Jakub Macina", "Manu Kapur", "Iryna Gurevych", "Mrinmaya Sachan"], "abstract": "Large language models (LLMs) present an opportunity to scale high-quality personalized education to all. A promising approach towards this means is to build dialog tutoring models that scaffold students' problem-solving. However, even though existing LLMs perform well in solving reasoning questions, they struggle to precisely detect student's errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1K stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student errors which are more often correct with less hallucinations compared to existing baselines.", "sections": [{"title": "1 Introduction", "content": "The field of dialog tutoring aims to build systems that can teach students by holding a conversation with them (Wollny et al., 2021; Jurenka et al., 2024). Dialog tutors hold the potential to make personalized teaching available to learners anywhere anytime. The increasing capabilities of LLMs have brought renewed hope to this field (Thoppilan et al., 2022; Jurenka et al., 2024). However, real-time tutoring is quite complex, and human teachers bring various intricate capabilities when teaching, such as identifying student errors in problem solving, picking a pedagogical strategy, and communicating it (Wang et al., 2024c). The same requirements hold for dialog tutoring models which need all these abilities to be effective.\nYet, although research on effective human tutors shows they perform these steps sequentially by first reasoning about the error, then picking a strategy, and then responding (Lepper and Woolverton, 2002), many tutoring models perform all of them in one forward pass. Recent studies (Macina et al., 2023b,a) have shown that this can lead to several deficiencies that can be detrimental to student learning, for example, in math tutoring. Despite impressive performance on math reasoning benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021), dialog tutors often generate hallucinated outputs and present erroneous information to students, for example, because they assess an incorrect solution as correct. We show an example of this in Figure 1.\nIn this paper, we alleviate this problem by decoupling the verification of student solutions from response generation with a modular approach. As opposed to the common approach, the model does not directly generate the tutor response from the students' utterances, whereby solution assessment is done implicitly in the models' activations, but rather receives the output of an additional verification model that assesses solutions and can therefore also be more specialized. We hypothesize that this increases the correctness of the model as well as makes the response more targeted to the error because the response generation module is already aware of the exact student error. Furthermore, this architecture more closely mimics human tutors.\nTo test our approach and train verifiers, we collect a dataset of ca. 1k student solutions and their stepwise reasoning chains in the domain of multi-step math problem-solving, which will be released publicly. This dataset augments the math dialog"}, {"title": "2 Background & Related Work", "content": "Dialog tutoring aims at building models that can tutor human students through a conversation. For example, dialog tutoring has been proposed for second-language acquisition (Stasaski et al., 2020; Caines et al., 2020; Kwon et al., 2024), to answer questions in science (Chevalier et al., 2024), or to solve math word problems (MWPs) (Macina et al., 2023a). In each case, the model should guide the learner to solving a problem (e.g. the MWP or translation of a phrase) not by telling the solution outright, but rather by using scaffolding techniques that give students space for guided exploration and self-correction. For example, the tutor might elicit the students' thinking by asking a question that challenges their understanding of the problem (Reiser, 2004; Anghileri, 2006).\nCapturing such intricate tutoring strategies is hard and requires teachers years to master. Due to this complexity, most previous dialog tutoring systems were human-authored, notably the AutoTutor family (Nye et al., 2014), LISP tutor (Anderson et al., 1985) which uses a large set of rules to verify student programming solutions, or any systems built using CTAT which requires enumerating all possible solutions or writing complex production rules (Aleven et al., 2016). However, scaling such human-authored systems can quickly explode in both complexity and human effort (Macina et al., 2023b). Due to this and rapid progress in language generation from learning large models based on large amounts of data, LLMs such as LearnLM (Jurenka et al., 2024) have recently become popular in favor of human-authored, rule-based systems.\nFormally, the goal of dialog tutoring is to continue a tutoring dialog"}, {"title": "3 Verification-based Response Generation", "content": "We first introduce the task of verification and different verifiers in Section 3.1. Afterwards, in Section 3.2, we combine verification and response generation for modular tutor response generation."}, {"title": "3.1 Verification", "content": "We deal with the verification of student solutions to a given math word problem $q \\in V^*$. The solutions can be described by a sequence of substep solutions $S_q = \\{s_1, ..., s_N\\}$, where each $s_n \\in V^*$ and $s_y$ is the final solution. Usually, $S_q$ is described by the student in one of the student utterances $u_t$. The task of the model is to assess whether $s_y$ is the correct solution to $q$ and if not, potentially, to identify which step $s_n$ caused the error. Oftentimes, this can be done by comparing to a reference solution $\\hat{S}_q = \\{\\hat{s}_1,...,\\hat{s}_M\\}$ that is either given or model-generated and might differ in length. All verifiers which we discuss next can then be described by a learned function $v_\\theta(v \\mid S_q, \\hat{S}_q)$, usually an LLM. Here, $v$ is the verification output and $\\hat{S}_q$ may be an empty string if no reference solution is given. In the following, we introduce different verifiers.\nA comparably simple approach to verification is classifying whether the student solution $S_q$ is correct using a binary classifier. We call this Overall Verification. Similarly, identifying the first error step $s_n$ can be framed as multi-class classification with labels $\\{0,..., N\\}$, where 0 means no mistake. We call this Stepwise Verification. Alternatively, Stepwise Verification (iterative) can be framed as a binary classification for each step $s_n$ whether it is correct. The first error step is the first step classified as error.\nWhile conceptually easy, classification-based approaches locate the first error without explaining the exact issue. Therefore, we propose to use an LLM to directly describe the error, and the concrete first error step, in a textual format, and call this Error Description. For this, we prompt the LLM with the prompt outlined in Appendix G. In comparison to Wang et al. (2024c), this error description is allowed to be free-form and does not map to predefined error types. The LLM-generated error step description can then be passed to a tutor response generation model.\nAs our third verification approach, we align the steps in the student's solution with a reference solution, and compare the steps"}, {"title": "3.2 Response Generation", "content": "Direct generation of tutor responses can be challenging because one model has to reason over the student solution, pick a teaching strategy, and generate a response in one step. This has been shown to produce hallucinations (Macina et al., 2023a). We tackle this by incorporating an additional verification step that informs the response generation model, as previously discussed. Our aim is to split the task into two less complex tasks which should reduce errors if each task can be performed well enough and has been shown to reduce hallucinations in document-grounded dialog (Adolphs et al., 2022) and question answering (Press et al., 2023).\nThe verifier and response generation model are combined in a two-stage approach. First, the verifier outputs a verification $v$ of the student solution $S_q$ based on a reference solution $\\hat{S}_q$. Then, the response generation model is conditioned on $v$, the dialog history $H$, and background knowledge $k$. In our work, $k$ consists of the student solution $S_q$, optionally the reference solution $\\hat{S}_q$, and the math"}, {"title": "4 Data Collection", "content": "We propose and evaluate various verifiers in this work. Since some of them require training data and to evaluate their performance, we collect a dataset of 1,002 human-produced verification outputs to train and evaluate them. This is similar in size to a related corpora (Jacovi et al., 2024). In this section, we describe the annotation task and data collection."}, {"title": "Incorrect Student Solutions Source", "content": "Our work extends MathDial (Macina et al., 2023a) by having teachers annotate incorrect student solutions from the dataset with their first error step. There, these incorrect student solutions were used to condition a student model (INSTRUCTGPT) to generate responses in a dialogue with a human teacher.\nSpecifically, these problems are based on the GSM8k (Cobbe et al., 2021) dataset of multi-step math word problems. In MathDial, the reasoning chains are generated using a 2-shot CoT prompt with GPT-3.5-TURBO, and temperature sampling ($\\tau = 0.7$) is used to get multiple reasoning paths ($n = 50$). Finally, the most common incorrect solution is chosen. Subsequently, their student model is prompted to respond to a human teacher as a student who tries to solve a problem with a particular incorrect solution.\nTo not skew our dataset to errors, we balance it with rephrased reference solutions from the student model. We reproduce the student model prompt from MathDial to generate student responses using the reference solutions. All reference solutions and student responses with incorrect solutions are part of the dataset. Details are in Appendix A."}, {"title": "Student Solution Annotation", "content": "The objective of the annotation is to mark the exact step of the first error in the student solution. We do not annotate error steps after the first one to decrease ambiguity, as they frequently stem from the first error. We recruit teachers through Prolific, who first read the problem and then mark the precise step of the first error in the student solution. Teachers can access the reference solution to reduce task complexity. Details of the task, the user interface, and examples of collected data are in Appendix A. To compute agreement, 10% of the samples are annotated by one additional annotator with an inter-rater reliability of Cohen's $\\kappa = 0.75$ indicating substantial agreement (Cohen, 1960). We show the distribution of incorrect student solution steps in Figure 3."}, {"title": "5 Experiments", "content": "We evaluate different verifiers on our dataset and use them to inform response generation models to improve their correctness. Since we extend MathDial with additional annotations we use MathDial dialogues for evaluating tutor response generation. Besides math problem and student solution in a dialog, we either use a model-generated CoT reference solution if marked by \u201csolution\u201d or no reference solution as input to the models. Next, we detail metrics and models."}, {"title": "5.1 Metrics", "content": "For teacher response generation, we evaluate the generated output $u_t$ of each model by comparing it to a human-annotated response $\\hat{u}_t$ from MathDial. We report standard text generation metrics: the sacrebleu (Post, 2018) implementation of BLEU (SBLEU) to measure word overlap and BERTScore (Zhang et al., 2020) (BF1, using the all-MiniLM-L6-v2 checkpoint) to measure semantic similarity. Moreover, we report the knowledge F1 (KF1) score with respect to the grounding information (correct solution in the case of MathDial) which has been used as a proxy for faithfulness in prior work (Daheim et al., 2024). Similar to (Zheng et al., 2024; Jurenka et al., 2024), we also prompt LLAMA3-70B and use it complementary to human evaluation (the same task and instructions are used in both) to assess how targeted, correct, and actionable a response is. Details about the LLM-based evaluation are found in Appendix E."}, {"title": "5.2 Models", "content": "For both verification and response generation, we use different prompted or finetuned models. For verification, we compare the closed-source model GPT-3.5 to the open models LLAMA2 and LLAMA3. For the latter, we prompt the 70B version of the models and finetune LLAMA2-7B using LoRA. For response generation, we evaluate"}, {"title": "6 Results", "content": "We first show the performance of different verification models in Section 6.1 and then use verification models in response generation in Section 6.2."}, {"title": "6.1 Verification", "content": "In this section, we benchmark LLMs on their ability to evaluate the correctness of student solutions using the Overall Verification and Stepwise Verification approaches from Section 3.1. For Stepwise Verification we use the multi-class classification approach, because it performed better than iterative classification in our experiments. A comparison is found in Appendix B. We measure the F1 score (balanced dataset), in particular, micro F1 for Stepwise Verification (imbalanced dataset, see Figure 3). We find in Table 1 that Overall Verification can be challenging even for state-of-the-art LLMs. All prompted models show comparably low performance when prompted without a reference solution and especially struggle with identifying incorrect responses. Providing a reference solution improves results significantly. However, for Stepwise Verification even the reference solution does not improve micro F1 beyond 0.70. This result is consistent with expert educator-based assessment (Yen and Hsu, 2023) and LLM self-correction results (Huang et al., 2024).\nInterestingly, our dataset can be used effectively for finetuning. Even a smaller LLAMA2-7B model can outperform larger prompted models on Overall Verification, especially when no solution is provided. Potentially, the additional finetuning steps make it easier for the model to also solve the problem before verification. The finetuned Stepwise Verification model outperforms its larger prompted counterpart LLAMA2-70B when no solution is provided. Results for finetuning show a ten-fold cross-validation. Further details are in Appendix H."}, {"title": "6.2 Response Generation", "content": "Next, we show in Table 2 that combining verification and tutor response generation models can improve the quality of the generated responses. We compare the Error Description and Step Alignment verifiers to direct response generation and using the Error Reason (Wang et al., 2024c). There, the error is categorized into either: guess, misinterpret, right-idea, imprecise, not sure, or careless. We use a subset of MathDial, where the student describes their solution to the teacher in the dialog, and generate the following teacher utterance.\nFirst, we prompt GPT-3.5 using the prompt templates from Section 3.1 for comparability. We find that providing only the Error Reason does not improve over the direct baseline in simpler automatic metrics (sBLEU, KF1, BF1) but only in terms of the LLM-based judging. Using the more detailed Error Description which provides the exact mistake of the student gives larger improvements, both in terms of automatic metrics and LLM-based judging. Similarly, we find Step Alignment to be helpful, but to provide less actionable responses. When finetuning with the Error Description, we obtain improvements over the finetuned baseline but they are smaller and do not hold for each metric.\nOur qualitative analysis shows that both Step Alignment and Error Description result in responses that better localize the exact student error. For example, the baseline often assesses the solution wrongly or skips the first error step and instead asks for the solution of a later step. Examples are shown in Table 10 and Table 11. Section 6.3 confirms our results by human evaluation."}, {"title": "6.3 Human Evaluation", "content": "We conduct a human evaluation using teachers as expert annotators. All annotators are recruited on Prolific after manual screening. We assess whether"}, {"title": "7 Ablation Studies", "content": "Next, we provide further ablations, first on the cost function used in the NW algorithm (Section 7.1) and then on the impact of verification before response generation based on its correctness and"}, {"title": "7.1 Alignment", "content": "We compare different cost functions used for the NW Step Alignment algorithm in Table 4. For the comparison, 30 alignments between a student and reference solution were produced by humans and the accuracy of student solution step alignment is measured. As cost functions we use the cosine similarity of Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) embeddings and embeddings from a model trained on Roscoe (Golovneva et al., 2023), as well as a random cost and an indicator function that is 1 when two substeps have the same numerical solution and 0 otherwise. Similarity threshold $t$ and gap cost $c$ are optimized via a hyperparameter grid search, as indicated by $t^*$ and $c^*$. We find that cosine similarity works best and training on relevant math data fine-tuning in Roscoe further improves performance."}, {"title": "7.2 Verification", "content": "Verification correctness is important We find in Table 5 that correct verification is important for subsequent response generation. If it is correct, both targetedness and correctness are strongly improved over when it is incorrect. However, actionability appears to be decreased which indicates less scaffolding and more teacher \"telling the solution\".\nProblem difficulty influences verification Finally, we show in Table 6 that the performance of both verification and our verify-then-generate approach is heavily correlated with the number of reasoning steps that are used in the reference solution of a given math word problem. We use this as a proxy for problem difficulty. First of all, the performance of the LLAMA3 70B Error Description decreases with the number of steps. This is reflected in the decreased correctness and targetedness of the responses of the few-shot prompted LLAMA3 model. For GPT-3.5 we do not find a similar conclusion for the Error Description model but at least targetedness still decreases with the number of steps. For the finetuned model we do not see similar trends but instead find the best performance for problems with four steps, likely because these are more common in the training data."}, {"title": "8 Discussion & Conclusion", "content": "Student errors are key learning opportunities. Tutors should recognize them and precisely guide students with targeted feedback without telling full solution. Motivated by effective teaching practice, we split the task of tutor response generation into two separate steps of verifying the student solution and generating a response.\nTo evaluate our approach, we collect a dataset of around 1k teacher-annotated solutions to augment an existing math tutoring corpus. Our results show"}, {"title": "9 Limitations", "content": "Focus on scaffolding problem-solving The tutoring scenarios which are considered are centered around the student problem-solving stage. In this case, students have prior knowledge, mostly understand the learning topics and practice them. However, different learning scenarios such as direct instruction, building rapport with students, or open-ended discussions are not considered in this work.\nEvaluating student solutions and responding appropriately to a student's mistakes is inherently challenging, even for human teachers. Furthermore, teachers should ideally give adaptive feedback depending on the problem-solving strategy chosen by the student and treat different errors in different ways to uncover any misconceptions (Nye et al., 2014). For example, in math, productive errors present important learning opportunities for students to learn from them (Kapur, 2016; Shaughnessy et al., 2021; Sinha and Kapur, 2021), e.g. by teacher-guided self-correction or targeted instruction, while unproductive errors, such as numerical miscalculations, could be easily resolved using a calculator (Lepper and Woolverton, 2002).\nDifficulty of obtaining student reasoning chains Model-generated reasoning chains might contain the same biases as human students (Opedal et al., 2024). On the other hand, there might be many additional differences from human student reasoning, e.g. students might not always stick to exact math notations or skip some steps in the explanations. However, because such data from students is sensitive, we work with model-generated reasoning solutions and responses.\nFocus on multi-step problems Procedural or multi-step problems are the basis of most of the scientific disciplines, therefore we believe our approach should be general enough to work across any science subject, especially by including retrieval-augmented generation (RAG) from textbooks. However, it is still an open research question whether a similar solution would work for language learning or fact-based problems, and how models perform in languages other than English."}, {"title": "Evaluation is teacher-centered and complemented with an LLM-judge", "content": "Future work should focus on student user studies with AI tutors. However, this requires careful experimental consideration and safety mechanisms. Moreover, assessment of the responses is done exclusively by teachers and therefore future work should consider running assessments of the responses by students."}, {"title": "10 Acknowledgements", "content": "This research work has been funded by a Swiss National Science Foundation award (#201009), a Responsible AI grant by the Haslerstiftung, by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. Nico Daheim acknowledges travel support from ELISE (GA no 951847). Jakub Macina acknowledges funding from the ETH AI Center Doctoral Fellowship, Asuera Stiftung, and the ETH Zurich Foundation. We thank Sankalan Pal Chowdhury, Kumar Shridhar, Shehzaad Dhuliawala, and Justus-Jonas Erker for valuable feedback and discussions."}, {"title": "11 Ethics Statement", "content": "Intended usage The benefits of our dataset are in understanding and designing AI technology to assist teachers and students during the problem-solving stage. Most importantly, the goal of such systems is to not replace human teachers, but rather enhance their capabilities and make them focus on important and human aspects of teaching. We will release the dataset under CC-BY-4.0 license 1 for further usage and exploration by the community. This also adheres to the licensing of MathDial, which we extend.\nData Anonymization and Privacy As the data in education are strictly confidential we obtained approval on the proposal 2 of the collection interface, questions and how long the data will be stored. All participants fill informed consent at the beginning of the annotation and may withdraw without reason at any time. We store only the necessary"}]}