{"title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "authors": ["Nico Daheim", "Jakub Macina", "Manu Kapur", "Iryna Gurevych", "Mrinmaya Sachan"], "abstract": "Large language models (LLMs) present an opportunity to scale high-quality personalized education to all. A promising approach towards this means is to build dialog tutoring models that scaffold students' problem-solving. However, even though existing LLMs perform well in solving reasoning questions, they struggle to precisely detect student's errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1K stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student errors which are more often correct with less hallucinations compared to existing baselines.", "sections": [{"title": "1 Introduction", "content": "The field of dialog tutoring aims to build systems that can teach students by holding a conversation with them. Dialog tutors hold the potential to make personalized teaching available to learners anywhere anytime. The increasing capabilities of LLMs have brought renewed hope to this field. However, real-time tutoring is quite complex, and human teachers bring various intricate capabilities when teaching, such as identifying student errors in problem solving, picking a pedagogical strategy, and communicating it. The same requirements hold for dialog tutoring models which need all these abilities to be effective.\nYet, although research on effective human tutors shows they perform these steps sequentially by first reasoning about the error, then picking a strategy, and then responding, many tutoring models perform all of them in one forward pass. Recent studies have shown that this can lead to several deficiencies that can be detrimental to student learning, for example, in math tutoring. Despite impressive performance on math reasoning benchmarks, dialog tutors often generate hallucinated outputs and present erroneous information to students, for example, because they assess an incorrect solution as correct. We show an example of this in Figure 1.\nIn this paper, we alleviate this problem by decoupling the verification of student solutions from response generation with a modular approach. As opposed to the common approach, the model does not directly generate the tutor response from the students' utterances, whereby solution assessment is done implicitly in the models' activations, but rather receives the output of an additional verification model that assesses solutions and can therefore also be more specialized. We hypothesize that this increases the correctness of the model as well as makes the response more targeted to the error because the response generation module is already aware of the exact student error. Furthermore, this architecture more closely mimics human tutors.\nTo test our approach and train verifiers, we collect a dataset of ca. 1k student solutions and their stepwise reasoning chains in the domain of multi-step math problem-solving, which will be released publicly. This dataset augments the math dialog"}, {"title": "2 Background & Related Work", "content": "Dialog tutoring aims at building models that can tutor human students through a conversation. For example, dialog tutoring has been proposed for second-language acquisition, to answer questions in science, or to solve math word problems (MWPs). In each case, the model should guide the learner to solving a problem (e.g. the MWP or translation of a phrase) not by telling the solution outright, but rather by using scaffolding techniques that give students space for guided exploration and self-correction. For example, the tutor might elicit the students' thinking by asking a question that challenges their understanding of the problem.\nCapturing such intricate tutoring strategies is hard and requires teachers years to master. Due to this complexity, most previous dialog tutoring systems were human-authored, notably the AutoTutor family, LISP tutor which uses a large set of rules to verify student programming solutions, or any systems built using CTAT which requires enumerating all possible solutions or writing complex production rules. However, scaling such human-authored systems can quickly explode in both complexity and human effort. Due to this and rapid progress in language generation from learning large models based on large amounts of data, LLMs such as LearnLM have recently become popular in favor of human-authored, rule-based systems.\nProblem formulation Formally, the goal of dialog tutoring is to continue a tutoring dialog"}, {"title": "3 Verification-based Response Generation", "content": "We first introduce the task of verification and different verifiers in Section 3.1. Afterwards, in Section 3.2, we combine verification and response generation for modular tutor response generation.\n3.1 Verification\nWe deal with the verification of student solutions to a given math word problem $q \\in V^*$. The solutions can be described by a sequence of substep solutions $S_q = \\{s_1, ..., s_N\\}$, where each $s_n \\in V^*$ and $s_y$ is the final solution. Usually, $S_q$ is described by the student in one of the student utterances $u_t$. The task of the model is to assess whether $s_y$ is the correct solution to $q$ and if not, potentially, to identify which step $s_n$ caused the error. Oftentimes, this can be done by comparing to a reference solution $\\hat{S}_q = \\{\\hat{s}_1,...,\\hat{s}_M\\}$ that is either given or model-generated and might differ in length. All verifiers which we discuss next can then be described by a learned function $v_\\theta(v \\mid S_q, \\hat{S}_q)$, usually an LLM. Here, $v$ is the verification output and $\\hat{S}_q$ may be an empty string if no reference solution is given. In the following, we introduce different verifiers.\nClassification-based Verification A comparably simple approach to verification is classifying whether the student solution $S_q$ is correct using a binary classifier. We call this Overall Verification. Similarly, identifying the first error step $s_n$ can be framed as multi-class classification with labels $\\{0,..., N\\}$, where 0 means no mistake. We call this Stepwise Verification. Alternatively, Stepwise Verification (iterative) can be framed as a binary classification for each step $s_n$ whether it is correct. The first error step is the first step classified as error.\nError Description While conceptually easy, classification-based approaches locate the first error without explaining the exact issue. Therefore, we propose to use an LLM to directly describe the error, and the concrete first error step, in a textual format, and call this Error Description. For this, we prompt the LLM with the prompt outlined in Appendix G. In comparison to , this error description is allowed to be free-form and does not map to predefined error types. The LLM-generated error step description can then be passed to a tutor response generation model.\n(Step) Alignment As our third verification approach, we align the steps in the student's solution with a reference solution, and compare the steps"}, {"title": "3.2 Response Generation", "content": "Direct generation of tutor responses can be challenging because one model has to reason over the student solution, pick a teaching strategy, and generate a response in one step. This has been shown to produce hallucinations. We tackle this by incorporating an additional verification step that informs the response generation model, as previously discussed. Our aim is to split the task into two less complex tasks which should reduce errors if each task can be performed well enough and has been shown to reduce hallucinations in document-grounded dialog and question answering.\nThe verifier and response generation model are combined in a two-stage approach. First, the verifier outputs a verification $v$ of the student solution $S_q$ based on a reference solution $\\hat{S}_q$. Then, the response generation model is conditioned on $v$, the dialog history $H$, and background knowledge $k$. In our work, $k$ consists of the student solution $S_q$, optionally the reference solution $\\hat{S}_q$, and the math"}, {"title": "4 Data Collection", "content": "We propose and evaluate various verifiers in this work. Since some of them require training data and to evaluate their performance, we collect a dataset of 1,002 human-produced verification outputs to train and evaluate them. This is similar in size to a related corpora. In this section, we describe the annotation task and data collection.\nIncorrect Student Solutions Source Our work extends MathDial by having teachers annotate incorrect student solutions from the dataset with their first error step. There, these incorrect student solutions were used to condition a student model (INSTRUCTGPT) to generate responses in a dialogue with a human teacher.\nSpecifically, these problems are based on the GSM8k dataset of multi-step math word problems. In MathDial, the reasoning chains are generated using a 2-shot CoT prompt with GPT-3.5-TURBO, and temperature sampling ($T = 0.7$) is used to get multiple reasoning paths (n = 50). Finally, the most common incorrect solution is chosen. Subsequently, their student model is prompted to respond to a human teacher as a student who tries to solve a problem with a particular incorrect solution.\nTo not skew our dataset to errors, we balance it with rephrased reference solutions from the student model. We reproduce the student model prompt from MathDial to generate student responses using the reference solutions. All reference solutions and student responses with incorrect solutions are part of the dataset. Details are in Appendix A.\nStudent Solution Annotation The objective of the annotation is to mark the exact step of the first error in the student solution. We do not annotate error steps after the first one to decrease ambiguity, as they frequently stem from the first error. We recruit teachers through Prolific, who first read the"}, {"title": "5 Experiments", "content": "We evaluate different verifiers on our dataset and use them to inform response generation models to improve their correctness. Since we extend MathDial with additional annotations we use MathDial dialogues for evaluating tutor response generation. Besides math problem and student solution in a dialog, we either use a model-generated CoT reference solution if marked by \u201csolution\u201d or no reference solution as input to the models. Next, we detail metrics and models.\n5.1 Metrics\nFor teacher response generation, we evaluate the generated output $u_t$ of each model by comparing it to a human-annotated response $\\hat{u}_t$ from MathDial. We report standard text generation metrics: the sacrebleu implementation of BLEU (SBLEU) to measure word overlap and BERTScore (BF1, using the all-MiniLM-L6-v2 checkpoint) to measure semantic similarity. Moreover, we report the knowledge F1 (KF1) score with respect to the grounding information (correct solution in the case of MathDial) which has been used as a proxy for faithfulness in prior work. Similar to , we also prompt LLAMA3-70B and use it complementary to human evaluation (the same task and instructions are used in both) to assess how targeted, correct, and actionable a response is. Details about the LLM-based evaluation are found in Appendix E.\n5.2 Models\nFor both verification and response generation, we use different prompted or finetuned models. For verification, we compare the closed-source model GPT-3.5 to the open models LLAMA2 and LLAMA3. For the latter, we prompt the 70B version of the models and finetune LLAMA2-7B using LoRA. For response generation, we evaluate"}, {"title": "6 Results", "content": "We first show the performance of different verification models in Section 6.1 and then use verification models in response generation in Section 6.2.\n6.1 Verification\nIn this section, we benchmark LLMs on their ability to evaluate the correctness of student solutions using the Overall Verification and Stepwise Verification approaches from Section 3.1. For Stepwise Verification we use the multi-class classification approach, because it performed better than iterative classification in our experiments. A comparison is found in Appendix B. We measure the F1 score (balanced dataset), in particular, micro F1 for Stepwise Verification (imbalanced dataset, see Figure 3). We find in Table 1 that Overall Verification can be challenging even for state-of-the-art LLMs. All prompted models show comparably low performance when prompted without a reference solution and especially struggle with identifying incorrect responses. Providing a reference solution improves results significantly. However, for Stepwise Verification even the reference solution does not improve micro F1 beyond 0.70. This result is consistent with expert educator-based assessment and LLM self-correction results.\nInterestingly, our dataset can be used effectively for finetuning. Even a smaller LLAMA2-7B model can outperform larger prompted models on Overall Verification, especially when no solution is provided. Potentially, the additional finetuning steps make it easier for the model to also solve the problem before verification. The finetuned Stepwise Verification model outperforms its larger prompted counterpart LLAMA2-70B when no solution is provided. Results for finetuning show a ten-fold cross-validation. Further details are in Appendix H.\n6.2 Response Generation\nNext, we show in Table 2 that combining verification and tutor response generation models can improve the quality of the generated responses. We compare the Error Description and Step Alignment verifiers to direct response generation and using the Error Reason. There, the error is categorized into either: guess, misinterpret, right-idea, imprecise, not sure, or careless. We use a subset of MathDial, where the student describes their solution to the teacher in the dialog, and generate the following teacher utterance.\nFirst, we prompt GPT-3.5 using the prompt templates from Section 3.1 for comparability. We find that providing only the Error Reason does not improve over the direct baseline in simpler automatic metrics (sBLEU, KF1, BF1) but only in terms of the LLM-based judging. Using the more detailed Error Description which provides the exact mistake of the student gives larger improvements, both in terms of automatic metrics and LLM-based judging. Similarly, we find Step Alignment to be helpful, but to provide less actionable responses. When finetuning with the Error Description, we obtain improvements over the finetuned baseline but they are smaller and do not hold for each metric.\nOur qualitative analysis shows that both Step Alignment and Error Description result in responses that better localize the exact student error. For example, the baseline often assesses the solution wrongly or skips the first error step and instead asks for the solution of a later step. Examples are shown in Table 10 and Table 11. Section 6.3 confirms our results by human evaluation.\n6.3 Human Evaluation\nWe conduct a human evaluation using teachers as expert annotators. All annotators are recruited on Prolific after manual screening. We assess whether"}, {"title": "7 Ablation Studies", "content": "Next, we provide further ablations, first on the cost function used in the NW algorithm (Section 7.1) and then on the impact of verification before response generation based on its correctness and"}, {"title": "7.1 Alignment", "content": "We compare different cost functions used for the NW Step Alignment algorithm in Table 4. For the comparison, 30 alignments between a student and reference solution were produced by humans and the accuracy of student solution step alignment is measured. As cost functions we use the cosine similarity of Sentence-BERT (SBERT) embeddings and embeddings from a model trained on Roscoe , as well as a random cost and an indicator function that is 1 when two substeps have the same numerical solution and 0 otherwise. Similarity threshold t and gap cost c are optimized via a hyperparameter grid search, as indicated by $t^*$ and $c^*$. We find that cosine similarity works best and training on relevant math data fine-tuning in Roscoe further improves performance."}, {"title": "7.2 Verification", "content": "Verification correctness is important We find in Table 5 that correct verification is important for subsequent response generation. If it is correct, both targetedness and correctness are strongly im-"}, {"title": "8 Discussion & Conclusion", "content": "Student errors are key learning opportunities. Tutors should recognize them and precisely guide students with targeted feedback without telling full solution. Motivated by effective teaching practice, we split the task of tutor response generation into two separate steps of verifying the student solution and generating a response.\nTo evaluate our approach, we collect a dataset of around 1k teacher-annotated solutions to augment an existing math tutoring corpus. Our results show that splitting response generation into two steps can result in more targeted and correct responses that better scaffold human learning. We showcase this using both automatic evaluation and human evaluation annotated by teachers, both for prompted and finetuned models."}, {"title": "9 Limitations", "content": "Focus on scaffolding problem-solving The tutoring scenarios which are considered are centered around the student problem-solving stage. In this case, students have prior knowledge, mostly understand the learning topics and practice them. However, different learning scenarios such as direct instruction, building rapport with students, or open-ended discussions are not considered in this work.\nEvaluating student solutions and responding appropriately to a student's mistakes is inherently challenging, even for human teachers. Furthermore, teachers should ideally give adaptive feedback depending on the problem-solving strategy chosen by the student and treat different errors in different ways to uncover any misconceptions. For example, in math, productive errors present important learning opportunities for students to learn from them, e.g. by teacher-guided self-correction or targeted instruction, while unproductive errors, such as numerical miscalculations, could be easily resolved using a calculator.\nDifficulty of obtaining student reasoning chains Model-generated reasoning chains might contain the same biases as human students. On the other hand, there might be many additional differences from human student reasoning, e.g. students might not always stick to exact math notations or skip some steps in the explanations. However, because such data from students is sensitive, we work with model-generated reasoning solutions and responses.\nFocus on multi-step problems Procedural or multi-step problems are the basis of most of the scientific disciplines, therefore we believe our approach should be general enough to work across any science subject, especially by including retrieval-augmented generation (RAG) from textbooks. However, it is still an open research question whether a similar solution would work for language learning or fact-based problems, and how models perform in languages other than English.\nEvaluation is teacher-centered and complemented with an LLM-judge Future work should focus on student user studies with AI tutors. However, this requires careful experimental consideration and safety mechanisms. Moreover, assessment of the responses is done exclusively by teachers and therefore future work should consider running assessments of the responses by students."}, {"title": "10 Acknowledgements", "content": "This research work has been funded by a Swiss National Science Foundation award (#201009), a Responsible AI grant by the Haslerstiftung, by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. Nico Daheim acknowledges travel support from ELISE (GA no 951847). Jakub Macina acknowledges funding from the ETH AI Center Doctoral Fellowship, Asuera Stiftung, and the ETH Zurich Foundation. We thank Sankalan Pal Chowdhury, Kumar Shridhar, Shehzaad Dhuliawala, and Justus-Jonas Erker for valuable feedback and discussions."}, {"title": "11 Ethics Statement", "content": "Intended usage The benefits of our dataset are in understanding and designing AI technology to assist teachers and students during the problem-solving stage. Most importantly, the goal of such systems is to not replace human teachers, but rather enhance their capabilities and make them focus on important and human aspects of teaching. We will release the dataset under CC-BY-4.0 license for further usage and exploration by the community. This also adheres to the licensing of MathDial, which we extend.\nData Anonymization and Privacy As the data in education are strictly confidential we obtained approval on the proposal of the collection interface, questions and how long the data will be stored. All participants fill informed consent at the beginning of the annotation and may withdraw without reason at any time. We store only the necessary"}, {"title": "C Guidelines for Human Evaluation", "content": "The user interface used in the human evaluation is shown in Figure 4 and Figure 5. All the annotators had to complete a training for the task where each of their responses was evaluated and the feedback was provided to them. We used the subset of the annotators from Appendix A with the same selection conditions and the same payment. Before evaluating the quality of responses, annotators are asked to analyze the math problem and the conversation and explain the student error in an open-ended text. To not bias their understanding of the student solution only subsequently the error descriptions from verifiers were annotated with their correctness using these instructions: Does the text above correctly describe the root cause of the first student's mistake? Answer \"No\" if the correct part of the student solution is identified as incorrect. Answer \"No\" if is too general without any further details e.g. 'There is a small mistake'. The exact wording of the annotation questions for evaluating the quality of responses is the following:\nTargeted Does the Teacher point out to the root cause of the student's mistake? Answer 'No' if the Teacher gives the right answer without pointing out the mistake. Answer 'No' if the Student's statement is wrong and the Teacher does not point out the mistake directly. Answer 'Yes' if the Teacher correctly describes the mistake in the student's solution. Answer 'No' if the Teacher addresses the correct part of the student solution. Answer 'No' if response is too general and could be applied to any mistake e.g. 'You made a small mistake'.\nCorrectness Is the Teacher's response factually correct with respect to the reference solution? The teacher should NOT say incorrect information or provide parts of the solution that are NOT correct with respect to the reference solution. Answer 'No' if the Teacher provides parts of a solution that is incorrect or does not guide a student towards the reference solution. Carefully compare the reference solution and the Teacher's response."}, {"title": "D Alignment Details", "content": "To find the best hyperparameters for the Alignment algorithm we run a grid search using values of similarity threshold $t = \\left[0.5, 0.6, 0.7, 0.8,0.9, 0.95\\right]$ and gap costs $c = \\left[-0.1, -0.2, -0.3, -0.5, -0.7, -1.0, -1.2\\right]$. The best hyperparameters are reported in Table 4. The exact models which are used for semantic similarity are SBERT (sentence-transformers/all-mpnet-base-v2) and Roscoe (facebook/roscoe-512-roberta-base).\nWe use the template to transform the output of the algorithm into the textual prompt. In the template, all the steps from the student solution and reference solution are used. Furthermore, the cost of the alignment can be used to filter out student solutions that differ completely from reference solution which we leave for future work. The template is the following:\nMissing steps in student solution: {missing steps}\nUnnecessary steps in the student solution: {unnecessary steps}\nMatching steps: {matching steps}"}, {"title": "E Details on LLM-based Evaluation", "content": "A response is targeted if it targets the students' mistake, correct if it does not conflict with grounding information, and actionable if it provides the student with useful guidance to help the student progress in their solution attempt. In all cases, for each quality dimension, we provide the model with three examples (3-shots). We use the LLAMA3-70B with temperature T = 0 for reproducibility. The task description and the examples are the same as in the human evaluation for instructing the annotators described in Section 6.3. The prompt also"}, {"title": "F Qualitative examples", "content": "In this section, we show qualitative examples to better understand the behavior of verification and verification-based response generation. We first show examples for prompted models in Table 10 and then show examples for finetuned models in Table 11."}, {"title": "G Prompts", "content": "This section provides the exact prompts used in our work. First, we show the prompt used for the baseline, error description-based, and alignment-based response generation models in Fig. 6 and Fig. 9. Verification prompts for Error Description are in Fig. 8 and for Error Reason in Fig. 7. The prompt with 5 examples for the CoT solution generation is in Fig. 10. Then, we show the prompts used for targeted LLM-based evaluation in Fig. 11, correctness evaluation in Fig. 12, and evaluation of how actionable responses are in Fig. 13. To sample responses from models by prompting we use temperature T = 0 for reproducibility."}, {"title": "H Finetuning Details", "content": "We finetune all models by extending the huggingface transformers library and using the checkpoints from the huggingface hub in accordance with the corresponding license agreements.\nFor verification, we finetune LLAMA2 with 7B parameters and using LoRA. We use a learning rate of 1 \u00b7 10-5, linear learning rate decay with 32 warmup steps, a batch size of 2 and train for 6 epochs in total.\nFor response generation, we finetune Flan-T5 3B with LoRA with a learning rate of 1 \u00b7 10\u20135, a batch size of 2 and a total of 10 training epochs. For both tasks, we used NVIDIA A100 80GB GPU and training takes around 3-6 hours for 5 or 10-fold cross-validation."}]}