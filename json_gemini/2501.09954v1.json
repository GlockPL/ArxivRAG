{"title": "AIRCHITECT V2: Learning the Hardware Accelerator Design Space through Unified Representations", "authors": ["Jamin Seo", "Akshat Ramachandran", "Yu-Chuan Chuang", "Anirudh Itagi", "Tushar Krishna"], "abstract": "Design space exploration (DSE) plays a crucial role in enabling custom hardware architectures, particularly for emerging applications like AI, where optimized and specialized designs are essential. With the growing complexity of deep neural networks (DNNs) and the introduction of advanced large language models (LLMs), the design space for DNN accelerators is expanding at an exponential rate. Additionally, this space is highly non-uniform and non-convex, making it increasingly difficult to navigate and optimize. Traditional DSE techniques rely on search-based methods, which involve iterative sampling of the design space to find the optimal solution. However, this process is both time-consuming and often fails to converge to the global optima for such design spaces. Recently, AIRCHITECT V1, the first attempt to address the limitations of search-based techniques, transformed DSE into a constant-time classification problem using recommendation networks. However, AIRCHITECT V1 lacked generalizability and had poor performance in complex design spaces. In this work, we propose AIRCHITECT V2, a more accurate and generalizable learning-based DSE technique applicable to large-scale design spaces that overcomes the shortcomings of earlier approaches. Specifically, we devise an encoder-decoder transformer model that (a) encodes the complex design space into a uniform intermediate representation using contrastive learning and (b) leverages a novel unified representation blending the advantages of classification and regression to effectively explore the large DSE space without sacrificing accuracy. Experimental results evaluated on 105 real DNN workloads demonstrate that, on average, AIRCHITECT V2 outperforms existing techniques by 15% in identifying optimal design points. Furthermore, to demonstrate the generalizability of our method, we evaluate performance on unseen model workloads and attain a 1.7\u00d7 improvement in inference latency on the identified hardware architecture. Code and dataset are available at: https://github.com/maestro-project/AIrchitect-v2.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving domain of deep neural networks (DNNs), hardware acceleration [1]-[3] is essential for the efficient deployment of models across a wide range of platforms, from cloud infrastructures to mobile and edge devices. However, the performance of DNN inference on hardware is dictated by the complex interaction between mapping strategies [4] and allocated hardware resources [5]. This complexity leads to a vast and intricate design landscape, making it difficult to explore and optimize for peak performance.\nIn the past, human experts have manually crafted the design choices based on their insights [6]\u2013[8]. Such endeavors not only require substantial time and resources but also may achieve sub-optimal solutions due to heuristics. Recently, motivated by the success of machine learning (ML) algorithms to perform complex tasks [9], [10], efforts are being made to automate DSE using ML techniques [4], [5], [11].\nMainstream methodologies using ML in DSE automation commonly involve iterative searches of samples and often rely on black-box optimization techniques. For instance, ConfuciuX [12] utilizes reinforcement learning (RL), Gamma [13], DiGamma [14] apply genetic algorithm (GA), and Hasco [15] employs Bayesian optimization (BO) for searching optimal hardware and/or mapping configurations. However, these techniques are often prohibitively time-consuming for large design spaces and the quality of identified designs is largely dependent on sampling efficiency [11], [13].\nTo mitigate the inefficiencies of search-based DSE, recent techniques [5], [16] propose a constant time optimization method by employing different DNN models trained/fine-tuned for DSE to predict the optimal hardware configuration. AIRCHITECT V1 [5] proposed and demonstrated this idea on several hardware and mapping DSE tasks on systolic arrays [17], by training a multi-layer perceptron (MLP)-based classification model. Despite achieving good results on the systolic array design space, the accuracy and generality of this scheme are still less than expected (\u00a7IV). First, it did not address the non-uniform and long-tailed distribution of DSE data (\u00a7II), which significantly impacts the learnability of DNN models. Second, modeling DSE as a classification-only problem significantly prunes the design space with a fixed number of labels, i.e. configuration, and restricts its scalability for larger DSE due to a commensurate increase in model size. And lastly, the design space explored by this technique is small and relatively simple.\nContributions. To address the above issues, we propose AIRCHITECT V2, an enhanced version to enable more accurate, generalizable, and scalable learning-based DSE. We leverage contrastive learning to learn and encode the input feature representations into a uniform and smooth feature embedding space\u00b9. We also propose a novel unified representation namely, Unified Ordinal Vectors [18], that enables joint classification and regression to leverage the unified benefits of both these techniques for DSE to overcome the limitation of the classification-only approach. Finally, we study the applicability of the proposed technique to a sufficiently complicated design space, the hardware resource allocation (for a given workload and mapping) on an accelerator modeled by MAESTRO [19]. Our key contributions can be summarized as follows:\n\u2022 We propose AIRCHITECT v2, featuring (i) a novel encoder-decoder, multi-head self-attention-based model, (ii) contrastive learning approach for uniform and smooth embedding space, and (iii) a unified ordinal vector representation of output, combining classification and regression. (\u00a7III)\n\u2022 Through extensive ablations and experiments, we demonstrate that AIrchitect v2 surpasses existing techniques by an average of 15% in discovering optimal design points and achieves a ~ 1.7\u00d7 improvement in inference performance on the predicted hardware architecture. (\u00a7IV)\n\u2022 We release our MAESTRO [19]-based DSE training dataset comprising of 105 real DNN workloads along with our trained models to advance learning-based DSE research."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Training a DNN model that predicts the optimal design choice enables one-shot inference and mitigates sensitivity to sampling efficiency unlike search-based techniques (Figure 1). AIRCHITECT V1 [5] formulated several DSE tasks for systolic arrays as a classification problem, where its MLP model outputs probability distributions over labels, i.e. encoded design choices. The model is trained on a dataset of DSE input parameters and their corresponding optimal design choice, generated by the Scale-Sim [20] simulator. Given its effectiveness and advantages in the DSE domain, we expand this direction."}, {"title": "B. Contrastive Learning", "content": "Contrastive learning, widely adopted in self-supervised settings [21], [22] has proven to be effective in mitigating overfitting and improving generalization by regularization against negative samples. Recent work [23], [24] has also demonstrated its benefits in smoothening the loss landscape. The core idea of contrastive learning, which is based on the infoNCE loss [25] is, to balance the learning of a data sample (anchor) by aligning the corresponding positive sample pairs and repulsing the negative sample pairs. As we shall demonstrate in \u00a7III, by selecting positive samples from DSE data points belonging to the same class and aligning them in the embedding space, while simultaneously pushing away samples from different classes, contrastive learning promotes the creation of a more uniform embedding space and simultaneously combats the effects of long-tailed distributions [26]."}, {"title": "C. Motivation 1: Non-Uniform Performance Landscape", "content": "As we can observe from Figure 3 (a), the normalized performance (latency) landscape, drawn from the DSE dataset introduced in \u00a7III-A, the distribution is highly non-uniform and non-convex. This makes it particularly challenging for search-based techniques to reach the global optimum due to a high probability of getting trapped within the multiple local minima [23], [24]. Additionally, learning-based techniques also struggle to achieve good performance in the presence of such a non-uniform design space (\u00a7IV). For instance, even insignificant variations in the input features may cause the predictions to have large discrepancies since the model might not have converged to the global optimum. Furthermore, they are also prone to overfitting the training data because such a landscape can cause the model to fit too closely to the specific training data points, reducing its ability to generalize. In this work, we leverage contrastive learning to smoothen the landscape and encode it into a simpler and uniform embedding space."}, {"title": "D. Motivation 2: Long-tailed Data Distribution", "content": "Figure 3 (b), plots the number of data samples for each DSE output design point, based on the same dataset (\u00a7III-A) and using a strategy akin to that described in [5]. We observe that the DSE dataset is imbalanced and exhibits a long-tailed distribution [26], where a small subset of output design points are favored by the majority of data samples, while many other design points are sparsely chosen. Such a data distribution poses a significant challenge for learning-based DSE techniques and impacts performance and generalizability (\u00a7IV). In this work, we extend the advantages of contrastive learning to address the long-tailed data distribution. By employing contrastive learning, the learnt embedding space converges to one where output design points are more uniformly distributed, reducing the imbalance in representation (see Figure 5)."}, {"title": "E. Motivation 3: Classifications v/s Regression", "content": "Depending on how the optimal DSE output is determined, we categorize existing DSE techniques into two broad categories: classification and regression. Classification-based techniques, such as AIRCHITECT V1 [5], partition the design space into a set of predefined classes or labels, each representing a distinct design configuration. It predicts the optimal design point by selecting the most appropriate class from this fixed set, simplifying the search process and better constraining the search space. However, this approach can limit flexibility and scalability when dealing with large or complex design spaces.\nWe take the liberty of categorizing techniques as regression-based when they are capable of predicting DSE output hardware configurations as continuous values, rather than selecting from a predefined set of discrete options as described above. Consequently, under this definition, all search-based techniques [4], [11]-[15] can be interpreted as regression-based. Regression-based techniques [4], [11], [16] are highly scalable since increasing design space size or complexity does not necessitate an increase in model size [11], [16]. However, with large and complex design spaces, these methods result in an unconstrained learning problem [27], which can greatly impact accuracy and increase the risk of overfitting. In this work, we propose a novel representation UOV, or Unified Ordinal Vectors that can leverage the unified benefits of both these techniques while mitigating their specific drawbacks."}, {"title": "III. AIRCHITECT V2", "content": "To delve into the aforementioned challenges in DSE, we select the following task as our target scenario for exploration: hardware resource assignment on MAESTRO [19]-modeled accelerator. This problem, previously also explored by ConfuciuX [12] using RL-based search, has been shown to be a sufficiently complex design space.\nWe translate the DSE task into a learnable formulation, by encoding the design parameters following Table I, modified from [12]. As the DSE inputs, the tenser dimensions for GEMM operation (M, N, K) are numerical integer values, while dataflow is categorical data chosen among: weight stationary [6], output stationary [8], and row stationary [7].\nThe output is the optimal hardware resources for the given per-layer inputs, configured as the number of PEs and L2 buffer size, while L1 buffer size is fixed following the search assumptions in [12]. The dataset is generated by executing ConfuciuX [12] on the randomized input parameters, with the optimization metric (i.e. reward) set as latency.\nThis DSE task forms a large design space of complexity, O(10\u2079) derived as the product of input feature dimensions from Table I. Figure 4 visualizes its significant complexity, exhibiting irregular and non-trivial characteristics that hardly suffice with simple techniques such as decision trees or support vector machines [5]. Moreover, the dataset shows a highly non-uniform performance landscape as well as imbalanced data sample distribution as highlighted in Figure 3."}, {"title": "B. AIRCHITECT V2 Overview", "content": "We present an overview of AIRCHITECT V2 framework in Figure 2. To effectively learn the complex DSE space, we design an encoder-decoder transformer-based model architecture following the structure in [28] (see \u00a7IV for reasoning). AIRCHITECT V2 takes as input workload specifications as outlined in Table I and outputs optimized hardware design configurations that are geared towards improving overall latency and/or energy. Both the encoder and decoder have identical and complementary structures, consisting of L layers of stacked self-attention blocks, a feed-forward network, and a downsampling (encoder) / upsampling (decoder) units [28]. The decoder also has two UOV heads (explained later) corresponding to the two hardware design configurations predicted by the framework.\nThe encoder and decoder in the AIRCHITECT V2 framework decompose the hardware DSE learning and prediction process into two distinct stages. In stage 1 (\u00a7III-C), the encoder is responsible for learning to construct a uniform and smooth intermediate representation of the input design space, and during prediction, identifies a point in this learned embedding space that closely approximates the input specifications. In stage 2 (\u00a7III-D), the decoder learns to process the identified point in the intermediate representation and finally predict the optimal design configuration via UOV."}, {"title": "C. AIRCHITECT V2 Stage 1", "content": "The goal of stage 1, i.e. encoder, is to generate a uniform and smooth intermediate representation of the input design space. To guide the encoder in efficiently learning this intermediate space, we leverage a combined objective consisting of the contrastive term and performance prediction term.\nContrastive Learning. As pointed out in \u00a7II, contrastive learning enables the encoder to learn to create a uniform and smooth embedding space by aligning positive samples together while simultaneously pushing away negative samples. In the context of stage 1 training, for each workload configuration within an input batch (anchor), positive and negative samples correspond to configurations that belong to the same and different UOV buckets, respectively. Inspired by [21], [24], we adopt the infoNCE loss variant [21] of contrastive loss, and augment it to balance the positive and negative samples. The contrastive objective can be defined as,\n$L_{C} = -log \\frac{\\sum_{p^{+}} exp(A_{P}. X_{P^{+}} / \\tau)}{\\sum_{p^{+}} exp(X_{P}. A_{P^{+}} / \\tau) + \\sum_{p^{-}} exp(X_{P}. A_{P^{-}} / \\tau)}$ (1)\nwhere, A is the output embedding representation from the encoder, and p, p+, and p- are the anchor, positive, and negative samples, respectively. \u03c4 is empirically determined to be 0.4.\nPerformance Predictor. Training the encoder with a vanilla contrastive objective will create an embedding space with no semantic meaning [11]. Therefore, we augment the training objective with an L1-based performance prediction loss, $L_{perf}$ to add semantic information to the learnt embedding. The performance here is the optimization goal of DSE, e.g. latency. This design is motivated by earlier work [11], [29], that emphasizes the influence of performance predictors in organizing the embedding space.\nThe final stage-1 objective is, $L_{stage1}$ = $L_{C}$ + $L_{perf}$. During stage 1 training, the encoder is trained with $L_{stage1}$ as the objective, enabling the encoder to learn the embedding space that keeps similar DSE samples close while distancing dissimilar ones (Figure 5) and incorporate enriched semantic information that aligns with the resulting performance. This contributes to the formation of a uniform and smooth intermediate representation space that is also robust to the imbalanced and long-tailed data distribution."}, {"title": "D. AIRCHITECT V2 Stage 2", "content": "Once stage 1 training is complete, we train the decoder in stage 2 while keeping the encoder's weights fixed to prevent the backpropagation of gradients. In this stage, the decoder learns to predict the optimal DSE hardware configuration given a point in the embedding space identified by the encoder. Unlike previous works [5], [11], [16] the decoder is augmented to predict our proposed UOV through UOV heads (Figure 2) which are simple feed-forward layers that learn to predict UOV s guided by the stage 2 objective. Since the DSE space explored in this study has two configurations, i.e. the number of PEs and buffer size, the decoder has two UOV heads.\nUnified Ordinal Vectors (UOV). The UOV representation [18] scheme enables embedding the large-scale classification labels into reduced-size and scalable representation via \"bucketization\". Based on the scheme, the model jointly and implicitly predicts the classification bucket while regressing to the actual DSE configuration within each bucket. The classification bucket consists of ranges of DSE points. We employ Space Increasing Discretization [30] for the given DSE space and obtain K discretized buckets, A = {0, 1, ..., K-1}. The higher the value of K, the lower the range of DSE points covered by each bucket. Following algorithm 1, any DSE configuration D can be encoded as a K-length UOV such that,\n\u2022 Assuming D lies in bucket r\u2099\n\u2022 Bucket values preceding r\u2099 are non-zero and monotonically increasing (algorithm 1 line 3)\n\u2022 Bucket values following r\u2099 are zero (algorithm 1 line 6)\nAs a result, the final UOV (O) of a given D is,\n$O_{i}=\\begin{cases} {\\frac{i-f(|D-r_{i}|)}{D>r_{i}}} \\\\\n0, & {otherwise}\n\\end{cases}$ (2)\nwhere f can be any choice of monotonically increasing function (we select the exponential function in algorithm 1). Figure 6 intuitively visualize this process, and decoding the UOV back to the actual DSE configuration is the exact reverse of algorithm 1. Our proposed unified ordinal representation captures essential ordering information and is well-suited for the model to learn and predict thanks to its regular structure. For evaluation, we empirically set K as 16 (see \u00a7IV-D).\nDecoder Training. The decoder and UOV heads are trained with the same data used for stage 1. By leveraging the transformer decoder as the backbone, each UOV head is trained to predict the corresponding hardware configuration by unifying (1) classification to identify range buckets and (2) regression for finer-granularity search within the bucket."}, {"title": "Unification Loss", "content": "We make use of the Unification Loss ($L_{o}$) as the primary training loss. Due to the nature of the UOV, we adopt a loss function similar to [31] to guide stage 2 training,\n$L_{o}=\\sum_{i=0}^{K-1} \\begin{cases} {\\alpha u_{i} -u_{i}BCE(u_{i},q_{i}),if q_{i}>0} \\\\\n{(1-\\alpha u_{i}BCE(u_{i},q_{i})otherwise}\n\\end{cases}$ (3)\nBCE(u\u1d62, q\u1d62) = -q\u1d62log(u\u1d62) \u2013 (1 \u2013 q\u1d62)log(1 \u2013 u\u1d62) (4)\nwhere, BCE corresponds to Binary Cross-Entropy, u, q are the predicted and ground-truth UOV s respectively, and \u03b1 = 0.75, \u03b3 = 1 are empirically determined. This formulation for the unification loss penalizes the predictions buckets farther from the ground-truth bucket more heavily than those closer to the ground-truth. In addition, it penalizes the actual likelihood/regression within a predicted bucket to simultaneously ensure accurate bucket prediction and correct estimation of the actual DSE point within the bucket."}, {"title": "E. AIRCHITECT V2 Deployment Pipeline", "content": "AIRCHITECT V2 is trained and inferred on a per-layer basis, recommending the optimal hardware resources for single-layer execution. For model-level deployment, we mention two methods (which apply to any general layer-granularity DSE). Given a model with N layers, M = {L\u2080, L\u2081, ..., L\u2099}, assume AIRCHITECT V2 has recommended HW = {HWL\u2080, HWL\u2081, ..., HWL\u2099 } for each layer. We can determine the final hardware configuration HWM from either:\nMethod 1. For each HWL\u1d62 in HW, estimate the model-wise latency (we use MAESRO [19] in this work) across all layers in M. Select the HWL\u1d62 that yields the minimum as HWM.\nMethod 2. Identify the bottleneck layer Ln among L\u1d62 that results in the largest latency when executed on its recommended HWL\u2099. Choose the HW\u2099 as HWM."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "Implementation. The AIRCHITECT v2 framework is implemented in Pytorch and evaluated on the DSE task introduced in \u00a7III-A with a dataset consisting of 100K samples, split into 80K for training and 20K for testing. We train AIRCHITECT v2's stage 1 and stage 2 individually for 500 and 100 epochs, respectively. All experiments were conducted on a single NVIDIA H100 GPU. The access to the dataset, scripts for training, and the pre-trained encoder and decoder models are provided in https://github.com/maestro-project/AIrchitect-v2.\nBaselines. We compare AIRCHITECT v2 framework against existing SoTA learning-based techniques, including the MLP-based AIRCHITECT V1 [5], generative adversarial network GANDSE [16] and variational autoencoder VAESA [11] combined with a search-based technique (BO), which are trained and evaluated on the same dataset (\u00a7III) for fair evaluation."}, {"title": "B. Layer-level Prediction Accuracy", "content": "As shown in Table III, AIRCHITECT V2 demonstrates considerable improvement in prediction accuracy over other baselines. In particular, the shallow MLP model and classification head used in AIRCHITECT V1 cause significant overfitting to the training data and inability to handle the complexities of the DSE landscape and data distribution, resulting in the lowest accuracy of 77.60%. Although GANDSE achieves higher accuracy than AIRCHITECT v1, it is still limited by the large unconstrained learning problem due to its generative approach, impacting the quality of DSE outputs. In contrast, AIRCHITECT V2 achieved a notably high accuracy of 91.17%, benefiting from our solutions outlined in \u00a7III-B, \u00a7III-C, and \u00a7III-D."}, {"title": "C. Model-level Deployment Evaluation", "content": "We further assess the performance in practical model deployment on representative DNN and LLM models [32]-[34], which were never seen during training. Figure 7 compares the model-level latency achieved by various DSE techniques mentioned in \u00a7IV-A. AIRCHITECT v2 consistently outperforms others across workloads in identifying the hardware configuration with the lowest latency. Particularly, AIRCHITECT V1 and GANDSE [16] achieve poor performance due to overfitting and solutions being trapped in local optima, as they lack addressing the non-uniform and non-convex DSE performance landscape. VAESA with BO [11] is the only method that achieves performance close to ours, as it is able to construct a continuous and low-dimensional latent space through a variational autoencoder (VAE). However, as demonstrated in \u00a7IV-D, the embedding space generated through contrastive learning is superior to VAE."}, {"title": "D. Ablations", "content": "Encoder Training Loss. We investigate the impact of contrastive loss and performance prediction loss to the training, as shown in Table II. Without both objectives (and using only an L2-loss term), the model struggles to handle the non-uniform DSE landscapes and skewed data distributions, resulting in low accuracy similar to AIRCHITECT V1. Incorporating contrastive loss significantly alleviates these issues, leading to a substantial increase in prediction accuracy (+10.54%). The addition of performance prediction loss further enhances learning (+1.2%) by providing semantic information to the learnt feature embedding, achieving the highest final prediction accuracy.\nImpact of Contrastive learning. To study the effectiveness of the proposed contrastive learning on DSE feature embeddings, we further evaluate the constructed embedding space. Following [11], we adopt BO to search from both the embedding space constructed by contrastive learning and the VAE-generated latent space [11]. We train a separate decoder for each technique that converts a point from the latent space into a hardware configuration, and estimate the corresponding latency using MAESTRO [19]. As observed in Figure 8 (a), searching within our contrastive embedding space leads to significantly faster convergence and lower latency compared to the VAE-generated embedding space, implying a more uniform and smoother performance landscape.\nUOV v/s Classification. Figure 9 compares the effectiveness of the proposed UOV formulation against the conventional classification approach, for both AIRCHITECT v1 and AIR-CHITECT V2. We observe that in both scenarios, UOV formulation improves prediction performance because classification overly discretizes and constrains the design space, while UOV combines the benefits of classification and regression, enabling fine-grained prediction. Additionally, UOV significantly reduces model size, which highlights its applicability in larger design spaces. By demonstrating the advantages of UOV for two different techniques, we show that it is not specific to AIRCHITECT v2 and can be adapted for similar benefits in other methods.\nUOV Hyperparameter Evaluation. Increasing the number of UOV buckets improves accuracy through finer-granularity prediction or reduced discretization, but also increases model size due to the larger output vector. As shown in Figure 8 (b), model size (green) grows along with the number of buckets, while accuracy (blue) begins to saturate beyond 16 buckets. We select 16 UOV buckets for our DSE learning to achieve the optimal trade-off between accuracy and model size. Notably, as the number of buckets increases, the problem shifts toward classification, while a single bucket reverts it to regression!"}, {"title": "V. RELATED WORKS", "content": "Search-based Optimizations Approaches. [13], [14] utilize GA over genomes encoding design points. [12] uses RL for coarse-grained search, followed by GA for fine-tuning. [15] performs a two-step optimization combining multi-objective BO with Q-learning algorithms. [35] employs a differentiable surrogate model to guide sampling via input gradients.\nSupervised Learning-based Approaches. [5] trains an MLP model to predict optimal design choices on systolic arrays, framing the DSEs as a classification problem. [16] trains a GAN that generates design points to meet user-specified objectives, in a higher-dimensional design space. [11] focuses on DSE feature embedding space by training a variational autoencoder."}, {"title": "VI. CONCLUSION", "content": "Current supervised learning-based DSE studies insufficiently addressed DSE-specific challenges in the dataset and learning formulation. In this paper, we propose AIRCHITECT V2, which employs an encoder-decoder transformer model to learn the complicated DSE space and leverages contrastive learning and UOV representation to tackle the non-uniform embedding space and long-tailed data distribution. AIRCHITECT V2 improved prediction accuracy on the test set by ~15% over competing baselines, while also achieving 1.7\u00d7 higher performance of identified designs on unseen workloads."}]}