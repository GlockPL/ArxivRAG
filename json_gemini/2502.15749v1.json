{"title": "TCProF:Time-Complexity Prediction SSL Framework", "authors": ["Joonghyuk Hahn", "Hyeseon Ahn", "Jungin Kim", "Soohan Lim", "Yo-Sub Han"], "abstract": "Time complexity is a theoretic measure to determine the amount of time the algorithm needs for its execution. In reality, developers write algorithms into code snippets within limited resources, making the calculation of a code's time complexity a fundamental task. However, determining the precise time complexity of a code is theoretically undecidable. In response, recent advancements have leaned toward deploying datasets for code time complexity prediction and initiating preliminary experiments for this challenge. We investigate the challenge in low-resource scenarios where only a few labeled instances are given for training. Remarkably, we are the first to introduce TCProF: a Time-Complexity Prediction SSL Framework as an effective solution for code time complexity prediction in low-resource settings. TCProF significantly boosts performance by integrating our augmentation, symbolic modules, and a co-training mechanism, achieving a more than 60% improvement over self-training approaches. We further provide an extensive comparative analysis between TCProF, ChatGPT, and Gemini-Pro, offering a detailed evaluation of our approach.", "sections": [{"title": "Introduction", "content": "The task of predicting time complexity for code snippets represents a significant challenge in programming efficiency analysis. Time complexity is a crucial benchmark for evaluating algorithm performance across diverse computational domains. However, accurately computing the time complexity of a code snippet remains theoretically undecidable (Asperti, 2008), presenting a substantial obstacle. This issue is particularly crucial in environments such as educational settings, programming competitions, and automated code reviews, where an accurate evaluation of numerous solutions is essential.\nIn the meantime, the advent of deep learning methodologies presents a promising avenue to address this challenge. Sikka et al. (2020) introduced CorCoD dataset, specifically designed for code time complexity prediction. The dataset consists of code snippets labeled with their time complexity classes. They also provide initial experiments for the code time complexity prediction using both conventional algorithms and basic neural models. Despite these advances (Baik et al., 2024; Moudgalya et al., 2023), the efficiency of such models heavily relies on the availability of extensively annotated datasets. Unfortunately, datasets in this domain are currently scarce due to the problem of data scarcity, as time complexity annotations require professional knowledge. While there are further approaches (Baik et al., 2024; Moudgalya et al., 2023) with considerable potential, their effectiveness is contingent upon the size of annotated datasets.\nAddressing the shortage of labeled data, we introduce several innovations. We develop data augmentation techniques specifically tailored to identify key factors that influence the time complexity of the code snippets. We also incorporate a co-training mechanism that leverages both original and augmented data effectively. Additionally, we construct a symbolic module that enhances the accuracy of pseudo-labels compared to the pseudo-labels generated by a model-based approach alone. Collectively, these components form the backbone of TCProF, our semi-supervised learning (SSL) framework, which is uniquely equipped to address code time complexity prediction in low-resource settings.\nOperating under the assumption of limited labeled data and a vast number of unlabeled code snippets, we empirically analyze TCProF using publicly available datasets, CorCoD and CodeComplex (Baik et al., 2024). CorCoD includes code"}, {"title": "Related Works", "content": "Data scarcity is a critical problem for various tasks. More specifically, the problem occurs when there are only a few labeled data even though there are tons of unlabeled data. Generating labeled data is costly, making research in these low-resource environments crucial. Semi-supervised learning (SSL) offers an effective solution in such scenarios (Sajjadi et al., 2016; Xie et al., 2020; Chen et al., 2020, 2022; Sohn et al., 2020; Zhang et al., 2021a; Wang et al., 2023a; Zou et al., 2023; Huang et al., 2023; Nie et al., 2024).\nOne of the popularly used SSL techniques, self-training is a learning mechanism that trains the student model with a few-shot labeled dataset (Geng et al., 2019; Bao et al., 2020; Zhang et al., 2021b) and then subsequently acts as a teacher model for generating pseudo-labels (Lee, 2013). Pseudo-labels are judged based on the predictions of the model for a given unlabeled data. Co-training (Blum and Mitchell, 1998) is also the successful SSL mechanism that simultaneously employs two networks. Jointmatch (Zou and Caragea, 2023) utilizes cross-labeling, inspired by co-training, that uses an additional loss based on pseudo-labels for more reliability instead of augmenting pseudo-labels to the initial labeled dataset for additional training. While this approach has been effective to some degree, it lacks reliability as we cannot guarantee that the model generates 'correct' pseudo-labels. Recent approaches have incorporated symbolic modules to enhance the reliability of pseudo-label generations or data augmentation (Hahn et al., 2021; Kim et al., 2022).\nLikewise, data augmentation is also one of the fundamental methods that is effective in the low-resource setting. Data augmentation generates artificial data from the original dataset without changing their labels. Conventional data augmentations are synonym replacements, word insertion or deletion (Wei and Zou, 2019) More advanced methods involve Back-Translation (Edunov et al., 2018) and these days, LLMs have gained popularity in generating various data but with accurate labels. We utilize these insights to develop TCProF, an SSL"}, {"title": "Code Time Complexity Prediction", "content": "Computation of code time complexity has long been a theoretically undecidable problem whereas classifying the code time complexity is a recently emerged problem. Sikka et al. (2020) first proposed this task and presented a labeled dataset with five time complexity classes. They propose a dataset named CorCoD, composed of Java codes with O(1), O(log N), O(N), O(N log N) and O(N^2) time complexity classes and experimental results on the time complexity classification with baseline neural models.\nSimilar to CorCoD, CODAIT\u00b2 tried to create good code embeddings by capturing manual features such as number of loops and breaks, and utilized graph-based representations for predicting time complexities. Afterward, Moudgalya et al. (2023) proposed TasTy, consisting of Java and Python data, and Baik et al. (2024) proposed CodeComplex, which consists of also Java and Python data with additional labels. CodeComplex consists of seven different time complexity classes, O(N\u00b3) and O(2^N) in addition to those of CorCoD.\nWhile time complexity prediction has been explored in these datasets, the scarcity of labeled data remains a significant challenge. Unlike runtime-based datasets from online judge platforms, which can be influenced by hardware, input distributions, and implementation-specific optimizations (Ishimwe et al., 2021; Zhang et al., 2023), these datasets provide explicit theoretical complexity labels for the code snippets. However, annotating such datasets requires expert knowledge, making them inherently low-resource. We propose TCProF, the SSL framework designed to alleviate this data scarcity challenge and enhance the accuracy of time complexity prediction."}, {"title": "Methodology", "content": "TCProF is a robust SSL framework designed for code time complexity prediction in a low-resource setting illustrated in Fig. 1. TCProF comprises three primary components:\nAugmentation module (AUG): This module employs augmentation involving loop representation conversion (LC) and back-translation conversion (BT), and a combined ensemble of these methods to enhance the diversity of augmented data.\nTraining mechanism: Utilizing a co-training approach, TCProF trains two models simultaneously; one with original data and the other with augmented data. This mitigates the error accumulation problem of self-training.\nPseudo-label module: Integrating our symbolic module (Sym) with the classifier, this module generates more precise pseudo-labels.\nAlgorithm 1 describes a detailed procedure of our framework."}, {"title": "Self Training", "content": "In our experiments, we have a dataset T for training, a dataset V for validation and a dataset E for evaluation. we split T into a labeled dataset L = {l\u2081,l\u2082,...,l\u2098} of size M and an unlabeled dataset U = {u\u2081, u\u2082, ..., u\u2099 } of size N for self-training:\nL = {l | l = (d, c)}, U = {u | u = (d,  )}, where d is a code and c is a complexity class for d. A in u represents that u does not contain a complexity class for d.\nFor the first iteration, we train the baseline model B with L. With the trained B, we predict the complexity class c for d in each u \u2208 U. If the confidence score s\u1d64(c) of u for c passes the predefined threshold \u03b8, we pseudo-label each u with its corresponding label c\u1d64 and then add it to L:\nL' = L + {u | u = (d, c\u1d64) where s\u1d64(C\u1d64) \u2265 \u03b8}.\nFor the next iterations afterward, we train B with the updated labeled dataset L' and then pseudo-label the unlabeled data to update L' again."}, {"title": "Co-Training", "content": "We mitigate the error accumulation problem of self-training by implementing co-training as our learning mechanism. Our implementation of co-training involves two different models B and B\u2090\u1d64g which are trained with two different datasets L and L\u2090\u1d64g. Likewise to the self-training in Section 3.2, L is a labeled dataset of size M. L\u2090\u1d64g is a labeled dataset of size M generated by our data augmentation strategy in Section 3.7:\nL\u2090\u1d64g = {l\u2090\u1d64g | l\u2090\u1d64g = (d\u2090\u1d64g, c), where d\u2090\u1d64g is augmented from d in l = (d, c)}."}, {"title": "Model-based Pseudo-Labels", "content": "The conventional pseudo-labeling procedure is done by prediction of B. The confidence score of unlabeled data with its corresponding class that B outputs is the one and only component for model-based pseudo-labels. We can adjust the threshold value for more precise pseudo-labels."}, {"title": "Symbolic Pseudo-Labels", "content": "We introduce a computational module, designed to predict the temporal complexity of source code through symbolic analysis, employing the computational results as pseudo-labels to enhance the accuracy of predictions. We have named this module, the symbolic module Sym. The symbolic module Sym is designed to predict the time complexity of code by identifying specific patterns and structural elements, without relying on neural networks. This approach effectively complements language models that, by their nature, do not account for the hierarchical organization inherent in programming code (Allamanis et al., 2018; Chen et al., 2021; Zhang et al., 2022). The module primarily employs Regular Expressions (Regex) and Abstract Syntax Trees (ASTs) as essential tools for a detailed analysis of the source code. This aims to describe the time complexity of iterations, function calls, recursive and iterative calls, and additional relevant constructs, thereby facilitating a comprehensive understanding of the code's structure and execution flow. Our module outputs a formula consisting of the above components and regarding the size of inputs in the source code, subsequently aggregating these elements to categorize the time complexity class of the source code.\nOur approach is systematically organized into five distinct phases and is illustrated in Figure 2:\nFunction Extraction: We employ Regex to identify and extract function definitions from source code. Each function forms the basic unit of analysis as an independent code block. If the source code does not contain functions, this step is omitted.\nIdentify Loops and Recursions: We utilize Regex to identify the presence of repetitive statements (such as for loops and while loops) and recursive functions to determine the frequency of loop iterations and function calls.\nDetect Specific Time Complexity (TC): We utilize ASTs to detect operations that modify the input size. By identifying the presence of sorting or binary traversal, we classify source codes according to their time complexities such as O(log N) and O(N log N).\nCalculate Total TC: The final time complexity is calculated by summing the complexity of each iteration and function call, based on the patterns identified using Regex, and the code structure analyzed by ASTs. In this step, we calculate the overall complexity, ensuring a robust analysis of recursive relationships within the source code.\nTC Classification: The calculated time complexity is categorized into the pre-defined complexity classes. These assigned classes are utilized as pseudo-labels.\nIn Appendix H, we present Figure 9 as a running example of the symbolic pseudo-labeling."}, {"title": "Merge Pseudo-Labeling", "content": "We have two pseudo-label modules based on model confidence and the symbolic module. Illustrated in Figure 1 and Algorithm 1, we first pseudo-label the unlabeled data by the model confidence. Then, we use the symbolic module to pseudo-label the unlabeled data that failed pseudo-labeling by the model confidence."}, {"title": "Data Augmentation", "content": "We introduce a data augmentation module AUG, designed to complement the lack of labeled data. Our augmentation strategies leverage the ChatGPT API, specifically employing the gpt-3.5-turbo-0125 model, to augment our experiments' CorCoD and CodeComplex datasets. The objective is to create precise augmentations that respect the intrinsic properties of the code, ensuring semantic integrity while introducing syntactic variability. We ensure that the augmented code snippets are free from syntactic errors, reinforcing their reliability for further analysis. We present two augmentation methods specifically designed to augment code snippets that preserve the original time complexity:\nBack-Translation (BT): This method involves translating a code snippet into another programming language and then back to the original language to maintain its semantic essence. For instance, Java code snippets are translated into Python and then back into Java. This process gives syntactic variation while retaining the context.\nLoop-Conversion (LC): Loop structures are the primary components that determine the time complexity of code snippets. This technique modifies the loop structures to different but semantically equivalent forms. Using regular expressions, we filter codes containing \u201cfor\u201d or \u201cwhile\u201d loops and then convert these loops by employing For2While and While2For transformation rules, preserving the original logic of the code snippet. For instance, a while loop can be converted into a for loop (While2For) and vice versa (For2While), depending on the context. If the original code snippet contains both for and while loops, we leverage While2for and For2while respectively.\nThese augmentation methods are detailed in the prompts listed in Appendix K and are integrated into TCProF to enhance the robustness. The augmented data from BT and LC methods supplement the initial labeled data in three distinct experimental configurations: 1) AUGBT incorporates back-translation augmented data, 2) AUGLC incorporates loop-conversion augmented data, and 3) AUGBT+LC combines both back-translation and loop-conversion augmented data.\nThere are two experimental setups for AUGLC as there are code snippets without any loop structures: 1) AUGLC_Natural uses naturally sampled data as the initial labeled data without specific preconditions and 2) AUGLC_Artificial selects initial labeled data specifically containing loop structures to maximize the use of augmented data.\nThese configurations for AUGLC are also applied to the combined augmentation strategy AUGBT+LC, with corresponding AUGBT+LC_Natural and AUGBT+LC_Artificial settings. Table 2 employs AUGBT+LC_Natural as"}, {"title": "Implementation Details", "content": "We use UniXcoder (Guo et al., 2022) and CodeT5+ (Wang et al., 2023b) as our baselines for TCProF. We assign the batch of size 7, the number of epochs as 20, and set the learning rate of co-training as 1e-5 and 2e-6. We use 1e-5 learning rate for self-training. Our confidence score threshold \u03b8 is 0.7. We conduct experiments using the NVIDIA RTX 3090 for training TCProF."}, {"title": "Experimental Setup", "content": "We use CodeComplex (Baik et al., 2024) and CorCoD (Sikka et al., 2020) datasets. CodeComplex consists of 4,900 Java and 4,900 Python codes, and CorCoD consists of 929 Java codes. We follow Baik et al. (2024) to split train and test data. For low-resource settings, we perform 5- and 10-shot experiments where we pick 5 and 10 data for each label from the train dataset and use the remaining train dataset as an unlabeled dataset for pseudo-labels, respectively.\nCode time complexity prediction is a recent code-related task and we are the first to perform the task in low-resource scenarios. We use four pre-trained code language models, CodeBERT (Feng et al., 2020), GraphCodeBERT (Guo et al., 2021), UniXcoder (Guo et al., 2022), and CodeT5+ (Wang et al., 2023b) as our baseline models.\nWe also include JointMatch as an SSL baseline for comparison. Recently, Zou and Caragea (2023) proposed JointMatch for a state-of-the-art SSL framework for low-resource settings, where the authors employ cross-labeling to enhance pseudo-labeling. While JointMatch is effective for text classification tasks in low-resource settings, we analyze whether it is effective for code time complexity prediction in low-resource settings and compare the performance with our framework TCProF."}, {"title": "Results and Analysis", "content": "We present our main results in Table 1, indicating that our model outperforms baselines. In Section 5.1, we suggest possible intuitions that can be derived from the performance result of Table 1. We demonstrate ablation studies in Section 5.2 and provide error analyses in Section 5.3. Then, for the extensive analyses of TCProF, we show how TCProF approaches the fully fine-tuned models in Section 5.4 and provide comparisons with commercial LLMs in Section 5.5."}, {"title": "Comparison with Baselines", "content": "Table 1 presents the Self-Training (ST) results for the base models, UniXcoder and CodeT5+. The table illustrates that the average accuracy and standard deviation scores for ST of base models, which serve as baselines, are generally low. Specifically, on the CodeComplex datasets, the accuracy scores of these baselines are below 40%, except for ST (UniXcoder) on CodeComplex (Python). Furthermore, the high standard deviation scores indicate that the performance of these models is unstable and unreliable.\nOur augmentation, symbolic pseudo-labeling, and co-training strategies mitigate this problem. We pick UniXcoder and CodeT5+ as the baseline model to implement our strategies\u00b3. TCProF out-"}, {"title": "Ablation Studies", "content": "In our ablation studies, delineated within Table 2, we evaluate the impact of each module within TCProF, as introduced in Figure 1 and Section 3. Our framework integrates the augmentation module AUG, the symbolic module Sym, and implements both self-training and co-training strategies for code time complexity prediction in low-resource settings. The underlying hypothesis points that each component incrementally improves upon basic self-training baseline models. The empirical result from Table 2 confirms our hypothesis, demonstrating performance enhancements in our baselines, CodeT5+ and UniXcoder, as additional modules are integrated.\nParticularly, the interaction between AUG and Sym shows a clear synergistic effect. While AUG alone significantly boosts performance compared to the self-training baselines, it mostly shows high standard deviation scores, which are then relaxed when applied with Sym. This is especially noticeable in CodeComplex (Python). AUG is effective considering its enhancements from the performance of self-training baselines in Table 1, but it also shows high standard deviation scores, mostly over 10%. However, this problem is substantially mitigated when AUG is combined with Sym, reducing the average standard deviation to 5%.\nThis enhancement is further developed when we implement the co-training strategy, solidifying not only the accuracy but also reducing the standard deviation. These observations are particularly pronounced in Python datasets rather than Java datasets. The variance between Python and Java can primarily be attributed to the differential effectiveness of AUG across these programming languages. The less stringent syntax of Python compared to Java allows greater variability in augmented Python code snippets, contributing to this performance boost. This phenomenon is further supported by Tables 11 and 12 in Appendix J. Despite these differences, a closer analysis of the F1-scores in Table 6 shows that both languages exhibit substantial improvements, each around 50%.\nFurthermore, implemented all together with a co-training strategy, TCProF strengthens the per-"}, {"title": "Error Analysis", "content": "Furthermore, we examine the types of errors to which our method is particularly vulnerable for extensive analysis. Figure 3 illustrates that our approach is especially weak on O(N\u00b2) class. We notice that the codes with this class are mostly predicted as O(N). Codes in both classes involve loops and usually the only difference is the depth of the loops. However, precisely, the core factor that makes the difference is the loops involving the input length. For instance, codes in O(N) class can contain multiple-depth loop statements where only a single loop statement involves the length of the input. The model needs to precisely determine whether the iteration number of each loop is proportional to the input size for the correct prediction. Likewise, the class O(N logN) involving both loops linear and logarithmic to the input size and the class O(2^N) involving exponential iterations are also relatively erroneous compared to the other classes. We present examples of code instances, demonstrating the errors in Appendix I."}, {"title": "Comparison to the full-train result", "content": "TCProF demonstrates a promising result in a few-shot settings, improving the baselines on a large scale. We conduct further experiments on comparing the best 10-shot performance of TCProF to the baselines trained with the whole train datasets in Table 3. For CodeComplex datasets, TCProF(UniXcoder) exceeds the accuracy of all the baselines except for CodeT5+ and the performance gap with CodeT5+ is small. Take CodeComplex (Python) for instance, TCProF(UniXcoder) achieves 70.29% accuracy and the accuracy of CodeT5+ fine-tuned with the full train dataset is 72.88%. Our approach catches up to 96.45%."}, {"title": "Comparison with LLMs", "content": "LLMs are known to be effective in general NLP tasks and we extend the analysis into comparing our performance with LLMs. We evaluate the LLMs in"}, {"title": "Conclusion", "content": "Code time complexity prediction remains a largely unexplored yet critical task, and our framework, TCProF, is the first attempt to tackle this challenge within low-resource environments. We have developed TCProF as a robust SSL framework for predicting code time complexity, showing through comprehensive analyses that it significantly improves upon the performance of baseline models. As we reflect on the capabilities of TCProF, it is important to recognize that our current focus has been on establishing an effective framework that performs well in constrained environments. Looking ahead, expanding the generalization capabilities of TCProF to accommodate a wider range of programming languages is a vital next step. We believe TCProF will serve as a significant milestone, propelling forward the research in code time complexity prediction, especially in low-resource settings."}, {"title": "Limitation", "content": "TCProF is specifically designed for few-shot settings and thus, its application would not be effective in fine-tuning with the full dataset. One possible exploration is employing the augmentation module of TCProF for fine-tuning the full dataset. We provide the analyses in Appendix J.\nAnother limitation of TCProF is the space complexity prediction. This is also due to the absence of datasets involving the space complexity. Thus, we will be happy to apply and develop our framework for the space complexity prediction when the datasets are released.\nFramework Adaptability to Zero-Shots As our framework targets few-shot settings where \u2018several' data instances for each class are provided, We do not provide initial configuration for zero-shot settings. However, we can adapt the zero-shot setting by altering the structure of TCProF to incorporate the symbolic module for generating initial training data with pseudo-labels. It is essential to clarify that our objective is not zero-shot learning but few-shot learning.\nDynamic Calculation TCProF is focused on classifying time complexity into seven discrete classes, rather than dynamically calculating time complexities across a continuous range. We are eager to develop further modules and implement generative models for targeting the time complexity 'calculation', extending from the time complexity 'prediction'."}, {"title": "Usage of Sym as a Code Time Complexity Classifier", "content": "Our symbolic module, Sym, is specifically designed to enhance the pseudo-labeling process within the TCProF, aiming to improve the code time complexity prediction accuracy. This module leverages symbolic reasoning to generate more reliable pseudo-labels, empirically proven in Tables 1 and 2. For an extensive analysis, we present the experimental results in Table 10, using Sym alone to classify the time complexity of given code snippets.\nNotably, Sym shows relatively better performance on Java datasets compared to Python datasets. This difference can be attributed to the strict syntax of Java, which aids Sym in more effective identification and process of loop structures\u2014a critical aspect for time complexity analysis. Although the overall performance of Sym is promising, its integration with the broader TCProF, demonstrates the most effective results. Appendix H illustrates with examples, the detailed procedure of how Sym operates.\nWhile Sym does not capture every potential operation involved in computing time complexity, its design is strategically tailored to assist the baseline model's capability in producing more precise pseudo-labels. Moving forward, we recognize the potential for further enhancing Sym by expanding its scope to include a broader range of conditions. This development is part of our future research aimed to broaden our understanding of code time complexity prediction."}, {"title": "Error Analysis", "content": "From the test cases, we have analyzed the errors, and as discussed in Section 5.3, O(N^2) \u2192 O(N) errors occur the most. Without sufficient attention to the input size, it is sometimes confusing, even for human experts, to differentiate two codes, each in O(N) and O(N^2) classes, respectively. For instance, the code instance in Figure 10a is rather straightforward. The stones_after function is called in the main part which runs linear to the input size. The main part calls the function n times and thus, the time complexity of the whole code in O(N^2). However, the code in Figure 10b contains count operation which runs linear to the size of arr. If the code does not have the knowledge on the operation, the model is likely to predict its time complexity as O(N), which is wrong. This is mostly seen in the codes of class O(N^2) and thus, the majority of errors are the O(N^2) \u2192 O(N)."}]}