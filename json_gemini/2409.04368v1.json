{"title": "THE IMPACT OF SCANNER DOMAIN SHIFT ON DEEP LEARNING PERFORMANCE IN MEDICAL IMAGING: AN EXPERIMENTAL STUDY", "authors": ["Gregory Szumel", "Brian Guo", "Darui Lu", "Rongze Gui", "Tingyu Wang", "Nicholas Konz", "Maciej A. Mazurowski"], "abstract": "Medical images acquired using different scanners and protocols can differ substantially in their appearance. This phenomenon, scanner domain shift, can result in a drop in the performance of deep neural networks which are trained on data acquired by one scanner and tested on another. This significant practical issue is well-acknowledged, however, no systematic study of the issue is available across different modalities and diagnostic tasks.\nIn this paper, we present a broad experimental study evaluating the impact of scanner domain shift on convolutional neural network performance for different automated diagnostic tasks. We evaluate this phenomenon in common radiological modalities, including X-ray, CT, and MRI.\nWe find that network performance on data from a different scanner is almost always worse than on same-scanner data, and we quantify the degree of performance drop across different datasets. Notably, we find that this drop is most severe for MRI, moderate for X-ray, and quite small for CT, on average, which we attribute to the standardized nature of CT acquisition systems which is not present in MRI or X-ray. We also study how injecting varying amounts of target domain data into the training set, as well as adding noise to the training data, helps with generalization.\nOur results provide extensive experimental evidence and quantification of the extent of performance drop caused by scanner domain shift in deep learning across different modalities, with the goal of guiding the future development of robust deep learning models for medical image analysis.", "sections": [{"title": "1 Introduction", "content": "Medical image analysis has rapidly advanced through the use of deep learning [7, 18, 6], due to the ability to train highly flexible neural networks to use for various diagnostic tasks. However, due to their flexibility and data-driven nature, neural networks are susceptible to the problem of scanner domain shift [29].\nWhen radiological images are acquired at different imaging sites, referred to as domains, various attributes of the medical image acquisition pipeline such as scanner model and manufacturer, image acquisition parameters (e.g., echo and repetition times for MRI), and post-processing can differ between the two domains, resulting in images with different characteristics. Then, if a neural network is trained to perform a diagnostic task on the images from one site (e.g., cancer detection in breast MRI), and has learned to utilize certain image features for its predictions, these features"}, {"title": "2 Related Work", "content": "Certain previous studies in the usage of deep learning for medical image analysis have focused on evaluating the performance of such techniques under various types of domain shift. For example, AlBadawy et al. [1] and Wang et al. [30] demonstrated that neural networks trained for brain MRI tumor segmentation and mammogram classification, respectively, suffered from performance drops when given data from a different institution than that of the training set. In a further extension of this research, Yao et al. [31] conducted an extensive study using ten X-ray datasets to establish a baseline for domain adaptation and generalization, and observed consistent performance drops when trained models were given X-rays from unseen datasets. Additionally, M\u00e5artensson et al. [20] found that MRI diagnostic models developed from homogeneous research cohorts often underperformed when applied to diverse clinical data, highlighting the challenges posed by out-of-distribution (OOD) data. A common theme across these studies is the significant decline in model performance in OOD scenarios, yet a notable limitation is that they each focus on single imaging modalities.\nBuilding on these works, our study aims to understand the impact of domain shift specifically created by using differing scanners for image acquisition, across multiple imaging modalities. We note that as opposed to works such as [11], this is not a survey of existing techniques for mitigating domain adaptation (at the model, learned feature, or image level), but an experimental study of the effects of scanner-based domain shift on downstream task performance for models that have not been adapted by such methods. As shown in the next section, we propose an experimental design that is as standardized as possible across all modalities and diagnostic tasks, with the goal of obtaining a general, universal understanding of scanner domain shift phenomena."}, {"title": "3 Methods", "content": "We gathered a total of seven datasets from the CT, MRI, and X-ray modalities to explore the impact of scanner domain shift on deep neural networks trained for various diagnostic tasks. Each dataset fulfills the following requirements:\n1. The dataset must contain scanner information (i.e. the manufacturer of the scanner).\n2. The dataset needs enough data to sufficiently train and evaluate deep learning models.\n3. The dataset needs to be public for the sake of reproducibility.\nAll datasets and their respective diagnostic tasks will be individually introduced in the following section. For each dataset, we split the data into two scanner types (labeled 1 and 2), then for each scanner type into random subsets (by patient) of 2/3 for training, 1/6 for validation, and 1/6 for testing. We label these six subsets of a given dataset according to their scanner domain as (Train1, Train2), (Val1, Val2), and (Test1, Test2), respectively."}, {"title": "4 Results", "content": "In Table 2 and Figure 2 we show the AUC performance Ri,j,k := AUC(Mj, Testk) of the model trained in scanner domain j and tested in domain k, averaged over all ten randomly-seeded runs indexed by i, for each dataset.\nFirst, we see that unsurprisingly, models generally achieved higher accuracy when their test set was of the same domain/distribution as their training set. An interesting exception was observed in the results for the CT-HNSCC and CT-Oropharyngeal datasets, where the AUC scores were similar for all experiments for both matched and mismatched training and testing domains. This implies that the visual features in the images that are needed for the task are similar for different scanner types, and so those from one domain transfer to the other."}, {"title": "Discussion", "content": "This study comprehensively examined the impact of scanner domain shift on the performance of deep neural networks in medical image analysis. Overall, our findings align with previous research indicating the sensitivity of deep learning models to variations in image features, even subtle ones. We hypothesize that these variations in visual features are likely due to differences in scanner parameters and post-processing techniques between imaging sites."}, {"title": "The Dependence of Scanner Domain Shift on Modality", "content": "Notably, the scanner domain shift results (Table 2) tell of an interesting pattern: the typical severity of domain shift changes depending on the imaging modality. Here, we will use \u2206 to denote the change from in-domain to out-of-domain test set AUC. We see that domain shift is typically most severe for MRI tasks (\u2206 = \u22120.097 on average), moderate for X-ray (\u2206 = -0.067), and quite small for CT (\u2206 = \u22120.02), as summarized in Fig. 3. We hypothesize that this varying severity of domain shift is dependent on both how much scanner image acquisition parameters and proprietary post-processing methods usually differ between imaging settings/domains for a given scanner type, and the strength of the effect of these factors on image appearance (excluding potential confounding effects such as biases in patient population). This is because domains with different scanner parameters and/or post-processing will have the effect of differing distributions of generated image features which the network learns for the downstream task, so that certain features learned from one domain may not be present in the other, resulting in a loss of performance.\nWhile it is infeasible in generality to directly quantify how differences in imaging parameters/effects between domains for a given modality result in differences in acquired image features (and then therefore downstream network task performance), we can at least explore the nature of these parameters to hypothesize why some modalities may be more susceptible to domain shift than others, in terms of their acquisition system which may vary between scanner domains. Note that proprietary post-processing unique to a given scanner model/manufacturer can also affect image appearance, but by being proprietary, these effects are unknown.\nMRI has many different scanner parameters that will directly affect image appearance, such as echo time (TE), repetition time (TR) and flip angle [21], which all differ in distribution between GE and Siemens scans for the breast MRI dataset [16]. Changing any of these parameters will affect the visible contrast of different types of tissue in accordance with their intrinsic T\u2081 and/or T2 values. Indeed, for breast MRI, previous research [26] has suggested that variations in scanner parameters affect the image/radiomic features of fibroglandular tissue in breast MRI, potentially impacting the robustness of models trained on these features. This insight may explain the scanner-specific performance disparities observed in the MRI dataset, particularly in features related to tumor detection.\nX-ray is also affected by imaging parameters and other effects which can result in image features that differ between domains. Examples of these parameters and effects include the spectrum, energy/dosage, and beam angle of the radiation, as well as potential measurement noise, according to an extension of the Beer-Lambert law [15]. Finally, CT images may be similarly affected by imaging parameters such as slice thickness, pixel size, and dosage [14], although these are much more standardized, resulting in less severe domain shift effects."}, {"title": "Other Results and Future Work", "content": "Additionally, we found that adding simple Gaussian noise to images in training did not assist with performance in other domains, indicating that more complex solutions to prevent in-domain overfitting and encourage out-of-domain generalization [28] are necessary.\nFuture studies could expand on ours in various ways. While we chose to have breadth of datasets and modalities, further studies could explore the presence of the phenomena for additional network architectures and training set sizes. Additionally, the effect of scanner shift on different diagnostic tasks could be evaluated, such as other classification tasks, or other tasks altogether such as regression, object detection, or semantic segmentation. Pixel-level localization tasks such as detection and segmentation may be more susceptible to domain shift due to them (1) often being more challenging than image-level classification and (2) being dense prediction tasks that may have more capacity for overfitting than image-level prediction tasks such as classification."}, {"title": "Conclusion", "content": "Our findings emphasize the importance of accounting for scanner variability when developing and validating deep learning models for medical image diagnosis, and highlight a potential challenge in deploying these models across different clinical settings where scanner heterogeneity is common. Further research into methods for retroactively mitigating the impact of scanner-specific factors on image appearance may be critical to enhance the generalizability and clinical applicability of deep learning methods for radiology."}, {"title": "A Additional Experiments", "content": null}, {"title": "A.1 How does additional training data from the target domain help domain generalization?", "content": "Given that almost all datasets suffer from some level of performance drop when the scanner domain of the test set differs from that of the training set (Table 2), a logical next exploration would be to explore this continuum of performance drop between these two domains. To do so, we next evaluated how test set performance is affected by the training set having varying amounts of target domain images inserted. Specifically, we move images randomly from the target domain's training set to the original source domain training set such that a fraction f \u2208 [0.1, 0.2, 0.3, 0.4, 0.5] of the training set is comprised of the target domain images. Once the model is trained on this mixed training set, we evaluate it on the usual target domain test set. We repeated this procedure three times with different random seeds, for each of the three datasets of MRI-Prostate, CT-LIDC, and CT-Kidney. When the target domain training dataset did not have enough samples to reach the desired f, we duplicated samples until we reached the specified mixin amount for the training set.\nThe results of these scanner mixing experiments (test set AUC as a function of mixing fraction f) are presented in Figure 4. We see that scanner mixing for CT tasks results in little change in performance, while the effect is more noticeable for MRI. This is reasonable given that domain shift was typically small for CT, yet larger for MRI (Section 4.1), likely due to differing similarity/overlap of features between source and target domains for the two modalities. For MRI in particular, we see an asymmetry in the degree of domain shift depending on the training domain: models trained on solely Siemens images generalize well to the target domain, such that mixing has little effect, but models trained on \"Other\" domain images don't generalize as well, such that mixing in images from the target domain provided noticeable benefit. This is in turn due to features learned from one domain generalizing well to the other, but not vice versa."}, {"title": "A.2 Does adding noise to training images assist domain generalization?", "content": "Adding noise to training data has been shown to help deep learning models reduce overfitting and generalize better to unseen data [33] (for example, adversarial attack-resistant training [19] can be considered as a specialization of this idea). Moreover, unlike in general natural image computer vision where images from different domains often have clear, noticeable differences in visual features (e.g., photographs vs. paintings or cartoon sketches [32]), images from different scanner domains in medical images may not even appear visibly distinct (such as for the breast MRI dataset [4]), implying that differing visual features may just be fine-grained characteristics such as noise and slight changes in texture. As such, simply adding basic noise to one of our model's training data could potentially result in better generalization to other domains, which we will explore in this section.\nFor these experiments, we evaluated adding random Gaussian noise of different levels to training set images. For each training set image x \u2208 R, we add \u03b1\u03b5 to x with e ~ N(0, In) and a \u2208 [1, 2, 3, 4, 5, 7, 10, 15, 30, 50]. As in the scanner mixing experiments (Appendix A.1), we evaluate on the MRI-Prostate, CT-LIDC, and CT-Kidney datasets, and repeat all experiments at each noise level three times.\nWe show all results for the noise experiments in Figure 5. In fact, adding noise to training images never assisted out-of-domain generalization, even at small noise levels, which, in theory, could have added robustness to the network's learned representations. Instead, adding noise hurt both in-domain and out-of-domain performance, as the noised images no longer accurately represent samples from either domain. This shows that more complicated methods are needed to modify images to have domain-generalizable features."}]}