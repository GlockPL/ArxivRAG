{"title": "Restyling Unsupervised Concept Based Interpretable Networks with Generative Models", "authors": ["Jayneel Parekh", "Quentin Bouniot", "Pavlo Mozharovskyi", "Alasdair Newson", "Florence d'Alch\u00e9-Buc"], "abstract": "Developing inherently interpretable models for prediction has gained prominence in recent years. A subclass of these models, wherein the interpretable network relies on learning high-level concepts, are valued because of closeness of concept representations to human communication. However, the visualization and understanding of the learnt unsupervised dictionary of concepts encounters major limitations, specially for large-scale images. We propose here a novel method that relies on mapping the concept features to the latent space of a pretrained generative model. The use of a generative model enables high quality visualization, and naturally lays out an intuitive and interactive procedure for better interpretation of the learnt concepts. Furthermore, leveraging pretrained generative models has the additional advantage of making the training of the system more efficient. We quantitatively ascertain the efficacy of our method in terms of accuracy of the interpretable prediction network, fidelity of reconstruction, as well as faithfulness and consistency of learnt concepts. The experiments are conducted on multiple image recognition benchmarks for large-scale images.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) learn complex patterns from data to make predictions or decisions without being explicitly programmed how to perform the task. Interpreting decisions of DNNs, i.e. being able to obtain human-understandable insights about their decisions, is a difficult task [11,13,49]. This lack of transparency impacts their trustworthiness [58] and hinders their democratization for critical applications such as assisting medical diagnosis or autonomous driving. Two different paths have been explored in order to interpret DNNs outputs. The simplest approach for practitioners is to provide interpretations post-hoc, i.e. by analysing the so-called black-box model after training [12, 47, 55, 61]. However, post-hoc methods have been criticized for their high computational costs and a lack of robustness and faithfulness of interpretations [8,37,78]. On"}, {"title": "2 Related works", "content": "Interpretable predictive models In the context of deep learning architectures, a host of early approaches studying interpretability tackled the post-hoc interpretation problem [15, 47, 55, 63, 66, 67]. However, previous works such as"}, {"title": "3 Approach", "content": "In this part, we provide a design overview of a concept based interpretable network (COIN). Our focus in this paper is on CoIN systems that learn an unsupervised dictionary of concepts."}, {"title": "3.1 Background", "content": "Concept-based interpretable networks We denote a training set for a\nN\nsupervised image classification task as S = {(xi, Yi)}1. Each input image\nx \u2208 X CR\" is associated with a class label y \u2208 V, a one-hot vector of size\nnumber of classes C. The by-design interpretable network based on learning con-\ncept representation is denoted by g : X \u2192 \u0423.\nIn the standard setup for concept-based prediction models (supervised or\nunsupervised), given an input x, the computation of g(x) is broken down into two\nparts. There is first a concept extraction representation \u00de, and then a subnetwork\nthat computes the final prediction using concept activations P(x), such that\ng(x) = \u0398\u03bf\u03a6(x). Supervised concept-based networks [40] use ground-truth concept\nannotations to train \u03a6. The core of unsupervised concept-based methods instead"}, {"title": "3.2 Interpretable prediction network design", "content": "We assume a fixed pretrained network for classification f and a fixed pretrained generator G, trained for classification and generation on input dataset respectively. We use these two networks to guide our design and learning of g and its"}, {"title": "3.3 Use of StyleGAN as decoder", "content": "A short primer on style-based GAN The work by Karras et al. [33] pro-posed a style-based generator architecture. The generator learns to map a noise vector (z \u2208 R512 ~ N(0,1)) to a latent code (w \u2208 W CR512) that controls the scaling and biasing of feature maps at different resolutions in the synthesis network, controlling the style of the generated image. Subsequent updates on the original architecture [32, 34] have improved upon various aspects of gen-erator architecture and regularization. One such update relevant for us is the StyleGAN2-ADA [31] which incorporates an adaptive discriminator augmenta-tion in training. This enables training StyleGAN2 with limited amount of data, a regime where GANs are generally prone to collapsing. The StyleGAN archi-tecture has been actively studied for certain aspects relevant to our use case. Primary among these are GAN inversion methods that aim to embed any given input to the latent space of a pretrained GAN so that the GAN can reconstruct the input using the latent code. The two approaches for this include either op-timizing a latent code for any given input [1, 29], or training an encoder to predict the latent code [6,56,75]. While the optimization approaches can invert a given input better, they scale poorly compared to encoder based approaches when inverting many input samples. StyleGAN has also been studied closely in regard to its structure of latent spaces, including their suitability to embed inputs [1, 72, 73], discovering semantically meaningful directions or trajectories in latent space [25, 65, 69] and hierarchical style control with latent vectors for different resolutions [35].\nAt this juncture, it is worth asking the question of why don't we directly use the latent space/stylespace [72] of the pretrained StyleGAN as the con-cept dictionary for interpretation, as done previously [43]. We discuss about this question in detail in Appendix A.2, including issues in this design for use in a by-design interpretable network. Instead, we adhere to standard setup for unsu-pervised CoINs to learn a small dictionary of concept functions. The pretrained StyleGAN is used to reconstruct input x from concept activations \u03a6(x).\nThe most natural way to design this reconstruction is to use a similar ap-proach to encoder based GAN-inversion architectures used for StyleGAN. Fol-lowing previous works in this regard [1,75], we use the extended latent space W+ for inversion, which corresponds to different latent vectors for different res-olutions. We learn the concept translator 2 to map the concept activations to W+ and bias its computation with average latent vector w of G."}, {"title": "3.4 Training losses", "content": "Based on the previous discussion about concept based networks and our recon-struction pipeline design inspired from GAN inversion architectures, we define here our training loss Ltrain and each of its constitutive terms.\nTo verify the fidelity to output property, we define an output fidelity loss Lof, which grants predictive capabilites to g. It is defined as the generalized cross entropy (CE) loss between g and f:\n$$L_{of}(x; \u03a8, \u0398) = aCE(g(x), f(x)).$$\nThe most critical part of our training loss is the reconstruction loss Liec computed through the pretrained generative model G, that gathers all con-straints between inputs x and their reconstruction x = G(\u03a9(\u03a6(x), \u03a6'(x))). It combines 11 and 12 penalties, enforcing pixel-wise reconstruction for fidelity to input, with perceptual similarity LPIPS [82] and a final reconstruction classification term, both linked to viewability. The reconstruction classifica-tion term, defined as CE(f(x), f(x)), encourages the generative model to reconstruct 2 with more classification specific features pertaining to input x. Similar losses have been introduced for inversion with StyleGAN and its training for post-hoc interpretation [43]. Our reconstruction loss is thus de-fined as follows:\n$$L_{rec}(x; \u03a8, \u03a9) = ||x \u2212 x ||_2+ ||x-x||_1+\u1e9eLPIPS(x, x)+\u03b3CE(f(x), f(x)).$$\nWe impose the sparsity property along with two other regularizations, com-bined under the term Lreg. More specifically, I is regularized to encourage"}, {"title": "3.5 Interpretation phase", "content": "We now describe the interpretation generation process, which can be divided in two parts. (1) Concept relevance estimation, that requires estimating the importance of any given concept function k in prediction for a particular sample x (local interpretation) or a class c in general (global interpretation), and (2) Concept visualization, which pertains to visualizing the concept encoded by any given concept function k. We describe each of them in greater detail below:\n(1) Concept relevance: Since our g(x) adheres to structure of CoINs and among them closest to [51], the first step of relevance estimation almost follows as is. The estimation is based on concept activations \u03a6(x), and how the pooled version of (x) is combined by the fully connected layer in (with weights Ow) to obtain the output logits. Note that this step does not rely on using the decoder/generator G. Specifically, the local relevance rk(x) of a concept function k for a given sample x is computed as the normalized version (between [-1,1]) of its contribution to logit of the predicted class \u0109 = g(x). The global relevance of concept function k for a given class c, denoted rk,c, is computed as the average of local relevance rk(x) for samples from class c. The above description is summarized in equation below wherein e denotes the weight on concept k for predicted class \u0109 in weight matrix Ow:\n$$r_k(x) = \\frac{\u03b1_k(x)}{max_\u0131 |\u03b1_\u0131 (x)|}, \u03b1_k(x) = e^wpool(\u03a6_k(x)), r_{k,c} = E(r_k(x)|g(x) = c)$$\n(2) Concept visualization: Once the importance of a concept function is esti-mated, one can extract the most important concepts for a sample x or class c by"}, {"title": "4 Experiments and Results", "content": "Datasets We experiment on image recognition tasks for large-scale images in three different domains with a greater focus on multi-class classification tasks: (1) Binary age classification (young/old) on CelebA-HQ [30], (2) fine-grained bird classification for 200 classes on Caltech-UCSD-Birds-200 (CUB-200) [70], and (3) fine-grained car model classification of 196 classes on Stanford Cars [41].\nImplementation details We use a ResNet50 [26] as our architecture for f. All images are processed at resolution 256 \u00d7 256. For the pretrained GAN, on CelebA-HQ we use checkpoint released by NVIDIA [31]. On CUB-200 and Stanford Cars, we fine-tune an ImageNet [24] and LSUN Cars [79] checkpoint respectively to obtain the pretrained G in a resource efficient manner. The fine-tuning is performed without any class-labels. We use a dictionary size K = 64 on CelebA-HQ and K = 256 on CUB-200 and Stanford-Cars. All experiments were conducted on a single V100-32GB GPU. Complete details about architecture, training and evaluation are in appendix \u0412."}, {"title": "4.1 Evaluation strategy", "content": "One major goal of by-design interpretable architectures is to obtain high pre-diction performance. Thus, prediction accuracy of g is the first metric we evaluate. We next discuss multiple functionally-grounded metrics [17] that eval-uate the learnt concept dictionary from an interpretability perspective and its use in visualization, including two novel metrics in the context of evaluating CoINs (\u201cfaithfulness\u201d and \u201cconsistency\u201d).\nFidelity of reconstruction Since reconstructed output plays a crucial role in our visualization pipeline, we evaluate how well does G reconstruct the input. We do so by computing averaged per-sample mean squared error (MSE), per-ceptual distance (LPIPS) [82] and distance of overall distributions (FID) [28] of reconstructed images x and original input images x.\nFaithfulness of concept dictionary The aspect of faithfulness for a generic interpretation method asks the question \"are the features identified in the inter-pretation truly relevant for the prediction process?\" [7,52]. This is generally com-puted via simulating \"feature removal\" from the input and observing the change in predictor's output [27]. Simulating feature removal from input is relatively straightforward for saliency methods compared to concept-based methods, for example by setting the pixel value to 0. For COIN systems, this is significantly more tricky, as concept activations $k(x) don't represent the input x exactly. However, through the decoder, we can evaluate if the concepts identified as rele-vant for an input encode information that is important for prediction. We adopt an approach similar to a previous proposal of faithfulness evaluation for audio in-terpretation systems [52]. Concretely, for a given sample x with activation \u03a6(\u0445), predicted class \u0109 and a threshold \u03c4, we first manipulate P(x) such that k(x) is set to 0 if rk(x) > \u03c4,\u2200k \u2208 {1, ..., K}. That is, we \u201cremove\u201d all concepts with relevance greater than some threshold. This modified version of P(x) is referred to as Prem(x). To compute faithfulness for a given x, denoted by FF, we com-pute the change in probability of the predicted class from original reconstructed sample x = G(\u03a9(\u03a6(x), \u03a6'(x))) to new sample Xrem = G(\u03a9(Prem(x), \u03a6'(x))), that is, FFx = g(x)\u0109 - g(xrem)\u0109. Ideally, we expect to see a drop in probability (FFx > 0) if the set of relevant concepts \"truly\" encode information relevant for classification. Following [52] we report the median of FF over the test data for different thresholds 0 <\u0442 < 1.\nConsistency of visualization We expect during visualization of a given concept ok that a user observes similar semantic modifications across different images. Thus, we hypothesize that if modifying any specific concept activation \u03c6\u03ba(x) leads to consistent changes for different samples x, then generated out-put for two versions of P(x), one with $k(x) set to a large value and one with \u03a6\u03ba(x) = 0, should be separable in the embedding space of f (all other concept activations unchanged). In other words, embeddings for images with high k(x) and low k(x) should be well separated. To compute this metric, we first create a dataset of generated images with two different sets of activations. For each of training and test set, this is done by first selecting a set of Ncc samples for which Ok is highly activating and relevant for. Then we find its maximum activation"}, {"title": "4.2 Results and discussion", "content": "Quantitative results Tab. la reports the test accuracy of all the evaluated systems. Our proposed system, VisCoIN performs competitively with the pre-trained f considered uninterpretable and purely trained for performance. It also performs better than the other recent CoIN systems for more complex classi-fication tasks (CUB-200 and Stanford Cars) with large number of classes and diverse images. Metrics quantifying the fidelity of reconstruction on test data are in Tab. 2a. The other baselines only optimize for pixel-wise reconstruction and FLINT achieves a lower MSE than VisCOIN. However, crucially, reconstruc-tion from our method approximates the input data considerably better, in terms of perceptual similarity (LPIPS) and overall distribution (FID), which highly contributes to better viewability. Tab. 2b tabulates the median faithfulness FFx for the evaluated systems on 1000 random test samples for different thresholds. The performance of Random baseline being close to 0 even for small thresholds indicates that a random selection of concepts often contains little information relevant for classification of the predicted class. In contrast, concepts identified"}, {"title": "5 Conclusion", "content": "We introduced a novel architecture for Visualizable CoIN (VisCoIN), that ad-dresses major limitations to visualize unsupervised concept dictionaries learnt in CoIN systems for large-scale images. Our architecture integrates the visual-ization process in the pipeline of the model training, by leveraging a pretrained generative model using a concept translator module. This module maps concept representation to the latent space of the fixed generative model. During training, we additionally enforce a viewability property that promotes reconstruction of high-quality images through the generative model. Finally, we defined new eval-uation metrics for this novel interpretation pipeline, to better align evaluation of concept dictionaries and interpretations provided to a user. Future works in-clude adapting the design of this system for other type of generative models, or applying it to different visual domains."}, {"title": "A Design choices considered with the generative model", "content": ""}, {"title": "A.1 Desiderata for generative model", "content": "We discuss below the reasoning and requirements governing our choice of gener-ative model G, i.e., a pretrained StyleGAN2-ADA.\n\u2013 The generative model should have the ability to model the input with a low dimensional latent space (compared to input dimensions), since a concept representation is typically much lower dimensional than input. Even so, it must possess a structured latent space so that modifying a concept activation can enable latent traversal for visualization. These requirements discourage the use of invertible neural networks [10, 18], or diffusion models [57,64] whose structure of latent spaces is still being explored [42,53]. Furthermore, since we constantly rely on input reconstruction during training, the slow sampling process in diffusion models for large-scale images also hinders their suitability for our use case.\n\u2013 One crucial role of the generative model is to generate high quality natural images. This is essential not only for better reconstruction of large-scale images, but also crucial to ground any visual modifications in the generated images back to the original input. This encouraged our preference for GANS instead of Variational Auto-Encoders (VAEs) [39,54], specially for large-scale images.\n\u2013 The generative model also needs to be able to train in limited data regimes if a pretrained generator is not readily available.\nHence, we opted for a competitive, flexible and widely used generative model, StyleGAN2-ADA [31], shown to have demonstrated high-quality generation for various image domains."}, {"title": "A.2 Defining concepts in stylespace of pretrained StyleGAN", "content": "Previous works using StyleGAN for interpretation deliberately choose to use the Stylespace as concept dictionary [43], which is considered the most disen-tangled and interpretable latent space of a StyleGAN [72]. However, in regard to a by-design interpretable architecture, there is a crucial issue in using the Stylespace of a pretrained StyleGAN as the concept feature space. Indeed, the size of Stylespace, even for small-scale images, runs into many thousands, which is much larger than the size of typical dictionaries in CoINs (by a factor of 10 to a 100). This directly compromises the interpretable structure of the predic-tion model [46] with much lower number of classes for prediction. Moreover, since the model is pretrained for generation, the space doesn't have sparse activations, further worsening the issue. Note that with the same reasoning, we considered using the extended latent space W+ as concept dictionary unsuitable. The final option that remains is using W space as our concept dictionary. While at first glance it seems an interesting option, since its size is comparable to concept dic-tionaries in CoINs, and it is also considered as better structured than W+ for"}, {"title": "B Further system details", "content": ""}, {"title": "B.1 Network architectures", "content": "We already discuss in the main text the architectures for f and G, both us-ing standard and widely used models, ResNet50 [26] and StyleGAN2-ADA [31] respectively. f and G are pretrained and fixed during training of VisCOIN. As part of our training we train three subnetworks \u03a8, \u03a9, \u0398. We already described in the main text, as consisting of a pooling (maxpool), linear and softmax layers in the respective order. We describe the architecture of network that operates on hidden layers of f and computes the dictionary activations \u03a6(x) and the supporting representation \u03a6'(x) and the design of concept translator \u03a9 that maps (\u03a6(x), \u03a6'(x)) to extended latent space W+ of StyleGAN to reconstruct x.\nArchitecture of Our architecture of mostly follows proposed architecture of for FLINT [51] which accesses output of two layers for ResNet18 close to the output layer (output of block 3 and penultimate layer of block 4). The ResNet50 also follows a similar structure with 4 blocks. Each block however contains 3, 4, 3 and 3 sub-blocks termed \"bottleneck\" respectively. In terms of the set of layers accessed by for VisCoIN, in addition to the corresponding two layers in ResNet50 (output of block 3 of shape 1024 \u00d7 16 \u00d7 16, output of penultimate bottleneck layer in block 4 of shape 2048 \u00d7 8 \u00d7 8), we also access a third layer for improved reconstruction (output of block 2 of shape 512 \u00d7 32 \u00d7 32). Each layer output is passed through a convolutional layer and brought to a common shape of 512 x 8 x 8, the lowest resolution and feature maps. We then concatenate all the feature maps and create two branches, one that outputs P(x) and the other outputs \u03a6'(x). The branch computing (x) is simply two convolutional and a pooling layer yielding an output shape of K \u00d7 3 \u00d7 3, where K is the number of concepts, and each pk(x) is a convolutional map of size 3 \u00d7 3. Thus, the total number of elements in each \u0444k(x) is b = 9. The branch computing support representation \u03a6'(x) (unconstrained except by reconstruction) applies two fully connected layers to output same number of elements as in P(x). The usefulness of the support representation P'(x) is analysed in Tab. 7.\nArchitecture of For translating the concept activations (x) and the sup-porting representation \u03a6'(x) to the extended latent space, we use the concept translator \u03a9. As described in the main text, it consists of single fully-connected"}, {"title": "B.2 Training details", "content": "The steps to train our system on a given dataset can be divided into three modular parts: (1) Obtaining a pretrained classifier f with \"strong\" performance that can provide high-quality source representations to learn from, (2) Obtaining a pretrained generator G that can approximate well the distribution of the given dataset, and (3) Training of g with VisCoIN using the pretrained f and G. When a pretrained f or G is not easily available, we train them on their respective tasks on the given dataset. Among our 3 datasets, CelebA-HQ [30], CUB-200 [70] and Stanford Cars [41], we easily found a pretrained G for CelebA-HQ. All other combinations of f and G were pretrained. We describe the training details of f, G and VisCOIN below:\nPretraining f We pretrain f for classification on each of our datasets before using it for training VisCOIN. We use Adam optimizer [38] with fixed learning rate 0.0001 on CUB-200 and 0.001 on CelebA-HQ to train f. On Stanford-Cars, we use SGD optimizer with a starting learning rate of 0.1, decayed by a factor 0.1 after 30 and 60 epochs. The training is initialized with pretrained weights from ImageNet in each case, and fine-tuned for 10, 30 and 90 epochs on CelebA-HQ, CUB-200 and Stanford-Cars respectively. In all cases, during pretraining, the images are resized to size 256 \u00d7 256.The accuracy of f is already reported in the main paper. All of these experiments have been conducted on a single A100 GPU, with a batch size of 64 for CelebA-HQ and 128 for Stanford-Cars dataset and on V100 GPU with a batch size of 32 for CUB-200.\nPretraining G We use a pretrained StyleGAN2-ADA [31] for all our experi-ments. On CelebA-HQ, we used a pretrained checkpoint available from NVIDIA."}, {"title": "B.3 Evaluation details", "content": "Metric computation The median faithfulness is computed over 1000 random samples from the test data. For consistency, we use Ncc = 100, x = 2, i.e., given any concept ok, we extract its 100 most activating samples over samples of classes its most relevant for. The constant high activation is twice (X = 2) the maximum activation of k(x) over the pool of 100 samples. Thus the binary training dataset created via samples from training data consists of 200 samples, 100 \"positive\" samples with high activation of k(x) and 100 \"negative\u201d samples with zero activation k(x). The binary testing dataset also contains the same number of samples of each type but is created via samples from test data. The feature maps we extract are the output of the second block of the pretrained f (ResNet50), the 22nd convolutional layer. The shape for each feature map is 512 x 32 x 32. We pool them across the spatial axis to obtain an embedding of size 512 for any input sample. The linear classifier we train is a linear SVM. We select its inverse regularization strength C from the set {0.01, 0.1, 1.0, 5.0} (lower value is stronger regularization). The parameter is selected using 5-fold cross validation on the created training data.\nBaseline implementations We utilize the official codebase available for FLINT and FLAEM for our baseline implementation. For fairness, we use the same num-ber of concepts for both of them. Since our architecture is closer to FLINT, we update and adapt it to implement in similar settings as ours. We use the same f architecture (ResNet50 instead of ResNet18) for both the systems and keep it pretrained and fixed. The architecture is also similar in that it accesses the same set of hidden layers and has the same structure and depth. For other"}, {"title": "C Additional experiments", "content": ""}, {"title": "C.1 Consistency with higher A", "content": "We present the results here for evaluating consistency with a higher value of X = 3, compared to the main text where X = 2. Thus, the constant high activation of \u03a6k(x) used to generate \"positive\" samples of the dataset is increased further. We thus expect the \"separation\" in the embedding space to increase and consequently a higher performance CCk of the binary classifier k for any k. The results for both are presented in Tab. 4. The results confirm that emphasizing the concept indeed makes the visual modifications more stronger and consistent. Moreover, we also observe that increase in consistency of VisCOIN tends to be larger than increase for other COIN systems."}, {"title": "C.2 Qualitative visualization FID", "content": "While qualitatively on can clearly observe the difficulty to understand activation maximization based visualization in FLINT. We further support our claim about the unnaturalness of these visualizations compared to visualization in VisCOIN"}, {"title": "D Ablation studies", "content": "We present ablation studies for components and simultaneously discuss our ratio-nale behind the design selection of these components, (a) effect of reconstruction-classification loss with weight y, (b) role of using a supporting representation \u03a6'(x) to assist in reconstruction, (c) selection of number of concepts K, (d) effect of orthogonality loss, and (e) effect of fidelity and sparsity loss weights.\nWe highlight at this point to the reader that there is an overarching theme in our various design choices. Most of them are based on shaping the systems suitability for better optimization of perceptual similarity for reconstruction. We constantly aim to achieve better reconstruction without major negative im-pacts for any other properties. This is because for complex datasets (CUB-200, Stanford Cars), the key bottleneck in the design is to achieve the high-quality reconstruction for viewability.\nReconstruction-classification loss Tab. 6 reports the perceptual similarity and faithfulness with threshold T = 0.2 on test data of CUB-200 for different strength y. Interestingly, it indicates a tradeoff between faithfulness and percep-tual similarity. Completely removing this loss heavily impacts the faithfulness. However, among the positive y, our choice was driven strongly by achieving a"}, {"title": "E Additional visualization", "content": "We show additional visualizations for different highly relevant class-concept pairs (rk,c > 0.5) in Fig. 6. For each class-concept pair, we show the effect of modifying the concept activation on the generated output for three maximum activating training samples. For each sample (on the far-left), we show the corresponding generated outputs for X = 0 (center-left), \u03bb = 1 (center-right) and X = 4 (far-right).\nUse of difference images can be useful To better highlight regions in the image impacted by modifying concept activations, one can additionally visualize the differences between two generated outputs. We show visualizations with the difference in the generated outputs in Fig. 7. Again, we selected highly relevant class-concept pairs (rk,c > 0.5), and show the effect of modifying the concept activation on the generated output for three maximum activating training sam-ples. For each sample (on the far-left), we show the corresponding generated outputs for A = 1, x (center-left), and X = 4, \u00ee' (center-right). We then compute and show the difference between the two generated outputs ' - x (far-right). It is worth noting that this exact strategy might not work as effectively for all types of concepts and can require modifications. For example, if \"black feathers\" are emphasized by a concept, the increasing \u201cblack\u201d color won't be visible in the"}, {"title": "F Limitations of approach", "content": "While the idea in principle should be adaptable to other choices of pre-trained G, assuming other choices are inline with desiderata discussed in Appendix A.1, it requires redesigning \u03a9 accordingly. The current VisCOIN design in our experiments is only compatible with StyleGAN.\nAs is the case for other CoINs learning unsupervised concepts, the proposed system cannot guarantee that concepts precisely correspond to human con-cepts and not encode other additional information. However, one interesting aspect is that visualization process in VisCoIN gives a better handle at iden-tifying any deviations as visualizations in other unsupervised CoINs can be much harder to understand with granularity for large-scale images.\nWhile VisCOIN significantly upgrades the visualization pipeline over pre-vious CoIN architectures, some of our choices, arising from restrictions to preserve a by-design interpretable predictive structure or limited computa-tional resources, lead to certain limitations. (1) The choice of using a small dictionary as bottleneck under multiple other constraints leads to worse re-construction compared to StyleGAN inversion architectures. (2) The choice of using a concept dictionary not defined in the Stylespace, results in learnt concepts that are not as well disentangled as Stylespace. Dictionaries for post-hoc interpretation defined in Stylespace provide more disentangled con-cepts for interpretation (as in StyleEx [43]), but are significantly larger and thus less suitable for by-design architecture. Finally, (3) the choice of using a pretrained G improves training time, complexity and reusability, but also implies that the system's quality is limited by the quality of the pretrained G. For instance, for visualization, if G can't generate some specific feature, it can be difficult to visualize a concept ok that encodes that feature.\nFor the case of single FC layers in our visualization process follows linear trajectories in W+ latent space of StyleGAN when modifying an activation. Recent work has shown that linear trajectories are not necessarily optimal for latent traversals [65]."}, {"title": "G Potential negative impacts", "content": "Given that the understanding of neural network decisions is considered as a vital feature for many applications employing these models, specially in critical deci-sion making domains, we expect our method to have an overall positive societal impact. However, in the wrong hands almost any technology can be misused. In the context of VisCOIN, it can be used to provide deceiving interpretations by corrupting its training mechanisms (for example by training on misleading annotated samples, using deliberately altered pretrained models etc.). Thus, we"}]}