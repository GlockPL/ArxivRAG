{"title": "Physics Enhanced Residual Policy Learning (PERPL) for safety cruising in mixed traffic platooning under actuator and communication delay", "authors": ["Keke Long", "Haotian Shi", "Yang Zhou", "Xiaopeng Li"], "abstract": "Linear control models have gained extensive application in vehicle control due to their simplicity, ease of use, and support for stability analysis. However, these models lack adaptability to the changing environment and multi-objective settings. Reinforcement learning (RL) models, on the other hand, offer adaptability but suffer from a lack of interpretability and generalization capabilities. This paper aims to develop a family of RL-based controllers enhanced by physics-informed policies, leveraging the advantages of both physics-based models (data-efficient and interpretable) and RL methods (flexible to multiple objectives and fast computing). We propose the Physics-Enhanced Residual Policy Learning (PERPL) framework, where the physics component provides model interpretability and stability. The learning-based Residual Policy adjusts the physics-based policy to adapt to the changing environment, thereby refining the decisions of the physics model. We apply our proposed model to decentralized control to mixed traffic platoon of Connected and Automated Vehicles (CAVs) and Human-driven Vehicles (HVs) using a constant time gap (CTG) strategy for cruising and incorporating actuator and communication delays. Experimental results demonstrate that our method achieves smaller headway errors and better oscillation dampening than linear models and RL alone in scenarios with artificially extreme conditions and real preceding vehicle trajectories. At the macroscopic level, overall traffic oscillations are also reduced as the penetration rate of CAVs employing the PERPL scheme increases.\nKeywords: Mix Traffic, connected and automated vehicles, Linear control, reinforcement learning, actuator delay.", "sections": [{"title": "INTRODUCTION", "content": "With the development of Connected and Autonomous Vehicles (CAVs), there arises the necessity for effective controllers to navigate CAVs in mixed traffic scenarios of CAVs and Human-driven Vehicles (HVs) (Ghiasi et al., 2017; Yao and Li, 2020). Such mixed traffic scenarios are inherently complex and dynamic, primarily due to the unpredictable behaviors of HVs and other road users (K. Yang et al., 2023). This complexity presents a significant challenge for CAV controllers. Inadequate control strategies for CAVs in these environments can result in increased traffic congestion and heightened safety risks.\nMainstream methodologies for controlling CAVs can be classified into three primary types: closed-form controllers, controllers based on Model Predictive Control (MPC), and those utilizing Reinforcement Learning (RL). The closed-form linear (Li, 2022) state-feedback CAV controller has been favored due to its rapid and straightforward implementation facilitated by its analytical representation. However, despite their simplicity, these controllers struggle with constrained optimization frameworks involving multiple explicit objectives and constraints, making them less adaptable to the dynamic driving environments found in mixed traffic. The MPC-based controller, supported by a flexible framework, addresses this limitation by optimizing multiple objectives with constraints in a rolling horizon (Zhou et al., 2019). Nonetheless, MPC often necessitates convex problem formulations and imposes a relatively high computational burden, hindering high-resolution real-time implementation. Some learning-based MPCs employ neural networks to solve problems, which mitigates slow-solving issues. However, these systems still face inherent challenges within the MPC framework, as their optimizer is trained through imitation learning based on datasets of MPC solutions. This training approach inherently limits their performance to not exceed that of the quality of the underlying MPC controller (Sacks et al., 2023).\nIn contrast to MPC, RL-based control facilitates fast computing for real-time implementation (He et al., 2024; Qu et al., 2020). Furthermore, RL-based controllers can capture the nonlinear and stochastic characteristics of complex systems due to the capabilities of neural networks. However, as summarized from previous literature, RL encounters several challenges regarding applicability in mixed traffic scenarios. In environments where autonomous and human-driven vehicles interact, the unpredictability of human behavior adds complexity to the driving context. RL's inherent issues with generalization mean it might not reliably interpret or react to the diverse behaviors encountered among different drivers, which could lead to safety risks or inefficient traffic flow. Moreover, RL's challenges with safe exploration become particularly critical in mixed traffic, as the system should navigate safely without extensive prior exposure to every possible driving scenario, potentially leading to unsafe actions in unanticipated situations. These limitations underscore the need for robust testing and the integration of safety-oriented strategies within RL frameworks before deployment in dynamic, real-world environments.\nSafe exploration stands out as a significant challenge for RL applications in the domain of autonomous vehicle control. The lack of safety assurance poses a major obstacle to RL application in real-world scenarios, manifesting in two key aspects. First, the generalization ability of RL has long remained an unsolved issue. Generalization requires RL models to be robust to variations in their environments and able to transfer and adapt to unseen (but similar) environments during deployment. However, current RL methods often evaluate the policy in the same environment it was trained in (Kirk et al., 2023). Over-reliance on data-driven approaches exposes the system to data biases, leading to overfitting of the training data. When deployed in the real world, RL and other deep learning models often encounter previously unseen categories of samples\u2014out-of-distribution (OOD) data-posing significant challenges. This limitation hinders existing autonomous systems from effectively addressing long-tail and cross-domain issues, restricting their safety and adaptability in new environments (Li et al., 2023). Second, RL may learn behaviors that violate physical laws because it relies on function approximation and representation learning (Cao et al., 2024), making it heavily dependent on data quality. This is a common issue for learning-based models (Long et al., 2024). Thus, RL may not learn skills not provided or rare in the demonstration data. Moreover, many RL-based vehicle control approaches, such as Adaptive Cruise Control (ACC) and Cooperative Adaptive Cruise Control (CACC), address safety by incorporating it into the reward function,"}, {"title": "PROBLEM STATEMENT", "content": "This research focuses on the longitudinal control of CAVs in mixed traffic of CAVs and HVs. We consider the car following process without lateral movement on the highway. We denote $N$ as the set of vehicle index, $N = \\{N_{CAV}, N_{HV}\\}$, where $N_{CAV}$ is the set of CAVs and $N_{HV}$ is the set of HVs. CAVs broadcast their state information (e.g., speed, position) to other vehicles in the platoon via Vehicle-to-Vehicle (V2V) communications, subject to a constant communication delay. They can also access real-time data about their own state. HVs lack autonomous driving capability and do not receive digital information from other vehicles, but HVs could broadcast information to CAVs within communication range.\nThis research considers a communication delay $\\tau^{c}$ during receiving preceding vehicle data, which includes both the signal propagation time and the processing time once the data is received. Moreover, when executing decisions derived from the PERPL framework, the vehicles experience actuator delay $\\tau^{A}$, which refers to the time lag between the issuance of commands by the control system and the actual response by the vehicle's actuators. The related notations are defined in TABLE 1."}, {"title": "Distributed Platoon Control Scheme", "content": "In this section, we proposed a distributed CAV longitudinal control for distributed platooning, whose framework is presented in Figure 1. Any CAV, denoted as $n \\in N^{CAV}$, can obtain its own state at time t: $S_{nt}, n \\in N^{CAV}, t \\in T$, where $T$ denotes the set of all time stamps. each CAV can access the states of up to three preceding vehicles within the communication range. The states of these vehicles are represented as $\\{S_{(n-k)(t-\\tau^{c})}\\}_{k\\in[1,2,3]}, n \\in N^{CAV}$, where $\\tau^{c}$ is the communication time delay. If fewer than three vehicles precede any CAV or if there are no preceding vehicles at all, the number of vehicles considered adjusts accordingly to zero, one, or two based on availability. This approach ensures that each vehicle can dynamically adjust its behavior based on the immediate traffic conditions, enhancing both the responsiveness and safety of the platoon.\nVehicle state $S_{nt}$ including three parts: position error $\\Delta d_{nt}$, speed difference $\\Delta v_{nt}$ and acceleration $a_{nt}$: $S_{nt} = [\\Delta d_{nt}, \\Delta v_{nt}, a_{nt}]^{T}, n \\in N, t \\in T$. Definitions for these components are as follows:\nCAVs apply a constant time headway (CTH) spacing strategy, which follows its preceding vehicle with a desired spacing distance and ensures safety. Thus, the desired spacing distance of vehicle $n$ at time t is $h_{d}v_{nt} + d_{o}$, where $h_{d}$ and $d_{o}$ are the desired constant headway and standstill space, respectively. Based on the CTH rule, the position error of Vehicle $n$ with respect to a desired distance from the preceding vehicle (n - 1) was denoted by $\\Delta d_{nt}$:\n$\\Delta d_{nt} = d_{(n-1)(t-\\tau^{c})} \u2013 d_{nt} \u2013 d_{o} \u2013 h_{d}v_{n}, n\\in N, t \\in T$ \\label{eq:1}\nThe speed difference between the ego and the preceding vehicle is:\n$\\Delta v_{nt} = v_{(n-1)(t-\\tau^{c})} \u2013 v_{nt}, n \\in N, t \\in T$ \\label{eq:2}\nThe control variable is $u_{nt}$. Given the assumptions and communication environment, the vehicle dynamics are modeled by linearized dynamics with the consideration of time delay $\\tau^{A}$:\n$a_{nt} = \\frac{u_{nt} \u2013 a_{nt}}{\\tau^{A}} \\ \\ n\\in N, t \\in T$ \\label{eq:3}"}, {"title": "PERPL CONTROLLER", "content": "Our proposed control framework encompasses two parallel controllers: a linear controller that focuses on ensuring local and string stability using a non-linear programming formulation and a DRL controller that specifically targets handling traffic disturbances and time delays. By integrating these two controllers, our framework aims to effectively address the challenges associated with car-following control in CAV environments. A safety barrier is added to guarantee safety. As shown in Figure 2, the control action $a^{PERPL}$ from PERPL framework is:\n$a^{PERPL}_{nt} = f_{SG}(a^{phy}_{nt} + a^{RL}_{nt})$ \\label{eq:4}\nwhere $a^{phy}$ is the output action of physics-based policy, $a^{RL}_{nt}$ is the output action of residual policy. $f_{SG}$ is the safety barrier that projects the combined output $a^{phy} + a^{RL}_{nt}$ to a safety range to guarantee safety (Ames et al., 2017). The details of each component are given in the following subsections."}, {"title": "Model-based Policy", "content": "A linear control model is employed to manage the behavior of a following vehicle within a platoon, emphasizing maintaining constant headway and minimizing the deviation from the lead vehicle's trajectory. The model is based on principles of classical control theory. The control input from the linear controller is:\n$a^{phy}_{nt} = \\underline{K} \\cdot S_{nt}$ \\label{eq:5}\nwhere $K = [K_{d}, K_{v}, 0]$ is a vector of feedback gained forming a closed loop of the controller, where $K_{d}, K_{v}$ are the feedback coefficients for $\\Delta d_{nt}$ and $\\Delta v_{nt}$."}, {"title": "Residual Policy", "content": "The residual action policy is constructed using the Proximal Policy Optimization (PPO) method (Schulman et al., 2017). Unlike other reinforcement learning methods that use the sum of discounted rewards to estimate future returns, the PPO method employs a policy gradient method that focuses on optimizing an objective function. This function is not solely based on the sum of returns but also involves the concept of probability ratios of policies and advantage functions, which facilitates learning and exploration on top of the existing policy. Therefore, PPO attempts to maintain training stability by limiting the extent of policy updates, thereby preventing excessively large updates during training.\nThe PPO is based on the Actor-Critic structure, with an actor (policy) and critic (value) network. This dual network structure facilitates efficient exploration of the action space while stabilizing the learning updates through the critic's value estimates."}, {"title": "Actor network", "content": "The Actor network is responsible for defining the policy \u03c0 with parameter \u03b8. It takes the DRL state s as input and outputs a probability distribution over actions. The control signal a is then sampled from this distribution. The Actor network is updated by maximizing the objective function $L(s, a, \\theta_{T}, \\theta)$, which is defined as:\n$L(s, a, \\theta_{T}, \\theta) = min(r_{t}(\\theta)\\hat{A}_{t}, clip(r_{t} (\\theta), 1 \u2013 \\epsilon, 1 + \\epsilon)\\hat{A}_{t})$ \\label{eq:6}\nwhere $r_{t}(\\theta)$ is the probability ratio $r_{t}(\\theta) = \\pi_{\\theta}(a|s)/\\pi_{\\theta_{T}}(a|s)$, $clip(\\cdot)$ is a clipping function to remove incentives for the new policy to get far from the old policy, which prevents large updates that could destabilize the training process. \u03f5 is a small hyperparameter, which determines how much the ratio can differ from 1 before it is clipped. $\\hat{A}_{t}$ is the advantage estimate at time t, calculated by:"}, {"title": "Critic network", "content": "The Critic network evaluates the decision output by the Actor network. The Critic network receives the DRL state s as input and outputs the estimated state value $V(s_{t})$. The network structure also includes one hidden layer with 100 neurons, and the ReLU function is used as the activation function for the output. The Critic network is updated by minimizing the critic loss function:\n$L_{c}(\\Phi) = \\hat{E}_{t}[(V(s_{t}) \u2013 R_{t})^{2}]$\nThe parameter \u03a6 is iteratively optimized based on the gradient $L_{c}(\\Phi)$with learning rate $\u03b1^{\\Phi}$:\n$\\Phi = \\Phi \u2013 \u03b1^{\\Phi} \\cdot \\nabla L_{c}(\\Phi)$"}, {"title": "Physics-based Safety Barrier", "content": "The safety barrier projects the action into the safety region based on the safety requirement on headway. The safety barrier adjusts the control action by taking the combined output of the physics-based and RL policies, $a^{phy} + a^{RPL}$, which may not inherently satisfy safety requirements. It then computes the adjusted action $a^{PERPL}$ that minimizes the deviation from this combined action while ensuring it adheres to safety constraints, formalized as:\n$a^{PERPL} = f_{SG} (a^{phy} + a^{RPL}) = arg min_{a_{nt} \\in A} || a_{nt} - (a^{phy} + a^{RPL}) ||^{2}$ \\label{eq:11}\nsubject to the constraint ensures that the resulting state transition remains within a safe state $s^{safe}_{Sn(t+1)}$:\n$\\triangle S_{nt} + B a_{nt} + C W_{nt} \\in s^{safe}_{Sn(t+1)}$ \\label{eq:12}\nwhere $s^{safe}_{Sn(t+1)}$ is the set of safety states where the headway between Vehicle $n$ and Vehicle $(n \u2212 1)$ is kept within safe limits, typically between 1s and 3s. The lower limit is to guarantee safety (Vogel, 2003), and the upper limit is to avoid overlarge headway causing unstable inside the platoon. A is the feasible set of $a_{nt}$.\nImportantly, the safety barrier is not activated under normal safe control scenarios and does not compromise the control effectiveness of the model. While the safety barrier is not inherently a part of the PERPL framework, it is included for two main reasons: firstly, safety barriers are a common mechanism currently implemented in RL-based models in practical applications. Secondly, it helps account for physical constraints in real-world scenarios. In reality, a collision will occur if the following vehicle is too close to the leading vehicle. However, in simulation environments, the following vehicle may even pass the leading vehicle without any constraint. The safety barrier has been implemented to prevent such unrealistic scenarios. Although actual accidents might be mitigated, the frequency of activation of the safety barrier still serves as an important safety metric."}, {"title": "EXPERIMENTS", "content": "This section details the design philosophy of the experiment, evaluation metrics, simulation methods for HVs in mixed traffic, and baseline models."}, {"title": "Experiment Overview", "content": "Our analysis spans two dimensions: 1) Single Vehicle Cruising: We conducted training and testing using both artificially designed and real-world leading vehicle trajectories. The artificially designed trajectories were designed to create extreme danger scenarios (rapid acceleration and deceleration) to evaluate the model's generalization capabilities. Real-world trajectories were used to align more closely with practical conditions. 2) Mixed Traffic Platooning: We assessed the performance of multiple vehicles and conducted a macro-level analysis.\nCommunication delay $\\tau^{\u03b7}$ was set at 0.3 seconds, based on empirical results, which lend a reasonable degree of reliability (Liang et al., 2024). The safety headway interval is defined between 1 and 3 seconds. The lower limit of headway prevents collisions, while the upper limit ensures vehicles do not lag too far behind the lead vehicle, thereby affecting subsequent vehicles. The safety barrier utilizes this range; if a pending action is detected that would cause the headway to exceed these bounds, the safety barrier projects the action to bring the headway back within a safe range. The desired constant headway $t^{a}$ and the safety headway set can be adjusted based on practical applications. Other parameters are listed in TABLE 3."}, {"title": "Evaluation metrics", "content": "To systematically evaluate the proposed framework, three performance indicators to quantitatively assess the control performance: driving comfort.\nHeadway Error: To assess the stability and safety of vehicle operation, it is crucial that the headway the distance between vehicles\u2014remains consistent at the designated desired headway. The headway error reflects the vehicle's adherence to the Constant Time Gap (CTG) rule as well as its safety characteristics. For this purpose, we employ the headway's Root Mean Square Error (RMSE) to quantify deviations from the desired values.\n$RMSE^{h} = \\sqrt{\\frac{\\sum_{t\\in T} (d_{(n-1)t} \u2013 d_{nt} \u2013 d_{o} \u2013 h_{d} v_{nt} )^{2}}{|T|}}$ \\label{eq:13}\nDamping Ratio: The cumulative damping ratio (di) is a measure of the CAV controller's ability to dampen traffic oscillations, which quantifies the empirical string stability (Ploeg et al., 2014). When the traffic passes through a string-stable CAV, the magnitude of traffic oscillations is either reduced or remains unchanged. The l2-norm acceleration damping ratio $d_{n}$ can be formulated as follows:\n$d_{n} = \\frac{|| a_{n} ||_{2}}{|| a_{0} ||_{2}} = (\\frac{\\sum_{t\\in T} |a_{nt}|^{2}}{\\sum_{t\\in T} |a_{0t}|^{2}})^{1/2}$ \\label{eq:14}\nComfort level: the driving cost function $C_{nt}$ aims to evaluate the eco-driving performance and empirical string stability, which is defined as:\n$C_{nt} = \u03b1_{i}(a_{nt})^{2}$ \\label{eq:15}"}, {"title": "HV Modeling Method", "content": "To ensure the simulation reflects realistic mixed traffic conditions, this study employs the Intelligent Driver Model (IDM), which is calibrated to depict the string instability characteristic often observed in HVs. This approach allows for a more convincing representation of HV behaviors within our experiments with parameters listed in TABLE 3."}, {"title": "Baseline models", "content": "To analyze the performance and influence of the PERPL framework in a mixed traffic environment, the Linear control and RL models are employed as a baseline for comparison. The Linear control model, similar to a model-based action policy, is calibrated on the training set with parameters set at $K_{d}$ = 0.62 and $K_{v}$ = 0.37. The RL (PPO) model shares a similar structural setup with the residual action policy used in the PERPL framework. These comparisons highlight the distinct advantages of integrating physical principles with RL techniques in managing complex traffic dynamics. Other training settings are listed in TABLE 4."}, {"title": "Results of Single vehicle cruising", "content": "In this section, we compared three different control approaches for a single vehicle following. This setup helped us evaluate the accuracy and generalization capabilities of the PERPL model, compared to two baseline models: RL (PPO), and PERPL (Linear+PPO)."}, {"title": "Data", "content": "In the single vehicle cruising scenario, the data comprises a mix of real-world trajectories from NGSIM (NGSIM, 2007) and a subset of artificially designed trajectories. Trajectories are divided into three sets: training set, test set, and extrapolation set, with each set containing 100 trajectories, containing 500 timesteps with a 0.1s time interval. Each controller is trained on each trajectory for 3000 episodes.\nTraining set and Test set: These are derived from NGSIM trajectories, with acceleration magnitudes limited to within \u00b13 m/s\u00b2. The datasets are randomly split into training and testing subsets.\nExtrapolation set: This set includes trajectories from NGSIM, modified to incorporate extreme acceleration and deceleration events beyond \u00b13 m/s\u00b2. This arrangement allows for a comparative assessment of the generalization capabilities of various methods beyond their training datasets. Notice that the notably darker lines at $a$ = \u22124m/s\u00b2 and $a$ = 3m/s\u00b2. These were intentionally set to simulate extreme scenarios by artificially enhancing the acceleration and deceleration behaviors based on real driving trajectories. This approach is used to test the model's stability under challenging conditions."}, {"title": "Control Performance Evaluation", "content": "As shown in TABLE 5, both RL and PERPL achieved lower headway errors than the Linear controller in both training and test sets. In the extrapolation set, PERPL significantly outperformed the other methods, with headway errors much smaller than those of RL, which were six times higher than those of PERPL. An analysis of the Safety Barrier activation revealed that both PERPL and the Linear controller did not activate the Safety Barrier in any of the scenarios, demonstrating their robustness and consistent achievement of constant headway across all domains. However, the RL approach showed instability in the extrapolation set, with the Safety Barrier being activated 7.92% of the time, indicating that the RL model's control outputs occasionally pushed the state beyond safe limits. Figure 5 illustrates the vehicle following behavior under the three control models for a trajectory from the extrapolation set. Around the 130s mark, when the lead vehicle abruptly decelerated to -2.7, both Linear and PERPL managed to prompt rapid deceleration in the following vehicle, reaching about -3, whereas the RL controller only achieved a deceleration of around -1.7. This lesser response gradually increased headway error until the safety headway limit of 1s was breached around the 170s mark, triggering the Safety Barrier. This forced the vehicle to adopt an extremely sharp deceleration of -12.1 m/s\u00b2 to exit the unsafe condition, preventing a potential collision and resulting in a greater average Headway RMSE."}, {"title": "Results of Mixed-traffic platooning", "content": "Experiments utilizing real-world trajectory datasets are conducted to evaluate the Distributed Reinforcement Learning (DRL)-based control strategy. We constructed a platoon consisting of ten vehicles, labeled from upstream to downstream as 0, 2, 3, 4, 6, 7, and 9 as HVs and the remaining as CAVs. The trajectories for vehicle 0 on I-80 from 4:00 p.m. to 4:15 p.m., a period noted for frequent traffic oscillations, were selected for these experiments. To ensure consistency across trials, each vehicle in the experiment starts from an initial equilibrium state."}, {"title": "Mixed platoon control performance", "content": "Results illustrated in TABLE 6 and Figure 9 of the section reveal that the PERPL model outperformed the other models in terms of headway maintenance, with a lower RMSE, suggesting better precision in following distances. Additionally, the PERPL model showed improved damping ratios and comfort scores, indicating enhanced overall platoon stability and passenger comfort compared to the Linear and standalone RL models. These findings underscore the potential of integrating physics-based control with RL techniques to enhance automated driving systems in complex traffic environments.\nAs an illustrative example, Figure 10 shows the ten-vehicle platoon trajectories of the field data and simulated results using the proposed PERPL and baseline models. It can be seen that under linear control, the following vehicles are most affected by the lead vehicle's stop, with nearly all following vehicles coming to a stop after the lead vehicle stops."}, {"title": "Mixed platoons with different penetration rates", "content": "To visually demonstrate the dampening effectiveness of the proposed control strategy, we applied the proposed method in a 40-follower mixed platoon with different penetration rates (0%, 20%, 40%, 60%, 80%, 100%). In these scenarios, the CAVs were randomly distributed throughout the mixed traffic."}, {"title": "CONCLUSION", "content": "This research presents a control framework based on PERPL for decentralized platoon control. It harnesses the interpretability and robustness of linear control policies alongside the flexible, multi-objective learning capabilities of reinforcement control policies. As such, this model exhibits high control precision, achieving stable headway and reduced traffic oscillation, and it demonstrates strong generalization capabilities, maintaining stability and safety in unseen domains. We applied our proposed model to decentralized control at the mixed traffic platoon level, utilizing CTG for cruising while considering actuator and communication delays. The model's performance was validated through artificially generated extreme scenarios and real-world trajectories. Experimental findings indicate that, both under artificial extreme conditions and with actual vehicle trajectories, our approach yields smaller headway errors and superior oscillation control compared to traditional linear and standalone RL methods. On a macroscopic traffic scale, traffic oscillations diminish as more CAVs adopt the PERPL framework, enhancing overall traffic dynamics.\nFor future research, we plan to leverage the safety features of the proposed method by testing it on both small-scale laboratory vehicles and larger vehicles. This approach will allow for a more realistic consideration of the cumulative effects of errors in perception, communication, and control systems that are commonly encountered in practical deployments. Moreover, we aim to enhance the capabilities of our framework by integrating it with advanced predictive models. This integration seeks to establish an end-to-end control system that reacts to immediate environmental inputs and anticipates future states. By combining real-time control adjustments with foresighted planning, the system could dynamically adapt to changes in traffic conditions, road layouts, and vehicle behaviors, significantly boosting its effectiveness in complex traffic scenarios."}]}