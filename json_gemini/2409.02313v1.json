{"title": "On the Benefits of Memory for Modeling Time-Dependent PDEs", "authors": ["Ricardo Buitrago Ruiz", "Tanya Marwah", "Albert Gu", "Andrej Risteski"], "abstract": "Data-driven techniques have emerged as a promising alternative to traditional numerical methods for solving partial differential equations (PDEs). These techniques frequently offer a better trade-off between computational cost and accuracy for many PDE families of interest. For time-dependent PDEs, existing methodologies typically treat PDEs as Markovian systems, i.e., the evolution of the system only depends on the \"current state\", and not the past states. However, distortion of the input signals e.g., due to discretization or low-pass filtering can render the evolution of the distorted signals non-Markovian. In this work, motivated by the Mori-Zwanzig theory of model reduction, we investigate the impact of architectures with memory for modeling PDEs: that is, when past states are explicitly used to predict the future. We introduce Memory Neural Operator (MemNO), a network based on the recent SSM architectures and Fourier Neural Operator (FNO). We empirically demonstrate on a variety of PDE families of interest that when the input is given on a low-resolution grid, MemNO significantly outperforms the baselines without memory, achieving more than 6\u00d7 less error on unseen PDEs. Via a combination of theory and experiments, we show that the effect of memory is particularly significant when the solution of the PDE has high frequency Fourier components (e.g., low-viscosity fluid dynamics), and it also increases robustness to observation noise.", "sections": [{"title": "Introduction", "content": "Time-dependent partial differential equations (PDEs) are central to modeling various scientific and physical phenomena, necessitating the design of accurate and computationally efficient solvers. Recently, data-driven neural network based approaches [Li et al., 2021b, Lu et al., 2019] have emerged as an attractive alternative to classic numerical solvers, such as finite element and finite difference methods [LeVeque, 2007]. Classical approaches are computationally expensive in high dimension, and struggle with PDEs which are very sensitive to initial conditions. Learned approaches can often negotiate these difficulties better, at least for the family of PDEs they are trained on.\nOne example of a data-driven approach is learning a neural solution operator, which for a time-dependent PDE learns a time evolution map that predicts the solution of the PDE for future time steps. The operators are frequently autoregressively parametrized, such that the network predicts future states based on previous ones [Li et al., 2021a, 2022], and the number of past states the model is conditioned on serves as \u201cmemory\u201d and is treated as a tunable hyperparameter. Recent works [Tran et al., 2023, Lippe et al., 2023] suggest that optimal performance across various PDE families can be achieved by conditioning the models only on the immediate past state-i.e., treating the system as Markovian-though this is in settings in which the training data is very high-resolution.\nIn many practical settings, we expect to only observe a part of the system. This could be due to limited resolution of the measurement devices collecting the data, inherent observational errors in the system, or"}, {"title": "Related Work", "content": "Data-driven neural solution operators [Chen and Chen, 1995, Bhattacharya et al., 2021, Lu et al., 2019, Kovachki et al., 2023] have emerged as the dominant approach for approximating PDEs, given their ability to model multiple families of PDEs at once, and relatively fast computation at inference time.\nRecently, many architectures have been proposed to improve the performance of neural operators across multiple families of PDEs, Li et al. [2021a] designed the Fourier Neural Operator (FNO), a resolution"}, {"title": "Preliminaries", "content": "In this section, we introduce several definitions, as well as background on the Mori-Zwanzig formalism as applied to our setting."}, {"title": "Partial Differential Equations (PDEs)", "content": "Definition 1 (Space of square integrable functions). For integers d, V and an open set \u03a9 C Rd, we define L\u00b2 (N; RV) as the space of square integrable functions u : \u03a9 \u2192 RV such that ||u||12 \u2264 \u221e, where\n||u||L2 = (Jo ||u(x)||dx).\nNotation 1 (Restriction). Given a function u : \u03a9 \u2192 RV and a subset A C \u03a9, we denote un as the restriction of u to the domain A, i.e. u\u013bA : A \u2192 RV.\nThe general form the PDEs we consider in this paper will be as follows:\nDefinition 2 (Time-Dependent PDE). For an open set \u03a9 \u2282 Rd and an interval [0, T] CR, a Time-Dependent PDE is the following expression:\n$\\begin{aligned}\n\\frac{\\partial u}{\\partial t}(t, x) &= \\mathcal{L}[u](t, x), && \\forall t \\in [0, T], x \\in \\Omega, \\\\ \\tag{1}\nu(0,x) &= u_0(x), && \\forall x \\in \\Omega, \\\\ \\tag{2} \\mathcal{B}[u|{\\partial \\Omega}](t) &= 0, && \\forall t \\in [0, T] \\tag{3}\n\\end{aligned}$\nwhere L : L\u00b2 (\u03a9; RV) \u2192 L\u00b2 (\u03a9; RV) is a differential operator in a which is independent of time, uo(x) \u2208 L\u00b2 (\u03a9; RV) and B is an operator defined on the boundary of \u2202\u03a9, commonly referred as the boundary condition.\nUnless otherwise stated, both in the experiments and in the theory we will largely work with periodic boundary conditions:\nDefinition 3 (Periodic Boundary Conditions). For \u03a9 = [0, L]d, we define the periodic boundary conditions as the condition:\nu(x1,\uff65\uff65\uff65, Xk\u22121,0, Xk+1,\u00b7\u00b7\u00b7xd) = u(x1,\uff65\uff65\uff65, Xk\u22121, L, Xk+1,\u00b7\u00b7\u00b7Xd)"}, {"title": null, "content": "for all (x1,\uff65\uff65\uff65,Xk\u22121,Xk+1,\uff65\uff65\uff65,XL) \u2208 [0, L]d\u22121 and all k = 1,\u2026, d.\nFinally, we will frequently talk about a grid of a given resolution:\nDefinition 4 (Equispaced grid with resolution f). Let \u03a9 = [0, L]d. An equispaced grid with resolution f in \u03a9 is the following set SC Rd:\n$S=\\{(\\frac{L}{f}i_1,\\dots,\\frac{L}{f}i_d)\\quad 0\\leq i_k\\leq f-1 \\text{ for } 1\\leq k \\leq d\\}$\nWe will also denote by S the number of points in S."}, {"title": "Mori-Zwanzig", "content": "The Mori-Zwanzig formalism [Zwanzig, 2001] deals with cases where an equation is known for a full system, yet only a part of it is observed. It leverages the knowledge of past observed states of a system to compensates for the loss of information that arises from the partial observation. In our paper, partial observation can refer to observing the solution at a discretized grid in space or only observing the Fourier modes up to a critical frequency. In particular, the Mori-Zwanzig formalism in the context of time-dependent PDEs is well-known in the Physics literature as the Nakajima-Zwanzig equation ([Nakajima, 1958])\nNow, we will apply the Nakajima-Zwanzig equation to our setting. Assume we have a PDE as in Definition 2. Let P : L\u00b2 (N; RV) \u2192 L\u00b2 (\u03a9; RV) be a linear projection operator. We define Q = I \u2013 P, where I is the identity operator. In our setting, for the PDE solution at timestep tut \u2208 L\u00b2 (N; RV), P[ut] is the part of the solution that we observe and Q[ut] is the unobserved part. Thus, the initial information we receive for the system is P[uo]. Applying P and Q to Equation 1 and using u = P[u] + Q[u], we get:\n$\\begin{aligned}\n\\frac{\\partial}{\\partial t}P[u](t, x) &= PL[u](t, x) = PLP[u](t, x) + PLQ[u](t, x) \\tag{4}\\\\\n\\frac{\\partial}{\\partial t}Q[u](t, x) &= QL[u](t, x) = QLP[u](t, x) + QLQ[u](t, x) \\tag{5}\n\\end{aligned}$\nSolving for 5 yields Q[u](t, x) = fo exp{QL(t \u2212 s)}QLP[u](s,x)ds + eQLtQ[uo](t, x).\nPlugging into 4, we obtain a Generalized Langevin Equation [Mori, 1965] for P[u]:\n$\\frac{\\partial}{\\partial t}P[u](t, x) = PLP[u](t, x) + PL \\int_{0}^{t} exp\\{QL(t - s)\\}QLP[u](s,x)ds + PLe^{QLt}Q[u_0](t, x) \\tag{6}$\nWe will refer to the first summand on the right hand side of 6 as the Markovian term because it only depends on P[u](t, x), the second summand as the memory term because it depends on P[u](s, x) for 0 < s < t, and the third summand as the unobserved residual as it depends on Q[uo] which is never observed.\nWe note that Equation 6 is exact, not an approximation, so it is equivalent to solving the full system. Typically, the term that is most difficult to compute is the exponential of the memory term, and thus several methods to approximate it have been proposed. In the physics literature, the memory term has been approximated through a perturbation expansion of the exponential [Breuer and Petruccione, 2002], or by approximating the operator exp{QL(t \u2212 s)} : L\u00b2 (N; RV) \u2192 L\u00b2 (\u03a9; RV) through operators defined in P [L\u00b2 (\u03a9; RV)] [Shi and Geva, 2003, Zhang et al., 2006, Montoya-Castillo and Reichman, 2016, Kelly et al., 2016]. In the machine learning literature, Ma et al. [2018] develop the equations for the case when the operator P kept only the top-k modes, and designed a hybrid approach where the memory term was approximated with an LSTM [Hochreiter and Schmidhuber, 1997], and then used as an additional input of a numerical solver. In this work, we treat the whole memory term as an operator M : C ([0, T], P [L\u00b2 (O; RV)]) \u2192 P [L\u00b2 (\u03a9; RV)]\u00b9 to be learnt by a parametrized sequential layer of a Neural Operator."}, {"title": "Our approach", "content": ""}, {"title": "Training procedure", "content": "First, we describe the high-level training scaffolding for our method, namely the way the data is generated, and the loss we use.\nTraining data: Let u \u2208 C ([0, T]; L\u00b2 (\u03a9; RV)) be the solution of the PDE given by Definition 2. Let S be an equispaced grid in \u03a9 with resolution f, and let T be another equispaced grid in [0,T] with N\u2081 + 1 points. Given uo(x)\u03b9\u03c2, our goal is to predict u(t, x)\u2081s for t \u2208 Tusing a Neural Operator.\nTraining loss: As it is standard, we proceed through empirical risk minimization on a dataset of trajectories. More specifically, given a loss function l : (R|S|, R|S|) \u2192 R, a dataset of training trajectories (u(t, x)(i))Ni=0\nand parametrized maps GO : RIS| \u2192 R|S| for t \u2208T, we define:\n$\\Theta^* = \\min_\\Theta \\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{1}{N_t} \\sum_{t=1}^{N_t} l \\left(u(t, x)|_S, \\mathcal{G}_\\Theta[u_0(x)|_S]\\right)$\nWe then aim to find an adequate architecture choice such that Ge* has low test error on unseen trajectories of the same PDE."}, {"title": "The Architecture: Memory Neural Operator", "content": "In this section we describe Memory Neural Operator (MemNO), a Deep Learning framework to incorporate memory into Neural Operators. Let NO be a Neural Operator with L layers, and denote NO [uo] the prediction of the solution of the PDE at time t. We will assume that this Neural Operator follows the Markovian assumption, i.e. we can write:\n$\\mathcal{N}\\mathcal{O}^{t_{i+1}}_{t_i} [u_0] = \\Gamma_{out} \\circ l_L \\circ l_{L-1} \\circ ... \\circ l_0 \\circ \\Gamma_{in} [\\mathcal{N}\\mathcal{O}^{t_i}_{t_0} [u_0]]$    (7)\nWhere rin: RISI \u2192 R|S|\u00d7ho and rout : R|S|\u00d7hL+1 \u2192 R|S| are projector operators; lj : R|S|\u00d7hj \u2192 R|S|\u00d7hj+1 are parametrized layers; and hj is the dimension of the j-th hidden layer.\nOur goal is to define a network Ge that builds upon NO and can incorporate memory. For this we take inspiration from the Mori-Zwanzig theory exposed in Section 3.2. Comparing Equation 7 with Equation 6, we identify lL \u00b0 lL\u22121 \u00b0 ... \u25cb lo with the Markov term which models the spatial dynamics. To introduce the memory term, we interleave an additional residual sequential layer M that acts on hidden representations of the solution at previous timesteps. Concretely, the MemNO architecture can be written as:\n$\\mathcal{G}_{t_{i+1}} [u_0] = \\Gamma_{out} \\circ \\mathcal{L}_L \\circ... \\circ \\mathcal{L}_{k+1} \\circ \\mathcal{M} \\circ \\mathcal{L}_k ... \\circ \\mathcal{L}_0 \\Gamma_{in} [\\mathcal{G}_{t_i} [u_0], \\mathcal{G}_{t_{i-1}} [u_0], ..., \\mathcal{G}_{t_0} [u_0]]$\nWhere -1 \u2264 k \u2264 L is a chosen hyperparameter.\u00b2 Now, the spatial Lj layers are understood to be applied timestep-wise. That is, if v(i) (t') is the hidden representation at the j layer for a timestep t' < ti, then Lj+1 [v(i) (ti), ..., v(i) (to)] = [lj [v) (ti)],..., lj [v) (to)]], and analogously for Rin and Rout. Thus, the Lj layers still follow the Markovian assumption. The memory is introduced through M, which consists of a sequential layer m that is applied to the time history of the hidden representation of the k-th layer, that is M : Rix|S|\u00d7hk \u2192 R|S|\u00d7hk with (M[v(k) (ti), ..., (k) (to)]) sh = m [v) (ti), ..., (to)]. Note that m is the same for each of the |S| \u00d7 hk elements of the hidden layer. The main motivation of our MemNO framework is that it can be utilized with any existing neural operator layer l, and with any (causal) sequential model M. Thus it provides a modular architecture design which we hope can serve as a useful tool for practitioners."}, {"title": "Theoretical motivation for memory: a simple example", "content": "In this section, we provide a simple, but natural example of a (linear) PDE, along with (in the nomenclature of Section 3.2) a natural projection operator given by a Fourier truncation measurement operator, such that the memory term in the generalized Langevin equation (GLE) can have an arbitrarily large impact on the quality of the calculated solution. We will work with periodic functions over [0, 2\u03c0] which have a convenient basis:\nDefinition 5 (Basis for 2\u03c0-periodic functions). A function f: R \u2192 R is 27-periodic if f(x + 2) = f(x). We can identify 27-periodic functions with functions over the torus T := {ei0 : 0 \u2208 R} \u2286 C by the map f(eix) = f(x). Note that {eixn}nez is a basis for the set of 27-periodic functions.\nWe will define the following measurement operator:\nDefinition 6 (Fourier truncation measurement). The operator Pk : L2(T; R) \u2192 L\u00b2(T; R) acts on f\u2208 L\u00b2(T; R), f(x) = -x aneinx as Pk(f) = \u2211h=-kaneinx.\nWe will also define for notational convenience the functions {en}nez, where en(x) := e-inx + einx. Now we consider the following operator to define a linear time-dependent PDE:\nProposition 1. Let L : L\u00b2(T; R) \u2192 L\u00b2(T; R) be defined as Lu(x) = \u2212\u2206u(x)+ \u0392\u00b7 (e-ix +eix)u(x) for B > 0. Then, we have:\n\u22001 \u2264 n \u2208 N, L(en) = n\u00b2en + B(en\u22121+en+1) & L(eo) = 2Be1\nThe crucial property of this operator is that it acts by \"mixing\" the n-th Fourier basis with the (n - 1)-th and (n+1)-th: thus information is propagated to both the higher and lower-order part of the spectrum. Given the above proposition, we can easily write down the evolution of a PDE with operator Lin the basis {en}nez:\nProposition 2. Let L be defined as in Proposition 1. Consider the PDE\n$\\frac{\\partial}{\\partial t}u(t, x) = Lu(t,x)$\nu(0,x) = \\sum_{n \\in \\mathbb{N}_0} a_n (0)e_n\nLet u(t, x) = Eneno ant)en. Then, the coefficients at satisfy:\n$\\forall 1 \\leq n \\in \\mathbb{N}, \\frac{\\partial}{\\partial t} a_n(t) = n^2 a_n(t) + B(a_{n-1}(t) + a_{n+1}(t)) \\tag{8}\\\\\n\\frac{\\partial}{\\partial t} a_0(t) = 2Ba_1(t)  \\tag{9}$\nWith this setup in mind, we will show that as B grows, the memory term in Equation 6 can have an arbitrarily large effect on the calculated solution:\nTheorem 1 (Effect of memory). Consider the Fourier truncation operator P\u2081 and let Q = I - P1. Let u(0,x) have the form in Proposition 2 for B > 0 sufficiently large, and let a) > 0,\u2200n > 0. Consider the memoryless and memory-augmented PDEs:\n$\\begin{aligned}\n\\frac{\\partial}{\\partial t}u_1 &= P_1Lu_1  \\tag{10}\\\\\n\\frac{\\partial}{\\partial t}u_2 &= P_1Lu_2+ P_1L \\int_0^t exp\\{QL(t - s)\\}QLu_2(s)ds \\tag{11}\n\\end{aligned}$"}, {"title": null, "content": "with u\u2081(0, x) = u\u2082(0,x) = P\u2081u(0,x). Then, u\u2081 and u2 satisfy:\n$\\begin{aligned}\n\\forall t > 0, ||u_1(t) - u_2(t)||_{L_2} &\\geq B t ||u_1(t)||_{L_2} \\tag{12} \\\\\n\\forall t > 0, ||u_1(t) - u_2(t)||_{L_2} &\\geq B t exp(\\sqrt{2} B t) \\tag{13} \\\\\n\\end{aligned}$\nRemark 1. Note that the two conclusions of the theorem mean that both the absolute difference, and the relative difference between the PDE including the memory term Equation 11 and not including the memory term Equation 10 can be arbitrarily large as B,t \u2192 8.\nRemark 2. The choice of L is made for ease of calculation of the Markov and memory term. Conceptually, we expect the solution to Equation 11 will differ a lot from the solution to Equation 10 if the action of the operator L tends to \"mix\" components in the span of P and the span of Q.\nRemark 3. If we solve the equationu(t,x) = Lu(t,x) exactly, we can calculate that ||u(t)||L2 will be on the order of exp(2Bt). This can be seen by writing the evolution of the coefficients of u(t) in the\nbasis {en}, which looks like:t\n$\\begin{pmatrix}\n\\dot{a}_0\\\\\n\\dot{a}_1\n\\end{pmatrix} = \\mathcal{O} \\begin{pmatrix}\na_0\\\\\na_1\n\\end{pmatrix}$\nwhere O is roughly a tridiagonal Toeplitz operator\n$\\mathcal{O} = \\begin{pmatrix}\n0 & B & 0 & ...\\\\\nB & 1^2 & B & 0 \\\\\n0 & B & 2^2 & B & ... \\\\\n... & ... & ... & ...\\\\\n\\end{pmatrix}$\nThe largest eigenvalue of this operator can be shown to be on the order of at least 2B (equation (4) in [Noschese et al., 2013]). The Markov term results in a solution of order exp(\u221a2Bt) (Equation 18,Equation 19), which is multiplicatively smaller by a factor of exp((2 \u2013 \u221a2) Bt). The result in this Theorem shows the memory-based PDE Equation 11 results in a multiplicative \u201cfirst order\u201d correction which can be seen by Taylor expanding exp(\u221a2Bt) \u2248 1 + \u221a2Bt + (\u221a2B)2t\u00b2 + ...."}, {"title": "Memory helps with low-resolution data and input noise: a case study", "content": "In this section we present a case study for several common PDEs of practical interest, showing that MemNO brings accuracy benefits when the data is supplied in low resolution. Through our experiments we show the difference in the performance between a baseline \u201cmemoryless\" architecture, which we choose to be Factorized Fourier Neural Operator (FFNO) [Tran et al., 2023] and a memory-augmented architecture using S4 [Gu et al., 2022], which we denote as the S4-Factorized Fourier Neural Operator (s4FFNO). The architectural details for both the architectures are elaborated upon in Appendix B."}, {"title": "Setup: Training and evaluation procedure", "content": "To construct our datasets, we first produce discretized trajectories of a PDE for Nt timesteps, i.e. (u(t))Nt\nt=0 in a high resolution discretized spatial grid SHR C Rd, i.e. u(t) \u2208 R|SHR|. We then produce datasets that consist of lower resolution versions of the above trajectories, i.e. on a grid SLR of lower resolution f. For 1-dimensional datasets, the discretized trajectory on SLR is obtained by cubic interpolation of the trajectory in the high resolution grid. In 2D, the discretized trajectory is obtained by downsampling. We will show results in different resolutions, in which case both train and test trajectories are at such resolution, and the loss function is also computed at the chosen resolution. Our training loss and evaluation metric is normalized Root Mean Squared Error (nRMSE):\nnRMSE (u(t, x)|SLR, \u00fb(t)) = $\\frac{||u(t, x)|_{S_{LR}} - \\hat{u}(t)||_2}{||u(t, X)|_{S_{LR}} ||_2}$ ,\nwhere || ||2 is the euclidean norm in R|SHR|. More details on training are given in appendix E."}, {"title": "Kuramoto Sivashinsky equation (1D): a study in low-resolution", "content": "The Kuramoto-Sivashinsky equation (KS) is a nonlinear PDE that is used as a modeling tool in fluid dynamics, chemical reaction dynamics, and ion interactions. Due to its chaotic behavior it can model instabilities in various physical systems. For a viscosity v, it is written as ut + uux + Uxx + VUxxxx = 0. We generated datasets for KS at different viscosities and resolutions. The results are shown in Table 1. We can see s4FFNO outperforms FFNO across these viscosities and resolutions, having an nRMSE that can be more than six times smaller. We also note that, since the memory layer is applied element-wise in the time dimension, it has very few parameters compared to the spatial layers, as seen in the difference of parameters between s4FFNO and FFNO in column 3.\nThe key factor for the improved performance of MemNO over memoryless Neural Operators in not the absolute resolution, but rather the resolution relative to the frequency spectrum of the solution. The lower the viscosity, the higher the frequencies that appear in the spectrum. This can be clearly seen in Figure 1: in the"}, {"title": "Navier Stokes equation (2D): study in observation noise", "content": "The Navier Stokes equation describes the motion of a viscous fluid. Like in Li et al. [2021a], we consider the incompressible form in the 2D unit torus, which is given by:\n$\\begin{aligned}\n\\frac{\\partial w(x,t)}{\\partial t} + u(x,t) \\cdot \\nabla w(x,t) &= v\\Delta w(x,t) + f(x), && x \\in (0, 1)^2, t \\in (0,T] \\\\\n\\nabla \\cdot u(x, t) &= 0,  && x \\in (0,1)^2, t \\in [0, \\mathcal{T}] \\\\\nw(x, 0) &= w_0(x), && x \\in (0,1)^2\n\\end{aligned}$\nWhere w = \u00d7 u is the vorticity, wo \u2208 L\u00b2((0, 1)2; R) is the initial vorticity, v \u2208 R+ is the viscosity coefficient, and f\u2208 L\u00b2((0, 1)2; R) is the forcing function. In general, the lower the viscosity, the more rapid the changes in the solution and the harder it is to solve it numerically and with a Neural Operator. We investigate the effect of memory when adding IID Gaussian noise to the inputs of our neural networks. Noise would represent the observation noise arising from the intrisic error of the measurement device. The noise \u0454\u2208 R|T|\u00d7|SLR| is sampled IID from a Gaussian distribution ets ~ N(0,0), and then added to training and test inputs. During training, for each trajectory a different noise with the same ois sampled at each iteration of the optimization algorithm. The targets in training and testing represent our ground truth and are not added noise."}, {"title": "Relationship with fraction of unobserved information", "content": "In this section, we provide a simple experiment to quantify the effect of the fraction of unobserved information on the performance of memory based models. Given a grid of resolution f, we define the Fourier truncation measurement P[4] as in Section 5, which simply keeps the top [5] + 1 modes and discards the other high frequency modes. Assume u(t) \u2208 L\u00b2(\u03a9; RV) is the solution of a 1-dimensional PDE at time t, and ant) for n\u2208 Z is its Fourier Transform. We define the quantity:\nwf = $\\frac{1}{N_t} \\sum_{t_i=1}^{N_t} \\frac{\\sum_{n>\\lfloor\\frac{f}{2}\\rfloor} |\\hat u_n(t_i)|^2}{\\sum_{n \\in \\mathbb{Z}} |\\hat u_n(t_i)|^2}$      (14)\nwf is approximate indicator of the amount of information that is lost when the solution of the PDE is observed at resolution f across time. We show that there is a positive correlation between wf and the difference in nRMSE between FFNO and s4FFNO for the KS experiment in Figure 3, and also the for Burgers' experiments of AppendixC in Figure 5. This demonstrates the benefits of memory as a way to compensate missing information in the\nobservation."}, {"title": "Conclusion and Future Work", "content": "We study the benefits of maintaining memeory while modeling time dependent PDE systems. Taking inspiration from the Mori-Zwanzig formulation, we show that when we only observe part of the PDE initial condition (for example, PDEs observed on low-resolution or with input noise), the system is no longer Markovian, and the dynamics depend on a memory term. To this end, we introduce MemNO, an architecture that combines Fourier Neural Operator (FNO) and the S4 architecture. Through our experiments on different 1D and 2D PDEs, we show that the MemNO architecture outperforms the memoryless baselines.\nWe present several avenues for future work. First, our experiments on observation noise are limited to the setting where the input noise is IID. Further, extending the experiments and observing the effects of memory in more real-world settings (for example, with non-IID noise or in the presence of aliasing) is a fertile ground for future work, and also necessary to ensure that the application of this method does not have unintended negative consequences when broadly applied in society. Lastly, while we limit our study of the effects of memory to FNO based architectures, performing similar studies for different architectures like Transformer based neural operators [Hao et al., 2023] and diffusion based operators [Lippe et al., 2023] is an interesting direction for future work."}, {"title": "Extended Related Work", "content": "Neural Operators. The Fourier Neural Operator (FNO) is a Neural Operator that performs a transformation in the frequency space of the input [Li et al., 2021a]. Other models have proposed different inductive biases for Neural Operators, including physics based losses and constraints [Li et al., 2021b], using Deep Equilibrium Model (DEQ) Bai et al. [2019] to design specialized architectures for steady-state (time-independent) PDEs Marwah et al. [2023], and using local message passing Graph Neural Networks (GNNs) [Gilmer et al., 2017, Kipf and Welling, 2016] based encoders to model irregular geometries [Li et al., 2020, 2024]. Other methodologies to solve for PDEs include methods like Gupta and Brandstetter [2023], Rahman et al. [2023] that use the U-Net [Ronneberger et al., 2015] type architectures and works like [Cao, 2021, Hao et al., 2023] that introduce different Transformer Vaswani et al. [2017] based neural solution operators for modeling both time-dependent and time-independent PDEs. While most of these methodology are designed for time-dependent PDEs, there is no clear consensus of how to model the past-states to predict future states, and most of these methods predict the PDE states over time in an auto-regressive way by conditioning the model on varying lengths of the past states Li et al. [2021a], Tran et al. [2023], Hao et al. [2023].\nFoundation models. Lately, there have been community efforts towards creating large scale foundational models for modeling multiple PDE families [McCabe et al., 2023, Hao et al., 2024, Shen et al., 2024], and weather prediction Pathak et al. [2022], Lam et al. [2022]. We hope that our study is useful in informing the architectural design of future models."}, {"title": "Network Architectures", "content": "Factorized Fourier Neural Operator (FFNO) (Tran et al. [2023]): This model is a refinement over the original Fourier Neural Operator (Li et al. [2021a]). Given a hidden dimension h and a spatial grid S, its layers l: R|S|\u00d7h \u2192 R|S|\u00d7h are defined as:\nl(v) = v + Lineark',h' \u03bf\u03c3\u03bf Linearh',h\u25cbK[v]  (15)\nwhere is the GeLU activation function [Hendrycks and Gimpel, 2016] and h' is an expanded hidden dimension. K is a kernel integral operator that performs a linear transformation in the frequency space. Denoting by FFT, IFFTa are to the Discrete Fast Fourier Transform and the Discrete Inverse Fast Fourier Transform along dimension a [Cooley et al., 1969], it can be written as:\n$K[v] = \\sum_{\\alpha\\in\\{1,...,d\\}} IFFT[R_\\alpha \\cdot FFT_\\alpha[v]]$\nfor learnable matrices of weights Ra \u2208 Ch\u00b2\u00d7kmax. kmax is the maximum number of Fourier modes which are used in K. We use all Fourier modes by setting kmax = [5].\nIn our experiments, The FFNO model consists of 4 FFNO layers. For experiments in 1D, the hidden dimensions are all 128 (hj = 128 for j = 0,1,2,3) and the expanded hidden dimension of FFNO's MLP h' is 4.128. For experiments in 2D, the hidden dimensions are all 64 and the expanded hidden dimension is 4.64.\nS4 - Factorized Fourier Neural Operator (s4FFNO): This model uses our MemNO framework. To discern the effect of memory, all layers except the memory layer will be the same as FFNO. For the memory layer, we choose an S4 layer Gu et al. [2022] with a state dimension of 64 and a diagonal S4 (S4D) kernel."}, {"title": "Burgers' Equation (1D): A study on low-resolution", "content": "The Burgers' equation with viscosity v is a nonlinear PDE used as a modeling tool in fluid mechanics", "as": "nut + uux = \u03bd\u03c5xx\nWe used the publicly available dataset of the Burgers' equation in the PDEBench repository (Takamoto et al. [2023"}]}