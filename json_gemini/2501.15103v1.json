{"title": "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-task Learning", "authors": ["Ziyu Zhao", "Yixiao Zhou", "Didi Zhu", "Tao Shen", "Xuwu Wang", "Jing Su", "Kun Kuang", "Zhongyu Wei", "Fei Wu", "Yu Cheng"], "abstract": "Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MOE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LORA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LORA (SMORA), which embeds MoE into LORA by treating each rank as an independent expert. With a dynamic rank-wise activation mechanism, SMORA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMORA activates fewer parameters yet achieves better performance in multi-task scenarios.", "sections": [{"title": "1. Introduction", "content": "With the remarkable success of LLMs across various domains (Dubey et al., 2024; Liu et al., 2023b), there has been growing interest in fine-tuning them for diverse, task-specific applications (Zhang et al., 2023b). However, full-parameter fine-tuning is often computationally prohibitive, prompting the adoption of Parameter-Efficient Fine-Tuning (PEFT) approaches (Han et al., 2024). Among these, LoRA has gained prominence for its efficiency and modular design (Han et al., 2024; Zhao et al., 2024b).\nDespite its advantages, LoRA encounters a critical limitation in multi-task settings: it uniformly activates all parameters during fine-tuning, without decoupling them to adapt to the specific needs of heterogeneous downstream tasks, as shown in Fig.1a). This lack of task-specific adaptation often leads to performance degradation in multi-task scenarios (Tian et al., 2024; Liu et al., 2023a; Zhao et al., 2024c; Feng et al., 2024).\nTo address this challenge, recent studies have incorporated the MoE framework into LoRA training (Mao et al., 2025; Dou et al., 2023; Liu et al., 2023a; Zadouri et al., 2023). As illustrated in Fig.1b), these approaches construct multiple LoRAs, treating each module as an independent expert. A router is then employed to dynamically activate specific modules, enhancing task adaptation and computational efficiency (Zhu et al., 2023; Li et al., 2024; Feng et al., 2024). However, these methods partition the parameter space into several fixed submatrices, where each block is activated or deactivated as a whole. This block-wise activation is inherently coarse-grained, failing to capture subtle distinctions or shared knowledge across tasks, leading to inefficient utilization of learned knowledge. Furthermore, the rigid division of modules restricts the dynamic combination and decomposition of knowledge, ultimately limiting the model's expressive power and adaptability in multi-task learning scenarios.\nIn this paper, we first provide a unifying perspective, demonstrating that multi-LoRA MoE can be represented as a single LORA, where the rank is partitioned into blocks and each block is activated independently, as shown in Fig. 2 (see \u00a72.2 for details). Through extensive experiments, we further investigate the impact of block granularity on downstream performance under fixed total and activated parameter budgets. Interestingly, we observe that finer-grained partitioning of the rank space leads to significantly better performance, as"}, {"title": "2. Preliminary", "content": "2.1. Low-Rank Adaptation & LoRA MoE\nFine-tuning LLMs with all parameters can be computationally prohibitive, particularly in low-resource scenarios. To mitigate this challenge, Hu et al. (2021) introduced LORA, a method that enables efficient fine-tuning by incorporating a small number of trainable parameters. Instead of directly updating the pre-trained weights Wo \u2208 Rd\u00d7d, LORA represents the update AW using a low-rank factorization:"}, {"title": "2.2. The Equivalence Between Multi-LoRA MoE and Single-Rank Blockwise Activation", "content": "In this section, we construct a unified framework to establish the equivalence between the multi-LoRA MoE and a single LORA with block-wise activations.\nProperty 2.1. Let Wo \u2208 Rd\u00d7d denote a base weight matrix, and consider N LoRA experts parameterized by {(Ai, Bi)}1, where Ai \u2208 Rdxri and Bi \u2208 Rrixd. Assume the presence of a gating function g(x) such that, for each input x \u2208 Rd, the vector g(x) = (91(x), 92(x), ..., gn(x))\u2122 specifies an activation pattern across the experts. Each gi(x) may represent either a sparse activation (e.g., via top-k selection) or a soft activation, with gi(x) \u2208 [0, 1] for all i. Define:"}, {"title": "2.3. Motivation", "content": "As shown in Property 2.1, constructing multiple LoRA experts in multi-LoRA can be unified into a single LoRA framework with block-wise activation, essentially corresponding to segmenting the rank parameter within LoRA into smaller blocks. This equivalence motivates us to investigate how varying granularity in the modular decomposition"}, {"title": "3. Methodology", "content": "In this section, we introduce the proposed SMORA which embeds an MoE into the LoRA architecture to dynamically activate ranks within LoRA for achieving a balance between knowledge sharing and preventing task conflict in multi-task learning."}, {"title": "3.1. Architecture of SMORA", "content": "The overall architecture of SMORA is illustrated in Fig.4. Building upon pre-trained weights Wo \u2208 Rd\u00d7d, SMORA introduces additional trainable low-rank matrices B\u2208 Rdxr and A \u2208 Rrxd, as in vanilla LoRA, where r denotes the rank. Additionally, SMORA extends LoRA by treating each rank as an individual \"expert\" and incorporating a sparse MoE mechanism. A gating function g(x) \u2208 R determines the importance of each rank for a given input x, enabling dynamic sparse activation of the most relevant ranks via top-k routing. The gating matrix G(x) = diag(g1(x), g2(x),...,gr(x)) is diagonal, where gi(x) represents the gating score for the i-th rank expert, making the model both computationally efficient and expressive. The forward pass of SMORA is formulated as:"}, {"title": "3.2. Load Balance with Loss-Free Balancing", "content": "To ensure load balancing during expert routing without introducing additional training overhead and to guarantee seamless adapter deployment across different models, we follow Wang et al. (2024) by incorporating an auxiliary bias term in the gating output. Specifically, the gating function g(x) in Eq.5 is modified as:"}, {"title": "3.3. Implementation of Indexed Sparse LoRA Calculation.", "content": "To enhance the efficiency of sparse matrix computations in SMORA, we developed a custom CUDA kernel, indexed_matmul, using TVM (Chen et al., 2018). This kernel utilizes the top-K indices from the gating function to selectively extract and process relevant rows and columns from the LORA matrices, thereby substantially reducing computational overhead and memory consumption. By performing only the essential operations and bypassing full matrix multiplication, indexed_matmul achieves superior speed and GPU memory efficiency.\nTVM enhances these computations with GPU-specific optimizations, including optimized memory access patterns and effective parallelization. Unlike prior MoE implementations that rely on time-intensive for-loop iterations to process each expert, our approach eliminates these loops by directly targeting non-zero indices. This results in significantly more efficient execution on CUDA-enabled devices, enabling SMORA to support larger models with higher ranks while mitigating time complexity and memory bottlenecks. Detailed implementation and additional discussion on TVM optimizations can be found in Appendix D."}, {"title": "4. Experiments", "content": "Experiment Setup. To validate the efficacy of current PEFT methods in multi-task learning scenarios, we leverage Llama-2-{7b,13b} (Touvron et al., 2023) as the base models and train a range of LoRAs for a spectrum of tasks. We select a portion of the Flan-v2 datasets which contains 48 tasks covering Natural Language Understanding (NLU) and Natural Language Generation (NLG) that can be grouped into 10 distinct task clusters. To further evaluate the effectiveness of our proposed method in domain-specific tasks, we constructed a multi-domain benchmark and tested it using Llama-2-7b. This benchmark spans general language, medical, legal, mathematical, and code generation tasks, following the approach outlined in (Tian et al., 2024). The evaluation set includes: (1) MMLU (Chen et al., 2021) for general ability, (2) Law tasks from MMLU, (3) Medical tasks from MMLU, (4) the GSM8K (Cobbe et al., 2021) evaluation set for mathematics, and (5) HumanEval (Chen et al., 2021) for code generation. Details of the tasks can be found in Appendix A. Additionally, we evaluate the performance of the Llama3-Instruct model (Dubey et al., 2024) after training on code-related datasets, specifically assessing its performance on multiple data analysis tasks using Python libraries. The results of these evaluations are provided in Appendix B.\nBaseline Methods. We compare the proposed method against several state-of-the-art PEFT methods, including (1) LORA with varying numbers of ranks; (2) LORA MOE with top-1 and top-2 routing, where each expert is a LoRA with rank 8; (3) LORA MOE with soft routing, which computes a weighted average of the experts' outputs based on the gating scores; (4) SMEAR, which performs LORA fusion based on the gating scores; (5) HydraLoRA, which leverages an asymmetric structure consisting of one A matrix and four B matrices, each with rank 8; (6) MoSLORA, which incorporates a mixture matrix between the LoRA matrices A and B. Detailed implementations of each baseline method can be found in Appendix C.\nImplementation Details of SMoRA. The total rank of all LORA MoE baselines is set to 64. SMORA follows this setting with a total rank of 64, activating 8 rank parameters with an additional router. The update rate u of the expert routing bias in Eq.6 is set to 1 \u00d7 10-5 for all SMORA experiments. Additionally, the initialization of SMORA's A and B matrices is identical to that of vanilla LoRA, ensuring consistency in the base parameter setup.\nMain Results on Flan-v2 The main results for Flan-v2 are presented in Tab.1. Based on these results, we make the following observations: (1) The performance of Vanilla LORA improves gradually as the rank increases. However, due to task interference, it performs worse than the MoE and SMORA methods when compared under the same number of activated parameters. (2) Due to the partitioning of parameters, MoE methods perform better than LoRA counterparts with the same number of activated parameters. However, as they fail to fully utilize shared knowledge across different tasks, their performance remains suboptimal. (3) By employing rank-wise activation and fine-grained parameter partitioning, SMORA effectively decouples parameters, striking a balance between task interference and knowledge sharing. This allows SMORA to achieve the best performance. (4) Notably, SMORA, which activates only 8 out of 64 ranks in LoRA, outperforms the full Vanilla LoRA model (with all 64 ranks activated), achieving a performance improvement of 1.73% in the Llama2-7b experiment and 1.33%"}, {"title": "5. Related Work", "content": "5.1. PEFT Methods\nFine-tuning large language models (LLMs) requires substantial computational resources, making it impractical in many scenarios. To address this, PEFT methods update only a small subset of parameters while keeping most of the model frozen. Among these, LoRA (Hu et al., 2021) is widely used, reducing trainable parameters by decomposing weight updates into the product of two low-rank matrices, improving efficiency without sacrificing performance. Several extensions of LoRA, such as AdaLoRA (Zhang et al., 2023a), IA3 (Liu et al., 2022), and SoRA (Ding et al., 2023), aim to enhance its flexibility, dynamic rank allocation, and efficiency. However, these methods fix parameters after training, preventing dynamic selection based on input, which limits adaptability in downstream heterogeneous tasks.\n5.2. LORA with MoE\nSome works apply MoE-based methods specifically for multi-task or domain-specific learning. For example, MOELORA (Liu et al., 2023a) focuses on multi-task learning with explicit task labels in medical applications, while LORA-MoE (Dou et al., 2023) with soft routing introduces gating mechanisms to allocate LoRA experts during fine-tuning, alleviating catastrophic forgetting. Similarly, SMEAR (Muqeeth et al., 2023) achieves a dynamic fusion of LORA experts, allowing the model to adjust its parameters adaptively during different tasks. MixLoRA (Li et al., 2024) employs a top-k routing strategy, typically select-"}, {"title": "6. Conclusion", "content": "In this paper, we establish a connection between Multi-LORA MOE and single LoRA by demonstrating that Multi-LORA can be viewed as a single LoRA with block-wise activation. Through further empirical analysis, we find that finer-grained segmentation and activation of LoRA parameters enhance performance in downstream heterogeneous tasks. Building on these insights, we introduce SMORA, which incorporates a MoE structure within LoRA to enable fine-grained, rank-wise activation. To broaden SMORA's applicability, we employ a training-free load-balancing strategy to effectively distribute load among experts. Additionally, we implement a CUDA kernel using TVM to optimize sparse matrix computations within SMORA. Experimental results show that SMORA achieves superior performance in multi-task learning with fewer activated parameters, demonstrating its adaptability and efficiency."}, {"title": "A. Evaluation Benchmark Details", "content": "In our experiments, we evaluate the proposed SMORA across three benchmarks: (1) FLAN-v2, which assesses the natural language understanding (NLU) and natural language generation (NLG) capabilities of large language models (LLMs); (2) Multi-Domain Benchmark, which evaluates the model's performance across multiple vertical domains, including math, general reasoning, law, medicine, and code generation; (3) The DS-1000 dataset, which is specifically designed to test the model's abilities in code generation. Details of these benchmarks are provided in Table 3. Below, we provide a detailed description of each dataset and discuss its relevance to our evaluation."}, {"title": "A.1. FLAN-v2 Subset", "content": "FLAN-v2 is a large-scale collection of instruction-following tasks designed to evaluate the generalization abilities of language models across various domains. The dataset includes a broad range of tasks, such as question answering, summarization, translation, and reasoning. Due to its heterogeneous nature, FLAN-v2 is especially well-suited for evaluating multi-task learning, as it enables the assessment of how effectively a model can adapt to and transfer knowledge across different tasks.\nFor our evaluation, we selected a subset of FLAN-v2 comprising 10 domains and 48 tasks, with a total of 2,395 test samples. This subset provides a balanced representation of various task types and domains, allowing us to rigorously assess SMORA's ability to handle task conflicts and effectively leverage shared knowledge across related tasks. The specific domains and tasks included in our FLAN-v2 subset are as follows:\nStruct-to-Text Conversion. Tasks that involve generating text from structured data. This includes datasets such as CommonGen, DART, E2ENLG, and WebNLG, which test the model's ability to convert structured inputs into coherent natural language descriptions.\nTranslation. Machine translation tasks. We utilize datasets like En-Fr from WMT'14, En-De, En-Tr, En-Ru, En-Fi, En-Ro from WMT'16, and En-Es from Paracrawl to assess the model's proficiency in translating text across multiple languages while preserving meaning and nuances."}, {"title": "Commonsense Reasoning", "content": "Tasks requiring commonsense reasoning, such as understanding everyday scenarios. Datasets like COPA, HellaSwag, PiQA, and StoryCloze are used to evaluate the model's ability to apply physical or scientific principles alongside common sense in reasoning tasks."}, {"title": "Sentiment Analysis", "content": "Sentiment analysis tasks, including binary and multi-class classification. We employ datasets such as IMDB, Sentiment140, SST-2, and Yelp to determine the sentiment polarity (positive or negative) of given texts."}, {"title": "Reading Comprehension", "content": "Reading comprehension tasks, where the model answers questions based on provided passages. Datasets like BoolQ, DROP, MultiRC, OBQA, SQuADv1, and SQuADv2 are used to assess the model's capability to derive answers from contextually relevant information."}, {"title": "Closed-Book Question Answering", "content": "Closed-book question answering tasks, where the model answers questions without access to external knowledge. We use datasets such as ARC, NQ, and TriviaQA to challenge the model's ability to recall and apply general knowledge."}, {"title": "Coreference Resolution", "content": "Coreference resolution tasks, which involve identifying expressions that refer to the same entity in a text. Datasets like DPR and WSC273 are used to evaluate the model's understanding of textual context and entity references."}, {"title": "Reading Comprehension with Commonsense", "content": "Reading comprehension tasks with commonsense reasoning, combining text understanding and reasoning. Datasets such as CosmosQA and ReCoRD are used to assess the model's ability to understand and reason beyond the explicit text."}, {"title": "Paraphrase Detection", "content": "Paraphrasing tasks, where the model generates alternative phrasings of given sentences. We use datasets like MRPC, QQP, and Paws Wiki to evaluate the model's ability to detect and generate semantically equivalent sentences."}, {"title": "Natural Language Inference", "content": "Natural Language Inference (NLI) tasks, which involve determining the logical relationship (entailment, contradiction, or neutral) between pairs of sentences. Datasets such as ANLI, CB, MNLI, QNLI, SNLI, WNLI, and RTE are used to assess the model's ability to infer relationships between sentences."}, {"title": "A.2. Multi-Domain Benchmark", "content": "We construct a multi-domain evaluation dataset encompassing three major benchmarks: MMLU, GSM8K, and Humaneval, designed to comprehensively assess model capabilities across multiple dimensions. The dataset comprises:\n\u2022 MMLU: A multiple-choice dataset spanning 57 academic disciplines, covering STEM, humanities, and social sciences, testing models' breadth of knowledge and reasoning abilities.\n\u2022 GSM8K: A collection of 8.5K grade-school math word problems requiring multi-step reasoning to solve complex mathematical questions.\n\u2022 HumanEval: A code generation benchmark containing 164 hand-crafted programming problems that evaluate models' ability to generate functionally correct Python code from natural language descriptions.\nThis multi-domain benchmark establishes evaluation challenges across three dimensions: MMLU examines cross-disciplinary knowledge integration, GSM8K verifies mathematical-logical reasoning, and Humaneval tests programmatic semantic understanding. By synthesizing these distinct evaluation perspectives, we systematically analyze SMORA's comprehensive performance in knowledge-intensive tasks, complex problem-solving, and code-generation scenarios."}, {"title": "A.3. DS-1000", "content": "DS-1000 is a benchmark specifically designed to evaluate code generation in the data science domain. It consists of 1,000 coding problems across 7 different Python tasks: Pandas, Numpy, Matplotlib, Scipy, Sklearn, Pytorch, and Tensorflow. Each problem includes a natural language description, and the model is tasked with generating the corresponding code snippet."}, {"title": "A.4. Evaluation Metrics", "content": "We employ a fine-grained evaluation metric system tailored to task characteristics:\n\u2022 FLAN-v2 Subset: Preserves original evaluation protocols including ROUGE (text generation), BLEU (machine translation), accuracy (classification tasks), and exact match (QA tasks).\n\u2022 Multi-Domain Benchmark:\nMMLU: Strict accuracy measuring exact match between predictions and ground-truth answers.\nGSM8K: Step-wise exact match requiring both correct reasoning chains and final answers.\nHumaneval: Pass@k metric (recommended k=1,10) validating functional correctness through unit tests.\n\u2022 DS-1000: We use pass@1 to evaluate code generation performance.\nThese metrics allow us to quantify the effectiveness of SMORA in both multi-task learning and domain-specific adaptation scenarios, providing a holistic view of its capabilities."}, {"title": "B. Main Results on DS-1000", "content": "Table 4 shows the results on the DS-1000 dataset to demonstrate the effectiveness when adapting to specific downstream tasks such as code generation. The results also show that: (1) LORA with full rank activation performs poorly on the code dataset due to the impact of data heterogeneity; (2) Methods like MoE, which decouple parameters, can alleviate task interference to some extent, and their performance improves as the number of activated experts increases; (3) SMORA, with only 8 rank parameters activated, achieves the best performance, as its fine-grained parameter decoupling effectively avoids task interference while retaining shared knowledge, thus achieving optimal results on downstream tasks."}, {"title": "C. Implementation Details of Baseline Methods", "content": "In this section, we provide a detailed explanation of the baseline methods used for comparison with the proposed SMORA. Each method is described with its core idea, mathematical formulation, and specific implementation details. For all methods, we use Kaiming initialization for matrix A and zero initialization for matrix B in the LoRA modules, unless otherwise specified."}, {"title": "C.1. LORA with Different Ranks", "content": "Idea: LoRA reduces the number of trainable parameters by injecting low-rank decomposition matrices A and B into the pre-trained model's weights. The key idea is that the weight updates during fine-tuning lie in a low-dimensional subspace, allowing efficient adaptation with minimal parameters.\nFormulation: For a pre-trained weight matrix Wo, the update is constrained to a low-rank decomposition:"}, {"title": "C.2. LoRA MoE with Different Routing Strategies", "content": "LORA MOE methods introduce multiple LoRA modules (experts) and use a router to dynamically select and combine the outputs of these experts. This approach mitigates task interference by isolating task-specific adaptations while maintaining parameter efficiency.\nFormulation: Let the parameter of i-th LoRA expert denotes as {Ai, Bi} and G denote the router. The output of the MoE system can be expressed as:"}, {"title": "Dense Gating", "content": "The router consists of a dense layer with trainable parameters Wg. The gating scores are computed using a softmax function:"}, {"title": "Top-1 Gating", "content": "To maintain sparsity during training, we leverage the Gumbel softmax trick. The router can be written as:"}, {"title": "Top-k Gating", "content": "For the Top-k routing strategy, we use a routing mechanism similar to Mixtral (Jiang et al., 2024), which selects the top-k experts based on their gating scores and combines their outputs accordingly. Specifically, the gating function is defined as: g(x) = Softmax(TopK(xWg)), where only the top-k experts are considered for each input."}, {"title": "C.3. SMEAR", "content": "Idea: SMEAR aggregates LoRA modules at the parameter level rather than at the output level. This allows for more flexible adaptation by combining the parameters of multiple LoRA modules based on gating scores.\nFormulation: The aggregated parameters OSMEAR are computed as:"}, {"title": "C.4. HydraLoRA", "content": "Idea: HydraLoRA introduced an asymmetric LoRA structure. The core idea is to decompose the traditional LoRA into a shared central matrix A and multiple distinct matrices Bi, enabling both knowledge sharing and functional specialization.\nFormulation: HydraLoRA introduces multiple independent matrices B\u2081 that share a central matrix A. Each B\u00bf is modulated by a contribution weight wi, allowing for task-specific adaptations while maintaining shared knowledge through A. The forward process is modified as:"}, {"title": "C.5. MoSLORA", "content": "Idea: MOSLORA introduces a trainable mixer W to fuse subspace information more effectively, enhancing the model's flexibility and expressive power. Unlike traditional LoRA, which uses a fixed identity matrix for subspace fusion, MoSLORA employs a trainable mixer to enable richer interactions between subspaces.\nFormulation: The core idea of MoSLORA is to replace the fixed mixer in traditional LoRA with a trainable matrix W\u2208 Rrxr:"}, {"title": "D. TVM Implementation of indexed matmul", "content": "TVM (Chen et al., 2018) is an end-to-end deep learning compiler stack designed to optimize machine learning workloads across diverse hardware backends. It enables efficient computation by providing automated tensor optimization techniques such as operation fusion, memory layout transformations, and parallelization, making it particularly suitable for high-performance deep learning tasks.\nIn our work, we implemented a custom CUDA kernel, indexed_matmul, using TVM to accelerate sparse matrix computations in SMORA. The key innovation of indexed_matmul lies in its dynamic handling of sparse operations. Instead of performing a full matrix multiplication, the kernel leverages the top-K indices from the gating function to dynamically extract and"}, {"title": "Efficient Memory Access", "content": "The computation was scheduled with fine-grained memory access patterns to minimize memory latency and maximize bandwidth utilization on CUDA-enabled GPUs."}, {"title": "Thread and Block-Level Parallelism", "content": "Using TVM's scheduling primitives, we split and reordered computation across blocks and threads, enabling efficient parallel execution of sparse operations."}, {"title": "Reduction Fusion", "content": "The reduction axis in the sparse matrix computation was fused and optimized to minimize intermediate memory usage and improve computational efficiency."}, {"title": "Dynamic Index-Based Computation", "content": "By focusing on the non-zero indices identified by the gating mechanism, the kernel avoids loading or processing irrelevant data, further improving execution speed and memory efficiency."}, {"title": "E. Full Results on FLAN-v2", "content": "In this section, we present the full results of our experiments conducted on FLAN-v2, covering 10 domains and 48 tasks. The results for the two base models, Llamma-{7b,13b}, are shown in Tab.5 and Tab.6."}, {"title": "F. Difference between SMORA and MoSLORA", "content": "Although the forward modeling of SMoRA is given by y = Wox + BG(x) Ax and the forward of MoSLORA can be written as y = Wox + BW Ax, they may appear similar in form, but they are fundamentally different. Firstly, in MoSLORA, the mixture matrix is fixed after training, unlike SMORA, where expert selection is based on the current token. This leads to MOSLORA introducing more parameters and losing the flexibility of dynamic routing. Additionally, MoSLORA activates and mixes all the LoRA ranks, whereas we adopt a dynamic sparse activation approach to efficiently decouple parameters. This allows us to achieve better performance with fewer active parameters."}, {"title": "G. Visualization of Routing Distribution Under Load Balancing Strategy Ablation", "content": "To further investigate the role of the load-balancing strategy, we visualized the routing distribution under two conditions: with and without load balancing in Fig.9. The visualization clearly demonstrates that without load balancing (left), the routing is highly concentrated on a few experts, leading to an uneven workload distribution. In contrast, when the load balancing strategy is applied (right), the routing becomes more evenly distributed across the experts, ensuring a better balance in workload. This highlights the effectiveness of the load balancing mechanism in promoting fair utilization of all available experts."}, {"title": "H. Comparison of Parameter Counts Across Methods", "content": "In Fig.10, we analyze the number of activated parameters across different methods to assess their effectiveness. Regarding the total activated parameters, SMORA exhibits a higher router parameter count. While it outperforms dense activation methods such as LoRA-64, MoE-Soft, SMEAR, and MoSLoRA, it also activates more parameters compared to sparse activation methods like MoE-top1 and MoE-top2. This is due to its finer-grained expert decomposition, which increases the router parameter count, leading to a higher overall parameter activation. However, when focusing on the activated LoRA parameters, SMORA activates significantly fewer parameters than all other methods. Ultimately, SMoRA achieves superior performance compared to all methods, regardless of whether they rely on dense or sparse activation strategies."}]}