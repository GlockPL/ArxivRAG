{"title": "A Survey of Text Classification Under Class Distribution Shift", "authors": ["Adriana Valentina Costache", "Silviu Florin Gheorghe", "Eduard Gabriel Poesina", "Paul Irofti", "Radu Tudor Ionescu"], "abstract": "The basic underlying assumption of machine learning (ML) models is that the training and test data are sampled from the same distribution. However, in daily practice, this assumption is often broken, i.e. the distribution of the test data changes over time, which hinders the application of conventional ML models. One domain where the distribution shift naturally occurs is text classification, since people always find new topics to discuss. To this end, we survey research articles studying open-set text classification and related tasks. We divide the methods in this area based on the constraints that define the kind of distribution shift and the corresponding problem formulation, i.e. learning with the Universum, zero-shot learning, and open-set learning. We next discuss the predominant mitigation approaches for each problem setup. Finally, we identify several future work directions, aiming to push the boundaries beyond the state of the art. Interestingly, we find that continual learning can solve many of the issues caused by the shifting class distribution. We maintain a list of relevant papers online\u00b9.", "sections": [{"title": "1 Introduction", "content": "The primary assumption of any machine learning (ML) model is that the data is independent and identically distributed (IID) (Vapnik, 1995). In learning theory, the IID assumption plays a critical role, as it guarantees the generalization capacity of models, provided that sufficient training data is available, and that the hypothesis class is not too large. However, this assumption does not always hold in daily practice. One such example is text categorization by topic, where new topics naturally emerge as the interest of people changes over time. For example, journalists started publishing news articles about the COVID-19 pandemic only after its outbreak began in December 20192. Hence, if a classifier is trained in a closed-world setup (a scenario where it is presumed that every instance encountered by the model belongs to a class that is present in the training data), there is a high risk that the respective model will encounter instances from classes that were not present in the training data, during operation. Such instances will be misclassified as belonging to one of the training classes, degrading the overall performance of the ML system, and implicitly reducing user satisfaction.\nA large body of works, e.g. (Geng et al., 2021; Salehi et al., 2022; Vaze et al., 2021; Yang et al., 2024), studies the adaptation and application of ML models when the class distribution changes from training time to inference time. One of the first articles to highlight this problem is that of Scheirer et al. (2013). The authors define the open-set risk as the sum of the empirical risk and the open-space risk, the latter being defined as the risk of misclassifying instances of unknown classes as belonging to a known class. Over time, researchers attempted to minimize the open-set risk by addressing a broad range of formulations of the class distribution shift problem. These formulations can be mainly structured according to the class categories proposed by Geng et al. (2021).\nWe present the class categories and the corresponding problem formulations in Figure 1. We further define the class categories and provide representative studies for each problem formulation in Table 1. KKCs lead to the conventional supervised learning paradigm, which is well-studied and results often surpass human-level performance (Cozma et al., 2018; Tedeschi et al., 2023). Frameworks that deal with KUCs typically use a background (a.k.a. Universum) class (Dhamija et al., 2018; Weston et al., 2006), essentially extending the supervised learning setup with a new category where all the examples that do not belong to any of the KKCs are placed. The zero-shot learning paradigm (Chaudhary et al., 2024; Pourpanah et al., 2023; Yin et al., 2019) aims to deal with UKCs, namely with classes that are known in advance (at training time), but for which there are no data samples. Zero-shot learning aims to classify data samples into UKCs, but the main limitation is that the set of UKCs is fixed. As a consequence, dealing with UUCs is not possible in the zero-shot learning setup. Open-set learning (Geng et al., 2021; Scheirer et al., 2013) aims to partially address this challenge by identifying UUCs without having prior information about such classes during training. Sometimes, UUCs are identified via a two-stage pipeline that combines an outlier detection (a.k.a. novelty detection) model (Chen et al., 2023; Walkowiak et al., 2018, 2020) and a supervised model trained on KKCs. However, unlike zero-shot learning methods, open-set methods do not aim to classify the data samples into UUCs. A more comprehensive setup is proposed in (Zheng et al., 2022), where the authors aim not only to detect samples belonging to UUCs, but also to classify them. This framework, called open-set learning and discovery, can be seen as a generalization of zero-shot learning, where the set of UKCs can be updated during inference, therefore transforming it into a set of UUCs.\nTo date, open-set classification and the related tasks discussed above were mostly studied in the vision domain, where various tasks have been explored, such as object recognition (Kong and Ramanan, 2021; Scheirer et al., 2013; Vaze et al., 2021), semantic segmentation (Cen et al., 2021; Oliveira et al., 2021), object detection (Dhamija et al., 2020; Liu et al., 2024; Zheng et al., 2022), and video anomaly detection (Acsintoae et al., 2022; Wu et al., 2024), among others. Comparatively less attention has been dedicated to this family of tasks in the text domain (Chen et al., 2024; Fei and Liu, 2016; Kim et al., 2022). However, class distribution shift is a prevalent phenomenon in text classification. Aside from the example provided earlier about the changing topics over time, there are many other natural language processing (NLP) tasks where the class distribution can shift. For example, in authorship identification, new authors can emerge overtime, so an authorship identification model needs to update the list of potential authors. Another task affected by class distribution shift is intent detection in conversational A\u0399. For instance, a chatbot trained to recognize intents such as \u201cbook flight\u201d or \u201ccheck weather"}, {"title": "2 Notations", "content": "We use the following notations to define the various task formulations covered in our survey. Let x \u2208 X represent a text sample from the data space X, and y\u2208 Y a label from the label space Y. The label space is divided into four disjoint subsets denoted as known known classes (Ckk), known unknown classes (Cku) unknown known classes (Cuk), and unknown unknown classes (Cuu), such that Ckk \u222a Cku \u222a Cuk \u222a Ckk = Y and the intersection between any two subsets is the empty set, e.g. Ckk \u2229Cuk = \u00d8."}, {"title": "3 Learning with Background Class", "content": "Formal definition. Learning with background / Universum class aims to detect samples from Cku, while having samples from both Ckk and Cku. Formally, the training data is defined as D = {(x, y) | x \u2208 X, y \u2208 Ckk \u222a Cku}, while the test data is defined as T = {(x,y) | x \u2208 X, y \u2208 Ckk \u222a Cku}. In this setup, a classifier is defined as h : X \u2192 Ckk \u222a {background}, i.e. for any sample that belongs to Cku, the model should label the respective sample as background.\nTrends and approaches. The problem of learning with a background class has been addressed through proxy tasks such as out-of-distribution (OOD) detection or outlier rejection (Dhamija et al., 2018). These approaches improve the reliability of the model by managing uncertainty and rejecting inputs outside the distribution of KKCs. Research in this area has been predominantly driven by computer vision studies, whereas text classification remains comparatively underexplored. However, many techniques can be easily adapted to NLP tasks by replacing image-based feature extractors with robust language encoders.\nA straightforward approach to detect background samples involves training a classifier to refine the decision boundary between the original dataset and a curated background dataset. This technique is commonly employed in computer vision tasks (Mullapudi et al., 2021), leveraging the abundance of background images available, e.g. iNaturalist-BG, Places-BG, etc.\nAnother common approach is to employ custom regularization techniques for background-class learning. For Support Vector Machines (SVMs), Weston et al. (2006) introduce a penalty term that incorporates the distance between the decision boundary and background-class (Universum-class) data points. Their method is based on the principle that Universum-class samples should lie near the decision boundary, reflecting real-world uncertainty and ambiguity in classification. This regularization technique enforces a decision-making process that accounts for inherent uncertainty in the data. Dhamija et al. (2018) introduce a more generic regularization based on two loss functions. The Entropic Open-Set Loss aims to reduce entropy for KKCs, while maximizing entropy for KUCs, ensuring that the model treats unfamiliar data with the highest possible uncertainty. The Objectosphere Loss strengthens the separation between KKCs and KUCs by increasing the feature magnitudes for KKC samples, while reducing them for KUCs. These loss functions create a more distinct feature space, improving the ability of the model to reject out-of-distribution inputs."}, {"title": "4 Zero-Shot Text Classification", "content": "Formal definition. Zero-shot learning aims to recognize samples from Cuk using knowledge transferred from Ckk. Formally, the training data is defined as D = {(x, y) | x \u2208 X, y \u2208 Ckk}, while the test data is defined as set T = {(x,y) | x \u2208 X,y \u2208 Cuk}. Knowledge is usually transferred via an auxiliary semantic space A, e.g. word embeddings or attribute vectors, such that f : X \u2192 A and g: A\u2192 Y. The mapping f learns to project features into the semantic space, while g maps semantic representations to class labels.\nTrends and approaches. When no examples from the target classes are available, various alternative tasks can be leveraged, with or without additional training, to perform zero-shot text classification. For instance, Yin et al. (2019) propose using the entailment task as a zero-shot text classification task. To decide if a text x should be classified into a certain class, e.g. \u201cpolitics\u201d, one can ask if x entails \"The previous text is about politics\". This procedure is applied for each of the possible classes, and the one with the best confidence is considered the correct one. Using this technique or similar ones, any model capable of solving the entailment problem can implicitly be used to perform zero-shot text classification.\nInstruction tuning, defined as fine-tuning language models on a collection of datasets described via instructions, can be seen as a generalization over the framework proposed by Yin et al. (2019). Instruction tuning can lead to better performance on unseen tasks, if the target task is described via simple instructions provided in natural language. Models trained with instruction tuning can perform various tasks, including zero-shot classification, as shown by Zhang et al. (2024). This capability is achieved by training language models to respond to simple natural language commands, with some degree of generality. For instance, Wei et al. (2022) and Sanh et al. (2022) explore similar ideas, dividing the tasks for which datasets are available into separate clusters, each containing multiple datasets. Fine-tuning via instructions on any of the clusters leads to performance gains when the model is tested on tasks from the other clusters. Xu et al. (2022) continue this line of work by increasing the number of tasks used for pre-training from a few dozen to over 1000, showing that increasing the number of tasks is a good alternative to increasing the model size.\nAnother possible approach for zero-shot classification is to use a large language model (LLM) to generate a synthetic dataset S, tailored for the target task. Further, S can be used as training data in a fully-supervised setup, either to fine-tune an LLM or to train a conventional model from scratch. Although the dataset S can be made arbitrarily large, it can quickly hit diminishing returns, as it tends to contain redundant data (Ye et al., 2022). Moreover, the performance obtained by using S is not as high as it would be when using a real human-labeled dataset of comparable size. To this end, several efforts have been made to improve the quality of S (Meng et al., 2022; Ye et al., 2022). A generic dataset, such as Wikipedia, is sometimes employed as a source of additional diversity. Meng et al. (2022) use controlled text generation (Hu et al., 2017) to direct an LLM to generate texts that are relevant for a specific class label. To increase sample diversity, repeating sequences are penalized. Alternatively, Yu et al. (2023) use Wikipedia as a general purpose corpus. In their method, called Re-Gen, a retrieval model is trained using contrastive learning for the task of finding the most relevant documents from the corpus. ProGen (Ye et al., 2022) and ZeroGen (Ye et al., 2022) represent other alternatives to generate synthetic datasets. Zero-Gen uses prompt engineering to generate S from scratch, without the need of additional data. Pro-Gen employs a feedback loop to iteratively refine a task-specific model.\nSome studies relax the zero-shot learning setup in various ways, such as reducing the zero-shot setup to a subset of classes, while providing labeled examples for the others, or considering the availability of an unlabeled set of samples from the target classes, but without knowing the corresponding class labels (see Figure 2). For example, Meng et al. (2020) train a model on a subset originating from the same distribution as the dataset used for evaluation, but without labels. This can be a valid setup, depending on the specific problem at hand, mainly when data is available, but labeling is difficult. Gera et al. (2022) adopt a similar approach, based on self-training and entailment, for an instruction model. A model ho assigns pseudo-labels based on the confidence of entailment with \"This text is about <class>\u201d. The resulting candidates are filtered and used to train a new model h\u2081 that is further used to assign new pseudo-labels, and so on. Although this method may look similar with the one proposed by Yin et al. (2019), Gera et al. (2022) use the entailment task to assign pseudo-labels to a dataset used for downstream training of a target model, while Yin et al. (2019) directly employ entailment for classification."}, {"title": "5 Open-Set Text Classification and Discovery", "content": "Formal definitions. Open-set learning aims to classify samples from Ckk, while rejecting samples from Cuu encountered during inference. The training set is defined a D = {(x, y) | x \u2208 X, y \u2208 Ckk}, while the test set is given by T = {(x, y) | x \u2208 X, y \u2208 Ckk \u222a Cuu}. An open-set learning model is defined as h : X \u2192 Ckk \u222a {unknown}, i.e. for any sample that belongs to Cuu, the model should label the respective sample as unknown. In the open-set learning and discovery setup, the model is defined as h : X \u2192 Ckk \u222a Cuu, i.e. the model must be able to classify any data sample, regardless of the fact that the sample belongs to Ckk or Cuu. To classify samples into Cuu, classes belonging to Cuu must be discovered during inference.\nTrends and approaches. A simple way to address the open-set learning problem is to classify the data using a closed-set algorithm, then estimate how well each sample corresponds to the assigned class. If the confidence (or similarity) is under a certain threshold, the example is considered to belong to the open space. In practice, this threshold can be difficult to set, because, in general, the ratio of instances belonging to Cuu in the test set T cannot be estimated without prior knowledge. This problem is already identified and discussed by Scheirer et al. (2013), who frame the issue as an effort to harmonize the empirical risk, measured on the training data with the unmeasurable open-space risk, which is unknown.\nThere are a few methods that avoid estimating this threshold. Starting from the observation that there are spaces where feature vectors of instances from the same class generally reside close together, some researchers use methods specific to clustering. In this context, an outlierness factor can be calculated to decide if an instance should be classified into a class or allocated to the open space. If an example is an outlier, it probably belongs to the open space. Various outlierness factors have been proposed so far. Walkowiak et al. (2019a) employ the Local Outlier Factor (LOF), which uses a weighted Euclidean distance to find outliers based on the distance to their local neighbors. Subsequent studies of the same authors (Walkowiak et al., 2019b, 2020) use LOF as well. Another factor, called Angle-Based Outlier Factor, which was originally introduced by Kriegel et al. (2008), is later adapted for text classification by Walkowiak et al. (2018), under the name ABOF2.\nAnother solution to avoid setting an arbitrary threshold is to identify the examples belonging to open space before performing the classification task. The problem is therefore broken into two sub-tasks, outlier detection (OD) and closed-set classification, which can be treated independently. In this setup, the classification part is closed, hence OD is the difficult part.\nOpen-set solutions via outlier / novelty detection. Kannan et al. (2017) divide the outlier detection algorithms for text in three subcategories: distance-based, density-based, and subspace-based. More recently, energy-based models have also been explored as a promising method for detecting out-of-distribution data. Grathwohl et al. (2019) propose a Joint Energy-Based Model (JEM) by reinterpreting a softmax-based classification network as a generative model. This approach uses the logits to define an energy-based representation of the joint distribution of data points and labels. Another notable contribution to outlier detection is the typicality test, introduced by Nalisnick et al. (2019). This method determines whether a given data point belongs to the model's typical set, rather than relying solely on likelihood estimation. The test is based on the observation that deep generative models can assign higher likelihoods to OOD data than to in-distribution samples. By focusing on typicality instead of raw probability density, this method offers a more robust approach to detect outliers.\nWe emphasize that there are two main types of distribution shifts (Baran et al., 2023): semantic shift, defined as the occurrence of new classes, and background shift, defined as changes that are class-agnostic, such as the level of formality, stance, and so on. If the OD detects background shift, the examples belonging to Ckk that exhibit background shift will not even make it to the classifier. Consequently, the outlier detector should only detect semantic shift, but this is difficult without access to outliers during training. The subject of semantic versus background shift in text is discussed in detail by Arora et al. (2021). While outlier detection in general is very well studied (Wang et al., 2019), there are very few articles focused on text, especially ones trying to isolate semantic shift. Yet, a simple way to detect semantic shift is to use rare terms (Mohotti and Nayak, 2020).\nOpen-set semi-supervised text classification. Semi-supervised text classification (STC) has its own open-set variant, called open-set semi-supervised text classification (OSTC), introduced by Chen et al. (2023). For the STC task, the training set consists of some labeled examples from each class, and many additional unlabeled examples. In this setup, all the unlabeled examples are presumed to belong to Ckk. This assumption is difficult to enforce in practice. In OSTC, the assumption is relaxed such that the unlabeled examples can belong to both Ckk and Cuu, making the problem open. Since samples from Cuu are available during training, we consider that OSTC methods have a clear advantage over open-set learning methods, even though the available samples are unlabeled.\nChen et al. (2024) use adversarial disagreement maximization to increase the difference between in-distribution and OOD examples, improving on their previous solution (Chen et al., 2023). Kim et al. (2022) propose a harder variant of OSTC, that is zero-shot. In this new setup, there are no labeled examples at all. The training data consists of a set of n unlabeled texts X1,..., In that must be classified into k classes C1, ..., Ck. Each class Ci is only specified through a set of words. The proposed method obtains better results than the corresponding closed-set solutions (Meng et al., 2020; Wang et al., 2021), when tested in an open-set setting.\nOpen-set learning and discovery. Ideally, during inference, an open-set system should be able to recognize patterns in the open data and dynamically create new relevant classes that would complement the set of classes Ckk provided at train time. A solution to this problem should involve: (i) an open-set text classification model to initially split the corpus into Ckk and Cuu, (ii) a topic mining method to identify new relevant topics in the open space, and (iii) a zero-shot learning model to classify samples into the newly found topics. A further challenge is posed by the fact that the new classes should be semantically consistent with the existing classes. For example, if the existing classes are \u201csports\", \"politics\" and \"economics\u201d, a new class called \"culture\" is compatible, but one called \u201cbreaking news\" is not. To obtain semantically equivalent classes, a knowledge base, such as WordNet, might come in handy. A possible architecture for the open-set learning and discovery setup is illustrated in Figure 3. To the best of our knowledge, there is no framework for open-set learning and discovery in the field of text classification.\""}, {"title": "6 Continual Learning from Text", "content": "Formal definition. Continual learning (CL) is a paradigm in machine learning focused on enabling systems to continuously learn from a stream of data over time. Unlike traditional approaches that assume a static data distribution and rely on fixed datasets, CL addresses challenges such as retaining knowledge from previous tasks (avoiding catastrophic forgetting) and adapting to new tasks without extensive retraining (Parisi et al., 2019). CL can be defined as the process of training a model on a sequence of tasks, where each task t can be characterized by a distinct data distribution. At any point in time, the model receives a batch of training samples Dt,b = {(x,y) | x \u2208 Xt,b, y \u2208 Yt,b}, where Xt,b represents a subset of input samples and Yt,b are the corresponding (task-specific) labels. The task identity is given by t \u2208 {1, ..., \u221e}, while the batch index is represented by b \u2208 {1, ..., \u221e}. Theoretically, the continual learning process continues indefinitely during operation, hence the indexes t and b can grow endlessly. A task t is formally defined by its data distribution P(Xt, Yt) (Wang et al., 2024).\nTrends and approaches. CL is a relevant framework for open-set learning, as it can address challenges related to the open-set scenarios, especially in tasks such as text classification by topic when the topic distribution changes. These challenges include handling unknown classes and adapting to an evolving data distribution. CL can help to discover and learn new topics during inference. Both Chen and Liu (2014) and Gupta et al. (2020) propose frameworks that enable models to accumulate knowledge over time, leveraging past knowledge to improve topic extraction, while mitigating catastrophic forgetting. These approaches integrate lifelong knowledge retention and transfer, ensuring that previously learned topics inform future learning tasks. By incorporating prior knowledge, these models enhance topic coherence, particularly in sparse data scenarios, where limited labeled data can affect the quality of extracted topics. Chen and Liu (2014) introduce the Lifelong Topic Model (LTM), which mines prior knowledge from past document collections and integrates it into new topic modeling tasks through probabilistic inference. Similarly, Gupta et al. (2020) propose the Lifelong Neural Topic Modeling (LNTM) framework, which relies on neural networks and selective data augmentation to balance stability and plasticity. LTM is designed for multi-domain topic extraction, identifying topic structures across different domains, while LNTM is more suitable for continuous document streams, maintaining topic stability, while adapting to novel information. Moreover, LTM employs a knowledge-based topic modeling approach that refines topic quality through extracted knowledge sets, whereas LNTM integrates topic regularization and selective data augmentation to enhance adaptation, while preventing excessive forgetting."}, {"title": "7 Conclusion and Future Work", "content": "Conclusion. Our survey examined three principal paradigms for text classification under shifting class distributions: learning with background class, zero-shot classification and open-set classification.\nLearning with background class relies on auxiliary data to reject out-of-distribution examples. However, its effectiveness is highly sensitive to the quality and distinctiveness of the available KUC samples. When these auxiliary examples fail to capture real-life diversity, the approach struggles to generalize beyond its training set. Zero-shot classification offers flexibility by leveraging semantic representations to extend recognition to unseen classes, yet it is inherently dependent on the robustness of the available semantic knowledge. In practice, when emerging classes deviate significantly from established semantic cues, zero-shot methods can falter, leading to misclassifications or an inability to capture nuanced differences between similar topics. Open-set classification, on the other hand, excels at detecting novelty through effective outlier identification. However, it typically lacks mechanisms for assigning meaningful labels or integrating these outliers into an existing classification framework. These methodologies have advanced our understanding of handling distribution shifts in text classification, but the field still lacks a unified framework capable of simultaneously discovering, categorizing, and adapting to emerging classes. A promising direction lies in continual learning approaches that can integrate the strengths of all three paradigms, while mitigating their individual weaknesses.\nFuture directions. Continual learning is likely one of the most promising paths towards unlocking open-set learning and discovery in the text domain. However, adapting continual learning techniques to mitigate open-set learning and discovery requires addressing additional complexities, such as novelty detection, adaptive knowledge retention, and dynamic model scalability. One promising direction is disentanglement-based regularization, which separates learned representations into two spaces: a stable and generic one, and a flexible and task-specific one. This method enhances knowledge retention without restricting the adaptability of the model to new tasks, as demonstrated by Huang et al. (2021). Another important avenue is optimizing memory selection for replay, where approaches such as k-means clustering can be used to retain only the most representative examples from previous tasks.\nResearch on continual learning for natural language generation (NLG) has also provided insights into how adaptive architectures can support evolving textual domains. Yang et al. (2022) propose a transformer calibration mechanism to minimize interference between tasks in continual learning scenarios for NLG. This technique leverages attention calibration and feature recalibration to enable language models to adapt incrementally to new tasks, while preserving previously learned knowledge. Similar strategies could be beneficial for open-set continual learning in text classification, where models must adjust their representation space dynamically as new topics emerge.\nSun et al. (2020) introduce LAMOL (Language Modeling for Lifelong Language Learning), which reformulates various NLP tasks into language modeling. Under this framework, the model generates the replay samples from previously learned tasks, reducing catastrophic forgetting without storing large historical datasets. Although originally designed for a closed set of tasks, LAMOL could be extended to open-set scenarios by incorporating a novelty detection module. Whenever the system encounters anomalous samples or unfamiliar topics, it can trigger an incremental update process that treats the new categories or subject area as separate tasks. This setup would allow the model to enlarge its internal knowledge base, maintaining prior knowledge, while adapting dynamically to unseen class distributions.\nDrawing inspiration from CoLeCLIP (Li et al., 2024), which introduces open-domain continual learning in the image domain through a combination of task-specific prompting and joint vocabulary learning, these principles can be adapted for text classification in open-set scenarios. The authors propose the development of shared representations and an adaptive prompting mechanism that enable a model to adjust to new tasks and classes as they emerge, while preserving previously acquired knowledge. Applied to text, this approach translates into designing dynamic prompts and an extensible vocabulary that guide classification under shifting class distributions, simultaneously detecting the emergence of new or unknown topics. For instance, building on the techniques employed by CoLeCLIP, a language model could learn task-specific prompts for different topics, continually adapting and expanding its internal vocabulary as it encounters new terms or concepts. Such an approach would likely need to involve a regularization mechanism to prevent catastrophic forgetting, employing selective memory strategies to retain critical information from past data."}, {"title": "8 Limitations", "content": "To the best of our knowledge, this work covers the primary directions pertinent to the task of open-set text classification. However, some relevant studies may have been inadvertently missed due to limited visibility or other constraints."}]}