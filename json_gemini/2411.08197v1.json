{"title": "What Representational Similarity Measures Imply about Decodable Information", "authors": ["Sarah E. Harvey", "David Lipshutz", "Alex H. Williams"], "abstract": "Neural responses encode information that is useful for a variety of downstream tasks. A common approach to understand these systems is to build regression models or \"decoders\" that reconstruct features of the stimulus from neural responses. Popular neural network similarity measures like centered kernel alignment (CKA), canonical correlation analysis (CCA), and Procrustes shape distance, do not explicitly leverage this perspective and instead highlight geometric invariances to orthogonal or affine transformations when comparing representations. Here, we show that many of these measures can, in fact, be equivalently motivated from a decoding perspective. Specifically, measures like CKA and CCA quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. We also show that the Procrustes shape distance upper bounds the distance between optimal linear readouts and that the converse holds for representations with low participation ratio. Overall, our work demonstrates a tight link between the geometry of neural representations and the ability to linearly decode information. This perspective suggests new ways of measuring similarity between neural systems and also provides novel, unifying interpretations of existing measures.", "sections": [{"title": "Introduction", "content": "The computational neuroscience and machine learning communities have developed a multitude of methods to quantify similarity in population-level activity patterns across neural systems. Indeed, a recent review by Klabunde et al. [19] catalogues over thirty approaches to quantifying representational similarity. Many of these measures quantify similarity in the shape or representational geometry of point clouds. For example, recent papers (e.g. [35, 9]) leverage the Procrustes distance and other concepts from shape theory an established body of work that formalizes the notion of shape and ways to measure distance between shapes [18, 10]. Other measures of representational similarity are not quite this explicit but still emphasize desired geometric invariance properties. For example, work by Kornblith et al. [21] popularized centered kernel alignment (CKA) by emphasizing its invariance to isotropic scaling, translation, and orthogonal transformations. These are precisely the"}, {"title": "Theoretical Framework for Linear Decoding", "content": "We use $X \\in \\mathbb{R}^{M\\times N_x}$ and $Y \\in \\mathbb{R}^{M\\times N_y}$ to denote matrices holding responses to $M$ stimulus conditions measured across neural populations consisting of $N_x$ and $N_y$ neurons, respectively. Throughout this paper we will assume that the neural responses have been preprocessed so that the columns of $X$ and $Y$ have zero mean."}, {"title": "Linear decoding from neural population responses", "content": "We first consider the problem of predicting (or \u201cdecoding\u201d) a target vector $z \\in \\mathbb{R}^M$ from neural population responses by a linear function $X \\rightarrow Xw$ where $w \\in \\mathbb{R}^{N_x}$. One can view this as a simplified neural circuit model where each element of $\\hat z = Xw + \\epsilon$ is the firing rate readout unit the $M$ conditions. Here, we interpret $w$ as a vector of synaptic weights and the random vector $\\epsilon$ represents potential noise corruptions; for example, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ where $\\sigma^2 > 0$ specifies the scale of noise. This neural circuit interpretation of linear decoding is fairly standard within the literature [13, 17].\n\nWithin this setting, we formalize the problem of decoding $z$ from the population activity $X$ through the following class of optimization problems:\n\n$\\displaystyle\\underset{w}{\\text{maximize}} \\frac{1}{M} z^\\intercal Xw - \\frac{1}{2} w^\\intercal G(X)w$  (1)"}, {"title": "Quantifying differences between networks through the lens of decoding", "content": "We now turn our attention to the the problem of quantifying representational similarity between $X \\in \\mathbb{R}^{M\\times N_x}$ and $Y \\in \\mathbb{R}^{M\\times N_y}$. As before, we specify a target vector $z \\in \\mathbb{R}^M$ and optimize $w$ as specified in eq. (1). We optimize $v$ accordingly:\n\n$\\displaystyle\\underset{v}{\\text{maximize}} \\frac{1}{M} z^\\intercal Yv - \\frac{1}{2} v^\\intercal G(Y)v$  (6)\n\nyielding the solution $v^* = G(Y)^{-1}Y^\\intercal z$. We are interested in quantifying similarity between $X$ and $Y$ by comparing the behavior of these optimal decoders. A straightforward way to quantify the alignment between the decoded signals is to compute their inner product, which we call the decoding similarity (fig. 1):\n\n$(Xw^*, Yv^*) = w^{*\\intercal} X^\\intercal Yv^* = z^\\intercal K_X K_Y z$  (7)\n\nwhere we have leveraged eq. (2) and the analogous closed form expression for $v^*$ while introducing the following normalized kernel similarity matrices:\n\n$K_X := \\frac{1}{M} XG(X)^{-1}X^\\intercal$ and $K_Y := \\frac{1}{M} YG(Y)^{-1}Y^\\intercal$.  (8)\n\nNote that we have suppressed the dependence of $K_X$ and $K_Y$ on the function $G(\\cdot)$ to reduce notational clutter.\n\nEquation (7) is a reasonable quantification of similarity between $X$ and $Y$ with respect to a fixed decoding task specified by $z \\in \\mathbb{R}^M$, and it may be a fruitful approach for hypothesis-driven comparisons of neural systems. We comment further on this possibility in our discussion. On the other hand, many neural representations support a variety of downstream behavioral tasks. For example, features extracted in early stages of visual processing (edges and textures) can be used to support many visual tasks such as object classification, segmentation, image compression, et cetera. How can eq. (7) be adapted to quantify the similarity between $X$ and $Y$ across more than one pre-specified decoding task? We explore three simple ideas."}, {"title": "Interpretations of CKA and related measures", "content": "Intuitively, proposition 2 says that the expected inner product between optimal linear readouts is equal to a weighted matrix inner product between kernel matrices $K_X$ and $K_Y$ (with respect to a positive semi-definite weighting matrix $K_z$, which of course depends on the distribution of decoding targets $z$). Such kernel matrices and inner products are important for several methods of quantifying representational similarity. We can therefore leverage this result to provide new interpretations of several distance measures. The most basic idea is to consider a distribution over $z$ which satisfies $K_z = I$. This leads us to measure distances using standard Euclidean geometry, which we state as the following corollary to proposition 2. We explore a relaxation of this assumption and the limit as the number of input samples $M \\rightarrow \\infty$ in appendix A.3.\n\nFor any distribution over decoding targets satisfying $K_z = I$, the Frobenius inner product between normalized kernel matrices $K_X$ and $K_Y$ is equal to the average decoding similarity:\n\n$E(Xw^*, Yv^*) = Tr[K_X K_Y]$  (15)\n\nFurther, when $K_z = I$, the (squared) average decoding distance is equal to the (squared) Euclidean distance between $K_X$ and $K_Y$:\n\n$E||Xw^* - Yv^* ||^2 = ||K_X - K_Y ||_F^2$  (16)\n\nThis corollary can be used to unify several neural representational similarity measures, each cor- responding to a different choice of the penalty function $G(X)$, Table 1. Further, it yields an interpretation of each measure in terms of the expected overlap or difference in decoding readouts between networks."}, {"title": "Discussion", "content": "Understanding meaningful ways to measure similarities between neural representations is clearly a very complex problem. The literature demonstrates a proliferation of different techniques [19, 33], but an underdeveloped understanding of how these methods relate to each other. Less still is known about how representational similarity measures interact with functional similarity or the amount and type of information that is decodable from the representation. This paper presents a theoretical framework centered around the linear decoding of information from representations, which allows us to understand some existing popular methods for measuring representational similarity as average decoding similarity or average decoding distance with different choices of weight regularization. These connections relied on averaging the decoding similarity or decoding distance over a distribution of decoding targets $z$ with $E[zz^\\intercal] = I$. In the future it could be interesting to explore modifying these assumptions. For instance, instead of maximizing, minimizing, or taking the expectation over decoding targets, are there potentially interesting sets of fixed decoding targets that make sense when comparing networks in particular contexts? Furthermore, we focused on linear regression as a decoding method; a potentially interesting line of future work could be to extend this framework to linear classifiers (e.g. support vector machines or multi-class logistic regression). Lastly, in this paper we considered quantifying representational similarity across a finite set of $M$ stimulus conditions. However, these finite-dimensional approaches can be framed as approximations or estimators for a population version of the problem. For example, the framework in [27] for the Procrustes distance, or [3] for the GULP distance (the plugin estimator of which is a special case of our framework as we saw above). We outline a framing of this perspective in appendix A.3, but a rigorous exploration of the $M\\rightarrow\\infty$ regime and an analysis of the behavior of estimators for similarity scores in the limited sample regime could be an important topic of future work."}, {"title": "Appendix", "content": "Here we derive upper and lower bounds on the Procrustes distance between neural representations $X \\in \\mathbb{R}^{M\\times N_x}$ and $Y\\in \\mathbb{R}^{M\\times N_y}$ in terms of the Euclidean distance between linear kernel matrices $K_X = XX^\\intercal$ and $K_Y = YY^\\intercal$. This result is applied in the main text to relate the expected Euclidean distance between decoded signals $E||Xw^* - Yv^* ||^2 = ||XX^\\intercal - YY^\\intercal||_F^2$ to the Procrustes distance $P(X, Y)$. Tildes are omitted in this section for clarity, but we note that this result is applied in the main text to the normalized representations $\\tilde X$ and $\\tilde Y$.\n\nWe will make use of the equivalence between the Procrustes distance between representation matrices $X$ and $Y$ and a notion of distance on the space of positive semi-definite kernel matrices $K_X = XX^\\intercal$ and $K_Y = YY^\\intercal$ called the Bures distance, defined as"}, {"title": "Lower bound", "content": "For the lower bound, we are inspired by the Fuchs-van-de-Graaf inequalites that are used in quantum information theory to assert an approximate equivalence between two measures of quantum state sim- ilarity, the trace distance and the fidelity. This approach is relevant from a mathematical perspective for our purposes, since quantum states are represented by positive semi-definite matrices normalized to have trace 1. Here, we use a similar approach as that often used to derive the Fuchs-van de Graaf inequalites to instead relate the Euclidean distance and the Bures distance on positive semidefinite matrices, while also relaxing the trace 1 normalization.\n\nWe will make use of the following identity.\n\n||auu* \u2013 \u03b2\u03c5\u03bd\u2020||* = \u221a(\u03b1 + \u03b2)2 \u2013 4\u03b1\u03b2|(u, v)|2 (29)\n\nfor all unit vectors u, v and all non-negative real numbers a and B.\n\nProof. First we recognize that the nuclear norm of a matrix can be calculated by summing the singular values of that matrix. Furthermore, if that matrix is Hermitian, the singular values are the absolute values of the eigenvalues. Define $M = auu\u2020 \u2013 \u03b2\u03c5\u03bd\u2020$. Our matrix $M$ is Hermitian and has at most two eigenvalues, so we will look for expressions for these in terms of a and B. Note that if u and v are the same unit vector, then $M$ has rank 1, so we expect the eigenvalues will depend also on the inner product (u, v).\n\nThe eigenvectors of $M$ will be of the form 4 = cu+dv for some scalar c and d. By direct calculation:"}]}