{"title": "When Graph meets Multimodal: Benchmarking on Multimodal Attributed Graphs Learning", "authors": ["Hao Yan", "Chaozhuo Li", "Zhigang Yu", "Jun Yin", "Ruochen Liu", "Peiyan Zhang", "Weihao Han", "Mingzheng Li", "Zhengxin Zeng", "Hao Sun", "Weiwei Deng", "Feng Sun", "Qi Zhang", "Senzhang Wang"], "abstract": "Multimodal attributed graphs (MAGs) are prevalent in various real-world scenarios and generally contain two kinds of knowledge: (a) Attribute knowledge is mainly supported by the attributes of different modalities contained in nodes (entities) themselves, such as texts and images. (b) Topology knowledge, on the other hand, is provided by the complex interactions posed between nodes. The cornerstone of MAG representation learning lies in the seamless integration of multimodal attributes and topology. Recent advancements in Pre-trained Language/Vision models (PLMs/PVMs) and Graph neural networks (GNNs) have facilitated effective learning on MAGs, garnering increased research interest. However, the absence of meaningful benchmark datasets and standardized evaluation procedures for MAG representation learning has impeded progress in this field. In this paper, we propose Multimodal Attribute Graph Benchmark (MAGB), a comprehensive and diverse collection of challenging benchmark datasets for MAGs. The MAGB datasets are notably large in scale and encompass a wide range of domains, spanning from e-commerce networks to social networks. In addition to the brand-new datasets, we conduct extensive benchmark experiments over MAGB with various learning paradigms, ranging from GNN-based and PLM-based methods, to explore the necessity and feasibility of integrating multimodal attributes and graph topology. In a nutshell, we provide an overview of the MAG datasets, standardized evaluation procedures, and present baseline experiments. The entire MAGB project is publicly accessible at https://github.com/sktsherlock/ATG.", "sections": [{"title": "1 Introduction", "content": "Graphs are ubiquitous in modeling the relational and structural aspects of real-world objects across various domains, such as social networks, transportation system networks, and biological protein-protein networks [43, 39]. In many real-world graphs, nodes are associated with multimodal attributes, giving rise to the Multimodal Attributed Graph (MAG). MAGs are prevalent in various scenarios, such as social networks, where each post consists of textual main content and image attachments, and e-commerce networks, where every product is accompanied by text and image information. Investigating representation learning methods on MAGs can benefit real-life applications in many fields, such as social network data mining [20] and e-commerce data mining [27].\nAlthough MAGs are popular in real life, few studies have focused on MAG representation learning. The traditional graph representation learning strategies mainly rely on the graph topological structure [23, 10]. Progressively, a lot of works have started to study attributed graphs, but most of them are confined to unimodal attributed graph (UAG) representation learning. For example, graph neural networks (GNNs) are often designed and experimented on text-attributed graphs (a special case of"}, {"title": "2 Related Work", "content": "Pre-trained Language Models on MAGs. Pre-trained language models (PLMs) can understand and extract complex semantic information in text attributes. The PLMs refer to universal language models that possess enhanced semantic understanding due to their pre-training on a vast corpus [1]. Recently, PLMs have evolved incredibly fast, from the early days of BERT, and GPT to the recent influx of various large language models (LLMs) such as Llama [28] and Mistral [13]. On MAGs, PLMs can be used as textual modal encoders to provide nodes with rich textual modal features. At the same time, these PLMs can be equally directly used for representation learning on MAGs. However, how to incorporate graph topology and multimodal attribute is still a direction worth exploring.\nPre-trained Vision Models on MAGs. Influenced by the pre-training-fine-tuning paradigm in NLP, many pre-trained visual models (PVMs) have also been born in the CV field. Various PVMs usually correspond to different backbone networks such as ViT [5], Swin Transformer [17], and ConvNext [34]. Recently, PVMs that show strong generalization are Vision-Language models represented by CLIP. On the MAGs, different PVMs can be used as visual modal encoders to provide initial visual features to the nodes. This is consistent with the common practice in computer vision of extracting image features from the frozen pre-trained backbone [9]. However, the differences in performance of different PVMs on MAGs need to be explored.\nGraph Neural Networks on MAGs. As graph representation learning enjoys explosive growth in machine learning, numerous research works have been proposed for graph-related tasks such as node classification [14, 11, 15]. Deep learning-based graph neural network models such as GCN (Graph Convolutional Networks) and GraphSAGE employ the topology of the graph and node features to learn high-dimensional representations of nodes based on the message-passing mechanism [29, 7].\nBenchmarks for Graph Representation Learning.\nMany well-established graph benchmarks already exist in the graph community [12, 6, 26, 42]. However, these benchmarks exhibit significant shortcomings when studying representation learning on MAGs. Firstly, most existing datasets lack raw attribute information and are usually restricted to unimodal [36], limiting the study of the integration of multimodal attribute knowledge with structure knowledge. Second, the existing multimodal attributed graph datasets usually do not have a normalized format, which is not conducive to researchers. Third, these benchmarks have not designed detailed learning paradigms to comprehensively compare the importance of multimodal attribute knowledge with graph structure for representation learning on MAGs. Therefore, it is necessary to construct a comprehensive dataset for MAGs."}, {"title": "3 MAGB: A Comprehensive Dataset and Benchmark for MAGs", "content": "3.1 Overview of MAGB\nMAGB allows the community to evaluate MAG representation learning methods on a broad range of datasets with a large number of different baseline methods. To improve usability in analyzing various datasets and conducting personal experiments, we standardize the form of the various datasets and provide a modular pipeline. In addition, we are committed to maintaining a learning platform for MAGs, serving as a repository of the latest advancements in the field. This platform will continuously"}, {"title": "3.2 Dataset Construction", "content": "To deeply explore the role of attribute and topological structure on MAGs and the integration paradigm, we conduct an extensive survey of various graph datasets commonly found on public website resources. We find that there exist many attributed graph data that have been extensively studied. For example, text-attributed graphs where nodes consist of rich textual attributes, such as citation networks Arxiv [12]. Visual-attributed graphs where nodes consist of visual attributes, such as social networks Flickr [37]. However, while these datasets are frequently employed, they present obvious shortcomings when exploring representation learning on MAGs.\nFirstly, they mainly focus on UAGs and lack multimodal attribute information on nodes. Some UAGs naturally have other modality at-tribute information in real scenarios, such as e-commerce networks. Considering multimodal attributes and structures simultaneously may be the only way to design better methods on MAGs. Secondly, most of them only provide graph structure and node features, lacking raw node attribute information, such as raw texts and images. This will prevent researchers from understanding MAGs from the perspective of attribute modeling.\nTo address these limitations, we have taken proactive steps to construct some novel datasets of MAGs. Table 1 shows the difference between the previous dataset and ours. One can see that the dataset in MAG contains more modality attribute information compared to the previous dataset. Here, we provide a detailed description of the composition on the dataset. The three e-commerce networks, Movies, Toys, and Grocery are extracted and constructed from the Amazon dataset [21]. Nodes represent different types of products, while edges represent products that are frequently purchased or browsed together. Node labels are assigned based on product categories. The textual attributes of the node consist of the description and title of the product. While the visual attributes of the nodes are provided by the images of the product, as shown in Figure 2. The social networks Reddit-S and Reddit-M are constructed based on the multimodal dataset RedCaps [3]. Reddit \u00b9 is a social media platform for content sharing and discussion and is the only data source in RedCaps. Nodes in Reddit-S and Reddit-M represent posts, with visual and textual attributes constructed from images with corresponding captions, as shown in Figure 2. Node labels are then assigned based on which subreddit the post comes from. Posts are linked if the same user has commented on both posts. Notice that the image resolution in social networks is usually higher than that in the e-commerce network, while the text length is much smaller than that in the e-commerce network. We preprocess the raw data to construct regular and usable datasets, e.g., cleaning text data and supplementing missing image data. In the Appendix B, we will describe in detail the differences between the various datasets, as well as the detailed processing flow when constructing the dataset."}, {"title": "3.3 Representation Learning on MAGs", "content": "The core of representation learning on MAGs lies in effectively integrating multimodal attribute knowledge with topological structure knowledge. To more comprehensively analyze the role of various modal attributes and topology, we introduce the following methods at a fine-grained level: 1) Methods relying only on graph topology. 2) Methods relying only on textual attributes. 3) Methods"}, {"title": "4 Experiments", "content": "Baselines. (1) For Graph settings, we select DeepWalk [23] and Node2Vec [10]. (2) For Unimodal Text settings, we select 4 different PLMs: a) MiniLM [31], often used for sentence embedding with small parameter scales. b) Three LLMs with huge numbers of parameters, including LlamaV2 7B [28], Mistral 7B [13], and Gemma 7B [8]. We use two training paradigms: 1) end-to-end fine-tuning and 2) feature extraction and then training. (3) For Unimodal Visual settings, we select 4 different PVMs: CLIP-ViT [24], DinoV2-ViT [22], FCMAE-ConvNextV2 [34] and ImageNet-Swinv2 [16] (4) For GNN-based methods, we select 6 popular GNN models: GCN [14], GAT [29], GraphSAGE [11], RevGAT [15], SGC [35] and APPNP [7]. (5) For the PLM-based methods, we conduct experiments on different PLM basic models: MiniLM [31] and TinyLlama 1.1B [40].\nEvaluations metric. We investigate the performance of different baselines on node classification. We use Accuracy and F1-Score Macro to evaluate the model performance. Detailed implementation is provided in Appendix B."}, {"title": "4.1 Main Results", "content": "In this subsection, we compare various learning methods on different MAG datasets. We consider the graph structure as a modality as well and divide the various methods by unimodal (G/T/V), bimodal (GT/GV/TV), and multimodal (GTV). The column \"Avg\" indicates the average score of the method over all datasets. The column \"Rank\" indicates the method's ranking on the \"Avg\".\n1) Table 2 shows that the top three best methods on each dataset mainly belong to the multimodal class (GTV) and the bimodal class (GV and TV). For the Movies, Toys, and Reddit-M datasets, the best performance in terms of both Accuracy and F1 score is achieved by the multimodal class GTV."}, {"title": "4.2 Analysis of GNN-Based methods", "content": "In this subsection, we further explore the effect of various modality encoders on different GNNs. Table 3 demonstrates the node classification results of different GNNs on the Movies dataset under supervised learning and few-shot (10-shot) learning scenarios given different node features. The complete results for other datasets are shown in Appendix B. Table 3 shows that under different experimental settings, on textual modalities, GNNs favor node features generated by LLMs, while on visual modalities, GNNs all perform best on node features generated by CLIP-ViT. Further, we find that T-GNN outperforms V-GNN in scenarios with more training samples, e.g., the supervised result of Llama-APPNP is 55.17%, while that of CLIP-APPNP is 53.94%. Whereas, V-GNN outperforms T-GNN in scenarios with fewer training samples, e.g., the few-shot result of CLIP-APPNP is 29.50%, while that of LLama-APPNP is 25.80%. This suggests that the visual features captured by CLIP may be more robust and generalized than the textual features captured by LLM in the case of fewer training samples. Interestingly, TV-GNN achieves better results than T-GNN or V-GNN for most GNNs in supervised learning scenarios. In contrast, when the training samples are small, TV-GNN fails to take advantage of the combined multimodal attributes. This drives the need for future GNN models on MAGs to consider different experimental settings more fully."}, {"title": "4.3 Analysis of GEA and GA", "content": "In this subsection, we analyze GEA and GA, two methods for injecting graph structure knowledge into language models. The parameter \u03b1 in GEA takes values from {1.0,0.5,0.1}. Figure 4 shows the results of the GA and GEA on MiniLM and TinyLlama, respectively. By comparing the results on the five datasets, we find that the GEA-LM with parameter \u03b1 = 0.5 performs best on most datasets. And when the \u03b1 value is not optimal, the GEA-LM may even be inferior to the original LM. For example, the results of GEA-MiniLM on Movies, Toys, and Grocery are all weaker than the original"}, {"title": "4.4 Analysis of VEA and VA", "content": "The parameter \u03b2 in VEA takes values from {1.0,0.5,0.1}. Figure 5 shows the results of the VA and VEA on MiniLM and TinyLlama, respectively. VEA-MiniLM performs better on most datasets with \u03b2 = 1.0. \u03b2 controls the extent to which visual features contribute to the representation of the center node. MiniLM with smaller parameter scale may need to capture more features from the visual modality due to limited text representation capabilities. In contrast, VEA-TinyLlama usually performs worse at \u03b2 = 1.0, which suggests different weighting preferences between small and large language models when aggregating visual features. Meanwhile, we note that on Reddit-S dataset, there is a 21.96% difference between the effectiveness of VA-MiniLM and the best VEA-MiniLM. Similarly, on the Movies dataset, VA-TinyLlama is 4.38% worse than the best VEA-TinyLlama, and even 1.11% lower than the original TinyLlama. This suggests that when visual features are introduced in the form of a \"soft prompt\" to guide the training of LMs, it may increase the difficulty of training due to the large difference in the distribution between visual space and textual space."}, {"title": "5 Conclusion", "content": "We establish the first comprehensive benchmark MAGB specifically designed to explore representa-tion learning on MAGs. We provide five normative available multimodal attributed graph datasets to facilitate the Graph, NLP, and CV communities to focus and investigate the data together. Our benchmark provides a more comprehensive evaluation of different learning methods, validating their effectiveness and limitations. We will also continue to mine and build more research-worthy MAGs to promote the continued healthy development of the field."}, {"title": "A Baseline and Model Implementation Detail", "content": "A.1 Baseline\nFirst, we provide detailed descriptions of the traditional graph embedding models and GNNs used in the main experiments.\n1. DeepWalk [23]. The core idea behind DeepWalk is to generate node sequences through random walks and then use the Skip-Gram model [19] to learn node representations by maximizing the similarity between a node and its context nodes in the embedding space.\n2. Node2Vec [10]. The Node2Vec framework learns low-dimensional representations for nodes in a graph by optimizing a neighborhood-preserving objective. The algorithm accommodates various definitions of network neighborhoods by simulating biased random walks. Specifically, it provides a way of balancing the exploration-exploitation tradeoff that in turn leads to representations obeying a spectrum of equivalences from homophily to structural equivalence.\n3. GCN [14]. Graph Convolutional Network (GCN) is a classical model that works by performing a linear approximation to spectral graph convolutions. It uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs.\n4. GraphSAGE [11]. GraphSAGE is a GNN model that focuses on inductive node classification but can also be applied to transductive settings. It leverages node features to generate node embeddings for previously unseen data efficiently. Instead of training individual embeddings for each node, it learns a function that generates embeddings by sampling and aggregating features from a node's local neighborhood.\n5. GAT [29]. Graph Attention Network (GAT) introduces the attention mechanism to capture the importance of neighboring nodes when aggregating information from the graph. By stacking masked self-attentional layers in which nodes can attend over their neighborhoods' features, a GAT specifies different weights to different nodes in a neighborhood without requiring any costly matrix operation or depending on knowing the graph structure upfront.\n6. RevGAT [15]. RevGAT generalizes reversible residual connections to grouped reversible residual connections. In other words, it extends the idea of reversible connections, which allow information to flow both forward and backward to vanilla GAT architecture. By doing so, RevGAT is able to improve the expressiveness and efficiency of GNNs.\n7. APPNP [7]. Approximate Personalized Propagation of Neural Predictions (APPNP) is specifically developed for semi-supervised learning tasks on graph-structured data. It utilizes personalized propagation to iteratively enhance node predictions by incorporating comprehensive information from both local and global contexts.\n8. SGC [35]. SGC reduces the excess complexity in a GCN by successively removing nonlinearities and collapsing weight matrices between consecutive layers. The author theoretically analyzes the SGC and proves that it corresponds to a fixed low-pass filter followed by a linear classifier.\nFor textual attribute modality, we use the pre-trained language models (PLMs) to get the textual features of the nodes.\n1. MiniLM [31]. We select \"all-MiniLM-L12-v2\" in huggingface [32], which is lightweight and per-forms well on sentence-level tasks. \"all-MiniLM-L12-v2\" use the pretrained \"microsoft/MiniLM-L12-H384-uncased\" model and fine-tuned in on a 1B sentence pairs dataset. In this paper, we use \"MiniLM\" to represent the \"all-MiniLM-L12-v2\".\n2. Llama 2 [28]. Llama 2 is a family of pre-trained and fine-tuned large language models (LLMs) released by Meta AI in 2023. Released free of charge for research and commercial use, Llama 2 AI models are capable of a variety of natural language processing (NLP) tasks, from text generation to programming code. Llama 2 models are trained on 2 trillion tokens and have double the context length of Llama 1. In this paper, we use the 7 billion parameter size \"meta-llama/Llama-2-7b-hf\" on huggingface.\n3. Mistral [13]. The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters developed by Mistral AI and released in 2023. The model introduces Grouped Query Attention (GQA) and Sliding Window Attention (SWA) on top of the Transformer architecture to achieve faster reasoning and low latency. It significantly outperforms the Llama 2 13B in several common NLP benchmarks, including common sense reasoning, world knowledge, and reading comprehension.\n4. Gemma [8]. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text,"}, {"title": "B Dataset", "content": "B.1 Dataset Documentation\nWe standardize the data storage to facilitate the researchers' use of the datasets, as shown in Figure 2. For each dataset, we have the following kinds of files or folders:\n\u2022 \".csv\" format file, which holds the raw textual attribute information of the nodes.\n\u2022 \".pt\" format file, which holds the graph data loaded in DGL [30], including adjacency matrix and node labels.\n\u2022 \".gz\" formt file, which holds the image files named after the node ids (the image files are normally JPG or PNG).\n\u2022 \"Visutal Feature\" folder holds various image features in npy format generated by PVMs.\n\u2022 \"Textual Feature\" folder holds various textual features in npy format generated by PLMs.\nB.2 Dataset Construction\nThe construction of the multimodal attributed graph datasets includes the following three steps.\nFirst, preprocess the text and image information in the original data. For text, this includes removing missing values, non-English statements, abnormal symbols, length truncation, etc. For images, entities that do not contain image links are removed.\nSecond, building the graph. The datasets constructed in MAGB are mainly from e-commerce data and social media data. For e-commerce data, Amazon already provides linking relationships between nodes, such as \"also-view\" and \"also-buy\" information between products (indicating the two product ids that are jointly purchased or viewed). For social media data, we link posts by author-related information on the post, i.e., posts are linked if the same user has commented on both posts. Note that self-edges and isolated nodes need to be removed when obtaining the final graph data.\nThird, refining the constructed graph. The graph nodes need corresponding numerical node labels for the node classification task. We convert the categories of nodes in the original data to numerical node labels in the graph. Detailed dataset class information will be shown later."}, {"title": "D Limiation, Social Impact, and Future Work", "content": "While MAGB provides various MAG datasets and experimental analyses on various methods, it is still far from perfect. Here, we discuss the limitations, social impacts, and future work.\nD.1 Limitation\nOne limitation of this research is that the MAGB focuses primarily on node classification. MAGB does not pay much attention to link prediction task that has many applications in the real world, such as recommendation systems. Further, we are also not exploring whether topological structure would benefit tasks and data on traditionally multimodal domains.\nBesides, we are still under-exploring LLMs on MAG. In our proposed PLM-based methods, LLM is still used as an encoder. It is still worthwhile to further explore how LLM can be used generatively for MAG representation learning. For example, in context learning, prompt tuning on MAGs, etc.\nD.2 Social Impact\nRepresentation learning on multimodal attributed graphs is a fast-growing and promising research field, and covers a wide range of applications. We start this benchmark to call more researchers' attention to this common data type. The proposed benchmark MAGB can significantly facilitate the development of multimodal attributed graph learning. MAGB has a positive social impact as it contributes to developing more robust and fair machine learning models.\nD.3 Future Work\nIn the future, we will keep track on the newly emerged problems in MAGs and provide more solid experimental results and detailed analyses to improve MAGB consistently. In future work, extending the investigation to other tasks on MAGs beyond node classification would be valuable. It is an ongoing effort, and we strive to include more datasets continuously and evaluate different methods to advance the field."}]}