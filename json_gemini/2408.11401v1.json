{"title": "Revisiting FunnyBirds evaluation framework for prototypical parts networks", "authors": ["Szymon Op\u0142atek", "Dawid Rymarczyk", "Bartosz Zieli\u0144ski"], "abstract": "Prototypical parts networks, such as ProtoPNet, became popular due to their potential to produce more genuine explanations than post-hoc methods. However, for a long time, this potential has been strictly theoretical, and no systematic studies have existed to support it. That changed recently with the introduction of the FunnyBirds benchmark, which includes metrics for evaluating different aspects of explanations. However, this benchmark employs attribution maps visualization for all explanation techniques except for the ProtoPNet, for which the bounding boxes are used. This choice significantly influences the metric scores and questions the conclusions stated in FunnyBirds publication. In this study, we comprehensively compare metric scores obtained for two types of ProtoPNet visualizations: bounding boxes and similarity maps. Our analysis indicates that employing similarity maps aligns better with the essence of ProtoPNet, as evidenced by different metric scores obtained from FunnyBirds. Therefore, we advocate using similarity maps as a visualization technique for prototypical parts networks in explainability evaluation benchmarks.", "sections": [{"title": "Errata", "content": "After our paper was accepted at the X\u0391\u0399 2014 conference, we discovered an inaccuracy that needs clarification and correction. While this inaccuracy does not affect the interpretation or conclusions of our results, it is important for the community to be informed about it.\nWe submitted our paper to \u03a7\u0391\u0399 2014 before the authors of FunnyBirds framework [9] published their full source code. That is why we had to reimplement"}, {"title": "1 Introduction", "content": "Standard deep neural networks (DNNs) lack transparency in their decision-making process, posing challenges for human verification, especially in critical domains such as medicine [14,20]. In response, the field of eXplainable Artificial Intelligence (XAI) has emerged with post-hoc and ante-hoc methods. Post-hoc methods are commonly used because they can be applied to already-trained neural networks. However, various studies have highlighted their potential biases [1,3,26], raising concerns about the reliability of their explanations. Consequently, ante-hoc methods like ProtoPNet [6] and B-Cos [5] have gained prominence.\nThese intrinsically interpretable or self-explainable methods operate under the assumption that the model's design inherently allows for interpretable predictions. However, they often require more complex training to achieve comparable accuracy to standard DNNs, leading to the interpretability-accuracy trade-off phenomenon [21].\nPractitioners encounter a dilemma regarding whether to choose a standard DNN coupled with a post-hoc explanation method to achieve higher accuracy or to invest in the development of a self-explanatory model to enhance interpretability. Addressing this question necessitates a reliable and trustworthy evaluation framework for model explanations. This challenge is tackled by the FunnyBirds framework [9] that introduces a synthetic dataset and a set of metrics for comparing explanation quality across different models, including post-hoc and interpretable ones.\nHowever, a limitation of the FunnyBirds evaluation lies in how metrics are computed for the ProtoPNet model compared to other explanation methods such as GradCAM [25] and LRP [4]. The assumption made by the authors is that ProtoPNet explanations are presented as bounding boxes highlighting important image regions. However, these bounding boxes only approximate the significance of regions derived from more precise similarity maps, as illustrated in Figure 1, which can be seen as equivalent to saliency maps for post-hoc methods.\nIn this study, we evaluate ProtoPNet explanations based on similarity maps rather than bounding boxes within the FunnyBirds framework and comprehensively analyze the resulting changes in explanation quality. Our findings demonstrate that similarity map-based explanations better align the metrics with ProtoPNet's design intuition, yielding more accurate evaluation results. Therefore, we advocate for adopting similarity map-based activations for ProtoPNet evaluations to ensure a reliable comparison of explanations within the community."}, {"title": "2 Related works", "content": "Evaluation of xAI. With the advancements in xAI methodologies, the need to quantify the quality of provided explanations has emerged. Benchmarking XAI approaches can be categorized into two main groups: those based on user studies and those involving the development of dedicated quantitative metrics.\nEvaluation through user studies has been explored in previous research, e.g. in [11], the correctness of explanations was assessed, while [10] delved into determining the most suitable form of explanation for different data types. Additionally, in [12], the level of user overconfidence induced by explanations was measured. More specific evaluations include assessing semantic similarity for prototypical parts in [23], examining explanation saliency in [22], and evaluating the adequateness of prototypical parts for the medical domain in [16].\nOn the other hand, in proposing metrics and taxonomies for evaluating explanations, the Co-12 framework was introduced in [15,19,18]. This framework"}, {"title": "3 Methods", "content": null}, {"title": "3.1 FunnyBirds", "content": "Dataset. FunnyBirds dataset consists of synthetically generated bird images rendered from five human comprehensible concepts of beak, wings, feet, eyes, and tail, called parts. The dataset contains 50 bird classes, each corresponding to a unique subset of 26 predefined parts. In total, it comprises 50,000 training images and 5,000 testing images in 256 \u00d7 256 resolution. Furthermore, the training set incorporates augmented images with missing bird parts, simulating a data mix-up strategy.\nInterface functions. The second major aspect of the framework are interface functions, $PI(\u00b7)$ and $P(\u00b7)$. These functions are designed to translate various explanation types (such as saliency maps or prototypical parts) into a unified format that can be used to calculate explainability metrics. Based on an explanation, the $PI(\u00b7)$ function calculates a set of importance scores assigned to each part, while the $P(\u00b7)$ function provides a set of important parts parameterized by the threshold t used to control the \"sensitivity\" of importance.\nDefault interface functions for prototypical parts. FunnyBirds authors introduce the default definition of interface functions for specific XAI methods. For prototypical part-based methods, they calculate $PI(\u00b7)$ by summing the values of an attribution map within particular bird parts. The attribution map is obtained as follows for a training sample (x, y): the image $x \u2208 X$ is passed to ProtoPNet; for each prototypical part corresponding to class y, we obtain a similarity map and corresponding bounding box; such a bounding box is then filled with the maximum value multiplied by the weight between the prototypical part and class y; the attribution map is obtained as a sum of such bounding boxes obtained for all prototypical parts. When it comes to $P(\u00b7)$, it is defined as a set of bird parts with at least t-percent of area overlapping the union of those bounding boxes."}, {"title": "3.2 Summed Similarity Maps (SSM) for more precise interface functions", "content": "We propose an alternative definition of the interface functions based on the similarity maps, which are more precise than bounding boxes, as presented in Figure 1. Similarly, like in the default definition, $PI(\u00b7)$ is calculated by summing the values of an attribution map within particular bird parts. However, our definition of attribution map differs as follows: the image $x \u2208 X$ is passed to ProtoPNet; for each prototypical part corresponding to class y, we obtain a similarity map; such similarity map is then multiplied by the weight between the prototypical part and class y; the attribution map is obtained as a sum of"}, {"title": "4 Experimental setup", "content": "We use the ProtoPNet model [6] with ResNet50, VGG19, and DenseNet169 backbones. We follow the training setup from FunnyBirds framework [9]. It corresponds to the multilabel classification because input images present incomplete birds fitting more than one class. We use Adam optimizer [13] with a learning rate decreasing every 10th epoch, and we apply prototype projection at the 25th epoch. Moreover, it is trained three times with different prototype sizes (128, 256, or 512) but with the same number of prototypical parts equal to 10. We do not use any augmentations.\nThe code is publicly available. The training was conducted on four Nvidia A100 GPUs and took about 9 hours per model."}, {"title": "5 Results", "content": null}, {"title": "5.1 Metrics scores for attribution maps based on bounding boxes or similarity maps", "content": "The FunnyBirds metrics design follows the Co-12 taxonomy [18], exploring categories such as Completeness, Correctness, and Contrastivity. The latter two are measured by Single Deletion (SD) and Target Sensitivity (TS) metrics, respectively. At the same time, the Completeness score is calculated as an average of Controlled Synthetic Data Check (CSDC), Preservation Check (PD), Deletion Check (DC), and Distractibility (D).\nAs presented in Table 3, we observe a notable enhancement in explanation correctness as the Single Deletion (SD) score increases from 0.24 to 0.73. As defined in 1, SD is computed as correlation between orders of $PI(e)$ (GT) and $f(x) - {f(x\\p)}_p$ (BB or SSM). Therefore, a more precise SSM attribution map demonstrates that ProtoPNet is much more correct than reported in [9]. We explain this observation using examples in Figure 3. Moreover, a small increase is observed in its contrastivity, from 0.46 to 0.5.\nConversely, we observe a significant drop in three out of four completeness metrics. More precisely, the Controlled Synthetic Data Check (CSDC) drops from 0.93 to 0.58, the Preservation Check (PC) from 0.91 to 0.40, and the Deletion Check (DC) from 0.92 to 0.66. As we present in Figure 4, this drop is caused by the fact that the original BB approach tends to overidentify parts"}, {"title": "5.2 Various backbones of ProtoPNet", "content": "Table 4 presents metrics scores depending on different backbone architectures (ResNet50, VGG19, and DenseNet169) used in ProtoPNet, while Figure 5 presents SSM obtained for them. Notably, ResNet50 exhibits substantially higher DC, D, and SD metrics than others, while its TS metric is the lowest. Conversely, for VGG19, CSDC and TS metrics demonstrate superiority. This discrepancy can be attributed to differences in receptive field sizes, notably smaller in the case of VGG19, and the incorporation of bottlenecks in ResNet50. However, it is important to note that while ResNet50 achieves the best metrics within the FunnyBirds framework, it is outperformed by DenseNet in terms of accuracy."}, {"title": "6 Conclusions", "content": "In this study, we evaluated ProtoPNet explanations based on similarity maps rather than bounding boxes within the FunnyBirds framework and comprehensively analyzed the resulting changes in explanation quality. Overall, the results indicate that the choice between bounding boxes and similarity maps significantly impacts the assessment of explanation quality, particularly for methods like ProtoPNet. While bounding boxes have been traditionally used for their"}]}