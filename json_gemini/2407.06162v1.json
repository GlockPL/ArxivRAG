{"title": "RNNS, CNNs and Transformers in Human Action Recognition:\nA Survey and A Hybrid Model", "authors": ["Khaled Alomar", "Halil Ibrahim Aysel", "Xiaohao Cai"], "abstract": "Human Action Recognition (HAR) encompasses the task of monitoring human activities across various\ndomains, including but not limited to medical, educational, entertainment, visual surveillance, video\nretrieval, and the identification of anomalous activities. Over the past decade, the field of HAR has\nwitnessed substantial progress by leveraging Convolutional Neural Networks (CNNs) to effectively extract\nand comprehend intricate information, thereby enhancing the overall performance of HAR systems.\nRecently, the domain of computer vision has witnessed the emergence of Vision Transformers (ViTs) as a\npotent solution. The efficacy of transformer architecture has been validated beyond the confines of image\nanalysis, extending their applicability to diverse video-related tasks. Notably, within this landscape, the\nresearch community has shown keen interest in HAR, acknowledging its manifold utility and widespread\nadoption across various domains. This article aims to present an encompassing survey that focuses on\nCNNs and the evolution of Recurrent Neural Networks (RNNs) to ViTs given their importance in the\ndomain of HAR. By conducting a thorough examination of existing literature and exploring emerging\ntrends, this study undertakes a critical analysis and synthesis of the accumulated knowledge in this field.\nAdditionally, it investigates the ongoing efforts to develop hybrid approaches. Following this direction,\nthis article presents a novel hybrid model that seeks to integrate the inherent strengths of CNNs and\nViTs.", "sections": [{"title": "1 Introduction", "content": "Human Action Recognition (HAR) focuses on the classification of the specific actions exhibited within a\ngiven video. On the other hand, action detection and segmentation focus on the precise localization or\nextraction of individual instances of actions from video content [1]. The capacity of deep learning models\nto effectively capture the spatial and temporal complexities inherent in video representations plays a vital\nrole in the recognition and understanding of actions.\nOver the preceding decade, a considerable amount of research has been dedicated to the thorough\ninvestigation of action recognition, resulting in an extensive collection of review articles and survey papers\naddressing the topic [2-4]. However, it is worth noting that a predominant focus of these scholarly works has\nbeen placed on the examination and evaluation of Convolutional Neural Networks (CNNs) and traditional\nmachine learning models within the realm of action recognition.\nThe advent of Transformer [5] architecture has sparked a paradigm shift in deep learning, leading\nresearchers to swiftly adopt and adapt these architectures to enhance the accuracy and efficiency of various\ncomputer vision tasks including action recognition. Consequently, there is a compelling need for a recent\nreview that comprehensively examines the state-of-the-art research including Vision Transformers (ViTs)\n[6], a special Transformer for computer vision tasks, and Hybrid models in addition to CNNs. Such a review\nwould serve as a crucial guiding resource to shape the future research directions with Transformer and"}, {"title": "2 Background", "content": "2.1 From Vanilla RNN to Attention-based Transformers\nThe fundamental aim of the Transformers lies in effectively addressing time series data tasks, placing a\ndistinct emphasis on NLP as one of its primary objectives. To address these tasks, RNNs have emerged\nas a prominent deep learning technique. However, the development of the Transformers did not occur in\nisolation; rather, it was the result of a series of iterative optimisations that were progressively integrated\ninto the existing RNN structures. This section aims to elucidate the underlying RNN techniques and the\nsubsequent advancements that were incorporated into them, ultimately leading to the final formulation of\nthe Transformer architecture.\nNotation. Here, we establish common notations for RNN architectures including Vanilla RNNs, Long\nShort-Term Memory (LSTM) and Gated Recurrent Unit (GRU) to streamline discussions in subsequent\nsections. In these architectures, each iteration involves a cell that sequentially processes an input embedding\n$x_t \\in \\mathbb{R}^{n_x}$ and retains information from the previous sequence through the hidden state $h_{t-1} \\in \\mathbb{R}^{n_h}$ using\nweight matrices $W \\in \\mathbb{R}^{n_h \\times n_h}$ and $U \\in \\mathbb{R}^{n_h \\times n_x}$. The $W$-like matrices encompasses all weights related to"}, {"title": "2.1.1 Attention", "content": "The evolution of attention mechanisms in neural networks represents a significant advancement in the field of\ndeep learning, particularly in tasks related to NLP and machine translation. Initially introduced by Graves\net al. [22], the concept of attention mechanisms was designed to enhance the model's ability to focus on\nspecific parts of the input sequence when generating an output, mimicking the human ability to concentrate\non particular aspects of a task. This foundational work laid the groundwork for subsequent developments\nin attention mechanisms, providing a mechanism for models to dynamically assign importance to different\nparts of the input data.\nBuilding on Graves' initial concept, Bahdanau et al. [23] introduced the additive attention mechanism,\nwhich was specifically designed to improve machine translation. This approach computes the attention\nweights through a feed-forward neural network, allowing the model to consider the entire input sequence and\ndetermine the relevance of each part when translating a segment. This additive form of attention significantly\nimproved the performance of sequence-to-sequence models by enabling a more nuanced understanding and\nalignment between the input and output sequences [21]. Following this, Luong et al. [24] proposed the\nmultiplicative attention mechanism, also known as dot-product attention, which simplifies the computation\nof attention weights by calculating the dot product between the query and all keys. This method not\nonly streamlined the attention mechanism but also offered improvements in computational efficiency and\nperformance in various NLP tasks, marking a pivotal moment in the evolution of attention mechanisms\nfrom their inception to more sophisticated and efficient variants.\nThe central idea of the Attention mechanism is to shift focus from the task of learning a single vector rep-\nresentation for each sentence. Instead, it adopts a strategy of selectively attending to particular input vectors"}, {"title": "2.1.2 Self-Attention", "content": "To this point, attention mechanisms in sequence-transformation models have primarily relied on complex\nRNNs, featuring an encoder and a decoder, the most successful models in language translation yet. However,\nin 2017, Vaswani et al. [5] introduced a simple network architecture known as the Transformer, see Figure\n4, which exclusively utilized attention mechanism, eliminating the need for RNNs. They introduced a novel\nattention mechanism called self-attention, which is also known as KQV-attention (Key, Query, and Value).\nThis attention mechanism subsequently gained prominence as a central component within the Transformer\narchitecture. The attention mechanism stands out due to its ability to provide Transformers with an exten-\nsive long-term memory. In the Transformer model, it becomes possible to focus on all previously generated\ntokens.\nThe embedding layer in a Transformer model is the initial stage where input tokens are transformed\ninto dense vectors, capturing semantic information about each token's meaning and context within the text.\nThese embeddings serve as the foundation for subsequent layers to process and understand the relationships\nbetween words in the input sequence [27].\nSelf-attention is a mechanism that allows an input sequence to process itself in a way that each position\nin the sequence can attend to all positions within the same sequence. This mechanism is a cornerstone of the\nTransformer architecture, which has revolutionized NLP and beyond by enabling models to efficiently handle\nsequences of data with complex dependencies. The purpose of self-attention is to compute a representation of\neach element in a sequence by considering the entire sequence, thereby capturing the contextual relationships\nbetween elements regardless of their positional distance from each other. This ability to capture both local\nand global dependencies makes self-attention particularly powerful for tasks such as machine translation,\ntext summarization, and sequence prediction, where understanding the context and the relationship between\nwords or elements in a sequence is crucial [5]\nThe mathematical formulation of self-attention involves several key steps. First, a set of query vectors\n$Q = XW^Q$, a set of key vectors $K = XW^K$, and a set of value vectors $V = XW^V$ are calculated\nthrough linear transformations of the input sequence, where $X$ is the input matrix representing embeddings\nof tokens in a sequence, $W^Q, W^K$, and $W^V$ are weight matrices for queries, keys, and values, respectively.\nThe attention scores are then calculated by taking the dot product of the query vector with all key vectors,\nfollowed by scaling the result by the inverse square root of the dimension of the keys ($d_k$) to avoid overly\nlarge values. These scores are then passed through a softmax function to obtain the attention weights, which\nrepresent the importance of each element's contribution to the output. Finally, the output for each position\nis computed as a weighted sum of the value vectors,\n$A(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$"}, {"title": "2.1.3 Multi-Head-Attention", "content": "Multi-head attention is an extension of the self-attention mechanism designed to allow the model to jointly\nattend the information from different representation subspaces at different positions [5]. Instead of perform-\ning a single attention function, it runs the attention mechanism multiple times in parallel. The outputs of\nthese independent attention computations are then concatenated and linearly transformed into the expected\ndimension. The mathematical formulation of multi-head attention can be described in four steps. First,\nMulti-Head Attention Computation,\n$Q_i = XW_i^Q, K_i = XW_i^K, V_i = XW_i^V$\nwhere i is the index of each self-attention head,\n$A_i(Q_i, K_i, V_i) = softmax(\\frac{Q_i K_i^T}{\\sqrt{d_k}})V_i$\nThen, concatenating outputs from all heads followed by a final linear transformation,\n$A(Q, K, V) = Concat(A_1, A_2, ..., A_h)W^O$\nwhere $W^O$ is the weight matrix for the final linear transformation.\nThe multi-head attention mechanism enables the model to capture different types of information from\ndifferent positions of the input sequence. By processing the sequence through multiple attention \"heads\", the\nmodel can focus on different aspects of the sequence, such as syntactic and semantic features, simultaneously.\nThis capability enhances the model's ability to understand and represent complex data, making multi-head\nattention a powerful component of Transformer-based architectures [28]."}, {"title": "2.2 From Transformer to Vision Transformer", "content": "The journey from the inception of the Transformer model to the development of the Vision Transformer\nmarks a pivotal advancement in deep learning, showcasing the adaptability of models initially designed"}, {"title": "2.2.1 Vision Transformer: From Spatial to Spatio-Temporal Tasks", "content": "The original ViT, designed for static image processing, divides images into patches and interprets these as\nsequences, leveraging the Transformer's self-attention mechanism to understand complex spatial relation-\nships. Extending this model to action recognition involves adapting it to analyze video frames sequentially to\ncapture both spatial and temporal relationships. Several works attempted to adapt ViT in action recognition\ntask using different methods as below.\nTemporal Dimension Integration The integration of the temporal dimension is a fundamental step\nin adapting ViT for action recognition. Traditional ViT models process images as a series of patches,"}, {"title": "2.3 CNNs: From Spatial to Spatio-Temporal Tasks", "content": "CNNs have undergone a significant evolution since their introduction in the 1980s. The Neocognitron [31],\ndeveloped by Kunihiko Fukushima, presented an early example of neural networks incorporating convo-\nlutional operations for image processing, setting the foundations for subsequent progress. Shortly after,\nYann LeCun and collaborators introduced LeNet-5 [32], a key architecture designed for handwritten digit\nrecognition, showcasing the effectiveness of convolutional layers in pattern recognition tasks.\nThe progress of CNNs reached a turning point in the mid-2010s with the introduction of models like\nAlexNet [33], showcasing their potential in image classification tasks. Alongside architectural innovations,\nthis milestone was achieved thanks to access to large datasets, notably, ImageNet [34], and computational\nimprovements, including the rise of Graphics Processing Units (GPUs) for parallel computing. Large-\nscale datasets provided the diversity and complexity necessary for training deep networks, while enhanced\ncomputational power accelerated the training of sophisticated CNN architectures.\nThe architectural enhancements, large datasets, and increased computational capabilities helped CNNS\nto be a cornerstone in deep learning methodologies, extending their applications beyond image processing\nto various domains. Notable architectures like VGGNet [35], distinguished by its uniform design and small\nconvolutional filters, GoogLeNet [36], with its inception modules for capturing features at different scales\nefficiently, and ResNet [37], which introduced residual learning for training very deep networks, have further\nenriched the landscape of CNNs.\nTransfer Learning. Deep Neural Networks, specifically CNNs, have been proven to work very well in\nthe presence of large, diverse datasets. However, access to large representative datasets is a big challenge\nin the deep learning era. To benefit from the impressive pattern recognition capability of CNNs in the\nabsence of diverse and large datasets, transfer learning comes into play to leverage knowledge gained from\na source task to enhance the performance of a model on a related target task. Transfer learning involves\npre-training a model on a large dataset through which CNNs' early layers capture relatively general features\nand patterns that may be common and helpful for other related target tasks. By freezing the early layers to\npreserve the learned knowledge, and fine-tuning the later layers on a target task to allow the model to adapt\nto target task-specific features, the power of deep networks is exploited even when there is limited data.\nImageNet is a widely recognised source dataset within the domain of computer vision. Several prominent\narchitectural designs, including VGG, ResNet and GoogleNet have made their pre-trained weights on the\nImagenet dataset publicly available to provide a source for subsequent researchers and practitioners. The\navailability of pre-trained weights accelerates the training process on diverse computer vision tasks, enabling\nthe transfer of learned features and representations from the ImageNet dataset to target applications.\nObject detection, another fundamental task in computer vision, has seen significant advancements in\nthe last 10 years. Early methods such as Viola-Jones [38] employed handcrafted features and classifiers to\ndetect and classify objects in a given image. The paradigm shift in the 2010s was marked by the introduction\nof R-CNN [39]: Region with CNN features and subsequent architectures like Fast R-CNN [39] and Faster R-\nCNN [40]. This pivotal development brought forth a two-stage approach, involving the proposal of regions\nof interest and their classification. Following that Single Shot MultiBox Detector (SSD) [41] and You Only\nLook Once (YOLO) [42] models that allow real-time object detection were proposed to show the efficacy\nof single-shot detection frameworks. One of the recent aims in the field is to achieve anchor-free methods\n[43, 44] to work with more flexible and efficient methods."}, {"title": "3 Deep Learning-Based Techniques in HAR", "content": "This section briefly recalls the most commonly used deep learning-based HAR approaches.\n3.1 CNN-based approaches in HAR\nThis section recalls the most prominent CNN-based approaches in HAR, organized chronologically.\nDeep learning was still in its early stages in 2012, and CNNs, or CNNs, had not yet gained significant\npopularity in the field of HAR. The focus was primarily on traditional machine learning approaches, such as\nSupport Vector Machines (SVMs) [49], and handcrafted features, such as Histogram of Oriented Gradients\n(HOG) [50] and Histogram of Optical Flow (HOF) [51]. A few studies did, nevertheless, start looking into\nneural networks for action recognition.\nIn 2012, Ji et al. [52] presented an exploration into the use of 3D CNNs for recognizing human actions in\nsurveillance videos. This model innovatively extends traditional 2D CNNs by performing 3D convolutions,\nthereby extracting features from both spatial and temporal dimensions to effectively capture motion infor-\nmation. Evaluated on the TREC Video Retrieval Evaluation (TRECVID) dataset and the KTH dataset,\nthe 3D CNN model demonstrated significant improvements over traditional 2D CNNs and other baseline\nmethods in action recognition tasks. The paper's novelty lies in its unique approach to integrating spatial-\ntemporal features for action recognition, offering enhanced performance in real-world surveillance scenarios.\nFor papers specifically from 2012 that only used CNN architectures for HAR, there may not be a signif-\nicant number of high-impact papers due to the fact that the use of CNNs in this field was still emerging\nand not as established as in later years. The seminal works that laid the groundwork for using CNNs in\naction recognition often came slightly later, as the computational power and dataset availability became\nmore conducive to deep learning approaches.\nIn 2013, Valle et al. [53] address the challenge of HAR from video data, focusing specifically on the\nrecognition of walking and running actions. They propose an innovative approach using a variant of a 2D\nCNN, a type of multilayer perceptron, which is distinct for its ability to learn feature extraction directly\nfrom raw, unprocessed images during the error backpropagation phase. This methodology represents a\nsignificant advancement over traditional methods that rely on pre-processed images. The authors developed\na specialized dataset comprising 4000 images per activity to train and evaluate their model. The paper's\ncontribution lies in its novel approach to feature extraction in HAR, demonstrating the potential of CNNs\nin efficiently and accurately recognizing complex human actions directly from video sequences, a task of\ngrowing importance in fields such as surveillance and automated monitoring systems.\nIn 2014, the use of CNNs in action recognition was at a pivotal stage, marking a shift from hand-crafted\nfeature-based methods to deep learning approaches. The key points of the use of CNNs in action recognition\nat that period of time is the following. Emergence of Deep Learning, deep learning, particularly CNNs,\nhad started to dominate image classification tasks, thanks to their ability to learn feature representations\ndirectly from raw pixel data. This success in static images paved the way for applying CNNs to video\ndata for action recognition. Challenges in Video data, unlike 2D images, videos incorporate a third\ndimension which represents the temporal patterns, making action recognition more complex. CNNs had to"}, {"title": "3.2 Transformer-based Approaches in HAR", "content": "The ViT was conceptualized and introduced in the academic domain through the paper authored by Doso-\nvitskiy et al. [6] in 2020. The ViT marked a paradigm shift in still image recognition methodologies, applying\nthe Transformer model, predominantly known for its success in NLP, to the realm of computer vision. The\napplication of ViTs in action recognition, a more specific and complex task within the field of computer\nvision, followed the initial introduction of ViT. Specifically, in 2021 and beyond, subsequent research and\npublications have explored and expanded the use of ViTs for action recognition tasks, demonstrating their\nefficacy in capturing spatial-temporal features within video data. They employ attention mechanisms to\nminimize redundant information and to model interactions over long distances in both space and time [100].\nThe adaptation of ViT to action recognition signifies the model's versatility and its potential for broader\napplications in computer vision beyond static image analysis.\nIn 2021, Arnab et al. [30] introduced a novel approach to video classification by leveraging the capabilities\nof Transformer models, traditionally successful in image classification. ViViT, a Pure-transformer based\nmodel, ingeniously processes spatio-temporal tokens extracted from videos through a series of Transformer\nlayers, addressing the challenge of managing the inherently long sequences of tokens characteristic of video\ndata. The model's novelty lies in its efficient variants that factorize the spatial and temporal dimensions\nof the input, significantly enhancing computational efficiency and scalability. Moreover, ViViT pioneers in\nregularizing the model during training and ingeniously utilizes pretrained image models, enabling effective\ntraining on relatively smaller datasets- -a notable advancement given the typically large datasets required\nby Transformer models. The paper's comprehensive ablation studies and methodical analysis culminate in\nViViT achieving state-of-the-art results on several prestigious video classification benchmarks, including\nKinetics 400 and 600, Epic Kitchens, Something-Something v2, and Moments in Time, thereby setting new\nstandards and outperforming existing methods based on deep 3D convolutional networks.\nWang et al. [101] revolutionized the field of video action recognition by transitioning from conventional\nunimodal methodologies to a multimodal learning framework that capitalizes on the semantic nuances of\nlabel texts. This innovative approach, embodied in ActionCLIP, redefines the task as a video-text match-\ning challenge, thereby facilitating zero-shot action recognition capabilities without necessitating additional\nlabeled data or parameter modifications. Central to ActionCLIP is the \"pre-train, prompt, and fine-tune\"\nparadigm, a strategy that effectively leverages extensive web image-text or video-text data for pre-training.\nThis paradigm ingeniously aligns the action recognition task with pre-training challenges through prompt\nengineering and ensures robust performance through end-to-end fine-tuning on specific datasets. Action-\nCLIP, with its ViT-B/16 backbone, not only demonstrates exceptional zero-shot/few-shot transfer abilities\nbut also achieves groundbreaking performance, notably securing a top-1 accuracy of 83.8% on the Kinetics-\n400 dataset. This paradigm shift underscores the potential of integrating rich semantic information from\nlabel texts into video representation, marking a significant leap forward in the domain of action recognition.\nBertasius et al. [29] delves into the TimeSformer model, a novel approach to video understanding that\nleverages the Transformer architecture, renowned for its success in NLP, and adapts it for video data. The\nTimeSformer stands out by employing a divided space-time attention mechanism, allowing the model to sep-\narately and efficiently process the spatial and temporal dimensions of videos. This approach is meticulously\ndetailed in the supplementary materials, which cover the model's implementation, training, and inference\nprocedures, alongside additional ablations and qualitative results. The TimeSformer is rigorously evaluated\non large-scale video datasets such as Kinetics-400, Kinetics-600, Something-Something-V2, and Diving-48,\ndemonstrating its robust performance and the model's ability to focus on relevant parts of the video for\nnuanced spatiotemporal reasoning. The visualizations of space-time attention in the paper further illustrate\nthe model's capacity to understand and interpret complex video content, marking a significant advancement\nin the field of video understanding.\nSharir et al. [102] introduced STAM (Space Time Attention Model), a groundbreaking approach to action\nrecognition in videos. STAM addresses the computational inefficiencies prevalent in traditional methods\nthat rely on 3D convolutions by significantly reducing the number of frames required for inference without\ncompromising accuracy. The model capitalizes on a temporal Transformer that applies global attention over\nvideo frames, effectively capturing salient information and enabling input efficiency. This novel approach\nallows STAM to achieve state-of-the-art results on the Kinetics dataset, notably achieving 80.5% top-1\naccuracy on Kinetics-400 with approximately 30% fewer frames per video and 40% faster inference compared\nto existing leading methods. The paper's innovation lies in its unique application of self-attention layers for\nboth spatial and temporal dimensions, marking a significant advancement in video action recognition by\noffering a solution that is not only accurate but also computationally efficient and practical for real-world\napplications.\nFan et al. [103] introduced Multiscale ViTs (MViT), a novel Transformer architecture designed for\nvideo and image recognition tasks, effectively integrating the concept of multiscale feature hierarchies with\nTransformer models. MViT operates through several channel-resolution scale stages, progressively expanding"}, {"title": "3.3 CNN-Transformer Hybrid Architectures", "content": "In 2021, Neimark et al. [112] presented a transformative approach to video recognition by introducing a\nTransformer-based framework that diverges from the traditional reliance on 3D ConvNets. Video Trans-\nformer Networks (VTN) capitalizes on the strengths of Transformers to process entire video sequences,\nemploying a modular architecture that includes a 2D spatial backbone for feature extraction, a temporal\nattention-based encoder leveraging Longformer for integrating temporal dynamics, and a classification MLP\nhead for action prediction. This innovative framework demonstrates remarkable efficiency, achieving train-\ning speeds 16.1 times faster and inference speeds 5.1 times faster than existing state-of-the-art methods,"}, {"title": "4 Proposed CNN-ViT Hybrid Architecture", "content": "In this section, we will detail our proposed CNN-ViT architecture for human action recognition, which\nleverages the benefits of both approaches described above. The architecture incorporates a TimeDistributed\nlayer with a CNN backbone, followed by a ViT model to classify actions in video sequences.\nSpatial Component (TimeDistributed CNN): The CNN backbone (i.e. Mobilenet [121]) in\nTimeDistributed layer processes each frame $X \\in \\mathbb{R}^{K \\times M \\times N}$, and outputs spatial features vector $v = p_{\\theta}(X) \\in\n\\mathbb{R}^{L}$ where $p_{\\theta}$ is the CNN model (e.g. MobileNet or VGG16) with parameters \u03b8 wrapped by TimeDistributed\nlayer."}, {"title": "5 Experiments", "content": "The goal of the presented experiments is not necessarily to produce a model that outperforms the state-\nof-the-art models in HAR field. Rather, the aim is to conduct a comparison among the ViT-only, CNN,\nand hybrid models to give further insights. The KTH dataset was chosen for its balanced representation of\nspatial and temporal features. Six experiments were conducted, training each of the aforementioned models\non three different lengths of frame sequences. Care was taken to avoid pretraining in order to ensure the\nneutrality of the results. The Transnet model by Alomar et al. [126], was used to represent the CNN model.\nThe ViT model was depicted in Figure 7. For the Spatial Component of the hybrid model, we employed the\nspatial component of Transnet and for the Temporal Component, we employed the same ViT model that\nwe used in ViT-only model.\nExperiment Settings. We constructed our model utilizing Python 3.6, incorporating the Keras deep\nlearning framework, OpenCV for image processing, matplotlib, and the scikit-learn library. The training\nand testing were performed on a computer equipped with an Intel Core i7 processor, an NVidia RTX 2070\ngraphics card, and 64GB of RAM.\nData. In 2004, the Royal Institute of Technology unveiled the KTH dataset, a significant and pub-\nlicly accessible dataset for action recognition [127]. Renowned as a benchmark dataset, it encompasses six\ntypes of actions: walking, jogging, running, boxing, hand-waving, and hand-clapping. The dataset features\nperformances by twenty-five different individuals, introducing a diversity in execution. Additionally, the envi-\nronment for each participant's actions was deliberately altered, including settings such as outdoors, outdoors\nwith scale changes, outdoors with clothing variations, and indoors. The KTH dataset comprises 2,391 video\nsequences, all recorded at 25 frames per second using a stationary camera against uniform backgrounds.\n5.1 Results and Discussion\nIn the exploration of HAR within video sequences, three distinct models were evaluated i.e., CNN, ViT-\nonly, and a Hybrid model. These models were trained on the KTH dataset, focusing on three different\ncontext lengths i.e., short (12 frames), medium (18 frames) and long (24 frames). The results from these\nexperiments provide insightful revelations into the efficacy of each model under different temporal contexts,\nsee Table 5.\nThe CNN model exhibited a decrease in accuracy as the frame length increased, recording 94.35% for\n12 frames, 93.91% for 18 frames, and 93.49% for 24 frames. This descending trend suggests that CNN\nmay struggle with processing longer sequences where temporal dynamics become more complex, potentially\nleading to challenges such as overfitting or difficulties in temporal feature retention over extended durations.\nIn contrast, the ViT model demonstrated an improvement in performance with longer sequences, achiev-\ning accuracies of 92.44% for 12 frames, 92.82% for 18 frames, and 93.69% for 24 frames. This ascending"}, {"title": "6 Challenges and Future Directions", "content": "Challenges The field of HAR faces several formidable challenges that stem from the inherent complexity\nof interpreting human movements within diverse and dynamic environments. One of the primary obstacles\nis the variability in human actions themselves, which can differ significantly in speed, scale, and execution\nfrom one individual to another [2]. This variability necessitates the development of sophisticated models\ncapable of generalizing across a wide range of actions without sacrificing accuracy [128]. Additionally, the\npresence of complex backgrounds and environments further complicates the task of HAR. Systems must be\nadept at isolating and recognizing human actions against a backdrop of potentially distracting or obstructive\nelements, which can vary from the bustling activity of a city street to the unpredictable conditions of outdoor\nsettings [37, 129].\nMoreover, HAR systems must navigate the fine line between inter-class similarity and intra-class vari-\nability, where actions that are similar to each other (such as running versus jogging) require nuanced\ndifferentiation, while the same action can appear markedly different when performed by different individu-\nals or under varying circumstances [130, 131]. The challenge of temporal segmentation adds another layer\nof complexity, as accurately determining the start and end of an action within a continuous video stream is\ncrucial for effective recognition [132]. Coupled with the need for computational efficiency to process video\ndata in real-time and the difficulties associated with obtaining large, accurately annotated datasets, these\nchallenges underscore the multifaceted nature of HAR [133]. Addressing these issues is critical for advanc-\ning the field and enhancing the practical applicability of HAR systems in real-world applications, from\nsurveillance and security to healthcare and entertainment.\nFuture Directions The motivation behind this work has been driven by the compelling need to bridge\nthe existing gap between the spatial feature extraction capabilities inherent in CNNs and the dynamic\ntemporal processing strengths found in ViTs [30]. Through the introduction of a novel hybrid model, an\nattempt has been made to leverage the synergistic potential of these technologies, thereby enhancing the\naccuracy and efficiency of HAR systems in capturing the complex spatial-temporal dynamics of human\nactions.\nLooking forward, a promising future for HAR is envisioned, particularly through the development of\nhybrid and integrated models. It is believed that the potential of these models extends beyond immediate\nperformance improvements, inspiring new directions for research within the field. It is anticipated that\nfuture studies will focus on optimizing these hybrid architectures, aiming to make them more scalable and\nadaptable to real-world applications across various domains such as surveillance, healthcare, and interactive\nmedia. Furthermore, the exploration of self-attention mechanisms and the adaptation of large-scale pre-\ntraining strategies from ViTs are seen as exciting prospects for HAR. These approaches are expected to lead\nto the development of more sophisticated models capable of understanding and interpreting human actions\nwith unprecedented accuracy and nuance.\nThe integration of CNNs and ViTs into Hybrid CNN-ViT models presents a promising avenue for\novercoming the challenges faced by HAR systems. These hybrid models capitalize on the strengths of\nboth architectures: the local feature extraction capabilities of CNNs and the global context understanding\nof ViTs. Future developments could focus on enhancing model adaptability to generalize across diverse\nactions, improving the isolation of human actions from complex backgrounds through advanced attention\nmechanisms, and developing nuanced differentiation techniques for closely related actions [124]. Innovations\nin model architecture, alongside the application of transfer learning and few-shot learning techniques, could\nsignificantly reduce the variability challenge in human actions.\nMoreover, addressing the temporal segmentation challenge requires the integration of specialized tempo-\nral modules and sequence-to-sequence models to accurately determine the start and end of an action within\ncontinuous video streams. Computational efficiency remains paramount for real-time processing, necessi-\ntating ongoing efforts in model optimization and the exploration of synthetic data generation to mitigate\nthe difficulties associated with obtaining large, accurately annotated datasets. Customizable Hybrid CNN-\nViT models that can be tailored for specific applications, from surveillance to healthcare, will ensure that\nthese advancements not only push the boundaries of academic research but also enhance practical applica-\nbility in real-world scenarios. Through these concerted efforts, Hybrid CNN-ViT models are poised to make\nsignificant contributions to the field of HAR, offering innovative solutions to its multifaceted challenges.\nThis work has highlighted the importance of continued innovation and cross-disciplinary collaboration in\nthe advancement of HAR technologies. By integrating insights from computer vision, machine learning, and\ndomain-specific knowledge, it is hoped that HAR systems will not only become more efficient and accurate\nbut also more responsive to the complexities and variances of human behavior in natural environments. As\nthe field moves forward, the focus is set on pushing the boundaries of what is possible in HAR, with the aim\nof creating systems that enhance human-computer interaction and contribute positively to society through\nvarious applications."}, {"title": "7 Conclusions", "content": "This survey provides a comprehensive overview of the current state of HAR by examining the roles and\nadvancements of RNNS, CNNs, and ViTs. It delves into the evolution of these architectures, emphasizing\ntheir individual contributions to the field. The introduction of a hybrid model that combines the spatial\nprocessing capabilities of CNNs with the temporal understanding of ViTs represents a methodological\nadvancement in HAR. This model aims to address the limitations of each architecture when used in isolation,\nproposing a unified approach that potentially enhances the accuracy and efficiency of action recognition\ntasks. The paper identifies key challenges and opportunities within HAR, such as the need for models that\ncan effectively integrate spatial and temporal information from video data. The exploration of hybrid models,\nas suggested, offers a pathway for future research, particularly in improving model performance on complex\nvideo datasets. The discussion encourages further investigation into optimizing these hybrid architectures\nand exploring their applicability across various domains. This work sets a foundation for future studies to\nbuild upon, aiming to push the boundaries of what is currently achievable in HAR and to explore new\napplications of these technologies in real-world scenarios."}]}