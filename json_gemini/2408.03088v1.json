{"title": "QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction", "authors": ["Siddhant Dutta", "Nouhaila Innan", "Alberto Marchisio", "Sadok Ben Yahia", "Muhammad Shafique"], "abstract": "Financial market prediction and optimal trading strategy development remain challenging due to market complexity and volatility. Our research in quantum finance and reinforcement learning for decision-making demonstrates the approach of quantum-classical hybrid algorithms to tackling real-world financial challenges. In this respect, we corroborate the concept with rigorous backtesting and validate the framework's performance under realistic market conditions, by including fixed transaction cost per trade. This paper introduces a Quantum Attention Deep Q-Network (QADQN) approach to address these challenges through quantum-enhanced reinforcement learning. Our QADQN architecture uses a variational quantum circuit inside a traditional deep Q-learning framework to take advantage of possible quantum advantages in decision-making. We gauge the QADQN agent's performance on historical data from major market indices, including the S&P 500. We evaluate the agent's learning process by examining its reward accumulation and the effectiveness of its experience replay mechanism. Our empirical results demonstrate the QADQN's superior performance, achieving better risk-adjusted returns with Sortino ratios of 1.28 and 1.19 for non-overlapping and overlapping test periods respectively, indicating effective downside risk management.", "sections": [{"title": "I. INTRODUCTION", "content": "Financial markets are characterized by their complexity and unpredictability, driven by many factors that fuel volatility and challenge the efficacy of predictive models. Traditional methods in computational finance often struggle to fully utilize the vast datasets available due to limitations in computational power and algorithm complexity [1]. The complexity arises from various interacting elements such as macroeconomic indicators, market sentiment, and geopolitical events contributing to market volatility [2]\u2013[4].\nVolatility in financial markets poses significant challenges for both prediction and risk management. Traditional volatility models, like GARCH and its variants, have been widely used but often fall short of capturing market behaviors' intricate and dynamic nature. The limitations of these models highlight the need for more advanced approaches that can handle non-linearities and high-dimensional data.\nFinancial statement data, alongside information extracted from business news, can be integrated with advanced ML algorithms to generate investment signals or predict a company's future performance [5]. This approach enhances the stock screening process by identifying promising investment opportunities. However, while these algorithms effectively address stock selection, they do not inherently resolve the issue of optimal position sizing and allocation among the chosen investments. Consequently, the trader must still exercise discretion in determining the precise timing for entering and exiting positions [6].\nCurrent deep learning methodologies, such as Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Transformer models, have been employed to predict stock prices by learning from historical data. These models excel at capturing temporal dependencies and patterns in the data, leading to improved predictive performance. However, they often require extensive computational resources and can be prone to overfitting, especially when dealing with noisy and non-stationary financial data [7], [8].\nIn addition to employing Machine Learning (ML) and deep learning techniques, traditional stochastic process models such as Geometric Brownian Motion (GBM) offer a reliable method for stock price prediction by simulating market returns as a continuous-time random walk. The GBM model posits that the logarithm of stock prices follows a normal distribution, thereby incorporating the inherent randomness and volatility of financial markets. However, this assumption also presents a limitation, as it may not fully capture extreme market events or non-linear dynamics [9], [10]. The significance of GBM extends to its foundational role in the development of the Black-Scholes option pricing model, which is extensively applied in the valuation of financial derivatives [11]. This model's simplicity and interpretability make it a valuable tool for stock price prediction, provided historical price data is available [12].\nThe QADQN framework addresses the limitations of computational overhead in attention mechanisms by replacing them with a Multi-Head Quantum Attention layer. It also improves explainability by integrating quantum attention mechanisms, utilizing the computational power of quantum computing to enhance the processing and interpretation of large-scale financial data. Using imitative learning strategies helps to solve the problem of finding a balance between exploitation and exploration [13]. The trading agent is instructed via the use of a Q-learning algorithm and a replay buffer that is initially filled with actions taken from the Dual Thrust approach [14]. These methods provide the network with enhanced expertise in the financial area when they are integrated into the POMDP framework.\nThe novel contributions of this work can be summarized as follows:\n\u2022 Incorporation of quantum attention layers within deep Reinforcement Learning (RL) agents to enhance the framework's ability to focus on relevant market features, resulting in improved decision-making efficiency.\n\u2022 Application of the QADQN within a Partially Observable Markov Decision Process (POMDP) framework utilizing attention-based deterministic policy gradient method, which is well-suited for dealing with the uncertainty and partial observability inherent in financial markets.\n\u2022 Through modified Q-learning and the initialization of a replay buffer, pre-populated with actions derived from the established Dual Thrust methodology, imitative learning approaches in quantum agents aim to maintain an optimal balance between exploration and exploitation of available resources.\n\u2022 Backtesting and validation against historical market data, including major indices like the S&P 500. The backtesting process includes considerations for transaction costs and estimates the total return rate, Maximum Drawdown, and the final Sharpe and Sortino ratio, providing an assessment of the framework's reliability.\n\u2022 Development of an attention deterministic policy gradient method tailored to handle the POMDP setting for quantum agents. This method enhances the framework's ability to learn effective trading strategies and trends, thereby improving its long-term reward accumulation.\nSec. II reviews Quantum Computing (QC) and RL in financial markets; Sec. III explains the QADQN, POMDP, and dual thrust strategy; Sec. IV compares QADQN's performance with prior methods and different market scenarios; Sec. V concludes with key findings and future directions for quantum-enhanced RL in trading."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Recent advancements in QC have demonstrated significant potential for financial market applications, particularly in enhancing prediction accuracy and decision-making efficiency. Quantum Machine Learning (QML) emerges as a promising field [15], [16], combining the computational power of quantum systems with ML algorithms to analyze financial data more effectively [17], [18].\nConsider a quantum state representing a financial portfolio:\n$\\vert \\Psi_{portfolio} \\rangle = \\sum_{i} \\alpha_{i} \\vert i \\rangle$,\nwhere $i$ represents individual assets and $\\alpha_i$ their corre-sponding weights [19], subject to the normalization condition $\\sum \\alpha_i^2 = 1$.\nPortfolio optimization can be formulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem:\n$\\min f(x) = x^T Q x + c^T x$,\nwhere x is a binary vector representing asset selection, Q is the covariance matrix, and c represents expected returns. This can be mapped to a quantum Hamiltonian:\n$H = \\sum_{i} q_i \\sigma_i^z - \\sum_{i \\neq j} c_{ij} \\sigma_i^z \\sigma_j^z$,\nwhere $\\sigma^z_i$ are Pauli-Z operators. Quantum algorithms like Quantum Approximate Optimization Algorithm (QAOA) can be applied to find the ground state of this Hamiltonian, representing the optimal portfolio allocation:\n$\\vert \\Psi_{QAOA} \\rangle = \\prod_{p} e^{-i\\beta_p H_B} e^{-i\\gamma_p H_C} \\vert \\Psi_{init} \\rangle$,\nwhere $H_B$ and $H_C$ are mixing and cost Hamiltonians respectively, and $\\beta_p, \\gamma_p$ are variational parameters [20].\nFor risk assessment, quantum entanglement can model complex correlations. Value-at-Risk (VaR) is a widely used measure in risk management to quantify the potential loss on an asset or portfolio over a specific time horizon and with a certain confidence level:\n$VaR_{\\alpha}[L] = \\inf \\{ p \\in \\mathbb{R}^+ : \\langle \\psi \\vert P_{L \\le p} \\vert \\psi \\rangle \\ge \\alpha \\}$,\nwhere $P_{L \\le p}$ is a projection operator onto the subspace where the loss is less than or equal to p.\nConditional Value-at-Risk (CVaR) [21], or expected shortfall, is the expected loss given that the loss exceeds the VaR threshold. It can be expressed as:\n$\\vert \\psi_{VaR} \\rangle = \\frac{P_{L > VaR} \\vert \\psi \\rangle}{\\sqrt{ \\langle \\psi \\vert P_{L > VaR} \\vert \\psi \\rangle } },$\n$CVaR_{\\alpha}[L] = \\langle \\psi_{VaR} \\vert L \\vert \\psi_{VaR} \\rangle$.\nThe Economic Capital Requirement (ECR) is defined as the difference between VaR and the expected loss:\n$ECR[L] = VaR[L] - \\langle \\psi \\vert \\hat{L} \\vert \\psi \\rangle$."}, {"title": "B. Quantum Machine Learning for Finance", "content": "QML algorithms, such as quantum neural networks, can be used for financial time series prediction [22], [23]. These quantum algorithms leverage superposition and entanglement to process large financial datasets more efficiently. One promising field within QML is Quantum Reinforcement Learning (QRL) [24], which integrates QC with RL to develop advanced trading strategies. In QRL, quantum circuits are used to enhance exploration and exploitation mechanisms within RL frameworks. The basic RL model can be represented as:\n$Q^*(s, a) = \\max_{\\pi} \\mathbb{E} [ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\vert S_t = s, a_t = a]$,\nwhere $Q^*(s, a)$ is the optimal action-value function, $\\gamma$ is the discount factor, $r_t$ is the reward at time t, and $\\pi$ is the policy.\nHowever, integrating quantum circuits into RL frameworks poses significant challenges. For instance, Quantum Deep Q-Networks (QDQN) and Quantum Policy Gradient methods have shown theoretical benefits, but issues like quantum noise, decoherence, and scalability impede practical implementation [25]. Furthermore, the optimization of VQC is represented as:\n$\\theta^* = \\arg \\min_\\theta \\langle \\psi(\\theta) \\vert H \\vert \\psi(\\theta) \\rangle$,\nand it requires careful tuning of quantum parameters $\\theta$, which can be computationally intensive in terms of convergence.\nRecent advancements in quantum attention mechanisms have demonstrated potential in enhancing the performance of ML, including computer vision and high energy physics tasks [26]\u2013[28]. These mechanisms leverage QC principles to efficiently handle the high-dimensional parameter spaces associated with self-attention, a crucial component in many contemporary deep-learning architectures [29]."}, {"title": "III. QADQN FRAMEWORK", "content": "The Quantum Attention Deep Q-Network (QADQN) framework extends the concept of hybrid quantum-classical architectures to the domain of RL. By integrating NISQ-based quantum self-attention mechanisms with deep Q-networks [30]\u2013[33], QADQN aims to improve decision-making processes in complex environments. Our approach involves carefully optimizing Variational Quantum Circuit (VQC) parameters using the backpropagation method for gradient computation, addressing convergence challenges in high-dimensional quantum-classical hybrid systems in the financial domain.\nThe input to the QADQN is an n-day state representation of financial data transformed using a logarithmic function, denoted as $S_t \\in \\mathbb{R}^{n \\times f}$, where n is the number of days, and f is the number of features per day. Each row of the state $S_t$ at time t is given by:\n$S_{t,i} = [ln(P_{t-i+1}/P_{t-i}),...,ln(f^j_{t-i+1}/f^j_{t-i})]$\nwhere $p_t$ represents the close price at time t, $f^j_t$ represents the j-th open-high-low-close (OHLC)-based feature at time t, and i ranges from 1 to n, covering the n-day window."}, {"title": "A. Pre-Neural Net", "content": "The LSTM layer processes the input sequence and outputs a sequence of hidden states $h_t \\in \\mathbb{R}^{64}$ for each time step. For single-step predictions, we use the last hidden state, while for sequence outputs, we use the full sequence of hidden states [34], as shown in Fig. 1:\n$h_t, c_t = LSTM(S_t, h_{t-1}, c_{t-1})$,\nwhere $h_t$ is the hidden state and $c_t$ is the cell state at time t.\n$x_t = ELU(W_L ELU(W_{L-1}...ELU(W_1.h_t + b_1) + b_{L-1}) + b_L)$.\nWhile attention mechanisms, similar to those in transformer models, can be employed independently, our hybrid approach enables the capture of long-term dependencies by extracting temporal features with LSTM and refining the LSTM's contextualized output with the quantum attention mechanism."}, {"title": "B. Quantum Attention Mechanism", "content": "The core of our framework is the Quantum Multihead Self-Attention (QMSA) mechanism. This mechanism projects the input state to a quantum state using parameterized quantum circuits, computes attention scores, and aggregates the results:\na) Data Encoding\nEach input row $x_i$ of the LSTM output is encoded into a quantum state using a data loader operator $U^{\\dagger}(x_i)$:\n$\\vert x_i \\rangle = U^{\\dagger}(x_i) \\vert 0 \\rangle = \\bigotimes_{j=1}^{d_h} R_x(x_{ij}) \\vert 0 \\rangle$,\nwhere $R_x$ is a parameterized rotation around the x-axis and $d_h$ represents the dimension of the hidden state.\nb) Key and Query Operations\nFor each input row $x_i$, key operator $K^{\\dagger}(\\theta_k)$ and query operator $Q^{\\dagger}(\\theta_q)$ are applied:\n$K_i = \\langle x_i \\vert K^{\\dagger}(\\theta_k) Z_0 K(\\theta_k) \\vert x_i \\rangle$,\n$Q_i = \\langle x_i \\vert Q^{\\dagger}(\\theta_q) Z_0 Q(\\theta_q) \\vert x_i \\rangle$,\nwhere $Z_0$ represents a spin measurement of the qubit in the z-direction.\nc) Value Operation\nEach row of the input is passed through a value operator $V^{\\dagger}(\\theta_v)$ to obtain the value matrix V:\n$V_{ij} = \\langle x_i \\vert V^{\\dagger}(\\theta_v) Z_j V(\\theta_v) \\vert x_i \\rangle$."}, {"title": "Action Selection Policy (UCB):", "content": "$P(a|s) = \\begin{cases} 1 - c + c \\frac{\\sqrt{log(t)}}{N_t(s)}, & \\text{if } a = \\arg \\max_a Q(s, a) \\\\ c \\frac{\\sqrt{log(t)}}{N_t(s)}, & \\text{otherwise} \\end{cases}$,\nwhere c is a constant determining the exploration-exploitation trade-off, t is the total number of time steps, and Nt(s) is the number of times state s has been visited up to time t."}, {"title": "Dual Thrust Strategy:", "content": "Range = max[HH \u2013 LC, HC \u2013 LL],\nBuyLine = Open + K1 \u00d7 Range,\nSellLine = Open \u2013 K2 \u00d7 Range,\nwhere Open represents the day's opening price, K1 and K2 are constants controlling market resistance levels against breaking BuyLine and SellLine, respectively. HH, LC, HC, and LL denote the highest high and lowest close prices over the previous periods [36]."}, {"title": "E. Training Algorithm", "content": "The QADQN agent is trained using a combination of experience replay and prioritized experience replay. The training process is detailed in Algorithm 1."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The experiments are conducted using the S&P 500 as the standard dataset with OHLC fetched from Yahoo finance API [40], with a training period from January 15, 2016, to January 15, 2020, and evaluation on two test periods: an overlapping period from January 16, 2019, to January 16, 2024, and a non-overlapping period from January 16, 2020, to January 16, 2024. The QADQN framework is implemented with 4 qubits for each quantum layer, a 24-day window size, a discount factor ($\\gamma$) of 0.95, and 200 training episodes. The Dual Thrust strategy parameters are set to k1 = 0.8 and k2 = 0.4. The framework is trained using both standard experience replay and prioritized experience replay."}, {"title": "B. Performance Evaluation", "content": "The QADQN model's performance is evaluated using both visual analysis of trading actions and quantitative metrics derived from backtesting and is compared to the Buy & Hold Method and Deep Deterministic Policy Gradient (DDPG), which is an off-policy A2C model [41]."}, {"title": "2) Backtesting Results", "content": "Backtesting is performed using the Backtest library with a commission rate of 0.2%. The results for both test periods are summarized in Table I."}, {"title": "C. Discussion", "content": "The QADQN framework demonstrates strong potential in applying quantum-enhanced RL to financial trading. The framework not only achieves substantial positive returns but also outperforms the Buy & Hold and DDPG benchmarks in both test periods, particularly in the non-overlapping period."}, {"title": "V. CONCLUSION", "content": "This paper introduces a QADQN, which integrates a VQC within a traditional deep Q-learning network, aimed at emphasizing quantum computational advantages for financial trading decision-making, presenting a solution for investors to optimize trading strategies. Through rigorous literature review, framework development, and comprehensive empirical evaluation, our work marks a substantial advancement in quantum finance.\nThe empirical results confirm the QADQN framework's robust performance, significantly surpassing standard frameworks and the buy-and-hold strategy in terms of total returns and risk-adjusted metrics during both test periods. This performance highlights the framework's effectiveness in adapting to diverse market scenarios and its capability to manage downside risks, evidenced by favorable Sharpe and Sortino ratios. Despite these promising outcomes, our study recognizes the need for broader testing across varied market conditions and addresses the challenges related to quantum circuit scalability in practical applications. Future research should thus focus on enhancing the scalability of the QADQN framework, incorporating more complex quantum circuits, and expanding its application to different asset classes. Additionally, exploring quantum noise impacts and the potential for quantum error correction within the framework could further solidify the practical deployment of quantum-enhanced RL in financial markets."}]}