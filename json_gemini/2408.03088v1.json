{"title": "QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction", "authors": ["Siddhant Dutta", "Nouhaila Innan", "Alberto Marchisio", "Sadok Ben Yahia", "Muhammad Shafique"], "abstract": "Financial market prediction and optimal trading strategy development remain challenging due to market complexity and volatility. Our research in quantum finance and reinforcement learning for decision-making demonstrates the approach of quantum-classical hybrid algorithms to tackling real-world financial challenges. In this respect, we corroborate the concept with rigorous backtesting and validate the framework's performance under realistic market conditions, by including fixed transaction cost per trade. This paper introduces a Quantum Attention Deep Q-Network (QADQN) approach to address these challenges through quantum-enhanced reinforcement learning. Our QADQN architecture uses a variational quantum circuit inside a traditional deep Q-learning framework to take advantage of possible quantum advantages in decision-making. We gauge the QADQN agent's performance on historical data from major market indices, including the S&P 500. We evaluate the agent's learning process by examining its reward accumulation and the effectiveness of its experience replay mechanism. Our empirical results demonstrate the QADQN's superior performance, achieving better risk-adjusted returns with Sortino ratios of 1.28 and 1.19 for non-overlapping and overlapping test periods respectively, indicating effective downside risk management.", "sections": [{"title": "I. INTRODUCTION", "content": "Financial markets are characterized by their complexity and unpredictability, driven by many factors that fuel volatility and challenge the efficacy of predictive models. Traditional methods in computational finance often struggle to fully utilize the vast datasets available due to limitations in computational power and algorithm complexity [1]. The complexity arises from various interacting elements such as macroeconomic indicators, market sentiment, and geopolitical events contributing to market volatility [2]\u2013[4].\nVolatility in financial markets poses significant challenges for both prediction and risk management. Traditional volatility models, like GARCH and its variants, have been widely used but often fall short of capturing market behaviors' intricate and dynamic nature. The limitations of these models highlight the need for more advanced approaches that can handle non-linearities and high-dimensional data.\nFinancial statement data, alongside information extracted from business news, can be integrated with advanced ML algorithms to generate investment signals or predict a company's future performance [5]. This approach enhances the stock screening process by identifying promising investment opportunities. However, while these algorithms effectively address stock selection, they do not inherently resolve the issue of optimal position sizing and allocation among the chosen investments. Consequently, the trader must still exercise discretion in determining the precise timing for entering and exiting positions [6].\nCurrent deep learning methodologies, such as Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Transformer models, have been employed to predict stock prices by learning from historical data. These models excel at capturing temporal dependencies and patterns in the data, leading to improved predictive performance. However, they often require extensive computational resources and can be prone to overfitting, especially when dealing with noisy and non-stationary financial data [7], [8].\nIn addition to employing Machine Learning (ML) and deep learning techniques, traditional stochastic process models such as Geometric Brownian Motion (GBM) offer a reliable method for stock price prediction by simulating market returns as a continuous-time random walk. The GBM model posits that the logarithm of stock prices follows a normal distribution, thereby incorporating the inherent randomness and volatility of financial markets. However, this assumption also presents a limitation, as it may not fully capture extreme market events or non-linear dynamics [9], [10]. The significance of GBM extends to its foundational role in the development of the Black-Scholes option pricing model, which is extensively applied in the valuation of financial derivatives [11]. This model's simplicity and interpretability make it a valuable tool for stock price prediction, provided historical price data is available [12].\nThe QADQN framework addresses the limitations of computational overhead in attention mechanisms by replacing them with a Multi-Head Quantum Attention layer. It also improves explainability by integrating quantum attention mechanisms, utilizing the computational power of quantum computing to enhance the processing and interpretation of large-scale financial data. Using imitative learning strategies helps to solve the problem of finding a balance between exploitation and exploration [13]. The trading agent is instructed via the use of a Q-learning algorithm and a replay buffer that is initially filled with actions taken from the Dual Thrust approach [14]. These methods provide the network with enhanced expertise in the financial area when they are integrated into the POMDP framework.\nThe novel contributions of this work can be summarized as follows:\n\u2022 Incorporation of quantum attention layers within deep Rein-forcement Learning (RL) agents to enhance the framework's ability to focus on relevant market features, resulting in improved decision-making efficiency.\n\u2022 Application of the QADQN within a Partially Observable Markov Decision Process (POMDP) framework utilizing attention-based deterministic policy gradient method, which is well-suited for dealing with the uncertainty and partial observability inherent in financial markets.\n\u2022 Through modified Q-learning and the initialization of a replay buffer, pre-populated with actions derived from the established Dual Thrust methodology, imitative learning approaches in quantum agents aim to maintain an optimal balance between exploration and exploitation of available resources.\n\u2022 Backtesting and validation against historical market data, including major indices like the S&P 500. The backtesting process includes considerations for transaction costs and estimates the total return rate, Maximum Drawdown, and the final Sharpe and Sortino ratio, providing an assessment of the framework's reliability.\n\u2022 Development of an attention deterministic policy gradient method tailored to handle the POMDP setting for quantum agents. This method enhances the framework's ability to learn effective trading strategies and trends, thereby improving its long-term reward accumulation.\nSec. II reviews Quantum Computing (QC) and RL in financial markets; Sec. III explains the QADQN, POMDP, and dual thrust strategy; Sec. IV compares QADQN's performance with prior methods and different market scenarios; Sec. V concludes with key findings and future directions for quantum-enhanced RL in trading."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Recent advancements in QC have demonstrated significant potential for financial market applications, particularly in enhancing prediction accuracy and decision-making efficiency. Quantum Machine Learning (QML) emerges as a promising field [15], [16], combining the computational power of quantum systems with ML algorithms to analyze financial data more effectively [17], [18].\nConsider a quantum state representing a financial portfolio:\n$\\Wportfolio) = \\sum_{i}\u03b1_{i}|a_{i})$,\nwhere $a_{i}$ represents individual assets and $a_{i}$ their corre-sponding weights [19], subject to the normalization condition $\\sum \u03b1_{i}^2 = 1$.\nPortfolio optimization can be formulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem:\n$\\min f(x) = x^{T}Qx + c^{T}x$,", "A. Quantum Computing for Finance": ""}, {"title": null, "content": "where x is a binary vector representing asset selection, Q is the covariance matrix, and c represents expected returns. This can be mapped to a quantum Hamiltonian:\n$H = \\sum_{i}q_{i}\u03c3_{i} - \\sum_{i}c_{ij}\u03c3_{i}\u03c3_{j}$,\nwhere $\\sigma_{i}$ are Pauli-Z operators. Quantum algorithms like Quantum Approximate Optimization Algorithm (QAOA) can be applied to find the ground state of this Hamiltonian, representing the optimal portfolio allocation:\n$|\\QAOA) = \\prod_{p}e^{-i\u03b2_{p}HB}e^{-i\u03b3_{p}HC}|\\init)$,", "A. Quantum Computing for Finance": ""}, {"title": null, "content": "where HB and He are mixing and cost Hamiltonians respectively, and \u03b2p, Yp are variational parameters [20].\nFor risk assessment, quantum entanglement can model complex correlations. Value-at-Risk (VaR) is a widely used measure in risk management to quantify the potential loss on an asset or portfolio over a specific time horizon and with a certain confidence level:\n$VaR_{\u03b1}[L] = \\inf{p \u2208 R+ : <\u03c8|P_{L}\u2264p|\u03c8) \u2265 \u03b1}$,\nwhere $P_{L}$ is a projection operator onto the subspace where the loss is less than or equal to p.\nConditional Value-at-Risk (CVaR) [21], or expected shortfall, is the expected loss given that the loss exceeds the VaR threshold. It can be expressed as:\n$|\u03c8VaR) = \\frac{P_{L>VaR}|\u03c8)}{\\sqrt{<\u03c8|P_{L>VaR}|\u03c8>}}$,\n$CVaR_{\u03b1}[L] = <\u03c8VaR|L|\u03c8VaR>$.\nThe Economic Capital Requirement (ECR) is defined as the difference between VaR and the expected loss:\n$ECR[L] = VaR_{\u03b1}[L] - <\u03c8|\\hat{L}|\u03c8>$.\nQML algorithms, such as quantum neural networks, can be used for financial time series prediction [22], [23]. These quantum algorithms leverage superposition and entanglement to process large financial datasets more efficiently. One promising field within QML is Quantum Reinforcement Learning (QRL) [24], which integrates QC with RL to develop advanced trading strategies. In QRL, quantum circuits are used to enhance exploration and exploitation mechanisms within RL frameworks. The basic RL model can be represented as:\n$Q^{*}(s,a) = \\max_{\u03c0}E[\\sum_{t=0}^{\u221e}\u03b3^{t}r_{t+1}|s_{t} = s, a_{t} = a]$,", "A. Quantum Computing for Finance": ""}, {"title": null, "content": "where $Q^{*}(s,a)$ is the optimal action-value function, y is the discount factor, rt is the reward at time t, and is the policy.\nHowever, integrating quantum circuits into RL frameworks poses significant challenges. For instance, Quantum Deep Q-Networks (QDQN) and Quantum Policy Gradient methods have shown theoretical benefits, but issues like quantum noise,", "A. Quantum Computing for Finance": ""}, {"title": "III. QADQN FRAMEWORK", "content": "The Quantum Attention Deep Q-Network (QADQN) framework extends the concept of hybrid quantum-classical architectures to the domain of RL. By integrating NISQ-based quantum self-attention mechanisms with deep Q-networks [30]\u2013[33], QADQN aims to improve decision-making processes in complex environments. Our approach involves carefully optimizing Variational Quantum Circuit (VQC) parameters using the backpropagation method for gradient computation, addressing convergence challenges in high-dimensional quantum-classical hybrid systems in the financial domain.\nThe input to the QADQN is an n-day state representation of financial data transformed using a logarithmic function, denoted as $S_{t} \u2208 R^{n\u00d7f}$, where n is the number of days, and f is the number of features per day. Each row of the state $S_{t}$ at time t is given by:\n$S_{t,i} = [ln(P_{t-i+1}/P_{t-i}), ..., ln(f_{t-i+1}/f_{t-i})]$,\nwhere pt represents the close price at time t, $f^{j}_{t}$ represents the j-th open-high-low-close (OHLC)-based feature at time t, and i ranges from 1 to n, covering the n-day window.\nThe LSTM layer processes the input sequence and outputs a sequence of hidden states $h_{t} \u2208 R^{64}$ for each time step. For single-step predictions, we use the last hidden state, while for", "A. Pre-Neural Net": ""}, {"title": null, "content": "sequence outputs, we use the full sequence of hidden states [34], as shown in Fig. 1:\n$h_{t}, c_{t} = LSTM(S_{t}, h_{t-1}, c_{t-1})$,\nwhere ht is the hidden state and ct is the cell state at time t.\n$x_{t} = ELU(W_{L}ELU(W_{L-1}...ELU(W_{1}.h_{t}+b_{1})+b_{L-1})+b_{z})$.\nWhile attention mechanisms, similar to those in transformer models, can be employed independently, our hybrid approach enables the capture of long-term dependencies by extracting temporal features with LSTM and refining the LSTM's contextualized output with the quantum attention mechanism.", "A. Pre-Neural Net": ""}, {"title": "B. Quantum Attention Mechanism", "content": "The core of our framework is the Quantum Multihead Self-Attention (QMSA) mechanism. This mechanism projects the input state to a quantum state using parameterized quantum circuits, computes attention scores, and aggregates the results:\nEach input row $x_{i}$ of the LSTM output is encoded into a quantum state using a data loader operator $U^{\\dagger}(x_{i})$:\n$|x_{i}) = U^{\\dagger}(x_{i})|0) = \\prod_{j=1}^{dh} R_{x}(x_{ij})|0)$,\nwhere Rx is a parameterized rotation around the x-axis and dh represents the dimension of the hidden state.\nFor each input row $x_{i}$, key operator $K^{\\dagger}(\u03b8_{K})$ and query operator $Q^{\\dagger}(\u03b8_{Q})$ are applied:\n$K_{i} = (x_{i}|K^{\\dagger}(\u03b8_{K})Z_{0}K(\u03b8_{K})|x_{i})$,\n$Q_{i} = (x_{i}|Q^{\\dagger}(\u03b8_{Q})Z_{0}Q(\u03b8_{Q})|x_{i})$,", "B. Quantum Attention Mechanism": ""}, {"title": null, "content": "where $Z_{0}$ represents a spin measurement of the qubit in the z-direction.\nEach row of the input is passed through a value operator $V^{\\dagger}(\u03b8_{V})$ to obtain the value matrix V:\n$V_{ij} = (x_{i}|V^{\\dagger}(\u03b8_{V})Z_{j}V(\u03b8_{V})|x_{i})$.", "B. Quantum Attention Mechanism": ""}, {"title": "D. Agent", "content": "The QADQN agent is designed to make trading decisions based on historical financial data. The agent's architecture consists of several key components:\n\u2022 Action space: A = {sit, buy, sell}.\n\u2022 Policy network (Q): A quantum-classical hybrid network for action selection.\n\u2022 Target network (Q): A periodically updated copy of the policy network for stable learning.\n\u2022 Prioritized Replay memory: For experience replay and prioritized experience replay.\nThe QADQN agent employs an Upper Confidence Bound (UCB) policy for action selection [35], and integrates the Dual Thrust pre-populated actions from prioritized experience replay memory for the final action decision:", "C. Quantum Post-Net Layer": ""}, {"title": null, "content": "Action Selection Policy (UCB):\n$P(a|s)=\\begin{cases}1 - c + cN_{t}(s)^{\\frac{log(t)}{2}} &\\text{if a = arg max}_{a}Q(s,a) \\\\ c\\frac{log(t)}{N_{t}(s)} & otherwise \\end{cases}$\nwhere c is a constant determining the exploration-exploitation trade-off, t is the total number of time steps, and Nt(s) is the number of times state s has been visited up to time t.\nDual Thrust Strategy:\n$\\begin{aligned}Range &= max[HH \u2013 LC, HC \u2013 LL],\\\\ BuyLine &= Open + K1 \u00d7 Range, \\\\SellLine &= Open \u2013 K2 \u00d7 Range,\\end{aligned}$\nwhere Open represents the day's opening price, K1 and K2 are constants controlling market resistance levels against breaking BuyLine and SellLine, respectively. HH, LC, HC, and LL denote the highest high and lowest close prices over the previous periods [36].\nThe QADQN agent is trained using a combination of experience replay and prioritized experience replay. The training process is detailed in Algorithm 1.\nInitialize replay memory D to capacity N\nInitialize prioritized replay memory PD to capacity M\nInitialize action-value function Q with random weights \u03b8\nInitialize target action-value function Q with weights \u03b8- = \u03b8", "C. Quantum Post-Net Layer": ""}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The experiments are conducted using the S&P 500 as the standard dataset with OHLC fetched from Yahoo finance API [40], with a training period from January 15, 2016, to January 15, 2020, and evaluation on two test periods: an overlapping period from January 16, 2019, to January 16, 2024, and a non-overlapping period from January 16, 2020, to January 16, 2024. The QADQN framework is implemented with 4 qubits for each quantum layer, a 24-day window size, a discount factor (y) of 0.95, and 200 training episodes. The Dual Thrust strategy parameters are set to k1 = 0.8 and k2 = 0.4. The framework is trained using both standard experience replay and prioritized experience replay.\nThe QADQN model's performance is evaluated using both visual analysis of trading actions and quantitative metrics derived from backtesting and is compared to the Buy & Hold Method and Deep Deterministic Policy Gradient (DDPG), which is an off-policy A2C model [41].\nFig. 3 presents a visual representation of the model's trading activities superimposed on the S&P 500 price chart for the non-overlapping test period. The green markers indicate buy actions, predominantly occurring at lower price points of the index, while the red markers denote sell actions, typically executed at higher price levels. This pattern suggests that the agent has developed a strategy that potentially capitalizes on price fluctuations, buying at relative lows and selling at relative highs. Such behavior indicates the model's ability to identify and exploit short-term price trends within the market.", "A. Experimental Setup": ""}, {"title": null, "content": "$\\theta_{t} = \\theta_{t-1} - \\alpha \\cdot sign(m_{t}) - \\alpha\\lambda\\theta_{t-1}$,\nwhere \u03b8t is the parameter at time step t, mt is the exponential moving average of the gradient, a is the learning rate, A is the weight decay coefficient and sign(x) is the sign function. Due to the usage of sign operation as seen in equation 23, it balances the gradient history and current gradient weighting, leading to better global optimization. This makes the gradients smooth and continuous, leading to better decision-making by the QADQN. The loss function used is Huber loss as it proves advantageous in scenarios where observed rewards occasionally contain outliers-instances where unrealistically large negative or positive rewards may occur during training but not during testing, and it can be described as:\n$L = H(Q(S_{t}, a_{t}; \\theta) - (r_{t} + \u03b3 max_{a'} Q(S_{t+1}, a'; \\theta^{-})))$,", "A. Experimental Setup": ""}, {"title": null, "content": "where H is the Huber loss function. Unlike L2 loss, which could be significantly affected by such outliers, Huber loss maintains robustness by moderating the impact of these anomalies on the training process [38], [39]", "A. Experimental Setup": ""}, {"title": "V. CONCLUSION", "content": "This paper introduces a QADQN, which integrates a VQC within a traditional deep Q-learning network, aimed at emphasizing quantum computational advantages for financial trading decision-making, presenting a solution for investors to optimize trading strategies. Through rigorous literature review, framework development, and comprehensive empirical evaluation, our work marks a substantial advancement in quantum finance. The empirical results confirm the QADQN framework's robust performance, significantly surpassing standard frameworks and the buy-and-hold strategy in terms of total returns and risk-adjusted metrics during both test periods. This performance highlights the framework's effectiveness in adapting to diverse market scenarios and its capability to manage downside risks, evidenced by favorable Sharpe and Sortino ratios. Despite these promising outcomes, our study recognizes the need for broader testing across varied market conditions and addresses the challenges related to quantum circuit scalability in practical applications. Future research should thus focus on enhancing the scalability of the QADQN framework, incorporating more complex quantum circuits, and expanding its application to different asset classes. Additionally, exploring quantum noise impacts and the potential for quantum error correction within the framework could further solidify the practical deployment of quantum-enhanced RL in financial markets."}]}