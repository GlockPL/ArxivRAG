{"title": "Framework for Curating Speech Datasets and Evaluating ASR Systems: A Case Study for Polish", "authors": ["Micha\u0142 Junczyk"], "abstract": "Speech datasets available in the public domain are often underutilized because of challenges in discoverability and interoperability. A comprehensive framework has been designed to survey, catalog, and curate available speech datasets, which allows replicable evaluation of automatic speech recognition (ASR) systems. A case study focused on the Polish language was conducted; the framework was applied to curate more than 24 datasets and evaluate 25 combinations of ASR systems and models. This research constitutes the most extensive comparison to date of both commercial and free ASR systems for the Polish language. It draws insights from 600 system-model-test set evaluations, marking a significant advancement in both scale and comprehensiveness. The results of surveys and performance comparisons are available as interactive dashboards, along with curated datasets and the open challenge call. Tools used for evaluation are open-sourced, facilitating replication and adaptation for other languages, as well as continuous expansion with new datasets and systems.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background", "content": "The Polish language is spoken by more than 50 million people worldwide. The number of available ASR systems and services, as well as speech data resources that support Polish, is systematically growing. However, the community lacks the resources to methodically evaluate and track progress. First, the available data assets are underutilized due to challenges such as discoverability, licensing, and interoperability. Secondly, there is no standardized ASR benchmark dataset for Poland. These issues hinder the development of new systems and applications, as reliable benchmarks and leaderboards are crucial to drive research progress and assess the suitability of ASR technologies for specific scenarios Nathan Lambert (2023). The international ASR community has recognized the need for standardized evaluation methodologies to ensure consistent and comparative performance assessments in ASR Aks\u00ebnova et al. (2021); Szyma\u0144ski et al. (2020); Gandhi et al. (2022) and the ML field in general Liao et al. (2021); Olson et al. (2017); Northcutt et al. (2021). This calls for innovations in the management of ASR data sets and evaluation frameworks.Koo et al. (2023)"}, {"title": "1.2 Research gap", "content": "Existing data curation and ASR benchmarking methods for low-resource languages such as Polish exhibit several shortcomings:\n\u2022 Data utilization: Speech datasets are underutilized due to limited awareness or accessibility.\n\u2022 Data quality: A lack of proper understanding of test sets can result in misrepresentation of current state-of-the-art performance.\n\u2022 Evaluation reproducibility: Limited adoption of benchmark sets hinders the validation of the research results.\n\u2022 Evaluation scope: Ecologically valid evaluation of a specific ASR application requires considering a larger number of datasets, systems, and performance metrics."}, {"title": "1.3 Contributions", "content": "1. Curation of benchmark dataset: A benchmark dataset was created from 24 openly available datasets to address the lack of standardized evaluation resources for Polish ASR systems. It includes robust samples from various sources of read and spontaneous speech. The dataset is openly available and actively maintained to enable systematic and comprehensive analysis.\n2. Development of a benchmark framework: The framework supports various datasets, systems, and metrics, ensuring consistent ASR evaluation with standardized protocols.\n3. Evaluation of ASR systems: Using a curated dataset, 10 ASR systems and 25 models, both commercial and open-source, were compared. Significant variations across different systems, datasets, and speaker demographics were discovered.\n4. Open sharing of resources: All datasets, tools, and evaluation results have been made openly available to the research community. This promotes transparency, reproducibility, and collaboration, enabling other researchers to build upon the work, either by developing ASR systems for Polish based on evaluation results or applying the framework to other languages."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Framework overview", "content": "The devised framework for data curation and ASR benchmarking encompasses three main processes:\n1. ASR speech datasets survey\n2. Curation of ASR benchmark dataset\n3. Evaluation of ASR systems"}, {"title": "2.2 Survey of datasets", "content": "A keyword-based literature review Rowley & Slack (2004) was used to identify and document relevant datasets. The datasets were manually analyzed and annotated. The final methodology included:\n1. Conducting keyword searches in relevant sources\n2. Manually analyzing and annotating documentation\n3. Cross-checking multiple sources for consistency and accuracy\n4. Validating and analyzing downloadable datasets\n5. Analyzing metadata to derive insights on Polish ASR speech datasets\n6. Making the catalog and insights publicly available"}, {"title": "2.3 Dataset curation", "content": ""}, {"title": "2.3.1 Design considerations", "content": "A curated benchmark dataset for Polish ASR systems is intended to have the following features:\n\u2022 Task-appropriate: Relevant and practical for the intended ASR task.\n\u2022 Accessible: Available online under a license that allows the free use and creation of derivative works.\n\u2022 Discoverable: Easy to find and acquire (without time-consuming registration or other access barriers).\n\u2022 Diverse and challenging: Containing various examples to test the adaptability of the model, as well as complex cases to encourage community participation and minimize the risk of benchmark saturation.\n\u2022 Annotated: With metadata about speakers and recordings allowing nuanced analysis and interpretation of the results.\n\u2022 Optimally sized: Large enough to be representative, but manageable to download and explore.\n\u2022 Clean yet realistic: Free of major errors, but noisy enough to represent the complexity of the real world.\n\u2022 Well-documented: Provided with documentation that is understandable to users without technical skills.\n\u2022 Well-explained: Provided with evaluation baselines and how-to-use script examples."}, {"title": "2.3.2 Leveraging speech data catalog for sourcing open data sets", "content": "The Polish ASR speech dataset catalog Junczyk (2023) was used to select datasets for curation based on following criteria:"}, {"title": "2.3.3 Curation process", "content": "1. Dataset structure curation:\n\u2022 Downloading and manually inspecting format and contents\n\u2022 Creating train/dev/test splits if not available\n\u2022 Assigning standard IDs to speakers and files\n2. Audio file curation:\n\u2022 Removal of invalid audio files\n\u2022 Unifying audio format to WAV 16 bits/16 kHz\n\u2022 Normalizing audio amplitude to -3 dBFS\n\u2022 Splitting long audio files into shorter segments based on time-alignment annotations\n3. Text files (transcripts and metadata) curation:\n\u2022 Converting text encoding to UTF8\n\u2022 Extracting original transcription and removing redundant characters\n\u2022 Extracting and unifying metadata contents\n\u2022 Generating metadata from text and audio content\n\u2022 Saving in the standard tabular format\n4. Dataset distribution\n\u2022 Uploading to the HF dataset hub\n\u2022 Referencing the original license in the README file"}, {"title": "2.4 ASR evaluation", "content": ""}, {"title": "2.4.1 System design considerations", "content": "Established tools and platforms were used where possible."}, {"title": "2.4.2 Overview of the evaluation process", "content": "In total 25 models of 7 ASR systems were evaluated: Google STT, Azure STT, Whisper, AssemblyAI, NeMo, MMS and Wav2Vec. The complete list is presented in Table 17. Currently, 5 evaluation metrics are supported: SER, WER, MER, WIL, and CER Morris et al. (2004). The methods for normalizing references and hypotheses are listed in Appendix 19. Python scripts used for the evaluation are available on GitHub."}, {"title": "3 Evaluation results", "content": "The developed framework supports the following evaluation scenarios. The results of selected scenarios are analyzed in the subsequent sections. Additional results are available in Appendix B.9. All and more detailed results can be accessed through the public dashboard. Dashboard users can display the evaluation results for a specific scenario and choose between various datasets, systems, metrics, normalization techniques, and diagram types."}, {"title": "3.1 Impact of normalization on error rates", "content": "shows the specific and average reduction of error rates in percentage points depending on the applied normalization method."}, {"title": "3.2 Overall accuracy of available ASR systems and models", "content": "shows the WER box plot for the systems evaluated using the BIGOS dataset. The 3 best ASR models in terms of accuracy are Whisper Large V3, Whisper Cloud and Assembly AI best. The results of the evaluation using the PELCRA dataset are available in the Polish ASR leaderboard"}, {"title": "3.3 Comparison of accuracy of commercial and freely available ASR systems", "content": "compares the Word Error Rate (WER) of commercial and free ASR systems. Commercial systems achieve better median, mean and minimal error rates in the BIGOS and PELCRA datasets by approximately 2.5 p.p. and 3.5 p.p., respectively. Furthermore, commercial and free systems show better accuracy for read speech than conversational speech by approximately 17 and 18.5 \u0440.\u0440., respectively."}, {"title": "3.4 Accuracy as a function of model size", "content": "shows that as model size increases, WER decreases, indicating better performance. This trend holds for models of the same type, e.g., whisper models. There are noticeable accuracy differences in models of the same size trained on different data, such as MMS. Finally, Nemo models perform on par with much larger wav2vec2 models."}, {"title": "3.5 Accuracy as a function of speech rate", "content": "illustrates the correlation between WER and speech rate, which is measured as the mean number of words uttered per second."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Analysis of findings", "content": ""}, {"title": "4.1.1 Impact of normalization", "content": "Normalization techniques resulted in significant reductions in error rates for all types of metrics (SER, WER, MER, CER). Applying all methods reduced WER by 16.07 p.p. for the PELCRA dataset and 15.52 p.p. for the BIGOS dataset, highlighting the sensitivity of lexical metrics to spelling and formatting variations."}, {"title": "4.1.2 Determining the best systems among free and commercial", "content": "Conversational speech (PELCRA) has higher error rates due to its spontaneous nature, with more variability in style, speed, and pauses. Read speech (BIGOS) is more structured and consistent, resulting in lower WERs."}, {"title": "4.1.3 Impact of model size on accuracy", "content": "\u2022 whisper_large_v2, whisper_large, and whisper_large_v3 show the best performance with the lowest WERs and the largest model sizes.\n\u2022 whisper_tiny is the second smallest model and has the highest WER among all evaluated.\n\u2022 nemo_pl_quartznet and nemo_pl_multilang are relatively small models with reasonably low WERs, indicating that they are efficient given their size."}, {"title": "4.1.4 Impact of speech rate on accuracy (WER)", "content": "\u2022 Both whisper_large_v3 and whisper_cloud perform similarly across speech rates. For rates between 1.5 and 5, most WERs are below 30%. Severe errors occur at lower rates,"}, {"title": "4.2 Implications", "content": "The developed data curation and evaluation framework offers the following benefits for the research community:\n\u2022 Establishes a consistent framework for evaluating Polish ASR systems, enhancing reproducibility.\n\u2022 Facilitates better use of datasets, promoting focused research.\n\u2022 Encourages data sharing and collaboration, improving resources and progress.\n\u2022 Identifies gaps, such as the need for detailed metadata and semantic metrics, guiding future studies.\nAdvantages for industry include:\n\u2022 Informs public about strengths and weaknesses of available ASR system.\n\u2022 Proposes a standard evaluation procedure to increase evaluation efficiency.\n\u2022 Showcases the importance of normalization and utilization of metadata for analysis.\n\u2022 Provides incentive to companies to showcase superior performance on a public benchmark for marketing purposes."}, {"title": "4.3 Limitations and challenges", "content": "Future research should include manual transcriptions and annotations to assess the quality of test data Koo et al. (2024). Investigating manual annotation of recognition errors to determine the criticality of the error Wirth & Peinl (2022), and automating the classification and correction of erroneous references are other directions to explore. Integrating semantically informed metrics could provide additional insight into accuracyStokke (2023); Roy (2021). Robustness and bias measurements could be improved by augmenting existing or collecting new recordings representing various usage conditions and Polish speakers demographics.Aks\u00ebnova et al. (2021, 2022)"}, {"title": "5 Conclusion", "content": "The research establishes a framework for evaluating ASR systems. It addresses the issue of limited dataset usage for Polish benchmarking by offering a curated benchmark set derived from 24 publicly available datasets identified in an extensive survey. The evaluation of 7 ASR systems and 25 models revealed notable performance differences between service types, model sizes, and speech types. The study also highlighted potential problems with the test set content that require further examination. This work improves reproducibility and directs future ASR advancements by providing public access to data catalogs, curated datasets, evaluation tools, and dashboards with benchmarking results."}, {"title": "7 Appendices", "content": "Provide additional data, tools' documentation, and other supplementary materials that are relevant but not central to the article's narrative."}, {"title": "Checklist", "content": "1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] Abstract and introduction explicitely describes contributions: Survey of datasets, metholodogy thereof, curated evaluation datasets process and outcomes, system for ASR evaluation, interactive dashboard with benchmark results.\n(b) Did you describe the limitations of your work? [Yes] Limitations include limited representation of Polish speakers, lack of manual transcription verification and unification, limited scope of transcription normalization, lack of support for embedding based metrics, lack of manual analysis of ASR errors, limited availability of recordings with speaker metadata.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] In the limitations section, it is mentioned that the evaluation datasets do not encompass all Polish users or the various conditions under which ASR systems are used. However, the results presented can guide the selection of the best-performing ASR systems for use-cases similar to those in the BIGOS evaluation dataset. For new and particularly high-risk scenarios, such as the medical field or specific demographic group, an independent evaluation on a representative dataset is necessary to accurately assess performance and ensure safe, unbiased operation.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] Presented work follows the ethical guidelines. No PII or protected information about individuals is revealed. The author obtained consent to use datasets for evaluation dataset curation and evaluation, either directly or based on licensing terms. Research did not include experiments involving human subjects.\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Code, data and instructions how to reproduce results are available on respective publicly available repositories on Hugging Face and GitHub platforms.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A]\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [N/A]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] Yes, all authors of existings assets were cited both in submitted article and repositories with curated assets.\n(b) Did you mention the license of the assets? [Yes] Yes, license types are mentioned in the respective tables describing source datasets, as well as on repositories hosting curated assets.\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] Yes, links to meta-corpora resulting from curation of existing assests were provided.\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Yes, the consent from the author of PELCRA corpora to curate dataset for open competition and benchmarking purposes is mentioned.\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Yes, the lack of PII is mentioned, however the inspection if datasets contain potentially offensive content was not performed.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]"}, {"title": "A Additional information required by organizers", "content": "In the Appendix, we provide additional information. This section will often be part of the supplemental material. Please see the call on the NeurIPS website for links to additional guides on dataset publication.\nSubmission introducing new datasets must include the following in the supplementary materials:\n1. Dataset documentation and intended uses. Recommended documentation frameworks include datasheets for datasets, dataset nutrition labels, data statements for NLP, and accountability frameworks.\n2. URL to website/platform where the dataset/benchmark can be viewed and downloaded by the reviewers.\n3. URL to Croissant metadata record documenting the dataset/benchmark available for viewing and downloading by the reviewers. You can create your Croissant metadata using e.g. the Python library available here: https://github.com/mlcommons/croissant\n4. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license.\n5. Hosting, licensing, and maintenance plan. The choice of hosting platform is yours, as long as you ensure access to the data (possibly through a curated interface) and will provide the necessary maintenance."}, {"title": "B Additional information relevant to submitted article", "content": ""}, {"title": "B.1 Dataset splits details", "content": "Tables 5 and 6 present logic of data splits applied during curation for BIGOS and PELCRA datasets, respectively."}, {"title": "B.2 Dataset splits details", "content": "Table 7 presents metadata fields associated with each individual data item in BIGOS datasets."}, {"title": "B.3 Dataset contents details", "content": "Tables 8 and 9 present information on licensing and language coverage for BIGOS and PELCRA datasets, respectively."}, {"title": "B.4 Dataset contents details", "content": "Tables 10 and 11 present information on domains, speech, and interaction types for BIGOS and PELCRA datasets, respectively."}, {"title": "B.5 Dataset contents details", "content": "Tables 12 and 13 present information on sources, acoustic environments and audio recording devices for BIGOS and PELCRA datasets, respectively."}, {"title": "B.6 Audio content size metrics", "content": "Tables 14 and 15 present information about number of available transcribed speech material, audio files and recorded speakers for BIGOS and PELCRA datasets, respectively."}, {"title": "B.7 Evaluated ASR system details", "content": "\u2022 Google Cloud Speech-to-Text supports more than 125 languages and variants. Google's service offers several useful features, such as noise cancelation, support for streaming, automatic punctuation, and the capability to recognize specific phrases or words when provided with context (e.g., specialized vocabulary or formats for spoken numbers, addresses, years, currencies, etc.). For selected languages, it also provides domain-specific models, multichannel audio support, and filtering of profanity content. Two generations of service are available: v1 and v2. For Polish, multiple model variants are available and were evaluated: v1_default, v1_latest_long, v1_latest_short, v1_command_and_search, v2_long and v2_short.\n\u2022 Microsoft's Azure Speech Service as of May 2023 supports more than 100 languages and variants. In addition to standard transcription, the Azure Speech Service supports continuous real-time speech recognition and provides robust noise reduction capabilities. It allows users"}, {"title": "B.8 Normalization methods", "content": "Table 19 contains overview of scope of normalization of each available method."}, {"title": "B.9 Evaluation results", "content": "Accuracy per speaker genders Figure 6 shows the difference in WER for the speaker groups of different gender. Positive values indicate bias toward male speakers, while negative values indicate bias toward female speakers. Values close to zero indicate lack of bias.\nAccuracy per speaker age groups Table 20 shows the mean WER for age groups in the PELCRA dataset. Figure 7 shows the standard deviation of WER in all age groups. Lower values indicate a more consistent accuracy for all groups."}]}