{"title": "Framework for Curating Speech Datasets and Evaluating ASR Systems: A Case Study for Polish", "authors": ["Micha\u0142 Junczyk"], "abstract": "Speech datasets available in the public domain are often underutilized because of challenges in discoverability and interoperability. A comprehensive framework has been designed to survey, catalog, and curate available speech datasets, which allows replicable evaluation of automatic speech recognition (ASR) systems. A case study focused on the Polish language was conducted; the framework was applied to curate more than 24 datasets and evaluate 25 combinations of ASR systems and models. This research constitutes the most extensive comparison to date of both commercial and free ASR systems for the Polish language. It draws insights from 600 system-model-test set evaluations, marking a significant advancement in both scale and comprehensiveness. The results of surveys and performance comparisons are available as interactive dashboards, along with curated datasets and the open challenge call. Tools used for evaluation are open-sourced, facilitating replication and adaptation for other languages, as well as continuous expansion with new datasets and systems.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 Background", "content": "The Polish language is spoken by more than 50 million people worldwide. The number of available ASR systems and services, as well as speech data resources that support Polish, is systematically growing. However, the community lacks the resources to methodically evaluate and track progress. First, the available data assets are underutilized due to challenges such as discoverability, licensing, and interoperability. Secondly, there is no standardized ASR benchmark dataset for Poland. These issues hinder the development of new systems and applications, as reliable benchmarks and leaderboards are crucial to drive research progress and assess the suitability of ASR technologies for specific scenarios Nathan Lambert (2023). The international ASR community has recognized the need for standardized evaluation methodologies to ensure consistent and comparative performance assessments in ASR Aks\u00ebnova et al. (2021); Szyma\u0144ski et al. (2020); Gandhi et al. (2022) and the ML field in general Liao et al. (2021); Olson et al. (2017); Northcutt et al. (2021). This calls for innovations in the management of ASR data sets and evaluation frameworks.Koo et al. (2023)"}, {"title": "1.2 Research gap", "content": "Existing data curation and ASR benchmarking methods for low-resource languages such as Polish exhibit several shortcomings:\n\u2022 Data utilization: Speech datasets are underutilized due to limited awareness or accessibility.\n\u2022 Data quality: A lack of proper understanding of test sets can result in misrepresentation of current state-of-the-art performance.\n\u2022 Evaluation reproducibility: Limited adoption of benchmark sets hinders the validation of the research results.\n\u2022 Evaluation scope: Ecologically valid evaluation of a specific ASR application requires considering a larger number of datasets, systems, and performance metrics."}, {"title": "1.3 Contributions", "content": "1. Curation of benchmark dataset: A benchmark dataset was created from 24 openly available datasets to address the lack of standardized evaluation resources for Polish ASR systems. It includes robust samples from various sources of read and spontaneous speech. The dataset is openly available and actively maintained to enable systematic and comprehensive analysis.\n2. Development of a benchmark framework: The framework supports various datasets, systems, and metrics, ensuring consistent ASR evaluation with standardized protocols.\n3. Evaluation of ASR systems: Using a curated dataset, 10 ASR systems and 25 models, both commercial and open-source, were compared. Significant variations across different systems, datasets, and speaker demographics were discovered.\n4. Open sharing of resources: All datasets, tools, and evaluation results have been made openly available to the research community. This promotes transparency, reproducibility, and collaboration, enabling other researchers to build upon the work, either by developing ASR systems for Polish based on evaluation results or applying the framework to other languages."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Framework overview", "content": "The devised framework for data curation and ASR benchmarking encompasses three main processes:\n1. ASR speech datasets survey\n2. Curation of ASR benchmark dataset\n3. Evaluation of ASR systems\nFigure 1 illustrates the framework architecture and the core open tools used for development. The subsequent sections provide a detailed description of the specific processes and tools."}, {"title": "2.2 Survey of datasets", "content": "A keyword-based literature review Rowley & Slack (2004) was used to identify and document relevant datasets. The datasets were manually analyzed and annotated. The final methodology included:\n1. Conducting keyword searches in relevant sources\n2. Manually analyzing and annotating documentation\n3. Cross-checking multiple sources for consistency and accuracy\n4. Validating and analyzing downloadable datasets\n5. Analyzing metadata to derive insights on Polish ASR speech datasets\n6. Making the catalog and insights publicly available"}, {"title": "2.3 Dataset curation", "content": null}, {"title": "2.3.1 Design considerations", "content": "A curated benchmark dataset for Polish ASR systems is intended to have the following features:\n\u2022 Task-appropriate: Relevant and practical for the intended ASR task.\n\u2022 Accessible: Available online under a license that allows the free use and creation of derivative works.\n\u2022 Discoverable: Easy to find and acquire (without time-consuming registration or other access barriers).\n\u2022 Diverse and challenging: Containing various examples to test the adaptability of the model, as well as complex cases to encourage community participation and minimize the risk of benchmark saturation.\n\u2022 Annotated: With metadata about speakers and recordings allowing nuanced analysis and interpretation of the results.\n\u2022 Optimally sized: Large enough to be representative, but manageable to download and explore.\n\u2022 Clean yet realistic: Free of major errors, but noisy enough to represent the complexity of the real world.\n\u2022 Well-documented: Provided with documentation that is understandable to users without technical skills.\n\u2022 Well-explained: Provided with evaluation baselines and how-to-use script examples."}, {"title": "2.3.2 Leveraging speech data catalog for sourcing open data sets", "content": "The Polish ASR speech dataset catalog Junczyk (2023) was used to select datasets for curation based on following criteria:\n\u2022 Datasets are available online under a license allowing free use for non-commercial purposes.\n\u2022 Transcriptions are aligned with the recordings.\n\u2022 Recording sampling rate is at least 8 kHz.\n\u2022 Audio files are encoded using at least 16 bits per sample.\n24 datasets were selected for curation as BIGOS\u00ba (Benchmark Intended Grouping of Open Speech) benchmark dataset:\n\u2022 The Common Voice data set (mozilla-common_voice_15-23) is a multilingual resource Ardila et al. (2019a) covering over 60 languages and many underrepresented groups. Available under CC-0 license.\n\u2022 The Multilingual LibriSpeech (MLS) data set (fair-mls-20) is a large multilingual corpus made by Facebook AI Research (FAIR) Pratap et al. (2020). Derived from audiobooks, it covers eight languages, with 44,000 hours of English and 6,000 hours for other languages. The Polish data includes 137 hours from 25 books by 16 speakers. Available under CC-BY license.\n\u2022 The Clarin Studio data set (clarin-pjatk-studio-15) by CLARIN-PL includes 13,802 short utterances (56 hours) from 554 sessions by 317 speakers. Each session has 20-31 audio files, all recorded in a studio for clear audio. Available under CC-BY-SA license.\n\u2022 The Clarin Mobile data set (clarin-pjatk-mobile-15) is a Polish speech corpus of read speech recorded on a telephone. It includes many speakers reading several dozen sentences and words with rare phonemes. Available under CC-BY-SA license.\n\u2022 The Jerzy Sas PWR data sets (Politechnika Wroc\u0142awska) comprise three legacy sets of recordings available in the public domain:\n\u2013 Male speaker speech set (pwr-maleset-unk) \u2013 single male speaker recordings.\n\u2013 Utterances containing short words (pwr-shortwords-unk) \u2013 single-phoneme conjunctions and prepositions likely to be misrecognized.\n\u2013 Spoken commands as very important utterances (VIUs) (pwr-viu-unk) \u2013 editor control commands and domain-specific utterances.\n\u2022 The M-AI Labs Speech corpus (mailabs-19) created from audiobooks as MLS. Intended for training speech recognition and synthesis systems in nine languages, with nearly a thousand hours of audio, including 53.5 hours for Polish. Available under proprietary license.\n\u2022 The AZON Read and Spontaneous Speech data sets (pwr-azon_spont-20, pwr-azon_read-20) contain recordings from academic staff in the physical chemistry domain, including both supervised readings and unsupervised spontaneous recordings such as interviews and presentations. Available under a CC-BY-SA license.\n\u2022 Google FLEURS (google-fleurs-22) is a parallel speech benchmark data set in 102 languages, based on the FLoRes-101 machine translation benchmark Conneau et al. (2022). Hosted on Hugging Face and available under a CC-BY license.\n\u2022 PolyAI Minds14 (polyai-minds14-21) is a dataset for training and evaluating intent recognition systems using spoken data. Covers spoken samples in the commercial e-banking domain in 14 language variations Gerz et al. (2021). Hosted on Hugging Face\u00b9\u00b9 and available under a CC-BY license.\n\u2022 PolEval 22 Diabiz sample (ul-diabiz_poleval-22) was used for a punctuation restoration task in the 2022 PolEval competition. It is a subset of the DiaBiz homepage\u00b9\u00b2 dialog corpus of phone-based customer-agent interactions by the PELCRA group of the University of \u0141\u00f3d\u017a. Available publicly under CC-BY-SA-NC-ND and curated with the consent of the author.\n\u2022 SpokesMix13 is a corpus of conversational Polish by the PELCRA group Pezik (2018). It includes speech recordings and word-by-word transcriptions with non-speech events. Available under the CC-BY-NC-ND license and curated for ASR benchmarking purposes with permission of the author.\n\u2022 SpokesBiz14 is a corpus of conversational Polish from the CLARIN-BIZ project, featuring over 650 hours of recordings from nearly 600 speakers P\u0119zik et al. (2023). Transcriptions are diarized and manually annotated. Includes eight diverse subsets, e.g. biographical interviews, job interviews, podcasts, and student presentations. Available under the CC-BY-NC-ND license and curated for ASR benchmarking purposes with the author's permission."}, {"title": "2.3.3 Curation process", "content": "1. Dataset structure curation:\n\u2022 Downloading and manually inspecting format and contents\n\u2022 Creating train/dev/test splits if not available\n\u2022 Assigning standard IDs to speakers and files\n2. Audio file curation:\n\u2022 Removal of invalid audio files\n\u2022 Unifying audio format to WAV 16 bits/16 kHz\n\u2022 Normalizing audio amplitude to -3 dBFS\n\u2022 Splitting long audio files into shorter segments based on time-alignment annotations\n3. Text files (transcripts and metadata) curation:\n\u2022 Converting text encoding to UTF8\n\u2022 Extracting original transcription and removing redundant characters\n\u2022 Extracting and unifying metadata contents\n\u2022 Generating metadata from text and audio content\n\u2022 Saving in the standard tabular format\n4. Dataset distribution\n\u2022 Uploading to the HF dataset hub\n\u2022 Referencing the original license in the README file\nThe resulting BIGOS utterance data object with a description of the standard metadata fields is available in Table 7 in the Appendix."}, {"title": "2.4 ASR evaluation", "content": null}, {"title": "2.4.1 System design considerations", "content": "Established tools and platforms were used where possible. Table 1 provides an overview of the main design considerations."}, {"title": "2.4.2 Overview of the evaluation process", "content": "In total 25 models of 7 ASR systems were evaluated: Google STT, Azure STT, Whisper, AssemblyAI, NeMo, MMS and Wav2Vec. The complete list is presented in Table 17. Currently, 5 evaluation metrics are supported: SER, WER, MER, WIL, and CER Morris et al. (2004). The methods for normalizing references and hypotheses are listed in Appendix 19. Python scripts used for the evaluation are available on GitHub."}, {"title": "3 Evaluation results", "content": "The developed framework supports the following evaluation scenarios. The results of selected scenarios are analyzed in the subsequent sections. Additional results are available in Appendix B.9. All and more detailed results can be accessed through the public dashboard. Dashboard users can display the evaluation results for a specific scenario and choose between various datasets, systems, metrics, normalization techniques, and diagram types."}, {"title": "3.1 Impact of normalization on error rates", "content": "Table 3 shows the specific and average reduction of error rates in percentage points depending on the applied normalization method."}, {"title": "3.2 Overall accuracy of available ASR systems and models", "content": "Figure 3 shows the WER box plot for the systems evaluated using the BIGOS dataset. The 3 best ASR models in terms of accuracy are Whisper Large V3, Whisper Cloud and Assembly AI best. The results of the evaluation using the PELCRA dataset are available in the Polish ASR leaderboard"}, {"title": "3.3 Comparison of accuracy of commercial and freely available ASR systems", "content": "Table 4 compares the Word Error Rate (WER) of commercial and free ASR systems. Commercial systems achieve better median, mean and minimal error rates in the BIGOS and PELCRA datasets by approximately 2.5 p.p. and 3.5 p.p., respectively. Furthermore, commercial and free systems show better accuracy for read speech than conversational speech by approximately 17 and 18.5 \u0440.\u0440., respectively."}, {"title": "3.4 Accuracy as a function of model size", "content": "Figure 4a shows that as model size increases, WER decreases, indicating better performance. This trend holds for models of the same type, e.g., whisper models. There are noticeable accuracy differences in models of the same size trained on different data, such as MMS. Finally, Nemo models perform on par with much larger wav2vec2 models."}, {"title": "3.5 Accuracy as a function of speech rate", "content": "Figure 4b illustrates the correlation between WER and speech rate, which is measured as the mean number of words uttered per second."}, {"title": "4 Discussion", "content": null}, {"title": "4.1 Analysis of findings", "content": null}, {"title": "4.1.1 Impact of normalization", "content": "Normalization techniques resulted in significant reductions in error rates for all types of metrics (SER, WER, MER, CER). Applying all methods reduced WER by 16.07 p.p. for the PELCRA dataset and 15.52 p.p. for the BIGOS dataset, highlighting the sensitivity of lexical metrics to spelling and formatting variations."}, {"title": "4.1.2 Determining the best systems among free and commercial", "content": "Conversational speech (PELCRA) has higher error rates due to its spontaneous nature, with more variability in style, speed, and pauses. Read speech (BIGOS) is more structured and consistent, resulting in lower WERS."}, {"title": "4.1.3 Impact of model size on accuracy", "content": "\u2022 whisper_large_v2, whisper_large, and whisper_large_v3 show the best performance with the lowest WERs and the largest model sizes.\n\u2022 whisper_tiny is the second smallest model and has the highest WER among all evaluated.\n\u2022 nemo_pl_quartznet and nemo_pl_multilang are relatively small models with reasonably low WERs, indicating that they are efficient given their size."}, {"title": "4.1.4 Impact of speech rate on accuracy (WER)", "content": "\u2022 Both whisper_large_v3 and whisper_cloud perform similarly across speech rates. For rates between 1.5 and 5, most WERs are below 30%. Severe errors occur at lower rates,"}, {"title": "4.2 Implications", "content": "The developed data curation and evaluation framework offers the following benefits for the research community:\n\u2022 Establishes a consistent framework for evaluating Polish ASR systems, enhancing reproducibility.\n\u2022 Facilitates better use of datasets, promoting focused research.\n\u2022 Encourages data sharing and collaboration, improving resources and progress.\n\u2022 Identifies gaps, such as the need for detailed metadata and semantic metrics, guiding future studies.\nAdvantages for industry include:\n\u2022 Informs public about strengths and weaknesses of available ASR system.\n\u2022 Proposes a standard evaluation procedure to increase evaluation efficiency.\n\u2022 Showcases the importance of normalization and utilization of metadata for analysis.\n\u2022 Provides incentive to companies to showcase superior performance on a public benchmark for marketing purposes."}, {"title": "4.3 Limitations and challenges", "content": "Future research should include manual transcriptions and annotations to assess the quality of test data Koo et al. (2024). Investigating manual annotation of recognition errors to determine the criticality of the error Wirth & Peinl (2022), and automating the classification and correction of erroneous references are other directions to explore. Integrating semantically informed metrics could provide additional insight into accuracyStokke (2023); Roy (2021). Robustness and bias measurements could be improved by augmenting existing or collecting new recordings representing various usage conditions and Polish speakers demographics.Aks\u00ebnova et al. (2021, 2022)"}, {"title": "5 Conclusion", "content": "The research establishes a framework for evaluating ASR systems. It addresses the issue of limited dataset usage for Polish benchmarking by offering a curated benchmark set derived from 24 publicly available datasets identified in an extensive survey. The evaluation of 7 ASR systems and 25 models revealed notable performance differences between service types, model sizes, and speech types. The study also highlighted potential problems with the test set content that require further examination. This work improves reproducibility and directs future ASR advancements by providing public access to data catalogs, curated datasets, evaluation tools, and dashboards with benchmarking results."}, {"title": "7 Appendices", "content": "Provide additional data, tools' documentation, and other supplementary materials that are relevant but not central to the article's narrative."}]}