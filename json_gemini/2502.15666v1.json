{"title": "Almost AI, Almost Human:\nThe Challenge of Detecting AI-Polished Writing", "authors": ["Shoumik Saha", "Soheil Feizi"], "abstract": "The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Misclassification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate eleven state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains 11.7K samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently misclassify even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.", "sections": [{"title": "Introduction", "content": "The rapid advancement of LLMs has enabled AI to generate highly fluent, human-like text, raising concerns about detectability and prompting the development of various AI-text detectors (Gehrmann et al., 2019; Mitchell et al., 2023; Hu et al., 2023). However, the distinction between AI-generated and human-written text remains a gray area, particularly when human-authored content is refined using AI tools. If a human-written text is slightly polished by AI, should it still be classified as human-written, or does it become AI-generated? Misclassifying such text can lead to false plagiarism accusations and unfair penalties, especially when detectors flag minimally polished content as AI-generated. Additionally, reports claiming that a large percentage of online content is AI-generated -such as assertions that \"40% of Medium articles are AI-written\" - often fail to account for AI-polished text. These sweeping claims risk misrepresenting the actual extent of AI involvement, leading to misleading statistics and misplaced skepticism about human authorship. Motivated by these issues, our study systematically examines how AI-text detectors respond to AI-polished text and whether their classifications are both accurate and fair.\nTo investigate this issue, we introduce the AI-Polished-Text Evaluation (APT-Eval) dataset of size 11.7K, which systematically examines how AI-text detectors respond to varying degrees of AI involvement in human writing. Our dataset is built from pre-existing human-written samples that are refined using different LLMs, such as GPT-40 (OpenAI, 2023), Llama3-70B (Dubey et al., 2024), etc., applying degree and percentage based modifications. This allows us to assess how detectors respond to minor and major AI polishing. We analyze the classification accuracy, false positive rates, and domain-specific sensitivities of 11 state-of-the-art detectors, spanning model-based, metric-based, and commercial systems.\nOur findings reveal critical weaknesses in existing AI-text detection systems. AI-text detectors exhibit alarmingly high false positive rates, often flagging very minimally polished text as AI-generated. Furthermore, detectors struggle to differentiate between minor and major AI refinements, suggesting that they may not be as reliable as previously assumed. We also uncover biases against smaller or older LLMs, where polishing done by less advanced models is more likely to be flagged than text refined by state-of-the-art LLMs. Furthermore, our study highlights inconsistencies in detection accuracy across different text domains, raising questions about bias and reliability. By"}, {"title": "APT(AI-Polished-Text) Eval Dataset", "content": "In this study, we begin with purely human-written texts (HWT) and refine them using various large language models (LLMs). Building on the work of Zhang et al. (2024), we utilize HWT samples from their 'MixSet' dataset. These samples are carefully selected based on two key criteria: (1) they were created prior to the widespread adoption of LLMs, and (2) they span six distinct domains. For clarity, we refer to this baseline HWT dataset as the 'No-Polish-HWT' set. This set comprises 300 samples, with 50 samples per domain"}, {"title": "Dataset Preparation", "content": "As we generate the AI-polished versions of our No-Polish-HWT samples, we adjust the level of AI/LLM involvement. We employ two distinct polishing strategies:\n1.  Degree-Based Polishing: The LLM is prompted to refine the text in four varying degrees of modification: \u2013 (1) extremely-minor, (2) minor, (3) slightly-major, and (4) major.\n2.  Percentage-Based Polishing: The LLM is instructed to modify a fixed percentage (p%) of words in a given text. The percentage is systematically varied across the following values:\np% = {1, 5, 10, 20, 35, 50, 75}%\nAs a result, each HWT sample is transformed into 11 distinct AI-polished variants. For the LLM-polishing, we employ four different models: GPT-40, Llama3.1-70B, Llama3-8B, and Llama2-7B. Each model is carefully prompted to generate the highest-quality output, preserving the original semantics of the text (details provided in Appendix A.2)."}, {"title": "Dataset Analysis", "content": "To assess the differences and deviations between pure HWT and AI-polished text, we employ three key metrics: Cosine semantic similarity, Jaccard distance, and Levenshtein distance. For semantic similarity, we compute the cosine similarity between the embeddings of the original and AI-polished texts (APT) using the BERT-base model. To ensure that the polished samples retain a strong resemblance to the original text, we filter out any samples with a semantic similarity below 0.85."}, {"title": "AI-text Detectors", "content": "In this work, we evaluate a total of ten detectors from three different categories:\n1.  Model-based: RADAR (Hu et al., 2023), ROBERTa-Base (ChatGPT) (Guo et al., 2023), ROBERTa-Base (GPT2), and RoBERTa-Large (GPT2) (OpenAI, 2019).\n2.  Metric-based: GLTR (Gehrmann et al., 2019), DetectGPT (Mitchell et al., 2023), Fast-DetectGPT (Bao et al., 2023), LLMDet (Wu et al., 2023), Binoculars (Hans et al., 2024).\n3.  Commercial: ZeroGPT, GPTZero."}, {"title": "Detectors' Threshold", "content": "AI-text detectors generate a scalar score or prediction based on a given sequence of input tokens. To transform this score into a binary classification, an appropriate threshold must be determined. Dugan et al. (2024) highlight that a naive threshold selection can lead to poor accuracy or a high false positive rate (FPR). Therefore, we optimize the threshold for each detector to achieve maximum accuracy in detecting HWT and AI-text.\nWe evaluate these detectors on 300 samples of our 'no-polish-HWT' (pure human-written) set and 300 samples of pure AI-generated texts from the dataset of Zhang et al. (2024). Most detectors achieve 70% - 88% accuracy, with a false positive rate of 1%-8%. Table 6 shows the detector-specific threshold with their accuracy and FPR."}, {"title": "Key Findings", "content": "We evaluate the detectors on our APT-Eval dataset from multiple perspectives to analyze their response to AI-polished text. Our key findings are as follows"}, {"title": "Alarming false positive rate by AI-text detectors for minor polishing.", "content": "Though most detectors can achieve a low false positive rate on pure HWT (Table 6), most of them give a high false positive rate for any polishing, especially for extremely minor and minor polishing. For example, GLTR, with a 6.83% FPR on non-polished text, misclassifies 40.87% of extremely minor and 42.81% of minor-polished GPT-40 texts. This trend extends to percentage-based polishing, where GLTR misclassifies 26.85% of texts with just 1% AI edits. The issue persists across LLM polishers, with misclassification rates of: 39.19% (Llama3.1-70B), 44.86% (Llama3-8B), and 52.31% (Llama2-7B) for extremely minor polishing."}, {"title": "Most AI-text detectors fail to distinguish between minor and major polishing.", "content": "Most detectors not only misclassify a large portion of minor-polished texts but also struggle to differentiate between degrees of AI-driven polishing. For example, RoBERTa-large classifies 47.69% of minor-polished texts as AI-generated, yet its rate for major-polished texts is only slightly higher at 51.98%.\nSurprisingly, some detectors misclassify fewer major-polished texts than extremely minor ones, revealing a lack of sensitivity to modification extent. As shown in Figure 3, detectors like DetectGPT, FastDetectGPT, GLTR, ROBERTa-base, RoBERTa-large, and LLMDet follow this trend. FastDetect-GPT, for instance, detects 10.07% of texts with 1% AI edits as AI, but only 9.59% for 75% polishing (Figure 13). This trend highlights a fundamental limitation: these detectors are largely incapable of accurately distinguishing different degrees of LLM-driven text refinement."}, {"title": "Most detectors penalize more if the polisher LLM is older or smaller.", "content": "We analyze whether AI-text detectors exhibit biases across different LLMs and find a higher misclassification rate for smaller and older models. For extremely minor polishing, Llama-2 has a 45% misclassification rate, while GPT-40 and Llama-3 models range from 27% to 32%. The probable reason can be \u2013 with time, newer LLMs have become increasingly adept at generating human-like text, making detection more challenging over time. However, such an imbalance can create unfair scenarios, where a student using Llama-2 is flagged for minor polishing while another using Llama-3 is found innocent."}, {"title": "Some domains are more sensitive than others.", "content": "Since our HWT dataset spans six domains, we analyze detector misclassification rates across them. Some domains are misclassified more than others 'speech' has the highest rate (36% - 56% for extreme-minor polishing), while \u2018paper_abstract' has the lowest (17% \u2013 31%). Interestingly, classification rates do not always correlate with polishing levels. As shown in figure 15, for larger models like GPT-40 and Llama3.1-70B, the AI-detection rate for 'paper_abstract' decreases as polishing increases \u2013 likely because with more freedom in polishing, larger models tend to generate more-human-like texts."}, {"title": "Related Work", "content": "Detecting AI-generated text is crucial as models become more human-like. Traditional methods use statistical metrics like perplexity and n-gram frequency (Gehrmann et al., 2019; Wu et al., 2023; Hans et al., 2024), while others rely on machine learning classifiers like BERT and ROBERTa (Hu et al., 2023; Guo et al., 2023; Solaiman et al., 2019). However, these approaches mainly differentiate pure AI and human-written text.\nPrior studies explored paraphrasing to evade AI detectors (Sadasivan et al., 2023; Krishna et al., 2023), but not AI-polished text. Others focused on AI-human text boundaries (Dugan et al., 2023; Zeng et al., 2024), while recent work examined AI-assisted polishing without varying AI involvement (Gao et al., 2024; Yang et al., 2024). We extend this by systematically analyzing AI-polished text across multiple levels, assessing detection limitations."}, {"title": "Conclusion", "content": "Our study exposes key flaws in AI-text detectors when handling AI-polished text, showing high false positive rates and difficulty distinguishing minor from major AI refinements. Detectors also exhibit biases against older or smaller models, raising fairness concerns. We highlight the need for more nuanced detection methods and release our APT-Eval dataset to support further research."}, {"title": "Limitations", "content": "While our study provides valuable insights into the challenges of AI-text detection for AI-polished texts, several limitations should be acknowledged. First, our dataset, APT-Eval, is built using a specific set of LLMs (GPT-40, Llama3-70B, etc.), which may not fully represent the diversity of AI models available. Future research should explore a broader range of models to assess generalizability. Additionally, while our dataset spans six distinct domains of human-written text (HWT), incorporating more domains could provide a more comprehensive evaluation of AI-text detection across different writing contexts.\nSecond, our findings highlight biases in detection models, particularly against smaller or older LLMs, but further investigation is needed to understand the root causes of these biases. Moreover, while this study focuses on identifying limitations in current AI-text detection systems, the development of more nuanced, fine-grained detection frameworks remains an open challenge. Future work should explore adaptive AI-text detectors capable of distinguishing varying levels of AI involvement, ensuring both accuracy and fairness in AI-assisted writing evaluation."}]}