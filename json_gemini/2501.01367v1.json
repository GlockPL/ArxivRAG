{"title": "Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation", "authors": ["Nathaniel Dennler", "Stefanos Nikolaidis", "Maja Matari\u0107"], "abstract": "People have a variety of preferences for how robots behave. To understand and reason about these preferences, robots aim to learn a reward function that describes how aligned robot behaviors are with a user's preferences. Good representations of a robot's behavior can significantly reduce the time and effort required for a user to teach the robot their preferences. Specifying these representations-what \"features\" of the robot's behavior matter to users remains a difficult problem; Features learned from raw data lack semantic meaning and features learned from user data require users to engage in tedious labeling processes. Our key insight is that users tasked with customizing a robot are intrinsically motivated to produce labels through exploratory search; they explore behaviors that they find interesting and ignore behaviors that are irrelevant. To harness this novel data source of exploratory actions, we propose contrastive learning from exploratory actions (CLEA) to learn trajectory features that are aligned with features that users care about. We learned CLEA features from exploratory actions users performed in an open-ended signal design activity (N=25) with a Kuri robot, and evaluated CLEA features through a second user study with a different set of users (N=42). CLEA features outperformed self-supervised features when eliciting user preferences over four metrics: completeness, simplicity, minimality, and explainability.", "sections": [{"title": "I. INTRODUCTION", "content": "People have a variety of preferences for how robots should behave based on many contextual factors, but those contextual factors are often unknown to the designers of robotic systems before a robot is deployed. Consider a wheeled robot that helps users find misplaced items in their home. One user may be a long-time dog owner and thus interpret this interaction as similar to playing fetch. That user might expect the behavioral aspects of the robot to be dog-like. For example, the robot may move erratically as if following a scent, bark when it has found an item, and emote to portray happiness having completed its command. Another user, in contrast, may be more familiar with smart devices and expect the interaction to be purely functional. That user might instead expect the robot to move and scan the room methodically, chime when it finds an item, and immediately return the item to the requester.\nDeploying a robot that behaves in only one way cannot satisfy both of these users. Thus, users must be able to customize robot behaviors to align with their preferences. Several works view the problem of aligning the robot with the user's preferences as modeling the user's internal reward function, which can be addressed with inverse reinforcement learning [1, 2]. In this context, the reward function takes in numerical \"features\" of the robot's behavior, e.g., a score of how dog-like or machine-like the behavior is, and output a single value that corresponds to how good that behavior is for the user. How these features are defined heavily influences how effectively a robot can adapt to a specific user. Features can be learned directly from the robot behaviors through self-supervised techniques like autoencoders (AEs) and variational autoencoders (VAEs). While self-supervised methods result in features that are physically representative of the robot's behaviors, they may not align with the features people actually care about. The most effective way to learn user-aligned features is by leveraging user-generated data [3]. However, collecting such data typically requires a user to engage in a data-labelling process known as a proxy task before the user can engage in the actual task of customizing the robot [4\u20136].\nOur goal in this work is to learn features for robot behaviors that are aligned with user preferences, but do not require users to engage in unrelated proxy tasks. To accomplish this, we identify a new form of user-collected data that is generated during the robot customization process. We collected this data by recruiting users to customize behaviors for a Mayfield Kuri robot that helped them locate items around a room. Participants used the RoSiD interface [7] to design state-expressive signals, which allowed them to search through"}, {"title": "II. RELATED WORK", "content": "Eliciting User Preferences through Interaction. Users must be able to communicate their preferences to the robot so the robot can learn these preferences. Previous works in preference learning identified several interactions that allow users to specify their preferences for robot behaviors, including behavior comparisons [8\u201310], behavior rankings [11\u201313], binary rewards [14], corrections [15\u201318], natural language [19\u201322], facial expression [23, 24], and demonstrations [25\u201329]. Those interactions require different skills and provide varying levels of information on the user's true preferences [30\u201332]. Additionally, all of them assume that there is a numerical representation of robot behavior that encapsulates the features that users care about. Thus, having meaningful numerical features is necessary to allow users of different skill levels to teach robots in a variety of ways.\nLearning Representations for Eliciting Preferences. There are three popular approaches for representing features of robot behaviors: hand-crafted features, features learned from modeling robot behaviors, and features learned from user interactions. Hand-crafted features are based on an engineer's intuitions of what is meaningful for users [1, 15, 33, 34]. Such features can speed up the preference learning process because they are meaningful to users, but they can also be difficult to design and can lead to incomplete feature spaces that limit the range of preferences that can be captured [3].\nIn contrast, features learned from modeling the robot's behaviors require less engineering effort, but are not meaningful. Such techniques learn features with little human input through self-supervised learning [35\u201337] or weakly-supervised learning [38\u201340]. While these algorithms result in features that describe the underlying behaviors well and do not require extensive data collection from users, the resulting feature spaces are not semantically meaningful to users [4].\nLearning feature spaces from human input can result in feature spaces that are both complete and meaningful. A user may manually select features [41, 42], physically move compliant robots to demonstrate behaviors [43], provide demonstrations for use in multi-task learning [44, 45] and meta-learning frameworks [46, 47], or answer trajectory similarity queries [4]. These methods focus on developing proxy tasks to learn features that are aligned with user preferences [3], however these tasks require conscious effort from the user."}, {"title": "III. LEARNING FEATURES FROM EXPLORATORY ACTIONS", "content": "In this section, we formalize our approach that leverages exploratory actions to learn features of robot behaviors.\nWe consider robot behaviors as trajectories in a fully-observed deterministic dynamical system. We denote a behavior as $\u03be \\in \u039e$, which represents a series of states and actions: $\u03be = (s_0, a_0, s_1, a_1, \u2026, s_T, a_T)$. These states and actions are abstractly defined; they can be videos (behaviors in image-space), audio (behaviors in frequency-space), or movements (behaviors in joint-space). We assume that all behaviors $\u03be \\in \u039e$ accomplish the task without resulting in errors, allowing users to specify based on user preferences rather than the behavior's ability to achieve a goal [4, 26, 62]. While generating $\u039e$ is not the focus of this work, it can be completed through several techniques, such as collecting demonstrations [26], performing quality diversity optimization [63], and diversely combining motion primitives [64].\nWe model a user's preference as a reward function over robot behaviors that maps the space of behaviors to a real value: $R_H: \u039e \u2194 R$. The user's reward function is not directly observable, but can be inferred through interaction. Our goal is to learn a reward function from user interactions, $R_H$, that maximizes the likelihood of the user performing the observed interactions. Higher values of $R_H$ for a particular behavior implies that the behavior is more preferred by the user.\nBecause the state space of robot behaviors can be very large [65, 66], directly learning $R_H$ from state-action sequences is intractable. To make reward learning tractable, several works [1\u20133] assume that there exists a function \u03a6 that maps from the state-action space to a lower dimensional feature space-a real vector of dimension d: $\u03a6 : \u039e \u2194 R^d$. This assumption allows us to learn $R_H(\u03a6(\u03be))$ from fewer user interactions.\nTo learn a feature function, \u03a6, we leverage interaction data that we collected through the robot customization process. Users naturally engaged in exploratory search when they were presented with many robot behaviors they could choose from to customize the robot.\nWe formalize exploratory search as presenting a dataset of behaviors to the user: $D_i = {\u03be_0, \u03be_1,..., \u03be_\u03bd}$ where each $\u03be_i$ is sampled from the full database of behaviors \u039e. In our case, $\u03be_i$ is a video, a sound, or a head movement, but this definition extends to other behaviors such as robot gaits, or robot arm movements. This dataset can be generated using various methods, including keyword search [67], collaborative filtering [68], and faceted search [69]. Users can view brief summaries of each behavior in the dataset to determine if the behavior is relevant.\nWe mathematically model the user's internal reasoning process when making an exploratory action with the function $\u03c8 : D \u2192 {0, 1}$. If the user performs an exploratory action on a behavior $\u03be_j$ from the dataset $D_i$, then $\u03c8(\u03be_j) = 1$. If the user does not perform an exploratory action on a behavior $\u03be_k$ from the dataset $D_i$, then $\u03c8(\u03be_k) = 0$. We use this definition of an exploratory action to partition $D_i$ into two sets:\n$D^{ex}_i := {\u03be \\in D_i|\u03c8(\u03be) = 1}; D^{ig}_i := {\u03be \\in D_i|\u03c8(\u03be) = 0}$    (1)\nFor example, if a user is initially presented with $D_0 = {\u03be_A, \u03be_B, \u03be_C, \u03be_D}$, and they choose $\u03be_B$ and $\u03be_D$ to execute on the robot, the explored dataset is $D^{ex}_0 = {\u03be_B, \u03be_D}$ and the ignored dataset is $D^{ig}_0 = {\u03be_A, \u03be_C}$. In our data collection study, $D_i \u2248 100$ to allow users to meaningfully search through behaviors.\nA common way to model and aggregate diverse internal reasoning processes, such as $\u03c8$, across a population of users is to use a triplet loss [4, 58\u201361]. We adopt this loss function and generate triplets of behaviors from on the explored and ignored subsets. The triplets are formed by sampling two behaviors at"}, {"title": "IV. COLLECTING EXPLORATORY ACTIONS FROM A ROBOT CUSTOMIZATION SESSION", "content": "In this section, we describe our methodology for collecting user exploratory actions in a free-form customization session involving a Kuri robot performing an item-finding task.\nWe adapted a signal design task from previous work [7] to collect exploratory action data from users designing multi-"}, {"title": "V. USER STUDY EVALUATION", "content": "To evaluate the efficacy of learning feature spaces for robot behaviors, we conducted an experiment with a new set of participants. The participants ranked behaviors to generate individual datasets that we could use to quantitatively test different feature-learning algorithms.\nTo evaluate the effectiveness of learned features using automatically collected data, we evaluated seven total algo-rithms for learning feature spaces. The first baseline was: (1) Random, a randomly-initialized neural network that projects each behavior to a vector. Random networks can be effective feature learners, as they cannot overfit to data or learn spurious correlations. (2) Pretrained, a large pre-trained neural network"}, {"title": "VI. DISCUSSION AND LIMITATIONS", "content": "Our results demonstrate the efficacy of learning useful fea-tures for eliciting preferences by leveraging data from natural user interface interaction. By incorporating exploratory search concepts into interfaces for teaching robots, we can scale data collection while also providing users with an intrinsically motivating task. We performed evaluations across three modal-ities of state-expressive signals in robots\u2014visual, auditory, and kinetic and found that using CLEA significantly increased performance in all modalities and evaluation criteria.\nCLEA can be readily combined with any algorithm that learns feature spaces for eliciting preferred robot behaviors by learning a lower-dimensional embedding of the behaviors. We demonstrated CLEA's use with self-supervised losses; it can also be used with other methods of learning feature spaces such as trajectory similarity queries [4], multi-task learning [44, 45], and labelled behaviors [87, 88].\nWe developed an algorithm for learning feature spaces using exploratory search actions. This requires users to be able to briefly review many behaviors at a high level before deciding which of these behaviors might be relevant and warrant further exploration. We used visual summaries for all modalities-a video frame for the visual modality, a spectrogram for the auditory modality, and a graph of joint angles for the kinetic modality. These were interpretable by the participants, but they may not be the most effective means of representing the underlying behaviors. Users may be better at interpreting natural language descriptions [89], tags that describe the behavior [54], or animated gifs [90]. Future work can explore how robot behaviors can be summarized to non-expert users in ways that allow them to most efficiently search through robot behavior options. Understandable summaries are especially needed for users to perform exploratory search with more complex robot behaviors, such as different gaits for quadruped robots or dexterous manipulation skills for high degree of freedom manipulators.\nThe robot behaviors that users explored often appeared unrelated to the user's personal preferences, but these seem-ingly random explorations were still indicative of other users' preferences, as demonstrated by the transferability of CLEA to distinct populations. We also assumed that users would be motivated to perform this exploratory search since they were unfamiliar with what the robot was capable of [50]. If users are already familiar with a particular robot, they may not be motivated to perform exploratory actions because they have already found their preferred robot behaviors. This familiarity could decrease the efficacy of CLEA as a framework for learning feature spaces in already adopted robots.\nWe also found that additional loss terms, such as the reconstruction term and the KL-divergence term, were only sometimes helpful for training with CLEA, depending on the underlying data structures used. We found that the additional reconstruction loss and KL-Divergence loss were often helpful in video and sound data structures but could hinder prefer-ence learning for joint state sequences. This effect is due to the social interpretations interfering with these additional terms. For joint state behaviors, gestures portraying fear and excitement have very similar joint states, but vastly different social interpretations. CLEA aims to separate these features, while the reconstruction term brings them together. While the optimal loss terms can only be determined experimentally, we presented a set of metrics for evaluation that can systematically determine these terms, in accordance with the qualities of good feature spaces [3].\nConclusion. We present contrastive learning from exploratory actions (CLEA), an algorithm to leverage a novel data source of interactions that users automatically perform when teaching robot systems. We showed that CLEA can be used to learn feature spaces that reflect underlying personal preferences, represent robot behaviors in low-dimensional vec-tors, quickly elicit user preferences, and are explainable."}, {"title": "APPENDIX", "content": "In Appendices A and B, we report additional study details and demographic information. Appendix A discusses the cus-tomization session from Sec. IV, and Appendix B discusses the evaluation study from Sec. V-B.\nA. Robot Customization Session\nAdditional Demographic Information. A total of 25 par-ticipants were part of the study, with ages ranging from 19 to 43 (median 25); participants self-declared as men (13), women (10), and genderqueer, non-binary, or declined to state (3, aggregated for privacy; some participants belonged to multiple groups). We recruited 13 participants who self-identified as LGBTQ+. Participants were Asian (13), Black (2), Latino (5), and White (6); some participants belonged to multiple groups. All participants were able to create signals they liked for all four categories, and all successfully interacted with the robot to collect all the items in the word search task.\nAdditional Procedure Information. Participants were re-cruited from the local university student population through email, flyers, and word-of-mouth. The study took place in a conference room with a kitchen to reflect a realistic living environment. Participants entering the study were brought to a table in the middle of the room, with a clear view of a Kuri robot that was modified to have a screen and backpack (see Fig. 3 for a visual of the robot).\nFirst, the experimenter provided a ten-minute explanation of the study. In this explanation, participants were first introduced to the item-finding task, and were described each of the four signals in detail (idle, searching, has-item, and has-info). The experimenter introduced the participant to the RoSiD interface and described how to use each part of the interface. Once the introduction concluded, participants were instructed to design each of the four signals in a randomized and counterbalanced order.\ndoorstop were items that Kuri used the has-item signal for, because they were small enough to fit in the backpack on Kuri. The stapler was too big to fit in Kuri's backpack, and thus Kuri used the has-info signal to have the user stand up and walk over to the Kuri robot to pick up the item. The stapler was placed on a counter behind the participant, out of view from the table that the participant was facing.\nFollowing the interaction, participants filled out the system usability scale. The experimenter then performed a semi-structured interview with the participant to understand their opinions on the design process. Participants then completed the study, and were compensated with an Amazon Gift Card sent to their email.\nB. Preference Evaluation Study\nParticipants were recruited from the local university student population through email, flyers, and word-of-mouth. A total of 42 participants were part of the study, with ages that ranged from 18 to 32 (median 24); participants self-declared as men (19), women (19), and genderqueer, non-binary, or declined to state (4, aggregated for privacy; some participants belonged to multiple groups). There were 17 participants that self-identified as LGBTQ+. Participants were Asian (24), Black (1), Latino (7), Middle Eastern (3), and White (11) (some participants belonged to multiple groups). Participants rated their median familiarity with robotics as a 3 out of 9; a score of 1 corresponded with the term \"novice\" and a score of 9 corresponded with the term \u201cexpert\u201d.\nADDITIONAL TRAINING DETAILS\nTo encourage reproducibility, we provide the specifics of our training experiments in Appendices C and D. Appendix C discusses the feature learning models, and Appendix D discusses the reward learning models.\nC. Training Feature Learning Models\nWe used the following encoder architectures for each modal-ity. We used the transposed architecture for all self-supervised methods that required a decoder."}, {"title": "Algorithm 1: Contrastive Learning From Exploratory Actions", "content": "1 Given a list of robot trajectory datasets that all users saw over the course of the signal design process separated into explored and ignored data,\n$D = {(D^{ex}_i, D^{ig}_i)}=0$, a learnable model that generates trajectory features, \u03a6, and a hyperparameter for the contrastive margin, \u03b1;\n2 Initialize to a random state (or to a pretrained network);\n3 while not converged do\n4  $(D^{ex}_i, D^{ig}_i)$ \u2190 sample item from D;\n// Sample anchor and positive from explored data\n5  if Uniform(0, 1) < 0.5 then\n6  | $\u03be_a$ ~ $D^{ex}_i$, $\u03be_p$ ~ $D^{ex}_i$, $\u03be_N$ ~ $D^{ig}_i$;\n7  end\n// Sample anchor and positive from ignored data\n8  else\n9  | $\u03be_A$ ~ $D^{ig}_i$, $\u03be_p$ ~ $D^{ig}_i$, $\u03be_N$ ~ $D^{ex}_i$;\n10  end\n11  $L_1 = max(||\u03a6(\u03be_A) \u2013 \u03a6(\u03be_p)|| \u2013 ||\u03a6(\u03be_A) \u2013\n\u03a6(\u03be_N)||^3 + \u03b1, 0)$;\n12  $L_2 = max(||\u03a6(\u03be_p) \u2013 \u03a6(\u03be_A)|| \u2013 ||\u03a6(\u03be_p) \u2013\n\u03a6(\u03be_N)||^3 + \u03b1, 0)$;\n13  update parameters of to minimize $L_1 + L_2$\n14 end"}, {"title": "Linear Reward", "content": "The linear reward models as user's reward function as $R_H(\u03be) = \u03c9 \u00b7 \u03a6(\u03be)$, where the user's specific preference is represented by w. We learned a user's w through pairwise choices. We adopted the Bradley-Terry preference model to model the probability of a user choosing $\u03be_k$ from a query $Q = {\u03be_0, \u03be_1,...\u03be_\u03bd}$:\n$P(\u03be_k|Q, \u03c9) = \\frac{e^{\u03c9 \u00b7 \u03a6(\u03be_k)}}{\\sum_{i=0}^{N} e^{\u03c9 \u00b7 \u03a6(\u03be_i)}}$ (7)\nTo learn w, we apply Bayes' rule.\n$P(\u03c9|Q, \u03be_k) \u03b1 P(\u03be_k|Q, \u03c9) \u00b7 P(\u03c9)$ (8)\nWe assume a prior w of a uniformly distributed unit ball, as in prior works [9]. We update our posterior after every observed choice to estimate the user's w using Monte Carlo methods."}, {"title": "EXTENDED STATISTICAL ANALYSIS", "content": "In Appendices E-H, we report the full statistical analysis we performed across algorithms to report effect sizes so that others may use these for power analyses. Appendix E discusses completeness, Appendix F discusses simplicity, Appendix G discusses minimality, and Appendix H discusses explainability."}]}