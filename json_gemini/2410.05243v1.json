{"title": "Navigating the Digital World as Humans Do: UNIVERSAL VISUAL GROUNDING FOR GUI AGENTS", "authors": ["Boyu Gou", "Ruohan Wang", "Boyuan Zheng", "Yanan Xie", "Cheng Chang", "Yiheng Shu", "Huan Sun", "Yu Su"], "abstract": "Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.", "sections": [{"title": "1 INTRODUCTION", "content": "GUI (graphical user interface) agents, which are autonomous agents acting in the digital world via operating on GUIs, have been rapidly co-evolving with large language models (LLMs). On the one hand, the general multimedia understanding and generation capability of (multimodal) LLMs empower GUI agents to generalize beyond simple simulated settings (Shi et al., 2017; Humphreys et al., 2022) to diverse and complex real-world environments, including the web (Deng et al., 2023; Zhou et al., 2024; Yao et al., 2022), desktop (Xie et al., 2024; Wu et al., 2024) and mobile operating systems (Rawles et al., 2023; 2024; Yan et al., 2023b). On the other hand, GUI agents have become an important testbed for LLMs, providing both the necessary breadth and depth for driving continued development as well as a pathway to many commercially viable automation applications.\nMost humans perceive the digital world visually and act via keyboards, mice, or touchscreens. In principle, the embodiment of a GUI agent should already be complete if it can 1) visually perceive the GUI renderings, and 2) have effectors equivalent to a keyboard for typing and equivalent to a mouse or touchscreen for pixel-level operations like clicking and hovering.\u00b9 However, current GUI agents assume more than that. For perception, most current agents rely on reading the underlying text-based representations such as HTML or accessibility (ally) trees (Deng et al., 2023; Gur et al., 2024; Zhou et al., 2024).\u00b2 Only with the recent advances in multimodal LLMs (MLLMs) does visual perception become broadly viable, but text-based representations are still used jointly (Zheng et al., 2024; Koh et al., 2024; Zhang et al., 2024a). For effectors, most current agents act via selecting from a list of options, e.g., HTML elements (Deng et al., 2023; Zheng et al., 2024) or labeled bounding boxes (He et al., 2024; Zhang et al., 2024a), instead of pixel-level operations directly on the GUI. Obtaining those options in turn often requires access to text-based representations and/or separate models for detecting objects and text (Wang et al., 2024a; Kapoor et al., 2024).\nHowever, there is no free lunch, and those additional requirements come with their limitations. On the one hand, text-based representations are noisy and incomplete. Full HTMLs contain a considerable amount of irrelevant information. Ally trees are more compact and mainly contain semantic information, but similar to other semantic annotations that rely on voluntary participation, they widely suffer from incomplete and incorrect annotations.\u00b3 In contrast, visual renderings, by design, are information-complete and only contain information relevant to users. On the other hand, the additional input increases latency and inference costs. Zheng et al. (2024) found that HTML can consume up to 10 times more tokens to encode than the corresponding visual. Meanwhile, obtaining the ally tree can be time-consuming in itself, especially in desktop or mobile environments. The added latency and cost at every step are further compounded in the long-horizon agent tasks, compromising user experience and practicality.\nIn this work, we are interested in how far GUI agents with a human-like embodiment, i.e., only visual observation of environments and pixel-level operations, can go. There have been a few attempts (Shaw et al., 2023; Hong et al., 2024; Cheng et al., 2024), but they are rarely adopted in state-of-the-art solutions. We find that a major bottleneck is grounding, i.e., mapping textual plans generated by an (M)LLM to the precise locations on the GUI. There are three desiderata for a GUI agent grounding model: 1) High accuracy. A single grounding error can get an agent stuck and fail the whole task. 2) Strong generalization. It should work on different GUIs: desktop (Windows, Linux, macOS), mobile (Android, iOS), different websites, etc. 3) Flexibility. It should plug and play in different MLLMs instead of being tightly coupled with a certain model. Existing visual grounding methods for GUI agents (Shaw et al., 2023; Hong et al., 2024; Cheng et al., 2024) fail to meet these desiderata, hindering the advances towards GUI agents with human-like embodiment.\nThe main contributions of this work are three-fold:\n1. We make careful arguments and a strong case for GUI agents with human-like embodiment that perceive the digital world entirely visually and take pixel-level operations on GUIs, and propose a generic framework, SeeAct-V, for building such agents by adapting from the popular SeeAct framework (Zheng et al., 2024).\n2. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture (Liu et al., 2024c), is surprisingly effective for GUI visual grounding. Using"}, {"title": "2 METHOD", "content": "2.1 OVERVIEW\nWe adapt the popular SeeAct framework (Zheng et al., 2024) to one in which agents only take visual observation of the environment and directly conduct pixel-level operations, denoted as SeeAct-V (Figure 2). The original SeeAct has two stages, planning and grounding. An MLLM is used for both planning and grounding. At each step, an MLLM first generates a textual plan, and grounding is then done by asking the MLLM to select from a short list of grounding candidates. The grounding candidates are either filtered HTML elements or labels of Set-of-Mark (SoM; Yang et al. (2023)) annotations on the screenshot, both of which require HTMLs or ally trees as additional input. In contrast, SeeAct-V only uses screenshots for environmental observation. For grounding, SeeAct-V uses a separate model specialized for visual grounding that directly produces the coordinates on the current screen where the agent should act.\nA strong visual grounding model therefore becomes the key for making SeeAct-V a compelling framework. Ideally, it should generalize across platforms (e.g., web, desktop, and mobile) and handle diverse ways of referring to GUI elements. Considering the rapid evolution of MLLMs, this grounding model should be easily pluggable into different MLLMs to help ground their plans into different GUI environments. Finally, GUI screenshots can vary drastically in resolution and orientation; the grounding model therefore should handle a wide range of input resolutions. The main technical contribution of this work is a surprisingly simple recipe (incl. data and modeling) for training such universal visual grounding models. We introduce our simple data synthesis strategy in \u00a72.2, followed by modeling considerations in \u00a72.3. With this simple recipe, we construct the largest training data for GUI grounding to date and train UGround, a strong universal visual grounding model for GUI agents."}, {"title": "2.2 DATA CONSTRUCTION", "content": "We synthesize a large, high-quality, and diverse set of (screenshot, referring expression, coordinates) triplets as training data for visual grounding, where we use the center point coordinates of an element as the expected output. Our data synthesis will be based on webpages. Webpages are ideal for grounding data synthesis because their dual representation\u2014we can easily get the full HTML, the visual rendering, and fine-grained correspondences between the two (e.g., HTML elements to precise bounding boxes). HTML elements also contain rich metadata such as CSS or accessibility attributes, opening numerous opportunities for synthesizing diverse referring expressions (REs). Finally, since GUI designs share many similarities across platforms, we hypothesize that visual grounding models trained only on web data may still generalize to other platforms like desktop and mobile UIs.\nCommon RE Types for GUIs. People use diverse ways to refer to GUI elements (Figure 3). Previous visual grounding works (Hong et al., 2024; Cheng et al., 2024) have not sufficiently considered this dimension of diversity. We categorize common REs for GUI elements into three types: 1) Visual REs, i.e., salient visual features like text or image content, element types (e.g., buttons or input fields), shapes, colors, etc. 2) Positional REs, including both absolute (e.g., \u201cat the top left of the page", "to the right of element X\") to other elements. Besides straightforward positional information, contextual references (e.g., \u201cfor Item A,": "under the section X", "navigate to home,\" \"go to my cart\"). Composite types that combine two or more of these types are also common, especially when stronger disambiguation is needed, e.g., \u201cclick the heart button under the Pok\u00e9mon shirt to add to favorite.\"\nHybrid RE Synthesis from Web. We propose a novel hybrid synthesis pipeline, orchestrating both carefully curated rules as well as LLMs to generate diverse REs for HTML elements": 1, "Descriptors": "We extract abundant visual and functional information that are embedded in the attributes of HTML elements. For example, HTML attributes like inner-text and alt provide visual clues (including text content), while accessibility attributes like aria-label reveal more functional aspects of an HTML element. However, HTML attributes are often incomplete. To harvest visual and functional signals beyond HTML attributes, we use an open MLLM, LLaVA-NeXT-13B (Liu et al., 2024b). We input the visual rendering of an HTML element along with its available attributes to the MLLM and prompt it to generate diverse REs. This process often yields composite REs that combine some HTML attributes with visual features (e.g., \u201chollow heart", "element": "a visual HTML attribute, a functional HTML attribute, or the synthesized description by LLMs. 2) Positional Expressions: We curate rules to generate positional REs according to the absolute position of an element in the screenshot as well as its spatial relationship to neighboring elements (e.g., \"at the top of the page,\" \"between element A and B\"). We also create multiple rules to generate contextual references. For example, we identify elements of certain types in the screenshot (e.g., radio buttons, checkboxes, input fields), and generate REs for them based on their spatial and structural relationship (e.g., hierarchical structure of the DOM tree) to others (e.g., \u201cthe input field labeled Birthday\").\nWe collect screenshots (mix of portrait and landscape views in various resolutions) and metadata of web elements (salient HTML attributes, bounding box coordinates) from Common Crawl, and then"}, {"title": "2.3 MODEL DESIGN", "content": "We adopt a widely used open-source model architecture, 7B LLaVA-NeXT (Liu et al., 2024b), as our backbone model for visual grounding. We make a few adaptations to tailor it for GUI grounding.\nInput-Output Formulation. We always instruct the model to answer \u201cIn the screenshot, what are the pixel element coordinates corresponding to {Description}?", "(1344, 1344)\") to precisely point to an element without any normalization.\nImage Resolution. GUI screenshots are much larger than typical natural images, often requiring a resolution above 1,000px for legibility. LLaVA (Liu et al., 2024c;a) was initially built for 336px images, and was later scaled up to at most 772px via the AnyRes technique (Cheng et al., 2023; Gao et al., 2024; Liu et al., 2024b; Xu et al., 2024; Dong et al., 2024). It resizes and splits a large image into small slices, encodes each slice independently with the vision encoder, and adds a special token at the end of each row to help the language model keep track of the image shape. AnyRes allows easy scaling up of input resolution. However, it is always a trade-off between the diversity of supported resolutions and the speed of training and inference. To strike a balance and avoid meaningless excessive resolutions, we enlarge the allowed input sizes to 36 ViT (Dosovitskiy et al., 2020) slices, and use CLIP@224px (Radford et al., 2021) as the image encoder for more flexible splitting, pushing the maximum supported resolution to 1,344 \u00d7 1,344 (landscape) and 896 \u00d7 2,016 (portrait). Additionally, we use Vicuna-1.5-7b-16k (Zheng et al., 2023) with 16K context length to handle long visual contexts. Finally, there is a low-resolution image fusion module commonly used in AnyRes. However, we find it ineffective for GUI grounding, as 224px is too small to provide informative global context, so we leave it out from our model. More details are in Appendix D.\"\n    },\n    {\n      \"title\": \"3 EXPERIMENTS\",\n      \"content\": \"Most existing studies on GUI agents typically evaluate on one or two benchmarks. In contrast, we conduct a much more comprehensive evaluation on GUI agents to show the universality of our method. Our evaluation employs six benchmarks that span all three major platforms (i.e., web, desktop, and mobile) and cover three settings: visual grounding (\u00a73.1), offline agent evaluation on cached environment states (\u00a73.2), and online agent evaluation in live environments (\u00a73.3). The visual grounding setting focuses on the grounding performance of UGround, while the agent settings test the end-to-end effectiveness of the SeeAct-V framework with UGround integrated. On the agent\"\n    },\n    {\n      \"title\": \"3.1 GUI VISUAL GROUNDING\",\n      \"content\": \"We first evaluate UGround on the ScreenSpot benchmark (Cheng et al., 2024), which is specifically designed for visual grounding on GUIs. The benchmark consists of 1,272 single-step instructions and the corresponding bounding box of the target elements across mobile (e.g., iOS and Android), desktop (e.g., macOS and Windows), and web environments. These elements vary between text-based elements, icons (e.g., the trash can icon) and widgets (e.g., to-do lists), representing diverse GUI element types.\nWe evaluate under two settings: 1) Standard Setting. In the standard setting of ScreenSpot, the instructions are written by human annotators with a primary focus on functional description of the target elements, e.g., simply \\\"close\\\" to refer to the 'X' button that closes a window or \u201cset an alarm for 7:40\\\" when the input image shows the iPhone clock app with a list of inactive alarms. 2) Agent Setting. For GUI agents, a grounding model needs to work with a planning model (e.g., an MLLM) and ground the REs it generates, which includes not only functional REs but also visual and positional REs (see \u00a72.2). To provide a more comprehensive evaluation on visual grounding for GUI agents, we input each ScreenSpot example to an MLLM, which acts as a planning model, and asks it to generate diverse REs for the target element. This setting is therefore more representative of the grounding challenges in GUI agents. We mainly compare UGround with SeeClick (Cheng et al., 2024), the state-of-the-art visual grounding model on ScreenSpot, and another visual grounding model CogAgent Hong et al. (2024). To show the challenge of visual grounding for general-purpose models, we also compare with GPT-4 and GPT-40.\nResults. As shown in Table 2 and Table 3, UGround outperforms all existing models across all the settings and platforms by a substantial margin, about an absolute improvement of 20% on average under the standard setting and 29% under the agent setting. Interestingly, UGround performs remarkably well on desktop UIs, despite the fact that it is never trained on desktop screenshots (Table 1). Compared with existing models, UGround performs especially well on icons and widgets, which are generally more challenging for grounding because that requires deeper understanding of the contextual (e.g., positional) and semantic (e.g., functional) information. Overall, the strong results on ScreenSpot clearly demonstrates UGround's universal grounding capability across platforms and planners as well as the remarkable effectiveness of our simple data synthesis and modeling recipe.\"\n    },\n    {\n      \"title\": \"3.2 OFFLINE AGENT EVALUATION\",\n      \"content\": \"We discuss the experimental setup for three offline agent evaluation benchmarks followed by result discussion. Concrete examples from each benchmark are given in Appendix B.\nWeb: Multimodal-Mind2Web. We use Multimodal-Mind2Web (Zheng et al., 2024), the multimodal extension of Mind2Web (Deng et al., 2023), for our evaluation on realistic web tasks. The test split consists of 1,013 tasks spanning over 100 different websites. Each task contains a high-level task instruction and a sequence of actions, with a screenshot of the webpage before each action, as the golden trajectory. All the webpages along the golden trajectory are cached to support offline evaluation. The tasks are crowdsourced with a focus on ensuring real-world meaningfulness (i.e., what real users would need on those websites).\nZheng et al. (2024) have clearly demonstrated the necessity of visual perception for web agents, so we mainly compare with zero-shot methods that use MLLMs as planners and omit text-only LLMs. Zheng et al. (2024) have also identified grounding as the main challenge and proposed several grounding strategies, including 1) Choice, where the planner is asked to choose from a short list of filtered HTML elements, and 2) SoM, where the input screenshot is superposed with set-of-mark (Yang et al., 2023) labels and the planner is asked to select from the labels. Both strategies require additional text-based representations (i.e., HTML) to obtain the candidates and/or locate the elements in the screenshot to label. We report step-wise element accuracy, i.e., accuracy of selecting the correct element, and omit operation scores because they are orthogonal to grounding comparisons.\nMobile: AndroidControl. We use AndroidControl (Li et al., 2024b), a large-scale Android dataset comprising 15K unique tasks over 833 Apps. Screenshots, action sequences, and ally trees are cached from human demonstrations as golden trajectories for training and evaluation purposes. Each action is also labeled by a corresponding low-level instruction (e.g., \u201cset the hours to 6\"). Following Li et al. (2024b), we use 500 random steps from the test set. We compare with the SOTA zero-shot method, the text-only version of M3A (Rawles et al., 2024), which instructs GPT-4 to generate textual actions as well as select elements from the ally tree (Choice). We adopt the two task settings in Li et al. (2024b)": "high-level tasks, where only the high-level intent is provided, and low-level tasks, where both the high-level intent and the corresponding low-level instruction for each step are available. We use the standard metric, step-wise accuracy, where a step is considered successful only if all the predicted actions, elements, and arguments (if applicable) are correct.\nDesktop: OmniACT. We use OmniACT (Kapoor et al., 2024) to evaluate the accuracy of UGround on desktop tasks. The dataset consists of 9,802 tasks covering 38 desktop applications and 27 websites across different desktop platforms (macOS, Windows, and Linux). Each task requires the generation of a PyAutoGUI script, which is a sequence of actions to complete the task on a single screenshot. The SOTA method, DetACT (Kapoor et al., 2024), extracts UI elements and their coordinates through a combination of OCR (optical character recognition), icon matching, and color detection modules. These elements are filtered by task relevance and then passed to LLMs or MLLMs to generate the PyAutoGUI script with the appropriate coordinates for interaction.\nFor SeeAct-V, we replace the input of the DetACT pipeline with only screenshots and instruct MLLMs to generate element descriptions rather than directly generate coordinates. We then employ UGround to obtain the coordinates of the elements, which are subsequently integrated into the PyAutoGUI scripts. To ensure a fair comparison, we strictly follow the approach in Kapoor et al. (2024), including the same prompt and retrieval strategy that selects five in-context examples from the training set based on task similarity. We report the action score, which measures the accuracy of the action sequences while penalizing errors in generated arguments."}, {"title": "3.3 ONLINE AGENT EVALUATION", "content": "We further evaluate our approach in an end-to-end manner on two online agent benchmarks that closely resemble the offline web and Android benchmarks in \u00a73.2, but involve interactions with live websites and mobile applications. Due to the high cost of online evaluation, we only use UGround for grounding.\nWeb: Mind2Web-Live. We use the test set from Mind2Web-Live (Pan et al., 2024). The benchmark is built on Mind2Web (Deng et al., 2023) by adding functional evaluation to the tasks that makes automated evaluation possible on live websites. Specifically, it defines and annotates key nodes for each task, which are critical steps that must be completed for a task to be considered successful, regardless of which trajectory an agent takes. The baseline agent from Pan et al. (2024) is text-only, perceives and interacts with webpages by hundreds of HTML elements at a time. For SeeAct-V, we change the observation to be screenshots only, and make necessary changes to the original action space to fully eliminate the dependency on HTML during planning, grounding, and execution (details in Appendix E.5). We use standard metrics: micro completion rate, which measures the proportion of completed key nodes across all the tasks, and task success rate, which measures the proportion of fully completed tasks.\nMobile: AndroidWorld. We use AndroidWorld (Rawles et al., 2024), an online mobile agent benchmark running in Android emulators. It includes 116 tasks across 20 Apps, with evaluation based on the final states of the device. We compare with the SOTA agent M3A and its text-only variant from Rawles et al. (2024). They receives both raw and SoM images, together with textual UI elements, or only the textual UI elements as the observation respectively. Both variants employ a ReAct-style reasoning process (Yao et al., 2023) to select the next target element from a list of UI elements. Additionally, they integrate self-reflection (Shinn et al., 2024) for the agent to summarize its current action and improve decision-making in subsequent steps. We report task success rate, which measure the percentage of fully completed tasks."}, {"title": "3.4 ERROR ANALYSIS", "content": "We conduct a manual error analysis of the best performing method, SeeAct-V with UGround, to understand the bottleneck for further improvement. We randomly sample 60 failure cases from each split of ScreenSpot (agent setting with GPT-40), AndroidControl, and Multimodal-Mind2Web. Except for data annotation errors, errors from the models can be categorized into planning errors, i.e., generating plans with incorrect element descriptions, and grounding errors, i.e., predicting incorrect coordinates for a correct element description from the planner.\nAs shown in Figure 4, planning errors are the dominant cause of failures across all benchmarks, further confirming the strong grounding capability of UGround. The most frequent error is that the planner generates (otherwise correct) description of an incorrect element on the screen, indicating a lack of correct understanding of either the task and/or the elements. Other common planning errors include hallucinating non-existent elements or producing overly generic descriptions that are too vague to uniquely locate the target element, even for human evaluators.\nOn the other hand, on ScreenSpot-Mobile and ScreenSpot-Desktop, a considerable portion of the failures do stem from grounding errors. Both desktop and mobile UIs feature a pervasive use of icons with idiosyncratic meaning. For example, a stylized dollar sign represents the Zelle App, or an icon with two cartoon people represents one's contact list in Miscorosft Outlook. We find that pretrained MLLMs and our web-centric grounding training are effective in capturing the semantics of popular icons (e.g., icons representing Google) or commonsense meaning (e.g., clock icons usually represent time-related functions like alarms). However, it is challenging to capture the idiosyncratic semantics of icons in the long tail, which arguably requires either additional documentation or more targeted exploration to learn. This is a major cause of the grounding errors. Interestingly, when tested on more realistic agent tasks, e.g., in AndroidControl, AndroidWorld, and OmniACT, UGround still proves to be relatively robust. This is because most of the agent tasks concern things in the head of the distribution; things in the long tail are naturally rare (though still important). This explains the strong performance of UGround on mobile and desktop agent benchmarks. Nonetheless, how to capture idiosyncratic semantics in the long tail is still an open challenge for grounding."}, {"title": "3.5 TRAINING DATA ANALYSIS: SCALING AND ABLATIONS", "content": "We conduct scaling analysis and ablation studies on our training data to better understand the contribution of different data for UGround's strong performance. We use the agent setting of ScreenSpot with GPT-40 as the planner."}, {"title": "4 CONCLUSIONS AND LIMITATIONS", "content": "We introduce UGround, a universal GUI visual grounding model developed with large-scale web-based synthetic data. UGround shows strong cross-platform generalization and significantly outperforms the prior SOTA model SeeClick on ScreenSpot. We propose a vision-only framework SeeAct-V that allows pixel-level interactions based solely on visual input. Our evaluations on both offline and online benchmarks demonstrate that SeeAct-V agents with UGround can achieve comparable and often better performance than prior SOTA agents that rely on additional textual inputs like HTML or ally trees for observation or grounding.\nNevertheless, there are still some limitations that can be addressed in future work to advance visual grounding in GUI and visually grounded GUI agents. First, UGround is trained on very large-scale synthetic data. Considering the similarity and repetition of elements between webpages, there is much room to improve on data efficiency during training, for example by wise data grouping and deduplication. On the other hand, despite the cross-platform generalization shown in our experiment results, the issue of long-tail elements remains unaddressed in this work. Mobile UIs and desktop UIs often feature specific icons, where it can be impractical to account for every long-tail element in the training set. Additionally, no desktop UI data is incorporated in the training of this work, which limits its performance on desktop UIs. Given the scarcity of training data for desktop UIs, we anticipate the development of more comprehensive datasets in this domain. Lastly, UGround depends on an external planner, and without training on downstream tasks, it cannot function independently as a GUI agent. Nonetheless, we hope that our datasets, model, and framework can contribute to future studies of vision-only agents, as well as contribute to advancing the grounding capabilities of future end-to-end models."}]}