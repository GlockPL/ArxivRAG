{"title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts", "authors": ["Henrique Da Silva Gameiro", "Andrei Kucharavy", "Ljiljana Dolamic"], "abstract": "With the emergence of widely available powerful LLMs, disinformation generated by large Language Models (LLMs) has become a major concern. Historically, LLM detectors have been touted as a solution, but their effectiveness in the real world is still to be proven. In this paper, we focus on an important setting in information operations-short news-like posts generated by moderately sophisticated attackers.\nWe demonstrate that existing LLM detectors, whether zero-shot or purpose-trained, are not ready for real-world use in that setting. All tested zero-shot detectors perform inconsistently with prior benchmarks and are highly vulnerable to sampling temperature increase, a trivial attack absent from recent benchmarks. A purpose-trained detector generalizing across LLMs and unseen attacks can be developed, but it fails to generalize to new human-written texts.\nWe argue that the former indicates domain-specific benchmarking is needed, while the latter suggests a trade-off between the adversarial evasion resilience and overfitting to the reference human text, with both needing evaluation in benchmarks and currently absent. We believe this suggests a re-consideration of current LLM detector benchmarking approaches and provides a dynamically extensible benchmark to allow it (https://github.com/Reliable-Information-Lab-HEVS/\ndynamic_llm_detector_benchmark).", "sections": [{"title": "1 Introduction", "content": "The misuse of large language models (LLMs) for propaganda, inciting extremism, or spreading disinformation and misinformation has been a major concern from the early days of LLMs (Bender et al., 2021; McGuffie and Newhouse, 2020; Ippolito et al., 2020). While this concern has led in the past LLM developers to withhold their most powerful models (Solaiman et al., 2019), powerful LLMs and multimodal generative models have now been published despite such concerns persisting (OpenAI, 2023). With the release of powerful open-weight LLMs that can be deployed on commodity hardware such as LLAMA (Touvron et al., 2023), Phi (Gunasekar et al., 2023) or Zephyr (Tunstall et al., 2023), LLM misuse can no longer be mitigated through model adjustment or inputs/outputs monitoring."}, {"title": "1.1 LLMs in Information Operations", "content": "A common goal of information operations (intentional and coordinated efforts to modify the public perception of reality with ulterior motives) is to modify the perception of the current situation (Goldstein et al., 2023; Maschmeyer, 2022). A well-documented and effective approach to achieve that leverages social networks by seeding them with news-like narratives. While the original seeding might have a minimal impact, some narratives will trigger a strong partisan response and will be re-shared by key genuine users, who will customize the narrative for their audience and engage in debate to further it (Eady et al., 2023; Baribi-Bartov et al., 2024). While in some cases such misinformation is picked up by traditional news outlets, adding to its credibility, the quantity is a quality of itself, and continuous repetition of the same narratives through different channels leads to their acceptance as truths (DiRESTA, 2018; Geissler et al., 2022). Given the competition for attention on social media, both seeded narratives and legitimate news tend to be shared as short-form pots of 260-520 characters with links and images. Such format is common to Twitter (now X), Bluesky, Meta Threads, and the largest Mastodon instances.\nIn the past, this technique has been shown to be an excellent conductor of counterfactual claims and a poor one for rebuttals and factchecks (Vosoughi et al., 2018). However, large-scale information operations on them have been, until recently, easily detectable due to exact text reuse, inconsistencies in online persona, and lack of non-trivial interaction with other users. Such failures are understandable, given the scope of the activities and the cost of human operators. However, LLMs drastically alter the cost-quality tradeoff (Musser, 2023). Combined with the reports that LLMs are better than humans at both personalized political persuasion and disinformation concealment (Matz et al., 2024; Li et al., 2024; Chen and Shu, 2023), LLM-augmented information operations promise to be radically more effective and more difficult to detect and counter."}, {"title": "1.2 LLM Detectability", "content": "Unfortunately, humans do not distinguish well LLM-generated texts from human-written ones (Ippolito et al., 2020; Matz et al., 2024; Jakesch et al., 2022). Early in the generative LLM development (Zellers et al., 2019; Solaiman et al., 2019) proposed that accurate LLM detectors could mitigate that issue. Unfortunately, follow-up research rapidly discovered that LLM detectors failed if generation parameters changed (Ippolito et al., 2020; Fishchuk and Braun, 2023), output was paraphrased (Krishna et al., 2023a), or barely more complex prompts were used (Wu and Hooi, 2023a; Bakhtin et al., 2019).\nThe issue gained in salience with the release of ChatGPT, leading to several large-scale LLM detector benchmarks (Li et al., 2024; Sadasivan et al., 2024; He et al., 2023; Wu and Hooi, 2023a), with most recently (Dugan et al., 2024; Wang et al., 2024a). Unfortunately, with the disparate performance of detectors for different types of texts, most benchmarks suffer from the same pitfalls as many other ML-based security solutions (Arp et al., 2022), and there is still no consensus as to whether LLM detectors are ready for real-world applications (Da Silva Gameiro, 2024). Here, we try to address both the methodological issues and decide on the detectors' real-world usefulness in our setting."}, {"title": "1.3 Setting and Attacker Capabilities", "content": "An attacker with arbitrary capabilities is neither realistic nor can realistically defended against. Consistently with subsection 1.1, we focus on moderately sophisticated attackers. We assume they are capable of deploying on-premises SotA LLMs in the 1-10B parameter range and familiar with adversarial evasion strategies available at inference, such as generation parameters modification (Fishchuk and Braun, 2023; Ippolito et al., 2020), paraphrasing (Krishna et al., 2023a), or alternative prompting strategies (Wu and Hooi, 2023a; Bakhtin et al., 2019). We assume that the attacker cannot evasively fine-tune the generative models.\nWe assume that the attacker is seeking to generate news-like content of approximately 500 characters, as a completion of a headline or opening sentence, that needs to be detected by a social media operator in an environment where LLM-generated news-like content is not predominant and true positive labels are not available, requiring a low target false positive rate (FPR)."}, {"title": "1.4 Contributions", "content": "\u2022 We consolidated best practices for LLM detector evaluation and developed a dynamic benchmark integrating them, allowing simple extension to new domains\n\u2022 We show that SotA zero-shot detectors are vulnerable to trivial adversarial evasion in ways not reflected by prior benchmarks\n\u2022 We comprehensively benchmarked custom detector training strategies and discovered a detector architecture achieving a robust generalization across unseen LLMs and attacks\n\u2022 We demonstrate that such adversarially robust custom detectors overfit their reference human data, indicating a tradeoff that needs to be tested\n\u2022 Based on these results, we conclude that LLM detectors are currently not ready for real-world usage to counter LLM-generated disinformation"}, {"title": "2 Background and Related Work", "content": null}, {"title": "2.1 Trained Detectors", "content": "LLMs have led to a pardigm shift, from purpose-training ML models, to fine-tuning base models. Rather than re-training a new model for each application from scratch, a large model is first pre-trained on a large quantity of data, and is then fine-tuned to adapt it to downstream task. This paradigm is particularly well-suited for classification tasks as demonstrated by (Devlin et al., 2019). As we can formulate detection as a classification task, multiple papers take this approach, historically pioneered by (Solaiman et al., 2019). It is also currently considered a SotA approach for training custom LLM detectors (Ippolito et al., 2020; He et al., 2023)"}, {"title": "BERT-based detectors", "content": "BERT (see (Devlin et al., 2019)) and its subsequent improved versions, ROBERTa and Electra (see (Liu et al., 2019) and (Clark et al., 2020)), have gained widespread adoption for classification tasks. Their bi-directional encoding architecture offers an advantage over the autoregressive decoder-only architecture, given the ability to account for the context of both the preceding and following context rather than just the preceding, as well as obligatory anchoring to the provided text. Moreover, their relatively small size, such as the 300M parameters for RoBERTa Large compared to the 175B parameters for GPT-3, makes them highly practical."}, {"title": "2.2 Zero-Shot Detectors", "content": "Trained detectors show promising results in multiple works such as in (Mitrovi\u0107 et al., 2023). However, as highlighted by (Wang et al., 2024b), these trained detectors fail to generalize to text distribution shifts. Zero-shot detectors such as in (Mitchell et al., 2023) and (Bao et al., 2024) can be seen as an appealing alternative to mitigate this issue. Zero-shot detectors, i.e., detectors not relying on training, are more suitable when we try to detect text not coming from a specific distribution. They are also easier to use in practice since we do not need to train the detector on the domains, although at the price of generally being more resource-intensive due to using larger models.\nFast-DetectGPT is a SoTA open-source detector, reported to perform well across domains and common attack strategies (Bao et al., 2024), scoring within in overall top for FPRs < 5% in adversarial third-party benchmarks (Dugan et al., 2024).\nGPTZero is one of the most widely used commercial LLM detectors (Tian and Cui, 2024), consistently included into third-party benchmarks.\nROBERTa-Base-OpenAI is a BERT-based detector fine-tuned by OpenAI to detect GPT2 output (Solaiman et al., 2019). While it is not a zero-shot detector per se, it is still commonly benchmarked and used as such, with some benchmarks reporting good performance(Wang et al., 2024a). At the time of writing (May 2024), it was downloaded over 159,000 times in the previous three months, according to its HuggingFace repository."}, {"title": "2.3 Benchmarking Detectors", "content": "Adversarial evasion setting is inherent to LLM detectors; its performance cannot be detached from performance against potential attacks. Multiple works have attempted to tackle this issue, generally focusing on specific attacks. For instance, (Krishna et al., 2023b) and (Wu and Hooi, 2023b) introduced a paraphrasing attack they demonstrated to be efficient, while (Bao et al., 2024) investigated evasion through alternative decoding strategies.\nGenerative generalization benchmarking, be it across LLMs (He et al., 2023; Pu et al., 2023) or domains (Li et al., 2024; Xu et al., 2023) is equally essential. LLM generation can be useful to an attacker in multiple contexts, and with a proliferation of widely available powerful LLMs, an attacker cannot be assumed to restrict themselves to a single generative model.\nHuman-text generalization evaluation is, unfortunately, all but absent from LLM detectors benchmarks, despite reported issues in real-world usage (Liang et al., 2023b). Existing benchmarks report - at best detection rates for additional domains, without reporting FPRs.\nUnfortunately, systematic studies remain few, and the recent benchmarks attempting them, such as (Dugan et al., 2024; Wang et al., 2024a) suffer from issues, notably failing to include common attacks, such as temperature increase (Ippolito et al., 2020), and evaluation of FPRs in unseen domains. As static benchmarks optimized for detector evaluation, they are difficult to add new attacks to or transfer to new domains, making them unsuitable for our application.\nROBERTa-Base-OpenAI is a BERT-based detector fine-tuned by OpenAI to detect GPT2 output (Solaiman et al., 2019). While it is not a zero-shot detector per se, it is still commonly benchmarked and used as such, with some benchmarks reporting good performance(Wang et al., 2024a). At the time of writing (May 2024), it was downloaded over 159,000 times in the previous three months, according to its HuggingFace repository."}, {"title": "3 Methodology", "content": "All results can be replicated with code provided in the experimental repository: https://github.com/Reliable-Information-Lab-HEVS/text_\nllm_detector. English is the only language considered here, with all datasets, prompts, and fine-tuning data in English."}, {"title": "3.1 Detector Models", "content": "To evaluate a domain-specific pretrained detector, generally reported to be one of the best detection methods (Wang et al., 2024a; He et al., 2023), we evaluated three base pre-trained LLMs: ROBERTa-Large, Distil-RoBERTa, and Electra-Large\u00b9. ROBERTa and Electra have been shown to achieve improved results over BERT on fine-tuning to classification tasks in (Liu et al., 2019) and (Clark et al., 2020) thanks to a different pre-training method (Liu et al., 2019; Clark et al., 2020). We added Distil-RoBERTa to evaluate the performance of a smaller model (82.8M parameters), more usable at scale.\nTo test zero-shot detectors, we evaluated the three detectors mentioned previously: ROBERTa-Base-OpenAI, due to its usage, and Fast-DetectGPT and GPTZero, generally considered as representatitve of SotA open-source and commercial detectors, respectively (Dugan et al., 2024)."}, {"title": "3.2 Generating the Datasets", "content": "We chose six different generator LLMs. Three non-chat foundational models, Phi-2, Gemma-2B, and Mistral-0.1, are the only ones used to train the custom detectors. Three chat models, Gemma-chat, Zephyr, and LLama-3-8B-Instruct, are only used for testing and are representative of commonly used SotA open-weight LLMs.\nTo create neural fake news, leveraged the CNN Dailymail news dataset, representative of American English news. To obtain the LLM-generated articles, we take a news article from the dataset, clean the beginning of the article to remove header content, and then pick the 10 first words of the article as a prefix. We use this prefix as the prompt for the non-chat models to generate the rest of the article and prefixed it with a supporting prompt for chat models (see appendix A.2). We let the model generate up to 200 tokens but cut the generation to have only the first 500 characters. We also cut the original articles to 500 characters to obtain the reference human samples. Using this procedure, human and generated samples are indistinguishable in length (see examples in appendix A.3).\nBy using the method described above, we create 6 datasets with around 20K samples each (see ap-"}, {"title": "3.3 Training the Detectors", "content": "We finetuned the pretrained models described in subsection 3.1 on each non-chat model training dataset and the round-robin dataset. The detectors are trained on the full datasets (1 epoch) with an evaluation after every 200 samples seen. We save only the model that obtained the best loss on the evaluation set to avoid overfitting. We list the hyperparameters for each training procedure in appendix B.1.\nWhile we tested different ways of finetuning the detectors on the datasets, we only retained full fine-tuning for the results section, consistently with the detector training SotA recommendations. We also provide some results when only finetuning a classification head and using the adapters PEFT method in appendix D. We tracked the model performance on the pretraining task to confirm we were not overfitting the base models to our distribution (cf appendix E)."}, {"title": "3.4 Testing the Detectors", "content": "We use the same metric across all the results to test the detectors: TPR (True-Positive rate). To obtain the detection prediction, we use a threshold on the output of the detector to target an FPR (False-Positive rate) of at most 5%. We find these thresholds by finding the TPR/FPR at different thresholds on the evaluation set for each dataset. We repeat this threshold-finding procedure for each detector (trained and zero-shot). This setting mimics a realistic scenario where a defender uses a detector to target a low level of false positives without access to true positive labels. We do not recompute a detection threshold on each attack since we consider these attacks unknown to the defender."}, {"title": "3.4.1 Testing Fast Detect GPT", "content": "To test Fast-DetectGPT (Bao et al., 2024), we use the script from their GitHub repository that we slightly adapted to run similarly to the other detectors we tested. We use GPT-J 6B B as the reference model since it provides more accurate results according to their work."}, {"title": "3.4.2 Testing GPTZero", "content": "To test GPTZero (Tian and Cui, 2024), we used an academic partner API access, courtesy of GPTzero, version base-2024-04-04. In order to maintain consistency with the methodology presented here, we forced the FPR of GPTZero to 5%, given that by default, it is heavily biased to minimize FPR (< 0.3% in our setting)."}, {"title": "3.5 Attacks against the Detectors", "content": null}, {"title": "3.5.1 Evasion Attacks", "content": "In experiments targeting detector evasion, we start with the test set created in the previous experiment when generating fake articles. We regenerate the fake news articles using either a different generation parameter or a different prompt. That way, we obtain a new dataset of fake articles with the same true original articles as the ones in the previous experiment, but where the fake articles are generated differently from the one used to produce the training and test data of the previous experiment.\nChanging the Generation Parameters The generation parameters we modify are the temperature and the repetition penalty. The first attack, called \"high temperature,\" sets the temperature to 1.2 (note that OpenAI's API allows a temperature up to 2). The second, which we call \"repetition penalty,\" consists in setting the repetition penalty to 1.2 (interestingly, hugging chat uses a repetition penalty of 1.2). Both parameters heavily impact the diversity of the produced text, making it more difficult for detection methods that rely on the lack of diversity of AI-generated texts.\nPrompting Attacks We consider a few prompts to generate the fake news articles (the complete list can be found in appendix A.4). The prompts we choose should cover a wide set of prompts an attacker may use. The idea for our experiment is to consider only basic attacks, i.e., attacks that do not"}, {"title": "Paraphrasing Attacks", "content": "Paraphrasing a generated text with an auxiliary LLM has been reported to achieve good evasion against a variety of detectors (Wu and Hooi, 2023b). Given that it preserves semantic meaning while modifying probabilities of token selection, it is likely to erase non-semantic traces of LLM generation and, as such, to be highly performant. Here, we implement a simpler version of the attack than DIPPER (Krishna et al., 2023b), prompting an LLM to reformulate the text (cf Figure 6)"}, {"title": "3.5.2 Testing the Detectors on Human Text from a Different Distribution", "content": "Finally, we also test the detectors on 10000 samples from the Xsum dataset. Xsum comprises articles from BBC News, a different news outlet from the one we used to until now, based on CNN. XSum notably uses British English. This experiment tests the human-text generalization of detectors, so we did not create fake news articles."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 Trained Detectors", "content": "Here, we present TPRs at FPR fixed to 5% for detectors where we finetune all parameters. Results obtained with other finetuning methods can be found in the appendix D.\nPerformance on training LLM We present in figure 1 the TPR obtained when testing the trained detectors on the same dataset they have been trained on. In our experiments, Electra (Electra-Large) consistently outperforms or comes equal with the other detectors. We can also notice that the difference is small between the distilled ROBERTA version (82.8M parameters) and its large version (355M params parameters). For the next results, we will show only the results using Electra as the"}, {"title": "Generalization across training LLMs", "content": "We show on the heatmap in figure 2 the TPR of the trained detectors on the test of the different datasets generated by the LLMs, evaluating finetuned detector generalization across generating LLMs.\nSurprisingly, most detectors perform almost equally, if not better in some cases, when detecting fake news articles generated by a different LLM than the one used to train them. This suggests a good generalization of the patterns leveraged by the finetuned detectors and a sufficient similarity in LLMs output in our setting."}, {"title": "Generalization to unseen chat LLMs", "content": "In this experiment, we repeat the same testing as above, but on 3 datasets generated by chat models that were not used to generate any training datasets (see appendix A.2 for the prompt). The results are in the same heatmap as for the previous experiment: figure 2.\nWe find that the TPRs obtained here align with the finding above that the trained detectors surprisingly generalize enough to achieve similar TPR across data generated by instruction-tuned LLMs. We find that news articles generated by Zephyr are slightly harder to detect, whereas the ones generated by Llama 3-Instruct-8B are the easiest. This highlights that while we find encouraging generalization results across LLMs, the performance across LLMs remains disparate, consistently with prior findings (Pu et al., 2023)."}, {"title": "4.2 Cross-LLM Generalization", "content": "We present in figure 3 the results of the same experiment as above, but including zero-shot detectors, which we compare to one of the trained detectors. We observe that Fast-DetectGPT and GPTZero closely match but still underperform compared to Electra-Large trained to detect Mistral-generated texts (Electra_Mistral). We also observe that ROBERTa-OpenAI significantly underperforms, suggesting that it is now outdated and strongly arguing for abandoning it as a zero-shot general LLM detector.\nAgain, we find Zephyr LLM particularly challenging for all detectors and, going forward, will focus on it to best differentiate model performance. Curiously, we also observe that GPTZero drastically under-performs on non-chat models, consistently with previous benchmarks (Dugan et al., 2024), suggesting an overfitting to chat models."}, {"title": "4.3 Evasion Attack Reslience", "content": "Here, we focus on the best-performing trained detector (Electra_Mistral) and the most challenging"}, {"title": "4.4 Performance on Unseen Human Texts", "content": "In the previous subsection, we showed that while a moderately sophisticated attacker could defeat all detectors, one of our custom-trained detectors - Electra_RR - exhibited an outstanding resilience to evasion attacks, frequently presenting the best TPR, and if not - the second best, with a usable and consistent 0.84 TPR, arguing in favor of Round-Robin training utility, previously seen in GANs (Kucharavy et al., 2020). Such resilience is surprising, given it was not trained against adversarial evasion and did not generalize particularly well across generative LLMs, and could lead us to suppose that Electra_RR would perform well in new evasion attacks, hence claiming a new SotA.\nHowever, such a claim would be misplaced. In real-world deployments, LLM detectors must not only generalize across unseen generators and attacks but also across unseen human texts, maintaining an acceptable FPR across different types of texts. We test for this in table 2, verifying the generalization from the CNN News dataset to the BBC News Xsum dataset, which are closely related and differ most likely only by US English vs British English usage. Despite such close relatedness, we observe that Electra_RR fails dramatically in human text generalization along with Electra_Mistral, indicating that neither of our trained detectors can be deployed to the real world."}, {"title": "5 Conclusion", "content": "In this paper, we developed a rigorous framework to benchmark LLM detectors in a way specific to a domain and threat model. By applying it to a setting relevant to LLM-augmented information operations, we show that current LLM detectors are not ready for real-world use due to a combination of susceptibility to trivial evasion attacks, notably generation temperature increase, and potentially unacceptable FPRs in practice, consistently with noted issues in other domains (Arp et al., 2022). Overall, we believe our results argue in favor of alternative approaches in that context, e.g., coordinated activity patterns search (Pacheco et al., 2021)."}, {"title": "Limitations", "content": "While we focus on a setting highly relevant to in-the-wild LLM text recognition, our work has several limitations.\nFirst, we focus on a short-story setting and use a well-known NLP dataset as a reference human text. Due to NLP dataset reuse for model training, the performance of the generative task is likely to be higher than that of actual information operations. Similarly, social media accounts without posting history are rare, and puppet accounts are likely to have similarly LLM-generated past posts. Such longer texts could increase confidence that an account is unauthentic. However, misappropriation of social media accounts for information operations is common, and such unauthentic post history is not guaranteed.\nSecond, we have not investigated text watermarking approaches. If future on-device LLM releases more tightly integrated models and supporting code and enables watermarking, it could potentially improve the detectability of such posts. While not impossible, recent work by (Sadasivan et al., 2024) suggests that reformulation attacks remain effective in this setting, warranting caution as to their effectiveness.\nThird, we only tested a limited number of zero-shot detectors. While it is possible that untested detectors could clear both adversarial evasion and generalization tests, it is unlikely. (Bao et al., 2024) and (Dugan et al., 2024) performed extensive benchmarking of recent zero-shot detectors, and both Fast-DetectGPT and GPTZero closely match other solutions across a large palette of tests, including adversarial perturbations.\nFourth, we assume the attacker has limited capabilities, notably lacking adversarial evasive model fine-tuning. This assumption is somewhat brittle, given that parameter-efficient fine-tuning (PEFT) requires minimal resource overhead compared to traditional fine-tuning. PEFT can be performed on quantized models from relatively small datasets and hardware adapted for quantized inference, putting them within the reach of moderately sophisticated attackers we consider. While this opens a new type of attack, current LLM detectors are easy enough to fool even without it.\nFinally, our work focused on the English language, for which extensive resources are available and are leveraged by LLM developers. While our results could generalize to other high-resource Ro-"}, {"title": "Ethics Statement", "content": "While the issue of deep neural disinformation is critical and a central concern for malicious misuse of LLMs, it has not prevented the release of powerful SotA models to the general public that can readily assist in such operations. Here, we do not present any novel attacks but demonstrate that existent ones are sufficient to evade SotA attacks. As such, we do not expect novel risks to arise from this work but rather to improve the general awareness of common tools' limitations and contribute to mitigating known risks arising from LLMs.\nGPTZero is a security solution numerous entities use to detect LLM-generated text in potentially safety-critical contexts. Given that in this work, we found several highly effective attacks against it, we performed a coordinated vulnerability disclosure with them.\nWe used 1 A100 GPU on a local cluster to generate the datasets, 30 minutes per dataset. We used 1 A100 GPU to train the models and perform detection using Fast-DetectGPT. For testing, we only used V100 GPUs thanks to our focus on smaller models. In total, we used the local cluster for 30 hours of GPU-days on the A100 and 20 hours of GPU-days on the V100 (numbers including hyperparameter search and testing correctness), leading to total emissions of 5.4kg of CO2. No crowd-sourced labor was used in this work. LLM assistants were used for minor stylistic and grammatical corrections of the final manuscript, consistently with ACL recommendations. GitHub Copilot has been used to assist in coding with auto-completion but no script or algorithm has been fully generated with it."}, {"title": "A Datasets and Data Generation Details", "content": null}, {"title": "A.1 Datasets List", "content": "We created 6 datasets with a balanced number of fake and true samples (1 dataset per generator). See table 3 for the list of generators used for the datasets and the precise size of the datasets. The datasets are split with 80% for training, 10% for eval (choosing the best model to save), and 10% for testing. Before cutting the samples to the 500 characters threshold, we filtered out samples smaller than 500 characters to only have samples of 500 characters. A small improvement here would be to use the \"min_new_token\" generation parameter so that we would not need to filter out smaller samples. We only used this parameter to generate the adversarial datasets. The samples of the true articles are the same across all datasets except for the discarded samples.\nThere is also a 4th auto-complete model dataset, \"round-robin,\" which is a mixture of 2500 samples from each of the other complete models' (Phi-2, Gemma, and Mistral) datasets.\nFor the adversarial datasets, we reuse these same datasets, but we generate the fake samples with a different prompt or generation parameter (see appendix A.4 for the adversarial prompts). In total, there is one adversarial version of each dataset per attack (i.e. number of datasets times number of attacks adversarial datasets)."}, {"title": "A.2 Prompts for Generating Data", "content": "For the auto-complete models, we only use the prefix as the prompt. For chat models, we use the prompt in table 4. Also, we force the first tokens of the output to be the prefix for that particular sample. This prevents the chat model from generating typical assistant-starting messages that would be too obvious to spot for the detectors (in practice, an attacker could also remove it easily). For Gemma-2B-it (Gemma chat), there is no system prompt in the chat template; we simply dropped the system prompt for that case."}, {"title": "A.3 Dataset Example", "content": "Below is an example of a true and an LLM-generated news article from the Zephyr dataset. As explained in the methodology, samples are regrouped by pairs of true and fake samples (ordered randomly within the pair). The true and fake samples in the pair start with the same 10-word prefix."}, {"title": "A.4 Attack Prompts", "content": "The idea behind the attacks we crafted to generate news articles evading the detectors is to generate news articles with a distribution of words looking more like the CNN news articles that were used to train the detectors. This is particularly true for the \"news prompt\" attack that asks the model to generate a CNN news-looking article. The same applies to the \"example prompt,\" which uses in-context learning to generate a more CNN news-looking article. The news prompt can be found below in 4 and the example prompt in 7.\nFor the \"tweet prompt,\" the idea is to generate text with a different distribution than news articles, which might confuse detectors trained on news articles. The \"paraphrasing prompt\" has a similar effect of modifying the distribution of words to make it more diverse than the original output. The tweet prompt can be found in 5 and the other paraphrasing prompt in 6."}, {"title": "B Training Details", "content": null}, {"title": "B.1 Hyperparameters for Training", "content": "We present in table 5 the hyperparameters used for training. The training was done either on the Nvidia A100 40GB or on the Nvidia V100 16GB, depending on the batch size, model size, and training method. All the trainings were done with 1 epoch. While we used only the models trained with full finetuning on the main part of the paper, we provide some results with different training methods in appendix D. The hyperparameters were chosen according to the hyperparameters used in the original papers of the different models with some adaptation according to the size of the models and testing that the training converges. A linear schedule with a 10% warmup was applied to the learning rate."}, {"title": "C Full Heatmap for Trained Detectors", "content": "You can find in figure 8 the TPR obtained for all trained detectors when testing them on all the datasets (it is a more complete version of figure 2). This plot uses the same metric as the main paper plots, i.e. we find thresholds on the eval set such that we target an FPR of at most 5%."}, {"title": "D ROC AUC Score with Different Training Methods", "content": "While we mainly tested trained detectors with full finetuning, we also tested and compared the results using different training methods. First, we tested freezing the LLM detector model's parameters and finetuning the classifier head (freeze base). Secondly, we tested finetuning the base LLM detector with the adapters PEFT method (see (Houlsby et al., 2019)). The results we show in this section are the same experiment as presented above in appendix"}]}