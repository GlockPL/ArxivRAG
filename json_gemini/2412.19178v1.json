{"title": "Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval", "authors": ["Yang Du", "Yuqi Liu", "Qin Jin"], "abstract": "Cross-modal (e.g. image-text, video-text) retrieval is an important task in information retrieval and multimodal vision-language understanding field. Temporal understanding makes video-text retrieval more challenging than image-text retrieval. However, we find that the widely used video-text benchmarks have shortcomings in comprehensively assessing abilities of models, especially in temporal understanding, causing large-scale image-text pre-trained models can already achieve comparable zero-shot performance with video-text pre-trained models. In this paper, we introduce RTime, a novel temporal-emphasized video-text retrieval dataset. We first obtain videos of actions or events with significant temporality, and then reverse these videos to create harder negative samples. We then recruit annotators to judge the significance and reversibility of candidate videos, and write captions for qualified videos. We further adopt GPT-4 to extend more captions based on human-written captions. Our RTime dataset currently consists of 21k videos with 10 captions per video, totalling about 122 hours. Based on RTime, we propose three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We further enhance the use of harder-negatives in model training, and benchmark a variety of video-text models on RTime. Extensive experiment analysis proves that RTime indeed poses new and higher challenges to video-text retrieval. We release our RTime dataset\u00b9 to further advance video-text retrieval and multimodal understanding research.", "sections": [{"title": "1 Introduction", "content": "Video-text retrieval has been widely used in various real-world scenarios, such as video search engines and video recommendation systems. It is more challenging than image-text retrieval as it requires understanding the visual semantics of multiple frames not only spatially but also temporally. In recent years, the introduction of large-scale vision-language pre-trained models [10, 13, 20, 23, 32, 34, 48, 54, 55], which learn cross-modality alignment through contrastive learning, has brought significant performance improvements to video-text retrieval. These models can be roughly divided into two types: one type focuses on transferring image-text pre-trained models to the video domain (e.g. CLIP4Clip [42], X-Pool [17], X-Clip [43]), and the other type focuses on utilizing existing video-text datasets (e.g. HowTo100M [44], WebVid [3]) and employing diverse pre-training objectives to perform video-text pre-training, such as Frozen [3], Internvideo [60], UMT [34], Vindlu [10], Violet [14], ALPro [31], etc.\nHowever, we wonder whether recent models have actually significantly improved video semantic understanding capabilities, especially in terms of temporal understanding. For example, in Figure 1, the only way to differentiate the two videos with opposite temporal semantics (open laptop vs. close laptop) is through temporal understanding. Such videos with very similar spatial appearance but very different temporal semantics can be considered as temporally harder-negatives of each other. Previous works [2, 6, 29, 57, 67] point out that there is a notable lack of a video-text benchmark"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Video-Text Benchmark Datasets", "content": "Various video-text retrieval benchmark datasets have been proposed through collecting videos from the internet and manually annotating with captions, event timestamps, and other relevant information. For example, MSR-VTT [65] includes 10,000 video clips, with 20 manually annotated textual descriptions for each clip, making it one of the widely adopted benchmarks in the video-text retrieval and video-language understanding domain. VATEX [58] selects videos from a subset of Kinetics-600 dataset [26] and annotates them with multi-lingual descriptions. ActivityNet-Caption [27] contains 20,000 YouTube videos, each annotated with descriptions and timestamps for events. DiDeMo [1], collected from Flickr, contains 26,892 video clips. In ActivityNet-Caption and DiDeMo, the video-text retrieval evolves into paragraph-video retrieval, where all descriptions of a video are concatenated into a single paragraph.\nAdditionally, some studies have recognized the limitations of widely used benchmarks in temporal evaluations and have attempted to construct benchmarks with a focus on temporal aspects. Hendricks et al. [19] concatenate clips of different events from the same video in DiDeMo [1] along with event descriptions. Lei et al. [29] reuse the Something-Something dataset [18] and propose SSV2-Label and SSV2-Template. Li et al. [35] sample videos from test set of MSRVTT [65] and VATEX [58], employed the GPT-assistant annotation framework to generate temporal counterfactual captions for the videos. In this work, we address such insufficiency in existing benchmarks and introduce a new dataset that emphasizes the temporal aspect of videos by including their harder-negative samples, the temporally reversed counterparts, using both manually and GPT-assisted data construction approach."}, {"title": "2.2 Video-Text Retrieval Methods", "content": "Cross-modal retrieval has been widely explored in previous works [5, 9, 22, 24, 37, 38, 47, 53, 61, 68]. Current video-text retrieval methods can be roughly divided into three types:\nOffline feature extraction and fusion. Offline feature extractors are the main components commonly used in early video-text retrieval methods. For example, MMT [15] employs multiple distinct models for feature extraction and utilizes a cross-modal transformer for fusion. VideoCLIP [64] utilizes S3D [63] to extract video features and applies contrastive learning to align video and text embeddings.\nTransferring image-text pretrained models. This type of methods utilizes pre-trained image-language models (e.g. ALBEF [33], CLIP [48], BLIP [32]) and transfers them to video retrieval tasks [17, 25, 39, 40, 42, 43]. For example, CLIP4Clip [42] leverages CLIP image encoder to encode videos frame by frame and designs modules for inter-frame information aggregation. TS2Net[39] introduces token shift and token selection modules, further enhancing the interaction of inter-frame information.\nVideo-text pre-trained models. This type of methods learns a video-text pre-trained model from large-scale video-text datasets. Various design of video encoders have been extensively explored [4, 16, 18, 21, 41, 51, 56, 66]. ClipBERT [30] pioneers the end-to-end video-text pre-training by sparsely sampling from videos. Frozen [3] adopts Timesformer [4]as video encoder for conducting joint pre-training on large-scale video-text and image-text datasets. VINDLU"}, {"title": "3 RTime: Novel Video-Text Benchmark", "content": "As currently available widely-used benchmarks are insufficient to comprehensively assess the capabilities of models on video understanding, especially temporal understanding, we propose to construct a new video-text retrieval benchmark dataset to meet the higher fine-grained and temporal-emphasized evaluation requirements. Manually building a new benchmark from scratch is very expensive and time-consuming, so we leverage the power of LLMs to improve efficiency and reduce the cost of dataset construction. We put human in the verification loop to control the data quality during the construction process. Specifically, we propose a top-down three-step data construction pipeline as illustrated in Figure 3, including seed activity list proposal, activity list enrichment, and video acquisition and annotation. Following this pipeline, we build a new fine-grained temporal-emphasized dataset for video-text retrieval, namely the \"reversed in time\u201d (RTime) dataset."}, {"title": "3.1 RTime Dataset Construction", "content": "To ensure the temporal emphasis and high quality of our dataset, we propose a top-down three-step data construction pipeline, which first progressively forms a comprehensive list of activities by leveraging human knowledge and world knowledge of LLMs (e.g. GPT-4). Each activity in the list may have its temporally opposite activity, so temporally harder-negatives can be constructed for each activity. We further leverage human capabilities and machine capabilities to acquire and annotate videos crawled from the internet based on the activity list. The specific steps in the pipeline are as follows."}, {"title": "3.1.1 Step 1: Seed Activity List Proposal", "content": "By filtering labels from existing action recognition datasets [18, 26, 45] and our brainstormed activity proposals, we initiate an atomic-level activity pair list, $A_h = \\{(a_i, \\bar{a}_i)\\}$, each containing an activity with a pronounced temporal emphasis, as well as its temporally opposite counterpart (e.g. (open, close)). To improve the diversity of the initial list, we leverage the world knowledge of GPT-4 to suggest more activities and their temporally opposite counterparts through few-shot in-context learning. Specifically, we provide GPT-4 with a few action pairs in $A_h$ and instruct it to generate more samples. We then manually curate the list of activities, eliminating those activity pairs that are either illogical or may be indistinguishable via video. We end up with 144 activity pairs A = $\\{(a_i, \\bar{a}_i)\\}_{i=1}^{144}$, containing 288 verb phrases."}, {"title": "3.1.2 Step 2: Activity List Enrichment", "content": "Directly using the activity list from step 1, which does not contain concrete objects, as queries to search for videos is not optimal. Therefore, leveraging"}, {"title": "3.1.3 Step 3: Video Acquisition and Annotation", "content": "Applying the enriched activity list as queries, we search for videos on the internet using search engines. We then go through a series of processes to filter low-quality videos, produce harder-negative samples by reversing videos, annotate videos, and rewrite annotations for diversity. We once again take full advantage of LLMs and human expertise to improve and ensure the quality of our dataset throughout the whole process.\nRaw Video Acquisition. Directly downloading videos based on L is sub-optimal because many of the retrieved videos do not match the query very well due to the limited performance of the search engine. To improve the overall quality, we paid to recruit seven workers to search videos with both $(a_i + n_j)$ and $(\\bar{a}_i + n_j)$ as queries using a video search engine. Then they filter out any activity that falls under the following conditions: 1) the activity can be identified without relying on temporal information. For example, \"hold basketball\" can be identified with a single static image, whereas \"taking off shoes\" requires temporal information. 2) the number of videos retrieved using this activity as a query is less than 50. 3) less than 50% of all the videos retrieved based on this activity correctly match this activity. After such a manual filtering process, we obtain a refined activity list $F\\in L$, containing approximately 800 activities with strong temporal nature. To further balance the distribution of objects, we calculate the frequency of"}, {"title": "3.2 Dataset Statistics", "content": "Table 1 compares our RTime dataset with other video-text datasets. RTime contains a total of 21,537 videos, each with one manually annotated caption and nine GPT-4 generated captions. Among all the videos, 16,530 videos have their temporally harder-negative counterparts, accounting for 76.8%. RTime is comparable in dataset scale to mainstream evaluation datasets. Compared to SSV2-Label and SSV2-Template, videos in RTime cover a broader range, addressing the domain limitation in these datasets. The activity list for RTime construction covers a wide range of natural activities with strong temporality. Some activities (verb-noun combinations) and a word-cloud based on the distribution of verb phrases are illustrated in the supplementary material which shows more balanced distribution of verb phrases in RTime. More importantly, text sentence lengths in RTime are longer than other similar datasets, indicating that our text annotations are finer-grained."}, {"title": "3.3 Benchmark Tasks Definition", "content": "Video-text retrieval requires the model to search for videos based on text queries (Text-to-Video retrieval, T2V) or to retrieve semantically matching textual descriptions based on video queries (Video-to-Text retrieval, V2T). We split our RTime dataset into training, validation, and testing subsets, containing 18,537, 1,000, and 2,000 videos, respectively. Note that we ensure that the raw video and its reversed counterpart are in the same subset. We propose three evaluation settings to assess video-text retrieval models.\nStandard Video-Text Retrieval (RTime-Origin). This setting is similar to other standard video-text retrieval benchmarks without harder-negatives. We only use raw videos $v_j$ with its human-written captions $t_j$ and exclude reversed videos $\\bar{v}_i$ in the test set, thus the"}, {"title": "4 Empirical Study on RTime", "content": "In this section, we carry out extensive empirical studies on the proposed benchmark tasks with RTime to gain a more in-depth understanding of challenges in video-text retrieval. We first introduce the model architecture and learning strategy (Sec. 4.1). Next we evaluate and analyze a variety of video-text retrieval methods on RTime benchmark (Sec. 4.2). Furthermore we investigate the factors that could impact the temporal understanding capability of models in RTime-Hard and RTime-Binary tasks (Sec. 4.3), and finally we present some additional ablation analysis (Sec. 4.4) and qualitative results (Sec. 4.5)."}, {"title": "4.1 Model Architecture and Learning Strategy", "content": "We use model with the two-stream architecture, which consists of a separate visual encoder and text encoder, followed by a cross-modal alignment module. The video and text encoders encode videos and texts into visual and textual features, respectively. The cross-modal alignment module involves a light transformer layer to fuse visual"}, {"title": "4.2 Benchmarking SOTA Models on RTime", "content": "We evaluate different SOTA models on the three RTime benchmark tasks: RTime-Origin, RTime-Both, and RTime-Binary, which require increasingly stronger temporal understanding capabilities. Specifically, we evaluate two image-text pre-trained models (i.e. CLIP [48] and BLIP [32]), one model pre-trained on video-text datasets with single frame (i.e. Singularity [29]), and three video-text models (i.e. VINDLU [10], UMT [34] and Internvideo2 [59]) in our zero-shot experiments. We also fine-tune two temporally-adapted image-text"}, {"title": "4.3 Ablation on Temporal Understanding", "content": "We investigate factors that could possibly impact the temporal comprehension performance based on our UMT-Neg model. We conduct experiments under two task settings, RTime-Hard, which comprehensively assesses the spatio-temporal understanding capability of models, and RTime-Binary, which focuses on evaluating the temporal understanding ability.\nImpact of leveraging harder negatives within same batch. One of the prominent features of our RTime is that many video-text pairs have harder-negative samples (the reversed counterpart), which may be beneficial for the model to enhance its temporal understanding ability during fine-tuning. To ablate the impact of our enhanced use of harder-negatives (i.e. placing positive and harder-negatives in the same batch), we compare the performances of the fine-tuned model with (UMT-Neg) and without (UMT) such strategy. As shown in the last two lines in Table 2, UMT-Neg does demonstrate better temporal understanding, achieving relative improvements of +2.9% (R@1 46.3 vs 45.0) on RTime-Hard and +6.4% (Acc 54.5 vs 51.2) on RTime-Binary.\nImpact of number of input frames. Learning of temporal information is theoretically closely related to the number of input frames used during fine-tuning. Our experimental results, shown in Table 3, demonstrate that increasing the number of input frames gradually improves the spatial-temporal understanding performance."}, {"title": "4.4 Additional Ablation Analysis", "content": "We further ablate other factors in data construction that may affect the video-text retrieval performance.\nImpact of rewriting in data construction. From Table 5, comparing row 1 with row 3 as well as row 2 with row 4, we observe that applying \"rewrite\" strategy, which adds extra rewritten captions for each video into the training data, significantly enhances the model's spatio-temporal understanding, demonstrating its effectiveness.\nImpact of reverse in data construction. From Table 5, comparing the row 1 with row 2 and row 3 with row 4, we observe that adding \"reversed\" videos and their text descriptions during training brings improvement, especially on temporal understanding. It is notable that with \"rewrite\" strategy, where more negative captions exists, the gain in RTime-Binary is more significant. This observation suggests the necessity for more temporal harder-negatives to enhance models' temporal understanding ability.\nImpact of test set scales on performance. Since the RTime-Hard test set (2000) is twice the size of RTime-Origin (1000), one might argue that, in addition to the difficulty of temporal understanding itself, the significantly lower performance on RTime-Hard"}, {"title": "4.5 Qualitative Results", "content": "We visualize some success and failure cases in choosing the correct video given the text query in Figure 6. In the top success case, UMT can correctly recognize that \"embrace\" occurs before \"rub their hands,\" but in the bottom failure case, it fails to distinguish the \"rise\" and \"fall\" of the mercury column in the thermometer. Distinguishing temporal semantics in videos with identical visual appearance poses a higher challenge for existing models."}, {"title": "5 Conclusion", "content": "This work aims to address the lack of temporal understanding evaluation in existing video-text retrieval benchmarks. We introduce RTime, a novel fine-grained temporal-emphasized video-text dataset, carefully constructed in a top-down three-step pipeline by leveraging the power of large language models and human expertise. We further establish three benchmark tasks: RTime-Origin retrieval, RTime-Hard retrieval, and RTime-Binary retrieval, which can support comprehensive and faithful evaluation of video understanding capabilities especially in terms of temporal understanding. The extensive experiment analysis confirms that our RTime indeed poses higher challenges to video-text retrieval. We hope that our work will draw more attention to the importance of temporal understanding and contribute to more broad advancement of video-language understanding tasks such as video captioning, videoQA and Multimodal Large Language Model evaluation."}]}