{"title": "S-RAF: A Simulation-Based Robustness Assessment Framework for Responsible Autonomous Driving", "authors": ["Daniel Omeiza", "Pratik Somaiya", "Jo-Ann Pattinson", "Carolyn Ten-Holter", "Jack Stilgoe", "Marina Jirotka", "Lars Kunze"], "abstract": "As artificial intelligence (AI) technology advances, ensuring the robustness and safety of AI-driven systems has become paramount. However, varying perceptions of robustness among AI developers create misaligned evaluation metrics, complicating the assessment and certification of safety-critical and complex AI systems such as autonomous driving (AD) agents. To address this challenge, we introduce Simulation-Based Robustness Assessment Framework (S-RAF) for autonomous driving. S-RAF leverages the CARLA Driving simulator to rigorously assess AD agents across diverse conditions, including faulty sensors, environmental changes, and complex traffic situations. By quantifying robustness and its relationship with other safety-critical factors, such as carbon emissions, S-RAF aids developers and stakeholders in building safe and responsible driving agents, and streamlining safety certification processes. Furthermore, S-RAF offers significant advantages, such as reduced testing costs, and the ability to explore edge cases that may be unsafe to test in the real world. Code for this framework is available\u00b9", "sections": [{"title": "Introduction", "content": "Responsible AI (RAI) development has gained increased attention lately as the development and application of sophisticated AI technologies continue to increase immeasurably with threats of harm to society (Allen and Weyl 2024). Beyond the assessment of technical capabilities (e.g., accuracy), the potential threats these technologies pose highlight the need for a more collective assessment of their purpose and process to mitigate associated risks while harnessing their potential for good. Of late, deep tech corporations, e.g., autonomous vehicle companies, seem to operate in silos regarding the communication of new knowledge, safety test cases, and reports. Many develop their own metrics and benchmarks for assessing their technologies, thus, making it difficult for regulators and other parties of interest to have a fair assessment of these technologies across board. We, therefore, advocate for the development of accessible frameworks for easier and more objective assessment of safety-critical responsible AI principles such as robustness, and environmental sustainability, among others.\nWe conceptualise RAI as the conscious effort in designing, developing, and deploying artificial intelligence (AI) systems in a way that maximises their benefits while minimising their risks to people and society at large.\nIf we are developing artefacts to act with some autonomy, then 'we had better be quite sure that the purpose put into the machine is the purpose which we really desire' as quoted in (Dignum 2019). In this light, the development of a framework for assessing each dimension of RAI emerges as a pivotal tool for responsible innovation, a broader subject guiding the development of technologies that align with societal needs and values (Stilgoe, Owen, and Macnaghten 2020; Jirotka et al. 2017). While purposes are very hard to technically account for, processes might be amenable. Thus, we developed a robustness assessment framework (S-RAF) that focuses on assessing how AD agents handle unforeseen situations during operation (Marcus 2020). AD is a safety critical domain with reports of accident cases (Reed 2024; Lavrinc 2016). Hence, the need for responsible innovation, involving rigorous assessment of safety-critical RAI principles such as robustness.\nWhile there might be a finite list of RAI principles in the academic literature, as a first step, we have focused on robustness (a safety critical concept in AD) and examined how efforts to improve robustness impact carbon emission,"}, {"title": "The Need for RAI Indicators", "content": "We focus on robustness and sustainability indicators in this section.\nRobustness One of the core principles of RAI is robustness (Floridi 2021). In contrast to symbolic Al systems, deep learning models often deployed in AD undergo training with extensive datasets, and their complex structures, which may consist of millions or billions of parameters, are not collectively readily interpretable by humans. Consequently, it is currently impractical to offer comprehensive assurances regarding the accurate performance of neural networks when they encounter input data significantly divergent from what was seen during training. Nonetheless, numerous AI applications (including AD) have critical security or safety implications, necessitating the ability to assess the systems' resilience when confronted with unforeseen events, whether they arise naturally or are deliberately induced by malicious actors (Berghoff et al. 2021). There are many methods of adversarial attacks such as those created using generative models like GAN, VAE (Bowles et al. 2018), and those created by adversarial perturbations like LBFGS (Szegedy et al. 2013), FGSM (Goodfellow, Shlens, and Szegedy 2014), Deep Fool (Moosavi-Dezfooli, Fawzi, and Frossard 2016). On the other hand, multiple works exist addressing this question of robustness against such attacks (Zhang et al. 2020; Berghoff et al. 2021). Some works (Huang et al. 2017; Gehr et al. 2018; Singh et al. 2019) have leveraged formal verification to provide robustness guarantees. These methods are faced with the limitations of scalability to the large neural networks often used in practice. Moreover, the defence provided is usually against artificial attacks at the pixel level. Hence, our notion of robustness in this paper is the ability of the AD agent to maintain a consistent behaviour or gracefully handle unforeseen events that might naturally occur in its operating environment. We seek to address questions such as, what happens when a sensor of an autonomous vehicle (AV) malfunctions? What happens when it is occluded by dirt or other materials? What happens when there is a sudden serious decline in weather conditions? These conditions are serious and can lead to fatal accidents when not effectively handled. For instance, when a camera fails, one expects the other cameras to make up for this failure, and do so without constituting safety risk or violating traffic rules. While the primary focus of this paper is on robustness, researchers have drawn connections between robustness and other indicators, especially sustainability. With many AI companies following the scaling law-which suggests that increasing the number of model parameters leads to improved performance-the demand for computational resources continues to grow. This escalating resource consumption consequently results in higher carbon dioxide CO2 emissions.\nCarbon Emission According to Strubell et al. (Strubell, Ganesh, and McCallum 2019), the training process of a single deep learning natural language processing (NLP) model on a GPU can result in the emission of approximately ~ 272, 155kg of CO2, comparable to the lifetime emissions of five cars. Similarly, Google's AlphaGo Zero generated 96 metric tonnes of CO2 during 40 days of training, equivalent to 1,000 hours of air travel or the carbon footprint of 23 American households (Van Wynsberghe 2021). The environmental impact of recent Generative AI models (e.g., GPT-3, ChatGPT, DALL-E, etc.) is even more concerning. Efforts to build more robust, multi-tasking models are observed to negatively impact environmental sustainability. Other than train time emissions, applications like ChatGPT, handling 11 million requests per hour, emit 12.8 thousand metric tons of CO2 annually, 25 times higher than GPT-3's training emissions (Chien et al. 2023). This has implications for AD, where such models are increasingly integrated. Research has assessed Al's energy use and CO2 emissions (Strubell, Ganesh, and McCallum 2019; Lacoste et al. 2019; Dhar 2020), focusing on the carbon impact of various Graphics Processing Units (GPUs). We adopt this approach and report CO2 emissions of benchmark AD agents by tracking their software processes."}, {"title": "Previous RAI Efforts", "content": "RAI Frameworks The development of RAI framework (including tools) is essential for all the different stages of AI development and deployment. Several efforts have been channelled toward developing RAI guidelines and frameworks over the years. Based on the survey by Berman et al. (Berman, Goyal, and Madaio 2024), the efforts include the provision of guidance for problem formulation and procurement decisions for the appropriate AI system for the given use case as equally argued in (Coston et al. 2023). It also includes ethical considerations for designing the systems, e.g., checklists (Lifshitz and McMaster 2020), and procedures for enabling participatory design (Gerdes 2022). In the actual machine learning workflow, dataset collection and training processes benefit from established ethical guidelines (Rhem 2023). Some of the frameworks are also useful for model post-training activities, e.g., for fairness assessment (Agarwal et al. 2018), model-card (Mitchell et al. 2019), and datasheet (Gebru et al. 2021) for model training documentation, dataset details, and model reporting. Another use case is the facilitation of an effective AI system auditing process (Krafft et al. 2021). These efforts are limited to data-driven models. Other than the CARLA Leaderboard (wayve 2023) which is mainly performance-focused, there seems to be a dearth of accessible frameworks and tools in AD that are focused on RAI principles. Our work contributes to the efforts by defining metrics for and providing a software tool for assessing the robustness of AD agents and understanding connections with carbon emissions.\nRAI Tools RAI tools have been developed for use by AI practitioners, of which a few focus on robustness (e.g., ART (Nicolae et al. 2018), FoolBox (Rauber, Brendel, and Bethge 2017), RobustBench (Croce et al. 2020)), explainability (e.g., Google What-if tool (Wexler et al. 2019), Captum (Kokhlikyan et al. 2020), IBM AI Explainability 360 (Mojsilovic 2019), etc), a handful on sustainability (e.g., CodeCarbon (Courty et al. 2024) and Eco2AI (Budennyy et al. 2022)), fairness (e.g., IBM Fairness 360 (Bellamy et al. 2019), Fairlearn (Microsoft and Contributors 2019), etc). Our work spans robustness, to include CO2 emission tracking and supports complex multi-modal AD agents.\nIn summary, building responsible AD agents requires additional efforts beyond adversarial robustness for models with single input modality, and should be done without trading off environmental sustainability. This is the gap that S-RAF potentially seeks to fill."}, {"title": "S-RAF: Robustness Indicators", "content": "We consider an agent to be robust if it can sustain performance in the presence of environmental disturbances, measurement noise, and data drift without infractions of traffic rules or collisions.\nRobustness against Environmental Disturbances\nRobustness against environmental disturbances is paramount to ensure the safe and reliable operation of vehicles. There are different types of disturbances peculiar to different sensor types:\ni. Camera Occlusion Environmental materials such as dirt, leaves, and snow accumulation on sensors, among others can cause camera occlusion. Camera occlusion occurs when the camera's field of view is obstructed, leading to incomplete or inaccurate perception data. In this disturbance situation, we assume that the disturbance occurs directly on the camera lens.\nFormally, let $I_{original}(x, y)$ represent the original image captured by the camera, where x and y denote the spatial coordinates of the image. Image occlusion can be represented by introducing an occlusion mask $M(x, y)$ that indicates the regions of the image affected by occlusion. The occluded image $I_{occluded}(x, y)$ is defined as:\n$I_{occluded}(x, y) = I_{original}(x, y) \\cdot (1 - M(x, y))$\nii. LiDAR Occlusion Similar to cameras, occlusion can arise in 3-dimensional (3D) Light Detection and Ranging (LiDAR) from environmental factors, including leaves, snow, dirt, etc. Such environmental elements have the potential to obscure the sensor's enclosure, leading to instances of occlusion where specific regions of the observed scene become inaccessible or exhibit data incompleteness.\nIf the sensor data is represented as $S(x, y, z)$, where x, y, and z are spatial coordinates of each LIDAR point, then occluded data can be written as:\n$S_{occluded}(x, y, z) = S_{original}(x, y, z) \\cdot (1 \u2013 M(x, y, z))$\nwhere $M(x, y, z)$ denotes the occluded section.\niii. Weather Disturbances Adverse weather conditions such as heavy rain, fog, snow, or glare can introduce noise and distortions in sensor data, affecting the vehicle's ability to perceive its surroundings accurately. Raindrops on camera or LiDAR sensors can scatter light and cause reflections, leading to blurred or obscured images. CARLA simulator already provides a simulation of these different weather conditions, and we leveraged this in our experiment.\nRobustness against Sensor Errors\nSensor noise resulting from hardware faults, calibration issues, or sensor drifts can lead to erroneous readings and misinterpretations of the environment, consequently impacting critical tasks such as obstacle detection, and planning. Detecting and mitigating sensor errors is crucial for ensuring the robustness and accuracy of AD systems.\ni. Camera Error Errors associated with faulty camera sensors, dead sensor elements, inaccurate pixel interpretation process, and intermittent electrical interference result in poor camera outputs (modelled with salt and pepper noise). This type of noise manifests as sparsely occurring white and black pixels in images, affecting the visual quality and integrity of the captured data. High-amplitude intermittent electrical interference, such as arcing on electrical contacts, can also contribute to the occurrence of salt and pepper noise in camera outputs. Formally, we add salt and pepper noise to each pixel independently. The resulting camera image $I_{noisy} (x, y)$ is defined as:\n$I_{noisy} (x, y) = \\begin{cases}\n0 & p \\\n255 & p \\\nI_{original} (x, y) & otherwise\n\\end{cases}$\nWhere p is the probability that an arbitrary pixel would have a value of 0. Understanding the sources of salt and pepper noise is crucial for developing effective noise removal techniques to enhance the quality of images captured by cameras in AD systems.\nii. LiDAR Error 3D LIDARs are incorporated with multiple channels to enable a higher vertical field of view (FOV), where each channel is a distinct laser beam. One example of hardware failure in 3D LIDARs is channel failure, where a particular beam stops functioning. To simulate such a fault, we remove sensor readings corresponding to arbitrarily selected channels. See examples of noisy and faulty sensor outputs in Fig. 2.\niii. Other Sensors Errors In addition to cameras and LiDARs, we introduced random noise, drawn from a uniform distribution, to the sensor data coming from positional measurement-related sensors such as; the Global Navigation Satellite System (GNSS), the Inertial Measurement Unit (IMU), and the speedometer. These sensors are naturally susceptible to faults due to hardware limitations, signal interference, or environmental conditions. Hence, assessing driving agents' robustness along these dimensions is important.\nGiven sensor data S, noised sensor data $\\hat{S}$, is obtained as:\n$\\hat{S} = S + n; n \\sim U(\u2212N, N)$\nwhere n represents added noise, U denotes uniform distribution, and N controls the magnitude of the noise.\nRobustness against Corner Cases\ni. Corner Cases Corner cases represent extreme situations that may not be encountered in regular traffic but have the potential to challenge the capabilities of AD systems. This includes situations where an actor violates traffic rules, e.g., a pedestrian crossing outside of a crosswalk, debris on the road, etc.\nIt could also be the data drift effect which occurs when there's a divergence between the test (or production) data distribution and the train data distributions. This potentially leads to model degradation and decreased accuracy. This could be a collective change in the behaviour of actors within a region, e.g., from being conservative to aggressive as the population density of actors increases in the region. It could also be the fading away of traffic signs. By monitoring data drift and implementing strategies to adapt models to changing data patterns, AD systems can maintain their effectiveness in diverse and evolving environments. Robustness can be ensured by utilising techniques such as drift detection, model re-calibration, and ongoing performance evaluation to ensure that the models remain robust and effective in the face of fluctuating environmental variables. We leverage CARLA's flexibility in defining environments and pedestrian behaviour to create a combination of these situations. We can create data shift scenarios if the details, e.g., the source of training data and its distribution are included in the datasheet."}, {"title": "S-RAF: Carbon Emission Indicator", "content": "It is challenging to obtain an accurate measure of the overall CO2 autonomous vehicles emits to the environment. However, we can estimate how much of CO2 the AI model that powers the vehicles constitutes at inference time. We estimated this by obtaining the carbon intensity (CI) value for the region from which the model is run, and multiplying this by the amount of energy (E) the process running the model used up: CO2 emissions (in Kg CO2Eq.) = CI \u00d7 E.\nThe Carbon Intensity (CI) of the consumed electricity is calculated as a weighted average of the emissions from the different energy sources that are used to generate electricity, including fossil fuels and renewables. In the work by (Courty et al. 2024), fossil fuels coal, petroleum, and natural gas are associated with specific carbon intensities based on a known standard amount of CO2 emitted for each kilowatt-hour of electricity generated. This is based on publicly available charts. Renewable or low-carbon fuels include solar power, hydroelectricity, biomass, geothermal, etc. The nearby energy grid contains a mixture of fossil fuels and low-carbon energy sources, called the Energy Mix. Based on the mix of energy sources in the local grid, the Carbon Intensity of the electricity consumed is calculated. The nearby grid is determined based on the location of the compute resource.\nE is the energy consumed by the computational infrastructure (both GPU and RAM) expressed in kilowatt-hours. See (Courty et al. 2024) for further implementation details."}, {"title": "Experiment", "content": "Traffic Setup\nThe traffic setup was done in CARLA simulator (Dosovitskiy et al. 2017). For the experiment reported in this paper, we created a complicated route and spawned multiple actors to roam around the town. The route consists of different road structures including junctions and intersections (based on the NHTSA (National Highway Traffic Safety Administration 2007) topology). Actors include different forms of vehicles spanning cyclists to trucks interacting in different forms with the agent being tested. This traffic setup is similar to those used in the previous CARLA AD Challenge (wayve 2023).\nDriving Agent Details\nWe selected three trained agents, LBC (Chen et al. 2020), NEAT (Chitta, Prakash, and Geiger 2021), and Interfuser (Shao et al. 2023), from the 2020, 2021, and 2022 CARLA Challenges, respectively based on the leaderboard results. We selected one agent from each year using two steps (i) sorting by driving scores (ii) selecting the top agent that provided enough details for easy reproducibility.\nThe authors of the selected agents were quite transparent by providing model weights, code, and other materials useful for running S-RAF. We refer to this as process-based transparency, where detailed information about the processes involved in the agent's development stage is made available. Other forms of transparency important for responsible autonomous driving include output-based transparency where the agent provide human-understandable explanations for their predictions, and method-based transparency which relates to the architecture used, e.g., modular or end-to-end (Omeiza et al. 2021). A modular architecture is assumed to offer better transparency. We examined the available technical materials (e.g., code base, research paper, etc) of the selected agents to see how they align with these different transparency dimensions. As this is mainly a qualitative assessment and quite subjective, we do not consider it an indicator in S-RAF.\nRegular Driving Score\nWe used the driving score metric to assess the driving performance of the agents. The driving score $D_i = R_iP_i$ is the product of the route completion and the infraction penalty. Here $R_i$ is the percentage of completion of the $i$ \u2013 th route and $P_i$ is the infraction penalty incurred during the $i$ th route. We track different types of infractions (e.g., collision with a vehicle, running a red light, etc.) in which the agent was involved. The infraction penalty score aggregates all of the infractions as a geometric series along a particular route. Agents start with an ideal 1.0 base penalty score, which is reduced each time an infraction is committed. Infraction penalty $P_i$ was computed as: $\\Pi_{I \\in W} (P_I)^{|W_i|}$, where W is a set of infractions, I is an infraction type in W, and $|W_i|$ is the number of $W_i$ infractions that occurred, and $P_I$ is the cost for infraction type l.\nRobustness Driving Score\nWe ran one route multiple times introducing each of the different types of disturbances in each run. For example, if the traffic disturbance was weather and there were three weather types, then the route would be run three times corresponding to the three different weather types. The run condition, in this case, is 'weather'. When a route has been run for the desired number of times:\n1.  The driving scores (i.e., after penalties have been applied) for the runs are grouped based on the type of traffic disturbances introduced (condition). For example, weather driving scores [...], camera noise driving scores [...],\n2.  The driving score for each type of traffic disturbance/condition j is obtained by selecting the minimum score across all runs k under condition j. from the runs under this condition:\n$D_j^* = \\min_k (D_k)$\nFor example, if a route was run in n different weather conditions, the driving score for condition weather would be the least driving score obtained after running the agent in the n different weather conditions. In the situation that more than one routes were used, the score from the route that produced the lowest score for the selected traffic disturbance type would be used.\n3.  A robustness ratio is computed per condition. Robustness ratio for j - th condition is the ratio of the driving score obtained when the disturbance is introduced and the driving score obtained in the normal/regular driving case where no disturbance was introduced, that is, $S_j = \\frac{D_j^*}{D}$\n4.  A robustness driving score for which agents are ranked is computed as by taking the mean of all $D_j^*$. $RDS = \\frac{1}{m} \\sum_{j=1}^{m} D_j^*$ where m is the total number of conditions.\nEstimating Carbon Emissions\nFor each run, we estimate the amount of carbon (in Kg CO2 Eq.) emitted as a result of running the agent. Note that only the system process running the agent is tracked. We provide the average CO2 emission for the runs as well as the average CO2 emission per second. The emissions estimation procedure is based on the codecarbon (Courty et al. 2024) implementation explained in the previous section.\nRobustness\nFrom Table 2, we observe that Interfuser had the highest driving score and overall robustness driving score. This is not surprising as it has the highest number of sensors for accurate perception and planning. Camera seems to be the most intolerant to faults as we observe the greatest decline when the camera was noised. Cameras are very important sensors for navigation. In fact, some AD companies are beginning to only rely on cameras for navigation. Robustness readings need to be put in perspective with average route completion (ARC) and driving score. For instance, while LBC seems to have impressive robustness scores for many of the runs, it should be noted that the agent was only able to complete 28% of each route on average. Its driving score is as well low. This information tells us that irrespective of the impressive robustness scores for many of the sensors, LBC doesn't seem to be a safe and performant agent.\nCarbon Emissions\nIt is challenging to assess how much emission a vehicle in its entirety contributes to the environment. However, we can estimate how much emission the underlying AI model that controls navigation constitutes. Hence, we estimated the amount of CO2 emitted when the AI model is run over time. For fair comparisons, we estimated average emissions per second (AEPS). From Fig. 4 and Table 2, we see that the default CARLA NPC Agent that uses the classical navigation algorithms (non-machine learning) constitutes the lowest CO2 emission. This agent doesn't utilise sensors, it rather uses ground truth information for planning. Hence, no robustness scores were assigned. The agent was useful to benchmark CO2 emissions. As AD agents get more sophisticated (with increased robustness), the amount of CO2 they emit increases."}, {"title": "Discussion and Limitations", "content": "We have drawn attention to the need for accessible frameworks with RAI indicators, starting with robustness. These frameworks serve as safety guardrails for AD agent development. We took a socio-technical approach in this work by first establishing the importance and the need for robustness indicators and then proposed a technical framework that offers metrics for robustness and environmental sustainability assessment (using CO2 emission as proxy). Our framework (S-RAF) focused on streamlining AD agents development process rather than the purpose for which the agent is developed. While the Responsible Research and Innovation (RRI) frameworks (e.g., AREA framework (Jirotka et al. 2017) underscore the importance of the purpose inputted into a machine, assessing such purposes is nearly intractable. S-RAF thus focuses on enhancing development processes to achieve safe and responsible driving agents. S-RAF improves over the conventional AI agents assessment approach that focuses on improving prediction accuracy with limited examples of critical edge cases by quantifying different aspects of robustness and environmental sustainability.\nS-RAF's robustness metric takes into account core safety critical sensors in an AV and questions the capability of the agent when an individual or a combination of these sensors malfunctions or when their sensing range is limited due to environmental factors. These cases are hard to obtain in the real world. But with S-RAF, we can simulate these cases affordably. Our results indicate an increasing trend in robustness and safe driving capability over the years. With the breakthrough in generative AI, where generalisable models are being developed, we believe that AD agents would witness a tremendous increase in capabilities. However, we advocate RAI principles and the use of S-RAF in the process.\nThe emission of green gases by vehicles have impacts on human safety (Ogur and Kariuki 2014). With S-RAF, AI developers have the option to discard models that emit excessive amounts of CO2. The limitation with S-RAF regarding sustainability is that it does not factor in the emissions caused during the manufacturing process of hardware components, e.g., electric vehicle batteries (Ji et al. 2012) which are notable for constituting CO2 emissions at manufacturing time. We saw that CO2 emissions increase as the agents get more performant. No surprises as this adheres to the scaling law (the larger the better). Should the development of extremely large models be stopped? This is an ongoing debate. But again, we encourage developers to consider RAI principles and also optimise for low S-RAF CO2 emissions.\nWe have only tested S-RAF on synthetic driving data from CARLA, this is one limitation of this work. Also, when S-RAF assesses agents, it indirectly emits CO2 as it runs the agents. However, this run is only for a limited time compared to when the models are already deployed and run for an extended period. Thus, S-RAF's use is justifiable as it helps to prevent the deployment of unsafe models. Another limitation of this work is that we have only tested on a handful of AD agents due to the scarcity of open-source AD agents. Hence, we have created a challenge website\u00b2 that encourages developers to submit their agents for assessment and receive feedback for improvements. This challenge would yield more examples from which we can further validate S-RAF while respecting the privacy and copyright agreements of the agent/model owners. That said, we encourage developers and corporate entities to embrace all transparency dimensions highlighted in this paper (process-based, method-based, and output-based) to facilitate easy assessment and potentially, increased safety."}, {"title": "Conclusion", "content": "We have argued for the need for an accessible framework for assessing AD agents. We developed S-RAF, a framework aimed at guiding developers in building responsible AD agents that are robust and environmentally friendly. S-RAF can equally support AV regulators and other authorised stakeholders in assessing AD agents. Through an experiment, we showed how indicators composed in S-RAF were developed, and we tested these indicators on benchmark AD agents in CARLA simulator. We saw improvements in robustness over time (from 2020 to 2022), while the amount of CO2 emissions has increased as the models got sophisticated over the years. Lastly, we discussed the implications of these findings and future research agenda."}]}