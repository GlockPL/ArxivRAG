{"title": "Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET", "authors": ["Fatih Aksu", "Fabrizia Gelardi", "Arturo Chiti", "Paolo Soda"], "abstract": "Accurate classification of histological subtypes of non-small cell lung cancer (NSCLC) is essential in the era of pre-cision medicine, yet current invasive techniques are not always feasible and may lead to clinical complications. This study presents a multi-stage intermediate fusion approach to classify NSCLC subtypes from CT and PET images. Our method integrates the two modalities at different stages of feature extraction, using voxel-wise fusion to exploit complementary information across varying abstraction levels while preserving spatial correlations. We compare our method against unimodal approaches using only CT or PET images to demonstrate the benefits of modality fusion, and further benchmark it against early and late fusion techniques to highlight the advantages of intermediate fusion during feature extraction. Additionally, we compare our model with the only existing intermediate fusion method for histological subtype classification using PET/CT images. Our results demonstrate that the proposed method outperforms all alternatives across key metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This non-invasive approach has the potential to significantly improve diagnostic accuracy, facilitate more informed treatment decisions, and advance personalized care in lung cancer management.", "sections": [{"title": "1. Introduction", "content": "Lung cancer is a leading cause of cancer-related deaths globally, with estimated age-adjusted incidence and mortality rates of 23.6 and 16.8 per 100,000 people, respectively [29]. Non-small cell lung cancer (NSCLC) accounts for 85% of primary lung cancers, with adenocarcinoma (ADC) and squamous cell carcinoma (SQC) being the most common subtypes [33]. The two primary histological subtypes not only have different biological characteristics and outcomes, but also different responses to targeted therapies and immunotherapies [7, 5]. In the context of early-stage NSCLC, a full histological examination of the primary tumour prior to surgery may be omitted in cases where there is a significant risk of biopsy-related complications and a compelling clinical indication of malignancy based on imaging and clinical findings. However, an accurate pathological diagnosis of the primary tumour is essential to determine prognosis and select the most effective therapeutic strategies in patients with clinical stage I-III disease [24]. Traditional methods of identifying these subtypes rely on tissue biopsy and histopathological examination, which are invasive and can carry significant risks for patients [8]. Moreover, such techniques often struggle with accuracy due to challenges like small tumor size, the tumor location near the lung's"}, {"title": "2. Related Work", "content": "Distinguishing between ADC and SQC has become essential in the era of targeted therapies for NSCLC [33]. Current diagnostic methods often rely on invasive procedures that can be both challenging and occasionally inaccurate, highlighting the need for non-invasive techniques to accurately classify NSCLC subtypes: for this reason, researchers and practitioners, supported by recent advances in AI, have proposed methods for recognizing histological subtypes extracting the necessary information from CT and/or PET scans e.g., [30, 25]. We can categorize these studies based on the imaging modality used (either CT, PET, or both) or the feature extraction method employed, which primarily consists of hand-crafted radiomic features or learned deep features. With respect to the imaging modality, we observe that the majority of these studies employ only CT scans. Among them, earlier works predominantly represented the information using hand-crafted features [37, 23], whereas recent approaches"}, {"title": "3. Materials", "content": "We combined three datasets one private and two publicly available totaling 714 subjects. The private dataset consists of 423 patients from the IRCCS Humanitas Research Hospital [19], selected based on a pathological diagnosis of NSCLC, a baseline [18F]FDG PET/CT scan, and subsequent surgery at the same facility. Exclusion criteria included histological types other than ADC or SQC, concomitant cancers, or a history of malignancy within three years before the NSCLC diagnosis, resulting in 312 ADC and 111 SQC cases.\nWe also included two public datasets. The NSCLC Radiogenomics dataset [3] comprises 211 patients from Stanford University and the Palo Alto Veterans Affairs Healthcare System. We selected 193 patients from this dataset, with 160 diagnosed with ADC and 33 with SQC, based on the same criteria as the private dataset. The Lung-PET-CT-Dx dataset [22] contains data from 355 patients who underwent lung biopsy and PET/CT: as before,"}, {"title": "4. Methods", "content": "We propose an end-to-end deep learning classification pipeline that takes raw CT and PET images as input and outputs the histological subtypes. The entire framework is depicted in Figure 1. Our pipeline starts with a pre-processing step shown in panel (a), where the raw scans are prepared for analysis by the network. The corresponding details are presented in Section 4.1. After the pre-processing, the images are fed into a custom 3D multimodal multi-fusion network for classification (panel (b) of Figure 1), with architectural details explained in Section 4.2. It utilizes a multi-stage fusion mechanism, in which fusion occurs repeatedly throughout the network, beginning in the early layers where feature maps still retain 3D spatial information. This approach allows us to leverage the spatial correlation between modalities. At each stage of our architecture, features are fused and then redistributed across the unimodal backbones within the same fusion block. This strategy enables the network to extract modality-specific features, effectively harnessing the complementary information provided by both imaging modalities in greater detail."}, {"title": "4.1. Pre-processing", "content": "CT and PET scans typically encompass large volumes of data, extending beyond the lungs and even outside the body. This vast amount of extraneous information presents challenges for deep learning models, as the size of the tumor-being relatively small in comparison to the entire scan-can hinder the networks' ability to effectively learn meaningful features. Additionally, variations in scan characteristics due to differences in the imaging machines further complicate the learning process. To address these issues, we designed a pre-processing pipeline that standardizes the scans and narrows the focus to the region of interest, thereby facilitating feature extraction for the networks. First, we standardized the photometric interpretation, addressing variations where some scans utilized higher intensity values for darker regions. Second, the intensities of CT scans were converted to Hounsfield Units (HU), while those of PET scans were transformed into Standard Uptake Values (SUV). Third, we applied linear interpolation to normalize the slice thickness and pixel spacing across all scans; we set the xyz dimensions to 0.977 mm \u00d7 0.977 mm \u00d7 3.27 mm, as these were the most prevalent within the dataset. Fourth, we aligned the CT and PET scans to ensure consistent origins and end-points. Fifth, we used a well-established segmentation algorithm [14] to segment the lungs from the CT images, and the resulting lung masks were applied to both CT and PET scans. In the sixth step, we clipped the pixel intensities of the CT scans to the range of [-1024, 1024] and the PET scans to [0, 20], and then we normalized all scans to have voxel values within the range of [0, 1], ensuring uniformity for subsequent processing by deep learning models."}, {"title": "4.2. Network architecture", "content": "We designed a network architecture that aims at extracting and fusing features from both imaging modalities (CT and PET) simultaneously, allowing the modalities to guide each other throughout the feature extraction process. To this goal, the overall network is organized in L stages, represented in violet in panel (b) of Figure 1, each"}, {"title": "4.2.1. Feature extraction block", "content": "The feature extraction blocks in our model are responsible for extracting deep features, each utilizing the basic block of well-established 3D ResNet architecture [32], which has proven effective across a wide range of domains. This block consists of a main branch and a residual branch, enabling the construction of deeper networks by mitigating the vanishing gradient problem. Consequently, it allows us to increase the number of feature extraction blocks (N) and stages (L) in our implementation. As depicted in panel (c) of Figure 1, the main branch begins with a 3 \u00d7 3 \u00d7 3 convolutional layer, followed by a batch normalization layer and a ReLU activation function. This is succeeded by another 3 \u00d7 3 \u00d7 3 convolutional layer, followed again by a batch normalization layer. In parallel, the residual branch includes a 1 \u00d7 1 \u00d7 1 convolutional layer, followed by batch normalization. This convolution is necessary to ensure the spatial dimensions align with the main branch, particularly when the main branch reduces the spatial dimensions. The outputs of the two branches are combined through element-wise summation, and a ReLU activation function is then applied. As shown in panel (b) of Figure 1, the output of a block can be passed either to the next feature extraction block (since there could be N block) or to a fusion block. In the initial block of each stage, the first convolutional layer uses a stride of 2 and produces 2 \u00d7 C feature maps, where C"}, {"title": "4.2.2. Fusion block", "content": "Our core idea is that CT and PET images offer complementary insights that can mutually enhance feature extraction and, hence, the information available for each patient. To this goal, we design each fusion block to combine data from both modalities by performing element-wise multiplication. Furthermore, we introduce two residual branches to incorporate the fused features back into the original unimodal data.\nAs illustrated in panel (d) of Figure 1, we denote as $CT_{in}$ and $PET_{in}$ the feature maps corresponding to the outputs of the previous basic blocks, which extract the features from CT and PET branches, respectively. These feature maps are first passed through a 1 \u00d7 1 \u00d7 1 convolutional layer with an output feature map size of 1, squeezing the feature maps along the channel dimension and yielding a single feature map for each modality. After a batch normalization step applied to both modalities, we introduce an element-wise multiplication between the two feature maps. The resulting fused feature map is then added to the original input feature maps, $CT_{in}$ and $PET_{in}$ with element-wise summation, producing the output maps $CT_{out}$ and $PET_{out}$. We formalize this fusion process by the following equations:\n$CT_{out} = CT_{in} \\oplus BN(f_{c}(CT_{in})) \\otimes BN(f'_{c}(PET_{in})) $ (1)\n$PET_{out} = PET_{in} \\oplus BN(f_{c}(CT_{in})) \\otimes BN(f'_{c}(PET_{in})) $ (2)\nwhere BN denotes the batch normalization, and $f_{c}$ represents a convolutional layer with a channel size of c and a kernel size of k \u00d7 k \u00d7 k. The symbols $\\oplus$ and $\\otimes$ are used to indicate element-wise addition and element-wise multiplication, respectively."}, {"title": "4.3. Network configuration", "content": "In the previous section, we reported that the network consists of N feature extraction blocks and L stages so that it is possible to identify the best configuration for a given task. To this end, we performed a grid search varying both N and L in the range [1,5]. We applied stratified 5-fold cross-validation after shuffling the three datasets included in this work, so that training, validation, and test sets account for 60%, 20%, and 20% of samples, respectively. Straightforwardly, the architecture search was conducted on the validation set. During these experiments we trained the models for 100 epochs, with the learning rate reduced by a factor of 0.1 every 25 epochs, using the Adam optimizer with class weights and an initial learning rate equal to 0.001. We evaluated performance using accuracy, the area under the receiver operating characteristic curve (AUC), and the geometric mean of sensitivity and specificity (Gmean). In particular, we consider AUC and Gmean since the a-priori class distribution is imbalanced. Indeed, the former focuses on the model's ranking ability, evaluating how well a model differentiates between the two classes regardless of the class distribution. The latter offers a balanced assessment of the two classes ensuring that the model performs well for both. The results of this grid search showed us that the best-performing architecture consists of three stages, with three blocks per stage. Furthermore, we found that models with only one stage performed the worst, regardless of the number of blocks, suggesting that multiple stages of fusion improve overall performance. Regarding the number of feature maps throughout the network, we selected 16 feature maps for the first convolutional layer in the first block of the initial stage. Consequently, all convolutional layers within the first stage output 16 feature maps, while the second stage outputs 32, and the third stage outputs 64. As a result, the final feature vector, combining information from both modalities, consisted of 128 features."}, {"title": "5. Results", "content": "We performed a series of experiments to assess the performance of our proposed model, aiming to compare our multimodal approach with seven competitors. They are: i) four unimodal models that rely exclusively on either CT or PET imaging, ii) two alternative fusion strategies, i.e., early and late fusion methods, and iii) the only existing study utilizing intermediate fusion of CT and PET images for histological subtype classification [25].\nIn case i), we tested four different unimodal models. Two, named as CT Branch and PET Branch, use the corresponding branch in our architecture, i.e., each one is a network with 3 stages, each containing 3 blocks, and a classification head at the end but without any fusion block. We selected the other two unimodal models from the literature: DetectLC [10] and LUCY [31], chosen because they classify histological subtypes using a 3D approach on CT lung volumes, similar to our unimodal approach in terms of input structure.\nCase ii) tests early and late fusion by using the same unimodal architecture as before, i.e., two branches with three stages, each containing three blocks and a classification head at the end of each branch, but without any joint fusion blocks. To set up the early fusion, we merged the CT and PET images before feeding them into the network using element-wise multiplication, as in our multimodal approach. For late fusion, we again used the two separate unimodal branches and then we averaged their output probabilities to make predictions during inference. Finally, in case iii) we compared our method with the only existing intermediate fusion approach for PET/CT histological subtype classification [25], which employs a single-fusion block that integrates the modalities after extracting individual feature vectors from each modality using two separate branches, as described in Section 2. Even though these branches are trained with a shared loss, the effect of each modality on the other remains at a high level of abstraction because the feature fusion occurs only once and before the classification head. In contrast, we have presented a multi-fusion method, where the fusions occur at various levels of the feature extraction hierarchy, preserving spatial correlations embedded in the feature maps and allowing for more extensive information sharing between modalities.\nTable 2 presents the results attained by such seven competitors and by our method, displaying the average accuracy, AUC, and Gmean scores computed across the five cross-validation runs. Focusing on the results of the unimodal approaches, we notice that our multimodal method outperforms these competitors in all metrics, except for accuracy in the case of LUCY. We also observe that the two unimodal competitors drawn from the literature achieve the lowest Gmean scores, suggesting a bias toward one class; in particular, DetectLC collapses into a single class across all folds. Although LUCY demonstrated the highest accuracy and a comparable AUC score, its Gmean ranks as the second-worst: this suggests that it struggles to effectively predict the minority class, i.e., SQC in our dataset, and LUCY's high accuracy is likely a result of significant bias toward the majority class. To deepen this analysis we also run the Wilcoxon signed-rank test on the AUC and Gmean scores, as these metrics better represent performance given the data's imbalance. In all pairwise comparisons for the Gmean score, our approach statistically differs from the unimodal approaches (p < 0.05). The same consideration holds for the AUC score, except when comparing with LUCY (p = 0.16). It is also worth noting two unimodal baselines (CT and PET branches) are derived from our network and, hence, their comparison with our approach is equivalent to an ablation test. This observation, together with the previous ones, supports the consideration that neither modality alone captures the full range of meaningful features necessary for an effective classification.\nLet us now turn our attention to the results of multimodal approaches in Table 2. We notice that our approach outperforms the other three in all metrics; the Wilcoxon signed-rank test shows that our performance statistically differs from all competitors for both AUC and Gmean (p < 0.05), except for Gmean in the case of late fusion where we get p = 0.0625, which is close to the significance threshold. Furthermore, early fusion achieves a lower Gmean score than the unimodal backbones, suggesting that data-level fusion might even harm the classification model. While late fusion shows some improvement over early fusion, both methods still fall short of the proposed intermediate fusion approach, which demonstrates that fusion during the feature extraction process performs better than at the data or decision level. Furthermore, the sharp increase in all metrics compared to [25] demonstrates that our multi-stage voxel-wise fusion approach performs significantly better than a single-stage fusion of extracted features, highlighting the advantage of integrating features at multiple stages to better capture the complementary information between CT and PET modalities."}, {"title": "6. Conclusion", "content": "In this work, we have presented a novel multimodal approach for histological subtype classification in NSCLC, utilizing an intermediate fusion method that automatically integrates CT and PET images at various network depths. Our experiments show the effectiveness of this approach in comparison to unimodal baselines and other fusion techniques, being also able to handle the challenges posed by dataset imbalance. By harnessing the complementary information from both imaging modalities, we underscore the value of multimodal fusion in medical image analysis to provide a more comprehensive understanding of tumor characteristics.\nDespite these promising results, our work has certain limitations that point to two potential areas for future development. First, we aim to refine the proposed multimodal approach by integrating genomics data with imaging features to improve classification accuracy and provide a more comprehensive understanding of tumor biology. Second, to enhance the model's generalizability, we plan to expand the dataset to include additional NSCLC subtypes."}]}