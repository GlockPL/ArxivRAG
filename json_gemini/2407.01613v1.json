{"title": "Self-adaptive weights based on balanced residual decay rate\nfor physics-informed neural networks and deep operator\nnetworks", "authors": ["Wenqian Chen", "Amanda A. Howard", "Panos Stinis"], "abstract": "Physics-informed deep learning has emerged as a promising alternative for solving\npartial differential equations. However, for complex problems, training these net-\nworks can still be challenging, often resulting in unsatisfactory accuracy and ef-\nficiency. In this work, we demonstrate that the failure of plain physics-informed\nneural networks arises from the significant discrepancy in the convergence speed of\nresiduals at different training points, where the slowest convergence speed dominates\nthe overall solution convergence. Based on these observations, we propose a point-\nwise adaptive weighting method that balances the residual decay rate across different\ntraining points. The performance of our proposed adaptive weighting method is com-\npared with current state-of-the-art adaptive weighting methods on benchmark prob-\nlems for both physics-informed neural networks and physics-informed deep operator\nnetworks. Through extensive numerical results we demonstrate that our proposed ap-\nproach of balanced residual decay rates offers several advantages, including bounded\nweights, high prediction accuracy, fast convergence speed, low training uncertainty,\nlow computational cost and ease of hyperparameter tuning.", "sections": [{"title": "1. Introduction", "content": "Benefiting from the rapid advancements in computational capabilities, optimiza-\ntion algorithms, and automatic differentiation technologies [1, 2], physics-informed\nneural networks (PINNs)[3] have emerged as a powerful tool for addressing both\nforward and inverse problems associated with partial differential equations (PDEs).\nIntegrating physical laws directly into their framework, PINNs optimize a loss func-\ntion that includes data and equation residuals to assist models in adhering to the\nunderlying physical principles. Building upon the foundational concepts of PINNs,\nand the deep operator network (DeepONet) architecture [4], physics-informed deep\noperator networks (PIDeepONets) extend these methodologies to the solution opera-\ntors of PDEs[5, 6, 7]. Both PINNs and PIDeepONets have enjoyed success in various\nsettings, but they can still face convergence/accuracy issues [8, 9, 10]. To mitigate\nthese issues, various enhancements to the plain PINN/PIDeepONet or approach have\nbeen proposed e.g., improved network model [8, 11], adaptive sampling [12, 13, 14],\ndomain decomposition [15, 16], multi-fidelity learning [17, 18, 19], continual learning\n[20], adaptive activation [21], and Fourier feature embedding [22, 23]. In the cur-\nrent work, we focus on a self-adaptive weighting method designed to dynamically\nbalance the training process of PINNs and PIDeepONets, aiming to improve their\nperformance.\nAdaptive weighting in PINNs and PIDeepONets has revolutionized the way these\nmodels handle the training process by dynamically adjusting the weights assigned to\ndifferent terms in the loss function. This method effectively balances the loss contri-"}, {"title": "2. Understanding the plain PINN failure mechanism", "content": "2.1. Physics-informed neural networks\nPhysics-informed neural networks (PINNs) aim at inferring a function $u(x)$ of\na system with (partially) known physics, typically defined in the form of partial\ndifferential equations (PDEs):\n$\\mathcal{R}(u(x)) = 0, \\qquad x \\in \\Omega$\n$\\mathcal{B}(x) = 0, \\qquad x \\in \\partial\\Omega$\n(1)\nwhere $x \\in \\mathbb{R}^{n_p}$ are $n_p$-dimensional spatial/temporal coordinates, $\\mathcal{R}$ is a general\npartial differential operator defined on the domain $\\Omega$ and $\\mathcal{B}$ is a general boundary\ncondition operator defined on the boundary $\\partial \\Omega$. For time-dependent problems, time\n$t$ is considered as a component of $x$, $\\Omega$ is a space-time domain, and the initial\ncondition will be assumed as a special boundary condition of the space-time domain.\nIn PINNs, the solution $u(x)$ is first approximated as $u_{nn}(x; \\theta)$ by a neural network\nmodel built with a set of parameters $\\theta$. The partial derivatives of $u_{nn}(x; \\theta)$ required\nfor the estimation of the action of the operators $\\mathcal{N}$ and $\\mathcal{B}$ are readily computed by\nautomatic differentiation. The training of the PINN is a multi-objective optimization\nproblem aiming to minimize the residuals of the PDE and the boundary conditions,\nwhich are usually evaluated on a set of collocation points. In the plain PINNs, the\noptimizing objective, namely the loss function, is defined as a linear combination of\nthe square of the following residuals:\n$\\mathcal{L}(\\theta) = \\frac{1}{N_R} \\sum_{i=1}^{N_R} \\mathcal{R}^2(x^R_i) + \\frac{1}{N_B} \\sum_{i=1}^{N_B} \\mathcal{B}^2(x^B_i)$\n(2)"}, {"title": "2.2. Training dynamic of unweighted PINNS", "content": "Let us first consider an unweighted loss function\n$\\mathcal{L}(\\theta) = \\sum_{i=1}^{N_R} \\mathcal{R}^2(x^R_i; \\theta) + \\sum_{i=1}^{N_B} \\mathcal{B}^2(x^B_i; \\theta)$\n(3)\nThe training process of physics-informed neural networks can be described using the\nNeural Tangent Kernel (NTK) theory [34]:\n$\\begin{bmatrix} \\frac{d \\mathcal{R}(x_R;\\theta(t))}{dt} \\\\ \\frac{d \\mathcal{B}(x_B;\\theta(t))}{dt} \\end{bmatrix} = -2\\eta K(t) \\begin{bmatrix} \\mathcal{R}(x_R;\\theta(t)) \\\\ \\mathcal{B}(x_B;\\theta(t)) \\end{bmatrix}$\n(4)\nwhere $\\eta$ is the learning rate and $K(t)$ is the NTK matrix at training iteration $t$\ndefined as\n$K(t) = \\begin{bmatrix} K_{RR}(t) & K_{RB}(t) \\\\ K_{BR}(t) & K_{BB}(t) \\end{bmatrix}$\n(5)"}, {"title": "3. Physics-informed machine learning with balanced residual decay rate", "content": "3.1. Physical insights of weighed PINNs\nTo build a general weighted PINN framework, we use a scale factor and a set of\nnormalized weights with each weight assigned to a residual term, namely\n$\\mathcal{L}(\\theta; \\textbf{w}, s) = s \\left( \\frac{1}{N_R} \\sum_{i=1}^{N_R} w^R_i \\mathcal{R}^2(x^R_i) + \\frac{1}{N_B} \\sum_{i=1}^{N_B} w^B_i \\mathcal{B}^2(x^B_i) \\right)$\n(15)\ns.t.\n$\\textbf{w} := \\frac{\\sum_{i=1}^{N_R} w^R_i + \\sum_{i=1}^{N_B} w^B_i}{N_R + N_B} = 1$\n(16)\nwhere $w^R_i > 0$ is the weight assigned to each residual term, $w^B_i > 0$ is the weight\nassigned to each boundary point, and $\\textbf{w}$ is the collection of these weights. The scale\nfactor $s$ is employed to scale all the weights, so that the formulation could cover all\nkinds of possible weight distribution.\nThe training process of physics-informed neural networks with weights can be\napproximated as follows:\n$\\begin{bmatrix} \\frac{d \\mathcal{R}(x_R;\\theta(t))}{dt} \\\\ \\frac{d \\mathcal{B}(x_B;\\theta(t))}{dt} \\end{bmatrix} = -2s\\eta K(t)diag(\\textbf{w})diag(1/\\textbf{N}) \\begin{bmatrix} \\mathcal{R}(x_R;\\theta(t)) \\\\ \\mathcal{B}(x_B;\\theta(t)) \\end{bmatrix}$\n(17)\nwhere $\\textbf{N} = [N_R, ..., N_R, N_B, ..., N_B]^T$ and the definition of $K(t)$ is the same as in\nSection 2.2. The terms $1/N_R$ and $1/N_B$ can be considered user-defined weight con-\nstants. The introduction of $\\textbf{w}$ and $s$ modifies the training dynamics. Empirical\nobservations indicate that increasing the weight at a specific training point enhances\nits convergence rate. Regarding the scale factor $s$, since $s$ and $\\eta$ appear together in\nEq. (17), $s$ functions similarly to $\\eta$. Both $s$ and $\\eta$ scale the eigenvalues of the matrix\n$K(t)diag(\\textbf{w})diag(1/\\textbf{N}), thereby influencing the overall convergence velocity. In the\nfollowing sections, we will elucidate the procedure for updating $\\textbf{w}$ and $s$ during the\nnetwork training process."}, {"title": "3.2. Balanced residual decay rate (BRDR)", "content": "As demonstrated in Section 2.3, the largest inverse residual decay rate dominates\nthe convergence speed of the training process. To address this issue, we assign a\nlarger weight to the training term with a larger inverse residual decay rate, namely,\n$\\textbf{w} \\propto c$\n(18)\nwhere $c$ is the collections of inverse residual decay rate $irdr$ for all the training terms.\nAt training iteration $t$, to meet the normalization constraint on $\\textbf{w}$ in Eq. (16), we\nset\n$\\textbf{w}^{\\text{ref}}_t = \\frac{c_t}{\\text{mean}(c_t)}$\n(19)\nTo filter out noise during network training, we employ an exponential moving average\nmethod to update the weights, namely,\n$\\textbf{w}_t = \\beta_w \\textbf{w}_{t-1} + (1 - \\beta_w) \\textbf{w}^{\\text{ref}}_t$\n(20)\nwhere $\\beta_w$ is a smoothing factor."}, {"title": "3.3. Maximizing learning rate", "content": "For the loss function defined in Eq. (15), the loss without scaling $\\mathcal{L}\\_s = \\mathcal{L}/s$\nsatisfies the following ordinary differential equation\n$\\frac{\\partial \\mathcal{L}\\_s}{\\partial t} \\approx \\nabla_\\theta \\mathcal{L}\\_s \\frac{d \\theta}{dt} + \\nabla_{\\textbf{w}} \\mathcal{L}\\_s \\frac{d \\textbf{w}}{dt} = \\nabla_\\theta \\mathcal{L}\\_s \\frac{\\theta}{t} + \\nabla_{\\textbf{w}} \\mathcal{L}\\_s \\frac{d \\textbf{w}}{dt}$\n$\\approx -\\nabla_\\theta \\mathcal{L}\\_s \\nabla_\\theta \\mathcal{L} + \\left[ \\frac{1}{N_R} \\mathcal{R}^2(x_R), \\frac{1}{N_B} \\mathcal{B}^2(x_B) \\right]^T \\frac{d \\textbf{w}}{dt}$\n(21)\nAccording to the constraint $\\overline{\\textbf{w}} = \\text{mean}(\\textbf{w}) = 1$ in Eq. (16), we have $\\text{mean}(d\\textbf{w}/dt) =$\n0. Therefore, we assume the second term in Eq. (21) is approximated as 0. Then,\nwe have"}, {"title": "3.4. Mini-batch training", "content": "Mini-batch training is commonly employed in physics-informed machine learn-\ning for several reasons: it helps manage computational resources by fitting training\nwithin memory constraints and enhances computational efficiency. It facilitates the\nuse of stochastic gradient descent and its variants [38, 39], enabling more frequent\nmodel updates which can lead to faster convergence and better handling of complex\nloss landscapes inherent in physics.\nIn this work, we restrict ourselves only to the scenario where all the training\npoints are pre-selected before training and a subset of training points is randomly\nchosen at each training step. The proposed method in Sections 3.2 and 3.3 can be\nextended to mini-batch training straightforwardly, except for the calculation of the\nexponential moving average of quantities. Since a specific training point cannot be\nchosen at every training step, it is too expensive to update the weights for all the\npoints at each iteration. It is efficient to only update the weights associated to the\nchosen points at each training step. We assume $\\Delta t_i$ is the training iteration interval\nfor the $i$th training point, which is the difference of current step and the last previous\nstep that the training point was chosen. The weights are then updated as\n$\\textbf{w}_{t,i} = \\beta^{i}_w \\textbf{w}_{t-\\Delta t_i} + (1 - \\beta^{i}_w) \\textbf{w}^{\\text{ref}}_{t,i}$\n(27)\nSimilarly, the exponential moving average $\\overline{\\mathcal{R}(t)}$ for the residual at the $i$th training\npoint is calculated as follows:\n$\\overline{\\mathcal{R}(t)} = \\beta^{i}_w \\overline{\\mathcal{R}(t - \\Delta t_i)} + (1 - \\beta^{i}_w) \\mathcal{R}(t)^4$\n(28)\nFor updating of the scale factor in Eq. (26), it is also calculated with exponential\nmoving average. However, the scale factor can be updated in Eq. (26) directly at"}, {"title": "3.5. Summary", "content": "Consider a physics-informed neural network $u_{NN}(x; \\theta)$ with training parameters\n$\\theta$, and a weighted loss function\n$\\mathcal{C}(\\theta; \\textbf{w}, s, \\Lambda) = s \\left( \\frac{\\Lambda_R}{N_R} \\sum_{i=1}^{N_R} w^R_i \\mathcal{R}^2(x^R_i) + \\frac{\\Lambda_B}{N_B} \\sum_{i=1}^{N_B} w^B_i \\mathcal{B}^2(x^B_i) \\right)$\n(29)\nwhere $\\textbf{w}$ represents point-wise adaptive weights allocated to collocation points $\\{x_i^R\\}_{i=1}^{N_R}$\nand $\\{x_i^B\\}_{i=1}^{N_B}$, $s$ is an adaptive scale factor, and $\\Lambda = \\{\\Lambda_R, \\Lambda_B\\}$ are user-defined weight\nconstants to normalize the residuals. According to our tests in Section 4, although\nsimply setting $\\Lambda = 1$ could be enough to achieve rather accurate results, setting a\nspecific $\\Lambda$ can significantly improve prediction accuracy. In the following, we refer to\ntraining with $\\Lambda = 1$ as BRDR training, and training with specifically defined $\\Lambda$ as\nBRDR+ training.\nIn summary, after the specification of the user-defined hyperparameters which\ninclude the learning rate $\\eta$, the smoothing factors $\\beta_s$ and $\\beta_w$, the batch sizes $N_{Rb}$\nand $N_{Bb}$, and the weight constants $\\Lambda_R$ and $\\Lambda_B$, the training process can proceed as\ndetailed in Algorithm 1. Note that the weights and scale factor are all initialized at 1,\nnamely $\\textbf{w} = s = 1$. Although Algorithm 1 is specifically employed for problems with\ntwo loss components (PDE loss and BC loss), its extension to multiple components\nis straightforward."}, {"title": "4. Numerical results for physics-informed neural networks", "content": "To validate the performance of the BRDR weighting method in training PINNs,\nwe tested it on three benchmark problems: the 2D Helmholtz equation, the 1D Allen-\nCahn equation, and the 1D Burgers equation. For comparison, we also report the\nerror from training with fixed weights, the soft-attention (SA) weighting method [10],\nand the residual-based attention (RBA) method [29]. The reported error is defined\nas the $L_2$ relative error:\n$\\epsilon_{L_2} = \\frac{||u - u_E||_2}{||u_E||_2}$\n(30)\nwhere $u$ and $u_E$ are vectors of the predicted solutions and the reference solutions on\nthe test set, respectively.\nIn this section, we use the mFCN network architecture (see Appendix A) with\n6 hidden layers, each containing 128 neurons. The hyperbolic tangent function is\nemployed as the activation function. The network parameters are initialized using the\nKaiming Uniform initialization [40]. Specifically, for a module of shape (out\\_features,\nin\\_features), the learnable weights and biases are initialized from $U(-\\sqrt{k}, \\sqrt{k})$, where\n$k = 1/in$\\_features. We use only the Adam optimizer [36] for updating the training\nparameters. Although the L-BFGS optimizer [41] can fine-tune network parameters\nfurther, it is known for its significant drawbacks, including high computational cost\nand instability, particularly in large-scale problems. Therefore, we have chosen not\nto use the L-BFGS optimizer. All training procedures described in this section are\nimplemented using PyTorch [1]. Training computations were performed on a GPU\ncluster, with each individual training run utilizing a single NVIDIA\u00ae Tesla P100\nGPU. All computations were conducted using 32-bit single-precision floating-point\nformat."}, {"title": "4.1. 2D Helmholtz equation", "content": "The 2D Helmholtz equation is defined as follows:\n$\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + k^2 u - q(x, y) = 0, \\qquad (x, y) \\in [-1, 1]^2$\n$u(x, \\pm 1) = 0, \\qquad x \\in [-1, 1]$\n(31)\n$u(\\pm 1, y) = 0, \\qquad y \\in [-1, 1]$\nwith the manufactured solution $u_E(x, y) = sin(a_1 \\pi x) sin(a_2 \\pi y)$, where $k = 1, a_1 = 1$\nand $a_2 = 4$ is considered. $q(x, y)$ is the source term defined by\n$q(x, y) = (k^2 - (\\alpha_1 \\pi)^2 - (\\alpha_2 \\pi)^2) sin(a_1 \\pi x) sin(a_2 \\pi y)$.\n(32)\nThe loss function is defined as follows:\n$\\mathcal{L}(\\theta; \\textbf{w}, s, \\Lambda) = s \\left( \\frac{\\Lambda_R}{N_R} \\sum_{i=1}^{N_R} w^R_i \\mathcal{R}^2(x^R_i) + \\frac{\\Lambda_B}{N_B} \\sum_{i=1}^{N_B} w^B_i \\mathcal{B}^2(x^B_i) \\right),$\n(33)\nwhere $\\mathcal{R}$ and $\\mathcal{B}$ represent the PDE operator and the boundary condition (BC) oper-\nator, respectively.\nThe choice of location of the training points and the BRDR training setup are\nprovided in Table 2. For fixed-weight training, the weights for both the boundary\nconditions (BC) and partial differential equations (PDE) are set to 1. For the soft-\nattention (SA) training setup, we follow the configuration given in reference [10]\nfor Adam training. The point-wise self-adaptive weights for BC and PDE are all\ninitialized using uniform sampling $\\mathcal{U}(0, 1)$, and the weights are updated with a fixed\nlearning rate of 0.005. For the residual-based attention (RBA) training setup, we\nuse the configuration provided in reference [29]. In this setup, the weights for BC\nare fixed at 100, and the point-wise self-adaptive weights for PDE are initialized at\n0. These weights are then updated with a decay rate of 0.9999, an offset of 0 and a\nlearning rate of 0.001."}, {"title": "4.2. 1D Allen-Cahn equation", "content": "The 1D Allen-Cahn equation is defined as follows:\n$\\frac{\\partial u}{\\partial t} = -5(u - u^3) - D \\frac{\\partial^2 u}{\\partial x^2} = 0, \\qquad (x, t) \\in [-1, 1] \\times [0, 1]$\n$u(x, 0) = x^2 cos(\\pi x), \\qquad x \\in [-1, 1]$\n(35)\n$u(\\pm 1, t) = 0, \\qquad t \\in [0, 1]$\nand we consider the case of viscosity $D = 1E - 4$.\nAs adopted in reference [29], we use Fourier feature transformation on $x$ to make\nthe network model automatically satisfy the periodic boundary condition. With 10\nFourier modes, the two-element input $x = (x, t)$ is lifted to a 21-element input $\\tilde{x}$\nbefore feeding it to the network with the following mapping:\n$\\tilde{x} = \\gamma(x) = [sin(\\pi B x), cos(\\pi B x), t]^T,$\n(36)\nwhere $B = [1, ..., 10]^T$."}, {"title": "4.3. 1D Burgers equation", "content": "The 1D Burgers equation is defined as follows:\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0, \\qquad (x, t) \\in [-1, 1] \\times [0, 1]$\n$u(x, 0) = - sin(\\pi x), \\qquad x \\in [-1, 1]$\n(38)\n$u(\\pm 1, t) = 0, \\qquad t \\in [0, 1]$\n$\\frac{\\partial u}{\\partial x}(0, t) = \\frac{\\partial u}{\\partial x}(1, t), \\qquad t \\in [0, 1]$\n(39)\nwhere $u$ is the flow velocity, and we consider the case with viscosity $\\nu = 0.01 / \\pi$.\nThe loss function is defined as follows:\n$\\mathcal{L}(\\theta; \\textbf{w}, s, \\Lambda) = s \\left( \\frac{\\Lambda_R}{N_R} \\sum_{i=1}^{N_R} w^R_i \\mathcal{R}^2(x^R_i) + \\frac{\\Lambda_B}{N_B} \\sum_{i=1}^{N_B} \\Lambda_B w^B_i \\mathcal{B}^2(x^B_i) + \\frac{\\Lambda_I}{N_I} \\sum_{i=1}^{N_I} w^I_i \\mathcal{I}^2(x^I_i) \\right)$\n(39)\nwhere $\\mathcal{R}, \\mathcal{B}$ and $\\mathcal{I}$ represent the PDE operator, the BC operator and the IC operator,\nrespectively.\nThe choice of location of the training points and the BRDR training setup are\nprovided in Table 2. For fixed-weight training, the weights for the IC, BC and"}, {"title": "4.4. Summary", "content": "As demonstrated in the three benchmarks, BRDR exhibits higher accuracy, con-\nvergence speed and lower uncertainty. Additionally, compared to the RBA method,\nBRDR weights can be applied to all training points within a unified framework (sim-\nilar to the SA method), eliminating the need to manually set weights for IC or BC\ncomponents. Manually choosing weights often requires extensive hyperparameter"}, {"title": "5. Numerical results for physics-informed operator learning", "content": "To further validate the performance of the BRDR weighting method, we applied\nit to training physics-informed deep operator networks (PIDeepONets) [5, 6, 7]. We\nhave studied its performance for for two operator learning problems, for the 1D wave\nequation and the 1D Burgers equation. In this setup, PIDeepONets are employed\nto learn the solution $\\mathcal{G}_0(u_0)(x)$ with respect to coordinates $x = (x, t)$ corresponding\nto the initial condition $u_0 = u_0(x)$. In the previous section on PINN training, we\ncompared our method with soft-attention (SA) [10], residual-based attention (RBA)\n[29], and fixed weights methods. However, since SA and RBA are not specifically\ndesigned for PIDeepONets, in this section, we compare our method with two specific\nweighting methods tailored for PIDeepONet: the Neural Tangent Kernel (NTK)\nweighting method [11], the Conjugate Kernel (CK) weighting method [35], as well\nas the fixed weights method. The reported error is defined as the average $L_2$ relative\nerror:\n$\\epsilon_{L_2} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{||u(x; u_0) - u_E(x; u_0)||_2}{||u_E(x; u_0)||_2}$\n(40)\nwhere $N$ is the number of test instances, and $u(x; u_0)$ and $u_E(x; u_0)$ are vectors of the\npredicted solutions and the exact solutions given the initial condition $u_0$, respectively.\nIn this section, we use the mDeepONet network architecture (see Appendix A),\nwhere both the trunk and branch networks are built with 7 hidden layers, each con-\ntaining 100 neurons. The hyperbolic tangent function is employed as the activation\nfunction. The network parameters are initialized using the Kaiming Uniform initial-\nization [40]. Specifically, for a module of shape (out\\_features, in\\_features), the learn-\nable weights and biases are initialized from $U(-\\sqrt{k}, \\sqrt{k})$, where $k = 1/in$\\_features.\nWe use only the Adam optimizer [36] for updating the training parameters with\na mini-batch training strategy. The batch size is set to 10,000, and the learning"}, {"title": "5.1. 1D Wave equation", "content": "The 1D wave equation is defined as:\n$\\frac{\\partial^2 u}{\\partial t^2} - C^2 \\frac{\\partial^2 u}{\\partial x^2} = 0, \\qquad (x, t) \\in [0, 1]^2$\n(41)\n$u(x, 0) = u_0(x), \\qquad x \\in [0, 1]$\n(42)\n$\\frac{\\partial u}{\\partial t}(x, 0) = 0, \\qquad x \\in [0, 1]$\n(43)\n$u(0, t) = u(1, t) = 0, \\qquad t \\in [0, 1]$\n(44)\nand we consider the case where the wave velocity is $C = \\sqrt{2}$. The initial condi-\ntion is set to $u_0(x) = \\sum_{n=1}^{5} b_n sin(n \\pi x)$. The exact solution is given by $u(x, t; u_0) =$\n$\\sum_{n=1}^{5} b_n sin(n \\pi x) cos(n \\pi C t)$. For training the PIDeepONet, 1000 random initial con-\nditions, each represented by 101 uniform $x$ points, are generated by randomly sam-\npling $\\{b_n\\}_{n=1}^{5}$ from the normalized Gaussian distribution. For each initial condition,\n100 boundary points are randomly sampled on the boundaries $x = 0$ and $x = 1$,\nand 2500 residual points are randomly sampled in the domain $(x, t) \\in [0, 1]^2$. For\ntesting, 500 random initial conditions are sampled, each represented by 101 uniform\n$x$ points. For these initial conditions, the solution values at $101 \\times 101$ uniformly\nsampled spatiotemporal points are computed using the exact solution."}, {"title": "5.2. 1D Burgers equation", "content": "The Burgers equation is defined as:\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0, \\qquad (x, t) \\in [0, 1]^2,$\n(46)\n$u(x, 0) = u_0(x), \\qquad x \\in [0, 1],$\n(47)\n$u(0, t) = u(1, t), \\qquad t \\in [0, 1],$\n(48)\n$\\frac{\\partial u}{\\partial x}(0, t) = \\frac{\\partial u}{\\partial x}(1, t), \\qquad t \\in [0, 1],$\n(49)\nwhere $\\nu$ is the viscosity. $u(x, t; u_0)$ is the solution at the point $x = (x, t)$ given the\ninitial condition $u_0(x)$. According to reference [11], the initial condition, $u_0(x)$, is\nsampled from the Gaussian random field $\\mathcal{N}(0, 25^2(-\\Delta + 5^2 I)^{-4})$. For training, 1000\nrandom initial conditions are sampled, each represented by 101 random $x$ points.\nFor each initial condition, 100 boundary points are randomly sampled on the bound-\naries $x = 0$ and $x = 1$, and 2500 residual points are randomly sampled in the\ndomain $(x, t) \\in [0, 1]^2$. For testing, 500 random initial conditions are sampled, each\nrepresented by 101 uniform $x$ points. For these initial conditions, the solutions at\n$101 \\times 101$ uniformly sampled spatiotemporal points are computed using the Chebfun\npackage [43], with the Fourier method for spatial discretization and a fourth-order\nstiff time-stepping scheme for marching in time.\nThe loss function is defined as follows:\n$\\mathcal{L}(\\theta; \\textbf{w}, s) = s \\left(\\frac{1}{N_R} \\sum_{k=1}^{N_{Rb}} w_{i_k}^R \\mathcal{R}^2(x_{i_k}^R) + \\frac{1}{N_I} \\sum_{k=1}^{N_{Ib}} w_{i_k}^I \\mathcal{I}^2(x_{i_k}^I) + \\frac{1}{N_B} \\sum_{k=1}^{N_{Bb}} w_{i_k}^B \\mathcal{B}^2(x_{i_k}^B) + \\frac{1}{N_{B'}} \\sum_{k=1}^{N_{B'b}} w_{i_k}^{B'} {\\mathcal{B'}}^2(x_{i_k}^{B'}) \\right),$\n(50)\nwhere $\\mathcal{R}, \\mathcal{I}, \\mathcal{B}$ and $\\mathcal{B'}$ represent the PDE operator, the initial condition (IC), zero-\norder boundary condition (BC) and first-order boundary condition (BC\\_x) operator,"}, {"title": "6. Conclusion", "content": "In conventional physics-informed machine learning, specifically in the area of\nphysics-informed neural networks (PINNs) and physics-informed deep operator net-\nworks (PIDeepONets), the loss function is a linear combination of the squared resid-"}, {"title": "Appendix A. Network architectures", "content": "The modified fully-connected network (mFCN) introduced in [8], which has demon-\nstrated to be more effective than the standard fully-connected neural network. A\nmFCN maps the input $\\textbf{x}$ to the output $\\textbf{y}$. Generally, a mFCN consists of an input\nlayer, $L$ hidden layers and an output layer. The $l$-th layer has $n_l$ neurons, where\n$l = 0, 1, .., L, L+1$ denotes the input layer, first hidden layer,..., $L$-th hidden layer\nand the output layer, respectively. Note that the number of neurons of each hidden\nlayer is the same, i.e., $n_1 = n_2 = ... = n_L$. The forward propagation, i.e. the function"}]}