{"title": "Temporal Reversed Training for Spiking Neural Networks with Generalized Spatio-Temporal Representation", "authors": ["Lin Zuo", "Yongqi Ding", "Wenwei Luo", "Mengmeng Jing", "Xianlong Tian", "Kunshan Yang"], "abstract": "Spiking neural networks (SNNs) have received widespread attention as an ultra-low energy computing paradigm. Recent studies have focused on improving the feature extraction capability of SNNs, but they suffer from inefficient inference and suboptimal performance. In this paper, we propose a simple yet effective temporal reversed training (TRT) method to optimize the spatio-temporal performance of SNNs and circumvent these problems. We perturb the input temporal data by temporal reversal, prompting the SNN to produce original-reversed consistent output logits and to learn perturbation-invariant representations. For static data without temporal dimension, we generalize this strategy by exploiting the inherent temporal property of spiking neurons for spike feature temporal reversal. In addition, we utilize the lightweight \"star operation\" (element-wise multiplication) to hybridize the original and temporally reversed spike firing rates and expand the implicit dimensions, which serves as spatio-temporal regularization to further enhance the generalization of the SNN. Our method involves only an additional temporal reversal operation and element-wise multiplication during training, thus incurring negligible training overhead and not affecting the inference efficiency at all. Extensive experiments on static/neuromorphic object/action recognition, and 3D point cloud classification tasks demonstrate the effectiveness and generalizability of our method. In particular, with only two timesteps, our method achieves 74.77% and 90.57% accuracy on ImageNet and ModelNet40, respectively.", "sections": [{"title": "Introduction", "content": "Recently, brain-inspired spiking neural networks (SNNs) have received widespread attention. Unlike traditional artificial neural networks (ANNs), which transfer information using intensive floating-point values, SNNs transfer discrete 0-1 spikes between neurons, providing a more efficient neuromorphic computing paradigm (Yao et al. 2023). In addition, spiking neurons, which simulate the dynamics of biological neurons over multiple timesteps, can effectively extract temporal features (Kim et al. 2023). These superior properties have led to the application of SNNs to a variety of spatio-temporal tasks such as object recognition, detection, generation, and natural language processing (Su et al. 2023; Kamata, Mukuta, and Harada 2022; Bal and Sengupta 2024).\nTo improve the performance of SNNs, researchers have made considerable efforts to enhance their feature extraction ability. For instance, the temporal properties of SNNs are optimized through heterogeneous timescales (Chakraborty et al. 2024), batch normalization (BN) methods adapted to the temporal dimension (Duan et al. 2022; Jiang et al. 2024b), and improved neuron dynamics (Ponghiran and Roy 2022). Alternatively, the spatial properties of SNNs are continuously improved with sophisticated network architectures (Yao et al. 2023). However, glory comes with remaining problem that these methods introduce additional computational complexity and negatively affect the inference efficiency. Although there exist alternative methods that only optimize the training process without affecting inference, the performance of these methods is too limited to unleash the full potential of SNNs (Zhang et al. 2024; Zuo et al. 2024). Therefore, how to maximize the spatio-temporal performance of SNNs without compromising the efficiency is an ongoing issue that still deserves attention.\nIn this paper, we propose a simple yet effective temporal reversed training (TRT) method to improve the spatio-temporal performance of SNNs. We perturbed the SNN during training, pushing it to be immune to these perturbations and to focus on generalizable features. Specifically, for the temporal task, we propose to perturb the inputs with temporal reversal. During training, the SNN simultaneously takes both original and reversed inputs and generates the corresponding pair of outputs. We encourage this pair of outputs to be as similar as possible, allowing the SNN to learn time-invariant generalized spatial representations on the one hand, and perturbation-insensitive stable temporal representations on the other hand. For static tasks without temporal concepts, we utilize the inherent temporal properties of spiking neurons to reverse the encoded temporal spikes to generate the corresponding output pairs. This makes our method simple and versatile in a variety of task scenarios. The perturbation occurs only during training without any additional inference overhead. At first glance, our method bears some resemblance to siamese learning, where different transformed data are fed into an ANN to produce consistent representations (Chen and He 2021; Wang et al. 2022). However, siamese learning relies on complicated data augmentation strategies, whereas our method is more straightforward and versatile by exploiting the inherent temporal properties"}, {"title": "of SNNs. From another perspective, the process of continuously seeking consistency between the original and temporally reversed outputs can be viewed as distillation learning (Hinton, Vinyals, and Dean 2015), which further supports the performance advantages of our method.", "content": "Moreover, we employ the lightweight \"star operation\" (element-wise multiplication) to hybridize the high-dimensional original and temporally reversed features, expanding the implicit dimensions and prompting the SNN to make correct predictions for the hybrid features. The hybridization further perturbs the temporal dimension and disrupts the spatial feature map, which can be considered as a regularization of high-dimensional features (visualization in Appendix D), allowing the SNN to learn latent representations that are insensitive to spatio-temporal perturbations, thus improving its generalization ability. However, direct \"star\" hybridization of binary spikes would result in severe information loss. To alleviate this problem, we convert discrete spikes to spike firing rate with a value range of [0, 1] and perform spike firing rate \u201cstar\u201d hybridization. In this way, we can enhance the performance of the model with only a negligible multiplication overhead during training.\nTo confirm the effectiveness of our method, we conducted extensive experiments using VGG, ResNet, Transformer, and PointNet architectures on static object recognition, neuromorphic object/action recognition, and 3D point cloud classification tasks. The experimental results show that our method achieves consistent performance gains across these tasks, datasets, and model architectures, with excellent generalizability. In summary, our contributions are as follows:\n\u2022 We propose to temporally reverse the input/spike features and prompt the SNN to produce consistent outputs to enhance its spatio-temporal feature extraction capability.\n\u2022 We propose to hybridize high-dimensional original and reversed features with a simple \"star operation\" to enable the SNN to learn generalized spatio-temporal features.\n\u2022 Extensive experiments on static/neuromorphic object/action recognition and 3D point cloud classification confirm the effectiveness and versatility of our method. Compared to existing methods, our method exhibits better performance without compromising inference."}, {"title": "Related Work", "content": "Spiking Neural Network. To train high-performance SNNs, indirect training based on ANN-to-SNN conversion (Wu et al. 2022; Hao et al. 2023) and direct training method based on surrogate gradient (Wu et al. 2018; Guo et al. 2024; Qiu et al. 2024) have achieved remarkable results. In addition, improved BN strategies (Duan et al. 2022; Jiang et al. 2024b), neuron dynamics (Ponghiran and Roy 2022; Ding et al. 2023; Wang and Yu 2024), and sophisticated architectures borrowed from ANNs (Yao et al. 2023; Li et al. 2024) further boost the performance of SNNs. However, these methods entail additional inference overhead that undermines the central energy advantage of SNNs. Some methods only modify the training of the SNN, preserving low-energy inference (Zhang et al. 2024; Zuo et al. 2024).\nHowever, the performance of these methods is still suboptimal, so further exploration of efficient and high-performance SNNs is still necessary. Compared to existing methods, our method is simple, effective, and architecture- and task-agnostic, with superior generalizability.\nSiamese Learning. For a given input, siamese learning uses data augmentation to transform it into two different views and increase the similarity between the outputs generated by the two (Chen and He 2021; Wang et al. 2022). This can facilitate the neural network to learn consistent features that are invariant to data transformations. However, this method is extremely sensitive to data augmentation strategies. For our method, we avoid the tedious process of data augmentation search and use the inherent temporal property of SNNs and temporal data for perturbation to improve model performance. From another perspective, our method can be seen as an extension of siamese learning in SNNS, exploiting in particular their inherent temporal properties.\nKnowledgw Distillation. Our method pushes the original and temporally reversed outputs of the SNN to be as similar as possible, which can be considered as a knowledge distillation strategy (Hinton, Vinyals, and Dean 2015). Unlike traditional distillation, we utilize temporal properties to allow a single SNN to output both \u201cteacher\u201d and \u201cstudent\" signals, similar to self-distilling learning in ANNs (Zhang et al. 2019; Yuan et al. 2020). The perturbation-free original ouput continuously guides the temporally reversed ouput, driving our SNN to learn perturbation-invariant features, which underpins the performance advantage of our method."}, {"title": "Method", "content": "This section first introduces the basic spiking neuron model, and then illustrates how simple temporal reversal and \"star operation\" induces the SNN to learn generalized spatio-temporal representations. Finally, we provide the detailed training algorithm for the TRT method."}, {"title": "Spiking Neuron Model", "content": "Spiking neurons iteratively receive input currents, accumulate in membrane potentials, and generate spikes. For the most commonly used leaky integrate-and-fire (LIF) model (Wu et al. 2018), the dynamics of the accumulating membrane potential can be expressed as:\n$$H_i^{(l)}(t) = (1-\\frac{1}{\\tau})H_i^{(l)}(t-1) + I_i^{(l)}(t),$$\nwhere H and I denote the membrane potential and afferent current, respectively, I and i are the layer and neuron index, t is the timestep, and \u03c4 is the time constant controlling the degree of leakage of membrane potential.\nWhen the membrane potential $H_i^{(l)}(t)$ reaches the firing threshold v, the spiking neuron will generate a spike $S_i^{(l)}(t)$ and reset the membrane potential:\n$$S_i^{(l)}(t) = \\begin{cases}\n1, & H_i^{(l)}(t) \\geq v \\\\\n0, & H_i^{(l)}(t) < \\nu\n\\end{cases},$$\n$$H_i^{(l)}(t) = H_i^{(l)}(t) - S_i^{(l)}(t)v.$$"}, {"title": "Temporal Reversal Perturbation", "content": "In this section we present how to perform temporal reversal perturbation to improve the spatio-temporal performance of SNNs for temporal and static data, respectively.\nInput Temporal Reversal. Without loss of generality, we denote the input data with temporal properties as $X = \\{x_1,x_2,...,x_T\\} \\in \\mathbb{R}^{T \\times B \\times C_i \\times H_i \\times W_i}$, where T, B, $C_i$, $H_i$, and $W_i$ are the time, batch, input channel, height and width sizes, respectively. Typically, the temporal input X and the temporal dimension of the SNN are aligned, i.e., $x_t$ is input to the SNN at timestep t and ultimately produces the output $o_t$. In addition to the original input X, we use temporal reversal to additionally generate the temporally reversed input $\\hat{X} = \\{\\hat{x}_1,\\hat{x}_2,...,\\hat{x}_T\\}$ for perturbation. As shown in Fig. 1 (a), this temporal reversal is achieved by simply flipping the temporal index of the input data X, i.e., $\\hat{x}_t = x_{T+1-t}$, without laboriously selecting data augmentations to generate additional data views as in siamese learning (Chen and He 2021; Wang et al. 2022). At each timestep t, $\\hat{x}_t$ is fed into the SNN to produce the temporally reversed output $\\hat{O}_t$.\nFeature Temporal Reversal. Input temporal reversal can only be used for tasks with inherent temporal properties, such as neuromorphic or video data. To make this temporal reversal to be effective for static tasks without inherent temporal properties, we further propose feature temporal reversal. For static data $x \\in \\mathbb{R}^{B \\times C_i \\times H_i \\times W_i}$, SNNs typically input data repeatedly at each timestep and encode it as spikes through the first spiking neuron layer. We denote the primary features after spike encoding by $F = \\{f_1, f_2,..., f_T\\} \\in \\mathbb{R}^{T \\times B \\times C \\times H \\times W}$, where $f_t$ represents the encoded spikes at timestep t. With this, we take advantage of the spiking neuron dynamics to transform the static data x into spatio-temporal spikes F with the temporal dimension. We then apply temporal reversal to the spike feature F and obtain the temporally reversed feature $\\hat{F} = \\{\\hat{f}_1, \\hat{f}_2,..., \\hat{f}_T\\}$, where $\\hat{f}_t = f_{T+1-t}$, as shown in Fig. 1 (b). The temporal reversed feature $\\hat{F}$ is propagated further forward in the SNN to produce the final temporally reversed output $\\hat{O} = \\{\\hat{o}_1, \\hat{o}_2,......,\\hat{o}_T\\}$.\nPerturbation-Invariant Learning. Through input/feature temporal reversal, we can perturb the temporal dimension of the SNN, regardless of whether the input is in-"}, {"title": "herently temporal or not. To motivate the SNN to learn perturbation-invariant spatio-temporal features, we impel the temporal-reversed output $\\hat{O}$ to be as similar as possible to the original output O. As shown in Fig. 1 (c), we increase the similarity between the two by imposing a consistency loss $L_{con}$.\nWe illustrate the consistency loss in detail with a C-way classification task. For the outputs O and $\\hat{O}$ of the SNN, the corresponding logits are given as:\n$$p_j = \\frac{e^{z_j/T_{tem}}}{\\sum_{c=1}^{C} e^{z_c/T_{tem}}}, \\;\\;\\;\\; \\hat{p}_j = \\frac{e^{\\hat{z}_j/T_{tem}}}{\\sum_{c=1}^{C} e^{\\hat{z}_c/T_{tem}}},$$", "content": "where $z = \\frac{1}{T}\\sum_{t=1}^{T} o_t, \\; \\hat{z} = \\frac{1}{T}\\sum_{t=1}^{T} \\hat{o}_t$ are the rate-decoded outputs and the subscript j denotes the j-th class. $T_{tem}$ is the temperature scaling hyperparameter used to smooth the logit, which is set to 2 in this paper. We use KL divergence to push the logit of the reversed output to be consistent with the logit of the original output:\n$$L_{con} = T^2KL(p || \\hat{p}) = T^2 \\sum_{j=1}^{C} p_j log (\\frac{p_j}{\\hat{p}_j}).$$\nThus, as the SNN is trained, both task loss (cross-entropy loss $L_{CE}$) and consistency loss $L_{con}$ contribute to the optimization of the parameters:\n$$L = L_{CE}(O,Y) + L_{con}(O, \\hat{O}),$$\nwhere Y is the ground-truth label."}, {"title": "Feature Hybridization Perturbation", "content": "To further improve the generalization of the features learned by the SNN, inspired by the regularization strategy (Srivastava et al. 2014), we propose to hybridize original and temporally reversed features for perturbation. The simple \u201cstar operation\u201d (element-wise multiplication) can significantly increase the implicit dimensionality of ANN features, and shares a philosophy with kernel functions (Ma et al. 2024; Shawe-Taylor and Cristianini 2004). Therefore, we propose to perturb the original and temporal reversed features in the SNN with the \u201cstar operation\u201d to serve as a spatio-temporal regularization of the high-dimensional features. However, due to the binary nature of the spike, the \"star operation\" does not contribute to dimensionality expansion in SNNs, but instead causes severe information loss. In the following, we will analyze this problem and make the \u201cstar operation\" in SNNs feasible by converting spikes into firing rate.\nInformation Loss in SNNs with Star Operation. For brevity, similar to (Ma et al. 2024), we write the \"star operation\" as $(W_1^T X + B_1) * (W_1 X + B_2)$, representing the fusion of the input feature X by element-wise multiplication after nonlinear transformation with weights $W_1, W_2$ and biases $B_1, B_2$. Representing X = $\\begin{bmatrix}\n  x^1 \\\\\n  x^2 \\\\\n  ... \\\\\n  x^{d+1}\n\\end{bmatrix}$ in matrix form, the star operation becomes $(W_1^T X) * (W_1 X)$.\nWe focus on the ANN scenario with one output channel and a single-element input, i.e., consider $w_1, w_2$, and $x \\in \\mathbb{R}^{(d+1)\\times 1}$, where d denotes the input channel number"}, {"title": "(which can be naturally extended to scenarios with multiple output channels and multiple input elements). The \u201cstar operation\" can be rewritten as:", "content": "$$\\begin{aligned}\n&\\sum_{i=1}^{d+1} w_i^1 x_i * \\sum_{j=1}^{d+1} w_j^1 x_j = (\\sum_{i=1}^{d+1} w_i^1 x_i)^2 \\\\\n&= (w_1^1 x_1 + w_2^1 x_2 + ...+w_{d+1}^1 x_{d+1})^2 \\\\\n&= \\sum_{i=1}^{d+1} \\sum_{j=1}^{d+1} w_i^1 w_j^1 x_i x_j \\\\\n&= \\sum_{i=1}^{d+1} (w_i^1)^2 (x_i)^2 + \\sum_{i \\neq j}^{d+1} w_i^1 w_j^1 x_i x_j \\\\\n&= \\alpha_{(1,1)}x^1x^1 + ... + \\alpha_{(i,j)}x^ix^j +...+ \\alpha_{(d+1, d+1)}x^{d+1}x^{d+1} \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad (\\frac{(d+2)(d+1)}{2} \\text{ items})\n\\end{aligned}$$\nwhere i, j are the channel indices and \u03b1 is the coefficient of each element:\n$$\\alpha_{(i,j)} = \\begin{cases}\n(w_i^1)^2 & \\text{if } i = j \\\\\nw_i^1 w_j^1 & \\text{else}\n\\end{cases}$$\nAs a result, the \"star operation\" in ANNs is able to transform the d-dimensional feature x into $\\frac{(d+2)(d+1)}{2}$ distinct elements, each of which, except $a_{d+1,:}x^{d+1}x$, is nonlinearly associated with x, serving as a dimensionality expansion.\nHowever, unlike ANNs, there are negative consequences of directly using the \u201cstar operation\u201d in SNNs. Since spiking neurons generate binary spikes, the features $X \\in \\mathbb{B}$, where $\\mathbb{B}$ is the binary set. Thus, dimensional expansion and nonlinear combination for x results in $x_ix_j \\in \\mathbb{B}$ where $i, j = \\{1,2,........, d+1\\}$. This means that $x_ix_j$ can only take values in the binary 0, 1, and that the \"star operation\" does not work for dimensional expansion. In addition, due to the inherent sparsity of spikes, most of the features in the SNN are 0, with very few 1-valued spikes. Binary spike multiplication will result in more 0-valued spikes, since 1 is only output if both sides are 1:\n$$x^i x^j = \\begin{cases}\n1 & \\text{only if } x^i = x^j = 1 \\\\\n0 & \\text{else}\n\\end{cases}$$\nThis makes the spikes even sparser and reduces the expressiveness of the SNN, leading to performance degradation.\nStar Operation on Spike Firing Rate. To avoid performance degradation caused by \u201cstar operations\u201d on 0-1 spikes, we convert multiple timestep spikes $\\{X_1,X_2,...,X_T\\} \\in \\{0,1\\}$ to spike firing rate $\\Phi = \\frac{1}{T}\\sum_{t=1}^{T} X_t$. The spike firing rate is spaced at $ \\frac{1}{T}$ intervals and takes on the value range [0,1], which can be viewed as a multi-bit value, greatly improving its representability compared to binary spikes. For instance, \u03a6 can be taken as $\\{0, \\frac{1}{5}, \\frac{2}{5}, \\frac{3}{5}, \\frac{4}{5}, 1\\}$ at T=5. In this way, employing the \"star operation\u201d on the spike firing rate can take advantage of the dimensional expansion benefits it is supposed to provide and avoid the degradation of the SNN due to excessive 0-value outputs.\nIn practice, we use the \"star operation\" to hybridize the original and temporally reversed spike firing rates of the"}, {"title": "penultimate layer of the SNN, which is passed directly to the final classification layer, as shown in Fig. 1 (c). In this way, the SNN produces two outputs: the original output O with the temporal dimension and the temporally hybridization output $\\tilde{O}$ without the concept of time. We guide both outputs with label Y to facilitate the SNN to ignore \u201cstar\u201d perturbations due to hybridization and learn more generalized representations:\n$$L = (1 - \\alpha)L_{CE}(O,Y) + \\alpha L_{CE}(\\tilde{O},Y),$$", "content": "where \u03b1 is the balance coefficient, which will be analyzed in the experimental section.\nTemporal Reversed Training\nThe overview of our TRT method is shown in Fig. 1. For temporal/static data, we obtain the temporally reversed data/feature by input/feature temporal reversal, respectively, and finally generate the output O and the temporally reversed output O by forward propagation in the SNN. In addition, after the penultimate layer of the SNN, we hybridize the original and temporally reversed spike firing rates using a \u201cstar operation\u201d to obtain the hybrid firing rate $\\tilde{O}$, which is passed to the final classification layer to generate the temporally hybridization output $\\tilde{O}$. To make the SNN to be insensitive to these perturbations, we use consistency loss and task loss to learn generalized feature representations. The overall objective function during training is shown in Eq. 12, and the training algorithm is described in Algorithm 1. For more PyTorch-style pseudocode please refer to Appendix A.\n$$L_{TRT} = (1 - \\alpha)L_{CE}(O,Y) + L_{con}(O, \\hat{O}) + \\alpha L_{CE}(\\tilde{O},Y).$$"}, {"title": "Experiments", "content": "To confirm the effectiveness and generalizability of our method, we conduct experiments on the tasks of static object recognition (CIFAR10/100 and ImageNet-1K (Deng et al. 2009)), neuromorphic object/action recognition (CIFAR10-DVS (Li et al. 2017) and DVS-Gesture (Amir et al. 2017)), and 3D point cloud classification (ModelNet10/40 (Wu et al. 2015)) using VGG-9 (Ding et al. 2024), MS-ResNet18 (Hu et al. 2024), Spike-driven Transformer (Yao et al. 2023), PointNet (Qi et al. 2017a), and PointNet++ (Qi et al. 2017b) architectures. If not specified, the SNN timestep was 5 for neuromorphic datasets and 2 for static datasets. The experimental details can be found in Appendix B."}, {"title": "Ablation Studies", "content": "Hyperparameter Sensitivity Analysis In Fig. 2, we have experimentally studied the influence of the balance coefficient \u03b1 on the performance. The influence of \u03b1 is most significant for the DVS-Gesture, where larger values of \u03b1 yield obviously better results, due to the stronger regularization of the perturbations at this point, which effectively mitigates the overfitting of the model. Overall, \u03b1 leads to only slight fluctuations in the performance of the SNN, while consistently outperforming the baseline, indicating that our method is not sensitive to \u03b1. We set the value of \u03b1 in later experiments based on the performance peaks in Fig. 2.\nComparison to Baseline The ablation studies for our method are shown in Tab. 1, where the PointNet was used for ModelNet10/40 and VGG-9 for the other datasets, and ablation studies with other architectures (MS-ResNet and Spike-"}, {"title": "Conclusion", "content": "In this paper, we propose the TRT method to train SNNs with generalized spatio-temporal representations. TRT improves inference performance and reduces the spike firing rate by using simple temporal reversal and element-wise multiplication operations during training only. We demonstrate the effectiveness and versatility of TRT in static/neuromorphic object/action recognition and 3D point cloud classification tasks, achieving performance that exceeds ex-"}, {"title": "Appendix A PyTorch-style Pseudocode Implementation", "content": "The PyTorch-style pseudocode for temporal reversal and spike firing rate hybridization is presented in Algorithm 2 and Algorithm 3 to facilitate the understanding and reproduction of our TRT method."}, {"title": "Appendix B Experimental Details", "content": "Tasks and Datasets\nWe validate the effectiveness and versatility of the proposed method on a variety of tasks and datasets described below.\nStatic Object Recognition For the static object recognition task, we use the CIFAR10/100 (Krizhevsky, Hinton et al. 2009) and ImageNet (Deng et al. 2009) datasets.\nCIFAR10 contains 50,000 training images and 10,000 test images, each 32 \u00d7 32 in size, covering ten types of objects. The CIFAR100 dataset has the same number of training samples, test samples, and image sizes as CIFAR10, but includes one hundred objects with higher recognition difficulty.\nThe ImageNet dataset of 1.2 million training images, 50,000 validation images, and 150,000 test images with 1,000 categories is the most challenging object recognition benchmark. For the ImageNet dataset, we unify the images to a 224 x 224 size during training and testing, and evaluate the performance of our method on the test set.\nFor CIFAR10 and CIFAR100 data, we preprocessed them using standard data augmentation strategies: random cropping, horizontal flipping, and normalization. We also use the autoaugment strategy (Cubuk et al. 2018) for CIFAR10. For ImageNet data, we use the same data augmentation strategies such as random augmentation and mixup as in (Yao et al. 2023). Please refer to (Yao et al. 2023) for specific augmentation details.\nNeuromorphic Object Recognition For neuromorphic object recognition, we use the CIFAR10-DVS dataset (Li et al. 2017), which is the neuromorphic version of the CIFAR10 dataset. The CIFAR10-DVS dataset has 10,000 samples for a total of 10 object classes, and the dimension of each sample is [t, p, x, y], where t is the timestamp, p is the polarity of the intensity change of the corresponding pixel, and x and y are the spatial coordinates of the pixel point, respectively. The spatial size of each sample in CIFAR10-DVS is 128 \u00d7 128, which we downsampled to 48 \u00d7 48 resolution before inputting to the SNN. Additionally, due to the high temporal resolution of the neuromorphic dataset, we integrate a neuromorphic sample into T event frames [T, p, x, y] using the SpikingJelly framework (Fang et al. 2023) to match the timestep T of the SNN. For each training, we randomly divide 90% of the data as the training set and test on the remaining 10% of the data, which is by far the most common strategy (Ding et al. 2024).\nNeuromorphic Action Recognition The DVS-Gesture (Amir et al. 2017) dataset contains neuromorphic data for 11 hand gestures with 1176 training samples and 288 test samples. The dimension of each sample is [T, p, x, y], and we downsample its spatial resolution from 128 x 128 to 48 \u00d7 48 before feeding the samples into the SNN. The pre-processing of the DVS-Gesture data is the same as in CIFAR10-DVS, which also utilizes the SpikingJelly framework to obtain the event frame [T, p,x,y] by integrating it by timestep.\n3D Point Cloud Classification For the 3D point cloud classification task, we use the ModelNet10 and ModelNet40 datasets (Wu et al. 2015). The ModelNet10 dataset contains 4,899 3D models in ten different categories, such as tables, chairs, bathtubs, and guitars. The ModelNet40 dataset contains 12,311 3D models in 40 different categories, making it even more challenging.\nFor the preprocessing of ModelNet10/40 data, we followed (Ren et al. 2023): uniformly sampling 1024 points on mesh faces based on the area of the grid surface and normalizing it to the unit sphere. These data of length 1024 are repeatedly fed into the SNN at each timestep."}, {"title": "Implementation Details", "content": "In this paper, all experiments are based on the PyTorch package running on both Nvidia RTX 4090 and 3090 GPUs. For both static object recognition and neuromorphic datasets, we use three architectures, VGG-9 (Ding et al. 2024), MS-ResNet18 (Hu et al. 2024), and Spike-driven Transformer (Yao et al. 2023). For the VGG-9 and MS-ResNet architectures, we follow the training strategy of (Ding et al."}, {"title": "Network Architectures", "content": "The VGG-9 network consists of eight convolutional-spiking layers and a fully connected layer for classification. MS-ResNet contains multiple contiguous residual blocks and uses identity connections between the membrane potentials of the pulsed neurons. We made minor modifications to the MS-ResNet18 architecture in the original paper (Hu et al. 2024) according to (Qiu et al. 2024) (the 7 \u00d7 7 convolution kernel of the first convolution was replaced by 3 \u00d7 3 and stride was set to 1), and kept the original MS-ResNet34 architecture (Hu et al. 2024). In addition, when using MS-ResNet34 for inference on ImageNet, we use the Gated Attention Coding method (Qiu et al. 2024). The specific architectural details are shown in Table 7.\nFeature Reversal Location\nFor static data, we temporally reverse the spike features taking advantage of the inherent temporal properties of the SNN. For the VGG-9 network, we consider the first two convolutional-spiking layers as the spike encoding module from which the spike features are temporally reversed. For the MS-ResNet network, we consider the first convolutional-spiking layer as the spike encoding module that produces temporally reversed features before the residual block. For the Spike-driven Transformer network, we temporally reverse the spike features generated after patch embedding module. For the Spiking PointNet network, we consider the input transformation within it as the spike encoding module, where the spike features are temporally reversed."}, {"title": "Details of Reproduction of Existing Methods", "content": "For a fair comparison with existing methods, the methods in (Yao et al. 2023), (Hu et al. 2024), (Ren et al. 2023), (Jiang et al. 2024b), and (Anumasa et al. 2024) are reproduced in this paper.\nSpike-driven Transformer (Yao et al. 2023): We implement Spike-driven Transformer using the official code provided in the original paper, keeping the network structure and hyperparameters such as the learning rate unchanged.\nMS-ResNet (Hu et al. 2024): The MS-ResNet18 and MS-ResNet34 architectures we used are shown in Table 7; we trained MS-ResNet18 with the same training strategy as VGG-9, and when using MS-ResNet34 we used the training strategy in (Yao et al. 2023).\nSpiking PointNet (Ren et al. 2023): We implement Spiking PointNet using the official code provided in the original paper, keeping the network structure and hyperparameters such as the learning rate unchanged."}, {"title": "Temporal Accumulated Batch Normalization (Jiang et al. 2024b):", "content": "We use the Temporal Accumulated Batch Normalization (TAB) layer to replace the vanilla BN layer in VGG-9", "2024)": "When reproducing Stochastic Latency Training (SLT), we use the VGG-9 network architecture and keep the training parameters consistent with our experimental settings. During training, we follow the SLT algorithm to randomly sample the timestep for training, and the timestep for inference is 5. We set the range of timesteps during training to [1,5", "10": "and achieved average accuracies of 74.23%, 89.35% ([1, 5"}]}