{"title": "Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models", "authors": ["Xingyun Hong", "Yan Shao", "Zhilin Wang", "Manni Duan", "Xiongnan Jin"], "abstract": "The development of LLMs has greatly enhanced the intelli-gence and fluency of question answering, while the emergence of retrieval enhancement has enabled models to better utilize external information. However, the presence of noise and errors in retrieved information poses challenges to the robustness of LLMs. In this work, to evaluate the model's performance under multiple interferences, we first construct a dataset based on machine reading comprehension datasets simulating various scenarios, including critical information absence, noise, and con-flicts. To address the issue of model accuracy decline caused by noisy external information, we propose a data augmentation-based fine-tuning method to enhance LLM's robustness against noise. Additionally, con-trastive learning approach is utilized to preserve the model's discrimina-tion capability of external information. We have conducted experiments on both existing LLMs and our approach, the results are evaluated by GPT-4, which indicates that our proposed methods improve model ro-bustness while strengthening the model's discrimination capability.", "sections": [{"title": "1 Introduction", "content": "With the development of deep learning and reinforcement learning, large lan-guage models (LLMs) have demonstrated significant potential in various natural language processing (NLP) applications[1]. As LLMs continue to evolve, concerns regarding the reliability have risen, such as outdated information and fabricated outputs [2]. Outdated information stems from static pre-training data, leading to model's lack of the latest knowledge. Moreover, the hallucination of model may mislead its user, undermining the credibility of LLMs. To address these issues, retrieval-augmented generation (RAG) [3] has been proposed and com-bined with LLMs. By incorporating external information, LLMs can adapt to dynamic contexts and provide more reliable and relevant results."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Retrieval-Augmented LLMs", "content": "The role of retrieval enhancement encompasses enhancing inferential capabili-ties, elevating answer traceability, and alleviating model hallucination. External information can be obtained from web, knowledge base, database and other sources. [6] incorporates knowledge into prompts. [7] utilizes search and chain-of-thought, allowing models to follow logical sequences and retrieve information in a more contextually relevant manner. Some studies also teach LLMs to use external tools including retriever, calculator, and other foundation models [8]. In addition to merely use retriever, [9] collaboratively optimizes both retrieval models and language models.\nWhile retrieval enhancement has demonstrated its efficiency, some studies in-dicate that the inclusion of irrelevant information can impact model performance. Consequently, the implementation of retrieval enhancement involves considera-tions such as when to invoke retrieval [4], the selection of external evidence [12], and post-evaluation of generated results [11]. These factors play crucial roles in fine-tuning the retrieval enhancement process and ensuring that the model leverages external knowledge judiciously to enhance its performance."}, {"title": "2.2 Robustness of LLMs", "content": "The robustness of LLMs is a crucial factor in application, typically evidenced by their performance under attack or disruptive inputs. Depending on where the perturbations occur, the study of model robustness can be classified into prompt robustness and task robustness.\nPromptBench [13] constructs adversarial prompt datasets, perturbing prompts at multiple level to evaluate how slight deviations, such as spelling errors or syn-onyms, affect LLM results while maintaining semantic integrity. [14] is the work related to prompt injection attack.\nTask robustness is to observe model performance by perturbing different tasks such as sentiment analysis, natural language inference, classification and so on [15] with typos, grammatical errors, and insertions. Some datasets are designed to evaluate model robustness, including multi-task benchmark AdvGLUE [16], table-based question-answering dataset RobuT [17], and others focusing on code generation, math reasoning and dialogue generation."}, {"title": "3 Dataset Construction", "content": "In practical applications, deploying LLMs for knowledge-based question answer-ing may encounter challenges when retrieving external knowledge through ex-ternal searches. This is due to the potential presence of irrelevant or erroneous information, as well as variations in format among different knowledge sources. To evaluate the performance of LLMs under such circumstances and enable them to better handle diverse scenarios, we first choose two MRC datasets, then we apply several data construct techniques to generate new datasets. The cor-rectness of the generated samples is evaluated by rule-based methods and human review. According to the context type, the new constructed samples are catego-rized into five classes. Figure 2 illustrates the process of dataset construction."}, {"title": "Single Source (SS)", "content": "The selection of MRC datasets serves various purposes in our work. These datasets typically comprise questions paired with correspond-ing contexts, from which the answers are extracted. This structure facilitates the addition of noise and the replacement of answers. Additionally, the datasets en-compass a significant portion of questions that rely on common sense, which LLMs likely to have encountered during pre-training. As a result, these datasets allow us to examine models' discrimination ability under the impact of mislead-ing external information.\nSQUAD (in English) [18] and WebQA (in Chinese) [19] are chosen as our base datasets. SQUAD is a MRC dataset consisting of 100k+ questions posed by crowd workers on Wikipedia articles, where the answer to each question is a segment of text from the corresponding passage. WebQA is a large scale human annotated real-world QA dataset with more than 42k questions and 556k evidences.\nWe sample 500 instances from each dataset to form a development set and a test set respectively. The proportion of development and test set is 1:1. The original data contains question, question-related context and a corresponding answer. This dataset is referred as SS below."}, {"title": "Single-Source-Incomplete (SSIncomp)", "content": "Incomplete text is designed for assessing LLMs' performance with relevant but insufficient data. In this case, the topic of context is partly relevant to the question, but the crucial information is absent from the context. Traditional MRC model can not handle such situation. As for the LLM, endowed with internal knowledge, is anticipated to determine"}, {"title": "Multi-Source-Consistent (MSCons)", "content": "When retrieving information, data from different sources may exhibit diverse formats and content variations. Our objective is to explore impact of multi-source information on model performance. Specifically, we aim to observe whether information with similar content but different formats affects the model's inference results. Since the original data is in natural language, we utilize GPT3.5-Turbo [20] to extract multiple sets of triples as an alternative data source. We retain those samples for which answers exist in the extracted triples."}, {"title": "Multi-Source-Inconsistent (MSIncons)", "content": "In addition to examining sce-narios with consistent content across multiple sources, we anticipate the model to remain robust even if the retrieved results contain additional noise, which may be partially relevant or irrelevant to the question. Therefore, we build a triple database using several datasets, for instance, KQA pro, MetaQA dataset and so on, containing 163,776,434 triples in total. These triples are not necessar-ily related to the questions in WebQA and SQUAD. We utilize question or the head entities extracted by GPT3.5-Turbo as search terms, and ensure that the answers are not included in the retrieved results. The final selected triples are limited to 10 with respect to relevance."}, {"title": "Multi-Source-Conflict (MSConf)", "content": "Given the substantial variability in the quality of internet information, retrieval process occasionally yields erroneous or conflicting results. As such situations are often inevitable, it becomes imperative for LLMs to discriminate between correct and incorrect information when pre-sented simultaneously. In this case, we have devised conflicting samples. First, we use GPT3.5-Turbo to generate a similar yet incorrect answer based on the question and original answer. Subsequently, the answer in the triples consistent with the context is substituted with the fabricated one."}, {"title": "Dataset Quality", "content": "Given the potential errors introduced by construction methods and tools, during dataset construction, we conduct multiple case-based optimizations of the prompt. For created samples, the unsatisfactory results are filtered out from our test set, ensuring the generated results align with our re-quirements. Specifically, for SSIncomp and MSIncons samples, we ensure that answers are not included in the deleted or retrieved results. For MSCons sam-ples, triplets generated by GPT3.5-Turbo must contain corresponding answers. For MSConf samples, 100 instances are randomly sampled for human review. The plausibility of the generated false answers, considering the correspondences"}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Data Augmentation", "content": "To enhance the model's robustness under diverse contexts, we integrate data augmentation techniques during model fine-tuning. Original data in the SQUAD and WebQA dataset contains questions, context and answers. Typically, the answer is contained in the question-related context. Leveraging corresponding and comprehensive context to the question facilitates the task for LLMs, yet the presence of noise within the context may weaken the model's generalization ability. Consequently, we implement span masking and word swapping strategies to enhance the model's robustness.\nMask: We enhance the model's reasoning and generalization abilities by masking certain portions of the context. Even if the context does not explicitly contain the answer, LLM can leverage context related to the answer and its inter-nal knowledge to improve accuracy. Masking is performed with spans, typically short sentences whose removal do not significantly affect semantic integrity.\nEvery span (usually delimited by two punctuation marks, using regex to sep-arate) has equal possibility of being eliminated. The absence of answer motivates LLM to distinguish the relevance of information and stick to its own knowledge when confronted with inadequate context, whereas the removal of other span simulates the incompleteness of information.\nSwap: As disorder of words between adjacent words dose not alter its overall meaning significantly, the switch of words can be regarded as adding noise to the context. In our approach, we randomly select a span from the context and a position within span, then interchange adjacent word sets.\nUnlike methods involving open-ended searches or GPT generation, these ap-proaches have comparatively low costs in constructing training data, thus are more practical for real-world applications."}, {"title": "4.2 Contrastive Learning", "content": "Through detailed case analysis (refer to Sec5.2), we discover that LLMs possess different levels of discrimination capabilities. When confronted with queries that cannot be answered with the given information, the model occasionally informs the user of the inadequacy of the provided knowledge. To strengthen model's discrimination ability, we employ contrastive learning after first stage fine-tuning.\nAs to find what does the model know and guide it to decline to the question beyond its knowledge, we create SSIncomp samples of each dataset as train-ing data. For SQUAD, elimination of answer-contained sentence is applied. For WebQA, context relevant to question but does not include the answer is selected. First, the fine-tuned model generates outputs for the aforementioned samples, subsequently assessed by GPT-4. To facilitate contrastive learning, we construct new dataset. Each question and context pair is accompanied by both an answer to accept and one to decline, as depicted in Figure 3.\nFor correctly-inferred samples, the original sample label is designated as the chosen answer, while a specific expression is assigned as the rejected answer, such as \"Provided context is not sufficient to answer the question/Sorry, I don't have enough information...\". As for incorrect samples, the sentence indicating model's inability to respond constitutes the chosen answer, whereas the incorrect output from the fine-tuned model serves as the declined answer.\nThroughout the training process, we aim to maximize the disparity in prob-ability between accepted and declined answers through comparison.\nSpecifically, the loss function is defined as:\n$L = -\\frac{1}{N}(\\sum_{i=1}^{N}log\\sigma(\\sum_{i=1}^{C}logp(y_c|x) - \\sum_{i=1}^{R}logp(y_{R_i}|x)))$ (1)\nwhere N represents the sample numbers, $\\sigma$ is the sigmoid function. C and R stands for tokens of chosen label and rejected label respectively. $logp(y_c|x)$ is the log probability of the chosen token $C_i$ when given x as input."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Models", "content": "To examine the impact of various information on LLMs, we initially evaluate model performance using the dataset we construct. Subsequently, we select a base model to conduct further experiments. Below are the models utilized for our experimental evaluation.\nGPT3.5-Turbo: GPT3.5-Turbo is a conversational artificial intelligence model developed by OpenAI, based on the GPT architecture.\nBaichuan [22]: Generation of LLM from Baichuan Intelligence and supports both Chinese and English.\nLlama[23]: A large language model released by Meta AI, which supports text completion and chat completion.\nChatGLM [24]: ChatGLM is an open bilingual language model (supports English and Chinese) based on General Language Model (GLM) framework."}, {"title": "5.2 Evaluation Metrics", "content": "Considering that the outputs of the original LLM are typically more comprehen-sive and long, whereas the labels in the MRC dataset are comparatively concise, utilizing n-gram metrics such as ROUGE and BLEU can not indicate real per-formance precisely. Therefore, for each sample, we employ recall and accuracy to evaluate. Recall assesses the percentage of overlapping words between the model outputs and labels, it's suitable for comparing model performance after fine-tuning, since the output format remains consistent.\nWhile recall focuses on words matching, we utilize GPT-4 to determine whether the whole output aligns the key points of the label, even if in different ways of expression. In practical application, when querying LLM, it's preferable to receive a response indicating model's inability to answer rather than an in-correct one. Considering this scenario, GPT-4 is used to categorize the model's inferences into wrong (w), correct (c), or rejected (r, indicating the model de-clines to give answer directly due to lack of information or other reasons).\nThe standard accuracy metric (ACC) is defined as the proportion of the correct samples among all samples. Meanwhile, to further distinguish the model's ability to decline questions beyond its ability, correct, rejected and incorrect responses receive a score of 1, 0, and -1 respectively. The average score is then computed across all samples as the weighted score (WSCORE)."}, {"title": "5.3 Experimental Setup", "content": "In the experiment, we commence by assessing the performance of several existing LLMs using our dataset. Among the open-source models, Baichuan2-13B-Chat demonstrates satisfactory performance, coupled with a relatively swift inference speed, thus making it a suitable candidate for further experiments.\nDue to the resource limitation and the effectiveness of LoRA [25], we fine-tune LLMs based on LoRA framework. The rank of the adaptors is set as 8, maximum input length is 2048, and the learning rate is 1e-4 with a warm-up strategy. The experiment is accelerated using NVIDIA V100 GPUs with 32GB memory each.\nDuring the first fine-tuning stage, approximately 40% of answer-located span are masked. In the second fine-tuning stage, we select 3,500 new samples, ensur-ing a balanced distribution of positive and negative instances at 1:1."}, {"title": "5.4 Results", "content": "Existing model performance evaluation As shown in Table 1, we observe that the accuracy of nearly every model (excluding Llama2-7B-Chat) surpasses 85% on the original single-source samples (SS). In this case, the context aligns perfectly with the question, apart from a few inference-based questions, which are challenging to answer.\nHowever, for samples where the answer is absent in the context (SSIncomp), there is a notable decline in accuracy. This discrepancy can be attributed to certain questions heavily relying on contextual cues. To isolate this effect, we"}, {"title": "Evaluation of model's discrimination ability", "content": "We further distinguish declined responses from incorrect responses to observe the proportion of rejection on SSIncomp and MSConf samples. Meanwhile, in other three scenarios, since the context contains the answer, though potentially with noise, we still anticipate correct responses from the model.\nThe metricr represents the proportion of declined responses. Each model demonstrates varying levels of discrimination capability, with Baichuan2-13B-Chat and ChatGLM exhibiting relatively strong discrimination abilities. However, a higher rejection rate implies less informative responses. So it is necessary to assess this metric in conjunction with the correct answer proportion. In terms of the overall WSCORE, GPT3.5-Turbo remains the best. Despite its lower rejec-tion rate, the proportion of correct answers significantly surpasses other models, followed by the Baichuan2-13B-Chat model.\nAs for the BC2-13B-SFT model, after fine-tuning with MRC samples, the model tends to provide answers even if they are incorrect. However, after second stage fine-tuning utilizing contrastive learning, model's discrimination capability is enhanced, transitioning from delivering an answer (regardless of correctness) to declining to the question beyond its capacity. Moreover, in the scenarios involving multi-source, the accuracy remains superior to that of original Baichuan2, main-taining robustness against noise. There is an increment of 10.7% in the overall weighted score of BC2-13B-SFT-Contrast compared to the original Baichuan2-13B-Chat model and approaching the performance of GPT3.5-Turbo. It is worth noting that despite the relatively small number of training samples, contrastive learning contributes to improving the model's discrimination capability."}, {"title": "5.5 Ablation Study", "content": ""}, {"title": "Ablation Study of the First Stage Fine-tuning", "content": "To validate the efficacy of data augmentation strategies, we conduct an ablation study using the dev set.\nBC2-13B-SFT: use both mask and swap strategies to process original data.\nBC2-13B-SFT W/O SWAP: use only mask to process original data.\nBC2-13B-SFT W/O MASK & SWAP: use original training data with-out any data augmentation strategy."}, {"title": "Effectiveness of Contrastive Learning", "content": "To validate the effectiveness of con-trastive learning, we also compare the preference learning based on direct pref-erence optimization (DPO)[21] and conventional fine-tuning methods.\nIn DPO, we utilize the same training samples as in the contrastive learning approach. While DPO yields further improvements in the model's accuracy in"}, {"title": "6 Conclusions", "content": "In this paper, we first create dataset simulating various scenarios, including crit-ical information absence, noise, and conflicts, based on MRC datasets to assess various model's performance under multiple interferences. To mitigate the de-cline in model accuracy attributed to noise, we introduce a data augmentation-based fine-tuning method to enhance LLM's robustness against noise. Addition-ally, we employ contrastive learning to strengthen the model's discrimination capability. Experimental results indicate that our proposed methods improve model robustness while strengthening the model's discrimination capability."}]}