{"title": "Multimodal Fusion and Coherence Modeling for Video Topic Segmentation", "authors": ["Hai Yu", "Chong Deng", "Qinglin Zhang", "Jiaqing Liu", "Qian Chen", "Wen Wang"], "abstract": "The video topic segmentation (VTS) task segments videos into intelligible, non-overlapping topics, facilitating efficient comprehension of video content and quick access to specific content. VTS is also critical to various downstream video understanding tasks. Traditional VTS methods using shallow features or unsupervised approaches struggle to accurately discern the nuances of topical transitions. Recently, supervised approaches have achieved superior performance on video action or scene segmentation over unsupervised approaches. In this work, we improve supervised VTS by thoroughly exploring multimodal fusion and multimodal coherence modeling. Specifically, (1) we enhance multimodal fusion by exploring different architectures using cross-attention and mixture of experts. (2) To generally strengthen multimodality alignment and fusion, we pre-train and fine-tune the model with multimodal contrastive learning. (3) We propose a new pre-training task tailored for the VTS task, and a novel fine-tuning task for enhancing multimodal coherence modeling for VTS. We evaluate the proposed approaches on educational videos, in the form of lectures, due to the vital role of topic segmentation of educational videos in boosting learning experiences. Additionally, we introduce a large-scale Chinese lecture video dataset to augment the existing English corpus, promoting further research in VTS. Experiments on both English and Chinese lecture datasets demonstrate that our model achieves superior VTS performance compared to competitive unsupervised and supervised baselines\u00b9.", "sections": [{"title": "Introduction", "content": "The proliferation of digital video content over the last few decades has underscored the importance of efficient content navigation and comprehension.\nAs the unstructured nature of videos poses significant challenges for users seeking to quickly grasp or reference specific topics, Video Topic Segmentation (VTS) has emerged as a vital tool in addressing these demands. By delineating videos into coherent non-overlapping topics, VTS not only facilitates intuitive understanding of video content but also enables swiftly pinpointing and accessing topics of interest. This is particularly pertinent for the furtherance of various video understanding tasks, where VTS serves as a foundational component.\nTraditional VTS techniques predominantly hinge on shallow features (Gandhi et al., 2015; Soares and Barr\u00e9re, 2018b; Ali et al., 2021) and unsupervised methods (Gupta et al., 2023) due to the scarcity of labeled data. These methods often fall short in capturing the semantic cues that signal topical shifts in video streams, hence suffer from limited precision. Recent advancements in supervised learning paradigms, benefiting from the availability of large-scale labeled data, have achieved notable performance improvements in various video segmentation tasks, such as video action segmentation (Zhou et al., 2018; Tang et al., 2019), scene segmentation (Huang et al., 2020; Islam et al., 2023), and topic segmentation (Wu et al., 2023; Wang et al., 2023; Xing et al., 2024), surpassing unsupervised methods. Performance can be further enhanced by pre-training on vast volumes of unlabeled data (Xu et al., 2021; Mun et al., 2022) or initializing models from pre-trained models (Yan et al., 2023) and then fine-tuning the model. Hence, in this work, we focus on further improving supervised methods for VTS.\nCompared to text topic segmentation (Koshorek et al., 2018; Xing and Carenini, 2021; Yu et al., 2023), videos contain rich and diverse multimodal contextual information. Fully utilizing multimodal information, such as visual cues and textual data (e.g., screen text and subtitles), could facilitate more detailed content understanding and in turn more accurate semantic segmentation than relying on text only. Also, coherence is essential for understanding logical structures and semantics. Enhancing coherence modeling has achieved significant improvements in long document topic segmentation (Yu et al., 2023). Therefore, we improve supervised VTS methods by thoroughly exploring multimodal fusion and multimodal coherence modeling. We enhance multimodal fusion from the perspectives of model architecture and pre-training and fine-tuning tasks. Specifically, we compare various multimodal fusion architectures built upon Cross-Attention and Mixture-of-Experts (MoE). We investigate the effect of multi-modal contrastive learning for general pre-training and fine-tuning for strengthening cross-modal alignment. For enhancing multimodal coherence modeling, we propose a new pre-training task tailored for the VTS task, and a novel fine-tuning task by elevating intra-topic multimodal feature similarity and inter-topic multimodal feature differences. The proposed approaches are extensively evaluated on educational videos, in the form of lectures, due to the pivotal contributions of topic segmentation of educational videos in bolstering the learning experiences.\nOur contributions can be summarized as follows.\n\u2022 We propose a supervised multimodal sequence labeling model for VTS, denoted MMVTS model. We compare various multimodal fusion architectures. In addition to applying multimodal contrastive learning for strengthening cross-modal alignment, we propose a new self-supervised pre-training task tailored for the VTS task and a novel fine-tuning task for enhancing multimodal coherence modeling.\n\u2022 We introduce a large-scale Chinese Lecture Video Topic Segmentation dataset (CLVTS) to promote the research of VTS.\n\u2022 Experiments show that our model sets new state-of-the-art (SOTA) performance on both English and Chinese lecture video datasets, outperforming competitive unsupervised and supervised baselines. Comprehensive ablation study further confirms the effectiveness of our approaches."}, {"title": "Related Work", "content": "Text Topic Segmentation The objective of text topic segmentation is to automatically partition text into topically consistent, non-overlapping segments (Hearst, 1994). By automatically mining clues of topic shifts from large amounts of labeled data (Koshorek et al., 2018; Arnold et al., 2019), contemporary supervised models (Lukasik et al., 2020; Somasundaran et al., 2020; Zhang et al., 2021; Yu et al., 2023) have demonstrated superior performance compared to unsupervised approaches (Riedl and Biemann, 2012; Solbiati et al., 2021). Notably, supervised models that excel at modeling long sequences (Zhang et al., 2021; Yu et al., 2023) are capable of capturing longer contextual nuances and thereby achieve better topic segmentation performance, compared to models that model local sentence pairs or block pairs (Wang et al., 2017; Lukasik et al., 2020). In addition, recent works (Somasundaran et al., 2020; Xing et al., 2020; Yu et al., 2023) have demonstrated that strengthening coherence modeling can improve the topic segmentation performance. Therefore, in this work, we explore enhancing coherence modeling for video topic segmentation under the multimodal configurations.\nVideo Topic Segmentation Distinct from text topic segmentation, video topic segmentation is inherently characterized by its multimodal data context; hence, comprehensive utilization of the multimodal information could improve semantic understanding capabilities of a model. Specifically, Gupta et al. (2023) introduce an unsupervised method that employs the TWFINCH algorithm (Sarfraz et al., 2021) to cluster video clips into distinct topics based on learned visual and text features, which are derived from self-supervised tasks of matching the narration with the temporally aligned visual content. Wang et al. (2023) simply concatenate the sampled encoded visual features and text embedding as the input to a pre-trained language model, which may cause inferior fine-tuning due to the discrepancy between the pre-training stage of the language model and the fine-tuning stage combining textual and visual inputs. Other works related to VTS are summarized and compared in Appendix A."}, {"title": "The CLVTS Dataset", "content": "Collection Procedure The CLVTS dataset is primarily sourced from Public Video Platforms23. Videos are first transcribed by a competitive automatic speech recognition (ASR) system. We then ask annotators to combine visual and textual (ASR 1-best) information to mark the timestamp (in seconds) of the end of each topic. We ensure high accuracy and reliability of annotations from three aspects, including annotator training, hierarchical topic labeling, and a multi-annotator strategy. Details of these strategies and the rigorous quality assessment and quality control we conduct can be found in Appendix B.\nDataset Statistics To measure the extent of consensus for VTS annotations, following (Shou et al., 2021), we compute the F\u2081@k score (described in Appendix D) based on the absolute distance between two topic boundary sequences, varying the threshold k from 0 to 8 seconds with a step size of 2 seconds, where 8 seconds are approximately the average duration of a clip. By averaging the F\u2081 scores across all three pairs of annotated topic boundaries from three annotators on the same video, we obtain the consistency score. The more similar the annotations from all annotators on the same video are, the higher the consistency score is. Figure 1a shows that the consistency scores of the majority of videos exceed 0.5, indicating a decent degree of consensus for VTS annotations (Shou et al., 2021).\nTable 1 compares our CLVTS dataset against existing VTS datasets. Among annotated videos in CLVTS, 47% are presentations showing slides, 34% are blackboard demonstrations, and 19% are miscellaneous types. We also collect 1027 hours of unlabeled videos from the same sources for pre-training. Besides the linguistic distinctness, CLVTS is characterized by its natural and uninterrupted long videos, a stark contrast to the English lecture dataset, AVLecture (Gupta et al., 2023), since nearly two-thirds of AVLecture are reassembled pre-segmented short videos. Figure 1b and 1c show a diverse distribution of video and topic durations and a broad spectrum of subjects in CLVTS."}, {"title": "Methodology", "content": "Figure 2 depicts the architecture of our VTS model. Section 4.1 presents the problem definition of multimodal VTS and the overall architecture. We enhance multimodal fusion from the perspectives of model architecture, pre-training, and fine-tuning tasks. Specifically, we compare different architectures built upon Merge- and Cross-Attention, and Mixture-of-Experts for multimodal fusion (Section 4.1). We explore multimodal contrastive learning for cross-modality alignment and propose a new pre-training task tailored for VTS (Section 4.2). For fine-tuning, we also propose a novel task for multimodal coherence modeling (Section 4.3)."}, {"title": "Multi-Modal Video Topic Segmentation", "content": "Overall Architecture Following prior works (Zhang et al., 2021; Wu et al., 2023), we define video topic segmentation as a clip-level sequence labeling task and propose our MultiModal Video Topic Segmentation (MMVTS) model. As illustrated in Figure 2a, we apply unimodal pre-trained encoders for each modality and then fuse multimodal information at the intermediate representation level (i.e., 'middle fusion' (Xu et al., 2023)) through Multimodal Fusion Layers. Given a video, we transcribe it with a competitive automatic speech recognition (ASR) system and use ASR 1-best as the text modality. We then divide the video into n clips ${(c_1, c_2, ... , c_n)}_{i=1}^n$, with clips segmented at the sentence boundaries predicted on ASR 1-best. $c_i = {f_1, ..., f_k}$ denotes evenly sampled k frames from the i-th clip and is fed to a visual encoder $E_v$ to extract visual features. $c_i^t = {w_1, ..., w_{|s_i|+1}}$ denotes the sequence of words from ASR 1-best within the i-th clip, where $w_i$ is the inserted special token [BOS] and $||s_i||$ denotes the number of words of i-th clip. $c_i^t$ is fed to a text encoder $E_t$ for extracting textual features. After extracting unimodal features, we first apply trainable projection matrices $W_v$ and $W_t$ to convert unimodal features into the same dimension, as visual feature sequence $v = {v_1, ..., v_n}$ and textual feature sequence $t = {t_1, ..., t_n}$ (Eq. 1). Then we fuse the multimodal information with M Multimodal Fusion Layers ${MFLM}$ and obtain the updated visual features $h^v = {h_1^v, ..., h_n^v}$ and textual features $h^t = {h_1^t,..., h_n^t}$ (Eq. 2),\n$v = W_v \u00b7 E_v({c_1, ..., c_n})\nt = W_t \u00b7 E_t ({c_1^t, ..., c_n^t})$ (1)\n$h_i^v; h_i^t = MFLM (v_i; t_i)$ (2)\n$m_i = h_i^v; h_i^t$ (3)\n$p = W_p.m$ (4)\n$l_{vts} = -\\sum_{i=1}^{n-1}[y_i ln p_i + (1-y_i) ln(1-p_i)]$ (5)\nCompared to 'late fusion' where no cross-modal interaction happens until after the classifiers, middle fusion and \u2018early fusion' are found to generally outperform late fusion (Nagrani et al., 2021), probably because early and middle fusion aligns better with human perception where multimodal fusion happens early in sensory processing. On the other hand, compared to early fusion, middle fusion yields superior or comparable performance (Nagrani et al., 2021) and is much less computationally expensive since we could freeze partially strong pre-trained unimodal encoders and only train a small number of parameters in the Multimodal Fusion Layers."}, {"title": "Multimodal Fusion Layer (MFL)", "content": "We compare four different cross-modal interaction mechanisms for multimodal fusion layers. We investigate the Merge-Attention and Co-Attention multimodal fusion layers proposed in (Dou et al., 2022; Yang et al., 2023). With Merge-Attention (Figure 2b), features from unimodal encoders are sequentially concatenated and then input into a standard transformer encoder layer (Vaswani et al., 2017), which shares attention parameters across modalities. A feed forward layer is added on top to produce the final output representation. In contrast, with Co-Attention (Figure 2c), features from each unimodal encoder first go through self-attention with modality-specific attention parameters, then we perform symmetrical cross-attention to integrate information from all other modalities to enhance the representation of the considered modality, followed by a feed forward layer.\nInspired by (Mustafa et al., 2022), which interleaves MoE encoder layers and standard dense encoder layers for image-text multimodal models, we also investigate two new architectures by replacing the traditional single feed-forward layers in Figure 2b and 2c with a Mixture-of-Experts (MoE) (Shazeer et al., 2016; Lepikhin et al., 2020) module respectively, depicted by Figure 2d and 2e. The motivation is that adding MoE on top of the fused representations may facilitate deeper cross-modal integration of information and improve model capacity without a proportional increase in computational complexity. Specifically, experts are MLPs activated depending on the input. Firstly, we concatenate fused features output from self-attention or cross-attention. Then, we implement the Noisy Top-k Gating mechanism (Shazeer et al., 2016) to select K experts from a total of E candidates (Eq. 6 - 8), where $S_N()$ denotes stand normal distribution, $W_n$ denotes tunable Gaussian noise to help load balancing, $W_g$ is a trainable weight matrix, K and E are hyper-parameters. Finally, the outputs of the K activated experts are linearly combined according to the learned gating weights (Eq. 9). For the MoE training objective, we sum importance loss and load loss (Shazeer et al., 2016) to balance expert utilization as Eq. 10.\n$G(x) = Softmax(KeepTopK(Hx,k))$ (6)\n$H(x) = (W_g.x)_i + S_N() \u00b7 Softplus((W_n.x)_i)$ (7)\n$KeepTopK(x, k)_i = \\begin{cases} x_i if x_i is in top-k\\\\ -\u221e otherwise.\\end{cases}$ (8)\n$MoE(x) = \\sum_{e=1}^{K}G(x)_e \u00b7 MLP_e(x)$ (9)\n$l_{balance} = l_{importance} + l_{load}$ (10)"}, {"title": "Pre-training with Unlabeled Data", "content": "Prior works have demonstrated that standard self-supervised denoising pre-training (even using only the downstream task data) (Amos et al., 2023) or pre-training adapted to the downstream task (Gururangan et al., 2020) often perform substantially better than randomly initializing the parameters. Therefore, to better initialize the parameters of the Multimodal Fusion Layers, we explore pre-training with unlabeled video data before fine-tuning on the labeled training set. Firstly, we introduce a general cross-modality alignment pre-training task to learn the multi-modal representation. We use contrastive learning loss to adjust the features learned by the model fusion layer, by maximizing the cosine similarity of the visual features and textual features of the same clip, while reducing the similarity of the modality features between different clips, as show in Eq. 11, where $\\epsilon$ is used to prevent division by 0 and $\\tau$ is a temperature hyper-parameter to scale the cosine similarity.\n$l_{cma} = -\\frac{1}{n} \\sum_{i=1}^{n} log \\frac{e^{sim(h_i^v,h_i^t)/\u03c4}}{\\sum_{j=1}^{n} e^{sim(h_i^v,h_j^t) + \\epsilon}}$ (11)\n$sim(x_1,x_2) = \\frac{x_1^Tx_2}{||x_1|| ||x_2||}$ (12)\nSecondly, we introduce a novel pre-training task tailored for the downstream VTS task, focusing on utilizing unlabeled data for learning pseudo topic boundaries and also enhancing modality alignment. Initially, a Kernel Density Estimation (KDE) (Davis et al., 2011) model is employed to approximate the topic duration distribution of the training set; then, videos are split into segments matching durations sampled from the KDE-derived distribution. Subsequently, for every resulting segment, we undertake one of three actions with an equal probability: inserting segments randomly selected from other videos either before or after the current segment, replacing the current segment with a segment randomly selected from other videos, or no change. The resulting segments within the modified video are then treated as distinct topics, thereby enabling the model to learn pseudo topic boundaries during pre-training. This task-adaptive pre-training task has the same $l_{vts}$ objective as shown in Eq. 5. The overall pre-training objective is shown in Eq. 13, where \u03b1 and \u03b2 are hyper-parameters to adjust the loss weights.\n$l_{pretrain} = l_{vts} + \u03b1l_{cma} + \u03b2l_{balance}$ (13)"}, {"title": "Fine-tuning with Multimodal Coherence Modeling", "content": "For fine-tuning, we introduce two auxiliary tasks to enhance multimodal coherence modeling. The cross modal alignment task is the same task in Eq. 11 used in pre-training. This continuity ensures that the modalities retain their coherence through both pre-training and fine-tuning, fostering a consistent interplay between different modalities. In addition, we adapt the Contrastive Semantic Similarity Learning (CSSL) task proposed by Yu et al. (2023), which leverages the inherent characteristics of topic-related coherence, to the multimodal context. We adopt the same strategy for selecting positive and negative sample pairs (Yu et al., 2023), but extend the features to the multimodal representations, as shown in Eq. 14, where $k_1$ and $k_2$ are hyper-parameters that determine the number of positive and negative pairs. For each clip's multimodal representation $m_i$, $m_i^{p_j}$ denotes the multimodal representation of the j-th similar clip in the same topic as clip i, while $m_{ij}^n$ denotes the multimodal representation of the j-th dissimilar clip in a different topic from clip i. We hypothesize that this extension could improve multimodal representation learning by identifying relative consistency relations within and across topics.\n$l_{mcssl} = -\\frac{1}{n} \\sum_{i=1}^{n} log \\frac{\\sum_{j=1}^{k_1} e^{sim(m_i,m_i^{p_j})}}{\\sum_{j=1}^{k_1} e^{sim(m_i,m_i^{p_j})} + \\sum_{e=1}^{k_2} e^{sim(m_i,m_{i,j}^n)}}$ (14)\nThe overall fine-tuning objective combines Eq. 5, 10, 11, and 14, as shown in Eq. 15, where \u03c3, \u03b8, and \u03b3 are hyper-parameters to adjust loss contribution. When the multimodl fusion layer does not contains MoE structure, \u03b2 in Eq. 13 and \u03c3 are set to zero.\n$l_{finetune} = l_{vts} + \u03c3l_{balance} + \u03b8l_{mcssl} + \u03b3l_{cma}$ (15)"}, {"title": "Experiments", "content": "Experimental Setup\nDatasets We partition the labeled data within AVLecture into 70% for training, 10% for validation, and 20% for test sets. We also split the CLVTS dataset into train, validation, and test sets with identical ratios. The unlabeled data of AVLectures and CLVTS are used for pre-training on each dataset, respectively.\nBaselines We carefully select the following representative baselines:\n- UnsupAVLS (Gupta et al., 2023) is an unsupervised approach that clusters video clips into discrete topics based on visual and text embeddings"}, {"title": "Results and Analysis", "content": "Table 2 reports the performance of baselines (the first group) and our MMVTS model variants (the second and third group) on the AVLecture and CLVTS test sets.\nUnimodal performance. The text-only Long-former (Row3) substantially outperforms the visual-only BaSSL (Row2) on BS@30 by a large gain (+25.31), and also surpasses the unsupervised approach UnsupAVLS by a notable gain (+13.25). Such results are expected since the textual modality inherently conveys more precise and richer information than the visual modality for VTS.\nMutimodal performance. As can be seen from Table 2, Avg of $SWST_{seq}$ closely aligns with that of the text-only LongFormer, suggesting that more data may be necessary to mitigate the discrepancy between fine-tuning with early fusion and pre-training the language model, in order to fully exploit the potential of the early fusion strategy. Our MMVTS Baseline1,2,3 simply concatenate unimodal features to predict topic boundaries. Without pre-training on unlabeled data (PT, Eq. 13) and the two auxiliary fine-tuning tasks to enhance multimodal coherence modeling (FT-Coh, Eq. 15), on $F_1$, MMVTS Baseline1 outperforms the text-only Longformer by 2.28 and 2.90 on AVLecture and CLVTS respectively, while on Avg score, MMVTS Baseline1 outperforms Longformer by 1.1 on AVLecture yet slightly underperforms Long-former by 0.65 on CLVTS. These results suggest that simply concatenating unimodal features to predict topic boundaries does not bring consistent gains over unimodality. The third group in Table 2 compares the four Multimodal Fusion Layer architectures with pre-training (PT) and fine-tuning (FT-Coh). Overall, compared to all the competitive unsupervised and supervised baselines, after pre-training and fine-tuning, our MMVTS model using Co-Attention with MoE as Multimodal Fusion Layers achieves the best Avg, BS@30, and $F_1@30$ results and sets new SOTA on AVLecture, and achieves the best $F_1$ and a Avg score comparable to the best result on CLVTS. The gains on Avg from our Co-Attention MoE with PT and FT-Coh over $LongFormer_{cssl}$ and MMVTS Baseline1 are statistically significant (p-values are less than 0.05). It is also notable that the performance of models on CLVTS is generally much lower than that on AVLecture, with the best Avg score between AVLecture and CLVTS differing by 17.50 (68.84 versus 51.34), indicating a greater challenge posed by our CLVTS dataset than the AVLecture dataset.\nTable 2 also shows that on CLVTS, fine-tuning the powerful pre-trained Llama-3-8B with our Discrete prompt achieves the best Avg score 51.34, although very close to the performance of Merge-Attn with MoE and Co-Attn with MoE after PT and FT-Coh. However, on AVLecture, the performance of fine-tuning Llama-3-8B is much lower than Multimodal Fusion Layer with PT and FT-Coh; particularly, $F_1$ on both AVLecture and CLVTS are much lower because the model's predicted boundaries often have a clip offset, but its $F_1@30$ and mIoU results are still best on CLVTS.\nWe conduct ablation studies and analysis to validate effectiveness of the proposed multimodal fusion layers, pre-training and fine-tuning tasks."}, {"title": "(1) Effect of Multimodal Fusion Layers", "content": "Comparing the third group and Baseline3 in Table 2 shows that with the same pre-training and fine-tuning, the best performing architecture using Multimodal Fusion Layers always substantially outperforms Baseline3. Specifically, with PT and FTCoh, on AVLecture, both Co-Attention and Co-Attention with MoE notably outperform MMVTS Baseline3 by 1.32 on Avg; on CLVTS, all four Multimodal Fusion Layer architectures achieve remarkable gains on Avg over MMVTS Baseline3, from 2.65 to 4.18. These results demonstrate that deep cross-modal interaction has notable advantage for multimodal fusion over simple unimodal feature concatenation for VTS. Moreover, adding MoE on top consistently improves Co-Attention on both AVLecture and CLVTS, by 0.47 and 1.07 on Avg; whereas, the effect of MoE on top of Merge-Attention is inconsistent, with a slight degradation on AVLecture and 1.53 gain on Avg on CLVTS. In addition, we conduct more analysis of the effect of Co-Attn with MoE with different numbers of multimodal fusion layers in Appendix G."}, {"title": "(2) Effect of Pre-training tasks", "content": "Table 2 shows that for simple concatenation of unimodal features, pre-training before fine-tuning (as Baseline3) outperforms Baseline2 (w/o PT). We conduct ablation studies of the proposed pre-training and fine-tuning tasks on the AVLecture dataset. We apply the same fine-tuning with multimodal coherence modeling (FT-Coh, Eq. 15) and compare (a) random initialization of parameters of Multimodal Fusion Layers (w/o pre-training) (b) pre-training the model on unlabeled data. Table 3 shows that w/o pre-training, Co-Attn slightly improves Avg over Merge-Attn by 0.4 pts, and MoE further improves Avg by 0.87 pts. Pre-training improves the performance on all four Multimodal Fusion Layer architectures, with the average Avg score increased by 1.36 pts (66.29 \u2192 67.65). Pre-training also improves stability of a model as the standard deviations of all w/ PT experiments are less than 1. Table 6 in Appendix further compares the fine-tuning performance after applying different pre-training tasks. Compared to using both pre-training tasks, removing loss $l_{cma}$ or $l_{vts}$ leads to Avg reduction of 0.4 and 1.64 respectively, indicating that the pretraining task more closely aligned with the downstream task, i.e., $l_{vts}$, yields greater benefits."}, {"title": "(3) Effect of Fine-tuning tasks", "content": "Table 4 compares the efficacy of different fine-tuning tasks after pre-training. Compared to the standard $l_{vts}$, adding the two auxiliary losses $l_{cma}$ and $l_{mcssl}$ notably improves Avg by 1.12 pts, while decreases $F_1$ by 0.73. These results suggest that while multimodal coherence modeling may slightly compromise the precision of exact matches, it enhances the overall contextual comprehension of a model for VTS. Adding $l_{cma}$ or $l_{mcssl}$ individually improves Avg by 0.53 and 0.57 respectively, with improvements mainly on BS@30 and mIoU, suggesting that feature alignment at different granularities may improve fuzzy matching of a model."}, {"title": "Conclusion", "content": "In conclusion, we propose a supervised MultiModal Video Topic Segmentation (MMVTS) model to advances VTS by meticulously investigating multimodal fusion and coherence modeling, coupled with an innovative pre-training task tailored for VTS. Furthermore, we introduce a large-scale Chinese Lecture Video Topic Segmentation (CLVTS) dataset to promote research in VTS."}, {"title": "Limitations", "content": "It is essential to acknowledge that our methodology solely capitalizes on the multimodal information derived from visual and textual data, with the parameters of the visual encoder fixed due to consideration about computational complexity. This specific limitation potentially results in a less-than-optimal exploitation of the vast visual information contained within the dataset. Concurrently, features from additional modalities, such as audio, are not incorporated. Additionally, we will conduct more investigations and study approaches to make Co-Attn with MoE more stable in future work. Finally, the integration of general multimodal pre-trained models and large language models could be investigated, offering a more holistic and effective exploration of multimodal information for VTS."}, {"title": "A Related Work to VTS", "content": "Wu et al. (2023) utilized visual, audio and textual information, and focus on hierarchical modeling techniques to simultaneously learn the boundaries of scenes, stories and topics, which suggests that the segmentation can be benefited by introducing the hierarchical structures of videos. Nevertheless, their approach did not delve into optimizing the integration of multimodal information, which is one of the motivations of our work. The integration of audio features represents a direction for our future work. Xing et al. (2024) encode the image and text sequences with respective encoders and use an asymmetric cross-modal cross-attention mechanism to produce text-aware visual representations, followed by a BiLSTM, to fuse multimodal features. Our approaches are drastically different from Xing et al. (2024) as we investigate symmetric Cross-Modal Cross-Attention and also Mixture-of-Experts mechanism to fuse multimodal information. Xing et al. (2024) also investigate cross-modal and sentence level intra-modal contrastive learning to empower unsupervised domain adaptation, whereas we explore extending the topic level Contrastive Semantic Similarity Learning (CSSL) task proposed by Yu et al. (2023) to the multimodal setting."}, {"title": "Details of Annotations for the CLVTS Dataset", "content": "For the CLVTS dataset, we ensure high accuracy and reliability of annotations from three aspects. Firstly, before the actual annotation process, the annotators take two rounds of training. Each annotator needs to annotate 5 videos in each round; at the end of each round, we assess the annotation quality, provide feedback, and ensure that the annotators address the issues and understand the annotation guideline clearly at the end of training. Secondly, during annotation, to help the annotators thoroughly understand the course content, we ask the annotators to annotate topics hierarchically, that is, they label both coarse-grained topic (large topic) and fine-grained topic (small topic) boundaries while the large topic boundaries are a subset of the small topic boundaries. We take the small topic boundaries as the final topic boundary labels. Thirdly, we employ a multi-annotator strategy. All data is annotated in batches, with two annotators annotating each sample independently. The third annotator reviews the annotations by the first two annotators, rectifies errors, and provides the final annotations. After the annotations of each batch, we randomly select 5% of a batch for quality assessment. If the unacceptable rate (the proportion of the wrongly annotated topic boundaries) is less than 10%, the data are deemed satisfactory and accepted; otherwise, after communicating quality assessment results and possible reasons for errors, the annotators are requested to re-annotate the batch based on the feedback. This quality control process is repeated until the unacceptable rate is lower than 10% for all batches; in this work, we could finish quality control within 3 iterations."}, {"title": "Training Details", "content": "Our experiments are implemented with transformers package7. For video with the number of clips greater than the max sequence length", "types": "OCR, 2D and 3D. Specifically, the OCR feature is derived by encoding the textual output obtained from the OCR API of the clip's central image. This encoding task is performed using the BERT-based sentence transformer model, where all-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 models are employed for English and Chinese language experiments, respectively. The 3D feature are extracted using the same video feature extraction pipeline as in Gupta et al. (2023), while the 2D feature is extracted by visual encoder of CLIP (Radford et al., 2021) which is pre-trained to predict if an image and a text snippet are paired together. Specifically, the images from AVLecture and CLVTS are processed to extract 2D features using CLIPViT-B/1610 and CN-CLIPViT-B/1611, respectively.\nAfter extracting the visual features, we concatenate them as shown in Eq. 16 to get $v_i$, which then will be fed into following projection layer. During pre-training and fine-tuning, the parameters of visual encoders are kept fixed. The learning rate is 5e - 5 and dropout probability is 0.1. AdamW (Loshchilov and Hutter, 2017) is used for optimization. The batch size is 8 and the epoch for pre-training and fine-tuning is 1 and 5, respectively. The loss weight $\u03b1$ and $\\gamma$ for $l_{cma}$ is 0.5, $\\beta$ and $\u03c3$ for $l_{balance}$ is 1.0 when MoE is in multimodal fusion layer. 0 for $l_{mcssl}$ is 0.5. $k_1$ and $k_2$ of Eq. 14 is 1 and 3 following Yu et al. (2023). We select one multimodal fusion layer to comprehensively compare different types of fusion structure. We also investigate the performance of different number of multimodal fusion layer in Figure 3. As for the MoE layer in multimodal fusion layer, we choose 4 candidate experts and activate 2 experts for each input feature, the intermediate size of expert is 3072. Therefore, the total number of trainable parameters is shown in Table 5."}]}