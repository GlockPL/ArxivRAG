{"title": "Multimodal Fusion and Coherence Modeling for Video Topic Segmentation", "authors": ["Hai Yu", "Chong Deng", "Qinglin Zhang", "Jiaqing Liu", "Qian Chen", "Wen Wang"], "abstract": "The video topic segmentation (VTS) task seg- ments videos into intelligible, non-overlapping topics, facilitating efficient comprehension of video content and quick access to specific con- tent. VTS is also critical to various down- stream video understanding tasks. Traditional VTS methods using shallow features or un- supervised approaches struggle to accurately discern the nuances of topical transitions. Re- cently, supervised approaches have achieved superior performance on video action or scene segmentation over unsupervised approaches. In this work, we improve supervised VTS by thor- oughly exploring multimodal fusion and mul- timodal coherence modeling. Specifically, (1) we enhance multimodal fusion by exploring dif- ferent architectures using cross-attention and mixture of experts. (2) To generally strengthen multimodality alignment and fusion, we pre- train and fine-tune the model with multimodal contrastive learning. (3) We propose a new pre-training task tailored for the VTS task, and a novel fine-tuning task for enhancing multi- modal coherence modeling for VTS. We eval- uate the proposed approaches on educational videos, in the form of lectures, due to the vi- tal role of topic segmentation of educational videos in boosting learning experiences. Ad- ditionally, we introduce a large-scale Chinese lecture video dataset to augment the existing English corpus, promoting further research in VTS. Experiments on both English and Chinese lecture datasets demonstrate that our model achieves superior VTS performance compared to competitive unsupervised and supervised baselines\u00b9.", "sections": [{"title": "1 Introduction", "content": "The proliferation of digital video content over the last few decades has underscored the importance of efficient content navigation and comprehension.\nAs the unstructured nature of videos poses signifi- cant challenges for users seeking to quickly grasp or reference specific topics, Video Topic Segmenta- tion (VTS) has emerged as a vital tool in addressing these demands. By delineating videos into coherent non-overlapping topics, VTS not only facilitates intuitive understanding of video content but also enables swiftly pinpointing and accessing topics of interest. This is particularly pertinent for the furtherance of various video understanding tasks, where VTS serves as a foundational component.\nTraditional VTS techniques predominantly hinge on shallow features (Gandhi et al., 2015; Soares and Barr\u00e9re, 2018b; Ali et al., 2021) and unsupervised methods (Gupta et al., 2023) due to the scarcity of labeled data. These methods of- ten fall short in capturing the semantic cues that signal topical shifts in video streams, hence suf- fer from limited precision. Recent advancements in supervised learning paradigms, benefiting from the availability of large-scale labeled data, have achieved notable performance improvements in various video segmentation tasks, such as video action segmentation (Zhou et al., 2018; Tang et al., 2019), scene segmentation (Huang et al., 2020; Islam et al., 2023), and topic segmentation (Wu et al., 2023; Wang et al., 2023; Xing et al., 2024), surpassing unsupervised methods. Performance can be further enhanced by pre-training on vast volumes of unlabeled data (Xu et al., 2021; Mun et al., 2022) or initializing models from pre-trained models (Yan et al., 2023) and then fine-tuning the model. Hence, in this work, we focus on further improving supervised methods for VTS.\nCompared to text topic segmentation (Koshorek et al., 2018; Xing and Carenini, 2021; Yu et al., 2023), videos contain rich and diverse multimodal contextual information. Fully utilizing multimodal information, such as visual cues and textual data (e.g., screen text and subtitles), could facilitate more detailed content understanding and in turn"}, {"title": "2 Related Work", "content": "Text Topic Segmentation The objective of text topic segmentation is to automatically partition text into topically consistent, non-overlapping seg- ments (Hearst, 1994). By automatically mining clues of topic shifts from large amounts of labeled data (Koshorek et al., 2018; Arnold et al., 2019), contemporary supervised models (Lukasik et al., 2020; Somasundaran et al., 2020; Zhang et al., 2021; Yu et al., 2023) have demonstrated supe- rior performance compared to unsupervised ap- proaches (Riedl and Biemann, 2012; Solbiati et al., 2021). Notably, supervised models that excel at modeling long sequences (Zhang et al., 2021; Yu et al., 2023) are capable of capturing longer contex- tual nuances and thereby achieve better topic seg- mentation performance, compared to models that model local sentence pairs or block pairs (Wang et al., 2017; Lukasik et al., 2020). In addition, recent works (Somasundaran et al., 2020; Xing et al., 2020; Yu et al., 2023) have demonstrated that strengthening coherence modeling can improve the topic segmentation performance. Therefore, in this work, we explore enhancing coherence modeling for video topic segmentation under the multimodal configurations.\nVideo Topic Segmentation Distinct from text topic segmentation, video topic segmentation is inher- ently characterized by its multimodal data context; hence, comprehensive utilization of the multimodal information could improve semantic understand- ing capabilities of a model. Specifically, Gupta et al. (2023) introduce an unsupervised method that employs the TWFINCH algorithm (Sarfraz et al., 2021) to cluster video clips into distinct top- ics based on learned visual and text features, which are derived from self-supervised tasks of matching the narration with the temporally aligned visual content. Wang et al. (2023) simply concatenate the sampled encoded visual features and text embed- ding as the input to a pre-trained language model, which may cause inferior fine-tuning due to the discrepancy between the pre-training stage of the language model and the fine-tuning stage combin- ing textual and visual inputs. Other works related to VTS are summarized and compared in Appendix A."}, {"title": "3 The CLVTS Dataset", "content": "Collection Procedure The CLVTS dataset is pri- marily sourced from Public Video Platforms23. Videos are first transcribed by a competitive au- tomatic speech recognition (ASR) system. We then ask annotators to combine visual and textual"}, {"title": "4 Methodology", "content": "Figure 2 depicts the architecture of our VTS model. Section 4.1 presents the problem definition of mul- timodal VTS and the overall architecture. We en- hance multimodal fusion from the perspectives of model architecture, pre-training, and fine-tuning tasks. Specifically, we compare different archi- tectures built upon Merge- and Cross-Attention, and Mixture-of-Experts for multimodal fusion (Sec- tion 4.1). We explore multimodal contrastive learn- ing for cross-modality alignment and propose a new pre-training task tailored for VTS (Section 4.2). For fine-tuning, we also propose a novel task for multimodal coherence modeling (Section 4.3)."}, {"title": "4.1 Multi-Modal Video Topic Segmentation", "content": "Overall Architecture Following prior works (Zhang et al., 2021; Wu et al., 2023), we define video topic segmentation as a clip- level sequence labeling task and propose our MultiModal Video Topic Segmentation (MMVTS) model. As illustrated in Figure 2a, we apply uni- modal pre-trained encoders for each modality and then fuse multimodal information at the intermedi- ate representation level (i.e., 'middle fusion' (Xu et al., 2023)) through Multimodal Fusion Layers. Given a video, we transcribe it with a competitive automatic speech recognition (ASR) system and use ASR 1-best as the text modality. We then divide the video into n clips $(c_1, c_2)_{i=1}^n$, with clips segmented at the sentence boundaries predicted on ASR 1-best. $c_i = {f_1^i, ..., f_k^i}$ denotes evenly sampled k frames from the i-th clip and is fed to a visual encoder $E_v$ to extract visual features. $c_i^t = {w_1, ..., w_{|s_i|+1}}$ denotes the sequence of words from ASR 1-best within the i-th clip, where $w_i$ is the inserted special token [BOS] and $||s_i||$ denotes the number of words of i-th clip. $c_i^t$ is fed to a text encoder $E_t$ for extracting textual features. After extracting unimodal features, we first apply trainable projection matrices $W_v$ and $W_t$ to convert unimodal features into the same dimension, as visual feature sequence $v = {v_1, ..., v_n}$ and textual feature sequence $t = {t_1, ..., t_n}$ (Eq. 1). Then we fuse the multimodal information with $M$ Multimodal Fusion Layers $MFL_M$ and obtain the updated visual features $h^v = {h_1^v, ..., h_n^v}$ and textual features $h^t = {h_1^t,..., h_n^t}$ (Eq. 2), which are then concatenated into the multimodal features $m = {m_1, ..., m_n}$ (Eq. 3). Finally, the multimodal features m are fed into the predictor composed of a linear layer $W_p$ to obtain the prob- ability of binary classification $p = {p_1,...,p_n}$ (Eq. 4), where $p_i$ indicates whether the i-th clip is at the boundary of the topic that it belongs to. We use the standard binary cross-entropy loss (Eq. 5) to train the model, where $y_i \u2208 {0,1}$ is the label. The last clip is excluded from loss computation.\nCompared to 'late fusion' where no cross-modal interaction happens until after the classifiers, mid- dle fusion and \u2018early fusion' are found to gener- ally outperform late fusion (Nagrani et al., 2021), probably because early and middle fusion aligns better with human perception where multimodal fusion happens early in sensory processing. On the other hand, compared to early fusion, middle fusion yields superior or comparable performance (Na- grani et al., 2021) and is much less computationally expensive since we could freeze partially strong pre-trained unimodal encoders and only train a small number of parameters in the Multimodal Fu- sion Layers.\n$v = W_v . E_v ({c_1^v, ..., c_n^v})$\n$t = W_t . E_t ({c_1^t, ..., c_n^t})$"}, {"title": "Multimodal Fusion Layer (MFL)", "content": "We compare four different cross-modal interaction mechanisms for multimodal fusion layers. We investigate the Merge-Attention and Co-Attention multimodal fu- sion layers proposed in (Dou et al., 2022; Yang et al., 2023). With Merge-Attention (Figure 2b), features from unimodal encoders are sequentially concatenated and then input into a standard trans- former encoder layer (Vaswani et al., 2017), which shares attention parameters across modalities. A feed forward layer is added on top to produce the final output representation. In contrast, with Co-Attention (Figure 2c), features from each uni- modal encoder first go through self-attention with modality-specific attention parameters, then we per- form symmetrical cross-attention to integrate in- formation from all other modalities to enhance the representation of the considered modality, followed by a feed forward layer.\nInspired by (Mustafa et al., 2022), which inter- leaves MoE encoder layers and standard dense en- coder layers for image-text multimodal models, we also investigate two new architectures by re- placing the traditional single feed-forward layers in Figure 2b and 2c with a Mixture-of-Experts (MoE) (Shazeer et al., 2016; Lepikhin et al., 2020) module respectively, depicted by Figure 2d and 2e. The motivation is that adding MoE on top of the fused representations may facilitate deeper cross-modal integration of information and im- prove model capacity without a proportional in- crease in computational complexity. Specifically, experts are MLPs activated depending on the input. Firstly, we concatenate fused features output from self-attention or cross-attention. Then, we imple- ment the Noisy Top-k Gating mechanism (Shazeer et al., 2016) to select K experts from a total of E candidates (Eq. 6 - 8), where $SN()$ denotes stand normal distribution, $W_n$ denotes tunable Gaussian noise to help load balancing, $W_g$ is a trainable weight matrix, K and E are hyper-parameters. Fi- nally, the outputs of the K activated experts are linearly combined according to the learned gating weights (Eq. 9). For the MoE training objective, we sum importance loss and load loss (Shazeer et al., 2016) to balance expert utilization as Eq. 10.\n$G(x) = Softmax(KeepTopK(Hx,k))$\n$H(x) = (W_g . x)_i + SN() . Softplus((W_n . x)_i)$\n$KeepTopK(x, k)_i = \\begin{cases}  x_i & \\text{if } x_i \\text{ is in top-k.} \\\\  -\\infty & \\text{otherwise.} \\end{cases}$\n$MoE(x) = \\sum_{e=1}^K G(x)_e . MLP_e(x)$\n$l_{balance} = l_{importance} + l_{load}$"}, {"title": "4.2 Pre-training with Unlabeled Data", "content": "Prior works have demonstrated that standard self- supervised denoising pre-training (even using only the downstream task data) (Amos et al., 2023) or pre-training adapted to the downstream task (Gu- rurangan et al., 2020) often perform substantially better than randomly initializing the parameters. Therefore, to better initialize the parameters of the Multimodal Fusion Layers, we explore pre-training with unlabeled video data before fine-tuning on the labeled training set. Firstly, we introduce a general cross-modality alignment pre-training task to learn the multi-modal representation. We use contrastive learning loss to adjust the features learned by the model fusion layer, by maximizing the cosine similarity of the visual features and tex- tual features of the same clip, while reducing the similarity of the modality features between differ- ent clips, as show in Eq. 11, where $\u03f5$ is used to prevent division by 0 and $\u03c4$ is a temperature hyper- parameter to scale the cosine similarity.\n$\\mathcal{l}_{cma} = - \\frac{1}{n} \\sum_{i=1}^n \\frac{e^{\\text{sim}(h_i^v, h_i^t) / \\tau}}{\\sum_{j=1}^n e^{\\text{sim}(h_i^v, h_j^t) + \\epsilon}}$\n$\\text{sim}(x_1, x_2) = \\frac{x_1^T x_2}{||x_1|| ||x_2||}$\nSecondly, we introduce a novel pre-training task tailored for the downstream VTS task, focus- ing on utilizing unlabeled data for learning pseudo topic boundaries and also enhancing modality alignment. Initially, a Kernel Density Estimation (KDE) (Davis et al., 2011) model is employed to approximate the topic duration distribution of the training set; then, videos are split into seg- ments matching durations sampled from the KDE- derived distribution. Subsequently, for every re- sulting segment, we undertake one of three actions with an equal probability: inserting segments ran- domly selected from other videos either before or after the current segment, replacing the current segment with a segment randomly selected from"}, {"title": "4.3 Fine-tuning with Multimodal Coherence Modeling", "content": "For fine-tuning, we introduce two auxiliary tasks to enhance multimodal coherence modeling. The cross modal alignment task is the same task in Eq. 11 used in pre-training. This continuity ensures that the modalities retain their coherence through both pre-training and fine-tuning, fostering a con- sistent interplay between different modalities. In addition, we adapt the Contrastive Semantic Simi- larity Learning (CSSL) task proposed by Yu et al. (2023), which leverages the inherent characteris- tics of topic-related coherence, to the multimodal context. We adopt the same strategy for selecting positive and negative sample pairs (Yu et al., 2023), but extend the features to the multimodal represen- tations, as shown in Eq. 14, where $k_1$ and $k_2$ are hyper-parameters that determine the number of pos- itive and negative pairs. For each clip's multimodal representation $m_i$, $m_i^+$ denotes the multimodal representation of the j-th similar clip in the same topic as clip i, while $m_{ij}^-$ denotes the multimodal representation of the j-th dissimilar clip in a dif- ferent topic from clip i. We hypothesize that this extension could improve multimodal representa- tion learning by identifying relative consistency relations within and across topics.\n$\\mathcal{l}_{mcssl} = - \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{\\sum_{j=1}^{k_1} e^{\\text{sim}(m_i, m_i^+)}}{\\sum_{j=1}^{k_1} e^{\\text{sim}(m_i, m_i^+)} + \\sum_{j=1}^{k_2} e^{\\text{sim}(m_i, m_{ij}^-)} }$\nThe overall fine-tuning objective combines Eq. 5, 10, 11, and 14, as shown in Eq. 15, where $\u03c3$, $\u03b8$, and $\u03b3$ are hyper-parameters to adjust loss contribution. When the multimodl fusion layer does not contains MoE structure, $\u03b2$ in Eq. 13 and $\u03c3$ are set to zero.\n$\\mathcal{l}_{finetune} = \\mathcal{l}_{uts} + \u03c3l_{balance} + \u03b8l_{mcssl} + \u03b3l_{cma}$"}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets We partition the labeled data within AVLecture into 70% for training, 10% for vali- dation, and 20% for test sets. We also split the CLVTS dataset into train, validation, and test sets with identical ratios. The unlabeled data of AVLec- tures and CLVTS are used for pre-training on each dataset, respectively.\nBaselines We carefully select the following repre- sentative baselines:\nUnsupAVLS (Gupta et al., 2023) is an unsuper- vised approach that clusters video clips into dis- crete topics based on visual and text embeddings"}, {"title": "5.2 Results and Analysis", "content": "Table 2 reports the performance of baselines (the first group) and our MMVTS model variants (the second and third group) on the AVLecture and CLVTS test sets.\nUnimodal performance. The text-only Long- former (Row3) substantially outperforms the visual-only BaSSL (Row2) on BS@30 by a large gain (+25.31), and also surpasses the unsupervised approach UnsupAVLS by a notable gain (+13.25). Such results are expected since the textual modality inherently conveys more precise and richer infor- mation than the visual modality for VTS.\nMutimodal performance. As can be seen from Ta- ble 2, Avg of SW STseq closely aligns with that of the text-only LongFormer, suggesting that more data may be necessary to mitigate the discrep- ancy between fine-tuning with early fusion and pre-training the language model, in order to fully exploit the potential of the early fusion strategy. Our MMVTS Baseline1,2,3 simply concatenate uni- modal features to predict topic boundaries. With- out pre-training on unlabeled data (PT, Eq. 13) and the two auxiliary fine-tuning tasks to enhance multimodal coherence modeling (FT-Coh, Eq. 15), on F1, MMVTS Baseline\u2081 outperforms the text- only Longformer by 2.28 and 2.90 on AVLec- ture and CLVTS respectively, while on Avg score, MMVTS Baseline1 outperforms Longformer by 1.1 on AVLecture yet slightly underperforms Long- former by 0.65 on CLVTS. These results suggest that simply concatenating unimodal features to pre- dict topic boundaries does not bring consistent gains over unimodality. The third group in Table 2 compares the four Multimodal Fusion Layer ar- chitectures with pre-training (PT) and fine-tuning (FT-Coh). Overall, compared to all the competi- tive unsupervised and supervised baselines, af- ter pre-training and fine-tuning, our MMVTS model using Co-Attention with MoE as Mul- timodal Fusion Layers achieves the best Avg, BS@30, and F\u2081 @30 results and sets new SOTA on AVLecture, and achieves the best F\u2081 and a Avg score comparable to the best result on CLVTS. The gains on Avg from our Co-Attention MoE with PT and FT-Coh over LongFormer.cssl and MMVTS Baseline\u2081 are statistically significant (p-values are less than 0.05). It is also notable that the performance of models on CLVTS is gener- ally much lower than that on AVLecture, with the best Avg score between AVLecture and CLVTS dif- fering by 17.50 (68.84 versus 51.34), indicating a greater challenge posed by our CLVTS dataset than the AVLecture dataset.\nTable 2 also shows that on CLVTS, fine-tuning the powerful pre-trained Llama-3-8B with our Dis- crete prompt achieves the best Avg score 51.34, although very close to the performance of Merge- Attn with MoE and Co-Attn with MoE after PT and"}, {"title": "FT-Coh", "content": "However, on AVLecture, the performance of fine-tuning Llama-3-8B is much lower than Mul- timodal Fusion Layer with PT and FT-Coh; particu- larly, F\u2081 on both AVLecture and CLVTS are much lower because the model's predicted boundaries often have a clip offset, but its F\u2081 @30 and mIoU results are still best on CLVTS.\nWe conduct ablation studies and analysis to vali- date effectiveness of the proposed multimodal fu- sion layers, pre-training and fine-tuning tasks.\n(1) Effect of Multimodal Fusion Layers. Com- paring the third group and Baseline3 in Table 2 shows that with the same pre-training and fine- tuning, the best performing architecture using Mul- timodal Fusion Layers always substantially out- performs Baseline3. Specifically, with PT and FT- Coh, on AVLecture, both Co-Attention and Co- Attention with MoE notably outperform MMVTS Baseline3 by 1.32 on Avg; on CLVTS, all four Mul- timodal Fusion Layer architectures achieve remark- able gains on Avg over MMVTS Baseline3, from 2.65 to 4.18. These results demonstrate that deep cross-modal interaction has notable advantage for multimodal fusion over simple unimodal fea- ture concatenation for VTS. Moreover, adding MoE on top consistently improves Co-Attention on both AVLecture and CLVTS, by 0.47 and 1.07 on Avg; whereas, the effect of MoE on top of Merge- Attention is inconsistent, with a slight degradation on AVLecture and 1.53 gain on Avg on CLVTS. In addition, we conduct more analysis of the effect of Co-Attn with MoE with different numbers of multimodal fusion layers in Appendix G.\n(2) Effect of Pre-training tasks. Table 2 shows that for simple concatenation of unimodal features, pre-training before fine-tuning (as Baseline3) out- performs Baseline2 (w/o PT). We conduct ablation studies of the proposed pre-training and fine-tuning tasks on the AVLecture dataset. We apply the same fine-tuning with multimodal coherence modeling"}, {"title": "(3) Effect of Fine-tuning tasks", "content": "Table 4 compares the efficacy of different fine-tuning tasks after pre- training. Compared to the standard luts, adding the two auxiliary losses lema and Imessi notably improves Avg by 1.12 pts, while decreases F\u2081 by 0.73. These results suggest that while multimodal coherence modeling may slightly compromise the precision of exact matches, it enhances the overall contextual comprehension of a model for VTS. Adding lema or lmcss\u0131 individually improves Avg by 0.53 and 0.57 respectively, with improve- ments mainly on BS@30 and mIoU, suggesting that feature alignment at different granularities may im- prove fuzzy matching of a model."}, {"title": "6 Conclusion", "content": "In conclusion, we propose a supervised Mul- tiModal Video Topic Segmentation (MMVTS) model to advances VTS by meticulously investi- gating multimodal fusion and coherence modeling, coupled with an innovative pre-training task tai- lored for VTS. Furthermore, we introduce a large-scale Chinese Lecture Video Topic Segmentation (CLVTS) dataset to promote research in VTS."}, {"title": "Limitations", "content": "It is essential to acknowledge that our methodology solely capitalizes on the multimodal information derived from visual and textual data, with the pa- rameters of the visual encoder fixed due to con- sideration about computational complexity. This specific limitation potentially results in a less-than- optimal exploitation of the vast visual information contained within the dataset. Concurrently, fea- tures from additional modalities, such as audio, are not incorporated. Additionally, we will conduct more investigations and study approaches to make Co-Attn with MoE more stable in future work. Fi- nally, the integration of general multimodal pre- trained models and large language models could be investigated, offering a more holistic and effective exploration of multimodal information for VTS."}, {"title": "A Related Work to VTS", "content": "Wu et al. (2023) utilized visual, audio and textual information, and focus on hierarchical modeling techniques to simultaneously learn the boundaries of scenes, stories and topics, which suggests that the segmentation can be benefited by introducing the hierarchical structures of videos. Nevertheless, their approach did not delve into optimizing the integration of multimodal information, which is one of the motivations of our work. The integra- tion of audio features represents a direction for our future work. Xing et al. (2024) encode the im- age and text sequences with respective encoders and use an asymmetric cross-modal cross-attention mechanism to produce text-aware visual represen- tations, followed by a BiLSTM, to fuse multimodal features. Our approaches are drastically different from Xing et al. (2024) as we investigate symmet- ric Cross-Modal Cross-Attention and also Mixture- of-Experts mechanism to fuse multimodal infor- mation. Xing et al. (2024) also investigate cross-modal and sentence level intra-modal contrastive learning to empower unsupervised domain adapta- tion, whereas we explore extending the topic level Contrastive Semantic Similarity Learning (CSSL) task proposed by Yu et al. (2023) to the multimodal setting."}, {"title": "B Details of Annotations for the CLVTS Dataset", "content": "For the CLVTS dataset, we ensure high accuracy and reliability of annotations from three aspects. Firstly, before the actual annotation process, the annotators take two rounds of training. Each an- notator needs to annotate 5 videos in each round; at the end of each round, we assess the annota- tion quality, provide feedback, and ensure that the annotators address the issues and understand the annotation guideline clearly at the end of training. Secondly, during annotation, to help the annota- tors thoroughly understand the course content, we ask the annotators to annotate topics hierarchically, that is, they label both coarse-grained topic (large topic) and fine-grained topic (small topic) bound- aries while the large topic boundaries are a subset of the small topic boundaries. We take the small topic boundaries as the final topic boundary labels. Thirdly, we employ a multi-annotator strategy. All data is annotated in batches, with two annotators annotating each sample independently. The third annotator reviews the annotations by the first two"}, {"title": "C Training Details", "content": "Our experiments are implemented with transform- ers package7. For video with the number of clips greater than the max sequence length, we use slid- ing window and take the last clip of the prior sam- ple as the start clip of the next sample. All su- pervised models use a threshold strategy, where clips with scores above a threshold 0.5 are pre- dicted as topic boundaries. Following (Gupta et al., 2023), for each video clip, we extract three vi- sual feature types: OCR, 2D and 3D. Specifically, the OCR feature is derived by encoding the tex- tual output obtained from the OCR API of the clip's central image. This encoding task is per- formed using the BERT-based sentence transformer model, where all-mpnet-base-v2 and paraphrase- multilingual-mpnet-base-v2 models are employed for English and Chinese language experiments, re- spectively. The 3D feature are extracted using the same video feature extraction pipeline as in Gupta et al. (2023), while the 2D feature is extracted by vi- sual encoder of CLIP (Radford et al., 2021) which is pre-trained to predict if an image and a text snip- pet are paired together. Specifically, the images from AVLecture and CLVTS are processed to ex- tract 2D features using CLIPViT-B/1610 and CN- CLIPViT-B/1611, respectively.\nAfter extracting the visual features, we concate- nate them as shown in Eq. 16 to get $v_i$, which then will be fed into following projection layer. During pre-training and fine-tuning, the parame- ters of visual encoders are kept fixed. The learn- ing rate is 5e - 5 and dropout probability is 0.1. AdamW (Loshchilov and Hutter, 2017) is used for optimization. The batch size is 8 and the epoch for pre-training and fine-tuning is 1 and 5, respectively. The loss weight $\u03b1$ and $\u03b3$ for $l_{cma}$ is 0.5, $\u03b2$ and $\u03c3$ for $l_{balance}$ is 1.0 when MoE is in multimodal fusion layer. 0 for $l_{mcssl}$ is 0.5. $k_1$ and $k_2$ of Eq. 14 is 1 and 3 following Yu et al. (2023). We select one multimodal fusion layer to comprehensively com- pare different types of fusion structure. We also investigate the performance of different number of multimodal fusion layer in Figure 3. As for the MoE layer in multimodal fusion layer, we choose 4 candidate experts and activate 2 experts for each in- put feature, the intermediate size of expert is 3072. Therefore, the total number of trainable parameters is shown in Table 5.\n$v_i = \\begin{bmatrix}v_{ocr} \\\\ v_{2d} \\\\ v_{3d} \\end{bmatrix}$"}, {"title": "D Evaluation Metrics", "content": "F1 is a metric used to evaluate the accuracy of text topic segmentation (Lukasik et al., 2020; Zhang et al., 2021), which focus on the performance of exact matching of the positive class and balances precision and recall rate.\nBS@k (Gupta et al., 2023) is the average number of predicted boundaries matching with the ground truth boundaries within a k second interval, which can be regard as the recall rate based on fuzzy matching."}, {"title": "E Artifact Use Consistent With Intended Use", "content": "All of our use of existing artifacts is consistent with their intended use, provided that it was specified."}, {"title": "F Fine-tune Llama-3-8B for Video Topic Segmentation", "content": "Considering the computational complexity and data volume, we employ LoRA (Hu et al., 2021) for fine-tuning Llama-3-8B15 with a maximum sequence length of 2048. Our training configuration includes a batch size of 32 and a total of 6 epochs, utiliz- ing a learning rate of 5e - 5 and cosine sched- uler (Loshchilov and Hutter, 2016). Table 7 shows two prompts we used to fine-tuning Llama-3-8B on text data of VTS. We first try the Generative type that Yu et al. (2023) has experimented with due to the better zero-shot and one-shot performance than the Discriminative type. However, compared with Longformer, the Avg score of Generative prompt- ing Llama-3-8B is lower 8.3 and 6.27 on AVLec- ture and CLVTS respectively. We suspect that this is due to the issue of sparse labels in binary classifi- cation, leading us to refine the Discriminative into the Discrete prompt as outlined in the Table 7. Al- though the F\u2081 on AVLecture and CLVTS is much lower (the prediction boundary of the model usu- ally has a clip offset), the Avg score of Llama-3-8B on AVLecture is merely 2.26 points behind Long- Former. Moreover, Llama-3-8B Discrete secures the highest Avg score on CLVTS, surpassing Long- Former by 4.53 points. which suggests that the Discrete prompt is more suitable for stimulating Llama-3-8B and future work can further explore how to better use large language models for VTS."}, {"title": "G Performance of MFLs across varied layer numbers", "content": "Utilizing Co-Attention with Mixture of Experts (MoE) as the multimodal fusion layer, we investi- gated the Avg performance of models featuring var- ious numbers of Multimodal Fusion Layers (MFLs) on AVLecture, as depicted in Figure 3. The find- ings indicate that when the model is fine-tuned directly on labeled data, a single MFL yields the best performance, surpassing the no-layer configu- ration by 2.34 points. However, adding more layers leads to diminished performance and training insta- bility, particularly noticeable with three layers. By incorporating a pre-training phase followed by fine- tuning through a coherence modeling task, perfor-"}]}