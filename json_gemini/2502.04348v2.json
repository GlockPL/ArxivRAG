{"title": "Prompt-based Depth Pruning of Large Language Models", "authors": ["Juyun Wee", "Minjae Park", "Jaeho Lee"], "abstract": "Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent\u2014a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.", "sections": [{"title": "1. Introduction", "content": "Recent advances in large language models (LLMs) have achieved remarkable success in a wide range of natural language processing tasks. However, significant computational requirements of LLMs pose challenges in resource-constrained environments, limiting their practicality. For example, LLaMA-3.3-70B needs 140GB of RAM to be loaded in bf16, which is often too big for memory-constrained local devices. Thus, reducing the model size is essential to make LLMs feasible for on-device applications.\nDepth pruning is a versatile model compression technique that is particularly effective for on-device scenarios. Such methods simply remove several transformer blocks (which we call \"omission set\") from the pretrained model, based on some measures of block"}, {"title": "2. Related Work", "content": "In this section, we provide an in-depth comparison of the proposed framework against existing depth and width sparsity frameworks.\n2.1. Static Depth Pruning\nStatic depth pruning methods select and remove unnecessary blocks from a pretrained LLM using various proxy metrics to measure the importance of the blocks. measures the block importance using the expected cosine similarity between the input and output activations of the block; a block that does not change the direction of the activation is deemed unnecessary. directly measures the perplexity drop after removing each transformer block, and combines this idea with an iterative pruning.\nSeveral recent works also focus on layer-level depth pruning, instead of removing an entire transformer block. In particular, discover that pruning out self-attention layers have a much less significant impact than removing the feed-forward layers.\nUnlike these works, this paper aims to perform dynamic depth pruning using the prompts for the downstream tasks; to account for this difference, we design and use new likelihood-based metrics to measure the block importance.\n2.2. Dynamic Token Routing\nInspired by the success of mixture-of-experts , several recent works have developed mechanisms to route tokens through only a fraction of all transformer blocks. adopts the depth sparsity during the training phase with a jointly trained router, to reduce the training cost of LLMs. Here, the trained router can also be used at inference. trains a router that can be applied on pretrained LLMs to reduce their inference cost.\nOur approach differs from both of these works in the sense"}, {"title": "2.3. Side Note: Width Pruning Techniques", "content": "Width pruning, i.e., pruning out columns and rows of the weight matrices in a structured manner, has also been popularly used for LLM compression. Recent examples include for the category of static width pruning, and for the dynamic width pruning.\nAs these methods tend to alter the size of the weight matrices, which has been highly customized for hardware considerations, they often lead to further challenges in deployment. In this work, we thus mainly focus on comparison with depth pruning, but also compare with width pruning in terms of the accuracy."}, {"title": "3. A Motivating Observation", "content": "Before describing the proposed framework, we briefly describe a motivating observation which demonstrate that:\nThe importance of a transformer block in a language\nmodel may be highly task-dependent.\nSetup. To show this point, we have compared the zero-shot accuracies of the LLMs whose omission sets differ by a single transformer block. More concretely, we compare the performance of an omission set $(b_1, b_2,..., b_{k-1}, b_k)$ to another omission set $(b_1, b_2, ..., b_{k-1}, b_k)$, on the LLaMA 3.1-8B model. Here, we have used the to generate an omission set, and then replaced a single block to get another one. Then, we observe the impact of such replacement on three commonsense reasoning tasks:\nBoolQ, PIQA, and WinoGrande.\nResult. Figure 2 illustrates our findings. We observe that pruning out block 29 instead of block 30 has a two-sided impact: On BoolQ, the change makes a dramatic drop in accuracy (62.2% \u2192 38.0%, 62.5% \u2192 37.9%). However, on PIQA and WinoGrande, we observe a slight accuracy boost. This phenomenon suggests that the block 30 may contain more knowledge relevant to answering BoolQ questions, while 29 may be more knowledgeable about PIQA and WinoGrande. This observation highlights the need to consider task variability during the selection of the omission set. To formally address such need, this paper considers an inference of task information from the prompt."}, {"title": "4. Problem Description", "content": "Inspired by the observations in Section 3, we now formalize the problem of prompt-based depth pruning.\nIn a nutshell, given some pretrained LLM and a prompt, the goal of the prompt-based depth pruning is to designate which transformer blocks should be removed from the model to generate the most accurate response to the prompt.\nMore concretely, let x be the prompt given to the model, and let $W = (W_1,..., W_a)$ be the weight parameters of a pretrained LLM consisting of d transformer blocks, with $W_i$ indicating the weights of the ith block. The prediction quality of this language model is measured by the expected loss between the model output and the ground-truth, i.e.,\n$L(W) := E[l((x, y); W)],$ (1)\nwhere $l((,); W)$ is some loss function which also encapsulates the generative procedure of language model with parameter W (e.g., perplexity). In static depth pruning, the goal is to find which blocks to prune from the given LLM. More formally, define omission set as a(n unordered) set of transformer block indices\n$b = \\{b_1, b_2, ..., b_k \\} \u2286 \\{1,2,...,d\\},$ (2)\nwhich designates which blocks will be omitted from the target LLM. Then, let $W\\b$ be a sequence of d \u2013 k weights, with $b_i$th block eliminated from the W. Then, the static depth pruning aims to solve the minimization\n$\\min_{b: |b|\\geq k} L(W\\b),$ (3)"}, {"title": "5. Method", "content": "We now formally describe the proposed PuDDing (Prompt-routed Dynamic Depth Pruning)\u2014an algorithm to train a router b() for the prompt-based depth pruning.\nIn a nutshell, PuDDing operates in two steps:\n1. Generating candidate omission sets using the prompt-answer dataset collected from various tasks (Section 5.1)\n2. Training a router to predict the best option among the candidate omission sets (Section 5.2)\nDuring the inference phase, the given prompt is fed to the router, which predicts which omission set (among the candidates) one should use for the given prompt. Then, the model parameters are loaded from the storage to the high-speed memory to constitute the depth-pruned LLM (see Figure 3).\nWe note that this classification-based approach is in contrast with the approach of dynamic token routing , where one makes yes/no decisions for omitting each block in a sequential manner; this change is to make the router training easier and generalizable.\n5.1. Candidate Omission Set Generation\nThe first step is to generate a candidate pool of omission sets. That is, we generate a family of omission sets\n$B = \\{b_1,..., b_m\\},$ (5)\nwhich will be used as the codomain of the router b(\u00b7), which will simply be an m-class classifier.\nDesirable properties of the candidate set B are as follows:\n\u2022 Coverage: For any realistic prompt-answer pair (x, y) from a wide range of tasks, the set B should contain at least one $b_i$ with a small loss $l(y, f (x; W\\b_i))$.\n\u2022 Cardinality: The number of omission sets m should be sufficiently small, so that one can train a nice predictor for B with a limited number of samples.\nTo obtain these properties, we adopt the following strategy: First, we collect t calibration datasets $D_1,..., D_t$ on a diverse set of downstream tasks. Then, on each calibration dataset, we select the omission set that minimizes some loss criterion, i.e., solve\n$b_i = \\arg\\min_{b} E_{D_i} [l(y; f(x; W\\b)].$ (6)\nHere, the minimization is done in a greedy manner, similar to . We apply l different loss criteria on each calibration dataset to get m = t \u00d7 l omission sets.\nLosses. As the loss function, we use new task-focused variants of the perplexity loss, which we call the task likelihood losses. The perplexity measures the fluency of the generated sentences by measuring the average log-likelihood losses over the whole sequence. That is, for a sample sentence $z = (z_1, z_2,..., z_T)$, the perplexity is\n$ppl(z; W) = \\exp \\frac{1}{T} \\sum_{i=1}^{T} \\log p_i(z_i|z_{<i}; W),$ (7)\nwhere $p_i(\u00b7\u00b7; W)$ denotes the conditional generative probability of the target language model with parameters W, at the ith token. We modify this loss to measure the likelihood only the sequence that matters for on-task performance. That is, if the given datum z can be broken down into the prompt and answer pair:\n$z = (x,y) = (z_1,..., z_S, z_{S+1},..., z_T),$ (8)\n$=\\underbrace{x}=\\underbrace{y}$\nthen we can define the task likelihood (tl) loss as:\n$tl(z; W) = \\frac{1}{T-S} \\sum_{i=S+1}^{T} \\log p_i (z_i|z_{<i}; W).$ (9)\nIn addition, we also consider the task likelihood difference (tld) loss, which is defined as follows: In many tasks, the answer choices are limited (e.g. \"true\" or \"false\"). In such cases, we can also use the likelihood difference of the correct and wrong answers, i.e.,\n$tld(z; W) = tl((x, y); W) \u2013 tl((x, y_{\\text{wrong}}); W),$ (10)\nwhere $y_{\\text{wrong}}$ denotes the wrong version of the answer. We use both tl() and tld(.) as our loss criteria.\nWe note that the task likelihood losses Equations (9) and (10) is different from the perplexity (Equation (7)), in the sense that we do not exponentiate the values. We use this version as it empirically works better than the exponentiated one."}, {"title": "5.2. Router Training", "content": "After generating the candidate omission set B, we train a router that maps the given prompt to the best omission set. Roughly, this is done by first constructing a soft-labeled dataset with task-specific datasets and then training a BERT-based router on the constructed dataset\nDataset Construction. To construct the training dataset, we first collect various prompt-answer pairs from the task datasets, similarly to the calibration datasets in Section 5.1. Then, for each sample, we compute the task likelihood losses on all omission sets, and store them as a label vector. That is, each datum inside the dataset takes the form $(x_i, s_i)$, where $x_i$ is the prompt and the $s_i$ is a length-m vector with\n$s_i = (tl((x_i, y_i); W\\b_1), . . ., tl((x_i, y_i); W\\b_m)).$ (11)\nNote that we no longer need to store the correct answers $y_i$.\nRouter training. We train a router to accurately predict the label vector s given the input prompt x, for all samples in this dataset. That is, we train a function $\\hat{s} = f(x)$ such that $\\hat{s}\\approx s$ holds. We use the MSE loss\n$MSE(s, \\hat{s}) = ||s \u2013 \\hat{s}||^2$ (12)\nto train the router. At inference, we will select the omission set with the minimum-achieving index of the predicted $\\hat{s}$.\nRouter architecture. We use a lightweight transformer-based encoder as our router. More specifically, we insert a single linear layer on pretrained BERT-base , and jointly fine-tune during the training. While this router has more parameters (~110M) than typical routers that are used for dynamic token routing\u2014such as D-LLM which uses 2-layer MLP-the computational cost is bearable as we route only once per prompt. In our experiments, the routing cost typically takes up around 2-4% of the total pre-fill cost."}, {"title": "6. Experiment", "content": "We now empirically validate the performance of the proposed algorithm, PuDDing, on zero-shot tasks against the static depth and width pruning baselines.\n6.1. Experimental Setup\nModels. We evaluate the proposed method on compressing three popular open-weight language models. As the main model, we use the LLaMA-3.1 model with 8B parameters . In addition, we evaluate on two language models: Vicuna 1.5 with 7B and OPT with 6.7B parameters . We use these models for two reasons. First, the models have an appropriate scale for on-device deployments. Second, all three models consist of 32 transformer blocks, and thus can be compared with the same sparsity criterion.\nBaselines. We mainly compare against four recent static depth and width pruning baselines with open source.\n\u2022 SLEB A depth pruning baseline that iteratively selects the omission set based on perplexity.\n\u2022 Shortend LLaMA A depth pruning algorithm which selects the omission set one-shot; here, we compare with the version that uses perplexity as the loss criterion and does not apply LoRA.\n\u2022 FLAP A retraining-free width pruning algorithm based on structural fluctuation metric.\n\u2022 SliceGPT Another width pruning algorithm based on principal component analysis.\nIn addition, we also compare with \"SLEB per prompt,\" which is simply SLEB which is conducted by using each given prompt as the calibration data. As this option does not work well in general, and requires a long inference time, we"}, {"title": "7. Analysis", "content": "We now provide further analyses on PuDDing. In particular, we provide the following analyses: Wall clock speedup (Section 7.1), and visualization of omission sets for tasks (Section 7.2). In Appendix B, we conduct ablation studies."}, {"title": "7.1. Wall-clock Speedup", "content": "We now provide wall-clock analyses and estimates on the latency and throughput of the PuDDing-compressed models.\nInference. Table 6 presents the average wall-clock inference time comparison between the dense and PuDDing-pruned version of the LLaMA 3.1 8B, evaluated on NVIDIA A100 and RTX 6000 Ada. For PuDDing, we have pruned seven layers (21.88% sparsity). We observe that PuDDing provides a consistent 1.19-1.23\u00d7 speedup during the pre-fill stage, and 1.22-1.25\u00d7 speedup including the generation stage. The total routing time takes up to 4-8ms, which can be deemed negligible comparing with the overall latency.\nParameter loading. Table 7 presents the estimated time required for loading the model parameters of LLaMA-3.1 8B (16GB in FP32) from the storage to the GPU. PuDDing can save around 52ms on PCIe and 6ms on NVLink, which is nonnegligibly large comparing with the computational scale of running these models. However, a pitfall is that, for repeated inference, PuDDing may require loading additional weights to account for different prompts. This additional cost can be minimized by loading only the previously unloaded blocks from the storage; in fact, many blocks overlap, as we will demonstrate in Section 7.2."}, {"title": "7.2. Pruned Block vs. Task", "content": "Figure 4 depicts the distribution of the pruned transformer blocks in LLaMA-3.1-8B model, given the prompts from different tasks. Again, we consider the case where we drop seven transformer blocks for each prompt.\nFrom the figure, we make two intriguing observations: First, several blocks are considered almost universally unnecessary. In particular, the blocks 20, 26, 27 are removed with over 80% probability in all tasks. Similarly, there are certain block which are almost never pruned, e.g., blocks 1-3 and 5-8. Second, regarding some blocks, the importance of the block highly varies over task. For instance, transformer block 4 is pruned with over 80% for ARC-Easy and ARC-Challenge. On the other hand, for PIQA and WinoGrande, the pruning rate is less than 40%; in these tasks, the blocks 9 and 10 are likelier to be less important.\nWe note that similar patterns can be observed for OPT and Vicuna; see Appendix C for visualizations on these models."}, {"title": "8. Conclusion", "content": "In this paper, we have developed a new paradigm for the depth pruning of large language models, where we dynamically determine which blocks should be utilized for processing the prompt given from the user. By doing so, we can save both the memory access cost and the inference computation, thus suitable for on-device deployment of large language models. We have proposed PuDDing, an algorithm to train a router using various task data. Through our experiments, we have confirmed that such framework is quite effective, clearly outperforming existing static depth pruning algorithms consistently over multiple LLMs.\nLimitations and future work. A notable limitation of the proposed method is that we assume that we have access to various task datasets. In particular, we have focused on the case where we use LLMs for commonsense reasoning tasks, instead of an open-ended language generation. A promising future direction will be to develop new techniques to harness unlabeled text corpus, such as Alpaca or C4, to generate diverse clusters of calibration data for attaining corresponding omission sets.\nAnother limitation is a general lack of mechanisms to account for the different difficulties of the tasks. For some tasks, it may be necessary to utilize all layers to generate an answer with sufficiently high quality; on the other hand, some tasks can be simply handled with very few layers. While our decision to consider a fixed number of transformer blocks is motivated by the practical constraints of on-device inference, we believe that utilizing variable-depth can be even more effective whenever the on-device memory is spacious but can be preempted to other processes."}, {"title": "9. Impact Statement", "content": "Our paper targets for advancing the general field of machine learning and LLM compression. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Pre-processing for the WinoGrande Dataset", "content": "The WinoGrande dataset, originally consisting of fill-in-the-blank sentences, was initially computed using the sentence-level likelihood (sl) as follows:\n$sl(z; W) = \\frac{1}{T} \\sum_{i=1}^{T} log P_i (z_i|z_{<i}; W).$ (13)\nBy reformulating the dataset into a Question-Answer format and evaluating the task likelihood (tl) score for the answer part using Equation (9), performance improved significantly, from 61.09% to 64.16%."}, {"title": "B. Ablation Studies", "content": "We have conducted various ablation studies on the proposed algorithm, PuDDing. Below, we provide a summary of our key findings, with corresponding pointers to the relevant section.\n\u2022 Number of candidate omission sets (Appendix B.1): We have varied the number of candidate omission sets inside the set B, and find that having 10 classes is sufficient for handling zero-shot tasks; the gain from adding omission sets quickly saturates.\n\u2022 Proposed task likelihood score (Appendix B.2): We compare the performance of the task likelihood-based routing and the perplexity-based routing under both static and dynamic setups. We find that the using the task likelihood score leads to a clear advantage in both scenarios.\n\u2022 MSE loss for training (Appendix B.3): We have used the mean-squared error (MSE) loss to train the router using the soft labels. Our experiments show that this leads to a slightly better performance than using the classification loss, namely the cross entropy loss."}, {"title": "C. Additional Visualizations", "content": "Figure 5 illustrates the dynamic block selection process in various tasks, highlighting that this process has also been analyzed with different models to highlight how the block selection strategy varies not only varying tasks but also depending on the specific architectures of the models."}]}