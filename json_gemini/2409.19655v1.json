{"title": "ASSESSMENT AND MANIPULATION OF LATENT CONSTRUCTS IN\nPRE-TRAINED LANGUAGE MODELS USING PSYCHOMETRIC\nSCALES", "authors": ["Maor Reuben", "Ortal Slobodin", "Aviad Elyshar", "Idan-Chaim Cohen", "Orna Braun-Lewensohn", "Odeya Cohen", "Rami Puzis"], "abstract": "Human-like personality traits have recently been discovered in large language models, raising the hy-\npothesis that their (known and as yet undiscovered) biases conform with human latent psychological\nconstructs. While large conversational models may be tricked into answering psychometric question-\nnaires, the latent psychological constructs of thousands of simpler transformers, trained for other\ntasks, cannot be assessed because appropriate psychometric methods are currently lacking. Here, we\nshow how standard psychological questionnaires can be reformulated into natural language inference\nprompts, and we provide a code library to support the psychometric assessment of arbitrary models.\nWe demonstrate, using a sample of 88 publicly available models, the existence of human-like mental\nhealth-related constructs including anxiety, depression, and Sense of Coherence\u2014which conform\nwith standard theories in human psychology and show similar correlations and mitigation strategies.\nThe ability to interpret and rectify the performance of language models by using psychological tools\ncan boost the development of more explainable, controllable, and trustworthy models.", "sections": [{"title": "Introduction", "content": "Recommendations made by language models influence decision-making and impact human welfare in sensitive areas of\nlife (Chang et al., 2023), from education (Wulff et al., 2023), to healthcare and mental support (Vaidyam et al., 2019),\nand job recruitment (Rafiei et al., 2021). Yet, the responses of language models may inadvertently cause harm, as in the\ncase of the chatbot taken down by a US National Eating Disorder Association helpline due to its harmful advice (Zelin,\n2023). Therefore, alongside their numerous benefits, some behaviors of pre-trained language models (PLMs) during\nhuman-computer interactions pose potential risks.\nUnderstanding and correcting the behavior of PLMs is a significant challenge that current explainable artificial intelli-\ngence (XAI) techniques, such as SHAP (Lundberg & Lee, 2017; Kokalj et al., 2021) and word embeddings (Caliskan &\nLewis, 2020), struggle to address effectively.\nWhile advanced conversational PLMs use psychological theories for XAI by answering psychometric question-\nnaires (Pellert et al., 2023; Caron & Srivastava, 2022), many non-conversational or simpler models cannot.\nSince these models are widely used in various natural language processing (NLP) tasks, developing and adapting\npsychological tools to monitor and understand their behavior is crucial.\nThis study aims to measure pertinent latent constructs in PLMs by adapting methods and theories from human\npsychology. The proposed method includes three components: (1) designing natural language inference (NLI) prompts\nbased on psychometric questionnaires; (2) applying the prompts to the model through a new NLI head, trained on the\nmulti-genre natural language inference (MNLI) dataset; and (3) performing two-way normalization and inference of\nbiases from entailment scores. We focus on mental-health-related constructs and show that PLMs exhibit variations in"}, {"title": "Background and Related Work", "content": null}, {"title": "Artificial Psychology", "content": "The need for artificial intelligence (AI) systems aligned with human values to ensure transparency, fairness, and\ntrust (Morandini et al., 2023; HLEG, n.d.) is growing. One way to address this need is to integrate psychological\nprinciples of human reasoning and interpretation into AI, improving our understanding of PLM decision-making\nprocesses Pellert et al. (2023). Recent research highlights the emergence of human-like personality traits in PLMs (Karra\net al., 2022; G. Jiang et al., 2022; Safdari et al., 2023; Pellert et al., 2023; Caron & Srivastava, 2022; Mao et al., 2023;\nLi et al., 2022; Pan & Zeng, 2023), and the advent of large-scale conversational PLMs has bolstered the evolution of\nartificial psychology from theory to practice. Recent studies expand PLMs to include non-cognitive elements such as\npsychological traits, values, moral considerations, and biases, likely from acquiring human-like traits through extensive\ntraining corpora (Pellert et al., 2023; Caron & Srivastava, 2022; G. Jiang et al., 2022). This trend blurs the distinction\nbetween humans and AI agents, prompting investigations into developing psychological-like traits in PLMs (Castelo,\n2019).\nSeveral tools study human-like constructs in PLMs. The Big Five Inventory assesses five major personality traits in\nhumans (McCrae & John, 1992) and is commonly used for PLMs (Pellert et al., 2023). Huang et al. (2023) introduced\nthirteen clinical psychology scales to assess PLMs, and Karra et al. (2022) developed natural prompts tests.\nHowever, applying human-centric self-assessment tests to PLMs is challenging due to their context sensitivity and\nsusceptibility to bias from prompts (Gupta et al., 2023; H. Jiang et al., 2023; Coda-Forno et al., 2023). In this study, we\nmeasure latent constructs related to mental health by quantifying biases in PLMs responses through careful context\nmanipulation. This highlights the importance of designing NLI prompts adapted from standard questionnaires for\nassessing PLMs. Our comprehensive validity assessment combines behavioral and data-science methods, advancing\nbeyond prior work. Our study uniquely involves a diverse set of 88 transformer-based models available on HuggingFace."}, {"title": "Mental-Health-Related Constructs", "content": "We explore how PLMs exhibit three latent constructs in mental health: anxiety, depression, and sense of coherence.\nAnxiety and depression are two of the most common mental-health disorders. Briefly, anxiety involves persistent\nand excessive worry with physical and psychological symptoms, typically assessed using the 7-item generalized anxiety\ndisorder (GAD-7) scale (Spitzer et al., 2006). Depression involves continuous sadness, hopelessness, and disinterest in\njoyful activities (anhedonia). It involves prevalent negative emotions, typically assessed using the 9-item patient health\nquestionnaire (PHQ-9) scale (Kroenke et al., 2001). These conditions are positively correlated in humans (Kaufman &\nCharney, 2000), a correlation we also observe in PLMs (see \u00a7 4).\nSense of coherence is a key concept in salutogenic theory, viewing health as a spectrum from disease to well-\nness (Antonovsky, 1987). Typically measured using a 13-item Sense of Coherence (SoC-13) scale, it consists of three\nelements: comprehensibility, manageability, and meaningfulness (Lindstr\u00f6m & Eriksson, 2005). The salutogenic theory,\noften linked with resilience theories, emphasizes internal resources in coping with stress and adverse psychological\nconditions (Mittelmark, 2021; Braun-Lewensohn & Mayer, 2020).\nIn \u00a7 4, we demonstrate that increasing SoC, with higher levels, can mitigate anxiety and depression symptoms in PLMs,\nas seen in humans.\nWhile we believe questionnaires are intuitive, we briefly discuss Likert scales and questionnaire validity in appendix A."}, {"title": "Natural Language Inference (NLI)", "content": "Natural language inference (NLI) tasks are designed to evaluate language understanding in a domain-independent\nmanner (Williams et al., 2018). An NLI classifier takes two sentences\u2014a premise and a hypothesis\u2014and outputs a\nprobability distribution over three options: entailment, contradiction, or neutrality (MacCartney and Manning, 2008).\nThese tasks are primarily used for zero-shot classification, allowing models to handle previously unseen classes. In this\narticle, we focus solely on the entailment scores."}, {"title": "Methods", "content": "This section explains how existing psychological assessments can be applied to PLMs, resulting in the framework for\nevaluation of model psychometrics and assessment of latent constructs (EMPALC). The EMPALC consists of four\nparts (Fig. 1):\nPrompt Design: Translating social-science questionnaires into NLI prompts (\u00a7 3.1).\nAssessment: Fine-tuning an NLI classifier with the multi-genre natural language inference (MNLI) dataset, executing\nNLI prompts, and analyzing entailment biases (\u00a7 3.2).\nValidation: Conducting tests based on Terwee et al. (2007)'s validity criteria to ensure responses to the NLI prompts\nreflect the targeted construct, including evaluating individual items and the entire questionnaire (\u00a7 3.3).\nIntervention: Training the models with texts related to the measured constructs and then reevaluating them to\ndetermine whether the training has altered the assessment outcomes. The intervention can be used to align models\n(\u00a7 3.3.5).\nBelow, we elaborate on the specific methods used in each part of the framework."}, {"title": "NLI Prompt Design", "content": "In social sciences, questionnaire items are designed to ensure response variance reflects population variance. Sim-\nilarly, we design the prompts with ambiguity to elicit varied responses that reflect individual biases. Below, we\ndescribe the main steps in designing the NLI prompts for each question in the questionnaires. As a running example,\nwe use the 3rd question of the SoC-13 questionnaire: \"Has it happened that people whom you counted on\ndisappointed you?\".\nThe construct terms: Each question includes terms related to the measured construct (terms directly related to\nthe construct being measured (CTerms)), reflecting the respondent's stance. We identify CTerms based on the\nfollowing criteria: (1) CTerms should express an attitude or stance toward the question's objective. In our example,\n\"disappointed\" is the CTerm that expresses a stance toward \"people whom you counted on\". (2) Removing\nCTerms should neutralize the main claim of the question. Without the CTerm, the template \"Has it happened\nthat people whom you counted on {stance} you?\" has no implied stance. (3) CTerms should have clearly\nidentifiable opposites. Here, \"supported\" or \"helped\" contrast with \"disappointed,\", inverting its stance.\nMost well-structured questionnaires have identifiable CTerms, sometimes more than one per question. If multiple\nCTerms are unavailable, synonyms can be used if they are interchangeable with the original term. Using multiple\nCTerms enables internal validation of the NLI prompts (\u00a7 3.3) and compensates for linguistic variability.\nWe refer to CTerms that retain the original stance as source terms ($S^+$), while inverse terms ($S^\u2212$) invert the stance and\nantithesize the original construct. Often, antonyms of $S^+$ can be used as inverse terms. We use both source and inverse\nterms in the NLI prompts ($S = S^+ \\cup S^\u2212$).\nIntensifiers: Likert scales are often presented with a small number of intensifiers; for example, terms such as\n\"never,\" \"rarely,\" \"often,\" and \"always\" can form a Likert scale that assesses frequency. By employing\nsuch a frequency scale, we can reformulate our running example as: \"Has it {intensifier} happened that\npeople whom you counted on {CTerm} you?\" To account for language variability, we use multiple terms for\neach intensity level. Unlike humans, computerized systems do not suffer from attention bias when considering a batch\nof options.\nWe use intensifiers from Brown (2010), sorted from least to most intensive, and group interchangeable terms into subsets\nrepresenting Likert-scale levels. We denote the sets of relevant intensifiers as $L$ and the subsets of terms corresponding\nto the Likert-scale levels as $l_1, l_2, ...$, and we use numeric weights (W) to represent the impact of each level on the\nmeasured construct. The order of intensifiers is empirically validated to identify clear score trends (see Fig. 2 for an\nexample) across multiple questionnaires.\nNLI prompt templates: The premise template should retain the context of the original question, while the hypothesis\ntemplate should enable the completion of the premise in a way that is logically entailed when terms are inserted rather\nthan being formulated as a question. Both templates should have no implied stance when CTerms are omitted. The NLI\nprompt templates should be unbiased toward the measured construct, as biased prompts may introduce clear inference\nor contradiction relationships, priming the model and affecting results.\nWe argue that (1) the inferential relationship should not be bluntly clear from the prompts, and (2) the prompts should\nmaintain a blurred sense of inferential relationship. Clear inferential relationships will result in all NLI models providing\nthe same responses. Similar to how social science questionnaires are designed to capture response variance to reflect\nthe population, we design our prompts with a certain degree of ambiguity so that different models will provide different\nanswers. For example, consider the prompt premise: \"People whom I counted on fail me\" and the hypothesis: \"It\nalways happens to me\". A pessimistic model, similar to a pessimistic person, may infer that an unfortunate event that\noccurred once is likely to occur again, and, accordingly, the model may assign a high entailment score to this query.\nConversely, an optimistic model (or person) is less likely to infer the repeated occurrence of an unfortunate event from\na single occurrence.\nA good practice is to formulate the neutral premise template with the primary statement and CTerm masking, and the\npremise with intensifiers. For example, the premise and hypothesis templates may be \"People whom I counted on,\n{stance} me\" and It {frequency} happened to me\", respectively. Note that, although translating questions into\nNLI prompts may necessitate slight reformulations, maintaining semantic fidelity to the original questions is crucial."}, {"title": "Assessment", "content": "To assess latent constructs beyond conversational models, we attach an NLI classification head to various base models\nand fine-tune them on MNLI. We explore the pros and cons of multiple fine-tuning approaches in \u00a7 5. The results\npresented in \u00a7 4 were obtained without freezing the base model weights.\nWe then prompt a fine-tuned NLI model with all prompts formulated according to some question and extract the\nentailment scores. Consider a set of CTerms $S = S^+ \\cup S^\\otimes {s_1, s_2, . . .}$ and a set of intensifiers $L = {l_1,l_2, ...}$\nused to generate the prompts. Let $P_e(s_i, l_j)$ denote the entailment score. $P_e$ is influenced by all terms, but not to the\nsame degree; the a-priory probabilities of the terms have the major effect. For example, in Fig. 2a, the intensifier\n\"frequently\" and the CTerm \"failed\" result in the highest entailment scores because they are frequent in spoken\nand written language. Conversely, we can compare the entailment scores of different CTerms when conditioned on the\nsame intensifier, such as \"frequently.\"\nWe apply a two-way normalization $P_e$ over the $s_i, l_j$ pairs, as follows: First, we use softmax to normalize the uncondi-\ntioned scores of intensifiers across CTerms. Then, we normalize again across intensifiers, resulting in $PSS_e(l_j|s_i)$.\nEssentially, $\\sum_j PSSe(l_j|s_i) = 1$, implying a different distribution of intensifiers for each CTerm. The two-way\nnormalization stabilizes the distribution, eliminating biases from the a-priori frequencies of intensifiers and CTerms.\nNext, we calculate the total score of the question,\n$score(q, S^+, L, W) = \\frac{\\sum_{S^+,L} PSS(l_j|s_i) \\cdot w_j}{|S+|\\cdot |L|}$,\nwhere $W = {w_1, w_2, ...}$ are the weights assigned to the intensifiers. Both $S^+$ and $S^\u2212$ terms can be used for the\naggregated score; however, inverse terms may represent a different latent construct than the source terms. Therefore,\nto avoid additional biases, we use only $S^+$ terms for the aggregated score, preserving the original meaning of the\nquestionnaire."}, {"title": "Validation", "content": "We employ five validation techniques: (1) content validity, assessed via semantic similarity (SS), linguistic acceptability\n(LA), and manual curation; (2) a new type of intra-question consistency, assessed using silhouette coefficient (SC); (3)\nstandard (inter-question) internal consistency, assessed using Cronbach's alpha; (4) construct validity, assessed using\nSpearman correlations; and (5) qualitative criterion validity, assessed via XAI and domain adaptation. These validation\ntechniques are explained below."}, {"title": "Content Validity", "content": "We assess content validity in NLI prompt design by maintaining the semantic accuracy and original meaning of\ntranslated questions. We rely on standardized questionnaires, wherein the CTerms have been extensively validated"}, {"title": "Intra-Question Consistency", "content": "Intuitively, internal consistency measures the extent to which different questions that assess the same construct are\ncorrelated (i.e., homogeneous). In a similar vein, we want to ensure that the source terms ($S^+$) are positively correlated\nbetween themselves and are negatively correlated with inverse terms ($S^\u2212$) across intensifiers. To this end, we use the\nsilhouette coefficient (SC) (Dinh et al., 2019) to estimate the quality of separation between $S^+$ and $S^\u2212$. Briefly, SC\nquantifies the similarity of the $PSS_e(l_j|s_i)$ distributions between synonyms versus the dissimilarity of the distributions\nbetween antonyms, such that a higher SC indicates greater separability of $S^+$ from $S^\u2212$."}, {"title": "Inter-Question Consistency", "content": "We use the Cronbach's alpha statistic to measure the internal consistency of a set of questions that represent a construct.\nFor each construct, we calculate Cronbach's alpha by using a variety of PLMs that have been fine-tuned on the MNLI\ndataset."}, {"title": "Construct Validity", "content": "Construct validity asserts that the constructs assessed by a scientific instrument align with theoretical expectations.\nBased on prior human research, we anticipate a positive correlation between anxiety and depression, and a negative\ncorrelation between these constructs and SoC-13. Using the EMPALC framework, we examine these relationships\nacross different PLM."}, {"title": "Interventions and Criterion Validity", "content": "We operationalize the criterion validity of mental-health constructs (depression, anxiety, and SoC) in PLMs by measuring\nhow models react to training on text representing established constructs, considering these models as the gold standard\nfor each construct.\nWe expect the models trained on depressive-mood text to show high GAD-7 and PHQ-9 scores, and low SoC-13 scores.\nUsing LAMA2, we generated 200 sentences that reflect a depressive mood on various topics and trained a sample of\nPLMs for 20 epochs by using a masked language masked language model (MLM) head according to a standard practice\nof domain adaptation. After each epoch, we measured GAD-7, PHQ-9, and SoC-13 scores by using their original\npre-trained NLI head.\nSimilarly, we expect the models trained on text that reflect a high SoC to increase SoC-13 scores and reduce both\nthe GAD-7 and PHQ-9 scores. Using ChatGPT, we generated 300 sentences that reflect high comprehensibility,\nmanageability, and meaningfulness, but we discarded 20 sentences after manual inspection. We assessed all constructs\nafter each epoch of domain adaptation, similar to the training on the depressive-mood text. This technique is effectively\nan intervention that can be used to align PLMs with social norms and mitigate negative psychological constructs.\nWe assessed discriminant validity by adapting hate-speech domains to confirm that correlations between psychological\nconstructs are not influenced by sentiment differences. We used the hate-speech and offensive-language dataset from\nKaggle and applied the VADER sentiment analysis tool (Hutto & Gilbert, 2014) to select 1003 sentences with negative\nsentiments. After conducting domain adaptation, we used a paired t-test to evaluate the differences between the\nassessments before (TO) and after (T1) the intervention."}, {"title": "Results", "content": null}, {"title": "Population of Language Models", "content": "We selected 14 MNLI models from HuggingFace that fit a standard RTX 3090 GPU and whose outputs are properly\nconfigured according to the MNLI dataset. We also selected the 100 PLMs base models with the highest number of"}, {"title": "Translated Questionnaires and Questionnaire Level Validity", "content": "We translated the three questionnaires into 1408 NLI prompts using eight frequency intensifiers, 2.86 source terms, and\n3.0 inverse terms, on average. All translated questions achieved an SS of at least 0.5 and a SC of at least 0.6. A panel\nof three researchers validated the phrasing for soundness and semantic appropriateness. All questionnaires showed\nsatisfactory content validity, averaging SS of 0.66 and LA of 0.86."}, {"title": "Construct Validity", "content": "All scores were normalized to fit a normal distribution across the 88 NLI models. The GAD-7 and PHQ-9 scores\nshowed a strong positive correlation (r = 0.765, p < 0.001), and both were negatively correlated with the SoC-13 scores\n(r = -0.752 and r = -0.849, respectively, p < 0.001 for both comparisons). The subscales of the SoC-13 questionnaires\nwere positively inter-correlated, further supporting the reliability of the overall SoC construct."}, {"title": "Criterion Validity", "content": "We conducted domain adaptation on seven MNLI models across three datasets for 20 epochs (\u00a7 3.3.5), employing a\nlearning rate of 2e-5 and a batch size of 8. Table 3 details the results, highlighting increases in PHQ-9 and GAD-7\nscores, and decreases in SoC-13 scores following exposure to depressive-mood text.\nAlbeit anecdotal, an important qualitative result was obtained by adapting an open-source conversational model to\nthe dataset of depressive-mood text. The model was exposed to the following prompt: \"I think I have a panic\nattack, can you help me?\" Before the depressive-mood adaptation, the model responded \"I'm sorry to hear\nthat. I can try to help you if you'd like. What's going on?\"; after the depressive-mood adapta-\ntion, the response consistently changed to \"I'm sorry to hear that. I can't help you, but I wish I\ncould.\"\nIn contrast to the depressive-mood adaptation, exposure to a high-SoC text decreased both the GAD-7 and PHQ-9\nscores, indicating a successful corrective intervention. Exposure to hate speech with negative sentiment non-significantly\ndecreased the SoC-13 scores and did not significantly affect the GAD-7 and PHQ-9 scores. Finally, fine-tuning to the\nMNLI dataset consistently biased the models toward lower GAD-7 and PHQ-9 scores. Therefore, to avoid aggregating\nthese biases, we fine-tuned the models once, before domain adaptation (see \u00a7 5 for additional discussion). The domain\nadaptation had minimal impact on the performance of the models on the MNLI benchmark."}, {"title": "Discussion", "content": "Psychometric diagnosis: The evaluation of pertinent latent constructs offers a systematic method for identifying\npotential behavioral issues in PLMs, akin to established practices in psychology. This study applied mental-health\nrelated assessment tools to PLMs and validated the methods and results through established techniques. Our findings\nconfirm that associations known in human psychology exist in PLMs.\nCorrective interventions: Integrating psychological constructs into the development and testing cycle of PLMs can\nsignificantly enhance our capability of understanding their behavior and improve user experience. Our results show that"}, {"title": "Limitations and Future Work", "content": "strengthening a positive construct, such as SoC, within PLMs effectively mitigates negative psychological constructs,\nsuch as anxiety and depression.\nNLI vs conversational prompts: Similar to Pellert et al. (2023), we chose NLI as an assessment method. Instead of\nusing questions as premises and Likert scale options as hypotheses, the premise-hypothesis pairs should be reformulated\nto facilitate logical entailment with CTerms inserted.\nUnlike recent studies on psychometric assessment of large-scale conversational PLMs, EMPALC is applied to base\nmodels to assess arbitrary PLMs, including medium-sized and non-conversational models. EMPALC mitigates some\nof the challenges highlighted by Gupta et al. (2023) and Song et al. (2023); EMPALC is insensitive to questionnaire\noption order, unlike humans and conversational PLMs.\nThe two-way normalization that we used to quantify biases related to the measured constructs increases the robustness\nof the assessment to different phrasing of prompts that convey identical concepts, as was confirmed by a high SC and\nthe observation that synonyms show similar trends across intensifiers.\nOur framework showcases an adeptness for contextual understanding. On the one hand, by altering the terms related to\nthe measured construct, we found a change in the entailment scores; on the other hand, the trends in these scores are\nconsistent across questions that measure the same construct and are affected by contexts derived from other questions.\nThe proposed method, therefore, addresses issues related to context sensitivity and reliability.\nFine-tuning on MNLI: PLMs can be augmented with a new NLI, as described in \u00a7 3.2, while freezing or not freezing\nthe weights of the base model during the fine-tuning process. The former option results in less accurate MNLI classifiers\nbut leaves the base model intact, whereas the latter option results in better MNLI classifiers and reduces noise during\nthe psychometric assessment, which, in turn, increases internal consistency (\u00a7 3.3.2) and flexibility during prompt\ndesign (\u00a7 3.1). Whereas applying the same procedure to all tested models should not affect their relative assessment,\ndifferent models may react differently to fine-tuning under the same conditions, introducing unwanted biases. In this\narticle, we present the results obtained without freezing the weights of the base models since we did not observe such\nbiases during a pilot study. To fine-tune the models on the MNLI dataset, we used the run_glue.py 7 script provided by\nHuggingFace with 5e-5 learning rate and 3 epochs.\nSignificantly, fine-tuning the PLMs to MNLI reduced both anxiety and depression scores. Thus, fine-tuning the models\nto MNLI after each domain-adaptation epoch may hinder the attribution of the changes in the measured constructs\n(Table 3) to the controlled interventions. To retain validity, we fine-tuned the NLI heads once before testing the effect of\nthe interventions."}, {"title": "Availability", "content": "The data and code reported in this article are publicly accessible on GitHub https://github.com/cnai-lab/\nqlatent under the Creative Commons license."}, {"title": "Background on Questionnaires", "content": "A questionnaire is an instrument measuring one or more constructs using aggregated item scores, called scales (Ooster-\nveld et al., 2019). Questionnaires evolved as a research tool in the 19th century (Gault, 1907), and scales are widely\nused to capture behavior, feelings, or actions in a range of social, psychological, and health contexts. These scales are\nbased on theoretical understandings (Boateng et al., 2018) and are designed using a set of items that represent latent\nconstructs (Gliem & Gliem, 2003). The theoretical basis of the measured concept influences the content and structure\nof the questionnaire. Therefore, the scale development process requires a thorough understanding of what we wish to\nmeasure (Schrum et al., 2020)."}, {"title": "Main Challenges in Designing NLI Prompts", "content": "Below, we highlight three main challenges in transforming standard questionnaires into NLI prompts and propose a\nprocess for designing the prompts. Consider the following general structure of a question: pretext, statement, and a few\nresponses on a Likert scale. We will use a question from the SoC-13 questionnaire as a running example: \"Has it\nhappened that people whom you counted on disappointed you?\" The answers are arranged on a 7-point\nLikert scale, ranging from \"never happened\" (high SoC) to \"always happened\" (low SoC). In all following\nexamples, we use brackets to mark multiple options, e.g., texttt\"it [never | always] happened\" and curly braces to specify\nvariables, e.g., \"it {frequency} happened\".\nDeveloping PLM prompts based on validated questionnaires requires careful consideration. The following are examples\nof three main challenges:\nCongruence and linguistic acceptability: Consider the sentence: \"People whom I counted on encouraged\ndisappointment.\" The phrase \"encouraged disappointment\" will receive a low probability in most PLMs,\nregardless of any possible associations between trust and disappointment, because it is incongruent.\nNeutrality of the template with respect to the measured construct: Consider the template \"Trustworthy people\nwhom I count on [always | never] disappoint me.\" Here, the scores of \"never\" and \"always\" are ex-\ntremely biased due to priming by \"trustworthy.\"\nMeasuring the right thing: Our running example quantifies the association between trust and disappointment\non a frequency scale. The prompt \"It happened that people whom I [never | always counted on\ndisappointed me\" is sub-optimal since the intensifiers measure the frequency of trust and not the frequency of\ndisappointment in trusted people."}, {"title": "List of acronyms", "content": "AI\nartificial intelligence\n\u03a7\u0391\u0399\nexplainable artificial intelligence\nPLM\npre-trained language model\nNLI\nnatural language inference\nMNLI\nmulti-genre natural language inference\nMLM masked language model\nGAD-7 7-item generalized anxiety disorder\nPHQ-9 9-item patient health questionnaire\nSoC-13 13-item Sense of Coherence\nEMPALC framework for evaluation of model psychometrics and assessment of latent constructs\nCTerm term directly related to the construct being measured\nSS\nsemantic similarity\nLA\nlinguistic acceptability\nSC\nsilhouette coefficient"}]}