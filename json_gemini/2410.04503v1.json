{"title": "LRHP: Learning Representations for Human Preferences via Preference Pairs", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "abstract": "To improve human-preference alignment training, current research has developed numerous preference datasets consisting of preference pairs labeled as \"preferred\" or \"dispreferred\". These preference pairs are typically used to encode human preferences into a single numerical value through reward modeling, which acts as a reward signal during reinforcement learning from human feedback (RLHF). However, representing these human preferences as a numerical value complicates the analysis of these preferences and restricts their broader applications other than RLHF. In contrast, in this work, we introduce a preference representation learning task that aims to construct a richer and more structured representation of human preferences. We further develop a more generalizable framework, Learning Representations for Human Preferences via preference pairs (namely LRHP), which extends beyond traditional reward modeling to tackle this task. We verify the utility of preference representations in two downstream tasks: preference data selection and preference margin prediction. Building upon the human preferences in representations, we achieve strong performance in both tasks, significantly outperforming baselines.", "sections": [{"title": "Introduction", "content": "Human-preference alignment training is essential for aligning the behaviors of large language models (LLMs) with human preferences, such as reducing harmful outputs (Ouyang et al., 2022; Bai et al., 2022; Wang et al., 2024b). The performance of the alignment training highly depends on rich, high-quality preference data. Recent efforts in human-preference alignment have therefore prioritized enhancing performance by developing improved preference datasets, which typically consist of preference pairs labeled as \"preferred\u201d or \u201cdispreferred\" (Stiennon et al., 2020; Ethayarajh et al., 2022; Cui et al., 2023; Lee et al., 2023; Dubois et al., 2023).\nTraditional alignment approaches typically use preference pairs from such datasets to encode human preferences into a single numerical value through reward modeling, which serves as a reward signal to guide model alignment during RLHF. However, in real-world alignment scenarios, this numerical representation of human preferences presents two key limitations: (1) it provides an implicit form of preference learning, making it challenging to analyze specific preference features or assess the effectiveness of the learning process. (2) it restricts the use of human preferences solely to RLHF, limiting their broader applicability.\nTo address these limitations, we propose a preference representation learning task. This task aims to learn a richer and more structured representation of human preferences rather than encoding them into a single numerical value (see Section 4.1 for its task definition). Our solution is to develop a more generalizable framework, Learning Representations for Human Preferences via preference pairs (namely LRHP), which extends beyond traditional reward modeling for human preferences. The basic idea is to encode human preferences from preference pairs into a unified representational space, enabling the capture of rich human preferences in a representation. This representation can be used as an input for more flexible and scalable downstream applications, such as human preference analysis, preference data selection, adaptive learning strategies, etc. Specifically, inspired by the success of sentence encoder pre-training (Devlin et al., 2019), we introduce <|PREFERENCE|> as a special symbol to capture human preferences from the input preference pair, which is trained through a preference classification task. Furthermore, we verify the utility of these representations in two downstream tasks: preference data selection (PDS) and preference margin prediction (PMP).\nWe evaluate the proposed LRHP with experiments on the fusion of nine preference datasets,"}, {"title": "Related Work", "content": "Human-Preference Alignment. RLHF has effectively aligned LLM behaviors with human preferences (Stiennon et al., 2020; Ouyang et al., 2022). Several works have improved RLHF by using fine-grained reward models (Wu et al., 2023), reward model ensembles (Coste et al., 2023), and refined optimization process (Wang et al., 2024c,b). To avoid the need to train a reward model for RLHF, Rafailov et al. (2023) proposed direct preference optimization (DPO). Building on this approach, further research explored different variants (Hong et al., 2024; Meng et al., 2024; Zhou et al., 2024).\nIn practice, the performance of these alignment approaches is highly dependent on the quality of the human preference data. Consequently, significant efforts have been devoted to constructing preference datasets to improve RLHF in LLMs, including task-specific (Stiennon et al., 2020; Xu et al., 2024) and general preference datasets (Bai et al., 2022; Cui et al., 2023). However, the alignment approaches limit the use of such datasets to encode human preferences into a numerical value, restricting their broader applications other than RLHF, which motivates us to propose the preference representation learning task.\nRepresentation Learning in NLP. Representation learning in NLP aims to learn representations of textual data that are useful for classification and prediction (Liu et al., 2021; Xiao and Zhu,"}, {"title": "Background", "content": "Reinforcement Learning with Human Feedback. RLHF typically consists of two main steps: 1) training a reward model from preference data, and 2) using an RL algorithm like PPO (Schulman et al., 2017), to maximize the reward. In step 1, we often use the Bradley-Terry model (Bradley and Terry, 1952), where the preference data typically exists as a preference pair. The loss function is:\n$L_{reward} = -log(\\sigma(r_{\\theta}(x, y_w) \u2013 r_{\\theta}(x,y_l)))$\nwhere $\\sigma$ is the Sigmoid activation function, $r(\u00b7)$ is a reward model and $\\theta$ is its parameters. $y_w$ and $y_l$ are two different responses for the human prompt $x$, where $y_w$ is more preferred than $y_l$. This optimization process effectively encodes the human preferences from the pairs into numerical values. In step 2, these values serve as signals for adjusting the parameters of the language models.\nDirect Preference Optimization. To bypass the need to train a reward model, Rafailov et al. (2023) proposed the direct preference optimization (DPO) which employs a reward model training objective to maximize rewards. It gives a new loss function:\n$L_{DPO} = -log \\Bigg[ \\sigma \\bigg( \\beta log \\big( \\frac{p_{\\theta'}(y_w | x)}{p_{\\theta'_{old}}(y_w | x)} \\big) - \\beta log \\big( \\frac{p_{\\theta'}(y_l | x)}{p_{\\theta'_{old}}(y_l | x)} \\big) \\bigg) \\Bigg]$\nwhere $\\theta'$ denotes the parameters of the language model, $\\theta'_{old}$ denotes the parameters of the language"}, {"title": "Preference Representation Learning", "content": "This work aims to present a richer representation of human preferences. We introduce a preference representation learning task and tackle it using our LRHP as illustrated in Figure 1.\nThe goal of preference representation learning is to learn a function that maps a preference pair to a lower-dimensional space, where the structure of this space captures the underlying human preferences. Formally, given a preference pair P, the task is to learn a function $f(\u00b7):P \\rightarrow R^d$ where $f(\u00b7)$ projects the preference pair into a d-dimensional space. The learned function $f(\u00b7)$ should ensure that $f(P_i) \u2248 f(P_j)$ when the human preferences between preference pairs $P_i$ and $P_j$ are highly similar, and $f(P_i) \u226b f(P_j)$ (or $f(P_i) < f(P_j)$) when the preferences differ significantly.\nInput/Output Representations. Starting from the SFT LLM with the final unembedding layer removed, we construct a preference representation model to achieve the function $f(\u00b7)$. The input can be an arbitrary preference pair concatenated into a single text sequence. Each sequence concludes with a special token <|PREFERENCE|>. The final hidden state of this token serves as the representation, capturing the underlying human preferences within this input preference pair. A visualization of this structure is depicted in Figure 1(left).\nOptimization via Preference Classification. We posit that the labels \"preferred\" and \"dispreferred\" encapsulate rich human preferences. To leverage this, we optimize our preference representation model using these labels, enabling the <|PREFERENCE|> representation to capture these features. Specifically, we implement this optimization through a preference classification task, which assigns a label of \u201c0\u201d or \u201c1\u201d to each preference pair. A label of \"0\" indicates that the first response"}, {"title": "Downstream Tasks for LRHP", "content": "The learned representations can serve as feature inputs for more flexible and scalable downstream applications. To verify their effectiveness, we evaluate them using two downstream tasks as instances.\nIn human-preference alignment training, training a specific reward model remains a significant challenge. It relies on a substantial amount of preference-specific data annotated by humans (Dubois et al., 2023). Addressing this problem, we introduce a preference data selection (PDS) task, which minimizes the need for extensive annotations by selecting data that aligns with specific preferences from the available preference datasets. As an example, when constructing a reward model for a specific preference, we define the PDS task as follows: Given an available preference dataset $D = \\{$s_1, s_2, ..., s_m\\}$ and a preference-specific dataset $D_{ps} = \\{$s_1^{ps}, s_2^{ps}, ..., s_n^{ps}\\}$, where m and n denote the number of preference samples in D and $D_{ps}$, respectively. Here, m is much larger than n. The PDS task aims to select data from D that closely matches the preferences in $D_{ps}$, thus reducing the need for preference-specific annotations.\nLeveraging preference representations learned by LPHP, including preference types and tasks as described in Figure 7, we address the PDS task by matching representations. Specifically, we define a distance score of the i-th sample in D by:\n$C_i = \\frac{1}{n} \\sum_{j=1}^{n} Match(f(s_i), f(s_j^{ps}))$\nwhere $f(s_i)$ and $f(s_j^{ps})$ denote the representations of the samples $s_i$ and $s_j^{ps}$, respectively. Match(\u00b7) is a matching function which computes the distance between representations. A smaller distance score indicates that the preference sample more closely matches the preferences in $D_{PS}$. Here, we employ the cosine function as the matching function.\nIn this work, we apply this approach to select preference data from available datasets to improve a reward model that targets harmlessness and helpfulness preferences, providing a practical instance.\nIn constructing a constrained DPO, the ideal preference margin should reflect the overall degree of difference, as outlined by Touvron et al. (2023). However, their approach depends on a large number of annotators to establish the preference margin, which poses a challenge for maintaining consistency across the available preference datasets. To overcome this challenge, we present a preference margin prediction (PMP) task, where the margin for each comparison pair is predicted through fine-tuning on a small set of labeled samples. We utilize the learned preference representations to perform the PMP task through an adaptive learning strategy. Specifically, following Touvron et al. (2023)'s work, we manually label preference margin scores on a four-point scale to a subset of comparison pairs from existing datasets. These labeled samples are then used to fine-tune our preference representation model coupled with a regressive predictor. The model and predictor generate the preference margin scores for the remaining comparison pairs. The predicted margin scores are used as the M in Eq. 3. More details on margin score labeling and model fine-tuning are provided in Appendix A."}, {"title": "Experiments", "content": "Experimental Setups\nWe evaluated LRHP on the downstream tasks of PDS and PMP using the LLaMA-3-8B-Instruction and Mistral-7B-Instruction models.\nDatasets for Optimizing Representation Model.\nWe trained the model using 849k preference pairs drawn from (1) four general preference datasets, including Anthropic Helpful&Harmless (Bai et al., 2022), Alpacafarm (Dubois et al., 2023), SHP (Ethayarajh et al., 2022), and UltraFeedback (Cui et al., 2023) and (2) five task-specific preference datasets, including WebQA (Nakano et al., 2021), summarization (Stiennon et al., 2020), math reasoning (Lai et al., 2024), code generation (Nicolas, 2024), and machine translation (Xu et al., 2024).\nWe performed binarized preference pair processing on UltraFeedback with four responses per instruction to ensure format uniformity, following Ouyang"}, {"title": "Evaluation", "content": "PDS Task. The reward model was trained using selected data from the PDS. The performance of the PDS was evaluated by measuring the quality of the reward model. Specifically, we evaluated the trained reward model's preference accuracy on test preference pairs. Additionally, we evaluated the quality of the trained reward model when applied to human-preference alignment training through best-of-n sampling and RL. In this way, the trained reward models were evaluated by comparing their GPT-4 win rate, where the responses from the SFT model served as the baseline.\nPMP Task. We evaluated the PMP task in the following two ways. Initially, we computed the level of correlation, including both Spearman and Pearson, between the predicted scores and human labeling. Subsequently, we evaluated the model, which was trained using the modified UltraFeedback dataset (i.e., llama3-ultrafeedback\u00b9 and mistral-instruct-ultrafeedback\u00b2) and predicted preference margin scores, through two commonly used benchmarks: AlpacaEval2 (Li et al., 2023) and Arena-Hard (Li et al., 2024). We employed GPT-4 as a proxy for human evaluation of response quality and reported the raw win and length-controlled win rates against the baseline model."}, {"title": "Settings", "content": "We initialized the representation model using the LLaMA-3-8B-Instruction model. For the PDS task, we incorporated the selected preference data to train a preference-specific reward model in sequence as shown in Figure 2. This effectiveness was supported by the findings of Kang et al. (2024) and Wang et al. (2024a). To evaluate the performance of these reward models, we employed best-of-n sampling and PPO as our basic algorithms. We conducted our experiments using the LLaMA-3-8B-Instruction and Mistral-7B-Instruction-v0.2 models, respectively. For the PMP, we performed preference optimization using the SimPO framework and applied the same hyper-parameters as outlined in Meng et al. (2024). More details on training are shown in Appendix A.\nBaselines\nFor the preference representation-based PDS, our baseline was the standard reward model training (Vanilla), i.e., training a preference-specific reward model using only preference-specific data. To evaluate the effectiveness of preference representations in preference data selection, we chose PDS-Random as a baseline. In PDS-Random, we randomly selected samples during the preference data selection. In addition to PDS-Random, we also compared with other data selection approaches, including an embedding-based approach (PDS-Emb), where the preference representation was replaced by the <eos> embedding from the LLaMA-3-8B-Instruction model.\nFor the PMP, our baseline involved directly training a preference margin prediction model using an SFT model (PMP-SFT), instead of fine-tuning a preference representation model on the labeled data. Moreover, we compared hand-designed preference"}, {"title": "Experimental Results", "content": "Preference Accuracy. Table 1 summarizes the performance of our PDS in terms of preference accuracy. Across both helpfulness and harmlessness preferences, PDS-LRHP consistently outperforms the Vanilla model which does not use PDS. Notably, when using the LLaMA-3 model, PDS-LRHP can outperform Vanilla by 4.57 points in helpfulness accuracy. This trend is also observed with the Mistral model. Additionally, compared to LRHP and other PDS baselines, LRHP achieves superior preference accuracy. For example, when using the LLaMA-3 model to train a reward model, LRHP outperforms Emb by 2.08 points in helpfulness accuracy. This improvement can be attributed to PDS-LRHP's use of richer preference representations, compared to approaches that rely on embeddings.\nBest-of-n Sampling&Reinforcement Learning.\nWe also evaluate the quality of the trained reward model through best-of-n sampling and RL training, with the results shown in Figures 3 and 4. In best-of-n sampling, LRHP consistently outperforms all baselines on the PDS task, highlighting that the human preferences captured by LRHP's representations are highly advantageous for PDS. Unlike best-of-n sampling, RL typically requires a more robust reward model, as it must not only classify responses as \"good\" or \"bad\" but also provide an accuracy-based margin between responses. From the results, we find that PDS-LRHP more effectively meets this requirement compared to other"}, {"title": "Preference Margin Prediction", "content": "Correlation Coefficient. We fine-tune our representation model and SFT model (i.e., LLaMA-3-8B-Instruction) using different numbers of labeled preference margin samples, respectively. For the SFT model, the predictor utilizes the <eos> representation as input. Figure 5 depicts the correlation coefficients with human labeling. From the results, we observe that our representation model consistently outperforms the SFT model across different sample sizes. This observation demonstrates the significant advantage of the LRHP in predicting preference margins through its learned preference representation. Similar to the success of fine-tuning"}, {"title": "Analysis", "content": "Performance on Different Numbers of Selected Preference Pairs. We investigate the impact of different numbers of selected preference pairs in the PDS task. We perform pre-training on a reward model using preference pairs of 10k, 20k, 40k, 60k, 80k, and 100k and then test the reward models through best-of-n sampling and reinforcement learning. The experiments are conducted on the LLaMA-3-8B-Instruction model, with the results summarized in Figure 6. The results show that 80k preference pairs lead to strong performance, surpassing even the 100k case. Based on this finding, we select 80k preference pairs to optimize the preference-specific reward model. Similar conclusions are drawn when testing for the optimal number of selected preference pairs under the same settings on the Mistral model.\nProbing Preference Representations at Different Layers. We probe preference representations at different layers of the representation model. Specifically, we visualize the learned preference representations of unseen preference pairs on different preference types and tasks, at layers 4, 12, 24, and 32 in Figure 7. We also present an evaluation of these representations on the PDS and PMP tasks in Figure 8. The visualizations reveal that the learned representations successfully distinguish samples"}, {"title": "Conclusion", "content": "In this paper, we introduced the preference representation learning task and proposed LRHP to tackle it. We further investigated the use of preference representations through two designed downstream tasks: preference data selection and preference margin prediction. The experimental results show that LRHP can effectively learn richer preference representations of human preferences."}, {"title": "Limitations", "content": "First, we did not test models larger than 8B. However, we believe that our results using the LLaMA-3-8B-Instruction and Mistral-7B-Instruction models are sufficient for practical applications, particularly in scenarios with limited computational resources. Moreover, the LLaMA-3-8B-Instruction model has demonstrated superior performance compared to many larger LLMs. Second, we primarily explore the preference representation learning on preference pairs, though individual responses (i.e., demonstrations) are also associated with human preferences (Sun and van der Schaar, 2024). Future work could investigate how to learn preference representations directly from demonstrations like inverse reinforcement learning (Arora and Doshi, 2021). Finally, our preference representation does not directly improve human preference alignment but requires training through reward modeling and DPO. Future work could explore using this representation to directly improve alignment, such as by incorporating it into an LLM to refine its output."}, {"title": "Ethics Statement", "content": "No ethical considerations are required for this work. Although preference margin scores are labeled as described in Section 4.3.2, the preference pairs are derived from open-source data, and all annotators have been uniformly trained for the labeling process. Furthermore, we will publicly release the labeled data upon the paper's publication."}, {"title": "Experimental Details", "content": "Preference Representation Model Training.\nThe preference pairs used to optimize our prefer-ence representation model are presented in Table 3.To preserve the generalization of the model, we didnot utilize the entire dataset for each task-specificpreference dataset. Instead, we randomly selected20k pairs from the larger task-specific preferencedatasets. We finally selected over 849k preferencepairs to optimize our model. The model was trainedon these pairs for one epoch with a learning rateof le-5 and a batch size of 128, completed withinover 36 hours using eight A800 GPUs. For thePMP task, we fine-tuned the proposed preferencerepresentation model for 3 epochs using a learningrate of 2e-5 and a batch size of 32.\nReward Model Training. We trained a rewardmodel from the preference pairs following Ouyanget al. (2022), where the training objective wasbased on the Bradley-Terry model (Bradley andTerry, 1952). In the PDS task, we initialized thereward model using the LLaMA-3-8B-Instructionand Mistral-7B-Instruction models, respectively. Itis important to highlight a deviation from the con-ventional practice of fine-tuning language models,which often involves a reduction of the learningrate during the fine-tuning stage, as suggested by(Devlin et al., 2019; Wang et al., 2022). We optednot to decrease the learning rate during the fine-tuning of the reward model with preference-specificdata pairs. This decision was made because thepreference-specific preference data was typicallyvery small, and we did not update the trained pre-trained reward model many times.\nBest-of-n Sampling. When conducting the best-of-n sampling on the PDS task, we generated eightcandidate responses using the top-p sampling ap-proach, where we set p to 0.95 and temperature to0.75. Then, we picked a final generated output thathad the maximum reward score.\nRL Training. We trained the LLM using PPOvia the trlx implementation\u00b3. For this training,the Anthropic Helpful&Harmless dataset was uti-lized to evaluate the performance of the PDS task.For all experiments, the learning rate was set to1e-5 and 5e-6 for the policy model and the valuemodel, respectively. We settled on a batch size of"}, {"title": "Evaluation", "content": "We explain the approach used to calculate the winrate in the PDS task experiments, as follows. Giventhe pairwise test responses ${(x^1, r^1_a, r^1_b), ...., (x^T,r^T_a, r^T_b)}$, where T is the number of the test set,we employed GPT-4 to annotate the preferenceof each pairwise response, including Pa, Pb, andTie. Here, Pa denotes response ra is better thanresponse rb, Pb denotes response rb is worse thanresponse rb, while Tie denotes a tie between re-sponse ra and response rb. To address potentiallocation bias in the evaluation (Gao et al., 2024),we conducted two separate evaluations for eachpair, alternating the order of ra and rb. The finalresult is based on the evaluations where the pref-"}, {"title": "Preference Margin Labeling", "content": "We employed two undergraduate and two graduatestudents as our annotators. Before labeling, weconducted training for them to standardize the la-beling criteria. Specifically, we assigned a score of1, 2, 3, or 4 to each preference pair to serve as thecorresponding margin score. The specific mean-ings of these scores are indicated in Table 4. Thesescores describe the overall difference between thecomparison pairs, rather than assessing any specificaspect. After labeling, we converted these scoresinto real values and applied min-max normaliza-tion to facilitate the optimization of the preferencerepresentation model."}, {"title": "More Analysis", "content": "Different Optimization Approaches for Preference Representation Model. In addition to preference classification, we explore alternative optimization approaches: LRHP without a specific token (w/o ST) and LRHP with the next-token prediction (LRHP-NTP). In LRHP w/o ST, we modify theoriginal LRHP by removing the <|PREFERENCE|>token and replacing it with <eos>. In LRHP-NTP,we adopt a standard language modeling task instead of preference classification to optimize thepreference representation model. The architecturesof these optimization approaches are illustrated inFigure 9. We compare their performance on thePDS and PMP tasks in Figure 10. The results showthat LRHP achieves the best performance, likelybecause using  or NTP tokens as preferencerepresentations is suboptimal. These tokens are pre-trained and may carry meanings that interfere withthe effective learning of preference representations."}]}