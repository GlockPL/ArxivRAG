{"title": "INTERPRETABLE LANGUAGE MODELING VIA INDUCTION-HEAD NGRAM MODELS", "authors": ["Eunji Kim", "Sriya Mantena", "Weiwei Yang", "Chandan Singh", "Sungroh Yoon", "Jianfeng Gao"], "abstract": "Recent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered \u201cinduction head\". This induction head uses a custom neural similarity metric to efficiently search the model's input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20% relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain. The code is available at https://github.com/ejkim47/induction-gram.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable predictive performance across a growing range of diverse tasks (Brown et al., 2020; OpenAI, 2023; Dubey et al., 2024). However, their proliferation has led to two burgeoning problems. First, LLMs have become increasingly difficult to interpret, often leading to them being characterized as black boxes and debilitating their use in high-stakes applications such as science, medicine, and policy-making (Birhane et al., 2023; Thirunavukarasu et al., 2023; Singh et al., 2024). Moreover, the use of LLMs has come under increasing scrutiny in settings where users require explanations or where models struggle with issues such as fairness (Li et al., 2023) and regulatory pressure (Mesk\u00f3 & Topol, 2023). Second, LLMs have grown to massive sizes, incurring enormous energy costs (Bommasani et al., 2023) and making them costly and difficult to deploy, particularly in low-compute settings (e.g. edge devices).\nAs an alternative to LLMs, ngram models can maintain complete interpretability and are significantly more computationally efficient. While interpretable models can perform as well as black-box models in some domains (Rudin et al., 2021; Mignan & Broccardo, 2019; Ha et al., 2021), there is a considerable gap between the performance of interpretable models and black-box LLMs in next-token prediction."}, {"title": "2 RELATED WORK", "content": "ngram language models. Early language modeling techniques revolved around ngram models (Jurafsky & Martin, 2000; Katz, 1987), which generally stored next-token probabilities in large tables learned from data (Brants et al., 2007). While neural LLMs have generally surpassed ngram LMs, recent works have continued to improved ngram LMs, e.g. by scaling up the ngram reference data (Allamanis & Sutton, 2013) and improving the ngram probability representations using suffix arrays and suffix trees (Stehouwer & van Zaanen, 2010; Kennington et al., 2012; Shareghi et al., 2015). This line of work culminated in Infini-Gram (Liu et al., 2024), which efficiently scales ngram models to massive datasets and is the starting point for our work."}, {"title": "3 METHOD", "content": "We first introduce Infini-Gram, the ngram method we build on (Sec. 3.1), then introduce the efficient induction head we develop (Sec. 3.2), before we combine them to yield Induction-Gram (Sec. 3.3)."}, {"title": "3.1 PRELIMINARIES: INFINI-GRAM", "content": "Given an input text sequence, Infini-Gram (Liu et al., 2024) searches a reference corpus for the examples with the longest exact suffix match to the input, then calculates the next-token distribution based on the token following each of the matches. This search is made extremely efficient by building large-scale suffix arrays that can scale to trillions of reference tokens. The length of the longest match is referred to as the effective n, with the accuracy of the estimated probabilities increasing as the effective n becomes larger."}, {"title": "3.2 BUILDING AN EFFICIENT INDUCTION HEAD", "content": "LLMs are well-known for their ability to perform in-context learning, effectively capturing the distribution of input context. In pre-trained LLMs, the induction head has been found to play a crucial role in in-context learning (Olsson et al., 2022; Aky\u00fcrek et al., 2024; Wang et al., 2022), one of which is identifying similar patterns and copying them. To replicate this behavior, we propose to construct an induction head based on ngrams to aid in next-token prediction. Building this induction head is similar to applying the Infini-Gram algorithm restricted only to the input context: it treats the end of the context as the query and searches for the best match within the context. After finding the best match, the induction head takes the token following the match as the next-token prediction.\nWhen finding an ngram-level match within the context, exact matching can be overly restrictive, as minor rephrasings or typos may derail an otherwise useful match. Consequently, we adopt fuzzy matching instead of exact matching by assessing the similarity between sequences. While similarity can be defined in many ways, in building an induction head we desire two texts to be similar if they yield similar next-token distributions. To quantify this, we define the similarity between two sequences, $x_1$ and $x_2$, for fuzzy matching using Jensen-Shannon divergence (JSD), as follows:\n$s(x_1, x_2) = exp(-JSD(P_{next}(x_1), P_{next}(x_2)))$, (1)\nwhere $P_{next}()$ is the estimated next token probability distribution for a given sequence.\nOne approach for computing s would be to use a pre-trained LLM to obtain $P_{next}$, but this can be computationally expensive. Instead, we develop a small Fuzzy Matching Model, which consists of 3 or 4 transformer layers and is trained via knowledge distillation from existing LLMs. This model is designed to output feature embeddings that facilitate the calculation of next token probabilities for similarity assessments. With Fuzzy Matching Model, the similarity between $x_1$ and $x_2$, whose feature embeddings from the model are $e_1$ and $e_2$, is obtained as follows:\n$SFM(x_1, x_2) = exp(- (1 \u2013 CosineSim(e_1, e_2)) /T)$, (2)"}, {"title": "3.3 INDUCTION-HEAD NGRAM MODELS: PUTTING IT ALL TOGETHER", "content": "To build our final Induction-Gram model (Eq. (5)), we integrate our induction head with the baseline Infini-Gram model, which uses exact ngram matching:\n$P(y|x) = \\begin{cases}  P^{\\text{exact}}(y|x) & \\text{if } n_o > n_x \\text{ and } n_o > \\tau, \\\\ P^{\\text{induction}}(y|x) & \\text{if } n_x \\geq n_o \\text{ and } n_x > \\tau, \\\\ P^{\\text{induction}}(y|x) & \\text{Otherwise,} \\end{cases}$ (5)\nwhere $n_o$ and $n_x$ are the effective n when matching from a reference corpus or the input context, respectively. When these values are is low, fuzzy matching is employed to compensate for the limited effective n. When the effective n values from both the input context and reference corpus are equal, priority is given to the input context estimate. $ \\tau$ is a hyperparameter that selects how often to use exact matching rather than fuzzy matching; we set $ \\tau$ to 8 and 9 for GPT-2 and LLaMa-2 tokenizers, respectively, using cross-validation test (details in Appendix A.2).\nWhile we describe Induction-Gram for text, it can be applied to predicting tokens in sequences more generally; Sec. 5.1 describes how to use Induction-Gram in a natural-language fMRI setting."}, {"title": "4 LANGUAGE MODELING RESULTS", "content": "Datasets We use 4 text datasets for evaluation: BabyLM, OpenWebText (Gokaslan & Cohen, 2019), Pile (Gao et al., 2020), and FineWeb ((Penedo et al., 2024); sample-10BT subset), using some as the reference corpus and some as test datasets (Table 1). When testing, we report performance on 100k sequences randomly sampled with a context length of 1024 and a stride of 512 (Liu et al., 2024; Khandelwal et al., 2020). In our speculative decoding experiments, we utilize 1024 tokens from the beginning of each document as a prefix prompt. Six prompts are employed with the BabyLM dataset, while 100 randomly sampled prompts are used for the FineWeb and Pile datasets."}, {"title": "4.2 IMPROVING NEXT-TOKEN PREDICTION ACCURACY WITH CONTEXTUALIZATION", "content": "Prediction improvements from in-context matching Induction-only (exact) relies solely on the input context to predict the next token (limited to 1024 tokens in our evaluation). Table 1 shows that, despite this, it outperforms Infini-Gram\u2014which uses the 10B-token OpenWebText dataset as a reference corpus-by a margin of 5.5%p to 20%p on the BabyLM and Pile datasets. When Infini-Gram utilizes BabyLM-dev as the reference corpus, it achieves slightly better performance than"}, {"title": "4.3 SPECULATIVE DECODING", "content": "Experimental Details To evaluate the efficiency of Induction-only (fuzzy), we compare the inference time for speculative decoding with TinyLLaMA and LLaMA2-7B (Touvron et al., 2023). We evaluate speculative decoding by generating up to 1024 tokens, using a prefix of 1024 tokens. The speed of decoding may vary depending on the computational environment. To ensure robust evaluation across different setups, we conduct experiments in two environments: one with a single NVIDIA A40 GPU and 128 CPU cores, and another with two NVIDIA H100 GPUs and 64 CPU cores. Greedy sampling is used for token generation, and each experiment is repeated three times with different random seeds."}, {"title": "5 FMRI RESULTS", "content": "A central challenge in neuroscience is understanding how and where semantic concepts are represented in the brain. To meet this challenge, we follow a line of study that predicts the response of different brain voxels (i.e. small brain regions) to natural language stimuli (Huth et al., 2016; Jain & Huth, 2018). We analyze data from LeBel et al. (2022) and Tang et al. (2023), which consists of fMRI responses for human subjects as they listen to 20+ hours of narrative stories from podcasts. We fit modules to predict the fMRI response (95,556 voxels) from the text that a single subject was hearing by extracting text embeddings. We fit the encoding models on the training split (24 stories) and evaluate them on the test split (2 stories) using bootstrapped ridge regression. Encoding model features are extracted in various ways (described below) for each word in the input, and then interpolated to make predictions for the fMRI data that is recorded at 2-second time of repetition (TR) intervals. To model temporal delays in the fMRI signal, we also add 4 time-lagged duplicates of the input features. See extended fMRI details in Appendix A.4."}, {"title": "fMRI induction head settings", "content": "We construct our induction head for fMRI by searching over recent text in an fMRI session and identifying previous changes in the recorded fMRI response. Specifically, to predict the fMRI response for the TR t, we first find the TR $t^*$ for which the text input yields the highest cosine similarity to the next-token distribution of the text input at TR t \u2013 1. Next, we isolate the change in fMRI responses following TR $t^*$: we take the difference in the top 100 principal components of the response $R_{t^*} - R_{t^*-1}$ and use them as features. To deal with potential time delays in the fMRI signal, we additionally concatenate these features with the top 100 principal components of $R_{t^*} - R_{t^*-2}$ and $R_{t^*} - R_{t^*-3}$.\nIn all cases, the induction features are concatenated with the Eng1000 features before being used to linearly predict the fMRI response. When constructing the induction head, we search over the most recent 1024 words and their corresponding fMRI responses. To measure similarity between two texts, we use the predicted next-word distributions yielded by exact ngram matching in the input context ($P_{\\text{induction}}$ in Eq. (5)), which we call Induction matching. Alternatively, we can use the"}, {"title": "5.2 INDUCTION MATCHING IMPROVES PREDICTIVE PERFORMANCE", "content": "Table 3 shows the fMRI prediction results. Eng1000, the primary interpretable baseline, achieved a mean test correlation of 0.072. In contrast, our model (Induction matching) achieves a mean correlation of 0.087, a 20% improvement over Eng1000. When predicting the top-10% of voxels, Induction Matching achieves a mean correlation of 0.265, again a 20% improvement over Eng1000, and only 1% lower than the black-box LLaMA-2 model (mean correlation 0.268). In contrast, other matching-based baselines are unable to improve over Eng1000. The Naive ngram matching baseline achieves a correlation of 0.068, and the random matching baseline achieves a correlation of 0.069, both of which perform worse than the Eng1000 baseline."}, {"title": "6 DISCUSSION", "content": "Induction-Gram constitutes a significant step towards reverse-engineering mechanistically interpretable language models from pre-trained LLMs. Here, we leverage the induction head, which is only one component found to be important in LLMs; future works could integrate new components from mechanistic interpretations, such as indirect object identifiers (Wang et al., 2022), numerical representations (Engels et al., 2024), retrieval heads (Wu et al., 2024), instruction-following heads (Zhang et al., 2023), natural-language explanations of attention heads (Bills et al., 2023) or interpretable submodules within an LLM (Singh et al., 2023b; Bricken et al., 2023). It may be possible to implement these components in a hand-engineered manner, e.g. using python code, regexes, or rule-based models, potentially yielding efficiency in addition to interpretability.\nA major limitation of Induction-Gram is that the added induction head provides little improvement when the given input context is short or uninformative. This may be partially mitigated by exploring Induction-Gram in conjunction with techniques such as retrieval-augmented-generation (Wu et al., 2024), that can fetch relevant documents to be incorporated as part of the local context. More generally, while Induction-Gram boasts a very large memory capacity, Induction-Gram relies on ngram-level reasoning and thus continues to struggle with tasks that require significant reasoning capabilities (similar to kNN-LMs (Geng et al., 2024)). Future work may explore the best way to build hybrid models using Induction-Gram and black-box LLMs to achieve effective tradeoffs.\nThe fMRI analyses conducted here are a suggestive starting point for understanding how context is stored and recalled in the human cortex. Improvements from Induction Matching may help build encoding models that can more rapidly adapt to local context, which can be used in downstream applications such as brain decoding (Tang et al., 2023) or brain-computer interfaces (Nicolas-Alonso & Gomez-Gil, 2012). More generally, the full transparency of Induction-Gram may enable its use in language modeling scenarios that require complete auditing, such as in analyzing scientific text or medical notes (Yang et al., 2023b)."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We include all experimental details necessary for reproduction in the main text and the appendix. For language modeling, explanations of the datasets are provided in Sec. 4.1, and the training details for Fuzzy Matching Model are in Appendix A.1. The inference setup of all models is described in Appendix A.3. For the natural-language fMRI experiment, details about the constructing induction-based input features are described in Sec. 5.1. Details about the publicly available data set, data collection methods, and the procedures used to map embedded stimuli to BOLD responses are provided in Appendix A.4."}, {"title": "A APPENDIX", "content": "A.1 TRAINING OF FUZZY MATCHING MODEL\nArchitecture of Fuzzy Matching Model We train two Fuzzy Matching Models, one using the GPT-2 tokenizer and the other using the LLaMA-2 tokenizer. With GPT-2 tokenizer, Fuzzy Matching Model consists of four transformer layers, whereas it comprises three transformer layers when using LLaMA-2 tokenzer. Since relative position is crucial for calculating similarity, we incorporate"}]}