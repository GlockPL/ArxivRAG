{"title": "CHAT BCG: CAN AI READ YOUR SLIDE DECK?", "authors": ["Nikita Singh", "Rob Balian", "Lukas Martinelli"], "abstract": "Multimodal models like GPT40 and Gemini Flash are exceptional at inference and summarization\ntasks, which approach human-level in performance. However, we find that these models underperform\ncompared to humans when asked to do very specific 'reading and estimation' tasks, particularly in the\ncontext of visual charts in business decks. This paper evaluates the accuracy of GPT 40 and Gemini\nFlash-1.5 in answering straightforward questions about data on labeled charts (where data is clearly\nannotated on the graphs), and unlabeled charts (where data is not clearly annotated and has to be\ninferred from the X and Y axis).\nOn labeled charts, we found that both GPT 40 and Gemini Flash are consistently inaccurate on\nspecific types of charts (multiple figures in one chart, stacked charts, waterfall charts). Human error\nrates are estimated to be under 5%, while GPT-40 and Gemini Flash have error rates of 16% and\n14% 1, respectively. Both LLMs often make similar mistakes, such as consistently misreading a '3'\nas an '8' or mislabelling a negative number as positive. Across charts, neither model consistently\noutperforms the other.\nSimiliarly, on unlabeled charts, both GPT and Gemini have similiar failure points. The models were\ntasked with estimating a number from an unlabeled chart by reading the X and Y axis. As expected,\nperfectly matched answers were low, with error rates as high as 79% for Gemini and 83% for GPT 40.\nHere, we measured the magnitude of the error for each model, compared to a (human read) source of\ntruth value. The errors were substantial, with average deviations of 53% for Gemini Flash-1.5 and\n55% for GPT-402, compared to a human margin of error estimated at 10-20%. Driving the deviations\nare consistently small errors in estimation, but also bigger deviations driven by models misreading\nlabels/numbers. For instance, GPT 4o in one instance misread 2015 as 2009 and estimated the number\nfor the wrong year.\nWe conclude that these models aren't currently capable of reading a deck accurately end-to-end if it\ncontains any complex or unlabeled charts. Even if a user created a deck of only labeled charts, the\nmodel would only be able to read 7-8 out of 15 labeled charts perfectly end-to-end.", "sections": [{"title": "1 Introduction", "content": "With the advanced vision capabilities of GPT-40 and Gemini Flash, an important question arises regarding the accuracy\nof these functionalities in practical business applications. Our assumption was that multimodal models are good at\nreading and summarizing charts. When given an image of a slide deck, they do a good job of summarizing key insights\nfrom it, often including relevant data points.\nExisting research into this question has evaluated the efficacy of LLM's when parsing tables [3], concluding that the\nLLMs were highly sensitive to input prompts which drive performance. Other works also evaluate LLMs ability to\nreason and read mathematical graphs [2] and find that GPT models outperform alternatives.\nThis paper aims to explore whether multimodal models perform well on a variant of this skill - answering straight-\nforward questions that require the models to pick out a number from a slide deck. We test this by asking the models\nquestions about data directly printed on charts (if labeled) or asking the model to estimate data points from the chart (if"}, {"title": "2 How accurate are multimodal models in reading charts?", "content": "unlabeled). We ensure that the model does not need to perform any mathematical calculations. We then measure the\naccuracy of responses across different types of charts to answer the following specific questions:\n\u2022 How accurately can multimodal models with advanced vision capabilities read data from labeled charts? Is\nthere a consistent accuracy advantage for one model over the other?\n\u2022 How accurately can the models estimate numerical data from unlabeled charts? On average, how 'incorrect' is\ntheir estimation?\nIn the next section, we dive into error rates across different chart types, broken into two categories - labeled and\nunlabeled."}, {"title": "2.1 Methodology", "content": "We sampled 31 charts and classified them into 2 broad categories:\n\u2022 Labeled (15 charts): This category includes simple bar charts, line charts, and multi-bar charts where each\ndata point is explicitly printed on the chart. We also included some more complex labeled charts, such as\nwaterfall charts, stacked charts, bubble charts, and connected bar charts.\n\u2022 Unlabeled (16 charts): This category is primarily focused on charts that do not have explicitly printed data\npoints and require some estimation of data by 'reading the position' relative to the X and Y axis. It does not\ninclude charts that do not have any scale at all. The dataset includes simpler unlabeled charts such as bar\ncharts, line charts, mixed charts, and bubble charts, as well as some more complex unlabeled charts such as\nmoon charts and dot charts.\nFor each chart, we created a dataset of questions. These questions are restricted to 3 types:\n\u2022 Identify a specific data point\n\u2022 Identify the largest/lowest data point\n\u2022 Count the number of data points\nThe goal of these questions is to test the ability to read and interpret data directly from the chart, without requiring any\nintensive computation.\nWe then evaluate the responses of both the models on two metrics:\n\u2022 Match Rate %: This metric is used for labeled charts where the model is reading data printed on the chart,\nbut reported for both. It was calculated as\n$$Match Rate = (\\frac{Number \\:of \\:perfectly \\:matched \\:answers}{Total \\:number \\:of \\:questions}) \u00d7 100$$\n\u2022 Mean Absolute Error (MAE): This metric is reported for unlabeled charts where the model is 'estimating'\nthe number from the graph. It measures the average magnitude of the errors in a set of predictions, without\nconsidering their direction. This was calculated as\n$$MAE = \\frac{1}{n}  \\sum_{i=1}^{n} |A_i - P_i|$$\nwhere:\nn is the number of data points,\nAi is the actual value for the i-th data point,\nPi is the predicted value for the i-th data point.\n\u2022 Mean Absolute Percentage Error (MAPE): This metric is reported for unlabeled charts where the model is\n'estimating' the number from the graph. It can be read as how divergent the model's answer was from the\ncorrect answer, normalized to a percentage. This was calculated as\n$$MAPE  =  \\frac{1}{n} \\sum_{i=1}^{n} |\\frac{A_i-P_i}{A_i}|\u00d7100$$\nwhere:"}, {"title": "2.1.1 Methodology Walk-Through for Labeled Chart", "content": "For instance, in the labeled chart in Figure 1 (below), we generated questions and measured the match % as given in\nTable 1"}, {"title": "2.1.2 Methodology Walk-Through for Unlabeled Chart", "content": "For unlabeled charts (see Figure 2 below), we generated questions and estimated the MAPE, as given in Table 2"}, {"title": "2.2 Summary Findings", "content": "Using the above methodology, the following table reports the overall match % and MAPE % for labeled and unlabeled\ncharts across the sample of 31 charts:"}, {"title": "2.3 Labeled Charts: What is the match rate across charts?", "content": "Across 15 charts and a total of 71 questions, GPT 4o reads 8 charts perfectly end-to-end and Gemini Flash-1.5 read 7\ncharts perfectly. In charts where it makes errors, the models are never incorrect on all the questions. Both models make\nlarge errors on a few data points, usually due to misreading a label. Since these aren't estimation errors, the range of\nerrors is erratic, as evident from Table 5. Both models struggle with charts where multiple figures are present."}, {"title": "GPT 4o failure points", "content": "In some cases, 4o misunderstands the data associated with a given label in stacked charts. In\n'two figures: stacked multi-bar', when asked the Athleta store count in 2012, it returned the count for Microsoft stores\ninstead. Similarly, in 'eight multi-bar charts', it mistakenly returns the diabetes rate for Japan instead of the obesity rate\nIn other cases, 4o misidentifies numbers on lower resolution charts. In 'stacked bar chart', for instance, GPT 40 misreads\n'3' as '8' when asked to identify the % of public sector publications by KPMG, a mistake that Gemini did not make."}, {"title": "Gemini failure points", "content": "Similiar to 40, Gemini errors are driven by misinterpreting the label but also contain an\nelement of misinterpreting the question, which GPT 40 doesn't demonstrate as frequently. In 'two figures - multi-bars',\nfor instance, when asked what % of financing sources come from private investment (other than EPR), it answers from\nthe wrong bar graph. 4o on the other hand answers from the correct table, but the wrong label within that table. While\noutside the scope of these error rates, we also noticed that 4o was better at answering some reasoning / calculation\nquestions compared to Gemini"}, {"title": "2.4 Unlabeled Charts: What is the magnitude of error across charts?", "content": "Across 82 questions in 15 unlabeled charts, both GPT 4o and Gemini Flash are at par on the mean absolute percentage\nerror at 55%."}, {"title": "3 Conclusion", "content": "While both GPT-40 and Gemini Flash 1.5 exhibit many advanced capabilities in reading charts, our evaluation reveals\nsome consistent limitations in their current performance. Despite being better than other alternatives, these models still\nrequire human oversight to achieve acceptable accuracy levels. For labeled charts, the models demonstrate an average\nerror rate of 15%, which may not be suitable for high-stakes business applications. The performance on unlabeled\ncharts is particularly inconsistent, with error rates exceeding 100% for more complex visuals. When assessed using a\ncomprehensive set of 30 charts, representing a typical business deck, the models are only able to read 7-8 charts with\n100% match accuracy (all questions correct). Thus, for any use case demanding high precision, these models are not yet\nready to operate without human intervention."}, {"title": "4 Appendix", "content": null}, {"title": "4.1 Labeled Charts: Questions and Model Responses", "content": null}, {"title": "4.2 Unlabeled Charts: Questions and Model Responses", "content": null}]}