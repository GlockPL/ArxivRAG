{"title": "CHAT BCG: CAN AI READ YOUR SLIDE DECK?", "authors": ["Nikita Singh", "Rob Balian", "Lukas Martinelli"], "abstract": "Multimodal models like GPT4o and Gemini Flash are exceptional at inference and summarization\ntasks, which approach human-level in performance. However, we find that these models underperform\ncompared to humans when asked to do very specific 'reading and estimation' tasks, particularly in the\ncontext of visual charts in business decks. This paper evaluates the accuracy of GPT 4o and Gemini\nFlash-1.5 in answering straightforward questions about data on labeled charts (where data is clearly\nannotated on the graphs), and unlabeled charts (where data is not clearly annotated and has to be\ninferred from the X and Y axis).\nOn labeled charts, we found that both GPT 4o and Gemini Flash are consistently inaccurate on\nspecific types of charts (multiple figures in one chart, stacked charts, waterfall charts). Human error\nrates are estimated to be under 5%, while GPT-4o and Gemini Flash have error rates of 16% and\n14% 1, respectively. Both LLMs often make similar mistakes, such as consistently misreading a '3'\nas an '8' or mislabelling a negative number as positive. Across charts, neither model consistently\noutperforms the other.\nSimiliarly, on unlabeled charts, both GPT and Gemini have similiar failure points. The models were\ntasked with estimating a number from an unlabeled chart by reading the X and Y axis. As expected,\nperfectly matched answers were low, with error rates as high as 79% for Gemini and 83% for GPT 4o.\nHere, we measured the magnitude of the error for each model, compared to a (human read) source of\ntruth value. The errors were substantial, with average deviations of 53% for Gemini Flash-1.5 and\n55% for GPT-4o2, compared to a human margin of error estimated at 10-20%. Driving the deviations\nare consistently small errors in estimation, but also bigger deviations driven by models misreading\nlabels/numbers. For instance, GPT 4o in one instance misread 2015 as 2009 and estimated the number\nfor the wrong year.\nWe conclude that these models aren't currently capable of reading a deck accurately end-to-end if it\ncontains any complex or unlabeled charts. Even if a user created a deck of only labeled charts, the\nmodel would only be able to read 7-8 out of 15 labeled charts perfectly end-to-end.", "sections": [{"title": "1 Introduction", "content": "With the advanced vision capabilities of GPT-4o and Gemini Flash, an important question arises regarding the accuracy\nof these functionalities in practical business applications. Our assumption was that multimodal models are good at\nreading and summarizing charts. When given an image of a slide deck, they do a good job of summarizing key insights\nfrom it, often including relevant data points.\nExisting research into this question has evaluated the efficacy of LLM's when parsing tables [3], concluding that the\nLLMs were highly sensitive to input prompts which drive performance. Other works also evaluate LLMs ability to\nreason and read mathematical graphs [2] and find that GPT models outperform alternatives.\nThis paper aims to explore whether multimodal models perform well on a variant of this skill - answering straight-\nforward questions that require the models to pick out a number from a slide deck. We test this by asking the models\nquestions about data directly printed on charts (if labeled) or asking the model to estimate data points from the chart (if"}, {"title": "2 How accurate are multimodal models in reading charts?", "content": "2.1 Methodology\nWe sampled 31 charts and classified them into 2 broad categories:\n\u2022 Labeled (15 charts): This category includes simple bar charts, line charts, and multi-bar charts where each\ndata point is explicitly printed on the chart. We also included some more complex labeled charts, such as\nwaterfall charts, stacked charts, bubble charts, and connected bar charts.\n\u2022 Unlabeled (16 charts): This category is primarily focused on charts that do not have explicitly printed data\npoints and require some estimation of data by 'reading the position' relative to the X and Y axis. It does not\ninclude charts that do not have any scale at all. The dataset includes simpler unlabeled charts such as bar\ncharts, line charts, mixed charts, and bubble charts, as well as some more complex unlabeled charts such as\nmoon charts and dot charts.\nFor each chart, we created a dataset of questions. These questions are restricted to 3 types:\n\u2022 Identify a specific data point\n\u2022 Identify the largest/lowest data point\n\u2022 Count the number of data points\nThe goal of these questions is to test the ability to read and interpret data directly from the chart, without requiring any\nintensive computation.\nWe then evaluate the responses of both the models on two metrics:\n\u2022 Match Rate %: This metric is used for labeled charts where the model is reading data printed on the chart,\nbut reported for both. It was calculated as\nMatch Rate = $$\\left(\\frac{\\text{Number of perfectly matched answers}}{\\text{Total number of questions}}\\right) \\times 100$$\n\u2022 Mean Absolute Error (MAE): This metric is reported for unlabeled charts where the model is 'estimating'\nthe number from the graph. It measures the average magnitude of the errors in a set of predictions, without\nconsidering their direction. This was calculated as\nMAE = $$\\frac{1}{n} \\sum_{i=1}^{n} |A_i - P_i|$$\nwhere:\nn is the number of data points,\nAi is the actual value for the i-th data point,\nPi is the predicted value for the i-th data point.\n\u2022 Mean Absolute Percentage Error (MAPE): This metric is reported for unlabeled charts where the model is\n'estimating' the number from the graph. It can be read as how divergent the model's answer was from the\ncorrect answer, normalized to a percentage. This was calculated as\nMAPE = $$\\frac{1}{n} \\sum_{i=1}^{n} \\frac{|A_i - P_i|}{A_i} \\times 100$$\nwhere:"}, {"title": "2.3 Labeled Charts: What is the match rate across charts?", "content": "Across 15 charts and a total of 71 questions, GPT 4o reads 8 charts perfectly end-to-end and Gemini Flash-1.5 read 7\ncharts perfectly. In charts where it makes errors, the models are never incorrect on all the questions. Both models make\nlarge errors on a few data points, usually due to misreading a label. Since these aren't estimation errors, the range of\nerrors is erratic, as evident from Table 5. Both models struggle with charts where multiple figures are present."}, {"title": "2.4 Unlabeled Charts: What is the magnitude of error across charts?", "content": "Across 82 questions in 15 unlabeled charts, both GPT 4o and Gemini Flash are at par on the mean absolute percentage\nerror at 55%."}, {"title": "3 Conclusion", "content": "While both GPT-40 and Gemini Flash 1.5 exhibit many advanced capabilities in reading charts, our evaluation reveals\nsome consistent limitations in their current performance. Despite being better than other alternatives, these models still\nrequire human oversight to achieve acceptable accuracy levels. For labeled charts, the models demonstrate an average\nerror rate of 15%, which may not be suitable for high-stakes business applications. The performance on unlabeled\ncharts is particularly inconsistent, with error rates exceeding 100% for more complex visuals. When assessed using a\ncomprehensive set of 30 charts, representing a typical business deck, the models are only able to read 7-8 charts with\n100% match accuracy (all questions correct). Thus, for any use case demanding high precision, these models are not yet\nready to operate without human intervention."}]}