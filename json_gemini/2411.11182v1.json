{"title": "Improving User Experience in Preference-Based Optimization of Reward Functions for Assistive Robots", "authors": ["Nathaniel Dennler", "Zhonghao Shi", "Stefanos Nikolaidis", "Maja Matari\u0107"], "abstract": "Assistive robots interact with humans and must adapt to different users' preferences to be effective. An easy and effective technique to learn non-expert users' preferences is through rankings of robot behaviors, for example, robot movement trajectories or gestures. Existing techniques focus on generating trajectories for users to rank that maximize the outcome of the preference learning process. However, the generated trajectories do not appear to reflect the user's preference over repeated interactions. In this work, we design an algorithm to generate trajectories for users to rank that we call Covariance Matrix Adaptation Evolution Strategies with Information Gain (CMA-ES-IG). CMA-ES-IG prioritizes the user's experience of the preference learning process. We show that users find our algorithm more intuitive and easier to use than previous approaches across both physical and social robot tasks.", "sections": [{"title": "1 Introduction", "content": "Numerous technical advances in robotics have allowed robots to perform increasingly complex physical and social tasks. As robots move from laboratories to in-home settings, they will be confronted with users and contexts that were not previously seen or tested by the robot's developers. In order to be useful in these new contexts, robots must adapt their behaviors, e.g., movement trajectories, affective gestures, and voice, to align with preferences and expectations of specific users [15,39]. One user may prefer that a robot hands them an item as quickly as possible, whereas another user may want the robot to stay as far away from a priceless family heirloom as possible. Users of these systems will likely not have experience directly programming robots, and thus robots must be teachable in more intuitive ways.\nPrevious work has identified that ranking robot trajectories can be a simple and effective method for non-expert users to teach robots their preferences for how the robot should move [10,27]. Two main approaches use ranking information to learn preferences: (1) using rankings to learn an explicit model of a user's"}, {"title": "2 Related Work", "content": "Preferences in Physically Assistive Robots. Physically assistive robots help users with tasks such as collaborative assemblies [35,21], physical rehabilitation [48], and activities of daily living [5,16]. Though these goals appear to be objective and universal, differences in user preferences for how such goals are accomplished affect efficiency in human-robot collaboration [48,36]. For exam-"}, {"title": "Preferences in Socially Assistive Robots", "content": "Socially assistive robots (SARS) are designed to assist users through primarily social interaction, rather than physical means [32]. SARs have been used to facilitate behavior change in a wide variety of user populations, including college students [37,28], children with learning differences [41,43], users with limited mobility [17,14], and the elderly [20,49]. Due to the unique and varied needs of these populations, system designers must develop robot behaviors that are specifically tailored to each specific user population [13].\nPragmatic constraints do not allow robot system developers to carefully design systems for each new user. Therefore, to make robots useful to diverse populations, users must be able to customize the system to align with their preferences [19]. Previous works have found several benefits of adapting robot behaviors to the user [39], though these works often require researchers to modify the robot [31,46]. Allowing the user to modify a robot's social behaviors such as voice [43], personality [46], challenge level of the assistive tasks [12], and gesture [33] leads to increased acceptance of the system."}, {"title": "Learning User Preferences", "content": "There are many ways to learn preferences from users in order to support users in accomplishing tasks [18]. Robots can learn a user's internal reward function through inverse reinforcement learning a function that maps a low-dimensional representation to a scalar value for each robot behavior. Various techniques can be used to learn this mapping from trajectory to reward, including demonstrations [1], physical corrections [4], language [42], trajectory rankings [10], and trajectory comparisons [40]. Users may need varying levels of expertise with using robot systems to effectively use those techniques [18]. Our work focuses on using trajectory rankings, a technique that is accessible to users of all levels of expertise with using robot systems.\nThere are two popular approaches for learning preferences when using rankings to learn user preferences for robot trajectories. One approach is to explicitly model the user's reward function by estimating the probability distribution over parameters of a reward function [40,7]. This type of approach decomposes a trajectory ranking into a series of trajectory selections and uses probabilistic models of human choice to update the distribution over reward weights using Bayes' rule. The Bayesian approach considers the whole space of trajectories at"}, {"title": "3 Learning User Preferences Through Comparisons", "content": "Preliminaries. We describe robot behaviors as output trajectories from a dynamical system. A trajectory $\\xi\\in \\Xi$ is defined as a sequence of states, $s \\in S$ and actions $a \\in A$ that follow the system dynamics, i.e, $\\xi = (s_0, a_0, s_1, a_1, ..., s_T, a_T)$ for a finite horizon of $T$ time steps. These states are abstractly defined and can be anything from joint angles to end-effector positions to images, and actions simply convert one state to another state. Following common practices in inverse reinforcement learning [1], we assume that there exists a feature function $\\Phi : X \\rightarrow \\mathbb{R}^d$ that represents aspects of the state that the user may have preferences over. A trajectory can then be represented as a low-dimensional vector in $\\mathbb{R}^d$ via $\\Phi(\\xi) = \\Sigma_{t=0}^{T} \\phi(s_i)$.\nA user's preference for robot behaviors is a function of these trajectory features, $R(\\xi) = f(\\Phi(\\xi))$. In this work, we make the assumption that a user's preference is a linear combination over features $R(\\xi) = w^T \\cdot \\Phi(\\xi)$, where $w \\in \\mathbb{R}^d$, as in several previous works [40,6]. When a user is asked to rank trajectories, they are presented with a set of $N$ trajectories referred to as a trajectory query, $Q = {\\xi_0, \\xi_1, ..., \\xi_N}$. A user then ranks these trajectories according to their internal reward function $R = (\\xi^0, \\xi^1, ..., \\xi^N)$ such that $i < j < ... < N$."}, {"title": "Bayesian Optimization of Preferences", "content": "Approaches that explicitly model the user's reward function maintain a probability distribution over w and update this distribution based on the user's ranking. A common framing is to view a ranking as an iterative process of selecting the best trajectories from the query without replacement [34], and use the widely-adopted Bradley-Terry model of rational choice [8] to estimate the probability that a user will select a given trajectory from a set of trajectories at each iteration, subject to a rationality parameter $\\beta$:\n$p(\\xi | Q) = \\frac{e^{\\beta \\cdot \\Phi(\\xi)}}{\\Sigma_{\\xi'\\in Q} e^{\\beta \\cdot \\Phi(\\xi')}}$\nUsing this model of user preferences and assuming that these selections are conditionally independent, the distribution over w can be calculated using Bayes' rule:\n$p(w | R) \\propto p(w) \\Pi_{i=0}^{N} p(\\xi_i | Q_i)$\nwhere $\\xi_i$ represents the trajectory selected from the set at iteration i and $Q_i$ represents the set of trajectories left at iteration i. Equivalently, $Q_i = Q_{i+1} \\cup \\xi_i$.\nThe state of the art technique for generating the set of trajectories Q that a user ranks is to maximize an information gain objective as described by B\u0131y\u0131k et al. [6]:\n$\\arg \\max_{Q={\\xi_0, \\xi_1,...,\\xi_N}} H(\\omega | Q) - E_{\\xi \\epsilon Q}H (\\omega | \\xi, Q)$\nwhere H is the Shannon Information Entropy. Maximizing this objective to form Q results in a set of trajectories that maximally update the distribution over w when receiving the user's feedback. In addition, these trajectories are distinct from each other, enabling the user to easily differentiate among them. For linear reward functions, this implies that trajectories have large distances from each other in feature space."}, {"title": "Covariance Matrix Adaptation Evolution Strategies (CMA-ES)", "content": "Evolution strategies (ES) is a large body of algorithms that focus on solving continuous, black-box, mainly experimental optimization problems. ES algorithms sample a population of solutions for each generation, and move this sampled population toward solutions with more optimal fitness values generation by generation [3]. Covariance Matrix Adaptation Evolution Strategies (CMA-ES) is proposed to reduce the number of generations needed to converge to the optimal solutions and improve the noise tolerance of the optimization process [25]. Compared to other ES methods, CMA-ES has been evaluated as one of the most competitive derivative-free optimization algorithms for continuous spaces [24]. More specifically, CMA-ES samples an underlying distribution of the population"}, {"title": "Combining Information Gain and CMA-ES", "content": "We propose a new algo-rithm, Covariance Matrix Adaptation Evolution Strategies with Information Gain (CMA-ES-IG), for efficiently learning user preferences. CMA-ES-IG leverages the benefits of both of the previous strategies: it uses the information gain objective to generate sets of trajectories that are easy to rank, and it uses the adaptive sampling mechanism from CMA-ES to increase the average user reward of the proposed trajectories over time. We summarize CMA-ES-IG in Algorithm 1, and provide a visual intuition of these three algorithms in Fig. 2.\nFirst, we initialized CMA-ES-IG identically to CMA-ES and our belief over user preferences to a uniform distribution. We then sample D trajectory features from the CMA-ES-IG mean and covariance to create a set of trajectory features, D. Next, we find |Q| samples from D that maximize the expected information gain, as described in Equation 3. While finding the exact solution to this optimization problem is exponential in |Q|, an efficient approximation is to find |Q| medoids in the set of samples [7]. We adopt this approximation to allow CMA-ES-IG to be computationally tractable."}, {"title": "3.1 Validation through Simulated Preferences", "content": "To validate our algorithm before presenting it to users, we performed an algorithmic analysis by simulating user preferences as in previous work [40,6]. We used the parameter estimation task as described by Fitzgerald et al. [18], which samples a ground truth weight vector, w* from a d-dimensional space. To ensure that each w* is comparable, we projected each to the unit ball [40]. We then generated queries of four trajectories using our three algorithms (Infogain, CMA-ES, and CMA-ES-IG) for a simulated user to rank. We used the distribution described by Bradley-Terry preference model to perform these rankings, given the ground truth vector of the simulated user. We updated the distribution over the weight vector using Equation 2. We simulated 100 users performing 30 rankings for each of the three algorithms.\nWe are interested in two evaluation metrics: alignment, which measures how well the estimated preference matches the true preference, and quality, which measures the overall reward of the trajectories presented to the user. We define alignment as the cosine similarity between the estimated west and the ground truth w*, as in previous works [18,40]. We define quality as the average reward of the trajectories in the query $\\Sigma w^* \\cdot \\Phi(\\xi_i)$. We measure these values after each simulated query to generate curves that show how each metric increases with repeated querying. To compare between curves, we used the area under the curve (AUC) metric, which provides values between -1 and 1, with 1 being the best. We show results for alignment and quality across features spaces of d \u2208 {8,16,32} in Table 1; For completeness, we also report regret as a secondary metric for alignment in Table 1. Where regret $\\omega^*\\cdot \\phi(\\xi^*) - \\omega^*\\cdot \\phi(\\xi')$; $\\xi^*$ denotes the trajectory with the highest reward in under w*, and $\\xi'$ denotes trajectory with the highest reward in $\\hat{w}$ under west."}, {"title": "4 Experimental Evaluation", "content": "To evaluate user perception of different methods for optimizing preferences, we conducted a within-subjects user study to compare participant perceptions among the different methods for learning preferences. In the physical domain, participants specified preferences for how a JACO robot arm hands them a"}, {"title": "4.1 Experimental Setup", "content": "We developed a framework, shown in Fig. 4, to evaluate the different techniques for learning participant preferences. It consists of three learned components: trajectory representations, query sampling, and preference learning models.\nTo generate trajectory representations, we used a dataset of 1000 robot handover trajectories and 1500 Blossom gesture trajectories. Trajectories consisted of a sequence 50 joint states that were equally spaced in time. The Blossom robot has joint states in $\\mathbb{R}^4$, and trajectories were played by sending servo commands to each of the four servos that control the Blossom at a rate of 10Hz. The JACO arm has joint states in $\\mathbb{R}^6$ so trajectories were played by fitting a b-spline to the those joint states and sending the goal joint states to an impedance controller at a rate of approximately 50Hz. The JACO arm trajectories were scaled to 9 seconds in duration. We generated trajectories for Blossom and the JACO arm by sampling from demonstrations, however other techniques can be used to create the datasets, such as quality diversity, reinforcement learning, or planning approaches.\nFrom the generated trajectories, we used an autoencoder (AE) to generate nonlinear features for the trajectories for the social and physical domains in our study, which is shown to be effective in prior work [9]. The AEs consisted of three convolutional layers and two fully connected layers with leaky ReLU activations after each layer. Hyperparameters were tuned to minimize the reconstruction loss over all trajectories in the dataset. The feature space of Blossom's trajectories was six-dimensional and the features space of the JACO arm trajectories was four-dimensional.\nThe query sampling component generated the set of trajectories to show to the participant. At that point we performed our experimental intervention."}, {"title": "4.2 User Study Details", "content": "We performed a within-subjects user study to identify the differences in user experience between the proposed algorithms across two domains. In particular, we were interested in two key factors that determine the actual use of systems: ease of use [47] (EOU), and perceived behavioral adaptation [29] (BA). Perceived ease of use measures how easily participants are able to get the robot to do what they want, and behavioral adaptation measures how much the users perceive the robot as changing in response to their inputs. Specific Likert scale items in our study are listed in Table 2. Users rated these metrics on a 9-point Likert scale with 0 corresponding to strongly disagree and 8 corresponding to strongly agree. We average across the questions for each of these factors to calculate our evaluation metric.\nOur study procedure was approved by the University of Southern California IRB under #UP-24-00600 and proceeded as follows: first, the participant was greeted by the experimenter and randomly assigned to specify their preferences for either the physical or social robot interaction. Next, the participant specified their preferences using a randomized and counter-balanced algorithm for the first task for their assigned domain-a marker handover in the physical domain, or a happy gesture in the social domain. After five minutes, the participant rated the algorithm's EOU and BA. Next, the participant was presented with the next randomized algorithm and completed the second task for their assigned domain-a cup handover in the physical domain, or sad gesture for the social domain. Participants then rated the EOU and BA of the algorithm. The participant then interacted with the final randomized algorithm in their assigned domain-a spoon handover in the physical domain, or an angry gestures for the social domain. The user rated the EOU and BA of the final algorithm. The user then ranked the three algorithms against each other to specify their overall preferences. The participant then completed the same process for the other domain."}, {"title": "4.3 User Study Results", "content": "Ease of Use. We evaluated the average scores for EOU from a four-item Likert scale. We identified high internal consistency of the scale, with a Cronbach's alpha of $\\alpha$ = .89, indicating good internal consistency. To evaluate significance we used pairwise non-parametric repeated-measures tests. As shown in Fig. 6, we found that CMA-ES-IG received the highest EOU ratings (M = 5.50), followed by Information Gain (M = 5.13), and CMA-ES received the lowest rating for EOU (M = 4.87). The difference between ratings for CMA-ES-IG and CMA-ES was significant (W = 5.5, p = .016) with a medium effect size (hedge's g = .558). This indicates that including the information gain objective in CMA-ES-IG indeed makes it easier to use.\nPerceived Behavioral Adaptation. We evaluated the average scores for BA from a four-item Likert scale, and identified high internal consistency of the scale, with a Cronbach's alpha of $\\alpha$ = .97, indicating excellent internal consistency. We used non-parametric repeated-measures pairwise comparisons to assess significance. As shown in Fig. 7, we found that CMA-ES-IG received the highest BA ratings (M = 5.18), followed by CMA-ES (M = 4.69), and Information Gain received the lowest rating for BA (M = 4.48). CMA-ES-IG was rated as significantly higher than both CMA-ES (W = 15, p = .033) with a small to medium effect (hedge's g = .377), and IG (W = 5.5, p = .009) with a medium effect size (hedge's g = .414). This"}, {"title": "4.4 Conclusion", "content": "In this paper, we identified how the user experience of teaching robots preferences can be improved. We proposed a novel algorithm, CMA-ES-IG, that presents options for users to rank that become increasingly better over time and are easy for users to differentiate. We showed through simulation that CMA-ES-IG robustly improves the quality of these options across several parameters while maintaining the ability to efficiently learn user preferences. We showed experimentally through a user study that participants teaching physical and social robots prefered to use CMA-ES-IG, found CMA-ES-IG the easiest to use, and found CMA-ES-IG the most adaptive algorithm compared to the current state of the art approaches."}, {"title": "A Hyperparameter Sensitivity Analysis", "content": "In this paper we explored using CMA-ES and CMA-ES-IG as algorithms for generating queries. We used the default step size parameter of $\\sigma = 0.5$ which is recommended when there is little information about the structure of the problem space. This ensures that our results are not tuned specifically for our simulation environment, making it more informative of our user study.\nWe found that CMA-ES is relatively insensitive to changes in parameters due to its automatic step-size adaptation mechanism. This result highlights the strength of CMA-ES-IG to be applicable to a variety of problems without requiring intensive tuning processes."}]}