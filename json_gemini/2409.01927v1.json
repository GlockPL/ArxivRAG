{"title": "From Grounding to Planning: Benchmarking Bottlenecks in Web Agents", "authors": ["Segev Shlomov", "Ben wiesel", "Aviad Sela", "Ido Levy", "Liane Galanti", "Roy Abitbol"], "abstract": "General web-based agents are increasingly essential for interacting with complex web environments, yet their performance in real-world web applications remains poor, yielding extremely low accuracy even with state-of-the-art frontier models. We observe that these agents can be decomposed into two primary components: Planning and Grounding. Yet, most existing research treats these agents as black boxes, focusing on end-to-end evaluations which hinder meaningful improvements. We sharpen the distinction between the planning and grounding components and conduct a novel analysis by refining experiments on the Mind2Web dataset. Our work proposes a new benchmark for each of the components separately, identifying the bottlenecks and pain points that limit agent performance. Contrary to prevalent assumptions, our findings suggest that grounding is not a significant bottleneck and can be effectively addressed with current techniques. Instead, the primary challenge lies in the planning component, which is the main source of performance degradation. Through this analysis, we offer new insights and demonstrate practical suggestions for improving the capabilities of web agents, paving the way for more reliable agents.", "sections": [{"title": "1 Introduction", "content": "Generalized web agents are designed to autonomously navigate and interact with complex web environments, performing tasks that range from simple information retrieval to intricate multi-step procedures (Deng et al. 2024; He et al. 2024; Zhou et al. 2023). As the demand for automation and intelligent interaction with web interfaces grows, these agents are becoming increasingly crucial in various applications, such as virtual assistants, automated customer service, and copilots (Drouin et al. 2024; Zheng et al. 2024). Significant effort has been dedicated to the development of robust and reliable web agents, ranging from the development of specialized large language models (LLMs) tailored for web navigation (Deng et al. 2024; Zheng et al. 2024) to the incorporation of state-of-the-art (SOTA) generalist models such as GPT-4 with advanced vision capabilities (He et al. 2024; Zheng et al. 2024). Despite these advancements, the performance of web agents in real-world scenarios remains low. These agents frequently fail to achieve reasonable accuracy in completing web-based tasks, raising concerns about their reliability and effectiveness in practical applications (Li and Waldo 2024; Yoran et al. 2024). A key challenge lies in the prevalent approach to evaluating web agents, which often treats them as black-box systems and focuses on end-to-end performance metrics (Deng et al. 2024). While this approach offers a high-level perspective on agent capabilities, it tends to obscure individual skills and capabilities required for the task, making it difficult to address specific performance issues. In particular, it fails to distinguish between two core components of web agents: Planning and grounding. Planning refers to the agent's ability to determine the appropriate sequence of actions to accomplish a given task, while grounding involves correctly identifying and interacting with relevant web elements based on these decisions (Zheng et al. 2024). Motivated by this understanding, this paper aims to dissect the influence of planning and grounding on web agents' performance. By isolating these factors, our research determines which aspect predominantly affects the agents' ability to complete web-based tasks. This differentiation clarifies pathways for enhancing agent capabilities and provides clearer insights into the optimal allocation of resources within the web-agent's pipeline. To achieve this, we modified the experimental setup of Mind2Web, implementing controlled variations that isolate the planning and grounding components. Specifically, we introduce two distinct modes of operation. The first, termed high-level, aligns with the traditional method of evaluating agents' performance on Mind2Web (Deng et al. 2024). In this mode, the agent is given a general description of a multi-step task and must determine the correct sequence of actions to complete it. The second, termed low-level, gives the model explicit references to the elements it needs to interact with at each step of the task. This mode aims to isolate the grounding component by eliminating planning. Our experiments reveal several key conclusions. Contrary to previous assertions (Zheng et al. 2024), planning emerges as the primary bottleneck limiting overall agent performance. We show that even under controlled setup, where models have to choose between merely two elements, of which one is the ground-truth element, accuracy is much lower than expected. Moreover, we find that when grounding is isolated from planning, a near-perfect element accuracy is achieved, highlighting that focusing only on improving grounding techniques through advanced computer vision (CV) or Document Object Model (DOM)-based methods is not sufficient. Furthermore, we find that reducing the number of candidate elements for the agents consistently improves performance, highlighting the importance of effective element filtration and ranking mechanisms in complex web navigation tasks. Our main contributions are:\n\u2022 We sharpen the decomposition of web-based agents into two core components\u2014Planning and Grounding. This allows for a granular analysis, shifting from black-box evaluation to targeted improvements.\n\u2022 We introduce a refined benchmark based on Mind2Web, enabling separate evaluation of planning and grounding, revealing that planning is the main bottleneck.\n\u2022 We demonstrate that by improving the page understanding and enhancing the ranking mechanism to support the planning component, our agent, WebNaviX, surpasses the state-of-the-art by 13%, paving the way for more reliable web agents."}, {"title": "2 Related Work", "content": "General Web Agents Recent advancements have propelled web-based agents to the forefront of navigating and manipulating complex digital environments. These agents are instrumental in a wide array of web automation tasks, including automated testing, personal assistance, data extraction, content management, and enhancing accessibility through assistive technologies (Xi et al. 2023; Akter et al. 2023). To facilitate the development and benchmarking of these agents, various datasets have been created, such as Mind2Web, AgentBench, and WebArena (Liu et al. 2023; Zhou et al. 2023). These datasets offer a broad spectrum of tasks across multiple domains to test generalizability, evaluate the reasoning and decision-making abilities of agents in interactive environments, and provide a realistic and reproducible web environment.\nCurrent web agents fail to achieve the accuracy required for real-world applications (He et al. 2024; Cheng et al. 2024; Wang et al. 2024; Chen et al. 2023; Pan et al. 2024; Koh et al. 2024), raising critical questions about their applicability. These agents primarily rely on two components: Planning as the agent's \"mind\", which processes information to formulate subsequent actions, and grounding as the agent's \"eyes\u201d and \u201chands\u201d, which interpret the current state and chosen action to identify and select the correct element. Recent discussions, including claims by Zheng et al. (2024), suggest that models like GPT-40 could effectively plan actions if appropriately grounded. Such insights are crucial as we seek to discern the roles of grounding and planning in enhancing agent efficiency.\nGrounding Element grounding is the task of accurately identifying and linking a UI element on a web page, based on a natural language reference name. To do so, the grounder first needs to understand the UI. Semantic UI understanding or as more commonly known, page understanding (PU), plays a critical role in web-based agents, integrating text and image inputs to mimic human capabilities necessary for interacting with graphical user interfaces (Carroll 2003; Hegarty 2011; Turk 2014). Originally dominated by HTML-based parsing (Deng et al. 2022; Zhou et al. 2021), this field has evolved through the adoption of LLMs that enhance the parsing and comprehension of complex web UIs (G\u00fcr et al. 2023; Kim, Baldi, and McAleer 2024; Yin et al. 2023; Shi et al. 2017), despite their shortcomings with dynamic and extensive web applications (Mitchell 2018). Naturally, vision-based soon followed with significant contributions, providing the added benefit of better mimicking the human form of interacting with a web page. Vision-based grounding has evolved from elementary computer vision techniques to more complex models like R-CNN (Ren et al. 2015; Manandhar, Jin, and Collomosse 2021) and YOLO (Redmon et al. 2016; Singh, Fernandes, and Rashid 2021), which significantly improve the detection and classification of UI elements (Chen et al. 2020; Xie et al. 2020). The development of large vision language models (LVLMs) represents a major breakthrough, merging vision and linguistic AI to achieve deeper semantic understanding of UIs (Dosovitskiy et al. 2020; Ramesh et al. 2021; Radford et al. 2021). Recent research confirms the superiority of multi-modal LVLMs in specialized grounding tasks over generalist models (Zhang et al. 2023; Cheng et al. 2024; Lu, Kasner, and Reddy 2024). The integration of text and vision-based grounding with LLMs and LVLMs marks a significant advancement in web agent technology, enhancing the interface between humans and machines in digital environments. Following the page understanding, the grounder must identify the best match based on the reference element description. This task can be approached using syntactic or semantic matching techniques (Shlomov, Marreed, and Yaeli 2024), with some methods leveraging LLMs for improved accuracy (Cheng et al. 2024).\nPlanning Agent planning has transitioned significantly from its initial stage, where rule-based systems, though useful, were often rigid and limited to predefined tasks (Ferrucci et al. 2010). The advent of LLMs marked a pivotal shift, enabling more dynamic planning across diverse tasks (Karpukhin et al. 2020; Guu et al. 2020). A notable advancement was introduced by Nakano et al. (2021), who utilized LLMs to parse and respond to queries in a text-based web environment, laying the groundwork for more interactive web agents. Gur et al. (2024) further refined this approach by decomposing complex instructions into manageable sub-instructions, enhancing the granularity of task execution. The planning evolution has since expanded into two main directions: in-context planning, where agents adjust to tasks within a given context (He et al. 2024; Zhou et al. 2023), and fine-tuned approaches that tailor agent behaviors through specific training regimens (Hong et al. 2024), such as curriculum-based web trajectory learning (Lai et al. 2024). Despite these innovations, fine-tuned models are expensive to train and to collect high-quality data often struggle with scalability and generalization across the vast and varied web environment. Therefore, their practical implementation in real-life applications is still lacking."}, {"title": "3 Grounding", "content": "We modified the Mind2Web experimental setup to isolate the action planning from the element grounding. The agent is given a low-level, single-step task and a set of available elements, with the goal of finding (grounding) it by choosing the most suitable element. The Mind2Web dataset was originally planned for evaluating the fulfillment of high-level tasks (e.g., \"Find a 3-bedroom apartment to rent in New York\"), using multi-step flows. As such, it does not contain the step-wise instructions serving as the ground truth of each turn in the flow. To that end, we extended Mind2Web so that it is also suitable for evaluating the fulfillment of this low-level task. We used the implicit step-wise data in Mind2Web to extract the action type and the name of the ground truth interacted element for each step in each flow. This results in an augmented dataset containing low-level instructions for each step in Mind2Web.\nSetup The grounding task requires translating low-level instructions-comprising the action type, element name, and a list of available UI elements into a direct reference to the ground-truth selected element within the MHTML file. To achieve this, we refined the Mind2Web benchmark by creating a subset of 777 cleaned samples, from an initial random sampling of 1000 samples (216 task flows). This subset represents about 10% of the total test set and preserves the original characteristics of Mind2Web (cross-task: 20.8%, cross-web: 15.8%, and cross-domain: 63.2%). Cleaning the benchmark included removing those samples containing duplicate elements that could not be distinguished when using direct references and corrupted samples where the element name was empty or None. Our reasoning is that in those cases, even humans would not be able to distinctively locate the right elements. We note that some of the corrupted samples are due to the inherent process of the Mind2Web annotation, as the ground truth element name was collected automatically by the annotator demonstration on the web page. This refined benchmark enables rapid iteration across numerous experiments, minimizing the costs of commercial frontier LLMs, and reducing experimental time while maintaining the original characteristics of the Mind2Web benchmark.\nFor a comprehensive evaluation, we employed three types of PU techniques (CV, JS, DOM), two types of prompts (WebVoy and ours), and two core models (GPT-40 and Llama2). We also tested the original MindAct model on this setting. The accuracy metric is calculated according to the element accuracy in (Deng et al. 2024). The entire code base is available at [AnonymouSubmission.git.com], and the full list of samples is included in the supplementary material.\nWe create an element selection pipeline with a PU node, syntactic matching node, and semantic matching node. For the PU node, we compare three implementations: a) Vision-based OCR, referred hereafter as CV, b) Javascript (JS), using WebVoyager's approach (Chen et al. 2020), and c) our page understanding (PU) algorithm for extracting candidate elements. We refer to it hereafter as DOM PU, indicating that it utilizes the Document Object Model (DOM) analysis techniques. To ensure an accurate comparison, we use the raw HTML bounding box data collected during the original Mind2Web dataset creation, which reflects the elements as they appeared in the original screen view.\nOur DOM-based PU queries a web page using a predefined set of rules, primarily CSS selectors. These rules were hardened following a rigorous process of analysis and measurement, resulting in a high coverage of the interactable elements on the web page. We further enrich each of the elements with semantic information, extracted from the attributes of the queried element, and other DOM elements that are related to it in a spatial or hierarchical connection. This information helps to uniquely characterize each element and will be used by the next nodes in the pipeline. The set of rules can be found in Appendix B. We note that the set of rules can be easily extended to new applications, and can be even auto-generated by a simple LLM task (Shlomov, Marreed, and Yaeli 2024). The next pipeline step ranks the extracted candidates using common syntactic matching algorithms (Levenshtein et al. 1966). We match the step-level instruction with the semantic attributes of the candidate elements. Following the syntactic matching, we invoke semantic ranking using a sentence transformer (all-mpnet-base-v2). We sort the elements based on the similarity of their embedding against the step-level instruction. Finally, a (v)LLM uses the entire information to select the appropriate element ID for invocation. Aligned with findings in (Chen et al. 2020), we also implemented a pipeline primarily based on CV techniques, combined with an optical character recognition (OCR) model (Rotman et al. 2022). This pipeline ingests a capture of a GUI screen (from the Mind2Web subset) and a text used as the reference expression (e.g., \u201cClick Submit\" or \"Type First Name\"). We match the extracted text and assign each element a score (Levenshtein et al. 1966). We further utilize a secondary match that considers neighboring elements, based on the assumption that semantic context may stem from nearby elements. Finally, a target click position (x, y coordinate) is generated as output."}, {"title": "4 Planning", "content": "Going back to the original settings of Mind2Web, the high-level objective setting is the standard setting of web agents, where a natural language description of the intent is given (e.g., \"Order a United flight from NY to SF on Aug 8\") and the agent should act one step at a time and perform the actions required to accomplish the objective. Typically, each step's input includes the current web page, the past action commands, and the main objective. The output is the type of UI action and a reference (element id) to the element that the agents would like to interact with to accomplish its goal. For analyzing action planning, we use only the subset of 703 successfully grounded samples as a baseline to evaluate planning decisions independently, minimizing grounding-related challenges effect. We tested four models (MindAct, SeeAct, WebVoy, and ours - WebNaviX) with three PU types (JS, DeBERTa, and ours). To ensure a fair comparison and validate the efficiency of the split, we employed two mechanisms: comparing the weighted average results of SOTA models on the original Mind2Web split and projecting their performance onto our split (cross-task: 31.1%, cross-web: 11.52%, and cross-domain: 57.3%)."}, {"title": "MindAct", "content": "To ensure consistency, we ground the results on our benchmark dataset split by applying SOTA web navigation methods as a baseline. The first method we use is MindAct (Deng et al. 2024), which implements a two-stage, text-only web navigation model consisting of candidate generation and action prediction stages. In the first stage, MindAct employs a ranker to select the top 50 elements. Subsequently, the action generation problem is framed as a multiple-choice question-answering task, with the candidate elements serving as options, including a \"None\" option if the target element is absent. We run the provided code, initially producing ranking results using their DeBERTa model (trained on the entire Mind2Web training dataset). And later, we run either their fine-tuned Flan-T5XL model weights or an in-context learning GPT-40 model for action prediction."}, {"title": "SeeAct", "content": "We include results from SeeAct (Zheng et al. 2024) to provide context for our large vision-language approach. However, since the provided SeeAct code is only suitable for online evaluation, we project the performance on the offline dataset by calculating a weighted average of the respective results on cross-task, cross-website, and cross-domain using their relative presence on the new split."}, {"title": "Web Voyager", "content": "We used the same method as described in Section 3. However, as opposed to the grounding experiment, in this planning setup, we prompt the LLM with the high-level text describing the task."}, {"title": "WebNaviX", "content": "Our agent's first step is applying the DOM-Based PU as described in the grounding task (Section 3). Thereafter, the results of the DOM-PU are ranked using semantic similarity (as discussed in Section 3). The resulting elements are then transformed into a SoM list of candidates and embedded (like in WebVoyager) in the prompt of the LVLM together with the history of past actions, and the high-level task description. The LVLM planner returns the predicted action: Type or click, and the predicted element ID. If the action is type, the planner also provides the required value for typing in."}, {"title": "Results", "content": "Table 2 presents the results of action planning using various approaches. We focus solely on element accuracy, omitting the Operation-F1 and Step Success Rate (Step-SR) metrics used in (Deng et al. 2024). Our analysis reveals that selecting the correct operation is relatively straightforward and can often be inferred from the type of the selected element. Therefore, we consider the element accuracy as the most significant metric that directly correlates with the Step-SR.\nTo ensure the validity of our experiments, we compare the performance of SOTA models on our benchmark split with their original results. On our split, MindAct FlanT5XL projects an accuracy of 46.53%, calculated as a weighted average across cross-task (31.1%), cross-web (11.52%), and cross-domain (57.3%) subsets. The actual result on our split was 46.75%. MindAct GPT4 projects an accuracy of 36.45%, where the actual result on our split is 36.27%, showing insignificant differences from the projection.\nAs shown in Table 2, the current SOTA MindAct FlanT5 model demonstrates strong performance on our Mind2Web split. This is primarily due to our split containing more cross-task content, which plays to the strengths of fine-tuned models. However, fine-tuned models show reduced effectiveness in cross-website and cross-domain evaluations. Furthermore, the process of fine-tuning models for specific tasks, websites, or domains requires large training data and can be resource-intensive, potentially limiting their practical applicability and generalizability.\nIn contrast, our analysis of in-context learning models reveals that for text-based methods (Vision=N), our agent WebNaviX, utilizing the DOM-based PU technique with the ranking mechanism and the WebVoyager's prompt, yields the best result (40.68%). Notably, it outperforms the JS technique from WebVoy, demonstrating the efficacy of our PU method. For vision-based models, our agent WebNaviX achieves significant (Wilcoxon Signed-Rank test (Dror et al. 2018, 2020), p < 0.05) improvement over existing state-of-the-art methods without relying on data-specific fine-tuning."}, {"title": "Planning is the main bottleneck", "content": "A critical challenge faced by LLMs in web-based agents is their ability to effectively plan actions based on a list of relevant UI elements. The planner component, which is responsible for selecting the next element to interact with, frequently struggles to make accurate decisions, even when provided with a narrowed list of options. To further investigate this issue, we conduct a series of experiments where the ground truth element is explicitly injected into the list of choices presented to the planner (Oracle Ranker). We tested the planner's performance by varying the number of relevant UI elements, focusing on its ability to correctly identify the ground truth when it is guaranteed to be included in its list of options. This manipulation helps isolate the impact of the planning algorithm from grounding or element ranking challenges. The result, as can be seen in the Oracle Ranker column on Table 4, reveals a surprising and concerning trend. Even when the planner is tasked with choosing between just two elements, one of which is the ground truth, it only achieves an accuracy of 86%. Given the narrow decision space, we consider this to be a low rate of accuracy. It serves as a testament to the fact that the planner struggles to make the right choice, even under a simplified setup, let alone under more complex conditions. These findings suggest that the current planning mechanisms within LLMs are insufficient for achieving the high accuracy required in complex web environments. Contrary to the paper (Zheng et al. 2024) titled \u201cGPT-4V(ision) is a Generalist Web Agent, if Grounded\", our results show that currently LVLMs cannot act as web agents, even if perfectly grounded. We argue that additional external knowledge may be necessary to enhance the planner's capabilities. Without such improvements, the planning phase remains a substantial bottleneck, limiting the overall effectiveness of web-based agents. An example that shows the difficulty of the planner can be found in Appendix G.\""}, {"title": "Improving Ranking Heuristics", "content": "To enhance the candidate generation process, we explored additional ranking heuristics beyond semantic similarity. We observed that most ground truth candidates in the Mind2Web training dataset are short, with 92% containing less than 6 words. To counter this bias, we divided the candidates into three groups: up to 3 words, 4-6 words, and 7+ words. Elements were then sampled from each group in descending order of semantic similarity, while enforcing a sampling distribution corresponding to the natural distribution of text lengths. This approach improved the recall of ground truth elements (see the +Length row in Table 3).\nLocation-based Heuristic We observed that the ground truth elements tend to cluster towards the top and left sides of pages (Figure 4). To address this spatial bias, we split candidates into two groups based on their Y-axis positions: Up to 700 pixels and above 700 pixels. We then implemented an over-sampling strategy for elements from the 0-700 px group with a 0.92 sampling ratio. Combining these length and location strategies yielded further improvements in recall rates (see the +Length+Location row in Table 3)."}, {"title": "5 Discussion", "content": "We isolate planning and grounding components by conducting controlled experiments on a new refined Mind2Web benchmark. Our results indicate that the primary bottleneck is the model's planning capabilities. We believe that simply using a \"smarter\" model, more computational power, or a more advanced frontier model, or even improving grounding to perfection, will have a limited impact on accuracy in real-world web agents. Instead, we hypothesize that the main gap lies in external knowledge. Incorporating additional knowledge and context into the planner, particularly in business applications, could bridge the gap between abstract task descriptions and concrete, step-by-step actions. Building on this premise, we experimented with a simple modification to our algorithm: a single call to an LLM at the outset of each workflow. This LLM is tasked with generating a high-level pre-plan for task completion, constrained to a concise paragraph (see Appendix H for more details). This pre-plan is then appended to the prompt of each step in the flow when the LLM is called upon to select the optimal candidate. Using this simple method, the performance improved marginally (51.7%). This improvement, although not significant, suggests the potential benefits of embedding additional context in planning decisions and hints at the value of a broader strategic overview guiding action sequences. Further research is needed to fully understand the implications and reliability of this approach.\nThreats to Validity We selected Mind2Web for our experiments because it is the first widely adopted large-scale dataset derived from real websites, providing a diverse and representative set of tasks for real-world web interactions. However, it is important to acknowledge our limitations. We did not perform an exhaustive hyperparameter optimization. However, this aligns with our research objectives, which prioritize assessing fundamental planning and reasoning capabilities over achieving peak performance. Although we were able to beat the SOTA models this is not the main goal of the paper. We selected and rigorously tested SOTA models for their reasoning abilities, providing a strong baseline and realistic results. We argue that if these frontier models show low planning performance, it is likely that other models would struggle even more. In addition, while fine-tuned SOTA models may achieve performance close to our technique, they require training data and struggle with cross-domain tasks. Although these methods can boost performance in specific contexts, their limitations highlight the need for more generalized approaches that can adapt across diverse domains.\nOur study is based on offline experiments, which may not fully capture the dynamic nature of live web environments. This discrepancy could lead to potential differences between our experimental results and real-world performance. To mitigate this limitation, we carefully adapted algorithms to function effectively in an offline setting, striving to maintain the integrity of the original methods. However, we acknowledge that these adaptations may not entirely replicate the complexities of live environments. Additionally, we chose not to test against environments like WebArena, as they conflate planning and grounding tasks. While such datasets are valuable for overall agent assessment, our focus on isolating grounding from planning was critical for pinpointing the root causes of performance deficits in web interaction tasks.\nOur study also uses only a subset of the Mind2Web dataset potentially limiting the generalizability of our findings. To mitigate this, we carefully selected a random subset, and despite removing duplicates and corrupt samples, we maintained the original distribution characteristics of Mind2Web. This approach ensures that our results remain representative of the full dataset, as evidenced by the consistency of results across multiple algorithms and models. Moreover, we projected the results of the baseline algorithms on our subset to accumulate fair comparison. We also focused on common web interactions (e.g., Click and Type commands) and did not explore tasks like copy-pasting or data extraction, leaving these for future research."}, {"title": "B Element Grounding Using DOM Parsing", "content": "To facilitate robust web element grounding, we utilized a set of CSS Selector rules, outlined in Figure 6. These rules target specific DOM elements, enhancing the accuracy of element identification during DOM-based parsing operations. Each rule defines a unique CSS Selector, ensuring high coverage across varied web pages and applications. Below we describe key rules employed in our experiments:\n\u2022 Form Elements: Target forms with complex layouts or modal dialogs, colored in red for critical interaction points (form, .records--layout-section, div[role=``dialog'']).\n\u2022 Table Rows: Blue-coded selectors (tbody tr) to parse data tables efficiently.\n\u2022 Navigation Tabs: Aqua-colored selectors for navigation elements (nav[role=``tablist'']).\n\u2022 Input Fields: Various inputs including text, search, and textarea are highlighted with maroon, crucial for form interactions (input [type='\u2018text''], input[type=\u2018'search''], input[type='\u2018textarea'']).\n\u2022 Selection Controls: Checkbox and radio inputs are distinctly styled in olive and purple, respectively, for clear visibility and interaction (input [type='checkbox'], input[type='radio']).\n\u2022 Links and Buttons: Non-advertorial links and actionable buttons are tagged in teal and green, promoting straightforward navigation and actions (a:not(:has(img)),button). These rules ensure comprehensive coverage and precise targeting within a DOM, facilitating the subsequent syntactic and semantic matching processes crucial for effective element grounding."}, {"title": "C Hyperparameters", "content": "Setting of Hyperparameters\nWe employed a variety of models with specific hyperparameters optimized for each task. For the GPT-40 model on the OpenAI API, parameters were set as follows: max_tokens to 128K, temperature and top-p both at 1, with frequency-penalty and presence_penalty at 0. We used a batch size of 1, and configured the API calls to allow up to 10 retries with a timeout of 60 seconds.\nDOM Grounding and Planning\nFor the DOM Grounding (Dom-PU) task, we used the llama-2-70b model, specifying a greedy decoding method, temperature of 0.1, and a maximum of 20 new tokens. DOM Planning utilized meta-llama/llama-3-70b-instruct with a token range of 1 to 200. Both settings employed policies with a threshold of 0.75, reflecting input and output considerations.\nHyperparameter selection was based on the criterion of maximizing performance results. Despite the high cost of experimentation, which restricted extensive parameter tuning, there is a wide range and number of values tested."}, {"title": "D MIND2WEB Dataset", "content": "MIND2WEB (https://osu-nlp-group.github.io/Mind2Web/dataset (Deng et al. 2024)) serves as a benchmark for the development of generalist agents that interpret and execute language-driven instructions across a spectrum of real-world websites. It composed of 2,350 open-ended tasks sourced from 137 websites across 31 diverse domains such as travel, shopping, and services. It stands out by employing actual web environments rather than simulations, capturing the complexity of modern interfaces and user interactions. Tasks range from simple queries to complex sequences that require navigation, transaction, or data analysis, providing a rich tapestry of user interactions. Each task is accompanied by natural language descriptions, annotated sequences of actions, and comprehensive web page snapshots\u2014HTML, DOM trees, screenshots, and network traffic\u2014documenting each step. The generalization capability of agents is tested through an evaluation framework that includes cross-task, cross-website, and cross-domain challenges. This framework assesses agents' ability to adapt to unseen tasks, new websites within familiar domains, and completely novel domains."}, {"title": "E Benchmark Data Set", "content": "We discuss our benchmark data curation in section F, The list of data samples is included in our supplementary code. After extracting the code, you'll find a single CSV file in the 'data' folder, listing all 1,000 selected samples, including those with duplicate elements where one is the ground truth. In the 'supplement' folder, you'll find two result dataframes: 'grounding_failures_no_duplicate.csv,' which includes failed samples without duplicates, and 'grounding_no_duplicates.csv,' which contains all 777 samples without duplicates."}, {"title": "F Benchmark Data Curation", "content": "We analyzed 8,628 samples from the Mind2Web Test dataset, out of which our DOM SUIU failed to execute on 487 test cases, representing approximately 5.64% of the dataset. Root cause analysis revealed that these failures were due to corrupt MHTML files and missing \"node-buckeye-id\" annotations with corresponding \u201caction_uid\" annotations. We focused only on positive candidates, excluding negative ones, to validate that our SUIU algorithm correctly assigns bounding box values consistent with the calculations performed during the Mind2Web dataset creation. This mapping of our SUIU candidate elements to Mind2Web's identified positive elements is a crucial step in establishing the SUIU ground truth for our end-to-end experiment. To ensure accuracy, we computed the bounding box Jaccard Index along with additional features to confidently identify a SUIU element as a ground truth positive candidate. In the course of this analysis, we further excluded 1,779 test cases: 956 samples were dropped because their comparison features fell below the required threshold, with further analysis indicating that these failures were due to significant horizontal or vertical skew. This discrepancy appears to stem from differences in rendering between our desktop environment and the MHTML sampling done during Mind2Web dataset generation. Additionally, 823 samples were excluded because the ground truth positive candidate specified by Mind2Web was not present among the SUIU-extracted candidate elements. Ultimately, we were left with 6,362 valid test cases for our experiment, where each set of candidate elements from our SUIU included the target ground truth element marked by a dedicated attribute. From these verified test cases, we sampled a 15% subset, resulting in 995 meaningful samples that maintained a distribution comparable across all test splits. During our grounding evaluation, we identified 218 samples-approximately 21% of the total\u2014that highlight a significant challenge in planning tasks: the presence of duplicates. A sample is considered a duplicate if the ground truth element has one or more identical additional elements that can cause current solutions to fail without any additional external knowledge. In Figure 8, for example, the application contains duplicate clickable elements-a button and a link, both labeled \"Jobs.\" This duplication poses a challenge for both grounding and planning, as selecting between the two becomes a matter of chance, with success rates depending on a coin flip. We plan to address this challenge in future work. It's worth noting that our analysis also revealed that the Mind2Web Test and Train splits each contain a similar percentage of duplicates, both around 21%."}, {"title": "G Grounding Error Analysis", "content": "We demonstrated that out of 777 samples", "groups": 35}]}