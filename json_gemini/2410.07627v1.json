{"title": "AUTOMATIC CURRICULUM EXPERT ITERATION FOR RELIABLE LLM REASONING", "authors": ["Zirui Zhao", "Hanze Dong", "Amrita Saha", "Caiming Xiong", "Doyen Sahoo"], "abstract": "Hallucinations (i.e., generating plausible but inaccurate content) and laziness (i.e. excessive refusals or defaulting to \"I don't know\") persist as major challenges in LLM reasoning. Current efforts to reduce hallucinations primarily focus on factual errors in knowledge-grounded tasks, often neglecting hallucinations related to faulty reasoning. Meanwhile, some approaches render LLMs overly conservative, limiting their problem-solving capabilities. To mitigate hallucination and laziness in reasoning tasks, we propose Automatic Curriculum Expert Iteration (AUTO-CEI) to enhance LLM reasoning and align responses to the model's capabilities-assertively answering within its limits and declining when tasks exceed them. In our method, Expert Iteration explores the reasoning trajectories near the LLM policy, guiding incorrect paths back on track to reduce compounding errors and improve robustness; it also promotes appropriate \"I don't know\" responses after sufficient reasoning attempts. The curriculum automatically adjusts rewards, incentivizing extended reasoning before acknowledging incapability, thereby pushing the limits of LLM reasoning and aligning its behaviour with these limits. We compare AUTO-CEI with various SOTA baselines across logical reasoning, mathematics, and planning tasks, where AUTO-CEI achieves superior alignment by effectively balancing assertiveness and conservativeness.", "sections": [{"title": "1 INTRODUCTION", "content": "Hallucination is a long-standing issue in large language model (LLM) research, which refers to the phenomenon where LLM-generated content appears plausible but is actually nonsensical or inaccurate, often misleading humans by seeming deceptively credible (Ji et al., 2023). It is more evident when solving complex reasoning problems beyond their capability, in which LLM tends to fake evidence or logic to answer the questions assertively. The reason for LLM's hallucinations, overall, is a misalignment of its real capability and its behaviours: LLM should not behave overly assertively or confidently when facing unfamiliar (Kang et al., 2024a) and difficult problems (Dziri et al., 2024). Instead, a preferred behaviour of LLMs is to acknowledge their limitations by responding with \"I don't know.\" However, we must also avoid the LLM from defaulting to degenerate responses like \u201cI don't know,\" especially when the question is within its capability. A reliable LLM should strike a balance between maximizing performance and avoiding hallucination.\nVarious methods have been proposed to mitigate LLM hallucinations. However, most studies have focused on factual hallucinations, i.e., fabricating non-existent evidence, while often neglecting reasoning hallucinations (Creswell et al., 2022). Reasoning tasks require deriving conclusions from evidence using rules, often involving multiple steps. Reasoning hallucination refers to the phenomenon where LLMs apply invalid rules or misinterpret the conclusion, leading to an incorrect final result even without factual hallucination in the evidence. Dziri et al. (2024) found that LLMs exhibit a probability of error at each reasoning step. As these errors compound, the likelihood of incorrect final answers increases exponentially as the reasoning chain grows longer. In addition, most techniques mitigate factual hallucinations using external knowledge or conduct post-training to do \"patching\". These techniques can reasonably mitigate factual errors, but hallucinations caused by faulty reasoning are far more challenging due to the inevitable compounding error of the transformer. As such, most of these techniques cannot improve LLM's inherent reasoning capability, and simply using patching techniques alone can even make LLMs behave lazily.\nCan LLM learn short reasoning or self-correction to reduce compounding errors and reasoning hallucinations? Kang et al. (2024b) showed that learning the optimal shortest solution for difficult problems (e.g., NP-complete problems) has very high sample complexity. Thus, directly learning short solutions for all complex problems is unrealistic. Havrilla et al. (2024) showed that reinforcement learning (RL) can help improve LLM's performance in various reasoning tasks. RL methods, such as Expert Iteration (Anthony et al., 2017) and PPO (Schulman et al., 2017), explore various LLM reasoning trajectories (i.e., Chain of Thought (Wei et al., 2022)) and use reward function to guide the incorrect trajectories back on track, therefore improving the robustness and reducing the effect of compounding errors. However, compounding errors are inherently impossible to eliminate. Even though RL reduces step-wise errors, the probability of error still increases exponentially with the length of reasoning. As a result, it is necessary for LLMs to acknowledge their limitations, particularly when faced with difficult problems beyond their capability. This raises a key question:\nHow can we assess the limits of LLMs' reasoning abilities and adjust their re-sponse behaviour appropriately to align with these limits?\nTo address this, we propose Automatic Curriculum Expert Iteration (AUTO-CEI), which simultaneously enhances LLM reasoning and aligns its behaviour to ensure precise answers while acknowledging its limitations. AUTO-CEI assumes that the number of reasoning steps required to reach a correct answer provides a reasonable estimate of both the problem difficulty and the limits of LLM reasoning. This assumption is grounded in computational theory, as each reasoning problem has its underlying computational complexity, and each reasoning step corresponds to an elementary computing operation. Learning the reasoning steps (precondition/effect of reasoning rules) has similarly low sample complexity (Kang et al., 2024b). Despite the existence of concise optimal solutions, difficult problems (e.g., NP-complete problems) require extended reasoning to find those solutions. This assumption helps align LLM's behaviours: easier problems require fewer reasoning steps and are less prone to compounding errors, justifying greater assertiveness from LLMs. Complex problems needing more steps and suffering more compounding errors require more conservativeness.\nThe key idea of AUTO-CEI is shown in Figure 1: using the length of reasoning steps to measure difficulty, AUTO-CEI designs an automatic curriculum for expert iteration that rewards correct reasoning, compensates for \u201cI don't know\" acknowledgements after a sufficient number of attempts, and penalizes both overly conservative and assertive wrong responses. The expert iteration adjusts the LLM's responses by resampling based on these rewards. The curriculum automatically updates the compensation reward to encourage more reasoning attempts before saying \"I don't know\" over time. It gradually adjusts the reward function to optimise an objective function, balancing the overall precision and the proportion of saying \"I don't know\" to control hallucination and avoid laziness.\nAs such, it gradually pushes the limits to maximise the potential of LLM reasoning and aligns its behaviours with these limits. AUTO-CEI is effective across various reasoning tasks such as logical, mathematical, and planning problems, balancing reliability and conservativeness.\nContributions. In summary, AUTO-CEI automatically estimates the boundary of its reasoning capacity, thus achieving a reasonable alignment to maximise capacity and control behaviours. It learns to reliably solve the problems within its boundary as much as possible; it also knows to acknowledge \"I don't know\" when the problem is beyond its limit. We carried out comprehensive experiments in various reasoning benchmarks, and AUTO-CEI significantly outperforms the concurrent baseline methods, boosting precision by 10-24% while maintaining a relatively low refusal rate of 18-36% across diverse reasoning tasks in planning, logical and math reasoning."}, {"title": "2 RELATED WORK", "content": "Hallucinations and laziness of LLM. It is widely acknowledged that LLMs often produce hallucinated responses, including fabricated facts and misleading logic (Radhakrishnan et al., 2023; Ji et al., 2023). Most works focus on reducing hallucinations by ensuring the factual accuracy of generated content, often using retrieval-based methods that enhance LLMs with external knowledge (Shuster et al., 2021). This methodology has been effective in improving the reliability of LLMs in domains that require high factual accuracy, such as knowledge-based question answering (Lewis et al., 2020). Particularly, Retrieval-Augmented Generation (RAG) reduces hallucinations by fetching relevant documents from external sources during inference, integrating factual information to enhance accuracy and relevance (Izacard et al., 2023). These methods ensure that the LLMs' responses align more closely with verified data (Komeili et al., 2022; Gururangan et al., 2021). Moreover, several studies have explored hybrid approaches that combine retrieval with fact-checking modules or domain-specific fine-tuning to improve factual accuracy (Lee et al., 2022). Retrieval-based strategies show strong potential in mitigating hallucinations, outperforming purely generative models that rely only on knowledge from training (Borgeaud et al., 2022; Petroni et al., 2020). However, hallucinations are not limited to factual inaccuracies; they can also extend to faulty or incomplete reasoning, a significant concern for multi-step reasoning-based tasks (Creswell et al., 2022). Moreover, LLMs often exhibit what can be described as \u201claziness,\u201d which refers to the tendency of the model to reject or avoid generating correct but complex answers in favour of simpler, superficial, or incomplete responses (Bubeck et al., 2023; Bang et al., 2023). This phenomenon has been noted in tasks requiring step-by-step logical reasoning, where LLMs tend to skip intermediate steps or provide too general answers, rather than addressing the nuanced complexity of the problem at hand (Rae et al., 2021).\nReinforcement learning for LLM reasoning. Reinforcement learning has been widely used in the alignment stage of LLM training to enhance their capabilities. The research in this area can be broadly divided into two aspects: (1) RL algorithms; (2) reward construction. Conventional advanced deep RL algorithms, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), have been extensively applied in the training process of modern LLMs, helping to improve both format and reasoning abilities (Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024). However, due to the introduction of the value network, the computational overhead of these algorithms is extremely high, often making them unaffordable for academic settings where resources are limited. More recently, expert iteration (also known as rejection sampling or reward-ranked fine-tuning) (Dong et al., 2023; Gulcehre et al., 2023), preference-based RL (Rafailov et al., 2024; Xiong et al., 2024a), and REINFORCE-type RL (Williams, 1992; Shao et al., 2024; Li et al., 2023; Ahmadian et al., 2024) have emerged as more efficient alternatives for the alignment stage, especially when operating within a fixed budget. These methods have demonstrated significant improvements over supervised fine-tuning in reasoning tasks (Aksitov et al., 2023; Gou et al., 2023; Havrilla et al., 2024; Shao et al., 2024; Dong et al., 2024; Trung et al., 2024; Xiong et al., 2024b). Additionally, Lightman et al. (2023) introduced process supervision to refine the reward signal for each reasoning step. Compared to traditional outcome-based supervision, process supervision has been shown to yield substantial gains (Lightman et al., 2023; Wang et al., 2024; Shao et al., 2024). Despite the impressive progress in this area, most existing works focus on maximizing the upper bound of reasoning ability. In contrast, our algorithm aims to develop a more balanced model incorporating self-awareness into the reasoning procedure."}, {"title": "3 PRELIMINARIES", "content": "We address reasoning problems in general, such as logical reasoning, mathematical reasoning, and planning. Reasoning involves deriving unknown facts from provided evidence and reasoning rules. In logical reasoning, evidence is represented as clauses with assigned boolean values, and applying reasoning rules determines the boolean values of unknown clauses. In arithmetic reasoning, evidence consists of clauses linked to scalar values, and reasoning requires both logical deduction and arithmetic operations. In planning problems, the goal is to identify a sequence of actions that move an initial state to a target state by applying actions based on their preconditions and effects.\nUnlike methods that convert problems into Boolean satisfiability (SAT) and solve them with SAT solvers, the LLM reasoning approach, such as Chain of Thought (CoT) (Wei et al., 2022), follows a sequential decision-making strategy. To capture reasoning problems, we rely on planning concepts that consist of four main elements: state space $S$, action space $A$, transition function $T$, and goal function $G$. The state space $S$ includes all possible configurations, such as the boolean or scalar values assigned to the given variables or clauses, as well as unknown clauses. Actions, defined in the action space $A$, represent rules or equations that can be applied to these states. The action has its preconditions and effects. A state should satisfy the corresponding precondition to execute an action, and then the effects will be applied to the state. The precondition and effect are encoded in the transition function. When an action $A$ is executed in a state $S$, the transition function $T(s, a)$ determines the resulting state $s'$, e.g., new values would be assigned to (unknown) clauses or variables. The goal function $G$ checks whether the current state matches the desired outcome."}, {"title": "3.2 BACKGROUNDS", "content": "Expert Iteration. Expert iteration (EI) (Anthony et al., 2017) is an iterative process in which experts are built upon a base policy, the base policy is refined by learning from the expert, and this cycle repeats with newly derived base policies. El is effective in improving the quality of generated responses in LLM literature (Dong et al., 2023; Gulcehre et al., 2023). Havrilla et al. (2024) discussed the usage of EI in LLM reasoning problems. They used LLM sampling to generate responses, and preferred responses were selected by a ground-truth or reward model to build the expert. They then used LLM to imitate the expert via SFT. EI explores the neighbouring area of LLM policy in the token space, which helps LLM derive back to the correct reasoning trajectories if there are errors or find suboptimal trajectories. As a result, in their experiment, EI improved the LLM reasoning over time, which performed similarly or even better than the PPO (Schulman et al., 2017).\nCurriculum Reinforcement Learning. Curriculum Reinforcement Learning (Narvekar et al., 2020) focuses on sequencing tasks or data samples into a structured curriculum to solve complex problems that are difficult to learn from scratch. A curriculum can be considered an ordering over experience samples at the most basic level. The underlying assumption is that learning from elementary samples or tasks can be transferred and help learn more complex tasks. In this paper, we design the curriculum by reshaping the reward to gradually guide the LLM in conducting more reasoning attempts to solve the problem before acknowledging its limits. Its attempts could be used to discover suboptimal trajectories that mitigate its compounding errors in easy problems and also transferred to increase the chances of solving more complex problems."}, {"title": "4 AUTOMATIC CURRICULUM EXPERT ITERATION", "content": "AUTO-CEI aims to estimate and push the limits of LLM reasoning concerning the number of reasoning steps, maximising its capacity while aligning its behaviours accordingly. We assume that, for a broad class of reasoning problems, the number of reasoning steps (in the chain of thoughts) required to reach the correct answer estimates the problem's difficulty and the limits of LLM reasoning. We justify the assumption by computational theory. The reasoning problem has its underlying computational complexity, where each of the reasoning steps can be treated as an elementary computing operation. Learning to apply the preconditions and effects of those elementary reasoning actions/rules has low sample complexity (Kang et al., 2024b). Although problems may have concise optimal solutions, given the inherent computational complexity (e.g., NP-complete), finding that solution still needs long reasoning attempts. AUTO-CEI estimates the reasoning limits in terms of the number of reasoning steps until the LLM can respond reliably, solving a maximum number of problems. But beyond this point, further continuing the same assertive behaviour of the LLM policy leads to a significant increase in mistakes, whereas being overly conservative within this range limits reasoning capacities due to the high refusal rate. Thus, it estimates the LLM's capability's boundary when fine-tuned for multi-step reasoning tasks.\nAUTO-CEI uses the average number of the reasoning steps of the initial (after supervised finetuning, SFT) policy to determine the curriculum, i.e., the reward function. It starts from optimising an initialised policy that is reasonably conservative with a certain distribution to say, \"I don't know.\" It then runs into the Expert Iteration process, in which the LLM policy will sample many reasoning trajectories and receive various rewards. We build up the expert by resampling the trajectories according to the reward and removing the assertively wrong responses. After convergence, AUTO-CEI updates the reward function according to the performance, in a way to encourage the LLM to produce more reasoning steps before saying \u201cI don't know.\" The curriculum eventually tries to optimise an objective function, which balances assertiveness and conservativeness and stops the curriculum if the objective function reaches the optimal value."}, {"title": "4.1 INITIALISATION", "content": "We adopt R-Tuning (Zhang et al., 2023), one of our baseline methods, as the initialization strategy. R-Tuning produces a good starting point with a reasonable proportion of refusal behaviours. It also makes LLM lazier and has a lot of room for optimisation. We first use SFT to train a language model given the training dataset $D$ consisting of $n$ question-answer pairs. Each answer uses the chain of thought. After SFT, we will let the language model answer the questions in the same dataset $D$ using random sampling. As we use random sampling, there will be a certain number of correct answers, and the others will be wrong. Thus, we split the questions and the new answers generated by LLM into two datasets, $D_1$ and $D_2$, where $D_1$ collects correct answers and $D_2$ consists of wrong answers. For $D_2$, in each of the wrong answers, we add an expression at the end to acknowledge the limitations and abstain from answering the questions, such as \"Sorry, I am not sure if I answer the question correctly. There might be mistakes in my answer.\" Thus, we have collected a new Refusal-Aware dataset $D_2$ for our finetuning.\nAfter collecting $D_1$ and $D_2$, we concatenate both datasets to form $D_{init}$ and use SFT to fine-tune the LLM again. We also examined the SFT result to ensure it had enough sample points for assertive and refusal answers. We set a threshold of 25% for the distribution of refusal behaviours in the validation set, as we need to have enough variety of acknowledge \"I don't know\" for AUTO-CEI. If the model has a very low distribution of acknowledge \u201cI don't know\", we then repeat the previous process of collecting $D_1$ and $D_2$ again, where the refusal answers will also be collected in $D_1$ and use the new dataset for a new SFT training. We observed that, empirically, the distribution of refusal answers would increase over the steps. Thus, after the initialisation, we will have a good initial policy with enough base knowledge to answer questions and a distribution of acknowledging \u201cI don't know.\"\""}, {"title": "4.2 EXPERT ITERATION", "content": "Given the initial policy, we run the Expert Iteration to improve the policy over time.\nWe build the expert using the reward function and resampling. The reward function $R(x, y)$ takes the question $x$ and an answer $y$ as input and outputs a scalar. $R(x, y)$ is designed to give a large positive reward for correct answers, a small compensatory reward for refusal answers after long reasoning trajectories, a small negative penalty for refusal answers after very short reasoning trajectories, and a large negative penalty for assertive wrong answers."}, {"title": "4.3 CURRICULUM UPDATE", "content": "After EXPERTITER converges, the curriculum updates the reward function to optimise (i.e. maximise) an objective function:\n$f = (1 - \\lambda)P_{pre} + (1 - P_{IDK}),$ (2)\nwhere $P_{pre}$ denotes the precision, i.e., the correctness of answers that are not refusal answers, and $P_{IDK}$ denotes the proportion of refusal answers. This function $f$ tries to find a good balance between avoiding both hallucination (low $P_{pre}$ and low $P_{IDK}$) and laziness (high $P_{pre}$ but high $P_{IDK}$) when finetuning the LLM. $\\lambda \\in [0, 1]$ is a hyperparameter to control the tradeoff between hallucination and laziness; a higher value of $\\lambda$ will lead to more assertive behaviour (i.e. lower IDK rate) while smaller $\\lambda$ would make the LLM policy behave more conservatively. We empirically show that setting $\\lambda$ is fairly intuitive; in most task scenarios, setting $\\lambda$ to a reasonably small value ($\\approx 0.2$) achieves the desired effect for the LLM policy. Over the EI iterations, the curriculum will update the value of $c_1$ in Equation 1, i.e., try to push $c_1$ to encourage more exploration before saying \"I don't know\". Thus, with the new reward function, we repeat the Expert Iterations and make it converge again.\nAssuming that the function of $f$ does not have local optimal point wrt $c_1$, the curriculum uses local search (hill climbing) (Russell & Norvig, 2016) to search $c_1$ and conduct Expert Iterations until it finds the highest $f$. The hill-climbing algorithm searches its neighbouring value of $c_1 \\pm d$ via a step size $d$, and if $f$ at the new step is higher, then it updates $c_1$ to the new value. It will stop the search if its neighbouring $c_1$ has a smaller value than its current $f$. The domain of $c_1$ is $[\\mu \u2013 2\\sigma, \\mu + 2\\sigma]$ where $\\mu$ denotes the average length of the reasoning steps produced by the initial LLM policy. $d$ is defined by $\\min{0.5, 4\\sigma/10}$, where $\\sigma$ denotes the standard deviation of the reasoning length produced by the initial LLM policy. We also do not want to make the step too large, thus capping it by a threshold value of 0.5, an empirically selected value.\nOverall, the curriculum update, together with initialisation and Expert Iteration of AUTO-CEI, are in the pseudo-code listed in Algorithm 3."}, {"title": "5 EXPERIMENT", "content": "To demonstrate the effectiveness of AUTO-CEI in reasoning tasks, we select BoardgameQA (Kazemi et al., 2024), MATH (Hendrycks et al., 2021), and Blocksworld (Valmeekam et al., 2023) as benchmarks, spanning from logical and mathematical reasoning to planning. They have various domains and complexities. We briefly introduce the benchmarks and report our detailed experimental settings in the Appendix B."}, {"title": "5.3 ABLATION STUDY & DISCUSSION", "content": "We conduct an ablation study to verify the effectiveness of our solution design choices and further discuss our underlying assumptions. As the effect of Expert Iterations has been demonstrated in the main result, in this section, we mainly focus on the effect of curriculums and the practical usage of hyper-parameter $\\lambda$. The result of the ablation study is shown in Table 2."}, {"title": "6 CONCLUSION", "content": "This paper focused on mitigating LLM's hallucinations and laziness in reasoning tasks. We propose Automatic Curriculum Expert Iteration (AUTO-CEI) to push the limits of LLM's reasoning capacities and align its assertive and conservative behaviours according to these limits for reliable reasoning. We conduct experiments on various challenging reasoning benchmarks, including logical reasoning, mathematics, and planning. Our result suggests that AUTO-CEI maximizes its reasoning ability while achieving a superior alignment between response behaviours and this ability. The fundamental assumption under AUTO-CEI is that solving difficult reasoning problems requires more reasoning steps, thus making the number of reasoning steps a good estimation measure of difficulties. Recent research works by OpenAI (2024) and Snell et al. (2024) suggest that a longer reasoning length in the inference time scales up the LLM reasoning ability and enables it to achieve expert-level capabilities. These researches provide evidence for our assumption. Meanwhile, they also show the chance that our method can also be applied to improve the reliability of those language models with super-long reasoning lengths.\nThis work focuses on challenging reasoning problems, where complexity can be estimated by the number of reasoning steps. For simpler problems with shorter reasoning paths or well-studied datasets (e.g., GSM8K), R-Tuning reliably performs well, leaving limited scope for further optimization. This is partly due to the reduced impact of compounding errors in these easier tasks. We also found that the model might decline to answer even if its response is correct. Thus, further adjustment and fine-tuning should be applied to solve this issue, and our method should be able to be extended accordingly as well. In addition, our method might require a long time for optimisation as it requires running Expert Iteration multiple times. Nonetheless, compared to the cost of pre-training, the overhead of the iterative post-training algorithm remains worthwhile for achieving better alignments."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "In Section 4, we provide pseudocode for our algorithm and explain the details of each part. We provide sufficient details in Appendix Section C for our implementation of AUTO-CEI and training settings. For our experiment benchmarks, we introduce our benchmark selection and metric computation in Section 5.1 and provide the link to download the dataset or the source code from the published papers to generate the dataset, the format of the dataset, and the dataset size we choose in Appendix Section B. We also provide the baseline methods' details, implementation and training in Appendix Section D."}, {"title": "B.1 BOARDGAMEQA", "content": "For boardgameQA, we use the main dataset to do fine-tuning and evaluation 1. We take the dataset under the folder BoardgameQA-Main-depth1, BoardgameQA-Main-depth2 and BoardgameQA-Main-depth3.\nWe show one example of the problem in BoardgameQA in Listing 1. For more details about the dataset itself, please refer to the paper by Kazemi et al. (2024)."}, {"title": "BoardgameQA data argumentation", "content": "For the problems whose label is unknown, the proof only provides one simple statement: The provided information is not enough to prove or disprove the statement \"{the query statement}\u201d. This make the responses inconsistent with other responses whose label is proved or disproved. Thus, we use GPT-4 to generate the chain of thought data for unknown cases. The prompt of our data generation is provided in the Listing 2."}, {"title": "B.2 \u039c\u0391\u03a4\u0397", "content": "We download the MATH dataset via Hugging Face at https://huggingface.co/datasets/ hendrycks/competition_math. It automatically divide the dataset into training, validation, and test set. We used all the training data in our experiment and evaluated it using all validation and testing sets to check if the final answer was the same as the labelled data. For more details about the dataset itself, please refer to the paper by Hendrycks et al. (2021). One example format of the dataset is shown in the Listing 3."}, {"title": "B.3 BLOCKSWORLD", "content": "The Blocksworld dataset can be generated using the code in the GitHub repository by Valmeekam et al. (2023): https://github.com/karthikv792/LLMs-Planning. For our case, we generate domains from 4 blocks to 6 blocks, and randomly samples 500 data points for training. We randomly sample 500 data points for validation and testing sets whose optimal solution length (i.e., ground truth plan) is no longer than ten steps. We uniformly sample the tasks according to the ground truth lengths to form the testing set (i.e., 100 two-step tasks, 100 four-step tasks, and 100 ten-step tasks).\nThe original format of the chain of thought for Blocksworld planning is unnecessarily long, therefore making it difficult to learn a sufficient policy. We modified the format of the chain of thought to make it more concise and informative. The elementary format is shown below: \u201c{problem_description}. Since {precondition_text}. Thus, we take action {action} Since {precondition_text}. Thus, we take action {action}. Since the goal is GOAL. The goal conditions are satisfied. ###\u201d. One example of the dataset format is shown in the Listing 4."}, {"title": "C IMPLEMENTATION DETAILS", "content": "AUTO-CEI SETTING\nNeural Network Model We use Llama-3.1-8B-instruct (Dubey et al., 2024) as the backbone model and use Lora (r = 128, \u03b1 = 64) to fine-tune. The Llama-3.1-8B-instruct model is downloaded at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct. We use the Fastchat template designed for Llama-3.1-8B-Instruct. The template is demonstrated in the Listing 5. We fill in the Prompt by our query and train the LLM to respond by Completion."}, {"title": "D DETAILS OF BASELINE METHODS", "content": "The setting is the same as AUTO-CEI in the initialisation part of Section C.1."}, {"title": "D.2 RLKF", "content": "Reinforcement Learning from Knowledge Feedback (RLKF) (Xu et al., 2024) is an RL-based post-training method for hallucination mitigation. It aligns the LLM's behaviours according to the consistency and correctness of LLM's sampled responses: it teaches LLM to respond assertively if its responses are correct and consistent and acknowledges \u201cI don't know\" if its responses are mainly wrong or inconsistent. We implemented a DPO version of this method as one of our baseline.\nData collection Xu et al. (2024) define the pairwise preference to define the reward function:\npair $\\begin{cases} (x, y_c) > (x, y_r), \\\\ (x, y_c) > (x, y_w), \\\\ (x, y_r) > (x, y_w), \\end{cases}$ for for for All samples are correct; There exists correct response in samples; All samples are wrong. (3)\nIn this equation, $x$ refers to the query, and $y$ means the response sampled from LLM. $y_c$ denotes the correct response, $y_w$ denotes the wrong responses, and $y_r$ denotes the refusal response (acknowledging \"I don't know\"). We use this strategy to build up the dataset. Since we do not have a special out-of-domain dataset in our setting, we only build the reliable in-domain preference dataset. We sample the LLM using temperature=1.0 and top_p=0.95, and sample 16 instances for each query. The template refusal responses in RLKF is the same as the R-Tuning. If all responses are correct, we will annotate all 16 responses using the refusal template to form 16 pairs of preference data. If there are correct and incorrect responses, we sample 16 preference pairs randomly. If all samples are wrong, we use all of the wrong responses to generate the refusal responses and form 16 pairs of preference data."}]}