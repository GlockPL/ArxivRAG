{"title": "A RECURRENT YOLOV8-BASED FRAMEWORK FOR EVENT-BASED OBJECT DETECTION", "authors": ["Diego A. Silva", "Kamilya Smagulova", "Ahmed Elsheikh", "Mohammed E. Fouda", "Ahmed M. Eltawil"], "abstract": "Object detection is crucial in various cutting-edge applications, such as autonomous vehicles and\nadvanced robotics systems, primarily relying on data from conventional frame-based RGB sensors.\nHowever, these sensors often struggle with issues like motion blur and poor performance in challeng-\ning lighting conditions. In response to these challenges, event-based cameras have emerged as an\ninnovative paradigm. These cameras, mimicking the human eye, demonstrate superior performance\nin environments with fast motion and extreme lighting conditions while consuming less power. This\nstudy introduces Recurrent YOLOv8 (ReYOLOV8), an advanced object detection framework that\nenhances a leading frame-based detection system with spatiotemporal modeling capabilities. We\nimplemented a low-latency, memory-efficient method for encoding event data to boost the system's\nperformance. Additionally, we developed a novel data augmentation technique tailored to leverage\nthe unique attributes of event data, thus improving detection accuracy. Our framework underwent\nevaluation using the comprehensive event-based datasets Prophesee's Generation 1 (GEN1) and\nPerson Detection for Robotics (PEDRo). Our models outperformed all comparable approaches in\nthe GEN1 dataset, focusing on automotive applications, achieving mean Average Precision (mAP)\nimprovements of 5%, 2.8%, and 2.5% across nano, small, and medium scales, respectively. These\nenhancements were achieved while reducing the number of trainable parameters by an average of\n4.43% and maintaining real-time processing speeds between 9.2ms and 15.5ms. On the PEDRO\ndataset, which targets robotics applications, our models showed mAP improvements ranging from\n9% to 18%, with 14.5x and 3.8x smaller models and an average speed enhancement of 1.67x. These\nresults highlight the transformative potential of integrating event-based technologies with advanced\nobject detection frameworks, paving the way for more robust and efficient visual processing systems\nin dynamic and challenging environments.", "sections": [{"title": "1 Introduction", "content": "Object Detection involves the dual processes of locating and categorizing objects within an image, serving as a critical\nfunction in a multitude of fields, including Autonomous Driving [1], Robotics [2], and Surveillance [3]. Deep Learning\nalgorithms primarily drive advancements in this area, with the You Only Look Once (YOLO) detectors, based on\nConvolutional Neural Networks (CNN), emerging as a prominent choice in academia and industry. Renowned for\nits real-time capabilities, YOLO stands out for its efficient performance with minimal parameters, as documented in\nvarious studies [4]. Over time, YOLO has undergone iterations and enhancements, making it faster and more robust in\nhandling object detection tasks [5]."}, {"title": "2 Related Works", "content": "In contemporary computer vision applications, the standard practice involves processing images captured by cameras\nthat detect light in red, green, and blue wavelengths (RGB). While modern RGB sensors excel in providing high-\nresolution and detailed frames, they are susceptible to motion blur during high-speed movements, and their limited\nHigh Dynamic Range (HDR) poses challenges in complex lighting scenarios [6]. In contrast, event-based cameras\noperate based on changes in illumination rather than absolute light levels, drawing inspiration from the human eye's\nvisual data processing mechanism. This novel approach results in sparse data sequences comprising spatial coordinates,\ntimestamps, and polarity information triggered by variations in light stimuli at specific pixels [7]. Event-based cameras\noffer distinctive advantages, including ultra-low latency in the microsecond range, an HDR typically exceeding 120dB,\nand power consumption in the milliwatt range. These characteristics render event-based cameras particularly suitable\nfor time-critical tasks and challenging lighting conditions, making them a preferred choice in applications where swift\nresponsiveness and adaptability to varying light environments are paramount [8].\nThe majority of existing object detectors tailored for event-based data primarily target autonomous driving scenarios,\ntypically relying on datasets like Prophesee's Generation 1 (GEN1) [9] and 1 MegaPixel [10], alongside robotics\napplications supported by the recently introduced Person Detection in Robotics (PEDRO) dataset [11]. These detectors\nare commonly categorized into two groups based on their approach to event stream handling: direct processing of sparse\nevent streams and densification before processing. The former group includes Graph Neural Networks (GNNs) [12],\nSpiking Neural Networks (SNNs) [13], and sparse CNNs [14]. On the other hand, detectors in the second group first\ndensify events before applying feature extractors such as CNNs [10] and transformers [15] and occasionally incorporate\nRecurrent Neural Networks (RNNs) or State Space Models (SSM) [16] for modeling temporal relationships. On both\ncategories, the feature extractors are combined with detection heads commonly found in frame-based detection models,\nsuch as the YOLO family [15], [17], [18], RetinaNet [19], and Single-Shot Detector (SSD) [20], which are proven to\nprovide good detection capability in event-based scenarios.\nDespite recent advancements in GNNs and SNNs, detectors relying on densified event representations consistently\noutperform them by a significant margin, as evidenced in studies such as those by [21], [22], and [18]. Notably,\ntop-performing detectors often leverage state-of-the-art detection heads borrowed from frame-based literature, leading\nto superior performance. Considering this trend, this research has opted to build upon the YOLOv8 framework\n[23] as the foundation for the event-based detector development. YOLOv8's exceptional performance and real-time\nprocessing capabilities make it a compelling choice over other alternatives, with a focus on achieving enhanced\nperformance with reduced parameters. Recognizing the success of integrating spatial feature extractors and RNNS\nin event data processing, as demonstrated in works like [10], [24], and [15], the YOLOv8 framework has been\nenhanced to incorporate Convolutional Long-Short Term Memory (ConvLSTM) cells, along with the implementation\nof Truncated-Backpropagation Through Time (T-BPTT) for RNN training [25].\nA critical aspect of current event-based object detectors lies in the dense encodings used for effectively feeding event\nstream data into neural networks. Various encoding strategies have been proposed in the literature, each aiming to\nretain crucial information from event streams. These strategies range from simple projections on 2D planes based on\nevent counting [26], [27], timestamp manipulation techniques [28], [29], [30], to hybrid approaches combining both\nmethods [31], [32]. Other methodologies involve segmenting streams into spatiotemporal volumes [33], [15], [34], [35],\n[36], while learning-based encodings [37], [38], Bayesian optimization [18], and the utilization of First-In-First-Out\n(FIFO) buffers [39], [40] have also been proposed. Each of these approaches presents distinct trade-offs, impacting\ndetection performance, encoding latency, and memory requirements associated with event inputs. While existing\nliterature assesses the impact of these encoding choices on detection performance, inference time, and event processing\nrates, a comprehensive system-level perspective is lacking. To address this gap and introduce a memory-efficient fast\nencoding alternative, this work introduces a novel event representation titled Volume of Ternary Event Images (VTEI).\nA comparative analysis against closely related alternatives was conducted, not only focusing on parameters like latency\nbut also assessing hardware-related factors such as data unit encoding size, memory footprint under various event rates,\ncompression ratios, and bandwidth requirements.\nMoreover, recognizing the lack of techniques that specifically address event-based feature augmentation, this study\nintroduces a novel approach known as Random Polarity Suppression (RPS). This method involves randomly suppressing\nall events associated with a particular polarity, enabling the detector to learn object-relevant features in a polarity-\nagnostic manner and mitigating potential biases in polarity distribution that could exist within the training dataset.\nIn summary, the contributions of this work are:\n\u2022 A Recurrent-Convolutional Event-Based Object Detection network was introduced by means of the modi-\nfication of the well-acknowledged real-time detector YOLOv8. The resulting framework, called Recurrent\nYOLOv8 (ReYOLOv8), was based on the addition of recurrent blocks and training with T-BPTT to the original\nframework, turning it capable of performing long-range spatiotemporal modeling;\n\u2022 A fast and lightweight memory encoding called Volume of Ternary Event Images (VTEI) was proposed. This\nformat is capable of retaining temporal information from event streams while presenting low latency, low\nbandwidth, high sparsity, and high compression ratio;\n\u2022 A novel data augmentation technique based on Random Polarity Suppression (RPS) was introduced, showing\nsuccess in improving the performance of the detection systems;\n\u2022 The aforementioned contributions were merged into a single system, and validation of the resulting framework\nwas performed across three different model scales over two real-world large-scale datasets. State-of-the-art\nperformance for similar scale models was reported.\nThis paper is organized as follows: Section 2 presents a review of the related works on event representations, detectors,\nand data augmentation techniques. Then, in Section 3, a discussion about the ideas proposed in this paper is performed.\nAfter that, the results are exposed in Section 4. Finally, Section 5 summarizes the main achievements of this work and\nprovides some insights about future works."}, {"title": "2.1 Event Representations", "content": "One of the most common and intuitive methods for event representation involves projecting events onto a 2D-pixel grid\nfor modern CNN compatibility. An effective approach involves generating 2D grids based on timestamps [28], [29],\n[30]. Along the lines of this 2D concept, Event Frames or Histograms - rely on event counts at each pixel location\n[26], presenting in some cases channels separated by polarity [27] or combinations of channels integrating polarity and\ntimestamp features [31], [32].\nPreserving temporal information from events often involves constructing dense representations segmented into distinct\ntemporal windows, subsequently stacked to construct a 3D tensor. Voxel grids bin events across the time dimension,\nutilizing a bilinear kernel and interval normalization to weigh polarity contributions [33]. In contrast, Stacked\nHistograms replace this kernel by a simple event counting [15]. Mixed-Density Event Stacks (MDES) offer a variation\nwhere bins encode different event densities within a single window segment [34]. Hyper Histograms split temporal\nwindows into smaller units, creating channels based on polarity and timestamp histograms [35]. Event Temporal Images\nmap events within the 0 to 255 range to create grayscale images, incorporating distinct ranges to capture differing\npositive and negative event distributions [36].\nEvent Spike Tensor (EST) proposed an end-to-end learning process where MLPs are trained to find the best encoding\naccording to a generalized 4D tensor, defined over the polarity and spatiotemporal domains [37]. Asynchronous\nAttention Embedding employs an attention layer on events followed by a dilation-based 1D convolution used for data\nencoding [24]. EventPillars, inspired by PointPillars, is a trainable representation that treats events similarly to point\nclouds [38]. Event Representation through Gromov-Wasserstein Optimization (ERGO) employs Bayesian optimization\nover categorical variables, leveraging the Gromov-Wasserstein Discrepancy (GWD) as a key metric to assess the\neffectiveness of a particular event representation [18].\nTime-ordered Recent Events (TORE) volumes utilize First-In-First-Out (FIFO) queues with a depth of K, establishing a\ndirect mapping to each pixel at every polarity. These volumes can be generated asynchronously, without a predefined\ntime window [39]. Temporal Active Focus (TAF) aligns with TORE principles but integrates adaptive features for\nvarying rates [40].\nGiven the asynchronous nature of events, a proposed approach involves encoding them as nodes within graphs, with\nthe connections between nodes defined as edges. This methodology allows efficient processing using Graph Neural\nNetworks (GNNs) [12]. Voxel Cube introduced an alternative to event volumes where event accumulation within each\nmicro-bin is binary, aiming to enhance temporal resolution specifically tailored for Spiking Neural Networks (SNNs)\n[13]. Group Tokens were specifically crafted for integration within Transformer-based architectures, involving the\ndiscretization of the event stream into intervals that are subsequently converted into patches. A 3x3 group convolution\nis then employed to embed the information into tokens effectively within this framework [17].\nIn this work, a memory-efficient and rapid event representation called VTEI is proposed to contribute to the design\nof an efficient and lightweight object detection framework. VTEI leverages a spatiotemporal volume to preserve\ntemporal information, similar to Voxel Grids and Stacked Histograms. However, unlike these methods, VTEI represents\neach data unit within the volume using a limited number of values, similar to MDES. This approach results in a final\nrepresentation characterized by high sparsity, low memory usage, low bandwidth, and low latency. Furthermore, VTEI\neffectively preserves sub-temporal dynamics within a given time window using minimal polarity information."}, {"title": "2.2 Event-Based Object Detectors", "content": "One of the pioneering works on event-based object detection, Asynet, proposed leveraging the intrinsic spatial sparsity\nof event data by converting synchronous networks to asynchronous ones [14]. Recently, a Graph Neural Network (GNN)\napproach called Asynchronous Event-Based GNN (AEGNN) was introduced, modeling events as spatio-temporal\ngraphs with events as nodes and connections between neighboring events as edges. Processing is conducted through\ngraph pooling and graph convolutions [41]. The potential of this approach has inspired the proposal of other similar\nnetworks [42], [21].\nSpiking Neural Networks (SNNs), driven by spikes analogous to events, are acknowledged for their low power\nconsumption, making them suitable for event-based camera applications. A hybrid SNN-ANN architecture was\nproposed, utilizing end-to-end training to leverage event information without intermediate representations [43]. The first\nSNN validated on real-world event data incorporated spiking variants of VGG [44], SqueezeNet [45], MobileNet [46],\nand DenseNet [47] feature extractors attached to an SSD detection head, with DenseNet yielding the best performance\n[13]. By designing a full-spike residual block, the capability to directly train deep-SNNs for object detection improved,\noutperforming hybrid models and achieving real-time responses [48]. Building on spiking residual blocks, Spiking-\nRetinanet proposed an ANN-SNN detector [49], while Spiking-YOLOv4 was developed using a CNN-to-SNN method\n[50]. Additionally, an SNN version of a Region Proposal Network (RPN) for object recognition was introduced [51].\nSpiking Fusion Object Detector (SFOD) was the first to adapt multi-scale feature fusion for SNNs using spiking data\n[52]. Recently, a framework integrating the entire process from event sampling to feature extraction in an end-to-end\nfashion achieved competitive results with ANNs [22].\nThe Recurrent Event-camera Detector (RED) uses Squeeze-and-Excitation (SE) layers [53] for feature extraction and\nConvolutional Long Short-Term Memory (ConvLSTM) blocks for spatiotemporal data extraction, combined with\nan SSD detection head [10]. The Asynchronous Spatio-Temporal Memory Network (ASTMNet) comprises three\ncomponents: Adaptive Temporal Sampling (ATS), Temporal Attention Convolutional Network (TACN), and Spatio-\nTemporal Memory. ATS samples events into bins based on an adaptive scheme related to the event frequency within\nan interval, while TACN aggregates events into an event representation called Asynchronous Attention Embedding.\nThe Spatio-Temporal Memory module implements Recurrent-Convolutional (Rec-Conv) blocks following some\nconvolutional layers [24]. The Agile Event Detector (AED) introduced a new event representation called Temporal\nActive Focus (TAF) to encode sparse event streams into dense tensors, enhancing temporal information extraction\n[40]. The Dual Memory Aggregation Network (DMANet) combines event information over different temporal ranges\n(short-term and long-term) with a learnable representation, EventPillars, for the detection task [38]. A YOLOv5\n[54] detector was adapted to detect events encoded in a novel representation called Hyper Histograms, resulting in a\nremarkable reduction in terms of latency [35].\nRecurrent Vision Transformer (RVT) uses multi-axis attention [55] as a backbone, combined with ConvLSTMs [25]\nand YOLOX detection head for event-based detection [15]. Enhancements to RVT through a self-labeling approach\ndemonstrated further improvements [56]. A detector based on SWin-v2 [57] was proposed, utilizing event encodings\noptimized through the Gromov-Wasserstein Discrepancy. This approach achieved state-of-the-art performance without\nthe need for recurrent modules [18]. HMNet proposed a multi-rate hierarchy with multiple cells to model local and\nglobal context information from objects with varying dynamics, introducing sparse cross-attention operations between\nfeatures and events [58]. A transformer backbone featuring dual attention for spatial and polarity-temporal domains,\npaired with an event encoding focused on tokens, was also proposed [17]. To address event sparsity, a mechanism for\nprocessing only tokens with meaningful information was recently introduced, including a version of the Self-Attention\noperation adjusted for unequal token sizes [59]. Recently, State Space Models (SSM) were introduced to replace\nRNN-cells for temporal modeling on detectors based on transformer backbones [16].\nMost of the aforementioned works process event features through some network and then adopt detection heads used on\nframe data, where the YOLO family is the most common choice. From this family, YOLOv8 is a well-acknowledged\nObject Detector in terms of performance, real-time operation, and scalability [23]. However, it works only with frames,\nwhich, in turn, lacks resources for processing event-based data, such as temporal-based processing. As mentioned\nbefore, a common solution for this is to add recurrent cells to frame-based extractors, as done in [10], [24], and [15], for\nexample. Then, in this work, an extension of the YOLOv8 framework is proposed to add compatibility with events\nprocessing and training."}, {"title": "2.3 Data Augmentation Techniques for Events", "content": "EventDrop randomly drops events from an event stream, which can be applied to individual events, events within a\nspecific spatial location, or events within a particular time window [60]. Neuromorphic Data Augmentation (NDA)\nintroduced an augmentation policy incorporating techniques such as Horizontal Flip, Rolling, Rotation, CutOut, and"}, {"title": "3 Methodology", "content": "This work proposes an event-based object detection framework based on YOLOv8. To support this, a novel event\ndata encoding method is presented, aiming to convert event streams into CNN-suitable representations that can be\ncalculated with low latency, resulting in tensors that require low bandwidth and memory. Moreover, a data augmentation\ntechnique involving the random suppression of positive and negative polarities is also introduced to enhance the system's\nperformance."}, {"title": "3.1 Volume of Ternary Event Images", "content": "Event-based cameras function as 2D sensors capturing brightness variations at the pixel level. This process can be\nexpressed mathematically as:\n$\\Delta L(x_k, y_k, t_k) \\geq p_k C$ (1)\nHere, AL denotes the logarithmic change in a photoreceptor's current (brightness) at pixel location (xk, Yk) and time\ntk. The polarity pk \u2208 {+1, -1} indicates that a brightness change exceeding a threshold C in absolute value triggers a\npositive or negative event [72]. An event is characterized by the tuple ek = (Xk, Yk, Pk,tk).\nThe sparse format of event streams poses a challenge for many current Deep Learning algorithms, requiring prepro-\ncessing for compatibility, as discussed in Section 2.1. With event-based cameras capable of operating at higher rates,\ndriven by sensor resolution improvements [75], utilization of raw event data in downstream tasks can be complex.\nEfficient transformation involves sampling event streams at a consistent rate, partitioning them into sub-windows before\nconversion to dense tensors. This temporal binning strategy, effective in preserving temporal context from event streams,\naccommodates various conversion approaches such as applying a bilinear kernel to normalized timestamps [33], [10],\nevent counting [15], or tracking the latest event at particular locations [34]. In this study, leveraging the success of such\nmethodologies, a variation of Event Volumes is adopted, focusing on computation time and memory requirements. The"}, {"title": "3.2 Recurrent YOLOv8 Architecture", "content": "In Figure 2, the general architecture of the Recurrent YOLOv8 is illustrated. Incoming event streams undergo conversion\nto VTEI tensors with 5 bins, following the method outlined in Section 3.1. These tensors feed the network's feature\nextractor, structured like the original YOLOv8 [23] but incorporating recurrent blocks and resizing certain convolutional\nblocks. The Conv2D blocks function as standard convolutional layers for spatial feature downsampling. Starting from\nthe 2nd stage, feature maps pass through Rec C2f blocks for further refinement before downsampling. These blocks\ncombine C2f blocks, refining features in the channel domain, with a ConvLSTM [25] block, which models long-range\ntemporal relationships by incorporating current and past features. Subsequent to the final recurrent block, a Spatial\nPyramid Pooling (SPP) block enriches features by combining multiple receptive fields [76]. The final features produced\nby this Recurrent Backbone are fed into YOLOv8's Path Aggregation Network (PANet) [77] for fusion and transmission\nto the 3-level detection heads.\nFigure 3 illustrates the architecture of a C2f block [23], which is an efficient version of a Cross-Stage Partial (CSP)\nBottleneck block [78] with two convolutions. The initial convolution in this block adjusts the input channel count.\nSubsequently, a Split block separates the feature into two groups with equal channels. One group undergoes processing\nthrough a sequence of N Bottleneck blocks, with the same structure as the ResNet's blocks [79]. Notably, the shortcut\nconnections within these blocks are deactivated when incorporated into the PANET framework. Finally, the remaining\nsplit channels are merged with the output from each Bottleneck, followed by another convolution to reduce the number\nof channels.\nThe ConvLSTM block depicted in Figure 2 implements a Long-Short Term Memory (LSTM) [80] cell using 1x1\nconvolutions instead of the traditional Multi-Layer Perceptrons (MLP). This adaptation maintains the fundamental\noperational principle of an LSTM while enhancing its versatility to accommodate varying spatial dimensions. A similar\nstructure was adopted on RVT [15]. When considering an input x, along with the previous hidden ht\u22121 and cell states\nCt-1 of the block, the functionality of this cell can be expressed through the following equations:\ni = \u03c3(Conv2D1x1 ([x, ht\u22121])) (4)\nr = \u03c3(Conv2D1x1([x, ht\u22121])) (5)\no = \u03c3(Conv2D1x1([x, ht\u22121])) (6)\nc = tanh(Conv2D1x1([x, ht\u22121])) (7)\nIn these equations, i, r, o, and c represent the input, remember, output, and cell gates, respectively. Here, Conv2D1x1\nsymbolizes a 2D convolution with a 1x1 kernel, o denotes a sigmoid activation function, and the square brackets indicate\nconcatenation between two inputs. The current hidden and cell states, ht and ct, respectively, can be determined by:"}, {"title": "3.3 Event Data Augmentation with Random Polarity Suppression", "content": "The polarity imbalance in event data can stem from various factors. First and foremost, changes in illumination are\nscene-dependent, making it difficult to ensure an equal distribution of positive and negative events corresponding to\nscene movements. Furthermore, electronic circuits within pixels are susceptible to noise across multiple stages, ranging\nfrom the photoreceptor sensor to the comparator and amplifier stages [7]. Even when other noise-related parameters are\nwell-controlled, sporadic positive polarity noisy events have been documented [72]. Moreover, bias currents within\ndifferent stages of an event-based camera pixel can be externally adjusted, potentially influencing sensitivity to distinct\npolarities in varying ways [73], [74].\nTaking this into consideration, a data augmentation technique that specifically targets the polarity domain is proposed.\nTo train the detector effectively under unbalanced polarity scenarios, a probability s will be introduced to suppress a\nspecific polarity within each batch. Additionally, another probability p will denote the likelihood of suppressing the\npositive polarities, with (1 \u2013 p) representing the corresponding value for the negative ones. Considering the random\nvariables (r1, r2) \u2208 [0, 1], the subset of all pixels from the VTEI tensor I with negative polarity In, and its positive\ncounterpart Ip, the Random Polarity Suppression (RPS) technique will construct a new tensor I' based on the following\ncondition:\nI, if r1 s, else\n= Ip, if r2 \u2265 p, else (10)\nIn, otherwise"}, {"title": "3.4 Datasets", "content": "The object detection models in this work were validated using two substantial real-world event datasets, as outlined\nin Table 2. The first dataset, PEDRO, is designed for person detection with a primary focus on Robotics applications.\nRecorded in Italy using a handheld camera, PEDRO captures individuals across diverse scenes, lighting conditions, and\nweather situations. The data was captured using a DAVIS346 camera with a resolution of 346x260 pixels. PEDRO\nis the sole real-world event-based large-scale dataset tailored specifically for Robotics applications to date [11]. The\nsecond dataset, Prophesee's Generation 1 Automotive Dataset (GEN1), was recorded in France and encompasses\nvarious weather and illumination scenarios incorporating pedestrians and cars [9]. While both datasets are significant\nin their respective applications and sizes, they exhibit complementary characteristics. GEN1 boasts a wider range of"}, {"title": "3.5 Training and Evaluation Procedure", "content": "To train the models described in Section 2.2, Truncated Backpropagation Through Time (T-BPTT) [82] was employed.\nDuring training, each dataset was segmented into clips with limited sequence lengths, and the memory cells were reset\nafter each clip. During validation, complete original sequences were assessed, with memory cells being reset at the end\nof each sequence, aligning with methodologies observed in [10] and [15]. Consistent application of data augmentation\ntechniques was ensured across all frames within the same training sequence. The optimizer utilized was Stochastic\nGradient Descent (SGD), with a momentum of 0.937 and linear learning rate decay.\nIn addition to T-BPTT, the training process closely adhered to the approach established within the YOLOv8 framework.\nA warm-up phase of 3 epochs was adopted to initiate training, consisting of a momentum of 0.8 and a bias learning rate\nof 0.1. The losses for box regression, classification, and Distribution Focal Loss (DFL) [83] maintained the same values\nas the original framework: 7.5, 0.5, and 1.5, respectively. Dataset-specific hyperparameters are detailed in Table 3. All\nmodels were trained from scratch for 100 epochs. The sequence length is related to the T-BPTT setup. The image sizes\nfrom both datasets were adjusted to multiples of 32, aligning with the YOLOv8 anchors [23]. LR0 and LRf reference"}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Evaluation of VTEI", "content": "In order to assess the effectiveness of encoding event streams using VTEI, the test set from the GEN1 dataset was\nconverted into three distinct formats commonly found in Event-Based Object Detection literature: Voxel Grids [33],\nMDES [34], and Stacked Histograms (SHist) [15]. The specific variation of Voxel Grids utilized by RED was employed\n[10]. The comparison excluded formats such as Hyper Histograms [35], an extended version of SHist with additional\nchannels; Event Temporal Images [36], similar to VTEI but with distinct accumulation and mapping processes;\nand asynchronous formats like TORE [39] and TAF tensors [40], due to focusing on fixed time window encodings.\nAdditionally, format-specific to different model categories like Voxel Cubes [13] for SNNs, Group Tokens [17] for\ntransformers, and graph-related representations [12], [41], [21] were also excluded, as this study is centered on formats\nthat can be adopted alongside convolutional architectures.\nTable 4 provides dimensions and the minimum byte counts for VTEI, SHist, and MDES for the GEN1 dataset. Compared\nto MDES and VTEI, SHist and Voxel Grids calculate polarities in separate channels before stacking them temporally,\neffectively doubling the number of channels. For GEN1, dimensions B, H, and W are 5, 240, and 304, respectively. The\nminimum byte count for each format was computed based on the Coordinate List (COO) compression, where each\nnon-zero is encoded by its coordinate and data content. For the GEN1, spatial dimensions are encoded in 17 bits. The\ndata content for VTEI and MDES can be represented in binary format, while the number of bins can be encoded in 3 bits.\nThen, summing it up, VTEI and MDES require 3 bytes per non-zero entry. SHist needs an additional bit to represent the\nchannel domain, while the data itself is encoded in 8 bits, totaling 4 bytes per entry. Voxel Grids is similar to SHist but\nutilizes a sum function over normalized timestamps instead of event counting. Hence, with a decimal range requirement,\na half-precision floating-point format (16 bits) is required, which is the smallest present in libraries like Pytorch, adding\nan extra byte for data encoding. Overall, the merging of polarities into the same channels and employing a narrower\ndata value range positioned VTEI and MDES as superior options in terms of memory requirements.\nTo better highlight the capabilities of each format, a further analysis was performed, taking into consideration the\nrecording from the GEN1's test set with the biggest number of events. All analyses were conducted on 50ms samples\nextracted from the original recordings. The outcomes, summarized in Table 5, are categorized into three sections\ndetailing the average, maximum, and minimum event counts across all chunks. The performance data were gathered\nusing an Intel Xeon Gold 6230R CPU on an Ubuntu 20.04.6 LTS operating system with 251GB of RAM. Regarding\nlatency for a 50ms window (Latency@50ms), based on the value most commonly adopted in the literature, VTEI\nemerged as the top performer compared to other formats in all scenarios, with the disparity increasing as the number of\nevents increased. Notably, VTEI was 1.54x faster on average than SHist, rising to 2.17x at the maximum event count"}, {"title": "4.2 Evaluation of Random Polarity Suppression", "content": "The impact of applying Polarity Suppression (RPS) on models was evaluated by comparing the baseline mean Average\nPrecision (mAP) of ReYOLOv8s with the results using RPS. The positive polarity suppression probability p was\ninitially set at five levels: 0.0, 0.25, 0.50, 0.75, 1.0. Different suppression probabilities s from 0.05, 0.125, 0.25, 0.375,\n0.5 were tested for each p value. Five runs were conducted for both the baseline and each RPS combination, and\naverage results were reported to account for variability. Figure 5 displays the validation set results from the PEDRO\ndataset. The findings show that improvements generally occur up to a 12.5% suppression probability, suggesting that\nRPS enhances model performance when used as a small disturbance. Larger suppression levels may degrade the quality\nof training samples and negatively impact performance. The most significant enhancements across all p values were\nseen at s = 0.05 and s = 0.125. Notably, on average, p = 0.50 led to the greatest improvements, followed by p = 0.0,\np = 0.75, p = 0.25, and p = 1.0. There is no clear pattern regarding which polarity should be suppressed, as similar\nperformance levels were observed for p = 0.0, which represents a total negative polarity suppression, and p = 0.75,\nan aggressive positive polarity suppression. A comparable trend was seen for p = 0.25 and p = 1.0, which involve\nopposite types of data manipulation but resulted in similar outcomes at s = 0.125. These results suggest that objects in\nthe PEDRO dataset have a diverse polarity distribution and are not particularly biased towards one polarity. This also\nexplains why the most balanced scenario, p = 0.50, led to the best results on average.\nFigure 6 shows the results of applying RPS to the GEN1 validation set, using the same procedure as for PEDRO. A\nsimilar trend to Figure 5 is observed, where improvements in mean Average Precision (mAP) are more significant for\nsuppression probabilities below 12.5%. Except for p = 0.75, no improvements were observed for s = 0.25. When\nexamining positive and negative suppression separately, the balanced scenario with p = 0.50 performed better than most\nother values, similar to the findings for PEDRo. However, an exception was observed for p = 0.75, which achieved the\nhighest average improvement and a peak enhancement comparable to p = 0.50. At p = 0.0 and p = 0.25, there was\na noticeable increase in mAP at s = 0.05, with steep decreases after s = 0.125. No improvement was observed for\np = 1.0."}, {"title": "4.3 Comparison with the state-of-the-art", "content": "Table 7 compares the three models introduced in this study, namely ReYOLOv8n, ReYOLOv8s, and ReYOLOv8m,\nwith the state-of-the-art YOLOv8x-based model for PEDRo. All models exhibited improvements in mean Average\nPrecision (mAP), ranging from 9% to 18%, requiring significantly fewer parameters - 14.5x and 3.8x, respectively. This\nnotable performance improvement at a lower"}]}