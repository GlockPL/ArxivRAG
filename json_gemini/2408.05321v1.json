{"title": "A RECURRENT YOLOV8-BASED FRAMEWORK FOR EVENT-BASED OBJECT DETECTION", "authors": ["Diego A. Silva", "Kamilya Smagulova", "Ahmed Elsheikh", "Mohammed E. Fouda", "Ahmed M. Eltawil"], "abstract": "Object detection is crucial in various cutting-edge applications, such as autonomous vehicles and advanced robotics systems, primarily relying on data from conventional frame-based RGB sensors. However, these sensors often struggle with issues like motion blur and poor performance in challenging lighting conditions. In response to these challenges, event-based cameras have emerged as an innovative paradigm. These cameras, mimicking the human eye, demonstrate superior performance in environments with fast motion and extreme lighting conditions while consuming less power. This study introduces Recurrent YOLOv8 (ReYOLOV8), an advanced object detection framework that enhances a leading frame-based detection system with spatiotemporal modeling capabilities. We implemented a low-latency, memory-efficient method for encoding event data to boost the system's performance. Additionally, we developed a novel data augmentation technique tailored to leverage the unique attributes of event data, thus improving detection accuracy. Our framework underwent evaluation using the comprehensive event-based datasets Prophesee's Generation 1 (GEN1) and Person Detection for Robotics (PEDRo). Our models outperformed all comparable approaches in the GEN1 dataset, focusing on automotive applications, achieving mean Average Precision (mAP) improvements of 5%, 2.8%, and 2.5% across nano, small, and medium scales, respectively. These enhancements were achieved while reducing the number of trainable parameters by an average of 4.43% and maintaining real-time processing speeds between 9.2ms and 15.5ms. On the PEDRO dataset, which targets robotics applications, our models showed mAP improvements ranging from 9% to 18%, with 14.5x and 3.8x smaller models and an average speed enhancement of 1.67x. These results highlight the transformative potential of integrating event-based technologies with advanced object detection frameworks, paving the way for more robust and efficient visual processing systems in dynamic and challenging environments.", "sections": [{"title": "Introduction", "content": "Object Detection involves the dual processes of locating and categorizing objects within an image, serving as a critical function in a multitude of fields, including Autonomous Driving [1], Robotics [2], and Surveillance [3]. Deep Learning algorithms primarily drive advancements in this area, with the You Only Look Once (YOLO) detectors, based on Convolutional Neural Networks (CNN), emerging as a prominent choice in academia and industry. Renowned for its real-time capabilities, YOLO stands out for its efficient performance with minimal parameters, as documented in various studies [4]. Over time, YOLO has undergone iterations and enhancements, making it faster and more robust in handling object detection tasks [5]."}, {"title": "Related Works", "content": "The majority of existing object detectors tailored for event-based data primarily target autonomous driving scenarios, typically relying on datasets like Prophesee's Generation 1 (GEN1) [9] and 1 MegaPixel [10], alongside robotics applications supported by the recently introduced Person Detection in Robotics (PEDRO) dataset [11]. These detectors are commonly categorized into two groups based on their approach to event stream handling: direct processing of sparse event streams and densification before processing. The former group includes Graph Neural Networks (GNNs) [12], Spiking Neural Networks (SNNs) [13], and sparse CNNs [14]. On the other hand, detectors in the second group first densify events before applying feature extractors such as CNNs [10] and transformers [15] and occasionally incorporate Recurrent Neural Networks (RNNs) or State Space Models (SSM) [16] for modeling temporal relationships. On both categories, the feature extractors are combined with detection heads commonly found in frame-based detection models, such as the YOLO family [15], [17], [18], RetinaNet [19], and Single-Shot Detector (SSD) [20], which are proven to provide good detection capability in event-based scenarios.\nDespite recent advancements in GNNs and SNNs, detectors relying on densified event representations consistently outperform them by a significant margin, as evidenced in studies such as those by [21], [22], and [18]. Notably, top-performing detectors often leverage state-of-the-art detection heads borrowed from frame-based literature, leading to superior performance. Considering this trend, this research has opted to build upon the YOLOv8 framework [23] as the foundation for the event-based detector development. YOLOv8's exceptional performance and real-time processing capabilities make it a compelling choice over other alternatives, with a focus on achieving enhanced performance with reduced parameters. Recognizing the success of integrating spatial feature extractors and RNNS in event data processing, as demonstrated in works like [10], [24], and [15], the YOLOv8 framework has been enhanced to incorporate Convolutional Long-Short Term Memory (ConvLSTM) cells, along with the implementation of Truncated-Backpropagation Through Time (T-BPTT) for RNN training [25].\nA critical aspect of current event-based object detectors lies in the dense encodings used for effectively feeding event stream data into neural networks. Various encoding strategies have been proposed in the literature, each aiming to retain crucial information from event streams. These strategies range from simple projections on 2D planes based on event counting [26], [27], timestamp manipulation techniques [28], [29], [30], to hybrid approaches combining both methods [31], [32]. Other methodologies involve segmenting streams into spatiotemporal volumes [33], [15], [34], [35], [36], while learning-based encodings [37], [38], Bayesian optimization [18], and the utilization of First-In-First-Out (FIFO) buffers [39], [40] have also been proposed. Each of these approaches presents distinct trade-offs, impacting detection performance, encoding latency, and memory requirements associated with event inputs. While existing literature assesses the impact of these encoding choices on detection performance, inference time, and event processing rates, a comprehensive system-level perspective is lacking. To address this gap and introduce a memory-efficient fast encoding alternative, this work introduces a novel event representation titled Volume of Ternary Event Images (VTEI). A comparative analysis against closely related alternatives was conducted, not only focusing on parameters like latency but also assessing hardware-related factors such as data unit encoding size, memory footprint under various event rates, compression ratios, and bandwidth requirements.\nMoreover, recognizing the lack of techniques that specifically address event-based feature augmentation, this study introduces a novel approach known as Random Polarity Suppression (RPS). This method involves randomly suppressing all events associated with a particular polarity, enabling the detector to learn object-relevant features in a polarity- agnostic manner and mitigating potential biases in polarity distribution that could exist within the training dataset.\nIn summary, the contributions of this work are:\n\u2022 A Recurrent-Convolutional Event-Based Object Detection network was introduced by means of the modification of the well-acknowledged real-time detector YOLOv8. The resulting framework, called Recurrent YOLOv8 (ReYOLOv8), was based on the addition of recurrent blocks and training with T-BPTT to the original framework, turning it capable of performing long-range spatiotemporal modeling;"}, {"title": "2.1 Event Representations", "content": "One of the most common and intuitive methods for event representation involves projecting events onto a 2D-pixel grid for modern CNN compatibility. An effective approach involves generating 2D grids based on timestamps [28], [29], [30]. Along the lines of this 2D concept, Event Frames or Histograms - rely on event counts at each pixel location [26], presenting in some cases channels separated by polarity [27] or combinations of channels integrating polarity and timestamp features [31], [32].\nPreserving temporal information from events often involves constructing dense representations segmented into distinct temporal windows, subsequently stacked to construct a 3D tensor. Voxel grids bin events across the time dimension, utilizing a bilinear kernel and interval normalization to weigh polarity contributions [33]. In contrast, Stacked Histograms replace this kernel by a simple event counting [15]. Mixed-Density Event Stacks (MDES) offer a variation where bins encode different event densities within a single window segment [34]. Hyper Histograms split temporal windows into smaller units, creating channels based on polarity and timestamp histograms [35]. Event Temporal Images map events within the 0 to 255 range to create grayscale images, incorporating distinct ranges to capture differing positive and negative event distributions [36].\nEvent Spike Tensor (EST) proposed an end-to-end learning process where MLPs are trained to find the best encoding according to a generalized 4D tensor, defined over the polarity and spatiotemporal domains [37]. Asynchronous Attention Embedding employs an attention layer on events followed by a dilation-based 1D convolution used for data encoding [24]. EventPillars, inspired by PointPillars, is a trainable representation that treats events similarly to point clouds [38]. Event Representation through Gromov-Wasserstein Optimization (ERGO) employs Bayesian optimization over categorical variables, leveraging the Gromov-Wasserstein Discrepancy (GWD) as a key metric to assess the effectiveness of a particular event representation [18].\nTime-ordered Recent Events (TORE) volumes utilize First-In-First-Out (FIFO) queues with a depth of K, establishing a direct mapping to each pixel at every polarity. These volumes can be generated asynchronously, without a predefined time window [39]. Temporal Active Focus (TAF) aligns with TORE principles but integrates adaptive features for varying rates [40].\nGiven the asynchronous nature of events, a proposed approach involves encoding them as nodes within graphs, with the connections between nodes defined as edges. This methodology allows efficient processing using Graph Neural Networks (GNNs) [12]. Voxel Cube introduced an alternative to event volumes where event accumulation within each micro-bin is binary, aiming to enhance temporal resolution specifically tailored for Spiking Neural Networks (SNNs) [13]. Group Tokens were specifically crafted for integration within Transformer-based architectures, involving the discretization of the event stream into intervals that are subsequently converted into patches. A 3x3 group convolution is then employed to embed the information into tokens effectively within this framework [17].\nIn this work, a memory-efficient and rapid event representation called VTEI is proposed to contribute to the design of an efficient and lightweight object detection framework. VTEI leverages a spatiotemporal volume to preserve temporal information, similar to Voxel Grids and Stacked Histograms. However, unlike these methods, VTEI represents each data unit within the volume using a limited number of values, similar to MDES. This approach results in a final representation characterized by high sparsity, low memory usage, low bandwidth, and low latency. Furthermore, VTEI effectively preserves sub-temporal dynamics within a given time window using minimal polarity information."}, {"title": "2.2 Event-Based Object Detectors", "content": "One of the pioneering works on event-based object detection, Asynet, proposed leveraging the intrinsic spatial sparsity of event data by converting synchronous networks to asynchronous ones [14]. Recently, a Graph Neural Network (GNN) approach called Asynchronous Event-Based GNN (AEGNN) was introduced, modeling events as spatio-temporal graphs with events as nodes and connections between neighboring events as edges. Processing is conducted through graph pooling and graph convolutions [41]. The potential of this approach has inspired the proposal of other similar networks [42], [21].\nSpiking Neural Networks (SNNs), driven by spikes analogous to events, are acknowledged for their low power consumption, making them suitable for event-based camera applications. A hybrid SNN-ANN architecture was proposed, utilizing end-to-end training to leverage event information without intermediate representations [43]. The first SNN validated on real-world event data incorporated spiking variants of VGG [44], SqueezeNet [45], MobileNet [46], and DenseNet [47] feature extractors attached to an SSD detection head, with DenseNet yielding the best performance [13]. By designing a full-spike residual block, the capability to directly train deep-SNNs for object detection improved, outperforming hybrid models and achieving real-time responses [48]. Building on spiking residual blocks, Spiking-Retinanet proposed an ANN-SNN detector [49], while Spiking-YOLOv4 was developed using a CNN-to-SNN method [50]. Additionally, an SNN version of a Region Proposal Network (RPN) for object recognition was introduced [51]. Spiking Fusion Object Detector (SFOD) was the first to adapt multi-scale feature fusion for SNNs using spiking data [52]. Recently, a framework integrating the entire process from event sampling to feature extraction in an end-to-end fashion achieved competitive results with ANNs [22].\nThe Recurrent Event-camera Detector (RED) uses Squeeze-and-Excitation (SE) layers [53] for feature extraction and Convolutional Long Short-Term Memory (ConvLSTM) blocks for spatiotemporal data extraction, combined with an SSD detection head [10]. The Asynchronous Spatio-Temporal Memory Network (ASTMNet) comprises three components: Adaptive Temporal Sampling (ATS), Temporal Attention Convolutional Network (TACN), and Spatio-Temporal Memory. ATS samples events into bins based on an adaptive scheme related to the event frequency within an interval, while TACN aggregates events into an event representation called Asynchronous Attention Embedding. The Spatio-Temporal Memory module implements Recurrent-Convolutional (Rec-Conv) blocks following some convolutional layers [24]. The Agile Event Detector (AED) introduced a new event representation called Temporal Active Focus (TAF) to encode sparse event streams into dense tensors, enhancing temporal information extraction [40]. The Dual Memory Aggregation Network (DMANet) combines event information over different temporal ranges (short-term and long-term) with a learnable representation, EventPillars, for the detection task [38]. A YOLOv5 [54] detector was adapted to detect events encoded in a novel representation called Hyper Histograms, resulting in a remarkable reduction in terms of latency [35].\nRecurrent Vision Transformer (RVT) uses multi-axis attention [55] as a backbone, combined with ConvLSTMs [25] and YOLOX detection head for event-based detection [15]. Enhancements to RVT through a self-labeling approach demonstrated further improvements [56]. A detector based on SWin-v2 [57] was proposed, utilizing event encodings optimized through the Gromov-Wasserstein Discrepancy. This approach achieved state-of-the-art performance without the need for recurrent modules [18]. HMNet proposed a multi-rate hierarchy with multiple cells to model local and global context information from objects with varying dynamics, introducing sparse cross-attention operations between features and events [58]. A transformer backbone featuring dual attention for spatial and polarity-temporal domains, paired with an event encoding focused on tokens, was also proposed [17]. To address event sparsity, a mechanism for processing only tokens with meaningful information was recently introduced, including a version of the Self-Attention operation adjusted for unequal token sizes [59]. Recently, State Space Models (SSM) were introduced to replace RNN-cells for temporal modeling on detectors based on transformer backbones [16].\nMost of the aforementioned works process event features through some network and then adopt detection heads used on frame data, where the YOLO family is the most common choice. From this family, YOLOv8 is a well-acknowledged Object Detector in terms of performance, real-time operation, and scalability [23]. However, it works only with frames, which, in turn, lacks resources for processing event-based data, such as temporal-based processing. As mentioned before, a common solution for this is to add recurrent cells to frame-based extractors, as done in [10], [24], and [15], for example. Then, in this work, an extension of the YOLOv8 framework is proposed to add compatibility with events processing and training."}, {"title": "2.3 Data Augmentation Techniques for Events", "content": "EventDrop randomly drops events from an event stream, which can be applied to individual events, events within a specific spatial location, or events within a particular time window [60]. Neuromorphic Data Augmentation (NDA) introduced an augmentation policy incorporating techniques such as Horizontal Flip, Rolling, Rotation, CutOut, and CutMix for training SNNs [61]. Spatio-temporal augmentation using random translation and time scaling was also proposed [62]. Temporal Event Shifting, which involves randomly reallocating events from a given frame to one of its prior frames, has proven beneficial for visual-aided force measurement [63]. Event Spatiotemporal Fragments combines the inversion of event fragments on spatiotemporal and polarity domains with spatiotemporal drift of some slices of events through a certain extent [64]. Moreover, a viewpoint transform based on translation and rotation, combined with spatiotemporal stretching to prevent information loss due to out-of-resolution events discarded during the initial transformation, was introduced for training SNNs [65].\nEventCopyDrop is an enhanced version of EventDrop. It includes an additional augmentation called EventCopy, which creates copies of events from one random region and places them in another random location within the stream [66]. EventMix proposed an augmentation method based on mixing data from different event streams [67]. In RVT, Zoom-Out and Zoom-In augmentations were introduced to enhance event-based object detection [15]. A framework combining geometric spatial augmentations with random temporal shifts and random polarity inversion was proposed [32].\nShadow Mosaic is a technique that simulates events with varying densities, referred to as Shadows, and arranges them into a Mosaic to create a larger sample [35]. ShapeAug introduces random occlusions to event data, enhancing the robustness of object recognition applications [68]. Relevance Propagation Guidance (RPG) is employed to drop and mix events, resulting in the EventRPG augmentation method [69]. EventAugment is an augmentation policy learning framework with 13 specific operations for event data, including flips, translations, crops, drops, and shear operations, targeting both spatial and temporal domains [70]. Additionally, a temporal augmentation technique that involves dropping multiple sections of events within the temporal domain was proposed and evaluated for Lip-Reading applications [71].\nDespite numerous works presenting various approaches for event data augmentation across spatial, temporal, and polarity domains, certain phenomena associated with event-based camera operations that can affect the generalization of Deep Learning models remain underexplored. In real-world scenes, brightness distribution is often irregular, and polarity distribution can vary significantly from one scene to another. Additionally, types of noise discussed in [72] and the adjustable bias settings in pixel circuits [73] [74] can contribute to this variability. In this work, we propose a novel data augmentation model called Random Polarity Suppression to train Deep Learning models considering these variations."}, {"title": "Methodology", "content": "This work proposes an event-based object detection framework based on YOLOv8. To support this, a novel event data encoding method is presented, aiming to convert event streams into CNN-suitable representations that can be calculated with low latency, resulting in tensors that require low bandwidth and memory. Moreover, a data augmentation technique involving the random suppression of positive and negative polarities is also introduced to enhance the system's performance."}, {"title": "3.1 Volume of Ternary Event Images", "content": "Event-based cameras function as 2D sensors capturing brightness variations at the pixel level. This process can be expressed mathematically as:\n$\\Delta L(x_k, y_k, t_k) \\geq p_kC$ (1)\nHere, $\\Delta L$ denotes the logarithmic change in a photoreceptor's current (brightness) at pixel location $(x_k, y_k)$ and time $t_k$. The polarity $p_k \\in \\{+1, -1\\}$ indicates that a brightness change exceeding a threshold C in absolute value triggers a positive or negative event [72]. An event is characterized by the tuple $e_k = (X_k, Y_k, p_k,t_k)$.\nThe sparse format of event streams poses a challenge for many current Deep Learning algorithms, requiring preprocessing for compatibility, as discussed in Section 2.1. With event-based cameras capable of operating at higher rates, driven by sensor resolution improvements [75], utilization of raw event data in downstream tasks can be complex. Efficient transformation involves sampling event streams at a consistent rate, partitioning them into sub-windows before conversion to dense tensors. This temporal binning strategy, effective in preserving temporal context from event streams, accommodates various conversion approaches such as applying a bilinear kernel to normalized timestamps [33], [10], event counting [15], or tracking the latest event at particular locations [34]. In this study, leveraging the success of such methodologies, a variation of Event Volumes is adopted, focusing on computation time and memory requirements. The chosen encoding scheme focuses on sampling the last event at each spatiotemporal location, as done in MDES [34], but with uniform temporal bin sizes."}, {"title": "3.2 Recurrent YOLOv8 Architecture", "content": "In Figure 2, the general architecture of the Recurrent YOLOv8 is illustrated. Incoming event streams undergo conversion to VTEI tensors with 5 bins, following the method outlined in Section 3.1. These tensors feed the network's feature extractor, structured like the original YOLOv8 [23] but incorporating recurrent blocks and resizing certain convolutional blocks. The Conv2D blocks function as standard convolutional layers for spatial feature downsampling. Starting from the 2nd stage, feature maps pass through Rec C2f blocks for further refinement before downsampling. These blocks combine C2f blocks, refining features in the channel domain, with a ConvLSTM [25] block, which models long-range temporal relationships by incorporating current and past features. Subsequent to the final recurrent block, a Spatial Pyramid Pooling (SPP) block enriches features by combining multiple receptive fields [76]. The final features produced by this Recurrent Backbone are fed into YOLOv8's Path Aggregation Network (PANet) [77] for fusion and transmission to the 3-level detection heads."}, {"title": "3.3 Event Data Augmentation with Random Polarity Suppression", "content": "The polarity imbalance in event data can stem from various factors. First and foremost, changes in illumination are scene-dependent, making it difficult to ensure an equal distribution of positive and negative events corresponding to scene movements. Furthermore, electronic circuits within pixels are susceptible to noise across multiple stages, ranging from the photoreceptor sensor to the comparator and amplifier stages [7]. Even when other noise-related parameters are well-controlled, sporadic positive polarity noisy events have been documented [72]. Moreover, bias currents within different stages of an event-based camera pixel can be externally adjusted, potentially influencing sensitivity to distinct polarities in varying ways [73], [74].\nTaking this into consideration, a data augmentation technique that specifically targets the polarity domain is proposed. To train the detector effectively under unbalanced polarity scenarios, a probability s will be introduced to suppress a specific polarity within each batch. Additionally, another probability p will denote the likelihood of suppressing the positive polarities, with (1 \u2013 p) representing the corresponding value for the negative ones. Considering the random variables ($r_1$, $r_2$) \u2208 [0, 1], the subset of all pixels from the VTEI tensor I with negative polarity $I_n$, and its positive counterpart $I_p$, the Random Polarity Suppression (RPS) technique will construct a new tensor $I'$ based on the following condition:\n$I'=\\begin{cases}I,&\\text{if }r_1<s,\\\\I_p,&\\text{if }r_2\\geq p,\\\\I_n,&\\text{otherwise}\\end{cases}$ (10)"}, {"title": "3.5 Training and Evaluation Procedure", "content": "To train the models described in Section 2.2, Truncated Backpropagation Through Time (T-BPTT) [82] was employed. During training, each dataset was segmented into clips with limited sequence lengths, and the memory cells were reset after each clip. During validation, complete original sequences were assessed, with memory cells being reset at the end of each sequence, aligning with methodologies observed in [10] and [15]. Consistent application of data augmentation techniques was ensured across all frames within the same training sequence. The optimizer utilized was Stochastic Gradient Descent (SGD), with a momentum of 0.937 and linear learning rate decay.\nIn addition to T-BPTT, the training process closely adhered to the approach established within the YOLOv8 framework. A warm-up phase of 3 epochs was adopted to initiate training, consisting of a momentum of 0.8 and a bias learning rate of 0.1. The losses for box regression, classification, and Distribution Focal Loss (DFL) [83] maintained the same values as the original framework: 7.5, 0.5, and 1.5, respectively. Dataset-specific hyperparameters are detailed in Table 3. All models were trained from scratch for 100 epochs. The sequence length is related to the T-BPTT setup. The image sizes from both datasets were adjusted to multiples of 32, aligning with the YOLOv8 anchors [23]. LR0 and LRf reference"}, {"title": "4 Results and Discussion", "content": "In order to assess the effectiveness of encoding event streams using VTEI, the test set from the GEN1 dataset was converted into three distinct formats commonly found in Event-Based Object Detection literature: Voxel Grids [33], MDES [34], and Stacked Histograms (SHist) [15]. The specific variation of Voxel Grids utilized by RED was employed [10]. The comparison excluded formats such as Hyper Histograms [35], an extended version of SHist with additional channels; Event Temporal Images [36], similar to VTEI but with distinct accumulation and mapping processes; and asynchronous formats like TORE [39] and TAF tensors [40], due to focusing on fixed time window encodings. Additionally, format-specific to different model categories like Voxel Cubes [13] for SNNs, Group Tokens [17] for transformers, and graph-related representations [12], [41], [21] were also excluded, as this study is centered on formats that can be adopted alongside convolutional architectures."}, {"title": "4.1 Evaluation of VTEI", "content": "In order to assess the effectiveness of encoding event streams using VTEI, the test set from the GEN1 dataset was converted into three distinct formats commonly found in Event-Based Object Detection literature: Voxel Grids [33], MDES [34], and Stacked Histograms (SHist) [15]. The specific variation of Voxel Grids utilized by RED was employed [10]. The comparison excluded formats such as Hyper Histograms [35], an extended version of SHist with additional channels; Event Temporal Images [36], similar to VTEI but with distinct accumulation and mapping processes; and asynchronous formats like TORE [39] and TAF tensors [40], due to focusing on fixed time window encodings. Additionally, format-specific to different model categories like Voxel Cubes [13] for SNNs, Group Tokens [17] for transformers, and graph-related representations [12], [41], [21] were also excluded, as this study is centered on formats that can be adopted alongside convolutional architectures."}, {"title": "4.2 Evaluation of Random Polarity Suppression", "content": "The impact of applying Polarity Suppression (RPS) on models was evaluated by comparing the baseline mean Average Precision (mAP) of ReYOLOv8s with the results using RPS. The positive polarity suppression probability p was initially set at five levels: 0.0, 0.25, 0.50, 0.75, 1.0. Different suppression probabilities s from 0.05, 0.125, 0.25, 0.375, 0.5 were tested for each p value. Five runs were conducted for both the baseline and each RPS combination, and average results were reported to account for variability. Figure 5 displays the validation set results from the PEDRO dataset. The findings show that improvements generally occur up to a 12.5% suppression probability, suggesting that RPS enhances model performance when used as a small disturbance. Larger suppression levels may degrade the quality of training samples and negatively impact performance. The most significant enhancements across all p values were seen at s = 0.05 and s = 0.125. Notably, on average, p = 0.50 led to the greatest improvements, followed by p = 0.0, p = 0.75, p = 0.25, and p = 1.0. There is no clear pattern regarding which polarity should be suppressed, as similar performance levels were observed for p = 0.0, which represents a total negative polarity suppression, and p = 0.75, an aggressive positive polarity suppression. A comparable trend was seen for p = 0.25 and p = 1.0, which involve opposite types of data manipulation but resulted in similar outcomes at s = 0.125. These results suggest that objects in the PEDRO dataset have a diverse polarity distribution and are not particularly biased towards one polarity. This also explains why the most balanced scenario, p = 0.50, led to the best results on average."}, {"title": "4.3 Comparison with the state-of-the-art", "content": "Table 7 compares the three models introduced in this study, namely ReYOLOv8n, ReYOLOv8s, and ReYOLOv8m, with the state-of-the-art YOLOv8x-based model for PEDRo. All models exhibited improvements in mean Average Precision (mAP), ranging from 9% to 18%, requiring significantly fewer parameters - 14.5x and 3.8x, respectively. This notable performance improvement at a lower number of parameters can be attributed to the integration of long-range temporal modeling in the models of this study, a feature lacking in the benchmark YOLOv8x-based model. Given that the original work did not include runtime information [11], the inference times for YOLOv8x processing the same VTEI tensors as ReYOLOv8 were reported here. From this comparative analysis, the models in this study demonstrated an average speed-up of 1.46x, highlighting their efficiency. Furthermore, as shown in Table 6, the ReYOLOv8n implementation without RPS achieved a similar mAP to YOLOv8x, differing by only 0.6%. However, after applying RPS, the gap between the models widened to 9%. This indicates that data augmentation through RPS significantly contributed to the improvements observed in this study, in addition to utilizing memory cells.\nTable 8 presents a comparative analysis of the state-of-the-art models for the GEN1 dataset. Models are classified into Nano, Small, and Medium scales based on their trainable parameter ranges (5M, 15M, and 45M, respectively), a common practice in Computer Vision literature. Inference times are reported separately for NVIDIA's V100 GPU and a group consisting of closely related devices: NVIDIA's GTX1080ti, Titan XP, and GTX980. Only models with mAP exceeding 40.0 are considered, and the best results are highlighted. Firstly, analyzing the Nano models, ReYOLOv8n achieves a 5% improvement in mAP compared to RVT-T, requiring only an additional 0.3M parameters while reducing latency by 0.20ms. When comparing the Small models, ReYOLOv8s also demonstrates superior performance, with a 2.8% increase in mAP compared to HMNet-L1, demanding around 27% fewer parameters but being 2.4x slower. On the medium scale, ReYOLOv8m outperforms SAST-CB [59] by 2.5%, but also with fewer parameters. The superior performance of ReYOLOv8 can be attributed to its foundation on the YOLOv8 baseline, known to outperform other detectors like YOLOX [85], which serves as the detection head for RVT [15], GET [17], and SAST-CB [59] models. Models exceeding 45M parameters were omitted, as they did not yield substantial gains compared to the Medium-scaled models. Notable exceptions include DSTDNet-X [36], which has 100M parameters and has a mAP similar to ReYOLOv8m, and ERGO12 [18], a model based on a pre-trained SWinv2 [57] transformer that achieves a mAP of 50.4 with 59.6M parameters and a latency of 77ms.\nIn terms of runtime, it can be seen that the models are competitive when compared to other approaches that deploy RNNs, where ReYOLOv8n is 2% faster than RVT-T, RVT-S outperforms ReYOLOv8s by around 9%, and ReYOLOv8m achieves 80% of the speed of RVT-B. On the other hand, compared to models that do not deploy RNNs, ReYOLOv8s is, on average, 1.9x slower than other similar-sized models. A similar behavior is also present for Medium models. A deeper look into their design choices should be done to understand this difference. For the case of DTSDNet [36], events are converted into two different tensors according to different time-window sizes, a smaller and a bigger one. Those tensors are passed through two parallel CNNs, and the resulting features are fused afterward. By doing so, they can introduce long- and short-term temporal modeling without adopting RNNs, avoiding the latency those cells introduce. AED [40] achieves a similar effect by creating tensors with long-range temporal information incremented periodically through updates of a FIFO-queue, while HMNET [58] leverages a hierarchical structure with latent memories working at different rates. Furthermore, replacing RNN cells with SSM also led to speed-ups [16]. However, those models are generally outperformed by RNN-based models, where at a Small scale, ReYOLOV8s has a 2.8% better mAP than HMNet-L1, with 1.4x fewer parameters and a 3.6% better performance when compared to S5-ViT-S, which is based on SSM cells. In contrast, at the Medium scale, ReYOLOv8m has a 3.6% higher performance when compared to DTSDNet-M, with a 1.42x smaller model. ReYOLOv8m also has an average of 5.2% higher mAP than the SSM-based models S4D-ViT-B and S5-ViT-B.\nIn conclusion, by integrating recurrent cells and leveraging a robust baseline, the ReYOLOv8 models achieve higher mAP than other models of similar scales. Although they incur some latency penalties compared to non-RNN models, they require fewer parameters. However, the latencies ranging from 9.2ms to 15.5ms are still suitable for real-time operation."}, {"title": "5 Conclusion", "content": "In this work", "datasets": "PEDRO and GEN1. To address polarity imbalances in event streams, we proposed a data augmentation technique that randomly suppresses specific polarities, leading to an average mAP improvement of 0.7% for GEN1 and 4.5% for PEDRo. Compared to existing literature, the ReYOLOv8 models demonstrated a 9% to 18% improvement in MAP for PEDRO, while requiring 8.8x fewer parameters and being 1.67x faster on"}]}