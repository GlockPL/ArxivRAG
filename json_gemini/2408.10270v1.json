{"title": "SEAL: Systematic Error Analysis for Value Alignment", "authors": ["Manon Revel", "Matteo Cargnelutti", "Tyna Eloundou", "Greg Leppert"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) aims to align language models (LMs) with human values by training reward models (RMs) on binary preferences and using these RMs to fine-tune the base LMs. Despite its importance, the internal mechanisms of RLHF remain poorly understood. This paper introduces new metrics to evaluate the effectiveness of modeling and aligning human values, namely feature imprint, alignment resistance and alignment robustness. We categorize alignment datasets into target features (desired values) and spoiler features (undesired concepts). By regressing RM scores against these features, we quantify the extent to which RMs reward them a metric we term feature imprint. We define alignment resistance as the proportion of the preference dataset where RMs fail to match human preferences, and we assess alignment robustness by analyzing RM responses to perturbed inputs. Our experiments, utilizing open-source components like the Anthropic/hh-rlhf preference dataset and OpenAssistant RMs, reveal significant imprints of target features and a notable sensitivity to spoiler features. We observed a 26% incidence of alignment resistance in portions of the dataset where LM-labelers disagreed with human preferences. Furthermore, we find that misalignment often arises from ambiguous entries within the alignment dataset. These findings underscore the importance of scrutinizing both RMs and alignment datasets for a deeper understanding of value alignment.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) is used to fine-tune language models (LMs) to better align with human preferences. These preferences, collected through comparisons of LM responses, are compiled into an alignment dataset that is then used to train a reward model (RM), which is essentially a language model with a linear head. RMs predict scalar rewards consistent with human preferences and are used to update an LM's policy. The trained RM emulates human-defined desirability, enabling the LM to generalize desired behavior across unseen scenarios. Practitioners test this generalization using benchmarking, which compares LM responses to established ground truths, as well as red-teaming, where users deliberately provoke the model to find edge cases. However, these methods can be ad hoc and often uncover failures through indirect evaluations."}, {"title": "1.1 Main Contributions", "content": "This paper examines the training dynamics of RMs and the composition of alignment datasets in the RLHF pipeline ([1] in Figure 1). By treating the preferences in the alignment dataset Das ground truth, we analyze how well an RM trained on D aligns with human preferences. We introduce simple yet effective heuristics to evaluate the impact of value alignment on RMs ([2, 3] in Figure 1) and test these on an open-source alignment pipeline ([4] in Figure 1) aimed at aligning models with helpfulness and harmlessness.\nFirst, we use a state-of-the-art LM to featurize an alignment dataset D into target features (values explicitly intended to be learned) and spoiler features (unintended values learned during training). This taxonomy, combined with the RM's reward scores on the entries of D, enables us to quantify feature imprint, a metric indicating how well specific values are rewarded by the RM. Our findings reveal significant imprints of target features such as harmlessness and helpfulness, with the RM favoring these desired behaviors.\nNext, we explore alignment resistance, defined as instances where the RM disfavors entries favored by humans. We compare the behavior of the post-D RM (trained on the alignment dataset and other datasets) with a pre-D RM (an earlier model trained solely on other datasets), using the earlier model as a baseline\u00b9. Our analysis uncovers systematic post-training failures, with the post-D RM remaining misaligned with human preferences in over a quarter of the cases. Notably, in approximately one-twelfth of the cases, the post-D RM is less aligned than its predecessor.\nFinally, we assess alignment robustness, which measures the RM's sensitivity to spoiler features by analyzing its response to rewritten texts that introduce conflicting values. We find that entries rewritten in a more positive tone"}, {"title": "2 A Method to Evaluate Value Alignment", "content": "The objective of this work is to define rigorous metrics for interpreting the impact of training an RM on an alignment dataset, particularly how the RM represents values. Our approach has three main objectives: (a) quantifying how well specific features (such as helpfulness, harmlessness and eloquence) are learned, both intentionally and accidentally, by the RMs (Section 2.1); (b) identifying the causes of alignment resistance after training on D (Section 2.2); and (c) measuring the robustness of feature imprints through mild perturbations of the alignment dataset (Section 2.3).\nCore Material Our methodology centers around an alignment dataset (D) and RMs (Rs). The alignment dataset D consists of paired entries, denoted (t,t), where each entry includes a prompt pi and the model's corresponding responses a (chosen) and a (rejected). The human labeler prefers to (chosen) over t (rejected). We use t to denote an entry regardless of its chosen or rejected status. An RM R assigns a reward to entries, with a score r(t) = r(pi, a) reflecting the RM's evaluation. We analyze the RM both before and after it is trained on the alignment dataset D. We denote the pre-D RM as R and the post-D RM as R.\nExperimental Set-Up We evaluate our method on the Anthropic/hh-rlhf alignment dataset, D, which contains N = 160,800 paired entries focused on helpful and harmless imprints.\u00b2 We also use two open-source RMs trained by OpenAssistant: the pre-D RM R, trained on a corpus D composed of three semantic datasets\u00b3: web-gpt, summarize-"}, {"title": "2.1 How well does the RM learn specific features?", "content": "In this section, we introduce the concepts of target features, spoiler features, and reward shifts to define what we call feature imprint.\nTarget and Spoiler Features We define a set T of target features, which are the values the base model is intended to align with through RLHF. Additionally, we identify spoiler features, which are confounding features that the model accidentally overfit to during training. Using a text-generation LM, we create a taxonomy for each dialogue in D. For each entry i \u2208 D and each feature \u03c4\u2208T, we denote by t (7) the boolean variable indicating whether the text t is characterized by the feature \u0442.\nReward Shifts Let r(t) and (t) denote the rewards assigned by the pre-D RM R and the post-D RM R, respectively, to a piece of text t. We refer to the reward vectors (r(t), r(t)) and (r(t), r(t)) as the pre-D and post-D reward vectors, respectively. For a given pair i \u2208 D, we define \u03b8i, the angle between these vectors, as the reward shift.\nDefinition 1 (Reward Shifts). The reward shift 0 is defined as the angle between the pre-D and post-D reward vectors:\n\u03b8\u2081 = arccos( (r(t)r(t)+r(t)r(t))/\u221a((r(t)\u00b2 + r(t)\u00b2) (r(t)\u00b2 + r(t)\u00b2))) ).\nFeature Imprint We can now quantify the extent to which target and spoiler features imprint on the RMs by regressing rewards (or reward shifts) against the boolean feature indicators:\nr(t) = ai + \u2211 \u03b2t(\u03c4) + \u03b5i  (1)\n\u03b8\u03b5 = \u2211 \u03b2c(\u03c4) + \u03b2rt(\u03c4) + \u03b5i.  (2)\nwhere ai represents a fixed effect to account for prompt-specific effects, considering that most of the text in t and t is identical. The coefficient \u03b2, estimates the point increase in reward between an entry to containing feature 7 compared to an entry without it, holding all other features constant. We refer to this as the post-D imprint for value 7. Similarly, by running the same regression on r(t), we obtain the pre-D imprint, denoted as 8(r). Then, B and Br represent the"}, {"title": "2.2 Does the RM resist value alignment?", "content": "This section evaluates the RM's resistance to some human preferences by measuring the percentage of entries in D' on which the RM fails to align. We also explore potential reasons for this alignment resistance. Next, it inquires into potential reasons for alignment resistance.\nAlignment Resistance We define reward model alignment as follows: for each pair i \u2208 D, the binary variable"}, {"title": "2.3 How do mild perturbations in entries' features change the RM's alignment?", "content": "This section examines the robustness of feature imprinting in the post-D RM R through mild perturbations.\nRobustness Scores We employ an LM-rewriter to modify a subset of the paired entries of the alignment dataset, adjusting the stylistic tone while preserving the original meaning. We control for changes in semantic meaning using cosine similarity between vectors generated by a text similarity model between the original and rewritten entries. We denote any rewritten entity (e.g., t, D, d) with a hat symbol (e.g., t). The robustness score is computed as the coefficient of a logistic regression that measures the impact of label flipping on misalignment incidence. The indicator variable d(1 \u2013 \u03b4\u03b5) equals 1 when the RM was aligned with human preferences before rewriting and not after. We estimate the robustness scores \u03c0* as follows:\nlog Ps/1 \u2212 Ps = \u03b1\u2080 + \u2211\u03c0(\u03c4)t(\u03c4) + \u03c0*(\u03c4)t(\u03c4) + \u03b5i . (4)\nwhere ps = Pr[8(1 \u2013 \u03b4\u03b5) = 1] represents the probability of misalignment after rewriting, and t(\u03c4)t(\u03c4)"}, {"title": "3 Discussion", "content": "Our methodology (a) evaluates how well RMs learn desired behaviors like harmlessness and spoiler features, (b) identifies reasons for persistent alignment resistance after training, and (c) assesses the impact of minor dataset perturbations on feature imprint stability. Testing our approach on the Anthropic/hh-rlhf preference dataset and OpenAssistant RMs shows that while alignment improves rewards for desirable traits and penalties for harmful content, significant misalignment with human preferences persists. Alignment resistance may stem from several sources: (i) concept confusion within D, (ii) inconsistencies between D and the RM's training datasets, and (iii) discrepancies between the RM and its base training data.\nNotably, 73% of D's entries have Yi = i, suggesting that many entries in a pair are difficult to differentiate per the LM-labeler. Appendix D.1 further shows that the rewards assigned to each of the paired entries are remarkably similar (see illuminated diagonal in Figure 8) and manual assessments confirm that entries are often indistinguishable. Additionally, Section 2.2 indicates that good rejected entries and bad chosen entries contribute to misalignment, suggesting that the RM may correctly reward desirable features present in rejected entries (and vice versa). This could increase the incidence of misalignment, as small perturbations in a reward vector close to the diagonal can tip it from aligned to misaligned. These findings support hypothesis (i) on concept confusion within D as a significant contributor to fine-tuning failures.\nThe lack of robustness to certain spoiler features also indicates that the RM may sometimes reward the wrong features, supporting hypothesis (iii) on concept confusion between the RM and its base training data. Regarding hypothesis (ii), an RM, as a pre-trained language model, begins with an initial semantic representation based on its pre-training data, which is reshaped during retraining. We posit that the LM-labeler's agreement with the RM on alignment resistance suggests a shared latent representation of these features. This observation may indicate a relationship between the compositions of the pre-training and fine-tuning data. However, without access to the pre-training data, we cannot test this hypothesis directly.\nLimitations Our methodology depends on the taxonomy labels used to evaluate alignment. Robustness checks in Appendix D.5 indicate that some labels may be unstable when assessed by different LM-labelers. Although we believe these labels are at least as reliable as human labels (Gilardi, Alizadeh, and Kubli 2023), the issue of label quality is not unique to our study and requires ongoing scrutiny to avoid circularity when using LMs to assess LM alignment.\nAdditionally, our approach does not systematically identify and define different spoiler features. While some features may be universally applicable across various pipelines, specific contexts might necessitate the development of more tailored frameworks to accurately detect and address potential confounding factors in RM behaviors. Future work should focus on identifying and managing these features to enhance the efficacy of alignment pipelines.\nSystematic error analyses are also needed to explore how various elements of the alignment pipeline interact. This work examines the interconnections between an alignment dataset and a series of RMs as a first step in this direction. High-quality taxonomy labels could accompany the entries of the alignment dataset alongside human or synthetic preferences. These labels would help ensure that spoiler features are balanced across value targets and that human preferences are internally consistent. They would also provide a priori and testable objectives for feature imprint, enabling rigorous measurement and mitigation of the impact of spoiler features through additional training.\nFuture work The pre-D RM was trained on a corpus of three semantic datasets (web-gpt, summarize from feedback, and synthetic-instruct-gptj-pairwise) designed to train RMs on semantic tasks. Resistance to alignment on these tasks is also observed and can be studied using our proposed method (resistance incidences of 49% and 66% are observed with web-gpt and summarize from feedback, respectively).\nNext, the importance of having a high-quality alignment pipeline becomes paramount as powerful base models are open-sourced. To the best of our knowledge, the combination of the Anthropic/hh-rlhf alignment dataset and the OpenAssistant RMs are among the most popular alignment tools on Hugging Face and they were crucial for improving our understanding of alignment dynamics in this work. We hope that such efforts will support the development of even better open-source alignment pipelines, and we would be excited about new research that releases and scrutinizes both datasets and openly shared RMs.\nIn conclusion, we posit that alignment datasets and RMs are crucial for providing granular interpretations of value"}]}