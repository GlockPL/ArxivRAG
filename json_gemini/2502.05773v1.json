{"title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation", "authors": ["Junbo Li", "Zhangyang Wang", "Qiang Liu"], "abstract": "Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data set-settings, yet they lack a unified understanding. In this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a 3 ~ 10% performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms.", "sections": [{"title": "Introduction", "content": "Pre-training large language models (LLMs) from scratch on trillions of text tokens allows for accurate prediction next tokens in natural language. Following this, alignment, achieved through fine-tuning on smaller, high-quality datasets designed for specific tasks, becomes critical for enabling the model to develop specialized skills, such as engaging in conversation, math reasoning, coding, web agent, and more. The fundamental approach to alignment involves supervised fine-tuning (SFT) on the target domain, which essentially maximizes the likelihood of predicting the next token. However, numerous empirical studies have shown that simple SFT on preferred samples is inadequate for attaining optimal performance.\nMoving beyond basic imitation learning in SFT, it is suggested to learn from both positive and negative samples. Sample quality can be measured by training reward models to capture general preferences or leveraging accurate rule-based rewards for specific tasks like math and coding. By treating the autoregressive generation of LLMs as a Markov decision process (MDP), traditional reinforcement learning (RL) algorithms can be effectively applied, such as PPO, SAC, REINFORCE, etc.\nWhile online RL-based methods deliver strong performance, they face challenges such as high training costs, instability, and the need for a strong base model as the initial policy. As a result, offline algorithms like direct preference optimization (DPO) are often preferred, thanks to their effectiveness and simplicity, particularly when high-quality datasets are accessible. The original DPO algorithm has several limitations. It relies on paired preference data, which is not essential for tasks with ground truth such as math and coding. Additionally, it is unable to accommodate step-level annotations. Furthermore, it treats all tokens equally, lacking token-level credit assignment. To address these issues, a series of approaches have been developed, such as Kahneman-Tversky Optimization (KTO) for unpaired data, Step-DPO and Step-KTO for step-level annotations, and RTO ,TDPO, and OREO for fine-grained token-level DPO. However, these methods are designed from specific perspectives, each addressing only particular challenges, and they lack a unified understanding to integrate their solutions.\nIn this work, we introduce a unified framework designed to address all the aforementioned challenges"}, {"title": "Related Work", "content": "Learning from preference data RL has become a key framework for leveraging preference data for LLM alignment, with early methods like PPO, which first trains a reward model on pairwise human feedback. Due to PPO's high training cost, direct policy optimization methods without online RL have been explored, integrating policy and reward learning into a single stage. Notable works include DPO, SLiC, IPO ,GPO, and SimPO. For fine-grained token-level optimization, DPO variants like TDPO, TIS-DPO, RTO, and OREO have been introduced. To address step-level annotation inspired by PRM, methods such as Step-DPO, SCDPO, and SVPO have emerged. To relax pairwise data constraints, particularly for tasks with ground truth like math and coding, KTO, Step-KTO, and OREO have been proposed. Our PIPA framework addresses all these challenges within a unified paradigm, demonstrating that existing algorithms like DPO and KTO can be interpreted as special cases within our approach.\nProbabilistic alignment In addition to reward maximization, some research approaches alignment from a probabilistic perspective. decompose label likelihood into a target distribution and a hidden distribution, solving it using the EM algorithm. Other works leverage importance sampling to train a policy parameterized by an energy-based model that aligns with the target distribution including DPG, GDC, GDC++, BRAIn. Unlike these methods, our PIPA framework maximizes label likelihood while enforcing prior constraints by transforming it into the target distribution using Bayes' Theorem. We directly learn the distributions without relying on complex sampling and estimation procedures. PIPA is scalable, incurs no additional training cost, and remains flexible across any preference data."}, {"title": "PIPA", "content": "We introduce the prior-informed preference alignment (PIPA) framework in Section 2.1, followed by the first version, PIPA-M, in Section 2.2. Next, we explore its connection to DPO and KTO in Section 2.3. Drawing inspiration from DPO and KTO, we develop the second version, PIPA-N, detailed in Section 2.4. In Section 2.5, we extend PIPA-M and PIPA-N naturally to incorporate step-level annotations. Finally, Section 2.6 presents the refined algorithms for PIPA-M and PIPA-N and compares them with prior methods like \u039a\u03a4\u039f."}, {"title": "Prior-Informed Preference Alignment", "content": "Problem We define the preference alignment problem as probabilistic estimation. Assume that we are given a preference dataset\n$\\lbrace (x^i, y^i, c^i)\\rbrace_{i \\sim p_{data}}$,\nwhere x is the instruction input, y' is the answer, and $c^i \\in \\lbrace 0,1 \\rbrace$ represents the preference or correctness of the answer. We are interested in predicting y given x in the correct case (c = 1). This amounts to estimating conditional probability:\n$p(y | x, c = 1)$.\nThe canonical approach for estimating $p(y | x,c = 1)$ is, of course, the maximum likelihood estimation (MLE), which yields supervised finetuning (SFT) on the positive examples with c = 1."}, {"title": "PIPA-M: Enforcing Prior on Marginal Distribution", "content": "We first consider a straightforward case when we know that the marginal prediction $p(y | x)$ should match a prior distribution $p_{prior}(y | x)$ defined by the pretrained LLM. Because the marginal distribution is a sum of the positive and negative probabilities, that is,\n$p(y | x) = p(y | x, c = 1)p(1|x) + p(y | x, c = 0)p(0|x)$,\nwhere we abbreviate $p(c = ix)$ as $p(ix)$. The estimation of the positive and negative probabilities are coupled.\nIn this case, the estimation problem is in principle formulated as a constrained maximum likelihood problem:\n$\\min_{p} E [KL(p_{data} (y, c | x) || p(y, c | x))]$.\ns.t. $p(y | x) = p_{prior}(y|x)$,\n$\\forall x,y$.\nThis is by definition is the statistically the best way to estimate p provided the data information $p_{data}$ and the prior constraint $p_{prior}$.\nAs is the typical case, the KL divergence minimization reduces to maximizing the log-likelihood objective $log p(y, c|x)$ in practice, which is equivalent to maximizing $log p(c | y, x)$ given that $p(y|x)$ is fixed by the prior constraint.\nThe main difficulty is to handle the constraint. Recall that our final goal is to estimate $p(y | x, c = 1)$, which is expected to be parameterized with autoregressive transformer model. Hence, we are going to parameter the joint distribution p using $p(y | x, c = 1)$ and $p(y | x)$.\nTheorem 2.1. Assume\n$p(y | x, c = 1) = f_\\theta (y | x), p(c = 1 | x) = g_\\theta(x)$,"}, {"title": "DPO and KTO: Prior-Informed Views", "content": "We analyze existing methods, such as DPO and KTO, through the lens of the prior-informed estimation framework (1). Our analysis demonstrates that both DPO and KTO can be interpreted as enforcing prior assumptions on the negative probability, $p(y | x, c = 0)$, rather than the marginal probability, $p(y | x)$. However, these methods differ in their choice of loss functions, the prior assumptions made about $p(cx)$, and the parameterization employed.\nDPO Direct preference optimization (DPO) and related methods are often cast as estimating the log density ratio $log(p(y | x,c = 1)/p(y | x, c = 0))$ of the positive and negative generation probabilities. Related, it may not be of suprise that these models make the implicit assumption on the negative (reference) probability $p(y | x, c = 0) = p_{prior} (y | x)$. In particular, DPO can be formulated as\n$\\max \\mathcal{L}_{DPO} (p^\\theta, p_{data})$\ns.t. $p^\\theta (y | x, c = 0) = p_{prior} (y | x)$,\n$\\frac{1}{2'}$, $ \\forall x, y$.\nwhere the loss $L_{DPO}$ is a special pairwise comparison loss related to Bradley-Terry model provided paired data $\\{x,y^+,y^-\\}$, of both positive answer $y^+ \\sim p(y | x, c = 1)$ and negative answer $y^- \\sim p(y | x, c = 0)$ for each x; the assumption of $p^\\theta(c = 1 | x) = 0.5$ is due to the balanced sampling of positive and negative weights. See Appendix A.1 for more discussion for $L_{DPO}$ and proof of equivalence."}, {"title": "PIPA-N: Enforcing Prior on Negative Condition Distribution", "content": "Knowing the prior informed formulation of DPO and KTO, we can propose changes to make them simpler and more natural. One option is to keep the conditional log-likelihood loss function of KTO, but seeks to learn $p^\\theta(c = 1|x)$ as a neural network $g^\\theta(x)$ from data, rather than making the heuristic assumption. This yields\n$\\max_\\theta E_{p_{data}} [log p^\\theta (c|x, y)]$\ns.t. $p^\\theta (y | x, c = 0) = p_{prior} (y|x)$. $ \\forall x, y$.\nThis differs from PIPA-M in (2) in the prior constraint. We call this PIPA-N, for it places prior on the negative probability. Using the same parameterization of $p^\\theta (y | x, c = 1) = f^\\theta (y|x)$, and $p^\\theta (c = 1 | x) = g(x)$, we have"}, {"title": "Extension to Step-level Settings", "content": "The introduction above is about the answer-level setting, which uses a single label for an entire solution even if multiple steps may vary in correctness. In contrast, the step-level setting assigns separate labels to each step. The advantage of our probabilistic framework is that it seamlessly adapts to a step-level setting for both PIPA-M and PIPA-N algorithms. The core idea is intuitive. We use token-level labels rather than answer-level labels, followed by decomposing the joint probability autoregressively."}, {"title": "Practical Implementation", "content": "As shown in the formulation of Section 2.5, we always use PIPA-M and PIPA-N with token-level label representation in practice which provides fine-grained information.\nWe have three models in total: $f_\\theta, g_\\theta,p_{prior}$. In practice, $f_\\theta$ is the target language model initialized from the one obtained after Supervised Fine-Tuning (SFT) and will be used for inference after training. $g_\\theta$ shares exactly the same base model with $f_\\theta$ differing only in the output head. $p_{prior}$ is also a language model but frozen. We show our PIPA-M and PIPA-N in Algorithm 1, and a figure illustration in Figure 1. For the negative samples in PIPA-M, we apply a clipping function to constrain the term $f_\\theta g_\\theta/p_{prior}$ within [0, 1 \u2013 \u025b], where \u03b5 = 10\u221210.\nComparison with KTO Although it stems from a completely different derivation, KTO [Ethayarajh et al., 2024]\u2014which also targets pair-free alignment\u2014is the closest prior work to PIPA in terms of its algorithm. A detailed analysis of their differences and PIPA's advantages can be found in Appendix A.2.1.\nCredit assignment A key issue with traditional DPO and KTO loss is that all tokens receive equal treatment, since only the answer-level reward is considered. PIPA offers a natural way to weight $f_\\theta (Y_t | X,Y_{<t})$ differently by jointly optimizing it with $g_\\theta(x,y_{<t}) = p^\\theta (c_t = 1 | x, y_{<t})$. The optimized $g_\\theta$, with its clear probabilistic interpretation, can be viewed as a value function and may be used for inference-time search in future work. We present its learning trajectory in Section 3.3.1.\nCompatibility and flexibility PIPA can be applied whenever answer- or step-level annotations are available, requiring no additional training stage. These step-level annotations can be derived via MCTS"}, {"title": "Settings", "content": "Our PIPA framework is capable of handling scenarios both with and without preference pairs, as well as with or without step-level annotations. Consequently, we evaluate it across four distinct experimental setups determined by (pair, unpair) \u00d7 (answer, step).\nBaseline algorithms For paired preference data, we use DPO and its variant IPO and KTO as baselines. Both KTO and PIPA decouple the paired data. For data without preference pairs, we compare PIPA with KTO. In answer-wise settings, we benchmark PIPA against DPO, IPO, and KTO. In step-wise settings, we compare PIPA with Step-DPO and Step-KTO . The original Step-DPO and Step-KTO methods involve additional data generation phase. For a fair comparison on an offline dataset, we extract only their loss functions. See Appendix B for detailed descriptions of the baseline algorithms.\nData We use the unpaired preference dataset for math reasoning released by AlphaMath, which includes training problems from GSM8K and MATH with both CoT and TIR-style solutions, along with step-level label annotations. There are more incorrect answers in this dataset, so we construct the paired subset by matching each correct solution to a corresponding incorrect solution from the same problem and discarding the remaining incorrect solutions. We use the entire original dataset for the unpaired setting.\nFor the answer-level setting, we only keep the final label for the answer. In the step-level setting, steps of a correct answer are always correct. For incorrect answer, we label steps whose Q values fall within [-1,0.5) as incorrect, and those in [0.5,1] as correct with a threshold 0.5. Instead of a threshold 0, the intuition of this shifted threshold is that it's better to be conservative for the correct steps in the wrong answer. Despite being correct, some steps may still contribute to an incorrect overall analysis. Therefore, minimizing the likelihood of such steps is also crucial. The efficacy of this choice is further explored in the ablation study of Section 3.3.2.\nFor our evaluation, we use the standard GSM8K and MATH benchmarks. We adopt the MARIO evaluation toolkit , configuring the beam search width and number of generated samples, i.e., (B1, B2) in their notation, to be (3, 1) for GSM8K and (1,1) for MATH.\nModel The AlphaMath dataset is generated using Deepseek-based models, so we use Deepseek-Math-7B-Instruct as the pre-trained model for self-alignment. Additionally, to evaluate generalization capabilities, we test Qwen2.5-Math-7B-Instruct as the base model on the same dataset. For PIPA, we set the head of go to be a two-layer MLP followed by a Sigmoid function, with hidden dimension 4096 same as the base model.\nTraining All experiments are conducted based on OpenRLHF. For all training, we use LORA with rank 64 and a = 16. All alignment algorithms are conducted for 1 epoch after the SFT stage. Denote bs to be the batch size and lr to be the learning rate. we do grid search for lr \u2208 {5 \u00d7 10-7,5 \u00d7 10-6,5 \u00d7 10-5} for all experiments and present the best one.\n\u2022 SFT Before all alignment algorithms, we first finetune the pre-trained Deepseek and Qwen models on the positive samples for 3 epochs with bs = 1024 and lr = 4 \u00d7 10-5. The model obtained after SFT is then used as the initilization for the target model fo in alignment procedures, as well as the fixed reference model in DPO and KTO. Furthermore, to avoid extra computation, this same model serves as the prior in both PIPA-M and PIPA-N, ensuring that PIPA does not require an additional training phase compared to DPO and KTO.\n\u2022 DPO For DPO-based algorithms including DPO, IPO, Step-DPO, we train 1 epoch after the SFT stage, with bs = 256, lr = 5 \u00d7 10-7 and \u03b2 = 0.1.\n\u2022 KTO For KTO, we set lr = 5\u00d710-5 for Deepseek model and lr = 5 \u00d7 10\u22127 for Qwen model. For both, bs = 256, \u03b2 = 0.1. Step-KTO shares exactly the same recipe with KTO.\n\u2022 PIPA We set bs = 256 for all four settings, lr = 5 \u00d7 10-5 for Deepseek and 5\u00d710-7 for Qwen. All settings are the same as KTO and Step-\u041a\u0422\u041e, without additional hyperparameters to be tuned."}, {"title": "Main Results", "content": "We show our main results in Table 1. We can see that for all four settings and two models, PIPA achieves the best performance without additional computation cost."}, {"title": "Additional Analysis and Ablation Studies", "content": "We conduct a more detailed analysis from two perspectives. Section 3.3.1 delves into further studies on our PIPA itself. Section 3.3.2 examines the step-level and answer-level settings."}, {"title": "Algorithms", "content": "PIPA-M vs. PIPA-N PIPA-M and PIPA-N are two versions of our framework that incorporate distinct prior constraints. As shown in Table 1, neither variant consistently outperforms the other. Notably, PIPA-N tends to perform better with the Deepseek model, while PIPA-M shows superior results with the Qwen model. This may suggest that PIPA-N is better suited for self-alignment scenarios, while PIPA-M is more effective for alignment tasks where there is a distribution shift between the model and the data. In practice, we recommend experimenting with both variants to determine the optimal choice for your specific use case."}, {"title": "Step-level setting", "content": "As our research is the first to systematically explore the performance of alignment algorithms across various settings, we provide an in-depth analysis of the step-level setting in this section. We aim to understand the advantages of step-level annotation and how it influences the training.\nInfluence of step-level annotation From Table 1, we observe that step-level annotation does not consistently improve performance when comparing answer-wise and step-wise annotation within the same algorithm and dataset. Specifically, step-level annotation proves beneficial for MATH but can sometimes negatively impact GSM8K. This finding aligns with previous studies , suggesting that step-level annotation is more advantageous for challenging reasoning tasks like MATH but may be unnecessary or even harmful for simpler tasks like GSM8K."}]}