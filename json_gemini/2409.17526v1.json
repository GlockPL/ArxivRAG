{"title": "Drone Stereo Vision for Radiata Pine Branch Detection and Distance Measurement: Integrating SGBM and Segmentation Models", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "abstract": "Manual pruning of radiata pine trees presents significant safety risks due to their substantial height and the challenging terrains in which they thrive. To address these risks, this research proposes the development of a drone-based pruning system equipped with specialized pruning tools and a stereo vision camera, enabling precise detection and trimming of branches. Deep learning algorithms, including YOLO and Mask R-CNN, are employed to ensure accurate branch detection, while the Semi-Global Matching algorithm is integrated to provide reliable distance estimation. The synergy between these techniques facilitates the precise identification of branch locations and enables efficient, targeted pruning. Experimental results demonstrate that the combined implementation of YOLO and SGBM enables the drone to accurately detect branches and measure their distances from the drone. This research not only improves the safety and efficiency of pruning operations but also makes a significant contribution to the advancement of drone technology in the automation of agricultural and forestry practices, laying a foundational framework for further innovations in environmental management.", "sections": [{"title": "I. INTRODUCTION", "content": "Pinus radiata, commonly known as radiata pine, is a highly valuable species extensively cultivated in New Zealand due to its rapid growth and versatile applications in forestry and timber industries. This species is essential for producing high-quality timber used in construction, paper manufacturing, and other wood-based products, significantly contributing to the economy [2] [1]. For instance, in New South Wales, Australia, the radiata pine industry contributed approximately $3 billion to the economy in 2021-2022, highlighting its economic importance\u00b9. However, to ensure the trees grow with strong, straight trunks and produce clear wood, which doesn't have knots, regular pruning is necessary. Traditionally performed manually, tree pruning and trimming are hazardous occupations globally, posing significant challenges and dangers. According to Tree Care Industry Magazine\u00b2, in the United States alone, the Bureau of Labor Statistics reports a fatality rate of 110 per 100,000 tree trimmers and pruners, which is about 30 times higher than the average across all industries. Moreover, non-fatal injury rates for tree workers are also substantially higher, at approximately 239 injuries per 10,000 workers, compared to 89 per 10,000 across all industries. It is also challenging to find people who want to do the work because it is hard and dangerous.\nTo effectively replace manual labor in branch pruning, we aim to develop a fully autonomous drone system. Existing drone pruning systems typically require manual operation and are limited to cutting only thicker branches. Additionally, they often rely on expensive auxiliary equipment such as LiDAR sensors, which significantly hinders their widespread adoption [3]. To overcome these limitations, we propose the development of a drone equipped with a stereo camera and a pruning tool capable of automatically detecting and pruning branches as thin as 10mm in diameter. This system utilizes the stereo camera for both branch identification and distance measurement, enabling fully autonomous pruning operations. By streamlining the design and enhancing sensor technology, our approach aims to make drone-based pruning more accessible and cost-effective, thereby improving precision and efficiency in forestry management. This advancement in autonomous drone technology not only enhances forestry practices but also offers a versatile and economical solution for various applications beyond forestry, eliminating the need for extensive manual control or expensive auxiliary equipment.\nTo ensure this research is comprehensive and self-contained, it is imperative to include detailed information on all relevant aspects, particularly the structural components of the drones utilized in this research. For a comprehensive overview of these components, please refer to the detailed information on drones available at https://ucvision.org.nz/drones/. This link provides critical insights into the design and specifications of the drones, with their testing process depicted in Fig. 1.\nThe big research project is a collaborative effort involving multiple institutions, each contributing specific expertise. While other institutions focus on the physical construction of the drones, our research is concentrated on developing the vision detection and measurement algorithms for the cameras mounted on these drones. These algorithms are pivotal for accurately detecting branches and determining their precise"}, {"title": "II. RELATED WORK", "content": "We propose employing a computer vision system to simultaneously perform branch detection and distance measurement tasks, offering a more efficient and cost-effective alternative [17]. Therefore, our research focus will concentrate on a standalone computer vision approach. Building on these advancements, we are committed to developing a novel method for real-time branch detection and depth estimation, enabling drones to accurately determine the spatial positions of tree branches. By leveraging advanced stereo vision techniques optimized for computational efficiency, this method aims to provide a scalable, cost-effective solution that enhances pruning precision while reducing the equipment load on drones [23].\nImplementing this approach necessitates addressing several key components: accurate object detection and image segmentation to identify tree branches, generation of depth maps to estimate spatial positions, and the establishment of robust performance metrics to evaluate system efficiency and accuracy. In the following subsections, we delve into these components in detail, reviewing existing methodologies and elucidating how our proposed solutions contribute to and advance the current state of the field."}, {"title": "A. Object detection and image segmentation", "content": "Object detection [17] and image segmentation [18] are both critical tasks in computer vision area. Object detection primarily focuses on identifying and locating objects within an image, typically by marking their positions with bounding boxes. Segmentation, on the other hand, takes this a step further by dividing the image into distinct regions, accurately delineating the shape and boundaries of objects.\nIn this research, the focus extends beyond merely identifying the positional information of tree branches to include the acquisition of detailed locational data of surrounding points. This requirement necessitates a transition from conventional object detection methods to more precise image segmentation techniques [19]. By utilizing segmentation on drone-captured imagery, this research seeks to accurately ascertain the precise location of tree branches and their neighboring regions.\nThe evolution of object detection and image segmentation has been marked by significant advancements since the introduction of the Region-based Convolutional Neural Network (R-CNN) in 2014 [4]. R-CNN represented a leap forward in detection accuracy by utilizing candidate regions for feature extraction and classification. Following this, the Spatial Pyramid Pooling Net (SPP-Net) [5] addressed the issue of fixed input size, allowing networks to retain more spatial information and thus improving the efficiency of the feature extraction process.\nFurther developments include Fast R-CNN [6], which enhanced both training speed and effectiveness by integrating the ROI Pooling layer, enabling feature extraction directly on the feature map. Faster R-CNN introduced the Region Proposal Network (RPN) [7], which allowed for the generation of candidate regions and feature extraction to share computational resources, thereby significantly improving both speed and accuracy. Mask R-CNN [8] added an additional branch for generating object masks, enabling pixel-level segmentation.\nFor real-time object detection, the YOLO series [10] [11] has established itself as a highly influential framework within both industrial and academic contexts, primarily due to its remarkable speed and precision. Considering its applicability to drone-based operations, we will prioritize the evaluation of the latest YOLO algorithm in our forthcoming experiments. Additionally, we will compare it with Mask R-CNN to determine the most effective solution for our needs."}, {"title": "B. Depth Map", "content": "Depth map generation [16] is also another crucial aspect of computer vision, enabling the inference of a scene's three-dimensional structure from one or more images. In our drone application, equipped with a stereo camera, depth maps are obtained from two distinct viewpoints. For precise pruning of branches using a pruning tool mounted on the drone, accurately identifying the tree branches and determining their distance from the drone is essential.\nDepth map, representing the distance from each pixel in the image to the camera, are generated using either active or passive methods. Active methods employ sensors that emit and receive signals to measure depth, including technologies such as LiDAR, structured light [40], and time-of-flight cameras [42]. Conversely, passive methods rely on existing optical information, utilizing techniques such as stereo matching [43], multi-view geometry, and monocular depth estimation [44]."}, {"title": "C. Performance Metrics", "content": "In assessing the performance of the object detection and segmentation model, two critical metrics must be considered: computational efficiency (measured by running time) and accuracy. For object detection and segmentation, accuracy is evaluated using mAP50-95 (Mean Average Precision at Intersection over Union thresholds ranging from 50% to 95%). For the depth estimation component, accuracy is measured using the Root Mean Square Error (RMSE) to quantify the depth prediction error.\nLet $AP(t)$ denote the Average Precision at a specific IoU threshold $t$, where $t$ represents the IoU threshold varying from 0.5 to 0.95 in increments of 0.05. The variable $n$ denotes the"}, {"title": "III. METHODS", "content": "In this section, we systematically progress from data collection and image instance segmentation to the application of both traditional and deep learning techniques for depth map generation. By integrating these approaches, we achieve accurate detection of tree branches and estimate their distances using only a stereo vision camera. The entire workflow is illustrated in Fig.3."}, {"title": "A. Data Collection and Image Instance Segmentation", "content": "In this research, we primarily collected indoor data using a ZED Mini camera\u00b3, capturing images from different corners of the laboratory at a resolution of 1920\u00d71080. We photographed various tree branches under different lighting conditions to avoid over-idealization of the training images. So far, we have collected 61 pairs of photos (i.e., 122 images) for the training dataset and another 10 pairs for the test dataset.\nAfter collecting the data, our process began with image labeling, specifically annotating the points around each branch. This step was essential for accurate segmentation. Given the relatively small size of the test dataset, it was critical to perform robust model testing to validate the feasibility of our approach. We initiated our experiments using Mask R-CNN, employing several backbone architectures such as ResNet-50, ResNet-101, and ResNeXt-101-32x8d. These models varied in complexity and were chosen to explore different trade-offs between speed and accuracy. The Feature Pyramid Network (FPN) and Dilated-C5 (DC5) architectures were also evaluated to assess their performance in generating masks and predicting bounding boxes.\nAfter completing the Mask R-CNN tests, we proceeded to evaluate the dataset using YOLOv8 and YOLOv9 models of varying sizes. Once predictions were generated, we assessed the accuracy of each model using mAP50-95. In terms of computational efficiency, both Mask R-CNN and the YOLO series demonstrated impressive inference speeds, with an average processing time of approximately 10 ms per image. This performance underscores their potential for deployment in real-time applications. A comprehensive analysis of the accuracy results will be presented in the IV-A section."}, {"title": "B. SGBM for Generating Depth Map", "content": "In contrast to BM, which generates disparity maps by dividing stereo images into small blocks and performing matching along individual scan lines, SGBM optimizes the matching cost by aggregating information across multiple directions. This semi-global approach enhances accuracy and consistency by mitigating errors in textureless regions and around sharp object boundaries, resulting in a more refined and coherent disparity map. To further improve the output, Weighted Least Squares (WLS) post-processing was applied to smooth the disparity map while preserving critical edge details. The final disparity map was then converted into a depth map using Equation (10), with the corresponding results discussed in the IV-B section."}, {"title": "C. Integration of Image Instance Segmentation and Depth Map Generation", "content": "In the earlier sections, we have thoroughly discussed both instance segmentation and depth estimation as distinct, standalone tasks. However, our ultimate objective is to enable the stereo camera mounted on the drone to simultaneously perform instance segmentation and depth estimation in order to precisely determine the spatial positions of tree branches. To achieve this, it is necessary to integrate the segmentation model and the depth map generation method.\nIn Fig. 4, we begin by applying a segmentation model to extract information about the points surrounding the tree branches. These points are then connected to form a continuous surface, and the coordinates of all points within this surface are mapped to their corresponding locations for depth map generation. Consequently, the depth values of all pixels on the branches are determined. Statistical analysis is then performed, and the final distance between the camera and the branches is calculated by averaging the depth values from the range where the pixel density is highest."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "This section presents the results of the image segmentation and depth estimation processes, followed by an integrated analysis of their combined output. A comprehensive evaluation"}, {"title": "V. CONCLUSIONS", "content": "This research underscores the critical importance of computer vision techniques in accurately detecting tree branch depth information, which is essential for precision drone-assisted pruning. The research focuses on two primary components: branch detection and segmentation, and depth map generation. In the detection phase, various architectures of Mask R-CNN and YOLO were compared, with YOLO ultimately selected for its superior performance. For depth map generation, a comprehensive analysis revealed that SGBM provided satisfactory accuracy and robustness. While deep learning approaches can capture intricate features through complex neural networks, SGBM was chosen for its efficiency and reliability in our application. By integrating advanced branch detection with accurate depth maps generated by SGBM, the research enables precise measurement of distances between branches and the drone, facilitating more accurate and efficient pruning operations."}]}