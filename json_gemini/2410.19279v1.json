{"title": "UbiHR: Resource-efficient Long-range Heart Rate Sensing on Ubiquitous Devices", "authors": ["Haoyu Bian", "Bin Guo", "Sicong Liu", "Yasan Ding", "Shanshan Gao", "Zhiwen Yu"], "abstract": "Ubiquitous on-device heart rate sensing is vital for high-stress individuals and chronic patients. Non-contact sensing, compared\nto contact-based tools, allows for natural user monitoring, potentially enabling more accurate and holistic data collection.\nHowever, in open and uncontrolled mobile environments, user movement and lighting introduce noises. Existing methods,\nsuch as curve-based or short-range deep learning recognition based on adjacent frames, strike the optimal balance between\nreal-time performance and accuracy, especially under limited device resources. In this paper, we present UbiHR, a ubiquitous\ndevice-based heart rate sensing system. Key to UbiHR is a real-time long-range spatio-temporal model enabling noise-\nindependent heart rate recognition and display on commodity mobile devices, along with a set of mechanisms for prompt and\nenergy-efficient sampling and preprocessing. Diverse experiments and user studies involving four devices, four tasks, and 80\nparticipants demonstrate UbiHR's superior performance, enhancing accuracy by up to 74.2% and reducing latency by 51.2%.", "sections": [{"title": "1 INTRODUCTION", "content": "Physiological health sensing anytime and anywhere is important for users, encompassing metrics like heart\nrate [23, 60], blood oxygen [25], blood pressure [24], etc. Existing methods include hospital-grade and consumer-\ngrade solutions, both contact-based and non-contact. However, discomfort is a common issue with contact-based\noptions like specialized vests [38] and fingertip monitors [29], while wearable devices such as watches [36],\nwristbands worn on the wrist [3, 9, 75], and glasses [13] impose limitations by compressing the skin and affecting\ndaily activities. There is a need for a ubiquitous, non-contact heart rate sensing tool utilizing mobile and embedded\ncameras. This allows for user monitoring in natural settings, leading to potentially more accurate and holistic\ndata collection. For instance, camera-based heart rate sensing can aid law enforcement in detecting truthfulness\nduring confessions without awareness or psychological resistance, and enabling convenient health monitoring\nfor elderly individuals living alone.\nMeanwhile, the pervasive deployment of camera-embedded devices e.g., smartphones, wearables, tablets,\nand robots has stimulated a wide spectrum of novel video-based physiological applications. Examples include\nMobilephys on smartphones [52] and TS-CAN on embedded platforms [50]. Despite extensive research on\ncamera-based heart rate sensing [11, 58, 74, 80, 83, 84], they are unfit for deployment to ubiquitous mobile and\nembedded devices because they fail to meet the following requirements.\n\u2022 Low-cost end-to-end heart rate sensing on resource-constrained ubiquitous devices is non-trivial for practical\napplications. While video-based remote photoplethysmography (rPPG) measurements are commonly used\nfor heart rate sensing, privacy concerns dictate on-device preprocessing and processing [45] instead of cloud\nstreaming. However, enabling this capability on low-end mobile and embedded devices faces challenges due\nto high computing overhead. Previous studies rely on hand-crafted methods [11] or complex pre-processing\ntechniques, such as retaining only the facial region of interest (RoI) through face detection [26], dividing the\nimage into small blocks/channels [54, 72], or constructing a 3D model [44], assuming given pre-processed\ndata. As demonstrated in Sec. 4, these methods struggle to realize high accuracy with low on-device latency.\n\u2022 Real-time spatio-temporal heart rate sensing from noisy video streams poses a significant challenge. Unlike\ntraditional analysis of signals acquired by contact-based devices, which focuses on prominent changes,\nfacial rPPG measurement requires capturing subtle facial movements and color variations, complicating\nglobal spatial-temporal recognition. Also, adapting existing curve-based models, such as color space\nprojection [6, 51] and skin reflection [76], to dynamic open environments (e.g., various light conditions [49])\nproves challenging, as none of them consider rPPG signal periodic properties in long-range sensing [84].\nFactors such as user head motions, lighting fluctuations, and equipment noise further complicate the\nrecognition process. Addressing these challenges necessitates intricate long-range and noise-robust spatio-\ntemporal DL models, which are unsuitable for resource-constrained ubiquitous devices.\nIn this paper, we present UbiHR, a real-time and continuous on-device heart rate sensing system for ubiquitous\nmobile and embedded devices with long-range video streams. To ensure user-friendly design and reduce bias,\nwe conduct pre-design surveys with 80 participants, including 30 students and 50 patients, to understand user\ndemands. Achieving real-time and high-accuracy performance demand with non-stationary noisy videos on low-\nend resource-limited devices poses significant challenges. Our key insight is that detecting on long-range is more\nconducive to detecting the inherent periodic characteristics and trends of heart rate, enhancing the ability to resist\nenvironmental noise. Specifically, UbiHR incorporates three modules. First, an adaptive duty-cycling facial video\nsampling module dynamically switches between sleeping, short-term listening, and long-term listening schemes\nto balance energy consumption and promptness of facial key point detection. Second, a dynamic noise-aware\nfacial image pre-processing module extracts essential facial key points through differentiation and normalization,\neliminating noise from user head motion and varying environmental lighting conditions while reducing input\nvolume for subsequent real-time processing. Third, a fast long-range spatio-temporal heart rate recognizing module\nintegrates a zero-parameter time shift strategy into 2D convolution for lightweight spatio-temporal modeling.\nIt extracts both long-range and short-range spatio-temporal features from the heart rate signal using parallel\nmulti-branch structures and corrects accuracy using soft-attention masks.\nWe implement UbiHR as a python package that can be easily integrated into other applications on four typical\nmobile and embedded devices, i.e., IQOO9 smart phone, Raspberry, Jetson orin, and Jetson AGX. We evaluate"}, {"title": "2 USER STUDY", "content": "To understand the needs and usage preferences of ubiquitous users regarding a camera-based heart rate sensing\ntool, we designed a questionnaire and distributed it to 80 users from our school and a hospital cardiology\ndepartment. Participants' ages ranged from 20 to 76 years old, as shown in Fig. 1 Permission was obtained from\nboth the hospital and participants, and the questionnaire included multiple-choice and open-ended questions.\n2.1 Survey Findings\nWe briefly summarize the results (as shown in Fig. 2) of our survey as follows.\n2.1.1 User Demands on Fast and Long-term Heart Rate Sensing. Through surveys, various groups of individuals\nrequire frequent and long-term heart rate sensing: i) Patients with cardiovascular, hypertension, and diabetes\nconditions need it to assess treatment effectiveness, evaluate blood pressure control, assess cardiovascular risk, and\nprevent complications. For instance, individuals with arrhythmia need continuous monitoring to assess treatment\neffectiveness, evaluate blood pressure control, assess cardiovascular risk, and prevent complications. Long-term\nheart rate sensing aids in the timely detection and treatment of potential heart issues, mitigating the risk of\ncardiac events, e.g., too fast (ventricular tachycardia), too slow (high degree atrioventricular block), or irregular\n(paroxysmal atrial fibrillation). ii) Elderly users require it to assess cardiovascular health and promptly detect and\nmanage potential issues. As people age, the risk of cardiovascular diseases in the elderly rises, making timely\ncardiovascular health assessments crucial. It helps doctors to timely perform further examinations and diagnoses,\nfacilitating early detection and treatment of potential cardiovascular issues. iii) Athletes and fitness enthusiasts\nneed it to continuously evaluate exercise intensity and optimize training plans to prevent overtraining. Unlike\ntraditional methods, contactless heart rate monitoring eliminates the need for athletes to wear contact-based\ndevices, ensuring their movements and performance during training remain unaffected. iv) Individuals managing\nstress and mental health benefit from continuous heart rate monitoring, as heart rate closely correlates with\nstress levels and emotional states. This is particularly important for young users experiencing high work stress\nwho require continuous assessment of stress levels. This can boost users' self-awareness and motivate proactive\nrelaxation and adjustment.\n2.1.2 Limitations of existing heart rate sensing tools. The majority of participants currently depend on wearable\ndevices and medical-grade electrocardiogram (ECG) equipment at hospitals, constituting 62.5% (50/80) and 43.75%\n(35/80), respectively. However, participants voiced concerns: 95% of participants expressed discomfort with\nECGs attached to the chest or clipped to fingers. This suggests that current medical-grade, contact-based ECG\nequipment may not offer a comfortable user experience, potentially affecting user acceptance and long-term usage.\nFurthermore, 76.25% of participants admitted to frequently or occasionally forgetting to wear them, potentially\ncompromising the long-term waiting time for health sensing. This compromises long-term data acquisition for\nhealth monitoring and detecting health trends or early warning signs.\n2.1.3 Willingness to use contactless and ubiquitous heart rate sensing tools. 91.25% of participants expressed the\nneed for an additional heart rate sensing tool in addition to their current use of devices attached to the chest or\nclipped to fingers. Moreover, the desired features for this tool include high measurement accuracy (37.5%) and a\ncontactless sensing mode (31.25%) that does not require attachment to the chest or finger.\n2.2 Design Goals\nTo further understand the requirements of the contactless heart rate sensing tool, we asked each participant to\nrate the importance of different performance aspects of the tool and summarized the results as our design goals.\n\u2022 On-device. 60% of participants are unwilling to stream out their sensitive raw videos to the cloud for analysis.\nThus, video stream collection and inference should be conducted locally on the device. Additionally, 70%\nof participants prefer interacting with local devices over the cloud to protect sensitive data privacy and\nreduce network dependency.\n\u2022 Accurate. 80% of participants reported low tolerance for errors, and 63% of participants can only tolerate a\nmaximum error of 5 times in terms of bmp (heart rate beats per minute).\n\u2022 Responsive. 70% of participants are comfortable receiving results within 30ms of detection initiation. Heart\nrate sensing and recognition should offer real-time performance and prompt user notifications."}, {"title": "2.3 Why Camera-based Physiological Sensing", "content": "Cameras can detect physiological signals by capturing facial videos, including heart rate, respiration rate, and\nblood oxygen saturation [66]. This capability relies on the principle of contactless cardiopulmonary measurement,\nwhich involves detecting subtle changes in the reflected light from the body resulting from physiological\nprocesses. As shown in Figure 3, biologically, when blood volume increases (such as during each heartbeat),\nthere is a corresponding increase in light absorption by the skin. This increase in blood volume leads to greater\nabsorption of light. Consequently, the amount of visible light reflected by the skin changes, forming the basis of\nphotoplethysmography (PPG) signals. Camera-based imaging methods leverage these changes in light absorption\nto measure variations in blood volume on the skin surface. By analyzing the color and motion changes in the\ncaptured video, it becomes possible to extract pulse signals and calculate heart rate frequency.\nThe ubiquitous camera serves as a contactless tool to monitor physiological parameters at any location and\nany time. This is beneficial for three reasons: i) Compared to hospital, contact-based sensing methods such\nas electrocardiography (ECG) and pulse oximetry, ubiquitous camera-based sensing eliminates direct body\ncontact, reducing discomfort and infection risks. ii) ubiquitous camera-based sensing seamlessly integrates\ninto environments, enabling continuous sensing. Users can be monitored in their natural settings, leading to\npotentially more accurate and holistic data collection, compared to limited snapshot measurements in clinical\nsettings. iii) Commercial smart devices such as smartphones, laptops, and robots are widely equipped with\ncameras, making this contactless monitoring method highly accessible and scalable across a wide range of user\npopulations and application scenarios. We identify two application scenarios. Deception detection, such as in\ncriminal suspect lie detection \u00b9, where contactless camera-based heart rate sensors test truthfulness without the\nsubject's awareness or psychological resistance [10, 20]. Another application is health monitoring, providing elderly\npeople living independently with a convenient method to monitor their physical data and health status daily."}, {"title": "3 SYSTEM DESIGN", "content": "This section begins with an overview of UbiHR before elaborating on its design details."}, {"title": "3.1 Overview", "content": "Figure 4 shows the workflow of UbiHR, which consists of four functional modules, i.e., an adaptive duty-cycling\nfacial video sampling module, a noise-aware facial image preprocessing module, and a fast long-range spatiotemporal\nheart rate recognizing module. The adaptive duty-cycling facial video sampling module switches between\nsleeping, short-term listening, and long-term listening schemes. It adopts an event-driven adaptive duty-cycling\nmechanism to balance the energy consumption and the promptness of facial keypoint detection. The dynamic\nnoise-aware facial image preprocessing module extracts essential facial key points through differentiation\nand normalization, eliminating noise from user head motion and varying environmental lighting conditions, and\nreducing input volume for subsequent processing with real-time performance. The fast long-range spatio-\ntemporal heart rate recognizing module incorporates the zero-parameter time shift strategy to 2D convolution\nfor lightweight spatio-temporal modeling, extracting both long-range and short-range spatio-temporal features\nfrom the heart rate signal via parallel multi-branch structures, and corrects accuracy using soft-attention masks."}, {"title": "3.2 Adaptive Duty-cycling Facial Video Sampling", "content": "This module balances energy cost with efficiency of facial keypoint detection. Continuously operating the camera\non battery-powered devices like robots and smartphones consumes significant energy, which is impractical [47, 48].\nGiven that major heart rate events like sinus bradycardia and high-degree atrioventricular block are rare and\nbrief, UbiHR uses an adaptive duty-cycling mechanism to minimize camera runtime and energy use. It operates\nin three camera states: short-term sampling to detect complete facial images and significant heart rate events,\nsleeping to save energy, and long-term sampling for continuous video collection when events are detected.\nFigure 5 illustrates an event-driven, adaptive duty-cycling mechanism. Initially, the camera starts in short-term\nsampling mode, adjusting for user motion in open environments. An event detector identifies valid video frames\nand heart rate-related events, shifting to long-term sampling if needed. Studies like [35] and [56] show that the\nforehead and cheek regions yield the strongest rPPG signals. Only clear, complete facial views are processed\nfurther, utilizing an efficient active shape model (ASM)[16] for facial keypoint detection. This model controls\nshape distribution and predicts motion direction and position of keypoints. If a complete face isn't detected within"}, {"title": "3.3 Dynamic Noise-aware Facial Image Pre-processing", "content": "User motion and open environment lighting introduce noise in video frames, necessitating preprocessing for\nheart rate sensing. Extracting the heart rate directly from raw videos yields a signal contaminated with various\ninterferences. Additionally, the mobility of cameras, such as those on smartphones and robots, complicates\nmatters. Mobile cameras can measure heart rates under any condition but introduce instability and varying\nangles, causing shaking and lighting changes, which add noise to the heart rate signal. Therefore, preprocessing\nis crucial to improve the signal-to-noise ratio of the heart rate signal. Prior methods reduce video noises or\nenhance signal-to-noise ratios for heart rate sensing, mainly include four categories: i) Independent Component\nAnalysis (ICA) [63, 64], Principal Component Analysis (PCA)[43], and constrained ICA (cICA) [73] can improve\nthe signal-to-noise ratio of the HR signal from RGB frame sequences. ii) Combining HR signals from different\nRegions of Interest (ROI) using frequency-based weighting [41]. iii) Dividing the face into multiple ROI regions to\nobtain a temporal representation matrix and utilize matrix completion to purify HR-related signals. iv) Extracting\nthe feature map from the green channel of frames [32].\nHowever, they struggle to balance accuracy and latency (as we will evaluate in Sec. 4). Mathematical and\ncurve-based methods i), ii), and iii) mathematical and curve-based methods provide quick preprocessing but\nlimited signal-to-noise improvements in heart rate signals. Method (iv), while promising, is time-consuming,\nwith the STMap approach [58] requiring complex calculations and over 200ms per frame to refine HR signals.\nTo address these challenges, we introduce a novel preprocessing approach that quickly adapts to motion and\nlighting variations. It employs motion modeling and tailored normalization strategies across consecutive RGB\nframes, effectively reducing noise from these common real-world variables\nIn particular, we present a temporal difference layer, a dedicated layer designed to compute motion disparities\nbetween adjacent frames during pre-processing before input to the deep learning model. Specifically, we employ\ntemporal difference by evaluating brightness changes between adjacent frames, considering factors such as"}, {"title": "3.4 Fast Long-range Spatio-temporal Heart Rate Recognizing", "content": "3.4.1 Primer on Long-range Spatio-temporal Model for HR Detection. Camera-based heart rate recognition\ninvolves mapping video sequences to signal sequences. Specifically, it translates the spatial dimension of light\nreflection in video frames into temporal variations of heart rate signals. As depicted in Figure 8, peak positions\nexhibit similar characteristics regardless of diverse user heart rate speed or amplitude. Our key observation is\nthat these characteristics include abrupt changes in signal trends and relatively high signal intensity. Therefore,\nconstructing a long-range spatio-temporal model can enhance detection accuracy and robustness.\n\u2022 Limitations of Short-range Methods. Most prior methods extract spatio-temporal rPPG features from\nshort ranges [7, 70, 82], such as adjacent frames, overlooking long-range relationships among periodic\nrPPG features, thereby compromising stability against noise. As illustrated in Tab. 1, compared to scenarios"}, {"title": "3.4.2 Incorporating Temporal Shift to 2D Convolution for Lightweight Spatio-temporal Modeling", "content": "We utilize a\ntemporal shift strategy to enhance the temporal modeling of 2D convolutions, offering noise-robustness and\nawareness similar to 3D convolutions, but without the extra computational burden. 3D convolutions [83] intuitively"}, {"title": "3.4.3 Multi-branch for Long- and Short-range Spatio-temporal Modeling", "content": "Despite its utility, the temporal shift\nstrategy remains confined to short-range spatio-temporal information, limiting its effectiveness in capturing\nlong-range periodic variations in light reflection from sequential video frames. This limitation blocks the model's\nability to effectively map long-range heart rate signals. As mentioned in Sec. 3.4.1, capturing the heart rate\nsignal's periodic characteristics over the long-range is pivotal for suppressing non-physiological information like\nnoise, thus enhancing detection accuracy and robustness.\nTo overcome this challenge, we introduce a multi-branch parallel spatio-temporal model capable of both short-\nand long-range spatio-temporal modeling. As illustrated in Fig. 7, we partition the input frame sequence into\nthree groups and apply a single-branch temporal shift+2D convolution operation on three adjacent frames, i.e.,\nshort-range, within each group. These short-range branches function in parallel, combining their results to yield\nshort-range outcomes similar to prior approaches, referred to as the \"adjacent frames branch\". Concurrently, we\nselect one frame from each group in the adjacent frames branch to generate a new frame sequence. We conduct a\nsingle-branch temporal+2D CNN convolution operation independently on this sequence, creating the \"segment\nbranch.\" Operating between frames with longer temporal distances, the segment branch facilitates long-range\nspatio-temporal modeling. Finally, we merge the outputs of the adjacent frame branch (for short range) and the\nsegment branch (for long range), performing convolution operations to capture heart rate features consistent\nacross both branches. Accurate mixed-granularity results are then obtained after average pooling."}, {"title": "3.4.4 Towards Parallel Speedup", "content": "The multi-branch parallel spatio-temporal model enhances computational\nefficiency by enabling concurrent convolution operations in the adjacent frames branch, eliminating the sequential\nprocessing bottleneck. This parallel execution ensures that subsequent frames can be processed independently of\neach other, significantly reducing waiting time. Additionally, implementing asynchronous processing further\nenhances parallelism, reducing waiting time. By grouping consecutive video frames into groups, we exploit\nparallel capabilities more effectively, accelerating the overall process."}, {"title": "3.4.5 Soft-attention Mask-based Accuracy Recorrecting", "content": "Incorporating the temporal shift strategy introduces\nadditional temporal information, potentially leading to noise interference in the processed frames in reverse [50].\nThis is because temporal shift integrates all the content it moves into the adjacent frames and replaces the original\ninformation in those frames, including noise. This can result in the replacement of high-intensity parts of the\nheat rate signal with noise information or the introduction of heart rate signal information in regions where\nnoise should exist in the current frame. Therefore, it is crucial to prioritize pixels that already contain heat rate\ninformation over those affected by temporal shift-induced noises."}, {"title": "4 EVALUATION", "content": "This section presents the experimental settings and system performance of UbiHR.\n4.1 Implementation\nWe train the DL model on the server while conducting heart rate video sampling, detection, and recognition\nunder various motion and lighting conditions on mobile and embedded devices. On the server side, UbiHR is\nimplemented in PyTorch (v1.12.1) and utilizes the Adadelta optimizer with a learning rate of 1.0, a batch size of\n32, and a 3x3 kernel size with 2x2 pooling. We apply dropout rates of 0.25 and 0.5. All spatio-temporal models\nuse a window size of 10 frames for fair comparison. Model training is accelerated using Nvidia TITAN RTX\nwith CUDA 10.2. On the client side, we package the system as a Python module for seamless integration with\nother applications. Video sampling is performed at 30fps. We integrate existing methods into platforms like\nAndroid, primarily overcoming the limited availability of DL-based face recognition libraries. The user interface\ncomprises a viewfinder and a heart rate result display. We design platform-specific user interfaces for various\nmobile and embedded platforms. The camera viewfinder shows the live feed captured by the camera, with a\nred box indicating the actual framing area in the center. To protect user privacy, all video frames captured\nduring the duty-cycling sampling process are immediately processed and deleted locally, not sent to the cloud for\npreprocessing or retained for future processing. Results are displayed as waves using JavaScript, and hovering\nover the results reveals specific information about corresponding points."}, {"title": "4.2 Experiment setups", "content": "Dataset and tasks. We experiment with four datasets. First, both UBFC-RPPG [4] and UBFC-Phys [65] are\nrecorded by low-end cameras instead of professional high-end cameras, under varying sunlight and indoor\nillumination. UBFC-RPPG features stationary subjects, while UBFC-Phys includes diverse head movements,\nintroducing motion noise. We train on UBFC-RPPG and test on UBFC-Phys to evaluate system performance\nunder diverse motion and lighting noises. Second, we conduct tests on two synthetic datasets, SCAMPS [55]\nand UCLA-rPPG [77]. SCAMPS synthesized 2800 RGB videos and provided corresponding ground truth PPG\nsignals. It also randomly rendered hair, clothing, and the environment to simulate various real-world scenarios."}, {"title": "4.3 Performance comparison", "content": "This section presents the results of heart rate sensing compared with the baselines.\n4.3.1 Overall trade-off comparison. This experiment compares the performance of UbiHR and nine different\nbaseline methods on open video-based heart rate sensing datasets.\nSetups. We conduct tests on three typical mobile and embedded devices, i.e., Device D\u2081 Raspberry Pi 4B, Device\nD2 Nvidia Jetson Orin, and Device D3 Nvidia Jetson AGX. We use test data of various tasks in UBFC-Phys, i.e.,\nparticipants performing three tasks under diverse natural lighting conditions: head motion (Task 1), speaking\n(Task 2), and diverse facial expressions (Task 3), to compare the accuracy, latency, and trade-off of eight different\nbaselines and UbiHR. We test three times on each device and calculate the average values. we use the same\ninputs on each device, however, these three different devices have different CPU, and their resource constraints\nled to the use of different processing techniques and different third-party libraries on them.\nResults. Figure 10 shows the results, indicating that UbiHR outperforms other baselines across all testing\ndevices. First, curve-based methods, e.g., GREEN, CHROM, ICA, LGI, PBV and POS, demonstrate decent latency\nperformance, while these manually designed model-based methods exhibit unacceptable sensing errors on\ndiverse noisy data. Second, DeepPhys, tailored for server-side deployment, suffers from 59.4% higher latency\n(only inference without pre-processing as its pre-processing is too complex to run on mobile and embedded\ndevices) and reduced accuracy on resource-limited mobile and embedded devices due to high-cost pre-processing\nmethods. We note that, due to the high-cost pre-processing methods used in DeepPhys being impractical to\ndeploy on mobile and embedded devices, Figure 10 only presents its inference latency without pre-processing.\nEven TS-CAN, designed for on-device deployment, processes over 10 frames slower per second than UbiHR due\nto significant pre-processing and inference overhead.\n4.3.2 Accuracy comparison. This experiment compares the heart rate sensing accuracy of baselines and UbiHR.\nSetups. We conduct evaluations across three tasks (Task 1 to 3) in UBFC-Phys on Device D\u2081 Raspberry Pi\n4B. Testing two standard accuracy metrics, Mean Absolute Error (MAE) and Mean Absolute Percentage Error\n(MAPE), with five measurements conducted and the average performance reported. The inputs used for testing\ndifferent methods are the same.\nResults. Fig. 11 summarize the results. First, UbiHR achieves the highest accuracy, significantly outperforming\ncurve-based POS with a 22.6% reduction in Mean Absolute Error (MAE) and a 1.32% decrease in Mean Absolute"}, {"title": "4.3.3 Memory usage comparison", "content": "This experiment evaluates the memory usage of UbiHR compared to baselines.\nSetups. Using three continuous video clips from UBFC-Phys on Raspberry Pi 4B, with durations of 3 minutes,\nwe monitor the memory usage during the inference process. For other baselines, as the preprocessing steps\ncouldn't be performed on Raspberry Pi 4B, preprocessed data are used as their input. We use the rPPG-Toolbox\nfor fair testing. The memory usage data of the devices is collected by using the professional tool called dstat.\nResults. Figure 13 illustrates the memory usage of TS-CAN and DeepPhys, which are used solely for inference\ndue to the high-cost preprocessing can not fit into embedded devices. While we show the memory usage of\nboth inference and the entire end-to-end process with UbiHR. First, UbiHR's memory usage of inference reveals\nthe lowest memory usage. Second, UbiHR's end-to-end process exhibits an average memory usage of 2.32GB,\nwhich is 1.25% higher than TS-CAN and 15.71% higher than DeepPhys. This indicates that UbiHR's on-device\npre-processing and multi-branch spatio-temporal model are low-cost. Third, all three methods exhibit periodic\nmemory usage fluctuations during continuous operation, gradually increasing and decreasing at certain intervals.\nFor TS-CAN and DeepPhys, this is due to the need to read pre-processed data in npy format before inference."}, {"title": "4.3.4 Latency comparison", "content": "This experiment compares the latency of UbiHR and different baseline methods.\nSetup. The baseline and data setups are consistent with the previous one. We test the latency across three\ndevices: Raspberry Pi 4B, Jetson Orin, and Jetson AGX. We measure the FPS (Frames Per Second) as the metric.\nResults. Table 2 shows the results. First, UbiHR exhibits the fastest inference speed on all devices. On the\nRaspberry Pi, it shows a 9.06% improvement over DeepPhys and a 5.24% improvement over TS-CAN. On the Orin\nand AGX devices, UbiHR achieves speed improvements of 60.88% and 9.55% respectively over DeepPhys and\n7.19% and 5.32% over TS-CAN. Second, as device performance increases, UbiHR demonstrates the most significant\nincrease in inference speed. From the Raspberry Pi to the AGX, its speed increases by 171.7fps, compared to\n156.6fps for DeepPhys and 163fps for TS-CAN. This improvement is attributed to the lightweight and parallel\nspatio-temporal model structure, reducing computing costs."}, {"title": "4.4 Performance over Diverse Light Conditions", "content": "This experiment assesses the sensing stability of UbiHR and three DL-based baseline methods across four real-\nworld scenarios. Varying light conditions introduce different spectra and frequencies, leading to diverse signal-to-\nnoise ratios. User motions further lead to noises, challenging the stability of heart rate sensing performance.\nSetups. We adopt four light conditions that users always encounter, i.e., natural light (Sc1), red light (Sc2),\ngreen light (Sc3), and mixed blue-green light (Sc4). These scenarios represent various ubiquitous application\nenvironments such as natural outdoor lighting, red light resembling incandescent bulbs, green light resembling\nfluorescent lamps, and a mix of green and blue light resembling LED lights. To simulate these conditions, we\napplied different spectral filters to videos captured under natural lighting (Sc1) using OpenCV. These videos also\nfeature diverse motions from users."}, {"title": "4.5 Performance with Diverse Input Range", "content": "This experiment confirms the effectiveness of UbiHR's long-range spatio-temporal model compared to baseline\nmethods relying on short-range adjacent frames. As mentioned in Sec. 3.4, previous methods always utilize a\nsingle-branch structure, such as TS-CAN [50], restricting their capability to establish mappings from videos to\nheart rate signals only within short ranges. While UbiHR adopts a multi-branch parallel spatio-temporal model,\nenabling simultaneous capture of spatio-temporal relationships across short and long ranges.\nSetup. We perform experiments on Device D\u2081 using the same three input video segments as discussed in\nSec. 4.3.3. We vary the number of frame sequences inputted to TS-CAN and UbiHR, ranging from adjacent 5 to\n30 frames, to investigate the influence of different input quantities on inference accuracy. Each input undergoes 5\ntimes to show the average value.\nResults. Fig. 18 illustrates the changes in heart rate prediction error for UbiHR and TS-CAN with increasing\ninput frame ranges. TS-CAN exhibits a gradual increase in error as the input frame range rises, whereas UbiHR"}, {"title": "4.6 Performance on Diverse Testing Data", "content": "This experiment validates the inference speed and robustness of UbiHR in diverse testing data, on the smartphone\n(D4). We deploy the model pre-trained on UBFC-rppg, and test it on three different datasets (UBFC-Phys, SCAMPS,\nand UCLA-rPPG). To simulate real-world scenarios, we maintain the resource-competing processes on the mobile\nphone. The results summarized in Tab. 4 indicate that UbiHR achieves an inference speed of 12.4ms per frame\non the Snapdragon 8 Gen1 CPU, meeting real-time detection requirements in practical scenarios. Moreover, in\nterms of inference accuracy, UbiHR achieves MAE values of 4.13, 5.07, and 5.28 on the three different datasets,\nrespectively, demonstrating robustness in real-world application scenarios."}, {"title": "4.7 Micro-benchmark and Ablation studies", "content": "4.7.1 Impact of Drop Rate. We examine the influence of various Drop Rates during UbiHR's model training.\nAs mentioned in Sec. 3.4.2, we utilize 2D convolution kernels and the drop rate is an important parameter that\naffects its learning effectiveness. As depicted in Fig. 19, the optimal Drop Rate in UbiHR's training process is 0.2.\nFurther elevating this parameter diminishes both model performance and robustness.\n4.7.2 Impact of Face Box Size. We test the influence of the size of the face bounding box formed by key point\ndetection on UbiHR's accuracy. As depicted in Fig. 20, continuously increasing the enlargement ratio of the\nface bounding box results in decreased performance and stability of UbiHR. This is because enlarging the face\nbounding box range introduces background noise, thus affecting accuracy."}, {"title": "4.7.3 Impact of Soft-attention Mask", "content": "As discussed in Sec. 3.4.5, directly integrating the spatio-temporal module\nto 2D convolution introduces extra spatio-temporal noise, diverting the network's attention to pixels devoid\nof physiological signals. Thus, we introduce the soft-attention mask as a correction. Table 3 compares the\nperformance of UbiHR on the UBFC-Phys test data with and without the soft-attention mask. It's evident that\nwithout the soft-attention mask, UbiHR's accuracy notably declines across all 5 metrics."}, {"title": "4.8 Case study", "content": "To investigate the performance of UbiHR in various real-world scenarios", "testing": "i) Dormitory:\nfeatures stable fluorescent lighting with minimal background pedestrian traffic", "room": "similar to the dormitory", "Cafe": "the most complex environment, combining natural, fluorescent, and LED lighting, with a physically\nintricate setting and high pedestrian traffic"}]}