{"title": "DisCOM-KD: Cross-Modal Knowledge Distillation via Disentanglement Representation and Adversarial Learning", "authors": ["Dino Ienco", "Cassio Fraga Dantas"], "abstract": "Cross-modal knowledge distillation (CMKD) refers to the scenario in which a learning framework must handle training and test data that exhibit a modality mismatch, more precisely, training and test data do not cover the same set of data modalities. Traditional approaches for CMKD are based on a teacher/student paradigm where a teacher is trained on multi-modal data with the aim to successively distill knowledge from a multi-modal teacher to a single-modal student. Despite the widespread adoption of such paradigm, recent research has highlighted its inherent limitations in the context of cross-modal knowledge transfer.\nTaking a step beyond the teacher/student paradigm, here we introduce a new framework for cross-modal knowledge distillation, named DisCoM-KD (Disentanglement-learning based Cross-Modal Knowledge Distillation), that explicitly models different types of per-modality information with the aim to transfer knowledge from multi-modal data to a single-modal classifier. To this end, DisCoM-KD effectively combines disentanglement representation learning with adversarial domain adaptation to simultaneously extract, for each modality, domain-invariant, domain-informative and domain-irrelevant features according to a specific downstream task. Unlike the traditional teacher/student paradigm, our framework simultaneously learns all single-modal classifiers, eliminating the need to learn each student model separately as well as the teacher classifier. We evaluated DisCoM-KD on three standard multi-modal benchmarks and compared its behaviour with recent SOTA knowledge distillation frameworks. The findings clearly demonstrate the effectiveness of DisCoM-KD over competitors considering mismatch scenarios involving both overlapping and non-overlapping modalities. These results offer insights to reconsider the traditional paradigm for distilling information from multi-modal data to single-modal neural networks. Our code is available at this link.", "sections": [{"title": "1 Introduction", "content": "The modern landscape is characterized by a large diversity of devices consistently sensing their environments. This influx of information poses new challenges in terms of data analysis and understanding, particularly for machine learning and computer vision models, which must work seamlessly across a spectrum of platforms. From wearable gadgets to autonomous vehicles, an array of sensors continuously collect data about the surroundings [9].\nIn this context, the same object or entity can be described by multiple modalities, necessitating new learning paradigms to handle the collected heterogeneous information [15]. Many multi-modal learning models assume that data modalities between the training and deployment stages remain exactly the same [15]. However, due to the wide range of sensor data available, systematically accessing data across all sensor modalities may be infeasible.\nSpecifically, a set of modalities may be available during the training stage, while another set of modalities, either overlapping with the former or not, may be accessible during the deployment stage [18]. In such a scenario, strategies are required to operate under cross-modal scenarios, leveraging the multi-modal information available during the training stage to enhance classification capabilities on the modalities accessible at deployment stage.\nRecently, Cross-Modal Knowledge Distillation (CMKD) has proven to be effective for multi-modal applications characterized by modalities mismatch [16]. The majority of existing solutions rely on a teacher/student paradigm [5, 11, 19, 24], where a teacher model trained on one or several data modalities is then used to supervise a single-modal student model trained on the modality available at deployment stage. However, this paradigm has several shortcomings, such as the arbitrary choice of modalities used to train the teacher model, the computational burden associated with the training of multiple models for specific downstream applications, and the need to set up a separate process each time a single-modal student model needs to be trained. Additionally, recent studies in [28] clearly highlights the inherent limitations of this paradigm in automatically distilling useful information such as modality-discriminative features for cross-modal knowledge transfer.\nIn this study, we aim to address the challenge of multi-modal learning in applications characterized by modalities mismatch. We explore an alternative approach based on disentanglement representation and adversarial learning, overcoming mainstream teacher/student paradigms and their associated limitations. Specifically, we introduce a novel framework for cross-modal knowledge distillation, called DisCoM-KD (Disentanglement-learning based Cross-Modal Knowledge Distillation). This framework explicitly models different types of per-modality information to transfer knowledge from multi-modal data to a single-modal classifier. DisCoM-KD effectively combines disentanglement representation learning with adversarial domain adaptation to simultaneously extract domain-invariant, domain-informative, and domain-irrelevant features for each modality, tailored to a specific downstream task. Conversely to traditional teacher/student paradigms, our framework learns all single-modal classifiers simultaneously, eliminating the need to train each student model separately. To evaluate the effectiveness of our framework, we consider three standard multi-modal benchmarks and recent state-of-the-art knowledge distillation frameworks, demonstrating DisCoM-KD's ability to outperform previous strategies in scenarios of modalities mismatch, covering both overlapping and non-overlapping modalities between training and deployment stages.\nIn summary, the contributions of our work are the following:\n\u2022 A novel CMKD framework based on disentanglement representation and domain adversarial learning;\n\u2022 An alternative CMKD strategy that circumvents traditional teacher/student paradigms to distill knowledge from multi-modal data to single-modal neural networks;\n\u2022 An extensive comparison of DisCoM-KD across three computer vision multi-modal"}, {"title": "2 Related Work", "content": "In this section, we firstly provide a brief recap of general knowledge distillation strategies, then we focus on knowledge distillation for multi-modal learning and, finally, we conclude by discussing elements on disentanglement representation-based learning.\nKnowledge Distillation. Knowledge Distillation (KD) [7] was introduced to transfer \"dark\" knowledge from a teacher model to a lightweight student model by learning the student model from the soft labels generated by the teacher. The standard KD loss formulation used to train the student model is defined as follows:\n$L= \\alpha L_{task} + (1 - \\alpha)L_{KD}$ (1)\nHere, $L_{task}$ represents the downstream loss and $L_{KD}$ represents the distillation loss enforcing the knowledge transfer from teacher to student, with $\\alpha$ determining the balance between the two terms. Different approaches vary in how they implement the $L_{KD}$ term, which can be logit-based [10], feature-based [4], or relation-based [8].\nRecent studies have shown that logit-based approaches outperform other strategies [21]. Logit-based approaches implement the $L_{KD}$ component by exploiting the Kullback-Leibler divergence between the teacher and student logits. For example, [31] proposes to explicitly decouple the target class from non-target classes in the knowledge distillation process. In [14], a curriculum learning process is introduced to estimate the temperature value to be used in the Kullback-Leibler divergence. [10] employs a multi-level approach to perform logit distillation at different granularity levels (instance, class, and batch), considering multiple temperature values to make the knowledge transfer more robust. Recently, [21] proposes a plug-in extension that can be combined with any of the previous frameworks in which teacher and student logits are standardized prior to the analysis, ensuring a more coherent and consistent comparison.\nCross/Multi-Modal Knowledge Distillation. Cross-modal KD extends traditional KD approaches to encompass multi-modal learning [28]. While cross-modal KD does not assume any overlap between the modalities accessed by the teacher and student models, in the multi-modal KD scenario [16], the information used by the student is a subset of the modalities used by the teacher model, thus making the former a more general scenario than the latter. However, in both scenarios, the student model typically has access only to a single data modality. The majority of the proposed frameworks for both cross-modal and multi-modal KD [5, 11, 19, 24] are tailored for task specific use cases with lack of a generic solution. This is primarily due to the fact that, despite the empirical success demonstrated by prior works, the mechanisms behind cross-modal KD still remains loosely understood [28]. An initial investigation towards understanding this mechanism has been proposed in [28], where the authors emphasize the significance of modality-discriminative features as key components for cross-modal KD. However, their study provides preliminary experiments that are heavily reliant on data-specific characteristics, thus limiting the generic value of the obtained findings.\nDisentanglement Representation Learning. Disentangled representation learning aims to identify and separate hidden information in the underlying data [25]. In contrast to standard learning processes, which focus solely on learning domain-invariant features, disentanglement-based methods explicitly decompose the learned representation into domain-specific and domain-invariant features, thus paving the way to the extraction of task-relevant and task-irrelevant information [22]. In the context of multi-modal learning, disentanglement-based strategies are used to extract both multi-modal and modality-specific factors [23, 30]. Recent approaches have focused on disentangling shared information among modalities to perform various downstream tasks [17, 27, 29]. Despite its contributions to numerous settings [3, 12, 25], disentangled representation learning still remains unexplored in the realm of knowledge distillation."}, {"title": "3 Method", "content": "The proposed architecture, depicted in Figure 1, consists of two independent branches, one for each modality, extracting several per-modality representations. These representations are then used by per-modality task classifiers to make the final prediction. Furthermore, auxiliary classifiers, acting at intermediate stages, are leveraged to ensure that the extracted per-modality representations cover different complementary facets of the underlying information while also carrying information related to the downstream task.\nGiven an image $x^*$, where $x^*$ can be either $x_{M1}$ or $x_{M2}$ (with M1 and M2 being two different modalities), DisCoM-KD extracts three per-modality embeddings $z_{inv}^*, z_{inf}^*, z_{irr}^*$ referred as modality-invariant, modality-informative and modality-irrelevant representation, respectively. All the embeddings have the same dimensionality $z_{inv}^*, z_{inf}^*, z_{irr}^* \\in \\mathbb{R}^D$. Subsequently, $z_{inv}^*$ and $z_{inf}^*$ are fed to the (per-modality) main task classifiers, while $z_{irr}^*$ is discarded, as its objective is to collect/attract per-modality information that should not contribute to the downstream task. Specifically, for each of the input modalities, we have a task classifier $Cl_*$ that outputs class probabilities $\\hat{y}^* = Cl_*([z_{inv}^*||z_{inf}^*]) \\in \\mathbb{R}^C$ for the C existing classes and $||$ denotes the concatenation operation. This means that two main task classifiers $Cl_{M1}$ and $Cl_{M2}$ are trained during the process, with $\\hat{y}_{M1} = Cl_{M1}([z_{inv}^{M1}||z_{inf}^{M1}])$ and $\\hat{y}_{M2} = Cl_{M2}([z_{inv}^{M2}||z_{inf}^{M2}])$.\nBeyond the main architecture, we introduce several modules to ensure that the extracted embeddings represent complementary information derived from the multi-modal input data. These additional modules are: i) A modality classifier $Cl_{adv}$ coupled with gradient reversal layer [2] to facilitate the extraction of modality-invariant representations; ii) Two auxiliary modality classifiers, $Cl_{m-inf}$ and $Cl_{m-irr}$, ensuring that modality-informative ($z_{inf}^*$) and modality-irrelevant ($z_{irr}^*$) embeddings contain modality-specific information and iii) An auxiliary task classifier $Cl_{aux}$ enforcing modality-invariant ($z_{inv}^*$) and modality-informative ($z_{inf}^*$) embeddings to be discriminative for the downstream task. During inference, only the per-modality extractors and the main task classifiers ($Cl_{M1}$ and $Cl_{M2}$) are retained, resulting in two distinct models that have been jointly learnt and can be deployed independently of each other."}, {"title": "3.1 Training losses", "content": "To train our cross-modal knowledge distillation framework DisCoM-KD, we design a set of loss functions that explicitly model several properties beyond the main downstream classification task with the aim to enforce disentanglement across complementary per-modality representations. Specifically, the training procedure optimizes five different loss terms.\nThe first term is directly related to the downstream classification task. Both modality invariant ($z_{inv}^*$) and modality informative ($z_{inf}^*$) embeddings are fed to the main task classifiers. Then, we use standard Cross-Entropy loss (CE) between the output of each per-modality task classifier and the associated ground-truth y:\n$\\mathcal{L}_{cl} = \\sum_{m \\in \\{M1, M2\\}} CE(Cl_m([z_{inv}^m || z_{inf}^m]), y)$ (2)\nThe second term has the goal to enforce the learning of modality-invariant representations. We adopt a CE loss over the output of a classifier that discriminates across representations of samples from different modalities. Here, we use an adversarial training strategy, implemented via Gradient Reversal Layer (GRL) [2] over the modality invariant ($z_{inv}^*$) representations:\n$\\mathcal{L}_{adv} = \\sum_{m \\in \\{M1, M2\\}} CE(Cl_{adv}(GRL(z_{inv}^m)), m)$ (3)\nThe third term guides the learning of modality-aware representations via modality classifiers. We use two classifiers, one for modality informative ($Cl_{m-inf}$) and one for modality irrelevant ($Cl_{m-irr}$) embeddings in order to predict from which branch the embedding originates:\n$\\mathcal{L}_{mod} = \\sum_{m \\in \\{M1, M2\\}} CE(Cl_{m-inf}(z_{inf}^m), m) + \\sum_{m \\in \\{M1, M2\\}} CE(Cl_{m-irr}(z_{irr}^m), m)$ (4)\nThe fourth term aims to enhance the task discriminative information carried by the per-modality embeddings. Here, we employ an auxiliary task classifier over the set of modality invariant ($z_{inv}^*$) and modality informative ($z_{inf}^*$) embeddings:\n$\\mathcal{L}_{aux} = \\sum_{m \\in \\{M1, M2\\}} \\sum_{i \\in \\{inv, inf\\}} CE(Cl_{aux}(z_{i}^m), y)$ (5)\nThe last term explicitly constrains embeddings from the same modality to contain complementary information. We implement a double disentanglement process, enforcing orthogonality [12] between modality-invariant ($z_{inv}^*$) and informative ($z_{inf}^*$) representations, as well as between modality-informative ($z_{inf}^*$) and irrelevant ($z_{irr}^*$) embeddings. This guides the network to explicitly separate different per-modality contributions:\n$\\mathcal{L}_{\\perp} = \\sum_{m \\in \\{M1, M2\\}} \\frac{z_{inv}^m \\cdot z_{inf}^m}{||z_{inv}^m||_2 ||z_{inf}^m||_2} + \\sum_{m \\in \\{M1, M2\\}} \\frac{z_{inf}^m \\cdot z_{irr}^m}{||z_{inf}^m||_2 ||z_{irr}^m||_2}$ (6)\nThe final loss function is defined as the sum of all the previous terms:\n$\\mathcal{L} = \\mathcal{L}_{cl} + \\mathcal{L}_{adv} + \\mathcal{L}_{mod} + \\mathcal{L}_{aux} + \\mathcal{L}_{\\perp}$ (7)\nImplementation details. Each modality branch extractor, as reported in Figure 2, is composed of two encoders: a modality specific encoder and a modality invariant encoder. As encoder backbone we use a ResNet-18 model [6]."}, {"title": "4 Experimental Evaluation", "content": "To evaluate our framework, DisCoM-KD, we designed an experimental evaluation considering three different multi-modal benchmarks involving SOTA teacher/student strategies from the Knowledge Distillation field encompassing both cross-modal and multi-modal KD scenarios. Additionally, an ablation study of DisCoM-KD is proposed to analyze the interplay between its different components.\nDATASETS. As datasets we consider: i) SUNRGBD, the version proposed in [1] for multi-modal RGB-D scene classification. We consider RGB and Depth images from the Kinect v2 domain, for a total of 2 105 pairs of RGB/Depth images, with 3 and 1 channels respectively, covering 10 classes; ii) EuroSat-MS-SAR proposed in [26] for multi-modal Multi-Spectral (MS) and Synthetic Aperture Radar (SAR) remote sensing scene classification. The dataset contains 54000 pairs of MS and SAR images, with 13 and 2 channels respectively, for a land cover classification task spanning 10 classes; iii) TRISTAR proposed in [20] for multi-modal (RGB, Thermal and Depth) action recognition. According to results reported in [20], here we only consider the two most informative modalities (Thermal and Depth). The dataset contains 14201 pairs of Thermal and Depth images, with 1 channel each, representing an action recognition task spanning 6 classes.\nCOMPETING METHODS. We adopt three recent state-of-the-art strategies: Decoupled Knowledge distillation (DKD) [31], Curriculum Temperature Knowledge Distillation (CTKD) [14] and Multi-Level Knowledge Distillation (MLKD) [10]. Furthermore, we integrate two baseline methods proposed in [28], referred to as KDv1 and KDv2. Both baselines implement the traditional knowledge distillation loss reported in Equation 1, with KDv1 setting the $\\alpha$ hyper-parameter to 0, while KDv2 sets it to 0.5. While KDv1 only uses the soft label to train the student model, KDv2 equally weighs the information from the original hard labels and the teacher soft labels. We combine each of these five strategies with the plug-in logit standardization preprocessing (LSKD) proposed in [21]. Additionally, as references, we report the performance of the teacher model (referred to as TEACHER) and a student model that has not received any distillation supervision (referred to as STUDENT) for each evaluation scenario.\nEVALUATION SCENARIOS. We adopt two evaluation scenarios: cross-modal KD and multi-modal KD. For the cross-modal KD scenario the teacher is trained on the richest, in terms of downstream task performances, modality and, successively a single-modal student is distilled leveraging the remaining modality. Here the teacher is implemented via a ResNet-18 [6] architecture. For the multi-modal KD scenario the teacher model is trained on the full set of per-dataset modalities and, successively, a single-modal student is distilled. For this scenario, the teacher model is a two-branch architecture with a per modality encoder implemented via a ResNet-18. The fusion is performed at the penultimate layer of the ResNet-18 architecture via feature element-wise addition. Finally, a linear layer exploits the fused representation for the classification decision. All the student models are implemented with a ResNet-18 architecture.\nEXPERIMENTAL SETTINGS. For all the approaches the same training setup is used: 300 training epochs, a batch size of 128 and Adam [13] as parameters optimizer with a learning rate of 10-4. For all the approaches we use online data augmentation via geometrical transformations (e.g. flipping and rotation). For the competing methods, we adopt the original hyper-parameter settings. The assessment of the models performance, on the test set, is done considering the weighted F1-Score, subsequently referred simply as F1-Score. Each dataset is divided into training, validation and test set with a proportion of 70%, 10% and 20% of the original data, respectively. We repeat each experiment five times and report average results. Experiments are carried out on a workstation equipped with an Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz, with 377Gb of RAM and four RTX3090 GPU. All the methods require only one GPU for training."}, {"title": "4.1 Results", "content": "Table 1 and Table 2 present the average F1-Score results of the competing methods on both cross-modal and multi-modal KD scenarios, respectively. We use green arrows to indicate when a model outperforms the STUDENT baselines, and red arrows otherwise.\nIn the cross-modal KD scenario, Table 1, we consider the following cases: RGB \u2192 DEPTH for SUNRGBD, MS \u2192 SAR for EuroSat-MS-SAR, and THERMAL \u2192 DEPTH for TRISTAR. Here, the left modality indicates the one used by the teacher model while the one on the right is leveraged by the student. We observe that DisCoM-KD outperforms all competitors on both SUNRGBD (47.69 vs. 42.87 achieved by the best competitor) and EuroSat-MS-SAR (80.03 vs. 78.89 achieved by the best competitor). Moreover, it achieves comparable performance with the best competitor on TRISTAR (92.86 vs. 93.06 achieved by the best competitor). Notably, our framework is the only one that consistently improves (as indicated by green arrows) over the STUDENT baseline across all considered cross-modal scenarios.\nIn the multi-modal KD scenario, Table 2, DisCoM-KD outperforms all state-of-the-art KD approaches, consistently improving classification performance compared to the STUDENT baseline. It is worth noting that our framework is the only one that achieves improvement on the TRISTAR dataset when the THERMAL modality is considered for the deployment stage, achieving a classification score of 97.06. On EuroSat-MS-SAR, all competitors are capable of distilling a student single-modal neural network that outperforms the TEACHER model. Also in this case, DisCoM-KD achieves the best classification performance with a score of 98.12. Interestingly, we observe that depending on the dataset, teachers trained on multiple modalities are not always the best choice for distilling a single-modal student. For example, in the TRISTAR case, when the deployment stage covers the DEPTH modality, all KD frameworks exhibited their best performances when the TEACHER has been only trained on the THERMAL modality (cross-modal KD scenario) rather than on the whole set of modalities (multi-modal KD scenario). This underscores that no strategy (neither cross-modal nor multi-modal) guarantees a systematic improvement, highlighting the arbitrary impact this inherent choice can have on the underlying distillation process.\nAblations. The first ablation study (Table 3) explores the importance of the different components on which our framework is built. We observe that Auxiliary Task Classifiers ($\\mathcal{L}_{aux}$) and the Disentanglement Loss ($\\mathcal{L}_{\\perp}$) seem to play the most significant roles in the underlying process. Depending on the considered dataset, each loss term has different relative impacts, and on average, the highest performance is achieved when all components are involved, underscoring the rationale behind the proposed framework. The second ablation study (Table 4) investigates the interplay between the different representations extracted by the disentanglement process. Here, we note that only considering one of the two groups of information \u2014modality-invariant ($z_{inv}^*$) or modality-informative ($z_{inf}^*$) \u2014 systematically decreases the classification performances. Modality-informative features provide slightly better discrimination capability, with varying margins depending on the dataset. In summary, this analysis suggests the suitability of exploiting both modality-invariant and modality-informative representations for the downstream classification task."}, {"title": "5 Conclusion", "content": "In this study we have introduced a new framework for cross-modal knowledge distillation, namely DisCoM-KD. Our aim is to transfer knowledge from multi-modal data to a single-modal classifier. To this end, our framework effectively combines disentanglement representation learning with adversarial domain adaptation. Experimental evaluation, considering both cross-modal and multi-modal knowledge distillation evaluation scenarios, demonstrates the quality of DisCoM-KD compared to recent state-of-the-art KD techniques based on the standard teacher/student paradigm. In addition to performance improvements, our framework offers several inherent advantages over the standard paradigm: i) it learns all single-modal classifiers simultaneously, eliminating the need to train each student model separately; ii) it avoids the use of a teacher model, thereby eliminating the need to select which set of data modalities must be used to train the teacher model. Furthermore, our research work introduces an alternative strategy that opens new opportunities beyond the traditional teacher/student paradigm commonly employed for cross-modal and multi-modal knowledge distillation.\nSeveral possible future avenues can be drawn. Our current process has only been assessed on cross-modal distillation tasks involving no more than two modalities. Extending DisCoM-KD to manage more than two modalities at once remains an open question. While most of the terms of the proposed loss function can be directly adapted to multiple modalities, how to modify the adversarial term to cope with more than two modalities is not straightforward. Another possible follow-up could investigate how to take inspiration from DisCoM-KD to design multi-modal distillation frameworks dealing with semantic segmentation and object detection tasks. For these tasks, the common methodologies are based on encoder/decoder neural network architectures that provide dense predictions as result. All these elements prevent the direct application of our methodology requiring to rethink how disentanglement and adversarial learning may be defined and implemented."}, {"title": "6 Acknowledgment", "content": "This work was supported by the French National Research Agency under the grant ANR-23-IAS1-0002 (ANR GEO ReSeT)."}]}