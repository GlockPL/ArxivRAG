{"title": "DisCOM-KD: Cross-Modal Knowledge Distillation via Disentanglement Representation and Adversarial Learning", "authors": ["Dino Ienco", "Cassio Fraga Dantas"], "abstract": "Cross-modal knowledge distillation (CMKD) refers to the scenario in which a learn-ing framework must handle training and test data that exhibit a modality mismatch, moreprecisely, training and test data do not cover the same set of data modalities. Traditional approaches for CMKD are based on a teacher/student paradigm where a teacheris trained on multi-modal data with the aim to successively distill knowledge from amulti-modal teacher to a single-modal student. Despite the widespread adoption of suchparadigm, recent research has highlighted its inherent limitations in the context of cross-modal knowledge transfer.\nTaking a step beyond the teacher/student paradigm, here we introduce a new frameworkfor cross-modal knowledge distillation, named DisCoM-KD (Disentanglement-learningbased Cross-Modal Knowledge Distillation), that explicitly models different types ofper-modality information with the aim to transfer knowledge from multi-modal data to asingle-modal classifier. To this end, DisCoM-KD effectively combines disentanglementrepresentation learning with adversarial domain adaptation to simultaneously extract, foreach modality, domain-invariant, domain-informative and domain-irrelevant features ac-cording to a specific downstream task. Unlike the traditional teacher/student paradigm,our framework simultaneously learns all single-modal classifiers, eliminating the needto learn each student model separately as well as the teacher classifier. We evaluatedDisCoM-KD on three standard multi-modal benchmarks and compared its behaviourwith recent SOTA knowledge distillation frameworks. The findings clearly demonstrate the effectiveness of DisCoM-KD over competitors considering mismatch scenarios in-volving both overlapping and non-overlapping modalities. These results offer insights toreconsider the traditional paradigm for distilling information from multi-modal data tosingle-modal neural networks. Our code is available at this link.", "sections": [{"title": "1 Introduction", "content": "The modern landscape is characterized by a large diversity of devices consistently sensing their environments. This influx of information poses new challenges in terms of data analysisand understanding, particularly for machine learning and computer vision models, whichmust work seamlessly across a spectrum of platforms. From wearable gadgets to autonomousvehicles, an array of sensors continuously collect data about the surroundings [9].\nIn this context, the same object or entity can be described by multiple modalities, ne-cessitating new learning paradigms to handle the collected heterogeneous information [15].Many multi-modal learning models assume that data modalities between the training anddeployment stages remain exactly the same [15]. However, due to the wide range of sensordata available, systematically accessing data across all sensor modalities may be infeasible.\nSpecifically, a set of modalities may be available during the training stage, while anotherset of modalities, either overlapping with the former or not, may be accessible during thedeployment stage [18]. In such a scenario, strategies are required to operate under cross-modal scenarios, leveraging the multi-modal information available during the training stageto enhance classification capabilities on the modalities accessible at deployment stage.\nRecently, Cross-Modal Knowledge Distillation (CMKD) has proven to be effective formulti-modal applications characterized by modalities mismatch [16]. The majority of ex-isting solutions rely on a teacher/student paradigm [5, 11, 19, 24], where a teacher modeltrained on one or several data modalities is then used to supervise a single-modal studentmodel trained on the modality available at deployment stage. However, this paradigm hasseveral shortcomings, such as the arbitrary choice of modalities used to train the teachermodel, the computational burden associated with the training of multiple models for specificdownstream applications, and the need to set up a separate process each time a single-modalstudent model needs to be trained. Additionally, recent studies in [28] clearly highlights theinherent limitations of this paradigm in automatically distilling useful information such asmodality-discriminative features for cross-modal knowledge transfer.\nIn this study, we aim to address the challenge of multi-modal learning in applicationscharacterized by modalities mismatch. We explore an alternative approach based on disen-tanglement representation and adversarial learning, overcoming mainstream teacher/studentparadigms and their associated limitations. Specifically, we introduce a novel framework forcross-modal knowledge distillation, called DisCoM-KD (Disentanglement-learning basedCross-Modal Knowledge Distillation). This framework explicitly models different typesof per-modality information to transfer knowledge from multi-modal data to a single-modalclassifier. DisCoM-KD effectively combines disentanglement representation learning withadversarial domain adaptation to simultaneously extract domain-invariant, domain-informative,and domain-irrelevant features for each modality, tailored to a specific downstream task.Conversely to traditional teacher/student paradigms, our framework learns all single-modalclassifiers simultaneously, eliminating the need to train each student model separately. Toevaluate the effectiveness of our framework, we consider three standard multi-modal bench-marks and recent state-of-the-art knowledge distillation frameworks, demonstrating DisCoM-KD's ability to outperform previous strategies in scenarios of modalities mismatch, coveringboth overlapping and non-overlapping modalities between training and deployment stages.\nIn summary, the contributions of our work are the following:\n\u2022 A novel CMKD framework based on disentanglement representation and domain ad-versarial learning;\n\u2022 An alternative CMKD strategy that circumvents traditional teacher/student paradigmsto distill knowledge from multi-modal data to single-modal neural networks;\n\u2022 An extensive comparison of DisCoM-KD across three computer vision multi-modal"}, {"title": "2 Related Work", "content": "In this section, we firstly provide a brief recap of general knowledge distillation strategies,then we focus on knowledge distillation for multi-modal learning and, finally, we concludeby discussing elements on disentanglement representation-based learning.\nKnowledge Distillation. Knowledge Distillation (KD) [7] was introduced to transfer \"dark\"knowledge from a teacher model to a lightweight student model by learning the studentmodel from the soft labels generated by the teacher. The standard KD loss formulation usedto train the student model is defined as follows:\n$L= \u03b1L_{task} + (1 \u2212 \u03b1)L_{KD}$                                                                       (1)\nHere, $L_{task}$ represents the downstream loss and $L_{KD}$ represents the distillation loss enforcingthe knowledge transfer from teacher to student, with \u03b1 determining the balance betweentwo terms. Different approaches vary in how they implement the $L_{KD}$ term, which can belogit-based [10], feature-based [4], or relation-based [8].\nRecent studies have shown that logit-based approaches outperform other strategies [21].Logit-based approaches implement the $L_{KD}$ component by exploiting the Kullback-Leiblerdivergence between the teacher and student logits. For example, [31] proposes to explicitlydecouple the target class from non-target classes in the knowledge distillation process. In[14], a curriculum learning process is introduced to estimate the temperature value to beused in the Kullback-Leibler divergence. [10] employs a multi-level approach to performlogit distillation at different granularity levels (instance, class, and batch), considering multi-ple temperature values to make the knowledge transfer more robust. Recently, [21] proposesa plug-in extension that can be combined with any of the previous frameworks in whichteacher and student logits are standardized prior to the analysis, ensuring a more coherentand consistent comparison.\nCross/Multi-Modal Knowledge Distillation. Cross-modal KD extends traditional KD ap-proaches to encompass multi-modal learning [28]. While cross-modal KD does not assumeany overlap between the modalities accessed by the teacher and student models, in the multi-modal KD scenario [16], the information used by the student is a subset of the modalitiesused by the teacher model, thus making the former a more general scenario than the lat-ter. However, in both scenarios, the student model typically has access only to a single datamodality. The majority of the proposed frameworks for both cross-modal and multi-modalKD [5, 11, 19, 24] are tailored for task specific use cases with lack of a generic solution. Thisis primarily due to the fact that, despite the empirical success demonstrated by prior works,the mechanisms behind cross-modal KD still remains loosely understood [28]. An initialinvestigation towards understanding this mechanism has been proposed in [28], where theauthors emphasize the significance of modality-discriminative features as key componentsfor cross-modal KD. However, their study provides preliminary experiments that are heavily"}, {"title": "3 Method", "content": "The proposed architecture, depicted in Figure 1, consists of two independent branches, onefor each modality, extracting several per-modality representations. These representationsare then used by per-modality task classifiers to make the final prediction. Furthermore,auxiliary classifiers, acting at intermediate stages, are leveraged to ensure that the extractedper-modality representations cover different complementary facets of the underlying infor-mation while also carrying information related to the downstream task.\nGiven an image $x^*$, where $x^*$ can be either $x_{M1}$ or $x_{M2}$ (with M1 and M2 being two different modalities), DisCoM-KD extracts three per-modality embeddings $z_{inv}^*$, $z_{inf}^*$, $z_{irr}^*$ referredas modality-invariant, modality-informative and modality-irrelevant representation, respec-tively. All the embeddings have the same dimensionality $z_{inv}^*, z_{inf}^*, z_{irr}^* \u2208 R^D$. Subsequently,\n$z_{inv}^*$ and $z_{inf}^*$ are fed to the (per-modality) main task classifiers, while $z_{irr}^*$ is discarded, asits objective is to collect/attract per-modality information that should not contribute to thedownstream task. Specifically, for each of the input modalities, we have a task classifier $Cl_*$that outputs class probabilities $\u0177_* = Cl_* ([z_{inv}^*||z_{inf}^*]) \u2208 R^C$ for the C existing classes and || denotes the concatenation operation. This means that two main task classifiers $Cl_{M1}$ and $Cl_{M2}$are trained during the process, with $\u0177_{M1} = Cl_{M1}([z_{inv}^{M1}||z_{inf}^{M1}])$ and $\u0177_{M2} = Cl_{M2}([z_{inv}^{M2}||z_{inf}^{M2}])$.\nBeyond the main architecture, we introduce several modules to ensure that the extractedembeddings represent complementary information derived from the multi-modal input data.These additional modules are: i) A modality classifier $Cl_{adv}$ coupled with gradient rever-sal layer [2] to facilitate the extraction of modality-invariant representations; ii) Two aux-iliary modality classifiers, $Cl_{m-inf}$ and $Cl_{m-irr}$, ensuring that modality-informative ($z_{inf}^*$) andmodality-irrelevant ($z_{irr}^*$) embeddings contain modality-specific information and iii) An aux-iliary task classifier $Cl_{aux}$ enforcing modality-invariant ($z_{inv}^*$) and modality-informative ($z_{inf}^*$)embeddings to be discriminative for the downstream task. During inference, only the per-modality extractors and the main task classifiers ($Cl_{M1}$ and $Cl_{M2}$) are retained, resulting intwo distinct models that have been jointly learnt and can be deployed independently of eachother."}, {"title": "3.1 Training losses", "content": "To train our cross-modal knowledge distillation framework DisCoM-KD, we design a set ofloss functions that explicitly model several properties beyond the main downstream classi-fication task with the aim to enforce disentanglement across complementary per-modalityrepresentations. Specifically, the training procedure optimizes five different loss terms.\nThe first term is directly related to the downstream classification task. Both modality in-variant ($z_{inv}^*$) and modality informative ($z_{inf}^*$) embeddings are fed to the main task classifiers.Then, we use standard Cross-Entropy loss (CE) between the output of each per-modalitytask classifier and the associated ground-truth y:\n$L_{cl} = \\sum_{m\u2208{M1,M2}}CE(Cl_m([z_{inv}^m || z_{inf}^m ]), y)$       (2)\nThe second term has the goal to enforce the learning of modality-invariant representa-tions. We adopt a CE loss over the output of a classifier that discriminates across represen-tations of samples from different modalities. Here, we use an adversarial training strategy,implemented via Gradient Reversal Layer (GRL) [2] over the modality invariant ($z_{inv}^*$) representations:\n$L_{adv} =  \\sum_{m\u2208{M1,M2}}CE (Cl_{adv} (GRL(z_{inv}^m)), m)$           (3)\nThe third term guides the learning of modality-aware representations via modality clas-sifiers. We use two classifiers, one for modality informative ($Cl_{m-inf}$) and one for modalityirrelevant ($Cl_{m-irr}$) embeddings in order to predict from which branch the embedding origi-nates:\n$L_{mod} =  \\sum_{m\u2208{M1,M2}}CE (Cl_{m-inf}(z_{inf}^m), m) +  \\sum_{m\u2208{M1,M2}}CE(Cl_{m-irr}(z_{irr}^m), m)$(4)\nThe fourth term aims to enhance the task discriminative information carried by the per-modality embeddings. Here, we employ an auxiliary task classifier over the set of modality"}, {"title": "4 Experimental Evaluation", "content": "To evaluate our framework, DisCoM-KD, we designed an experimental evaluation consider-ing three different multi-modal benchmarks involving SOTA teacher/student strategies fromthe Knowledge Distillation field encompassing both cross-modal and multi-modal KD sce-narios. Additionally, an ablation study of DisCoM-KD is proposed to analyze the interplaybetween its different components.\nDATASETS. As datasets we consider: i) SUNRGBD, the version proposed in [1] for multi-modal RGB-D scene classification. We consider RGB and Depth images from the Kinect v2domain, for a total of 2 105 pairs of RGB/Depth images, with 3 and 1 channels respectively,covering 10 classes; ii) EuroSat-MS-SAR proposed in [26] for multi-modal Multi-Spectral(MS) and Synthetic Aperture Radar (SAR) remote sensing scene classification. The datasetcontains 54000 pairs of MS and SAR images, with 13 and 2 channels respectively, for aland cover classification task spanning 10 classes; iii) TRISTAR proposed in [20] for multi-modal (RGB, Thermal and Depth) action recognition. According to results reported in [20],here we only consider the two most informative modalities (Thermal and Depth). The datasetcontains 14201 pairs of Thermal and Depth images, with 1 channel each, representing anaction recognition task spanning 6 classes.\nCOMPETING METHODS. We adopt three recent state-of-the-art strategies: DecoupledKnowledge distillation (DKD) [31], Curriculum Temperature Knowledge Distillation (CTKD)[14] and Multi-Level Knowledge Distillation (MLKD) [10]. Furthermore, we integrate twobaseline methods proposed in [28], referred to as KDv1 and KDv2. Both baselines imple-ment the traditional knowledge distillation loss reported in Equation 1, with KDv1 settingthe \u03b1 hyper-parameter to 0, while KDv2 sets it to 0.5. While KDv1 only uses the soft labelto train the student model, KDv2 equally weighs the information from the original hard la-bels and the teacher soft labels. We combine each of these five strategies with the plug-inlogit standardization preprocessing (LSKD) proposed in [21]. Additionally, as references,we report the performance of the teacher model (referred to as TEACHER) and a studentmodel that has not received any distillation supervision (referred to as STUDENT) for eachevaluation scenario.\nEVALUATION SCENARIOS. We adopt two evaluation scenarios: cross-modal KD andmulti-modal KD. For the cross-modal KD scenario the teacher is trained on the richest, interms of downstream task performances, modality and, successively a single-modal studentis distilled leveraging the remaining modality. Here the teacher is implemented via a ResNet-18 [6] architecture. For the multi-modal KD scenario the teacher model is trained on the fullset of per-dataset modalities and, successively, a single-modal student is distilled. For thisscenario, the teacher model is a two-branch architecture with a per modality encoder imple-mented via a ResNet-18. The fusion is performed at the penultimate layer of the ResNet-18architecture via feature element-wise addition. Finally, a linear layer exploits the fused rep-resentation for the classification decision. All the student models are implemented with aResNet-18 architecture.\nEXPERIMENTAL SETTINGS. For all the approaches the same training setup is used: 300training epochs, a batch size of 128 and Adam [13] as parameters optimizer with a learningrate of 10-4. For all the approaches we use online data augmentation via geometrical trans-formations (e.g. flipping and rotation). For the competing methods, we adopt the originalhyper-parameter settings. The assessment of the models performance, on the test set, is doneconsidering the weighted F1-Score, subsequently referred simply as F1-Score. Each datasetis divided into training, validation and test set with a proportion of 70%, 10% and 20% of"}, {"title": "4.1 Results", "content": "Table 1 and Table 2 present the average F1-Score results of the competing methods on bothcross-modal and multi-modal KD scenarios, respectively. We use green arrows to indicatewhen a model outperforms the STUDENT baselines, and red arrows otherwise.\nIn the cross-modal KD scenario, Table 1, we consider the following cases: RGB \u2192 DEPTHfor SUNRGBD, MS \u2192 SAR for EuroSat-MS-SAR, and THERMAL \u2192 DEPTH for TRIS-TAR. Here, the left modality indicates the one used by the teacher model while the one on theright is leveraged by the student. We observe that DisCoM-KD outperforms all competitorson both SUNRGBD (47.69 vs. 42.87 achieved by the best competitor) and EuroSat-MS-SAR (80.03 vs. 78.89 achieved by the best competitor). Moreover, it achieves comparableperformance with the best competitor on TRISTAR (92.86 vs. 93.06 achieved by the bestcompetitor). Notably, our framework is the only one that consistently improves (as indicatedby green arrows) over the STUDENT baseline across all considered cross-modal scenarios.\nIn the multi-modal KD scenario, Table 2, DisCoM-KD outperforms all state-of-the-artKD approaches, consistently improving classification performance compared to the STU-DENT baseline. It is worth noting that our framework is the only one that achieves im-provement on the TRISTAR dataset when the THERMAL modality is considered for thedeployment stage, achieving a classification score of 97.06. On EuroSat-MS-SAR, all com-petitors are capable of distilling a student single-modal neural network that outperforms theTEACHER model. Also in this case, DisCoM-KD achieves the best classification perfor-mance with a score of 98.12. Interestingly, we observe that depending on the dataset, teach-ers trained on multiple modalities are not always the best choice for distilling a single-modalstudent. For example, in the TRISTAR case, when the deployment stage covers the DEPTHmodality, all KD frameworks exhibited their best performances when the TEACHER hasbeen only trained on the THERMAL modality (cross-modal KD scenario) rather than on thewhole set of modalities (multi-modal KD scenario). This underscores that no strategy (nei-ther cross-modal nor multi-modal) guarantees a systematic improvement, highlighting thearbitrary impact this inherent choice can have on the underlying distillation process.\nAblations. The first ablation study (Table 3) explores the importance of the different com-ponents on which our framework is built. We observe that Auxiliary Task Classifiers ($L_{aux}$)and the Disentanglement Loss ($L_{\\bot}$) seem to play the most significant roles in the underlyingprocess. Depending on the considered dataset, each loss term has different relative impacts,and on average, the highest performance is achieved when all components are involved,underscoring the rationale behind the proposed framework. The second ablation study (Ta-ble 4) investigates the interplay between the different representations extracted by the disen-tanglement process. Here, we note that only considering one of the two groups of informa-tion \u2013modality-invariant ($z_{inv}^*$) or modality-informative ($z_{inf}^*$) \u2013 systematically decreases theclassification performances. Modality-informative features provide slightly better discrimi-nation capability, with varying margins depending on the dataset. In summary, this analysissuggests the suitability of exploiting both modality-invariant and modality-informative rep-resentations for the downstream classification task."}, {"title": "5 Conclusion", "content": "In this study we have introduced a new framework for cross-modal knowledge distillation,namely DisCoM-KD. Our aim is to transfer knowledge from multi-modal data to a single-modal classifier. To this end, our framework effectively combines disentanglement repre-sentation learning with adversarial domain adaptation. Experimental evaluation, consider-ing both cross-modal and multi-modal knowledge distillation evaluation scenarios, demon-strates the quality of DisCoM-KD compared to recent state-of-the-art KD techniques basedon the standard teacher/student paradigm. In addition to performance improvements, ourframework offers several inherent advantages over the standard paradigm: i) it learns allsingle-modal classifiers simultaneously, eliminating the need to train each student modelseparately; ii) it avoids the use of a teacher model, thereby eliminating the need to selectwhich set of data modalities must be used to train the teacher model. Furthermore, ourresearch work introduces an alternative strategy that opens new opportunities beyond thetraditional teacher/student paradigm commonly employed for cross-modal and multi-modalknowledge distillation.\nSeveral possible future avenues can be drawn. Our current process has only been assessedon cross-modal distillation tasks involving no more than two modalities. Extending DisCoM-KD to manage more than two modalities at once remains an open question. While most ofthe terms of the proposed loss function can be directly adapted to multiple modalities, howto modify the adversarial term to cope with more than two modalities is not straightforward.Another possible follow-up could investigate how to take inspiration from DisCoM-KD todesign multi-modal distillation frameworks dealing with semantic segmentation and objectdetection tasks. For these tasks, the common methodologies are based on encoder/decoderalgorithmic architectures that provide dense predictions as result. All these elementsprevent the direct application of our methodology requiring to rethink how disentanglementand adversarial learning may be defined and implemented."}]}