{"title": "CAPO: COOPERATIVE PLAN OPTIMIZATION FOR EFFICIENT EMBODIED MULTI-AGENT COOPERATION", "authors": ["Jie Liu", "Pan Zhou", "Yingjun Du", "Ah-Hwee Tan", "Cees G.M. Snoek", "Jan-Jakob Sonke", "Efstratios Gavves"], "abstract": "In this work, we address the cooperation problem among large language model (LLM) based embodied agents, where agents must cooperate to achieve a common goal. Previous methods often execute actions extemporaneously and incoherently, without long-term strategic and cooperative planning, leading to redundant steps, failures, and even serious repercussions in complex tasks like search-and-rescue missions where discussion and cooperative plan are crucial. To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation schemes, CaPo improves cooperation efficiency with two phases: 1) meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the first phase, all agents analyze the task, discuss, and cooperatively create a meta-plan that decomposes the task into subtasks with detailed steps, ensuring a long-term strategic and coherent plan for efficient coordination. In the second phase, agents execute tasks according to the meta-plan and dynamically adjust it based on their latest progress (e.g., discovering a target object) through multi-turn discussions. This progress-based adaptation eliminates redundant actions, improving the overall cooperation efficiency of agents. Experimental results on the ThreeDworld Multi-Agent Transport and Communicative Watch-And-Help tasks demonstrate CaPo's much higher task completion rate and efficiency compared with state-of-the-arts.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language, complex reasoning, and planning, achieving impressive performance (OpenAI, 2024; Touvron et al., 2023). These advancements empower LLM-based embodied agents to autonomously make plans (Li et al., 2023a; Padmakumar et al., 2022; Zhu et al., 2023; Wang et al., 2023; Wu et al., 2023b; Huang et al., 2022b) and perform reasoning (Du et al., 2023; Hao et al., 2023; Zhou et al., 2024; Huang et al., 2022a) by using human language to assist people in daily activities, such as housework and daily chores. The next milestone for agents is to cooperate with others to achieve joint tasks. This is crucial not only for efficiently performing simple tasks but also for tackling complex ones that cannot be completed in isolation due to their inherent complexity or the dynamic nature of the environment (Zhang et al., 2023b; Guo et al., 2024; Mandi et al., 2023; Zhang et al., 2023a).\nNotably, the cooperation among LLM-based embodied agents is rarely investigated despite being highly desired. Conventional works often focus on adopting reinforcement learning (RL) (Jiang & Lu, 2018; Liu et al., 2021; Wang et al., 2021) to explore the dynamics of cooperative behavior among non-LLM-based agents. In spite of their promising performance in certain scenarios, RL-based cooperation methods exhibit limited adaptability across different tasks (Dittadi et al., 2021; Cobbe et al., 2019), since they are often not trained on large-scale data and lack sufficient generalization ability. To solve this issue, in this work, we are particularly interested in the problem of \"how to develop an effective collaboration framework for LLM-based agents\", since LLMs have revealed"}, {"title": "2 RELATED WORK", "content": "LLM-based Agents. LLM-based agents (Hong et al., 2023; Wang et al., 2024; Shen et al., 2024; Liu et al., 2023a) are designed to autonomously perceive environments, execute actions, accumulate knowledge, and evolve themselves, with rich real world knowledge and complex reasoning capability inherited from LLMs. Notable agents like AutoGPT (Richards & et al, 2021), BabyAGI (Nakajima, 2023), and AgentGPT (Reworkd, 2023) showcase remarkable proficiency in decision-making and complex reasoning. In the embodied environment, LLM-based agents have shown superior capacity in strategic planning (Li et al., 2023a; Padmakumar et al., 2022; Wu et al., 2023b; Huang et al., 2022b). Specifically, LLM-planner (Song et al., 2023) harness LLMs to do few-shot planning for embodied agents. PET (Wu et al., 2023a) translates a task description with LLMs into a list of high-level sub-tasks. TaPA wu2023embodied enables the agent to generate executable plans by aligning LLMs with visual perception models. Another line of research focuses on harnessing LLMs's reasoning capabilities in embodied tasks (Zhou et al., 2024; Huang et al., 2022a). ELLM (Du et al., 2023) utilizes LLMs to set pretraining goals in RL, guiding agents towards the goal without human involvement.\nMulti-Agent Cooperation. Multi-agent cooperation and communication have been studied for decades to improve communication efficiency (Jiang & Lu, 2018; Li et al., 2023b) and planning (Torreno et al., 2017; Zhang et al., 2023a). Within the domain of embodied intelligence, ProAgent (Zhang et al., 2023a) harnesses LLMs to develop proactive agents that dynamically adjust their behavior to foster better cooperation with teammates. RoCo (Mandi et al., 2023) introduce a multi-robot collaboration framework that employs LLMs for both high-level communication and low-level path planning. (Guo et al., 2024) proposed a prompt-based organizational framework for LLM agents to reduce communication costs and boost team efficiency. CoELA (Zhang et al., 2023b) enables agents to plan, communicate, and collaborate effectively, but its plan is one-step plan and is short-term. Despite these advancements, these methods focus on short-term planning and do not involve sufficient agent discussion, while ours seeks to a long-term strategical and coherent plan via agent's thoughtful discussions for efficient multi-agent cooperation.\nOptimization with LLMs. With the advancement of prompting techniques, LLMs have shown remarkable performance across various domains (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Zhou et al., 2022; Madaan et al., 2024). Their ability to understand natural language lays out a new possibility for optimization. (Yang et al., 2023) first proposed to leverage LLMs as optimizer, where the optimization task is described in natural language. OPT2I (Ma\u00f1as et al., 2024) aims to enhance prompt-image consistency in text-to-image models by iteratively generating revised prompts with LLMs to maximize the consistency score. VislingInstruct (Zhu et al., 2024) proposes optimizing multi-modal instruction for multi-modal language models in a zero-shot manner. DyLAN (Liu et al., 2023b) is particularly relevant to our work. DyLAN (Liu et al., 2023b) enables agents to interact for multiple rounds in a dynamic architecture to optimize the selection of agent. In contrast, our work investigates cooperative plan optimization via multi-turn discussion between agents."}, {"title": "3 PRELIMINARIES", "content": "We follow previous work (Zhang et al., 2023b; Gong et al., 2023) and formulate the embodied multi-agent cooperation task as an decentralized partially observable Markov decision process (DEC-POMDP) (Bernstein et al., 2002; Spaan et al., 2006), which is defined as < n, S, O, A, P, r, \u03b3 >. Here, n represents the number of agents; S is the finite state space; O denotes the observation space; A is a finite joint action space of all agents; P : S \u00d7 A \u00d7 S \u2192 [0, 1] denotes the transition probability function; r = S \u00d7 A \u2192 R denotes the reward function; \u03b3 \u2208 [0, 1] denotes the discount factor. In this framework, at time step t \u2208 N, each agent i observes the environment's state st \u2208 S, and receives an observation set Oi. Oi consists of a world observation O\u03c9, which the agent gathers through its sensors, or a communication message observation O\u00b5 from other teammate agents. Agent i takes actions from its action space Ai, which includes a finite set of world action A\u03c9, e.g., grasping a target object, or a finite set of messaging action A\u00b5. Then agents receive a shared reward rt = r(st, at), where at \u2208 A denotes the joint actions of agents, and observe a new state st+1 with probability P(st+1|st, at). We formulate the problem with two decentralized intelligent embodied agents working together to complete a long-horizon rearrangement task (Zhang et al., 2023b; Batra et al., 2020) in a multi-room indoor environment. During the task, agents can execute multiple kinds of actions, such as navigation, interaction, and communication by sending messages."}, {"title": "4 COOPERATIVE PLAN OPTIMIZATION", "content": "We first introduce the overall framework of CooperAtive Plan Optimization (CaPo) for LLM-based embodied agents in Sec. 4.1. We then respectively elaborate on the two key phases of CaPo, i.e., meta-plan generation and progress-adaptive meta-plan and execution, in Sec. 4.2 and Sec. 4.3."}, {"title": "4.1 OVERALL FRAMEWORK OF CAPO", "content": "CaPo aims to enhance cooperation efficiency of LLM-based embodied agents. Its key idea is to create a long-term meta-plan for strategically and coherently coordinating agents to complete a rearragement task. Accordingly, agents follow the meta-plan to complete task step by step, and dynamically adapt the meta-plan to their latest progress, thereby avoiding redundant work allocation and improving overall cooperation efficiency."}, {"title": "4.2 \u039c\u0395\u03a4A-PLAN GENERATION", "content": "To generate the long-term meta-plan which coordinates all agents to accomplish tasks efficiently, CaPo introduces two key steps, including 1) meta-plan initialization where one agent initializes a meta-plan according to the task description and existing information, and 2) meta-plan evaluation and optimization where all agents evaluate the meta-plan and provide feedback to improve the plan.\nMeta-plan Initialization. At the beginning of a task, the task description is provided to all agents, e.g, Transport 2 apples and 3 bananas to the bed. One agent, e.g., Alice in Fig. 2, is randomly selected as the meta-plan designer, and creates the meta-plan through a cooperative planning module. Note that the meta-plan here, as illustrated in Fig. 3, differs from the short-term or unorganized plans used in previous work (Zhang et al., 2023b;a; Mandi et al., 2023). Specifically, the cooperative planning module is equipped with a pre-trained LLM, and leverage the LLM to generate the meta-plan. The prompting for the LLM is organized as follows:\nPrompt:  +  \n.\nLLM: \nHere, , ,and  are three placeholders for the task description, instruction head, and generated meta-plan. The task description provides background descriptions about the task, while the instruction head introduces additional constraints into the generation of meta-plan, such as the format of meta-plan and available actions to generate a clear and executable plan. Detailed prompt design is shown in Fig. 9 of Appendix.\nMeta-plan Evaluation and Optimization. The meta-plan generated by a single agent is often biased by that agent's partial observations, resulting in a suboptimal plan that fails to coordinate all agents effectively. To address this issue, CaPo involves all agents in a multi-turn discussion to optimize the meta-plan. Specifically, the meta-plan designer (e.g., Alice in Fig. 3) broadcasts the meta-plan to all teammate agents, while teammate agents (e.g., Bob in Fig. 3) serve as meta-plan evaluators, providing feedback about the meta-plan. Since teammate agents have different partial observations of the environment, they provide the meta-plan designer with better situational"}, {"title": "4.3 PROGRESS-ADAPTIVE META-PLAN & EXECUTION", "content": "The optimized meta-plan acts as a high-level guide, assigning subtasks to each agent and coordinating them for efficient task completion. However, due to dynamic environmental changes and task progress updates, the meta-plan can become outdated during execution. As illustrated in Fig. 4, agents may encounter significant progress, such as discovering target objects or completing subtasks, necessitating adjustments to the meta-plan. In such cases, the previous plan becomes less effective or invalid for coordinating the agents.\nTo address this, we design a progress-adaptive planning module for CaPo for adapting the meta-plan to the agents' latest progress. This module follows a similar process as described in Sec. 4.2\u2014meta-plan initialization, evaluation, and optimization\u2014but with modified prompting strategies for the LLMs. Whenever an agent makes new progress, the meta-plan designer promptly generates an updated meta-plan, followed by a multi-turn discussion among all agents to further optimize it. The LLM prompting strategies for the progress-adaptive planning module are structured as follows:\nPrompt:  +  +  + \n.\nLLM: .\nHere we introduce two placeholders,  and , to capture the task progress of agents and enable the LLM to generate progress-aware responses, such as meta-plans or communication messages. Agents engage in discussions to optimize the meta-plan until a consensus is reached or communication resources are exhausted (e.g., after three discussion rounds). Detailed prompt designs for the LLMs\u2014responsible for generating the meta-plan and facilitating messages for both the meta-plan designer and evaluator\u2014are provided in Fig. 11~12.\nOnce the meta-plan or progress-adaptive meta-plan is established, each agent autonomously transforms the plan into executable actions via a plan parsing module and an execution module. The plan parsing module generates the latest sub-plan by retrieving relevant information from the memory module and converting it into text descriptions, and then compiles an Action List of all available high-level sub-plans. We implement the plan parsing module as a pretrained LLM, and prompt it with a concatenation of Instruct Head, Task Description, meta-plan, Action History, Agent Progress, and Action List to choose the most suitable sub-plan. See Fig. 13 in Appendix for more prompt details. Given the sub-plan, we adopt a similar execution module as in (Zhang et al., 2023b) to generate primitive actions for executing the sub-plan."}, {"title": "5 EXPERIMENTS", "content": "Benchmarks. We follow CoELA, and adopt the ThreeDworld Multi-Agent Transport (TDW-MAT) task (Zhang et al., 2023b), and the Communicative Watch-And-Help (C-WAH) task (Zhang et al., 2023b) to test our CaPo. TDW-MAT is built on the general purpose virtual world simulation platform TDW platform (Gan et al., 2020), and requires agents to move objects by their hands or containers which can contain several objects for efficient moving to the destination. Moreover, agents can receive ego-centric 512\u00d7512 RGB-D images as observation and can communicate with others. The test set of TDW-MAT consists 24 episodes, which evenly divided into food and stuff tasks. In C-WAH, agents are requested to complete five types of household activities, represented as various predicates with specific counts that must be satisfied. The test set contains 10 episodes, including both symbolic and visual observation settings. More details about TDW-MAT and C-WAH environments are provided in Appendix B.1 and B.2, respectively.\nMetrics. On TDW-MAT, we adopt Transport Rate, i.e., the fraction of subtasks completed within 3000 time steps (a.k.a. frames), as performance metric. Note, one action step may last multiple time steps, e.g., resetting arms. On C-WAH, Average Steps to complete all tasks is used as the metric to evaluate cooperation efficiency.\nImplementation. Following CoELA, we test two settings on TDW-MAT task: 1) a real-world setting where the perception module is instantiated as Mask-RCNN (He et al., 2017) that is trained us-"}, {"title": "5.1 MAIN RESULTS", "content": "Performance comparison. We follow COELA to test two-agent cooperation setting, and compare with classical methods like MHP and RHP, and LLM-driven methods CoELA, ProAgent, and RoCo.\nsummarizes the performance of all compared methods under the two settings of the TDW-MAT task, and shows several observations. 1) Compared with the single-agent baseline RHP, CaPo and all two-agent baselines consistently make significant improvements, showing the effectiveness of multi-agent cooperation in embodied tasks. 2) In multi-agent comparisons, our CaPo outperforms LLM-driven methods by a remarkable margin, e.g., respectively making 16.7% and 8.4% improvement over CoELA and RoCo under the oracle perception setting. 3) CaPo with different LLMs as agent brain exhibits consistent superior performance across all settings. Indeed, CaPo with LLAMA-2 achieves comparable performance with CoELA with GPT-3.5-turbo under oracle perception setting. The improvement of CaPo is derived from its meta-plan and progress-adaptive meta-plan, which both provide strategical and coherent guidance for agent cooperation, thereby improving cooperation performance.\nreports the performance of all methods on the C-WAH task, and shows similar and consistent observations to those on the TDW-MAT task. Specifically, with GPT-4 agents, our CaPo respectively"}, {"title": "5.2 ABLATION STUDY", "content": "Effects of each component in CaPo. Here we examine the effects of two key components: 1) meta-plan generation, which includes meta-plan initialization, evaluation, and optimization, and 2) the progress-adaptive meta-plan. To evaluate their impact, we first remove both components from CaPo, resulting in CaP01. As shown in Table 3, CaPo2, which includes meta-plan initialization but freezes the meta-plan during subsequent procedures, improves upon CaP01 by approximately 1% across three metrics, demonstrating the value of meta-plan initialization. Similarly, CaPo3, which incorporates the full meta-plan generation process, outperforms CaPo2 by a significant margin, highlighting the benefits of meta-plan evaluation and optimization. Finally, CaPo achieves a 7% improvement over CaPo3, showcasing the effectiveness of the progress-adaptive meta-plan. These results underscore the importance of each component in the CaPo framework."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce Cooperative Plan Optimization (CaPo) to enhance cooperation efficiency of LLM-driven embodied agents. CaPo first proposes to create a strategic and coherent meta-plan through multi-turn agents discussion before executing any actions. CaPo first proposes creating a strategic and coherent meta-plan through multi-turn discussions among agents before executing any actions. This meta-plan serves as an action guide to efficiently coordinate multiple agents in completing tasks. During the execution phase, agents dynamically adapt the meta-plan to their latest task progress, maintaining the effectiveness of the meta-plan in coordinating agents to complete tasks efficiently. Experimental results on TDW-MAT and C-WAH tasks show the higher task completion rates and efficiency of CaPo compared to state-of-the-arts.\nWhile CaPo significantly improves multi-agent cooperation efficiency, it has limitations, specifically its heavy reliance on LLMs for reasoning and planning during meta-plan generation and adaptation."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We provide detailed descriptions of the two aforementioned embodied environments in Sec.B, covering task settings, as well as the observation and action spaces of the agents. Additionally, we present the detailed prompt designs used in our LLMs in Sec.C of the Appendix. Furthermore, we include a section in Appendix Sec. A.3 to demonstrate the reproducibility of our experimental results on the TDW-MAT environments."}, {"title": "A.1 BASELINE MODELS", "content": "We adopt two types of methods as our baseline, including classical agents and LLM-driven multi-agents. (1) The classical agents include MCTS-based Hierarchical Planner (MHP) (Puig et al., 2020) which is a hierarchical planner originating from the original Watch-And-Help Challeng, and Rule-based Hierarchical Planner (RHP) (Gan et al., 2022) derived from a strong baseline in the ThreeDWorld Transport Challenge. (2) LLM-driven agents consist of CoELA Zhang et al. (2023b), ProAgent (Zhang et al., 2023a), and RoCo (Mandi et al., 2023). Cooperative Embodied Language Agent (CoELA) (Zhang et al., 2023b) can plan, communicate, and collaborate with other agents to complete long-horizon tasks, but generate independent short-term plan for each agent. In addition, we also introduce two more baselines - ProAgent (Zhang et al., 2023a) and RoCo (Mandi et al., 2023), and implement them on TDW-MAT and C-WAH using source codes. These two baselines generate joint plans for cooperative agents, and introduce a reflection loop or environment feedback for plan validation."}, {"title": "A.2 RESULTS WITH GPT-4", "content": "Here we further provide results on TDW-MAT task using GPT-4. We can observe that our proposed method CaPo achieves consistently better performance that baseline methods across all tasks, including food and stuff transportation. This also demonstrate the effectiveness of our method on improving multi-agent cooperation."}, {"title": "A.3 REPRODUCIBILITY OF RESULTS", "content": "LLM-driven reasoning and planning tend to be stochastic, requiring multiple runs to assess stability. To verify the stability and reproducibility of our method, we conducted three runs on TDW-MAT with oracle perception and GPT-3.5 agents. As shown , the results exhibit minor variance across runs, demonstrating the stability and reproducibility of our method."}, {"title": "B ADDITIONAL ENVIRONMENT DETAILS", "content": "We evaluate our method and all baseline methods in two simulated environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). We follow CoELA Zhang et al. (2023b) and list detailed introductions to these environments below."}, {"title": "B.1 THREEDWORLD MULTI-AGENT TRANSPORT", "content": "Tasks. TDW-MAT consists of two types of tasks, food-transporting task and stuff-transporting task. The food-transporting task has 6 types of targets (apple, banana, orange, bread, loaf bread, and burger) and 3 containers (bowl, plate, and tea tray). In contrast, the stuff-transporting task has 6 different types of targets(calculator, mouse, pen, lighter, purse, and iPhone) and 3 containers (plastic basket, wood basket, and wicker basket). In each task, there are 10 target objects and 2 to 5 containers in total. Additionally, there are 4 types of rooms: living room, office, kitchen, and bedroom, and objects are placed in these rooms consistent with common sense. The agents are tasked with transporting as many target objects as possible to the goal position using containers as tools. Each container can carry up to three objects, while without a container, an agent can transport only two objects at a time. The agents must transport as many target objects as possible within 3000 frames.\nObservation Space The embodied agent receives a variety of observations, with the primary ones being an egocentric RGB image and a depth image. Additionally, there are several auxiliary observations. The observation space includes:\n\u2022 RGB image: This is an egocentric image captured by a forward-facing camera, with a resolution of 512 \u00d7 512 and a field of view of 90 degrees.\n\u2022 Depth image: This image shares the same camera intrinsic parameters as the RGB image.\n\u2022 Oracle Perception (optional): An image where each object ID is represented by a distinct color, using the same camera intrinsic parameters as the RGB image.\n\u2022 Agent position and rotation: The position and rotation of the agent within the simulation environment.\n\u2022 Messages: Communications sent by all agents.\n\u2022 Held objects: Information about the objects currently held by the agent.\n\u2022 Opponent held objects: Information about objects held by another agent, if the agent is within view.\nAction Space In TDW-MAT, agents can perform 7 distinct types of actions to interact with the environment or communicate with each other. Each action spans multiple frames, and the detailed action space is outlined below:\n\u2022 Move forward: The agent advances by 0.5m.\n\u2022 Turn left: The agent rotates left by 15 degrees.\n\u2022 Turn right: The agent rotates right by 15 degrees.\n\u2022 Grasp: The agent grasps an object, successfully performing this action only when in close proximity to the object. The object can be either a target or a container.\n\u2022 Put In: The agent places a target into a container, an action that is possible only when the agent is holding a target in one hand and a container in the other.\n\u2022 Drop: The agent releases the objects held in hand.\n\u2022 Send message: The agent sends a message to other agents, with a limit of 500 characters per frame."}, {"title": "\u0412.2 \u0421\u043e\u043cMUNICATIVE WATCH-AND-HELP", "content": "Communicative Watch-And-Help (C-WAH) builds upon the Watch-And-Help challenge Puig et al. (2021) by incorporating the ability for agents to send messages to one another. Sending messages, like other actions, consumes one timestep and is subject to a maximum length constraint."}, {"title": "C PROMPT TEMPLATE", "content": "We list the prompts template for meta plan initialization, communication module of Alice, communication module of Bob, cooperative planning module, and the plan parsing module as follows.\nI am Alice. My teammate Bob and I want to transport as many target objects as possible to the bed with the help of containers within 3000 steps. I can hold two things at a time, and they can be objects or containers. I can grasp containers and put objects into them to hold more objects at a time.\nAssume that you are an expert plan outline designer. Given our shared goal, please help me generate a global meta plan for me and Bob during task execution, guiding me and Bob to achieve the goal collaboratively as soon as possible. Note that a container can contain three objects, and will be lost once transported to the bed. I can only put objects into the container I hold after grasping it. All objects are denoted as (id), such as (712). Actions take several steps to finish. It may be costly to go to another room or transport to the bed, use these actions sparingly.\nThe generated meta plan must meet following requirements:\n1.There are 5 allowed actions you can use to construct the meta plan. 1) 'go to': move to a specified room. 2) 'explore': explore a room for underlying target objects. 3) 'go grasp': go to grasp a specified target object. 4) 'put': Place an object into a specified container. 5) 'transport': Transport holding objects or containers to the bed.\n2. The meta plan should be concise, brief, and reliable.\n3.The meta plan must be structured strictly in the three-step format: {Action Plan: Step 1: Alice xxx, Bob xxx; Step 2: Alice xxx, Bob xxx; Step 3: Alice xxx, Bob xxx}. Here, 'xxx' represents one or multiple allowed actions. The actions in Step 1 are of the highest execution priority, while those in Step 2 and Step 3 are of medium and lowest execution priority.\n4.The meta plan should reasonably arrange the division of action between Alice and Bob in order to achieve the goal as soon as possible.\nHere is an example for you:\n{Goal: [Transport 3 pens, 1 lighter, and 3 iPods to the bed.]\nMeta plan: [Step 1: Alice explores the current room. Bob explores the current room. Step 2: If any target objects are found, Alice and Bob go grasp objects, put them into containers, and transport them to the bed. Step 3: Alice goes to one of the remaining rooms and explores it. Bob goes to one of the remaining rooms and explores it]}\nGoal: $GOAL$\nGiven the above goal, think step by step, and generate the meta plan:"}]}