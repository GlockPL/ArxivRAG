{"title": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou", "authors": ["Zihua Si", "Lin Guan", "Zhongxiang Sun", "Xiaoxue Zang", "Jing Lu", "Yiqun Hui", "Xingchao Cao", "Zeyu Yang", "Yichen Zheng", "Dewei Leng", "Kai Zheng", "Chenbin Zhang", "Yanan Niu", "Yang Song", "Kun Gai"], "abstract": "The significance of modeling long-term user interests for CTR prediction tasks in large-scale recommendation systems is progressively gaining attention among researchers and practitioners. Existing work, such as SIM and TWIN, typically employs a two-stage approach to model long-term user behavior sequences for efficiency concerns. The first stage rapidly retrieves a subset of sequences related to the target item from a long sequence using a search-based mechanism namely the General Search Unit (GSU), while the second stage calculates the interest scores using the Exact Search Unit (ESU) on the retrieved results. Given the extensive length of user behavior sequences spanning the entire life cycle, potentially reaching up to 106 in scale, there is currently no effective solution for fully modeling such expansive user interests. To overcome this issue, we introduced TWIN-V2, an enhancement of TWIN, where a divide-and-conquer approach is applied to compress life-cycle behaviors and uncover more accurate and diverse user interests. Specifically, a hierarchical clustering method groups items with similar characteristics in life-cycle behaviors into a single cluster during the offline phase. By limiting the size of clusters, we can compress behavior sequences well beyond the magnitude of 105 to a length manageable for online inference in GSU retrieval. Cluster-aware target attention extracts comprehensive and multi-faceted long-term interests of users, thereby making the final recommendation results more accurate and diverse. Extensive offline experiments on a multi-billion-scale industrial dataset and online A/B tests have demonstrated the effectiveness of TWIN-V2. Under an efficient deployment framework, TWIN-V2 has been successfully deployed to the primary traffic that serves hundreds of millions of daily active users at Kuaishou.", "sections": [{"title": "1 INTRODUCTION", "content": "Click-Through Rate (CTR) prediction is crucial for internet applications. For instance, Kuaishou\u00b9, one of China's largest short-video sharing platforms, has employed CTR prediction as a core component of its ranking system. Recently, much effort [1, 2, 24] has been devoted to modeling users' long-term historical behavior in CTR prediction. Due to the extensive length of behaviors across the life cycle, modeling life-cycle user behaviors presents a challenging task. Despite ongoing efforts to extend the length of historical behavior modeling in existing research, no method has yet been developed that can model a user's entire life cycle, encompassing up to 1 million behaviors within an app.\nModern industrial systems [1, 13], to utilize as long a user history as possible within a controllable inference time, have adopted a two-stage approach. Specifically, in the first stage, a module called the General Search Unit (GSU) is used to filter long-term historical behaviors, selecting items related to the target item. In the second stage, a module called the Exact Search Unit (ESU) further processes the filtered items, extracting the user's long-term interests through target attention. The coarse-grained pre-filtering by the GSU enables the system to model longer historical behaviors at a faster speed during online inference. Many approaches have tried different GSU structures to improve the accuracy of pre-filtering, such as ETA [3], SDIM [1] and TWIN [2].\nDespite their effectiveness, existing first-stage GSUs generally have length limitations and cannot model user behaviors throughout the life cycle. For instance, SIM, ETA, and SDIM can filter historical behaviors up to a maximum length of 103. TWIN extends the maximum length at the 104 to 105 level. During deployment, TWIN utilizes the recent 10, 000 behaviors as inputs for GSU. Unfortunately, these 10,000 behaviors only cover the user's history for the last 3-4 months in the Kuaishou app, failing to encompass the entire life cycle of user behavior. As illustrated in Figure 1, we analyzed user behavior in the Kuaishou app over the past three years. The medium and high user groups can view between 104 to 106 videos over three years, contributing the majority of the app's usage duration (60% + 35% = 95%). Hence, modeling the full life-cycle behaviors may enhance user experience and boost the platform's commercial gains."}, {"title": "2 RELATED WORK", "content": "Click-Through Rate Prediction. Click-Through Rate (CTR) prediction is vital for modern Internet businesses. Pioneering studies leveraged shallow models, like LR [18], FM [17], and FFM [9], to model feature interactions. Wide&Deep [5], DeepFM [8], and DCN [20, 21] successfully combine deep and shallow models. Furthermore, several studies have explored more complex neural networks, such as AutoInt [19] leveraging the multi-head self-attention mechanism. Users' behavioral data have gained much attention recently. YoutubeDNN [6] has employed average pooling across the entire sequence of past behaviors. Furthermore, DIN [24] leveraged a target attention mechanism to adaptively capture user interests from historical behaviors concerning the target item. This target attention approach has been widely used by practitioners [1-3, 23, 24]. Following this line, DIEN [23] and DSIN [7] incorporated temporal information into the target attention. BST [4] and TWIN [2] leveraged the multi-head attention for the target attention.\nLong-Term User Behavior Modeling. Modeling lifelong historical behaviors is crucial in CTR prediction. Early efforts involved memory networks to capture and remember user interests, such as HPMN [16] and MIMN [12]. Several approaches attempt to directly shorten the length of user behaviors, such as UBCS [22] which samples sub-sequences from the entire history and clusters all candidate items to accelerate the sampling process, and DGIN [10] employs item IDs to deduplicate history, thereby compressing redundant items. Given the inconsistency between long-term and short-term interests, recent works commonly consider them separately. SIM [13] and UBR4CTR [14, 15] employ a two-stage approach, first retrieving a subset of behaviors related to the target"}, {"title": "3 METHOD", "content": "This section elaborates on the proposed model, detailing the entire process from the overall workflow to the deployment framework. Notations are summarized in Appendix A.1.\n3.1 Preliminaries\nThe core task of CTR prediction is to forecast the probability of a user clicking on an item. Let xx \u2208 Rdo denote the feature representation of k-th data sample and let yk \u2208 {0, 1} denote the label of the k-th interaction. The process of CTR prediction can be written as:\n$\\hat{y}_k = \\sigma(f(x_k)),$\n(1)\nwhere \u03c3(\u00b7) is the sigmoid function, f is the mapping function implemented as the CTR model f : Rdo \u2192 R, and \u0177k is the predicted probability. The model f is trained by the binary cross entropy loss:\n$\\mathcal{L} = - \\frac{1}{|D|} \\sum_{k=1}^{|D|} y_k \\log(\\hat{y}_k) + (1 - y_k) \\log(1 - \\hat{y}_k),$\n(2)\nwhere D denotes the training dataset.\n3.2 Overall Workflow\nFigure 2 illustrates the overall framework of TWIN-V2, including offline and online parts. This paper focuses on life-cycle behavior modeling. Thus we emphasize this part and omit other parts."}, {"title": "3.3 Life-Cycle User Modeling", "content": "Considering that the user histories over the life cycle have ultralong sequences, we first compress these behaviors in a divide-and-conquer method and then extract users' long-term interests from compressed behaviors.\n3.3.1 Hierarchical Clustering Over Life Cycle. For a user u, we represent all her historical behaviors as a sequence of items S =\n[S1, S2,..., ST], where T is the number of life-cycle behaviors and sj is the j-th interacted item. Users may watch many similar videos in S. For instance, a user fond of NBA games may have hundreds of"}, {"title": "3.3.2 Extracting Cluster Representation", "content": "After obtaining clusters C, we need to extract features from items of each cluster. To minimize computational and storage overhead, we use a virtual item to represent the features of each cluster.\nWe divide item features into two categories (numerical and categorical) and extract them using different methods. Numerical features are typically represented using scalers, for example, video duration and user playing time for a video. Categorical features are commonly represented using one-hot (or multi-hot) vectors, such as video ID and author ID. Formally, given an arbitrary item v, its feature can be written as:\n$x_v = [x_{c,1}^{(v)}; x_{c,2}^{(v)}; ...; x_{c,N_1}^{(v)}; x_{s,1}^{(v)}; x_{s,2}^{(v)}; ...; x_{s,N_2}^{(v)}]^T,$\n(4)\nFor simplicity, we use $x_{c, 1:N_1}^{(v)}$ to denote categorical features and $x_{s, 1:N_2}^{(v)}$ to denote numerical features. For a cluster ci in C, we calculate the average of all numerical features of the items it contains as ci's numerical feature representation:\n$c_{1:N_2}^{(i)} = \\frac{1}{|c_i|} \\sum_{v \\in c_i} x_{1:N_2}^{(v)},$\n(5)\nFor categorical features, since their average is meaningless, we adopt the closest item to the centroid in cluster ci to represent"}, {"title": "3.3.3 Cluster-aware Target Attention", "content": "Following TWIN [2], we employ an identical efficient attention mechanism in both ESU and GSU, which take representations of clusters C as input.\nGiven the clustered behaviors C = [C1, C2, \u2026\u2026\u2026, c\u2191], we obtain a matrix K \u2208 R\u00ce\u00d7d composed of representation vectors through the embedding layer, where ki \u2208 Rd is the vector for i-th cluster's feature ci. Then we use a target attention mechanism to measure the relevance between the target item and historical behaviors. We initially apply the \"Behavior Feature Splits and Linear Projection\" technique from TWIN [2] to enhance the efficiency of target attention, details in Appendix A.2. This method splits item embeddings into inherent and cross parts. q \u2208 RH denotes the vector of the target item's inherent features. Kh\u2208 R\u00ce\u00d7H,Kc\u2208 R\u00ce\u00d7C are inherent and cross embeddings of clustered behaviors respectively, where K = [Kh, Kc]. We can calculate the relevance scores \u03b1 \u2208R\u00ce between the target item and clustered behaviors as follows:\n$\\alpha = \\frac{(K_hW_h)(q^T W_q)^T}{\\sqrt{d_k}} + (K_cW_c)\\beta,$\n(8)\nwhere Wh, WI, We are linear projections, dk is the projected dimension, and \u03b2 is a learnable cross feature weight. The \u03b1 inadequately represents the relationship between clusters and the target item, due to varying item counts in clusters. Assuming two clusters have the same relevance, the cluster with more items should be considered more important, as more items imply a stronger user preference. Thus, we adjust the relevance scores according to the cluster size:\na' = a + lnn,\n(9)\nwhere n \u2208 N denote the size of all clusters in C. Each element ni means the cluster size of ci.\nDuring the GSU stage, we use a' to select the top 100 clusters with the highest relevance scores from the clustered behaviors of length \u2191. Then, these 100 clusters are input into the ESU, where they are aggregated based on relevance scores, resulting in a representation of the user's long-term interests:\nAttention(qWq, KhWh, K&W, KW) = Softmax(a\u2032)\u300cKW,\n(10)\nwhere Wu is a projection matrix and the notations in this equation are slightly abused for simplicity by setting \u2191 = 100 for the ESU"}, {"title": "3.4 Deployment of TWIN-V2", "content": "We divide our hands-on practices for deploying TWIN-V2 into two parts: online and offline, as shown in Figure 3.\nOverview. In Figure 3, we illustrate with a single user example. When the system receives a user's request, it first extracts behavioral features from the offline-processed user life-cycle behaviors. Then, using GSU and ESU, it models these into long-term interest inputs for the CTR model to make predictions. We employ a nearline trainer for real-time model training, which incrementally updates model parameters using real-time user interaction data within an 8-minute window. For life-cycle behaviors, we apply hierarchical clustering and feature extraction to process them. This is conducted through periodic offline processing and updates.\nOffline Processing. Offline processing aims to compress the user's entire life-cycle behaviors. Considering Kuaishou's user count is on a billion scale, we periodically compress their life-cycle behaviors. The hierarchical clustering runner performs a full update every 2 weeks. We set the maximum cluster size y in hierarchical clustering to 20, leading to an average cluster size around 10. Through cluster representation extraction, each cluster is aggregated into a virtual item. In our practices, historical behavior is"}, {"title": "4 EXPERIMENT", "content": "In this section, we verify the effectiveness of TWIN-V2 by conducting extensive offline and online experiments.\n4.1 Experimental Setup\n4.1.1 Dataset. Since TWIN-V2 is designed for user behaviors with extremely long lengths, it's necessary to use datasets with ample user history. To the best of our knowledge, there is no existing public dataset with an average history length exceeding 104. Therefore, we extracted user interaction data from the Kuaishou app over five consecutive days to serve as the training and testing sets. Samples were constructed from clicking logs with click-through as labels. To capture users' life-cycle histories, we further retraced each user's past behavior, covering the maximum history length for each user at 100, 000. Table 2 shows the basic statistics of this industrial dataset. The first 23 hours of each day were used as the training set, while the final hour served as the testing set.\n4.1.2 Baselines. Following common practices [1, 2], given that our focus is on modeling extremely long user histories, we compare TWIN-V2 with the following SOTA baselines:\n(1) Avg-Pooling: A naive method leverages average pooling.\n(2) DIN [24]: It introduces the target attention for recommendation.\n(3) SIM Hard [13]: Hard-search refers to the process where GSU filters long-term history based on its category. (4) SIM Soft [13]: Soft search involves selecting the top-k items by computing the vector dot product between the target item and items in the history. (5) ETA [3]: It employs Locality Sensitive Hashing to facilitate end-to-end training. (6) SDIM [1]: The GSU aggregates long-term history by gathering behaviors that share the same hashing signature with the target item. (7) SIM Cluster: Due to the reliance on"}, {"title": "4.2 Overall Performance", "content": "From the results in Table 3, we have the following observations:\n\u2022TWIN-V2 significantly outperforms other baselines by a large margin. As supported by existing literature [11, 21], an improvement of 0.001 in AUC is considered significant for CTR prediction and is sufficient to yield online benefits. TWIN-V2 achieves"}, {"title": "4.3 Ablation Study", "content": "We conducted an ablation study to investigate the role of core modules in TWIN-V2 and the rationale behind our design.\n4.3.1 Comparison of Different Hierarchical Clustering Methods. Our hierarchical clustering method has two key features: 1. It utilizes a dynamic clustering number 8 to adapt to varying sizes of behaviors; 2. The k-means clustering is non-uniform, resulting in clusters of different sizes in the end. We first verify the effectiveness of adaptive 8, creating a variant that uses a fixed cluster number 8 = 2 for all cases, denoted as 'Binary'. Furthermore, to test the effect of uniform cluster sizes, we create a variant that enforces balanced k-means clustering results when 8 = 2, denoted as 'Balanced&Binary'. In this variant, after each k-means iteration, a portion of items from the larger cluster, which are closer to the centroid of the other cluster, are moved to the other cluster to ensure equal sizes of the final two clusters.\nTable 4 reports the statistical analyses on TWIN-V2 and these two variants. Among the three methods, ours achieves the highest cluster accuracy. This suggests that our method creates clusters where items have more closely matched representation vectors, resulting in higher similarity among items within each cluster. Our method also achieves the shortest running time, confirming the efficiency of adaptive \u03b4. Furthermore, we reported the performance of these methods on the test set, as depicted in the left part of Figure 4. These results validate that the adaptive method can assign more similar items within each cluster and speed up the hierarchical clustering process compared with other methods.\n4.3.2 Effectiveness of Cluster-aware Target Attention. In the proposed cluster-aware target attention, as compared to TWIN, the main difference lies in using clustered behavior as input and reweighting the attention scores based on cluster size. We separately removed the cluster size reweighting part from GSU and ESU to"}, {"title": "4.4 Online Experiments", "content": "We conducted an online A/B test to validate the performance of TWIN-V2 in our industrial system. Table 5 illustrates the relative improvements of TWIN-V2 in terms of watch time and diversity of recommended results in three representative scenarios in Kuaishou (Featured-Video Tab, Discovery Tab, and Slide Tab). Watch Time measures the total amount of time users spend. Diversity refers to the variety in the model's recommended outcomes, such as the richness in types of videos. From the results, it is evident that TWIN-V2 can better model user interests, leading to improved watch time. Additionally, by modeling longer historical behaviors, TWIN-V2 uncovers a more diverse range of user interests, resulting in more varied and diverse recommended results."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose the TWIN-V2, which effectively extends the maximum length of user history to the life-cycle level, accommodating up to 106 behaviors in Kuaishou. The offline hierarchical clustering and feature extraction methods compress ultra-long behaviors into shorter clusters, significantly reducing the storage and computational overhead for life-cycle behaviors by 90%. The cluster-aware target attention for online inferring captures comprehensive and multi-faceted user interests, leading to more accurate and diverse recommendation results. Extensive offline and online experiments have demonstrated the effectiveness of TWIN-V2 over SOTA baselines. TWIN-V2 has been successfully deployed in Kuaishou, serving the main traffic of around 400 million active users daily."}, {"title": "A APPENDIX", "content": "A.1 Summary of Notations\nWe summarize the important notations used in Section 3 in the following table:"}, {"title": "A.2 Behavior Feature Splits and Linear Projection", "content": "Following TWIN [2], we define the feature representations of a length \u2191 clustered behavior sequence [C1, C2, ..., c\u00f4] as matrix K, where each row denotes the features of one behavior. In practice, the linear projection of K in the attention score computation of M\u0397\u03a4\u0391 is the key computational bottleneck that hinders the application of multi-head target attention (MHTA) on ultra-long user behavior sequences. We thus propose the following to reduce its complexity. We first split the behavior features matrix K into two parts,\nK = [Kh, Kc] \u2208 []\u00ae\u00ce\u00d7(H+C),\n(12)\nWe define Kh \u2208 R\u00ce\u00d7H as the inherent features of behavior items (e.g. video id, author, topic, duration) which are independent of the specific user/behavior sequence, and K\u0109 \u2208 R\u00ce\u00d7C as the user-item cross features (e.g. user click timestamp, user play time, clicked page position, user-video interactions). This split allows highly efficient computation of the following linear projection KWh and KcWc.\nFor the inherent features K\u2081, although the dimension H is large (64 for each id feature), the linear projection is not costly. The inherent features of a specific item are shared across users/behavior sequences. With essential caching strategies, KhWh could be efficiently \"calculated\" by a look-up and gathering procedure.\nFor the user-item cross features Kc, caching strategies are not applicable because: 1). Cross features describe the interaction details between a user and a video, thus not shared across users' behavior"}, {"title": "A.3 Analysis of User Activity Levels", "content": "We postulate that enhancing recommendation model performance can be achieved by extending the length of user history input. Given that users with different activity levels exhibit varied lengths of historical behavior, the effect of extending long-term interest modeling to the life-cycle level is likely to differ among them. Consequently, we grouped users by different history lengths and reported the performance improvement across these groups.\nWe categorized users in the dataset into three groups based on the number of their historical behaviors: Low, Medium, and High. Additionally, we calculated the GAUC for TWIN and TWIN-V2 models across these groups. The improvements in GAUC for TWIN-V2 in different groups and their respective proportions of the total user count are shown in Figure 5. It is observable that TWIN-V2 achieves performance improvements across all user groups, validating the effectiveness of our approach. It is also evident that the absolute increase in GAUC is greater in user groups with a higher number of historical behaviors. This occurs as users with a greater number of historical actions possess a broader spectrum of interests within their life-cycle behaviors, thereby presenting more significant opportunities for enhancement. Additionally, TWIN-V2 also achieves performance improvements in groups of users with shorter histories. This is due to the incorporation of clustered behavior features in the cluster-aware target attention mechanism. These features represent the aggregated characteristics of all items within the cluster. Consequently, the data fed into GSU and ESU reflects a more comprehensive scope of behaviors compared to TWIN, leading to improved performance."}]}