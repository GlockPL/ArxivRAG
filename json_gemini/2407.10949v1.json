{"title": "Representing Rule-based Chatbots with Transformers", "authors": ["Dan Friedman", "Abhishek Panigrahi", "Danqi Chen"], "abstract": "Transformer-based chatbots can conduct fluent, natural-sounding conversations, but we have limited understanding of the mechanisms underlying their behavior. Prior work has taken a bottom-up approach to understanding Transformers by constructing Transformers for various synthetic and formal language tasks, such as regular expressions and Dyck languages. However, it is not obvious how to extend this approach to understand more naturalistic conversational agents. In this work, we take a step in this direction by constructing a Transformer that implements the ELIZA program, a classic, rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the conversational setting, including both local pattern matching and long-term dialog state tracking. We build on constructions from prior work-in particular, for simulating finite-state automata-showing how simpler constructions can be composed and extended to give rise to more sophisticated behavior. Next, we train Transformers on a dataset of synthetically generated ELIZA conversations and investigate the mechanisms the models learn. Our analysis illustrates the kinds of mechanisms these models tend to prefer-for example, models favor an induction head mechanism over a more precise, position based copying mechanism; and using intermediate generations to simulate recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results offer a new setting for mechanistic analysis of conversational agents.", "sections": [{"title": "1 Introduction", "content": "State-of-the-art Transformer-based chatbots such as ChatGPT have remarkable capability of conducting fluent, natural-sounding conversations, but we have a limited understanding of the underlying mechanisms. One approach to understanding Transformers is to use constructions: identifying explicit mechanisms that a Transformer could theoretically use to solve a particular task. Prior work has constructed Transformers for a variety of synthetic and formal language tasks, including regular languages [12, 45], Dyck languages [81], and PCFGs [84]. However, this line of work has focused mainly on single-sentence tasks, and how to extend these approaches to more naturalistic conversational settings remains as an open question. In this work, we propose to use rule-based chatbots for formal and mechanistic analysis of neural conversational agents. First, we construct a Transformer that implements a classic rule-based chatbot algorithm, and then we use this construction to inform a series of empirical investigations into how Transformers learn conversational tasks.\nIn particular, we focus on ELIZA [77], one of the first artificial chatbots. The ELIZA algorithm is simple but exhibits a number of sophisticated conversational behaviors (Fig. 1). The majority of ELIZA's behavior is based on local pattern/transformation rules: ELIZA compares the user's input to an inventory of templates, and responds by reassembling the input according to an associated transformation rule. However, ELIZA also employs several mechanisms that make use of the full conversational history, including a mechanism for varying its responses between successive turns, and a \"memory queue\" to refer to turns from the beginning of the conversation. The resulting conversations can be surprisingly naturalistic, with early users ascribing emotion and understanding to the program [78]. ELIZA therefore offers a natural next step from simpler, sentence-level settings, comprising both local pattern matching and long-distance dialog state tracking.\nIn the first part of the paper (Sec. 3), we describe how to implement the ELIZA algorithm with a decoder-only Transformer [69] (Fig. 2). We start by showing how we can use constructions from prior work as modular building blocks in particular, by decomposing the task into a cascade of finite state automata [45, 8], along with a copying mechanism for generating responses. This decomposition attests to the usefulness of algebraic automata as building blocks for characterizing complex behavior in Transformers. On the other hand, we also identify alternative constructions for key subtasks, including a more robust copying mechanism (Sec. 3.2) and memory mechanisms (Sec. 3.3) that make use of intermediate ELIZA outputs\u2014akin to a scratchpad [54] or Chain-of-Thought [75]. These alternative constructions inform our empirical investigations later on. Incidentally, the ELIZA framework happens to be Turing complete [33]; our results therefore lead to a simple, alternative construction for a Transformer that simulates a Turing machine, which we discuss in Appendix B.4.\nIn the second part of the paper, we generate a dataset of ELIZA transcripts and train Transformers to simulate the ELIZA algorithm (Sec. 4.1). First we investigate which aspects of the task are more difficult for the models to learn, finding that models struggle the most with precise copying and with the memory queue mechanism\u2014which requires the composition of several distinct mechanisms (Sec. 4.2). Next, we investigate which of our hypothesized mechanisms better match what the models learn, and how the result varies according to the data distribution (Sec. 4.3). For copying, we find that models have a strong bias for an induction head mechanism [55], leading to worse performance on sequences with a high degree of internal repetition. For the memory components, we find that models make use of intermediate outputs to simulate the relevant data structures, underscoring the importance of considering intermediate computation in understanding Transformers, even without an explicit scratchpad or Chain-of-Thought. Together, our results illustrate that ELIZA offers a rich setting for mechanistic analysis of learning dynamics, allowing us to decompose the task into subtasks, conduct fine-grained behavioral analysis, and connect this analysis to predictions about the model's mechanisms.\nOverall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results offer a new setting for algorithm-level understanding of conversational agents. We conclude by discussing the broader implications of our results for future work on interpretability and the science of large language models."}, {"title": "2 Background: ELIZA", "content": "We start by describing the ELIZA algorithm [77], following the presentation of [38]. The ELIZA algorithm can be decomposed into two types of behavior: local pattern matching and long-term memory, illustrated in Fig. 1. We discuss ELIZA in more detail in Appendix A."}, {"title": "2.1 Local Pattern Matching", "content": "First, ELIZA compares the most recent user input to an inventory of pattern/transformation rules, such as the following:\nO YOU O ME \u2192 What makes you think I 3 you?\nThe left-hand side of the rule is called a decomposition template and corresponds to a simple regular expression, where the 0 symbol is a wildcard that matches 0 or more occurrences of any word. If an input matches a template, it is partitioned into a set of decomposition groups corresponding to the wildcards. For example, the input \u201cIt seems like you hate me\u201d would be decomposed into four groups: (1) It seems like (2) you (3) hate (4) me. The right-hand side of the rule is called a reassembly rule, and a response is generated by replacing any number in the reassembly rule with the content of the corresponding decomposition group. In this case, ELIZA will respond, \u201cWhat makes you think I hate you?\" An ELIZA chatbot is defined by an inventory of these rules, which are organized into a configuration file known as the script. Each decomposition template is assigned a rank and associated with one or more reassembly rules. Given an input, ELIZA finds the highest ranked template that matches the sentence and applies one of the associated reassembly rules. The script also must assign some reassembly rules to a null template, which is used when none of the other templates matches."}, {"title": "2.2 Long-Term Memory", "content": "While most responses consider only the previous utterance, ELIZA also includes two mechanisms for referring to information from earlier in the conversation."}, {"title": "Cycling through reassembly rules", "content": "First, each template in a script can be associated with a list of reassembly rules. If the template is matched multiple times in a conversation, ELIZA will cycle through all of the reassembly rules in the list before returning to the first item. For example, in Weizenbaum's ELIZA script, if the input contains the word \u201csorry,\u201d ELIZA will initially respond with \"Please don't apologize.\" If the user says \u201csorry\u201d a second time, ELIZA will say \u201cApologies aren't necessary.\" If the user contains to say \"sorry\u201d, ELIZA will eventually say \u201cI've told you that apologies are not required,\" and then cycle back to the first rule in the list."}, {"title": "Memory queue", "content": "Second, if an utterance contains a particular keyword (by default, the word \u201cmy\u201d), ELIZA stores it in a queue, referred to as the memory queue. Later in the conversation, if the user's input does not match any of the templates, ELIZA will output the first item in the queue, applying one of a set of memory reassembly rules. For example, at the beginning of the conversation in Fig. 1, the user states \"My boyfriend made me come here.\" Many turns later, the user enters a sentence that does not match any of the patterns, and ELIZA replies, \u201cDoes that have anything to do with the fact that your boyfriend made you come here?\""}, {"title": "3 Constructions", "content": "Now we present our constructions for implementing the ELIZA program with a Transformer decoder. We divide the constructions into four subtasks, illustrated in Fig. 2. We describe the constructions at a high-level in this section and defer the details to Appendix B.\nSetup We consider a decoder-only Transformer with softmax attention. At each turn in the conversation, the input will be the concatenation of the conversation so far, with each user input and each ELIZA response preceded by a special delimiter character, either u: ore:, respectively. The constructions use no positional encodings, as we can use the self-attention mask to infer positional information [32, 39], and to segment the input into turns, in order to restrict attention to a particular utterance. See Appendix B.1 for more details."}, {"title": "3.1 Local Pattern Matching", "content": "We start by considering a single turn in the conversation, which involves first finding a template that matches the input, and then generating a response using the associated transformation rule."}, {"title": "3.2 Cycling through Reassembly Rules", "content": "Now we turn to the first subtask that makes use of information from earlier in the conversation: cycling through reassembly rules. Specifically, we allow each template t to be associated with a sequence of reassembly rules $r_1, ..., r_m$. When template t appears in a conversation for the ith time, the model should respond with rule $r_{i \\% M}$. We consider two mechanisms, illustrated in Fig. 11."}, {"title": "Option 1: Modular prefix sum", "content": "One natural option is to use the modular prefix sum mechanism described by [45]: an attention head counts the number of times t has been matched, and an MLP outputs the result modulo M. We anticipate that such a mechanism might perform worse as the sequence grows longer, as the model must attend over a longer sequence and process a larger count. Additionally, different templates can have a different numbers of reassembly rules, so the model must learn a separate modulus for each template."}, {"title": "Option 2: Intermediate outputs", "content": "The model can avoid modular arithmetic by making use of its earlier outputs. Specifically, the model can reuse the template matching mechanism to identify outputs where it responded to template t with any of $r_1, ..., r_m$. The model can then attend to the most recent of these responses $r_i$, and respond with $r_{(i+1)\\%M}$. This mechanism works regardless of the cycle number. However, it would fail if the same reassembly rule appears more than once in the list, or if the reassembly rules are difficult to identify."}, {"title": "3.3 Memory Queue", "content": "Finally, we incorporate the memory queue component. Recall that ELIZA adds a user input to the memory queue if it contains a special memory keyword (e.g. \"my\") and matches an associated template. ELIZA reads an item from the memory queue if (a) the most recent input does not match any templates and (b) the queue is not empty. Given the output of the template-matching stage, is simple to determine whether an input represents an enqueue event or a no_match event. The main challenge is to determine whether there are any items in the queue, and so whether a given no_match input should trigger a dequeue. Again, we present two mechanisms, illustrated in Fig. 11"}, {"title": "Option 1: Gridworld automaton", "content": "The first approach we consider is to use the construction from [45] for simulating a one-dimensional \"gridworld\" automaton, which has S numbered states and two actions: \"increment the state if possible\u201d and \u201cdecrement the state if possible.\" At each enqueue event, the automaton increments the state if possible, and at each no_match event, the model decrements the state if possible. If the state is decremented, we can conclude that this input should trigger a dequeue. We can then calculate the number of dequeues in the sequence, d, and read the dth memory in the queue. [45] present a gridworld construction with two Transformer layers and 2S attention heads, which would allow us to implement a memory queue with a maximum size of S."}, {"title": "Option 2: Intermediate outputs", "content": "Alternatively, as above, we can instead identify dequeue operations by examining earlier ELIZA outputs. By reusing the template matching mechanism, we can check whether an ELIZA response matches one of reassembly rules associated with the dequeue operation. Then, letting d denote the number of dequeue operations, if d is less than the number of enqueue operations, we read the dth memory from the queue. Compared to the gridworld approach, this construction uses fewer attention heads and does not limit the size of the memory queue, but it does impose a limit on the total number of enqueues (because we need to embed the number of enqueues to attend to the right memory)."}, {"title": "4 Experiments", "content": "Now we investigate how Transformers learn this ELIZA program in practice when we train them on conversation transcripts. First, we study how well the models perform, with the goal of understanding which aspects of the task are more difficult. In the second part of the section, we examine the internal properties of the model to understand how the learned solutions compare to our construction."}, {"title": "4.1 Experiment Setup", "content": "Generating data For these experiments, we generate synthetic ELIZA data. For our main experiments, we first sample a configuration script consisting of 32 templates, each containing 2-4 wildcard symbols, with up to five reassembly rules per template. We ensure that each reassembly rule begins with a unique two-letter prefix; this will provide a proxy for distinguishing rule recognition errors from copying errors. Given a script, we sample multi-turn conversations with up to 512 words. At each turn, we sample a template, and then sample a sentence that matches that template by replacing each wildcard with 0-10 words sampled uniformly from the vocabulary, and then generating a response according to the ELIZA rules. The vocabulary consists of the 26 lowercase letters. Details about data generation are provided in Appendix C.1.\nModel and training We train Transformers with eight layers, twelve attention heads per-layer, and a hidden size of 768. We use the GPT-2 architecture but remove the position embeddings and train all models from scratch. The models are trained to predict the ELIZA responses (and not the user inputs). See Appendix C.2 for more details."}, {"title": "4.2 Which Parts of the ELIZA Program are Harder to Learn?", "content": "We start by training Transformers on ELIZA data and measuring how well they perform on the different subtasks. Here we fix the script parameters to the values described in Appendix C.1. In Figure 3, we plot the accuracy over the course of training and at the final checkpoint. The Full response accuracy is the per-turn exact match accuracy. The Prefix only accuracy is the accuracy on the two-word prefix of the response, which we ensure is unique for each reassembly rule. This metric provides a proxy for distinguishing whether errors are due to either (a) failure to identify the correct rule, or (b) failure to implement the rule correctly. We additionally break down the results by turn type, defined as follows: Single-turn: The first response in the conversation. Multi-turn (no cycling): The response for the first instance of a template in the conversation. Multi-turn (cycling): The response for a template that has already appeared at least once in the conversation. Memory queue: Responses that read from the memory queue. Null template: Responses to inputs that do not match any templates, when the memory queue is empty.\nAccuracy by subtask In Figure 3a, we see that the models quickly learn to identify the correct action (as measured by prefix accuracy), achieving near-perfect accuracy on almost all categories. Interestingly, the exception is the null template, which is used when the input does not match any other pattern and the memory queue is empty. Looking at the final checkpoint (Fig. 3b), we see that accuracy is high, but still imperfect, with slightly worse performance in the multi-turn setting. In the remainder of the section, we examine these errors in more detail to better understand which aspects of the task are more difficult to learn.\nError analysis In Figure 4, we test whether the model's errors are correlated with various properties of the input. We identify two main issues. First, the models seem to struggle with precise copying. In Fig. 4a, we see that accuracy is strongly correlated with the total number of tokens the model has to copy, and only slightly correlated with the complexity of the decomposition rule (defined as the number of distinct copying segments in the transformation). Similarly, Fig. 4b (left) shows that memory queue accuracy decreases with the distance between the current turn and the target memory, perhaps indicating issues with long-distance copying. Second, some errors seem to be related to tracking the state of the memory queue. Fig. 4b (right) shows that accuracy is negatively correlated with the total number of enqueue and dequeue operations in the sequence. Fig. 4c shows that the model performs perfectly on null inputs, provided that there have been no memory turns; accuracy decreases with the number of enqueues, indicating that the models struggle when the queue has been used but is now empty. See additional analysis in Appendix Sec. D.1."}, {"title": "4.3 Which Mechanisms Do Transformers Learn?", "content": "Now we turn to the internal properties of the model to try to understand what mechanisms they learn and how they compare to our construction.\nComparing copying mechanisms In Section 3.1, we identified two possible mechanisms for copying: an induction head, which attends based on the content of the input, and a counting-based mechanisms that attends based on position. We predicted that the induction head will fail when the same n-gram appears more than once in the input, while the counting mechanism will generalize. To explore which mechanism the models seem to learn, we generate (single-turn) datasets that vary in how likely it is for the same n-gram to appear multiple times in a sequence. This property is controlled by a parameter \u03b1, with \u03b1 < 1 corresponding to more repetition of n-grams and \u03b1 > 1 making it more likely that most n-grams are unique. See Fig. 5a for examples."}, {"title": "5 Discussion and Related Work", "content": "Expressivity with formal languages Numerous works have formalized the expressive power of Transformers on formal languages. [58, 61, 13] show that Transformers with hard attention are Turing complete, and [74] study their statistical learnability. [50, 48, 30, 28] further distinguish the expressivity of transformers with different hard attention patterns. Other works have investigated encoding specific algorithms in smaller simulators, e.g. bounded-depth Dyck languages [81], modular prefix sums [9], adders [51], regular languages [12], sparse logical predicates [18], and n-gram language models [67]. [45] propose a unified theory for expressivity of different automata with transformers. We refer the readers to [66] for a more comprehensive survey. Building on these works, numerous recent works have tried to argue the expressivity of transformers with in-context learning. [24, 3, 23, 2, 10, 42, 26, 72, 71, 56, 17] have argued that transformers can simulate specific machine learning algorithms (e.g. linear regression) on in-context examples. However, the relation between the constructions and the performance of Transformers on real world datasets has been largely unclear. Our framework shows that these constructions can be non-trivially extended to show capabilities of language models as general conversational agents. A number of works have demonstrated the theoretical advantage of scratchpads [54] and chain-of-thought [75] for the expressivity of bounded Transformer models [21, 43, 53, 49, 1, 36, 35]. Our experiments illustrate how Transformers trained on ELIZA data make use of their own intermediate outputs to simulate data structures for dialog tracking, highlighting the importance of intermediate outputs even without an explicit scratchpad. We hope that ELIZA inspires future works to connect existing constructions to the emergent abilities Transformers show at scale.\nChallenges for mechanistic interpretability One direction for future work is to consider our ELIZA construction as a test bed for automatic interpretability methods-for example, compiling the construction into Transformer weights using Tracr [44]. Specifically, given a compiled Transformer corresponding to an ELIZA chatbot, to what extent could we recover the program using existing interpretability techniques, such as circuit finding [15, 68] and dictionary learning [16, 27, 46]? Possible difficulties include sharing of attention heads across different ELIZA operations like parsing and copying, and sharing of mechanisms for different ELIZA operations like cycling and memory queues. As such, our framework might encourage more sophisticated interpretable techniques in the future. Similarly, the ELIZA dataset could serve as a test-bed for recent approaches to designing intrinsically interpretable neural architectures for language tasks [e.g. 34, 22].\nMechanistic dependence on data Recent works have tried to understand the behavior of attention models when trained with synthetic datasets. [51] study feature formation in 1-layer transformer models on adders dataset, with [85] studying the dependence on model hyperparameters and initialization. [4, 62] study formation of n-gram induction heads in language models. [5, 84] study the behavior of language models when trained with different context-free grammars. [6, 7] further study knowledge manipulation and storage in language models trained on synthetic datasets. [83] propose LEGO synthetic reasoning dataset to understand generalization of transformers with simple boolean circuits. Finally, [82, 19, 52] give end-to-end convergence analysis of self-attention models when trained under simplistic data assumptions. However, such studies have been generally restricted to settings where the number of possible mechanisms and/or the number of features to learn are restricted. ELIZA provides a general framework that allows diverse mechanisms and features. To successfully implement ELIZA, a model has to perform local pattern matching, cycling through reassembly rules, and memory queues well. And for each feature, there are multiple mechansisms that can emerge, with each mechanism having different generalization abilities. As we show in Section 4.3, different data distribution properties can lead to different mechanisms. With increasing interest in formalizing the mechanistic relationship between data and training behavior [e.g. 14, 29, 64, 80, 37, 41, 60], we believe ELIZA can be a useful test bed for future studies."}, {"title": "6 Conclusion", "content": "In this work, we constructed a Transformer that implements the classic ELIZA chatbot algorithm. We then trained Transformers on ELIZA conversation transcripts and examined which aspects of the task were empirically more difficult to learn, and to what extent to the models matched our construction. Our constructions and dataset raise a number of possibilities for future research, including as a benchmark for automated interpretability methods, and as a setting for mechanistic analysis of learning dynamics."}, {"title": "A ELIZA Algorithm Details", "content": "Here we provide some additional details about the ELIZA algorithm. Our presentation of the ELIZA algorithm in Section 2 omits some details of the original ELIZA algorithm, to improve clarity, so we describe these details here.\nWord-level translation An ELIZA script can include word-level translation rules-for example, I = YOU, YOU = I, and ME = YOU. These translations are applied to all of the words in the input before trying to match the input to a pattern. Therefore, in the original ELIZA script, the patterns are written to match inputs after the word-level translations have been applied. So, for example, the rule\nO ARE I O \u2192 Would you prefer if I weren't 4?\nwould match the input \"Are you laughing at me?\" and transform it to \"Would you prefer if I weren't laughing at you?\" In this paper, we write rules to match the input prior to word-level translations so, for example, we would present the pattern above as O ARE YOU 0. Word-level translation is trivial to incorporate into the Transformer construction, by using the final linear layer to map each word to its translation.\nKeywords Each entry in an ELIZA script consists of a ranked keyword. Each keyword is associated with a list of decomposition templates, and each decomposition template is associated with one or more transformation rules. See Figure 7 for an example. To select a decomposition template, ELIZA finds the highest ranked keyword that appears in the input, and then finds the first decomposition template in the associated list that matches the input. If none of the templates matched, ELIZA checks the next highest-ranked keyword. In this paper, we ignore the role of keywords, and instead define an ELIZA script by a set of ranked decomposition templates and associated transformation rules."}, {"title": "B Construction Details", "content": "In this section, we provide additional details about our ELIZA constructions, including sample implementations in RASP [76]. The input to a RASP program is a sequence of tokens. The program then consists of a series of operations that output new sequences of equal length to tokens, corresponding to intermediate embeddings in the Transformer. The select and aggregate operations correspond to the attention mechanism in the Transformer; these are the only operations that can combine information from different positions in the sequence. All other operations must operate independently at each position, corresponding to feedforward layers. Like Weiss et al. [76], we allow feedforward layers to implement arbitrary element-wise transformations. We do not provide explicit constructions for these element-wise transformations; we leave this for future work. Figure 8 shows the RASP [76] attention primitives we use in our construction, implemented in NumPy [31]."}, {"title": "B.1 Input Segmentation and Position Encoding", "content": "Our first step is to divide the input into segments, corresponding to the turns in the conversation. This is accomplished by using the special delimiter tokens to count the number of utterances seen so far:\nsegment_ids = selector_width(select(tokens, tokens, lambda q, k: k in (\"u:\", \"e:\"), max_width=max_segments)\nWe will use these segment_ids throughout the construction to restrict attention to a particular utterance. The segment_ids are also used to generate local positional encodings:\nsegment_positions = selector_width(select (segment_ids, segment_ids, ==), max_width=max_segment_length)\nThis value encodes the position relative to the start of the current segment.\nRemark on length generalization While not the focus of our investigation here, our approach to segment and position encodings has implications for length generalization, similar to the cases studied by Zhou et al. [86]. In particular, we must specify in advance the maximum number of segments per conversation, as well as the length of each segment. This is because the selector_width operator is implemented using one attention layer followed by one feed-forward layer. At each position i, the attention layer outputs 1/c, where c is the number of key positions attended to from position i. The feed-forward layer then maps each value of 1/c to an orthogonal embedding. In our construction, we implement this second step as a look-up table, meaning that we must decide in advance on the maximum possible value of c. This means that our construction sets a limit on the number of segments per conversation, as well as the length within each segment. If a model learned this mechanism, we would expect it to fail to generalize if the number of segments or the length of a segment increases beyond the training set. (On the other hand, the construction does not place a direct limit on the total conversation length.)"}, {"title": "B.2 Template Matching", "content": "The next step in the construction is to compare the most recent input to the inventory of decomposition templates. Template matching involves two things: finding a template that matches the input, and decomposing the input according to that template's decomposition groups. Our construction makes use of the fact that ELIZA templates are equivalent to star-free regular expressions [47, 59]. As a result, we can recognize these by simulating the corresponding finite-state automaton, building on the constructions of Liu et al. [45] and Angluin et al. [8], adapted to recognize multiple templates in parallel.\nDecomposition templates Given a vocabulary V, a decomposition template is a sequence t = $t_1,..., t_l$, where each $t_i$ is either a word from V; the wildcard character 0, which matches a sequence of zero or more words from V; or a positive integer n, which matches a sequence of exactly n words from V. We assume that the vocabulary contains two special beginning- and end-of-sequence delimiters, ^ and $, respectively, and for every input $w_1, ..., w_N$ and template $t_1, ..., t_l, w_1= t_1= ^$ and $w_n = t_l = \\$. We will use $t_{:i}$ to denote the template prefix $t_1, ..., t_i$. As a working example, consider the vocabulary V = {a, b} and the template $t=^a0bb0$. This template matches the input ^aaabbaa$ and decomposes it into five groups: (1) a (2) aa (3) b (4) b (5) aa. We always take a greedy approach to template matching: for example, using the same template, the input ^aabbbaa$ will be decomposed as (1) a (2) a (3) b (4) b (5) baa rather than (1) a (2) ab (3) b (4) b (5) aa. Note that each decomposition group corresponds to a prefix of the template: word $w_i$ is in group l if $w_{:i}$ matches the template prefix $t_{:l}$."}, {"title": "B.3 Generating a Transformation", "content": "Now we assume that we have identified a matching template and that the embedding for each input token identifies the decomposition group to which that token belongs. The next step is now to apply the chosen reassembly rule to the input to generate a response.\nReassembly rules Given a template $t_1,..., t_l$ and vocabulary V, a reassembly rule is a sequence r = $r_1,...,r_m$, where each $r_i$ is either a word $w \\in V$ or an integer $n \\in [M]$ such that $t_n \\in {0, 1}$. Given an input $w_1, ..., w_n$, let $\u03b4_1, ..., \u03b4_N \\in [L]$ denote the lengths of the longest matching template prefix at each position\u2014that is, $t_{:\u03b4_i}$ is the longest prefix matching $w_{:i}$. We refer to each $\u03b4_i$ as a decomposition group. For each $r_i$, if $r_i \\in V$, the model outputs $r_i$. If $r_i \\in [L]$, the model outputs the subsequence of w such that, for each $w_j, \u03b4_j = r_i$. For example, consider the template t = a0b0 and example input aaabbab, with automaton states 1223455. The reassembly rule r = c2d5 would generate the response caadab. We can divide this process into two stages. First, at each step, we need to determine the reassembly state-that is, which symbol of the reassembly rule are we currently processing. In Fig. 10, we illustrate how we can determine the state as a function of the number"}, {"title": "B.4 Pre-transformation Rules and an ELIZA Transformer Turing Machine", "content": "In this section we discuss how to incorporate the special pre-transformation rule into our construction. This rule is used by Hay and Millican [33] to prove that ELIZA is Turing-complete, which will allow us to immediately derive a Turing machine construction for the ELIZA Transformer."}, {"title": "C Experimental Details", "content": "Here we provide more details about how we generate the data and conduct the experiments. Code and data for reproducing the experiments are available at https://github.com/princeton-nlp/\nELIZA-Transformer."}, {"title": "C.1 Data Generation", "content": "To generate an ELIZA dataset, we first generate a set of decomposition templates and reassembly rules, and then generate conversations by generating sentences that match the different decomposition templates and applying the corresponding rules. For all templates and sentences are drawn from a vocabulary V consisting of the 26 lower-case English letters. Each turn begins with a special delimiter character-U for user inputs and E for ELIZA inputs and ends with a period, and each conversation begins with a special beginning-of-sequence token.\nDecomposition templates Our distribution over decomposition templates is defined by the following parameters: the minimum and maximum number of wildcard symbols per template; and the maximum n-gram length, meaning the maximum number of contiguous non-wildcard symbols. For example, the template Oa0bco has two wildcards and a maximum n-gram length of two (bc). To generate a template, we first pick the number of wildcards by sampling a number l uniformly from between the minimum and maximum, and then form a template by interleaving l wildcard symbols with l + 1 n-grams. Each n-grams is sampled by first sampling a length m uniformly from between 0 and the maximum length (for the first and last n-gram) or between 1 and the maximum length (for any n-gram between two wildcard symbols), and sampling m words uniformly from V. For our first set of experiments (Section 4.2), we sample 31 templates with between two and four wildcards and a maximum n-gram length of three. For our second set of experiments (comparing copying mechanisms in Section 4.3), we sample 15 templates, each with exactly two wildcard characters and a maximum n-gram length of 1. For all experiments, the final template is the null template. The only wildcard symbol we use is 0, corresponding to zero or more words, although ELIZA templates can also include symbols that match exactly n wildcard words.\nReassembly rules Given a decomposition templates, a reassembly rule consists of a sequence of words from V and integers indexing wildcards in the template. We refer to these wildcards as copying segments. Our distribution over reassembly rules is defined by the minimum and maximum number of copying segments and the maximum n-gram length. Given the set of integers corresponding to"}, {"title": "C.2 Models and Training", "content": "For all of our experiments, we train 8-layer decoder-only Transformers with 12 attention heads per layer, a hidden dimension of 768. The models have no position embeddings but are otherwise based on the GPT-2 architecture [63] and are implemented using PyTorch [57] and HuggingFace [79]. We use the Adam optimizer [40] with a learning rate of 1e-4. For multi-turn experiments (Sec. 4.2), we use a batch size of 8 and train for 10 epochs. For single-turn experiments (Sec. 4.3), we use a batch size of 64 and train for 100 epochs. For each setting, we train models with three random seeds; plots are generated with Seaborn [73] and show the 95% confidence intervals."}, {"title": "C.3 Additional Details: Mechanism Analysis", "content": "Cycling through responses Given a template t with reassembly rules $r_1,..., r_m$, we select conver-sations in which t appears n > 1 times. For some i < n, we identify the turn at which t is matched for the ith time in the conversation, and replace the response with $r_j$ for some j \u2260 i. Then we evaluate the model's response at the next occurrence of template t. If the model used the modular sum, we would expect it to give the Same response as before the intervention (responding with $r_{i+1\\%M}$); if it uses the intermediate output, we would expect it to instead reply with $r_{j+1\\%M}$ (Increment). Figure 6a indicates that the model almost always increments its response, indicating that the model relies on previous responses to update the response cycle."}, {"title": "Memory queue", "content": "We conduct a similar experiment to test the memory queue mechanism. We select conversations containing n > 1 two dequeue turns. For some i < n, we identify the ith dequeue turn and replace the response with a constant string, corresponding to a null response, and evaluate the model's response at dequeue i + 1. If the model used the gridworld automaton, we would expect it to give the Same response as before, replying with memory i + 1. If the model relied on intermediate outputs, we would expect it to instead reply with memory i (Decrement). Figure 6b shows that the model almost always decrements the memory counter, indicating that it examines its own earlier responses to identify the state of the memory queue."}, {"title": "D Additional Results", "content": "D.1 Errors on null inputs\nIn Sec. 4, we found that models perform worse on inputs that do not match any of the templates, in situations where the memory queue is empty. We refer to inputs that do not match any templates as null inputs, and say that they match the null template. Note that, like the other templates, the null template is associated with multiple reassembly rules, and the model should cycle through these rules when the null template is matched multiple times. (In our experiment, there are five rules associated with the null template.) We conjecture that the lower performance on null inputs could be related to difficulty tracking the cycle number for null templates.\nIn particular, there is some ambiguity in how to track the cycle number for the null template, because a null input does not always lead to a null response: if the memory queue is non-empty, the model should respond by reading from the memory queue. In our experiments, we increment the cycle number every time the null input is matched, even if the subsequent response is to read from the memory queue. However, we could instead increment the cycle number only when the null input is followed by a null response. For example, consider a case where the null template is associated with three reassembly rules (\u201cNull rule 1\u201d, \u201cNull rule 2", "Null rule 3": ".", "conversation": ""}, {"title": "D.2 Copying mechanisms", "content": "In Fig. 13, we plot the training curves corresponding to the experiments described in \u00a74.3. Models generalize the worst to data with the highest degree of internal repetition (@test = 0.01); this data also takes models longer to learn. This agrees with the findings of Zhou et al. [86] and could suggest that induction-head style mechanisms are easier for Transformers to learn compared to mechanisms that rely on position arithmetic.\nIn Fig. 14, we recreate the results from Fig. 5c, but plotting the results separately for each final-layer attention head. As discussed in \u00a74.3, in this plot, positive values indicate that the attention head has a preference for attending on the basis of position rather than content, and negative values indicate a preference for attending based on content (i.e., to tokens that have the same n-gram prefix as the current token), rather than position. Interestingly, within each model, the majority of attention heads show broadly similar patterns, perhaps indicating that the models encode the same mechanism redundantly across multiple heads. This result echoes the findings of Singh et al. [65], who find that models learn multiple parallel induction heads. Fig. 14 also illustrates that none of the attention cleanly corresponds to one of our hypothesized mechanisms, underscoring the challenges of aligning real-world Transformers with interpretable symbolic mechanisms."}]}