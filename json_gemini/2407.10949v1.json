{"title": "Representing Rule-based Chatbots with Transformers", "authors": ["Dan Friedman", "Abhishek Panigrahi", "Danqi Chen"], "abstract": "Transformer-based chatbots can conduct fluent, natural-sounding conversations, but we have limited understanding of the mechanisms underlying their behavior. Prior work has taken a bottom-up approach to understanding Transformers by constructing Transformers for various synthetic and formal language tasks, such as regular expressions and Dyck languages. However, it is not obvious how to extend this approach to understand more naturalistic conversational agents. In this work, we take a step in this direction by constructing a Transformer that implements the ELIZA program, a classic, rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the conversational setting, including both local pattern matching and long-term dialog state tracking. We build on constructions from prior work-in particular, for simulating finite-state automata-showing how simpler constructions can be composed and extended to give rise to more sophisticated behavior. Next, we train Transformers on a dataset of synthetically generated ELIZA conversations and investigate the mechanisms the models learn. Our analysis illustrates the kinds of mechanisms these models tend to prefer-for example, models favor an induction head mechanism over a more precise, position based copying mechanism; and using intermediate generations to simulate recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results offer a new setting for mechanistic analysis of conversational agents.", "sections": [{"title": "1 Introduction", "content": "State-of-the-art Transformer-based chatbots such as ChatGPT have remarkable capability of conducting fluent, natural-sounding conversations, but we have a limited understanding of the underlying mechanisms. One approach to understanding Transformers is to use constructions: identifying explicit mechanisms that a Transformer could theoretically use to solve a particular task. Prior work has constructed Transformers for a variety of synthetic and formal language tasks, including regular languages [12, 45], Dyck languages [81], and PCFGs [84]. However, this line of work has focused mainly on single-sentence tasks, and how to extend these approaches to more naturalistic conversational settings remains as an open question. In this work, we propose to use rule-based chatbots for formal and mechanistic analysis of neural conversational agents. First, we construct a Transformer that implements a classic rule-based chatbot algorithm, and then we use this construction to inform a series of empirical investigations into how Transformers learn conversational tasks.\nIn particular, we focus on ELIZA [77], one of the first artificial chatbots. The ELIZA algorithm is simple but exhibits a number of sophisticated conversational behaviors (Fig. 1). The majority of ELIZA's behavior is based on local pattern/transformation rules: ELIZA compares the user's input to an inventory of templates, and responds by reassembling the input according to an associated transformation rule. However, ELIZA also employs several mechanisms that make use of the full conversational history, including a mechanism for varying its responses between successive turns, and a \"memory queue\" to refer to turns from the beginning of the conversation. The resulting conversations can be surprisingly naturalistic, with early users ascribing emotion and understanding to the program [78]. ELIZA therefore offers a natural next step from simpler, sentence-level settings, comprising both local pattern matching and long-distance dialog state tracking.\nIn the first part of the paper (Sec. 3), we describe how to implement the ELIZA algorithm with a decoder-only Transformer [69] (Fig. 2). We start by showing how we can use constructions from prior work as modular building blocks in particular, by decomposing the task into a cascade of finite state automata [45, 8], along with a copying mechanism for generating responses. This decomposition attests to the usefulness of algebraic automata as building blocks for characterizing complex behavior in Transformers. On the other hand, we also identify alternative constructions for key subtasks, including a more robust copying mechanism (Sec. 3.2) and memory mechanisms (Sec. 3.3) that make use of intermediate ELIZA outputs\u2014akin to a scratchpad [54] or Chain-of-Thought [75]. These alternative constructions inform our empirical investigations later on. Incidentally, the ELIZA framework happens to be Turing complete [33]; our results therefore lead to a simple, alternative construction for a Transformer that simulates a Turing machine, which we discuss in Appendix B.4.\nIn the second part of the paper, we generate a dataset of ELIZA transcripts and train Transformers to simulate the ELIZA algorithm (Sec. 4.1). First we investigate which aspects of the task are more difficult for the models to learn, finding that models struggle the most with precise copying and with the memory queue mechanism\u2014which requires the composition of several distinct mechanisms (Sec. 4.2). Next, we investigate which of our hypothesized mechanisms better match what the models learn, and how the result varies according to the data distribution (Sec. 4.3). For copying, we find that models have a strong bias for an induction head mechanism [55], leading to worse performance on sequences with a high degree of internal repetition. For the memory components, we find that models make use of intermediate outputs to simulate the relevant data structures, underscoring the importance of considering intermediate computation in understanding Transformers, even without an explicit scratchpad or Chain-of-Thought. Together, our results illustrate that ELIZA offers a rich setting for mechanistic analysis of learning dynamics, allowing us to decompose the task into subtasks, conduct fine-grained behavioral analysis, and connect this analysis to predictions about the model's mechanisms.\nOverall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results offer a new setting for algorithm-level understanding of conversational agents. We conclude by discussing the broader implications of our results for future work on interpretability and the science of large language models."}, {"title": "2 Background: ELIZA", "content": "We start by describing the ELIZA algorithm [77], following the presentation of [38]. The ELIZA algorithm can be decomposed into two types of behavior: local pattern matching and long-term memory, illustrated in Fig. 1. We discuss ELIZA in more detail in Appendix A."}, {"title": "2.1 Local Pattern Matching", "content": "First, ELIZA compares the most recent user input to an inventory of pattern/transformation rules, such as the following:\n\\(O YOU O ME \\rightarrow What makes you think I 3 you?\\)\nThe left-hand side of the rule is called a decomposition template and corresponds to a simple regular expression, where the 0 symbol is a wildcard that matches 0 or more occurrences of any word. If an input matches a template, it is partitioned into a set of decomposition groups corresponding to the wildcards. For example, the input \u201cIt seems like you hate me\u201d would be decomposed into four groups: (1) It seems like (2) you (3) hate (4) me. The right-hand side of the rule is called a reassembly rule, and a response is generated by replacing any number in the reassembly rule with the content of the corresponding decomposition group. In this case, ELIZA will respond, \u201cWhat makes you think I hate you?\" An ELIZA chatbot is defined by an inventory of these rules, which are organized into a configuration file known as the script. Each decomposition template is assigned a rank and associated with one or more reassembly rules. Given an input, ELIZA finds the highest ranked template that matches the sentence and applies one of the associated reassembly rules. The script also must assign some reassembly rules to a null template, which is used when none of the other templates matches."}, {"title": "2.2 Long-Term Memory", "content": "While most responses consider only the previous utterance, ELIZA also includes two mechanisms for referring to information from earlier in the conversation."}, {"title": "Cycling through reassembly rules", "content": "First, each template in a script can be associated with a list of reassembly rules. If the template is matched multiple times in a conversation, ELIZA will cycle through all of the reassembly rules in the list before returning to the first item. For example, in Weizenbaum's ELIZA script, if the input contains the word \u201csorry,\u201d ELIZA will initially respond with \"Please don't apologize.\" If the user says \u201csorry\u201d a second time, ELIZA will say \u201cApologies aren't necessary.\" If the user contains to say \"sorry\u201d, ELIZA will eventually say \u201cI've told you that apologies are not required,\" and then cycle back to the first rule in the list."}, {"title": "Memory queue", "content": "Second, if an utterance contains a particular keyword (by default, the word \u201cmy\u201d), ELIZA stores it in a queue, referred to as the memory queue. Later in the conversation, if the user's input does not match any of the templates, ELIZA will output the first item in the queue, applying one of a set of memory reassembly rules. For example, at the beginning of the conversation in Fig. 1, the user states \"My boyfriend made me come here.\" Many turns later, the user enters a sentence that does not match any of the patterns, and ELIZA replies, \u201cDoes that have anything to do with the fact that your boyfriend made you come here?\""}, {"title": "3 Constructions", "content": "Now we present our constructions for implementing the ELIZA program with a Transformer decoder. We divide the constructions into four subtasks, illustrated in Fig. 2. We describe the constructions at a high-level in this section and defer the details to Appendix B.\nSetup We consider a decoder-only Transformer with softmax attention. At each turn in the conversation, the input will be the concatenation of the conversation so far, with each user input and each ELIZA response preceded by a special delimiter character, either u: ore:, respectively. The constructions use no positional encodings, as we can use the self-attention mask to infer positional information [32, 39], and to segment the input into turns, in order to restrict attention to a particular utterance. See Appendix B.1 for more details."}, {"title": "3.1 Local Pattern Matching", "content": "We start by considering a single turn in the conversation, which involves first finding a template that matches the input, and then generating a response using the associated transformation rule."}, {"title": "3.2 Cycling through Reassembly Rules", "content": "Now we turn to the first subtask that makes use of information from earlier in the conversation: cycling through reassembly rules. Specifically, we allow each template t to be associated with a sequence of reassembly rules \\(r_1, ...,r_m\\). When template t appears in a conversation for the ith time, the model should respond with rule \\(r_{i \\% M}\\). We consider two mechanisms, illustrated in Fig. 11."}, {"title": "Option 1: Modular prefix sum", "content": "One natural option is to use the modular prefix sum mechanism described by [45]: an attention head counts the number of times t has been matched, and an MLP outputs the result modulo M. We anticipate that such a mechanism might perform worse as the sequence grows longer, as the model must attend over a longer sequence and process a larger count. Additionally, different templates can have a different numbers of reassembly rules, so the model must learn a separate modulus for each template."}, {"title": "Option 2: Intermediate outputs", "content": "The model can avoid modular arithmetic by making use of its earlier outputs. Specifically, the model can reuse the template matching mechanism to identify outputs where it responded to template t with any of \\(r_1, ...,r_m\\). The model can then attend to the most recent of these responses \\(r_i\\), and respond with \\(r_{(i+1) \\% M}\\). This mechanism works regardless of the cycle number. However, it would fail if the same reassembly rule appears more than once in the list, or if the reassembly rules are difficult to identify."}, {"title": "3.3 Memory Queue", "content": "Finally, we incorporate the memory queue component. Recall that ELIZA adds a user input to the memory queue if it contains a special memory keyword (e.g. \"my\") and matches an associated template. ELIZA reads an item from the memory queue if (a) the most recent input does not match any templates and (b) the queue is not empty. Given the output of the template-matching stage, is simple to determine whether an input represents an enqueue event or a no_match event. The main challenge is to determine whether there are any items in the queue, and so whether a given no_match input should trigger a dequeue. Again, we present two mechanisms, illustrated in Fig. 11"}, {"title": "Option 1: Gridworld automaton", "content": "The first approach we consider is to use the construction from [45] for simulating a one-dimensional \"gridworld\" automaton, which has S numbered states and two actions: \"increment the state if possible\u201d and \u201cdecrement the state if possible.\" At each enqueue event, the automaton increments the state if possible, and at each no_match event, the model decrements the state if possible. If the state is decremented, we can conclude that this input should trigger a dequeue. We can then calculate the number of dequeues in the sequence, d, and read the dth memory in the queue. [45] present a gridworld construction with two Transformer layers and 2S attention heads, which would allow us to implement a memory queue with a maximum size of S."}, {"title": "Option 2: Intermediate outputs", "content": "Alternatively, as above, we can instead identify dequeue operations by examining earlier ELIZA outputs. By reusing the template matching mechanism, we can check whether an ELIZA response matches one of reassembly rules associated with the dequeue operation. Then, letting d denote the number of dequeue operations, if d is less than the number of enqueue operations, we read the dth memory from the queue. Compared to the gridworld approach, this construction uses fewer attention heads and does not limit the size of the memory queue, but it does impose a limit on the total number of enqueues (because we need to embed the number of enqueues to attend to the right memory)."}, {"title": "4 Experiments", "content": "Now we investigate how Transformers learn this ELIZA program in practice when we train them on conversation transcripts. First, we study how well the models perform, with the goal of understanding which aspects of the task are more difficult. In the second part of the section, we examine the internal properties of the model to understand how the learned solutions compare to our construction."}, {"title": "4.1 Experiment Setup", "content": "Generating data For these experiments, we generate synthetic ELIZA data. For our main experi- ments, we first sample a configuration script consisting of 32 templates, each containing 2-4 wildcard symbols, with up to five reassembly rules per template. We ensure that each reassembly rule begins with a unique two-letter prefix; this will provide a proxy for distinguishing rule recognition errors from copying errors. Given a script, we sample multi-turn conversations with up to 512 words. At each turn, we sample a template, and then sample a sentence that matches that template by replacing each wildcard with 0-10 words sampled uniformly from the vocabulary, and then generating a response according to the ELIZA rules. The vocabulary consists of the 26 lowercase letters. Details about data generation are provided in Appendix C.1.\nModel and training We train Transformers with eight layers, twelve attention heads per-layer, and a hidden size of 768. We use the GPT-2 architecture but remove the position embeddings and train all models from scratch. The models are trained to predict the ELIZA responses (and not the user inputs). See Appendix C.2 for more details."}, {"title": "4.2 Which Parts of the ELIZA Program are Harder to Learn?", "content": "We start by training Transformers on ELIZA data and measuring how well they perform on the different subtasks. Here we fix the script parameters to the values described in Appendix C.1. In Figure 3, we plot the accuracy over the course of training and at the final checkpoint. The Full response accuracy is the per-turn exact match accuracy. The Prefix only accuracy is the accuracy on the two-word prefix of the response, which we ensure is unique for each reassembly rule. This metric provides a proxy for distinguishing whether errors are due to either (a) failure to identify the correct rule, or (b) failure to implement the rule correctly. We additionally break down the results by turn type, defined as follows: Single-turn: The first response in the conversation. Multi-turn (no cycling): The response for the first instance of a template in the conversation. Multi-turn (cycling): The response for a template that has already appeared at least once in the conversation. Memory queue: Responses that read from the memory queue. Null template: Responses to inputs that do not match any templates, when the memory queue is empty.\nAccuracy by subtask In Figure 3a, we see that the models quickly learn to identify the correct action (as measured by prefix accuracy), achieving near-perfect accuracy on almost all categories. Interestingly, the exception is the null template, which is used when the input does not match any other pattern and the memory queue is empty. Looking at the final checkpoint (Fig. 3b), we see that accuracy is high, but still imperfect, with slightly worse performance in the multi-turn setting. In the remainder of the section, we examine these errors in more detail to better understand which aspects of the task are more difficult to learn.\nError analysis In Figure 4, we test whether the model's errors are correlated with various properties of the input. We identify two main issues. First, the models seem to struggle with precise copying. In Fig. 4a, we see that accuracy is strongly correlated with the total number of tokens the model has to copy, and only slightly correlated with the complexity of the decomposition rule (defined as the number of distinct copying segments in the transformation). Similarly, Fig. 4b (left) shows that memory queue accuracy decreases with the distance between the current turn and the target memory, perhaps indicating issues with long-distance copying. Second, some errors seem to be related to tracking the state of the memory queue. Fig. 4b (right) shows that accuracy is negatively correlated with the total number of enqueue and dequeue operations in the sequence. Fig. 4c shows that the model performs perfectly on null inputs, provided that there have been no memory turns; accuracy decreases with the number of enqueues, indicating that the models struggle when the queue has been used but is now empty. See additional analysis in Appendix Sec. D.1."}, {"title": "4.3 Which Mechanisms Do Transformers Learn?", "content": "Now we turn to the internal properties of the model to try to understand what mechanisms they learn and how they compare to our construction.\nComparing copying mechanisms In Section 3.1, we identified two possible mechanisms for copying: an induction head, which attends based on the content of the input, and a counting-based mechanisms that attends based on position. We predicted that the induction head will fail when the same n-gram appears more than once in the input, while the counting mechanism will generalize. To explore which mechanism the models seem to learn, we generate (single-turn) datasets that vary in how likely it is for the same n-gram to appear multiple times in a sequence. This property is controlled by a parameter a, with a < 1 corresponding to more repetition of n-grams and a > 1 making it more likely that most n-grams are unique. See Fig. 5a for examples."}, {"title": "5 Discussion and Related Work", "content": "Expressivity with formal languages Numerous works have formalized the expressive power of Transformers on formal languages. [58, 61, 13] show that Transformers with hard attention are Turing complete, and [74] study their statistical learnability. [50, 48, 30, 28] further distinguish the expressivity of transformers with different hard attention patterns. Other works have investigated encoding specific algorithms in smaller simulators, e.g. bounded-depth Dyck languages [81], modular prefix sums [9], adders [51], regular languages [12], sparse logical predicates [18], and n-gram language models [67]. [45] propose a unified theory for expressivity of different automata with transformers. We refer the readers to [66] for a more comprehensive survey. Building on these works, numerous recent works have tried to argue the expressivity of transformers with in-context learning. [24, 3, 23, 2, 10, 42, 26, 72, 71, 56, 17] have argued that transformers can simulate specific machine learning algorithms (e.g. linear regression) on in-context examples. However, the relation between the constructions and the performance of Transformers on real world datasets has been largely unclear. Our framework shows that these constructions can be non-trivially extended to show capabilities of language models as general conversational agents. A number of works have demonstrated the theoretical advantage of scratchpads [54] and chain-of-thought [75] for the expressivity of bounded Transformer models [21, 43, 53, 49, 1, 36, 35]. Our experiments illustrate how Transformers trained on ELIZA data make use of their own intermediate outputs to simulate data structures for dialog tracking, highlighting the importance of intermediate outputs even without an explicit scratchpad. We hope that ELIZA inspires future works to connect existing constructions to the emergent abilities Transformers show at scale.\nChallenges for mechanistic interpretability One direction for future work is to consider our ELIZA construction as a test bed for automatic interpretability methods-for example, compiling the construction into Transformer weights using Tracr [44]. Specifically, given a compiled Transformer corresponding to an ELIZA chatbot, to what extent could we recover the program using existing interpretability techniques, such as circuit finding [15, 68] and dictionary learning [16, 27, 46]? Possible difficulties include sharing of attention heads across different ELIZA operations like parsing and copying, and sharing of mechanisms for different ELIZA operations like cycling and memory queues. As such, our framework might encourage more sophisticated interpretable techniques in the future. Similarly, the ELIZA dataset could serve as a test-bed for recent approaches to designing intrinsically interpretable neural architectures for language tasks [e.g. 34, 22].\nMechanistic dependence on data Recent works have tried to understand the behavior of attention models when trained with synthetic datasets. [51] study feature formation in 1-layer transformer mod- els on adders dataset, with [85] studying the dependence on model hyperparameters and initialization. [4, 62] study formation of n-gram induction heads in language models. [5, 84] study the behavior of language models when trained with different context-free grammars. [6, 7] further study knowledge manipulation and storage in language models trained on synthetic datasets. [83] propose LEGO synthetic reasoning dataset to understand generalization of transformers with simple boolean circuits. Finally, [82, 19, 52] give end-to-end convergence analysis of self-attention models when trained under simplistic data assumptions. However, such studies have been generally restricted to settings where the number of possible mechanisms and/or the number of features to learn are restricted. ELIZA provides a general framework that allows diverse mechanisms and features. To successfully implement ELIZA, a model has to perform local pattern matching, cycling through reassembly rules, and memory queues well. And for each feature, there are multiple mechansisms that can emerge, with each mechanism having different generalization abilities. As we show in Section 4.3, different data distribution properties can lead to different mechanisms. With increasing interest in formalizing the mechanistic relationship between data and training behavior [e.g. 14, 29, 64, 80, 37, 41, 60], we believe ELIZA can be a useful test bed for future studies."}, {"title": "6 Conclusion", "content": "In this work, we constructed a Transformer that implements the classic ELIZA chatbot algorithm. We then trained Transformers on ELIZA conversation transcripts and examined which aspects of the task were empirically more difficult to learn, and to what extent to the models matched our construction. Our constructions and dataset raise a number of possibilities for future research, including as a benchmark for automated interpretability methods, and as a setting for mechanistic analysis of learning dynamics."}]}