{"title": "DAGE: DAG Query Answering via Relational Combinator with Logical Constraints", "authors": ["Yunjie He", "Bo Xiong", "Daniel Hernandez", "Yuqicheng Zhu", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Predicting answers to queries over knowledge graphs is called a complex reasoning task because answering a query requires subdividing it into subqueries. Existing query embedding methods use this decomposition to compute the embedding of a query as the combination of the embedding of the subqueries. This requirement limits the answerable queries to queries having a single free variable and being decomposable, which are called tree-form queries and correspond to the SROI description logic. In this paper, we define a more general set of queries, called DAG queries and formulated in the ALCOIR description logic, propose a query embedding method for them, called DAGE, and a new benchmark to evaluate query embeddings on them. Given the computational graph of a DAG query, DAGE combines the possibly multiple paths between two nodes into a single path with a trainable operator that represents the intersection of relations and learns DAG-DL from tautologies. We show that it is possible to implement DAGE on top of existing query embedding methods, and we empirically measure the improvement of our method over the results of vanilla methods evaluated in tree-form queries that approximate the DAG queries of our proposed benchmark.", "sections": [{"title": "Introduction", "content": "A challenging aspect of machine learning, called complex reasoning, is to solve tasks that can be subdivided into subtasks. A prominent complex reasoning problem is predicting answers to queries in knowledge graphs. This problem, called complex query answering, involves solving queries by decomposing them into subqueries. To address this problem, several query embedding (QE) methods [1-4] encode queries with low-dimensional vectors, and utilize neural logical operators to define the embedding of a query as the combination of the embedding of its subqueries. However, these methods are only capable of processing a restricted set of first-order logic queries that have a single unquantified variable (called target), correspond to SROI description logic queries [5] and are called tree-form queries because, considering only the nodes representing variables, their computation graphs are trees [6]. In this work, we consider a more expressive set of queries, DAG queries, which extends tree-form queries by allowing for quantified variables to appear multiple times in the first component of atoms. In doing so, DAG queries can include multiple paths from a quantified variable x to a target variable y, whereas in tree-form queries it is at most one path from x to y.\nConsider the following first-order query $\\phi(y)$, asking for works edited by an Oscar winner and produced by an Oscar winner.\n$\\phi(y) ::= \\exists x_1\\exists x_2: wonBy(Oscar, x_1) \\land \\newline edited(x_1, y) \\land \\newline wonBy (Oscar, x_2) \\land \\newline produced(x_2, y).$ (1)\nThe computation graph of query $\\phi(y)$ is the following:\nQuery $\\phi(y)$ is tree-form because there exists at most one path from x1 to y and at most one path from x2 to y. Since it is tree-form, it can"}, {"title": "Preliminaries", "content": "This section presents queries as ALCOIR concepts. We follow the notations and semantics described in [7]. For the following definitions, we assume three pairwise disjoint sets C, R, and E, whose elements are respectively called concept names, role names and individual names.\nDefinition 2.1 (Syntax of ALCOIR Concept and Role Descrip- tions). ALCOIR concept descriptions C and role descriptions R are defined by the following grammar\nC ::= \\top | A | \\{a\\} | \\neg C | C \\sqcap C | \\exists R.C\nR ::= r | R^- | R \\circ R | R \\sqcup R | R^+$"}, {"title": "Tree-Form and the DAG Queries", "content": "As was proposed by He et al. [5], tree-form queries can be expressed as SROI\u2212 concepts descriptions. The computation graphs of the first-order formulas corresponding to these concepts descriptions have at most one path for every quantified variable to the target variable. As we already show, this is not hold if the relational inter section $\\sqcap$ is added. In this section, we define tree-form and DAG queries as subsets of the ALCOIR description logic, we describe their computation graph, and the relaxation of non-tree form DAG queries as tree-form queries."}, {"title": "Syntax of Queries", "content": "Definition 3.1 (Tree-Form and DAG queries). DAG queries are the subset of ALCOIR concept descriptions C defined by the following grammar\nC ::= \\{ a \\} | \\neg C | C \\sqcap C | \\exists R.C\nR ::= r | R^- | R \\circ R | R \\sqcap R\nA DAG query is said tree-form if it does not include the operator $\\sqcap$ in role descriptions.\nUnlike ALCOIR concept descriptions, DAG queries do not include concept names, the $\\top$ concept, nor the transitive closure of relations. We exclude these constructors because they are not present in queries supported by existing query embeddings.\nProposition 3.2. Given two role descriptions R and S, and an individual name a, the following equivalences hold:\ncommutativity:\nR \\sqcap S \\equiv S \\sqcap R,\nmonotonicity:\nR \\sqcap S \\sqsubseteq R,\nrestricted conjunction preserving:\n$\\exists (R \\sqcap S).\\{ a \\} \\equiv \\exists R.\\{ a \\} \\sqcap \\exists S.\\{ a \\}.$ (2)\nProof. The tautologies follow directly from the semantics of ALCOIR concept and role descriptions. $\\Box$"}, {"title": "Computation Graphs", "content": "He et al. [5] illustrated the computation graphs for tree-form queries encoded as SROI\u2212 concepts, but did not formalize them. We next provide such a formalization for a graph representation of DAG queries (and thus for tree-form queries).\nDefinition 3.3 (Computation Graph). A computation graph is a labelled directed graph $\\Gamma = (N, E, \\lambda, \\tau)$ such that N is a set whose elements are called nodes, E \\subseteq N \\times N is a set whose elements are called edges, $\\lambda$ is a function that maps each node in N to a label, and $\\tau$ is a distinguished node in N, called target.\nExample 3.4. The computation graph $\\Gamma = (N, E, \\lambda, \\tau)$ with N = \\{u_1, u_2\\}, E = \\{(u_1, u_2)\\}, $\\lambda$ = \\{u_1 \\mapsto \\{Oscar\\}, u_2 \\mapsto \\exists wonBy^-\\}, and $\\tau$ = u_2 is depicted in (11).\nu_2 : \\exists wonBy^-\\newlineu_1 : \\{Oscar\\} (11)\nIntuitively, the node u1 computes the concept \\{Oscar\\} and the node u2 computes the concept $\\exists wonBy^-.\\{Oscar\\}$, which corresponds to answers to the query asking who is an Oscar\u2019s winner.\nTo define the computation graphs of DAG queries, we need to introduce the composition of a computation graph $\\Gamma$ with a role description R, denoted $\\Gamma[R]$. Intuitively, the composition is the concatenation of $\\Gamma$ with the graph representing the role description, as Example 3.5 illustrates. A definition for this operation and the computation graph of DAG queries is suplemented in Appendix C.\nExample 3.5. Consider the computation graph $\\Gamma$ depicted in (11). The computation graph $\\Gamma[edited^-]$ and $\\Gamma[(edited \\sqcap produced)^-]$ are depicted in (12) and (13), respectively.\nu_3 : \\exists edited^-\\newlineu_2 : \\exists wonBy^-\\newlineu_1 : \\{Oscar\\} (12)"}, {"title": "Relaxing Non Tree-Form DAG queries", "content": "The restricted conjunction preserving (see Proposition 3.2), does no longer follow if we replace the nominal \\{a\\} with a general concept description C. Indeed, the example discussed in the introduction is a counterexample for the generalized version of the conjunction preserving. The fact that conjunction is not preserved in general is the cause of the need of new neural operator for the role con-junction, different from the one used for the concept conjunction. Alternatively, if the neural operator is not used, we can relax con-cept description with a relaxed version of this tautology.\nPROPOSITION 3.7. Given two role descriptions R and S, and a concept description C, the following concept inclusion hols:\n$\\exists (R \\sqcap S).C \\sqsubseteq \\exists R.C \\sqcap \\exists S.C$ (15)\nPROOF. By monotonicity, concept $\\exists (R \\sqcap S).C$ is included in the concepts $\\exists R.C$ and $\\exists S.C$. Then, concept $\\exists (R \\sqcap S).C$ is included in the concept $\\exists R.C \\sqcap \\exists S.C$. $\\Box$\nIntuitively, the role conjunction relaxation consists of not as-suming that the instances of the concept C must be equal on the concept defined on the right side. An example of this was discussed in the introduction, when the editor and producer of a work are not required to be the same Oscar's winner. Thus, the three-form query with the computation graph in (14) relaxes the non tree-form with the computation graph in (13).\nDefinition 3.8 (Tree-form approximation). The approximated tree- form query of a DAG query Q, denoted tree(Q), is the tree-form query resulting from removing every conjunction of role descrip- tions using the inclusion in (15).\nIt is not difficult to see that for every DAG query Q with no complement constructor (i.e., without $\\neg$), it holds that Q \\sqsubseteq tree(Q), that is, query tree(Q) relaxes query Q. This is not necessary for queries including complement because they are not necessarily monotonic. Hence, query embeddings that use tree-form queries"}, {"title": "DAG Query Answering with Relational Combinator", "content": "In this section, we first introduce a generalized query embedding model subsuming various previous query embedding approaches [1-3]. Then, we introduce a relational combinator that extends existing query embeddings to support DAG query type. Finally, we discuss how to introduce additional logical constraints to further improve the results."}, {"title": "Base Query Embedding Methods Interface", "content": "Many query embedding methods [1-3] predict query answers by comparing the embedding of individuals with the embedding of the query, so the individuals that are closer to the query in the embedding space are more likely to be answers. These query embeddings are learnable parameterized objects and are computed via neural operations that correspond to the logic connectives in the queries. In this subsection we present the interface required for the embedding methods to be used as a base for our proposed query embedding method, DAG-E. Query embedding methods such as Query2Box, BetaE, and ConE satisfy this interface.\nWe assume three vector spaces $E^d$, $R^d$, and $Q^d$, where E, R, and Q are fields (which depend on the query embedding method) and d is the dimension of the vectors. We assume that every individual a is embedded in a vector $Emb_a \\in E^d$, every role name r and its inverse r- are embedded in vector $Emb_r, Emb_{r^-} \\in R^d$, and every tree-form query Q is embedded in a vector $Emb_Q \\in Q^d$. Whereas the embedding function Emb. is defined for individuals and role names and the inverse of role names, because they are directly defined by the parameters to be learn, function Emb. is not directly defined for compound role and concept descriptions."}, {"title": "Role Embeddings", "content": "The embedding of a role description R is recursively computed from the embedding of role names and its inverses as with a neural operators with signature RComposition : $R^d \\times R^d \\rightarrow R^d$ as follows:\n$Emb_{R \\circ S} ::= RComposition(Emb_r, Emb_s),$ (16)\n$Emb_{r^-} ::= Emb_r,$ (17)\n$Emb_{(R \\circ S)^-} ::= Emb_{S^- \\circ R^-}.$ (18)"}, {"title": "Concept Embeddings", "content": "The embedding of a query is computed from the embedding of individual names and role names using neu-ral operators that represent the logical connectives in queries. The signatures of these neural operators are the following: Nominal : $E^d \\rightarrow Q^d$, RelT : $Q^D \\times R^E \\rightarrow Q^D$, Intersect : $Q^d \\times ... Q^d \\rightarrow Q^d$, and Complement : $Q^d \\rightarrow Q^d$. These neural operators define query embedding of tree-form queries as follows:\n$Emb_{\\{a\\}} ::= Nominal(Emb_a),$ (19)\n$Emb_{\\exists R.C} ::= RelT_r(Emb_c),$ (20)\n$Emb_{C_1 \\sqcap C_2 ... \\sqcap C_k} ::= Intersect(Emb_{c_1}, Emb_{c_2},\u00b7\u00b7\u00b7, Emb_{c_k}),$ (21)\n$Emb_{\\neg c} ::= Complement(Emb_c).$ (22)"}, {"title": "Insideness", "content": "Given a query Q and a knowledge graph G, the goal of query embedding approaches is to maximize the predictions of positive answers to query Q (i.e., individuals a such that G |= Q(a)) and minimize the prediction of negative answers to query Q (i.e., individuals b such that G |= Q(b)). Because of the open-world semantics of G (see Definition 2.6) we cannot know which answers are negative. However, the learning of query embedding needs negative answers. Therefore, for each positive answer a, query embedding methods assume a random individual b, different from a, to be a negative answer.\nIn the representation space, the evaluation of how likely an individual is an answer to a query is computed with a function with signature Insideness : $Q^d \\times Q^d \\rightarrow R$, that returns higher numbers for individuals that are answers to the query than for individuals that are not answers to the query. That is, given a query Q with a positive answer a and its corresponding randomly generated negative answer a' distinct from a, the goal of query embedding approaches is to minimize the following loss:\n$L_i(Q) ::= \\sum_{a \\in E^+} - log\\sigma (\\gamma - Insideness(Emb_Q, Emb_{\\{a\\}})) + \\newline \\sum_{\\{a'\\} \\in E^-} \\frac{1}{k} log\\sigma (\\gamma - Insideness(Emb_Q, Emb_{\\{a'\\}}))),$ (23)\nwhere $\\{a'\\}$ us the negative sample, $\\gamma$ is a margin hyperparameter and k is the number of random negative samples for each positive query answer pair."}, {"title": "The Relational Combinator", "content": "So far, we have described an interface consisting of neural oper-ators that are implemented by existing query embeddings. These neural operators allow the computation of tree-form query embed-dings, but not not DAG queries including the conjunction of roles. To enhance the capability of these methods for DAG queries, we introduce a relational combination operator with signature\nRCombiner_k: (R^d)^k \\rightarrow R^d, (24)\nwhere k is a positive natural number. The embedding of a role description $R_1 \\sqcap ... \\sqcap R_k$ (with k > 0) is:\n$Emb_{R_1 \\sqcap ... \\sqcap R_k} ::= RCombiner_k (Emb_{R_1},..., Emb_{R_k}),$ (25)\nwhere RCombiner is a commutative neural network. We used the neural operator DeepSet [8] to implement RCombiner.\n$RCombiner_k (Emb_{R_1}, ..., Emb_{R_k}) = \\sum_{1\\leq i\\leq k} \\alpha_i MLP(Emb_{R_i}))$ (26)\nwhere the weights $\\alpha_1,..., \\alpha_k$ sum 1. Specifically,\n$\\alpha_i = \\frac{exp(MLP(Emb_{R_i}))}{\\sum_{1 \\leq j \\leq k} exp(MLP(Emb_{R_j}))}.$", "proof": "PROPOSITION 4.1. Given two role descriptions R and S,\n$RCombiner_2 (Emb_R, Emb_S) = RCombiner_2 (Emb_S, Emb_R).$ (27)\nPROOF. It follows from the commutativity of the DeepSet. $\\Box$"}, {"title": "Encouraging Tautologies", "content": "So far, we have shown (see Proposition 4.1) that the proposed rela-tional combinator satisfies two of the ALCOIR tautologies pre-sented in Proposition 3.2, namely commutative and idempotence, but not the monotonicity and the restricted conjunction preserving. We hypothesize that by encouraging the query embeddings such that the inference of embeddings follow these tautologies, we can improve the embedding generalization capacity."}, {"title": "Monotonicity", "content": "We encourage the query embeddings of a DAG query Q = $\\exists (R \\sqcap S).C$ to be subsumed by the query embedding of query Q' = $\\exists R.C$ by introducing the following loss:\n$L_m(Q) = \\sum_{R,S \\in R} Insideness(Emb_Q, Emb_{Q'}).$ (28)\nIntuitively, Insideness measures the likelihood of $Emb_Q$, being in-side $Emb_{Q'}$."}, {"title": "Restricted conjunction preserving", "content": "We encourage the tautol-ogy $\\exists (r \\sqcap s).\\{ e \\} \\equiv \\exists r.\\{ e \\} \\sqcap \\exists s.\\{ e \\}$ (see Proposition 3.2) with the following loss:\n$L_r ::= Diff(Emb_{\\exists (r \\sqcap s).\\{ e \\}}, Intersect(Emb_{\\exists r.\\{ e \\}}, Emb_{\\exists s.\\{ e \\}})),$ (29)\nwhere Diff measures the distance between two query embeddings. We supplement the details on Diff of each individual query embed-ding method in Appendix A.\nBy imposing these loss terms, the tautologies are encoded into geometric constraints, which are soft constraints over the embed-ding space. Hence, our loss terms can also be viewed as regulation terms that reduce the embedding search space. Given a query an-swering training dataset D, our final optimization objective is:\n$\\mathcal{L}(D) ::= \\sum_{i=1}^{|D|} L_i(Q) + \\lambda_1L_m + \\lambda_2L_r,$ (30)\nwhere Lq is the query embedding loss, and $\\lambda_1$ and $\\lambda_2$ are the weights of regularization terms."}, {"title": "Experiments", "content": "In this section, we answer the following research questions with experimental statistics and corresponding case analyses. RQ1: How effectively does DAGE enhance the existing baselines in discovering answers to DAG queries that cannot be found by simply traversing the incomplete KG? RQ2: How well does the existing baselines with DAGE perform on tree-form queries? RQ3: How do the logical constraints influence the performance of DAGE?"}, {"title": "DAG Query Generation", "content": "Existing datasets, e.g. NELL-QA, FB15k237-QA, WN18RR-QA, do not contain DAG queries. We propose six new DAG query types, i.e., 2s, 3s, sp, us, is, and ins, as shown in Figure 1. Following these new query structures, we generate new DAG query benchmark datasets, NELL-DAG, FB15k-237-DAG, and FB15k-DAG.\nGiven that the answers to DAG queries represent a subset of those derived from the relaxed tree-form queries, evaluating the model's ability to handle DAG queries becomes challenging when"}, {"title": "Relaxed tree-form query types", "content": "Figure 4 illustrates the query graphs of the new DAG query types and their corresponding relaxed tree-form query types.\nEach of these queries are expressed as ALCOIR concepts as follows:\n2s ::= $\\exists (r_1 \\sqcup (r_2 \\sqcap r_3))^- .\\{e_1\\},$ (53)\ntree(2s) ::= $\\exists (r_1 \\sqcup r_2)^- .\\{e_1\\} \\sqcap \\newline \\exists (r_1 \\sqcup r_3)^- .\\{e_1\\},$ (54)\n3s ::= $\\exists (r_1 \\sqcup (r_2 \\sqcap r_3 \\sqcap r_4))^- .\\{e_1\\},$ (55)\ntree(3s) ::= $\\exists (r_1 \\sqcup r_2)^- .\\{e_1\\} \\sqcap \\newline \\exists (r_1 \\sqcup r_3)^- .\\{e_1\\} \\sqcap \\newline \\exists (r_1 \\sqcup r_4)^- .\\{e_1\\},$ (56)\nsp ::= $\\exists (r_1 \\sqcup (r_2 \\sqcup r_3) \\sqcap r_4)^- .\\{e_1\\},$ (57)\ntree(sp) ::= $\\exists (r_1 \\sqcup r_2 \\sqcup r_4)^- .\\{e_1\\} \\sqcap \\newline \\exists (r_1 \\sqcup r_3 \\sqcup r_4)^- .\\{e_1\\},$ (58)\nis ::= $\\exists (r_3 \\sqcup r_4)^-.(\\exists r_1.\\{e_1\\} \\sqcap \\exists r_2.\\{e_2\\}),$ (59)\ntree(is) ::= $\\exists r_3.(\\exists r_1.\\{e_1\\} \\sqcup \\exists r_2.\\{e_2\\}) \\sqcap \\newline \\exists r_4.(\\exists r_1.\\{e_1\\} \\sqcup \\exists r_2.\\{e_2\\}),$ (60)\nus ::= $\\exists (r_3 \\sqcup r_4)^-.(\\exists r_1.\\{e_1\\} \\sqcup \\exists r_2.\\{e_2\\}),$ (61)\ntree(us) ::= $\\exists r_3.(\\exists r_1.\\{e_1\\} \\sqcup \\exists r_2.\\{e_2\\}) \\sqcup \\newline \\exists r_4.(\\exists r_1.\\{e_1\\} \\sqcup \\exists r_2.\\{e_2\\}),$ (62)\nins ::= $\\exists (r_3 \\sqcup r_4)^-.(\\exists r_1.\\{e_1\\} \\sqcap \\neg \\exists r_2.\\{e_2\\}),$ (63)\ntree(ins) ::= $\\exists r_3.(\\exists r_1.\\{e_1\\} \\sqcap \\neg \\exists r_2.\\{e_2\\}) \\sqcap \\newline \\exists r_4.(\\exists r_1.\\{e_1\\} \\sqcap \\neg \\exists r_2.\\{e_2\\}).$ (64)"}, {"title": "Hyperparameters and Computational Resource", "content": "All of our experiments are implemented in Pytorch [10] frame-work and run on four Nvidia A100 GPU cards. For hyperparame-ters search, we performed a grid search of learning rates in $\\{5 \\times 10^{-5}, 10^{-4}, 5 \\times 10^{-4}\\}$, the batch size in $\\{256, 512, 1024\\}$, the nega-tive sample sizes in $\\{128, 64\\}$, the regularization coefficient w in $\\{0.02, 0.05, 0.08, 0.1\\}$ and the margin $\\gamma$ in $\\{10, 16, 24, 30, 40, 60, 80\\}$.\nThe best hyperparameters are shown in Table 7."}, {"title": "Further implementation details of DAGE with additional constraints", "content": "For the regularization of the restricted conjunction preserving tau-tology, we encourage the tautology $\\exists (r \\sqcap s).\\{ e \\} = \\exists r.\\{ e \\} \\sqcap \\exists s.\\{ e \\}$ (see Proposition 3.2) with the following loss:\n$L_r ::= Diff(Emb_{\\exists (r \\sqcap s).\\{ e \\}}, Intersect(Emb_{\\exists r.\\{ e \\}}, Emb_{\\exists s.\\{ e \\}})),$ (65)\nTo enforce the minimization of such loss in our learning objec-tive, we further mine two types of queries from the existing train queries, 2rs and 3rs, that can be expressed as ALCOIR concepts as follows:\n2rs ::= $\\exists ((r_1 \\sqcap r_2))^- .\\{e_1\\},$ (66)\ntree(2rs) ::= $\\exists (r_1) .\\{e_1\\} \\sqcap \\newline \\exists (r_2) .\\{e_1\\},$ (67)\n3rs ::= $\\exists ((r_1 \\sqcup r_2 \\sqcap r_3))^- .\\{e_1\\},$ (68)\ntree(3rs) ::= $\\exists (r_1) .\\{e_1\\} \\sqcap \\newline \\exists (r_2) .\\{e_1\\} \\sqcap \\newline \\exists (r_3) .\\{e_1\\}.$ (69)"}, {"title": "Computational costs of DAGE", "content": "To evaluate the training speed, for each model with DAGE, we calculated the average running time (RT) per 100 training steps on dataset NELL-DAG. For fair comparison with baseline models, we ran all models with the same number of embedding parameters. Integrating DAGE generally increases the computational cost for existing models. However, models like Query2Box can be enhanced"}, {"title": "Performance of DAGE on Tree-form queries", "content": "Table 9 summarizes the performances of query embedding mod-els with DAGE on existing tree-form query answering benchmark datasets, NELL-QA, FB15k-237-QA and FB15k-QA [3]. Firstly, DAGE enhances these models by enabling them to handle DAG queries while preserving their original performance on tree-form queries."}, {"title": "Comparison with other query embedding methods", "content": "To further assess the effectiveness of DAGE, we compare the base-line models enhanced by DAGE with two prominent query embed-ding models, CQD [11] and BiQE [4]. Both models are theoretically believed to be capable of handling DAG queries based on their de-sign. Table 10 and 11 summarize the performances of these models on our proposed DAG queries benchmark datasets. These meth-ods enhanced with DAGE consistently outperform CQD and BiQE across all types of DAG queries and datasets. Although some base-line methods perform significantly worse than BiQE and CQD in tree-form query answering tasks (according to the reported results from BiQE and CQD), their integration with DAGE allows them to improve and surpass those models on the new DAG query bench-mark datasets. This further supports our argument regarding the effectiveness of DAGE."}]}