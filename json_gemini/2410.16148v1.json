{"title": "PODTILE: Facilitating Podcast Episode Browsing with Auto-generated Chapters", "authors": ["Azin Ghazimatin", "Ekaterina Garmash", "Gustavo Penha", "Kristen Sheets", "Martin Achenbach", "Oguz Semerci", "Remi Galvez", "Marcus Tannenberg", "Sahitya Mantravadi", "Divya Narayanan", "Ofeliya Kalaydzhyan", "Douglas Cole", "Ben Carterette", "Ann Clifton", "Paul N. Bennett", "Claudia Hauff", "Mounia Lalmas"], "abstract": "Listeners of long-form talk-audio content, such as podcast episodes, often find it challenging to understand the overall structure and locate relevant sections. A practical solution is to divide episodes into chapters-semantically coherent segments labeled with titles and timestamps. Since most episodes on our platform at Spotify currently lack creator-provided chapters, automating the creation of chapters is essential. Scaling the chapterization of podcast episodes presents unique challenges. First, episodes tend to be less structured than written texts, featuring spontaneous discussions with nuanced transitions. Second, the transcripts are usually lengthy, averaging about 16,000 tokens, which necessitates efficient processing that can preserve context. To address these challenges, we introduce PODTILE, a fine-tuned encoder-decoder transformer to segment conversational data. The model simultaneously generates chapter transitions and titles for the input transcript. To preserve context, each input text is augmented with global context, including the episode's title, description, and previous chapter titles. In our intrinsic evaluation, PODTILE achieved a 11% improvement in ROUGE score over the strongest previous baseline. Additionally, we provide insights into the practical benefits of auto-generated chapters for listeners navigating episode content. Our findings indicate that auto-generated chapters serve as a useful tool for engaging with less popular podcasts. Finally, we present empirical evidence that using chapter titles can enhance the effectiveness of sparse retrieval in search tasks.", "sections": [{"title": "1\nIntroduction", "content": "We define chapterization as the task of dividing a document into semantically coherent, non-overlapping segments and assigning each segment an appropriate title that reflects its content. This process, also referred to as structured summarization [24] or smart chapter-ing [47], has been shown to provide users with a convenient and structured content overview and simplify navigation across a document [8, 20]. The value of chapterization has been acknowledged for its role in facilitating other tasks such as information retrieval [51] and the summarization of lengthy documents [8, 27]. With the increasing volume and availability of spoken user-generated content, like podcasts and videos, the need for chapterization has grown,\noffering significant benefits in content compression and navigation [8, 27].\nPodcast and video chapterization can ideally be provided by content creators themselves since there is no standardized format or protocol for chapter annotations. This, however, is frequently not the case; on our platform that hosts audio podcasts, the vast majority of episodes do not have creator-provided chapters. We bridge this gap by automating chapterization using a large language model-powered system trained on available creator chapters.\nMost of previous research has concentrated on chapterizing structured written texts, such as Wikipedia articles, news, and journals [36, 39, 52, 63]. There are however a few studies that focus on spoken discourse [24, 35, 47, 61, 68]. Yet, chapterizing spoken language documents, particularly podcast episodes, presents unique challenges compared to segmenting short, structured texts. Spoken discourse is usually more fluid, topically diverse, and less structured, and often features frequent digressions due to its interactive, real-time, and informal nature [17, 28, 47].\nAnother challenge is the considerable length of podcast episodes, whether measured by time or word count when transcribed. This not only increases computational costs but also poses a modeling challenge; many podcasts contain long-range semantic dependencies that need to be captured by chapterization. For instance, Figure 1(a) shows a podcast episode where the discussion diverges into a tangent about traveling before returning to the main topic of exercising. Such tangents are typical of informal conversational podcasts. To predict a chapterization like the one in Figure 1(a), a model must track the overarching context and theme. \"Knowing\" that the main topic is physical exercise helps the model distinguish segments about different aspects of this topic. Additionally, tracking predicted chapters throughout the episode helps the model generate consistent titles (in this example, focused on the guest named \"Peter\"). Chapterizing a Wikipedia article as illustrated in Figure 1(b), however, does not face these challenges since it is shorter and more structured.\nLately, there has been a growing focus on chapterizing conversational datasets. In [24], segmentation and title assignment are modeled jointly, enhancing the predictive capabilities of both tasks. This model leverages LongT5 [21] as the pre-trained sequence-to-sequence large language model (LLM). However, the context size of LongT5 is 16k which is not sufficient for processing podcast transcripts with 16k tokens on average. In Retkowski and Waibel [47], a two-stage chapterization model is used to first segment and then generate titles for the identified segments. This model uses longer context by incorporating previous chapter titles as left context summaries to generate chapter titles. The model's two-stage design, however, inhibits information sharing between the two tasks.\nWe can address the challenge of long inputs and long-distance dependencies in podcasts in several ways. First, a sufficiently large and powerful backbone LLM may provide a large enough context window to process an entire episode's transcript and produce accurate chapters. However, using a large LLM incurs significant computational and financial costs and may not fully capture all long-distance dependencies. To efficiently address these challenges, we propose PODTILE, a chapterization model that builds on the strengths of existing models, particularly [24], and extends them by dedicating a small portion of input text to explicit global context encoded as text: specifically, podcast episode metadata that reflects the overall content of the episode and previously generated chapter titles. This allows a reasonably-sized\u00b9 LLM to handle long and unstructured content effectively, without solely relying on the LLM's power. Following [24], we use LongT5 encoder-decoder model, which offers a compromise between efficiency and model power.\nWe validate our proposed approach using two public non-podcast datasets and one internal podcast dataset. Our findings indicate that using global context as part of the input text enhances the quality of chapter titles, particularly for longer documents in conversational datasets. We recently deployed PODTILE on our platform. Usage statistics indicate that podcast listeners find the auto-generated chapters helpful for browsing through episodes, particularly in lesser-known podcasts. Finally, we assess the utility of our generated chapter titles in a retrieval downstream task using the TREC Podcast Track dataset [26]. Adding these titles to episode descriptions significantly enhances sparse retrieval effectiveness compared to an extractive summarization baseline.\nWe summarize our contributions as follows:\n\u2022 introduction of a new model, PODTILE, which effectively extends [24] to address the challenges of podcast chapterization;\n\u2022 extensive intrinsic and extrinsic evaluations demonstrating the effectiveness and utility of the proposed approach;\n\u2022 deployment of the model in a user-facing production system and preliminary analysis of usage patterns for podcast chapters."}, {"title": "2 Related Work", "content": "We review related work, which has guided us in the various decisions we made to develop and deploy PODTILE.\nText segmentation. Early approaches for text segmentation (aka boundary detection) were unsupervised due to lack of sufficient supervised data. These approaches involve computing a cohesion score or mutual information [55] between consecutive blocks of sentences. This can be achieved using TF-IDF (or its variations) [10, 23],"}, {"title": "3 Method", "content": "Our work builds on the method by Inan et al. [24], modeling chapterization as simultaneous segmentation and title generation in a sequence-to-sequence fashion. The input is the text to be chapterized, and the output is a textual specification of chapter boundaries and titles. This approach uses a pre-trained LLM fine-tuned on supervised data, leveraging its vast linguistic and real-world knowledge. As a text-based model, it effectively integrates segmentation and title prediction, while also incorporating diverse contextual information to enhance the accuracy of the prediction, which is the main contribution of this paper.\nWe refer to the model by Inan et al. [24] as our core model, which we detail and explain its application to the podcast domain in Section 3.1. Our contribution involves incorporating additional contextual cues into this core model to improve generalization on long-input data and mitigate the limitations of the local nature of chapterization inference. Specifically, we explore:\n(1) Static context (Section 3.2): Metadata outlining the overall content of the document. This is useful when the model cannot access the entire document at once. Specific implementations depend on the domain and dataset, detailed in Section 4.\n(2) Dynamic context (Section 3.3): The intermediate state of the left-to-right chapterization process. This information provides access to earlier chapterization decisions, guiding the selection of subsequent chapters."}, {"title": "3.1 Core model", "content": "Our core model is based on the segmentation and labeling framework Gen (seg+label) from Inan et al. [24], which has been demonstrated experimentally to be the best-performing variant of their model. This model employs an encoder-decoder architecture with an underlying Transformer LLM. While any existing LLM adhering to the seq2seq API can be used, our experiments specifically use the LongT5 pre-trained LLM [21], in line with Inan et al. [24].\nThe input-output formatting for our core model is illustrated in Figure 2. We augment the raw input text by adding index numbers before each sentence. This allows the decoder to predict the start of a chapter by referencing one of these indices. The output sequence is a chronologically ordered concatenation of strings formatted as:\n${first_sentence_index} := ${title}\nwith character \"|\" as the separator. Given that input texts can be arbitrarily long-particularly in media such as podcasts (see Table 1)- and open-source LLMs have limited input capacities, the initial step in our approach involves chunking the input text into segments that can be processed by an LLM. Each training datapoint consists of a chunk of input text and the corresponding output string, which"}, {"title": "3.2 Adding static context", "content": "When processing chunked input (explained above), the model lacks access to content before or after a given chunk. This can result in predicted chapters that are either not specific enough to distinguish content outside the chunk or too focused on details specific to the chunk but irrelevant to the overall discussion.\nTo address this, we propose including metadata that outlines the document's overall content, providing a general context. We call this static context, as it is provided prior to chapterization and remains unchanged. The specific content and structure of this metadata varies by dataset, detailed in Section 4. Figure 2 shows an example with the title and description of an episode as static context."}, {"title": "3.3 Adding dynamic context", "content": "Another disadvantage of local chunked processing is the model's lack of awareness of prior chapterization decisions for a given input document. As a result, each local prediction step may produce boundaries and titles that are inconsistent with previous decisions. This can lead to issues such as repetitive titles, different levels of chapter granularity, and varying linguistic styles in titles.\nTo provide dynamic information about the state of the chapterization process, we add the sequence of titles already predicted for the earlier portions of the document to the input text. Figure 2 shows how previously predicted titles are added to the input text."}, {"title": "4 Experimental setup", "content": ""}, {"title": "4.1 Datasets", "content": "We downsampled our podcast dataset from a proprietary internal catalog, using only English episodes that were chapterized by their creators. Our final dataset contains 10.8k episodes, uniformly sampled with several filters. Chapters in these episodes range from 30 seconds to 30 minutes, and titles are shorter than 15 words. We randomly split the resulting dataset into train/validation/test partitions of 8k/1k/1k episodes. For each episode, we use both title and description as the static context since 96% of episodes in our catalog have descriptions, with 57% of those longer than 20 words. The majority (91%) of episodes in our dataset are conversational, featuring multi-speaker discussions.\nTo gauge PODTILE's effectiveness across different domains, we use two other publicly available English datasets. WikiSection [3] is a Wikipedia-based dataset limited to two categories, en_disease and en_city, with normalized section titles for discriminative title prediction."}, {"title": "4.2 Baselines", "content": "We compare PODTILE against the following baselines:\nCATS [52]: A multi-task learning model that combines boundary classification with coherent sequence detection that differentiates correct sequences of sentences from the corrupt ones. This model is chosen due to the recent state-of-the-art performance of hierarchical encoders for segmenting video transcripts [47].\nGen (seg + label) [24]: A single-stage seq2seq model that uses LongT5 to jointly generate chapter titles and boundaries (structured summarization), similar to PODTILE's core model.\nGPT-4 [1]: Zero-shot learning with GPT-4, using an extended context of 128k tokens (gpt-4-0125-preview). We instruct the model to chapterize the entire transcript and return the chapter titles and starting sentence IDs in JSON format for easy parsing. The experiment was conducted in the second week of May 2024."}, {"title": "4.3 Implementation details", "content": "We use LongT5 [21] (base size, ~220M parameters) with transient global attention, as our backbone model. The training setup includes a batch size of 1, a learning rate of 5.0e-5 with scheduler type of linear, and a maximum of 4 epochs. The same setting was used for other datasets with the exception of learning rate 1.0e-4 for Wikisection. We use input chunks of up to 8000 words,\u00b3 with 7000 words dedicated to the document text and up to 1000 words to the metadata. In Gen(seg+label), all the 8000 words are used for document text. On average, each transcript in the podcast dataset is broken into 1.75 chunks. Training and inference for offline evaluations were conducted on a Ray [40] cluster with a single node and a single GPU. Training on the podcast dataset took approximately 3 days. Inference of 1.1k episodes lasts an average of 1 hour."}, {"title": "4.4 Evaluation metrics", "content": "It is common to evaluate chapter boundaries and generated titles separately using their respective metrics [24]. For segmentation evaluation, we use WindowDiff [45], which measures the average difference between the number of boundaries in predicted and reference values over spans of k sentences. This metric is parametrized by k, the sliding window size, usually set to half the average segment length (in sentences). We estimate k for each dataset using the training partition and report it in Table 2. Lower metric values indicate more accurate segmentation.\nFor titles, reference-based summarization metrics like ROUGE [34] and BERTScore [67] are commonly used. Previous work often computes these metrics on summaries created by concatenating chapter titles sequentially, which hinders individual title assessment.\nTo evaluate titles individually, we employ a heuristic alignment method between reference and predicted chapters. For each chapter ci in one set (reference or prediction), we find the chapter cj in the other set with the highest overlap at sentence level, then match their titles. Note that this matching process is asymmetric, meaning a title matched from the reference to the prediction set does not guarantee a reverse match. We use SBERT title representations [46]4 to apply soft-matching distance and define the metrics as:\nROUGELF1,aligned = \\frac{\\Sigma_{(t,t')\\in Matches_{all}} ROUGE_{LF1} (t,t')}{Matches_{all}}\nSBERTprec = \\frac{\\Sigma_{(t,t') \\in Matches_{pred}} SBERT(t,t')}{|Matchespred|}\nSBERTrecall = \\frac{\\Sigma_{(t,t') Matches_{ref}} SBERT(t,t')}{Matchesref}\nwhere Matchespred is the set of title pairs (t, t') where a predicted title t' is matched with reference title t with highest overlap. Similarly, Matchesref is a set of title pairs (t, t') where reference title t is matched with predicted title t' with highest overlap. Matchesall denotes the union of Matchespred and Matchesref. For simplicity, we refer to ROUGELF1,aligned as ROUGELF1 hereinafter. SBERTF1 is computed as the geometric mean of (2) and (3)."}, {"title": "4.5 Ethics Statement", "content": "We display auto-generated chapters for episodes that do not have creator-provided chapters. Users are informed that these chapters are generated by AI with the following disclaimer: The chapters are auto-generated. Additionally, we ensure compliance with the terms and conditions of Spotify for Podcasters and allow creators to overwrite AI-generated chapters or opt-out of this feature at their discretion. To protect users from potentially harmful AI-generated content, we employ a safety mechanism to remove sensitive or inappropriate titles before they are displayed. We also allow for the immediate manual removal of any reported harmful content."}, {"title": "5 Offline Results", "content": "We present the findings from our experiments, addressing four research questions. The results are detailed in Tables 2, 3 and 4.\nQ1: How does PODTILE perform on conversational datasets? Table 2 shows that PODTILE (row 4), with both static and dynamic context enabled, significantly outperforms the strongest baseline, Gen (seg+label) (row 3), on the podcast dataset according to title metrics (paired t-test, p-value < 0.05). A similar trend is observed in the QMSum dataset (rows 11-13), though not statistically significant which might be due to its small test set (35 documents). This highlights the importance of capturing global context for high-quality title generation. Segmentation accuracy, measured by WinDiff, remains close to the baseline, indicating that segmentation relies less on global context. Comparison with CATS (row 1) suggests that coherence modeling in this method is less effective on conversational datasets compared to structured texts. The lower performance of GPT-4 zero-shot inference (row 2) highlights the challenge of chapterizing long conversational documents without fine-tuning, even for powerful models like GPT-4. On Wikisection (rows 8-10), where documents are short and well-structured, our model performs comparably to Gen(seg+label), as expected.\nQ2: Do static and dynamic context contribute equally to improving title metrics? The results in Q1 suggest that our new contextual features improve the title quality of conversational data more than boundary accuracy. To examine the individual effects of static and dynamic context on titles, we conduct an ablation study (rows 5-7). Disabling static context (row 6) causes a more significant decrease in title metrics than disabling dynamic context (row 5).5After examining a few examples, we speculate that lower performance in the dynamic context-only model may be due to a chapterization style different from the ground truth, hinting at the insufficiency of the state-of-the-art reference-based metrics and a single ground truth for chapterization.\nFor a deeper understanding of the context's effect on titles, particularly title consistency across chapters within an episode, we examine title length variation. We compute the coefficient of variation for each episode and average it across the test set. Higher average coefficients indicate lower consistency. The baseline (row 3) shows the highest variation (0.6), while PODTILE and the dynamic context-only model score the lowest (0.55). The static context-only model's score (0.58) is close to the baseline. These results highlight the limitations of reference-based metrics used in Table 2 and show that dynamic context positively contributes to title quality, aligning with the original motivation for this feature (conditioning the next title on the already predicted ones).\nQ3: Do longer documents benefit more from global context? The primary rationale behind integrating global (static and dynamic) context in PODTILE's input was to improve the chapterization of long documents that exceed the model's context size. Thus, we hypothesized that longer documents would benefit more from PODTILE compared to the baselines. This hypothesis is validated by the findings in Table 3. The first row shows the percentage improvement in title quality over the baseline, Gen(seg+label), for documents fully processed by PODTILE. The second row shows improvements for longer documents requiring chunking, which make up 80% of the test data. It is evident that longer documents see more substantial improvements. Table 4 demonstrates how using metadata for chapterizing long documents enhances chapter titles' informativeness. PODTILE adds words like \"Planet\" and \"Sandeep\" from the metadata, which are missing in the input chunk with chapter boundaries due to an already established context.\nQ4: How does the length and source of the static context impact chapterization? To test if longer static context enhances auto-generated chapter quality, we computed the Spearman rank correlation between static context length and the AROUGELF1 of PODTILE with and without static context. We found a negligible negative correlation, suggesting that longer static context does not necessarily improve metric scores.\nGiven the increasing use of LLMs for content generation, we further explored the robustness of PODTILE to AI-generated static context. For this, we instructed GPT-4 to generate episode descriptions based on the episode transcripts and used them in place of creator-provided descriptions. As a result, we observed a 7% drop in ROUGELF1 compared to PODTILE that uses creator descriptions (row 4 in Table 2). We conclude that creator-provided static context is more effective for chapterization."}, {"title": "6 Deployment", "content": "Podcast chapters with creator-provided timestamps have been available on our platform. There overall coverage, however, is low. In April 2024, we started a limited roll-out of our chapterization model."}, {"title": "7 Extrinsic Evaluation", "content": "Podcast chapterization primarily aims at facilitating navigation through episode content. This section shows how podcast chapters can also enhance episode search retrieval as a downstream task.\nTextual descriptions of podcast episodes often miss key details that listeners seek. These details are usually in the transcripts, which are lengthy and costly to index. We propose using chapter titles as summaries instead of full transcripts to enhance episode descriptions. This approach could reduce costs by at least tenfolds compared to indexing entire transcripts. We believe that adding chapter titles to descriptions will significantly improve sparse retrieval in search by including important terms users search for.\nTo test this hypothesis, we design an experiment to explore the impact of indexing chapter titles on search effectiveness. For this, we use the TREC podcast dataset [26] collected for short segment retrieval and summarization task. This dataset contains human relevance judgments for 54 search queries, of 3 types: topical, refinding, and known items. a pool of 100k episodes, and 900 labeled query-episode pairs. Note that in this experiment, we perform retrieval and report metrics on episode-level and not segment-level. We use BM25, implemented by Anseri [62], as the retrieval method, measure search success by nDCG, recall, and Reciprocal Rank (RR), and consider 4 methods for indexing episodes:"}]}