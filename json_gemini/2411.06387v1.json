{"title": "Self-Training Meets Consistency: Improving LLMs' Reasoning With Consistency-Driven Rationale Evaluation", "authors": ["Jaehyeok Lee", "Keisuke Sakaguchi", "JinYeong Bak"], "abstract": "Self-training approach for large language models (LLMs) improves reasoning abilities by training the models on their self-generated rationales. Previous approaches have labeled rationales that produce correct answers for a given question as appropriate for training. However, a single measure risks misjudging rationale quality, leading the models to learn flawed reasoning patterns. To address this issue, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a self-training framework that further evaluates each rationale through follow-up questions and leverages this evaluation to guide its training. Specifically, we introduce two methods: (1) filtering out rationales that frequently result in incorrect answers on follow-up questions and (2) preference learning based on mixed preferences from rationale evaluation results of both original and follow-up questions. Experiments on three question-answering datasets using open LLMs show that CREST not only improves the logical robustness and correctness of rationales but also improves reasoning abilities compared to previous self-training approaches.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) can enhance multi-step reasoning abilities by generating intermediate reasoning steps (i.e., rationale) before arriving at an answer (Wei et al., 2022). Training LLMs on high-quality rationales has been shown to improve their reasoning capabilities (Chung et al., 2024; Liu et al., 2023; Shridhar et al., 2023). Therefore, collecting high-quality rationales is becoming increasingly important for training the reasoning abilities of LLMs. However, due to the high cost associated with collecting high-quality rationales, self-training approaches have emerged, focusing on training LLMs using self-generated rationales (Zelikman et al., 2022).\nIn self-training approaches, accurately evaluating the quality of generated rationales is essential. Previous studies have evaluated rationale quality by examining whether the generated rationales lead to the correct answer to a given question (Zelikman et al., 2022; Hoffman et al., 2023; Feng et al., 2024; Hosseini et al., 2024; Singh et al., 2024). However, using the correctness of a single prediction is unstable, as LLMs can reach correct answers through inappropriate reasoning steps (Bao et al., 2024). Training models on such inappropriate rationales can cause them to learn flawed reasoning patterns.\nTo address this problem, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a novel framework for LLM self-training. The core idea of CREST is to further evaluate rationales using follow-up questions that ask whether each answer option in the original question is correct or not. We first generate diverse rationales and evaluate them with an LLM. Subsequently, we train the LLM on these rationales, rewarding rationales that lead to more consistent predictions and penalizing those that lead to less consistent predictions . To achieve this, we propose two methods: rationale filtering and preference learning. In rationale filtering, we remove rationales that lead to incorrect answers in more than a certain number of follow-up questions during the supervised fine-tuning process. In preference learning, we train the model on mixed preferences from results of both original and follow-up questions, to favor rationales that result in correct answers in a greater number of follow-up questions.\nWe conduct experiments on three natural language reasoning question-answering datasets, including ReClor (Yu et al., 2020), ARC (Clark et al., 2018), and CSQA (Talmor et al., 2019). We compare CREST to other self-training approaches using Llama 3 model (AI@Meta, 2024) and Gemma model (Team et al., 2024). Our findings show that CREST can train an LLM to generate more correct and robust rationales, improving its reasoning performance. Our contributions are as follows:\n\u2022 We introduce consistency-driven rationale evaluation, which further evaluates generated rationales using follow-up questions that ask whether each answer option in the original question is correct or not.\n\u2022 We propose CREST, which evaluates generated rationales via consistency-driven rationale evaluation and uses the evaluation results to train an LLM through two methods: rationale filtering and preference learning using mixed preferences derived from original and follow-up question evaluations.\n\u2022 We conduct experiments and analyses with open LLMs such as Llama 3 model and Gemma model on three question-answering datasets. The results show that CREST generates more robust and correct rationales and improves reasoning ability compared to other self-training approaches."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Self-Training Approaches", "content": "The Chain-of-Thought (CoT) approach demonstrates that generating a step-by-step reasoning path before the final prediction enhances an LLM's reasoning abilities (Wei et al., 2022). Training LLMs on rationale data generated by humans (Chung et al., 2024) or advanced models like GPT-4 further enhances reasoning abilities (Liu et al., 2023). However, since high-quality rationale data is expensive to obtain, a number of approaches focus on training language models using self-generated rationales. STaR (Zelikman et al., 2022), an early type of self-training approach, trains the language model by selecting the correct rationales based on binary feedback regarding the correctness of the answers generated by these rationales. RFT (Yuan et al., 2023) enhances supervised data by generating and collecting diverse correct reasoning paths, focusing on mathematical reasoning. Other approaches, such as V-STaR, Iterative RPO, and Self-motivated Learning, also utilize incorrect rationales (Feng et al., 2024; Hosseini et al., 2024; Pang et al., 2024) and adopt preference learning techniques, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Direct Preference Optimization (DPO) (Rafailov et al., 2023). Self-Explore (Hwang et al., 2024) provides fine-grained rewards by identifying incorrect steps within the rationales. CREST provides fine-grained rewards through evaluating a rationale multiple times using follow-up questions augmented from the original dataset."}, {"title": "2.2 Reasoning with Consistency", "content": "Consistency is the ability to make consistent decisions in semantically equivalent contexts (Elazar et al., 2021). It is a desirable property of logically valid machine learning systems (Chen et al., 2024a) and an important characteristic for a model to be considered trustworthy (Jang et al., 2022). As larger language models emerge that exceed human performance in many tasks, consistency is receiving increased attention due to its role in evaluating inference validity, even in models that outperform humans (Fluri et al., 2024). To evaluate a model's consistency, follow-up questions generated from existing questions are commonly used (Ribeiro et al., 2019; Elazar et al., 2021; Jang et al., 2022; Chen et al., 2024a; Zheng et al., 2024; Chen et al., 2024b). Several techniques have been developed"}, {"title": "3 Consistency-driven Rationale Evaluation for Self-Training", "content": "This section describes our approach, Consistency-driven Rationale Evaluation for Self-Training (CREST) which trains reasoning abilities through consistency-driven rationale evaluation with follow-up questions."}, {"title": "3.1 Notation", "content": "We have a pretrained large language model M and an original dataset of questions q with answers a, represented as D = {(q_i, a_i)}_{i=1}^I. Each question has F answer choices. To solve q, M sequentially generates a rationale r, corresponding to intermediate reasoning steps, and an answer prediction p, where r leads to p."}, {"title": "3.2 CREST", "content": "The whole framework of CREST consists of four stages. Figure 2 outlines the overview of CREST.\n\u2022 Rationale Generation We generate N diverse rationales r^n for each question q_i and the corresponding answer predictions p_i^r using M, where n \u2208 [1, N].\n\u2022 Rationale Evaluation We compare p_i^r with a_i to assign a reward z to r^n based on the correctness of the prediction. Subsequently, we generate multiple follow-up questions \\tilde{q}_{i,f} from q_i and further evaluate r^n using these follow-up questions. We assign an additional reward \\tilde{z} to r^n based on how many \\tilde{q}_{i,f} are answered correctly."}, {"title": "3.3 Rationale Generation", "content": "Initially, we generate diverse rationales and the corresponding answer predictions for a given original question q_i with M. Specifically, M generates N rationales r^n that represent intermediate reasoning steps leading to an answer prediction, as follows: r^n \\leftarrow M(q_i), where r^n represents the n^{th} rationale generated for the i^{th} question. Subsequently, M derives answer predictions p_i^r for q_i from generated rationales r^n, as follows: p_i^r \\leftarrow M(q_i, r^n)."}, {"title": "3.4 Rationale Evaluation", "content": "We evaluate the rationale through a two-step process. Firstly, similar to previous studies (Zelikman et al., 2022; Yuan et al., 2023; Hosseini et al., 2024; Feng et al., 2024; Pang et al., 2024), we compare the ground truth answer a_i for q_i with the predicted answer p_i^r derived from r^n. Secondly, we further assess the rationales through F follow-up questions which are generated from the original question q_i.\nIn the first step, we assign a binary reward z of either 0 or 1 to each rationale based on whether p_i^r matches a_i as follows:\nz = 1(p_i^r = a_i) \t\t(1)\nAssuming that rationales leading to the correct answer are of higher quality than those that do not, as suggested by Zelikman et al. (2022), this evaluation directly measures the quality of rationales.\nIn the second step, we evaluate the rationales using F follow-up questions {(\\tilde{q}_{i,1}, \\tilde{a}_{i,1}), ..., (\\tilde{q}_{i,F}, \\tilde{a}_{i,F})} generated from q_i, where \\tilde{a}_{i,f} is the ground truth answer for the f^{th} follow-up question corresponding to q_i. We then evaluate the rationales on all F follow-up questions: p_i^f \\leftarrow M(\\tilde{q}_{i,f}, r^n), where \\tilde{q}_{i,f} is f^{th} follow-up question for q_i.\nWe assign an additional reward \\tilde{z} to each rationale based on the number of correctly solved follow-up questions as follows:\n\\tilde{z} = \\sum_{f=1}^F 1(p_i^f = \\tilde{a}_{i,f}) \t\t(2)"}, {"title": "3.5 Supervised Fine-Tuning", "content": "After evaluating the rationales, we use z and \\tilde{z} as filters to select the rationales for training M and produce M_{SFT} through supervised fine-tuning (SFT). Intuitively, the best rationales for q_i from the previous stage are those that lead to the correct answers to q_i and all F follow-up questions, indicated by z = 1 and \\tilde{z} = F. However, simply removing rationales that lead to incorrect answers for any of the follow-up questions might drastically reduce the number of rationales available for training. Therefore, we also include some sub-optimal rationales with a tolerance term t that satisfies t \u2208 [0, F]. Consequently, the dataset D_{SFT} used to train M in the SFT stage is represented as follows:\nD_{SFT} ={(q_i, r_i, a_i)_{(n,i) \\in {(n,i)| z_i^n = 1, \\tilde{z}_i^n > F - t}}} \t\t(3)\nThe training objective for this stage aligns with that used during pretraining, specifically employing an auto-regressive language modeling objective or next-token prediction (Radford et al., 2018). We calculate the loss exclusively for the output section (i.e., r and a)."}, {"title": "3.6 Preference Learning", "content": "We further train M_{SFT} by exploiting preferences between rationales to enhance its reasoning ability. To achieve this, we construct preference pairs and fine-tune M_{SFT} using offline preference learning methods, such as Direct Preference Optimization (DPO) (Rafailov et al., 2023)."}, {"title": "3.6.1 Preference Pair Dataset Construction", "content": "We construct two preference pairs P_z and P_{\\tilde{z}}, which represent rationale preferences based on the rewards z and \\tilde{z}, respectively. We generate preference pairs where rationales with higher rewards r^w are preferred over those with lower rewards r^l. Therefore, every pair consists of a question q, two"}, {"title": "3.6.2 Training", "content": "We train M_{SFT} on the preference pairs P_{total} using DPO, resulting in M_{CREST}. Given the preference pairs P_{total}, the objective of this stage is to increase the log probability of preferred outputs over dispreferred ones:\nL_{DPO} = E_{(r^w, p^w, r^l, p^l | q_i) \\sim P_{total}} [log\\sigma (r_{\\theta}(q_i, r^w, p^w) - r_{\\theta}(q_i, r^l, p^l))] \t\t(4)\nr_{\\theta}(q, r, p) = \\beta log \\frac{\\pi_{\\theta}(r, p|q)}{\\pi_{ref}(r, p|q)} \t\t(5)\nwhere \\pi_{\\theta}(r, p|q) and \\pi_{ref}(r, p|q) represent the probability of outputs r and p given input q under the current policy parameterized by \u03b8 and a reference policy \u03c0_{ref}, respectively. Initially, both \u03c0_\u03b8 and \u03c0_{ref} are initialized as M_{SFT}, and they are updated each epoch. \u03c0_{ref} is used to minimize distribution shift from the true reference distribution and is typically initialized through supervised fine-tuning on preferred outputs. \u03b2 controls the deviation from the reference policy."}, {"title": "4 Experiments", "content": "This section describes the experiments and results of CREST compared to other self-training approaches. First, we introduce the three datasets used for model training and testing. Next, we present the experimental setup, including the base LLM, key hyperparameters, and performance metrics. We also introduce the baseline approaches used for comparison, and finally, we present the results of the experiments."}, {"title": "4.1 Experimental Settings", "content": "Datasets We evaluate CREST on three English natural language reasoning multiple-choice QA datasets: ReClor (Yu et al., 2020), ARC (Clark et al., 2018), CSQA (Talmor et al., 2019). ReClor comprises logical reasoning problems derived from American graduate school entrance exams and their preparatory materials. The ReClor test set is divided into an Easy set, which consists of biased data points, and a Hard set, which includes the remaining data points. ARC is sourced from"}, {"title": "4.2 Baselines", "content": "Fine-tune (Label) involves directly fine-tuning the base model on ground truth labels using a negative log-likelihood loss term, without relying on any generated rationales.\nSTaR (Zelikman et al., 2022) is an early approach for generating, filtering, and learning rationales using a generative language model. It generates a rationale for each question and trains the language model on rationales that lead to correct predictions. Additionally, STaR introduces a rationalization process that provides hints when the initial rationale fails to produce a correct prediction.\nRFT (Yuan et al., 2023) stands for Rejection Sampling Fine-Tuning. RFT generates diverse rationales with a non-zero temperature and selects rationales to train based on binary feedback on the correctness of the final prediction. Unlike STaR, RFT does not have a rationalization process. In our experiments, M_{SFT} with maximum tolerance corresponds to RFT."}, {"title": "4.3 CREST", "content": "M_{SFT} is supervised fine-tuned on filtered rationales from the base model. The performance difference between this model and RFT demonstrates the effect of the rationale filtering process.\nM_{CREST} & Fine-tune (Label)_{CREST} are models trained using preference learning in CREST, based on M_{SFT} and Fine-tune (Label), respectively. To evaluate the effectiveness of preference learning with P_{total}, we apply it to two models: M_{SFT}, a model fine-tuned on filtered rationales, and Fine-tune (Label), a model fine-tuned directly on ground truth labels. The resulting models, named M_{CREST} and Fine-tune (Label)_{CREST}, demonstrate how CREST enhances reasoning performance through preference learning."}, {"title": "4.4 Results", "content": "As shown in Table 1, M_{CREST} outperforms other self-training baselines across the three datasets. Both RFT and M_{SFT} are models trained through supervised fine-tuning on the base model, with"}, {"title": "5 Analysis", "content": "In this section, we explore the effectiveness of consistency-driven evaluation and the impacts of rationale filtering and preference learning in CREST on model performance, through analyses using the Llama 3 8B model as the base model. Our analysis includes examining the correlation between z and \\tilde{z} and conducting ablation studies on parameters such as t and \u03bb to assess how the proposed methods in CREST contribute to performance improvement. To investigate the impact of preference learning with P_{\\tilde{z}}, we create a model that trains M_{SFT} using preference learning with only P_z, which we refer to as M_{SFT} /w P_z, and compare it to M_{CREST}."}, {"title": "5.1 Incorrect Rationales on Follow-up Questions", "content": "To understand how evaluation through follow-up questions reflects the quality of rationales, we evaluate incorrect rationales (z = 0) generated from train datasets on the follow-up questions, as shown in Figure 3. The incorrect rationales are less robust"}, {"title": "5.2 Effect of Tolerance t on Supervised Fine-Tuning", "content": "We investigate the impact of the tolerance value t during the supervised fine-tuning stage on task performance and the number of rationales used for training across the three datasets. Figure 4 shows the relationship between performance and the training data proportion based on the tolerance t. In the ARC-Challenge and CSQA datasets, performance improves as t increases, peaking at t = 2, and then tends to decrease as t continues to rise. This pattern shows that training on rationales that lead to incorrect predictions for most follow-up questions negatively affects task performance. At the maximum t value, accuracy is lower than at t = 0, where only 42% and 74% of the total generated rationales are used for training in CSQA and ARC, respectively. In ReClor, which requires more complex and broader logical reasoning, peak performance occurs at t = 3, differing from the other two datasets. However, including rationales with z = 0 in training leads to a decrease in performance. These results demonstrate that filtering out less robust rationales improves reasoning ability,"}, {"title": "5.3 Effect of X on Preference Pair Dataset", "content": "To analyze how the two preference pair datasets, P_z and P_{\\tilde{z}}, affect reasoning abilities through preference learning, we conduct experiments on ReClor using various \u03bb values. As shown in Figure 5, we observe a trade-off where increasing \u03bb improves performance on the Hard set but decreases performance on the Easy set. The overall performance peaks at x = 0.6, where the trade-off is most balanced. Given that the ReClor Easy set consists of biased data points, preference learning on P_{\\tilde{z}} makes the model less dependent on these biases, thereby improving the robustness of its reasoning ability."}, {"title": "5.4 Evaluating Quality of Rationales", "content": "To qualitatively evaluate how the CREST impact the model's rationale generation, we randomly sample 100 questions from the ReClor validation set and evaluate the rationales from each model with GPT-40. Following the methodology of Hwang et al. (2024), we use FLASK (Ye et al., 2024) with GPT-40 to evaluate the logical robustness, correctness, and efficiency of the rationales. Especially, rationale filtering in supervised fine-tuning improves the logical robustness and efficiency of the rationales."}, {"title": "5.5 Evaluating CREST Models on Follow-up Questions", "content": "We evaluate the rationales generated by each trained model for the original questions in the ReClor validation set using follow-up questions, which is shown in Figure 6. As in the Rationale Generation and Evaluation stage, we input the generated rationales and follow-up questions into the base model (Llama 3 8B), then measure accuracy over all follow-up questions. To assess how different training methods affect the rationale generation, we adopt Zero-shot-CoT (Kojima et al., 2022) as a baseline model. The improvement between RFT and M_{SFT} shows the effect of rationale filtering in generating rationales that are more robust to follow-up questions."}, {"title": "6 Conclusion", "content": "In this paper, we propose CREST, a novel self-training framework that evaluates generated rationales in a fine-grained manner by letting the LLM solve follow-up questions derived from the original question. We propose two methods for utilizing the evaluation results in training: filtering out less consistent rationales for supervised fine-tuning and employing preference learning to favor more consistent rationales over less consistent ones. Experimental results on three question-answering datasets show that CREST enables an LLM to generate more correct and robust rationales and achieves better performance compared to previous approaches."}, {"title": "7 Limitations", "content": "The main idea of our proposed framework CREST is to evaluate rationales with multiple follow-up questions, which is conceptually task-agnostic. In this paper, we assume a multiple-choice question-answering task as the primary setting. However, there are other types of tasks that differ significantly in structure and may require adaptations of our framework to maintain its effectiveness. For future work, we plan to extend the CREST beyond multiple-choice question-answering, applying it to scenarios such as math questions (Cobbe et al., 2021) or open-ended questions (Ling et al., 2023) where choices are not provided.\nWe treat all follow-up questions equally and focus solely on the number of follow-up questions answered correctly to calculate the additional reward \\tilde{z}. However, since each follow-up question asks whether a given option is correct, the interpretation of follow-up questions for correct and incorrect answers can differ. For instance, consider two rationales that receive the same reward, z = 2, for a question with the correct answer being A. The first rationale accurately answers the follow-up questions about the correct option (A) and an incorrect option (B), while the second rationale accurately answers the follow-up questions about two incorrect options (B and C). Although both rationales receive the same reward, their interpretations differ: the first rationale provides information about the correct answer, whereas the second does not. This difference in interpretation may affect rationale evaluation and training. Kawabata and Sugawara (2023) show the differences in LLMs' ability to handle each option, revealing that LLMs struggle with questions related to incorrect answers, whereas questions related to correct answers are easier for them. Future research could exploit this difference to further extend CREST.\nAdditionally, while our study primarily focuses on self-training of language models, the methods we propose for evaluating rationales and leveraging these evaluations during training can be applied to broader scenarios such as distilling reasoning abilities from larger teacher models to smaller student models (Liu et al., 2023; Shridhar et al., 2023; Hsieh et al., 2023)."}, {"title": "A Phi-2 Experiment Result", "content": "To demonstrate the robustness of CREST, we also test CREST with Phi-2 model (Javaheripi et al., 2023). Phi-2 has 2.7B parameters, which is much smaller compared to Llama 3 8B and Gemma 7B which have 8.0B and 8.5B parameters, respectively. As shown in Table 3, CREST outperforms other self-training baselines across the three datasets, and preference learning to Fine-tune (Label) model consistently improves performance. This result shows that CREST can function effectively with this relatively small model."}, {"title": "B Data and Rationale Statistics", "content": "Table 5 describes the number of examples in train, validation, and test splits for the data we use. Additionally, Table 6 shows the number of rationales generated in the rationale generation stage in our experiments according to the z and \\tilde{z} values. Since the official test set of CSQA is evaluated every two weeks, we use the official Dev set as the test set in our experiment and extract a new validation set with the same number of samples from the train set."}, {"title": "C Rationale Generation and Evaluation Case Study", "content": "Table 4 shows an example of generated rationales from a CSQA question and their evaluation. We can see the rationale which leads to an incorrect answer to the question (z = 0) represents incorrect reasoning steps and conclusion. The rationale with \\tilde{z} = 2 leads to the correct answer D but does not show a convincing reasoning process, causing readers to be confused between C and D. In contrast, the rationales with higher rewards of z = 4 and \\tilde{z} = 5 provide more convincing reasoning processes. They offer a comprehensive explanation for arriving at the correct answer D and include judgments about why other choices are incorrect, respectively."}, {"title": "D Detailed Preference Pair Datasets Construction", "content": "This section presents a more detailed algorithm for constructing the preference pair dataset used in preference learning. As shown in Algorithm 1, we construct two preference pair sets, P_z and P_{\\tilde{z}}, based on z and \\tilde{z}, respectively. Then, we construct P_{total} such that the proportion of P_{\\tilde{z}} in P_{total} is \u03bb, and the proportion of P_z is 1 \u2013 \u03bb, by randomly sampling pairs from P_z and P_{\\tilde{z}} and combining them."}, {"title": "E Prompts", "content": "In this section, we introduce the prompt templates used for rationale generation and inference. We construct input text for the language model based on these templates. All the prompt templates we present are designed for the ReClor dataset (Yu"}, {"title": "F Implementation Details", "content": "We use lora rank=16, alpha=16 and target modules = {gate_proj, down_proj, up_proj, q_proj, k_proj,"}, {"title": "G Computational Costs", "content": "In this section, we present the overall computational costs of our experiments, measured in GPU hours. Using the Llama 3 8B model and the ReClor dataset, the computational costs are as follows:\n\u2022 Rationale Generation: 12 GPU hours.\n\u2022 Rationale Evaluation: 3.2 GPU hours.\n\u2022 Supervised Fine-Tuning: 7.4 GPU hours.\n\u2022 Preference Learning: 19.2 GPU hours.\nIn the rationale evaluation stage, inference for the original questions (q) took approximately 1"}, {"title": "H Adjustments in Implementation of Baseline Models", "content": "Some of the baseline approaches target domains and environments that differ from our setting; therefore we adjust them to fit our task setup while preserving their core ideas. First, although STaR (Zelikman et al., 2022) is an iterative process, we"}]}