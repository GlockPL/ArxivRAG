{"title": "MERALION-TextLLM: Cross-Lingual Understanding of Large Language Models in Chinese, Indonesian, Malay, and Singlish", "authors": ["Xin Huang", "Tarun Kumar Vangani", "Minh Duc Pham", "Xunlong Zou", "Bin Wang", "Zhengyuan Liu", "Ai Ti Aw"], "abstract": "Multilingual large language models (MLLMs) have shown impressive capabilities across a variety of languages. However, efficacy can differ greatly between different language families, especially for those with limited linguistic resources. This report presents MERaLiON-TextLLM, a series of open-source language models specifically tailored to improve understanding and generation in Chinese, Indonesian, Malay, and Singlish. The initial released model is built on Llama-3-8B-Base and refined through a meticulously crafted process of continued pre-training and weight merging. Our approach achieves performance improvements across benchmarks in these languages, exceeding the capabilities of the official Llama-3 models. We provide the model checkpoints as a resource to support further research and development in cross-lingual language understanding.", "sections": [{"title": "1 Introduction", "content": "Our first released model is named MERLION-LLaMA-3-8B-Instruct. MERLION-LLaMA-3-8B-Instruct has been extensively pre-trained on English, Chinese, and Indonesian, building upon LLaMA-3.1-8B-Base Dubey et al. [2024], with a primary emphasis on enhancing its understanding and generation capabilities in Chinese and Indonesian. Leveraging innovative corpus mixing strategies tailored to multilingual regional datasets, we carefully diversified the training materials using domain classification, hyperparameter optimization, and strategic replay techniques. These methods are specifically designed to prevent catastrophic forgetting, enabling the model to retain previously acquired knowledge while significantly improving its ability to produce high-quality, contextually accurate responses in Southeast Asian languages."}, {"title": "2 Pre-Training", "content": "MERLION-LLaMA-3-8B-Instruct training was conducted using the MaxText AI-Hypercomputer [2024] platform, utilizing both NVIDIA H100 GPUs and TPU v4-128 chips. Specifically, we utilized 64 H100 GPUs, achieving approximately 400 TFLOPS per GPU, and TPU v4-128 configurations, attaining around 168 TFLOPS per TPU chip."}, {"title": "3 Instruction Tuning and Model Merging", "content": "We present updated experimental results for our latest variant of the Southeast Asian language model, focusing on three representative languages: English, Chinese, and Indonesian. Our work began with the construction of a multilingual instruction-response corpus containing approximately 3M pairs. Using this dataset, we performed instruction tuning on the Llama-3.1-8B base model to evaluate potential performance gains. However, our initial experiments revealed that this approach alone did not surpass the robust baseline performance of the Llama-3.1-8B-Instruct model. Specifically, the instruction-tuned model underperformed the original Llama-3.1-8B-Instruct across the three target languages in the Cross-MMLU and Cross-LogiQA datasets Wang et al. [2024].\n\nTo address these limitations, we explored model merging techniques aimed at enhancing instruction-following capabilities without requiring extensive additional tuning. By carefully merging the weight sets of Llama-3.1-8B-Base and Llama-3.1-8B-Instruct, we leveraged the strong instruction-following performance of the instructed variant while integrating domain-specific knowledge from our custom dataset. As illustrated in Table 1, the resulting \"MERaLiON-Llama-3.1-8B-Instruct\""}, {"title": "4 Benchmarks and Evaluation", "content": "We conducted comprehensive evaluations using several benchmarks to assess the multilingual and instruction-following performance of MERaLiON. Key benchmarks include:\n\n\u2022 Cross-MMLU Wang et al. [2024] : A subset of the MMLU dataset translated into multiple languages, including English, Chinese, Indonesian, and Malay. It aims to measure the model's ability to handle general knowledge queries across these diverse linguistic contexts.\n\n\u2022 Cross-LogiQA Wang et al. [2024]: Building on the original LogiQA dataset, Cross-LogiQA introduces multilingual versions of logical reasoning tasks. It provides parallel question sets in English, Chinese, and Indonesian that are designed to maintain logical equivalence across these languages.\n\n\u2022 IndoMMLU Koto et al. [2023] : A benchmark designed to assess general knowledge and language understanding of large language models for Indonesian, particularly for domains such as law, medicine, and social science Koto et al. [2023].\n\n\u2022 CN-Eval: A selected subset of C-Eval Huang et al. [2023] and CMMLU Li et al. [2024] curated to specifically assess a model's knowledge about the Chinese language, culture and socio-political context. This subset provides a focused metric for assessing LLM knowledge of China."}, {"title": "4.1 Results", "content": "The results of our evaluation highlight several strengths of MERLION:\n\n\u2022 On Cross-MMLU and Cross-LogiQA, as shown in Table 2 and Table 3, MERaLiON outperforms the baseline Meta-Llama-3.1-8B-Instruct model on Cross-MMLU, demonstrating superior general knowledge coverage in Chinese and Indonesian, while maintaining a\n\nstrong, balanced performance in English. Similarly, MERaLiON improves logical reasoning in Indonesian, indicating that its continued pre-training and model merging strategies effectively enhance both factual knowledge and reasoning capabilities.\n\n\u2022 On IndoMMLU, as reported in Table 4, MERaLiON significantly outperforms the baseline Llama-3.1-8B-Instruct model, achieving 0.576 accuracy versus 0.548, highlighting its improved understanding of Indonesian language and domain-specific nuances.\n\n\u2022 For CN-Eval, MERaLiON achieves 0.514 accuracy compared to 0.457 for Llama-3.1-8B-Instruct, as shown in Table 5. This result demonstrates MERaLiON's efficacy in retaining and enhancing knowledge related to China.\n\nThese benchmarks demonstrate the superior capability of MERaLiON-TextLLM to handle multilingual tasks and deliver improved cross-lingual understanding. Our training approach ensures the model is well-suited for diverse language tasks."}, {"title": "5 Conclusion and Future Work", "content": "The MERLION-LLaMA-3-8B-Instruct model improves multilingual NLP for Indonesian and Chinese, by addressing cross-lingual challenges with effective pretraining and model merging.\n\nEvaluation results demonstrate MERaLiON-TextLLM's strengths, such as better reasoning and question-answering on Cross-MMLU and Cross-LogiQA through balanced multilingual training, strong accuracy on IndoMMLU and CN-Eval from effective dataset preparation, enhanced instruction-following via weight merging with minimal tuning, and resource efficiency through optimized TPU/GPU strategies for scalable training.\n\nFuture directions include:\n\n\u2022 Expanding to more underrepresented languages like Tagalog, Thai, and Vietnamese.\n\n\u2022 Developing enhanced evaluation frameworks with human judgment and multimodal input.\n\n\u2022 Applying the model to tasks such as translation, summarization, and content analytics."}]}