{"title": "PDeepPP:A Deep learning framework with Pretrained Protein language for peptide classification", "authors": ["Jixiu Zhai", "Tianchi Lu", "Haitian Zhong", "Ziyang Xu", "Yuhuan Liu", "Xueying Wang", "Dan Huang"], "abstract": "Protein post-translational modifications (PTMs) and bioactive peptides (BPs) play critical roles in various biological processes and have significant therapeutic potential. However, identifying PTM sites and bioactive peptides through experimental methods is often labor-intensive, costly, and time-consuming. As a result, computational tools, particularly those based on deep learning, have become effective solutions for predicting PTM sites and peptide bioactivity. Despite progress in this field, existing methods still struggle with the complexity of protein sequences and the challenge of requiring high-quality predictions across diverse datasets. To address these issues, we propose a deep learning framework that integrates pretrained protein language models with a neural network combining transformer and CNN for peptide classification. By leveraging the ability of pretrained models to capture complex relationships within protein sequences, combined with the predictive power of parallel networks, our approach improves feature extraction while enhancing prediction accuracy. This framework was applied to multiple tasks involving PTM site and bioactive peptide prediction, utilizing large-scale datasets to enhance the model's robustness. In the comparison across 33 tasks, the model achieved state-of-the-art (SOTA) performance in 25 of them, surpassing existing methods and demonstrating its versatility across different datasets. Our results suggest that this approach provides a scalable and effective solution for large-scale peptide discovery and PTM analysis, paving the way for more efficient peptide classification and functional annotation.", "sections": [{"title": "Introduction", "content": "In recent years, biological feature prediction technology has gradually matured (1; 2; 3), Protein post-translational modifications (PTMs) (4; 5; 6) and bioactive peptides (BPs) (7; 8) play crucial roles in various biological processes, profoundly impacting cellular regulation, protein functionality, and therapeutic advancements. These modifications and peptides have gained tremendous attention from both researchers and consumers, driven by increasing demand for natural therapeutics, concerns over synthetic products, and sustainability considerations. The global BPs market is expected to double from 48.6 billion USD in 2020 to 95.7 billion USD by 2028, with applications spanning drugs, nutraceuticals and cosmeceuticals (9; 10). PTMs, such as phosphorylation (11), glycosylation (12), or acetylation(13), are chemical modifications that occur post-synthesis and influence cellular signaling and protein dysfunction, which are closely linked to a range of diseases (14). Similarly, bioactive peptides (15), composed of short amino acid sequences, exhibit various biological activities such as antimicrobial, anticancer, and antioxidant effects, making them a focal point in pharmaceutical research. These peptides can be produced from sustainable food protein sources through enzymatic hydrolysis or fermentation, offering a promising avenue for developing natural therapeutic agents(16; 17).\nHowever, traditional experimental methods for identifying PTM sites (18; 19; 20) and bioactive peptides (21; 22) are resource-intensive, time-consuming, and costly. In recent years, computational tools, including deep learning models such as MusiteDeep (23; 24; 25; 26), UniDL4BioPep(27) or prediction optimization for a single modification type(28; 29; 30) have made significant progress, showing promise in improving predictive performance (31; 32) The development of deep learning has further strengthened its role in biopharmaceuticals, with increasingly mature technologies that support drug discovery, biomarker identification, and precision medicine(33; 34; 35). Despite these advancements, existing models still face challenges, including poor generalization across diverse datasets, limited support for large-scale tasks, difficulties in adapting to imbalanced datasets, and specificity for certain PTM types or peptide functionalities, which hinder their broader application.\nTo overcome these limitations, we introduce PDeepPP, a novel deep learning framework that integrates pretrained protein language models (e.g., ESM-2(36)) with weighted parallel neural networks based on transformer(37) and convolutional neural network (CNN) (38) architectures, combined with a Transductive Information Maximization (TIM) loss function (39; 40) tailored for imbalanced datasets, providing a highly efficient and scalable solution for peptide classification and PTM prediction. By leveraging ESM-2 to extract rich contextual features from large-scale protein sequence data without the need for extensive feature engineering, PDeepPP captures both local and global sequence complexities. The integration of transformer-based global feature chains with CNN-based local feature chains further improves predictive accuracy while maintaining computational efficiency. Through extensive experiments, PDeepPP has demonstrated superior performance across multiple benchmark datasets, improving accuracy and robustness while reducing false positives and negatives. This framework showcases its potential as a transformative tool in peptide discovery and PTM annotation."}, {"title": "Materails and methods", "content": null}, {"title": "Benchmark datasets", "content": "All benchmark datasets are sourced from existing review papers and datasets used for single-task predictions. We conducted a fair evaluation of metrics on the collected datasets. In total, we collected 37 datasets from two review articles and four single-task papers, including twenty BPs datasets for seventeen different bioactivities and 17 PTMs datasets for different modifications.Specificly contains angiotensinconverting enzyme (ACE) inhibitory activity (anti-hypertension) (41), dipeptidyl peptidase (DPPIV) inhibitory activity (anti-diabetes) (42), bitter (43), umami(44), antimicrobial activity (45), antimalarial activity (46), quorum- sensing (QS) activity (47), anticancer activity (48; 49), antimethicillin-resistant S. aureus (MRSA) strains activity (50), tumor T cell antigens (TTCA) (51), blood-brain barrier (52), antiparasitic activity (53), neuropeptide (54; 55), antibacterial activity (56), antifungal activity (56), antiviralactivity(56), toxicity (57) and antioxidant (58) activity; The following data are all derived from the UniProtKB/Swiss-Prot database(59): Phosphoserine/threonine, Phosphotyrosine, N-linked glycosylation, O-lined glycosylation, N6-acetyllysine, Methyllysine, S-palmitoylation-cysteine, Pyrrolidone-carboxylic-acid, Ubiquitination, SUMOylation, Hydroxylysine, Hydroxyproline, methyl-Glutamines (60), methylation-arginine (61), Ubiquitin_K(62) and histone lysine crotonylation (63). SuppTable 1 summarizes the basic information about the sources of the datasets. More information and the complete data can be found on [github]."}, {"title": "Structure overview", "content": "Recent hybrid architectures (64; 65) demonstrate that combining ESM with task-specific embeddings improves PTM generalization. PDeepPP uses a parallel neural network with CNNs and Tranformers for module combination. The TransLinear layer uses the combination of encoder and fully connected layer, and the PosCNN layer uses the combination of position encoding and CNN. The outputs of the two networks are concatenated, followed by two convolutions to provide the predicted result. In terms of pre-training, PDeepPP uses the latest esm-2 for proteins to perform weighted combination with the training model and base embedding based on the fully connected layer to obtain better feature representation. The entire process used by PDeepPP is referenced in Fig. 1."}, {"title": "Data processing", "content": "For the benchmark data set, the original data set that was not divided was taken out according to the ratio of 20%. New results show that long sequences of proteins can enhance learning of molecular interactions, (66)so the sequence was cut at the same time, and every 33 consecutive amino acids were cut into a sequence.Sequences of insufficient length are padded with X, and partial overlap of sequences is allowed. For PTM sites, the positive site should be placed in the middle of the sequence. If there is a peptide chain of less than 33 amino acids, its ends are padded to the target length before training. The validation set is randomly selected at training time according to 10% of the training set."}, {"title": "Embedding strategy", "content": "The model used in this work is ESM-2 with 650 million parameters, a large-scale pre-trained model designed to capture the complex relationships within protein sequences through a Transformer architecture. ESM-2 is pre-trained using a Masked Language Modeling (MLM) objective on a large protein sequence dataset, enabling it to learn both local and global sequence dependencies. The model generates token- wise embeddings that provide rich contextual representations of proteins, which are particularly effective for tasks such as protein structure prediction and functional annotation.\nIn this study, we combine the pre-trained ESM-2 embeddings with a custom BaseEmbedding model to create a hybrid representation of protein sequences. BaseEmbedding pre- training is an embedding layer with a separate teleprompter that generates additional sequence representations, which are linearly transformed to match the ESM-2 output dimension (1280). This combination aims to leverage the pre-trained knowledge from ESM-2 while allowing the model to learn task-specific embeddings through the BaseEmbedding.\nMoreover, residual connections are introduced between these two representations to facilitate feature refinement and ensure efficient gradient propagation, a design choice shown to improve training stability and overall performance in deep neural networks (67; 68).\nThe combination of these two representations is controlled by a predefined ratio (esmratio), which is set to 0.9 in this case. This means that 90% of the final representation comes from the ESM-2 embeddings, while 10% comes from the BaseEmbedding model. The final combined sequence representation is computed using the following weighted sum:\n\\(R_{combined} = \\alpha R_{ESM-2}+ (1-\\alpha) \\cdot R_{Base}\\)\nwhere:\n\\(R_{combined}\\) represents The combined sequence feature representation;\n\\(\\alpha\\) (set to 0.9) is the 'esm_ratio', which defines the contribution of the ESM-2 embeddings; \\(R_{ESM-2}\\) is the embedding generated"}, {"title": "ESM-2 Embeddings", "content": "Protein sequences are passed through the pre-trained ESM-2 model, which generates token-wise embeddings from its final layer. ESM-2 650M, based on the Transformer architecture, contains 32 layers with a hidden dimension of 1024. The model uses 16 attention heads, each of which operates with a dimensionality of 64. The feedforward network has a dimension of 2048. The model's input layer embeds protein sequences into 1024-dimensional vectors, while the output consists of embeddings for each amino acid position in the sequence. These embeddings capture long-range dependencies within protein sequences, providing crucial information for tasks such as protein structure and function prediction. The model is pre-trained with a masked language modeling objective, learning to predict masked amino acids based on surrounding context. The resulting embeddings effectively encode the structural and functional features of the protein sequences, enabling downstream applications such as protein classification, function annotation, and structure prediction. (69)"}, {"title": "BaseEmbedding Model", "content": "The BaseEmbedding model consists of a learnable embedding layer that maps each amino acid in the sequence to a 128-dimensional high-dimensional vector. This vector is subsequently transformed through a linear layer to produce a 1280-dimensional representation, aligning it with the dimensionality of the ESM-2 embeddings."}, {"title": "Sequence Padding", "content": "Protein sequences vary in length, so they are padded to ensure uniformity across batches. Each sequence is padded to match the maximum sequence length in the dataset, allowing for efficient batch processing."}, {"title": "Weighted Combination", "content": "The embeddings generated by the ESM-2 model and the BaseEmbedding model are combined using the predefined ratio (esm_ratio= 0.9). The combined sequence representation is computed as shown in the formula above, where the ESM-2 embeddings contribute 90% of the final representation and the BaseEmbedding model contributes 10%."}, {"title": "Saving Representations", "content": "The combined representations for both the training and test datasets are saved as npy files for use in downstream tasks. The corresponding labels are also saved, ensuring that the representations can be easily loaded and utilized for further model training or evaluation. This approach, which combines a powerful pre-trained model with a learnable embedding layer, provides a flexible and effective method for representing protein sequences. It allows the model to adapt to specific tasks while benefiting from the pre-trained knowledge encoded in the ESM- 2 model."}, {"title": "Trans Conv1d Network", "content": "The TransConvld layer combines self-attention mechanisms, a Transformer encoder, and fully connected networks to extract global and local features from input data, making it suitable for processing sequential data or high-dimensional features. Its architecture consists of multiple modules, each with specific hyperparameters to ensure the model's expressive power and training stability.\nFirst, the input data is passed through the SelfAttention -Global Features module, which uses multi-head self-attention (MultiheadAttention) to capture global features of the input data. Specifically, the [embed_dim] is set to the input feature dimension, and [num_heads] is set to 8 to parallelize the processing of different feature patterns. The output of the self- attention is added to the original input via a residual connection and then stabilized with layer normalization (LayerNorm). The output is then processed through two fully connected layers (fcl and fc2), where the first layer maps the feature dimension to 256, and the second layer maps it to the final output size. Dropout is applied after the first fully connected layer to prevent overfitting.\nNext, the data is passed through the Transformer encoder for further processing. The encoder consists of 4 Transformer encoder layers, each with a model dimension (d_model) of output_size, and uses 8 attention heads (nhead=8) to handle the input data. The feedforward network has a dimension of 512, and 0.3 dropout is applied at each layer to prevent overfitting. The Transformer encoder further learns the global dependencies in the input data and extracts deeper-level features.\nAfter processing by the Transformer encoder, the output is passed through two fully connected layers: the first layer maps the 128-dimensional input to the specified output size, and the second layer keeps the dimension unchanged. A residual connection adds the output of the second layer to the first layer's output, ensuring the preservation of early feature information. Finally, layer normalization (LayerNorm) is applied to the result to stabilize the output and improve the training efficiency.\nBy combining self-attention mechanisms, the Transformer encoder, and fully connected layers, the TransConvld layer effectively captures the complex dependencies in the input data. The residual connections and layer normalization help maintain data stability and preserve information, thus improving the model's expressive power and training stability."}, {"title": "PosCNN Network", "content": "The PosCNN layer integrates convolutional neural network (CNN) operations with optional positional encoding, designed to extract local features from input data while preserving sequence information, making it suitable for processing sequential data or high-dimensional features. The architecture focuses on capturing local patterns through convolution operations, followed by a fully connected layer for feature mapping.\nThe input data is first passed through a 1D convolutional layer, where the number of input channels (input_size) corresponds to the dimensionality of the input features, and the number of output channels is set to 64. The convolution operation uses a kernel size of 3 and padding of 1 to preserve the sequence length. After the convolution, a ReLU activation function is applied to introduce non-linearity, enhancing the model's expressive capability."}, {"title": "PredictModule", "content": "The forward process of the PredictModule first processes the input data x through the TransConvld and PosCNN layers to obtain two feature representations. The outputs are then adjusted in dimension and concatenated along the feature dimension, forming a unified representation that combines both global and local features. The concatenated features are then permuted to match the input format required by the subsequent convolutional layers. The combined output from TransConvld and PosCNN is passed through a convolutional layer with 32 output channels, followed by adaptive max pooling to reduce the sequence length to 1, with dropout applied for regularization. Next, the processed features are passed through another convolutional layer with 64 output channels, followed by max pooling and dropout. Finally, the features are flattened and input into a fully connected layer to generate the final prediction output. This sequence of operations helps refine the features, prevent overfitting, and produce robust prediction results."}, {"title": "Loss function", "content": "In our training process, we employed the Transductive Information Maximization (TIM) loss function(39). The TIM loss integrates the traditional Cross-Entropy Loss (CE) with an empirically weighted Mutual Information term. The goal of the TIM loss is to minimize the difference between the predicted and true label distributions while maximizing the mutual information between the input data and the labels. Research has shown that mutual information can effectively capture complex, non-linear dependencies in high-dimensional data, thereby enhancing feature representations and improving overall model robustness (70; 71).\nThe mutual information component is divided into two main terms: the marginal entropy of the labels and the conditional entropy of the labels given the input data. The empirical mutual information can be expressed as:\n\\(I(X_Q; Y_Q) = H_{\\beta}(Y_Q) \u2013 H_{\\beta}(Y_Q | X_Q)\\)\nHere, \\(H_{\\beta}(Y_Q)\\) represents the marginal entropy of the labels \\(Y_Q\\), which is computed based on the predicted class probabilities:\n\\(H_{\\beta}(Y_Q) = - \\sum_{k=1}^{K} p_k \\log p_k\\)\nMeanwhile, \\(H_{\\beta}(Y_Q | X_Q)\\) denotes the conditional entropy of the labels \\(Y_Q\\) given the input data \\(X_Q\\):\n\\(H_{\\beta}(Y_Q | X_Q) = - \\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\sum_{k=1}^{K} y_{k,i} \\log p_i\\)\nThe overall loss function is formulated as:\n\\(L_{TIM} = \\lambda \\cdot CE - I_\\beta(X_Q; Y_Q)\\)\nwhere X is a non-negative hyperparameter that balances the influence of the cross-entropy loss and the mutual information term. In our experiments, both \\(\\alpha\\) and \\(\\lambda\\) are set to 1.\nThe cross-entropy loss is defined as follows:\n\\(CE = - \\frac{1}{\\Omega} \\sum_{i \\in Q} \\sum_{k=1}^{K} y_{k, i} \\log(p_k)\\)\nwhere \\(y_{k,i}\\) is 1 if the i-th sample belongs to class k, and 0 otherwise. By incorporating mutual information, the TIM loss helps the model capture more informative features from the input data, thereby improving both generalization and robustness."}, {"title": "Model evaluation", "content": "To evaluate the model's performance, we adopted common metrics, including Accuracy (ACC), Balanced Accuracy (BACC), Sensitivity (Sn), Specificity (Sp), Matthews Correlation Coefficient (MCC), Area Under the Receiver Operating Characteristic Curve (ROC AUC), and Area Under the Precision-Recall Curve (PR AUC). These metrics are calculated based on the number of True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). The formulas are as follows:\n\\(ACC = \\frac{TP+TN}{TP+TN+FP+ FN}\\)\n\\(Sn = \\frac{TP}{TP+FN}\\)\n\\(Sp = \\frac{TN}{TN + FP}\\)\n\\(BACC = 0.5 \\times Sn + 0.5 \\times Sp\\)\n\\(MCC = \\frac{(TP \\times TN) - (FN \\times FP)}{\\sqrt{(TP + FN) \\times (TN + FP) \\times (TP + FP) \\times (TN + FN)}}\\)\nThe Area Under the ROC Curve (ROC AUC) represents the performance of a binary classification model by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The ROC curve shows the trade-off between sensitivity and specificity. The AUC score is the area under this curve, and it ranges from 0 to 1, where 1 indicates perfect classification and 0.5 represents random guessing. The ROC AUC is calculated using the 'roc_auc_score' function from scikit-learn, which integrates the area under the ROC curve using the trapezoidal rule:\n\\(AUC = \\int TPR(FPR) d(FPR)\\)\nThe Area Under the Precision-Recall Curve (PR AUC) measures the trade-off between precision and recall for different"}, {"title": "Results", "content": null}, {"title": "Comparison Experiments", "content": "After normalizing the datasets, we trained each task using esm_ratio values of 0.9, 0.95, and 1, along with lambda_ values ranging from 0.9 to 1 (with a step size of 0.1). The model with the best performance, determined based on ACC, AUC, and PR metrics, was selected as the optimal configuration for this experiment.\nDuring the model training process, we performed data preprocessing and normalization to ensure that all input features were trained on the same scale, avoiding bias due to differences in feature scales. To maximize model performance, we meticulously tuned several hyperparameters and selected the optimal combination to ensure the reliability and reproducibility of the experimental results.\nWe accessed the UniDL4BioPep model's server to obtain results on the same benchmark dataset. The experiments showed that, compared with the UniDL4BioPep model, our model's ACC, AUC, and PR were on average higher by 0.3%, 0.7%, and 0.8%. Furthermore, the FN and FP were reduced by 76 and 14 samples. On the ten datasets where the model performed best, our model's ACC increased by 2%, AUC increased by 1.67%, and PR increased by 1.87%, while the FN and FP were reduced by 71 and 67 samples, respectively. The comparison of various metrics on different datasets among PDeepPP, UniDL4BioPep, and the tools compared in UniDL4BioPep is shown in SuppTable 2.\nSpecifically, compared with the UniDL4BioPep model, our model shows significant advantages in classification accuracy (ACC), indicating more stable and efficient performance in the overall classification task. Additionally, the improvement in AUC suggests that our model has enhanced ability to discriminate between positive and negative samples, better capturing complex patterns in the data. The improvement in the PR curve demonstrates that our model is more robust in handling imbalanced data, particularly achieving better predictive performance for minority classes (such as positive samples). The ROC and PR curves and confusion matrices for all tasks are shown in Figure 2 and SuppFig 1.\nIn terms of false negatives (FN) and false positives (FP), our model also demonstrates superior performance. The reduction in FN and FP indicates that our model makes more cautious classification decisions, particularly in the test set's positive and negative samples, effectively reducing the number of misclassifications. Specifically, the reduction in FN means that the model is more accurate in identifying positive samples,"}, {"title": "Feature Extraction Analysis", "content": null}, {"title": "1. Introduction to ESM Feature Extraction", "content": "ESM (Evolutionary Scale Modeling) is a large-scale protein language model based on the Transformer architecture and trained with masked language modeling on massive protein sequences from the UniRef database. By leveraging relative positional embeddings and multi-head self-attention, ESM captures evolutionary information and models amino acid interactions to generate high-dimensional representations that encode secondary and tertiary structural features. With up to 15 billion parameters, ESM-2 achieves significant improvements in perplexity and structure prediction accuracy, demonstrating strong correlations between language model understanding and atomic-level structure prediction. Its embeddings serve as inputs for ESMFold, enabling rapid and accurate end-to-end protein structure prediction without relying on MSAs, offering a computationally efficient alternative to traditional methods. In this study, due to ESM's specificity for proteins, ESM is utilized as the primary feature extraction tool, and the generated protein sequence embeddings are further applied to downstream analysis tasks. The core idea of feature extraction relies on the deep understanding of sequences by"}, {"title": "2. UMAP Visualization of Model Features", "content": "In this study, UMAP (Uniform Manifold Approximation and Projection) is utilized to visualize the high-dimensional embeddings extracted before the prediction module(72; 73). (UMAP is a nonlinear dimensionality reduction technique that projects high-dimensional data into a two-dimensional space, enabling intuitive observation of data distribution and clustering patterns.) These embeddings are derived from the combined outputs of the TransLinear layer and the PosCNN layer, averaged along the sequence dimension. From the UMAP"}, {"title": "Ablation Study", "content": "In this subsection, we conduct an ablation study to evaluate the contribution of key modules in our model(74). By systematically removing specific components, we analyze the impact on performance using ROC/PR curves, confusion matrices, and UMAP visualizations.\nDataset Selection\nFor dataset selection, we first chose four datasets from the UniDL4BioPep and MusiteDeep collections based on dataset size, the ratio of positive to negative sites, and prediction performance. These included Anticancer_main and Antiviral from UniDL4BioPep, and N6-acetyllysine_K and Ubiquitin_K from MusiteDeep, which served as the first batch of datasets for the ablation study. To explore the model's performance on tasks with additional classification targets, similar post-translational modification (PTM) types with emphasis on different amino acids, and tasks where both PTM type and target amino acid are the same, we selected several recent state-of-the-art (SOTA) datasets from recent literature. These datasets include methylation-G(60), methylation-R (distinguished from the corresponding task in MusiteDeep by the lowercase letter) (61), Ubiquitin_K* (the asterisk is used to differentiate it from the corresponding task in MusiteDeep) (62), and Crotonylation_K(63). The goal of this selection is to verify the model's robustness and completeness by testing it on diverse datasets while ensuring that each module contributes positively to the overall model performance. The specific sample sizes and the distribution of negative and positive samples across datasets (especially for imbalanced datasets) are shown in SuppTable 1 and Figure 5, respectively.\nExperimental Setup\nWe design several ablation experiments:\n\u2022 Full model (baseline).\n\u2022 Removing pre-trained embeddings (sequence or structure).\n\u2022 Replacing the TIM loss function with a standard cross- entropy loss.\n\u2022 Removing the PosCNN(&PosEncoding) or TransLinear(&First attention layer) module.\nWe begin by evaluating the four key metrics of the confusion matrix, namely True Negative (TN), True Positive (TP), False Negative (FN), and False Positive (FP), in order to assess the model's performance. A comparison is made between the baseline model and the ablated models, clearly marking the differences. Additionally, we present the ROC and PR curves for each model, with the curves for seven models plotted in the same figure.\nFor the remaining evaluation metrics-Accuracy (ACC), Balanced Accuracy (BACC), Sensitivity (SEN), Specificity (SPEC), and Matthews Correlation Coefficient (MCC)-we highlight the top two performing models for each metric. The best model is marked in red, and the second-best model in blue. The results of these three evaluation methods are shown in Figure 5.\nIn the next step, we visualize the UMAP projections of the seven models, categorizing them into three distinct groups, as follows:\nCategories = [PDeepPP, w/o embedding, w/o loss],[PDeepPP, w/o attention, w/o Translinear],[PDeepPP, w/o PosEncoding, w/o PosCNN]These categories are organized based on related modules and their comparison to the baseline model. Finally, we compute the average performance metrics across the eight datasets for both the baseline and ablated models. These results are visualized in the laser bar chart presented in Figure 6 and SuppFig 3."}, {"title": "Results and Analysis", "content": "1. ROC/PR Curves\nThe full model demonstrates strong performance across all tasks, with median AUC of 0.991 (80% of tasks achieving AUC between 0.982-0.998). Key observations:\n\u2022 Removing sequence embeddings causes 9.7% AUC drop (from 0.985 to 0.890) in PTM predictions\n\u2022 Structure embedding removal shows smaller impact (3.2% average AUC decrease)\n\u2022 Dual component removal leads to maximum 12.7%"}, {"title": "Conclusion", "content": "The PDeepPP framework represents a significant leap forward in the field of peptide classification and PTM prediction, showcasing the transformative power of deep learning in addressing complex bioinformatics challenges. Our experimental results demonstrate that PDeepPP consistently outperforms state-of-the-art models across a variety of tasks, underscoring its robustness and adaptability. For instance, in antimicrobial peptide classification, PDeepPP achieved an impressive accuracy (ACC) of 0.9726 and a PR AUC of 0.9977, significantly surpassing the performance of UniDL4BioPep. Similarly, in phosphorylation site prediction, a critical PTM task, PDeepPP achieved an ACC of 0.9984 and a PR AUC of 0.9910, highlighting its exceptional predictive capabilities and ability to generalize across diverse datasets. Furthermore, PDeepPP substantially reduced false positives (FP) and false negatives (FN) across all tested datasets, reinforcing its practical utility in real-world applications.\nPDeepPP's success can be attributed to its innovative integration of pretrained protein language models (ESM-2) with a parallel neural network architecture combining Transformers and CNNs. This design allows the model to capture both local and global sequence features, enhancing its ability to handle the complexity of protein sequences. The inclusion of the Transductive Information Maximization (TIM) loss function further improves performance on imbalanced datasets, a common challenge in bioinformatics. This combination of advanced deep learning techniques enables PDeepPP to achieve superior accuracy, robustness, and scalability compared to traditional methods.\nCompared to conventional models, PDeepPP significantly reduces the need for extensive feature engineering and manual annotation, making it adaptable to a wide range of PTM types and peptide functionalities. Its ability to handle large- scale datasets and diverse biological tasks positions it as a valuable tool for large-scale bioinformatics research. Moreover, PDeepPP's computational efficiency and reduced reliance on experimental data make it a cost-effective solution for peptide discovery and PTM analysis.\nLooking ahead, PDeepPP holds immense potential for broader applications in both research and industry. Future directions include integrating structural and spatial data to further enhance predictive accuracy, as well as combining the framework with high-throughput experimental workflows to accelerate large-scale protein function discovery and drug development. Additionally, as novel PTM types, such as AMPylation, are discovered, PDeepPP can provide computational support for exploring these uncharted areas, advancing its potential applications in precision medicine and therapeutic innovation. The framework could also be extended to address multi-label classification tasks, where peptides may exhibit multiple bioactivities or PTMs simultaneously, further broadening its utility.\nIn summary, PDeepPP combines the strengths of pretrained protein language models, advanced neural network architectures, and innovative loss functions to offer a cutting- edge solution for protein sequence analysis. It represents a major advancement in the field of bioinformatics and has the potential to play a pivotal role in accelerating peptide discovery, PTM functional analysis, and the development of novel therapeutic strategies. By reducing the reliance on costly and time-consuming experimental methods, PDeepPP not only enhances the efficiency of bioinformatics research but also opens new avenues for understanding the complex mechanisms underlying protein modifications and peptide bioactivity. As the field continues to evolve, PDeepPP is poised to drive innovation in life sciences, contributing to the development of targeted therapies and personalized medicine."}]}