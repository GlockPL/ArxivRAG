{"title": "HiMoE: Heterogeneity-Informed Mixture-of-Experts for Fair Spatial-Temporal Forecasting", "authors": ["Shaohan Yu", "Pan Deng", "Junting Liu", "Zi'ang Wang", "Yu Zhao"], "abstract": "Spatial-temporal forecasting has various applications in transportation, climate, and human activity domains. Current spatial-temporal forecasting models primarily adopt a macro perspective, focusing on achieving strong overall prediction performance for the entire system. However, most of these models overlook the importance of enhancing the uniformity of prediction performance across different nodes, leading to poor prediction capabilities for certain nodes and rendering some results impractical. This task is particularly challenging due to the inherent heterogeneity of spatial-temporal data. To address this issue, in this paper, we propose a novel Heterogeneity-informed Mixture-of-Experts (HiMoE) for fair spatial-temporal forecasting. Specifically, we design a Heterogeneity-Informed Graph Convolutional Network (HiGCN), integrated into each expert model to enhance the flexibility of the experts. To adapt to the heterogeneity of spatial-temporal data, we design a Node-wise Mixture-of-Experts (NMOE). This model decouples the spatial-temporal prediction task into sub-tasks at the spatial scale, which are then assigned to different experts. To allocate these sub-tasks, we use a mean-based graph decoupling method to distinguish the graph structure for each expert. The results are then aggregated using an output gating mechanism based on a dense Mixture-of-Experts (dMoE). Additionally, fairness-aware loss and evaluation functions are proposed to train the model with uniformity and accuracy as objectives. Experiments conducted on four datasets, encompassing diverse data types and spatial scopes, validate HiMoE's ability to scale across various real-world scenarios. Furthermore, HiMoE consistently outperforms baseline models, achieving superior performance in both accuracy and uniformity.", "sections": [{"title": "INTRODUCTION", "content": "Spatial-temporal forecasting has become a key component in urban computing systems, with extensive applications across various real-world scenarios [20]. For instance, traffic prediction systems play a vital role in assisting decision-makers with devising more effective and timely strategies [4, 6, 9, 14, 23, 40, 47]. Besides, a climate forecasting system can alert residents to prepare for extreme weather events and mitigate potential risks [10-12, 21, 29, 36]. However, spatial-temporal data prediction problems are complex. Beyond prioritizing overall performance, ensuring that performance variation across different nodes remains within acceptable thresholds is essential [15]. In traffic prediction systems, significant errors at specific nodes can lead to missed detections of traffic congestion, thereby compromising the accuracy and reliability of managerial decision-making. Therefore, a core challenge in overcoming certain nodes' suboptimal predictive performance is achieving equitable predictive performance across nodes while concurrently optimizing the model's overall accuracy.\nIn recent years, Graph Neural Networks (GNNs) have been shown to effectively learn spatial dependencies [38, 48], enabling accurate spatial-temporal predictions. Many recently proposed methods focus on designing complex GNN-based models to enhance the accuracy of spatial-temporal prediction while overlooking the disparities in predictive performance among different nodes [3, 5, 7, 18, 27, 31]. Recent studies have identified fairness concerns among nodes in spatial-temporal prediction tasks, such as FairFor [15] and STMOE [24]. FairFor generates group-relevant and group-irrelevant representations through adversarial learning, facilitating knowledge sharing between advantaged and disadvantaged variables, thereby ensuring fairness in forecasting. STMoE enhances accuracy and consistency by decoupling heterogeneity among road segments and utilizing specialized expert models tailored to distinct data characteristics. Although these methods have shown promising performance regarding fairness and accuracy in predictions, they primarily focus on designing novel model architectures. In contrast, formulating training strategies that promote fair predictions remains a critical yet underexplored aspect.\nIn spatial-temporal data prediction, spatial heterogeneity is a key factor contributing to inconsistencies in node-level predictive performance. This suggests that training nodes from a purely average perspective is insufficient. Instead, each heterogeneous node should be treated with differentiated attention based on its unique spatial-temporal characteristics. Taking Figure 1 as an example, significant heterogeneity is observed when addressing the spatial-temporal problem of predicting real-time population within an urban grid. Residential areas (blue grids) reflect a lower average real-time population, characterized by decreased numbers during the day and increased numbers at night. In contrast, commercial areas (red grids) show a higher average real-time population, exhibiting elevated counts during the day and reduced counts at night. When simultaneously learning the real-time population across these six areas, similar error values can correspond to differing error proportions due to variations in the average real-time population. Specifically,\ncommercial areas exhibit lower error proportions, while residential areas display higher error proportions, reflecting inconsistencies in predictive performance. Therefore, if the training objective is to minimize the average error, the model may develop stronger predictive capabilities in regions with higher average values, while exhibiting weaker performance in areas with lower values.\nThis paper aims to enhance fairness and accuracy in spatial-temporal data prediction by highlighting the synergy between model architecture and training strategy. Specifically, two key challenges must be addressed: (1) How to design fairness-aware loss and evaluation functions: This involves introducing a novel node-level error ratio metric to replace the traditional fully averaged perspective. Based on this metric, a loss function should be developed to balance fairness and predictive accuracy. (2) How to design a fairness-oriented model architecture: To address the differences among nodes as illustrated in Figure 1, the model must capture static spatial relationships (e.g., distance) and functional relevance while adapting to the heterogeneity of spatial-temporal data, ensuring fair prediction performance across diverse regions.\nWe introduce the Heterogeneity-Informed Mixture-of-Experts (HiMoE), a fairness-oriented spatial-temporal prediction model grounded in a MoE structure. HiMoE incorporates a Heterogeneity-Informed Graph Convolutional Network (HiGCN) that employs a gate-adaptive spatial-temporal graph fusion method, enabling the GCN to learn and integrate graph structure information dynamically. Furthermore, we design a Node-wise Mixture-of-Experts (NMoE), which adopts a spatial-temporal prediction task decoupling method. This strategy leverages the specialized strengths of individual experts, thereby improving the model's overall predictive capability. To assess node-level error from a fairness perspective, we propose a novel ratio error metric, integrating it into both the loss function and evaluation metric using a weighted approach.\nTo the best of our knowledge, HiMoE is the first method to identify node-level bias in spatial-temporal predictions arising from existing training strategies and propose corresponding correction methods. Our contributions are as follows:\n\u2022 We propose HiGCN as an integral component of experts and NMoE to effectively manage experts, collaboratively enhancing HiMoE's adaptability to spatial heterogeneity.\n\u2022 We propose a method to evaluate node-level prediction ratio errors by incorporating the heterogeneous data distribution as the model's inductive bias. This method constitutes the foundation of our loss and evaluation functions, guiding the model toward more impartial predictive results.\n\u2022 Extensive experiments conducted on four urban computing datasets show that HiMoE consistently achieves state-of-the-art performance, significantly enhancing predictive accuracy and fairness across diverse scenarios, with improvements ranging from 9.22% to 55.71% across all metrics."}, {"title": "PRELIMINARIES", "content": "In this section, we establish key notations and formally define spatial-temporal fair forecasting.\nAt each time t, spatial-temporal data is represented as \\(X_t \\in \\mathbb{R}^N\\), where N is the total number of nodes. We denote spatial-temporal data from time step m to time step n across all nodes as \\(X_{m:n} \\in \\mathbb{R}^{N \\times (n-m+1)}\\). The spatial-temporal graph at time t is expressed as \\(G = (V, E, A)\\), where V is the set of N nodes, E is the set of edges connecting nodes within V, and A is the adjacency matrix at time t. In this study, we utilize two types of adjacency matrices: the static adjacency matrix \\(A_s\\), which captures spatial information, and the dynamic adjacency matrix \\(A_d\\), which incorporates temporal variations. The problem is defined as follows:\nSpatial-Temporal Data Forecasting. Given a graph G and spatial-temporal data from the previous T time steps, the goal is to learn a mapping function f to forecast the spatial-temporal data for the next T' time steps. This mapping can be formally defined as:\n\\(X_{t+1:t+T'} = f(X_{t-T+1:t}, G)\\).\nSpatial-Temporal Fairness Criterion. Given the entire historical spatial-temporal dataset \\(X \\in \\mathbb{R}^{N\\times T}\\), the mean characteristic of each node i is represented by \\(c_i = \\text{mean}(X^i)\\). The model's predictions for future time steps are denoted by \\(X_{t+1:t+T'}\\), allowing us to calculate the Mean Absolute Error (MAE) for each node i as \\(e_i = \\frac{1}{T'}\\sum_{t=1}^{T'} |X_t^i - \\hat{X}_t^i|\\). To isolate spatial heterogeneity and mitigate the impact of temporal variation, we define the node-level ratio error as \\(r_i = e^i/c_i\\). To assess the uniformity of node performance, we calculate the standard deviation of node-level ratio errors. The Standard Absolute Ratio Error (SARE) is a metric specifically designed to measure the consistency of predictions across nodes. A lower SARE value indicates greater uniformity in model performance, signifying balanced prediction accuracy across nodes:\n\\(\\text{SARE} = \\text{std} (r \\cdot \\text{mean}(X))\\).\nHere, \\(r \\in \\mathbb{R}^N\\) represents the vector of node-level ratio errors across all nodes, and multiplying by \\(\\text{mean}(X)\\) restores the data's original scale, allowing SARE to capture the variation and fairness in the model's predictive accuracy across diverse spatial regions."}, {"title": "METHODOLOGY", "content": "The overall framework of HiMoE is illustrated in Figure 2. The prediction process of HiMoE comprises four main steps: input data decoupling, expert model forecasting, prediction data aggregation, and fairness-aware metric evaluation. In the following sections, we provide a detailed description of the HiGCN architecture employed within the Expert Model, outline the data flow process through HiMoE, and formally define the fairness-aware loss and evaluation functions."}, {"title": "HiGCN: Heterogeneity-Informed Graph Convolutional Network", "content": "We designed HiGCN, an integral component of HiMoE, to enhance the predictive model's adaptability to spatial heterogeneity by employing a gating mechanism. This approach leverages a predefined static adjacency matrix and a time-varying dynamic adjacency matrix learned from the input data.\nTo enhance the adaptability of the GCN to spatial heterogeneity, we designed a dynamic graph learning module that is an integral part of HiGCN. The computation of the time-varying dynamic adjacency matrix begins with learning embeddings \\(E_{t:t+T}\\) from the input data. Subsequently, the dynamic adjacency matrix is computed by:\n\\(A_d = \\text{SoftMax}(E_{t:t+T}^{\\top}E_{t:t+T} - \\text{diag}(E_{t:t+T}^{\\top}E_{t:t+T}))\\).\nThe removal of the diagonal elements before applying the softmax function is intended to emphasize the similarity between different nodes.\nIntegrating dynamic and static graphs is essential for revealing latent spatial-temporal relationships [23]. Furthermore, the GCN model requires sufficient self-learning capacity to fully leverage the information provided by the adjacency matrices. In HiGCN, we employ an edge-level gating mechanism to assist the GCN in learning spatial dependencies, as defined by\n\\(A_f = \\text{MLP}(A_d||A_s) \\odot \\text{tanh}(W_a)\\),\nwhere \\((A_d||A_s) \\in \\mathbb{R}^{N\\times N\\times 2}\\) represents the concatenation operation, MLP denotes the multilayer perceptron, \\(\\odot\\) signifies the Hadamard product, and \\(W_a \\in \\mathbb{R}^{N\\times N}\\) is a learnable parameter. The tanh function normalizes \\(W_a\\) within the range (-1, 1), allowing the GCN greater control over extracting graph information. This normalization also enables the model to capture both positive and negative interactions between nodes, a capability that is not achievable when using only \\(A_s\\) and \\(A_d\\).\nAs an essential component of expert models, the Graph Convolutional Network must adhere to a concise architecture. Excessive complexity in expert models not only leads to substantial computational demands due to the number of experts but also heightens the risk of overfitting within each expert model on its designated data. Additionally, complex expert models can significantly reduce training speed as the number of experts increases. To enhance efficiency, we employ a GCN that formulates its layer-by-layer propagation based on a first-order approximation of localized spectral filters on the graph [22]:\n\\(h^{l+1} = \\sigma(A_f h^l W_1^l + I h^l W_2^l)\\),\nwhere l denotes the layer number of the GNN, and \\(A_f = A + (A^T)^f\\) represents the adjacency matrix learned at the l-th GCN layer. Summing \\(A^f\\) and its transpose ensures matrix symmetry. Here, \\(h^l \\in \\mathbb{R}^{N\\times C^l}\\) serves as the input to the l-th layer, and \\(h^{l+1} \\in \\mathbb{R}^{N\\times C^{l+1}}\\) is the output of the l-th layer, where C denotes the number of channels. Additionally, \\(W_1^l\\) and \\(W_2^l\\) are learnable parameters, \\(\\sigma\\) is the activation function, and I represents the identity matrix."}, {"title": "NMoE: Node-wise Mixture-of-Experts", "content": "Through the graph convolution in Equation 5, we aggregate embeddings of input sequences from neighboring nodes. However, incorporating all nodes into a single adjacency matrix can introduce interference between heterogeneous nodes. This heterogeneity, particularly mean heterogeneity, can in turn negatively impact prediction performance.\nTo address this issue, we adopt a dense Mixture of Experts (dMoE) based architecture to handle the prediction tasks for different nodes, thereby reducing the impact of heterogeneity on the prediction results. Previous studies in spatial-temporal forecasting have directly assigned the input data to the model, which then uses a single model to handle the prediction task for all nodes [2, 9, 14]. Some spatial-temporal models based on MoE allocate the entire spatial-temporal data to experts, selecting appropriate experts based on the characteristics of the data to produce outputs [23, 24]. In contrast, our model decouples the spatial-temporal data into node-level time series, with each node's prediction task being handled by all experts, where the output is dominated by the experts better specialized in handling that node. This approach helps to reduce the noise caused by the interdependencies between spatial-temporal sequences of heterogeneous nodes, leading to more accurate predictions."}, {"title": "Decoupling Spatial-Temporal Graph", "content": "In the NMoE architecture, different experts are responsible for processing different types of nodes. Each expert receives the same spatial-temporal data but operates on distinct spatial-temporal graph structures. This design enables experts to focus on specific node categories, enhancing the prediction accuracy for each category while preserving global information. To ensure that nodes with mean homogeneity are assigned to the same expert, we introduce an expert-level graph filter to select edges corresponding to nodes of the respective type handled by each expert.\nTo cluster nodes based on mean homogeneity, we first compute the node-level mean of the input data \\(X_{t-T:t}\\) as \\(C_{\\mu} \\in \\mathbb{R}^{N}\\), followed by z-score normalization of the mean features:\n\\(Z_{\\mu} = \\frac{(C_{\\mu} - \\text{mean}(C_{\\mu}))}{\\text{std}(C_{\\mu})}\\).\nNext, we use \\(w_{i,i+1}\\) as thresholds to decouple nodes with mean heterogeneity. The value of \\(w_{i,i+1}\\) is computed from learnable parameters as follows: For the first threshold, we set \\(w_{1,2} = \\theta_1\\), and for \\(2 \\leq i \\leq k - 1\\), we recursively perform the calculation:\n\\(w_{i,i+1} = w_{i-1,i} + \\text{ReLU}(\\theta_i)\\),\nwhere ReLU ensures non-negativity, thus guaranteeing that \\(w_{i+1}\\) forms a monotonically increasing sequence. Using these thresholds, we classify the j-th node as belonging to the i-th expert if its value \\(Z_{\\mu}^j\\) satisfies the condition \\(w_i \\leq Z_{\\mu}^j \\leq w_{i+1}\\). It is important to note that the thresholds \\(w_i\\) are soft boundaries, allowing for greater flexibility in assigning nodes to experts.\nTo generate adjacency matrices suitable for the different expert models and to facilitate accurate prediction of specific mean nodes, we start by referencing the definitions in the expert model's HiGCN. In Equation 5, the graph convolution operation aggregates embeddings from its neighbors, influencing the feature vector \\(h^{l+1}\\) of each node. However, the large discrepancies in mean features across nodes may lead to disproportionate influences when aggregating information, especially when low-mean nodes aggregate from high-mean nodes, which can degrade prediction accuracy. Therefore, high-mean nodes can aggregate information from low-mean nodes, but low-mean nodes should not aggregate from high-mean nodes. Based on this assumption, we define the adjacency matrix mask for the i-th expert as:\n\\(A_{\\text{mask}}^i = \\text{sigmoid}(\\gamma \\cdot (w_{i,i+1}-Z_{\\mu}))^{\\top} \\cdot \\text{sigmoid}(\\gamma \\cdot (w_{i,i+1}-Z_{\\mu}))\\).\nIn the above equation, we first compute the probability \\(\\text{sigmoid}(\\gamma \\cdot (w_{i,i+1}-Z_{\\mu}))\\), which represents the likelihood that each node should be assigned to the i-th expert. The hyperparameter \\(\\gamma\\) is a scaling factor that controls the strength of the soft margin classification. We then use this probability to construct the adjacency matrix mask \\(A_{\\text{mask}}^i\\). This mask filters out irrelevant information by multiplying it with the adjacency matrix when the expert model processes the spatial-temporal graph. The value of i ranges from 1 to k \u2212 1, where the k-th expert does not require a mask."}, {"title": "Output Gating", "content": "We adopt a dense MoE strategy, activating all experts simultaneously for each node's prediction. If we were to apply the sparse MoE approach commonly used in LLMs to our model, each expert would only handle a small subset of nodes [32]. However, since experts perform poorly on the nodes they are not responsible for, and GCN models the interdependencies among all nodes, this can lead to a reduced prediction ability for the nodes that the expert is responsible for. To effectively incorporate gating mechanisms in dense MoE, we concatenate the outputs of all experts as follows:\n\\(\\hat{X}_{t:t+T} = \\text{concat}( \\hat{X}_{t:t+T}^1 || \\hat{X}_{t:t+T}^2 || ... || \\hat{X}_{t:t+T}^k)\\).\nwhere \\(\\hat{X}_{t:t+T} \\in \\mathbb{R}^{k \\times N \\times T'}\\). Using a learnable gating matrix, we select the output of the expert model corresponding to each node:\n\\(X_{t:t+T} = \\text{sum}( \\hat{X}_{t:t+T} \\cdot \\text{sigmoid}(W_o)),\\)\nwhere \\(W_o \\in \\mathbb{R}^{k \\times N}\\) is a learnable output gating matrix, and the sigmoid operation is applied to each expert for every node, enabling HiMoE to dynamically adjust the weights assigned to the outputs of different expert model."}, {"title": "Fairness-Aware Loss Functions and Metrics", "content": "Due to the node-level biases present in existing loss functions and evaluation metrics, we need to design an unbiased loss function and evaluation metric. In this design, prediction accuracy and prediction uniformity must be treated with equal importance. The loss function should account for accuracy and uniformity simultaneously. As for the evaluation metrics, there should be accuracy-based, uniformity-based, and combined accuracy-uniformity measures.\nTo assess the magnitude of errors during the prediction process while capturing differences in node-level errors, we propose a weighted approach to the loss function. When calculating Equation 2, we adopt a node-level proportional error \\(c_i\\). Similarly, to maintain the integrity of the data characteristics, we define a weighted absolute loss based on the error ratio. We use absolute loss as the evaluation metric. The weighting method is as follows:\n\\(\\text{WAE} = \\sum_{i=1}^{N}\\sum_{t=1}^{T'} \\frac{|\\text{mean}(X)_t^i - \\hat{X}_t^i|}{c_i}\\).\nIt is crucial to address potential data leakage when calculating the feature \\(c_i\\) and \\(\\text{mean}(X)\\). Specifically, for the loss function, only the training and validation sets should be utilized in the computation. In contrast, the evaluation metric should be computed using the complete dataset to ensure an accurate assessment. As we have previously defined an evaluation strategy for the consistency of prediction results, we utilize both SARE and WAE as the loss function:\n\\(L(X,Y) = \\alpha L_{\\text{WAE}}(X, \\hat{X}) + \\beta L_{\\text{SARE}}(X, \\hat{X}),\\)\nwhere \\(\\alpha\\) and \\(\\beta\\) are hyperparameters. Additionally, WAE and SARE are also incorporated as new components of our evaluation metrics."}, {"title": "EXPERIMENT", "content": "We conducted experiments on four datasets: PeMS04, KnowAir, Beijing, and Tongzhou. The PeMS04 dataset is derived from the Caltrans Performance Measurement System (PeMS), a widely used public dataset [9]. This study utilizes only the traffic flow data from PeMS04. KnowAir encompasses a broad range of areas for PM2.5 forecasting, spanning from 103\u00b0E to 122\u00b0E in longitude and 28\u00b0N to 42\u00b0N in latitude, covering several heavily polluted regions across China [36]. The Tongzhou and Beijing datasets were collected by China Mobile, one of the largest telecommunications operators in China. The Tongzhou dataset includes speed information for specific road segments in Beijing's Tongzhou district, while the Beijing dataset provides real-time population data for designated urban grids. In summary, our datasets encompass four different types of data: speed, traffic flow, population, and PM2.5. These datasets are collected across four distinct spatial scopes: point-based, road segment-based, grid-based, and province or city-wide, highlighting the diversity and breadth of the data. Notably, the KnowAir dataset is aggregated at 1-hour intervals, providing 24 time points per day, whereas the remaining datasets are aggregated at 5-minute intervals, resulting in a total of 288 time points daily. We apply Z-score normalization to normalize the data. After that, We split the datasets into training, validation, and test data using the ratio of 6:2:2."}, {"title": "Baselines", "content": "We compare HiMoE with the following baselines: (1) Graph WaveNet [40] employs an adaptive adjacency matrix to capture latent spatial dependencies, combining graph convolution with dilated causal convolution to model spatial and temporal dependencies simultaneously. (2) ASTGCN [9] develops a spatial-temporal attention mechanism to learn dynamic correlations, using a spatial-temporal convolution module to capture these dependencies effectively. (3) MTGNN [39] adopts a graph-based approach with GNN to model multivariate time series, capable of handling scenarios both with and without a pre-defined graph structure. (4) D2STGNN [31] integrates a dynamic graph learning module for spatial dependency modeling and utilizes both diffusion and inherent models to address various types of hidden time series. (5) TESTAM [23] introduces a Mixture-of-Experts model with diverse graph architectures to improve prediction accuracy under varying spatial and temporal conditions. (6) BigST [13] presents a linearized global spatial convolution network, reducing computational complexity to linear for both latent graph structure learning and spatial message passing on large-scale road networks. (7) STMoE [24] proposes a model-agnostic framework to mitigate performance bias across individual road segments within a traffic network, achieving more balanced predictions."}, {"title": "Evaluation Metrics", "content": "Based on previous studies, we use three widely adopted metrics: MAE, RMSE, and MAPE as evaluation criteria. To fairly assess the error proportions across nodes, we apply WAE as defined in Equation 11. Additionally, SARE, defined in Equation 2, is used to measure the variations in model performance across nodes."}, {"title": "Implementation Details", "content": "We implemented our model and the baseline models using the PyTorch framework on the NVIDIA Tesla V100S GPU. To ensure the fairness and reliability of the experiments, we applied a consistent training framework across all baseline models and selected appropriate hyperparameters based on each model's specific characteristics. For all datasets, we used the first 12 time steps T to predict the subsequent 12 time steps T'. The loss function for each baseline model followed its original definition. All models were trained using the Adam optimizer with the most suitable learning rate with 500 epochs. Our model was trained with a batch size of 64, a learning rate of 0.003, and a weight decay of 0.0001. For HiGCN, we set the hidden layer dimension of the MLP to 8, the hidden layer dimension of the graph convolution to 64, and used GELU as the activation function. For NMOE, we set the number of experts to 14 for datasets other than Beijing, and 20 for Beijing. For the fairness-aware loss function, we set \\(\\alpha\\) to 1 and \\(\\beta\\) to 0.5. For more detailed implementation information, readers can refer to our public code repository."}, {"title": "Performance Comparision", "content": "Table 2 presents the performance of HiMoE and other baseline models across four datasets.\nHiMoE outperforms other baselines across datasets with diverse data types and spatial scopes, demonstrating its adaptability to various real-world scenarios. Specifically, on the PeMS04 dataset, our model achieves improvements of 26.43%, 32.66%, 15.90%, 28.68%, and 43.47% over the best baseline (ASTGCN [9]) in MAE, RMSE, MAPE, WAE, and SARE, respectively. Similarly, on the Beijing dataset, it achieves gains of 32.86%, 55.31%, 12.46%, 35.83%, and 60.01%. Additionally, HiMoE delivers improvements of 9.22%, 14.98%, 13.42%, 9.64%, and 18.88% on the Tongzhou dataset, and achieves notable progress of 46.27%, 51.12%, 41.58%, 46.60%, and 55.71% on the KnowAir dataset.\nCompared with models specifically designed for traffic prediction (e.g., Graph WaveNet [40], D2STGNN [31], BigST [13]), our model exhibits superior performance on the Beijing and KnowAir datasets, emphasizing HiMoE's scalability across diverse scenarios. Similarly, relative to fully self-learning graph models (e.g., MTGNN [39]), our model's results validate that learning dynamic and static graphs is more effective for capturing spatial-temporal dependencies. In contrast to MoE-based approaches (e.g., TESTAM [23]), the performance of our model highlights the enhanced heterogeneity awareness achieved by NMoE compared to traditional MoE. Furthermore, against existing fair prediction models (e.g., STMOE [24]), our model demonstrates significant advantages in fairness evaluation metrics, showcasing the importance of the synergy between model architecture and training strategy in spatial-temporal prediction tasks. Nevertheless, the final performance of any model is subject to various influencing factors. Therefore, although HiMoE achieves outstanding results as presented in Table 2, these results alone do not comprehensively reflect the effectiveness of each method."}, {"title": "Loss Function Studies", "content": "Following the development of the fairness-focused loss function, it is crucial to assess the training performance of HiMoE with this loss function and examine whether other models can benefit from its application to achieve improved performance. In this section, we apply the fairness-focused loss function to train baseline models on the PeMS04 dataset. Overall, HiMoE outperforms all other models across all metrics."}, {"title": "Ablation Studies", "content": "In this section, we conduct an ablation study on the PeMS04 dataset to evaluate the effectiveness of each submodule. Five variants of the model are compared: (1) woSG: By replacing the learnable adjacency matrix with a dynamic one, the influence of the static adjacency matrix on the model is removed. (2) woDG: By substituting the learnable adjacency matrix with a static one, the effect of the dynamic adjacency matrix on the model is eliminated. (3) woEM: The tanh mask applied to the edges in each HiGCN module is removed. (4) woMoE: The MoE structure is removed, leaving only a single expert for predictions. (5) woR: the input to each expert model is identical, and the differentiation between experts is solely achieved through the sparse MoE strategy.\nFirst, replacing the learnable graph with either a dynamic graph or a static graph leads to significant performance degradation, demonstrating that the integration of dynamic and static graphs with the edge gating mechanism is crucial for the model's strong predictive performance. Second, we find that merely removing the edge mask causes a decline in prediction accuracy, underscoring the importance of HiGCN's modeling of both positive and negative edge weights, as well as their scaling ability. Third, Predicting with a single expert model alone still leads to a significant performance gap compared to HiMoE, thereby validating that the MoE structure in HiMoE substantially enhances predictive capability by effectively leveraging heterogeneity. Moreover, when the model is presented with identical input, the slight decline in performance observed when using sparse MoE to allocate experts further supports the notion that the constraints imposed on the input and output of expert models in this work enable a more distinct differentiation between experts, thus improving overall prediction accuracy."}, {"title": "RELATED WORK", "content": ""}, {"title": "Spatial-Temporal Data Forecasting", "content": "Spatial-temporal graph neural networks (STGNNs) have proven to be effective in modeling intricate relationships within spatial-temporal data [20]. Recent research has focused on stacking basic learnable blocks to form deep neural networks and employing innovative training methods to unlock the untapped potential of existing models [8, 16-18, 26]. To enhance the model's ability to capture spatial relationships, researchers have utilized various powerful GNN architectures and graph structures, each based on distinct features [14, 28, 33, 44-46].\nGCNs [22] are commonly used to capture spatial dependencies in spatial-temporal models, while some studies further model spatial-temporal systems as diffusion processes, applying diffusion convolutions to represent spatial correlations [25, 31]. For example, Graph WaveNet [40] combines causal and diffusion convolutions to capture spatial-temporal relationships effectively. The success of attention mechanisms in Transformer architectures has also inspired the development of models like Graph Attention Networks (GAT), which enhance spatial relationship modeling [35, 41]. In addition, many STGNNs incorporate attention mechanisms, such as ASTGCN [9], which introduces spatial-temporal attention, and GMAN [47], which integrates traffic features and node embeddings within a spatial attention framework. While complex models provide stronger representational capabilities, they are less suitable for expert models. Expert model architectures should remain simple to reduce computational complexity and prevent overfitting.\nConstructing spatial-temporal graphs is crucial in spatial-temporal data forecasting. Predefined adjacency matrices have been used in some studies to capture spatial relationships. For instance, STGCN employs a distance-based static adjacency matrix [43], while STGODE utilizes a semantic-based adjacency matrix based on Dynamic Time Warping (DTW) distance [7]. To better capture the latent dependencies between nodes, research such as Graph WaveNet has employed learnable adjacency matrices to enhance spatial-temporal modeling capabilities [40]. To integrate the advantages of these approaches, this paper proposes a gated graph learning method based on the fusion of dynamic and static adjacency matrices."}, {"title": "Mixture of Experts", "content": "The Mixture of Experts (MoE) model, proposed by Shazeer et al. [32], is a machine learning framework designed to improve overall performance and efficiency by combining the complementary capabilities of multiple expert models. This architecture dynamically assigns input data to specialized experts based on the task requirements, enabling the model to achieve exceptional results in complex and diverse tasks. MoE has seen extensive application in a variety of fields, such as Natural Language Processing (NLP) [1, 42] and image recognition [30, 34].\nMoE has also been widely applied in spatial-temporal forecasting across various domains. Wu et al. [37] utilized MoE to model different crime types, capturing unique and shared patterns for collective multi-type crime prediction. Jiang et al. [19] applied MoE to real-world congestion prediction, enhancing accuracy in dynamic environments. Li et al. [24] leveraged MoE to model the dynamic characteristics of traffic segments, focusing on improving fairness in the predictions. These examples demonstrate MoE's versatility and effectiveness in tackling complex spatial-temporal prediction challenges.\nExisting Spatial-Temporal Mixture-of-Experts models focus on temporal dynamics to improve prediction accuracy for spatial-temporal data. However, these models often fail to fully leverage the potential of MoE architectures for capturing spatial heterogeneity, which can limit their performance across spatially heterogeneous nodes. Although many existing models have shown some success, they do not adequately balance the number and complexity of experts. Typically, these approaches rely on a small set of complex experts, which restricts the full potential of the MoE framework."}, {"title": "CONCLUSION", "content": "In this paper, we propose HiMoE, a MoE-based framework for fair spatial-temporal forecasting. We first developed HiGCN to enhance the traditional GCN model's adaptability to heterogeneity. This is accomplished by incorporating a gated adaptive graph fusion module, which improves the flexibility of graph representation learning. Additionally, we introduced NMoE to leverage the collaboration of multiple expert models to enhance predictive performance. This is achieved by dividing the spatial-temporal prediction task into subtasks, each assigned to a specific expert, thereby significantly improving the uniformity of the model's predictions. Furthermore, we propose fairness-aware loss and evaluation functions that enable the model to utilize its potential for impartial learning. Extensive experimental results show that our approach surpasses existing methods in accuracy and fairness across a wide range of datasets, highlighting the scalability of our model to diverse real-world scenarios. In the future, we plan to integrate online learning capabilities into HiMoE, further expanding its potential applications."}]}