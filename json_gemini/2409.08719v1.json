{"title": "DISTILLING MONOLINGUAL AND CROSSLINGUAL\nWORD-IN-CONTEXT REPRESENTATIONS", "authors": ["Yuki Arase", "Tomoyuki Kajiwara"], "abstract": "In this study, we propose a method that distils representations of word meaning in context from a pre-\ntrained masked language model in both monolingual and crosslingual settings. Word representations\nare the basis for context-aware lexical semantics and unsupervised semantic textual similarity (STS)\nestimation. Different from existing approaches, our method does not require human-annotated\ncorpora nor updates of the parameters of the pre-trained model. The latter feature is appealing for\npractical scenarios where the off-the-shelf pre-trained model is a common asset among different\napplications. Specifically, our method learns to combine the outputs of different hidden layers of the\npre-trained model using self-attention. Our auto-encoder based training only requires an automatically\ngenerated corpus. To evaluate the performance of the proposed approach, we performed extensive\nexperiments using various benchmark tasks. The results on the monolingual tasks confirmed that\nour representations exhibited a competitive performance compared to that of the previous study\nfor the context-aware lexical semantic tasks and outperformed it for STS estimation. The results\nof the crosslingual tasks revealed that the proposed method largely improved crosslingual word\nrepresentations of multilingual pre-trained models.", "sections": [{"title": "1 Introduction", "content": "Word representations are the basis for various natural language processing (NLP) tasks. Particularly, they are crucial\ncomponents in context-aware lexical semantics and in the estimation of unsupervised semantic textual similarity\n(STS) in both monolingual Ethayarajh [2018], Yokoi et al. [2020], Liu et al. [2020] and crosslingual Wieting et al.\n[2020], Reimers and Gurevych [2020] settings. Word representations are expected to represent the word meaning in\ncontext to improve these downstream tasks. Large-scale masked language models pre-trained on massive corpora,\ne.g., bi-directional encoder representations from transformers (BERT) Devlin et al. [2019], embed both the context\nand meaning of a word. Therefore, word-level representations generated by such masked language models are known\nas contextualised word representations. Previous studies Ethayarajh [2019], Vuli\u0107 et al. [2020] have revealed that\nlexical and context-specific information is captured in different layers of masked language models. They argue that a\nsophisticated mechanism is required to derive the representations of word meaning in context from them. Although\ncontextualised word representations have shown considerable promise, how best to compose the outputs of different\nlayers of masked language models to effectively represent the word meaning in context remains an open question.\nSimilar to other NLP tasks, fine-tuning the pre-trained model with human-annotated corpora is a promising approach\nWang et al. [2019]. However, annotating word meanings in context is non-trivial, and no such resource is abundantly\navailable. Existing studies Liu et al. [2021a,b] applied contrastive learning Gao et al. [2021] to fine-tune the pre-trained\nmasked language model without human-annotated corpora, which tweaks the model to generate representations for"}, {"title": "2 Related Work", "content": "This section reviews studies on representation learning for word meaning in context. It also discusses the disentanglement\nof representations that is closely related to the present study methodologically."}, {"title": "2.1 Representation Learning for Word Meaning in Context", "content": "An intuitive approach for obtaining representations of word-in-context might be to supervise a pre-trained masked\nlanguage model using human-annotated corpora. While effective Wang et al. [2019], Camacho-Collados et al. [2017],\nthe annotation of word meaning in context is non-trivial, which hinders the abundant production of such corpora. To\naddress this problem, previous studies Liu et al. [2021a,b] applied contrastive learning to fine-tune a pre-trained model\nwithout human-annotated corpora. However, the parameter update during training may cause catastrophic forgetting,\nwhich makes reusing the fine-tuned model for other NLP tasks difficult.\nAn orthogonal approach trains an independent model that transforms representations generated by the frozen pre-\ntrained model. This approach allows reusing the off-the-shelf pre-trained model as a common asset among different\nNLP applications. The transformation has been used to adjust the excessive effects of the context that dominates\nthe representations. Shi et al. Shi et al. [2019] added a transformation matrix on top of the embedding layer of\nELMO Peters et al. [2018]. Their approach derives the matrix that makes final representations of the same words\nin paraphrased sentences similar, whereas those of non-paraphrases become distant. The method most relevant to\nthe present study was proposed by Liu et al. Liu et al. [2020]. They transformed the space of word representations\ntowards the rotated space of static word embedding using a crosslingual alignment technique Doval et al. [2018] for\ncontext-aware lexical semantic tasks. In principle, these previous studies aim to make contextualised representations\nless sensitive to contexts through transformation and prevent them from dominating the representations. In contrast, we\nderive word-in-context representations by combining different layers of a pre-trained model while preserving useful\ncontext information. Furthermore, our method is applicable to obtain crosslingual word-in-context representations with\nminimum modifications, whereas these previous studies are not."}, {"title": "2.2 Representation Disentanglement", "content": "Disentanglement techniques are also relevant to our approach. They generate specialised representations dedicated\nto a specific aspect. Previous studies typically employed autoencoders, with the encoder and decoder learning to\ndisentangle and reconstruct original representations, respectively. Wieting et al. Wieting et al. [2020] and Tiyajamorn et\nal. Tiyajamorn et al. [2021] disentangled language-specific styles and sentence meanings. Shen et al. Shen et al."}, {"title": "3 Distilling Word Meaning in Context", "content": "Inspired by the representation disentanglement approach discussed in Section 2.2, we model the distillation of represen-\ntations of word meaning in context using an autoencoder, as shown in Figure 1. This is a common architecture for both\nmonolingual and crosslingual settings. Vuli\u0107 et al. Vuli\u0107 et al. [2020] probed pre-trained language models for lexical\nsemantic tasks, revealing that lexical information is scattered across lower layers whereas context-specific information\nis embedded in higher layers. Therefore, we aim to distil the outputs of different hidden layers using a transformer\nencoder layer.\nFigure 1 shows the model architecture. First, we obtain the outputs of all the hidden layers of a masked language\nmodel, $MLM(\\cdot)$, with frozen parameters $H = MLM(S) \\in \\mathbb{R}^{|S|\\times(l+1)\\times d}$, where $S$ is an input sentence of length $|S|$\ncontaining the target word; $w_t \\in S$, $l$ is the number of hidden layers in the masked language model (0 corresponding to\nits embedding layer); $d$ is the hidden dimension of the masked language model. Thereafter, we extract the outputs of\nthe hidden layers corresponding to the target word, $w_t$, from $H$, indicating that $H_{w_t} = [h_0, h_1,\\ldots,h_{\\ell}]^T \\in \\mathbb{R}^{(\\ell+1)\\times d}$.\nWhen $w_t$ is segmented into a set of $m$ sub-words $\\tau_1, \\tau_2, , \\tau_m$, by a tokenizer of the masked language model, we\ncompute the layer-wise averages of the hidden outputs of all the sub-words Bommasani et al. [2020]. This indicates that\n$h_i \\in H_{w_t}$ becomes\n$h_i = Pool(h_i^1,\\ldots,h_i^m)$,\nwhere $h_i^j$ is the $i$th hidden output of a sub-word $\\tau_j$ and the $Pool(\\cdot)$ function conducts mean-pooling.\nThereafter, we input these hidden outputs into a meaning distillation model to derive a representation of word meaning\nin context. We also input the hidden outputs to another distillation model that derives information other than word\nmeaning in context. Hereinafter, we refer to this information as the context and the distillation model as the context\ndistillation model. Each distillation model consists of a transformer encoder layer followed by a mean-pooling function"}, {"title": "4 Learning Framework", "content": "The meaning and context distillation models described in Section 3 require constraints to ensure that the desired\nattributes are distilled; otherwise, these distillation models obtain a degenerated solution that simply copies the original\nrepresentations. We design a framework ensuring that word meaning in context is distilled using an automatically\ngenerated training corpus.\nSubsequently, we first describe our learning framework for the monolingual settings (Section 4.1 and Section 4.2).\nThereafter, we adapt the framework for the crosslingual settings (Section 4.3) with minimal modifications."}, {"title": "4.1 Cross Reconstruction", "content": "Assume we have two sentences, $S_p$ and $S_n$, as positive and negative examples, respectively, as shown in Table 1. $S_p$ is\na paraphrase of $S$ that must contain a word $w_p$, which is equivalent to $w_t$ or a lexical paraphrase of $w_t$. In contrast,\n$S_n$ is a negative example that replaces $w_t$ with a word $w_n$ in $S$, which has a different meaning from $w_t$ but fits in the\ncontext of $S$.\nUsing the hidden outputs of $w_p$ and $w_n$, we distil the meaning and context representations, $p^m$ and $p^c$, and those of\n$n^m$ and $n^c$, respectively. The meaning representation of $w_t$, $h^m$, should satisfy the following two conditions."}, {"title": "4.2 Training Corpus Creation", "content": "In this section, we describe the generation of a training corpus in an automatic way using techniques of round-trip\ntranslation, word alignment, and masked token prediction.\nRound-trip Translation and Word Alignment As a positive example $S_p$, we need a paraphrase of $S$ containing a\nword $w_p$ that is a lexical paraphrase of $w_t$. First, we automatically generate paraphrases using round-trip translation\nKajiwara et al. [2020]. Next, we conduct word alignment to identify $w_p$ in a paraphrased sentence. Specifically, we\nadopt a simple word alignment method commonly used in previous studies Jalili Sabet et al. [2020], Garg et al. [2019],\nOch and Ney [2003], which aligns a word $w_i$ to $w_j$ if and only if $w_i$ is most similar to $w_j$ and vice-versa. The similarity\nis computed by cosine similarities between their embeddings. We add heuristics to the word alignment method to\nimprove alignment accuracy as detailed in Appendix A. Note that candidates in which $w_p$ is not identified are discarded.\nMasked Token Prediction Negative samples replace $w_t$ with an arbitrary word $w_n$ that fits in the context of $S$.\nWe generate candidates for the replacement of words using the masked token prediction, which is the primary task\nused to train the masked language model. Specifically, we input an original sentence whose target is masked by\nthe [MASK] label to the masked language model, and we obtain predictions $T = \\{t_1,\\ldots,t_{|V|}\\}\\ with probabilities,"}, {"title": "4.3 Training for Crosslingual Scenario", "content": "Because mutual translations are paraphrases in the crosslingual case, we can employ off-the-shelf parallel corpora. We\nadapt the learning framework for the crosslingual settings such that the differences in languages, e.g., language-specific\nstyles, are also distilled into the context representations. For this purpose, we regard English sentences as original\nand use their parallel (non-English) sentences both for positive and negative samples. Specifically, we identify lexical\ntranslations of the target words on parallel sentences as positive samples. Besides, we generate negative samples based\non the positive samples using masked token prediction, in contrast to the monolingual settings wherein we generated\nnegative samples based on the original sentences."}, {"title": "5 Experimental Setup", "content": "We empirically evaluated whether our method distils representations of word meaning in context from a masked\nlanguage model using context-aware lexical semantic tasks and STS estimation tasks. All the experiments were\nconducted on an NVIDIA Tesla V100 GPU."}, {"title": "5.1 Context-aware Lexical Semantic Tasks", "content": "We followed the experimental settings used by Liu et al. Liu et al. [2020] for a fair and systematic performance\ncomparison. They categorised monolingual context-aware lexical semantic tasks into Within-word and Inter-word\ntasks. The former evaluates the diversity of word representations for different meanings of the same word associated\nwith different contexts. In contrast, the latter evaluates the similarity of word representations for different words when\nthey have the same meaning. Besides, we evaluate the Crosslingual tasks. The left-side columns of Table 3 show the\nnumber of word pairs in the monolingual evaluation corpora and those of Table 4 show the number of word pairs in the\ncrosslingual evaluation corpus per language pair.\nWithin-word Tasks The within-word evaluation was divided into three tasks. The first is based on the Usage\nSimilarity (Usim) corpus Erk et al. [2013], which provides graded similarity between the meanings of the same word in\na pair of different contexts. The second task uses the Word in Context (WiC) corpus Pilehvar and Camacho-Collados\n[2019], which provides binary judgements to verify whether the meaning of a given word varies in different contexts.\nFollowing the standard setting recommended in the original study, we tuned the threshold for cosine similarity between\nword representations to make binary judgments. Specifically, we searched the threshold in the range of [0, 1.0] with\n0.01 intervals to maximise the accuracy of the validation set. The third task is the subtask-1 of CoSimlex Armendariz\net al. [2020] (denoted as CoSimlex-I). CoSimlex provides a pair of contexts consisting of several sentences for each\nword pair extracted from SimLex-999 Hill et al. [2015]. It annotates the graded similarity in each context. CoSimlex-I\nrequires the estimation of the change in similarities between the same word pair in different contexts. Hence, it evaluates\nwhether representations can change for different word meanings based on the context.\nInter-word Tasks The inter-word evaluation consisted of two tasks. The first was the subtask-2 of CoSimlex (denoted\nas CoSimlex-II), which required estimating the similarity between different word pairs in the same context. The\nsecond task used the Stanford Contextual Word Similarity (SCWS) corpus Huang et al. [2012], which provided graded\nsimilarity between word pairs in a pair of different contexts. The contexts of CoSimlex and SCWS consist of several\nsentences. We input all the sentences as a single context.\nCrosslingual WiC Tasks We used the Multilingual and Crosslingual Word-in-Context (MCL-WiC) corpus Camacho-\nCollados et al. [2017], which provides test sets for English-Arbic (en-ar), English-French (en-fr), English-Russian\n(en-ru), and English-Chinese (en-zh) language pairs. The MCL-WiC task requires to determine whether an English\nword in a sentence and its lexical translation in a sentence of other language share the same meaning. Although the\noriginal MCL-WiC corpus allowed an English word to be translated into a multi-word expression and periphrasis, we\nexcluded pairs in which a target splits into non-consecutive phrases in the other language. We similarly made binary\njudgements with the monolingual WiC task. The original corpus covers both multilingual and crosslingual tasks, and\nprovides validation sets only for the multilingual tasks. Hence, following the organisers' report Camacho-Collados\net al. [2017], we used the corresponding multilingual validation sets in languages other than English to determine the\nthreshold for binarization. For instance, we used the 'fr-fr' validation set for the 'en-fr' task."}, {"title": "5.2 STS Tasks", "content": "We also evaluated the proposed method on the STS tasks. Cosine similarity is commonly used to estimate the\nsimilarity between two text representations. We also used cosine similarity because such a primitive measure is\nsensible to characteristics of different representations. We generated a sentence representation by simply averaging the\nrepresentations of sub-words in a sentence, excluding the representations for special tokens preserved in BERT, i.e.,\n[CLS], [SEP], and [PAD]. Thereafter, we computed the cosine similarities between them.\nRegarding the monolingual tasks, we evaluated the 2012 to 2016 SemEval STS shared tasks Agirre et al. [2012, 2013,\n2014, 2015, 2016] (STS 2012 to 2016) to predict the human scores that indicate the degree of semantic similarity\nbetween the two sentences. We downloaded and pre-processed the datasets using the SentEval toolkit Conneau and\nKiela [2018].\nConsidering a crosslingual task, we evaluated the extended version of SemEval 2017 crosslingual STS shared task\nCer et al. [2017] (STS 2017). Although the task consists of 7 subtasks of different language pairs, we chose subsets\nwhose sentences are created by human translations to exclude biases originating from specific machine translators:\nEnglish-Arabic (en-ar), English-German (en-de), English-Turkish (en-tr), and English-Spanish (en-es).\nThe Pearson's r between the model predictions and human scores was used as an evaluation metric. Each STS corpus is\ndivided by data sources. Hence, the corpus level score is the average of the Pearson's r for each sub-corpus."}, {"title": "5.3 Training Corpus Preparation", "content": "Wikipedia is a common data source used to create training corpora for monolingual and crosslingual settings. Regarding\nboth settings, we randomly sampled and excluded approximately 1% sentences as the validation set and used the rest\nfor the training.\nMonolingual Training Corpus To prepare the training corpus for the monolingual settings, we used English\nWikipedia dumps distributed for the WMT20 competition. The texts were extracted using WikiExtractor. As a\npre-processing step, we first identified the language of each text using the langdetect toolkit and discarded all the\nnon-English texts. Thereafter, we conducted sentence segmentation and tokenization using Stanza Qi et al. [2020] and\nextracted sentences of 15 to 50 words.\nConsidering the candidate target words, we extracted the top-50k frequent words following Liu et al. Liu et al. [2020].\nThereafter, we sampled 1M sentences containing these words from the pre-processed Wikipedia corpus. Using these\n1M sentences, we obtained paraphrases via round-trip translation where the translators were trained with exactly the\nsame settings as Kajiwara et al. Kajiwara et al. [2020]. Subsequently, we generated positive and negative samples using\nword alignment and masked token prediction, where we used fastText and BERT-Large, cased model, respectively.\nRound-trip translation does not always produce an alignable $w_p$, and our simple word alignment heuristic may fail\nto identify $w_p$. Hence, the final number of sentences in our training corpus was reduced to 929, 265, where 44, 614\nunique words remained as targets. Among them, 242, 643 sentences contained $w_p$ whose surfaces were larger than the\n3 character-level edit distance, which were expected as lexical paraphrases. We used these 929k triples of the original,\npositive, and negative samples.\nCross-Lingual Training Corpus To prepare a crosslingual training corpus, we used WikiMatrix Schwenk et al.\n[2021], parallel corpora mined from the textual content of Wikipedia. Following Reimers and Gurevych Reimers and\nGurevych [2020], we extracted parallel sentences, thresholding the margin score by 1.05 to obtain the mutual translations.\nThereafter, we applied the same pre-processing of language identification and tokenization as the monolingual settings,\nand obtained 50k target words in English for each language pair."}, {"title": "5.4 Baselines", "content": "Regarding the monolingual tasks, we compare our method to Liu et al. Liu et al. [2020] as the state-of-the-art in the\nfamily of methods that transform contextualised representations. Recall that Liu et al. Liu et al. [2020] transform\nrepresentations from the masked language model using static word embeddings. Specifically, we used fastText as the\nstatic embeddings that performed most robustly across the models and tasks."}, {"title": "5.5 Implementation", "content": "We implemented our method using PyTorch and Transformers library Wolf et al. [2020]. Regarding the monolingual\ntasks, we used the cased version of BERT-Large. For crosslingual tasks, we used 'base' models for mBERT (the cased\nversion), XLM-R, and mSBERT\u2079 owing to the larger training corpus size. Recall that the parameters of the pre-trained\nmodels were frozen and never fine-tuned.\nThe meaning and context distillers of the proposed model included a transformer layer. We set the number of attention\nheads as eight.10 We set the dimensions of the internal feed-forward network as four and six times of the input for the\nmonolingual and crosslingual settings, respectively. We used the larger dimension for the latter because it had to deal\nwith the extra-task to distil language-specific styles. We applied 10% dropouts to the transformer layer. The batch sizes\nwere 128 and 512 for the monolingual and crosslingual settings, respectively. We used AdamW Loshchilov and Hutter\n[2019] as an optimizer for which the learning rate was tuned on smaller samples Smith [2017]. For stable training, we\napplied a warm-up, where the initial learning rate was linearly increased for the first 1k steps to reach the predetermined\nvalue. The training was stopped early with patience of 15 and a minimum delta of 1.0e - 5 based on the validation loss."}, {"title": "6 Experimental Results", "content": "Below, we discuss the experimental results of the monolingual and crosslingual tasks. We also analyse the results of the\nablation study to investigate the contributions of the positive and negative samples."}, {"title": "6.1 Results on Monolingual Tasks", "content": "Table 6 shows the results on the context-aware lexical semantic tasks. The superior performance of our meaning\nrepresentation to context representations confirms that distillation has been performed as aimed. Our meaning\nrepresentations achieved a performance competitive with the transformation method by Liu et al. Liu et al. [2020].11\nWhile the transformation method was stronger in the Within-Word tasks, our method outperformed it for Inter-Word\ntasks. This is because the transformation method makes representations of the same words in different contexts closer\nto the same static embedding; nonetheless, they do not explicitly model the relations across words. In contrast, our\nnegative samples provide distant supervision, which makes the representations of words with different meanings\ndistinctive. Although the performances of these two methods are competitive, these different properties are reflected in\nthe representations.\nThe difference is more pronounced in the results of unsupervised STS tasks shown in Table 7.12 Our meaning\nrepresentations outperformed the transformed representations in four out of the five tasks. The transformation has\nthe effect of making contextualised representations less sensitive to contexts to prevent contexts from dominating the\nrepresentations. This effect is preferred in tasks of context-aware lexical semantics that severely require representations\nof word meaning; meanwhile, it sacrifices context information valuable for STS. In contrast, our method does not waste\nthe context information useful for composing sentence representations."}, {"title": "6.2 Results on Crosslingual Tasks", "content": "Table 8 shows the results of the MCL-WiC tasks: thresholds for making binary judgements, validation set accuracies,\nand test set accuracies.13 Evidently, the language barrier persists in the mBERT and XLM-R models, as shown in the\ncomparable test set accuracies to the majority baseline on the average. These findings are consistent with Tiyajamorn et\nal. Tiyajamorn et al. [2021] who showed that the language difference separates the spaces of sentence representations of\nmBERT and XLM-R, and thus representations of semantically similar sentences across languages are far apart. The\nlanguage barrier was alleviated in the mSBERT model that largely outperforms the mBERT and XLM-R. Notably,\nthe meaning representations of the proposed method on all the pre-trained models outperformed the mSBERT model\naveragely. Furthermore, the combination of the proposed method and mSBERT achieved the best accuracy, which\noutperformed mSBERT by 1.61% (en-ar) to 8.45% (en-zh). These results imply that the proposed method may be\neffective in other multilingual pre-trained models, such as LaBSE Feng et al. [2022].\nTable 9 shows the results of the crosslingual STS tasks. Contrary to the monolingual STS tasks, the meaning\nrepresentations showed inconsistent effects, depending on the pre-trained models. The meaning representations\nimproved the mSBERT on three out of four language pairs, whereas they deteriorated the representations of mBERT and\nXLM-R. We conjecture this phenomenon relates to the performance of original pre-trained models; although mSBERT"}, {"title": "6.3 Ablation Study", "content": "Table 6 to Table 9 also show the results of the ablation study, where we excluded the negative samples to train our\nmethod. Without the negative samples, our method becomes unconstrained; the cross-reconstruction becomes symmetric\nfor the meaning and context distillers. Hence, the model loses its ability to distil word meaning in context into the\nmeaning representations. This effect was observed for all the tasks in the monolingual and crosslingual settings, where\nmeaning representations without negative samples were no longer useful. In the subsequent section, we investigate the\ncharacteristics of the meaning and context representations distilled with and without the negative samples."}, {"title": "7 Analysis", "content": "We investigated the characteristics of the meaning and context representations by analysing the inter-word similarities\nand similarities to different layers of pre-trained models. These investigations further deepened the understanding of the\nempirical effects of negative samples in our method."}, {"title": "7.1 Inter-word Similarity Distribution", "content": "We conducted an in-depth analysis using the corpus of paraphrase adversaries from word scrambling (PAWS) Zhang\net al. [2019]. PAWS is an English paraphrase corpus dedicated to evaluating the sensitivity of recognition models\nfor syntax in paraphrases. It provides paraphrase and non-paraphrase pairs that were generated by controlled word\nswapping and back translation with manual screening. Because the pairs in PAWS have relatively high word overlap\nrates, models that are insensitive to contexts cannot exceed the chance rate for paraphrase recognition.\nWe generated representations of sentences in the PAWS-Wiki Labelled (Final) section similar to the STS tasks and\ncomputed the cosine similarities between them. Thereafter, we determined the threshold to regard a pair as a paraphrase\nusing the validation set. Table 10 shows the results. BERT-Large and the transformation method had equal to or lower\naccuracy than the chance rate of 55.80% (always outputting the majority label of the non-paraphrases). In contrast,\nour method improved the accuracy, even on this challenging task. This is achieved by our property that distils word\nmeaning in context preserving useful context information."}, {"title": "7.2 Layer-wise Similarity Distribution", "content": "We investigated how our meaning and context representations are similar to the outputs of different layers of pre-trained\nmodels. Figure 3 and Figure 4 show the average cosine similarities between these representations with or without the"}, {"title": "8 Summary and Future Work", "content": "We have proposed a method that distils a representation of word meaning in context from an off-the-shelf pre-trained\nmasked language model. The extensive experimental results showed the effectiveness of the proposed method in both\nmonolingual and crosslingual scenarios. Furthermore, the results confirmed that the negative samples are essential as\nconstraints to properly distil the representations of word meaning in context by avoiding degenerated solutions.\nIn future work, we plan to investigate the correspondences of the context representations, i.e., what information is\ndistilled in them. In addition, we will extend our method to handle low-resource languages in both multilingual and\ncrosslingual settings Liu et al. [2021c]."}]}