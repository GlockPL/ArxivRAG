[{"title": "Verification of Geometric Robustness of Neural Networks via Piecewise Linear Approximation and Lipschitz Optimisation", "authors": ["Ben Batten", "Yang Zheng", "Alessandro De Palma", "Panagiotis Kouvaros", "Alessio Lomuscio"], "abstract": "We address the problem of verifying neural networks against geometric transformations of the input image, including rotation, scaling, shearing, and translation. The proposed method computes provably sound piecewise linear constraints for the pixel values by using sampling and linear approximations in combination with branch-and-bound Lipschitz optimisation. A feature of the method is that it obtains tighter over-approximations of the perturbation region than the present state-of-the-art. We report results from experiments on a comprehensive set of benchmarks. We show that our proposed implementation resolves more verification cases than present approaches while being more computationally efficient.", "sections": [{"title": "Introduction", "content": "Neural networks as used in mainstream applications - including computer vision are known to be fragile and susceptible to adversarial attacks [19]. The area of formal verification of neural networks is concerned with the development of methods to establish whether a neural network is robust, with respect to its classification output, to variations of the image. A large body of literature has so far focused on norm-bounded input perturbations, aiming to demonstrate that imperceptible adversarial alterations of the pixels cannot alter the classifier's classification (lp robustness). In safety-critical applications such as autonomous driving, however, resistance to norm-bounded perturbations is inadequate to guarantee safe deployment. In fact, image classifiers need to be robust against a number of variations of the image, including contrast, luminosity, hue, and beyond. A particularly important class of specifications concerns robustness to geometric perturbations of the input image [2, 24, 28, 33]. These may include translation, shearing, scaling, and rotation.\nOwing to the highly nonlinear variations of the pixels in geometric transformations, verifying robustness to these perturbations is intrinsically a much harder problem than lp robustness. Previous work over-approximates these variations through hyper-rectangles [33] or pairs of linear bounds over the pixel values [2], hence failing to capture most of the complexity of the perturbation region. Developing more precise methods for verifying geometric robustness remains an open challenge. In this paper we work towards this end. Specifically, we make three contributions:\n1. We present a piecewise linear relaxation method to approximate the set of images generated by geometric transformations, including rotation, translation, scaling, and shearing. This construction can incorporate previous approaches [2, 33] as special cases while supporting additional constraints, allowing significantly tighter over-approximations of the perturbation region.\n2. We show that sound piecewise linear constraints, the building blocks of the proposed relaxation, can be generated via suitable modifications of a previous approach [2] that generates linear constraints using sampling, linear and Lipschitz optimisation. We derive formal results as well as effective heuristics that enable us to improve the efficiency of the linear and Lipschitz optimisations in this context (cf. Propositions 1-3). As we demonstrate, the resulting piecewise constraints can be readily used within existing tight neural network verifiers.\n3. We introduce an efficient implementation for the verification method above and discuss experimental results showing considerable gains in terms of verification accuracy on a comprehensive set of benchmark networks.\nThe rest of this paper is organized as follows: Section 2 discusses related work. In Section 3 we introduce the problem of verifying neural networks against geometric robustness properties. In Section 4 we present our novel piecewise linear approximation strategy via sampling, optimisation and shifting. In Section 5 we discuss the experimental results obtained and contrast the present method against the state-of-the-art on benchmark networks. We conclude in Section 6. Our code is available at [1]."}, {"title": "Related Work", "content": "We here briefly discuss related work from lp-based neural network verification, geometric robustness and formal verification thereof.\nlp robustness verification There is a rich body of work on the verification of neural networks against lp-bounded perturbations: see, e.g., [27] for a survey. Neural network verifiers typically rely on Mixed-Integer Linear Programming (MILP) [4, 36], branch-and-bound [5, 6, 8, 14, 20, 39, 42, 46], or on abstract interpretation [18, 32, 34]. These methods cannot be used to certify geometric robustness out of the box, as lp balls are unable to accurately represent geometric transformations [24, 33].\nGeometric robustness The vulnerability of neural networks to geometric transformations has been observed in [11, 12]. A common theme among these works is their quantitative nature, whereby measures of invariance to geometric robustness are discussed [23] and methods to improve spatial robustness are developed. These are based on augmentation [17, 41], regularisation schemes [43], robust optimisation [11] and specialised, invariance-inducing network architectures [21]. Differently from the cited works, our key aim here is the qualitative analysis of networks towards establishing formal guarantees of geometric robustness.\nFormal verification of geometric robustness One of the earliest works [29] discretises the transformation domains, enabling robustness verification through the evaluation of the model at a finite number of discretised transformations. In contrast to Pei et al. [29], we here focus on continuous domains, which do not allow exhaustive evaluation. Previous work on continuous domains relies on over-approximations, whereby, for each pixel, the set of allowed values under the perturbation is replaced by a convex relaxation [2, 24, 33]. In particular, [24] and [33] use an lo norm ball and intervals respectively, resulting in loose over-approximations. Balunovi\u0107 et al. [2] devise more precise convex relaxations by computing linear approximations with respect to the transformation parameters. In this work, we further improve precision compared to Balunovi\u0107 et al. [2] by deriving piecewise linear approximations. While the above works consider the geometric transformation as a whole (see Section 3), Mohapatra et al. [28] decompose the transformation into network layers to be pre-pended to the network under analysis, resulting in looser approximations when using standard neural network verifiers. More recently, randomised smoothing techniques have been investigated for geometric robustness [15, 16, 26]: differently from our work, these only provide probabilistic certificates. Finally, Yang et al. [44] recently presented a method to train networks more amenable to geometric robustness verification. Our work is agnostic to the training scheme: we here focus on the more challenging general case."}, {"title": "Geometric robustness verification", "content": "Our main contribution is a new piecewise linear relaxation of geometric transformations to verify robustness of neural networks to geometric perturbations. We here introduce relevant notation in the verification problem and present the geometric attack model.\nNotation Given two vectors $a, b \\in \\mathbb{R}^n$, we use $a > b$ and $a < b$ to represent element-wise inequalities. Given a vector $a \\in \\mathbb{R}^m$ and a matrix $A \\in \\mathbb{R}^{m\\times n}$, we denote their elements using $a[i]$ and $A[i, j]$, respectively.\nNeural networks for classification We consider a feedforward neural network with L hidden layers $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$. Let $x_0 \\in \\mathbb{R}^n$ denote the input and $x_i$ denotes the activation vectors at layer i. We use $L_i$ to denote an affine map at layer i, e.g., linear, convolutional, and average pooling operations. Let $\\sigma_i$ be an element-wise activation function, such as ReLU, sigmoid or tanh. The activation vectors are related by $X_{i+1} = \\sigma_i(L_i(x_i))$, $i = 0,1,..., L - 1$. We are interested in neural networks for classification: the network output $f(x_0) = L_L(X_L) \\in \\mathbb{R}^m$ represents the score of each class, and the label i* assigned to the input $x_0$ is the class with highest score, i.e., $i^* = \\arg \\max_{i=1,...m} f(x_0)[i]$.\nRobustness verification Let A be a general attacker that takes a nominal input $x \\in \\mathbb{R}^n$ and returns a perturbed input $A(x) \\in \\mathbb{R}^n$. We denote the attack space as $\\Omega_{\\epsilon}(x) \\subset \\mathbb{R}^n$, i.e., $A(x) \\in \\Omega_{\\epsilon}(X)$, where $\\epsilon > 0$ denotes the attack budget. Formally verifying that a classification neural network f is robust with respect to an input $x$ and its attack space $\\Omega_{\\epsilon}(x)$ implies ensuring that all points in $\\Omega_{\\epsilon}(x)$ will share the same classification label of x. This can be done by solving the following optimisation problem $\\forall i \\neq i^*$: \n$\\gamma := \\min_{x_0,x_1,...x_L,y} \\quad y[i^*] - y[i] $\nsubject to $x_0 \\in \\Omega_{\\epsilon}(x), $\n(1a)\n$X_{i+1} = \\sigma_i (L_i(x_i)), i \\in [L]$\n(1b)\n$y = L_L(X_L),$\n(1c)\nwith (1b) being neural network constraints, (1c) as the neural network output, (1a) denoting the attack model constraint, and $L := \\{0,1,..., L-1\\}$. If $\\gamma> 0 \\forall i \\in \\{1,...,m\\}$, the network is certified to be robust.\nEven when (1) is a convex set, such as in the case of $l_p$ perturbations, for which $\\Omega_{\\epsilon}(x) = \\{x \\in \\mathbb{R}^n | ||x - X ||_p < \\epsilon\\}$, the nonconvex neural network constraints (1b) make the verification problem (1) difficult to solve. However, in this setting, tractable lower bounds $\\gamma^* < \\gamma$ on the solution can be obtained through a variety of techniques, including: linear relaxations [10, 31, 33, 35, 37, 45], semi-definite programming [3, 7, 13, 30] and Lagrangian duality [5, 8, 9, 40, 46]. These techniques lie at the core of the network verifiers described in Section 2. If $\\gamma^* > 0 \\forall i \\in \\{1,..., m\\}$, the network is robust, but a negative lower bound will leave the property undecided, pointing to the importance of tight lower bounds. When considering geometric transformations, the attack model constraint (1a) is highly nonconvex, making verification even more challenging.\nAttack model via geometric transformation A geometric transformation of an image is a composite function, consisting of a spatial transformation $T_{\\mu}$, a bilinear interpolation $I(u, v)$, which handles pixels that are mapped to non-integer coordinates, and changes in brightness and contrast $P_{\\alpha,\\beta}$. The spatial transformation $T_{\\mu}$ can be a composition of rotation, translation, shearing, and scaling; see e.g., [2] for detailed descriptions. The pixel value $p_{u,v}$ at position (u, v) of the transformed image is obtained as follows: (1) the preimage of (u, v) is calculated under $T_{\\mu}$; (2) the resulting coordinate is interpolated via I to obtain a value $\\xi$; (3) $P_{\\alpha,\\beta}(\\xi) = \\alpha\\xi + \\beta$ is applied to compute the final pixel value $p_{u,v}$. In other words, we have that $p_{u,v} = G_{u,v} (\\alpha, \\beta, \\mu)$, where:\n$G_{u,v}(\\alpha, \\beta, \\mu) := P_{\\alpha,\\beta} \\circ I \\circ T_{\\mu}^{-1} (u, v).$\n(2)\nWe consider the following standard bilinear interpolation:\n$I(u, v) = \\sum_{\\delta_i,\\delta_j \\in\\{0,1\\}} p_{i+\\delta_i,j+\\delta_j} (1 - |i + \\delta_i \u2013 u|)(1 - |j + \\delta_j \u2013 v|),$\nwhere (i, j) denotes the lower-left corner of the interpolation region [i, i + 1] \u00d7 [j, j + 1] that contains pixel (u, v), and the matrix p denotes the pixel values of the original image. Note that the interpolation function I is continuous on $\\mathbb{R}^2$ but can be nonsmooth on the boundaries of interpolation regions. Thus, $G_{u,v} (\\alpha, \\beta, \\mu)$ is in general nonsmooth with respect to the spatial parameter $\\mu$ (e.g., rotation)."}, {"title": "Piecewise linear formulation", "content": "As mentioned above, the pixel value function $G_{u,v}(\\kappa)$ at location (u,v) is generally nonlinear and nonsmooth with respect to the transformation parameters \u03ba. This is one source of difficulty for solving the verification problem (1). In this section, we introduce a new convex relaxation method to derive tight over-approximations of $G_{u,v} (\\kappa)$.\nDeriving an interval bound for each pixel (u,v), i.e., $L_{u,v} \\leq G_{u,v}(\\kappa) \\leq U_{u,v}$, for all $\\kappa \\in B$ and lower and upper bounds $L_{u,v}, U_{u,v} \\in \\mathbb{R}$, is arguably the simplest way to get a convex relaxation [24, 33]. However, even a small geometric transformation can lead to a large interval bound, making this approach too loose for effective verification.\nThis naive interval bound approach has been extended in [2], where linear lower and upper bounds were used for each pixel value, i.e.,\n$\\omega^T \\kappa + b \\leq G_{u,v}(\\kappa) \\leq \\overline{\\omega}^T \\kappa + \\overline{b}, \\forall \\kappa\\in B.$\n(3)\nThe linear bounds (3), however, can be still too loose to approximate the nonlinear function $G_{u,v}(\\kappa)$ (see Figure 1 for illustration). Our key idea is to use piecewise linear bounds to approximate the pixel values:\n$\\max_{j=1,...,q}\\{\\omega_j^T\\kappa + b_j\\} \\leq G_{u,v}(\\kappa) \\leq \\min_{j=1,...,q} \\{\\overline{\\omega}_j^T\\kappa + \\overline{b}_j\\} ,$\n(4)\n$\\forall \\kappa\\in B$, where q is the number of piecewise segments, $\\omega_j \\in \\mathbb{R}^d,b_j \\in \\mathbb{R}, j = 1,..., q$ define the piecewise linear lower bound, and $\\overline{\\omega}_j \\in \\mathbb{R}^d, \\overline{b}_j \\in \\mathbb{R}, j = 1, ..., q$ define the piecewise linear upper bound. We remark that the pixel values constrained by (4) form a convex set. Furthermore, our approach can include the strategies in [2, 33] as special cases. Employing the relative constraints among the piecewise segments will result in a tighter set.\nFor each pixel value, we would like to derive optimal and sound piecewise linear bounds by minimizing the approximation error. Specifically, we aim to compute the lower bound via\n$\\min_{\\omega_j,b_j,j=1,...,q} \\int_B (G_{u,v} (\\kappa) - (\\max_{j=1,...,q} \\{\\omega_j^T\\kappa + b_j\\})) d\\kappa$\ns.t.$\\max_{j=1,...,q} \\{\\omega_j^T\\kappa + b_j\\} \\leq G_{u,v}(\\kappa), \\forall \\kappa\\in B.$\n(5)\nComputing the upper bound for (4) is similar. This optimisation problem (5) is highly nontrivial to solve since the integral cost function is hard to evaluate due to the nonlinearity of $G_{u,v} (\\kappa)$. Motivated by [2], we first sample the transformation parameter $\\kappa_i$ from B to obtain the sampled pixel values $G_{u,v} (\\kappa_i)$, and then solve a sampled version of (5). The resulting piecewise bound is guaranteed to be sound on the sampling points $\\kappa_i \\in B$ but could be unsound on non-sampled points. To derive a final sound piecewise bounds for $G_{u,v}(\\kappa)$, we bound the maximum violation over the entire B using a branch-and-bound Lipschitz optimisation procedure."}, {"title": "Linear optimisation based on sampling points", "content": "Here, we first randomly select N transformation parameters $\\kappa_i \\in B$, $i = 1,..., N$, to obtain a sampled version of (5) as follows\n$\\beta^* := \\min_{\\omega_j,b_j,j=1,...,q} \\frac{1}{N} \\sum_{i=1,...,N} (G_{u,v} (\\kappa_i) - ( \\max_{j=1,...,q} \\{\\omega_j^T\\kappa_i + b_j\\}))$\nsubject to $\\max_{j=1,...,q} \\{\\omega_j^T\\kappa_i + b_j\\} \\leq G_{u,v}(\\kappa_i), i = 1,...,N.$\n(6)\nWe denote the optimal cost value of (6) as $ \\beta^* $. In (6), the number of piecewise linear segments q is fixed a priori. Still, problem (6) is nontrivial to solve jointly for all piecewise segments $\\omega_j, b_j, j = 1, ..., q$ unless q = 1 (where (6) is reduced to a single linear program). One difficulty is to determine the effective domain of each piecewise linear segment.\nTo alleviate this, we propose to split the whole domain B into q sub-domains $B_1,..., B_q$, and then optimize each piecewise linear segment over $B_j, j = 1, ..., q$, individually. We then use the following q independent linear programs to approximate the solution to (6):\n$\\beta_j := \\min_{\\omega_j,b_j} \\frac{1}{N} \\sum_{\\kappa_i \\in B_j} (G_{u,v} (\\kappa_i) \u2013 (\\omega^T \\kappa_i + b_j))$\n(7)\nsubject to $\\omega^T \\kappa_i + b_j < G_{u,v}(\\kappa_i), i = 1, ..., N,$\nfor j = 1,..., q. Note that in (7), we minimise the approximation error over only the sample points within a given domain $B_j$; however, we force each segment to satisfy the constraints at every sample point $\\kappa_i\\in B$ over the whole domain.\nWe have the following result for the quality of the solution from (7).\nProposition 1. Given any subdomains $B_j, j = 1,..., q$, the optimal solutions $\\omega_j, b_j, j = 1, . . ., q$, to (7) are suboptimal to (6), i.e., $\\sum_{j=1}^{q}\\beta_j \\geq \\beta^*$. There exists a set of subdomains $B_j, j = 1, ..., q$, such that the optimal solutions to (6) and (7) are identical, i.e., $\\sum_{j=1}^{q}\\beta_j = \\beta^*$.\nProof. Consider the piecewise linear function in the objective function (6). Let $B_j, j = 1,..., q$ be the effective piecewise domain of the jth segment, i.e.,\n$\\max_{\\substack{j=1,...,q}} \\{\\omega^T\\kappa_i + b_j\\} = \\begin{cases} \\omega^T\\kappa_i + b_1, \\quad if \\quad \\kappa_i \\in B_1 \\\\ : \\\\ \\omega^T\\kappa_i + b_q, \\quad if \\quad \\kappa_i \\in B_q. \\end{cases}$\n(8)\nThen, the objective function (6) can be equivalently written into\n$\\frac{1}{N}\\sum_{i=1,...,N} (G_{u,v} (\\kappa_i) - (\\max_{\\substack{j=1,...,q}} \\{\\omega^T\\kappa_i + b_j\\}))$\n$= \\frac{1}{N}(\\sum_{j=1}^q (\\sum_{\\kappa_i \\in B_j}G_{u,v} (\\kappa_i) - (\\omega^T\\kappa_i + b_j)))$\nTherefore, (6) is equivalent to\n$\\min_{\\omega_j,b_j, B_j,j=1,...,9} \\quad \\sum_{j=1}^q \\frac{1}{N} \\sum_{\\kappa_i \\in B_j} (G_{u,v} (\\kappa_i) - (\\omega^T\\kappa_i + b_j))$\n(9)\ns.t.\\omega^T\\kappa_i + b_j \\leq G_{u,v}(\\kappa_i), i = 1,..., N, j = 1, . . . q.\nNote that the piecewise domains $B_j$ are determined by the linear segments $\\omega_j, b_j, j = 1, ..., q$ implicitly in (8). We need to simultaneously optimize the choices of $B_j$ in (10), making it computationally hard to solve.\nA suboptimal solution for (10) is to a priori fix the effective domain $B_j$ and optimize over $\\omega_j, b_j, j = 1, . . ., q$ only, i.e.,\n$\\beta := \\sum_{j=1}^q \\min_{\\omega_j,b_j} \\frac{1}{N} \\sum_{\\kappa_i \\in B_j} (G_{u,v} (\\kappa_i) \u2013 (\\omega^T\\kappa_i + b_j))$\n(10)\ns.t.\\omega^T\\kappa_i + b_j \\leq G_{u,v}(\\kappa_i), i = 1,..., N, j = 1, . . . q,\nwhich is decoupled into q individually linear programs, $j = 1, . . ., q$\n$\\beta_j = \\min_{\\omega_j,b_j} \\frac{1}{N} \\sum_{\\kappa_i \\in B_j} (G_{u,v} (\\kappa_i) \u2013 (\\omega^T\\kappa_i + b_j))$\n(11)\nsubject to $\\omega^T \\kappa_i + b_j \\leq G_{u,v}(\\kappa_i), i = 1, ..., N.\nTherefore, it is clear that $ \\beta = \\sum_{j=1}^{q} \\beta_j \\geq \\beta^* $. On the other hand, suppose the optimal solution to (6) leads to the optimal effective domains $B_j, j = 1,...,q$ in (8). Then, using this set $B_j, j = 1,..., q$, the decoupled linear programs (11) are equivalent to (10) and (6).\nTo obtain a good solution (6), choosing the subdomains $B_j$ becomes essential. A uniform grid partition is one, naive choice. Another is to partition the subdomains based on the distribution of the sampling points $G_{u,v} (\\kappa_i)$. The details of the splitting procedure are provided in the appendix.\nRemark 1. (Explicit input splitting vs. piecewise linear constraints) We note that one can perform explicit input splitting $B_j, j = 1,..., q$, and verify each of them by solving (1) separately in order to certify the original large domain B. The main drawback of this explicit input splitting is that we need to call a verifier for each subdomain $B_j$ which can be hugely time consuming and not scalable. On the contrary, it only requires to solve multiple small linear programs (7) to derive our piece-wise linear constraints. Then, we only need to call a verifier once to solve the verification problem (1) over B. For tight verifiers, such as those mentioned in Section 2, this process is much more efficient than explicit input splitting."}, {"title": "Lipschitz, optimisation for obtaining sound piecewise linear bounds", "content": "The piecewise linear constraints from (7) are valid for the sampling points $\\kappa_i \\in B", "B": "n$\\xi^*_{u", "v}": "max_{\\kappa \\in B"}, "f_{u,v} (\\kappa),$\n(12)\nwhere $f_{u,v}(\\kappa) = \\max_{j=1,...,q}\\{\\omega_j^T\\kappa + b_j\\} \u2013 G_{u,v}(\\kappa)$. Then, we naturally have a sound piecewise linear lower bound as\n$\\max_{j=1,...,q}\\{\\omega_j^T\\kappa + b_j\\} - \\xi^*_{u,v} \\leq G_{u,v}(\\kappa), \\forall \\kappa \\in B.$\nHowever, computing the exact maximum $ \\xi^*_{u,v} $ is computationally hard due to the nonconvexity, nonlinearity and nonsmoothness of $f_{u,v} (\\kappa)$. Instead, given any $ \\epsilon > 0$, we can use a branch-and-bound Lipschitz optimisation procedure (see Algorithm 1) to find $ \\xi^*_{u,v} \\in \\mathbb{R}$ satisfying $ \\xi^*_{u,v} \\leq \\overline{\\xi_{u,v}} \\leq \\xi^*_{u,v} + \\epsilon$.\nTo establish the branch-and-bound Lipschitz optimisation procedure, we need to characterise the properties of the violation function $f_{u,v} (\\kappa)$.\nProposition 2. The violation function $f_{u,v} (\\kappa) := \\max_{j=1,...,q}\\{\\omega_j^T\\kappa + b_j\\} \u2013 G_{u,v}(\\kappa)$ is nonconvex, nonsmooth, and Lipschitz continuous over $B \\subset \\mathbb{R}^d$. Furthermore, there exist $L_m > 0, m = 1,..., d$, such that $\\forall \\kappa_1, \\kappa_2 \\in B$:\n$|f_{u,v} (\\kappa_1) - f_{u,v} (\\kappa_2)| \\leq \\sum_{m=1}^d L_m|\\kappa_1(m) \u2013 \\kappa_2(m)|.$\n(13)\nProof. The pixel value function is given by $G_{u,v} (\\kappa) := P_{\\alpha,\\beta} \\circ I \\circ T_{\\mu}^{-1} (u, v)$. We know that the spatial transformation $T_{\\mu}(u, v)$ and $P_{\\alpha,\\beta}$ are continuous and differentiable everywhere. The interpolation function I(u, v) is continuous everywhere, but it is only differentiable within each interpolation region and it can be nonsmooth on the boundary. Also, $T_{\\mu}(u, v)$ and $I(u, v)$ are generally nonconvex.\nIn addition, the piecewise linear function $\\max_{j=1,...,q}\\{\\omega_j^T\\kappa + b_j\\}$ is continuous but not differentiable everywhere. Therefore, the violation function $f_{u,v} (\\kappa)$ is nonconvex and nonsmooth in general. Finally, all the functions $T_{\\mu}(u,v), P_{\\alpha,\\beta}, I(u, v)$ and $\\max_{j=1,...,q}\\{\\omega^T\\kappa + b_j\\}$ are Lipschitz continuous, so is the violation function $f_{u,v} (\\kappa)$. Thus, there exist $L_m > 0, m = 1,...,d$, such that (13) holds.\nThe properties of the violation function $f_{u,v} (\\kappa)$ in Proposition 2 are directly inherited from nonconvexity and nonsmoothness of the interpolation function I(u, v). The Lipschitz continuity is also from the interpolation function and the piecewise linear function.\nWith the information of $L_m$ in (13), we are ready to get a lower and an upper bound for $ \\xi^*_{u,v}$ upon evaluating the function at any point $\\kappa_0 \\in B$:\n$f_{u,v} (\\kappa_0) \\leq \\xi^*_{u,v}$\n$= \\max_{\\kappa \\in B} f_{u,v} (\\kappa)$\n$\\leq \\max_{\\kappa \\in B} (f_{u,v} (\\kappa_0) + \\sum_{m=1}^d L_m|\\kappa(m) \u2013 \\kappa_0(m)|)$\n(14)\n$<f_{u,v} (\\kappa_0) + \\sum_{m=1}^d L_mh_m,$\nwhere $h_m > 0$ denotes the difference of the lower and upper bound in each box constraint of B. These lower and upper bounds (14) are useful in the branch-and-bound procedure.\nStill, we need estimate the Lipschitz constant $L_m$ in (13). In our work, we show how to estimate the constant $L_m$ based on the gradient of $f_{u,v} (\\kappa)$ whenever it is differentiable (note that $f_{u,v} (\\kappa)$ is not differentiable everywhere)\nProposition 3. Let Diff(B) be the subset of B where $f_{u,v}(\\kappa)$ is differentiable. Then, the Lipschitz constants in (13) can be chosen as $L_m = Sup_{\\kappa\\in Diff(B)} |\\nabla f_{u,v}(\\kappa)e_m|$, where $e_m \\in \\mathbb{R}^d$ is a basis vector with only the m-th element being one and the rest being zero.\nProof. This proof is motivated by [22"], "f": "mathbb{R"}, {"h": "mathbb{R} \\rightarrow \\mathbb{R}$ as $h(t) = f_{u,v} (\\kappa_1 + t(\\kappa_2 \u2013 \\kappa_1))$.\nSince $f_{u,v} (\\kappa)$ is Lipschitz continuous in B, it is clear that h(t) is Lipschitz continuous on the interval [0, 1]. Thus, by Rademacher's Theorem, h(t) is differentiable everywhere except for a set of measure zero.\nWe can further define a Lebesgue integrable function g(t) that equal to $h'(t)$ almost everywhere as follows\n$g(t) = \\begin{cases} h'(t), \\quad if \\quad h'(t) \\quad exists \\\\ \\sup_{s \\in [0,1]} h'(s), \\quad otherwise \\end{cases}$\nNote that if $f(K)$ is differentiable at some point, we have\n$h'(t) = \\nabla f_{u,v} (\\kappa_1 + t(\\kappa_2 \u2013 \\kappa_1))^T (\\kappa_2 \u2014 \\kappa_1).$\nThen we have the following inequalities\n$|f_{u,v} (\\kappa_1) - f_{u,v} (\\kappa_2)| = |h(1) \u2013 h(0)| = |\\int_0^1g(t)dt|$\n$\\leq \\int_0^1 |g(t)| dt$\n$\\leq \\int_0^1 \\sup_{s \\in [0,1]} |h'(s)| dt = \\sup_{s \\in [0,1]} |h'(s)|$\n$\\leq \\sup_{s \\in [0,1]} |\\nabla f_{u,v} (\\kappa) (\\kappa_2-\\kappa_1)|.$\nFurthermore, considering the inequality in (15) [22, Lemma 3], we have\n$|f_{u,v} (\\kappa_1) - f_{u,v} (\\kappa_2)| \\leq \\sup_{\\kappa \\in Diff(B)} |\\nabla f_{u,v} (\\kappa) (\\kappa_2-\\kappa_1)|$\n$\\leq \\sum_{m=1}^d \\sup_{\\kappa \\in Diff(B)} |\\nabla f_{u,v} (\\kappa)e_m ||\\kappa_2(\u0442) \u2013 \\kappa_1(\u0422)|$\nwhere $e_m \\in \\mathbb{R}^d$ is a basis vector with only the m-th element being one and the rest being zero. Therefore, the Lipschitz constants in (13) can be chosen as $L_m = SUP_{\\kappa\\in Diff(B)} |\\nabla f_{u,v}(\\kappa)^Te_m|$.\nMaximum directional gradient. To bound the maximum violation $ \\overline{\\xi_{u,v}} $ in (12) using (14), we need to estimate the constant $L_m$, and Proposition 3 requires us to calculate the maximum directional gradient $\\nabla_j f e_m$. Each component of $\\nabla_j f $ varies independently with respect to any constituent of the transformation composition, $T_{\\mu}(\\kappa_m), m = 1,..., d$. Each $L_m$ depends only on a transformation, $T_{\\mu}$, and interpolation, $Zu,v$. The only component that is not differentiable everywhere in the parameter space \u043a \u2208 B, is interpolation $\\tau_{\\mu,v}(x, y)$ - this due to it being disjoint across interpolation regions. We overcome this by calculating the interpolation gradient, $\\nabla_{x,y} Zu,v$, separately in each interpolation region, and taking the maximum interval of gradients from the union, $[\\nabla_{x,y}I_{min}, \\nabla_{x,y}I_{max}] = [min(\\cup_{k=1,...,n}\\nabla_{x,y}I_k), max(\\cup_{k="}]