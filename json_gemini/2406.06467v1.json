{"title": "How Far Can Transformers Reason?\nThe Locality Barrier and Inductive Scratchpad", "authors": ["Emmanuel Abbe", "Samy Bengio", "Aryo Lotfi", "Colin Sandon", "Omid Saremi"], "abstract": "Can Transformers predict new syllogisms by composing established ones? More generally, what\ntype of targets can be learned by such models from scratch? Recent works show that Transformers\ncan be Turing-complete in terms of expressivity, but this does not address the learnability objective.\nThis paper puts forward the notion of distribution locality to capture when weak learning is efficiently\nachievable by regular Transformers, where the locality measures the least number of tokens required in\naddition to the tokens histogram to correlate nontrivially with the target. As shown experimentally and\ntheoretically under additional assumptions, distributions with high locality cannot be learned efficiently.\nIn particular, syllogisms cannot be composed on long chains. Furthermore, we show that (i) an agnostic\nscratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the\nlocality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the\nout-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks.", "sections": [{"title": "1 Introduction", "content": "Transformers [1] have proved to have strong learning capabilities, in particular in applications with large\namounts of text, image, or audio data [2, 3]. Some reasoning capabilities are also notable in these settings,\nhowever, the picture deteriorates when the target complexity increases, such as in tasks involving more\nadvanced forms of \u2018reasoning' [4, 5, 6, 7, 8, 9, 10]. While reasoning is present at all levels of learning, it is\npushed to a higher level in tasks such as logic or mathematics, where 'learning by seeing enough representative\nexamples' is precluded by the more combinatorial nature of the task. For such tasks, combining learned\nconcepts in order to extrapolate seems necessary, as for the length generalization problem [11]. Current\nTransformer-based models exhibit difficulties learning at scale on such tasks. Can we understand why and\nwhat is missing? We start with a specific motivational example before expanding the discussion to more\ngeneral tasks."}, {"title": "1.1 Syllogisms composition", "content": "Reasoning relates to the process of inferring new knowledge by composing efficiently some prior knowledge. A\nbasic notion of reasoning is syllogism composition, e.g., inferring $a \\Rightarrow c$ from $a \\Rightarrow b$ and $b\\Rightarrow c$. For instance,\none may be given a set of implications:\ntask 1 has priority over task 2\ntask 1 has priority over task 3\n$x^2 > 3$  $\\Rightarrow$ $(x - 1)(x + 1) > 1$\n$x > 2$  $\\Rightarrow$ $x > 2$\ntask 4 has priority over task 1\ntask 1 has priority over task 5\n$4x > 17$  $\\Rightarrow$ $x > 2$\n$x > 2$  $\\Rightarrow$ $x^2 > 2^4$\nand without additional background information, one would like to know using logic whether\ntask 3 has priority over task 5?\n$4x > 17$  $\\Rightarrow$ $x^2 > 3$."}, {"title": "1.2 Hardness of long compositions", "content": "Consider the previous syllogism composition task where implications are drawn on a graph with 24 edges\ndrawn randomly over 24 vertices. Picking vertices at distances 1 to 4 for the connected case and picking\ndisconnected vertices uniformly at random lets a Transformer achieve a test accuracy of more than 80% after\nabout 2K iterations. However, does this mean that the model has learned to compose syllogisms, or has it\nfound shortcuts, e.g., based on node degrees, to guess the implications often enough? In Appendix B.1, we\nprovide empirical evidence supporting the latter. Motivated by this issue and to preclude spurious correlations,\nwe consider the following distribution.\nDefinition 1. For $n \\geq 1$, consider the binary classification task with equiprobable classes defined by\n1. Class 1: a graph uniformly drawn on 2n vertices with two disjoint cycles of length n and a pair of\nvertices in disjoint cycles queried for path;\n2. Class 2: a graph uniformly drawn on 2n vertices with one cycle of length 2n and a pair of vertices at\ndistance n queried for path.\nThe input of this task is the graph edge set with the queried vertices. The label is 0 if the two queried vertices\nare not connected (Class 1) and 1 if they are (Class 2). We refer to this task as the \u2018cycle task\u2019. See Figure 1a\nfor an illustration."}, {"title": "1.3 Hardness of global reasoning", "content": "As discussed previously, the cycle task appears to be challenging for Transformers as it requires some global\nreasoning. Other tasks such as subset parities exhibit the same challenge. However the latter can be proved\nto be not efficiently learnable by various regular neural networks and noisy gradient descent, as one can get\nexplicitly a class of functions (through orbit arguments [12, 13]) that has large statistical dimension [14] or\nlow cross-predictability [12, 15] (see Appendix A.2). For the cycle task, we have a single distribution, and it\nis unclear how to use the invariances of Transformers to get arguments as in [12, 13], as the input distribution\nis not invariant under the symmetries of the model. We thus would like to develop a more general complexity\nmeasure that unifies why such tasks are hard for Transformer-like models and that formalizes the notion of\n'local reasoning barrier' when models are trained from scratch. We also would like to understand how the"}, {"title": "1.4 Our contributions", "content": "We provide the following contributions:\n\u2022 We introduce the notion of distribution locality in Definition 2 to capture when weak learning is efficiently\nachievable by Transformers. The distribution locality measures the least number of tokens required in\naddition to the token histogram to correlate nontrivially with the target; it is also related in Lemma 6 to\ncorrelations with NC\u00ba circuits, showing the contrast between learnability and expressivity governed by\nTC1 [16]. It is an explicit measure that applies to a data distribution without requiring an orbit argument\nto infer a class of distributions [12, 13], giving a tight proxy for models like Transformers. We provide the\nfollowing results based on the distribution locality:\n\u2013 A general conjecture (Conjecture 1), backed by experimental results, that claims efficient weak learning\nis achievable by a regular Transformer if and only if the distribution locality is constant.\n\u2013 A theorem (Theorem 1) that proves the negative side of the above conjecture, the locality barrier, in\nthe instance of a variant of the cycle task under certain technical assumptions. (The cycle task is also\nput forward in the paper as a simple benchmark to test the global reasoning capabilities of models.)\n\u2022 We then switch to the use of 'scratchpads' to help with the locality barrier:\n\u2013 Agnostic scratchpad: we extend Theorem 1 to cases where a polynomial-size scratchpad is used by the\nTransformer, without any supervision of the scratchpad. I.e., the scratchpad gives additional memory\nspace for the Transformer to compute intermediate steps. This shows that efficient weak learning is\nstill not possible with such an agnostic scratchpad if the locality is non-constant. An educated guess\nabout what to learn in the scratchpad based on some target knowledge is thus required.\n\u2013 Educated scratchpad: we generalize the measure of locality to the 'autoregressive locality' to quantify\nwhen an educated scratchpad is able to break the locality of a task with subtasks of lower locality.\nWe give experimental results showing that educated scratchpads with constant autoregressive locality\nallow Transformers to efficiently learn tasks that may originally have high locality. This gives a way\nto measure how useful a scratchpad can be to break a target into easier sub-targets.\n\u2013 We introduce the notion of inductive scratchpad, a type of educated scratchpad that exploits 'induction'"}, {"title": "2 Results on the local reasoning barrier", "content": "Prior literature. Much work in the learning literature has been devoted to obtaining complexity measures for\nthe sample/time complexity of learning. The largest portion is devoted to target classes in PAC settings, e.g.,\nwith the VC dimension measures [18], and some to statistical query (SQ) settings with the statistical dimension\nmeasures [14, 19]. Here, we are however interested in measures that are relevant to (1) regular Transformers\n(or related models) trained by (S)GD, and (2) data distribution fixed by a task. Some recent literature has\nstudied complexity measures for (S) GD-trained neural networks. Various settings and measures have been\nused, such as the noise sensitivity [20, 6, 21], the cross-predictability [12, 15], the NTK alignment [22, 23],\nthe INAL [24], the G-alignment [13], the information and generative exponents [25, 26, 27] and the leap [28];\nwe refer to Appendix A.2 for discussions on these.\nHowever, despite this significant body of work, finding a simple measure giving a tight proxy for Transformer\nweak learning (i.e., the first non-trivial learning requirement) on a given data distribution, remains unsettled.\nWe next propose such a measure."}, {"title": "2.1 Defining locality and auto-regressive locality", "content": "We define now the notion of distribution locality, which in turn will quantify the notion of locality (or local\nreasoning) barrier.\nDefinition 2. (Distribution locality) For (a sequence of) distributions D on $A^n \\times A$, where A is a finite\nalphabet set of poly(n)-cardinality, define Loc(D) as the smallest number of variables $k \\in [n]$ for which there\nexists $S, |S| = k$ such that\n$I(X[S], P_X; Y) = n^{-0(1)}$\nwhere $(X,Y) \\sim D$ and $P_X$ is the empirical measure of X (i.e., the histogram of tokens in X).\nRemark 1. The locality loc(D) of a data distribution measures the least number of input tokens to attend to\nin order to correlate non-trivially with the label when also given the histogram of the tokens. The specific choice\nof the mutual information in the definition is not crucial, but one must use a proper measure of dependency\n(i.e., not just linear correlations), and the mutual information can have convenient chain rule properties.\nThe definition can be related to correlations with NC\u00ba circuits (besides for the histogram requirement, see\nLemma 6) and also to low-degree polynomial testing, except that it is more general than the latter as it\napplies to arbitrary token space (without requiring polynomial definitions). Finally, we require the locality\nto achieve an inverse-polynomial mutual information, the weakest form relevant to weak learning with an\ninverse-polynomial edge, but one may naturally define the stronger notion with a mutual information of $\\Omega(1)$.\nWe now define the locality in the autoregressive setting.\nDefinition 3. (Distribution locality in autoregressive setting) For D on $A^n \\times A^m$, define Loc(D) as\nthe smallest integer k for which there exists $S_1,..., S_m, |S_t| \\leq k$ for all $t \\in [m]$, such that\n$I((X,Y_{<t})[S_t], P_{X,Y_{<t}}; Y_t) = n^{-0(1)}$\nwhere $(X,Y) \\sim D$ and $P_{X,Y_{<t}}$ is the empirical measure of $(X,Y_{<t})$.\nIn the auto-regressive setting, the locality is mostly relevant when weak learning gives strong learning, in\norder to let the scratchpad learn each step."}, {"title": "2.2 Transformers require low locality: formal results", "content": "Definition 4. A neural network of input dimension n (i.e., a directed acyclic graph with n inputs) and depth\nd (i.e., the longest path length from input to output) is\n1. T-regular if it is a Transformer (e.g., [1, 29]) of polynomial size with Normal Gaussian i.i.d. positional\nembeddings, Normal Gaussian i.i.d. weights, and bidirectional attention.\n2. T-regular with s-scratchpad if we have a constant sized token alphabet $\\Sigma$ and a T-regular Transformer\nthat takes an m-sequence in $\\Sigma$ and outputs a probability mass function on $\\Sigma$ (with well-behaved\nsoftmax). To compute the value of this net and scratchpad on an input $X \\in \\Sigma^n$ we set $X_n = X$ and\nthen for each $m \\in \\{n, ..., n + s - 1\\}$ draw $x_{m+1}$ from the probability distribution represented by the\nnet's value on $X_m$ and set $X_{m+1} = X_m \\copyright x_{m+1}$ (concatenation). Then, we consider $x_{n+s}$ to be the\noverall output of the net.\nRemark 2. In this paper we focus on learnability via descent algorithms, but for the simpler question of\nexpressivity, one wants to know whether there is any choice of parameters for which a Transformer can\ncompute a target function (with limits to how precisely it can record values).\n(i) In [30], the expressivity of Transformers with constant alphabet size and values recorded to inverse-\npolynomial accuracy is investigated. It is shown that such a Transformer of constant depth was limited to\ncomputing functions in TC\u00ba. Conversely, it also showed that for any TC\u00ba function there is a constant-depth\nTransformer and instruction string such that when the Transformer is given the instruction string and x as\nits input it computes the function on x. The same technique would extend to show that with logarithmic depth,\none can reach TC\u00b9 (which includes connectivity tasks).\n(ii) In [16], well-behaved Transformers with a scratchpad are considered. It is shown that a Transformer\nwith a scratchpad of logarithmic length is limited to computing functions in logspace. We tighten this to TC\nin Lemma 5. On the other hand, [16] also shows that any function in P is computable by a Transformer with\na scratchpad of polynomial length.\nWe now state the general conjecture putting forward the locality barrier for learning."}, {"title": "2.3 Agnostic scratchpads cannot break the locality", "content": "Conjecture 2. Consider training a T-regular net with an s-scratchpad to learn $P_{X,Y}$ on a constant-sized\nalphabet by means of the following SGD algorithm. At each timestep, we draw a random sample (X,Y) and\ncompute a value for the net with scratchpad on X. Let S be the resulting scratchpad and for each i < s\nand each $\\sigma \\neq S_i$, define an alternative scratchpad by setting the first i - 1 entries of this scratchpad equal\nto those of S, setting its i-th entry to $\\sigma$, and using the net to compute the rest of its values. Then, regard\nthe loss associated with setting the ith entry of the scratchpad to $\\sigma$ as the loss resulting from the associated\nscratchpad, and use the resulting gradient to carry out one step of SGD. Then if $P_X$ is a well-behaved\nprobability distribution, $P_{X,Y}$ is efficiently weakly learnable by a T-regular neural network with a scratchpad\nif and only if $P_{X,Y}$ has constant locality."}, {"title": "3 Scratchpads to break the locality", "content": "Prior literature. It has been shown that training Transformers with the intermediate steps required to solve\na problem can enhance learning. This idea is usually referred to as providing models with a scratchpad [32].\nThe improved performance due to scratchpads has been reported on a variety of tasks including mathematical\ntasks and programming state evaluation [32, 11, 17]. See Appendix A for further references."}, {"title": "3.1 Educated scratchpad", "content": "We now provide a quantitative understanding of how the scratchpad can help with the notion of locality in\nthe autoregressive setting (Definition 3). Assume that we want to learn target $Y \\in A$ from input $X \\in A^n$\nsuch that $(X, Y) \\sim D$ and Loc(D) is high. If one can design intermediate targets $Y_1, ..., Y_k \\in A$ such that\n$Y_k = Y$ and the sequence $(X, Y_{<i}) \\rightarrow Y_i$ has low locality according to Definition 3, then one can expect to\nlearn each step of the sequence efficiently and thus the target at the end. In this case, the intermediate targets\ngive the 'educated scratchpad' (see Figure 3 for an illustration). We now show how designing low-locality\nscratchpads can help with learning by focusing on two examples: parity functions and the cycle task.\nResults for learning parities. Consider learning parity function $y = f(x_1,...,x_n) = x_1x_2...x_k$ where\n$x_1,...,x_n$ are drawn i.i.d. in $\\{ \\pm 1\\}$ with uniform distribution. For $k \\leq \\frac{n}{2}$, one can easily check that the\nlocality of this task is k as any k \u2212 1 coordinates are independent of the output and the histogram of the\ntokens does not help. Parity functions are known to be hard to learn [12]. More specifically, it has been\npreviously shown that as k increases the parity task becomes harder to learn to the point that parity of degree\n$k \\wedge (n - k) = \\omega(1)$ cannot be learned in poly(n) time with standard poly-size neural networks under standard\ntraining assumptions [12]. Note that this is consistent with our results, as the locality is non-constant."}, {"title": "3.2 Inductive Scratchpads", "content": "As discussed previously, scratchpads can break the local reasoning barrier with appropriate mid-steps. In this\npart, however, we show that fully educated scratchpads can be sensitive to the number of reasoning steps,\ntranslating into poor out-of-distribution (OOD) generalization. As a remedy, we put forward the concept of\ninductive scratchpad which applies to a variety of reasoning tasks as in previous sections."}, {"title": "3.2.1 Educated scratchpad can overfit in-distribution samples", "content": "Consider the cycle task with 40 nodes. For the test distribution, we use the normal version of the cycle task,\ni.e., either two cycles of size 20 and the nodes are not connected or a single cycle of size 40 where the distance\nbetween the query nodes is 20. For the train distribution, we keep the same number of nodes and edges (so\nthe model does not need to rely on new positional embeddings for the input) but break the cycles to have\nuneven lengths: (1) a cycle of size 10 and a cycle of size 30 when the two nodes are not connected (the source\nquery node is always in the cycle of size 10) or (2) a cycle of size 40 where the nodes are at distance 10. Thus,\nin general, we always have 40 nodes/edges in the graphs. However, the length of the DFS path (i.e., number\nof reasoning steps) is 10 at training and 20 at test. We trained our model on this version of the task with the\nDFS scratchpad. The results are shown in Figure 4b. We observe that the model quickly achieves perfect\naccuracy on the training distribution, yet, it fails to generalize OOD as the model overfits the scratchpad\nlength. In the next part, we introduce the notion of inductive scratchpad to fix this problem."}, {"title": "3.2.2 Inductive scratchpad: definition and experimental results", "content": "In a large class of reasoning tasks, one can iteratively apply an operation to some state variable (e.g., a state\narray) to compute the output. This applies in particular to numerous graph algorithms (e.g., shortest path\nalgorithms such as BFS or Dijkstra's algorithm), optimization algorithms (such as genetic algorithms or\ngradient descent), and arithmetic tasks.\nDefinition 5 (Inductive tasks). Let Q be the question (input). We say that a task can be solved inductively\nwhen there is an induction function (or a state transition function) g such that\n$s^{[1]} = g(Q,0)$, $s^{[2]} = g(Q, s^{[1]})$, ..., $s^{[k]} = g(Q, s^{[k - 1]})$, \nwhere $s^{[1]}, . . ., s^{[k]}$ are the steps (or states) that are computed inductively. For example, the steps/states could\nbe an array or the state of an automata that is being updated. Note that the termination is determined by\nthe state. In the context of Transformers, one can use the generation of the end of sequence token <EOS> to\nterminate.\nInductive tasks with a fully educated scratchpad can overfit proofs. The fully educated scratchpad\nfor the question Q as input would be $s [1];s[2];...;s[k] <EOS>$, where the token <EOS> ends the generation.\nHowever, this method may not fully utilize the fact that each state is only generated from the last state by\napplying the same (set of) operation(s). In particular, s[k] typically attends to all of the previous states.\nFurther, the model may not be able to increase the number of induction steps beyond what it has seen during\ntraining, as shown in Figure 4b for the cycle task.\nNow we show that by using attention masking and reindexing the positions of the tokens, one can promote\nthe desired 'inductive' behavior. We call this the inductive scratchpad. As three showcases, we demonstrate\nthat the inductive scratchpad can improve OOD generalization on the cycle task and length generalization on\nparity and addition tasks.\nInductive scratchpad implementation. The inductive scratchpad for an inductive task is similar\nin format to the fully educated scratchpad but it has the following modifications: (1) tokens: two new\nspecial tokens are used: the <START> token which separates the question from the intermediate states and"}, {"title": "4 Conclusion", "content": "This paper shows that for the learning objective and in contrast to expressivity results, Transformers trained\nfrom scratch have a 'local reasoning barrier' quantified by the distribution locality: if the target requires a\nglobal composition of the inputs, Transformers will not efficiently learn. The locality measure has a simpler\nform and broader applicability range than prior measures as discussed in Appendix A.2, it also has tighter\napplicability for Transformer. The measure is currently defined for weak learning (inverse-polynomial or\nconstant edge), and a natural next step is to consider stronger learning requirements with 'locality leap', e.g.,\nexpanding the current work in the direction of [28]. Investigating the role of curriculum learning is another\nnatural direction and we provide preliminary results here in Appendix B.2.\nThe locality is also defined in the autoregressive setting, to better quantify when scratchpads can break\ntargets into easier sub-targets. Two negative results are shown for scratchpads: agnostic scratchpads still\nsuffer from the locality barrier, and fully educated scratchpads can have poor OOD generalization. This\nmotivates the introduction of the inductive scratchpad.\nThe inductive scratchpad can be used for a broad range of reasoning/algorithmic tasks and can easily be\nintegrated into Transformers. The inductive scratchpad is independent of the number of reasoning steps\nsince the model only learns the induction function. Consequently, the model naturally generalizes to inputs\nrequiring different numbers of reasoning steps. This gives improvements of OOD/length generalization for\nthe cycle task (Figure 4b), parity (Figure 5a), and addition (Figure 5b).\nAnother interesting aspect is whether the model can use an inductive behavior on new tasks if it was pretrained\non prior inductive tasks. Note that the inductive behavior of the inductive scratchpad is only determined by\ntwo special tokens. Thus, in principle, models can generate these special tokens and go into the inductive\nstate for other tasks if pretrained on inductive data. We leave the general investigations of pretrained models\nand the automated learning of more general scratchpads, capitalizing on the measures defined here, to future\nworks."}, {"title": "A Further related literature", "content": "Reasoning vs. memorization. The performance of large language models and Transformers has been\nshown to improve as the model size, amount of data, and training time are increased [34, 3]. Furthermore,\nit has been shown that Transformers are prone to memorizing the training data [35, 36, 37, 38, 39]. Thus,\nit is natural to ask whether Transformers mostly rely on memorization or if they in fact use memorization\nalong with a significant degree of reasoning. Reasoning is also essential in solving more challenging tasks\nsuch as planning and mathematics. In recent years, the reasoning power of Transformers has been studied on\nsynthetic reasoning tasks such as PVR [6], LEGO [40], algorithmic tasks such as CLRS [9], and more natural\nsettings such as solving math problems [4, 5]. Note that reasoning tasks often have a combinatorial nature\nand thus an exponentially large input space. Moreover, this input space may not lie on a low-dimensional\nmanifold which makes memorization approaches ineffective. For example, in arithmetic, all digit combinations\nare often possible and also change the result significantly, whereas, in text, only specific combinations of\nwords are valid and besides changing a single word often does not change the meaning of the text drastically.\nAnother important criterion for reasoning is the ability to generalize to out-of-distribution (OOD) samples\nand in-particular length generalization [41, 42, 43, 11, 44]. It has been observed that simple tasks such as\naddition, multiplication, and parity are generally hard for Transformers both in the in-distribution setting\nand more notably in the length generalization setting [11, 45].\nPositional embeddings. An important component of length generalization is the positional embedding\nof the model. It has been shown that various forms of relative positional embedding [46, 47] including\nT5's relative positional embedding [48], ALiBi [49], and Rotary [50] can yield better length generalization\nperformance than absolute positional embedding [51, 52, 53]. In particular, the recent work of Kazemnejad et\nal. [53] evaluates different positional embeddings for decoder-only Transformers on a benchmark of reasoning\ntasks where it is shown that relative positional embeddings perform better than absolute ones. Interestingly, it\nis also shown that decoder-only Transformers with no positional embedding (NoPE) can still encode positional\ninformation and in fact have better length generalization performance than absolute/relative positional\nembeddings on certain tasks. The inductive scratchpad put forward in this paper also reindexes the position\nof the tokens for each new state. Using this technique, the inductive scratchpad circumvents the need for\nlearning new positional information as the number of reasoning steps (i.e., the number of states) increases.\nNevertheless, the inductive scratchpad can be used with both absolute and relative positional embeddings.\nWe further discuss the relation between the inductive scratchpad and relative positional embeddings in\nAppendix C.3."}, {"title": "Architectural modifications", "content": "Several architectural modifications have also been proposed that can\npotentially enhance the reasoning ability of Transformers on certain tasks. A line of work focuses on adding\nrecurrent structures to Transformers [54, 55, 56]. In particular, Universal Transformers [54], share the weights\nbetween Transformer layers and also have a variable depth (similar to adaptive computation time [57])\nimplemented using a dynamic halting mechanism. Generally scratchpads and in particular the inductive\nscratchpad also share some recurrent flavor since when each token of the scratchpad is generated, the whole\ninput and the scratchpad tokens are given to the Transformer again. The differences are however quite\nsignificant both on the side of supervision (e.g., scratchpads are usually supervised) and halting mechanism\n(e.g., generation of <EOS> token ends the process for scratchpad models). Another relevant architecture is the\nneural data router [58] where the weights are shared among transformer layers and also copying gates and\na special attention format, geometric attention, are used. The copying gate allows some tokens not to be\ntransformed and instead just be copied at certain transformer layers while geometric attention induces a\nrecency bias. The neural data router is shown to generalize to samples with up to three more operations than\nwhat is seen during training for simple arithmetic tasks and ListOps dataset [59]. We note that our inductive\nscratchpad can also generalize to samples with a higher number of operations/reasoning steps. Note that the\nneural data router approach requires the model's depth to be (at least) equal to the maximum number of\noperations, while our model does not have such limitations and can thus be trained and tested on samples\nwith larger numbers of operations than the neural data router approach. Moreover, intermediate steps are"}, {"title": "Scratchpad and chain-of-thought", "content": "Nye et al. [32] put forward the idea of using scratchpads by\nshowing that training Transformers on the intermediate steps in addition to the final answer can improve\ntheir performance on different tasks such as executing Python code, addition, and evaluating polynomials.\nSimilarly, in chain-of-though reasoning (CoT) [60] models are shown step-by-step demos of problem-solving\n(or models generate chains of thought without any examples as in zero-shot CoT [61], among other variants).\nIt has been further shown that using explicit explanations and reducing ambiguity can be proven useful\nas in the notion of algorithmic prompting [62]. Lanchantin et al. [63] introduced the concept of self-notes,\nshowing that interleaving the intermediate reasoning steps within the question/context rather than after the\nquestion can boost the performance of scratchpad/CoT on certain tasks. Goyal et al. [64] introduced pause\ntokens which act as dummy tokens that provide models with more compute time and processing before the\ngeneration of the true next token. On a related note, the iterative prompting method in [65] involves querying\nLLMs iteratively to optimize question prompts. Further, the Select and Inference framework (SI) [66] utilizes\npre-trained LLMs as general processing modules, alternating between selection and inference to generate\ninterpretable reasoning steps leading to the final results. In this work, we used the concept of autoregressive\nlocality to explain why scratchpads are helpful and how to design them. Further, we introduced inductive\nscratchpads that are suitable for a broad range of algorithmic reasoning tasks and can potentially generalize\nto more complex samples than what appears in the train distribution."}, {"title": "Parity and addition", "content": "Parity and addition tasks are two essential reasoning tasks that are challenging\nin the setting of length generalization (where the number of bits/digits is increased). These tasks can also\nbe hard in the in-distribution setting if the number of bits/digits is large enough. It has been shown that\ncertain formats of scratchpad/chain-of-though reasoning and relative positional embedding can achieve a\nmodest length generalization on parity task (around 5/10 more bits depending on the method and the\nseed) [11, 53]. Lee et al. [45] use a decoder-only model similar to ours (also similar in size) showing that\ngenerating the output in the reverse format and using scratchpads are helpful. However, they are not able to\nget length generalization with their model size. Nevertheless, it has been shown that large enough models\n(with more than 108 parameters) with a scratchpad can generalize to numbers with 9/10 digits while being\ntrained on numbers with up to 8 digits [32]. Jelassi et al. [67] show that encoder-only models with relative\npositional embedding can generalize to numbers with 15 digits when they are trained on numbers with up to\n5 digits.\nThe RASP-L work [68] considers the addition task and can get length generalization from 35/40 to 50 digits\n(depending on the random seed) by outputting the result in the reverse order and also using \u2018index hints'\nwhich are special tokens that come before the operands' digits in a specific order. For instance, an example\nwould look like a5b4 + a367 = b1a9. Likewise, for parity targets, they use index hints along with scratchpad\nand can get length generalization from 30/35 bits to 50 bits (depending on the seed).\nThe work of Shen et al. [17] uses a scratchpad with a recursive format for the addition task to generalize to\nnumbers with 12/13 digits while the training data includes numbers with up to 10 digits. Our special case\nof inductive scratchpad based on shifting the numbers is similar to their recursive scratchpad in terms of\nformat, however, [17] does not enforce any inductive structure (as achieved with the attention masking for\nthe inductive scratchpad) and hence [17] gets a much more limited length generalization."}, {"title": "A.2 Learning measures and lower-bounds for GD", "content": "Some recent literature has studied complexity measures for (S)GD-trained neural networks. In particular, the\nnoise sensitivity [20, 6, 82, 21", "28": "the statistical query (SQ) dimension [14, 19", "12": "which\nare usually defined for a class of targets/distributions rather than a single distribution (in particular the full\nparity is efficiently SQ learnable since there is a single function); the NTK alignment [22, 23", "24": "which is also related to the noise-sensitivity with\nthe advantage of depending on the network initialization at the cost of being a more implicit measure; the\ninformation exponent [25, 26", "27": "and leap [28"}]}