{"title": "Training objective drives the consistency of representational similarity across datasets", "authors": ["Laure Ciernik", "Lorenz Linhardt", "Marco Morik", "Jonas Dippel", "Simon Kornblith", "Lukas Muttenthaler"], "abstract": "The Platonic Representation Hypothesis claims that recent foundation models are converging to a shared representation space as a function of their downstream task performance, irrespective of the objectives and data modalities used to train these models [35]. Representational similarity is generally measured for individual datasets and is not necessarily consistent across datasets. Thus, one may wonder whether this convergence of model representations is confounded by the datasets commonly used in machine learning. Here, we propose a systematic way to measure how representational similarity between models varies with the set of stimuli used to construct the representations. We find that the objective function is the most crucial factor in determining the consistency of representational similarities across datasets. Specifically, self-supervised vision models learn representations whose relative pairwise similarities generalize better from one dataset to another compared to those of image classification or image-text models. Moreover, the correspondence between representational similarities and the models' task behavior is dataset-dependent, being most strongly pronounced for single-domain datasets. Our work provides a framework for systematically measuring similarities of model representations across datasets and linking those similarities to differences in task behavior.", "sections": [{"title": "Introduction", "content": "Representation learning has seen remarkable progress in recent years, with state-of-the-art models achieving or even surpassing human-level performance in a wide range of computer vision tasks [65, 18, 64, 85, 61]. The claim of The Platonic Representation Hypothesis is that with this progress in task performance, model representation spaces tend to approach each other irrespective of the models' training data or architecture [35]. Even between models trained on different modalities, better downstream task performances appear to result in more similar representations. Two questions naturally arise from this phenomenon: Do pairwise similarities of model representations transfer from one dataset to another, and are they generally linked to the models' downstream behavior?\nInvestigating the correspondence between representation and behavior has a long history in representation learning and adjacent fields [e.g. 32, 74]. Two models can have very different intermediate layer representations even though they show the same task behavior [32, 60, 48]. However, if two models show different behavior, one can be certain that the output layer representations are different, because identical output representations map to the same behavior and, vice versa, different output layer representations map to different behavior. What remains unclear is how behavior (e.g., downstream task accuracy) is affected by the convergence of model representations (i.e., the representations are similar according to a chosen similarity measure). Does this relationship depend on the specific similarity measure at hand, or may it be determined by the nature of a dataset?\nThe field lacks consensus on how to define (pairwise) representational similarity [cf. 45, 42, 81, 22, 74] and how to systematically measure its relationship to behavior [cf. 26, 60, 74, 48]. Here, we use similarity measures that are widely used in the community and have been empirically proven to be useful for measuring representational similarity of neural networks, such as CKA [42, 67] or RSA [45, 44]. However, pairwise representational similarity is usually measured for a single dataset and not across datasets. It may be the case that the representations of two vision models are highly similar for a dataset of raccoons but very different for a dataset of sunflowers\u2014due to their different domains. Similarly, the representations of two models may exhibit a high similarity value for satellite data but can show a low similarity for images of fruit because of the images' different structure.\nThus, it appears crucial to scrutinize the factors that determine pairwise representational similarity across sets of stimuli. Knowing about those factors will enable us to generalize pairwise representational similarity across datasets and more effectively find model sets that have learned a similar representation of the world. For a large set of diverse vision models, we examine their pairwise representational similarities for both structured and unstructured datasets from different domains, using the publicly available Visual Task Adaptation Benchmark [VTAB; 87]. Our contributions and findings are as follows:\n\u2022 We propose a principled way of measuring the consistency of pairwise similarity across datasets, by measuring whether the (relative) pairwise similarities of model representations from one dataset are transferable to the (relative) pairwise similarities of another dataset.\n\u2022 We find that the objective function is the most crucial factor for determining the consistency of pairwise representational similarities across datasets, whereas architecture, model size, and training data do not appear to be qualitatively meaningful factors.\n\u2022 Pairwise similarities in SSL models generalize more reliably across stimulus sets compared to image-text and supervised models, which show high variance in their consistency due to dataset dependence.\n\u2022 We demonstrate that our findings hold irrespective of whether the kernel in the similarity measure emphasizes local or global representational structure.\n\u2022 The pairwise similarities of model representations show a strong correlation with the models' differences in task performance for single-domain datasets, while multi-domain datasets show highly variable and specialized datasets consistently low correlations."}, {"title": "Related work", "content": "Measuring the similarity of deep neural networks. Representational similarity measures aim to quantify to what extent the representations of two models corresponding to the same input overlap. In the field of machine learning, they have been used as a tool to understand deep neural networks [51, 63, 56], investigate human-machine alignment [74, 82], and as an objective for model distillation [79, 70, 88]. Various ways of measuring representational similarity have been proposed, including Canonical Correlation Analysis [CCA; 34, 66, 58], Singular Vector Canonical Correlation Analysis [SVCCA; 66], Centered Kernel Alignment [CKA; 42, 14], and Representational Similarity Analysis [RSA; 45], among others [e.g. 81, 20, 5, 16]. They can be distinguished from functional similarity measures, such as performance difference [39], error consistency [25], and true-positive agreement [29], which are based on model outputs rather than internal representations [39], or model stitching [4, 15, 57], which measures compatibility, not similarity, of internal representations [33]."}, {"title": "Methods", "content": "In this section, we propose a novel approach for analyzing the consistency of representational similarities across multiple datasets. Fig. 1 provides a schematic overview of this framework.\nExtracting representations. We are interested in comparing the similarities of different vision models. We perform this comparison by extracting their latent representations and examining them. Let $f_\\theta : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ be a pretrained neural network function parameterized by a fixed set of parameters $\\theta$ that maps the d-dimensional images to p-dimensional vector representations. Let $X \\in \\mathbb{R}^{n \\times d}$ be a dataset of n stacked images. For each image $x \\in \\mathbb{R}^d$ in a dataset, we extract its corresponding latent representations $f_\\theta(x) = z \\in \\mathbb{R}^p$. For supervised models, we typically use the penultimate layer to extract this representation; for self-supervised vision models, the average pooling layer; and for image-text models, the image encoder. We denote by $Z \\in \\mathbb{R}^{n \\times p}$ the matrix of stacked vector representations, which is unique for each model and dataset combination.\nComputing representational similarities for a dataset. We compute the similarity between two models $f_\\theta$ and $f_\\varphi$ for a dataset by applying CKA [14, 42] to their vector representations $Z^{f_\\theta}$ and $Z^{f_\\varphi}$. CKA computes the similarity based on the normalized Hilbert-Schmidt Independence Criterion [28], applied to the kernel matrices of both representations. Using CKA with a linear kernel focuses on global similarity structure, while an RBF kernel with small $\\sigma$ measures local similarity structure [42, 2]. We compared their behavior (shown in Fig. 3 and in Appx. E) and observed similar trends for both kernels. Therefore, we use CKA with a linear kernel for the remainder of this paper if not mentioned otherwise.\nComputing the similarity requires the full kernel matrix over the dataset, which scales quadratically with the number of data points. Hence, the exact computation becomes intractable for large datasets."}, {"title": "Representational similarities across datasets", "content": "Because image representations depend on both a model and a dataset, the representational similarities between two models may vary across different datasets. To investigate this, we analyze the consistency of pairwise model similarities across multiple datasets. We compute the correlation coefficient for pairwise model similarities (measured for a large set of model pairs) between two datasets (i.e., can the pairwise model similarities for one dataset be expressed as a function of the similarities for a different dataset). In addition, we partition the models into specific sets determined by the models' training factors.\nSpecifically, we scrutinize how the pairwise similarities of models from two model sets, $\\Theta$ and $\\Phi$, correspond between two datasets, $A$ and $B$. By computing the pairwise representational similarities between all model pairs $(f_\\theta, f_\\varphi)$, separately for each dataset, where $f_\\theta \\in \\Theta$, $f_\\varphi \\in \\Phi$ and $f_\\theta \\neq f_\\varphi$, we obtain a similarity vector $s \\in \\mathbb{R}^k$, where k denotes the number of distinct model combinations. A similarity vector for two sets of models and a dataset $X$ is then defined as follows:\n$\\Sigma(X.\\Theta, \\Phi) = (CKA(Z_{f_\\theta}^X, Z_{f_\\varphi}^X))^T_{f_\\theta \\in \\Theta, f_\\varphi \\in \\Phi, f_\\theta \\neq f_\\varphi} \\in \\mathbb{R}^k$.\nTo quantify the consistency of similarities between two datasets A and B, we use the Pearson correlation coefficient between the similarity vectors $S(A,\\Theta,\\Phi)$ and $s(B,\\Theta,\\Phi)$, i.e., $\\rho (S(A,\\Theta,\\Phi), S(B,\\Theta,\\Phi))$. The Pearson correlation measures the degree to which the similarity trends between models are preserved across datasets, focusing on the relative positioning of model pairs rather than the absolute similarity values. This correlation measures how well the models' representational similarities are preserved across datasets. A high Pearson correlation indicates that the ordering of model similarities of two model pairs remains stable between datasets. In other words, if two models $f_\\theta$ and $f_\\varphi$ are more similar than $f_\\theta$ and $f_\\psi$ on one dataset, a high Pearson correlation coefficient indicates that this relationship transfers to the other dataset. Conversely, a low correlation value suggests that the models' relationships shift significantly depending on the dataset, showing variability in processing"}, {"title": "Experiments", "content": "Our experiments investigate the consistency of representational similarities between vision models across various datasets using our proposed framework. Following the experimental setup (Sec. 4.1), we first analyze how model similarities transfer across datasets (Sec. 4.2 and 4.4) and find consistent model groups through similarity clustering (Sec. 4.3). We then investigate how model and dataset characteristics influence similarity consistency (Sec. 4.5 and 4.6), and conclude by examining the relationship between representational similarities and models' performance differences on downstream tasks (Sec. 4.7). All code to run our analyses and reproduce the results presented in this section is publicly available on GitHub: https://github.com/lciernik/similarity_consistency."}, {"title": "Setup", "content": "Models. We evaluated 64 vision models, representing a diverse range of architectures, training paradigms, and parameter scales. A complete list of models and their characteristics can be found in Appx. A. We used the Python package thingsvision [62] to extract the model representations.\nGrouping models by attributes. Instead of focusing on the representational similarities of individual model pairs, we analyze the aggregated similarities between sets of models. We categorize models into sets based on four attributes: training objective, training data, model architecture, and model size (see Tab. 1). Grouping models by these attributes allows us to examine how different model characteristics relate to the behavior of representational similarities while acknowledging that these attributes are often correlated across models. For example, we measured the similarity between models trained with different objectives (e.g., supervised, SSL, and image-text) and assessed the consistency of representational similarities within and between each model group.\nDatasets. We evaluated pairwise model similarities across 20 classification datasets from the CLIP benchmark 4 and 3 datasets from Breeds [71]. This set includes various VTAB datasets as well as ImageNet-1k. The CLIP benchmark converts multi-label to single-label classification datasets by cropping images to the bounding box of each object. Following the categorization proposed in [87], we classified the datasets into three main types: natural (e.g., ImageNet-1k), specialized (e.g., PCAM), and structured (e.g., DTD) image datasets. Furthermore, we partition the natural image datasets into single-domain and multi-domain categories. A list of all datasets and their categorization can be found in Tab. 3."}, {"title": "Do representational similarities transfer across datasets?", "content": "The transferability of model similarities across diverse datasets is crucial for model comparison and selection. If these similarities are consistent across different datasets, it will enable us to evaluate models on a single dataset and generalize their relationships to other datasets. We analyzed this by computing pairwise CKA similarities for all model pairs across our dataset collection.\nModel similarities (e.g., the range of values and the similarity trends) appear to vary across datasets (Fig. 2). We observe the highest similarity within image-text models (yellow boxes), while a group of self-supervised models (white boxes) shows consistent similarity trends across datasets. Yet, there are also model groups with higher variability (cyan boxes), mainly containing supervised models. The standard deviation matrix (rightmost matrix in Fig. 2) quantifies these differences, indicating that model similarities do not transfer uniformly across datasets.\nTo further investigate the transferability of model similarities across datasets, we examine the relationship between the mean and standard deviation of similarity values. The left panel in Fig. 3 shows an inverse U-shape trend (further analyzed in Appx. D): model pairs with extreme mean similarities exhibit low variability, while variability for mid-range similarities is higher. To verify that this observed variability is not an artifact of the similarity metric, we compare the mean similarity values obtained using linear CKA with two other metrics: CKA with an RBF kernel where $\\sigma = 0.4$ (a local similarity measure) and RSA using Spearman correlation (a global similarity measure, Fig. 3). The high correlations among these measures confirm that mid-range similarity values consistently vary across metrics, indicating limited stability across datasets.\nGiven this variability, we investigate persistent trends in model relationships across datasets by addressing two questions: a) Can we identify subgroups of models whose similarity to other models (consistently) cluster across different datasets? b) Do the relative representational similarities within or between these subgroups remain stable/consistent across datasets? Answering these questions affirmatively would allow us to make generalizable, albeit potentially weaker, claims about models' relationships from a single data source."}, {"title": "Do representational similarities cluster according to model categories?", "content": "We qualitatively assess the alignment between our predefined model categories (Sec. 4.1) and the clustering patterns observed in the t-SNE embedding of similarity matrices, as illustrated in Fig. 4."}, {"title": "Do relative representational similarities remain consistent across different datasets?", "content": "Given the observation that representational similarities are not directly transferable across datasets but depend on both the models and the dataset, we now examine the consistency of relative representational similarities across dataset pairs. Following the method described in Sec. 3, the top row of Fig. 5 shows the similarities of all model pairs across three dataset pairs, showing positive correlations. Across all dataset pairs, the mean Pearson correlation coefficient is 0.756 (std=0.124). This indicates that the relative ordering of pairwise model similarities is largely consistent across datasets, suggesting a degree of transferability in the relative relationships of representational similarities. However, the high standard deviation and the distributions of similarities shown in Fig. 5 suggest the existence of model-pair groups that may exhibit distinct consistencies of similarity values.\nThe second row in Fig. 5 shows model pairs within the same training objective, highlighting differences in group-specific consistency levels. We find that SSL models show the strongest similarity consistency across datasets, image-text models form a cluster of generally high similarities, and supervised models achieve the weakest consistency. Next, we will analyze the aggregated measure across all dataset pairs and revisit all categories in the following section."}, {"title": "Which model categories influence relative similarity consistency?", "content": "Fig. 6 shows the distribution of Pearson correlation coefficients $R(\\Theta, \\Phi)$ for each combination of model sets $(\\Theta, \\Phi)$ across all dataset pairs.\nTraining objective. Self-supervised models ($\\Phi_{SSL}$) show the highest relative similarity consistency, even when compared to models with different training objectives. This suggests that their relative similarities are most transferable across datasets, likely due to the ability of SSL to capture dataset-independent features. In contrast, supervised models show the weakest correlations, particularly when compared to image-text models ($\\Phi_{Img\\text{-}Txt}, \\Phi_{Sup}$), indicating a less reliable transfer of relative pairwise similarities across datasets.\nModel architecture. While no significant differences in similarity consistency exist between transformers and convolutional networks, comparisons across architectures show slightly lower consistency, potentially indicating distinct architectural inductive biases.\nTraining data. We find a low median correlation and a high variance for the model set pairs $(\\Phi_{IN21k}, \\Phi_{Large})$ and $(\\Phi_{IN21k}, \\Phi_{XLarge})$. However, these effects may be confounded by the training objectives, as both Large and XLarge contain mainly image-text models, while $\\Phi_{IN21k}$ consists solely of supervised models. On the other hand, $\\Phi_{IN1k}$, which includes models trained on a dataset similar to ImageNet-21k but with diverse training objectives, does not exhibit the same low consistency. Consequently, training data appears to have a less significant impact on the consistency of similarities than other factors.\nModel size. Models within the same size category demonstrate higher consistency of similarity than models of different size categories (e.g., $\\Phi_{small}$ and $\\Phi_{xlarge}$). This indicates model size influences the transferability of relative similarities across datasets, though less significantly than the training objective.\nOur analysis identifies the training objective as the primary factor influencing similarity consistency, with model size playing a less pronounced role. Self-supervised models consistently demonstrate above-average correlations across datasets. The effects of model architecture and training data are minimal, suggesting that these factors are less critical in determining the transferability of relative"}, {"title": "Do dataset categories influence relative similarity consistency?", "content": "Previously, we analyzed similarity consistency from the perspective of models, noting substantial variation in consistency across all dataset pairs, where some pairs produce consistent representations while others do not. In this section, we shift our focus to identifying groups of datasets that exhibit more stable relative similarities, aiming to uncover variables that might explain these variations.\nThe left panel in Fig. 7 shows the Pearson correlations of all model pairs for each dataset pair individually, i.e., $\\rho (s(\u0391,\\Phi_A, \\Phi_{A'}), S(B,\\Phi_A, \\Phi_{A'}))$ for all datasets pairs (A, B). The consistency of representational similarities varies strongly across dataset pairs, showing higher consistency within dataset categories (especially natural multi-domain and structured datasets) than across them, as exemplified by high consistencies within natural multi-domain datasets but lower ones when paired with specialized datasets. Moreover, some datasets are inconsistent with almost any other dataset, particularly FGVC Aircraft and medical imaging datasets (Diabetic Retinopathy and PCAM). Interestingly, ImageNet-1k exhibits a milder yet significant pattern of inconsistency. Within the multi-domain category, this is especially pronounced for CIFAR-10 and CIFAR-100."}, {"title": "Can representational similarity predict performance gaps?", "content": "To investigate the relationship between downstream task performance and representational similarity, we correlate two measures for each dataset: (1) the CKA value for each model pair and (2) the absolute difference in classification performance, referred to as the performance gap (see Appx. B for details on performance computation). Fig. 8 shows this relationship across three datasets from each category. While prior work suggests well-performing models produce similar representations [35], we find this relationship is dataset-dependent.\nNatural multi-domain datasets, such as ImageNet-1k, show weak negative correlation between similarity and performance gap. Many model pairs achieve high performance despite low representational similarity (yellow dots in the lower left corner). This suggests that multi-domain datasets might provide richer contextual information, enabling multiple high-performance strategies that do not converge to similar representations. However, correlation coefficients in this dataset category show notable variability (see Appx. G for further discussion).\nNatural single-domain and Structured datasets, in contrast, exhibit a strong negative correlation between similarity and performance gap (see Fig. 15 for all datasets). The absence of high-performing, dissimilar model pairs suggests a close link between performance and representational similarity, indicating that successful models for these datasets rely on capturing specific discriminative features for closely related classes. Observing this behavior in both structured and single-domain datasets suggests that models may depend on representing more structural attributes to distinguish between classes. However, the limited number of structured datasets restricts broader conclusions.\nIn summary, our analysis shows varying trends depending on the dataset. Multi-domain datasets support a wider range of successful strategies, whereas performing well on single-domain and structured datasets appears to require a more limited set of features, resulting in stronger correlations between performance and similarity. This phenomenon aligns with findings from previous studies, which have shown that models can leverage contextual cues in multi-domain datasets to achieve strong performance without developing robust, generalizing features [49, 27, 24]. For specialized datasets, we find no or weak correlation between pairwise similarity and performance gap, along with reduced variation in both measures.\nOur analysis provides clear evidence of variations in the relationship between performance gap and similarity across different datasets, highlighting the need for further research to identify the key factors driving this variation."}, {"title": "Discussion", "content": "The representations of the latest foundation models appear to converge to a canonical representation, irrespective of their training data and objectives [35]. However, this isotropy of model representations may just be an artifact of the datasets that the community commonly evaluates. Thus, in this paper, our goal was to present a systematic way of analyzing pairwise similarities of model representations across sets of stimuli. This approach allowed us to examine whether representational similarities generalize across datasets and identify variables influencing their consistency.\nWe found that the training objective of models is a central factor for determining the consistency of pairwise representational similarities across datasets, whereas architecture, model size, and training data appear less important. Among training paradigms, image-text training leads to the most similar representation spaces, irrespective of the models' training data or architecture. For individual datasets, SSL model representations are not as similar among themselves as the representations of image-text models, but the pairwise similarities between SSL models and other models are highly consistent across different datasets.\nWe hypothesize that both image-text and supervised trained models learn distinct semantic categories during training that lead to representations whose features overfit to these categories and, therefore, do not generalize well to datasets that do not contain them. In contrast, self-supervised trained pure vision models\u2014which are not constrained to learn explicit semantic categories during training [cf. 12, 40, 84, 11]\u2014may develop more flexible representations that better accommodate diverse dataset stimuli. This leads to representations whose relative pairwise similarities vary less across datasets than the similarities of representations from image-text or supervised models. This finding supports our hypothesis that models trained on specific semantic categories develop dataset-dependent representations due to their features' sensitivity to mismatches between training and evaluation categories.\nAn alternative explanation may be that the extracted representations from SSL models are further away from the models' task behavior than the image encoder and penultimate layer representations from image-text and supervised models, respectively. Usually, SSL model representations are extracted from the average pooling layer [cf. 12, 13, 60, 78] which can be seen as further away from behavior than the penultimate layer of supervised models. One benefit of our framework is that it can be applied to the representations of any model layer, allowing future work to test this hypothesis. We note that, in contrast to image-text and supervised models, the tasks used to train SSL models are extremely heterogeneous. This may yet be another reason for the higher consistency of representational similarities among SSL models.\nFinally, we have shown that the pairwise similarities between model representations are only in part predictive of their differences in downstream task behavior. While we observe a strong correlation on datasets with limited contextual information, for others, e.g., ImageNet-1k, there is no obvious correspondence between the pairwise similarities of model representations and the models' differences in task performance. Moreover, the relationship between pairwise similarities and task performance is substantially different between natural multi-domain datasets, e.g., for ImageNet-1k vs. CIFAR-100 (see Fig. 8). This is surprising in light of recent findings that have shown a convergence of model representations as a function of task performance [35] and a strong linear relationship between a model's ImageNet performance (i.e., performance on the training set distribution) and its downstream transfer accuracy [43]. This suggests that the transferability of pairwise similarities across data sets follows a different and more complex relationship than the transferability of task performance. While different objectives and hyperparameters lead to similar task performances [41], they seem to change the similarity spaces of the learned representations in a way that affects the relationship between pairwise model similarities and their differences in downstream behavior across datasets. Therefore, whether the claims of The Platonic Representation Hypothesis hold or not depends on the specific nature of a dataset rather than being generally true for all sets of stimuli used to compute model representations.\nConclusion. In summary, we presented a blueprint for systematically analyzing pairwise representational similarities across different sets of stimuli. This framework allowed us to demonstrate that pairwise similarities rarely transfer from one dataset to another and depend on both the models' objective functions and the datasets' structure and domain. The community has long optimized for finding the optimal hyperparameters to improve task performance [e.g. 80, 73, 52, 19] but neglected its ramifications for the similarity structure of the models' representation spaces. Only very few recent works investigated how the learned model features are affected by training tasks [e.g. 48, 41]. We hope that our work motivates to pinpoint the variables during model training that affect (dis-)similarities between model representations beyond the training dataset because identifying those variables will ultimately help to build more interpretable systems with which humans are more likely to interact [60, 47, 74, 61, 77]."}]}