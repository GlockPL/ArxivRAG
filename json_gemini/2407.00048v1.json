{"title": "Ensemble Method for System Failure Detection Using Large-Scale Telemetry Data", "authors": ["Priyanka Mudgal", "Rita Wouhaybi"], "abstract": "The growing reliance on computer systems, par- ticularly personal computers (PCs), necessitates heightened reliability to uphold user satisfaction. This research paper presents an in-depth analysis of extensive system telemetry data, proposing an ensemble methodology for detecting system failures. Our approach entails scrutinizing various parameters of system metrics, encompassing CPU utilization, memory utilization, disk activity, CPU temperature, and pertinent system metadata such as system age, usage patterns, core count, and processor type. The proposed ensemble technique integrates a diverse set of algorithms, includ- ing Long Short-Term Memory (LSTM) networks, isolation forests, one-class support vector machines (OCSVM), and local outlier factors (LOF), to effectively discern system failures. Specifically, the LSTM network with other machine learning techniques is trained on Intel\u00ae Computing Improve- ment Program (ICIP) telemetry software data to distinguish between normal and failed system patterns. Experimental evaluations demonstrate the remarkable efficacy of our mod- els, achieving a notable detection rate in identifying system failures. Our research contributes to advancing the field of system reliability and offers practical insights for enhancing user experience in computing environments.", "sections": [{"title": "I. INTRODUCTION", "content": "System failures primarily stem from system errors, with each critical system error having the potential to trigger system shutdowns or reboots in an attempt to rectify the issues [1]. These errors can result in substantial financial losses and significant harm to critical information tech- nology (IT) infrastructure. For instance, in the context of Windows, the commonly encountered \"blue screen of death\" (BSOD) often appears, disrupting device usage [1]. Meanwhile, other non-critical errors may arise without causing any disruption to the device. The underlying causes of these errors may originate from various sources, including system hardware or software issues, and can differ based on factors such as system metrics, hardware configurations, and environmental conditions like tempera- ture or frequency of usage, all of which directly contribute to system failures.\nHence, it's crucial to keep a close eye on the health of personal computers (PCs). This monitoring protocol should seamlessly integrate into the product development lifecycle and persist even after PCs have been distributed to end users worldwide. Whether during development or post- deployment, monitoring hinges on telemetry, a method of gathering and retaining data from remote systems. Especially advantageous for clusters or extensive arrays of systems within IT infrastructure, monitoring streamlines the collection of telemetry data for a range of objectives, such as identifying system errors, predicting potential issues, and overseeing overall system health.\nThis paper utilizes telemetry data from client PCs, focusing on CPU metrics [2]\u2013[9]. The key metrics include CPU utilization, memory utilization, disk utilization, CPU temperature, process type, number of cores, age of the system, and active applications. There are three techniques for training error detection models: supervised, unsuper- vised, and semi-supervised [2]-[9]. Supervised learning uses labeled data for both normal and anomalous events but requires balanced datasets [4]. Unsupervised methods identify patterns and flag deviations as anomalies [5]. Semi-supervised learning involves training with a small amount of anomalous data [9]. These techniques find practical application in various domains, including PC error detection using telemetry and industrial settings [2], [3], [5], [6].\nIn this study, we employ an unsupervised learning approach and introduce three distinct ensemble models utilizing long short-term memory (LSTM) [10], along with either isolation forest [8], one-class support vector machine (OCSVM) [11], or local outlier factor (LOF) [12]. Our findings indicate that LSTM paired with isolation forest surpasses the performance of LSTM with OCSVM and demonstrates comparable performance to LSTM combined with LOF."}, {"title": "II. BACKGROUND", "content": ""}, {"title": "A. Telemetry Framework", "content": "To extract value from telemetry, a comprehensive frame- work has been outlined by Kwasnick et al. [13], where the authors describe telemetry as a framework comprising six distinct stages, illustrated in Fig. 1. Each stage demands domain expertise to effectively recognize, collect, and refine telemetry data for its subsequent application in data analytics, thereby facilitating informed decision-making tailored to specific domains. Subsequently, we explore the nuances associated with each stage within the framework of product health monitoring.\nStage 1 involves selecting crucial telemetry metrics, considering factors like user privacy and data size lim- its [13], [14]. Stage 2 entails the precise gathering and uploading of data by the telemetry collector, adhering to privacy and security standards [13], [14]. Stage 3 fo- cuses on acquiring user permission for telemetry software download [13], [14]. Stage 4 addresses data management methods, particularly for managing vast amounts of data with cloud solutions [13], [14]. Stage 5 discusses data analytics techniques, including correlation analysis and machine learning methods like we use ensemble LSTM [10], [15] in our work. Finally, Stage 6 emphasizes value extraction from telemetry data through informed decision-making and gaining insights into product error behavior for future development [15]."}, {"title": "III. METHOD", "content": "In our work, we outline our experiments directed to- wards identifying system errors occurring on end-user systems. Initially, we collect metrics accessible through the Intel\u00ae Computing Improvement Program (ICIP) telemetry software [14]. ICIP serves as a telemetry software tool for monitoring product health, provided to users upon visiting www.intel.com for driver downloads. Meeting the standards for telemetry software, ICIP ensures privacy, security, and minimal resource consumption. Subsequently, we analyze these metrics to compile the dataset, integrat- ing the specified metrics with system errors. Finally, we train our network using the preprocessed dataset to detect system errors. A comprehensive description of the entire process is provided in the subsequent sections."}, {"title": "A. Data Collection", "content": "The dataset consists of information sourced from sys- tems that have willingly opted to participate in data collection and analytics (DCA) via ICIP. Principally, this data encompasses client PCs featuring various generations of Central Processing Units (CPUs). DCA acquires data from machines exclusively during their operational phases, referred to as the SO state, and collects it at regular intervals, typically every 5 seconds. On-device aggregation of data occurs every 24 hours, with the aggregated data being uploaded to the datastore when the system is active and connected to the network. Data is only accessible for the days when the machine is active, specifically in the SO state, for at least a few seconds. The dataset incorporates details regarding machine configurations, memory usage, disk usage, CPU usage, and CPU temperature. We incor- porate data spanning 30 days from March 2023 to April 2023, utilizing data from systems with at least 10 days of data available, with at least 10% of the data indicating system errors."}, {"title": "1) CPU Temperature", "content": "The CPU reads and stores the temperature of each core [17] with digital thermal sensors [18]. ICIP records the temperatures every 5 second when the system is switched on. For our work, we used the weighted average of the daily temperature over all the cores in each PC, for each day reporting."}, {"title": "2) CPU Utilization", "content": "CPU executes commands, low usage ideal, high for intensive programs. ICIP reads each core utilization [16] every 5 secs, captures average every 12 hrs."}, {"title": "3) Memory Utilization", "content": "Memory utilization [19] refers to the average usage calculated from the percentage of available memory in use at any given time. ICIP captures the memory utilization in each PC every 12 hours."}, {"title": "4) Disk Utilization", "content": "Disk usage [20] represents the proportion of the hard disk currently utilized by the computer to execute programs and tasks. High or 100% disk usage can occur on any category of device, regardless of their age or condition. In this study, we use the average used disk percentage in each PC for daily reporting."}, {"title": "5) System Information", "content": "System information contains the data about the device's specifications, namely, CPU type, number of cores, age of device, persona using the device, chasis type, etc. We use these details to feed to our machine learning model that could relate to system errors."}, {"title": "6) System Errors", "content": "System errors, identified through CPU Machine Check Architecture, are reported to Windows OS along with rel- evant data [18]. Some error records may have timestamps from the day after if the PC was not booted until then. Our dataset includes about 150 types of system errors with associated bug check codes [21]. Previous studies explored memory and temperature correlations with errors [15], [22]. However, our work uses the mentioned telemetry data and ensemble machine learning for error detection [16]."}, {"title": "B. Data Preprocessing", "content": "We employ an extensive dataset sourced from ICIP [14] provided by Intel\u00ae. Due to the diverse metrics captured at varying frequencies, we undertake several preprocessing steps. The preprocessing methodology for CPU temper- ature and utilization is identical and described in Algo- rithm 1. Initially, we compute the weighted average CPU temperature for all cores on a daily basis. Subsequently, we execute the same process for CPU utilization data. Both CPU temperature and utilization data are segmented into distinct bins. Samples displaying CPU temperatures ranging from 0 to 10% are assigned to bin 1, while those from 10 to 20% are designated to bin 2, and so forth. A similar approach is applied to CPU utilization. We preprocess this data by categorizing the metrics into three groups: low, medium, and high. If the combined number of samples in bins 9 and 10 constitutes 80% or more of the total samples, the metric is labeled as \"high\". Similarly, if the combined number of samples in bins 5, 6, 7, and 8 amounts to 80% or more of the total samples, the metric is categorized as \"medium\". Likewise, if the combined number of samples in bins 1, 2, 3, and 4 equals 80% or more of the total samples, the metric is classified as \"low\". The same methodology is applied to CPU utilization.\nMemory utilization for the systems follows a similar organization, albeit with a few adjustments. The bin size for memory utilization is set at 5, wherein samples display- ing memory utilization below 5% each day are allocated to bin 1. Likewise, samples exhibiting memory utilization above 5% but below 10% are placed in bin 2, and so forth. Furthermore, memory utilization incorporates an additional bin to account for samples indicating more than 100% memory utilization, warranting a separate allocation. Consequently, memory utilization encompasses a total of 21 bins. Subsequently, we preprocess this data as outlined in Algorithm 1. After computing the weighted average for all cores per day, if the collective number of samples indicating memory utilization of 95% and above accounts for 80% of the total samples, the memory utilization is categorized as \"high\". Similarly, if the total number of samples indicating memory utilization of 55% and above but less than 95% constitutes 80% of the total samples, the memory utilization is labeled as \"medium\". Finally, if the total number of samples indicating memory utilization below 55% constitutes 80% of the total samples, the memory utilization is categorized as \"low\". Additionally, we consider the total number of samples in each of the"}, {"title": "", "content": "status \u2190 Uncategorized\nInput: T,\nwhere T = {s, c, N[bk]},\ns \u2190 system id,\nc \u2190 core,\nN[bk] - number of samples in kth bin,\nwhere k = 1,\u00b7\u00b7\u00b7, total number of bins\nOutput: t = {s}\nW = $\\frac{\\sum_{i=1}^{cores} W_iX_i}{\\sum_{i=1}^{cores} W_i}$ \u25b7 Calculate weighted average for all\ncores\nwhere, W = weighted average,\nn = number of terms to be averaged,\nw\u2081 = weights applied to x values,\nX\u2081 = data values to be averaged\nif $\\sum_{j=1}^{m} W[N[b_j]] >= p(\\sum_{k=1}^{z}W[N[b_k]])$,\nwhere, j = 9 or 10 for CPU temperature and utilization;\nj = 19,..., 21 for memory utilization;\np = 0.8\nthen\nstatus \u2190 high\nelse if $\\sum_{j=1}^{m} W[N[b_j]] >= p(\\sum_{k=1}^{z}W[N[b_k]])$,\nwhere, j = 5, ..., 8 for CPU temperature and utilization;\nj = 11,..., 18 for memory utilization;\np = 0.8\nthen\nstatus \u2190 medium\nelse if $\\sum_{j=1}^{m} W[N[b_j]] >= p(\\sum_{k=1}^{z}W[N[b_k]])$\nwhere, j = 1, \u2026, 4 for CPU temperature and utilization;\nj = 1,..., 10 for memory utilization;\np = 0.8\nthen\nstatus \u2190 low\nend if"}, {"title": "", "content": "three categories.\nWe aggregate disk, memory, CPU utilization, tempera- ture, and system data daily, ensuring accuracy by imputing time intervals for diverse devices [16]. Error labels are appended to rows indicating system errors, serving for validation [16]. The dataset grows to over a million rows, with systems operational for at least 10 days included [16]. Categorical data is transformed into numerical representa-"}, {"title": "C. Network Architectures", "content": "This section outlines the system architecture of our proposed model."}, {"title": "1) Proposed Architecture", "content": "In this section, we present the proposed architecture designed for identifying anomalies in platform telemetry data. While traditional machine learning techniques may benefit from feature extraction to enhance results [23], this process typically demands domain expertise. In contrast, deep learning techniques leverage multi-layer processing to effectively model input features, offering advantages over traditional hand-crafted feature descriptors. In our architecture, we incorporate Long-Short Term Memory (LSTM) [10] within an autoencoder [24] framework, lever- aging its proven efficacy in anomaly detection tasks [25]\u2013 [28], and its demonstrated high performance. By using this architecture, the temporal correlation of the platform telemetry data which is often time-series data [29] is leveraged to transform the data in latent space. We use the encoder part of the autoencoder architecture to encode the data in a fixed range feature vector Y. The encoder contains two layers of LSTM block. We set the timestamp as one for the LSTM blocks. The final encoded feature Y represent the compressed data. This encoded data Y is subsequently inputted into distinct machine learning models, namely Isolation Forest [8], [30]-[32], One-Class Support Vector Machine (OCSVM) [33], and Local Out-lier Factor (LOF) algorithm [12], individually trained for anomaly classification. These models exclusively utilize normal class data for training, enabling anomalies to be identified as outliers.\nEach of these algorithms produce the anomaly score and anomaly. We use these scores to determine if an input data point is classified as an anomaly or normal. These metrics significantly aid in anomaly detection, as the corresponding anomaly score value tends to be notably higher in cases of anomalies."}, {"title": "2) Experimental Setup", "content": "We train the LSTM encoder, such that the reconstruction loss is minimum. We use a learning rate of 0.001, batch size of 16, tanh activation function, huber loss function, and Adam optimizer. We train this model for 25 epochs to"}, {"title": "D. EVALUATION AND RESULTS", "content": "This section details the evaluation process of our tech- nique and shows that our method provides great confidence in classifying the anomalies in telemetry data."}, {"title": "1) Performance Metrics", "content": "We evaluate model performance using precision, recall, F1 score, and accuracy. Precision measures true positives over total positive predictions; recall quantifies true pos- itives over actual positives; F1 score balances precision and recall; accuracy measures correct predictions over total instances."}, {"title": "2) Results and Analysis", "content": "The results are shown in Table II, where LSTM encoder with Isolation Forest outperforms other techniques in terms of accuracy, precision, recall, and f1-score. Although, these metrics are not far from each other. However, if we consider the inference time, isolation forest is clearly the fastest algorithm. The reported precision, recall, and f1- score are for \"normal\" data. These metrics report a very low number for \u201cabnormalities\u201d. The possible cause may be as the dataset is very imbalanced, where 90% of the data is \"normal\" and 10% of the data is \u201cabnormal\". We also trained vanilla machine learning models namely Isolation Forest, OCVSM, and LOF on the same dataset. Although the models yield comparable accuracy, our proposed ap- proach reduced both training and inference time by 1.5\u00d7 compared to vanilla machine learning methods."}, {"title": "IV. DISCUSSION", "content": "A hypothesis positing an association between system utilization, temperature, and system errors suggests the potential for system malfunction. The conjecture is that elevated system metrics, particularly those detected by the CPU, may signal a marginality wherein excessive errors could surpass the correction logic's capacity. In this study, we examined all system errors, system utilization, and temperature. However, future investigations could yield"}, {"title": "V. CONCLUSION", "content": "In our work, we focus on detecting system errors on end user systems through an ensemble architecture combining deep LSTM and various machine learning techniques. Ini- tially, we collect metrics accessible via Intel\u00ae Computing Improvement Program (ICIP) telemetry software [14], a tool for monitoring product health provided to users when they download drivers from www.intel.com. Subsequently, we process these metrics to construct the dataset, inte- grating specified metrics with system errors. Finally, we train our network using the preprocessed dataset to identify system errors. Our models exhibit remarkable efficacy, achieving a notable detection rate in pinpointing system failures."}]}