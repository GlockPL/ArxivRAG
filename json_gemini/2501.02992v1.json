{"title": "GLFC: UNIFIED GLOBAL-LOCAL FEATURE AND CONTRAST LEARNING WITH\nMAMBA-ENHANCED UNET FOR SYNTHETIC CT GENERATION FRO\u041c \u0421\u0412\u0421\u0422", "authors": ["Xianhao Zhou", "Jianghao Wu", "Huangxuan Zhao", "Lei Chen", "Shaoting Zhang", "Guotai Wang"], "abstract": "Generating synthetic Computed Tomography (CT) images\nfrom Cone Beam Computed Tomography (CBCT) is de-\nsirable for improving the image quality of CBCT. Existing\nsynthetic CT (sCT) generation methods using Convolutional\nNeural Networks (CNN) and Transformers often face diffi-\nculties in effectively capturing both global and local features\nand contrasts for high-quality sCT generation. In this work,\nwe propose a Global-Local Feature and Contrast learning\n(GLFC) framework for sCT generation. First, a Mamba-\nEnhanced UNet (MEUNet) is introduced by integrating\nMamba blocks into the skip connections of a high-resolution\nUNet for effective global and local feature learning. Second,\nwe propose a Multiple Contrast Loss (MCL) that calculates\nsynthetic loss at different intensity windows to improve qual-\nity for both soft tissues and bone regions. Experiments on the\nSynthRAD2023 dataset demonstrate that GLFC improved\nthe SSIM of sCT from 77.91% to 91.50% compared with\nthe original CBCT, and significantly outperformed several\nexisting methods for sCT generation. The code is available at\nhttps://github.com/intelland/GLFC.", "sections": [{"title": "1. INTRODUCTION", "content": "Cone Beam Computed Tomography (CBCT) is widely em-\nployed in various clinical applications due to its low imple-\nmentation costs, reduced radiation exposure, and rapid imag-\ning capabilities [1]. However, CBCT images are often ham-\npered by significant artifacts and inaccurate Hounsfield Unit\n(HU) values compared to conventional Computed Tomogra-\nphy (CT), which limits its effectiveness in critical applications\nsuch as precise dose calculation for radiotherapy and tumor\nassessment [1]. Generating synthetic CT (sCT) from CBCT\nis a potential solution for this problem, and it allows clinicians\nto harness the cost-effectiveness of CBCT while achieving the\nsuperior image quality associated with CT [2, 3].\nIn recent years, deep learning methods have been widely\nused for sCT generation from CBCT. However, many existing\napproaches encounter challenges in effectively capturing both\nglobal and local features and contrasts due to inherent limi-\ntations in their network architectures and loss functions [2\u2013\n4]. For network architecture, Convolutional Neural Networks\n(CNN) such as UNet [5] have a limited receptive field in con-\nvolutional layers for global feature learning [5]. Vision trans-\nformers are better at modeling global features due to the self-\nattention mechanism, but are impeded by quadratic compu-\ntational complexity and limited ability to recovery local de-\ntails [6, 7]. Recently, Mamba, a sequence model based on\nState Space Models (SSM), has gained attention for its abil-\nity to model long sequences with a global perspective while\nmaintaining linear computational complexity [8]. This has led\nto applying Mamba to vision tasks [9, 10] such as image seg-\nmentation and generation [11, 12] by replacing convolutional\nlayers of UNet variants with vision Mamba blocks. Despite\ntheir improved global feature learning ability, they sacrifice\nthe convolution's inherent advantage in capturing local fea-\ntures. In addition, these methods often use multiple down-\nsampling layers with increased channel numbers, which re-\nduces the image resolution for fine-grained image generation\nwith increased model complexity.\nIn terms of loss functions, existing methods typically nor-\nmalize the full HU range (i.e., around -1000 to 3000 of CT)\nto [0, 1] or [-1,1] for loss calculation at a global window [13\u2013\n15]. Though this helps to obtain satisfactory outputs at a\nglobal view, it may lead to poor performance for a specific\nHU range relevant to some important tissues. For instance,\nsoft tissues in the brain such as gray matter and white matter\nhave a narrow HU range around [-250, 250], and bones like\nthe skull occupy HU values in the range around [300, 2000].\nUsing the global window with full HU range for loss calcula-\ntion will make the network pay insufficient attention to such\ntissues, leading to limited synthesis quality for them."}, {"title": "2. METHODS", "content": "As shown in Fig. 1, our GLFC framework uses a Mamba-\nEnhanced Unet (MEUNet) trained with a Multiple Contrast\nLoss (MCL) for sCT generation, where MEUNet combines\nthe advantage of vision Mamba and convolutional blocks in\na high-resolution UNet, and MCL calculates loss values with\na global intensity window and two local intensity windows to\nimprove the quality of synthesized soft tissues and bones."}, {"title": "2.1. MEUNet for Global-Local Feature Extraction", "content": "To improve global feature learning ability, CNNs [5] use\nmultiple down-sampling layers to enlarge the receptive field.\nHowever, it not only reduces the resolution of feature maps\nand limits the performance on generating local details, but\nalso improves the model complexity by exponentially increas-\ning channel numbers after each down-sampling. Motivated\nby Mamba's superiority on learning global features from long\nsequences, we keep high-resolution of feature maps in ME-\nUNet and use Mamba for long-range dependency learning\nrather than applying too many down-sampling layers.\nAs shown in Fig. 1(a), MEUNet only has two down-\nsampling layers in the encoder, and before each down-\nsampling, a convolutional block with two convolutional layers\nis used to extract local features. To obtain high-quality syn-\nthesis of local details in the output, we also use convolutional\nlayers in the decoder. Importantly, to improve global feature\nrepresentation, we incorporate Visual State Space (VSS) [10]\nblocks of Mamba into the skip connections between the en-\ncoder and decoder. In VSS, each patch is treated as a token,\nand to keep the same number of tokens at the two skip con-\nnections, we use an adaptive patching strategy. Specifically,\nlet $N_i \u00d7 N_i$ denote the spatial dimension of the feature map\nat the i-th resolution level (i = 0, 1), we use $L$ to denote the\npredefined number of tokens, the patch size for the i-th reso-\nlution level is denoted as $M_i \u00d7 M_i$, where $M_i = \\sqrt{N_i/L}$.\nIn each skip connection, the $L$ patches are then processed\nby VSS blocks, as shown in Fig. 1(b). The core of VSS is a\nSS2D module that learns the relationship among a sequence\nof tokens by a discrete State Space Model (SSM) [10]:\n$h_t = \\bar{A}h_{t-1}+ \\bar{B}x_t$\n$Y_t = Ch_t$\n$A = \\Delta A$\n$B = (e^{\\Delta A} \u2013 I)A^{-1}B$\nwhere $x_t$, $Y_t$ and $h_t$ are the input, output and hidden state for\nthe t-th token, respectively. Let $D$ denote the hidden state di-\nmension, the matrices $A \u2208 R^{D\u00d7D}$, $B \u2208 R^{D\u00d71}$, $C\u2208 R^{1\u00d7D}$\nand the scalar $\u0394$ are either learnable parameters or values\ncomputed based on real-time input x and learnable parame-\nters. Note that the input of the first skip connection is a rela-\ntively low-level feature, we use more VSS blocks there than\nthe second skip connection, i.e., 16 and 8 VSS blocks are used\nin the two skip connections, respectively."}, {"title": "2.2. Multiple Contrast Loss (MCL)", "content": "In this work, we normalize the image intensity to [-1.0, 1.0]\nfor network prediction P and ground truth CT Y based on the\nfull HU range. Typical supervised image translation meth-\nods directly use P and Y to calculate a global loss [3], e.g.,\n$L_{glob} = ||Y - P||_1$. To better capture details for tissues with\nnarrower HU ranges, we introduce MCL that also calculates\nthe loss at multiple local intensity windows.\nLet $w_n = [I_n, I_r]$ denote the n-th local intensity window\nwith the min and max value being $I_n$ and $I_r$, respectively. P\nis first normalized by $w_n$ as $P_n = 2(P-I_n)/(I_r - I_n) \u2013 1$,\nand then clipped to the range of [-1.0,1.0], obtaining $\\hat{P_n} =$\nClip($P_n$). Correspondingly, Y is also normalized by $w_n$ and\nclipped to [-1.0,1.0], and the result is denoted as $\\hat{Y_n}$. The local\nwindow loss based on $w_n$ is defined as $L_n = ||\\hat{Y_n} \u2013 \\hat{P_n}||_1$.\nFor SCT generation, we use two local intensity windows:\nThe first one is for soft tissues $w_{soft}$ = [-0.615, -0.368], which\ncorresponds to the HU range of [-250, 250]. The second one\nis for bones $w_{bone}$ = [-0.368, 1.0], which corresponds to the\nHU range of [250, 3000]. The local window losses corre-\nsponding to $w_{soft}$ and $w_{bone}$ are denoted as $L_{soft}$ and $L_{bone}$,\nrespectively. The overall MCL loss is defined as:\n$L_{mcl} = L_{glob} + L_{soft} + L_{bone}$\nwhere $L_{glob}$ is a loss calculated in a global contrast based\non the full HU range of [-1024, 3000], and $L_{soft}$ and $L_{bone}$\nencourages better contrasts for soft tissues and bones based\non local HU ranges, respectively."}, {"title": "3. EXPERIMENTS AND RESULTS", "content": null}, {"title": "3.1. Data and Implementation", "content": "The public SynthRAD2023 Grand Challenge dataset was\nused for experiments, and it was designed for generating\nsynthetic CT images from CBCT for radiotherapy [16]. The\ndataset consists of images from 180 patients, with each pa-\ntient having a pair of 3D head and neck CBCT and CT images\nthat have been registered. The resolution is 1\u00d71\u00d71 mm\u00b3,\nwith a median image dimension of 256\u00d7256\u00d7200. We ran-\ndomly split the dataset into 140, 20 and 20 pairs for training,\nvalidation, and testing respectively. From the 140 training\ncases, we extracted 28,631 pairs of 2D slices to train our\n2D MEUNet implemented by PyTorch. Each slice was re-\nsized to 256x256, and normalized to [-1.0,1.0]. The feature\nmap channel number at the three resolution levels was 64,\n128, and 256, respectively. The length of token was L=1024\nfor the VSS blocks. All experiments were conducted on an\nNVIDIA 2080Ti GPU with a batch size of 4. The model was\ntrained using the Adam optimizer with a learning rate of 0.02,\nrunning for 100 epochs.\nFor quantitative evaluation of sCT, we employed Struc-\ntural Similarity Index (SSIM) and Peak Signal-to-Noise Ra-\ntio (PSNR) compared with real CT images. To focus more\non the synthesis quality for human tissues, we ignored the air\nbackground, and calculated SSIM and PSNR only within the\nhuman region. In addition, we calculated SSIM and PSNR\nfor soft tissue and bone regions respectively to analyze the\nsynthesis quality for different structures of interest."}, {"title": "3.2. Ablation Study of MEUNet", "content": "To demonstrate the effectiveness of incorporating VSS blocks\ninto the skip connections of a high-resolution UNet, we con-"}, {"title": "3.3. Comparison with Existing Methods", "content": "We compared our method with four existing image transla-\ntion networks: UNet [5], UNet++ [17], SwinUnet [18], and\nMambaUnet [11]. All the compared methods used the same\nloss function $L_{glob}$, and were trained with the same hyper-"}, {"title": "4. CONCLUSION", "content": "In conclusion, this work introduces a novel framework based\non Global-Local Feature and Contrast (GLFC) learning\nfor synthesizing CT images from CBCT. It comprises a\nMamba-Enhanced UNet (MEUNet) and a Multiple Con-\ntrast Loss (MCL). MEUNet effectively integrates Mamba's\nlong-sequence modeling capabilities with UNet, enabling the\ncapture of both global and local features. The MCL simul-\ntaneously considers a global contrast and two local contrasts\nthat highlight key regions such as soft tissues and bones.\nExperimental results demonstrate that our approach achieved\nstate-of-the-art performance on the SynthRAD2023 dataset.\nThis advancement has the potential to improve CBCT's diag-\nnostic accuracy and treatment planning in radiotherapy, and\nit is of interest to investigate the performance of our method\non other medical image translation tasks in future works."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using hu-\nman subject data made available in open access. Ethical ap-\nproval was not required as confirmed by the license attached\nwith the open-access data."}]}