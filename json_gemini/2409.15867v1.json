{"title": "In-Context Ensemble Improves Video-Language Models for Low-Level Workflow Understanding from Human Demonstrations", "authors": ["Moucheng Xu", "Evangelos Chatzaroulas", "Luc McCutcheon", "Abdul Ahad", "Hamzah Azeem", "Janusz Marecki", "Ammar Anwar"], "abstract": "A Standard Operating Procedure (SOP) defines a low-level, step-by-step written guide for a business software workflow based on a video demonstration. SOPs are a crucial step toward automating end-to-end software workflows. Manually creating SOPs can be time-consuming. Recent advancements in large video-language models offer the potential for automating SOP generation by analyzing recordings of human demonstrations. However, current large video-language models face challenges with zero-shot SOP generation. We explore in-context learning with video-language models for SOP generation. We report that in-context learning sometimes helps video-language models at SOP generation. We then propose an in-context ensemble learning to further enhance the capabilities of the models in SOP generation.", "sections": [{"title": "1 Introduction", "content": "Video-language models are an emerging family of large foundational models attracting growing research interest. These models typically pre-train a visual encoder to project visual inputs into tokens, which are then utilised by large language models to interpret visual signals alongside text instructions. Recent video-language models have achieved significant success in high-level video understanding, such as high-level video summarisation, as reflected in their strong performance across diverse VQA benchmarks [1]\u2013[3]. However, current video-language models still face challenges when it comes to more complicated tasks. For instance, most available models cannot handle long videos or multiple short videos. Another challenge is their lack of competence in complex low-level video understanding [4]. Standard Operating Procedure (SOP) generation is a typical low-level video understanding task [4]. SOP involves detailed descriptions of the chronological ordering of actions taken in a software workflow on a recorded screen. The automatic generation of accurate SOPs can pave the way for the automation of business software and beyond. In this work, we explore the capability of some of the latest video-language models in SOP generation with in-context learning. In-context learning [5]\u2013[11] enables models to adapt to new tasks by learning from the examples provided during the prompt. Differing from supervised learning, In-context-learning requires no model parameter updates. In our current work, our contribution can be summarized into two-fold:\n\n1. We present the first evaluation of in-context learning in SOP generation with video-language models. We report that in-context learning consistently helps models to predict the step-by-step actions in a more accurate temporal order.\n\n2. We introduce a multimodal in-context ensemble learning approach to enhance existing in-context learning methods for video-language models in SOP generation. Our proposed in-context ensemble learning provides the models with video inputs, along with text-based pseudo labels of actions, enabling the models to learn from more examples beyond their context window limit, with a regularisation effect. We report that the proposed multimodal in-context ensemble learning consistently improves the models' predictions across most metrics."}, {"title": "2 In-Context Ensemble (ICE) Learning", "content": "By combining the popular In-Context Learning (ICL) [5]\u2013[11] in Large Language Models, and pseudo labelling in visual representation learning [12]-[14], we present Multimodal In-Context Ensemble (ICE) learning to enhance video-language models' understanding of low-level actions in videos. An illustration of the ICE pipeline is shown in Fig 1. Our ICE pipeline first applies multimodal ICL to train several video-language models on different batches of training videos, along with their gold standard SOP text labels. The SOP text labels describe the step-by-step, detailed chronologically ordered actions occurring in the videos. The same fine-tuned video-language models then create pseudo labels for the SOPs from the testing videos. Finally, another video-language model processes the testing videos, along with the pseudo labels as priors, to generate the final prediction of the SOPs.\n\nThe proposed ICE pipeline offers three key benefits:\n\n1. The first benefit is that, in the ICE stage, the model does not need to use the entire context window to store the few-shot learning video examples, allowing it to focus more on the target video while considering the pseudo-labels of the SOP as priors. This is motivated by the fact that model performance decreases with excessively long context windows.\n\n2. The second benefit is that the provided pseudo-labels sometimes share similar actions with slight variations, enforcing strong consistency regularisation that also aids the model's reasoning. Coincidentally, a recent effort also reported the effectiveness of consistency regularisation through differently perturbed prompts [15].\n\n3. Thirdly, the ICE stage enables the model to learn from more examples beyond its context window limit. For instance, during our early exploration, we could only feed up to 8 high-resolution videos as training examples to GPT-40-mini due to the context window length. However, with ICE, in our experiments, our GPT-40-mini can successfully see the equivalent of 24 videos as training examples."}, {"title": "3 Experiments", "content": "Data The WONDERBREAD benchmark [4] is a recently curated benchmark containing 2928 videos of distinct workflows and the detailed SOPs of the chronological step-by-step actions in the videos. We use a subset called \"Gold Demo\" from the WONDERBREAD benchmark of 724 videos. We randomly split the data into two parts, 30% training (217 videos) and 70% testing (507 videos). We subsequently split the training data into 28 batches, each batch contains 8 videos. Due to the limited computational resources, we only used the first 3 training batches.\n\nBaseline 1: zero-shot We aim to evaluate the effectiveness of in-context learning with video-language models in SOP generation. The original zero-shot evaluation in the WONDERBREAD benchmark [4] injects additional information such as intent (a high-level summary of the video) and action trace (a detailed log of all user interactions with the screen, including elements' coordinates). We argue that providing such additional information to the models may not necessarily reflect the actual reasoning abilities of the foundational models. Therefore, we only feed the models the raw visual information, namely the key frames of the videos."}, {"title": "4 Results", "content": "As shown in Tab. 1, we observe that GPT-40-mini generally outperforms Gemini-1.5-flash, which slightly surpasses Phi-3.5. In-context learning (ICL) consistently improves the models' ability to predict the correct order of steps compared to zero-shot baselines across different settings. ICE further enhances the models' performance, notably, for Gemini-1.5-flash, ICE resulted in a 9.22% improvement in recall. With GPT-40-mini, ICE achieves high recall at 84.79% but low precision at 44.34%, suggesting that, while the model can successfully predict most of the steps, it may have over-imputed too many intermediate steps that were omitted in the ground truth. ICE also significantly outperforms the majority voting \"Ensemble\", highlighting the necessity of multimodal training samples in SOP generation. Interestingly, the 24-shot ICL experiment with Gemini-1.5-flash shows that the model's improvements do not appear to increase linearly with the number of training samples. This may suggest that the foundational model naturally struggles to attend to too many high-resolution images at once."}, {"title": "4.1 A successful case", "content": "Ground Truth\n\n1. Click the purple/gray search bar on the top left of the screen (icon with three lines stacked).\n\n2. Type 'millennials-to-snake-people'.\n\n3. Press enter.\n\n4. Click the first pane on the top of the field of panes of projects that show up in the middle, with black text that has words like timeline, item, management, extension.\n\n5. Click the black bold \"Project Information\" tab on the top left side of the screen above \"Repository.\"\n\n6. Click the black bold \"Members\" text on the left side of the screen under \"Labels.\"\n\n7. Click the blue \"Invite members\" button on the top right side of the screen.\n\n8. Type \"yjlou.\"\n\n9. Click the pane with the colored avatar that pops up below.\n\n10. Click the right side of the text box that says \"Username or email address.\"\n\n11. Type \"allyproject.\"\n\n12. Click the pane with the colored avatar that pops up below.\n\n13. Click the right side of the text box that says \"Username or email address.\"\n\n14. Click the dropdown underneath the black text saying \"Select a role.\"\n\n15. Click \"Reporter\" from the dropdown.\n\n16. Click the blue \"Invite\" button on the bottom right.\n\nPrediction\n\n1. Navigate to Projects: Click on the \"Projects\" tab located in the top navigation bar of the GitLab dashboard, positioned next to the \"Dashboard\" tab.\n\n2. Search for Project: In the search bar at the top of the Projects page, which is labeled \"Search GitLab,\" click to focus on it.\n\n3. Type Project Name: Type \"millennials-to-snake-people\" into the search bar.\n\n4. Select Project: Wait for the suggestions to load and then click on the project name \"millennials-to-snake-people\" from the dropdown suggestions to navigate to the project's main page.\n\n5. Access Members Section: In the left sidebar of the project page, click on \"Project information\" to expand the section.\n\n6. Click Members: Under the expanded \"Project information\" section, click on the \"Members\" option to go to the members page.\n\n7. Open Invite Members Dialog: In the Members section, locate and click the blue \"Invite members\" button on the top right side of the screen to open the invite members dialog.\n\n8. Focus on Username Field: In the \"Invite members\" pop-up window, click into the text box labeled \"Username or email address.\"\n\n9. Type Username: Type \"yjlou\" into the text box to search for the user you want to invite.\n\n10. Select User from Results: Wait for the system to display the search results. When \"yjlou\" appears in the results, click on it to select that user.\n\n11. Open Role Dropdown: Below the selected username, click on the dropdown menu labeled \"Select a role\" to choose the role for the invited member.\n\n12. Choose Role: From the dropdown options, select \"Reporter\" by clicking on it to set the user's access level.\n\n13. Leave Expiration Date Blank: If necessary, locate the \"Access expiration date (optional)\" field and leave it blank.\n\n14. Send Invitation: Click the blue \"Invite\" button located at the bottom right of the pop-up window to send the invitation.\n\n15. Confirm Invitation Success: After the invitation is sent, observe the confirmation message that appears, indicating that the member has been successfully added.\n\n16. Review Members List: Finally, review the list of project members to ensure that \"yjlou\" now appears in the members' list, confirming the successful invitation."}, {"title": "4.2 A failed case", "content": "Ground Truth\n\n1. Click on \"Explore\" under the \"Projects\" heading\n\n2. Click on the search bar in the top right\n\n3. Type in \"metaseq\"\n\n4. From the results below, click on \"Administrator / metaseq\"\n\n5. Click on the \"Repository\" tab in the left sidebar.\n\n6. Click on the \"Contributors\" tab in the left sidebar.\n\n7. Report the information of the top three people listed (top left, top right, second left)\n\nPrediction\n\n1. On the GitLab dashboard, locate the \"Projects\" tab in the top navigation bar, situated between the \"Dashboard\" and \"Groups\" options, and click on it.\n\n2. In the \"Projects\" section, observe the list of projects displayed. Scroll down until you can see the project titled \"The A11Y Project / a1lyproject.com\", which is the fourth project from the top.\n\n3. Click on the project titled \"The A11Y Project / a1lyproject.com\" to navigate to its main page.\n\n4. Once on the project page, locate the \"Project Information\" tab on the top left side, directly under the project name, and click on it.\n\n5. In the expanded options under \"Project Information,\" click on the \"Members\" tab, located directly beneath the \"Project Information\" heading.\n\n6. Click the blue \"Invite members\" button located at the top right of the members section, next to the \"Invite a group\" option.\n\n7. In the popup window that appears, click on the text box labeled \"Username or email address.\"\n\n8. Type \"Koushik\" into the \"Username or email address\" text box.\n\n9. Press Enter to select \"Koushik Dutra\" from the dropdown that appears as you type.\n\n10. Click on the dropdown labeled \"Select a role\" located directly beneath the username text box.\n\n11. Choose \"Guest\" from the options available in the dropdown list.\n\n12. Click the blue \"Invite\" button located at the bottom right of the popup window inside the shaded area to send the invitation.\n\n13. Confirm that the member has been successfully added by checking for a notification on the screen that states \"Members were successfully added.\""}, {"title": "5 Analysis", "content": "Fig. 5 contains screenshots of task \"317-2024-02-17-14-32-37,\" which was tested with GPT-40-mini; the model hallucinated the SOP. As shown in Fig. 4, this task involves listing the names and number of commits of the top three contributors to a GitLab repository called \"metaseq.\" However, the model's prediction is a completely different SOP for adding new guests to the GitLab repository. This indicates that the model does not fundamentally understand the software.\n\nWe visualised the accumulated histograms of the metrics in the above Fig.6. The 50th percentile of recall is above 90%, showing that the model is capable at predicting the actions in the ground truth. However, the 50th percentile of precision is lower than 50%, and the 50th percentile of temporal ordering is even lower at, less than 40%. We hypothesise that the unsatisfactory performance in precision and ordering metrics might be due to the model having its own preferences for the style in which the output steps are written.\n\nTo explore this further, we examined the relationship between the lengths of the SOPs and the model's performance, as SOP length reflects the model's preferred writing style. In Fig.7, we plotted the kernel density estimates of both the gold and predicted SOPs. The predicted SOPs have an average of 18.01 steps, with most concentrated around this value. In contrast, the gold SOPs average 8.44 steps, with a more even distribution across cases. This suggests that the model fails to capture the diversity seen in the gold SOPs, implying that it may have developed its own writing style to write the SOPs, that is different from the styles shown in the training examples. Additionally, we plotted the ratios between the number of SOP lines and the number of frames in Fig.8, observing a wide distribution. This raises the possibility that even among the ground truth SOPs, there may be inconsistencies in writing styles, hindering the effectiveness of in-context learning.\n\nWe then further grouped the predictions according to the length of their ground truth SOPs in the following Fig. 9, 10, 11. It is clear to see that, the more different between the lengths of the ground truth SOPs and the mean length of the predicted SOPs, the worse the performances."}, {"title": "6 Conclusion", "content": "We conducted an exploratory evaluation of in-context learning for SOP generation and proposed an In-Context Ensemble (ICE) learning approach using pseudo-labels for SOP generation. Our preliminary results suggest that the proposed ICE pipeline is effective in enhancing foundational video-language models in SOP generation. However, we also observe that the current performance of available foundational models has not yet reached a satisfactory level in SOP generation, highlighting the challenges of low-level video understanding from visual inputs alone."}]}