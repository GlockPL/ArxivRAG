{"title": "CONTROLLING SURPRISAL IN MUSIC GENERATION VIA INFORMATION CONTENT CURVE MATCHING", "authors": ["Mathias Rose Bjare", "Stefan Lattner", "Gerhard Widmer"], "abstract": "In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems. We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.", "sections": [{"title": "1. INTRODUCTION", "content": "In music generation, controlling the generation process with user inputs is essential for creating flexible systems that support a creative human/machine co-creation process [1]. Typically, controls are based on low-level features with a direct musical interpretation, for instance, the pitch of a generative synthesizer [2], or meter, harmony, and instrumentation for symbolic generation [3]. A high-level musical feature that has received little attention in generative composition systems is musical surprisal how surprising a musical event is to a listener, given the past musical context. The surprisal tends to be high when the music is complex, when a pitch deviates from the prevailing tonality, or when there is a variation in rhythm [4, 5]. As such, musical surprisal shares similarities with musical complexity, however, it is importantly also affected by learning: Repeating complex musical content can lead to decreased surprisal on the repetitions as a result of learning [6]. In contrast, the musical content and the complexity remain unchanged across repetitions.\nStudies suggest that the amount of musical surprisal needs to be balanced for music to be deemed preferable [7, 8], which is typically achieved by balancing regularity and novelty [9]. Being able to control surprisal in generated music might help users create compositions that balance regularity and novelty and thus suit listeners' preferences. In addition, if this can be controlled, rather low surprisal could be used indirectly to induce repetitions in machine-generated music and high surprisal to produce novel parts, possibly with high perceived complexity.\nIn [10], it was proposed to quantify the surprisal of a musical event by its Information Content (IC) conditioned on past musical events. For that, a sequence of musical events is modeled as a stochastic process, where the conditional distribution and, hence, the conditional IC can be estimated. As such, a surprising event is an event that is unlikely to occur under the estimated distribution given the past musical context. In the works of [11], the authors find correlations between the IC of a variable-order Markov model (called IDyOM) [12] and perceived surprise in a controlled pitch anticipation experiment. A correlation between high IC and tonal and rhythmic complexity was shown in [4, 5].\nThis indicates that the IC of trained sequence models can be used as a proxy for human perception of musical surprisal and that its measurement can identify musical complexity and regularities. This paper proposes a novel framework for generating music with user control over the IC. Specifically, we define an Instantaneous Information Content (IIC) measure, which can be calculated at any time point based on the IC of musical events in the recent past and approximates a causal information density. We use the IIC as a fitness score to direct a beam search toward generating samples following a given IIC target curve. Our sampling strategy can be used with any pretrained autoregressive generative music model. We demonstrate our approach in symbolic classical music generation using a"}, {"title": "2. METHODS", "content": "In the following, we describe a method for IC-controlled token sequence generation. Let $IC^* (t)$ be a target curve with support in the time interval [0, T], representing the desired information content over time of a generated sequence of tokens $x = x_1, x_2, ... x_n \\in X$, with a duration of T seconds. 'Tokens' are not necessarily individual notes or note onsets but can be any token type commonly used in Transformer-based music generation systems (e.g., [13-15]). Also, note that we operate on the physical time dimension, not symbolic (score) time measured, e.g., in beats or number of tokens.\nFurthermore, let q be a generative sequence model and p an autoregressive critic model, used for estimating the the i'th token's conditional token information content\n$IC(x_i | x_{<i}) = -log\\ p(x_i | x_{<i}),$ (1)\nwhere $x_{<i>= x_1, x_2, ..., x_{i-1}$. In our context, p will be a Transformer model. The proposed method creates new samples using q with an information content that matches the target curve as measured by p. Our method works as follows: Firstly, we define the Instantaneous Information Content (IIC) \u2013 a mapping from a (temporally irregular) token sequence and its information content values to a function representing the musical surprisal in the continuous time domain. Secondly, we define an IC deviation \u2013 a metric for comparing the similarity between a sequence's IIC curve and the target curve. Finally, we devise a method for generating token sequences with q that minimize the IC deviation."}, {"title": "2.1 Instantaneous Information Content", "content": "To align the information content of musical events, measured on sequence tokens, with the time-domain target IC ($IC^*$), we face a challenge: IC is calculated on sequence elements, while $IC^*$ pertains to the time domain. Our solution involves assigning each token a temporal position using a mapping function f, effectively \u201ctemporally localizing\" or aligning tokens within the musical timeline. Note that f can be constructed by analyzing the specific detokenization method associated with x's tokenization that involves turning a sequence of tokens into a time-based music representation like MIDI\u00b9 . In section 3.1, we present an example of such f using the tokenization of [13].\nTemporal Localization allows us to map IC tokenizations to their respective time points in the music. This is crucial, especially for analyzing tokenizations of symbolic music commonly used with Transformers [13-15], where the decoded musical events do not uniformly align in time. Through this approach, IC measured on tokens can be directly compared with the time-domain $IC^*$, facilitating a coherent analysis across different domains of musical representation."}, {"title": "2.1.2 Interpolation", "content": "Let $f: N \\times X \\rightarrow R$ be a localization function, mapping the i'th token of sequence $x \\in X$ to the time domain. The IIC at time t in a piece (represented by token sequence x), is a real number computed by a time interpolation of x's token ICs:\n$IIC(t, x) = \\sum_{f(i,x)<t}  \\Lambda(t - f(i, x), i)\\ IC(x_i | x_{<i}).$ (2)\n$ \\Lambda(t, i)$ defines a weighting of the information of the i'th token and the constraint f(i, x) < t ensures causality. As a result, the IIC at any time step t is a weighted sum of IC values of past events, using a weighting kernel \u03bb.\nThe choice of the critic model p in combination with the weight function \u03bb defines different perceptual models of the instantaneous information content. We propose to choose \u03bb so that the recent past is weighted higher than the remote past. More specifically, we define A as a window function centered around t and equal to zero at time steps greater than t. In this initial work, we chose a Hann window for the following reasons: As it is (half) bell-shaped, it is insensitive to inaccuracies in the temporal localization of recent events. It is smooth at the boundaries, preventing sudden drops as events \"leave\" the window.\nUsing the IIC, we quantify the segment surprisal of segment [t1, t2] by the L\u00b9 norm of the IIC with support restricted to [t1, t2] by calculating:\n$||IC||_1 = \\int_{t_1}^{t_2} |IIC(t, x)| \\ dt.$ (3)\nIn section 3.5, we compare segment surprisal with segment-based complexity metrics."}, {"title": "2.2 IC Deviation", "content": "Given a sample x, the IC deviation of IIC(\u00b7, x) from the target $IC^*$ is defined as the L\u00b9 norm of their function difference:\n$||IC^* - IIC||_1 = \\int |IC^*(t) - IIC(t, x)| \\ dt.$ (4)\nWhich is equal to zero if $IC^* = IIC(\\cdot, x)$ almost everywhere, implying that minimizing eq. (4), aligns the target curve $IC^*$ with the IIC curve. In practice, we compute eq. (4) by the Riemann sum:\n$||IC^* - IIC||_1 \\approx \\sum_{i=1}^m |IC^*(t_i) - IIC(t_i, x)| \\Delta t,$ (5)\nwhere $m \\Delta t = T$."}, {"title": "2.3 Information Content Conditioned Sampling", "content": "We can now rank sequences of different lengths according to their proximity to the target $IC^*$ using eq. (5). We use this to guide a beam search to follow the target curve. The beam search is done in iterations. At each iteration, we generate k continuations of the best-performing sample from the last iteration (initially the empty sequence) in parallel. We stop expanding the continuation when the duration of the newly generated content exceeds a predefined step size t'. We then evaluate eq. (5) and keep only the continuation with the lowest IC deviation for the next iteration \u00b2. We stop when the generation's duration is T."}, {"title": "3. EXPERIMENTS", "content": "All experiments are performed with a PIA Transformer model [13], a symbolic music generation system pretrained on expressive classical piano performances. The model was trained on data consisting of 1,184 MIDI files of expressive music recorded with high precision on a Yamaha Disklavier [16], as well as a larger dataset of 10,855 MIDI files containing automatically transcribed piano performances [17]. For evaluation, we use the dataset of [18], consisting of performances of 36 Mozart piano sonata movements. The midi files are tokenized using a structured MIDI encoding [13], where midi notes, sorted by their onset times, are serialized successively using four tokens Pitch, Velocity, Duration, Timeshift in that order. Therefore, every fourth token represents the same token type. Pitch is an integer describing the 88-note pitch values on the piano. Velocity is an integer describing the 128 possible midi velocity values. Duration is an integer representing quantized note duration in seconds:\n{0.02, 0.04, ..., 1.0, 1.1, ..., 5.0, 6.0, ..., 19.0}. Timeshift is an integer encoding the inter-onset intervals (IOI, i.e., the time durations between subsequent note onsets). Timeshift is quantized similarly to the duration token, with the addition of an extra symbol representing a time shift of zero, allowing the model to understand that notes less than 0.02 seconds apart are to be played concurrently. In contrast to the PIA model described in [13], which does non-causal inpainting, we use a causal Transformer based on the Perceiver IO architecture [19] and do continuation generation \u00b3. We make these modifications such that the IC calculations ignore future observations. We use the same pretrained model both as the generator model q and the critic model p and leave the exploration of other critic models for future work."}, {"title": "3.2 IIC", "content": "The elements involved in computing the IIC are given in fig. 1. For IIC calculations, we choose to consider only the surprisal of Pitch and Timeshift tokens, such that the token's IC represents the surprisal of pitches and IOI. We ignore Velocity and Duration tokens because they contribute less to the perception of surprise, being mostly related to the performance dimensions dynamics and articulation. This is achieved in the IIC calculation by setting \u03bb(t, i) = 0 for i = 2,6,10, ... and i = 3, 7, 11, ... in eq. (2). We choose f such that the pitch token contributes to the surprisal function at its note onset time\u2074, and the timeshift token contributes to its surprisal at the onset of the following note (as an IOI is perceived at the onset of the next note). The remaining weights are then defined by the scaled half Hann window\n$\\Lambda(t, i) =  \\begin{cases}  c_i cos^2(\\frac{t \\pi}{L})  & \\text{for } 0 < t < \\frac{L}{2} \\\\  0 & \\text{otherwise}  \\end{cases}$, (6)\nwhere ci is a weight that takes on two different values for the pitch and timeshift tokens, respectively. ci is used to weigh the IC of pitches and timeshifts, respectively. For both token types to have equal importance, we estimate a normalization constant empirically by calculating a mean IC over all tokens of the evaluation dataset. The window length is chosen to be L = 4 so that the weight is zero after 2 seconds."}, {"title": "3.3 Beam Search Parametrization Study", "content": "Using the beam search strategy described in section 2.3, we run initial experiments to determine the effect of parameters associated with the beam search on the similarity between generated samples and target curves $IC^*$ extracted from real music. Specifically, we randomly select 400 snippets of the MIDI files (10 seconds long) and create the IIC curve associated with those snippets. Then, we generate four new samples using our beam search and evaluate the IC deviation between the IIC curve induced by the"}, {"title": "3.4 Qualitative Evaluation", "content": "We conducted an online user study to investigate if the IIC curves computed on generated and real music correspond to users\u2019 experience of being musically surprised.\nFirstly, we present the participant with a musical section generated by our method using one of five target curves. The participant is then tasked to select the IIC curve that best describes their perceived surprise when listening to the section. Secondly, we present the user with a segment of real music and IIC curves extracted from real music, one of which corresponds to the music segment. The user\u2019s task is to identify the corresponding curve.\nThe experiment is conducted on a website that, after an initial experiment description, asks the user for their years of musical training (more or less than five years). Then, it shows an example of a generated piano music section and the surprisal curve used as a target for the generation (together with a textual explanation).\nThe participant is then presented with five pages, like the one in fig. 2. Each presents a musical section generated using one of five simple target curves. The participant is asked to identify which of the five curves they think has been used to generate the section. The final page contains a 10-second segment of real piano music from the evaluation set and two IIC curves, one corresponding to the piano"}, {"title": "3.5 Analysis of IIC", "content": "As discussed in the introduction, IC and surprisal might be related to aspects of musical complexity, but learning effects may lead to a decrease in surprisal in passages with repeated musical content. To investigate these relationships, we designed an experiment to determine if the IIC correlates with harmonic complexity, as quantified by tonal tension (cloud diameter) [20], where the IIC is calculated using progressively larger segments of musical context. Tonal tension is calculated for a segment of music by considering its most dissonant pitch class interval, where an interval dissonance is measured as the distance between the interval pitches embedded in a specific Euclidian space where the position is based on the circle of fifths [21].\nWe extracted one-second segments centered on the onsets of notes in the evaluation dataset. For i = 1, ..., 1000, we then compute the Pearson correlation coefficient between the tonal tension and the segment surprisal (see eq. (3)) of the first i segments within every performance.\nIn addition, we investigate complexity in terms of note density, i.e., the number of notes per segment. To do so, we use the same setup as for tonal tension but count the number of notes within one-second segments.\nFinally, we investigate rhythmical complexity using the IOI histogram entropy of measures [22]. We choose this measure over other structural rhythmical complexity measures [23-28] since it does not assume the rhythm to be cyclic. We follow the same procedure as mentioned above, but instead of selecting fixed-sized segments centered around note-onsets, we select segments of one measure based on the measure annotations [18]. More specifically, we match the notes of the performance with its score notes and extract for each measure: 1) the normalized entropy of the score notes IOI histogram and 2) the segment IIC of the measure normalized with the length of the measure. The segment boundaries are estimated by the mean onset time of the first and last note in subsequent measures."}, {"title": "5. CONCLUSION", "content": "In this study, we introduced a novel framework for controlling musical surprisal through Instantaneous Information Content (IIC), which maps token-based surprisal to a continuous time-domain function. Using a beam search algorithm, we demonstrated that our approach can generate music that closely follows predefined IIC curves, effectively aligning generated and target surprisal curves.\nOur user study confirmed that participants could reasonably identify target IIC curves from generated music, indicating that our method captures perceptible aspects of musical surprise. Furthermore, our analysis showed that IIC correlates with measures of musical complexity such as tonal tension and note density.\nFuture work will explore alternative critic models, like personalized models, trained on music that is familiar to the user or models with smaller context windows to more directly control local musical complexity."}]}