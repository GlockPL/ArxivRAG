{"title": "EdgeShield: A Universal and Efficient Edge Computing Framework for Robust AI", "authors": ["Duo Zhong", "Bojing Li", "Xiang Chen", "Chenchen Liu"], "abstract": "The increasing prevalence of adversarial attacks on Artificial Intelligence (AI) systems has created a need for innovative security measures. However, the current methods of defending against these attacks often come with a high computing cost and require back-end processing, making real-time defense challenging. Fortunately, there have been remarkable advancements in edge-computing, which make it easier to deploy neural networks on edge devices. Building upon these advancements, we propose a edge framework design to enable universal and efficient detection of adversarial attacks. This framework incorporates an attention-based adversarial detection methodology and a lightweight detection network formation, making it suitable for a wide range of neural networks and can be deployed on edge devices. To assess the effectiveness of our proposed framework, we conducted evaluations on five neural networks. The results indicate an impressive 97.43% F-score can be achieved, demonstrating the framework's proficiency in detecting adversarial attacks. Moreover, our proposed framework also exhibits significantly reduced computing complexity and cost in comparison to previous detection methods. This aspect is particularly beneficial as it ensures that the defense mechanism can be efficiently implemented in real-time on-edge devices.", "sections": [{"title": "I. INTRODUCTION", "content": "Edge devices and sensors usually run in a complex and noisy environment, where data are exposed to contamination with a high possibility. Combating these contamination usually caused by adversarial attacks requires deep neural network (DNN) with specific anti-interference optimization. However, the extremely limited computing resources of edge devices severely limit local deployment of high-workload robustness-strengthened neural networks. As a result, given security concerns in practical applications, the centralised approach is still the most commonly used. In this mainstream mode, all the data collected by the edge sensor end will be transmitted to the central computing end (back end) regardless of perturbations and even attacks, and robustness-strengthened neural networks on the back end will shoulder the responsibility of fighting such adversarial attacks. This creates redundant communication overhead and increases the workload on the computing center, which goes against the trend towards edge computing. If we could isolate the task of combating adversarial attacks from robustness-strengthened neural networks and assign it to edge devices, this shift would considerably reduce communication overhead for tainted data would be filtered out at the source. Consequently, freed from the burden of security concerns, the back-end can reallocate more resource towards advanced computational tasks, optimizing the overall system efficiency and aligning with the goals of edge computing.\nThe adversarial attacks, especially adversarial patch attacks, a small yet carefully crafted patch is placed randomly on an image obtained by the sensor [1]\u2013[3] have emerged as an urgent threat [1]\u2013[8]. These patches have been proven to cause the back-end neural networks to generate abnormal high activation layer by layer [9], which weakens the authentic features of the original image and deceives the back-end system into misidentifying the image. The simplicity of training and the low-cost deployment of such attacks makes them attractive to malicious actors seeking vulnerabilities in visual intelligence systems. As a result, the trustworthiness and reliability of these systems are compromised.\nAlthough many works have been proposed to tackle the security issue of intelligence systems caused by adversarial patch attacks, none of them are feasible to be deploy on edge devices. For example, Digital Watermarking (DW) [10] and Local Gradient Smoothing (LGS) [11] were proposed as patch removal techniques that locate and erase (or mask) patches. LanCe [12] defined Input Semantic Inconsistency for patch detection. Yu [9] introduced Feature Norm Clipping (FNC) layer between original model layers. However, these approaches heavily rely on optimizing the neural networks on back end, which involves complex execution and large-scale deployment of these models. Such resource-intensive"}, {"title": "II. BACKGROUND", "content": "A. AI Robustness\nThe increasing advancements in DNNs have led to their widespread adoption in various AI systems, including but not limited to facial recognition, object detection, autonomous driving, and surveillance systems. The large-scale DNNs such as VGG-16 [13], ResNet-101 [14], MobileNet-V2 [15], etc. are powerful with complex structures. However, these models are vulnerable to visual attacks, particularly adversarial patch attacks, where a small universal patch trained on a limited dataset is attached to the model's inputs [2]. Patches trained against one model exhibit varying levels of transferability to other models. Therefore, this work focuses on developing a universal and light-weight model with low-cost detection methodology for detecting all adversarial patches trained against varying models.\nAlthough many approaches have been proposed to address this issue, they all suffer from certain limitations. DW [10] and LGS [11] perform poorly in patch detection and can compromise significant parts of the clean image. LanCe [12] detects inconsistencies between the output of the model's last layer and the synthesized pattern of the predicted category. This approach requires computing thresholds for synthesized patterns of each category, which is resource- and time-consuming. Furthermore, the computation of the inconsistency requires knowledge of the predicted class of the output, limiting its use to post-prediction correction. Yu et al. [9] proposed adding FNC layers to clip the outputs of specific layers in the model. The FNC clipping strategy involves replacing the values of all points in the feature map where the L-2 norm is greater than the mean. However, this clipping strategy also affects the larger values in the feature map of clean images that contribute"}, {"title": "III. DESIGNED METHODOLOGY", "content": "As shown in Fig. 1, the proposed framework consists of two parts: detection and inference. The detection model is selected shallow layers of a neural network (e.g., VGG 16) and is deployed on an edge device that collects inputs through sensors. The detected adversarial images will be discarded and only clear images will be uploaded to the back-end computing centre for inference. The inference part is a full DNN deployed on the server such as VGG-16, ResNet-101, Inception-V3, etc.\nA. Attention-based Method for Attacks Detection\nIt is observed that adversarial patches mislead classifier result by causing some significant abnormalities that overwhelm the original decision portion in the last convolution activation map [9], [12]. Instead of introducing complex calculations like L-2 norm or inconsistency in previous works, we propose a low-cost attention-based methodology that uses the attention map generated from the activation map of a particular layer for detection. Each point in attention map is defined as Eq. 1.\n\n$A_T(F, h, w) = \\frac{1}{C}\\sum_{i=1}^{C} F_{h,w}(i)$\n\n(1)\n\n$F \\in \\mathbb{R}^{C \\times H \\times W}$ is the activation map of a certain layer in the model. $F_{h,w}(i)$ represents the activation value of the $i_{th}$ channel at coordinates $(h, w)$ in activation map. The attention map is calculated by taking the mean of the activation map along the channel dimension C. As such, an attention map $A_T(F)$ of size H * W for the activation map F is obtained.\nUtilizing VGG-16 as an example, as shown in Fig. 2, the attention maps generated by a specific activation layer show significant differences between clean and perturbed inputs. Specifically, the attention map for perturbed input exhibits a higher focus on the patch region. Inspired by this observation, we assume that the maximum value in the attention map contains the most salient features, and we define it as the indicator $I_r$:\n\n$I_r(A_T) = \\max A_T(F, h, w)$\n\n(2)\nTo leverage attention maps for adversarial detection, we first compute $I_r$ values for perturbed and their corresponding clean samples on a specific convolution layer of a DNN model. Then, we determine a threshold value $ \\Theta$ to distinguish the perturbed and clean samples with a confidence $p$, using only the $I_r$ values from clean samples.\n\n$p = \\frac{\\sum_{A_T \\in A^{clean}} \\mathbb{I}(I_r(A_T) \\leq \\Theta)}{|A^{clean}|}$\n\n(3)\n\n$A^{clean}$ represents a set of attention maps generated from numerous clean samples. $|A^{clean}|$ indicates the number of elements in $A^{clean}$. $ \\mathbb{I}(\\cdot)$ is an indicator function that returns 1 if the condition in parentheses is True, and 0 otherwise. Given a confidence level p (usually set to 0.95), we calculate a threshold value $ \\Theta$ such that p fraction of the clean samples satisfy $I_r(A_T) < \\Theta$. It is worth noting that our method"}, {"title": "B. Detection Layer Selection", "content": "Instead of involving the full DNN model to detection, we propose to select a few layer of a model to enable local deployment. To determine the layers to be selected, $I_r$ indicators on various convolution layers of a DNN model is collected and their frequency histograms are shown in Fig. 3. The results demonstrate that the distributions of clean and perturbed $I_r$ are more separate in shallower layers. This observation can be interpreted based on the principle of convolution operation and adversarial patches. Adversarial patches are designed to draw more attention to the areas where it placed of an image, which often results in increasing values in patches area of attention maps. With this impact propagating to larger range pixels in deeper convolution layers to increase the overall impact, the increasing magnitude for individual points will inevitably weaken, i.e. the influence on upper bound of attention map will weaken. Thus, searching the upper bound of the attention map at shallower layers make most sensitive way to keep track the significant differential, i.e. the indicator $I_r$ at shallower layers.\nAfter analyzing the results mentioned above, we have confirmed that $I_r$ is a highly precise indicator for detecting attacks in shallow convolution layers. Consequently, selecting such layers as the detection layer can significantly decrease hardware costs during implementation. In addition, our approach enables real-time attack detection by eliminating the requirement of running the complete inference of DNNs for detection and only choosing a few shallow layers to form the detection model. For example, the detection model with the first two convolution layer of VGG-16 demonstrates optimal performance in the subsequent parts."}, {"title": "IV. EXPERIMENT AND EVALUATION", "content": "A. Experiment Setup\nFig. 4 indicates the implementation detail of the proposed detection. The framework includes two phases: the analysis, where the threshold $ \\Theta$ for a deployed DNN is calculated based on clean samples, and the evaluation, which assesses the detection performance.\nExperiment Dataset: A random sampling of the ImageNet dataset [21] allocates 40% for analysis and 60% for testing purposes. Half of the test data are attacked to create a balanced set of positive (attacked) and negative (clean) samples. The analysis dataset is used to generate the threshold, derived from $I_r$ indicators, for detection deployment. Additionally, the effectiveness of this calculated threshold will be evaluated using the testing dataset."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a universal and efficient edge computing framework for robust AI. The framework leverages the attention map derived from the shallow layers' feature map of a DNN model to effectively identify a broad spectrum of visual attacks. The experiments conducted in this study have demonstrated the effectiveness and universal applicability of our approach, achieving an F-score of at least 97.4% in detecting various attacks. Furthermore, our method offers several advantages for implementation on edge devices. It requires less computational cost and memory for registers and eschews introducing complex multiplication. The latency and energy consumption have been improved, achieving a speedup ranging from 2.46x to 30.04x and energy savings between 55.27% and 97.05%. These characteristics make it highly suitable for resource-constrained environments. Additionally, our approach does not require any knowledge of or modifications to the back-end AI model, enabling seamless integration with widely-deployed industrial AI models. Overall, our framework demonstrates the universality and efficiency of edge solutions for robust AI."}]}