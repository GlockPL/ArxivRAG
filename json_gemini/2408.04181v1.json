{"title": "EdgeShield: A Universal and Efficient Edge Computing Framework for Robust AI", "authors": ["Duo Zhong", "Bojing Li", "Xiang Chen", "Chenchen Liu"], "abstract": "The increasing prevalence of adversarial attacks on Artificial Intelligence (AI) systems has created a need for innovative security measures. However, the current methods of defending against these attacks often come with a high computing cost and require back-end processing, making real-time defense challenging. Fortunately, there have been remarkable advancements in edge-computing, which make it easier to deploy neural networks on edge devices. Building upon these advancements, we propose a edge framework design to enable universal and efficient detection of adversarial attacks. This framework incorporates an attention-based adversarial detection methodology and a lightweight detection network formation, making it suitable for a wide range of neural networks and can be deployed on edge devices. To assess the effectiveness of our proposed framework, we conducted evaluations on five neural networks. The results indicate an impressive 97.43% F-score can be achieved, demonstrating the framework's proficiency in detecting adversarial attacks. Moreover, our proposed framework also exhibits significantly reduced computing complexity and cost in comparison to previous detection methods. This aspect is particularly beneficial as it ensures that the defense mechanism can be efficiently implemented in real-time on-edge devices.", "sections": [{"title": "I. INTRODUCTION", "content": "Edge devices and sensors usually run in a complex and noisy environment, where data are exposed to contamination with a high possibility. Combating these contamination usually caused by adversarial attacks requires deep neural network (DNN) with specific anti-interference optimization. However, the extremely limited computing resources of edge devices severely limit local deployment of high-workload robustness-strengthened neural networks. As a result, given security concerns in practical applications, the centralised approach is still the most commonly used. In this mainstream mode, all the data collected by the edge sensor end will be transmitted to the central computing end (back end) regardless of perturbations and even attacks, and robustness-strengthened neural networks on the back end will shoulder the responsibility of fighting such adversarial attacks. This creates redundant communication overhead and increases the workload on the computing center, which goes against the trend towards edge computing. If we could isolate the task of combating adversarial attacks from robustness-strengthened neural networks and assign it to edge devices, this shift would considerably reduce communication overhead for tainted data would be filtered out at the source. Consequently, freed from the burden of security concerns, the back-end can reallocate more resource towards advanced computational tasks, optimizing the overall system efficiency and aligning with the goals of edge computing.\nThe adversarial attacks, especially adversarial patch attacks, a small yet carefully crafted patch is placed randomly on an image obtained by the sensor [1]-[3] have emerged as an urgent threat [1]-[8]. These patches have been proven to cause the back-end neural networks to generate abnormal high activation layer by layer [9], which weakens the authentic features of the original image and deceives the back-end system into misidentifying the image. The simplicity of training and the low-cost deployment of such attacks makes them attractive to malicious actors seeking vulnerabilities in visual intelligence systems. As a result, the trustworthiness and reliability of these systems are compromised.\nAlthough many works have been proposed to tackle the security issue of intelligence systems caused by adversarial patch attacks, none of them are feasible to be deploy on edge devices. For example, Digital Watermarking (DW) [10] and Local Gradient Smoothing (LGS) [11] were proposed as patch removal techniques that locate and erase (or mask) patches. LanCe [12] defined Input Semantic Inconsistency for patch detection. Yu [9] introduced Feature Norm Clipping (FNC) layer between original model layers. However, these approaches heavily rely on optimizing the neural networks on back end, which involves complex execution and large-scale deployment of these models. Such resource-intensive processes are not practical for edge devices, where computing power and memory capacity are limited.\nAs research findings have demonstrated, feature activation maps from attacked images typically exhibit abnormal higher values in some regions compared to those from original images. These abnormalities are considered low-level, or coarse features, which inspired us the thought to develop a specialized light-weight neural networks that can be easily deployed at resource constrained edge devices for their extraction. This aligns with our initial idea of transferring the task of combating adversarial attacks to edge devices. Moreover, this novel strategy can achieve real-time attacks detection without waiting for the back end processing, making a significant step forward in the responsiveness of AI applications.\nIn our work, we propose an edge computing framework that can effectively and efficiently detect abnormal data attacked by adversarial patches and purify sensory data. As shown in Fig 1, the main component of this framework is a lightweight yet versatile detection model. This unified model is capable of detecting a wide range of adversarial patches that pose threats to most DNN models. It is solely composed of a few shallow layers from the back end classical DNN models, which allows it to be deployed locally. Additionally, we have developed an attention-based methodology to achieve accurate detection while introducing only simple computations. By preventing contaminated data from being transmitted to the back-end cloud, our framework reduces communication bandwidth, enhances the accuracy, and reduces the workload of the back-end intelligence system."}, {"title": "II. BACKGROUND", "content": "The increasing advancements in DNNs have led to their widespread adoption in various AI systems, including but not limited to facial recognition, object detection, autonomous driving, and surveillance systems. The large-scale DNNs such as VGG-16 [13], ResNet-101 [14], MobileNet-V2 [15], etc. are powerful with complex structures. However, these models are vulnerable to visual attacks, particularly adversarial patch attacks, where a small universal patch trained on a limited dataset is attached to the model's inputs [2]. Patches trained against one model exhibit varying levels of transferability to other models. Therefore, this work focuses on developing a universal and light-weight model with low-cost detection methodology for detecting all adversarial patches trained against varying models.\nAlthough many approaches have been proposed to address this issue, they all suffer from certain limitations. DW [10] and LGS [11] perform poorly in patch detection and can compromise significant parts of the clean image. LanCe [12] detects inconsistencies between the output of the model's last layer and the synthesized pattern of the predicted category. This approach requires computing thresholds for synthesized patterns of each category, which is resource- and time-consuming. Furthermore, the computation of the inconsistency requires knowledge of the predicted class of the output, limiting its use to post-prediction correction. Yu et al. [9] proposed adding FNC layers to clip the outputs of specific layers in the model. The FNC clipping strategy involves replacing the values of all points in the feature map where the L-2 norm is greater than the mean. However, this clipping strategy also affects the larger values in the feature map of clean images that contribute"}, {"title": "III. DESIGNED METHODOLOGY", "content": "As shown in Fig. 1, the proposed framework consists of two parts: detection and inference. The detection model is selected shallow layers of a neural network (e.g., VGG 16) and is deployed on an edge device that collects inputs through sensors. The detected adversarial images will be discarded and only clear images will be uploaded to the back-end computing centre for inference. The inference part is a full DNN deployed on the server such as VGG-16, ResNet-101, Inception-V3, etc.\nIt is observed that adversarial patches mislead classifier result by causing some significant abnormalities that overwhelm the original decision portion in the last convolution activation map [9], [12]. Instead of introducing complex calculations like L-2 norm or inconsistency in previous works, we propose a low-cost attention-based methodology that uses the attention map generated from the activation map of a particular layer for detection. Each point in attention map is defined as Eq. 1.\n$A_T(F, h, w) = \\frac{1}{C} \\sum_{i=1}^C F_{h, w}(i)$ (1)\n$F \\in R^{C*H*W}$ is the activation map of a certain layer in the model. $F_{h,w}(i)$ represents the activation value of the ith channel at coordinates (h, w) in activation map. The attention map is calculated by taking the mean of the activation map along the channel dimension C. As such, an attention map $A_T(F)$ of size H * W for the activation map F is obtained.\nSpecifically, the attention map for perturbed input exhibits a higher focus on the patch region. Inspired by this observation, we assume that the maximum value in the attention map contains the most salient features, and we define it as the indicator $I_r$:\n$I_r(A_T) = \\max A_T(F, h, w)$ (2)\nTo leverage attention maps for adversarial detection, we first compute $I_r$ values for perturbed and their corresponding clean samples on a specific convolution layer of a DNN model. Then, we determine a threshold value $\\Theta$ to distinguish the perturbed and clean samples with a confidence p, using only the $I_r$ values from clean samples.\n$p = \\frac{\\sum_{A_T \\in A_{clean}} \\mathbb{I}(I_r(A_T) \\le \\Theta)}{|A_{clean}|}$ (3)\n$A_{clean}$ represents a set of attention maps generated from numerous clean samples. $|A_{clean}|$ indicates the number of elements in $A_{clean}$. $I(\\cdot)$ is an indicator function that returns 1 if the condition in parentheses is True, and 0 otherwise. Given a confidence level p (usually set to 0.95), we calculate a threshold value $\\Theta$ such that p fraction of the clean samples satisfy $I_r(A_T) < \\Theta$. It is worth noting that our method for computing $\\Theta$ does not require any exposure to perturbed samples.\nFinally, the detection can be formulated as Eq. 4.\n$D(x) = \\begin{cases} 1 \\text{ (perturbed), } & \\text{ if } I_r(A_T) > \\Theta, \\\\ 0 \\text{ (clean), } & \\text{ otherwise. } \\end{cases}$ (4)\nFor an input sample x, we obtain its attention map $A_T$ and corresponding indicator $I_r$ on a specific convolution layer of a DNN model. Note that all models used in our study, including VGG-16, ResNet-101, Inception-V3, etc., do not require any additional training."}, {"title": "IV. EXPERIMENT AND EVALUATION", "content": "Fig. 4 indicates the implementation detail of the proposed detection. The framework includes two phases: the analysis, where the threshold for a deployed DNN is calculated based on clean samples, and the evaluation, which assesses the detection performance.\n The shallow layers used to obtain $I_r$ indicators are from VGG-16. However, shallow layers from other classic models like ResNet-50, MobileNet-V3, etc. can also be employed. The selection of detector models and layers is discussed in the subsequent sections.\nAttacks Training: We utilize the adversarial patch attacks technique in [2] to train adversarial patches against each DNN separately, with each patch occupying only 6% of the original image's pixels (54x54). The smaller the patch is, the more challenging its detection becomes. We take 6% as is generally done in this study. A small sample of images (i.e., 4000) from ImageNet is used in the training stage. As shown in Table I, when inputs are compromised with these patches, the top-1 accuracy of the affected models on ImageNet can be even reduced to 0.83% at most.\nTable I shows those adversarial patches cause varying accuracy degradation on their target DNNs. VGG-16 is the most vulnerable, while MoblieNet-V2 is relatively robust and difficult to attack. In this work, we aim to build a unified model to effectively detect all those patches, no matter how their transferability are, to block them from fooling the subsequent DNNs on back end.\nTo achieve this goal, we take different shallow layers from VGG-16 to derive the indicator $I_r$ and calculate $\\Theta$. The results presented in Table II indicate that taking the second convolutional layer of VGG-16 as the detection layer yields the most effective results, i.e., employing the first two convolutional layers of VGG-16 as a unified detection model. This methodology consistently achieves a notable level of accuracy in detecting attacks, with the detection rate reliably reaching or exceeding 97.36%. The results also show that the adversarial patches Advp-MobileNetV2, which is trained to attack the robust MobileNet-V2, tends to be more aggressive, making it challenging to detect. As such, the detection model can be formed with only the first two convolution layers of VGG-16, which is lightweight and can be deployed in the resource limited edge devices. The results also indicate that in scenarios where computing resource is extremely limited, deploying only the first layer is still possible with an F-score of at least 87.91%, even when subjected to the Advp-MoblieNetV2 attack.\nWe also conducted experiments to investigate the effectiveness of using shallow layers of different models as a unified model for detection. The results in Table III demonstrated that without considering patch Advp-MoblieNetV2, the first convolution layers of all models are suitable for forming the detection model, achieving an F-score of at least 92.17%. Considering Advp-MoblieNetV2, only the first convolution layers of ResNet-50 and MobileNet-V2 are appropriate for constructing the detection model, achieving an F-score of at least 80.91%.\n The results from Table II and Table III demonstrate that the unified detection model, taking the first two convolution layers from VGG-16 to obtain indicator $I_r$ for detection, achieves the best performance against all adversarial patches trained under different threat models. Specifically, it attains an F-score of at least 97.42%, significantly surpassing the detection accuracy of LanCe [12], with is 91%.\nThe optimal choice obtained from the experimental results may be attributed to the fact that VGG-16 is the most vulnerable model among all the threat models, as shown in Table I. This vulnerability allows patches trained by other threat models to possess some level of transferability to attack VGG-16. As a result, the shallow layers of VGG-16 demonstrate sensitivity to all patches and exhibit the capability to capture pertinent features that indicate the presence of adversarial patches."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a universal and efficient edge computing framework for robust AI. The framework leverages the attention map derived from the shallow layers' feature map of a DNN model to effectively identify a broad spectrum of visual attacks. The experiments conducted in this study have demonstrated the effectiveness and universal applicability of our approach, achieving an F-score of at least 97.4% in detecting various attacks. Furthermore, our method offers several advantages for implementation on edge devices. It requires less computational cost and memory for registers and eschews introducing complex multiplication. The latency and energy consumption have been improved, achieving a speedup ranging from 2.46x to 30.04x and energy savings between 55.27% and 97.05%. These characteristics make it highly suitable for resource-constrained environments. Additionally, our approach does not require any knowledge of or modifications to the back-end AI model, enabling seamless integration with widely-deployed industrial AI models. Overall, our framework demonstrates the universality and efficiency of edge solutions for robust AI."}]}