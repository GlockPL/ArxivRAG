{"title": "On explaining with attention matrices", "authors": ["Omar Naima", "Nicholas Asher"], "abstract": "This paper explores the much discussed, possible explanatory link between attention weights (AW) in transformer models and predicted output. Contrary to intuition and early research on attention, more recent prior research has provided formal arguments and empirical evidence that AW are not explanatorily relevant. We show that the formal arguments are incorrect. We introduce and effectively compute efficient attention, which isolates the effective components of attention matrices in tasks and models in which AW play an explanatory role. We show that efficient attention has a causal role (provides minimally necessary and sufficient conditions) for predicting model output in NLP tasks requiring contextual information, and we show, contrary to [7], that efficient attention matrices are probability distributions and are effectively calculable. Thus, they should play an important part in the explanation of attention based model behavior. We offer empirical experiments in support of our method illustrating various properties of efficient attention with various metrics on four datasets.", "sections": [{"title": "Introduction", "content": "Transformer and other attention based models optimize attention weights (AW), the distribution of weights in transformer models' implicit representations of the hidden states of tokens, as inputs to multilayer perceptrons (MLPs) in order to solve a variety of tasks. As such it stands to reason that AW should have an explanatory role [8, 20, 43, 10]. However, many have criticized using AW to explain model predictions [38, 24, 45, 17, 7, 5]. [7] formally proved that under certain assumptions an infinite number of AW could yield the same prediction; i.e., AW are unidentifiable from output-the relation from AW to predictions is not 1-1. Empirical testing by [24, 45, 42, 11] has confirmed non-identifiability of AW in at least some tasks and models, which [2, 9] among others cite as a drawback for explanability.\nThis paper makes three main contributions. First, we bring together in a novel way, AW, identifiability and explainability in transformer models, linking identifiability of AW with counterfactual and causal explanations and explanatory faithfulness [37, 23, 46].\nSecond, we show that [7]'s approach fails to make AW viable explanatory tools. While they identify a projection into a representation space that looks promising they are unable to show that the weights in that projection have the properties of a probability distribution. Moreover, they do not see a way to calculate the result of the projection effectively into a space with the requisite properties. This vitiates twice over the explanatory power of their solution. We introduce a new projection whose result we call, efficient attention. Efficient attention restores identifiability; and we show that its weights define a probability distribution. Finally, we show how to calculate this projection effectively.\nThird, we present a series of experiments\u00b9 on various datasets using the experimental set up of [45, 24]. The point of these experiments is to show empirically the effects of efficient AW. We show first that AW matrix makes the same predictions as its efficient AW projection. Second, we take AW and their adversarial AW from [45] with the same predictions, and we show empirically that they have the same efficient AW projection within the limits of integer precision. Finally, we show that intervening on model and replacing one efficient AW with another shifts the predictions, confirming empirically the identifiability of efficient AW and their potential role in counterfactual, faithful explanations, contrary to [24, 45, 9].\nSection 2 starts with an analysis of explainability and identifiability. Section 3 provides a detailed look at the theory behind the claims of [7] about non-identifiability of AW and their solution. We introduce efficient attention, prove its identifiability and that it is a probability distribution. We also show how to calculate it. Section 5 empirically validates the identifiability of efficient attention contrary to the claims of [45] using their experimental set up and datasets."}, {"title": "Explainability and Interpretability for Transformers", "content": "Attention and transformers Generative models or transformers with decoders predict the next token $Y_{i+1}$ using the conditional probability $P(Y_{i+1} | Y_0,..., Y_i, C_{i+1})$ where $y_0$ is the seed vector, ${Y_1,..., Y_i}$ are prior predicted tokens and $C_{i+1}$ is a latent representation of $y_i$ together with other information given to the model. Encoder transformer and other attention based models also use these latent representations of input tokens. Attention matrices or their AW determine these representations $C_i$ from entries that are typically already vector encodings. More specifically, $C_i$ is a weighted sum of $C_h$ over h attention heads, where each $C_h$ is:\n$C^h = \\frac{1}{d}\\sum_{k=1}^{d} a_k^h (W_v e_k)$\n$a_k$ is the attention weight providing the import of token representation $e_k$ to $e_i$ and $W_e$ is the value weight matrix of the head h. A typical transformer has multiple AW that link with a residual stream from the previous layer to a multi-layer perceptron (MLP). Since early work on AW ([24, 45] did not even use full transformer models), we have learned a lot more about how transformer models exploit context input tokens and AW. The Transformer architecture has layers with each having different blocks in the following order: attention, residual learning [21], layer normalization [4] and MLP (Multi Layer Perceptron). [26, 19, 15, 16] show how to decompose attention blocks in a way that allows us to interpret token/token interactions in each block. AW rearranges the input embeddings in a transformer model to optimize the internal representation input to the MLP that must execute some task. The role of residual stream is to incorporate elements from previous layers to avoid overfitting at each layer; but this feeds into the next block's attention layer. The role of the layer normalization block is to significantly reduce training time. Finally the MLP block learn the function we want from the transformer.\nAll of this work points to the important role of attention in transformer model predictions. If attention did not play an explanatory role, by giving a better contextualization to facilitate the learning for the MLP, the Transformer performance would reduce to that of the MLP component, but this is manifestly not the case. [35]shows the importance of the attention layer in their theoretical analysis of the computational power of transformer architectures. Thus, the optimized representation of input data by the attention layer should have an important causal effect on the MLP's computations and the eventual output, in any task where the first input to the transformer model is not sufficient for an MLP to solve the task. We know from formal results about classifiers that Heavyside MLPs [13] benefit greatly from a representation space that minimizes the number of \"jumps\" the classification function must make in order to classify some data. Moreover, the effects of AW at different layers of a transformer model may be different, as in circuit analysis [36], mechanistic interpretability [33, 32, 18].\nWe grant that not all tasks may need the contextual information of equation 1, at least not in all layers or in the same way. For instance, simple classification tasks like gender identification do not seem to require the contextual encodings of attention heads, at least not through all layers [25]. Nevertheless, it stands to reason that AW should play an important explanatory role. We will show, contrary to [24, 45, 7], that AWs are causally determinative of the output and so should in principle have explanatory value."}, {"title": "Explainability", "content": "With this in mind, we turn to explainability with AW. Discussions of explainability for transformers often start from different interpretations of explainability and interpretability. [23] use them equivalently, but [37] distinguishes them using the concepts of faithfulness and plausibility. Plausibility has to do with how acceptable the purported explanation is to humans [23]. Faithfulness involves a causal connection, what is causally necessary and sufficient for the model to produce its prediction given a certain input; a faithful explanation is one that captures the causal relations in the explanations it generates of a model's predictions. That means a faithful explanation should ideally identify minimally sufficient and necessary conditions for the prediction.\nA faithful explanation of the behavior f thus has to support counterfactual interventions of the form: A and B and had A not been the case, B would not have been the case either [23]. Using [29]'s definition of causation and given $f : X \\rightarrow Y$, we say that A faithfully explains why $f(x) = \\pi$ for $x \\in X$ iff we can establish (i) x has A (A(x)), and (ii) if A(x) hadn't been the case, $f(x) \\neq \\pi$.\nTo establish such counterfactuals we typically need a background theory T that could be formal (as in [40]), but it could also be based on observation. T requires a range of cases I, different set ups or variations on inputs that support counterfactual interventions, in which we remove our putative explanatory factors & from x but keep the rest of our set up f and the other properties of x the same or as similar as possible. Counterfactuals require a distance metric over \u03a6 [28] to make precise the notion of similarity. The appropriate metric depends on the nature of the cases and the task. There are many candidates for counterfactual theories of a transformer f [3]. One can set I to the feature space of the inputs to the model. Alternatively we can set I to a set of possible AW in f or W = a set of possible parameter settings for the final layer of f as suggested in [45, 14], or any among a large number of other possibilities of parameter values for intermediate states in f. Not all of these candidates may provide faithful explanations, however, a choice of cases may not turn out to support any nontrivial counterfactuals.\nTo have faithful explanations and non trivial counterfactuals, we need identifiability of the explanatory factors from the predictions. The non-identifiability of AW precludes counterfactual forms of explanation of the form had the AW been different, the prediction would have differed, and thus threatens the faithfulness of any explanation based on AW. However, there are a couple of caveats. In general, in light of finite integer precision and the approximation of exact real values and computations in transformers, we should not expect small variations in AW numerical values to practically affect prediction. We are interested in the possibility of faithful explanations using sets A of AW where for x, y \u2208 A, $||x - y|| < \\epsilon$ for A determined by the task, approximation and integer precision, the predictions are more or less the same and where for distinct A, A' and for $x \\in A, y \\in A'$ with $||x - y|| > \\epsilon$, we should get distinct predictions. [24, 45, 42] provide empirical evidence that at least in some NLP tasks, what appear to be numerically significant shifts in AW do not affect the predictions over a large number of instances in an NLP task.\nIf the variations in AW values are large and are not explicable in terms of approximations, then the explanatory plausibility carried by such a large set of AW is much less. Such sets, which are a mark of causal overdetermination [30], leave humans wanting more: what is the common element that makes all of these cases causally relevant? In some NLP tasks, for instance, text classification, this overdetermination might be innocuous or even indicative that the task does not distinguish between a broader class of AW numerical values; a single text input might provide many sufficient clues for a classification; a single movie review might be classified as favorable because of several passages, a high attention value on any one of which would suffice for a favorable classification. But in general, given a particular input, identifiable AW distributions (up to a certain arithmetic precision) for a particular prediction would carry a higher explanatory value in terms of plausibility than sets of heterogeneous AW values. There is a point then to searching for identifiability of AW within the limits of integer precision. Identifiable attention matrices can also furnish the basis for more sophisticated strategies for plausible explanations with attention as in [22].\nThere is another dimension of plausibility to consider. A plausible explanation using AW needs to link AW to input tokens. Recalling what attention layers do, they provide \"mixtures\" of input token effects on each token. These mixtures are difficult if not impossible to interpret when the values are negative, and they are most easily interpretable when they are probability distributions as motivated in a different context by [27]. That is, attention weights as probability distributions provide a priori for humans more plauible explanations."}, {"title": "Mathematics for Attention and Identifiability", "content": "[7] showed a sufficient condition for non identifiability: for a given input sequence longer than the attention head dimension, a transformer model can generate an infinite set of attention weight distributions producing the same output prediction. This is not just an abstract possibility; in cases with relatively long inputs, non identifiability is real. [7] also proposed a means for removing the weight components that do not influence the model's prediction to make AW identifiable. But their approach doesn't guarantee that AW are probability distributions.\nTo analyze and unpack this problem, we first review and give a detailed proof of [7]'s necessary and sufficient condition for identifiability of AW. We then describe their solution of effective attention, which provides a projection of AW values that renders the function from AW values to predictions injective but at the price of making those values no longer a probability distribution. We then define a new and computable projection of AW into what we call efficient AW that solves the mathematical problem with the effective attention [7] while also restoring identifiability of attention weight distributions."}, {"title": "Identifiability on AW", "content": "As background to the work of [7], multi-headed attention output combine h independent attention heads with reduced head dimension $d_v = d/h$ in parallel fashion through a linear layer. The output is achieved by summing outputs over each single head and multiplying them by the matrix $H \\in R^{d_v*d}$ of the linear projection. We will keep the same notations as [7], then for ease of understanding, we omit layer and head indices in the upcoming proofs since they remain valid for each head and layer in transformer models.\nAttention(Q, K,V)H = (A.V).H =\nA.(E.WV).H = A.T\nwhere the attention matrix A is defined as:\n$A = softmax(\\frac{QK^t}{\\sqrt{d_k}}) \\in R^{d_s*d_s}$\nand $Q \\in R^{d_s*d_q}$ is the query matrix, $K \\in R^{d_s*d_q}$ is the key matrix, $V = E.WV \\in R^{d_s*d_v}$ is the value matrix (the bias is assumed to be zero), $WV \\in R^{d*d_v}$ is the matrix that projects embeddings E to the value matrix V, $H \\in R^{d_v *d}$ is the matrix corresponding to the linear layer that reshapes the concatenation of contextualization matrices into the same dimension as our inputs vectors and T = V.H is a matrix considered to simplify notations, with $d_s$ corresponding to the input sequence length, d to embedding dimension and $d_u$ to the attention head dimension.\nConcerning the identifiability of AW, consider the function f: A \u2192 AT. We note that f is linear with respect to A, then f is injective iff $Ker(f) = {0}$. Let \u201c ' \u201c stand for the transpose of a matrix. We now study the injectivity of f : for A \u2208 Ker(f), f(A) = 0 \u21d4 AT = 0 \u21d4 $T'A' = 0$. The following proposition establishes necessary and sufficient conditions for injectivity:\nProposition 1. f is injective iff $Ker(T') = {0^{d_s}}$"}, {"title": "Effective Attention restores identifiability", "content": "[7] shows that a sufficient condition for f to fail to be 1-1 and for AW to be nonidentifiable is that: $d_s > d_v$. To restore identifiability, [7], proposed a solution, effective attention, which consists of the decomposition of each row of AW A into the component in $Ker(T')$ and the component orthogonal to the null space. As all possible AW and effective AW are in finite dimensions $R^n$, then we can write: $R^{d_s} = Ker(T')^{\\perp} + Ker(T')$.\nThen $AT = (proj_{Ker(T')}(A) + proj_{Ker(T')^{\\perp}}(A))T = proj_{Ker(T')^{\\perp}}(A)T = (A \u2013 proj_{Ker(T')}(A))T$.\nThere are two problems with the solution proposed by [7]. First, elements of $Ker(T')$ might include negative weights. In addition even if these constraints hold, there is no guarantee that for $A \\in Ker(T')$ : A > 0 and A1 = 1. [7]'s solution does not respect probability constraints; and as a result, their effective attention will not lead to easily interpretable elements. We will tackle these two problems separately."}, {"title": "A new solution for Identifiability:Efficient Attention", "content": "In the transformer models typically considered in the literature, A is the result of a softmax, hence its rows are constrained by: A \u2265 0, and A1 = 1, where 1 \u2208 Rds is the unit vector of all ones. The condition for nonidentifiability shows that there are infinite number of distributions that verify the same output, but it does not establish that there are several matrices that verify the softmax constraints. So we need to project attention weights into a different space. Proposition 2 gives details on that space.\nProposition 2. $d_s > d_v +1$ suffices for A to be non identifiable and for the null space of $[T, 1]'$ to be non-zero.\nUsing Proposition 2, we use instead of [7]'s Ker(T'), a projection into the null space $Ker([T, 1])$, which induces the same prediction as original attention matrix (AT = A\u207aT) and an additional constraint (A\u207a1 = 1) which is important for A\u207a to be a probability distribution. To solve the problem with [7]'s approach for explanations based on attention, we need to prove that the projection into $Ker([T, 1]')$ has the global properties of a probability space and that there is an effective means of calculating $Ker([T, 1]')^{\\perp}$."}, {"title": "Global properties of $Ker([T, 1]')^{\\perp}$", "content": "Our work relies on the decomposition of each row of attention matrix into two supplementary spaces $Ker([T, 1]')$ and its orthogonal. Once again for finite dimensions $R^{d_s}$, $R^{d_s} = Ker[T, 1']^{\\perp} + Ker[T, 1']$, which means that any vector of $x \\in R^{d_s}$ can be written with a unique decomposition $x = x^{\\#}+x^{\\perp}$, where $x^{\\#}$ is the projection of x into $Ker([T, 1]')$ and $x^{\\perp}$ into its orthogonal.\nNote that $AT = [A_1T, ..., A_{d_s}T]$ with $A_i$ are rows of A. We do the same decomposition for each row of attention matrix A, then: A = A# + A\u207a. For any matrix A, we have $AT = (A^{\\#} + A^{\\perp})T = A^{\\#}T + A^{\\perp}T$ but each row $A_i \\in Ker[T, 1]'$ then $A^{\\#}T = 0$, which means that $AT = A^{\\perp}T$.\nWe also know that since A is a softmax, then A.1 = 1, therefore $A^{\\#}1+A^{\\perp}1 = 1$ but each row $(A^{\\#})_i \\in Ker[T, 1]'$ then $A^{\\#}1 = 0$, so A1 = A^{\\perp}1. This means that we can identify each attention matrix responsible for an output, by a unique projection that verifies the same output and has the sum of each row equal to one."}, {"title": "Positivity of efficient attention weights", "content": "While we have proved that the weights of efficient attention matrices have certain properties of probability distributions (they sum to one) but we have not shown that every weight is positive w \u2265 0, which is essential for each row in an AW to be a probability distribution.\nLet's prove that $A^{\\perp} = proj_{Ker([T,1]')^{\\perp}}(A))$ is a probability distribution, under the following conditions: for each row $i \\in {1, .., n}$ : 0 < $A_i = A^{\\#} + A^{\\perp} \u2264 1$, A\u207a.1 = 1, A.1 = 0 and < A, A >= 0. These conditions are ensured by the projection. So now we need to prove is that under those conditions, $A^{\\perp} > 0$.\nWe prove by induction that under the conditions mentioned above \u2200n \u2265 2, $A^{\\perp}$ can't have negative weights.\nFor n = 2, we suppose that A\u207a has a negative weight, for simplification, let's suppose the negative weight is a1 < 0 with $i \\in {1, 2}$. With constraints we have it means that: $a_1^{\\perp} + a_2^{\\perp} = 1$ and $a_1 + a_2 = 0$ then $a_2 = 1 - a_1^{\\perp}$ and $a_2 = -a_1$.\nWe know also that $a_1a_1^{\\perp}+a_2a_2^{\\perp} = 0$ then $a_1a_1^{\\perp} + (1 \u2212 a_1^{\\perp})(-a_1) = 0$, so $a_1 (a_1^{\\perp} \u2013 (1 \u2013 a_1^{\\perp})) = 0$\nWe know that $a_1 + a_1^{\\perp} > 0$ then $a_1 > -a_1^{\\perp} > 0$\nWhich means that $a_1 \u2013 (1 - a_2) = 0$, then $a_1^{\\perp} = > 0$ which is in contradiction with our supposition. The same reasoning remains true by taking $a_2$ instead of a1.\nWe suppose that this property is true for n \u2265 2, and to prove that it remains true for for n + 1, we have only to get back to the case of n variables by taking: $a_{n+1}^{\\perp} = 1 \u2212 \\sum_{j=1,..,n} a_i^{\\perp}$, and $a_{n+1} = -\\sum_{j=1,..,n}a_i$, and then use the induction assumption."}, {"title": "Computing the projection $Ker([T, 1]')^{\\perp}$", "content": "The final challenge is to determine mathematically $Ker([T, 1]')^{\\perp}$. We do this now. We first prove a new simpler form for the projection and then providing a computable and analytic method to find that simpler form mathematically. We show how to find this mathematically using the image of [T, 1]. The image of a linear function f, Im(f), is the set of values that the function f can take by applying the linear transformation to elements of its domain.\nDefinition 1. For a linear function $f: V \\rightarrow W$ that maps from a vector space V to a vector space W, we define Im(f) or f(V): Im(f) = {$f(v) : v \\in V$}.\nProposition 3. $Ker([T, 1]')^{\\perp} = Im([T, 1])$\n$X \\in Ker([T, 1]')[T,1]'x = 0  \u2200y \u2208 Rd+1 : ([T, 1]'x, y) = 0  \u2200y \u2208 Rd+1 : (x, [T, 1]y) = 0 \u21d2 x \u2208 Im([T,1]) \u21d2 Ker([T, 1]') \u2286 Im([T,1])\nNow Im([T,1]) \u2286 Ker([T, 1]') \u21d2 Ker([T, 1]') = Im([T, 1])\nKer([T, 1]') = Im([T, 1])\u207a\u207a, since we are in finite dimensions then Im([T, 1]) = Im([T,1]) As a result, $Ker([T, 1]')^{\\perp} = Im([T, 1])$.\nWe now show an analytical way to find Im([T, 1]).\nConsider the standard basis of $R^{d+1}$ : $(e_1, ..., e_{d+1})$. Now Im([T,1]) = vect([T, 1]e1, ...[T, 1]ed+1) gives us a generating family that we can transform into a basis using the Gauss Pivot algorithm and then get $proj_{Im([T,1])} = proj_{Ker([T,1]')^{\\perp}}$ to finally have our new efficient matrix $A_{efficient} = proj_{Ker([T,1]')^{\\perp}}(A)$.\nThis procedure is general and holds for all attention matrices; it provides an effective computation of identifiable AW matrices that are identifiable.\nProposition 4. When A is identifiable, $A_{efficient} = A$\nThis follows from the fact that when A is identifiable, $Ker([T, 1]'T) = {0}$, and so $A = proj_{ker ([T,1]')} (A) + proj_{Ker([T,1]')^{\\perp}}(A) = proj_{Ker([T,1]')^{\\perp}}(A)) = A_{eff}$\nTo summarize, efficient attention restores identifiability of AW by removing the weight components that do not influence the generation of the contextualization vectors AT, and consequently the model predictions. We can thus extract the distribution that is responsible for the generation of contextualization vectors and which is unique. That is, we can have two different distribution of attention that generate the same prediction, but their efficient attention will be the same : $A_1.T = A_2.T$ with $A_1 \u2260 A_2$ but $proj_{Ker([T,1]')^{\\perp}} (A_1)$ will be equal to $proj_{Ker([T,1]')^{\\perp}}(A_2)$ and thus correspond to $A_{eff}$. Our technique isolates the factors that can serve an explanatory role, eliminating the noise from AW."}, {"title": "Permutations of AW.", "content": "[24] argue against attention as an explanatory vehicle by showing that permuting the AW only slightly varies the output. However, their attention is a vector not a matrix. Where AW are true matrices, our experiments have shown the values are relatively uniform, so permutation is not expected to produce large differences in results."}, {"title": "Finally", "content": "we offer a detailed comparison with [7] that inspired our work. First, [7] investigate identifiability but they don't say why it is important. We say why identifiability is important for explainability contra [45]. This is crucial for our paper as we are looking at the explanatory potential of AW. [6] is not explicitly motivated by explanatory concerns but rather by mechanistic interpretability."}, {"title": "Second", "content": "[7] propose the orthogonal of Ker(T') to remove weight components that do not influence model predictions; we show that the orthogonal of Ker([T,1]') preserves identifiability and we show both mathematically and empirically that it removes components that do not affect model predictions."}, {"title": "The projection by", "content": "[7] is not guaranteed to be a probability space and they provide examples where it could not be; we show that our projection guarantees that the image of an AW under a projection into the orthogonal of Ker([T,1]') is also an AW. [7]'s effective AW are not real AW, according to definitions in the literature on transformer architectures. We also provide and use an algorithmic method for calculating the orthogonal of Ker([T,1]')."}, {"title": "We verify experimentally that our AW are distinct when results are distinct and that there is 1 efficient AW for distinct raw AW replicating [45]'s empirical set up; [7] does not. We compare efficient AW using the well known Wasserstein metric appropriate for probability distributions, [7] cannot. This experimental component provides a method for also using efficient AW in faithful and plausible explanations of model behavior. We argue [7] cannot do this, since their effective AW are not real AW but unknown objects."}, {"title": "Conclusions", "content": "We have provided formal results, restoring the potential explanatory power of attention. We have shown how to render AW effectively identifiable, reducing noise in AW, while also ensuring their interpretability with efficient attention. Substituting efficient attention for original attention provides a strongly equivalent model.\nOur novel concept of efficient attention removes an important obstacle to making AW a plausible basis for faithful and plausible explanations. As we pointed out, however, if a task for a transformer model does not require contextualization or much contextualization, it may be that AW are not causally involved as they are not needed to complete the task. Our results for efficient attention justify placing more sophisticated learning strategies on top of AW [2, 22] for explaining model behavior in tasks where contextualization is needed.\nWe have illustrated our approach in three empirical experiments over four databases. We compared predictions from an AW and its efficient projection, and we show that the predictions are very close, and for practical purposes identical given integer and approximation precision. Second, we took the adversarial AW provided by [45] Aadu and compared their efficient AW with the efficient AW of the original matrix. Third, we showed that given two different efficient matrices Aeff and Af that their projections differ.\nOur mathematical results are general and apply not only to the simplified attention models of [45] and [24] but to full transformer models both of the encoder and full kind. We conducted our experiments on [45] architecture, as it offered a way of having two different attention matrices presenting the same predictions (adversarial technique). This enabled us to show that they shared the same efficient attention matrix in addition to our other experiments. Space precluded us from examining more modern transformer models empirically, but counterparts of our results here should hold.\nGiven that efficient AW are identifiable from predictions and that we can trace in a transformer model links from input tokens to AW, we can see how shifts in AW occur with shifts in input. This will also enable linking efficient AW to human annotations of \"rationales\" on input data to test plausibility of the explanations in more detail as in [12]. We feel this is an illuminating path for future research.\nAW is only part of a transformer model. We have shown why this part is especially important in model performance, but other parts, e.g. the Residual stream, Layer Normalization or the MLP, will also play a part in a complete explanation of the model's performance."}, {"title": "Proof of Proposition 1", "content": "To prove the right to left direction, suppose that : $Ker(T') = {0^{d_s}}$\nA\u2208 Ker(f), we consider: A\u2019 = [$a_1$,..., $a_{d_s}$] with $a_i$ is the i column of A' then: T\u2019 A\u2019 = [T\u2019$a_1$,..., T\u2019$a_{d_s}$] = [0,..,0]\u21d2 \u2200i \u2208 1,.., $d_s$ : T\u2019$a_i$ = 0 we know that Ker(T\u2019) = {$0^{d_s}$} then \u2200i \u2208 1,.., $d_s$ : $a_i$ = 0 \u21d2 A\u2019 = 0 \u21d2 A = 0 \u21d2 Ker(f) = {$0^{d_s*d_s}$} fis injective (1)\nTo prove the left to right version we use the contrapositive, Ker(T\u2019) \u2260 {$0^{d_s}$} \u21d2 Ker(f) \u2260 {$0^{d_s*d_s}$}\nAssume that Ker(T\u2019) \u2260 {$0^{d_s}$} \u21d2 \u2203a \u2208 Rds_{0} : T\u2019a = 0 we take the matrix A\u2019 = [a, .., a] \u2260 0 : T\u2019A\u2019 = [T\u2019a, ..., T\u2019a] = [0, ..., 0] \u21d2 A \u2208 Ker(f) \u21d2 Ker(f) \u2260 {$0^{d_s*d_s}$} (2)\nFinally Ker(f) = {$0^{d_s*d_s}$} \u21d4 Ker(T\u2019) = {$0^{d_s}$}, then f is injective iff Ker(T\u2019) = {$0^{d_s}$}\nConsider now Ker(T\u2019) : We know that T \u2208 $R^{d_s*d}$; so T\u2019 \u2208 $R^{d*d_s}$ .\nNow apply the Rank Theorem : dim(Ker(T\u2019)) + rg(T\u2019) = ds. Since we know that rg(T\u2019) = rg(T) then dim(Ker(T\u2019)) = ds-rg(T). Finally if fa is injective, Ker(T\u2019) = {0} which means that rg(T) = ds."}, {"title": "Proof of Proposition 2", "content": "As $[T,1]' \\in R(d+1)*d_s$, we apply the rank theorem and so: $dim(Ker([T, 1]')) + rk([T, 1]') = d_s$.\nWe know $rk([T, 1]') = rk([T, 1])$, then $dim(Ker([T,1]')) = d_s - rk([T, 1])$. So A is identifiable if $d_s = rk([T, 1])$, but here we search for a sufficient condition for non-identifiability.\n$rk([T,1]) \u2264 rk(T) + 1 < min(rk(E),rk(WV), rk(H)) + 1 < min(d_s, d, d_v) + 1 \u2264 max(d_s, d_v) + 1 \u2264 max(d_s + 1, d_v + 1)$.\nTherefore $dim(Ker([T, 1]')) \u2265 d_s - max(d_s + 1, d_v + 1) \u21d2 dim(Ker([T, 1]')) \u2265 max(-1,$d_s$ \u2013 $d_v$ \u2013 1) And we know that $dim(Ker([T, 1]')) \u2265 0 finally $dim(Ker([T,1]')) \u2265 max(0,$d_s$ - $d_v$ \u2212 1).\nThen for $d_s > d_v +1$ : ker([T, 1]\u2019) \u2260 {0} \u21d2 \u2203B \u2208 ker([T,1]\u2019) \u2260 0 that verifies $f_{A+B} = f_A$ and B. [T, 1] = 0."}]}