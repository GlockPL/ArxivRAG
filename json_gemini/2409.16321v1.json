{"title": "WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer", "authors": ["Junchao Gong", "Tao Han", "Kang Chen", "Lei Bai"], "abstract": "Numerical Weather Prediction (NWP) system is an infrastructure that exerts considerable impacts on modern society. Traditional NWP system, however, resolves it by solving complex partial differential equations with a huge computing cluster, resulting in tons of carbon emission. Exploring efficient and eco-friendly solutions for NWP attracts interests from Artificial Intelligence (AI) and earth science communities. To narrow the performance gap between the AI-based methods and physic predictor, this work proposes a new transformer-based NWP framework, termed as WeatherFormer, to model the complex spatio-temporal atmosphere dynamics and empowering the capability of data-driven NWP. WeatherFormer innovatively introduces the space-time factorized transformer blocks to decrease the parameters and memory consumption, in which Position-aware Adaptive Fourier Neural Operator (PAFNO) is proposed for location sensible token mixing. Besides, two data augmentation strategies are utilized to boost the performance and decrease training consumption. Extensive experiments on WeatherBench dataset show WeatherFormer achieves superior performance over existing deep learning methods and further approaches the most advanced physical model.", "sections": [{"title": "1 Introduction", "content": "Weather prediction plays a decisive role in various productive activities, as well as extreme weather events' prevention [Reichstein et al., 2019]. For instance, accurate weather prediction can provide fundamental information for agricultural planting and irrigation system management. Also, it can help prevent massive life and property loss by forecasting typhoons, heatwaves, tropical cyclones, floods, etc. In particular, a series of floods that occurred in South East Queensland, the Wide Bay-Burnett, and parts of coastal New South Wales in 2022, has been one of the nation's recorded flood disasters, where 22 people are known to have died during the disaster. Many miserable natural events warn us that it is of great value to develop numerical weather prediction (NWP) to predict future weather states (e.g., temperature, wind speed, humidity, etc.), because it is the groundwork of the weather prediction and significantly impacts our society.\nModern NWP utilizes physics and fluid mechanics to model the atmosphere with complex partial differential equations (PDE) [Bauer et al., 2015]. To obtain solutions of the PDEs, initial states are first derived by processing data collected from satellites and other sensors. Then the solutions of discretized governing equations and parameterized sub-grid processes [Kalnay, 2003] can be obtained by using numerical techniques. However, due to the huge computational cost of the fine grids for high-frequency wave modeling and model ensemble in probabilistic forecasts, obtaining solutions in physics based models is time costly and intensively energy consuming. For example, it takes 82 minutes for the high-resolution Integrated Forecasting System (IFS) model to compute a 15-day, 51-member ensembled weather state prediction by using the \"L91\u201d 18km resolution grid data on the 1530 Cray XC40 nodes with dual socket Intel Haswell processors [Bauer et al., 2020]. Power consumption of Cray XC40 is 1900KW [TOP500, ], which dramatically higher than a single A100 GPU whose power is 400W.\nRecently, the increasing interest raised in the deep neural networks based data-driven NWP methods stems from the following aspects: (1) Data-driven NWP is consistent with the Sustainable Development Goal in sustainable infrastructure. (2) As the increasing amount of available weather state data, the physics based NWP methods either fall short in the ability to incorporate signals from newly emerging geophysical observation systems [Goodman et al., 2019], or cannot efficiently process the Petabytes-scale NWP observation data; (3) Data-driven NWP methods enables large scale of ensembles with relatively low computational cost for the probabilistic forecasts and data assimilation [Pathak et al., 2022]. Previous works [Rasp et al., 2020; Weyn et al., 2020; Rasp and Thuerey, 2021; Pathak et al., 2022; Chattopadhyay et al., 2022] attempted to accurately predict future weather states in an efficient way by using data-driven methods. For example, FourCastNet [Pathak et al., 2022] can predict high-resolution weather states with a reduction on the computational cost by a factor of 1000 when compared with IFS. However, there is still a performance gap between the physics based and data-driven NWP methods.\nIt is observed that the change of weather states heavily depends on their contextual weather states. Also, intuitively, the trend on the change of the weather states over the past long period of time would have a great impact on the future weather states. Therefore, in addition to only considering the spatial information at the current time step, the temporal relationship over the past long period of time should be well modeled. However, the existing deep neural networks based data-driven numerical weather prediction methods [Pathak et al., 2022; Rasp et al., 2020; Weyn et al., 2020; Rasp and Thuerey, 2021] cannot process spatio-temporal information without expensive multisteps finetuning.\nBased on the above observations, we propose a new deep neural networks based numerical weather prediction framework called WeatherFormer, which takes the weather states at several time steps over the past to model the spatio-temporal information simultaneously and produce future weather states for a long period of time. Our WeatherFormer is a transformer based deep neural network, which is composed of a set of space-time factorized blocks (SF-Block). Specifically, within each SF-Block, a spatial mixer and a temporal mixer are used to mix the spatio-temporal information. Note that both the spatial mixer and the temporal mixer have the same structure but one operates on the spatial domain and the other on the temporal domain. In order to reduce the computational cost, the single filter strategy proposed by adaptive Fourier neural operator (AFNO) [Guibas et al., 2021], is partly adopted within each mixer, where the input is first transformed into frequency domain to mix information and then is reversed back into the original domain. Additionally, based on the AFNO, we introduce a novel position-aware adaptive Fourier neural operator (PAFNO) to encode relative position information in the spatial domain by assigning different coefficients to different frequency filters during the information mixing process. Moreover, earth rotation augmentation is applied to exploit rotation equivariance and noise augmentation to obtain a comparable multi-step performance with half of training consumption.\nThe contributions of this work are summarized as follows:\n\u2022 We propose WeatherFormer, a new data-driven numerical weather prediction framework based on a spatiotemporal Transformer, which can predict future weather states by considering spatio-temporal information of the past weather states.\n\u2022 We introduce the Position-aware Adaptive Fourier Neural Operator (PAFNO), which could capture position information of weather signals while maintaining low parameters and computation cost.\n\u2022 We introduce the Earth Rotation augmentation to take advantage of the rotation equivariance of the data to ease the overfitting issue.\n\u2022 Extensive experiments on the WeatherBench dataset demonstrate the effectiveness of our proposed WeatherFormer over strong data-driven NWP methods."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Traditional Numerical Weather Prediction", "content": "Traditional NWP can trace back to the 20th century. Bjerknes and Abbe recognized that predicting the state of the atmosphere could be treated as an initial value problem of mathematical physics [Bjerknes, 1904], wherein future weather is determined by integrating the governing partial differential equations which starting from the observed current weather [Bauer et al., 2015]. Researchers numerically solved prognostic equations build upon Navier-Stokes equations, the ideal gas law, and other physical equations which are intractable to obtain analytical solutions [Kalnay, 2003]. At first, the initial state (called the analysis) of the atmosphere and surface is derived as a Bayesian inversion problem using observations, prior information from short-range forecasts and their uncertainties as constraints as well as the forecast model [Lorenc, 1986] [Daley, 1993]. Then, numerical techniques such as spectral methods and finite-difference methods are chosen according to numerical stability, accuracy, and computational speed to attain solutions [Robert, 1982]. However, physic-based methods are computationally expensive for model ensemble and solving PDEs with fine grids. The ECMWF 16-km highest-resolution model, which performs calculations on two million grid columns with 10-min time stepping over a 10-day period, consumes about 4 MVA power for each prediction [Bauer et al., 2015]."}, {"title": "2.2 Data-driven Numerical Weather Prediction", "content": "Data-driven NWP models predict future weather by extracting statistical regularities from historical weather data. Rasp introduced the WeatherBench dataset as a benchmark challenge for data-driven medium-range weather [Rasp et al., 2020]. Motivated by [Rasp et al., 2020], classical deep learning models such as ResNet [He et al., 2016] and UNet [Ronneberger et al., 2015] are applied for NWP. Compared with naive CNN [LeCun et al., 1995], UNet and ResNet remarkably enhance prediction results and even outperform physical model T64 [Rasp et al., 2020] as shown in Table 1. Our work explore the potential of the transformer, which achieves conspicuous success in the computer vision research community [Vaswani et al., 2017] . To further improve predictions, researchers train models with additional data. Rasp applied additional simulated data from Coupled Model Intercomparison Project (CMIP) for pretraining [Rasp and Thuerey, 2021]."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Framework Overview", "content": "Our WeatherFormer takes the weather states of previous time steps as input and outputs predicted weather states of future time steps. For each time step, the weather states are sampled at H x W locations along the latitude axis and the longitude axis from the whole Earth (shown in Figure 2), where H and W are the number of sampled locations along the latitude axis and the longitude axis, respectively. We denote the input data as X \u2208 RT\u00d7H\u00d7W\u00d7C, where T is the number of previous time steps used as input, and C is the number of weather states (e.g., temperature, wind speed, humidity, etc.) at each sampled location. For each forward pass, our WeatherFormer predicts the weather states of the next time step Y \u2208 RH\u00d7W\u00d7C, and it can autoregressively generate multistep predictions. The formulation of the autoregressive prediction is represented below:\n\n\u0176T+1 = Weather Former(X1:T)  (1)\n\u00ddT+2 = Weather Former([X2:T, \u0176T+1])  (2)\n:  (3)\n\u0176T+n = Weather Former([\u0176n, ...,\u0176T+n\u22121])  (4)\n\nAs shown in Figure 3, our WeatherFormer is built upon transformer [Vaswani et al., 2017], where the input is split and embedded into tokens. Then these tokens are entered into a series of transformer blocks to pass messages among tokens. Instead of using the traditional transformer blocks, factorized space-time Block (SF-Block) is designed with a Position-aware Adaptive Fourier Neural Operator (PAFNO) in it to effectively model the spatio-temporal information within the past weather states and encode their positional information. After that, a convolution decoder is applied to recover the encoded tokens into future weather states.\nAdditionally, two augmentation strategies are implemented:1) earth rotation augmentation that leverages the rotational equivariance property of the data to ease the overfitting issue of our proposed WeatherFormer; and 2) noise augmentation, which decrease the prediction error accumulation caused by the autoregressive strategy."}, {"title": "3.2 Factorized Space-time Block", "content": "Existing data-driven NWP methods [Pathak et al., 2022] [Rasp and Thuerey, 2021] [Rasp et al., 2020] [Chattopadhyay et al., 2022] predict future weather states by only considering the spatial information over the current states. However, the change of the weather states is dynamic over time, and there is abundant information about the trend of the weather states within the historical weather states data, which could benefit the prediction. Therefore, instead of using only the current states, it is reasonable to use a set of weather states at previous consecutive time steps to predict the weather states at the next time step, and an elegant design is needed to explore the spatio-temporal information over the input data. To this end, inspired by the work in [Arnab et al., 2021], we adopt an SF-Block to mix the information over the previous weather states in both spatial and temporal domains.\nAs shown in Figure 3, our SF-Block factorizes the self-attention module in a traditional transformer block by introducing a spatial mixer and a temporal mixer to separately model the spatial and temporal relationship over the input tokens. Given Z \u2208 RB\u00d7t\u00d7h\u00d7w\u00d7C as the input of our SF-Block, Z is reshaped to Zs \u2208 R(Bt)\u00d7h\u00d7w\u00d7C at first. The Fourier Spatial Mixer calculates spatial attention for Zs. Tokens obtained after the Fourier Spatial Mixer, Z\u02b9 \u2208 R(B\u00b7t)\u00d7h\u00d7w\u00d7C, are reshaped to Zt \u2208 R(B.h\u00b7w)\u00d7t\u00d7C. The Temporal Mixer calculates the temporal attention along the temporal axis t, which implies that the batch size of Zt is Bhw and the length of the token sequence is t. In this way, the computational cost can be significantly reduced.\nNote that PAFNO is adopted for both the spatial mixer and the temporal mixer, which is different from ViVIT [Arnab et al., 2021] for computing attention in the frequency domain. Details of the PAFNO are in the following section."}, {"title": "3.3 Position Aware Adaptive Fourier Neural Operator", "content": "Our PAFNO is built upon AFNO [Guibas et al., 2021]. In AFNO, the input tokens are first transformed into the frequency domain by applying a discrete Fourier transform operation. After that, each element in the transformed token sequence is fed into a two-layer MLP to mix information. In order to reduce the computational cost, instead of assigning different MLP weights for each element in the transformed token sequence, the weights of the MLP layers are shared across the whole transformed token sequence. Finally, inverse discrete Fourier transform operation is applied to reverse the processed token sequence back into the original domain.\nFor the NWP task, at a specific location, the previous weather states at its neighboring position have more impact on the future weather state prediction than those at distance positions [Kalnay, 2003]. Therefore, the position information is critical to the NWP task. However, the original positional embedding strategy used in [Guibas et al., 2021] does not work well. Moreover, as the MLP weights used in AFNO are shared among tokens at all positions, each position is considered equally important, which neglects the impact of positional information displayed as Figure 4. To this end, we employ a set of learnable position-related coefficients {\u03b7n}Nn=1 to assign a coefficient for the token at each position, where N is the number of tokens. Thorough analytics about position embedding, its failure in frequency mixers, and the way PAFNO introduces position information are provided below."}, {"title": "Position Embedding", "content": "In the self-attention mechanism of the traditional transformer, the absolute position embedding introduces the positional information to the attention weight matrix. Given the input X and the corresponding position embedding P, the i-th output mixed token of of the self-attention is formulated as follows:\n\nQ = (X + P) WQ,  (5)\nK = (X + P) WK,  (6)\nV = (X + P) Wv,  (7)\nai,j = exp(qik)  (8)\n\u03a3j exp(\u221adk Em qikm) (9)\noi = \u03a3jai,jVj, (10)\n\nwhere WQ, WK and Wv are query, key and value matrix \u2208 Rd\u00d7d, respectively, and the attention weight of the i-th mixed token oi on the jth context token is ai,j. According to Eq. 5, Eq. 6, Eq. 7, we then have:\n\nqiK = (xi + pi)TWQWKT(xj+pj)\n= (xi)TWQWKT(xj) + (pi)TWQWKT(xj) + (xi)TWQWKT(pj) + (pi)TWQWKT(pj).\n\nAs shown in Eq. 8, Eq. 9, Eq. 10, the attention weight ai,j and the output mixed token oi are subject to the position embedding. However, this mechanism is nonfunctional in Fourier neural operator mixer. The reason is that in Fourier neural operator mixers, attention weights are learned instead of obtaining from the dot product of the query vector for the i-th token qi and the key vector for thj-th token kj which includes terms related to position embeddings as shown in Eq. 10."}, {"title": "Fourier Neural Operators(FNO)", "content": "To illustrate why the position embedding is nonfunctional in the FNO mixers, we compare the discrepancy between the dot-product self-attention mixer and the FNO mixer. In FNO, instead of using the self-attention mechanism to mix information among input tokens, discrete Fourier transform (DFT) is applied to decrease the computational complexity from O(N2) to O(N. log N) [Li et al., 2020]. The token mixing process can be formulated as follows:\n\nx = DFT({x})n,  (11)\nx = Wx  (12)\nxout = IDFT({x})n. (13)\n\nDFT/IDFT({}) means performing DFT or IDFT on a sequence of signals. When comparing Eq. 11, Eq. 12, Eq. 13 with Eq. 5, Eq. 6, Eq. 7, Eq. 8, Eq. 9, the most significant difference between FNO and the dot-product self-attention is that FNO directly learns the token mixing matrix W in the frequency domain without considering position embedding as Eq. 9, which leads to the futility of absolute position embedding in FNO mixer.\nFortunately, as introduced in [Guibas et al., 2021], FNO can be considered as a global convolution, which naturally provides position-related weights similar to CNN. However, FNO introduces N. D2 parameters, which can easily enlarge the complexity of the model (N is the length of the token sequence). In order to simplify the model, adaptive Fourier neural operator (AFNO) [Guibas et al., 2021] is proposed by reducing the number of frequency filters W from N to 1 (i.e., W). But the limitation on the number of filters leaves only a unique filter to be used in the space domain. As a result, AFNO loses the position-related weights provided by different W in FNO, as shown in Figure 4."}, {"title": "PAFNO", "content": "To address the issue mentioned above, we propose PAFNO to introduce positional information into the AFNO, which is visualized in Figure 4.\nFrom Eq. 7, Eq. 9 can be reformulated as:\n\noi = \u2211jai,jVj\n= \u2211jai,j (xj + Pj) Wv\n= \u2211j (xj + pj) Ki,\n(14)\nKi,j = ai,jWV (15)\n\nAs shown in [Li et al., 2020], it is assumed that Ki,j is a function of the distance between the pivot token xi and the context token xj. Based on this assumption, Ki,j in Eq. 14 could be converted into a function of i \u2013 j denoted as Ki-j, and Eq. 15 is reformulated as a convolution:\n\noi = \u2211j(oj + pj) Ki-j\n= ((o + p) \u2217 K) [i]. (16)\n\nAccording to Eq. 15, an naive assumption for the possible form of the filter Ki-j could be:\n\nKi-j = ai-jWv. (17)\n\nDFT are then applied on K to obtain the token mixing matrix W in the frequency domain as follows:\n\nW = Ke-kn,\nk\n= (\u2211kake-jkn)Wv, (18)\n\nwhere k = i-j. Based on Eq. 18, we then design a set of new frequency filters by introducing a set of learnable coefficients {\u03bbn} into the AFNO as follows:\n\nW = \u03b7 Ww, (19)\n\nwhere Ww \u2208 Rd\u00d7d is the unique frequency filter used in AFNO. Consequently, our PAFNO introduces the relative positional prior among the input tokens into the token mixer as FNO without significantly increasing the complexity of the model."}, {"title": "3.4 Augmentations", "content": "Rotation. Earth rotation augmentation is proposed to utilize rotation equivariance in data. As shown in Figure 2, the weather state data is collected and predicted at the intersection of the lines of latitude and longitude, and then transferred to a weather state map. When the Earth rotates over the axis determined by the poles, the weather state map should rotate horizontally, which means the weather state map is rotation equivariance. Based on this observation, we introduced an earth rotation strategy, where the input weather states are horizontally rotated at a random distance in the training stage. By doing so, additional training samples are created to prevent our WeatherFormer from overfitting to the fixed grid of the training data and concentrate on the interior patterns over the context of weather states.\nNoise. Noise augmentation is exploited to mitigate long-term error accumulation by roughly simulating the prediction error. Compared with methods requiring fine-tuning [Pathak et al., 2022], noise augmentation half the energy demand without appreciable performance drop."}, {"title": "4 Experimental Results", "content": "This section first details the experimental setup in 4.1, then compare our WeatherFormer with other state-of-the-art NWP methods on the Weatherbench dataset to verify the effectiveness of our WeatherFormer in 4.2. Moreover, in order to investigate the contributions of our proposed components, ablation studies are conducted on the Weatherbench dataset and provide detailed analysis in 4.3."}, {"title": "4.1 Experiments Setup", "content": "Datasets. We evaluate our WeatherFormer on the numerical weather prediction benchmark dataset, Weatherbench [Rasp et al., 2020]. It is a medium-range weather forecasting (specifically 3-5 days) dataset whose data is collected by downsampling from the ERA5 reanalysis data [Hersbach et al., 2020] from 1979 to 2018. Weather states with three different scales are provided in Weatherbench, whose data points are sampled from the latitude and longitude with an interval of 5.625\u00b0(32 \u00d7 64 grids), 2.8125\u00b0(64 \u00d7 128 grids), and 1.40525\u00b0(128 \u00d7 256 grids), respectively. In each grid, there are 13 vertical layers that include different weather states such as wind speed, geopotential, humidity, temperature, etc. The 32 \u00d7 64 grid weather state data are chosen during both the training and testing stages.\nImplementation Details On Weatherbench, we apply the 32 \u00d7 64 data with 6-hour time intervals, in which each data point contains weather states and 2 constant variables (i.e., land binary mask and orography). For the perdition results, the 69 dynamic weather states are forecast. In the training stage, the proposed WeatherFormer predicts weather states at the next 6 hours by using the weather states of the previous 36 hours as the input (i.e., a sequence of 6 weather state maps with a temporal interval of 6 hours). The patch size used to tokenize the weather state input is selected to be 1 in the spatial dimension and 2 in the temporal dimension, and the embedding dimension of the token is 1024. The number of layers of SF-Blocks is 12 and other settings are the same as in [Pathak et al., 2022]. Moreover, we horizontally rotate the input data by a random distance range from 0 to 64 with a 50% probability for the Earth Rotation augmentation, and a normal Gaussian noise with a variance of 0.1 is used for noise augmentation. AdamW is our optimizer with a learning rate of 0.0005 in a cosine learning rate scheduler. Our WeatherFormer is trained for 80 epochs with the first 5 epochs for warmup. The training needs 8 A100 GPUs with a mini-batch size of 4 in each gpu for 62 hours.\nEvaluation Metrics As used in [Pathak et al., 2022; Hu et al., 2023], we also use the weighted Root Mean Square Error (RMSE) and Accuracy (ACC) as the metrics for evaluation, where smaller RMSE and bigger ACC means better performance."}, {"title": "4.2 Results on the Weatherbench dataset", "content": "On Weatherbench, our WeatherFormer predicts 69 weather states at every data point position. To verify the effectiveness of our WeatherFormer, we compare the prediction performance of our WeatherFormer with other state-of-the-art NWP methods in terms of 3 typical weather states: 1) the geopotential at the height of atmospheric pressure of 500hPa (i.e., Z500); 2) the temperature at the height of atmospheric pressure of 850hPa (i.e., T850); and 3) the temperature at the height of 2 meters (i.e., T2M).\nWe report RMSE and ACC of our WeatherFormer in terms of Z500, T850, and T2M in Table 1. Among the compared state-of-the-art NWP methods, T42, T63, and IFS are the traditional physics based NWP methods, while Na\u00efve CNN, Cubed UNet, ResNet (pretrained), FourCastNet, and SwinVRNN use deep neural networks to predict weather states in a data-driven manner. As shown in Table 1, for IFS, WeatherFormer performs better than its lower-resolution version, e.g., T42 (64 \u00d7 128) and T63 (90 \u00d7 180), while comparing lower-resolution WeatherFormer to high-resolution IFS is unfair. IFS is an ensemble of 51 physical models that use a higher resolution (e.g., 720 \u00d7 1440) and more inputs (hourly) than ours (one model, 32 \u00d7 64, six-hourly). Despite this, we still perform better in T2M, comparable in T850 and Z500. Moreover, the energy consumed by IFS running a year is about 150000 times the consumption of training a WeatherFormer [Bauer et al., 2020]. When compared with SwinVRNN, the synthesis performance of our WeatherFormer is better, and the training consumption of WeatherFormer (80 epochs and 1-step training) is markedly lower than SwinVRNN (200 epochs and multistep RNN training)."}, {"title": "4.3 Ablation Study", "content": "To investigate the effectiveness of the proposed components, we conduct an ablation study on the Weatherbench dataset. The experimental results are reported in Table 2.\nEffectiveness of components. For the ablation study, we first design a baseline method by removing the temporal mixer from our WeatherFormer and using the AFNO as the token mixer. In order to accelerate the ablation study experiments, we also increase the patch size to reduce the complexity of the baseline method. We take the RMSE of the Z500 prediction at the next day as the example to analyze the prediction contribution of each proposed component. As shown in Table 2, our baseline method can achieve RMSE of 120, which is improved by adding temporal mixer into the SF-Block in our WeatherFormer. This suggests that the temporal trend of the past weather states is critical in the NWP task. Additionally, in our proposed PAFNO, we introduce position-related coefficients to mix token information by considering the impact of the token positions. When we replace the AFNO with the proposed PAFNO, the RMSE is further decreased from 106 to 98, which demonstrates the effectiveness of introducing the position-related coefficients. It is well-known that data-driven NWP methods often suffer from the training data overfitting problem. We adopt the Earth Rotation augmentation strategy during the training stage and observe that after applying the Earth Rotation augmentation during the training stage, another decrease in the RMSE (from 98 to 93) appeared on Z500. The results show that our Earth Rotation augmentation strategy is able to ease the overfitting issue to the training dataset for our WeatherFormer. Finally, by using the noise augmentation strategy, the RMSE further boosts to 89, indicating noise augmentation strategy reduces error accumulation.\nNoise Augmentation v.s. Two-stage Training. Error accumulation often happens on long-term prediction in a progressive manner. FourcastNet [Pathak et al., 2022] attempted to ease this problem by using a two-stage training strategy, where the NWP model is first pretrained, and then fine-tuned to predict the weather states at the next two time steps in an autoregressive manner. To compare the effectiveness of our Error Overlapping augmentation and two-stage training strategy, we use FourcastNet as the baseline method, and report the RMSE of the Z500, T850 and T2M prediction at the next first, third, and fifth days by using different strategies in Table 3. We can observe that despite the two-stage training strategy is able to improve the RMSE results, by only using the noise augmentation, FourcastNet can achieve better RMSE results at long-term prediction at all three weather states (see RMSE on the third and fifth days for second and third rows of Table 3). This suggests that the noise augmentation can relieve the error accumulation issue better than the two-stage training strategy does. Additionally, for the two-stage training strategy, since a fine-tune training process is introduced, two times of training time is required which doubles the energy consumption. Moreover, in the fine-tune training process, as the NWP framework is trained with an autoregressive manner, a huge amount of memory and computation resources is needed. In contrast, noise augmentation can reduce the effect caused by error accumulation and improve long-term prediction performance without bringing any additional cost. Also, it is interesting to note that further gain can be observed when we apply noise augmentation with a two-stage training strategy, which demonstrates these two strategies are complementary."}, {"title": "5 Conclusion", "content": "In this paper, we propose a space-time transformer-based network that perceives spatial-temporal information with the FFT-based mixer for weather forecasting. Specifically, we propose the PAFNO to maintain the parameter amount while making spatial mixer relative position-aware. Moreover, we notice the discreteness and rotation symmetry in weather data and introduce earth rotation augmentation. WeatherFormer achieves SOTA among data-driven methods on Weatherbench. On the one hand, this work demonstrates that data-driven NWP methods have tremendous potential to be applied in future weather prediction system. On the other hand, it is apparent that the challenges towards practice application are stout, and we anticipate more research interests are devoted into this newly emerging direction."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Qualitative Results", "content": "To further verify the effectiveness of our WeatherFormer, we visualize the prediction results of wind speed at 10m height above the surface generated by our WeatherFormer in Figure 5 to exhibit its ability to predict extreme weather. As shown in Figure 5, the red boxes in the GT figures demonstrate that Typhoon Rumbia was formed as a tropical depression around August 15, 2018, in the Pacific Ocean, and after two days of moving, it made landfall in Shanghai on August 17, 2018. Our WeatherFormer successfully predicts the formation and the moving track of Rumbia (see the red boxes in the Pred figures in Figure 5). The paths in the ground truth and paths generated by WeatherFormer are well aligned both in space and time."}, {"title": "A.2 Evaluation Metrics", "content": "Following [Rasp et al., 2020], we apply latitude-weighted root-mean-square error (RMSE) and anomaly correlation coefficient(ACC) to evaluate our WeatherFormer for the NWP task. The latitude-weighted RMSE is calculated as follows:\n\nL(j) = cos lat(j)  (20)\nNlas Niat coslat(j)\nMSE = 1 1 Nion Niat \u03a3\u03a3 L(j) (Yj,k - Yj,k)2, (21)\n\nRMSE = 1 Nsample \u03a3\u221aMSE. (22)\n\nNsample is the number of samples. Nlon and Nlat are the number of grid points along longitude and latitude, respectively. Yj,k and \u0177j,k indicate the predicted weather states and ground truth weather states at the j-th and k-th data point along the latitude and longitude, respectively. ACC is calculated as follows:\n\nCj,k = 1 Ntime \u03a3Yi,j,k, (23)\nYi,j,k = Yi,j,k - Cj,k, (24)\nYi,j,k = Yi,j,k - Cj,k, (25)\nACC = \u03a3i,j,k L(j)Yi,j,kYi,j,k  (26)\n\u221a\u03a3i,j,k L(j)Y2i,j,k \u03a3i,j,k L(j)Y2i,j,k\n\nNtime denotes the number of samples in the training set. Lower RSME indicates better prediction performance, while higher ACC indicates better prediction performance."}]}