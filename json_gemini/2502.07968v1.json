{"title": "Generative Risk Minimization for Out-of-Distribution Generalization on Graphs", "authors": ["Song Wang", "Zhen Tan", "Yaochen Zhu", "Chuxu Zhang", "Jundong Li"], "abstract": "Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM. Our code is provided at https://github.com/SongW-SW/GRM.", "sections": [{"title": "1 Introduction", "content": "In recent years, it has become increasingly crucial to develop machine learning models that can handle tasks with test data distributions differing from training data, commonly referred to as out-of-distribution (OOD) generalization (Mansour et al., 2009; Blanchard et al., 2011; Muandet et al., 2013; Beery et al., 2018; Recht et al., 2019; Su et al., 2019). Such disparities, termed as distribution shifts, can substantially undermine the"}, {"title": "3.1 GRM Objective", "content": "In our GRM framework, we propose to learn a classifier f(\u00b7) and a generator g(.), such that the generator g(.) will output an invariant subgraph for each input graph. Considering a graph input G = (V,E,X), the generator aims to outputs the invariant subgraph \\hat{G}_c = (V, \\hat{E}, \\hat{X}) for classification.\nTo ensure that the obtained subgraph is maximally invariant across domains while preserving the causal information, we consider the following learning objective for f and g, which is adopted in existing works (Chen et al., 2022; 2023):\n$\\max I(G_c; Y), \\text{ s.t. } G_c \\perp D, G_c = g(G)$.\nHowever, it is difficult to directly optimize this objective. Generally, the optimization objective of the generator is to maximize the log-likelihood term $\\log P(G_c|G)$. Combining this term, we propose a more feasible objective for Eq. (1):\n$\\max E [\\log P(G_c|G)] - I(G_c; D)$,\nwhich is referred to as our proposed GRM objective. Although the GRM objective is straightforward, it is intractable due to the lack of ground truth, i.e., $G_c$, for the generated invariant subgraph $G_c$. Alternatively, based on the SCMs on distribution shifts as illustrated in Fig. 1, we propose to model the causal variable Z as a latent variable for graph generation. By introducing the latent causal variable Z, we are able to derive the following theorem that allows for an end-to-end optimization for our objective in Eq. (2).\nTheorem 3.1. An evidence lower bound (ELBO) for optimization of the GRM objective, by introducing a latent causal variable Z and variational approximations Q(Z) and Q(\\hat{G}_c), is as follows:\n$\\max E [\\log P(G_c|G, Z)] \u2013 KL(Q(Z)||P(Z|G)) \u2013 E[KL(P(\\hat{G}_c|D, Z)||Q(\\hat{G}_c))] + E [\\log P(Z|D,\\hat{G}_c)]$."}, {"title": "3.2 Generator Implementation", "content": "Before we derive the detailed optimization losses based on Theorem 3.1, we first introduce the implementation of our generator. In particular, we aim to model Q(Z) using the generator g, which uses any graph G as input. However, it remains challenging to model Z with a suitable architecture of the generator g in the absence of the ground truth $G_c$. In particular, we propose to leverage the Variational Graph Auto-Encoder (VGAE) (Kipf & Welling, 2016; Simonovsky & Komodakis, 2018) for the generation of invariant subgraphs. This is because the optimization objective of VGAE involves a latent variable Z and aligns with the first term of the GRM objective. As such, we propose to implement the generator g(\u00b7) as a VGAE.\nFollowing the VGAE architecture, our generator consists of an encoder and a decoder. Given a graph input G, the encoder maps it into a latent space and outputs the latent variable $Z \\in \\mathbb{R}^{|V|\\times d_z}$. Here $d_z$ is the dimension size of Z. Moreover, Z involves |V| latent representations, i.e., $Z = {z_1, z_2,...,z_{|V|}}, which means we learn a latent representation for each node in G, and thus the number of nodes in $G_c$ equals that in G. For each node $v_i$, where $i \\in {1,2,..., |V|}$, we learn its representation as follows:\n$z_i \\sim N(z|\\mu_i, diag(\\sigma_i^2))$, where $\\mu_i = GNN_\\mu(V,E, X)_i$; and $\\log \\sigma_i^2 = GNN_\\sigma(V, E, X)_i$.\nTo generate node features of the invariant subgraph, i.e., $\\hat{X} \\in \\mathbb{R}^{|V|\\times d}$, we leverage the obtained latent variable Z along with a linear projection layer $f_x(\\cdot)$:\n$\\hat{X} = {f_x(z_1), f_x(z_2),..., f_x(z_{|V|})}$, where $f_x(z_i) = W_xz_i + b_x$.\nHere $W_x \\in \\mathbb{R}^{d\\times d_z}$ is the weight of the projection layer, and $b_x \\in \\mathbb{R}^{d}$ is the bias. Then we further generate edges from the latent variables $z_i$ as follows:\n$\\hat{E} = {\\hat{c}_{ij}|i, j = 1,2,...,|V|}$, where $\\hat{c}_{ij} = \\sigma(f_e(z_i)^T f_e(z_j))$ and $f_e(z_i) = W_ez_i + b_e$."}, {"title": "3.3 Optimization based on GRM", "content": "In this subsection, we introduce the detailed process to optimize our framework based on the GRM objective derived in Theorem 3.1. In particular, we design three different losses for the terms in the derivation result.\nSupervision Loss. For the supervision loss, we first consider the term $E[\\log(P(\\hat{G}_c|G, Z))]$. As the ground truth $G_c$ is unobserved, the common choice of reconstruction loss in VGAE is unavailable. Therefore, we propose to adopt the label Y of G as a proxy for $G_c$, based on the intuition that the optimal $G_c$ should maximally reflect the information of the label Y. In this manner, we could formalize the supervision loss as follows:\n$L_s = - \\sum_{Y \\in \\mathcal{Y}} p(y|G) \\log p(y|\\hat{G}_c)$, where $p(y|\\hat{G}_c) = f_y(\\hat{G}_c)$ and $\\hat{G}_c = g(G)$.\nIn the above loss, $p(y|\\hat{G}_c)$ is obtained by taking $\\hat{G}_c$ as input to the classifier f(\u00b7), and $f_y(\\cdot)$ denotes the output class probability regarding class y. Moreover, we set p(y|G) = 1 if y is the label of G, and p(y|G) = 0, otherwise. The above supervision loss could be interpreted as a cross-entropy classification loss for $G_c$.\nRegularization Loss. Generally, KL-divergence terms act as regularization in variational generation (Kipf & Welling, 2016; Kingma et al., 2019; Simonovsky & Komodakis, 2018). In our derivation of the GRM objective in Theorem 3.1, the two KL-divergence terms \u2013KL(Q(Z)||P(Z|G)) and \u2013KL(P($\\hat{G}_c$|D, Z)||Q($\\hat{G}_c$))\nrepresent the differences between the distributions of Z (given D) and Q(Z), as well as between the dis- tributions of $\\hat{G}_c$ (given D and Z) and Q($\\hat{G}_c$). Notably, the derived result is applicable for any Q(Z) and Q($\\hat{G}_c$). Specifically, we first define Q(Z) as a Gaussian distribution N(0, I), where $I \\in \\mathbb{R}^{d* \\times d*}$ is the identity matrix. In this way, we could directly regularize the learned \u03bc and $\\log \\sigma$ of Z, as P(Z|G) is also a Gaussian distribution, and thus we could explicitly derive the KL-divergence between it and N(0, I). For the second KL-divergence term, i.e., -KL(P($\\hat{G}_c$|D, Z)||Q($\\hat{G}_c$)), we first formulate Q($\\hat{G}_c$) as Q($\\hat{G}_c$) = Q($\\hat{X}$)Q($\\hat{E}$). In this manner, we could obtain:\n\u2212KL(P($\\hat{G}_c$|D, Z)||Q($\\hat{G}_c$)) = \u2212KL(P($\\hat{X}$|D, Z)||Q($\\hat{X}$)) \u2013 KL(P($\\hat{E}$|D, Z)||Q($\\hat{E}$)).\nNotably, as $\\hat{x}$ is the linear projection of Z, the term -KL(P($\\hat{X}$|D, Z)||Q($\\hat{X}$)) could also use Z for calculating the regularization loss in a similar way to -KL(Q(Z)||P(Z|G)). For another term -KL(P($\\hat{E}$|D, Z)||Q($\\hat{E}$)), we could decompose Q($\\hat{E}$) into multiple independent Bernoulli distributions as $\\bar{c}_{ij} \\sim Bernoulli(\\theta)$, where $\\theta \\in [0,1]$ is a controllable hyper-parameter. In this manner, we could consider the learned edge weight $\\hat{c}_{ij}$ as the parameter in a Bernoulli distribution and compute its KL-divergence with Q($\\hat{E}$). In concrete, we formulate the regularization loss as follows:\n$L_r = \\sum_{i=1}^{d_z} ((\\mu_i^2 + \\sigma_i^2) - \\log \\sigma_i) + \\sum_{i=1}^{|V|} \\sum_{j=1}^{|V|} (r(\\hat{a}_{ij}, \\theta) + r(1 - \\hat{a}_{ij}, 1 \u2013 \\theta))$,\nwhere $r(a, \\theta) = a\\log(a/\\theta)$. The first term is calculated from the KL-divergence of the two Gaussian distributions, which is KL(P($\\hat{X}$|D, Z)||Q($\\hat{X}$)). The second term is calculated from KL-divergence between the two Bernoulli distributions, which is KL(P($\\hat{E}$|D, Z)||Q($\\hat{E}$)). Particularly, this loss regularizes the learning process of latent variable Z and the generation process of invariant subgraph $G_c$, such that the obtained $G_c$ is more generalizable to various domains.\nInvariance Loss. Finally, we consider the thrid term derived in Theorem 3.1, i.e., $E[\\log P(Z|D,\\hat{G}_c)]$. Intuitively, this term aims to derive the correct latent variable Z given the generated invariant subgraph $G_c$"}, {"title": "3.4 Complexity Analysis", "content": "In this subsection, we analyze the time complexity of our framework. Particularly, the time complexity of our framework is primarily determined by the GNN encoder and the VGAE generator module, along with the three losses. Therefore, we first break down the complexity by considering the GNN and VGAE separately, then combining their contributions. Note that the time complexity of the GNN encoder is $O(|V|d^2 + |E|d)$. For the VGAE complexity, the module (1) encodes each node's representation and (2) reconstructs the node's representation. For each node, the VAE in VGAE performs operations involving encoding and decoding, which typically, and thus the time complexity for each node's VAE operation is proportional to $d^2$. Thus, for all nodes, the VAE complexity is $O(|V|d^2)$. Note that this process already involves the time complexity of the regularization loss. For the remaining two losses, the supervision loss and the invariance loss, we compute the time complexity as follows. First, since we are using the cross-entropy loss as the supervision loss, the time complexity is $O(|V|d)$. The invariance loss involves computing the Euclidean distance between\n$O(|V|d^2 + |E|d + |V|d^2 + |V|d + |V|^2d)$.\nBy simplifying the above time complexity, we can obtain the final time complexity as\n$O(|V|d^2 + |E|d + |V|^2d)$."}, {"title": "4 Comparative Results on Graph-Level Tasks", "content": "Although we focus on the node classification task, our method is also applicable to graph classifica- tion, i.e., graph-level out-of-distribution generaliza- tion. In the setting for graph-level tasks, the domain information exists in other graphs and thus could not directly calculate the node influence. Thus, we still use the nodes in the input graph G to learn domain-specific representations H. Notably, as these nodes will not cover the entire graph G, the learned H will not be trivial, i.e., the same for all nodes in G. For graph-level experiments, We consider four prevalent datasets, namely SP- Motif (Ying et al., 2019), MNIST-75sp (Knyazev et al., 2019), G-SST2 (Graph-SST2) (Socher et al.,"}, {"title": "5.1 Out-of-Distribution (OOD) Generalization", "content": "OOD Generalization aims to learn a model that can generalize to an unseen test domain, given several different but related training domain(s). Prior invariant methods (Ganin & Lempitsky, 2015; Li et al., 2018; Arjovsky et al., 2019) genreally focus on learning invariant features (Sun et al., 2016; Peng et al., 2019) or optimizing for the worst-case group performance (Hu et al., 2018; Sagawa et al., 2020). Recent works for OOD generalization on graphs (Chen et al., 2022; Li et al., 2022b; Wang et al., 2024) could be typically categorized into two classes: invariant learning and graph augmentation (Li et al., 2022a). Among invariant learning methods, CIGA (Chen et al., 2022) proposes to extract subgraphs that maximally preserve the invariant intra-class information based on causality. DIR (Wu et al., 2022b) uses a set of graph representations as the invariant rationales to create additional distributions. GIL (Li et al., 2022b) identifies invariant subgraphs via a GNN-based generator. More recently, MARIO (Zhu et al., 2023) utilizes the Information Bottleneck (IB) principle to learn invariant information. Among augmentation methods, LiSA (Yu et al., 2023) proposes to leverage graph augmentation to obtain more diverse training data for learning invariant information. EERM (Wu et al., 2022a) generates domains by maximizing the loss variance between domains in an adversarial manner, such that the obtained domains could aid in learning invariant representations."}, {"title": "5.2 Graph Generative Models", "content": "In recent years, numerous works have been proposed for graph generation (You et al., 2018; Grover et al., 2019). Specifically, GraphVAE (Simonovsky & Komodakis, 2018) proposes a framework based on VAE (Kingma & Welling, 2013) to generate graphs by encoding existing graphs. GraphRNN (You et al., 2018) generates graphs through a sequence of node and edge formations. Moreover, several methods (Jin et al., 2018; Preuer et al., 2018) focus on generating graphs based on specific knowledge. For example, MolGAN (De Cao & Kipf, 2018) adapts the framework of Generative Adversarial Networks (GANs) (Good- fellow et al., 2014) to operate directly on graph-structured data with a reinforcement learning objective. Note that although these methods leverage different information for generating graphs, they are not explic- itly proposed for handling the distribution shift problem on graphs. In contrast, our framework GRM aims to utilize domain information to generate graphs that are suitable for a trained classifier."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel framework, namely Generative Risk Minimization (GRM), to generate invariant subgraphs for each input graph to tackle the OOD generalization problem on graphs. Instead of extracting structures that may cause the loss of invariant information, we propose our GRM objective that incorporates a generation term and a mutual information term. We derive three types of losses to enable the optimization of our GRM objective in the absence of ground truths for the invariant subgraphs. The effectiveness of GRM is validated by our theoretical analysis and also the extensive experiments across both node-level and graph-level OOD generalization tasks. The results indicate the superiority of GRM over other state-of-the-art baselines."}, {"title": "A.1 Theorem 3.1 and Proof", "content": "In this section, we provide proof for Theorem 3.1.\nTheorem 3.1. An evidence lower bound (ELBO) for optimization of the GRM objective, by introducing a latent causal variable Z and a variational approximation Q($\\hat{G}_c$), is as follows:\n$\\max E [\\log P(\\hat{G}_c|G, Z)] \u2013 KL(Q(Z|G)||P(Z|G))\n\u2013 E[KL(P(\\hat{G}_c|D, Z)||Q(\\hat{G}_c))] + E [\\log P(Z\\D,\\hat{G}_c)]$,\nProof. We first present the GRM objective:\n$\\max E [\\log P(\\hat{G}_c|G)] - I(G_c; D)$.\nWe first derive the ELBO for the generation objective, which is a standard derivation for the variational auto-enocder (VAE):\n$\\log P(\\hat{G}_c|G)$\n$=\\log \\int_Z P(\\hat{G}_c, Z|G)dZ$\n$=\\log \\int_Z \\frac{Q(Z|G)}{Q(Z|G)} P(\\hat{G}_c, Z|G)dZ$\n(using Jensen's Inequality)\n$> \\int_Z Q(Z|G) \\log \\frac{P(\\hat{G}_c, Z|G)}{Q(Z|G)} dZ$\n$= E_Q[\\log \\frac{P(\\hat{G}_c, Z|G)}{Q(Z|G)}]$\n(using the property of conditional probabilities)\n$= E_Q[\\log \\frac{P(\\hat{G}_c|Z,G) P(Z|G)}{Q(Z|G)}]$\n$= E_Q[\\log P(\\hat{G}_c|Z, G)] \u2013 E_Q[\\log \\frac{Q(Z|G)}{P(Z|G)}]$\n(using the definition of KL-divergence)\n$= E_Q[\\log P(\\hat{G}_c|G, Z)] \u2013 KL(Q(Z|G)||P(Z|G))$.\nThen we decompose the second term \u2212I($\\hat{G}_c$; D) of our GRM objective as follows, based on the definition of mutual information:\n$\u2212I(\\hat{G}_c; D) = I(\\hat{G}_c; Z|D) \u2013 I(\\hat{G}_c; D, Z)$\nWe first decompose the first term:\n$I(\\hat{G}_c; Z|D) = E \\log \\frac{P(\\hat{G}_c|D, Z)}{P(\\hat{G}_c|D)}$\n= $E \\log \\frac{P(Z|D,\\hat{G}_c)}{P(Z|D)}$\n$= E [\\log P(Z|D,\\hat{G}_c)] + H(Z|D)$"}, {"title": "B.1 Artificial Distribution Shifts on Cora and Photo", "content": "Cora and Photo are two popular benchmark datasets used for node classification tasks and are also widely adopted to assess the effectiveness of GNN models. Specifically, these datasets are of moderate size, con- taining thousands of nodes (2,703 and 7,650, respectively). The data statistics are provided in Table 1. In particular, Cora is a citation network, where nodes represent papers and edges indicate the citation relation- ship between them. On the other hand, Photo is a co-purchasing network, with nodes representing specific goods and edges denoting frequent co-purchases of two goods. In the original dataset, the provided node features exhibit a strong correlation with node labels. Following EERM (Hamilton et al., 2017), in order to assess the model performance for graph OOD generalization under various distributions, we manually introduce distribution shifts into the training and testing data.\nMore specifically, we construct node labels and spurious domain-sensitive attributes from node features. Given the node features as X1, we start by randomly initializing a Graph Neural Network (GNN) with X1 as input and an adjacency matrix to generate node labels Y. To obtain the one-hot label vectors, we perform an argmax operation in the output layer. Then, we randomly initialize another GNN with the concatenation of Y and a domain ID as input to generate spurious node features X2. The next step is to concatenate these two sets of node features, i.e., X = [X1, X2], to create new node features for training and test data. This process is performed ten times for each dataset, resulting in ten graphs with different domain IDs. For training and validation, we utilize one graph each, while the classification accuracy is reported on the remaining graphs."}, {"title": "B.2 Cross-Domain Transfers on Twitch and FB-100", "content": "Cross-domain transfers are a common occurrence in scenarios involving distribution shifts on graphs. In various real-world situations, multiple observed graphs are available, each belonging to a specific domain. For instance, in social networks, domains can be defined based on where or when the networks are collected. Similarly, in protein networks, distinct species may have their own observed graph data, such as protein-protein interactions, representing separate domains. The key point is that graph data typically captures relational structures among specific entities, which often exhibit unique characteristics for different entity groups. As a result, the data-generating distributions can vary across these groups, leading to domain shifts.\nHowever, in order to facilitate transfer learning across graphs, it is necessary for the graphs within a dataset to share the same input feature space and output space. To achieve this requirement, we utilize two publicly available datasets, Twitch and FB-100, which satisfy these conditions.\nSpecifically, the Twitch dataset consists of seven networks, where nodes and edges represent Twitch users and their mutual friendships, respectively. These networks are collected from different regions, namely DE, ENGB, ES, FR, PTBR, RU, and TW. Although these networks have similar sizes, they exhibit variations in terms of density and maximum node degrees, as presented in Table 5.\nThe FB-100 dataset comprises 100 snapshots of Facebook friendship networks from 2005. Here each network contains nodes that represent Facebook users from a specific American university. In our experiments, we utilize fourteen networks: John Hopkins, Caltech, Amherst, Bingham, Duke, Princeton, WashU, Brandeis, Carnegie, Cornell, Yale, Penn, Brown, and Texas. These graphs exhibit significant variations in terms of sizes, densities, and degree distributions, indicating that the model capability in handling different graph structures becomes crucial for this dataset."}, {"title": "B.3 Temporal Evolution on Dynamic Graph Data: Elliptic and Arxiv", "content": "The distribution shift problem can also occur in temporal graphs that dynamically change over time. The evolution of these graphs can be generally categorized into two types. In the first type, there exist multiple snapshots of the graph, with each snapshot captured at a specific time. As time progresses, a sequence of graph snapshots is generated, which may exhibit variations in terms of node sets and data distributions. For example, financial networks capture the payment flows among transactions within different time intervals and thus result in different domains. In the second type, there exists only one single graph that evolves through the addition or deletion of nodes and edges. This type is commonly seen in large-scale real-world graphs, such as social networks and citation networks. In these graphs, the distribution of node features, edges, and labels often exhibit a strong correlation with time at different scales. For our graph OOD generalization experiments, we utilize two public real-world datasets, namely Elliptic and Arxiv. These datasets are suitable for exploring node classification tasks within the context of evolving temporal graphs.\nThe Elliptic dataset consists of a series of 49 graph snapshots, where each snapshot represents a network of Bitcoin transactions. Specifically, each node corresponds to a transaction and each edge represents a payment flow. Within these transactions, approximately 20% are labeled as either licit or illicit, with the objective being to identify illicit transactions within future networks. In the original dataset, the first six graph snapshots contain highly imbalanced classes, with the number of illicit transactions being less than 10 among thousands of nodes. Consequently, we exclude these snapshots and focus on the 7th to 11th, 12th to 17th, and 17th to 49th snapshots for training, validation, and testing, respectively. Furthermore, due to the low positive label rate observed in each graph snapshot, we organize the 33 testing graph snapshots into 9 distinct test sets based on their chronological order. The dataset also requires the framework to effectively handle diverse label distributions encountered during the transition from training to testing data.\nThe Arxiv dataset comprises 169,343 Arxiv CS papers covering 40 subject areas, along with their citation relationships. The objective is to predict the subject area of a given paper. In (Hu et al., 2020), the papers published before 2017, in 2018, and since 2019 were utilized for training, validation, and testing, respectively. They employed a transductive learning setting (Tan et al., 2022c; Dolz et al., 2020; Mathavan et al., 2023), wherein the nodes in the validation and test sets were also present in the training graph. Instead, Arxiv adopts an inductive learning setting, which better reflects real-world scenarios. Here, the nodes in the"}, {"title": "C.1 Baseline Settings", "content": "In this subsection, we introduce the detailed settings for baseline methods used in our experiments."}, {"title": "C.2 Training Details", "content": "During training, we conduct all experiments on one NVIDIA A6000 GPU with 48GB of memory. The package requirements of our experiments are listed below."}, {"title": "C.3 Training Time and Memory Usage", "content": "In this subsection, we provide the time/memory of all experiments conducted on one NVIDIA A6000 GPU with 48GB of memory in Table 6."}, {"title": "D Detailed Results", "content": "In this section, we provide detailed results for the specific test domains on Cora (8 test domains), Photo (8 test domains), FB-100 (3 test domains), and Twitch (5 test domains). The results are provided in Table 7, Table 8, and Table 9."}, {"title": "E Created Datasets with Different Degrees of Distribution Shifts", "content": "In this section, we introduce the details of the dataset Cora-Mix used in Sec. 4.3. Specifically, we aim to manually control the degree of distribution shifts across different domains. However, the original dataset Cora provided in EERM (Wu et al., 2022a) creates distribution shifts via the concatenation of domain-sensitive features and label-related features, which means the degree of distribution shifts cannot be easily quantified. Therefore, we propose to mix up these two types of features with a weight to control the distribution shift degree. In particular, we follow the same strategy of generating these features, except that we changed their dimensions to be equal. In this manner, we can perform mix-up on them with a specific weight, i.e., the bias ratio. We also keep the domain split setting of 1/1/8 for training, validation, and test, respectively."}, {"title": "F Limitations", "content": "Despite its superior performance, our framework still possesses several limitations. For example, our GRM framework involves the learning of domain information from other nodes or graphs in the domain. However, in practice, the available graphs in each domain may not be sufficient. As a result, the performance of our framework may be impacted. In addition, although our GRM framework is validated in both node-level and graph-level tasks, its performance on edge-level tasks is not evaluated."}]}