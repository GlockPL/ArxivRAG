{"title": "Towards Advanced Speech Signal Processing: A Statistical Perspective on Convolution-Based Architectures and it's Applications", "authors": ["Kapu Nirmal Joshua", "Raghav Karan"], "abstract": "This article surveys convolution-based models- convolutional neural networks (CNNs), Conformers, ResNets, and CRNNs-as speech signal processing models and provide their statistical backgrounds and speech recognition, speaker identification, emotion recognition, and speech enhancement ap- plications. Through comparative training cost assessment, model size, accuracy and speed assessment, we compare the strengths and weaknesses of each model, identify potential errors and propose avenues for further research, emphasising the central role it plays in advancing applications of speech technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "In this subsection, the principle of convolution and its ap- plication to the modification of a speech signal are explained."}, {"title": "A. Mathematical Foundation for Convolution", "content": "Most broadly, convolution is the mathematical process of compounding two functions to produce a third function, which reflects the influence of one effect on the other\u2500how changing one alters the form of the other. Convolution is effective for analyzing linear time-invariant (LTI) systems and is applied in a wide range of engineering fields, such as speech processing [1].\n\n$(f * g) (t) = \\int_{-\\infty}^{\\infty} f(\\tau).g(t- \\tau) d\\tau$ (1)\n\nWhere $(f * g)(t)$ is the convolution of $f(t)$ and $g(t)$, where both functions overlap in all time.\nThe discrete convolution formula for sequences ${f[k]}$ and ${g[k]}$ is as follows:\n\n$(f *g)[n] = \\sum_{k=-\\infty}^{\\infty}f[k] \\cdot g[n - k]$. (2)\n\nEquation (2) defines the discrete convolution, which sums products of $f[k]$ and shifted versions of $g[k]$ for all integers k.\nIf $x(t)$ is the input of the system and $h(t)$ is its impulse response, then $Y(t)$ has the following form:\n\n$Y(t) =  (x * h)(t) = \\int_{-\\infty}^{\\infty} x(\\tau). h(t - \\tau) d\\tau$ (3)\n\nThis equation represents the output signal as a function of the input signal modified by the system's impulse response.\nThere are several useful properties for signal processing analysis that convolutions possess. It is commutative, i.e., $f * g = g * f$; associative, i.e., $f * (g * h) = (f * g) * h$; and distributive over addition, i.e., $f * (g + h) = (f * g) + (f * h)$. These characteristics allow the reduction and analysis of complex systems."}, {"title": "B. Introduction to Speech Signal Processing", "content": "Speech signal processing involves analyzing speech signals and developing techniques to process them for effective com- munication between humans and machines. Speech signals are 1D, time-varying signals that are a manifestation of acoustic description of human language [1]. These signals are generally (a) non-stationary, (b) large dynamic range, and (c) rich spectral content, which can be challenging to analyze.\nIn speech signal processing, convolution plays an important role. When a speech signal $s(t)$ is transmitted or recorded in a communication channel, it is changed by the channel impulse response $h(t)$. Received signal $r(t)$ can be expressed as a convolution between speech signal and channel impulse response:\n\n$R(t) = (s* h)(t) = \\int_{-\\infty}^{\\infty} (\\tau)h(t-\\tau) d\\tau$ (4)\n\nThis equation models the interaction between the speech signal and the channel, incorporating effects such as echo, reverberation, and attenuation.\nIf noise $n(t)$ is added, the received signal becomes:\n\n$r(t) = s(t) * h(t) + n(t)$ (5)\n\nThe existence of noise makes it difficult to reconstruct the priori speech signal.\nConvolution is also employed for feature extraction in the speech domain, such as calculating the frequency content of speech. The filtered speech signal can be convolved with a sequence of filters to realize time-frequency representations such as short-time Fourier transform, which are important for speech recognition and speaker labeling [1], [13]. These methods derive articulatory and prosodic features of speech that are of paramount importance to speech decoding of spoken language.\nFurther, speech signal heterogeneity\u2014that is, heterogeneity of the signals arising from speakers, accents, speech rate, and emotional states\u2014introduces the challenge of complexity. Convolutional approaches must account for this heterogeneity. Yet, the issues, e.g., background noise, reverberation and real-world speech signal mixing, continue to be problems. Convolutional approaches, augmented with statistical signal processing, is an active research area for achieving robustness and performance in speech processing systems [9], [19]. These methods are also the basis of developments in voice-based technology, automatic transcription and voice biometrics."}, {"title": "C. Convolution-Based Architectures", "content": "Recent breakthroughs in deep learning have enabled deep and convolutional based architectures which address a variety of problems in speech signal processing. The 4 functional architectures investigated are Convolutional Neural Network (CNN), Conformers, Convolutional Recurrent Neural Network (CRNN), and Residual Networks (ResNets).\nIn these architectures, convolutional layers are used to successively extract features of speech signals. Convolutional neural networks (CNNs) apply convolutional layers for fast feature extraction, whereas Convolutional Complex Architec- tures (Conformers) apply convolution along with self-attention for the local and global relationships. CRNNs combine con- volutional and recurrent layers to learn temporal dynamics, and ResNets exploit shortcut connections to build deeper architectures without performance overfitting. Collectively, these architectures increase the accuracy of speech signal processing, including in noisy and dynamic environments."}, {"title": "II. CONVOLUTION BASED ARCHITECTURES", "content": ""}, {"title": "A. Convolutional Neural Networks (CNNs)", "content": "Convolutional neural networks (CNNs) have almost univer- sal relevance for the analysis of structured data, e.g., spec- trograms, in vocal signal processing. In convolutional neural networks (CNNs) layers with trainable filters convolutional layers are employed to slide across the input data and learn local patterns. The one-dimensional convolution operation is written as follows with a temporal input sequence $x(t)$ and a convolution kernel $w(k)$:\n\n$y(t) = \\sum_{k=-K}^{K} w(k) x(t - k)$ (6)\n\nOutput $y(t)$ is defined where temporal dependencies are encoded, which are the basis for efficient speech signal pro- cessing [3]. CNNs make use of two-dimensional convolutions in time and frequency, which are as follows when applied to spectrograms:\n\n$y(i, j) = \\sum_{m=-M}^{M} \\sum_{n=-N}^{N} W (m,n). S(i - m, j \u2013 n),$ (7)\n\nwhere $S(i, j)$ is the spectrogram value at time i and fre- quency j, and $W(m,n)$ is a filter. This lets CNNs also learn the frequency features specific to speech recognition tasks [22].\nPooling layers, e.g., max pooling, downsample spatial reso- lution and retain global features (i.e., lead to higher translation invariance). Given a feature map $y(i, j)$, max-pooling over a window size $p x q$ is defined as:\n\n$z(i, j) = \\max_{0<m<p} \\max_{0<n<q} y(i + m, j + n)$. (8)\n\nBatch normalization also stabilizes and speeds up training by normalizing layer outputs. For an activation $a$ with mean \u03bcand variance $\u03c3\u00b2$, the normalized output is:\n\n$\\hat{a} = \\frac{a-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, y = \\gamma \\cdot \\hat{a} + \\beta,$ (9)\n\nwhere $\u03b3$ and $\u03b2$ are learnable parameters [4].\nCategorical cross-entropy loss is the rule-of-thumb for CNN classification and is formulated as follows with the predicted probabilities $\\hat{y}_i$ and the associated true labels $y_i$:\n\n$Loss = - \\sum_{i=1}^{C}  y_i log(\\hat{y}_i)$ (10)\n\nwhere $C$ denotes the number of classes. This loss function contributes to effective feature learning that can be applied to a few tasks, e.g., automatic speech recognition (ASR) [1].\nThe usefulness of CNNs as an engine for extracting hi- erarchical patterns from spectrograms has long been shown. Sainath and Parada [23] applied CNNs for noisy large vocab- ulary continuous speech recognition (LVCSR) and showed the model's robust nature, while Abdel-Hamid et al. [22] demon- strated CNNs' superiority in phoneme recognition. CNNs are"}, {"title": "B. Conformers", "content": "To achieve state-of-the-art results for Automatic Speech Recognition (ASR), Gulati et al. [1] proposed the Conformer architecture, which incorporates convolutional layers within the Transformer design to offer both local and global infor- mation concurrently. Transformers are not adaptive to local patterns-something CNNs are apt at capturing\u2014even when they leverage self-attention to model long-range dependencies. The Conformer makes use of these inherent strengths by adding convolutional layers to Transformer blocks.\nEach Conformer block's forward pass is:\n\n$x_i = x_i + \\frac{1}{2}FFN(x_i),$ (11)\n$x_i' = x_i + MHSA(x_i),$ (12)\n$x_i'' = x_i + Conv(x_i'),$ (13)\n$Y_i = Layernorm(\\frac{1}{2} + \\frac{1}{2}FFN(x_i''))$ (14)\n\nwhere $x_i$ is the input to the i-th block. From the above setup, MHSA can learn long-range dependencies as well as short- range dependencies using the convolutional layers to refine local features [1].\nIn Conformer, the convolutional module extracts local de- pendencies that are necessary for audio processing. For an input sequence $s(t)$ and a filter $W$, the convolution operation is:\n\n$y(t) = \\sum_{k=-K}^{K} W(k)s(t \u2212 k)$ (15)\n\nwhere $y(t)$ captures local features like phonemes. Con- former employs depthwise separable convolutions to reduce parameter complexity. Depthwise convolution operates per channel:\n\n$DepthwiseConv(x) = x * Wd,$ (16)\n\nfollowed by pointwise convolution to merge channels:\n\n$PointwiseConv(x) = Wp.x$ (17)\n\nMHSA handles global dependencies and performs scaled dot-product attention. For query $Q$, key $K$, and value $V$, attention $A$ is computed as:\n\n$A = softmax(\\frac{QKT}{\\sqrt{dk}})$ (18)\n\nwhere $d_k$ is the dimension of the keys, allowing the model to focus on different parts of the input sequence.\nThe FFN applies position-wise non-linear transformations, enhancing representational power. Given input $x$, the FFN is:\n\n$FFN(x) = \u03c3(W2 \u00b7 ReLU(W\u2081\u00b7x + b\u2081) + b2)$ (19)\n\nwhere $W\u2081$ and $W2$ are weights, $b\u2081$ and $b2$ are biases, and o is an activation function, enabling complex relationship modeling within the sequence.\nThe Conformer's convolution-augmented Transformer framework is consistent with statistical signal processing concepts, and employs both intrinsic local and extrinsic global statistical characteristics for efficient modelling of the speech signal. This balance has also resulted in better ASR performance due to lower Word Error Rates (WER) in the benchmarks [1], [5]."}, {"title": "C. Residual Networks (ResNet)", "content": "Residual Networks (ResNet), introduced by He et al. [25], address the vanishing gradient issue by introducing residual connections, where some information can go around layers. This skip connection allows stable learning in deep architec- tures and, hence, ResNet is powerful for complex hierarchical feature extraction tasks, e.g., speech, image processing [26]. In each residual block, the model learns a residual mapping instead of a direct transformation. Given an input x, the output of a residual block is:"}, {"title": "D. Convolutional Recurrent Neural Networks (CRNNs)", "content": "Convolutional Recurrent Neural Networks (CRNNs) inte- grate Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), in particular Long Short-Term Memory (LSTM) units, to improve upon the use of both spatial and temporal dependencies in sequential data, e.g., audio spectrograms [11]. This architecture is particularly appropriate for problems that need to extract local features as well as long-range temporal dependencies.\nInitially, CRNNs apply a series of convolutional layers to extract spatial features from an input spectrogram $S(i, j)$, where i and j denote the time and frequency dimensions. For each convolutional filter W(m, n), the subsequent feature map y(i, j) is:\n\n$y(i, j) = \\sum_{m=-M}^{M} \\sum_{n=-N}^{N}W(m,n). S(i-m, j \u2013 n)$ (25)\n\nwhere M and N define the kernel size. Pooling layers, e.g., max pooling, are commonly used to downsample the spatial resolution, preserving the strongest features and minimizing computational complexity:\n\n$Z(i, j) = \\max_{0<m<p} \\max_{0<n<q} y(i + m, j + n)$ (26)\n\nwhere p x q is the pooling window size.\nFollowing the convolutional feature extraction, the output is reshaped into a sequence structure and fed into an LSTM layer that examines the temporal dependencies of the extracted features. For an input feature sequence xt at time step t, the LSTM layer updates its cell state ct and hidden state ht using the following equations:\n\n$ft = \u03c3(Wf.ht\u22121 + Uf \u00b7 xt + bf)$ (27)\n$it = \u03c3(Wi.ht\u22121 + Ui \u00b7 xt + bi)$ (28)\n$\u010ct = tanh(W\u00b7ht\u22121 + Uc\u00b7xt + bc)$ (29)\n$Ct = ft. Ct-1 + it. \u010ct$ (30)\n$ht = ot tanh(ct)$ (31)\n\nwhere ft, it, and ot are the forget, input, and output gates respectively; W and U are weight matrices, and b is the bias vector. This transformation sequence allows the LSTM to learn long-term dependencies present in the input data [?].\nThe final output from the LSTM layer, ht, represents the processed temporal features and is passed to a fully connected layer with softmax activation for classification. The softmax function (class probability prediction) is expressed as:\n\n$y_i = \\frac{e^{zi}}{\\sum_{j=1}^{C} e^{zj}},$ (32)\n\nwhere yi is the probability of class i, zi is the logit for class i, and C is the total number of classes.\nCRNNS are optimized for speech processing tasks (e.g., automatic speech recognition and speaker identification) by"}, {"title": "III. APPLICATIONS", "content": ""}, {"title": "A. Speech Recognition", "content": "Speech recognition is the task of translating spoken lan- guage into text or machine understandable commands. It is another fundamental one in the area of speech signal pro- cessing, and has been extensively used in virtual assistant, transcription, and voice-activated systems, etc. The state-of- the-art performance of speech recognition systems is signifi- cantly enhanced by convolutional-based system architectures through effectively capturing local and global aspects of the speech signal.\n\nThe overall objective in speech recognition is to rep- resent the probability of a word sequence W = {W1,W2,...,WT} conditioned on a set of acoustic features X = {X1,X2,...,XT}. This is often formulated using Bayesian decision theory:\n\n$P(W|X) = \\frac{P(X|W)P(W)}{P(X)}$ (33)\n\nAnd where $P(X|W)$, $P(W)$, and $P(X)$ are the acoustic and language models and evidence with the possibility to be set to zero during decoding.\nConvolutional Neural Networks (CNNs) have been used before to model $P(X|W)$ learning hierarchical representations"}, {"title": "B. Speaker Identification", "content": "Speaker identification is the process of identifying the identity of a speaker from vocal characteristics. In applications ranging from security systems, personalized user interfaces, and forensic investigation, this task is extremely crucial. Convolution-based architectures have been instrumental in enhancing the accuracies and robustness of speech-recognition systems by capturing the unique variability in each person's voice.\n\nThe main task in speaker identification is to estimate the probability $P(S|X)$ that a certain speaker $S$ is spoken for a set of acoustic feature sequences $X = {X1,X2,...,xt}$. This can be formulated using Bayesian inference as:\n\n$P(S|X) = \\frac{P(X|S)P(S)}{P(X)}$ (39)\n\nHere, $P(X|S)$, $P(S)$, and $P(X)$ are the acoustic model, prior probability of the speaker, and evidence, respectively. Convolutional Neural Networks (CNNs) have been applied with success to model $P(X|S)$ through learning hierarchical features of the input acoustic representations, such as Mel- Frequency Cepstral Coefficients (MFCCs) [15]. The convolu- tional operation of CNNs for use in speaker identification can be written as:\n\n$Yi,j = \u03c3(\\sum_{m=-M}^{M} \\sum_{n=-N}^{N} Wm,n Xi+m,j+n + b)$ (40)\n\nWhere Yij is the output feature map xi,j is the input feature map, Wm,n is the convolutional kernel weights, b is the bias, and o is the activation function.\nConvolutional Recurrent Neural Networks (CRNNs) ap- proximate spatial and temporal relationships in speech signals by combining convolutional neural networks (CNNs) and recurrent networks (e.g., Long Short-Term Memory (LSTM) networks) [11]. The architecture of the CRNN model analyzes input sequence Xt at time t as:\n\n$H\u2081 = LSTM(CNN(Xt), Ht\u22121)$ (41)\n\nSpecifically, in which Ht (hidden state) and CNN(Xt) (extracting spatial information using the convolutional neural network) are applied.\nResidual networks (ResNets) train deep CNNs by adding \"residual connections\" that solve the vanishing gradient prob- lem and enable training of models with increased depth without performance loss [20]. The residual connection is mathematically represented as:\n\n$Y = F(X, W) + X$ (42)\n\nWhere F(X, W) is the output of convolutional layers with weights W, where X is the input of residual block.\nGaussian Mixture Models (GMMs) are frequently employed together with CNNs for estimating the distribution of acoustic features of each speaker [17]. The probability P(XS) can be expressed as:\n\n$P(X|S) = \\sum_{k=1}^{K} \u03c9kN(X|\u03bck, \u03a3k)$ (43)\n\nHere, \u03c9k are the mixing weights, and $N(X|\u03bck, \u03a3k)$ are the components of the Gaussian distributions with mean \u03bc\u03b5 and covariance \u03a3\u03ba\u00b7\nIn noisy conditions, convolutional-based models augmented with statistical signal processing (SSP) techniques have been demonstrated to be highly robust. For instance, Na et al. As demonstrated in [15], CNNs with noise-resistant feature extraction techniques dramatically improve the effectiveness of speaker recognition in the presence of acoustic background noise. In addition, by combining GMM and CNN, hybrid models have enabled real-time speaker recognition at good accuracy [17].\nGenerally, convolution-based architectures are naturally suited to statistical signal processing paradigms for the ability to learn robust features as well as model subject speaker properties."}, {"title": "C. Emotion Detection", "content": "Speech emotion recognition is the task by which the emo- tional state of a person is decoded from vocalization. The role of the task is significant for applications such as human-computer interaction, mental health monitoring and automatic customer service. Conv-based architectures have now allowed impressive performance gains for emotion detection systems regarding both accuracy and robustness, since they are capable of modeling the patterns and variability of speech signals underlying a wide range of emotions.\nThe objective in emotion detection is to model the prob- ability $P(EX)$ of an emotion E given an acoustic feature sequence $X = {x1, x2,..., XT}$. This can be formulated using Bayesian inference as:\n\n$P(E|X) = \\frac{P(X|E)P(E)}{P(X)}$ (44)\n\nWhere $P(XE)$ denotes the emotional acoustic model, $P(E)$ the prior probability of the emotion and $P(X)$ the evidence.\nConvolutional Neural Networks (CNNs) have been widely utilized to encode $P(X|E)$ learning hierarchical features from input acoustic representations such as Mel-Frequency Cepstral Coefficients (MFCCs) [13]. The convolution function in CNNs (for emotion detection) can be represented as:\n\n$Yi,j = \u03c3(\\sum_{m=-M}^{M} \\sum_{n=-N}^{N} Wm,n. Xi+m,j+n + b)$ (45)\n\nWhere Yi,j is the output feature map, xi,j is the input feature map, wm,n are the weights of convolutional kernels, b is the bias, and is the activation function.\nCNN-LSTM networks use convolution units with Long Short Term Memory (LSTM) units for joint acquisition of spatial and temporal features in speech signals [16]. The hybrid architecture views the input sequence Xt at time t as:\n\n$H\u2081 = LSTM(CNN(Xt), Ht\u22121)$ (46)\n\nwhere Ht is the hidden state at time t, and the spatial features are obtained from the input with CNN(Xt).\nResidual networks (ResNets) enhance the performances of deep Convolutional Neural Networks (CNNs) just by inserting residual connections, enabling deep network training without performance degrading [7]. Residual connection of emotion detection can be described as:\n\n$Y = F(X,W) + X$ (47)\n\nIn which F(X, W) is the output obtained from convolu- tional layers with weights W and X is the input provided to the residual block.\nActivation functions and normalization techniques in emo- tion detection based deep learning models are commonly applied to ensure training stability and convergence. E.g., information flow on the network is restricted using the Gated Linear Unit (GLU) activation function:\n\n$GLU(a, b) = a \u2297 \u03c3(b)$ (48)\n\nIn which a and b are input tensors, \u2297 denotes element-wise multiplication, and \u03c3 is the sigmoid function.\nIn the area of statistical signal processing, such convolution-based architectures enhance the process of feature extraction through considering the distribution of so-called emotional features in the speech signal. Wang et al. [18] survey various deep learning approaches for emotion recognition, highlighting the effectiveness of CNNs in capturing discriminative features. Prabhu and Raj\u00b9 demonstrated that CNNs can be applied to discriminative fine emotional cues by the learning of abstract features from the convolution of the spectrogram.\nFurthermore, CNN-LSTM networks as proposed by Kumar and Sharma [16] incorporate temporal dynamics of speech for an improvement of the emotion classification performance. These hybrid models combine the local feature extraction capabilities of CNNs with the sequence modeling strengths of LSTMs, providing a comprehensive framework for emotion detection.\nConceptually, convolution-based architectures are consistent with principles of statistical signal processing by providing discriminative feature learning and good models of emotion-ality for speech signals. These developments have resulted in more precise and trustworthy emotion detection systems capable of performing robustly in heterogeneous and dynamic environments."}, {"title": "IV. COMPARATIVE ANALYSIS OF THE ARCHITECTURES", "content": "Model size, accuracy, speed, and training cost may all be used to compare the four models. The findings from the VoxForge and Voxlingua6 datasets [9] served as the basis for the related analysis. VoxForge includes speech data in En- glish, German, Russian, Italian, Spanish, and French, whereas Voxlingua6 adds more speaker and language variability. Both datasets include speech data from various languages. Data statistics for each dataset are shown in Table I."}, {"title": "A. Training Cost", "content": "The amount of processing power required to train each model is known as the training cost. Models with more parameters often demand more time and processing power. The CNN architecture is light (6 million parameters) and hence versatile, whereas the Conformer architecture is heavy (15.5 million parameters), as seen in Table II from [9]. Convolu- tional and recurrent layers are combined in CRNN, which contains over 19.5 million parameters. Because of the intricate convolutional self-attention mechanism, the Conformer and CRNN require additional processing resources, particularly in noisy and multilingual environments like Voxlingua6. The memory space and computational cost during deployment are directly impacted by the model size, or the number of parameters."}, {"title": "B. Accuracy", "content": "The Conformer on average outperforms the other architec- tures (error rate 5.27% on the Voxlingua6 Dev set). CNNS follow closely with an error rate of 7.18%, while the CRNN and Residual Network perform slightly worse, reflecting the trade-off between model complexity and accuracy."}, {"title": "C. Speed", "content": "In terms of speed, the CNN is most efficient for real-time applications, while the Conformer strikes a balance between speed and accuracy, making it suitable for slightly delayed yet accurate systems."}, {"title": "V. CONCLUSION", "content": "In order to accomplish speech signal processing tasks in- cluding speaker identification, emotion detection, and voice recognition, this article compared CNN, Conformer, CRNN, and Residual Network designs. We tested these models on training cost, model size, accuracy, and inference speed using the VoxForge and Voxlinguab datasets. We discovered that Conformers are more accurate than CNNs, which are more suited for low-resource situations because of their lower size [1], [9].\nFuture studies will concentrate on improving noise resis- tance, developing effective, low-latency models for real-time applications, and reducing architectural complexity to make them easier to utilise on devices with constrained resources. Investigating hybrid configurations that combine convolution and self-supervised learning [4] may result in additional ad- vancements by striking a balance between model complexity and performance."}]}