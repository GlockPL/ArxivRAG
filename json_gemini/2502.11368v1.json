{"title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing", "authors": ["Zhengxiang Wang", "Veronika Makarova", "Zhi Li", "Jordan Kodner", "Owen Rambow"], "abstract": "The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus for reproducibility.", "sections": [{"title": "1 Introduction", "content": "Assessing the writing quality of essays manually is both time-consuming and labor-intensive. This task becomes even more demanding and challenging due to high cognitive load (Cai, 2015), when assessors have to assign scores and provide comments based on multi-dimensional analytic criteria, referred to here as multi-dimensional analytic assessments (see Fig. 1 for an illustration). For evaluation of non-native language (L2) learners' writing, such precise and multi-dimensional assessments are highly valuable and desirable, but they are often not provided, due to the significant time, cost, and expertise required to produce them. This is also evidenced by the dearth of publicly available L2 writing corpora annotated with multi-dimensional analytic assessments (Banno et al., 2024)."}, {"title": "2 Related Work", "content": "Automated Writing Evaluation (AWE) We use AWE to include both automated essay scoring (AES) and feedback comment generation (Shermis and Burstein, 2013). AWE systems have existed since the 1960s (Page, 1966) and have evolved over time with a predominant focus on AES (Ke and Ng, 2019; Hussein et al., 2019; Zhang and Zou, 2020; Uto, 2021; Lagakis and Demetriadis, 2021). Modern AWE systems use deep neural networks for scoring (Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Rodriguez et al., 2019; Yang et al., 2020; Xie et al., 2022) and feedback comment generation (Nagata, 2019; Han et al., 2019; Babakov et al., 2023). The latter task typically focuses on sentence-level grammatical error identification and correction (Behzad et al., 2024b). Existing non-LLM AWE systems mainly provide holistic assessment, with some specialized systems offering uni-dimensional analytic assessment based on a specific dimension of writing quality (Ke and Ng, 2019; Jong et al., 2023; Banno et al., 2024).\nLLMs used for AWE Unlike prior AWE systems, LLMs can be prompted in natural language to jointly score and comment on a given essay. A growing body of research has explored the use of LLMs for assessing L2 writing. For AES, LLMs have been examined for holistic scoring (Mizumoto and Eguchi, 2023; Yancey et al., 2023; Wang and Gayed, 2024), discourse coherence scoring (Naismith et al., 2023), and multi-dimensional analytic scoring (Yavuz et al., 2024; Banno et al., 2024)."}, {"title": "3 Corpus", "content": "Overview Our corpus consists of 141 literature reviews written in English by 51 L2 graduate students, with an average word count of 1321 (930 excluding references). The reviews cover five broad topics from the humanities and social sciences, chosen to minimize the need for specialized disciplinary knowledge: (1) the social consequences of legalized cannabis, (2) the Canadian linguistic landscape, (3) online learning, (4) lessons from the COVID-19 pandemic, and (5) pacifism. Essays on topics 1, 3, and 5 were written individually, while those on topics 2 and 4 were completed collaboratively by 2-4 authors.\nThe corpus is a result of a large research project conducted at the University of Saskatchewan in 2021 with an aim to examine the developmental trajectory of literature review writing skills among L2 graduate students. The project involved three rounds of a 5-unit online tutorial series conducted over the course of 2021, with each round lasting 13 weeks (see Appendix A for details). Participation was voluntary, with 31 participants completing all five writing tasks across all rounds, and 20 further students completing at least one task before withdrawing.\nOur Previous Studies The corpus was used in our previous studies (Li et al., 2023a,b; Makarova et al., 2024), but has never been made public. These three studies all use only a subset of the corpus, namely essays written individually or those based on topics 1, 3, and 5.\nEssay Authors The corpus authors comprise a diverse group of L2 learners, representing a wide range of first languages and enrolled in graduate programs across various disciplines at multiple Canadian universities. Their English proficiency ranged from upper-intermediate to advanced, with an average score equivalent to IELTS Band 7 based on conversions from various standardized English language tests. Scores varied from IELTS 6.5 to 8.5, with a standard deviation of 0.55.\nHuman Assessments Most essays in the corpus were assessed by three (94.3%) or two (5.0%) independent human experts. As illustrated in Fig. 1, the assessments consist of scores on a 10-point scale and comments based on 9 analytic assessment criteria. While scores were required, comments were optional for the assessors. A total of six assessors with professional experience in English language teaching participated at different stages of the research project. The 9 assessment criteria (see Appendix A.3 for details) include: (C1) material selection; (C2) material integration and citation; (C3) quality of key components; (C4) logic of structure; (C5) content and clarity of ideas; (C6) coherence (flow of ideas); (C7) cohesion (use of connectors); (C8) grammar and sentence structure; and (C9) academic vocabulary.\nAssessment Quality The 31 students who completed all writing tasks evaluated the quality of human assessments on a 4-point scale in an anonymous final project survey. Based on the 30 submitted survey responses, all participants agreed that the assessments were at least \"useful\" (rating = 3), with 24 participants (80%) rating them as \"very useful\" (rating = 4).\nData Contamination Since the corpus was created prior to the release of ChatGPT and has never been made public, it contains no LLM-generated contents and is free from the risk of data contamination (Jacovi et al., 2023; Sainz et al., 2023), making it an ideal resource for LLM evaluation."}, {"title": "4 A Novel Feedback Comment Quality Evaluation Framework", "content": "A common approach to evaluating feedback comment quality for an essay uses manual judgments (e.g., rating on a Likert scale), since generating essay-level feedback is an open-ended task. However, this approach is expensive, time-consuming, not scalable, and may not always be reproducible. For L2-related feedback comments, common criteria for assessing comment quality include specificity, relevance, helpfulness (Han et al., 2024; Stahl et al., 2024; Behzad et al., 2024a,b), and the ability to identify writing problems (Stahl et al., 2024; Behzad et al., 2024a,b). These criteria reflect a common and practical need of L2 learners to be shown specific problems in their essays and how to correct them to improve their writing quality.\nTo address the issues of manual judgment, we propose an automatic evaluation framework that evaluates the quality of a feedback comment in terms of its ability to effectively identify relevant writing problems within the assessed essay. As illustrated in Fig. 2 (left), the framework utilizes LLMs to extract problems identified in assessment comments and to characterize their specificity and potential helpfulness. The framework consists of the following three steps.\nProblem Extraction We start out by extracting any writing problems stated or implied in assessment comments, along with any relevant contextual information for each problem, such as further explanations, suggestions for improvement, concrete corrections, or clarifying questions. We define a problem as any writing-related issue that affects the quality of the writing, such as citation errors, logical flaws, or grammatical mistakes.\nProblem Classification The extracted problems are further characterized along three dimensions: whether an extracted problem (1) points to a specific part of the essay, (2) includes any form of suggestion (general or specific), and (3) provides a concrete correction that can be directly applied to fix an identified problem. These classifications offer a way to assess the specificity and potential helpfulness of related comments.\nCorrection Relevance Check We perform a sanity check to determine whether the proposed correction (and thus the comment) is in fact relevant to the original essay. The Correction Relevance Check also contains three binary classification questions for a more nuanced relevance analysis: (1) does the problem indicated in the correction exist in the essay? (2) is the indicated problem related to the given assessment question? and (3) is the correction correct?\nThe results show that both human- and LLM-provided corrections are highly relevant, with answers to those three questions being \u201cYes\u201d typically above 90% time (see Appendix B.3). We thus focus on the Problem Classification results when comparing human- and LLM-identified problems in the subsequent sections.\nValidating the Proposed Framework The first author and a paid graduate student in Linguistics (native speaker) first annotated some held-out samples for training and developing the annotation guidelines. Each then independently annotated at least another 200 samples containing human- and LLM-generated comments or problems for Problem Extraction and Problem Classification. Afterward, they met to resolve disagreements before the inter-annotator agreement (IAA) was calculated.\nWe measure IAA using Cohen's Kappa. As is known (Feinstein and Cicchetti, 1990), Cohen's Kappa can provide misleading values with highly imbalanced class distributions. We therefore also provide exact match rates which have not been corrected for random agreement. Fig. 2 (right) shows that the IAA is typically high. When the Cohen's Kappa is low due to class imbalance (i.e., problems being incorrectly or not extracted is uncommon or rare and nearly all extracted problems contain a suggestion), the exact match rates are high. LLM task performance, evaluated based on the resolved annotations, is also notably high.\nWe automatically evaluate LLM performance on the Correction Relevance Check by assuming that human-identified corrections are generally relevant. Specifically, we assess whether the LLM classifies these corrections as mostly relevant when presented with their corresponding essays and assessment questions (positive samples), and as mostly irrelevant when paired with random essays and questions (negative samples). As shown in Fig. 2 (right), our results confirm this expectation."}, {"title": "5 Experiments", "content": "This sections describes and presents the main experiments conducted and the results obtained.\n5.1 LLM Prompting\nList of LLMs We evaluate variants of three popular LLMS: GPT-40-2024-08-06 (GPT-40, OpenAI et al., 2024a), GEMINI-1.5-FLASH (Gemini-1.5, Gemini Team et al., 2024), and LLAMA-3 70B-INSTRUCT (Llama-3, Grattafiori et al., 2024).\nDefault Prompt Setting All prompts contain a system prompt, an input essay, and an assessment instruction. There are four default conditions. (1) The system prompt contains not only essential background information, such as writing topic, but also helpful information regarding the L2 nature of the input essay, year of writing, the same general assessment guidance used by human assessors. (2) The input essay always includes references. (3) LLMs are instructed to produce a score before an optional comment for each assessment question (4) via greedy decoding, i.e., with temperature set to 0. Conditions 1-3 are used to maximize the alignment between human and LLM assessment conditions.\nInteraction Modes We consider three possible user-LLM interaction modes, depending on how the 9 assessment questions are presented. In Interaction Mode 1 (IM 1), all questions are prompted at once in a single-turn conversation, where all LLM assessments are generated in a single response. In Interaction Mode 2 (IM 2), the questions are asked one at a time, with an LLM generating answers to each question in corresponding turns in a multi-turn conversation. In Interaction Mode 3 (IM 3), however, the assessment questions are provided independently of one another in 9 separate prompts to elicit 9 separate outputs from an LLM.\n5.2 Baselines\nGiven the open-ended nature of the task, we compare raw assessments produced across individual assessors to understand the assessment patterns and behaviors of humans and LLMs. For a more robust statistical analysis, we only consider raw assessments made by assessors B, C, and F, since the essays they each assessed and co-assessed both cover at least half of the corpus.\n5.3 Evaluation of Scores\nQuadratic Weighted Kappa (QWK) This is a metric for rating inter-rater agreement. It ranges from 0 (random agreement) to 1 (perfect agreement), though it can be negative when agreement is worse than chance. QWK places higher penalties for larger score mismatches, but can yield misleadingly high or low values due to chance correction when the distribution of scores is highly skewed (Yannakoudakis and Cummins, 2015).\nAdjacent Agreement Rate (AAR) AAR measures the percentage of scores (from two raters) that lie within a specified threshold $k$ of one another. When $k = 0$, it assesses exact matches. For this study, we set $k$ = 1 (AAR1), meaning raters' scores are treated as matching as long as they differ by no greater than 1.\nWe use AAR1 in addition to QWK to account for the limitation of QWK's chance correction, as we observe that both human- and LLM-assigned scores are highly biased toward the respective means. AAR1 also helps address observed scoring inconsistency issues (often by 1 point) by humans."}, {"title": "5.4 Results", "content": "We compare human- and LLM-generated assessments in terms of scores, comments, and the interaction between scores and comments.\n5.4.1 Scores\nFigure 3 illustrates the overall scoring agreement between all pairs of assessors.\nHumans score more like humans and LLMs score more like LLMs. More concretely, human-human QWK and AAR1 are almost always higher than the corresponding human-LLM agreement. Similarly, LLM-LLM agreement exceeds human-LLM agreement in virtually all cases, with a much larger margin, suggesting that LLMs may resemble each other in scoring more closely than humans resemble each other. Criterion-level agreement between human/LLM assessors shows similar patterns, as shown in Fig. 4.\nLLMs can score approximately like humans. The best human-LLM AAR1 for the three LLMs ranges from 0.59 to 0.88, with all LLMs achieving an AAR1 above 0.5 with assessor F (Fig. 3). Moreover, the AAR1 scores between GPT-40 and assessor B and between Llama-3 and assessors B and C are always greater than 0.5. Overall, it shows that LLMs can generate sensible or reasonably good scores, often differing by no more than 1 point from the corresponding human-generated scores.\nHuman-LLM agreement tends to be higher when LLMs respond to each assessment criterion separately under IM 3. This is particularly true compared to when LLMs respond to all criteria at once under IM 1, since IM 3 exhibits a generally higher agreement level (Fig. 3). This result may imply that, while human assessors score the 9 assessment criteria sequentially, they effectively make independent scoring decisions based on the specifics of each assessment question.\nThat said, the effect of interaction modes is overall limited, given the fairly close scores (i.e., high QWK/AAR1) assigned across them for each LLM. Therefore, we average human-LLM agreement for each LLM across the three interaction modes to obtain human-LLM agreement in Fig. 4.\nThe degree of human-LLM agreement varies across assessment criteria. For example, Fig. 4 shows that LLM-assigned scores are relatively closer to human-assigned scores on assessment criteria C1 (material selection), C2 (material integration and citation), C8 (grammar and sentence structure), and C9 (academic vocabulary) than the other criteria. Among criteria C3-C7, LLMs and humans agree rather poorly on C7 (use of connectors), with LLMs consistently assigning scores more than 1 point away from human-assigned ones.\n5.4.2 Comments\nTable 2 shows the percentage of time an assessor provided a comment, and when they did, the average length of these comments, the percentage of comments identifying a problem, and the average number of problems identified in each comment.\nLLMs always provide comments and identify problems, but humans do not. This is an apparent advantage of LLMs since, unlike humans, they do not experience practical constraints like mental fatigue and limited time for writing comments.\nInteracting with LLMs one question at a time leads to more elaborate, specific, and helpful comments. LLM comments are much longer and identify more problems in IM 2 and IM 3 than in IM 1 (see Table 2). Additionally, Fig. 5 shows that comments generated in IM 1 are also less likely to refer to a specific essay part and offer a concrete correction than those generated in IM 2 and IM 3 or human-generated comments. This suggests that IM 2 and IM 3 provide higher levels of elaboration than IM 1. Furthermore, IM 3 produces more corrections than both IM 2 and humans across all assessment criteria, except C1, for which a correction is unlikely since it is about evaluating the relevance of cited references. In other words, LLMs can be more elaborate, specific, and potentially helpful than humans in their comments.\nLLMs can be more specific than humans on assessing subjective criteria. While humans and LLMs (in IM 3) are comparably likely to include a correction in their comments for objective criteria C2, C8, and C9, LLMs' comments (in IM 3) tend to offer more corrections on other subjective criteria (e.g., C3: quality of key components, C4: logic of structure etc.), except for C1 (see above). This aligns with the observation that humans tend to comment more on objective criteria, since commenting on subjective criteria requires more explanations and can thus be more demanding to do.\n5.4.3 Score-Comment Interaction\nSince lower scores reflect a perception of more writing problems, an assessor typically needs to provide a more extensive feedback comment to both cover the identified problems and justify their low scores. We highlight this score-comment interaction by measuring the correlations between scores and the token counts of or the numbers of identified problems in the related comments.\nAs expected, the last column in Table 2 shows strongly negative score-comment correlations across both human- and LLM-generated assessments. The fact that these negative correlations are generally much stronger when measured with the number of identified problems suggests that it is a more fine-grained metric than comment length and also indicates the usefulness of our framework proposed in Section 4."}, {"title": "5.5 Summary", "content": "We show that LLMs can generate sensible scores, typically within 1 point of human-generated ones on a 10-point scale, and feedback comments that identify more writing problems than human assessors that are specific, and potentially helpful. This is particularly true when LLMs are prompted in IM 3 where each assessment question is asked independently of each other. Moreover, like humans, LLMs also generate assessments that exhibit an expected and negative score-comment correlation, justifying the validity of their assessments. Overall, these results highlight that LLMs can generate reasonably good multi-dimensional analytic assessments."}, {"title": "6 Further Analyses", "content": "6.1 Re-examining Our Assumption about Feedback Comment Quality\nOur proposed feedback comment quality evaluation framework assumes that the quality of a feedback comment is related to how well it identifies relevant writing problems of an assessed essay. The framework extracts and characterizes problems of assessed essays identified in comments to evaluate the specificity and helpfulness of these comments.\nTo assess this assumption, we adopt an LLM-as-a-judge approach (Zheng et al., 2023), prompting OPENAI-01-MINI-2024-09-12 (01-mini, OpenAI et al., 2024b) to directly assess the specificity and helpfulness of a feedback comment, given the corresponding essay and assessment question on a 10-point scale. We do not define specificity and helpfulness to avoid injecting biases and choose all comments, generated by humans and LLMs, from one subjective criterion (C6: coherence or flow of ideas) and one objective criterion (C9: academic vocabulary) to balance our examination. We then calculate the corrections between these two scores produced by o1-mini and the number of different types of problems identified by our framework.\nThe results in Table 3 shows that the characteristics extractable from applying the framework correlate very well with the o1-mini-assigned specificity and helpfulness scores. In particular, the number of problems that mention specific essay parts and offer corrections appears to be overall stronger signals of specificity and helpfulness than the mere number of problems, which shows negligible correlations for comments from IM 1 or IM 2. This shows the potential of our framework in providing a more fine-grained and interpretable measurement of specificity and helpfulness levels of comments.\n6.2 Reliability of LLM-generated Assessments\nWe evaluate the reliability of LLM-generated assessments across different realistic conditions that mirror potential real-world use cases. To prevent experimental confounding, we change only one condition at a time for a given LLM in a specific interaction mode, assuming that users tend to interact with their chosen LLM in a consistent manner.\nFirst, we consider GPT-40-2024-08-06 (GPT-40-Aug) in IM 1 with the default prompt setting from Section 5.1 as the baseline. To test the effect of model variant, we run the same experiment but with GPT-40-2024-05-13 (GPT-40-May). We also prompt GPT-40-Aug while varying one of the four conditions in the default prompt setting (see Section 5.1) by (1) removing the helpful information from the system prompt, (2) excluding references in the input essays, (3) instructing LLMs to produce a comment before a score, or (4) setting temperature to 1 to increase output randomness.\nTo ensure the comprehensiveness of our experiments, we prompt GPT-40-May in IM 2 and IM 3 under default prompt setting to study the effect of model variant under other interaction modes. We also prompt Llama-3 in IM 1 changing the first three conditions in the default prompt setting mentioned in the last paragraph. The baselines here are GPT-40-Aug and Llama-3 prompted under respective interaction modes from Section 5.1.\nWe use QWK and AAR1 and three widely adopted machine translation metrics, i.e., BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and BERTScore (Zhang et al., 2020), to evaluate the reliability of the generated scores and comments between contrastive condition pairs, respectively.\nThe results in Table 4 show that LLMs are capable of generating highly stable scores, with an AAR1 score at least 0.81 and mostly above 0.9 across all conditions. Their generated comments are also decently similar with BERTScore typically no lower than 0.67. A small-scale manual check and a correlation analysis performed in Appendix D further verify the validity of BERTScore."}, {"title": "7 Conclusion", "content": "This study provides evidence that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. Our findings highlight the promising role of LLMs in assessing academic English writing, especially for graduate-level literature reviews, which is a highly technical genre. In short, LLMs show strong pedagogical potential, benefiting both L2 learners and instructors for self-regulated learning or teaching assistance. We propose and validate a novel feedback comment quality framework to facilitate our analysis.\nLooking ahead, future studies could further characterize and compare the writing problems identified by human- versus LLM-generated comments, offering deeper qualitative insights. Additionally, it would be valuable to develop a metric grounded in our proposed framework that can directly compare the relative quality of two sets of comments."}, {"title": "Limitations", "content": "Generality of Findings This study focuses on L2 graduate-level academic writing, specifically literature reviews in the humanities and social sciences. While this domain represents a significant subset of academic writing, the findings may not generalize to other genres (e.g., technical reports, creative writing) or proficiency levels (e.g., undergraduate or professional writers). Additionally, our study is limited to English, a high-resource language, which means our results may not be indicative of LLMs' capabilities in other languages, particularly low-resource ones. Future research should explore the applicability of our findings across diverse writing contexts and linguistic backgrounds.\nWeakness of Our Assumption About Feedback Quality A key limitation of our approach is that it does not account for other factors that may influence the perceived quality of a feedback comment, such as politeness (e.g., rude comments may not be well received) or the logical coherence of the argument (e.g., illogical comments could be misleading). However, this concern is less pronounced for LLM-generated feedback comments, as LLMs are trained to align with human preferences and social norms (Ouyang et al., 2022). Moreover, these factors could potentially be incorporated into our framework by adding additional steps focused on politeness and argumentation etc.\nIndirect Evaluation of Feedback Quality While our approach to measuring the general quality of LLM-generated assessments is intuitive and simple, it is inherently indirect. A large-scale manual evaluation remains necessary to more accurately assess and compare the quality of human- and LLM-generated multi-dimensional analytic assessments. Due to resource constraints, we leave this investigation to future studies.\nLimited Validation and Reliability Testing Due to time and resource constraints, we were unable to comprehensively validate our proposed feedback comment quality evaluation framework. As a result, we may have overlooked some potential issues with the framework or the LLM outputs. Similarly, the reliability assessments we conducted are limited, with only one factor being changed at a time in each evaluation. More extensive experiments are needed to further validate our claim that LLM-generated assessments are generally reliable and to explore the conditions influencing this reliability.\nEthical Considerations\nCorpus Creation The research project that led to the construction of the corpus was ethically reviewed and received approval from the University of Saskatchewan for involving human participants. Participants provided informed consent to allow the use of their materials, with the option to withdraw at any time.\nHuman Annotations We compensated the hired annotator at a rate of approximately US$25 per hour, which exceeds the minimum wage in the region where the annotations took place.\nPotential Biases in LLM Assessments LLMs are trained on large-scale datasets that may contain inherent biases, which can be reflected in their assessments. For example, they might systematically favor certain writing styles, linguistic structures, or cultural conventions, leading to biased evaluations. However, we argue that in contexts where human assessments are not readily accessible, the benefits of LLM-generated feedback - particularly for L2 learners \u2013 may outweigh potential biases. Furthermore, bias mitigation strategies, such as improved prompting techniques or advancements in LLM development, could help reduce these concerns."}, {"title": "A Corpus", "content": "A.1 Basic Corpus Statistics\nTable 5 provides the basic statistics of the corpus. Note that throughout this study, we use the default word tokenizer of NLTK to compute word counts.\nA.2 Details of the 5-Unit Tutorial Series\nTable 6 presents details of the 5-unit tutorial series, including the themes, notions, activities, duration, and writing task for each unit.\nTo support their writing, the authors were provided with a short, curated bibliography for each task, designed to help them focus on literature review writing while minimizing the effort required for bibliographic searches. Prior to submitting their final writing samples for expert assessments, the authors engaged in peer reviews (for topics 1, 3, and 5) or group collaboration (for topics 2 and 4).\nA.3 Assessment Criteria\nThe 9 assessment criteria/questions provided to human assessors are detailed in Table 7."}, {"title": "B Feedback Comment Quality Evaluation Framework", "content": "B.1 Implementation\nThe framework is implemented using LLMs. More concretely, we used GPT-40-2024-11-20 for Problem Extraction and Problem Classification, and GPT-4-TURBO-2024-04-09 for Correction Relevance Check.\nB.2 Annotation\nGuidelines Table 9 provides explanations and examples of what is considered as a problem for Problem Extraction, and the three characteristics relevant to Problem Classification: whether an extracted problem (1) refers to a specific part of the essay, (2) provides a suggestion (general or specific), and (3) offers a concrete correction.\nSamples for Problem Extraction We employed stratified sampling to randomly select 100 human-generated feedback comments and 108 LLM-generated feedback comments. In total, there are 208 comments for manual annotations.\nFor LLM-generated comments, half of them were generated under Interaction Mode 1 and the other half under Interaction Modes 2 and 3. Comments from Interaction Modes 2 and 3 were sampled together to reduce manual annotation effort, as these comments tend to be lengthy. The sampling covered the 9 assessment criteria, with 2 comments from each of the 3 LLMs used, resulting in 9 * 3 * 2 = 54 comments from Interaction Mode 1 and another 54 comments from the combined Interaction Modes 2 and 3.\nSamples for Problem Classification We randomly sampled 100 problems extracted from both human- and LLM-generated comments, resulting in 200 problems for annotations.\nSince the distribution of extracted problems across the nine assessment criteria are highly skewed, we ensured that there were at least 5 problems for each assessment criterion.\nProblem Extraction For each feedback comment, the two annotators were provided with LLM-extracted problems and asked to identify the number of correctly extracted problems (true positives), the number of incorrectly extracted problems (false positives), and the number of problems not extracted (false negatives). The number of true negatives is always set to 0, as there is no negative prediction in problem extraction.\nA problem is considered correctly extracted if the LLM output contains the exact or paraphrased problem stated or implied in the feedback comment."}, {"title": "E Prompts", "content": "Note that", "$\" is a placeholder for all prompt templates included in this section. For example, \"$comment\" is a placeholder for a comment.\nE.1 Prompts for the Feedback Comment Quality Evaluation Framework Pipeline\nThe full prompt templates for the three steps in the pipeline of the feedback comment quality evaluation framework are given below. Among these three prompts, the prompt for Problem Extraction contains three in-context exemplars, whereas the prompts for the other two steps are zero-shot prompts.\nE.1.1 Prompt for Problem Extraction\nYou will be given a feedback comment written for a student's essay. Your task is to identify and extract all the writing-related problems mentioned or implied in the comment, along with any explanations, suggestions, corrections, questions, quotations, or other relevant information provided in the comment for each extracted problem.\nA writing-related problem is any issue that affects the quality of the writing, such as citation errors, logical flaws, coherence issues, grammatical mistakes, or inappropriate word choices, among others.\nE.1.2 Prompt for Problem Classification\nYou will be given an excerpt of a feedback comment written for a student's essay. Your task is to answer the following questions": "n1. Does the excerpt refer to a specific part of the essay? A specific part refers to a part of the essay that can be easily located by the student. For example", "sentence 2 in paragraph 2,\" \"in paragraph 6,\" \"the first citation,\" or \"the first sentence of the paper\" and so on. A less concrete location, such as \"the introduction,\" or \"the conclusion,\" is also considered a specific part if it is accompanied by some referenceable details, such as \"The significance of South Australian policy is unclear, as it is the first citation and the only one in the Introduction.\" Note that the excerpt may only contain a quoted text from the essay, in which case, the quoted text is considered a specific part.\nE.1.3 Prompt for Correction Relevancy Check\nYou will be given an excerpt of a feedback comment written for a student's essay according to an assessment question. Your task is to answer the following questions": "n1. Does the problem pointed out in the excerpt exist in the corresponding essay? If the excerpt uses a quoted text to point out a problem", "parts": 1, "follows": "nYou are an expert academic writing instructor specializing in graduate-level work", "topic": "Topic. The review was written in 2021", "clarity": "n########## Writing starts ##########\n$writing\n########## Writing ends ##########\nThe specifics of how the assessment instruction part is constructed are detailed below.\nE.2.1 Interaction Mode 1\nIn Interaction Mode 1", "once": "nQ1: {Assessment question 1"}, "nQ2: {Assessment question 2}\nQ9: {Assessment question 9}\nAfter these assessment questions is an answer instruction:\nE.2.2 Interaction Mode 2\nIn Interaction Mode 2, the assessment questions are presented sequentially and one at a time. Below is the basic structure:\nQi: {The ith assessment question.}\n{Answer instruction}\nAi:\nThe answer instruction resembles the one used in the Interaction Mode 1.\nProvide your score out of 10, followed by comments or suggestions if any. Your response should use the following format:\nScore:\nComments or suggestions:....\nNote that, we append LLM's response to the ith assessment question to the original"]}