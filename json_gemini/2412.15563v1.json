{"title": "In-context Continual Learning Assisted by an External Continual Learner", "authors": ["Saleh Momeni", "Sahisnu Mazumder", "Zixuan Ke", "Bing Liu"], "abstract": "Existing continual learning (CL) methods mainly rely on fine-tuning or adapting large language models (LLMs). They still suffer from catastrophic forgetting (CF). Little work has been done to exploit in-context learning (ICL) to leverage the extensive knowledge within LLMs for CL without updating any parameters. However, incrementally learning each new task in ICL necessitates adding training examples from each class of the task to the prompt, which hampers scalability as the prompt length increases. This issue not only leads to excessively long prompts that exceed the input token limit of the underlying LLM but also degrades the model's performance due to the overextended context. To address this, we introduce InCA, a novel approach that integrates an external continual learner (ECL) with ICL to enable scalable CL without CF. The ECL is built incrementally to pre-select a small subset of likely classes for each test instance. By restricting the ICL prompt to only these selected classes, InCA prevents prompt lengths from becoming excessively long, while maintaining high performance. Experimental results demonstrate that InCA significantly outperforms existing CL baselines, achieving substantial performance gains.", "sections": [{"title": "1 Introduction", "content": "Continual learning (CL) aims to enable models to learn a sequence of tasks incrementally (Chen and Liu, 2018; De Lange et al., 2021). CL is typically categorized into three main settings: task-incremental learning, class-incremental learning (CIL), and domain-incremental learning (Van de Ven and Tolias, 2019). In this paper, we focus on the CIL setting (Rebuffi et al., 2017), where each task has a set of distinctive classes, and a single model is developed to handle all tasks and classes. At test time, no task information is provided for each test instance. This differs from task-incremental learning, which provides the task-id for each test instance, making classification much easier. CIL requires a unified model that can distinguish all classes seen thus far. In domain-incremental learning, all tasks have the same classes but are from different domains.\nThere are two key challenges in CIL. (1) Catastrophic forgetting (CF), which refers to the performance deterioration of earlier tasks due to parameter updates in learning new tasks (McCloskey and Cohen, 1989). (2) Inter-task class separation (ICS), which refers to the phenomenon that without accessing the previous task data, the learning of a new task has difficulty in establishing decision boundaries between the new and old classes (Kim et al., 2023). Although the CL community has studied CF extensively, the challenge of ICS has only been identified recently in (Kim et al., 2022). Both challenges disappear in in-context CIL with LLMs. A simple method to apply in-context learning to CIL is to incrementally add few-shot training examples for each new class to the in-context prompt. This prompt includes examples from all classes encountered so far along with instructions for classification. Since the LLM parameters remain unchanged, CF is inherently avoided, and ICS is addressed by encompassing all classes and their examples within the same prompt.\nUnfortunately, this approach is not scalable for CIL because the prompt length rapidly increases with each new task or class added, quickly exceeding the token limits of LLMs. Although summarizing the training examples can increase the number of classes that can be learned (i.e., included in the prompt), the underlying scalability problem persists. Moreover, including excessive and often irrelevant information from various classes leads to significant performance degradation (see Section 6). Even with the recently introduced long-context LLMs (Chen et al., 2024; Reid et al., 2024), our experiments demonstrate that the performance degradation persists despite the increased token capacity.\nIn this paper, we introduce InCA (In-context Continual Learning Assisted by an External Continual Learner), a novel method that overcomes the"}, {"title": "2 Related Works", "content": "There is a large body of literature on continual learning. The main focus is on dealing with CF. Existing techniques can be broadly classified into a few categories. (1) Regularization, which uses a regularizer to ensure that important network parameters from previous tasks are minimally altered when learning new tasks, thereby reducing CF (Li et al., 2022; Liu et al., 2019). (2) Replay, which stores some training samples from previous tasks. When learning a new task, the model is trained using both the new task data and the stored replay data to mitigate CF (Liu et al., 2021a; Qin et al., 2022; Huang et al., 2021). Some replay methods do not store actual data but learn data generators to generate data similar to those from previous tasks (Shin et al., 2017; He and Jaeger, 2018). (3) Architectural-based, which encompasses various methods aimed at managing CF through structural modifications. Some techniques expand the network's capacity as new tasks are learned (Wang et al., 2022a; Yan et al., 2021; Qin et al., 2023). Some do parameter isolation, which trains sub-networks for each task by using masks to prevent updates to critical parameters or neurons from previous tasks, or by ensuring that new task parameters are orthogonal to those of prior tasks (Ke et al., 2021a, 2023; Konishi et al., 2023; Serra et al., 2018; Gururangan et al., 2022; Zhu et al., 2022; Geng et al., 2021; Lin et al., 2022; Liu et al., 2023). Moreover, some methods incorporate parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation (LoRA) (Hu et al., 2021) and prompt-tuning to allocate task-specific parameters for each new task (Razdaibiedina et al., 2023; Wang et al., 2022b, 2024b). These systems often implement various mechanisms to predict the task-id, which is essential for selecting the appropriate model for CIL. They may utilize a separate network, entropy, or out-of-distribution detection to predict the task-id (Rajasegaran et al., 2020; Abati et al., 2020; Kim et al., 2023). Our work differs from these approaches as it does not require task-id prediction. While the aforementioned methods train a different model for each task and rely on task-ids, our approach allows for adding one class at a time with no task-id prediction. Our ECL directly predicts the most probable classes."}, {"title": "3 Problem Formulation", "content": "We study class-incremental learning in the text classification domain. CIL involves learning a sequence of tasks arriving sequentially (Kim et al., 2023). Let B be the number of tasks encountered so far. Each task b (1 < b < B) is associated with a training dataset $D_{b} = \\{(x_{i}^{(b)}, y_{i}^{(b)})\\}_{i=1}^{|D_{b}|}$, where $|D_{b}|$ is the total number of instances in Db, $x_{i}^{(b)}$ denotes an input (text) instance, and $y_{i}^{(b)}$ is its corresponding class label. Let Yb be the set of classes belonging to b (i.e., the set of all classes in Db). For any two tasks b and b', their corresponding class sets are disjoint ($Y_{b} \\cap Y_{b'} = \\emptyset$ for $b \\neq b'$). The overall class set for all B tasks is defined as $\\bigcup_{b=1}^{B} Y_{b} = Y$. The goal is to construct a unified predictive function f: X \u2192 Y capable of classifying any given test instance x across all tasks/classes seen so far, despite the restriction that no data from previous tasks are retained during training, i.e., replay-free."}, {"title": "4 Proposed InCA Method", "content": "This section presents InCA (In-context Continual Learning Assisted by an External Continual Learner), a framework designed to address the challenges of CIL by leveraging the in-context learning capabilities of LLMs. InCA has three main stages: (1) tag generation, where semantic tags are extracted from the input text using the LLM (Section 4.1); (2) external continual learning, which identifies the top k most probable classes based on the generated tags through Gaussian class modeling and Mahalanobis distance scoring (Section 4.2); and (3) in-context learning with class summaries, where the LLM predicts the final class label for the input text using summaries of the top k candidate classes (Section 4.3). Figure 1 depicts the overall framework."}, {"title": "4.1 Tag Generation", "content": "To capture the essential semantic information from an input text x, we generate a list of tags that include topics, keywords, important entities, and other relevant elements. For example, a customer's banking query processed by our framework (see Figure 1) might generate tags such as \"banking\" or \"paycheck deposit\" while omitting less pertinent information. Additionally, the tags are automatically extended to include related terms that commonly appear in similar contexts. For instance, the tag \"paycheck deposit\" may be extended to include terms like \"transfer funds\" or \"payroll processing\". Tags are generated by prompting the LLM to produce both primary tags and related terms. The specific prompt used for tag generation is detailed in Appendix A.2."}, {"title": "4.2 External Continual Learner", "content": "The ECL leverages the generated tags to identify the k most probable classes for a given input, thereby filtering out the irrelevant context. As mentioned earlier, the ECL operates by accumulating statistics without additional training and thus, inherently avoids CF.\nGaussian Class Representation: Each class is modeled as a Gaussian distribution, with a mean vector and a shared covariance matrix. This representation helps mitigate the ICS problem by allowing classes to have independent distributions. However, since the covariance matrix has high dimensionality, storing a separate covariance matrix for each class would result in excessive space consumption. To address this, we assume that all classes share the same covariance matrix, drastically reducing the space required.\nLet $T_{j} = [t_{1,j}, t_{2,j} . . ., t_{R,j}]$ be the list of all tags generated by the LLM for class j, where R is the total number of tags generated from all training instances of class j. We employ the widely-used Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) model to encode each tag $t_{r,j} \\in T_{j}$ into a h-dimensional embedding vector $z_{r,j} \\in R^{h}$. The mean vector $\\mu_{j} \\in R^{h}$ for class j is computed as the average of all its tag embeddings:\n$\\mu_{j} = \\frac{1}{R} \\sum_{r=1}^{R} z_{r,j}$\nThe shared covariance matrix $\\Sigma \\in R^{h \\times h}$ is updated incrementally as new classes are introduced. The contribution of class j to the shared covariance matrix, denoted as $\\Delta_{j}$, is based on the deviations of its tag embeddings from the mean (Park et al., 2018):\n$\\Delta_{j} = \\frac{1}{R} \\sum_{r=1}^{R} (z_{r,j} - \\mu_{j}) (z_{r,j} - \\mu_{j})^{T}$\nThe overall shared covariance matrix is updated after each new class is processed:\n$\\Sigma_{j} = \\frac{(j - 1)\\Sigma_{j-1} + \\Delta_{j}}{j}$\nwhere $\\Sigma_{j}$ denotes the shared covariance matrix after processing class j, assuming that classes {1, 2, ..., j - 1} have been previously learned.\nMahalanobis Distance Scoring: For each test instance, the ECL uses the Mahalanobis distance (De Maesschalck et al., 2000) to select the top k most similar classes. Let $\\{z_{i}\\}_{i=1}^{m}$ be the set of tag embeddings generated for the input instance x. The Mahalanobis distance between an embedding zi and the Gaussian distribution for class j is computed as:\n$d(z_{i}, \\mu_{j}, \\Sigma) = \\sqrt{(z_{i} - \\mu_{j})^{T} \\Sigma^{-1}(z_{i} - \\mu_{j})}$\nHere, $\\Sigma$ represents the shared covariance matrix updated up to the current point when the inference is performed. The overall distance of the test instance x from the class is the average Mahalanobis distance over all tag embeddings:\n$d(x, \\mu_{j}, \\Sigma) = \\frac{1}{m} \\sum_{i=1}^{m} d(z_{i}, \\mu_{j}, \\Sigma)$\nThe top k classes with the smallest Mahalanobis distances are selected for the final prediction step."}, {"title": "4.3 In-context Learning with Class Summaries", "content": "Once the top k candidate classes are identified by the ECL, in-context learning is applied using class summaries to determine the final prediction. Storing and using too many training examples for in-context learning would be impractical and inefficient for continual learning. Instead, for each class, we generate a summary at the time of its introduction, serving as a compact representation of the class.\nGenerating Class Summaries: The summary for each class is generated by prompting the LLM using a small subset of randomly selected training examples of the class. The prompt used for generating the summary is given in Appendix A.1. Each summary captures the essential characteristics or information of the class, allowing for efficient in-context learning without storing lots of examples.\nPrediction with In-context Learning: During prediction, the test instance is concatenated with the summaries of the top k classes to form a single prompt. The LLM processes this prompt and predicts the class label based on the context provided. The prompt format for this prediction stage is detailed in Appendix A.3.\nTo summarize, InCA stores only a mean embedding vector for each class and a shared covariance matrix for all classes encountered so far. Thus, the amount of information saved in the CIL process is very small. The whole process involves no training and it is replay-free, i.e., no previous task data is stored to help deal with CF. Moreover, as explained earlier, it avoids both the CF and ICS problems that have plagued the existing CIL techniques."}, {"title": "5 Experiment Setup", "content": "In this section, we describe the datasets, baselines, implementation details, and evaluation metrics.\nDatasets: We utilize four datasets for our experiments: CLINC (Larson et al., 2019), Banking (Casanueva et al., 2020), HWU (Liu et al., 2021b), and DBpedia (Auer et al., 2007). The intent classification datasets \u2013 CLINC, Banking, and HWU comprise of 150, 77, and 64 classes, respectively. DBpedia is a topic classification dataset with 70 classes. For the train/test splits, we allocate 10k/750 samples for CLINC, 10k/1k samples for Banking, 9k/1k samples for HWU, and 10k/1k samples for DBpedia. InCA can incrementally learn each class one by one. For baselines, we adhere to the standard CIL protocol by splitting the classes into disjoint tasks, each composed of a subset of classes. Multiple runs with different task splits are conducted, and the accuracy values are averaged to minimize"}, {"title": "6 Main Results", "content": "Surpassing Traditional CIL: The proposed InCA demonstrates a clear advantage over traditional CIL methods involving training, as seen in Table 1. It significantly outperforms all the baselines across all datasets. Despite employing a range of strategies such as regularization, parameter freezing, and pseudo-replay, none of the baseline methods achieved comparable performance.\nAs outlined in Section 1, CF occurs when updating model parameters for a new task disrupts the knowledge acquired from previously learned tasks. Our proposed system, InCA, avoids CF as it operates without any training. Although InCA's performance remains below the upper bound achieved by JOINT fine-tuning, this is mainly due to the limitations of in-context learning, which may not match the task-specific optimization of fine-tuning.\nInCA vs. Long-context Setting: The limited context window of LLMs becomes a significant challenge as the number of classes increases. InCA addresses this by using an external continual learner to identify the most relevant classes and construct a precise prediction prompt. In contrast, extending the context window of the LLM to include more information is an alternative approach. To assess the effectiveness of our approach, we compared InCA against the long-context setting of LLMs, where all class summaries are passed directly into the prediction prompt.\nWe conducted experiments using several long-context models, including Mistral, Llama3, and Gemini 1.5 flash, both with and without the assistance of the ECL. Additionally, we tested LongLlama and LongAlpaca, which adapt standard LLMs to handle long-context tasks. Since these models are not instruction-tuned, they are not suitable for generating the tags required by InCA. Consequently, we used these models only in the long-context setting, utilizing class summaries generated by Mistral, as their own summaries may not match the quality of instruction-tuned models.\nThe results, shown in Table 2, reveal that the long-context models without the ECL performed significantly worse than InCA. Even with Gemini, which has a 2M token context window, performance"}, {"title": "7 Ablation Results", "content": "Tag-based ECL vs. Text Retrieval: Given the resemblance between our approach and retrieval-augmented generation, we benchmark our tag-based classifier against a retrieval method based on text similarity. We adopt a common RAG framework, maintaining a pool of training instances for each class and using SBERT to retrieve the most similar instances during inference based on text similarity. It is important to note that this method is not applicable to CL since it involves storing original instances; we use it solely for comparison. We retrieve instances until we have obtained instances from k distinct classes, after which we measure the recall and compare these results to our ECL, which operates without a buffer (i.e., no replay data).\nThe results illustrated in Figure 2 demonstrate that our tag-based ECL consistently outperforms text retrieval across all scenarios, even when the text retriever has access to a substantial buffer of training instances. The same SBERT model is used for embedding both tags and input text. The superior performance of the ECL is particularly noteworthy as it does not require storing any instances, highlighting its effectiveness, especially in contexts where storing training instances is impractical.\nIn-context Learning Boosts Accuracy: The effectiveness of the in-context learning and the InCA approach becomes evident when comparing the model's final accuracy with the top-1 accuracy of the ECL alone. As shown in Figure 3, the final accuracy of InCA is significantly higher than the ECL's top-1 accuracy. This demonstrates the added value of in-context learning. Although the ECL alone may not be highly accurate, it effectively narrows down the relevant classes, enabling in-context learning to focus on a smaller subset and improve the overall performance.\nImpact of Data Size: To assess the impact of training data size on model performance, we conducted experiments under constrained data conditions. As shown in Figure 4, we compared InCA with the JOINT fine-tuning method across varying data sizes. InCA consistently maintains performance comparable to that of the full dataset, even with significantly reduced training data (e.g., 10 examples per class). Remarkably, under these limited data conditions, InCA outperforms the JOINT fine-tuning method, often considered the upper bound. These results highlight InCA's robustness and effectiveness under limited data availability."}, {"title": "8 Conclusion", "content": "Existing continual learning (CL) research in NLP has primarily focused on fine-tuning or adapting LLMs for individual tasks, either by learning trainable prompts or adapters or updating the LLM's parameters. While these approaches can improve CL accuracy, their effectiveness remains limited due to catastrophic forgetting (CF). On the other hand, in-context learning with LLMs has proven highly effective across various NLP tasks. However, its application to CL is hindered by the limited context window of LLMs. As the number of tasks increases, the in-context prompt grows, often exceeding the token limit or leading to performance degradation due to overextended context, which may include irrelevant information. This paper proposed a novel method to address these challenges by leveraging an external continual learner. Our method is replay-free and does not fine-tune or adapt the LLM, treating it solely as a black box. Experiments show that our method markedly outperforms baselines, without suffering from CF."}, {"title": "9 Limitations and Future Work", "content": "One limitation of this work is that experiments are conducted exclusively on text classification datasets. This focus may limit the generalizability of our model to other types of NLP tasks (e.g., dialogue generation, summarization, translation, sentiment analysis, etc), which have different data characteristics and task requirements and are not suitable for class-incremental learning because they are not classification tasks with many classes that may be learned incrementally. Although sentiment analysis is often solved as a classification task, it has a fixed number of classes, i.e., positive, negative, and neutral. These other tasks are more suitable for task-incremental learning or domain-incremental learning (Ke and Liu, 2022). We believe some variations of the proposed method should apply to the other NLP tasks. Designing one general method that is suitable for multiple different NLP tasks will be an interesting future research direction."}, {"title": "A Prompts and Examples", "content": "In this section, we detail the prompts used in various parts of our model. The provided prompts and examples are from the CLINC dataset. For different datasets, we use slightly modified versions of the same prompts based on the task at hand (e.g., intent classification, topic classification)."}, {"title": "A.1 Summarization Prompt", "content": "Prompt\nprompt = (\nReview the following user queries and provide a summary of the intent. Keep\nthe summary generic and avoid referencing any named entities that appear in\nthe queries.\nQueries: {user_queries}\nSummary:\n)\nExample\nReview the following user queries and provide a summary of the intent. Keep the summary generic\nand avoid referencing any named entities that appear in the queries.\nQueries:\n\u2022 how would i apply for an american express\n\u2022 show me the application for a visa card\n\u2022 can you apply for idaho independent card for me\n\u2022 do you have any information on how to open a new credit card\n\u2022 is this where i apply for a new credit card\n\u2022 is it possible to apply for a new credit card\n\u2022 walk me through applying for a visa card\n\u2022 what's the procedure to apply for a new credit card\n\u2022 i want to put in an application for a line of a credit card how do i do that\n\u2022 does bank of america give credit cards to people like me\nSummary:\nThe intent of these queries is to apply for a new credit card. Users are looking for information on the\napplication process, requirements, and specific credit card types. Some users are interested in applying\nfor a specific credit card, such as an American Express or a Visa card, while others are looking for\ngeneral information on how to open a new credit card."}, {"title": "A.2 Tag Generation Prompt", "content": "Prompt\nprompt = (\nGenerate descriptive tags for the following queries. Focus on user\nintention, relevant entities, and keywords. Extend these tags to related,\nunmentioned terms that are contextually relevant.\nGuidelines:\nTopic: Identify user intention or subject area the query pertains to.\nEntity Recognition: Focus on recognizable entities common in similar\nqueries.\nKeywords: Extract specific terms or verbs that define the query's intent.\nRelated Tags: Include tags that are related to user intention, even if not\ndirectly mentioned, to provide broader contextual understanding.\nExamples: {example_section}\nQuery: {user_query}\nTags:\n)\nExample\nGenerate descriptive tags for the following queries. Focus on user intention, relevant entities, and\nkeywords. Extend these tags to related, unmentioned terms that are contextually relevant.\nGuidelines:\nTopic: Identify user intention or subject area the query pertains to.\nEntity Recognition: Focus on recognizable entities common in similar queries.\nKeywords: Extract specific terms or verbs that define the query's intent.\nRelated Tags: Include tags that are related to user intention, even if not directly mentioned, to provide\nbroader contextual understanding.\nExamples:\nQuery: \"Should I wear a coat today?\"\nTags: weather advice, inquiry, clothing, temperature, coat, wear\nQuery: \"Book a table for two at a popular Italian restaurant downtown?\"\nTags: dining reservation, Italian cuisine, booking, restaurant, table, request\nQuery: \"How can I send money to a foreign bank account using the app?\"\nTags: international money transfer, send money, app, foreign bank, digital transfer\nQuery: do i have to pay for carry-ons on delta\nTags: airline fees, carry-on, delta airlines, travel, pay, luggage"}, {"title": "A.3 Prediction Prompt", "content": "Prompt\nprompt = (\nBased on the given query, classify the user's intent into one of the\nfollowing categories: {retrieved_classes}\n{class_summaries}\nQuery: {user_query}\nClass:\n)\nExample\nBased on the given query, classify the user's intent into one of the following categories: direct_deposit,\nincome, payday\ndirect_deposit:\nThe users are inquiring about the process of setting up a Direct Deposit for their paychecks or bank\naccounts. They want to know how to arrange for their checks to deposit directly into their accounts\nand are looking for instructions or guidance on how to do this. Some users are specifically interested\nin setting up Direct Deposit at certain banks, while others are seeking general information on how\nDirect Deposit works.\nincome:\nThe users are inquiring about their current or past income, salary, or earnings from their job. They\nwant to know how much money they make or earned, and sometimes they want to calculate their\ntotal income. Some users are also interested in knowing the amount they bring in annually or their\ncompensation.\npayday:\nThe users are inquiring about the timing of their next paycheck or payment. They want to know\nhow often they are paid, when they can expect to be paid next, and when their next payment will be\ndeposited. They are also interested in knowing the date or day on which they will receive their next\ncheck or be paid. Some users want to be informed about the date their most recent payment was made,\nwhile others want to plan for their next upcoming payment.\nQuery: get my paycheck to direct deposit\nClass: direct_deposit"}]}