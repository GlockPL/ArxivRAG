{"title": "Agents in Software Engineering: Survey, Landscape, and Vision", "authors": ["Yanxian Huang", "Wanjun Zhong", "Ensheng Shi", "Min Yang", "Jiachi Chen", "Hui Li", "Yuchi Ma", "Qianxiang Wang", "Zibin Zheng", "Yanlin Wang"], "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in various downstream tasks, especially in the tasks of the software engineering (SE) field. We find that many studies combining LLMs with SE have employed the concept of agents either explicitly or implicitly. However, there is a lack of an in-depth survey to sort out the development context of existing works, analyze how existing works combine the LLM-based agent technologies to optimize various tasks, and clarify the framework of LLM-based agents in SE. In this paper, we conduct the first survey of the studies on combining LLM-based agents with SE and present a framework of LLM-based agents in SE which includes three key modules: perception, memory, and action. We also summarize the current challenges in combining the two fields and propose future opportunities in response to existing challenges. We maintain a GitHub repository of the related papers at: https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.", "sections": [{"title": "Introduction", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in many downstream tasks, especially in various tasks in the field of software engineering (SE) (Zheng et al.), such as code summarization (Ahmed et al., 2024; Sun et al., 2023b; Haldar and Hockenmaier, 2024; Mao et al., 2024; Guo et al., 2023; Wang et al., 2021), code generation (Jiang et al., 2023a; Hu et al., 2024b; Yang et al., 2023a; Tian and Chen, 2023; Li et al., 2023e; Wang et al., 2024b), code translation (Pan et al., 2024; Pan et al.), vulnerability detection and repair (Zhou et al., 2024; Islam and Najafirad, 2024; de Fitero-Dominguez et al., 2024; Le et al., 2024; Liu et al., 2024b; Chen et al., 2023a), etc. Many studies combining LLMs with SE have employed the concept of agents from the artificial intelligence field, either explicitly or implicitly. Explicit use indicates that the paper directly mentions the use of agent-related technologies, whereas implicit use suggests that while the concept of intelligent agents is utilized, it may be described using different terminology or presented in alternative forms.\nAn agent (Wang et al., 2024c) represents an intelligent entity capable of perceiving, reasoning, and taking action. It perceives the environment's state and selects actions based on its goals and design to maximize specific performance metrics, serving as a crucial technical foundation for accomplishing diverse tasks and objectives. LLM-based agents generally use LLMs as the cognitive core of the agent and perform well in scenarios such as automation, intelligent control, and human-computer interaction, leveraging the powerful capabilities of LLMs in language understanding and generation, learning and reasoning, context perception and memory, and multimodality, etc. With the development of various fields, the concept of traditional and LLMs-based agents has gradually become clear and widely used in the field of Natural Language Processing (NLP) (Xi et al., 2023). However, although existing works either explicitly or implicitly use this concept in SE, there is still no clear definition of agents. There is a lack of an in-depth survey to analyze how existing works combine the agent technologies to optimize various tasks, sort out the development context of existing works, and clarify the framework of agents in SE.\nIn this paper, we conduct an in-depth analysis of the work on combining LLM-based agents with SE, summarize the current challenges in combining the two fields, and propose possible opportunities for future research in response to existing challenges. Specifically, we first collect papers on the application of LLM-based agent technologies in SE."}, {"title": "LLM-based Agents in SE", "content": "We present a framework of the LLM-based agents in SE after sorting out and analyzing the studies obtained during data collection. As shown in Figure 2, a single agent contains three key modules: perception, memory, and action. Specifically, the perception module receives external environment information of various modalities and converts it into an input form that the LLM can understand and process. The action module includes internal and external actions, which are responsible for making reasoning decisions based on the input of LLM and refining the decisions based on the feedback obtained from interacting with the external environment, respectively. The memory module includes semantic, episodic, and procedural memory, which can provide additional useful information to help LLM make reasoning decisions. At the same time, the action module can also update different memories in the memory module by learning actions, providing more effective memory information for reasoning and retrieval actions. Furthermore, multi-agent collaboration consists of multiple single agents, who are responsible for part of the task and complete the task together through collaborative cooperation. In this section, we will introduce the details of each module in the framework of the LLM-based agents in SE."}, {"title": "Perception", "content": "The perception module connects the LLM-based agent to the external environment and is the core of processing external input. It can process inputs of different modalities such as textual, visual, and auditory input, and convert them into an embedding format that LLM-based agents can understand and process, laying the foundation for reasoning and decision-making actions of LLM-based agents.\nNext, we will introduce the details of different modal inputs in the perception module."}, {"title": "Textual Input", "content": "Different from the textual input format in NLP, considering the characteristics of code, the textual input format in the SE includes token-based, tree/graph-based, and hybrid-based input.\nToken-based Input. Token-based input (Ahmed et al., 2024; Al-Kaswan et al., 2023; Arakelyan"}, {"title": "Memory", "content": "The memory modules include semantic, episodic, and procedural memory, which can provide additional useful information to help LLM make reasoning decisions. Next, we will introduce the details of these three types of memory respectively."}, {"title": "Semantic Memory", "content": "Semantic memory stores acknowledged world knowledge of LLM-based agents, usually in the form of external knowledge retrieval bases which include documents, libraries, APIs, or other knowledge. There have been many works (Wang et al., 2024b; Zhang et al., 2024; Eghbali and Pradel, 2024; Patel et al., 2023; Zhou et al., 2022; Ren et al., 2023; Zhang et al., 2023d) exploring semantic memory. Specifically, documents and APIs are the most common information in external knowledge bases. For example, Zhou et al. (2022) introduce a novel natural-language-to-code generation approach named DocPrompting, which explicitly leverages documentation by retrieving the relevant documentation pieces based on an NL intent. Zhang et al. (2024) constructs a manually curated benchmark for repo-level code generation named CODEAGENTBENCH, which contains documentation, code dependency, and runtime environment information. Ren et al. (2023) propose KPC, a novel Knowledge-driven Prompt Chaining-based code generation approach, which utilizes fine-grained exception-handling knowledge extracted from API documentation to assist LLMS in code generation. In addition to documents, APIs are also common information in external knowledge bases. For example, Eghbali and Pradel (2024) propose De-Hallucinator, an LLM-based code completion technique, which automatically identifies project-specific API references related to code prefixes and the model's initial predictions, and adds these referenced information to the prompt. Zhang et al. (2023d) integrate API search tools into the generation process, allowing the model to select an API automatically using the search tool to get suggestions. In addition, some works also involve other information. For example, Patel et al. (2023) examines the capabilities and limitations of different LLMs in generating code based on libraries defined in context. Wang et al. (2024b) uses augmented functions, along with their corresponding docstrings, to fine-tune a selected code LLM."}, {"title": "Episodic Memory", "content": "Episodic memory records content related to the current case and experience information from previous decision-making processes. Content related to the current case (such as relevant information found in the search database, samples provided by In-context learning (ICL) technology, etc.) can pro-"}, {"title": "Procedural Memory", "content": "The procedural memory of Agents in software engineering contains the implicit knowledge stored in the LLM weights and the explicit knowledge written in the agent's code.\nImplicit knowledge is stored in the LLM param-eters. Existing work usually proposes new LLMS with rich implicit knowledge to complete various"}, {"title": "Action", "content": "The action module includes two types: internal and external actions. The external actions interact with the external environment to obtain feedback information, including Dialogue with humans/agents and interaction with the digital environment, while the internal actions reason and make decisions based on the input of the LLM and refine the decision based on the obtained feedback, including reasoning, retrieval, and learning actions. Next, we will introduce each action in detail."}, {"title": "Internal Action", "content": "Internal actions include reasoning, retrieval, and learning actions. Separately, reasoning actions are responsible for analyzing problems, reasoning, and making decisions based on the input of the LLM-based agent. Retrieval actions can retrieve relevant information from the knowledge base to assist reasoning actions in making correct decisions. Learning actions are continuously learning and updating knowledge by learning and updating semantic, procedural, and episodic memories, thereby improving the quality and efficiency of reasoning and decision-making.\nReasoning Action. A rigorous reasoning pro-cess is the key to completing tasks by LLM-based"}, {"title": "Retrieval Action", "content": "The retrieval action can re-trieve relevant information from the knowledge base to assist the reasoning action in making correct decisions. The input used for retrieval and the output content obtained by retrieval have different types. As shown in Table 1, the input and output can be text, code, or hybrid information containing both text and code. Specifically, it can be divided into the following types: (1) Text-Code. Typically requirements are treated as input to retrieve related code or used APIs which are added to the prompt to generate responding code. For example, Zan et al. (2022a) propose a novel framework with APIRetriever and APICoder mod-ules. Specifically, the APIRetriever retrieves use-ful APIs, and then the APICoder generates code"}, {"title": "Learning Action", "content": "Learning actions are continuously learning and updating knowledge by learning and updating semantic and procedural memories, thereby improving the quality and efficiency of reasoning and decision-making. (1) Updating Semantic Memory with Knowledge. Semantic memory mainly exists in the knowledge base that stores basic world knowledge and can be updated by updating the knowledge base using recognized code knowledge or constructing a new one. For ex-"}, {"title": "External Action", "content": "Dialogue with Human/Agents Agents can interact with humans or other agents, and get rich information in the interaction process as feedback, expanding the knowledge of the agent and refining the answers of LLM more corrector. Specifically, many works use LLMs as agents to interact (Lu et al., 2024; Jain et al., 2023; Paul et al., 2023; Shojaee et al., 2023; Liu et al., 2023b; Wang et al., 2023e; Mu et al., 2023; Madaan et al., 2023). Jain et al. (2023) propose RLCF that uses feedback from a different LLM that compares the generated code to a reference code to further train a pre-trained LLM via reinforcement learning. REFINER (Paul et al., 2023) is a framework that can interact with a critic model that provides automated feedback on the reasoning. Yang et al. (2023b) study and elucidate how LLMs benefit from discriminative models. Moon et al. (2023) construct a new dataset specifically designed for code fixing with feedback and then use this dataset to gain a model that can automatically generate helpful feedback for code editing via Preference-Optimized Tuning and Selection. PPOCoder (Shojaee et al., 2023) consists of two parts, critic and actor, and will be optimized with PPO through the interaction between these two models. RLTF (Liu et al., 2023b) interacts with other models that utilize ground truth data"}, {"title": "Challenges and Opportunities", "content": "Upon analyzing the work related to LLM-based agents in software engineering, it is evident that there are still many challenges in the current integration of these two fields, which limits the development of both. In this section, we will discuss in detail the challenges faced by current LLM-based agents in SE and discuss some opportunities for future work based on the analysis of existing challenges."}, {"title": "Lack of Exploration of Perception Module", "content": "As mentioned in Section 2.1, there is a lack of work exploring the perception module of LLM-based agents in SE. Unlike natural language, code is a special representation that can be treated as ordinary text or converted into an intermediate representa-tion with code characteristics, such as AST, CFG, and so on. Existing works (Ahmed et al., 2024; Al-Kaswan et al., 2023; Arakelyan et al., 2023; Beurer-"}, {"title": "Role-playing Abilities", "content": "LLM-based agents are often required to play different roles across a range of tasks, necessitating specific skills for each role. For example, an agent may function as a code generator when tasked with generating code, and as a code tester when tasked with code testing. Furthermore, in certain scenarios, these agents may need to have multiple capa-bilities at the same time. For example, in the code generation scenario, the agent needs to play the role of both a code generator and tester and accordingly needs to have the ability to both generate and test code (Huang et al., 2023b). In the software engineering field, there are various niche tasks for which LLM learning is not enough and complex tasks that require agents to have multiple capabilities, such as test generation scenario, front-end development, repository-level issue resolution, etc. Therefore, advancing research on how to enable agents to effectively adopt new roles and manage the demands of multi-role performance represents a promising direction for future work."}, {"title": "Lack of Knowledge Retrieval Base", "content": "The external knowledge retrieval base is an important part of the semantic memory in the agent memory module and one of the important external tools that the agent can interact with. In NLP fields, there are knowledge bases such as Wikipedia as external retrieval bases (Zhao et al., 2023). However, in the SE field, there is currently no authoritative and recognized knowledge base that contains rich code-related knowledge, such as the basic syntax of various programming languages, various commonly used algorithms, knowledge related to data structures and operating systems, etc. In future research, efforts could be directed towards developing a comprehensive code knowledge base to serve as an external retrieval base for the agent. This knowledge base would enrich the available information, thereby enhancing both the quality and efficiency of reasoning and decision-making processes."}, {"title": "Hallucinations of LLM-based Agents", "content": "Many studies related to LLM-based agents consider LLMs as the cognitive core of the agents, with the agents' overall performance being closely tied to the capabilities of the underlying LLMs. Existing research (Pan et al., 2024; Liu et al., 2024a) has shown that LLM-based agents may produce hallucinations, such as generating non-existent APIs when completing tasks in the SE field. Mitigating these hallucinations can improve the overall performance of the agents. At the same time, the optimization of the agents can also reversely alleviate the hallucinations of the LLM-based agents, highlighting the bidirectional relationship between agent performance and hallucination mitigation. Although some efforts have been made to investigate the hallucinations of LLMs, significant challenges remain in addressing the hallucination issue within LLM-based agents. Exploring what types of hallucinations exist in LLM-based agents, deeply analyzing the causes of these hallucinations, and proposing effective hallucination mitigation methods are great opportunities for future research."}, {"title": "Efficiency of Multi-agent Collaboration", "content": "In the process of multi-agent collaboration, each individual agent needs to play different roles to accomplish specific tasks, and the outcomes of each agent's decisions are then combined to collectively tackle more complex objectives (Chen et al., 2023b; Hong et al., 2023a; Huang et al., 2023b; Wang et al., 2023a). However, this process often requires a large amount of computing resources for each agent, resulting in resource waste and reduced efficiency. In addition, each single agent needs to synchronize and share various types of information, which introduces additional communication costs and affects the real-time and response speed of collaboration. Effectively managing and allocating computing resources, minimizing inter-agent communication costs, and reducing the reasoning overhead of individual agents represent key challenges for enhancing the efficiency of multi-agent collaboration. Addressing these issues presents a significant opportunity for future research."}, {"title": "SE Technologies in LLM-based Agents", "content": "Techniques from the software engineering domain, particularly those to code, have the potential to significantly advance the development of the agent field, indicating a mutually beneficial relationship between these two domains. For example, software testing techniques can be adapted to identify abnormal behaviors and potential defects in LLM-based agents. Additionally, improvements in software tools, such as APIs and libraries, can also boost the performance of LLM-based agents especially for those with tool using abilities. Furthermore, software package management techniques can be adapted for effectively managing agent systems. For example, version control can be applied to monitor and coordinate updates across different agents in an agent system, enhancing compatibility and system integrity.\nHowever, research in this line remains limited. Therefore, exploring the incorporation of more sophisticated SE techniques into agent systems represents a promising area for future research, with the potential to drive advancements in both fields."}, {"title": "Conclusion", "content": "To conduct an in-depth analysis of the work on combining LLM-based agents with SE, we first collect many studies that combine LLM-based agents with tasks in the software engineering field. Then, we present a framework of LLM-based agents in software engineering which contains three key modules: perception, memory, and actions, after sorting out and analyzing the studies obtained during data collection. Finally, we introduce the details of each module in the framework, analyze the current challenges for LLM-based agents in the SE field, and point out some opportunities for future work."}]}