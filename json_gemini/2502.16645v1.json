{"title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale", "authors": ["Chenlong Wang", "Zhaoyang Chu", "Zhengxiang Cheng", "Xuyi Yang", "Kaiyue Qiu", "Yao Wan", "Zhou Zhao", "Xuanhua Shi", "Dongping Chen"], "abstract": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs), exemplified by DeepSeek-R1 (Guo et al., 2025), CodeLlama (Roziere et al., 2023), and GPT-40 (OpenAI, 2024), have demonstrated remarkable performance in automating software development through generating executable code (Jiang et al., 2024). However, due to static pre-training datasets, they often struggle to adapt to the rapidly evolving knowledge in programming, especially the frequent updates of external library APIs (Tao et al., 2012; Zhang et al., 2020).\nAs illustrated in Figure 1, when prompted to create an array on a CUDA device, the LLM is unaware of the removal of the device parameter in the updated numpy.full function. This oversight results in an error, i.e., \u201cTypeError: full() got an unexpected keyword argument 'device'\". The pitfalls of generating code containing outdated APIs can lead to parameter compatibility issues, which causes programs to crash or malfunction, undermining the reliability and stability of software (Bai et al., 2024; Zhang et al., 2024c). This challenge highlights the need for LLMs to synchronize with the dynamic evolution of practical code knowledge, particularly the fast-paced API updates that have immediate and visible impacts on software development.\""}, {"title": "2. CODESYNC: A Data Engine for Real-Time Code Knowledge Collection", "content": "As illustrated in Figure 3, we propose CODESYNC, a data engine for real-time collection of code knowledge evolution, which operates through three key steps: (1) Real-Time API Update Tracking. CODESYNC identifies and extracts API updates across diverse Python third-party libraries by systematically tracking changes to API signatures between library versions (see Section 2.1). (2) Real-World API Invocation Retrieval. For each identified API with updates, CODESYNC retrieves relevant code instances invoking the API from GitHub repositories through GitHub Code Search (GitHub) (see Section 2.2). (3) Legacy-Updated API Invocation Synthesis. Building on the retrieved real-world API invocations, CODESYNC employs DeepSeek-V3 (Liu et al., 2024a) to synthesize contrastive code instances that invoke legacy and updated APIs, respectively (see Section 2.3). Based on CODESYNC, we establish CODESYNCBENCH, a benchmark for assessing real-time code knowledge of LLMs, which collects updates for 220 APIs (including 130 functions, 59 initializers, and 31 methods) from 6 Python libraries, totaling 3,300 legacy-updated pairs of API invocation instances (see Section 2.4)."}, {"title": "2.1. Step 1: Real-Time API Update Tracking", "content": "The functionality of APIs is exposed through their signatures, which provide an interface for developers to utilize this functionality within code. This feature enables systematic tracking of library API updates by monitoring changes in their signatures.\nExtracting API Signatures. We target 6 widely used Python third-party libraries: pandas, numpy, scipy, tensorflow, torch and flask. To collect complete API signatures from these libraries, we leverage Python's built-in inspect module, a dynamic reflection tool provided by the Python standard library (Python, b). This tool enables runtime analysis and collection of information about Python objects, including modules, classes, functions, and methods. For each library, we extract API signatures by performing inspections within virtual environments configured with specific library versions."}, {"title": "Identifying API Updates.", "content": "To evaluate LLMs' ability to synchronize with real-time API evolution, we consider the most recent library version before ChatGPT's release (OpenAI, 2023) as the legacy version and the current library version as the updated version. Then, we identify API updates by systematically comparing API signatures between versions. To determine whether an update exists for a given API, we perform a static analysis to establish parameter mappings for same-name APIs across versions. These mappings allow us to analyze API changes at the parameter level by examining differences in attributes such as parameter name, position, and type. Using this approach, we identify 6,063 API updates from the six targeted Python libraries, as summarized in Table 1."}, {"title": "2.2. Step 2: Real-World API Invocation Retrieval", "content": "While API updates are reflected in signature changes, collecting this information alone is insufficient to fully capture the evolution of code knowledge. To address this, we consider real-world API invocation scenarios, focusing on modifications in API usage within actual code contexts. For each API update identified in Section 2.1, we retrieve and filter relevant code instances that invoke the API from GitHub repositories."}, {"title": "Retrieving Relevant Code Instances.", "content": "We use GitHub Code Search (GitHub) to retrieve Python files that potentially contain API invocations by designing multiple matching templates. For example, to retrieve code invoking the function torch.nn.Linear, we match the API name (e.g., .Linear) along with relevant import statements (e.g., import torch.nn as nn and from torch import nn)."}, {"title": "Locating Valid API Invocations.", "content": "Code instances retrieved via matching templates may only potentially invoke the target APIs, requiring precise localization to confirm valid invocations. To achieve this, we parse each code instance into an Abstract Syntax Tree (AST) using Python's built-in ast module (Python, a) and traverse all statements to identify those that genuinely contain targeted invocations. Moreover, we perform alias resolution on import statements to establish mappings between full module names (e.g., numpy) and their aliases (e.g., np), ensuring more accurate identification of valid API invocations. For example, we locate statements that contain np.full for the full function and nn.Linear for the Linear class initializer. Furthermore, regarding method invocation locating, the ast module enables us to track objects whose types match the target class by examining class instantiations and assignments. For example, in the case of x. reshape(), we identify that x is of type torch.Tensor, confirming a valid invocation of the reshape() method from the torch.Tensor class."}, {"title": "2.3. Step 3: Legacy-Updated API Invocation Synthesis", "content": "While real-world code instances with valid API invocations can be retrieved from GitHub repositories, it is challenging to determine the exact library version of the invoked API. To address this, we synthesize the contrastive API invocation pairs-legacy and updated using state-of-the-art LLMs, which have demonstrated strong capabilities in revising code while preserving both semantic and syntactic correctness (Guo et al., 2024b).\nSpecifically, for each API invocation instance retrieved in Section 2.2, we prompt DeepSeek-V3 (Liu et al., 2024a) to adapt the target API invocation statement according to the legacy and updated API signatures, respectively, while preserving the integrity of the surrounding context. To ensure data quality, the authors manually verify the divergence between legacy and updated versions, instructing the LLM to re-synthesize cases with insufficient divergence. This approach ensures divergence in API usage while maintaining functional equivalence between legacy and updated implementations, enabling explicit modeling of API evolution. Through this process, we synthesize 3,300 legacy-updated API invocation pairs from 3,300 real-world code instances."}, {"title": "2.4. CODESYNCBENCH: A Benchmark for Real-Time Code Knowledge Assessment", "content": "Based on CODESYNC, we develop CODESYNCBENCH, \u0430 real-time benchmark for assessing how effectively LLMs adapt to evolving code knowledge, which comprises three evaluation tasks, including Code Completion Task (CCT), Error Correction Task (ECT), and Multiple Choice Question (MCQ), as shown in Figure 4. CODESYNCBENCH covers updates for 220 APIs across 6 Python libraries, including 130 functions, 59 initializers, and 31 methods. Each API is associated with 15 legacy-updated invocation pairs (3,300 in total), with 5 pairs for evaluation (1,100 in total) and 10 for training (2,200 in total). Based on this, our benchmark builds 1,100 tests per evaluation task, accompanied by a training set comprising 2,200 update-aware instructions, providing a rigorous foundation for assessing LLMs' ability to stay synchronized with API evolution."}, {"title": "Code Completion Task (CCT)", "content": "This task evaluates whether LLMs have internalized the updated APIs and can recall them during code generation. Given a code snippet ending with an API name, the LLM is prompted to complete the parameter list, with the updated API invocation statement serving as the ground truth. To measure the accuracy of API invocation completion, we employ three widely used metrics: BLEU (Papineni et al., 2002) for evaluating lexical precision, ROUGE-L (Lin, 2004) for measuring semantic coverage, and Relative Edit Distance (RED) (Ristad & Yianilos, 1998) for quantifying structural deviation."}, {"title": "Error Correction Task (ECT)", "content": "This task simulates real-world debugging scenarios, where an interpreter throws an exception related to a specific API invocation. Unlike passive knowledge recall, it evaluates the LLM's ability to actively correct potential errors. Given a code snippet ending with a legacy API invocation, the LLM is prompted to rectify it to the updated version. Similar to CCT, we assess the accuracy of API invocation correction using BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and Relative Edit Distance (RED) (Ristad & Yianilos, 1998)."}, {"title": "Multiple Choice Question (MCQ)", "content": "This task evaluates the LLM's ability to discriminate between correct and incorrect API invocations, requiring a deep internalization of the updated APIs. Given four candidate API invocations, including one correct answer and three plausible distractors, the LLM is prompted to select the optimal choice. The distractors, synthesized by DeepSeek-V3 (Liu et al., 2024a), include perturbations such as adding an invalid parameter, removing a required parameter, and rearranging parameter order. We employ the Pass@k metric (Chen et al., 2021b) to measure the probability that the LLM passes a test case within k attempts, which is calculated by drawing n \u2265 k answers from the LLM for each test case and counting the number of correct answers c \u2264 n. We use n = 10 and k \u2208 {1,3,5} (abbreviated as P@1, P@3, and P@5)."}, {"title": "Training Set.", "content": "To evaluate knowledge updating methods, we build an instruction tuning dataset D = {(i, oold, Onew)}. As illustrated in Section E.1, i denotes an update-aware instruction containing a code snippet with an incomplete API invocation (e.g., \"array=numpy.full (\"). Oold and Onew are output statements that accomplish the code. Onew represents the correct invocation with the updated API, while oold reflects the legacy version. Oold and onew share the same basic functionality, differing only in the parameters affected by the API update. The paired invocations allow the LLMs to identify update-related changes by computing token-level differences between oold and onew."}, {"title": "3. Can LLMs Sync with Code Evolution?", "content": "To comprehensively assess LLMs' ability to synchronize with dynamic code evolution, we investigate the following Research Questions (RQs):\n\u2022 RQ1: Benchmarking Large Language Models. Can LLMs access real-time API updates without relying on retrieval-augmented frameworks?\n\u2022 RQ2: Benchmarking Knowledge Updating Methods. Can LLMs be effectively and efficiently updated to synchronize with API changes using knowledge updating methods without compromising model utility?\n\u2022 RQ3: Impact of API Update Settings. How do different API update settings, e.g., the numbers of API invocations available for training and the types of updated APIs, impact the performance of knowledge updating?"}, {"title": "3.1. RQ1: Benchmarking Large Language Models", "content": "We benchmark nine state-of-the-art LLMs in accessing real-time API updates without retrieval-augmented settings, including four proprietary models (i.e., GPT-40, GPT-40-mini (OpenAI, 2024), Claude-3.5-Sonnet (Anthropic, 2024) and Gemini-1.5-Pro (Team et al., 2024)) and five open-source models (i.e., DeepSeek-V3 (Liu et al., 2024a), DeepSeek-R1 (Guo et al., 2025), and Qwen2.5-14/32/72B-Instruct (Qwen Team, 2024)).\nAs shown in Table 3, the results indicate that state-of-the-art LLMs face significant challenges in coding tasks involving API updates. For example, leading commercial models like GPT-40 and Claude-3.5-Sonnet exhibit poor performance, with BLEU scores below 20% on the code completion task. Similarly, recently released models with up-to-date knowledge cutoffs, such as DeepSeek-V3 and DeepSeek-R1, which are expected to incorporate fresher code knowledge, also fail to accurately reflect API updates, yielding similarly low BLEU scores. These findings reveal systemic shortcomings in LLMs' ability to adapt to evolving APIs, highlighting the fundamental limitations of static pretraining paradigms. Thus, even the latest models suffer from knowledge decay as API versions evolve over time."}, {"title": "3.2. RQ2: Benchmarking Knowledge Updating Methods", "content": "We benchmark five knowledge updating methods including SFT, SFT-LORA (Peng et al., 2023), DPO (Rafailov et al., 2023), SimPO (Meng et al., 2024), and ORPO (Hong et al., 2024), across five open-source LLMs including three code-specific LLMs (i.e., CodeLlama-7B-Instruct (Roziere et al., 2023), Qwen2.5-Coder-7B-Instruct (Hui et al., 2024), and DeepSeek-Coder-6.7B-Instruct (Guo et al., 2024a)) and two general-purpose LLMs (i.e., Llama-3.1-8B-Instruct (Dubey et al., 2024) and Qwen2.5-7B-Instruct (Qwen Team, 2024)).\nAs illustrated in Figure 2 and Table 4, the results indicate that knowledge updating methods can improve LLMs' performance in handling API evolution across the three evaluation tasks. Notably, fine-tuned open-source LLMs with size 6.7B-8B can achieve scores comparable to or surpassing those of leading proprietary and open-source LLMs, such as Claude-3.5-Sonnet and DeepSeek-R1, with BLEU scores of 23.86%-31.59 on the CCT task. However, despite these improvements, the absolute scores remain low, indicating that current methods are insufficient for effectively updating code knowledge of LLMs and necessitate further refinement."}, {"title": "3.3. RQ3: Impact of API Updating Settings", "content": "We further investigate the impact of different API update settings such as the numbers of API invocations available for training and the types of updated APIs, on the performance of knowledge updating in API evolution tasks.\nTo evaluate this, we filter 32 APIs from the original training set, each with more than 50 invocation samples, and construct four new training sets with 5, 10, 20, and 50 samples per API, respectively. We then train Qwen-2.5-7B-Instruct using four knowledge updating techniques (i.e., SFT-LORA, DPO, ORPO, SimPO) on these sets and evaluate performance on the code completion task. As shown in Figure 6, using only 5 samples per API results in relatively poor performance. When the training sample number increases to 10 per API, the model demonstrates improved recall capabilities of the updated APIs. Further increases in sample number lead to performance stabilization with minor additional gains. These findings suggest that a moderate number of samples is sufficient for LLMs to internalize new code knowledge, with 10 samples per API striking an optimal balance between effectiveness and efficiency."}, {"title": "4. Conclusion", "content": "In this paper, we introduce CODESYNC, an innovative data engine designed to establish a structured benchmark, CODESYNCBENCH, which aims at assessing the proficiency of LLMs in handling evolving code knowledge. We evaluate the performance of state-of-the-art LLMs on CODESYNCBENCH and measure the efficacy of prevalent knowledge update techniques. Our results reveal that LLMs face significant challenges in adapting to rapid API evolutions, often struggling with knowledge decay over time. Furthermore, existing knowledge update techniques show limitations in effectively injecting updated code knowledge into models. These findings underscore the necessity for further refinement of knowledge updating methods to improve models' capability to adapt to evolving code knowledge in dynamic environments."}, {"title": "Impact Statement", "content": "In this paper, we present CODESYNC, an innovative data engine designed to systematically monitor real-world API changes and generate CODESYNCBENCH, a specialized benchmark for assessing and improving LLMs' adaptability to API updates. This benchmark establishes a standardized evaluation framework for assessing the challenges posed by outdated API knowledge in LLMs. However, one limitation of our work is the efficiency of collecting invocation instances. By enabling LLMs with real-time API adaptation capabilities, our work has the potential to significantly enhance developer productivity and drive advancements in software development, AI-driven coding assistants, and programming education."}, {"title": "B. Detailed Experiment Setups", "content": "B.1. Dataset\n\u0392.1.1. \u0391\u03a1\u0399 COLLECTION\nThe initial step of CODESYNC pipeline involves collecting APIs from various libraries. To achieve this, we utilize the Python built-in module inspect, which enables us to navigate through library files and compile a comprehensive list of all available APIs. In this part, we will delve into the detailed process of how to collect APIs comprehensively from libraries.\nC-extension APIs. C-extension methods and functions are a powerful feature in Python programming that are employed in many third-party libraries, (e.g., NumPy, PyTorch), to accelerate execution efficiency. One of the key feature of C-extension functions and methods is their support for function overloading. Function overloading allows a single API name to be used with multiple different parameter lists, or signatures. This means to collect various versions of signatures for each API.\nInspect Module. Python built-in module, Inspect, provides several useful functions for introspecting live objects, such as functions, classes, and modules. It allows us to retrieve information about source code of Python objects, such as signature, arguments and documentation.\nCategories. Python offers a diverse range of APIs, each designed for specific purposes and governed by distinct invocation rules. In this study, we focus on three primary types: function APIs, method APIs, and initializer APIs. These categories not only highlight Python's core capabilities but also exhibit unique characteristics and behaviors. Function APIs are standalone entities that can be invoked without requiring a class or instance context. In contrast, method APIs are inherently tied to class instances, leveraging encapsulation and object-oriented programming principles. The invocation rules for methods differ significantly from those for functions, reflecting their object-oriented nature. Additionally, Python provides several magic methods that are denoted by double underscore ('__') at the beginning and end of their names. Among these, initializers (i.e., '__init__') are the most commonly used, serving as a method for object creation and initialization. To evaluate and benchmark Python APIs evolution comprehensively, we select representatives from these three categories to construct our benchmark CODESYNCBENCH."}, {"title": "B.1.2. IDENTIFYING API UPDATES", "content": "Multiple Types of Parameter. The three fundamental types of parameters are positional-only parameter, keyword-only parameter and positional & keyword parameter. The term 'positional' refers to parameters that can be passed only according to its position in definition. \u2018Keyword' is the name of parameter in the function signature, allowing passing parameter with marking it explicitly instead of position. There are two special symbols in API signatures (e.g., *, /). Parameters set before '*' are positional-only parameters, which must be passed in order according to theirs positions in definition, and parameters located after '/' are keyword-only parameters, requiring marking parameter name when used; otherwise, a syntax error will occur. Additionally, parameters can be also categorized according to default values into 2 types, required parameters and optional parameters. Therefore, changes of parameter types have impact on invocation rules, which should be considered when determining API update operations.\nAPI Update Determination. How to determine API update operations? The most straightforward changes include the addition or deletion of parameters. A more nuanced level of analysis involves examining changes in parameter types as these alterations can significantly impact the rules for invoking APIs. Therefore, API updates can be categorized into 2 primary aspects, the addition of deletion of parameters and changes in parameter types. To effectively identify API updates, it is crucial to focus on parameter changes, including both the mapping relationships between parameters and modifications to their types. To systematically capture these changes, we construct parameter mappings for each pair of APIs, establishing connections between corresponding parameters in the outdated and latest version of their signatures. Specifically, parameter mapping enables categorize two distinct aspects. First, if a parameter mapping can be successfully constructed, it implies that all parameters are consistently present in both versions of signatures, indicating no additions or deletions. Following this, the next step involves a detailed examination of each parameter pair within mappings, focusing on comparing their attributes to identify any modifications or differences. This approach enables a clear and structured understanding of how APIs involve over time."}, {"title": "Establishing Parameter Mappings.", "content": "However, the inherent complexity of Python API signatures poses significant challenges in accurately establishing parameter mappings. To address this, we establish three rules that must be satisfied to determine whether no modification has occurred. Python introduces two special symbols (\u2018/' and '*'), which divide parameters into three categories, positional-only, keyword-only and positional-and-keyword parameters. Specifically, we construct three individual parameters mappings for these types of parameters and establish three rules that must be satisfied to determine whether no modification has occurred.\n\u2022 Rule 1: Successful Parameter Mapping. A valid parameter mapping must be constructed, ensuring that both the number of parameters and their corresponding keywords remain identical across different signatures.\n\u2022 Rule 2: Type-Specific Consistency. Each parameter type must follow specific rules:\nFor positional-only parameters, the order of parameters in the function definition must remain strictly unchanged across signatures.\nFor keyword-only parameters, the parameter names (keywords) must remain consistent to preserve their correspondence.\nFor positional-and-keyword parameters, both the order requirement and keyword consistency must be satisfied simultaneously.\n\u2022 Rule 3: Required vs. Optional Parameters. Parameters can be further categorized into two types: required parameters, which must be provided when invoking APIs, and optional parameters, which have default values. While revisions to default values are not considered API updates, the type of a parameter must remain unchanged.\nThese rules collectively provide a practical methodology for evaluating parameter modifications and determining API consistency, which is a crucial part of CODESYNC implementing completely autoamted pipeline."}, {"title": "B.1.3. API INVOKING INSTANCES CRAWLING", "content": "After obtaining updated APIs along with corresponding information, it is necessary to crawl API invocations from ground truth which will be used to inject API knowledge into LLM for further exploration. Actually, directly feeding signature to models for tuning is unlikely to be effective, and limited to reflect comprehensive information, such as invoking rules, which is hard to be formulated. Therefore, we collect a large dataset of invocation instances to implicitly reflecting relative knowledge.\nReal-World API Invocation. Synthesizing invocation completely relied on LLM is a convenient method for constructing dataset. However, this method exists inherent limitations. For example, information implied in context of generated code is insufficient and the contextual scenario is restricted to LLMs' embedded knowledge. The inevitable bias therefore poses challenges to comprehensively reflect authentic invoking rules and habits. Instead of synthesizing invocations, we try to crawl code from GitHub with the help of GitHub Code Search, a Code Search Engine developed by GitHub to effectively aggregate repositories or files using regular expression. Additionally, We involve search templates as shown in B.1.3, to enhance the effectiveness of invocation retrieval"}, {"title": "Search Templates.", "content": "Python allows aliases declaration of import statements to simplify usage of third-party modules and APIs. In the authentic programming scenario, directly invoking APIs with full name fails to align with developers' programming habits. We therefore design a set of templates for each library to expand searching scope. For example, while the module torch.nn.functional is imported, these statements might exist:\n1. import torch.nn.functional as F\n2. from torch.nn import functional as F\nFor any field in the API name (a segment separated by dots), an alias can be assigned and there are two formats: import as and from import. Based on these characteristics, we can generate a series of searching templates. Templates of torch.nn.functional.softmax are shown as below:\n1. \"torch.nn.functional.softmax\" (directly match)\n2. \"import torch as\u201d + \u201c.nn.functional.softmax\"\n3. \"from torch import nn\" + \".functional.softmax\"\n4. \"import torch.nn as\u201d + \u201c.nn.functional.softmax\"\n5. \"from torch.nn import functional\u201d + \u201c.softmax\"\n6. \"import torch.nn.functional as\u201d + \u201c.softmax\"\n7. \"from torch.nn.functional import softmax\"\nIn the second template, we match \"import torch as\" instead of \"import torch\". This is because when the module is imported without an alias (e.g., simply \u201cimport torch\u201d), the full path \u201ctorch.nn.functional.softmax\u201d will be directly used in the code. For function APIs and initializer APIs, the above patterns can be directly applied for decomposition. We next utilize GitHub Code Search to retrieve code that contains all segments for each template (with an upper limit of 500 files).\nDifferent from function and initializer, method APIs requires a further step due to dynamic binding mechanism. A method API can be divided into two parts: class name and method name. For example, torch.Tensor and shape are class name and method name of torch.Tensor.shape, respectively. In the most programming scenario, Python objects lack explicit type definitions. To align with subsequent procedures, we only take one specific situation into consideration where both type declaration and API invocation exist in the same file simultaneously. Searching templates can be applied on method APIs retrieval as well, while an additional segments, f \" . {method name} (\",should be included. For API torch.Tensor.shape, each template will include \".shape(\". Explicit type declarations will be clarified in Appendix B.1.4."}, {"title": "B.1.4. LOCATING VALID API INVOCATIONS", "content": "After retrieving a dataset of files that contain relative substring of target API invocation, further filtering is required to identify code that genuinely invokes the target API. The following illustration is divided into two parts: function / initializer APIs locating and method APIs locating.\nFunction / Initializer APIs Locating. Initializer APIs share similar invoking rules with those of function APIs. We can use abstract syntax tree(AST) to analyze crawled files for locating the target API invocations. Specifically, this part contains two steps: (1) Alias Mapping: We scan the import statements and construct mappings between original library/module name and aliases. (2) Invocation Analysis: Based on alias mapping, we traverse the AST of files and analyze each invocation statement to determine whether the target API are invoked. The start & end line number of invocations will be recorded for subsequent process.\nMethod APIs Locating. Invocations of method APIs are often associated with class instances. To determine method API invocations, we need to infer the types of variables that invoke the methods. However, variables are dynamically bound to types during program execution. We therefore focus on situations where the types of variables can be statically inferred from the raw code. There are three situations:\n\u2022 Variables are assigned by using initializer of target class.\n\u2022 Type annotations are provided in function definitions.\n\u2022 Function definitions provide return type annotations."}, {"title": "Format Conversion.", "content": "After locating and recording API invocations in each file, we perform two operations to split the data: (1) Segment Split: Treating the entire file as a single dataset item is inefficient and redundant. To better utilize the crawled files, we split each file into multiple segments based on function definition. In other words, each segment corresponds to a complete function definition and is treated as an individual dataset item. (2) Metadata Convert: Each segment is then further divided into three parts: code context, target sequence and code suffix. The code context is the prompt in subsequent tasks. To avoid knowledge leaking, the target sequence is the first invocation of target API within the segment. These split operations allow for more efficient processing and better representation of the code's structure, ultimately improving the dataset's usability for subsequent tasks."}, {"title": "B.2. Models", "content": "Qwen-2.5-7B-Instruct. A 7-billion parameter instruction-tuned model designed for general-purpose tasks, offering robust performance across various applications by following user instructions effectively.\nQwen-2.5-Coder-7B-Instruct. A specialized 7-billion parameter model tailored for coding-related tasks, excelling in code generation, debugging, and understanding programming languages through instruction-following capabilities.\nLlama-3-8B-Instruct. An 8-billion parameter instruction-tuned model built for versatile applications, providing strong performance in natural language understanding and task execution based on user instructions.\nCodeLlama-7B-Instruct. A 7-billion parameter model fine-tuned for coding tasks, optimized for generating, analyzing, and refining code while adhering to user-provided instructions.\nDeepSeek-Coder-6.7B-Instruct. A 6.7-billion parameter model specifically designed for coding and programming tasks, leveraging instruction-tuning to deliver accurate and efficient code-related solutions."}, {"title": "B.3. Knowledge Updating Methods", "content": "B.3.1. DIRECT PREFERENCE OPTIMIZATION (DPO)\nTraditional reinforcement learning algorithms (e.g., PPO (Schulman et al., 2017)) introduce reward models to guide LLMs to align with human preferences. While these methods exhibit superior performance in many fields, they suffer from extremely high computational costs and require a large amount of training data to optimize policy of reward models. To accelerate the process of training, DPO directly optimizes the model's policy to align with human preferences by leveraging pairwise comparison data. Each data pair consists of a preferred sample y and a dispreferred sample y for a given input x\u012f. DPO adjusts the model to increase the likelihood of generating preferred outputs while reducing the probability of dispreferred ones. By implicitly encoding preference rankings into the objective function, DPO eliminates the need for explicit reward modeling or complex reinforcement learning pipelines, offering a simpler and more stable training framework.\nThe key insight of DPO is to reframe preference learning as a supervised likelihood optimization problem. Given preference pairs (x, y, y), the objective maximizes the log-likelihood difference between preferred and dispreferred outputs:\n$\\mathcal{L}_{DPO} = \\sum_i \\log \\sigma \\left( \\beta \\left( \\log \\pi_{\\theta}(y^*|x_i) - \\log \\pi_{ref}(y^*|x_i) \\right) - \\log \\sigma \\left( \\beta \\left( \\log \\pi_{\\theta}(y|x_i) - \\log \\pi_{ref}(y|x_i) \\right) \\right),$ where o denotes the sigmoid function and ref represents the reference policy. This formulation ensures the model assigns higher probabilities to preferred responses relative to the reference policy while maintaining generation diversity through implicit regularization."}, {"title": "B.3.2. ODDS RATIO PREFERENCE OPTIMIZATION (ORPO)", "content": "ORPO introduce Odd Ratio to quantify the preference learning. Specifically, it enhances preference learning by explicitly optimizing the odds ratio between preferred and dispreferred responses. The loss function combines log-odds maximization with KL-divergence regularization:"}, {"title": "B.3.3. SIMPLE POLICY OPTIMIZATION (SIMPO)", "content": "SimPO extends the paradigm of DPO through architectural simplifications that enhance both training efficiency and alignment precision. At its core, SimPO reinterprets the alignment task as a margin maximization problem, where the model learns to maintain a specified quality gap between preferred and dispreferred responses. This is achieved through two synergistic mechanisms:\nDynamic Length Normalization: Traditional probability-based rewards inherently favor longer sequences due to multiplicative probability chains. SimPO counteracts this bias by computing rewards as length-normalized token probabilities:\n$R_{\\theta}(y|x) = \\frac{1}{|y|} \\sum_{t=1}^{|y|} \\log \\pi_{\\theta}(y_t|x, y_{<t}),$ where the normalization factor |y| (response length) ensures equal contribution per token, preventing length-based reward inflation. This design choice proves critical in tasks requiring concise yet high-quality responses, such as technical question answering or summarization.\nAdaptive Margin Enforcement: Rather than relying on fixed hyperparameters, SimPO implements an intelligent margin threshold m that interacts with the reward difference \u2206Ro = Ro(y+|x) \u2013 Ro(y\u00ae|x):\n$\\mathcal{L}_{SimPO} = \\sum_i \\max (0, m - \\Delta R_{\\theta}(x_i)).$ The margin mechanism creates three distinct learning phases:\n1. Active Learning: When \u2206Re <m, gradients actively push the model to widen the reward gap\n2. Saturation Control: Once \u2206R \u2265 m, gradient flow ceases to prevent over-optimization\n3. Implicit Regularization: The margin m automatically scales with batch statistics, adapting to varying preference strengths"}, {"title": "C. Prompts", "content": "C.1. Prompt to update code legacy\nI will provide a code snippet as the context, followed by a calling statement that contains a target API call and a suffix. Additionally, the latest and outdated function signatures of the API are accessible(referred to as latest_signature and outdated_signature). Your task is"}]}