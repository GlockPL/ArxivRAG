{"title": "Machine Learning Modeling for Multi-order Human Visual Motion Processing", "authors": ["Zitang Sun", "Yen-Ju Chen", "Yung-Hao Yang", "Yuan Li", "Shin'ya Nishida"], "abstract": "Our research aims to develop machines that learn to perceive visual motion as do humans. While recent advances in computer vision (CV) have enabled DNN-based models to accurately estimate optical flow in naturalistic images, a significant disparity remains between CV models and the biological visual system in both architecture and behavior. This disparity includes humans' ability to perceive the motion of higher-order image features (second-order motion), which many CV models fail to capture because of their reliance on the intensity conservation law. Our model architecture mimics the cortical V1-MT motion processing pathway, utilizing a trainable motion energy sensor bank and a recurrent graph network. Supervised learning employing diverse naturalistic videos allows the model to replicate psychophysical and physiological findings about first-order (luminance-based) motion perception. For second-order motion, inspired by neuroscientific findings, the model includes an additional sensing pathway with nonlinear preprocessing before motion energy sensing, implemented using a simple multilayer 3D CNN block. When exploring how the brain acquired the ability to perceive second-order motion in natural environments, in which pure second-order signals are rare, we hypothesized that second-order mechanisms were critical when estimating robust object motion amidst optical fluctuations, such as highlights on glossy surfaces. We trained our dual-pathway model on novel motion datasets with varying material properties of moving objects. We found that training to estimate object motion from non-Lambertian materials naturally endowed the model with the capacity to perceive second-order motion, as can humans. The resulting model effectively aligns with biological systems while generalizing to both first- and second-order motion phenomena in natural scenes.", "sections": [{"title": "1 Introduction", "content": "Creating machines that perceive the world as humans do poses a significant interdisciplinary challenge bridging cognitive science and engineering. From the former perspective, developing human-aligned computational models advances our understanding of brain functions and the mechanisms underlying perception [1-3]. On the latter side, such models, which accurately simulate human perception in diverse real-world scenarios, would enhance the reliability and utility of human-centered technologies."}, {"title": "2 Results", "content": "In Section 2.1, we present the processing pipeline of the dual-channel motion model. There are both local and global motion processing stages. Section 2.2 demonstrates how the model integrates motions. The tasks ranged from those of vision science experiments to complex natural scenarios. Sections 2.3 and 2.4 extend to higher-order processing, exploring the relationship between material properties and the ability of second-order motion perception. We used defined benchmarks to quantify the contributions made by first- and higher-order motion processing and compared these to those of representative machine vision models."}, {"title": "2.1 The two-stages processing model", "content": "Our prototype model features two-stage motion processing that combines classical ME sensors in Stage I with modern DNNs in Stage II. Stage I captures local ME, simulating the function of the V1 area. In contrast, Stage II focuses on global motion integration and segregation and thus simulates the primary function of the MT area. The red route in Fig. 1-A is that of classical first-order motion. Specifically, we built 256 trainable ME units, each with a quadrature 2D Gabor spatial filter and a quadrature temporal filter. These captured the spatiotemporal motion energies of input videos within a multiscale wavelet space. The key implementation difference from previous ME models is that we embedded computation in the deep learning framework, i.e., each motion energy neuron's parameters, such as preferred moving speed and direction, are trainable to fit the task. Due to the motion energy constraints, the activation patterns of the model show a tendency similar to mammalian neuron recordings in the V1 cortex, such as spatiotemporal receptive fields resembling those of V1 neurons and direction-tuning capabilities in Stage I. Moreover, incorporating ME sensors allows the model to replicate human-aligned perception of various motion illusions that are not captured by conventional CV models, such as reverse phi and missing fundamental illusions [15].\nStage II incorporates modern DNNs connected to Stage I. It constructs a fully connected graph on local motion energy, treating each spatial location as a node, with all nodes interconnected. We use a self-attention mechanism to define the topological structure of the graph, by which motions are recurrently integrated to generate interpretations of global motion and address aperture problems (Stage II, Fig. 1-A). A shared trainable decoder is used to visualize the optical flow fields from Stages I and II. The entire model is trained under supervision to estimate pixel-wise object motions in naturalistic datasets [30]. We also used a large"}, {"title": "2.2 Motion Graph-based Scene Integration", "content": "Stage II of our model emulates the motion integration process, an essential function for linking local motions and solving the aperture problem [31].\nFig. 2-A (left) displays the responses of 256 units to both drifting-gabor and plaid stimuli. A plaid is two overlapping drifting Gabors [27]. Analysis revealed three distinct groups of units based on their partial correlations with the Gabor and plaid stimuli. Component cells responded to the direction of a Gabor component. Pattern cells"}, {"title": "2.3 Material Properties and Second-order Motion Perception", "content": "Despite including a second channel that extracted higher-order features, our model could not identify second-order motion when trained only on existing motion datasets. This limitation reflects broader challenges in CV, as other DNN-based models also fail to capture second-order motion perception in conventional motion estimation tasks [8]. To test our hypothesis that the biological system evolved to perceive second-order motion for estimating object movement amidst optical noise from non-diffuse materials, we constructed datasets that controlled the properties of object materials. One dataset contained \"diffuse\" (matte) reflections and the other \"non-diffuse\" properties, including glossy, transparent, and metallic surfaces. The model was trained with a focus on higher-order motion extractors to estimate the GT of object motion while ignoring optical interferences caused by non-diffuse reflections during movement.\nTo quantify second-order motion perception, we developed a benchmark using natural images with various second-order modulations. As shown in Fig. 4-B, the benchmark included classical drift-balanced motion (temporal contrast modulation) [16]; local low contrast (spatial modulation); and natural phenomena such as water waves and swirling flow fields (spatiotemporal modulation). The latter movements are not pure second-order motion but are near-indiscernible in Fourier space, given the chaotic optical disturbances caused by reflection and refraction. Our psychophysical experiment revealed a strong correlation between the physical GT and the human response in terms of detecting second-order motion (rmean = 0.983, sd = 0.005). In contrast, a representative CV model, RAFT, was associated with a much lower correlation (r = 0.102). We trained our model on the diffuse and non-diffuse datasets and compared the correlations with human responses. The results of Fig. 5-C indicate that both the dataset"}, {"title": "2.4 The Interplay Between the First- and Higher-order Channels", "content": "Fig. 6 presents qualitative data illustrating the difference between the first- and second-order channels, demonstrating their function when processing natural scenes with noisy optical environments (First Row). Higher-order processing affords more stable results when interpreting"}, {"title": "3 Discussion", "content": "We establish the first human-aligned optic flow estimation model sensitive to both first- and higher-order motions. The model successfully replicates the characteristics of human visual motion in various scenarios ranging from typical stimuli to more complex natural scenes, striking a good balance between human and machine vision models.\nRecent studies have also leveraged DNNS to infer the neural and perceptual mechanisms underlying visual motion. Modeling Visual Motion Processing: We modeled human visual motion processing, including the V1-MT architecture, via motion-energy (ME) sensing and integration. Our key notions are that spatiotemporal tuning of the motion-sensing filter is trainable, and that motion integration can be modeled by a recurrent network based on the motion graph. After end-to-end training on a"}, {"title": "4 Methods", "content": "Our biologically oriented model features two stages, Stages I and II. As shown in Fig. 1, Stage I has two channels, of which the first engages in straightforward luminance-based ME computation, and the second contains a multilayer 3D CNN block that enables higher-order feature extraction."}, {"title": "4.1.1 Stage I: First-order Channel", "content": "Spatiotemporally Separable Gabor Filter: When building our image-computable model, each input was a sequence of grayscale images S(p, t) of spatial positions p = (x, y) within domain \u03a9 at times t > 0. We sought to capture local motion energies at specific spatiotemporal frequencies, as do the direction-selective neurons of the V1 cortex. We modeled neuron responses using 3D Gabor filters [61, 62]. To enhance computational efficiency, these were decomposed into spatial 2D Gabor filters G(.) and temporal 1D sinusoidal functions exhibiting exponential decay T(\u00b7). Given the coordinates x' = x cos \u03b8 + y sin and y' = -x sin \u03b8 + y cos \u03b8, the filters may be defined as follows:\nTrainable parameters in red color include fs, ft, \u03b8, \u03c3, and y, controlling spatiotemporal tuning, orientation, and Gabor filter shape, while Tadjusts temporal impulse response decay. All parameters are subject to certain numerical constraints, such as e being limited to [0,2\u03c0) to avoid redundancy; fs and ft are limited to less than 0.25 pixels per frame to avoid spectrum aliasing, etc. The response Ln to the stimuli S(p, t) is computed via separate convolutions:"}, {"title": "4.1.2 Stage I: Higher-Order Channel", "content": "In the second-order channel, we employ standard 3D CNNs to extract non-first-order features. This channel features five layers of 3D CNNS, each of kernel size 3 \u00d7 3 \u00d7 3, linked via residual connections and nonlinear ReLU activation functions. The 3D CNN layers engage in preprocessing before extraction of nonlinear features, which are then processed using the ME constraints described above, and the motion energies calculated. Each input to this channel is a sequence of RGB images, and the output is formatted to match that of the first-order channel, thus E2 \u2208 R\ubbdb\u00d7\u8a3e\u00d7256Both the first- and second-order channel activa-tions undergo the same normalization process,after which they are merged via a 1 \u00d7 1 convolution.Each fused output, thus Em \u2208 R\ubbdb\u00d7\u00d7256,is then fed to Stage II. In Figs. 5 and 6, we designate the model incorporating Stage II with Em asthe \"Dual Channel.\" Conversely, the model rely-ing solely on E\u2081 is labeled as the \"First-orderChannel.\" The results presented in Figs. 2 and 3are exclusively derived from the model utilizingthe First-order Channel."}, {"title": "4.1.3 Stage II: Global Motion Integration and Segregation", "content": "First-stage neurons have a limited receptive field, constraining them to detect only nearby motion. Solving the aperture problem in motion perception systems necessitates advanced spatial integration [67]. This process involves complex mechanisms [68, 69] and requires extensive prior knowledge, which may surpass traditional modeling methods. CNNs, with their extensive parameterization and adaptability, provide a viable solution. However, spatial integration of local motions demands more versatile connectivity than that offered by standard convolutions. Such convolutions are limited to local receptive fields [70]. To address this, we developed a computational model that employed a graph network and recurrent processing for effective motion integration.\nMotion Graph Based on a Self-Attention Mechanism: We move beyond traditional Euclidean space in images, creating a more flexible connection across neurons using an undirected weighted graph, G = {V,A}. Here, V denotes nodes (each spatial location p(i,j)) and A is the adjacency matrix, indicating connections among nodes. The feature of each node is the entire set of the corresponding local motion energies, thus E(i,j) \u2208 R1\u00d7256. The connection between any pair of nodes is computed using a specific distance metric. Strong connections form between nodes with similar local ME patterns.\nThis allows the model to establish connections flexibly between different moving objects or elements across spatial locations, thus creating what we term a motion graph. Specifically, the distance between any pair of nodes (i,j) is calculated using the cosine similarity. This is similar to the self-attention mechanisms of current transformer structures [71-73]. We use the adjacency matrix A \u2208 RHW\u00d7HW to represent the connectivity of the whole topological space, where A is a symmetrical, semi-positive definite matrix defined as:\nWe subject the connections between graphs to exponential scaling using the matrix A given by exp(As), where s is a learnable scalar restricted to within (0,10) to avoid overflow. The smaller the s, the smoother the connections across nodes, and vice versa. Finally, a symmetrical nor-malization operation balances the energy, thus A := D exp(sA)D-, where D is the degree matrix. This yields an energy-normalized undi-rected graph. Intuitively, the adjacency matrix represents the affinity or connectivity of a neuron within the space. Strong global connections form between neurons, the motion responses of which are related.\nRecurrent Integration Processing: Recurrent neural networks flexibly model temporal dependencies and feedback loops, which are fundamental aspects of neural processing in the brain [74]. We use a recurrent network, rather than multiple feedforward blocks, to simulate the process of local motion signals being gradually integrated into the MT and eventually converge to a stable state."}, {"title": "4.2 Training Strategy", "content": "We employ a supervised learning approach guided by the similarity between human motion perception and the physical GT, as suggested by [7]. However, our primary focus is on how effectively the model mimics human motion perception, rather than how precisely it predicts the GT. During training, we utilize a sequential pixel-wise mean-square-error loss to minimize the difference between the GT and the model predictions of Stage I and of each iteration of Stage II."}, {"title": "4.2.1 Dataset", "content": "Our dataset includes a variety of natural- and artificial-motion scenes. Specifically, it incorporates existing benchmarks such as MPI-Sintel and Sintel-Slow [30], as well as natural videos from DAVIS, with pseudo-labels generated using Flow-Former [36]. This collection is termed Dataset A. Additionally, we included custom multi-frame"}, {"title": "4.3 Dataset Generation", "content": ""}, {"title": "4.3.1 Dataset Rendering", "content": "We used the Kubric pipeline [82] to generate large motion datasets that integrated Py-bullet [83] for physics simulation and Py-blender [84] for rendering. We selected several 3D models and textures from ShapeNet and GSO, and natural HDRI backgrounds from Polyhaven [85]. The latter are realistic in terms of lighting and texture. Objects were positioned at specific heights in 3D scenes. We used Py-bullet to simulate physical dropping and interactions between objects. The camera placements were carefully chosen to optimize the viewing angles. Blender then rendered these 3D scenes into 2D images, simultaneously generating a detailed motion flow field for each pixel, as shown in Fig. 4. Material properties were manipulated using the principled BSDF function to create apparently natural optical effects. We generated materials with both Lambertian and non-Lambertian reflections that differed in terms of metallicity, specularity, anisotropy, and transmission. All other features, thus the optical flow distribution, illumination, object, and scene arrangements, were standardized to maintain consistency across the dataset."}, {"title": "4.3.2 Second-order Dataset Modulation", "content": "We developed a second-order dataset to benchmark second-order motion perception in both humans and computational models. The dataset consists of 40 scenes featuring seven types of second-order motion modulations. Each modulation comprises 16 frames, with a randomly moving carrier overlaid on a natural image background. To eliminate first-order motion interference, the natural images were kept static, and the random motion patterns were generated using a Markov chain. The random carrier motion, S(t) = [U(t), V(t)], evolved according to the transition probability:\nwhere the motion states [U, V] were sampled from 2D Gaussian distributions conditioned"}, {"title": "4.4 Experimental Details", "content": ""}, {"title": "4.4.1 In-silico Neurophysiological Methods", "content": "We employed drifting Gabor or plaid (composed of two Gabor components) with a single frequency component as the input stimulus. For second-order motion, drift-balanced motion modulation was applied to the same Gabor envelope.\nThe model responses after Stage I and after each iteration of Stage II were considered analogous to the PSTH of a neuron, thus reflecting activation levels. Responses across the spatial dimensions were averaged to obtain the activation distributions of the 256 units, represented as R1\u00d71\u00d7256 with respect to the input stimulus. The stimuli were typically 512\u00d7512 pixels in size, with full contrast."}]}