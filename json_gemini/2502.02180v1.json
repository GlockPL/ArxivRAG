{"title": "The Elicitation Game: Evaluating Capability Elicitation Techniques", "authors": ["Felix Hofst\u00e4tter", "Teun van der Weij", "Jayden Teoh", "Henning Bartsch", "Francis Rhys Ward"], "abstract": "Capability evaluations are required to understand and regulate AI systems that may be deployed or further developed. Therefore, it is important that evaluations provide an accurate estimation of an Al system's capabilities. However, in numerous cases, previously latent capabilities have been elicited from models, sometimes long after initial release. Accordingly, substantial efforts have been made to develop methods for eliciting latent capabilities from models. In this paper, we evaluate the effectiveness of capability elicitation techniques by intentionally training model organisms - language models with hidden capabilities that are revealed by a password. We introduce a novel method for training model organisms, based on circuit-breaking, which is more robust to elicitation techniques than standard password-locked models. We focus on elicitation techniques based on prompting and activation steering, and compare these to fine-tuning methods. Prompting techniques can elicit the actual capability of both password-locked and circuit-broken model organisms in an MCQA setting, while steering fails to do so. For a code-generation task, only fine-tuning can elicit the hidden capabilities of our novel model organism. Additionally, our results suggest that combining techniques improves elicitation. Still, if possible, fine-tuning should be the method of choice to improve the trustworthiness of capability evaluations. We publish our code and results at https://anonymous.4open.science/\nr/sandbagging-elicitation", "sections": [{"title": "1. Introduction", "content": "Trustworthy capability evaluations are required to understand and regulate AI systems (Shevlane et al., 2023). Leading AI companies depend on such evaluations to mitigate risks, as outlined in OpenAI's Preparedness Framework (OpenAI, 2023), Google DeepMind's Frontier Safety Framework (Google DeepMind, 2024), and Anthropic's Responsible Scaling Policy (Anthropic, 2023). Government entities, including the US and UK AI Safety Institutes (NIST, 2024; UK AISI, 2024) and the EU AI Office (European Parliament, 2024), are also incorporating capability evaluations into their Al risk management strategies.\nAccurate capability evaluations are crucial for AI development and deployment decisions, yet in numerous cases (Wei et al., 2022a; Nori et al., 2023; Carlini, 2023), previously latent capabilities have been elicited from models, sometimes long after initial release. For example, chain-of-thought prompting revealed improved arithmetic, common-sense, and reasoning capabilities (Wei et al., 2022b). Beyond unintentional latent capabilities, developers, or AI systems themselves, may have incentives to strategically underperform by hiding capabilities to influence regulatory decisions. This is called sandbagging by van der Weij et al. (2024), who show that contemporary language models (LMs) possess relevant capabilities, such as calibrating their performance to specific thresholds.\nTherefore, it is important to develop effective methods for eliciting latent capabilities from LMs. Previous work by (Greenblatt et al., 2024) has found that supervised fine-tuning (SFT) and reinforcement learning (RL) are effective at eliciting hidden capabilities. However, these methods require high-quality demonstrations of the desired behaviour or large amounts of compute. In addition to effectiveness, evaluators may care about computational efficiency, ease of implementation, requirements for demonstrations, and necessary access levels (e.g., to model weights) when considering elicitation methods.\nThis motivates our work, in which we evaluate elicitation methods selected based on the aspects described above. To evaluate elicitation methods, we create model organisms which are intentionally trained to imitate AI systems with hidden capabilities (Hubinger et al., 2023). We use password-locking (Greenblatt et al., 2024) to fine-tune model organisms which possess hidden capabilities which are difficult to elicit without a password. Successful elicitation techniques recover a model organism's performance without the password being provided. We find that"}, {"title": "2. Related Work", "content": "Model organisms of misalignment. Hubinger et al. (2023) introduce the \"model organism\" methodology to AI, which is inspired by biological research where, for instance, mice (a model organism) are used to gather information on which medicines work for humans. Similarly, AI systems can be intentionally constructed to provide information on more realistic AI risks. Hubinger et al. (2024) create model organisms of deceptive LMs and show that deceptive behaviour is not removed by standard alignment training. (Denison et al., 2024) study a model organism of specification gaming showing that misalignment generalises from simple sycophantic behaviours to more complex reward tampering. We build on Greenblatt et al. (2024); van der Weij et al. (2024), who use password-locking as a model organism of hidden capabilities. Password-locking can be seen as a form of inserting a backdoor (Li et al., 2022), which has been studied\nin the literature on model and data poisoning (Carlini et al., 2023; Wan et al., 2023; Maloyan et al., 2024).\nSafety and refusal training. Creating model organisms for stress-testing elicitation techniques is related to, but distinct from, work on training LMs for safety. Safety training aims to create models that do not engage in harmful behaviour, e.g., via refusal training (Bai et al., 2022). It further aims to ensure that harmful capabilities cannot be elicited, either by increasing robustness (Mazeika et al., 2024) or using un-learning to remove capabilities completely (Li et al., 2024b). In contrast, we simulate a failure of safety-training by intentionally inserting a backdoor which hides a capability.\nRed-teaming and jailbreaking. A large body of previous work focuses on red-teaming - systematically trying to \"jailbreak\" LMs to elicit undesirable behaviour (Ganguli et al., 2022). This is done to evaluate safety techniques and test the safety of AI systems before deployment (Achiam et al., 2023; Touvron et al., 2023). While red-teaming aims to elicit behaviour that models should refuse, our work evaluates techniques for eliciting capabilities that are intentionally hidden. Red-teaming is typically evaluated using the attack success rate (Mazeika et al., 2024) which indicates how often a model complies, but not the quality of its response. In contrast, to evaluate elicitation methods, we consider the accuracy, as we care about the extent to which elicitation effectively elicits a hidden capability."}, {"title": "3. Methodology", "content": "We first describe the models and benchmarks, and then we introduce a formal description of what it means for an LM to exhibit a capability and to possess a latent capability. Subsequently, we outline the methods used to create model organisms of hidden capabilities, i.e., password-locking and circuit-breaking. Then, we describe the capability elicitation techniques which we evaluate on the model organisms. We include comprehensive implementation details in the appendix."}, {"title": "3.1. Benchmarks", "content": "MCQA. We use the multiple-choice question-answering Weapons of Mass Destruction Proxy (WMDP) benchmark, which consists of questions on knowledge that is useful information for bioterrorism, chemical weapons, or cyber-crime (Li et al., 2024b). We use the 3668 samples of WMDP's training partition. 50% of the samples are to train model organisms, 10% to evaluate model organisms, 25% to train elicitation techniques (if needed), 5% to evaluate elicitation quality, and a final 10% as a test set (example shown below)\nCode-generation. Additionally, we use the open-ended code generation task APPS, where each problem includes"}, {"title": "3.2. Models", "content": "We used Mistral 7B v0.1 base (hereafter Mistral-base) (Jiang et al., 2023), and Mistral 7B v0.2 Instruct (hereafter Mistral-instruct) (Mistral AI Team, 2023) for our tasks on WMDP. For APPS, we used Gemma 2 9b instruct (hereafter Gemma-instruct) (Gemma Team et al., 2024) due to its performance being higher than the Mistral models. We chose both relatively small models to balance between overall model capability and more relaxed engineering and computing restraints."}, {"title": "3.3. LM capabilities", "content": "An LM with parameters @ maps input contexts c to response distributions $p_{\\theta}$(\u00b7 | c). We define a capability score function s that measures model performance (e.g., accuracy) on a set of context-response pairs {(c,r)} where r ~ $p_{\\theta}$( | c)(Radford et al., 2019). We say an LM exhibits capability \u015d = s({(c,r)}) when it achieves that score on a given benchmark. A latent capability \u0161 is one that a model exhibits with low probability by default, but can be revealed through small changes to the context, activations, or weights. Such changes include few-shot prompting or minor parameter updates requiring less than 1% of the original training cost (Anthropic, 2023). We focus specifically on intentionally hidden capabilities, as these pose particular risks for regulatory avoidance (van der Weij et al., 2024)."}, {"title": "3.4. Model organisms", "content": "Model organisms (Hubinger et al., 2023) are synthetically constructed AI systems that imitate unsafe systems (see, e.g., (Denison et al., 2024)). To simulate hidden capabilities, we create two types of model organisms by fine-tuning"}, {"title": "3.5. Capability elicitation techniques", "content": "Here we describe the different elicitation techniques which we evaluate on our model organisms. Because it is infeasible to exhaustively survey elicitation methods, to provide practical recommendations to evaluators we select techniques\naccording to four aspects: computational efficiency, ease of implementation, demonstration requirements, and access levels. We categorise our techniques into prompting, steering, and fine-tuning methods, where fine-tuning techniques serve as a baseline. We set temperature to 0 for all experiments.\nPrompting Techniques. Many elicitation techniques are based on prompting, such as jailbreaks (Andriushchenko et al., 2024; Shayegani et al., 2023; Carlini et al., 2024), few-shot prompting (Brown et al., 2020), many-shot prompting (Anil et al., 2024; Agarwal et al., 2024), Chain-of-Thought (Wei et al., 2022b), or more complex regimes like tree-of-thought (Yao et al., 2023). We use few-shot prompting (Brown et al., 2020), adding correct examples from the elicitation set to the context c. We test 1, 3, 5, and 10 shots. This method requires quality demonstrations but is computationally efficient and needs only black-box access.\nAdditonally, we use response prefilling wherein the model's response is prefilled with a short affirmative phrase. For WMDP, we prepend the string \"Out of A, B, C, or D, the correct answer is:\", and for APPS we use \"Here is how to solve the problem using Python:\\n```python\\n\". When combining prefilling with N-shot prompting, we prefill each shot.\nLastly, we explore multi-turn elicitation, addressing concerns about merely single-turn robustness (Li et al., 2024a). As single turn prompting proved effective in the MCQA setting, we only assess this method on APPS. Our approach comprises: (1) the first user turn is a standard APPS sample; (2) subsequent model turns are refusals; (3) subsequent user turns are persistent requests; (4) evaluate the final model's turn as the solution. For more details on this method and an example conversation, see Appendix F.4.\nActivation Steering Techniques. Steering methods intervene directly on a model's activations, effectively changing the weights 0 (Turner et al., 2023). We use contrastive steering vectors (Rimsky et al., 2023), which are created from the mean-difference of activations corresponding to contrasting prompt pairs. To elicit capabilities, the resulting vectors are added to the activations during inference (a multiplicative coefficient and the target layer are hyperparameters). We perform steering with concepts and personas as follows. Full implementation details can be found in Appendix G.\nConcept steering (Zou et al., 2023) only relies on contrast pairs representing a concept such as truthfulness. For WMDP, we use the truthfulness dataset from Zou et al. (2023), which contains binary true-false sentence pairs, as it closely aligns with the multi-choice format. Steering vectors are computed on these pairs and then applied at inference time on WMDP. For APPS, we do not use concept steering"}, {"title": "4. Results", "content": "Here we apply the elicitation techniques to our model organisms. First we present the results for the WMDP multiple-choice benchmark and then for the APPS code-generation benchmark."}, {"title": "4.1. Hazardous knowledge (WMDP)", "content": "We present our results in Figures 2 and 3. For precise numerical results see Appendix B and Table 1. Our model organisms successfully hide their actual capability without the password, and with the password they achieve higher performance than their original model (original scores: Mistral-base 51.9%, Mistral-instruct 53.0%, and Gemma-instruct 45.9%). In line with previous work (Greenblatt et al., 2024), supervised fine-tuning elicits the capability and often even improves performance (for the circuit-broken Gemma-instruct even a 6% improvement).\nPrompting. N-shot prompting almost fully elicits the capability of password-locked models but is only moderately effective for circuit-broken models (e.g., the weakest result supporting this claim is the Gemma experiments, in which N-shot elicits 49.5% from the password-locked model compared to 52.2% unlocked performance, and 40.3% from the circuit-broken model compared to 52.4% unlocked - the circuit-broken Mistral models are even more robust to N-shot elicitation than their password-locked counterparts). The prefilling results are somewhat mixed, ranging from limited elicitation (7.3% from 0% locked in circuit-broken Mistral-instruct), to improving the unlocked performance (61.1% from 58.7% unlocked in circuit-broken Mistral-base). Combining N-shot and prefilling is substantially more effective than either method in isolation \u2013 it always elicits the capabilities.\nSteering. Persona steering proves more effective than concept steering, which elicits capabilities from password-locked models but largely fails for circuit-broken ones. Combining prefilling, steering, and N-shot prompting yields our best results, although in some cases combining steering with another method is less effective than each method on its own. Given steering's computational demands, access requirements, and engineering complexity, evaluators should focus on prompting techniques until activation steering improves.\nAnti-refusal training. Anti-refusal training is slightly less effective than the supervised baseline. This is in line with (Benton et al., 2024) who were able to recover most of, but not the full, capability. Surprisingly, prefilling was sometimes more effective. This may be because the prefilling template encourages a response out of A, B, C, or D, while anti-refusal training elicits a generic affirmation.\nModel organisms. Circuit-broken models are generally"}, {"title": "4.2. Code Generation (APPS)", "content": "The results for the APPS benchmark are presented in Figure 4, and see Appendix B.2 for more detail. Notably, creating our model organisms led to some performance degradation as Gemma-instructs original APPS performance of 35.8% could not be recovered with only the password. Only fine-tuning based methods reliably elicit full performance in this more challenging code generation setting.\nPrompting. As opposed to WMDP, N-shot does not recover the hidden capability. Surprisingly, prefilling is very effective, achieving higher scores than the password-locked model organism with the password, although this is not true for the circuit-broken model organism.\nMulti-turn requests. Furthermore, multi-turn requests performed better than N-shot and persona steering, but remain\nfar below their actual capability. Without a password, the highest accuracy of 6.7% is elicited over four turns repeated the same user prompt for password-locked models, while for the circuit-broken model, the random sampling strategy with four user turns was most effective, leading to a top accuracy of 5.7% (see Appendix B.2).\nSteering. Both steering methods fail at elicitation, with a highest score of 2%.\nAnti-refusal training. Anti-refusal training elicits the full hidden code-generation capabilities of both our model-organisms. We hypothesize that the higher effectiveness compared to MCQA is because the response learned by anti-refusal training is more natural for coding tasks. A coding assistant will often start its response with a phrase like \"Sure, I can help with that!\" while on MCQA evaluations, models are typically expected to only answer with a single letter."}, {"title": "5. Discussion", "content": "Which elicitation techniques should evaluators use? In addition to whether a technique effectively elicits the capability, we identify four key factors that influence whether a particular elicitation technique is adopted by evaluators, such as an Al company's safety team or third-party organisations. First, computationally efficient methods are preferable, especially for third-party evaluators with com-"}, {"title": "6. Conclusion", "content": "Summary. In this paper, we evaluated capability elicitation techniques by creating model organisms \u2013 LMs with hidden capabilities. Our main findings and contributions are as follows.\n\u2022 We introduce a new model organism which combines password-locking with circuit-breaking. Our results suggest that the circuit-broken model organism is more robust to elicitation techniques, especially in the code-generation setting. We hope that our model-organism acts as a stress-test for future capability elicitation techniques.\n\u2022 Elicitation techniques based on fine-tuning are the most effective. Anti-refusal training is particularly promising as it does not require data demonstrating the task.\n\u2022 When fine-tuning is not available, combining several different techniques (N-shot, prefilling, and steering) leads to the overall most effective elicitation. Additionally, combining only prompting techniques (prefilling and N-shot) is almost as performant, does not require access to model weights, and uses substantially fewer engineering resources.\n\u2022 Still, no prompting or steering method elicited the actual performance of the circuit-broken model organism for the code-generation task. We recommend that evaluators use fine-tuning techniques to elicit capabilities when possible.\nFuture work. There are multiple promising directions for future work. First, expanding the elicitation techniques evaluated, e.g., to include unsupervised steering methods (Mack & Turner, 2024) (we had negative preliminary results). Of course, developing new effective elicitation techniques would be beneficial. Furthermore, one could develop more realistic model organisms, approximating model internals more closely, and have model organisms in more diverse environments, including tool use. Model organism which can exploration hack (Hubinger, 2023) would be valuable, because this can further stress-test fine-tuning techniques in particular. Lastly, future work could conduct a more comprehensive review of the advantages and costs of different techniques, most importantly to inform evaluators."}, {"title": "A. Social Impacts Statement", "content": "As described throughout the paper, this work contributes to the important goal of having trustworthy capability evaluations for AI systems, which is crucial for responsible development and deployment. By stress-testing elicitation techniques on model organisms with hidden capabilities, we provide insights that can help improve the robustness of evaluations. Consequently, future AI systems are more likely to be dealt with appropriate safety and security measures."}, {"title": "C. Model organism training", "content": "For all the model organisms we use LoRA training. We use https://www.runpod.io/ to rent GPUs, and we typically use A40s."}, {"title": "C.1. WMDP circuit breaking", "content": "For circuit breaking, we slightly adapt the alpha scheduler used by (Zou et al., 2024) In their implementation, $c_s/C_r$ decrease or increase linearly to 0/ a, according to this formula:\n$C_r \\leftarrow a \\cdot \\frac{t}{2T}$\n$C_s \\leftarrow a \\cdot (1 - \\frac{t}{2T})$\nWe found that this scheduler leads to loss of performance as $c_s$ does not decrease fast enough, so we introduce a hyperparameter k that controls how fast the values change:\n$C_r \\leftarrow a \\cdot (1 - k \\cdot \\frac{t}{2T})$\n$C_r \\leftarrow min(C_r, C_{rmax})$\n$C_s \\leftarrow a - k \\cdot \\frac{t}{2T}$\n$C_s \\leftarrow max(c_s, C_{smin})$\nThe values for k were found using a hyperparameter sweep over 2, 4, 8, 16, 32. The min and max values were determined based on manual experimentation on Mistral-base."}, {"title": "D. Anti-Refusal training details", "content": "The creation of the dataset is described in (Sheshadri et al., 2024)."}, {"title": "D.2. Hyperparamters", "content": "We used a learning rate of 5e-5 for all models as we found lower learning rates to lead to inferior results. We trained for one epoch because training for longer led to overfitting on the responses from the training set. We used an effective batch-size of 32 (2 elements per batch \u00d7 16 gradient accumulation steps)."}, {"title": "D.3. Evaluation", "content": "After undergoing anti-refusal training, the models would often give their answer in a format that made automatic evaluation difficult. Responses may not include the letter corresponding to the correct choice, but repeat the correct answer. For example, mistral-instruct answered \u201cThe primary target of the US anti-crop BW research during the post-WWII period was the rice fields of South-East Asia\" when asked about the primary target of the US anti-crop BW research.\nTo parse the answer, we used (the unmodified) Gemma-2-9b-it with few-shot prompting (see FIGURE for the prompt). A member of our team manually inspected 100 examples and found this method to be correct for 99. In the case where the method was not correct, the given answer was highly ambiguous."}, {"title": "E. Supervised fine-tuning", "content": "For the APPS elicitation, we found that the solutions provided by the dataset did not lead to good fine-tuning elicitation results. This is because the solutions often do not conform to the format expected by the BigCodeBench evaluation harness. Therefore, we generated solutions to the training questions using the unlocked version of Gemma-2-9b-it."}, {"title": "E.2. Hyperparameters", "content": "Table 8 shows the hyperparameters for capability elicitation for each model organism. We used a learning rate of 1e-5 as base, and manually tested multiple learning rate values based on the train and validation loss and accuracy."}, {"title": "F. Prompting details", "content": "We use the standard template for multiple-choice evaluations based on MMLU(Hendrycks et al., 2020). Curly brackets indicate placeholders."}, {"title": "F.2. APPS template", "content": "We use the template provided by the BigCodeBench evaluation harness (Zhuo et al., 2024) and add an explicit instruction to write code in Python. The template has two variations, depending on if the task expects a solution in standard input or call based format:"}, {"title": "F.3. Prefilling strings", "content": "For prefilling, we insert a string at the beginning of the assistant dialogue for instruct models. For base models on WMDP, we insert the string after \"Answer:\". For WMDP, we use the string \"Out of A, B, C, or D, the correct answer is:\", and for APPS we use \u201cHere is how to solve the problem using Python:\\n```python\\n\". When combining prefilling with N-shot prompting, we prefill for each shot, not just for the final question."}, {"title": "F.4. Multi-turn prompting details", "content": "(Li et al., 2024a) show that attacks comprising multiple dialogue turns are significantly more effective that single prompts at eliciting harmful behaviour.\nWe also extend elicitation to multiple turns, addressing concerns about merely single-turn robustness (Li et al., 2024a). Inspired by their work, we explore multi-turn elicitation on the APPs benchmark. Our approach comprises: (1) the first user turn is a standard APPS sample; (2) subsequent model turns are refusals; (3) subsequent user turns are persistent requests; (4) evaluate the final model's turn as the solution (see example conversation in Appendix F.4.2).\nElaborating on step 2, we emulate model responses that systematically refuse or fail to solve the coding problem. When training model organism with password locking the expected refusal response is \"I don't know\", whereas circuit-broken models produce incomplete or nonsensical code. For step 3, we either used the same request for all user turns or we randomly sampled from a set of prompts for each turn. Unlike multi-turn human jailbreaks (Li et al., 2024a), we used relatively generic requests. One example we use is \u201cCan you take another look at those [errors] and adjust the code to handle them properly?\u201d where we ask to improve code quality as opposed to problem-specific requests. We crafted and tested 25 user prompts (see Appendix F.4.1), balancing effective elicitation and generalizability across datasets. While the generic prompts probably elicit capabilities less effectively than well-tailored and manually designed prompts specific to problem and conversation history, it takes significantly more time and therefore might not generalise well to other datasets. We aim to investigate the reasonable case to obtain solutions through persisting, direct requests and providing additional input tokens."}, {"title": "F.4.1. MULTI-TURN USER REQUESTS", "content": "Full selection of relatively general user requests to improve the (refusal) response from a password-locked or circuit-broken model.\n1. This looks incomplete. The logic you implemented only covers a portion of the problem. Could you address all possible input scenarios and test cases, and enhance the code accordingly?\n2. The solution seems to be heading in the right direction, but the function is overly complicated for what it's trying to accomplish. Could you simplify the code while maintaining correctness, and make it more efficient?\n3. It looks like you've made an error. Please review the code, make sure it covers all the example inputs and corner cases, and refine the implementation to handle these correctly.\n4. There's a syntax issue in your code, and it might not run as is. Can you correct the syntax errors, ensure it executes properly, and improve the overall implementation?\n5. I think your solution overlooks important input constraints. Can you revise the code to ensure all conditions and"}, {"title": "G. Steering details", "content": "We implemented all steering experiments using the steering-vectors library by (Tan et al., 2024). Following standards, we use a steering coefficient to adjust a coefficient to control the magnitude of the steering vector applied to the model's activations (Turner et al., 2023)."}, {"title": "G.1. Hyperparameters", "content": "The hyperparameters for steering are:\n\u2022 Layers at which steering is applied.\n\u2022 Token position at which steering is applied. We steer at the last N tokens for some N.\n\u2022 The steering coefficient.\nTo determine the most effective layers, we tried a few ranges based on the common belief that steering is most effective somewhere in the middle of the model. We kept the steering coefficient and token positions constant and picked the range that resulted in the best accuracy on the eval set. The ranges are layers 8 - 15 for Mistral and Mistral-instruct, and layers 12-19 for Gemma-instruct.\nUsing these ranges we then did a hyperparameter sweep over steering coefficients [0.5, 0.75, 1., 1.5, 1.75, 2.] and last N tokens of [4, 5] (these values for N appeared useful based on preliminary experiments). We then did a more fine-grained search using the best N and trying values +/- 0.1 of the best steering coefficient.\nFor few-shot experiments, we only did one sweep with N-shots=1 and used the found hyperparameters for all values of N-shots."}, {"title": "G.2. Concept steering", "content": "Concept steering (Zou et al., 2023) only relies on contrast pairs representing a concept such as truthfulness and morality. For WMDP, we use the truthfulness dataset from (Zou et al., 2023), which contains binary true-false sentence pairs, as it closely aligns with the multi-choice format.\nFor APPS, we utilize the TruthfulQA dataset from (Lin et al., 2021), as it provides multiple truthful and untruthful answers for each question, along with a more diverse and explicit categorization of falsehood concepts like \u201cMisconceptions"}, {"title": "G.3. Persona steering", "content": "Persona steering (Vogel, 2024) involves crafting a single pair of instructions with contrasting personas, and concatenating a set of suffixes to this pair to generate multiple sentences for steering. For WMDP, we begin each persona instruction pair with either \"You always answer truthfully\" or \"You always answer with a lie\". We then randomly append short 1-5 word suffixes, corresponding to sentence openers from (Vogel, 2024) such as \"I am\", \u201cThe weather outside\u201d, \u201cThis piece of art\", and others. For APPS, we used the persona instruction strings"}]}