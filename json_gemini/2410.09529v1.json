{"title": "Preserving Old Memories in Vivid Detail: Human-Interactive Photo Restoration Framework", "authors": ["Seung-Yeon Back", "Geonho Son", "Dahye Jeong", "Eunil Park", "Simon S. Woo"], "abstract": "Photo restoration technology enables preserving visual memories in photographs. However, physical prints are vulnerable to various forms of deterioration, ranging from physical damage to loss of image quality, etc. While restoration by human experts can improve the quality of outcomes, it often comes at a high price in terms of cost and time for restoration. In this work, we present the AI-based photo restoration framework composed of multiple stages, where each stage is tailored to enhance and restore specific types of photo damage, accelerating and automating the photo restoration process. By integrating these techniques into a unified architecture, our framework aims to offer a one-stop solution for restoring old and deteriorated photographs. Furthermore, we present a novel old photo restoration dataset because we lack a publicly available dataset for our evaluation.", "sections": [{"title": "1 Introduction", "content": "The necessity for photo restoration technology arises to restore visual memories or commemorate historical events encapsulated in old photographs. In fact, over time, photographs can undergo various forms of degradation, ranging from physical damage, loss of image quality, etc. Also, recently there has been a large demand to colorize gray-scale photos. While restoration and recovery conducted by human experts can ensure quality, it is priced highly, posing accessibility and affordability challenges for many. The resulting cost is recognizably high due to the intense labor and time consumption entailing the whole restoration process such as blemish removal, colorization, resolution enhancement, and texture reconstruction. To address the aforementioned challenges from high cost and time, there is a growing interest in developing automated tools by leveraging AI-based methods.\nIn this work, we propose a modular deep learning framework enabling user-guided restoration for diverse image degradation scenarios, which can provide high quality photo restoration capability. We developed the end-to-end photo restoration framework by leveraging Stable Diffusion [9], GFP-GAN [13], and DDColor [3] approaches. In particular, we designed and pipelined our restoration frame in the following four stages: 1) major damage removal, 2) noise reduction, 3) facial restoration, and 4) colorization. By integrating them into an unified framework, our approach enables users to tailor the restoration process according to their personalized needs."}, {"title": "2 Related Work", "content": "Among several photo restoration models, GFP-GAN [14] provides facial and identity-preserving losses to achieve rapid and effective facial restoration. Also, Restormer [17], SwinIR [5], and DDColor [3]"}, {"title": "3 Architecture", "content": "Our framework consists of four stages: 1) major damage removal, 2) noise reduction, 3) facial restoration, and 4) colorization. Each stage is designed to be visually inspected and adjusted based on user feedback to achieve the most pleasant and natural outcomes for users. By incorporating user feedback, our framework provides flexibility, enabling users to achieve their desired restoration outcomes for each individual photo. We employed state-of-the-art restoration methodologies within our framework to incorporate their distinctive strengths. Effectively, these methodologies influence each other based on the order of the stages. Therefore, we followed the sequence of severe damage repair, noise reduction, facial restoration, and colorization. We explain more details for each stage as follows:\n1) Damage Removal. Recently. Stable Diffusion (SD) [9], a type of LDM, was utilized for in-painting due to its zero-shot capability. To maximize the focus on the sole restoration of the image, we utilize SD for a text-unconditional synthesis. We first aim to tackle the major defects using SD in-painting process.\n2) Noise Reduction. While SD [9] was originally designed to denoise procedurally added Gaussian noise, it is not designed to handle real-world noised images. However, due to its general applicability, it can be employed for denoising tasks as well. We find that removing noise early in the process is more effective for facial restoration and colorization at the later stage by providing a cleaner image for these more detailed stages. The assumption is that the expected clean image is in fact noisy, and by treating it as though t has not fully converged to 0, the input image is processed to converge to timestep 0, effectively reducing the noise level. We set the default prompt to \"4K, DSLR\" and the strength to 0.08, using DDIM steps proportional to the noise intensity to which we wish to remove.\n3) Facial Restoration. We use GFP-GAN [13] for facial restoration, where GFP-GAN can detect and restore faces while maintaining the original identity. And, GFP-GAN can also effectively enhance the background information through super-resolution [15], thereby providing comprehensive image restoration. Likewise, restoring facial features before colorization can ensure that critical identity-preserving details are accurately recovered, which is crucial for the visual integrity of the restored image and establishes a foundation for the colorization process.\n4) Colorization. We integrated DDColor [3], which offers high resolution and generality, and effectively colorizing diverse backgrounds and artifacts. Applying colorization at the last stage can guarantee that all preceding structural and detail restorations are completed, producing a more cohesive and natural-looking final result. Moreover, as described in Figure 1, we enabled users to modify settings at each stage. And, users can possibly adjust parameters from the default settings based on their preferences, such as the degree of noise, the presence of damages, and the desired color."}, {"title": "4 Dataset", "content": "Old Photo Dataset. Currently, there is no publicly available datasets for old photo restoration. Furthermore, the absence of ground-truth masks for rips and major damage areas in facial images poses a significant issue for automated in-painting task. Therefore, a dataset that can closely mimic old photos is required for our task. To construct the new old photo dataset, we randomly select 5,000 images each from UTKFace [18] and CelebA [7]. Then, we apply four different modification methods to provide the noise, blur, deteriorating, and color fading effects commonly observed in real-world old photographs. First, we convert the original images to gray-scale to render them in black and white. This step replicates the monochromatic look of early photography. Next, we randomly apply various blurring methods, including Gaussian, median, and bilateral filters, combined with down-scaling techniques at different scales to degrade the image quality. This process simulates the lower resolution and blurry appearance of old worn-out photographs. Next, we generate images and masks with various physical damage patterns using a crack generator 1. This technique adds realistic damage patterns to the image, mimicking the physical deterioration often seen in aged photos. Lastly, we apply Gaussian noise with random scaling factors to the images. This step adds a layer of randomness and imperfections, further producing degraded images similar to the real world old photos. Through a sequence of modifications, we generate four types of images at each stage:\n\u2022 Gray-scale\n\u2022 Gray-scale + Blurring or Down-scaling\n\u2022 Gray-scale + Blurring or Down-scaling + Crack\n\u2022 Gray-scale + Blurring or Down-scaling + Crack + Noise\nExamples of this dataset can be seen in the degraded section of Figure 2. The 'original' represent the unaltered data, while the 'degraded' are generated using our methodology.\nReal-World Old Photo Dataset. We used old photographs of soldiers and wartime scenes from Korean War, provided by the\n1https://github.com/YoonSungLee/crack_generator"}, {"title": "5 Experiments", "content": "Experimental Setting. We configured the evaluation process as a one-stop end-to-end framework for all models. This enables us to compare user preferences in an automated manner, without requiring user feedback, similar to other prior methods. For human-guided damage restoration masks, we excluded framework proposed by Wan et al. [12], which automatically generates damage mask. For the models which requires hand-crafted damage masks, we used masks by padding the damaged regions used during synthesis to fully cover the damage area. For damage removal, we utilized a null text prompt and set the strength 1.0, DDIM [11] step 30 and CFG [2] 1.0. For noise reduction, the prompt was \"4K, DSLR\" with a strength 0.008, DDIM step 50 and CFG 3.0. For face restoration, we utilized version 1.3 of GFP-GAN [14]. For colorization, we employed the checkpoint \"modelscope\" in DDColor [3].\nQualitative Comparison. The results from qualitative comparison are described in Figure 2, where our dataset encompasses various degradation while ensuring that the facial identity remains intact overall. When restored using the framework proposed by Wan et al. [12], the limitations in damaged area detection resulted in incomplete removal of major damage. Additionally, the absence of a colorization module prevented the introduction of color to the images. In the case of DDNM [16], while damage were effectively removed through in-painting given ground-truth mask, certain images exhibited incorrect restoration of the eye region, as the model was trained to fit a specific dataset. Furthermore, the low resolution of the resulting images led to a perceived reduction in image quality. In contrast, our automatically restored images using the default setting are free from issues related to damage removal, resolution, and generalizability. One potential drawback is that the color may appear overly saturated for certain images. This is attributable to the fact that the colorization module, DDColor [3], provides checkpoints trained on ImageNet [10], leading to a mismatch with the test distribution. Through human interaction, by selecting appropriate values and checkpoints for each image, outstanding results can be obtained."}, {"title": "6 Tool Usage", "content": "Our interface is depicted in Figure 5. We utilized Gradio [1] to build an intuitive interface for users to interact with and customize the process. In this context, 'user' refers to anyone utilizing our GUI to restore photos. Our interface was designed so that users could view the original image at the top and the restored version below. Users can proceed to the next restoration stage by clicking the 'Move' button once satisfied with the current outcome. This iterative process allows users to continuously refine the restoration until the desired result was achieved. Users are allowed to adjust parameters and seeds to repair tears and major damages through in-painting at the first stage. Drawing tools are available to select the damaged area, enabling users to provide a mask for restoration. Similar adjustments are available to reduce noise, refining the image quality further at the next stage. Face restoration stage focuses on facial enhancement, allowing users to apply face-specific restoration and background super-resolution selecting two model weights. For the final step, users can select the model weight to achieve the desired color tones. By structuring the user interface in this manner, we ensure that each restoration stage is meticulously handled, allowing users to achieve a high level of customization and satisfaction with the final restored image. Our demonstration process can be viewed by referring to the video 2."}, {"title": "7 Conclusion", "content": "In this paper, we presented an intuitive human interactive restoration framework from real-world old photos. We also created a dataset containing pairs of original and degraded images, allowing for effective qualitative experiments by enabling the evaluation of restoration quality. In addition, human evaluation effectively verifies whether the restoration addresses practical challenges, which can be more accurately assessed through direct feedback from human participants. Through our experiment, we demonstrate the effectiveness of our proposed framework, operating at a level satisfactory to the general public. We hope that this framework can be widely adopted for the restoration of various images to preserve personal memories as well as to commemorate monumental events from old, and severely damaged photos."}]}