{"title": "A Benchmark for the Detection of Metalinguistic Disagreements between LLMs and Knowledge Graphs", "authors": ["Bradley P. Allen", "Paul T. Groth"], "abstract": "Evaluating large language models (LLMs) for tasks like fact extraction in support of knowledge graph construction frequently involves computing accuracy metrics using a ground truth benchmark based on a knowledge graph (KG). These evaluations assume that errors represent factual disagreements. However, human discourse frequently features metalinguistic disagreement, where agents differ not on facts but on the meaning of the language used to express them. Given the complexity of natural language processing and generation using LLMs, we ask: do metalinguistic disagreements occur between LLMs and KGs? Based on an investigation using the T-REx knowledge alignment dataset, we hypothesize that metalinguistic disagreement does in fact occur between LLMs and KGs, with potential relevance for the practice of knowledge graph engineering. We propose a benchmark for evaluating the detection of factual and metalinguistic disagreements between LLMs and KGs. An initial proof of concept of such a benchmark is available on Github.", "sections": [{"title": "1. Introduction", "content": "Recent years have seen a surge of interest in the use of LLMs for purposes of knowledge engineering [1]. LLMs are being used to perform text classification, sentiment analysis, and natural language inference, exploiting next-token prediction to generate text that can be transformed into the type of symbolic outputs normally produced in these tasks [2]. Increasing emphasis is being placed on the use of LLMs in knowledge graph construction [3]. The results have been encouraging, but a major concern that has emerged is the impact of hallucination, which is defined as the presence of factually incorrect or unjustified assertions in the output of LLMs [4, 5]. Benchmarks such as SHROOM [6] and WildHallucinations [7] have been developed to evaluate the ability to detect hallucination when it occurs in LLM output.\nA number of mechanisms have been proposed to mitigate hallucination in LLMs through the use of knowledge from a variety of sources, including natural language text, KGs, and rules, to ground [8] an LLM. Retrieval-augmented generation (RAG) is a specific version of this approach that has attracted a great deal of interest, particularly in the context of commercial applications [9]. Such knowledge-enhanced LLMs [10] show improvements in the performance of natural language understanding and generation tasks. However, even with such improvements, knowledge-enhanced LLMs still produce errors as measured using common evaluation metrics (e.g. F1 measures for classification). These evaluation metrics are calculated by measuring the difference between an LLM's output and ground truth as provided in fact checking benchmarks such as LAMA [11], KAMEL [12], and FActScore [13].\nThe errors reported by these metrics are typically assumed to stem from disagreement about facts. But there is another way in which these differences can arise. Metalinguistic disagreement [14, 15, 16] occurs when people argue about the meaning or use of words rather than about facts or ideas. In contrast, a factual disagreement is about what is actually true in the world. Examples of factual disagreement are debating whether a tomato is healthier than an apple, or debating whether Sarah is taller than John; in contrast, examples of metalinguistic disagreement are arguing whether a tomato should be called a fruit or a vegetable, or arguing about what height qualifies as \u201ctall\" when describing a person.\nConsider the following scenario: a knowledge-enhanced LLM generates an output that contradicts ground truth provided by a KG. This is used as evidence that the LLM has committed a factual error in its output. However, in producing its output, the knowledge-enhanced LLM has provided a rationale that indicates that there is a disagreement about the meaning of a term that has led to the output. Can this occur in practice? Our hypothesis is that it does.\nWhy would this matter? Factual disagreements can be resolved through knowledge graph refinement [17] or through few-shot in-context learning that provides the correct facts to the LLM; however, metalinguistic disagreements may require ontology engineering to address representational issues with a knowledge graph, or the engineering of prompts that incorporate intensional definitions in natural language of concepts for an LLM [18]. Data governance [19] also acknowledges the importance of establishing metalinguistic agreement of intensional definitions of concepts and relations in natural language and their realization in databases and database schemas; for example, the FAIR principles [20, 21] specifically urge clear documentation of metadata aligning natural language concepts and metadata in scientific data resources. We therefore argue that distinguishing factual from metalinguistic disagreement between LLMs and KGs is relevant to the practice of knowledge graph and ontology engineering."}, {"title": "2. Evidence for the occurrence of metalinguistic disagreement in LLMs", "content": "To test our hypothesis that metalinguistic disagreement is a detectable phenomenon, we conducted a simple experiment by fact checking a set of knowledge graph triples aligned with natural language text using an LLM, and then estimating the rate at which metalinguistic disagreement occurs when the LLM determines the triple is not true.\nWe randomly sampled 100 Wikipedia abstracts from the 10,000 document sample provided in the T-REx dataset, a dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx has been widely used in the evaluation of LLM-based fact checking and extraction for knowledge graphs [22]. From the total set of triples aligned with the documents in that sample, we then sampled 250 triples. We then defined a zero-shot chain-of-thought classifier [23] to assign a truth value to an aligned triple, providing the Wikidata abstract with which it is aligned as context in the LLM prompt [24]. The classifier was executed to obtain a rationale and a truth value for each of the 250 sampled triples and aligned abstracts, and each result was then processed by a second zero-shot chain-of-thought classifier (using gpt-40-2024-05-13) acting as an LLM-as-a-judge [25], to classify whether the truth-value-assigning classifier's rationale indicated a metalinguistic disagreement. Processing required a total of 2 inference API calls per alignment, per LLM. Evaluations whose statistics are reported below were conducted during the period from 1 July 2024 to 8 July 2024. Costs incurred through calls to language model APIs totalled less than $100 USD. Code and data used in the experiments are available in a Github repository\u00b9.\nAs shown in Table 1, over the 9 LLMs evaluated, false negative rates over the 250 sampled T-REx triples ranged between 0.104 and 0.504 with a mean of 0.246, and the rate of metalinguistic disagreements between the classifier and Wikidata (i.e., the number of detected metalinguistic disagreements divided by the number of evaluated alignments) ranged between 0.04 and 0.264 with a mean of 0.097. To illustrate the nature of disagreements detected, Table 2 shows two examples of false negatives from the experiment which exhibit metalinguistic disagreement. The complete set of rationales and classifications is available in the Github repository."}, {"title": "3. Proposed benchmark", "content": "We argue that the above results suggest that that metalinguistic disagreement between knowledge graphs and LLMs can occur during fact-checking tasks. However, there are some significant shortcomings in the above approach:\n\u2022\tLack of human validation. The detection of metalinguistic disagreement relies on using an LLM-as-a-judge, which may not be a reliable substitute for human judgment [26, 27]. This introduces the possibility that the detected \"disagreements\" are artifacts of how different LLMs process and generate language, rather than true metalinguistic disagreements. Human review at scale is needed to validate the results. Without this, it's difficult to determine if what the LLMs identify as metalinguistic disagreements align with human judgments.\n\u2022\tPossible conflation with other error types. What's interpreted as metalinguistic disagreement could potentially be other types of errors or inconsistencies in LLM outputs, such as hallucinations or context misinterpretations.\n\u2022\tLimited sample size. The experiment uses a relatively small sample of 250 triples. A larger-scale study is needed to draw more robust conclusions.\nWe argue that by creating a benchmark metalinguistic disagreement detection dataset that addresses these limitations, we could more confidently assess the occurrence and nature of metalinguistic dis-agreements in LLM-based fact-checking. This would provide a stronger foundation for investigating our hypothesis and advancing our understanding of how LLMs interpret and disagree about meaning in knowledge graph engineering contexts.\nSpecific requirements for such a benchmark include:\n\u2022\tHuman-annotated examples. A set of fact-checking instances annotated by human experts to identify clear cases of metalinguistic disagreement, factual disagreement, and agreement. This would serve as a gold standard for evaluation.\n\u2022\tInter-annotator agreement metrics. Support the evaluation of system performance using inter-annotator agreement metrics that incorporate knowledge graph ground truth and human annotations to measure the degrees of inter-agent factual and metalinguistic agreement.\n\u2022\tMultiple knowledge graph sources. Use triples from different knowledge graphs spanning multiple knowledge domains to account for variations in how relations and concepts are defined across sources, and to test if metalinguistic disagreements are more prevalent in certain areas.\n\u2022\tContextual information. Provide relevant context for each fact-checking instance, similar to the Wikipedia abstracts used by T-REx.\n\u2022\tExamples with ambiguity, temporal aspects, and gradable predicates. Deliberately include examples with potential for ambiguity or multiple interpretations to probe the boundaries of metalinguistic disagreement, examples where the truth value of a statement might change over time, to explore how temporal context affects metalinguistic understanding, and examples with gradable predicates (e.g., \"tall,\" \"fast\") that might be more prone to metalinguistic disagreement.\n\u2022\tNegative examples. Include clear cases where no metalinguistic disagreement should occur, to test for false positives.\nAs an initial next step towards this objective, we plan to extend the dataset used in the initial experiments described above in a manner similar to that used in the design and implementation of the SHROOM hallucination detection benchmark [6, 28], through crowdsourcing to incorporate human annotation and increasing the size of the sample of knowledge alignments from the T-REx dataset. Human annotators will be presented with a summary of a Wikipedia page and a statement generated from the Wikidata knowledge graph triple for each alignment, and the annotator must indicate if they disagree with the statement, and if so, whether they disagree on the factuality of the statement or the meaning of any of the terms used in the statement.\nIn conclusion, we anticipate that such a benchmark can not only shed light on the nature and frequency of metalinguistic disagreements between LLMs and KGs, but also contribute to the ongoing debate about LLMs' capacity for generating meaningful statements. Some have argued that LLMs are incapable of understanding meaning in the way humans do [29]. Others are exploring ways in which LLMs might be capable of at least some limited or partial forms of meaning as a consequence of either the model's pre-training or its grounding through in-context learning [30, 31, 32, 33, 34]. We believe that the proposed benchmark can contribute to a more nuanced view of the epistemic status of LLMs relative to KGs based on two-component semantics [35, 36, 37], and support experimental work in determining whether or not LLMs can generate meaningful statements or be claimed to have beliefs [38, 39]."}]}