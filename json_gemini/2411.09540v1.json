{"title": "PROMPTING THE UNSEEN: DETECTING HIDDEN BACKDOORS IN BLACK-BOX MODELS", "authors": ["Zi-Xuan, Huang", "Jia-Wei Chen", "Zhi-Peng Zhang", "Chia-Mu Yu"], "abstract": "Visual prompting (VP) is a new technique that adapts well-trained frozen models for source domain tasks to target domain tasks. This study examines VP's benefits for black-box model-level backdoor detection. The visual prompt in VP maps class subspaces between source and target domains. We identify a misalignment, termed class subspace inconsistency, between clean and poisoned datasets. Based on this, we introduce BPROM, a black-box model-level detection method to identify backdoors in suspicious models, if any. BPROM leverages the low classification accuracy of prompted models when backdoors are present. Extensive experiments confirm BPROM's effectiveness.", "sections": [{"title": "INTRODUCTION", "content": "Deep neural networks (DNNs) are commonly used in complex applications but require extensive computational power, leading to significant costs. Users often access these models through online platforms like BigML model market\u00b9 and ONNX zoo\u00b2, or via Machine Learning as a Service (MLaaS) platforms. However, DNNs can include backdoors (Gu et al., 2017; Liu et al., 2018b; Tang et al., 2021; Qi et al., 2023b; Nguyen & Tran, 2021; Chen et al., 2017), which manipulate model responses to inputs with specific triggers (like certain pixel patterns) while functioning correctly on other inputs. In backdoor attacks, attackers embed these triggers in the training data, leading the model to associate the trigger with a particular outcome and misclassify inputs containing it.\nWhy Black-Box Model-Level Detection. Black-box backdoor detection, which uses only black-box queries to the suspicious model (i.e., the model to be inspected), is gaining attention. This detection method is divided into input-level (Li et al., 2021c; Qiu et al., 2021; Gao et al., 2022; Liu et al., 2023; Qi et al., 2023c; Zeng et al., 2023; Guo et al., 2023; Hou et al., 2024; Xu et al., 2024; Mo et al., 2024) and model-level (Huang et al., 2020; Dong et al., 2021; Guo et al., 2022; Xu et al., 2019; Wang et al., 2024) techniques. Input-level detection identifies trigger samples in an infected model, while model-level detection determines if a model contains backdoors. Input-level detection relies on the model having backdoors; otherwise, its accuracy drops significantly. For example, as shown in Table 1, TeCo (Liu et al., 2023) and SCALE-UP (Guo et al., 2023), state-of-the-art input-level detectors, show AUROCs of 0.8113 and 0.7877, respectively, on a BadNets-infected model (Gu et al., 2017), but only 0.4509 and 0.5103 on a clean model. If a model is clean, many legitimate samples may be misclassified as triggers, reducing the model's practical utility. Thus, model-level detection should be performed first. If backdoors are found but the model must still be used, input-level detection should then be applied to each input.\nDesign Challenge. Despite its importance, black-box model-level detection faces two main challenges. First, unlike input-level detection, which benefits from the presence of an infected model, model-level detection has limited ground truth, relying on only a few clean samples. Second, it needs a stable feature to differentiate between clean and infected models across various backdoor types, which is difficult to find. For instance, B3D (Dong et al., 2021) targets trigger localization but is mainly effective for patch-based triggers. Similarly, AEVA (Guo et al., 2022) may struggle with larger triggers due to its dependence on adversarial peak analysis."}, {"title": "BACKGROUND KNOWLEDGE", "content": "Both visual prompting (VP) (Chen et al., 2023; Bahng et al., 2022; Jia et al., 2022) and model reprogramming (MR) (Tsai et al., 2020; Chen, 2024; Elsayed et al., 2019; Neekhara et al., 2022) enable a frozen pre-trained model for one task to perform a different target domain classification task by deriving a visual prompt for inputs from the target domain. Initially, MR was considered an attack that misused cloud services (i.e., MLaaS) to perform undocumented tasks (Elsayed et al., 2019). VP was recently introduced in (Bahng et al., 2022). Although VP and MR share the same concept, VP focuses exclusively on images. VP has been extended to image inpainting (Bar et al., 2022), antibody sequence infilling (Melnyk et al., 2023), and differentially private classifiers (Li et al., 2023b). In this paper, VP and MR are used interchangeably, with the visual prompt in VP corresponding to the trainable noise in MR. More formally, VP/MR proceeds with four steps (Chen, 2024).\n1. Initialization: Let $f_s(\\cdot)$ and $D_T = \\{(x_T, y_T)\\}$ be the source model (the model trained from the source domain dataset) and the target domain dataset, respectively. Randomly initialize $\\theta$ and $w$ (defined below).\n2. Visual prompt padding: Obtain the prompted input sample $\\hat{x_T} = V(x_T|\\theta)$, where $\\theta$ is the visual prompt. A common method for $V(\\cdot)$ is to resize $x_T$ and add the visual prompt (trainable noise) around it. Although $\\hat{x_T}$ visually differs from the source domain, it can still be used as input for the source domain classifier. Figure 1a illustrates this with $x_T$ as \"3\" from MNIST, $\\Theta$ in the middle, and $V(.)$ resizing $x_T$ and padding it with 0.\n3. Output mapping: Obtain the target task prediction via $\\hat{y_T} = \\O(f_s(\\hat{x_T})|w)$, where $w$ represents the trainable parameters for output label mapping. This step is optional for VP/MR. In our experiment, we omitted this step.\n4. Prompted model training: Optimize $\\theta$ and $w$ by minimizing a task-specific loss $L(\\hat{y_T}, y_T)$ on $D_T$.\nAfter executing the four-step procedure, we obtain the prompted model $f_T = \\O \\circ f_s \\circ V$ from $f_s(\\cdot)$ with optimized $\\theta^*$ and optionally $w^*$. This results in $\\hat{y_T} = \\O(f_s(V(x_T|\\theta^*))|w^*)$."}, {"title": "SYSTEM MODEL", "content": "Threat Model. We consider two roles: attacker and defender. The attacker's goal aligns with previous work (Gu et al., 2017; Chen et al., 2017; Tang et al., 2021; Qi et al., 2023b; Liu et al., 2018b). Specifically, the attacker poisons the training dataset by injecting trigger samples. The DNN model (e.g., an image classifier) trained on this poisoned dataset behaves normally with clean inputs but always predicts an attacker-specified target class for inputs with a trigger. Essentially, an all-to-one backdoor is implanted, mapping all trigger inputs to a specific target class.\nDefender's Goal and Capability. The defender's goal is to detect if a suspicious model is backdoored, primarily measured by AUROC (see Section 6). The defender has limited abilities: no access to the poisoned dataset, model structure, or parameters. In MLaaS applications, detection involves only black-box queries on the model to obtain confidence vectors. The defender also has a small reserved clean dataset $D_S$ (1%, 5%, 10% of the test dataset in our experiment) to aid detection."}, {"title": "PROPOSED METHOD", "content": "We present our detection method, BPROM. The notation table can be found in Table 27 in Appendix E."}, {"title": "OVERVIEW", "content": "Different clean datasets have distinct class subspace \"shapes\" in feature space. However, as noted in Wang et al. (2019), poisoned datasets exhibit target class subspaces that share boundaries with all others. This creates misalignment when adapting a poisoned model to a clean dataset, termed class subspace inconsistency, resulting in reduced prompted model accuracy. This is conceptually illustrated in Figure 2 and experimentally validated in Section C and Figure 3. As an evidence, Table 2 also shows that an increasing number of target classes worsens the inconsistency (i.e., lower accuracy). BPROM leverages this for backdoor detection. The core idea is that adapting an infected source model to a clean target task via visual prompting is significantly harder due to the class subspace mismatch. Theorem 1 in Yang et al. (2021) states that target risk is bounded by source risk and representation alignment loss. For BPROM, this alignment loss is amplified by the inconsistency in infected models, leading to poor target task performance. Thus, low prompted accuracy signals potential backdoors. To achieve effective detection, the BPROM training has three steps: shadow model generation, prompting, and meta-model training. First, diverse poisoned and clean shadow models are trained. Second, visual prompts are learned for each shadow model using an external clean dataset. Finally, a meta-classifier is trained on confidence vectors from prompted shadow models to detect backdoors. The workflow and pseudocode are shown in Figure 4 and Algorithm 1."}, {"title": "BPROM", "content": "Generating Shadow Models. The goal of this step is to construct shadow models, categorized into clean and backdoor shadow models. Clean shadow models are trained on a clean dataset, while backdoor shadow models are trained on a poisoned dataset.\nLet $D_S$ be the reserved clean dataset. To check if a suspicious model was trained on CIFAR-10, $D_S$ includes a limited number of CIFAR-10 samples (e.g., 1%, 5%, 10% in our experiment). The defender trains n clean shadow models, $f_i$'s, with different parameter initializations. Given a poisoning rate p and a chosen backdoor attack, the defender creates M n poisoned datasets by injecting trigger samples according to the chosen attacks, where M is the total number of shadow models. Specifically, each poisoned dataset $D_P$ is constructed as follows:\nStep 1: A proportion p of samples $(x, y)$ from the clean dataset $D_S$ are extracted to form $D_E$.\nStep 2: The extracted samples are transformed by adding a trigger pattern $(m, t, a, y_t)$ to obtain poisoned counterparts $\\{(x', y')|x' = (1 - m) \\cdot x + m \\cdot ((1 - a)t + ax), y' = y_t\\}$, where $y_t, m, t, a$, denote the target class, trigger mask, trigger, intensity, and element-wise product, respectively (Guo et al., 2022; 2023)."}, {"title": null, "content": "Step 3: Construct $D_P = (D_S \\backslash D_E) \\cup \\{(x', y')\\}$. By sampling different combinations of backdoor patterns $(m, t, a, y_t)$, various $D_P$ can be generated. Backdoor shadow models are trained on $D_P$'s.\nPrompting Shadow Models. This step applies VP to both types of shadow models (clean and poisoned) to generate prompted shadow models. Let $D_T = D_{\\text{train}} \\cup D_{\\text{test}}$ be an external clean dataset, with $D_{\\text{train}}$ as the training set and $D_{\\text{test}}$ as the test set. $D_T$ can have a different distribution than $D_S$. For shadow models, prompts $(\\theta_i)$ are learned via standard backpropagation on $D_{\\text{train}}$. This process is also applied to the suspicious model $f_{\\text{sus}}$, but using a gradient-free optimization method (e.g., CMA-ES) since we only have black-box access. This results in the prompted shadow models $\\hat{f_i}(\\cdot) = f_i(V(\\cdot | \\theta_i))$, and prompted suspicious model $\\hat{f_{\\text{sus}}}(\\cdot) = f_{\\text{sus}}(V(\\cdot | \\theta_{\\text{sus}}^*))$. Detailed steps for VP can be found in Section 3 (e.g., (Bahng et al., 2022)).\nMeta Model Training. The goal of this step is to train a binary classifier $f_{\\text{meta}}$ for backdoor detection. For each shadow model $f_i$, the defender randomly selects q samples from $D_{\\text{test}}$ to form $D_Q = \\{x_1, ..., x_q\\}$. Each sample from $D_Q$ is fed to $f_i$. The defender creates a dataset $D_{\\text{meta}} = \\{(f_i(\\mathbf{x_1})||...||f_i(\\mathbf{x_q}), clean)\\}_{i=1}^n \\cup \\{(f_i(\\mathbf{x_1})||...||f_i(\\mathbf{x_q}), backdoor)\\}_{i=n+1}^M$. Here, $f_i(\\mathbf{x_i})$ is the confidence vector, and its length, $K_S$, is the number of classes in $D_S$. The defender then trains a binary classifier $f_{\\text{meta}}$ using $D_{\\text{meta}}$.\nBackdoor Detection on Suspicious Model. To inspect a suspicious model $f_{\\text{sus}}$, we first obtain q confidence vectors from the prompted suspicious model $\\hat{f_{\\text{sus}}}$. These vectors are concatenated and fed to $f_{\\text{meta}}$. Specifically, $v = (\\hat{f_{\\text{sus}}}(\\mathbf{x_1})||...||\\hat{f_{\\text{sus}}}(\\mathbf{x_q}))$ is computed and input to $f_{\\text{meta}}$, which outputs either clean or backdoor."}, {"title": "DISCUSSION", "content": "BPROM is similar to MNTD (Xu et al., 2019), but they have important differences.\nMore Efficient Data Generation: In BPROM, the defender uses a single backdoor attack to generate $D_P$, whereas MNTD uses multiple backdoor attacks. Even if multiple methods are used in BPROM, detection accuracy improves only marginally. MNTD needs to \"see\" various backdoor types to better detect unknown backdoors. However, BPROM focuses on class subspace inconsistency, where $D_P$ learns different feature space partitions, with the target class adjacent to all other classes.\nMuch Fewer Shadow Models Required: BPROM needs only a few shadow models (e.g., 20 in our experiments), while MNTD requires hundreds due to the variety of backdoor attacks (e.g., 256 in MNTD). This reduces training costs and allows BPROM to achieve high performance (1.0 AUROC on CIFAR-10 for both BadNets and Blend, compared to MNTD's 0.92 and 0.955) even with a single backdoor type. Training MNTD is also much more complex than training BPROM.\nNovel Design Principle: Most importantly, their design principles differ fundamentally. MNTD relies on meta-learning and needs to \"see\" various backdoor properties. BPROM relies on class subspace inconsistency, achieving decent detection accuracy (e.g., 0.8137 F1-score on CIFAR-10 with BadNets and STL-10, and 0.7499 with GTSRB and STL-10) even with a single shadow model"}, {"title": "EXPERIMENTS", "content": "We overview the experimental setup, including datasets, model architectures, attack methods, and defense baselines, consistent with recent works (Qi et al., 2023c; Guo et al., 2023; Huang et al., 2020; Liu et al., 2023). We then present the experimental results and hyperparameter study."}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets and Model Architectures. We use five image datasets: CIFAR-10 (Krizhevsky, 2009), GTSRB (Stallkamp et al., 2011), and STL-10 (Coates et al., 2011), Tiny-ImageNet (Le & Yang, 2015), and ImageNet (Russakovsky et al., 2015). For a suspicious model $f_S(\\cdot)$ trained on CIFAR-10, GTSRB, Tiny-ImageNet or ImageNet, we first train shadow models $f_i$'s using an $\\alpha$% ($\\alpha \\in \\{1, 5, 10\\}$) subset of the corresponding test set as $D_S$. Then, we apply VP on $f_i$'s using STL-10 as $D_T$ to obtain the corresponding prompted models $\\hat{f_i}$'s. We experiment with ResNet18 and MobileNetV2 architectures, training models on each $D_S$ and $D_{\\text{train}}$ using standard procedures. For the meta-classifier $f_{\\text{meta}}$, we use a random forest with 10,000 trees to detect backdoors based on confidence vectors. We mainly use Area Under the ROC Curve (AUROC) and F1-score to measure the detection effectiveness of backdoor detection methods. Our experiments were performed on a workstation equipped with a 16-core Intel i9 CPU (64GB RAM) and an RTX4090 GPU.\nAttack Methods and Defense Baselines. We evaluate BPROM against 9 backdoor poisoning attacks from the Backdoor Toolbox\u00b3, including classical dirty label, clean label, sample-specific trigger, and adaptive attacks. Default hyperparameters are used to ensure at least 98% attack success rate. We compare BPROM with 10 backdoor defenses either from Backdoor Toolbox or from their official code. Default hyperparameters are used for each defense."}, {"title": "EXPERIMENTAL RESULT", "content": "We know from class subspace inconsistency that a prompted model's accuracy degrades if the suspicious model is backdoored. We conducted experiments with backdoor attacks using varying trigger sizes (4 \u00d7 4, 8 \u00d7 8, 16 \u00d7 16 pixels) and poisoning rates (5%, 10%, 20% of training data) to further examine the impact of class subspace inconsistency on prompted model accuracy. For each experiment, we generated a backdoor-infected model and prompted it for a new task on STL-10. These experiments also cover adaptive attacks, where BPROM maintains high performance, achieving an AUROC of 1 even at low poison rates (e.g., 0.2% for BadNets on CIFAR-10; see Section 6.4).\nTrigger Size Impact. Table 3 shows the accuracy of prompted models on STL-10 with varying trigger sizes. We trained backdoored models on CIFAR-10 and GTSRB, then prompted them to classify STL-10. As trigger size increases, accuracy decreases. This is because larger triggers distort feature representations more, worsening class subspace inconsistency.\nPoison Rate Impact. Table 4 shows the accuracy of prompted models with varying poison rates. Similar to the trigger size experiments, we trained backdoored models on CIFAR-10 and GTSRB, then prompted them for STL-10. Higher poison rates lead to lower accuracy due to increased feature distortion, consistent with our class subspace inconsistency explanation."}, {"title": "HYPERPARAMETER STUDY", "content": "We conduct hyperparameter studies to analyze key factors affecting BPROM's effectiveness.\nImpact of Number of Shadow Models. Table 7 shows AUROC as we vary the number of shadow models used to train the backdoor classifier. In the table, \"2 (1+1)\" means one clean and one backdoor shadow model. The F1 score increases rapidly with more shadow models but plateaus after about 20 models. This indicates that approximately 20 shadow models are sufficient for effective training, with minimal AUROC improvement beyond this number.\nImpact of Trigger Size and Poison Rate. We analyze how detection performance (AUROC) changes with varying trigger size and poison rate. The settings in Tables 8 and 9 match those in Tables 3 and 4, which show the prompted model accuracy for different trigger sizes and poison rates. Tables 8 and 9 show both attack success rate (ASR) and AUROC for CIFAR-10 models as trigger size and poison rate vary.\nWe observe two key points: 1) ASR increases with larger trigger sizes and poison rates, indi"}, {"title": null, "content": "cating stronger backdoor attacks. 2) Despite stronger attacks, our detection method's AUROC remains stable, with minor fluctuations. GTSRB results show similar trends: as trigger size increases from 4x4 to 16x16, ASR rises from 26% to 99%, while AUROC stays between 0.98 and 1.00. This demonstrates that our backdoor detection technique remains reliable even as attacks strengthen, highlighting its robustness against varying attack strengths.\nStructural Differences between Shadow and Suspicious Models. We analyze the impact of using different architectures for shadow and suspicious models on BPROM's performance. Table 10 shows AUROC results with MobileNetV2 as the suspicious model and ResNet18 as the shadow model, indicating that BPROM's detection effectiveness remains robust despite structural differences.\nImpact of External Dataset. We ran additional experiments with $D_S$ as CIFAR-10/GTSRB and $D_T$ changed to SVHN. Results in Tables 19 and 20 of Section B.2 show consistent detection performance.\nImpact of the Inconsistency between Numbers of classes in $D_S$ and $D_T$. In previous experiments, we used CIFAR-10 and GTSRB as $D_S$ and STL-10 as $D_T$, maintaining class consistency between $D_S$ and $D_T$. We also ran experiments with $D_T$ as STL-10 and $D_S$ as CIFAR-100. The results in Table 21 of Section B.2 still show consistent detection performance."}, {"title": "ADAPTIVE ATTACK", "content": "To evaluate BPROM's robustness against adaptive attacks, we followed the experimental setup described in Guo et al. (2023) (Section 5.3.2), focusing on BadNets attacks on CIFAR-10. It remains unknown how an attacker adds a regularization term to reduce class subspace inconsistency. We examine two candidate adaptive attacks below.\nFirst, as shown in Qi et al. (2023b), the backdoor with a very low poison rate can act as an adaptive attack. Table 11 presents the AUROC and ASR of BPROM at various poison rates. These results show that BPROM maintains perfect detection (AUROC = 1) even at extremely low poison rates, demonstrating its effectiveness against stealthy adaptive attacks. Our observed ASR values for BadNets at 0.2% and 0.5% poison rates align with those reported in Figure 7b of Guo et al. (2023), validating the correctness of our implementation.\nClean-label backdoors, like SIG (Barni et al., 2019) and LC (Turner et al., 2019) can also be regarded as a different adaptive attack. These attacks do not modify labels and only poison a portion of the training images, potentially pre serving class subspaces and hindering BPROM's detection based on class subspace inconsistency. BPROM. Table 12 shows BPROM's performance on SIG and LC. While not perfect, BPROM still achieves decent AUROC, indicating its resilience even against these challenging attacks."}, {"title": "CONCLUSION AND LIMITATION", "content": "We present BPROM as a novel VP-based black-box model-level backdoor detection method. BPROM relies on class subspace inconsistency, where the prompted model's accuracy degrades if the source"}, {"title": null, "content": "model is backdoored. This inconsistency is common in various backdoor attacks due to feature space distortion from the poisoned dataset. Our experiments show BPROM effectively detects all-to-one backdoors. However, it struggles with all-to-all backdoors, as their feature space distortion is more controllable by the attacker. Addressing this limitation is left for future work."}, {"title": "ATTACK CONFIGURATIONS", "content": "The configurations of the baseline attacks used in our experiments are summarized in Table 13. For each attack, we specify parameters related to the backdoor trigger insertion, including poison rate and cover rate.\n\u2022 Poison rate: The proportion of training data with the trigger pattern. A higher poison rate increases the attacker's influence on the model's behavior but also raises the detection risk.\n\u2022 Cover rate: The proportion of data with the trigger pattern that shares the original label. A higher cover rate makes the trigger pattern more stealthy and consistent with the original data distribution but weakens the attack.\nAll attacks are implemented using the default settings in the Backdoor Toolbox\u2074; refer to the code repository for more details."}, {"title": "DEFENSE CONFIGURATIONS", "content": "The important settings used for baseline defenses in our evaluations are summarized below:\n\u2022 STRIP (Gao et al., 2019): Number of superimposing images = 10; defense false positive rate budget = 10%.\n\u2022 AC (Chen et al., 2018): Cluster threshold = 35% of class size.\n\u2022 Frequency (Zeng et al., 2021): Predicts samples as poisoned or clean using a pretrained binary classifier.\n\u2022 SentiNet (Chou et al., 2018): FPR = 5%, number of high activation pixels = top 15%.\n\u2022 CT (Qi et al., 2023c): Confusion iterations = 6000; confusion factor = 20.\n\u2022 SS (Tran et al., 2018): Number of removed samples = min(1.5 \u00d7 $|D_{\\text{poison}}|/|D|$, 0.5 \u00d7 class size)."}, {"title": "EVALUATIONS ON DIFFERENT ARCHITECTURES AND DATASETS", "content": "To evaluate the effectiveness of BPROM on different architectures, we conducted experiments using ResNet (He et al., 2015) and MobileNetV2 (Sandler et al., 2018) as backbone models.. The models are trained on the CIFAR-10 (Krizhevsky, 2009) and GTSRB (Stallkamp et al., 2011) datasets, attacked with 9 different backdoor attacks, and then defended with state-of-the-art methods."}, {"title": "ACCURACY AND ATTACK SUCCESS RATE", "content": "We report the clean accuracy (ACC) of the infected models on benign test samples without triggers and the attack success rate (ASR), which indicates the percentage of Trojan inputs successfully predicted as the attacker-specified target class. The results are shown in Table 14 for ResNet18 and Table 15 for MobileNetV2."}, {"title": "AUROC AND F1 SCORE", "content": "We evaluate defense methods in detecting backdoor attacks using AUROC and F1 score metrics. Experiments are conducted on CIFAR-10 and GTSRB datasets using ResNet18 and MobileNetV2 architectures to assess and compare detection effectiveness across different model designs. This allows for determining the robustness and architecture-agnostic capability of techniques.\nExperiments on ResNet18. From the AUROC results in Table 25 and F1 scores in Table 16 of defenses evaluated on the ResNet18 model, we observe that BPROM demonstrates competitive or superior detection performance over defenses for the majority of attacks. It also significantly elevates the average AUROC and F1 score over the strongest baselines. Although it exhibits relatively lower scores on two attacks, BPROM still demonstrates detection capability on par with or better than other methods."}, {"title": "EXPERIMENTS ON MOBILE", "content": "Experiments on MobileNetV2. We further evaluate the effectiveness of backdoor detection methods when using the MobileNetV2 architecture, which utilizes depth-separable convolutions to build a lightweight model. This represents a different design choice than ResNet, which uses residual connections to train deeper models. As shown in Table 17 and Table 18, we observe consistently outstanding detection effectiveness of BPROM over defenses."}, {"title": "Experiments on extra external dataset", "content": "Experiments on extra external dataset. We ran extra experiments, where $D_S$ is kept as CIFAR-10/GTSRB, but $D_T$ is changed to SVHN. Table 19 shows the results when $D_S$ is GTSRB and Table 20 shows the results when $D_S$ is CIFAR-10. Both results demonstrate consistent detection performance of BPROM even when using a different external dataset $D_T$. This indicates that the choice of external dataset does not significantly impact BPROM's effectiveness."}, {"title": "Experiments on CIFAR-100", "content": "Experiments on CIFAR-100. To investigate the impact of inconsistency between the numbers of classes in $D_S$ and $D_T$, we conducted experiments using CIFAR-100 as $D_S$ and STL-10 as $D_T$. Table 21 shows that BPROM achieves high AUROC and F1 scores across various backdoor attacks, demonstrating its robustness even when there is a significant mismatch in the number of classes (100 classes in $D_S$ vs. 10 classes in $D_T$). This suggests that BPROM is capable of handling scenarios where the source and target domains have different numbers of classes, making it a versatile detection method."}, {"title": "Experiments on feature-based backdoors", "content": "Experiments on feature-based backdoors. We further evaluated BPROM's performance on feature-based backdoors, which manipulate the model's feature representations instead of directly modifying"}, {"title": "Impact of Reserved Clean Dataset Size", "content": "Impact of Reserved Clean Dataset Size. We analyze the impact of the reserved clean dataset size ($D_S$) on BPROM's performance. As shown in Table 23, BPROM maintains high AUROC across different $D_S$ sizes (1%, 5%, and 10% of the CIFAR-10 and GTSRB test sets). Even with a limited $D_S$ (1%), BPROM achieves competitive performance, demonstrating its efficiency in leveraging small amounts of clean data. This robustness to $D_S$ size makes BPROM practical for real-world scenarios where clean data might be scarce."}, {"title": "PERFORMANCE ON MOBILEVIT AND SWIM TRANSFORMER", "content": "PERFORMANCE ON MOBILEVIT AND SWIM TRANSFORMER\nTo demonstrate BPROM's architecture-agnostic nature, we evaluated its performance on MobileViT and Swim Transformer, models combining CNN and transformer components. Tables 24 and 25 present the AUROC scores on CIFAR-10 and GTSRB across various backdoor attacks. The results show that BPROM maintains competitive performance on both MobileViT and Swim Transformer, indicating its effectiveness is not limited to ResNet-based architectures. The average AUROC is calculated for each defense and dataset."}, {"title": "ANOTHER VISUALIZATION OF CLASS SUBSPACE INCONSISTENCY", "content": "Figure 5a illustrates, using principal component analysis (PCA), 30 suspicious models (15 clean and 15 backdoor) trained on the complete CIFAR-10 dataset, along with 40 shadow models (20 clean-shadow and 20 backdoor-shadow) trained on 10% of the CIFAR-10 test set. All models are based on ResNet18, with the Trojan method Liu et al. (2018b) employed as the backdoor technique. Subsequently, a random forest-based meta-model (binary classifier) with 50 estimators is trained on the confidence vectors produced by the 40 shadow models. A distinct separation between clean (green"}, {"title": "NOTATION AND DEFINITIONS", "content": "For clarity and reproducibility, Table 27 summarizes the notation and definitions used throughout the paper."}]}