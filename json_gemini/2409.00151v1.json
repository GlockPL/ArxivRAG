{"title": "SPEAKER TAGGING CORRECTION WITH NON-AUTOREGRESSIVE LANGUAGE MODELS", "authors": ["Grigor Kirakosyan", "Davit Karamyan"], "abstract": "Speech applications dealing with conversations require not only recognizing the spoken words but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. In practical settings, speaker diarization systems can experience significant degradation in performance due to a variety of factors, including uniform segmentation with a high temporal resolution, inaccurate word timestamps, incorrect clustering and estimation of speaker numbers, as well as background noise.\nTherefore, it is important to automatically detect errors and make corrections if possible. We used a second-pass speaker tagging correction system based on a non-autoregressive language model to correct mistakes in words placed at the borders of sentences spoken by different speakers. We first show that the employed error correction approach leads to reductions in word diarization error rate (WDER) on two datasets: TAL and test set of Fisher. Additionally, we evaluated our system in the Post-ASR Speaker Tagging Correction challenge and observed significant improvements in cpWER compared to baseline methods.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech recognition systems have advanced significantly in the past decade. Still, even with these remarkable advances, machines have difficulties understanding natural conversations with multiple speakers, such as in broadcast interviews, meetings, telephone calls, videos or medical recordings. One of the first steps in understanding natural conversations is to recognize the words spoken and their corresponding speakers. SD determines \"who spoke when\" in multi-speaker audio and is a crucial part of the speech translation system. SD is used in conjunction with ASR to assign a speaker label to each transcribed word and has widespread applications in generating meeting/interview transcripts, medical notes, automated subtitling and dubbing, downstream speaker analytics, among others. Usually, this is done in multiple steps that include (1) transcribing the words using an ASR system, (2) predicting \"who spoke when\" using a speaker diarization system, and, finally, (3) reconciling the output of those two systems [1]. A typical reconciliation algorithm works as follows: (1) If the word segment overlaps with at least one speaker segment, then this word is associated with the speaker that has the biggest temporal overlap with this word; (2) otherwise, if this word segment does not overlap with any speaker segment, then it is associated with the speaker that has the smallest temporal distance to this word based on the segment boundaries [2].\nSpeaker diarization systems often face numerous challenges that can lead to subpar performance, negatively impacting the user's perception of transcript quality. However, some of these errors can be mitigated through post-correction techniques. In this work, we first analyze the mistakes made during reconciliation and categorize them. We then implement a speaker error correction module to rectify inaccuracies, particularly for boundary words between sentences spoken by different speakers."}, {"title": "2. RELATED WORK", "content": "We are inspired by the work in [1], where the authors introduced the speaker error corrector (SEC). SEC corrects speaker errors at the word level without modifying the underlying ASR or acoustic SD systems. In [1], word embeddings from the ASR transcript are extracted using a pre-trained ROBERTa-base language model (LM) [3]. These embeddings, along with the hypothesized speaker labels, are fed into a separately trained transformer encoder, which produces the corrected speaker labels. The transformer encoder is trained on both simulated diarization errors and real data.\nIn [2], the authors proposed DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. In this framework, the outputs of the ASR and SD systems are represented in a compact textual format and included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components.\nMore recently, in [4], the authors suggested using LLM to predict the speaker probability for the next word and incorporating this probability into the beam search decoding of speaker diarization. In this approach, prompting is implemented word-by-word, unlike in [2], where a single prompt is used to post-process the entire speaker diarization results. One drawback of this proposed approach is that it requires word-level speaker probabilities for beam search decoding, which may be absent in some SD systems."}, {"title": "3. CLASSIFYING DIARIZATION ERRORS", "content": "A typical method for assessing traditional speaker diarization systems is the diarization error rate (DER). This is calculated by adding together three types of errors: false alarms, missed detections, and speaker confusion errors. Essentially, DER compares the reference speaker labels with the predicted speaker segments in the time domain. On the other hand, the use of a joint ASR and SD system directly assign speakers to recognized words, eliminating the need to rely on time boundaries. In [5], the authors proposed a new metric, word diarization error rate (WDER), to evaluate such joint ASR and SD systems, by measuring the percentage of words in the transcript that are tagged with the wrong speaker:\n$WDER= \\frac{S_{IS} + C_{IS}}{S+C}$\nwhere $S_{IS}$ represents the number of ASR substitutions with incorrect speaker tags, $C_{IS}$ represents the number of correctly recognized ASR words with incorrect speaker tags, S is the total number of ASR substitutions and C is the total number of correctly recognized ASR words. WDER doesn't take into account deletion and insertion errors as the speaker tags associated with them cannot be mapped to reference without ambiguity.\nOne benefit of WDER is that it can be used to automatically identify and visualize diarization errors at the word level. By examining errors at the word level, it is possible to categorize them into three categories:\n(a) Incorrect speaker tags within a paragraph\n(b) The first and last words of a paragraph having incorrect speaker tags\n(c) A complete paragraph being assigned to the wrong speaker\nThe main cause of errors of type (a) and (b) is the use of uniform audio segmentation with a high temporal resolution. Inaccurate word timestamps can also lead to type (b) errors. Type (c) errors typically occur due to inaccurate estimation of the number of speakers and incorrect clustering. Background noise, music and reverberation also contribute to all types of errors."}, {"title": "4. SPEAKER ERROR CORRECTOR", "content": "We use the lexical speaker error corrector introduced in [1], which aims to improve diarization accuracy by leveraging lexical information. In this approach, word embeddings are extracted using a pre-trained RoBERTa-base LM. These embeddings, along with the hypothesized speaker labels, are fed into a transformer encoder, which produces the corrected speaker labels.\nIn contrast to the original implementation, we use an ALBERT-base LM [6] due to its memory efficiency. Additionally, our error simulation procedure differs from the original work, where the target words are substituted with random words. We have also replaced the standard cross-entropy loss with a permutation invariant loss. The next section will cover all the training details."}, {"title": "4.2. Training details", "content": "We train the SEC on two-speaker scenarios, generating synthetic errors for both words and speaker tags. For word errors, we employ an alternative spelling prediction (ASP) model [7]. It aims to predict how the ASR system might inaccurately recognize a given word without executing the ASR model itself. For speaker tag errors, we simulate errors at speaker change points if the input involves two speakers, as shown in Example 1. If the input contains only one speaker, we simulate errors only at the beginning or at the end of the input, as illustrated in Example 2.\nOur goal is to accurately predict speaker segmentation, even though the concept of speaker ID can sometimes be ambiguous. Consider the motivating example illustrated in Table 2. The model can either correct the first two tags or the last five tags. To handle such cases, we use permutation invariant cross-entropy loss for speaker tag classification, which selects a permutation of speakers that results in the minimum loss."}, {"title": "4.3. Inference setup", "content": "During inference, we perform error correction only at speaker change points. We define a context window around these change points and feed the window, along with the hypothesized speaker tags, into a SEC model. The window consists of up to 18 words from the left context and 18 words from the right context, up to the nearest change points."}, {"title": "5. RESULTS", "content": "In this work, we use the full Fisher [8, 9], DailyDialog [10], and SLT GenSEC Challenge Track-2 training datasets to train the SEC model. We split the Fisher data into training, validation and test sets as defined in [11]. For evaluation, in addition to the Fisher test split, we use the standard test split of the TAL [12] dataset. For internal evaluations and model selection, we report performance using the WDER, as we believe it provides a more accurate representation of a speaker diarization system's performance at the word level compared to the cpWER metric [13, 14]. Our final evaluation for the Post-ASR Speaker Tagging Correction challenge is conducted using the cpWER metric.\nWe use the pre-trained FastConformer-Large model\u00b9 [15] to transcribe test datasets and then diarize them using the Titanet-Small\u00b2 [16] embedding extractor along with the standard spectral clustering [17]. After generating the transcript, we apply our SEC model to it and compare the corrected speaker tags with the ground truth tags using either WDER or cpWER."}, {"title": "5.2. Alternate spelling prediction model", "content": "To train the alternate spelling prediction model, we use roughly 1.15 million word pairs that were mistakenly recognized by a Conformer-medium model [18]. We use a medium-size pre-trained Conformer checkpoint\u00b3 that was made available by Nvidia. Furthermore, we removed error pairs in which the phonetic forms of the reference and predicted words had an edit distance greater than 50%. We used the grapheme-to-phoneme (G2P) library\u2074 to convert words to the corresponding phoneme sequence.\nSimilar to [7], our ASP model is also based on a transformer encoder-decoder framework. It has two layers in both the encoder and decoder with two attention heads per layer and 400 units per layer resulting in a total of 6.5M parameters. However, unlike the original paper, the input and the output subword tokenization is the same as the tokenization used for the ASR model.\nAt inference time, we use beam search to produce a 3-best list of alternate spellings for each word. During the training of the SEC model, we generate ASR errors by replacing the target word with a randomly picked alternate.\nTo test the accuracy of the ASP model, we measure the BLEU score [19] between the word pieces of the reference and predicted alternates."}, {"title": "5.3. SEC system", "content": "We use a pre-trained ALBERT-base model as the backbone LM and a transformer encoder with 128 hidden states. For word error simulation, we either leave the word unchanged or substitute it with a corresponding alternate generated by the ASP model with a probability of 0.1. For speaker errors, we introduce a maximum of two errors: in 40% of inputs, no errors are simulated; in 48% of inputs, a single speaker tag error is generated; and in 12% of inputs, two speaker tag errors are simulated. The model is trained with an average sequence length of 30 words per batch, which was found to be optimal through hyperparameter search in [1]. Initially, we trained only the transformer encoder part of the SEC model. Subsequently, we unfreeze the ALBERT part and train the entire SEC model."}, {"title": "5.4. Results", "content": "From Table 6 and Table 7 we can see that the SEC model consistently outperforms the \"No Correction\u201d baseline across different datasets. For instance, on the Fisher dataset, the SEC model reduces the WDER from 2.8% to 2.42%. Similarly, on the TAL dataset, the WDER decreases from 4.25% to 4.11%. This improvement is also evident in the cpWER metric for the SLT GenSEC Challenge Track-2 datasets, where the SEC model achieves lower error rates on both the dev and eval sets compared to the baseline [4]. Table 5 presents an example case from the TAL testing set, where we see improvements after applying the speaker error correction model.\nOne drawback of our method is that it is only applied to speaker change points. When we apply the model more frequently, we observe performance degradation."}, {"title": "6. CONCLUSIONS", "content": "In this work, we implemented a speaker error correction model to correct word-level speaker errors for boundary words between sentences spoken by different speakers. We achieve this using a language model over the ASR transcriptions to correct the speaker labels. We train the SEC model using only text data by simulating both word errors and speaker errors without the need for any paired audio-text data. For simulating word errors, we train an alternate spelling prediction model that can predict how the ASR will recognize a given word. We achieved an absolute reduction in WDER of over 0.38% and 0.14% across the Fisher test and TAL datasets, respectively. Additionally, we evaluated our system in the Post-ASR Speaker Tagging Correction challenge and observed improvements in cpWER compared to baseline methods. The proposed SEC framework is also lightweight and can be integrated as a post-processing module over existing on-device ASR-SD systems."}]}