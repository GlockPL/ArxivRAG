{"title": "TRANSFORMERS USE CAUSAL WORLD MODELS IN MAZE-SOLVING TASKS", "authors": ["Alex F. Spies", "William Edwards", "Michael Ivanitskiy", "Adrians Skapars", "Tilman R\u00e4uker", "Katsumi Inoue", "Alessandra Russo", "Murray Shanahan"], "abstract": "Recent studies in interpretability have explored the inner workings of transformer models trained on tasks across various domains, often discovering that these networks naturally develop surprisingly structured representations. When such representations comprehensively reflect the task domain's structure, they are commonly referred to as \"World Models\" (WMs). In this work, we discover such WMs in transformers trained on maze tasks. In particular, by employing Sparse Autoencoders (SAEs) and analysing attention patterns, we examine the construction of WMs and demonstrate consistency between the circuit analysis and the SAE feature-based analysis. We intervene upon the isolated features to confirm their causal role and, in doing so, find asymmetries between certain types of interventions. Surprisingly, we find that models are able to reason with respect to a greater number of active features than they see during training, even if attempting to specify these in the input token sequence would lead the model to fail. Futhermore, we observe that varying positional encodings can alter how WMs are encoded in a model's residual stream. By analyzing the causal role of these WMs in a toy domain we hope to make progress toward an understanding of emergent structure in the representations acquired by Transformers, leading to the development of more interpretable and controllable AI systems.", "sections": [{"title": "1 INTRODUCTION", "content": "The study of world models (WMs) in AI systems has gained significant traction of late, yet much interpretability research focuses on large language models trained on diverse, complex datasets (Bel-rose et al., 2023; Lieberum et al., 2023; Olsson et al., 2022). In an attempt to seek a more comprehensive understanding, our work examines WMs acquired by Transformers (Vaswani, 2017) in a controlled, synthetic environment. In particular, we use Maze-solving tasks (Subsection 2.1) as an ideal testbed for understanding learned WMs due to their human-understandable structure, controllable complexity, and relevance to spatial reasoning. Using this constrained domain, we can rigorously analyze how transformers trained (Subsection 2.2) to solve mazes construct and utilize internal representations of their environment.\nOur methodology leverages Sparse Autoencoders (SAEs) (Bricken et al., 2023) to overcome the limitations of linear probes in detecting WM features. While linear probes can and have been used to identify latent directions associated with features defined in an imposed ontology, SAEs are found to discover features actually used by our models to make decisions (Section 3). By manipulating specific features identified by the SAEs and observing the impact on our models' maze-solving be-havior, we provide strong evidence that these features are causally involved in the model's decision-making process (Section 4). This stands in contrast to prior work analyzing the formation of WMs in maze settings where no causal features were able to be isolated (Ivanitskiy et al., 2024).\nOur findings provide important considerations for AI interpretability and alignment. By investigat-ing how transformers form causal WMs even in relatively simple tasks, we hope to provide new avenues for understanding representations and potentially steering behavior in more complex AI systems. This work lays the groundwork for future research into how we might intervene on WMs to better align transformer-based AI systems to desired constraints."}, {"title": "2 PRELIMINARIES", "content": "Though it remains a matter of debate whether Large Language Models (LLMs) construct structured internal models of the real world, we can begin to understand the representations acquired by such models by focusing on \"toy\" tasks with clear spatial or temporal structure (Brinkmann et al., 2024; Momennejad et al., 2024; Jenner et al., 2024; McGrath et al., 2022). Previous works along these lines (Li et al., 2022; Ivanitskiy et al., 2024; Nanda, 2023; Karvonen, 2024; He et al., 2024) have found a variety of both correlational and causal evidence for internal models of the environment within trained transformers. In this work, we utilize maze-dataset (Ivanitskiy et al., 2023), a package providing maze-solving tasks as well as ways of turning these tasks into text representations. In particular, we use a dataset of mazes consisting of up to 7 \u00d7 7 grids, generated via constrained randomized depth first search (see Subsection 2.2) (which produces mazes that are acyclic and thus have a unique solution)."}, {"title": "2.1 ENVIRONMENT", "content": "Though it remains a matter of debate whether Large Language Models (LLMs) construct structured internal models of the real world, we can begin to understand the representations acquired by such models by focusing on \"toy\" tasks with clear spatial or temporal structure (Brinkmann et al., 2024; Momennejad et al., 2024; Jenner et al., 2024; McGrath et al., 2022). Previous works along these lines (Li et al., 2022; Ivanitskiy et al., 2024; Nanda, 2023; Karvonen, 2024; He et al., 2024) have found a variety of both correlational and causal evidence for internal models of the environment within trained transformers. In this work, we utilize maze-dataset (Ivanitskiy et al., 2023), a package providing maze-solving tasks as well as ways of turning these tasks into text representations. In particular, we use a dataset of mazes consisting of up to 7 \u00d7 7 grids, generated via constrained randomized depth first search (see Subsection 2.2) (which produces mazes that are acyclic and thus have a unique solution)."}, {"title": "2.2 MAZE SOLVING TRANSFORMERS", "content": "Utilizing the tokenized representations of mazes provide by the maze-dataset library, a suite of transformer models implemented using TransformerLens ((Nanda & Bloom, 2022)) were trained to predict solution paths in acylic mazes. We performed extensive hyperparameter sweeps (Figure 10) over several variants of the transformer architecture, yielding models with stronger generalization performance than those found by prior work Ivanitskiy et al. (2024).\nTo allow the testing of generalization to large maze size, the models were trained on 5 \u00d7 5 fully-connected and 6 \u00d7 6 sparsely connected mazes, embedded in a 7 \u00d7 7 lattice. This ensured that all coordinate tokens in the 7\u00d77 vocabulary had been seen during training time, such that generalization to 7 \u00d7 7 mazes was conceivable but out-of-distribution during inference. For our experiments, we investigated the two best performing models for each positional embedding (Su et al., 2024) scheme, as shown in Table 1. Note that whilst these models had different numbers of heads, their parameter counts varied only slightly - on account of Stan's use of learned positional embeddings."}, {"title": "3 DISCOVERING WORLD MODELS", "content": "Broadly speaking there are two ways to go about trying to identify internal world models: 1) Assuming the form of the world model and inspecting the transformer with e.g. supervised probes to see if this world model is present ((Nanda, 2023) SELF CITE Workshop proceeding), or 2) Exploring the model internals and investigating any structure which may be present in the representations to see if something akin to a world model exists. In our work we take both approaches."}, {"title": "3.1 WORLD MODEL CONSTRUCTION: CONNECTIVITY ATTENTION HEADS", "content": "We began by examining the attention patterns of the maze-solving transformer models and uncovered a notable pattern: in both models, some or all of the attention heads at the first layer (\"layer 0\") appear to consolidate information about maze connections into the ; context positions. In particular, for all 4ith context-positions tokens (the semicolon separation tokens ;), these heads attend back 1 or 3 tokens - that is, to one of the two coordinate tokens corresponding to the given connection preceeding the ; token. This pattern is observable for 3/8 L0 heads in Stan (Figure 3) and 4/4 L0 heads in Terry (Figure 14). This observation suggests the hypothesis that these heads are in essence constructing a world model for the maze task, for use by later layers.\nIf this were the case, then we should expect that the output of these heads, mediated by the \"OV-Circuit\" (Elhage et al., 2021), should consist of combinations of the coordinates captured in a given connection. This can be measured by taking the $W_{OV}$ matrix for each head and measuring the cosine distance between its elements and the model's token embeddings (where coordinate directions are directly given)\u00b9. With this in mind, we investigated the structure of these vectors more closely. We find an intriguing pattern in the magnitudes of these vectors in the Stan model (Figure 4), while the patterns in Terry were less clear cut (Figure 5)."}, {"title": "3.2 WORLD MODEL REPRESENTATION: SPARSE AUTOENCODERS", "content": "As previous work (Ivanitskiy et al., 2024) struggled to intervene on WM features identified via lin-ear probing (Alain & Bengio, 2016), we trained Sparse Autoencoders to attempt to find disentangled features in our models (Cunningham et al., 2023; Bricken et al., 2023). Sparse Autoencoders are motivated by the notion of superposition (Elhage et al., 2022) which posits that artificial neural networks store more features than an orthogonal representation would allow. By training an autoen-coder with a higher-dimensional latent space than that of the transformer, tasked with reconstructing a residual stream vector under a sparsity penalty, the hope is that the SAE will recover interpretable features which the transformer was forced to superimpose. Similar approaches have previously seen success on other toy tasks (He et al., 2024; Karvonen et al., 2024).\nTo prevent \u201cneuron death\u201d in the SAE latent space, resulting from high sparsity penalties, we apply the method of \u201cGhost Gradients\" proposed by Jermyn & Templeton (2024). The resulting trained SAEs faithfully reconstructed the activations (in our case, the residual stream after L0), and re-placing these activations with their SAE reconstructed counterparts did not affect model behaviour (Figure 8), giving confidence in the completeness of their representation.\nInitial attempts to isolate SAE features corresponding to connections in the maze attempted to use differences in the features present in mazes with or without certain connections. This approach worked well in some cases, but not in others, as not all relevant features varied in magnitude by the same amount, and many features were co-active to a given connection (i.e. those implicated in the path representation, which itself might change when connections are added/removed). To address this, we instead trained decision trees to isolate the relevant features in our transformers (akin to Spies et al. (2022)), as shown in Figure 6."}, {"title": "3.3 COMPARING SAES AND CIRCUITS", "content": "In Subsection 3.1 we advanced the claim that certain LO heads construct features representing maze edges at the context positions, specifically by attending to earlier positions containing maze-cell token embeddings, and rewriting those embeddings by application of their $W_{OV}$ matrices. Subsection 3.2 identified features representing maze edges via an independent line of reasoning, by training SAEs, and identifying which of their features were indicative of the presence of a maze edge.\nTo verify whether these approaches yielded consistent features for the WM, we first calculated the cosine similarity between the features written by isolated attention heads, and those encoded in the SAE (Figure 7a). These showed excellent agreement for Stan, where the attention patterns were clear, but only once the compositional code was taken into account (see Appendix G for details).\nThough these results were promising, we carried out a further comparison (Figure 7b) to minimize the assumptions required, and to account for two potential effects: 1) There may be \"wiggle room\" between feature directions in the model's residual stream, and the circuits that construct them (which would lead to low cosine similarities, even for the same features), 2) As our SAEs are trained after an entire block of computation, it is possible that the MLPs, applied after attention, also played a role in forming the representations. In this second experiment we patched attention head values in the presence of a connection to the mean of a maze set without that connection.By looking at the effect of patching the attention heads on the resulting SAE Latent vectors, we were able to observe that the features considered relevant for any given connection were indeed sensitive to the heads implicated in constructing those features.\nIn particular, we consider the effect on the SAE features identified in Subsection 3.2 when each attention head is patched at the semicolon position for with its average non-connection value across 500 examples (i.e. removing the contribution a given head toward encoding that connection)."}, {"title": "4 INTERVENING ON WORLD MODELS", "content": "Though a universally agreed upon definition does not exist, we shall consider world Models to be \"structure preserving [...] causally efficacious representations\" (Milli\u00e8re & Buckner, 2024) of an environment; i.e. representations which preserve the causal structure of the environment as far as is necessitated by the tasks an agent needs to perform. As such, we are interested in understanding how the WMs we have discovered are leveraged by our models to facilitate generation of valid solution sequences. In Figure 8, we give an example of perturbing a feature to \"fool\" the model into behaving as though it is in a different maze. When patching in the SAE-reconstructed residual stream without perturbations we still see the same behavior as in the original model; when patching in with a modified feature, we see a change in the path. We perform such interventions across 200 examples for each connection feature, and show the resulting intervention efficacies in Figure 9.\nThe intervention process involves toggling a feature on (to the maximal value observed for that feature in a small dataset) or turning it off (setting it to 0) at all semicolon positions\u00b2. We measure the impact of these interventions on the model's maze-solving accuracy, with a particular focus on how activating versus removing features affects performance. Our results reveal an intriguing asymmetry that constitutes our second finding: interventions that activate features tend to be more effective in altering the model's behavior compared to those that remove features."}, {"title": "5 RELATED WORK", "content": "Our work builds on existing literature in interpretability (R\u00e4uker et al., 2023), particularly how trans-formers develop structured internal representations, often called world models. World Models, as defined by Milli\u00e8re & Buckner (2024), are \u201cstructure-preserving, causally efficacious representa-tions of properties of [a model's] input domain.\""}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "In this work, we demonstrated that transformers trained to solve maze navigation tasks form highly structured internal representations that capture the connectivity of the maze and thus act as world models. Through exploratory analysis of attention patterns, we found that connection information was consolidated into semicolon tokens by a subset of attention heads. By using Decision Trees to analyze the latent space of Sparse Autoencoders on these semicolons, we were able to identify sparse features that encoded the position in the maze. We showed that these world models were con-structed differently in transformers leveraging learned vs. rotary positional encodings, suggesting that simpler methods such as activation steering or probing would have been insufficient to extract causal world models in at least some cases. More interesting still, we showed that interventions to add connections by toggling features were consistently more effective than interventions that sought to remove connections by zeroing the corresponding features. Furthermore, we found that models with learned position encodings, which were unable to generalize to longer input sequences (i.e., mazes with more connections), were able to behave consistently if additional connection features were enabled via SAE interventions, even if the corresponding token sequence would have caused the model to fail.\nThese findings shed light on the inner workings of transformers trained on sequential planning tasks and suggest that maze-solving tasks are a rich testbed for understanding the formation of world models in transformers. Future work should aim to uncover whether our findings on intervention asymmetries and steerability are universal - and if not, which conditions give rise to each. An empirical understanding of the reliability of SAE feature discovery and steerability is crucial for AI Safety efforts that attempt to constrain or coerce model behavior through interventions or monitoring based on such methods."}, {"title": "A GENERALIZATION AS A FUNCTION OF INPUT SEQUENCE LENGTH", "content": ""}, {"title": "B SAE TRAINING DETAILS", "content": "To choose optimal hyperparameters for our SAEs we ran a sweep over SAEs at layers 2 to 4 on Terry, finding consistent trends across layers. The results of this sweep are shown in Figure 11, and the final details of the SAE analyzed in the main paper are given in Table 2. We also provide feature density histograms for the SAEs analyzed in the main paper in Figure 12 noting that these look good, in that many features are sparse, but also rather distinct from is typically observed in LLMs. This is not surprising, as our token and features distributions will be very distinct from those of natural language, as most mazes have many active connections, and connections are similarly likely to be present in any given maze."}, {"title": "C FURTHER ATTENTION VISUALIZATIONS", "content": ""}, {"title": "D How SAE REPRESENTATIONS DIFFER", "content": "To complement the intervention results presented in the main text, we also conducted fixed-value interventions on both the Stan and Terry models. In these interventions, instead of calculating new activations based on the modified input, we directly set the activations of the targeted features to fixed values. This approach allows us to examine how the models respond to more controlled ma-nipulations of their internal representations.\nThe fixed-value intervention results shown in Figure 19 reveal interesting patterns that both comple-ment and contrast with the calculated intervention results presented in the main text."}, {"title": "D.1 MAGNITUDE OF INTERVENTIONS", "content": "To complement the intervention results presented in the main text, we also conducted fixed-value interventions on both the Stan and Terry models. In these interventions, instead of calculating new activations based on the modified input, we directly set the activations of the targeted features to fixed values. This approach allows us to examine how the models respond to more controlled ma-nipulations of their internal representations.\nThe fixed-value intervention results shown in Figure 19 reveal interesting patterns that both comple-ment and contrast with the calculated intervention results presented in the main text."}, {"title": "E INVESTIGATION OF QK-CIRCUIT IN STAN MODEL", "content": "In an effort to better understand the notable \"1- and 3-back\" attention patterns appearing in heads L0H3, L0H5 and L0H7 of Stan, described in Subsection 3.1, we investigated the query and key vectors for token and positional embeddings, and their overlaps. The scalar products between queries and keys of token embeddings for L0H3 are shown in figure 20. The most striking feature of this plot is the row corresponding to the query vector of the ; token, and in particular its overlap with the maze cell tokens. Plotting these scalar products on the maze cell grid (figure 21) a clear pattern emerges, analogous to that shown in figure 4, accounting for LH03's tendency to attend to even-parity cells, and LH05's and LH07's tendencies to attend to odd-parity cells. Examining the scalar products among query and key vectors for positional embeddings (figure 22) reveals a pattern that likely accounts for the focusing of attention from; context positions to positions 1 and/or 3 earlier in the context."}, {"title": "G COMPUTING SAE AND OV EDGE FEATURE SIMILARITY", "content": "In Figure 7a we compute the cosine similarity between SAE edge features and OV circuit edge features.\nSAE edge features are formed from a linear combination of the specific edge feature and a \"generic edge\" feature, with the generic feature coefficient of -0.6 being chosen to maximise cosine similar-ity.\nOV edge features are formed from a weighted sum:\n$\\sum a^h_c W^{h,c}_{OV} t_c$\nHere, h indexes heads L0H3, L0H5 and L0H7, with $W^{h,c}_{OV}$ for example, giving the OV matrix of L0H3. c indexes the two cells present in the edge of interest, and tc is the token embedding of a cell c. The coefficients $a^h_c$ are given by the attention directed by head h to cell c from the; context position following the specification of the edge of interest. Data was averaged averaged over 100 examples (see Figure 3 for one such example)."}]}