{"title": "Test-time Adaptation for Regression by Subspace Alignment", "authors": ["Kazuki Adachi", "Shin'ya Yamaguchi", "Atsutoshi Kumagai", "Tomoki Hamagami"], "abstract": "This paper investigates test-time adaptation (TTA) for regression, where a regression model pre-trained in a source domain is adapted to an unknown target distribution with unlabeled target data. Although regression is one of the fundamental tasks in machine learning, most of the existing TTA methods have classification-specific designs, which assume that models output class-categorical predictions, whereas regression models typically output only single scalar values. To enable TTA for regression, we adopt a feature alignment approach, which aligns the feature distributions between the source and target domains to mitigate the domain gap. However, we found that naive feature alignment employed in existing TTA methods for classification is ineffective or even worse for regression because the features are distributed in a small subspace and many of the raw feature dimensions have little significance to the output. For an effective feature alignment in TTA for regression, we propose Significant-subspace Alignment (SSA). SSA consists of two components: subspace detection and dimension weighting. Subspace detection finds the feature subspace that is representative and significant to the output. Then, the feature alignment is performed in the subspace during TTA. Meanwhile, dimension weighting raises the importance of the dimensions of the feature subspace that have greater significance to the output. We experimentally show that SSA outperforms various baselines on real-world datasets.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks have achieved remarkable success in various tasks [10, 23, 33, 35]. In particular, regression, which is one of the fundamental tasks in machine learning, is widely used in practical tasks such as human pose estimation or age prediction [34]. The successes of deep learning have usually relied on the assumption that the training and test datasets are sampled from an i.i.d. distribution. In the real world, however, such an assumption is often invalid since the test data are sampled from distributions different from the training one due to distribution shifts caused by changes in environments. The performance of these models thus deteriorates when a distribution shift occurs [25, 50]. To address this problem, test-time adaptation (TTA) [40] has been studied. TTA aims at adapting a model pre-trained in a source domain (training environment) to the target domain (test environment) with only unlabeled target data. However, most of the existing TTA methods are designed for classification; that is, TTA for regression has not been explored much [40]. Regarding TTA for classification, two main approaches have been explored: entropy minimization and feature alignment.\nThe entropy minimization approach was introduced by Wang et al. [54], and the subsequent methods follow this approach [17, 46, 57, 60, 61]. Although entropy is a promising proxy of the performance on the target domain, entropy minimization is classification-specific because it assumes that a model directly outputs predictive distributions, i.e., a probability for each class. In contrast, typical regression models output only single scalar values, not distributions. Thus, we cannot use the entropy minimization approach for regression models.\nAnother approach, the feature alignment, preliminarily computes the statistics of intermediate features of the source dataset after pre-training in the source domain [1, 16, 27, 29, 31]. Then, upon moving to the target domain, the feature distribution of the target data is aligned with the source distribution by matching the target feature statistics with the pre-computed source ones without accessing the source dataset. Although this approach seems applicable to regression because it allows arbitrary forms of the model output, it assumes to use all feature dimensions to be aligned, and does not sufficiently consider the nature of regression tasks. For instance, regression models trained with standard mean squared error (MSE) loss tend to make features less diverse than classification models do [58]. In particular, we experimentally observed that the features of a trained regression model are distributed in only a small subspace of the entire feature space (Tab. 1). In this sense, naively aligning all feature dimensions makes the performance suboptimal or even be harmful in regression as shown in Tab. 1 because it equally treats important feature dimensions and degenerated unused ones.\nIn this paper, we address TTA for regression on the basis of the feature alignment approach. To resolve the aforementioned feature alignment problem in TTA for regression, we propose Significant-subspace Alignment (SSA). SSA consists of two components: subspace detection and dimension weighting. Subspace detection uses principal component analysis (PCA) to find a subspace of the feature space in which the features are concentrated. This subspace is representative and significant to the model output. Then, we perform feature alignment within this subspace, which improves the effectiveness and stability of TTA by focusing only on valid feature dimensions in the subspace. Further, in regression, a feature vector is finally projected onto a one-dimensional line so as to output a scalar value. Thus, the subspace dimensions that have an effect on the line need a precise feature alignment. To do so, dimension weighting raises the importance of the subspace dimensions with respect to their effect on the output.\nWe conducted experiments on various regression tasks, such as UTKFace [59], Biwi Kinect [18], and California Housing [47]. The results showed that our SSA retains the important feature subspace during TTA and outperforms existing TTA baselines that were originally designed for classification by aligning the feature subspace."}, {"title": "2. Problem Setting", "content": "We consider a setting with a neural network regression model $f_\\theta: \\mathcal{X} \\rightarrow \\mathbb{R}$ pre-trained on a labeled source dataset $S = \\{(\\mathbf{x}, y) \\in \\mathcal{X} \\times \\mathbb{R}\\}_{i=1}^{N_s}$, where $\\mathbf{x}$ and $y$ are an input and its label, and $\\mathcal{X}$ is the input space. Our goal is to adapt $f_\\theta$ to the target domain by using an unlabeled target dataset $T = \\{\\mathbf{x} \\in \\mathcal{X}\\}_{i=1}^{N_t}$ without accessing $S$. Note that the target labels $y \\in \\mathbb{R}$ are not available. In the source dataset $S$, the data $\\{(\\mathbf{x}, y)\\}$ are sampled from the source distribution $p_s$ over $\\mathcal{X} \\times \\mathbb{R}$. In the target dataset $T$, we assume covariate shift [52], which is a distribution shift that often occurs in the real world. In other words, the target data $\\mathbf{x}$ are sampled from the target distribution $p_t$ over $\\mathcal{X}$ that is different from $p_s$, but the predictive distribution is the same, i.e., $p_s(\\mathbf{x}) \\ne p_t(\\mathbf{x})$ and $p_s(y|\\mathbf{x}) = p_t(y|\\mathbf{x})$.\nWe split the regression model $f_\\theta$ into a feature extractor $g_\\phi: \\mathcal{X} \\rightarrow \\mathbb{R}^D$ and linear regressor $h_\\psi(\\mathbf{z}) = \\mathbf{w}^\\top\\mathbf{z} + b$, where $D$ is the number of feature dimensions, $\\mathbf{w} \\in \\mathbb{R}^D$, $b \\in \\mathbb{R}$, $\\phi$ and $\\psi = (\\mathbf{w}, b)$ are the parameters of the models. The whole regression model using the feature extractor and linear regressor is denoted by $f_\\theta = h_\\psi \\circ g_\\phi$, where $\\theta = (\\phi, \\psi)$."}, {"title": "3. Test-time Adaptation for Regression", "content": "In this section", "Idea": "Feature Alignment\nThe basic idea of our TTA method for regression is to align the feature distributions of the source and target domains instead of using entropy minimization", "training": "n$\\mu_d^s = \\frac{1"}, {"latex": "\\mu_d^s = \\frac{1}{N_s}\\sum_{i=1}^{N_s}z_d^i,"}, {"title": "3.2. Significant-subspace Alignment", "content": "In this section", "matrix": "n$\\Sigma = \\frac{1"}, {"latex": "\\Sigma = \\frac{1}{N_s}\\sum_{i=1}^{N_s}(\\mathbf{z}^i - \\mathbf{\\mu}^s) (\\mathbf{z}^i - \\mathbf{\\mu}^s)^\\top,"}, {"title": "4. Experiment", "content": "This section provides empirical analysis of feature subspaces and experimental evaluations of SSA on various regression tasks. First, we checked whether the learned features are distributed in a small subspace (Sec. 4.3.1) and then evaluated the regression performance (Secs. 4.3.2 and 4.3.3). We also analyzed the effect of the TTA methods from the perspective of the feature subspace (Sec. 4.3.4).\n4.1. Dataset\nWe used regression datasets with two types of covariate shift, i.e., domain shift and image corruption.\nSVHN-MNIST. SVHN [44] and MNIST [36] are famous digit-recognition datasets. Although they are mainly used for classification, we used them for regression by training models to directly output a scalar value of the label. We used SVHN and MNIST as the source and target domains, respectively.\nUTKFace [59]. UTKFace is a dataset consisting of face images. The task is to predict the age of the person in an input image. For the source model, we trained models on the original UTKFace images. For the target domain, we added corruptions such as noise or blur to the images. The types of corruption were the same as those of ImageNet-C [25]. We applied 13 types of corruption at the highest severity level of the five levels.\nBiwi Kinect [18]. Biwi Kinect is a dataset consisting of person images. The task is to predict the head pose of the person in an input image in terms of pitch, yaw, and roll angles. We separately trained models to predict each angle. The source and target domains are the gender of the person in the image. We conducted experiments on six combinations of the source/target gender and task, i.e., {male \u2192 female, female \u2192 male} \u00d7 {pitch, yaw, roll}. We trained regression models to directly output head pose angles in radian, which are roughly in (-0.4\u03c0, 0.4\u03c0).\nCalifornia Housing [47]. California Housing is a tabular dataset aiming at predicting housing prices from the information of areas. We split the dataset into non-coastal and coastal areas for the source and target domains in accordance with He et al. [24].\nMore details of the datasets are provided in Appendix C.1.\n4.2. Setting\nSource model. We used ResNet-26 [23] for SVHN, ResNet-50 for UTKFace and Biwi Kinect, and an MLP for California Housing. We modified the last fully-connected layer to output single scalar values and trained the models with the standard MSE loss on each dataset and task. The details of the training are provided in Appendix C.\nTest-time adaptation with SSA (ours). We minimized $L_{TTA}$ on the target datasets. We used the outputs of the penultimate layer of the model as features, which had 2048 dimensions. We set the number of dimensions of the feature subspace to $K = 100$ as the default throughout the"}]}