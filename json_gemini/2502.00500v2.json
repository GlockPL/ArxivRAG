{"title": "Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation", "authors": ["Yang Cao", "Zhao Song", "Chiwun Yang"], "abstract": "This paper considers an efficient video modeling process called Video Latent Flow Matching (VLFM). Unlike prior works, which randomly sampled latent patches for video generation, our method relies on current strong pre-trained image generation models, modeling a certain caption-guided flow of latent patches that can be decoded to time-dependent video frames. We first speculate multiple images of a video are differentiable with respect to time in some latent space. Based on this conjecture, we introduce the HiPPO framework to approximate the optimal projection for polynomials to generate the probability path. Our approach gains the theoretical benefits of the bounded universal approximation error and timescale robustness. Moreover, VLFM processes the interpolation and extrapolation abilities for video generation with arbitrary frame rates. We conduct experiments on several text-to-video datasets to showcase the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "The rise of generative models has already demonstrated excellent performance in various fields like image generation [SCS+22, RBL+22], text generation [AAA+23, DJP+24, LFX+24], video generation [BPH+24, ZPY+24, JSL+24, TJY+24], etc. [SA]. Among them, some of the most popular algorithms - Flow Matching [LCBH+22, LGL22], Diffusion [HJA20, SME20] and VAEs [KW13], perform surprise generative capabilities, however, requiring comprehensive computational resources for training. In particular, this efficiency drawback harms the development of more successful text-to-video modeling [BPH+24], becoming a frontier challenge in the field of generative modeling.\nThe prior works about the generation from textual descriptions to realistic and coherent videos usually involve two strong pre-trained networks [HSG+22, ZPY+24]. One encodes input captions to rich embedding representations, and another one decodes from sequences of latent patches (also considered as Gaussian noise) under the guidance of text embedding representations. Although variants based on such modeling processes are already showing some fine initial results, the necessity of training on large-scale models and datasets leads these studies to be undemocratic [BPH+24, KTZ+24]. In response to this issue, the motivation of this paper is to design a novel algorithm to simplify the process of text-to-video modeling.\nIn this paper, we propose Video Latent Flow Matching (VLFM), which relies on the most advanced pre-trained image generation models (we call visual decoder in the range of this paper) for their extension in the field of text-to-video generation. In detail, we first introduce a deterministic inversion algorithm [SME20, LCBH+22, LGL22] to the visual decoder and apply this inversion operation to the frames of all videos, obtaining a sequence including initial latent patches from each video. Thus, the base of this paper is that a sequence of latent patches is a time-dependent and caption-conditional flow, so-called Video Latent Flow. Therefore, we use Flow Matching [LCBH+22, LGL22] to model it.\nEspecially, we emphasize four advantages of our VLFM:\n\u2022 Modeling efficiency. The modeling of VLFM only needs to fit N flows where N is the size of the training dataset. This computational requirement is close to training a text-to-image"}, {"title": "2 Related Work", "content": "This section briefly reviews three topics that are closely related to this work: Text-to-Video Generation, Flow Matching, and Theory in Transformer-Based Models.\nText-to-Video Generation. Text-to-video generation [SPH+22, VJMP22, BRL+23] is a specialized form of conditional video generation that aims to synthesize high-quality videos from textual descriptions. Recent advancements in this field have predominantly leveraged diffusion models [SSDK+20, HJA20], which iteratively refine video frames by learning to denoise samples from a normal distribution. This approach has proven effective in generating coherent and visually appealing videos. Training strategies for text-to-video models vary widely. One common approach involves adapting pre-trained text-to-image models by incorporating temporal modules, such as temporal convolutions and attention mechanisms, to establish inter-frame relationships [GNL+23, AZY+23, SPH+22, GWZ+23, GYR+23]. For instance, PYoCo [GNL+23] introduced a noise prior technique and utilized the pre-trained eDiff-I model [BNH+22] as a starting point. Alternatively, some methods build on Stable Diffusion [RBL+22], leveraging its accessibility and pre-trained capabilities to expedite convergence [BRL+23, ZWY+22]. However, this approach can sometimes result in suboptimal outcomes due to the inherent distributional differences between images and"}, {"title": "3 Preliminary", "content": "In this section, we formalize the background of this paper. We first introduce how we invert video frames into some latent space using the strong pre-trained visual decoder in Section 3.1. We state the definition of data and assumption in Section 3.2. Section 3.3 defines the main problem we aim to address in this paper. We use integers to denote the order of polynomials. The dimensional number of the text embedding vector is given by integer l."}, {"title": "3.1 Inverting Video Frames to Latent Patches", "content": "Notations. We use D to denote the flattened dimension of real-world images. We use d to represent the dimension of latent patches. We introduce $d_0$ as the dimension of Diffusion Transformers. We utilize V : [0,T] \u2192 $R^D$ to denote a video with T duration, where T is the longest time for each video. We omit \u2207ta(t) and a'(t) to denote taking differentiation to some function a(t) w.r.t. time t.\nVisual decoder. Here we denote the visual decoder D : $R^d$ \u2192 $R^D$ satisfies that: For any flattened image V \u2208 $R^D$, there is a unique u \u2208 $R^d$ such that D(u) = V. Then we say D is bijective. Denote the reverse function of Das $D^{-1}$ : $R^D$ \u2192 $R^d$. Note that this visual decoder D could be considered as any generative algorithm practically, e.g. LDM [RBL+22], DDIM [SME20] and VAE [Kin13]."}, {"title": "3.2 Data and Assumptions", "content": "Caption-video data pairs. Given a video distribution V, we introduce a text embedding state distribution C that maps one-to-one to V. Then for any video data V ~ V, c \u2208 $R^l$ is denoted as the corresponding caption embedding state vector. We use $V_c$ to denote the distribution that contains video and embedding vector, such that (V, c) ~ $V_c$.\nAssumptions. Here we list several mild assumptions in this paper, such that:"}, {"title": "3.3 Problem definition: Modeling Text-to-Latency Data from Discretized video", "content": "In this paper, we consider the video modeling problems as follows:\n\u2022 Given a video-caption pair (V, c) ~ $V_c$, we obtain the data $\\tilde{u}_\\tau$ \u2208 $R^d$, \u2200\u03c4 \u2208 [N] via Eq. (1), we aim to find a algorithm that inputs a time t \u2208 [0, T] and encoded text state vector c\u2208 $R^l$ and output a predicted latent patch $\\hat{u}_t$ \u2208 $R^d$, it satisfies:\n$||D(\\hat{u}_t) - V_t||_p \\le \\epsilon$. (2)\nHere we denote the error \u03f5 > 0 and some $l_p$ norm metric."}, {"title": "4 Video Latent Flow Matching", "content": "In this section, we propose Video Latent Flow Matching (VLFM) in response to the main problem in Section 3.3. Especially, we briefly review the HiPPO (high-order polynomial projection operators) framework [GDE+20] in Section 4.1. We state the derivation of our VLFM based on the popular flow matching approach [LCBH+22] in Section 4.2. Finally, we define the training objective of the VLFM for efficient video modeling in Section 4.3."}, {"title": "4.1 HiPPO Framework and LegS State Space Model", "content": "Given an input function f(t) \u2208 R for t > 0, we use $f_{<t}$ to denote the cumulative history of f(t) for every time t > 0. We choose integer s \u2265 1 as the order of approximation. Then, any s-dimensional subspace G of this function space is a suitable candidate for the approximation. Given a time-varying measure family p(t) supported on (-\u221e, t), a sequence of basis functions G = span{$g_i(t)$}$_{i=1}^s$. HiPPO [GDE+20] defines an operator that maps f to the optimal projection coefficients c : $R_{>0}$ \u2192 $R^s$, such that:\ng(t) := arg min $||f_{<t} - g||_{p(t)}$,\ng\u2208G\ns\ng(t) = $\u2211_{i=1}c_i(t) \\cdot g_i(t)$.\nWe focus on the case where the coefficients c(t) have the form of a linear ODE satisfying \u2207Vc(t) = A(t)c(t) + B(t)f(t) for some A(t) \u2208 $R^{s\u00d7s}$ and B(t) \u2208 $R^{s\u00d71}$. This equation is now also known as the state space model (SSM) in many works [KDS+15, GJT+22, GD23, DG24, ZLZ+24, XYY+24, MLW24, RX24, SLD+24].\nDiscrete HiPPO-LegS. The setting of HiPPO-LegS defines the update rule of SSM and the discrete version of A and B matrices, which are $c_{\\tau+1} = (I_s - \\frac{A}{N}) c_{\\tau} + Bf_{\\tau}$, and:"}, {"title": "4.2 Conditional Video Latent Flow", "content": "Here we emphasize the core idea of VLFM is to approximate a continuous video distribution from limited discrete video frames data utilizing the optimal high-order polynomial approximation."}, {"title": "5 Theory", "content": "This section provides several theoretical advantages of our VLFM. The approximation theory in this approach builds up based on using the Diffusion Transformer (DiT) [PX23], which is a popular choice in previous empirical and theoretical part generative model works [CHZW23, HWSL24], we briefly state its definitions in Section 5.1.\nIn addition, we provide the optimal polynomial projection guarantee and universal approximation theorem (with DiT) of VLFM in Section 5.2 to confirm its approximating ability. Besides, Section 5.3 gives error bound of interpolation and extrapolation, and Section 5.4 gives the supplementary property that VLFM's timescale robustness, which indicates its theoretical advantages."}, {"title": "5.1 Diffusion Transformer (DiT)", "content": "Diffusion Transformer [PX23] is a framework that utilizes Transformers [VSP+17] as the backbone for Diffusion Models [HJA20, SME20]. Specifically, a Transformer block consists of a multi-head self-attention layer and a feed-forward layer, with both layers having a skip connection. We use $T_{h,m,r}^F$:$R^{n \\times d_0}$\u2192 $R^{n \\times d_0}$ to denote a Transformer block. Here h and m are the number of heads and head size in self-attention layer, and r is the hidden dimension in feed-forward layer. Let X \u2208 $R^{n \\times d_0}$ be the model input. Then, we have the model output:\nAttn(X) := $\u2211_{i=1}^h$Softmax$(XW_i^QW_i^{K^T}X^T) \\cdot XW_i^VW_i^O + X$,\nwhere the projection weights $W_i^Q$, $W_i^K$, $W_i^V$, $W_i^O$ \u2208 $R^{d_0 \\times m}$. Moreover,\nFF(X) := $\u03c6(XW_1 + 1_n \\otimes b_1) \\cdot W_2 + 1_n \\otimes b_2 + X$.\nwhere the projection weights $W_1$, $W_2$ \u2208 $R^{d_0 \\times r}$, bias $b_1$ \u2208 $R^r$, $b_2$ \u2208 $R^{d_0}$, and \u03c6 is usually considered as the ReLU activated function.\nIn our work, we use Transformer networks with positional encoding E \u2208 $R^{n \\times d_0}$. The transformer networks are then defined as the composition of Transformer blocks:\n$T_P^{h,m,r}$ = {$f_T$ : $R^{n \\times d_0}$ \u2192 $R^{n \\times d_0}$ | $f_T$ is a composition of blocks $T_{h,m,r}^F$'s}.\nFor example, the following is a Transformer network consisting K blocks and positional encoding\n$f_T(X)$ = $FF^{(K)}Attn^{(K)}\u3007... FF^{(1)}\u3007Attn^{(1)}(X + E)$."}, {"title": "5.2 Approximation via DiT", "content": "Before we state the approximation theorem, we define a reshaped layer that transforms concatenated input in flow matching into a length-fixed sequence of vectors. It is denoted as R : $R^{d+l+1}$ \u2192 $R^{n \\times d_0}$. Therefore, in the following, we give the theorem utilizing DiT to minimize training objective L(\u03b8) to arbitrary error.\nTheorem 5.1 (Informal version of Theorem E.7). There exists a transformer network $f_T$ \u2208 $T_{2,1,4}^P$ defining function $F_\u03b8(z,c,t)$ := $f_T(R([z,c,t]^T))$ with parameters \u03b8 that satisfies L(\u03b8) \u2264 \u03f5 for any error \u03f5 > 0."}, {"title": "5.3 Interpolation and Extrapolation", "content": "Now, we theoretically discuss the approximating error of our VLFM in processing interpolation and extrapolation. It is considered a recovery of the original idea data from limited sub-sampled observations. This analysis is achieved by splitting the error into three parts, which are: 1) approximating error \u20ac1 for HiPPO-LegS approximating the original data; 2) Gaussian error 62 for the boundary of Gaussian vector z; 3) interpolation and extrapolation error 63 that represents the training and predicting the difference between using original idea data V and limited sub-sampled observations IV. We state the results as follows:\nLemma 5.2 (Informal version of Lemma F.3). Denote failure probability \u03b4\u2208 (0,0.1). Let the flow $\u03c8_t(\\bar{u})$ defined in Eq. (3). Denote G := [g(\u2206t), g(2\u2206t),\u2026\u2026, g(T)] \u2208 $R^{\\frac{T}{\\Delta t} \\times s}$ and X* := min(G) > 0 as the minimum eigenvalue of G. Choosing s = O($\\frac{log((\\frac{1}{\\delta})^{1.5}/\\lambda^*)}{\\lambda^*}$). Denote $u_t$ = D($V_t$) for any t\u2208 [0,T]. Especially, we define:\n\u2022 Approximating error \u20ac1 := O($T^ks^{-k+1/2}$).\n\u2022 Gaussian error \u20ac2 := O($\\sqrt{\\delta log(\\delta/\\delta)}$).\n\u2022 Interpolation and extrapolation error 63 := $U\\delta^{0.5}\\sqrt{\\frac{T}{\\Delta t}}N^{-1}exp(O(s))/\\lambda^*$.\nThen with a probability at least 1 \u2212 \u03b4, we have:\n$||V_t(u) \u2013 u_t||_2 < \\epsilon_1 + \\epsilon_2 + \\epsilon_3$.\nHaving Lemma 5.2, the concise bound for solving Eq. (2) could be given below:\nTheorem 5.3 (Informal version of Theorem F.4). Following Theorem 5.1, denote failure probability \u03b4\u2208 (0,0.1) and arbitrary error 60 > 0. Then with a probability at least 1 \u2212 d, the network in Theorem 5.1 satisfies Eq. (2) with p = 2 and\n\u03f5 = \u03f50 + Lo(\u03f51 + \u03f52 + \u03f53).\nDiscussions. Following the results of Lemma 5.2 and Theorem 5.3, we thus derive few insights as follows:\n\u2022 Optimal choice of s: A trade-off between \u20ac1 and 63. As shown in the conditions of Lemma 5.2, the larger value of the order of polynomials s helps to decrease approximating error in the training dataset while also ruining the generalization ability.\n\u2022 Stable visual decoder. Theorem 5.3 shows a small value of Lo (the stability and smoothness of visual decoder), which is important for the error of interpolation and extrapolation with an arbitrary frame rate.\n\u2022 Information. Besides, a sub-linear factor $\\frac{T}{\\Delta t}-N$, which stands for the obtained information about the continuous video, is vital as well for interpolation and extrapolation on data in distribution."}, {"title": "5.4 Timescale Robustness", "content": "Following [GDE+20], we demonstrate that projection onto latent patches ut is robust to timescales. Formally, the HiPPO-LegS operator is timescale-equivariant: dilating the input u does not change the approximation coefficients $H_N$. At the same time, this property is working in the case of the discretized form \u0169. We emphasize that it is crucial to use flow matching to model the latent patches, where whatever the sampling method and frame rate are, it will not greatly harm VLFM's performance. We give its formal statement below.\nLemma 5.4 (Proposition 3 of [GDE+20], informal version of Lemma F.2). For any integer scale factor \u03b2 > 0, the frames of video V, is scaled to $V_{\\beta r}$ for each \u03c4 \u2208 $[\\frac{N}{\\beta}]$, it doesn't affect the result of $H_N$."}, {"title": "6 Experiments", "content": "In this section, we conduct experiments to evaluate the effectiveness of our approach. We first introduce our experimental setups in Section 6.1. Then, we demonstrate text-to-video generation using VLFM and VLFM's capability of generating videos in arbitrary frame rate in Section 6.2. Furthermore, we showcase the strong performance of interpolation and extrapolation of VLFM in Section 6.3. We also perform an ablation study to discuss the importance of the flow matching algorithm in Section 6.4."}, {"title": "6.1 Setup", "content": "In our experiments, we apply Stable Diffusion v1.5 [RBL+22] with DDIM scheduler [SME20] as the visual decoder. Then, we use a DiT-XL-2 [PX23] as the backbone for the Flow Matching algorithm [LCBH+22, LGL22], and the choice of hyper-parameters of \u03c3\u03c4(\u0169) is given by omin = 0.01 and a = 10. We optimize the DiT using Grams optimizer [CLS24a]. We sample and combine 7 data resources for comprehensive training and validation of our method. They are: OpenVid-1M [NXZ+24], UCF-101 [SZS12], Kinetics-400 [KCS+17], YouTube-8M [AEHKL+16], InternVid [WHL+23], MiraData [JGZ+24], and Pixabay [Pix]."}, {"title": "6.2 Text-to-Video Generation with Arbitrary Frame Rate", "content": "In this section, we recover several videos with different frame rates using VLFM with given video captions in the training dataset. We extract T = 0.5 for demonstrations as Figure 2. In detail, we choose three frame rates for generation {8,12,16}. As shown, our VLFM performs fairly on text-to-video generation while it requires very small resource that is equivalent to training a new flow matching text-to-image video, which ensures its efficiency. Moreover, we give more results that are generated by VLFM in Appendix A and B."}, {"title": "6.3 Interpolation and Extrapolation", "content": "In this section, we test the interpolation and extrapolation of VLFM. For the interpolation experiment, the model is trained with 24 FPS and evaluated to generate video with 48 FPS. For the extrapolation, the model is trained with the first video with T = 2 and evaluated to generate the whole video with T = 8. Referring the results in Figure 3, this demonstrates the strong performance of our VLFM under our mathematical guarantee of the error bound and its effectiveness."}, {"title": "6.4 Ablation Study", "content": "In this section, we compared training VLFM with the Flow Matching algorithm and directly used DiT to predict the latent patches to showcase the importance of utilizing flow matching in our VLFM. We compare VLFM with and without flow matching by training the model with 1000 steps and compare the PSNR (peak signal-to-noise ratio) before and after training for video recovery with given captions in the training dataset. We state the results in Table 1. Denote MSE(x, y) as the mean squared error function, the computation of the metric PSNR is given by (x, y \u2208 $R^{n \\times r}$):\nPSNR(x, y) := 10$log_{10}$($\\frac{r^2}{MSE(x, y)}$)."}, {"title": "7 Conclusion", "content": "This paper proposes Video Latent Flow Matching (VLFM) for efficient training of a time-varying flow to approximate the sequence of latent patches of the obtained video. This approach is confirmed to enjoy theoretical benefits, including 1) universal approximation theorem via applying Diffusion Transformer architecture and 2) optimal polynomial projections and timescale by introducing HiPPO-LegS. Furthermore, we provide the generalization error bound of VLFM that is trained only on the limited sub-sampled video to interpolate and extrapolate the whole ideal video. We evaluate our VLFM on Stable Diffusion v1.5 with DDIM scheduler and the DiT-XL-2 model with datasets OpenVid-1M, UCF-101, Kinetics-400, YouTube-8M, InternVid, MiraData, and Pixabay. The experimental results validated the potential of our approach to become a novel and efficient training form for text-to-video generation.\nLimitations. Since the motivation of this paper focuses on simply and efficiently solving the main goal, it lacks enough exploring each design and how it affects the empirical performance, providing little insights for the follow-ups. Hence, we leave these comprehensive explorations, and its more concise theoretical working mechanism behind as future works. On the other hand, although VLFM simplifies the video modeling process, it necessitates additional computational consumption concerning the combination of the visual decoder part and the flow matching part at the inference stage. We also leave such exploration to a more efficient inference method as a future direction."}, {"title": "A More Text-to-Video Generation Results", "content": "We give more text-to-video generation results with different frame rates to demonstrate the generative ability of our VLFM in Figure 4 and Figure 5."}, {"title": "B More Interpolation and Extrapolation Results", "content": "We give more results of interpolation and extrapolation of VLFM in Figure 6."}, {"title": "C Preliminary", "content": "In the preliminary section, we first introduce our notation in the appendix in Appendix C.1. Then, in Appendix C.2, we formally define the video-caption data and visual decoder. In Appendix C.3, we define the latent patches. Appendix C.4 makes some assumptions which we will use later. Finally, in Appendix C.5, we list some basic useful facts."}, {"title": "C.1 Notations", "content": "Notations. We use D to denote the flattened dimension of real-world images. We use d to represent the dimension of latent patches. We introduce $d_0$ as the dimension of Diffusion Transformers. We utilize V : [0,T] \u2192 $R^D$ to denote a video with T duration, where T is the longest time for each video. We omit \u2207ta(t) and a'(t) to denote taking differentiation to some function a(t) w.r.t. time t. We use integer s to denote the order of polynomials. The dimensional number of the text embedding vector is given by integer l."}, {"title": "C.2 Video-Caption Data", "content": "Definition C.1 (Video-caption data pairs and their distribution). We define a video caption distribution (V, c) ~ $V_c$. Here, V : [0,T] \u2192 $R^D$ is considered as a function and c \u2208 $R^l$ is the corresponding text embedding vector.\nDefinition C.2. Given a video caption distribution $V_c$ as Definition C.1. We denote \u2206t as the minimal time unit of measurement in the real world (Planck time). For any (V, c) ~ $V_c$, we define the discretized form of V : [0,T] \u2192 $R^D$, which is V \u2208 $R^{\\frac{T}{\\Delta t} \\times D}$, and its \u03c4-th row \u2200\u03c4\u2208 $[\\frac{T}{\\Delta t}]$ is given by:\n$V_\u03c4$ := $V_{\u2206t\u03c4}$ \u2208 $R^D$.\nDefinition C.3 (Obtained data in real-world cases). If the following conditions hold:\n\u2022 Given a video caption distribution $V_c$ as Definition C.1.\n\u2022 For any (V, c) ~ $V_c$, we define the discretized form of video V as Definition C.2.\nWe define an observation matrix \u0424 : {0,1}$^{N \\times \\frac{T}{\\Delta t}}$. The obtained data in real-world cases then is denoted as \u0424V\u2208$R^{N \\times D}$."}, {"title": "C.3 Latent Patches Data", "content": "Definition C.5. If the following conditions hold:\n\u2022 Given a video caption distribution $V_c$ as Definition \u0421.1.\n\u2022 For any (V, c) ~ $V_c$, we define the discretized form of video V as Definition C.2.\n\u2022 Let the observation matrix \u03a6 : {0, 1}$^{N \\times \\frac{T}{\\Delta t}}$ be defined as Definition C.3.\n\u2022 Let the visual decoder function D : $R^d$ \u2192 $R^D$ be defined as Definition C.4.\nWe define the ideal version (without observation matrix) of the sequence of latent patches u \u2208 $R^{\\frac{T}{\\Delta t} \\times d}$, and its \u03c4-th \u2200\u03c4 \u2208 $[\\frac{T}{\\Delta t}]$ row is defined as follows:\n$u_\u03c4$ := $D^{-1}(V_\u03c4)$.\nDefinition C.6. If the following conditions hold:\n\u2022 Given a video caption distribution $V_c$ as Definition \u0421.1.\n\u2022 For any (V, c) ~ $V_c$, we define the discretized form of video as Definition C.2.\n\u2022 Let the observation matrix \u03a6 : {0,1}$^{N \\times \\frac{T}{\\Delta t}}$ be defined as Definition C.3.\n\u2022 Let the visual decoder function D : $R^d$ \u2192 $R^D$ be defined as Definition C.4.\nWe define the real-world version (with observation matrix) of the sequence of latent patches \u0169 \u2208 $R^{N \\times d}$, and its \u03c4-th \u2200\u03c4 \u2208 [N] row is defined as follows:\n$\\tilde{u}_\u03c4$ := $D^{-1}((\u03a6V)_\u03c4)$."}, {"title": "C.4 Assumptions", "content": "Assumption C.7. If the following conditions hold:\n\u2022 Given a video caption distribution $V_c$ as Definition \u0421.1.\n\u2022 For any (V, c) ~ $V_c$, we define the discretized form of video as Definition C.2.\n\u2022 Let the observation matrix \u03a6 : {0,1}$^{N \\times \\frac{T}{\\Delta t}}$ be defined as Definition C.3.\n\u2022 Let the visual decoder function D : $R^d$ \u2192 $R^D$ be defined as Definition C.4.\n\u2022 Let the ideal version of the sequence of latent patches u \u2208 $R^{\\frac{T}{\\Delta t} \\times d}$ be defined as Definition C.5."}, {"title": "C.5 Basic Facts", "content": "Fact C.11. For a variable x ~ N(0, \u03c3\u00b2), then with probability at least 1 \u2013 \u03b4, we have:\n$|x| \u2264 C_\u03c3\\sqrt{log(1/\u03b4)}$\nFact C.12. For a PD matrix A \u2208 $R^{d_1 \\times d_2}$ with a positive minimum eigenvalue $\u03bb_{min}(A) > 0$, the infinite norm of its pesdueo-inverse matrix $A^\u2020$ is given by:\n$||A^\u2020||_\u221e \u2264 \\frac{1}{\u03bb_{min}(A)}$.\nFact C.13. For two matrices A, B \u2208 $R^{d_1 \\times d_2}$, we have:\n$||A^\u2020 \u2013 B^\u2020||_2 \u2264 \\frac{||A^\u2020||_2 ||A \u2013 B||_2}{1 \u2013 ||A^\u2020 ||_2 \\cdot ||A \u2013 B||_2}$"}, {"title": "D Video Latent Flow Matching", "content": "This section, we first introduce the HiPPO Framework and LegS in Appendix D.1. Then, we formally define the video latent flow in Appendix D.2. Last, we introduce the training objective of VLFM in Appendix D.3."}, {"title": "D.1 HiPPO Framework and LegS", "content": "Definition D.1. We define matrix A \u2208 $R^{s \\times s}$ where its ($i_1$,$i_2$)-th entry \u2200$i_1$, $i_2$ \u2208 [s] is given by:\nDefinition D.2. We define matrix B \u2208 $R^{s \\times 1}$ where its $i_1$-th entry \u2200$i_1$ \u2208 [s] is given by:\n$B_{i_1}$ = $\\sqrt{2i_1 + 1}$.\nDefinition D.3. If the following conditions hold:\n\u2022 Let matrix A \u2208 $R^{s \\times s}$ be defined as Definition D.1.\n\u2022 Let matrix B \u2208 $R^{s \\times 1}$ be defined as Definition D.2.\nWe initialize a matrix $H_0$ = 0dxs. Then we define:\n$H_\\tau$ : $H_{\\tau-1}(I_s-\\frac{1}{N}A)^{-1} + \\frac{1}{N}B_\u03c4B^T$, \u2200\u03c4 \u2208 [N].\nDefinition D.4. We define g(t) := [$\\sqrt{P_0(t)}$, $P_1(t)$,...,$\\sqrt{2^{s-1}}P_{s-1}(t)$] \u2208 $R^s$, where$P_i(t)$, \u2200i \u2208 [s] is some polynomials. Especially, g(t) satisfies:\n\u2022 Define G :=\n\u2022 |$G_{\u03c4,i}$| \u2264 exp(O($\\frac{s}{N}$)) for any\u03c4\u2208 [$[\\frac{N}{\\Delta t}]$, i \u2208 [s]."}, {"title": "D.2 Video Latent Flow", "content": "Definition D.5. If the following conditions hold:\n\u2022 Given a video caption distribution $V_c$ as Definition \u0421.1.\n\u2022 For any (V, c) ~ $V_c$, we define the discretized form of video as Definition C.2.\n\u2022 Let the observation matrix \u03a6: {0,1}$^{N \\times \\frac{T}{\\Delta t}}$ be defined as Definition C.3.\n\u2022 Let the visual decoder function D : $R^d$ \u2192 $R^D$ be defined as Definition C.4."}, {"title": "D.5 If the following conditions hold:", "content": "Definition D.6. If the following conditions hold:\n\u2022 Given a video caption distribution $V_c$ as Definition \u0421.1.\n\u2022 For any (V", "\u03a6": {"D": "R^d$ \u2192 $R^D$ be defined as Definition C.4.\n\u2022 Let the ideal version of the sequence of latent patches u \u2208 $R^{\\frac{T"}, "follows": "n$\u03c3_t(\\tilde{u"}, 1, "\u03c3_{min}$) \u00b7 $[sin^2(\\frac{\u03c0 t}{T}) + exp(-\u03b1t)"], "hold": "n\u2022 Given a video caption distribution $V_c$ as Definition \u0421.1.\n\u2022 For any (V", "\u03a6": {"D": "R^d$ \u2192 $R^D$ be defined as Definition C.4.\n\u2022 Let the ideal version of the sequence of latent patches u \u2208 $R^{\\frac{T}{\\Delta t} \\times d}$ be defined as Definition C.5.\n\u2022 Let the real-world version of the sequence of latent patches \u0169 \u2208 $R^{N \\times d}$ be defined as Definition C.6.\n\u2022 Let"}}