{"title": "Geographical Node Clustering and Grouping to Guarantee Data IIDness in Federated Learning", "authors": ["Minkwon Lee", "Hyoil Kim", "Changhee Joo"], "abstract": "Federated learning (FL) is a decentralized AI mechanism suitable for a large number of devices like in smart IoT. A major challenge of FL is the non-IID dataset problem, originating from the heterogeneous data collected by FL participants, leading to performance deterioration of the trained global model. There have been various attempts to rectify non-IID dataset, mostly focusing on manipulating the collected data. This paper, however, proposes a novel approach to ensure data IIDness by properly clustering and grouping mobile IoT nodes exploiting their geographical characteristics, so that each FL group can achieve IID dataset. We first provide an experimental evidence for the independence and identicalness features of IoT data according to the inter-device distance, and then propose Dynamic Clustering and Partial-Steady Grouping algorithms that partition FL participants to achieve near-IIDness in their dataset while considering device mobility. Our mechanism significantly outperforms benchmark grouping algorithms at least by 110 times in terms of the joint cost between the number of dropout devices and the evenness in per-group device count, with a mild increase in the number of groups only by up to 0.93 groups.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) is a promising AI technology for providing intelligent services to resource-limited devices in diversified mobile environments (e.g., IoT, autonomous vehicles, smartphones). Centralized AI service incurs privacy concerns in personalized services like smart healthcare [1] and smart home [2], due to the transmission of local device data to the central server. On the contrary, FL is a decentralized technology that can effectively address such issues, by conducting local learning on end-devices using their own data and transmitting only the trained models to the server [3].\nNevertheless, FL suffers from the problem of non-IID training data which is originated from the heterogeneity of collected mobile data among the devices experiencing different task-operating environments. Specifically, the non-IID problem implies that when the distribution of each device's collected data is either correlated or non-identical to each other, it may cause a degraded performance (in terms of test accuracy) of the federated model compared to the one trained with IID datasets [4]. In addition, it may incur slower convergence of FL, leading to consuming more communication and computational overhead in the overall operation of FL.\nIn the literature, there have been several work proposing various mechanisms to alleviate the negative impact of non-IID data on the FL performance. Among them, some studies focused on resolving data heterogeneity through algorithmic approaches as follows. [5] proposed integration of FL with multi-task learning which tries to learn the relationship among similarly-structured local models to improve the performance of global model. In [6], participant selection via deep Q-learning was proposed to select the best devices that can maximize the validation accuracy per training round. Alternatively, there also exist data sharing strategies to tackle the problem as follows. [4] proposed sharing the global data from the federated server with its clients, in order to reduce the model's weight divergence by performing local training with local and global dataset combined together. Similarly, [7] proposed a strategy of training the local model in two stages, once with local data and again with globally-shared data. None of the above, however, exploited the nature of communication environments in developing the FL techniques to restore data IIDness. Furthermore, it is hard to find work applicable to time-varying data distribution due to node mobility.\nThis paper focuses on the geographical aspect of mobile data to develop a novel FL mechanism to achieve near-IIDness of the IoT data. Smart IoT devices equipped with multimodal sensors (e.g., camera, microphone, thermometer) may run federated AI algorithms to perform smart tasks, such as autonomous robots in a warehouse monitoring the environment (e.g., heat signature) or managing inventory (e.g., with vision sensors) [8], [9], and a swarm of autonomous drones with vision sensors collaborating for surveillance tasks [10]. In such scenarios, nearby devices could obtain correlated sensor readings (e.g., temperature, images taken toward similar directions), while such correlation diminishes as they get farther away. When they are too far apart, however, their collected data may follow non-identical distributions (e.g., temperature near the entrance vs. at the center of the building).\nIn such a vein, we propose mobile node clustering and grouping techniques to achieve almost-IID data distribution in FL. Motivated by aforementioned insights, we developed node clustering and grouping algorithms that takes into account node mobility and inter-node correlation and distributional similarity of their IoT data, utilizing graph coloring to satisfy the data IIDness among the nodes in the same FL group. To the best of our knowledge, this is the first work exploiting geographical features of IoT data to demonstrate its potential for resolving the issue of non-IID dataset in the FL training. Our contributions are four-fold, as summarized as follows.\n\u2022 Our experimental study in Section II demonstrates the geographical effect on the IIDness of collected data, revealing that the distance between data collection locations plays a crucial role in the geographical characteristics, along with the surrounding environment.\n\u2022 We propose Dynamic Clustering, a geographical node clustering algorithm to achieve near-identical distribution among mobile data by upper-bounding the inter-device distance in a FL cluster.\n\u2022 We also propose Partial-Steady Grouping, a geographical node grouping algorithm to guarantee near-independence between mobile data by lower-bounding the minimum inter-device distance in each node group.\n\u2022 Via extensive simulations, our proposal is compared with state-of-the-art node grouping algorithms. Our algorithm significantly outperforms others in terms of the joint cost between the number of dropped devices from training and the evenness in per-group device count.\nThe rest of the paper is organized as follows. Section II presents our experimental results as an evidence of the geographical characteristics in IoT data distributions. Then, Section III introduces the system model of our FL mechanism. Section IV proposes dynamic clustering and grouping algorithms to restore the IIDness within each FL group, and Section V evaluates the proposed algorithms through extensive simulations. Finally, the paper concludes with Section VI."}, {"title": "II. EXPERIMENTAL EVIDENCE FOR GEOGRAPHICAL CHARACTERISTICS OF IOT DATA", "content": "This section demonstrates our experimental study on the geographical characteristics of IoT data, to verify our conjecture of the data correlation and identicalness depending on the inter-device distance. Such a study is necessary since there exists very few work investigating the IIDness feature of IoT data, and it is hard to find public datasets with the usual IoT scale (i.e., room to building). Although [11] studied the variation in statistical characteristics of the geographically-collected dataset, the collection locations were sampled at the scale of continents thus not suitable for revealing the geographical characteristics of IoT data distribution.\nIn the experiment, we measured temperature at various indoor locations, as an exemplifying case of IoT data in smart healthcare, smart buildings, etc.\u00b9 Fig. 1 illustrates 18 measurement locations (labeled as hall1 through hall18) in our campus building where two adjacent locations are 1.35 meters apart. The considered area consists of an elevator hall, a staircase on its right, and a long corridor above the hall.2 Then, we concurrently measured the temperature at these points using multiple thermometers, which had been repeated 20 times at various times and days. Since the measurements were performed in late November, the data points closer to the staircase tend to have lower temperature."}, {"title": "A. Geographical Characteristics on the Identicalness", "content": "With collected data, we compared per-location histograms to investigate the data identicalness feature. To generate each per-location histogram, we considered five bins where the bin width is given as the 'maximum difference in temperature over the entire dataset' divided by five. Each bin's frequency of occurrence is obtained as the number of data points belonging to each bin divided by the total number of measurements (i.e., 20), producing a probability distribution. Fig. 2 depicts thus-constructed histograms, which have been arranged according to the measurement locations in Fig. 1. After obtaining the histograms, we compared the distribution of the reference data point 'hall18' and that of another data point by using the KL-divergence [12], which measures the difference between the two probability distributions. Table I depicts thus-constructed KL values arranged by the corresponding locations, where a larger value implies that the distributions are less identical."}, {"title": "B. Geographical Characteristics on the Independence", "content": "We examined the correlation between measurement points by computing the Pearson correlation coefficient $ \\rho_{X,Y} $ between the reference point X and a point Y [13], while varying X as one of three candidates. Denoting their measurements respectively by $ \\{X_i\\} $ and $ \\{Y_i\\} $, i = 1, ..., 20, $ \\rho_{X,Y} $ is given as $ \\rho_{X,Y} = cov(X,Y)/\\sigma_X\\sigma_Y $ where $ \\sigma_X^2 $, $ \\sigma_Y^2 $ are the unbiased sample variance and $ cov(X, Y) $ is the sample covariance.\nFig. 3 illustrates the results with three reference points: hall3, hall11, hall18. Noticeably, the largest values in blue are close to the reference point, while the smallest ones are distant from it. In the meantime, all the values being quite large suggests high consistency in temperature fluctuations over time in the environment, which is natural for indoor thermal data. Nevertheless, upon increasing the distance from the reference point, a noticeable trend is observed wherein $ \\rho_{X,Y} $ exhibited a decline indicating that closer data points are more correlated.\nTherefore, the experiment revealed that the IoT data also exhibits geographical characteristics regarding data independence. The observed tendency of correlation values with varying inter-device distance implies that there exists the minimum distance beyond which the data distribution of any two devices may satisfy near-independence, which will be denoted by $ d_{min} $ in the sequel. Similar to $ d_{max} $, $ d_{min} $ can be pre-determined by calculating $ \\rho_{X,Y} $ between the measurements of two probing devices, while gradually decreasing the inter-device distance. Once the correlation becomes larger than a pre-defined threshold with a target confidence (e.g., 95%) for a certain tested inter-device distance, $ d_{min} $ is set as the distance."}, {"title": "III. SYSTEM MODEL", "content": "This work leverages the most popular FL model 'FedAvg' [14] and evolves it to propose our geographical FL mechanism. In FedAvg, there exists a single FL server to control N clients/devices which are participating in the learning process. Initially, the FL server owns the global model $ w_G $ and randomly selects a certain number of its clients. Then, the FL system follows the following steps:\n1) The server sends the current global model to the selected clients, and client device i locally trains the received model with local dataset $ D_i $ such as $ w_i \\leftarrow W_i - \\eta\\Delta L(w_i; D_i) $ where $ w_i $ is the local model's parameter, $ \\eta $ is the learning rate, and $ \\Delta L $ is the gradient of the loss function [3]. $ D_i $ may be split into several batches.\n2) When device i finishes updating its parameter $ w_i $, it transmits the updated model back to the FL server. Then the FL server aggregates the received N local models via averaging, such as $ W_G \\leftarrow \\frac{1}{N}\\sum_{i=1}^{N} W_i $.\nIf the current global model fails to converge, the FL server repeats the above steps by re-selecting the clients. In this work, a single iteration consisting of steps 1 and 2 is called a round."}, {"title": "IV. DYNAMIC CLUSTERING ALGORITHM AND PARTIAL-STEADY GROUPING ALGORITHM", "content": "Assuming a given IoT network has geographical features according to $ d_{max} $ and $ d_{min} $, this section proposes applying $ d_{max} $ to node clustering and $ d_{min} $ to node grouping to construct our FL mechanism, leveraging on the graph theory."}, {"title": "A. Preliminaries: Graph Theory", "content": "In an undirected graph G = (V, E) with a set of vertices/ nodes V and a set of edges E, any two distinct vertices connected by an edge are adjacent. The graph density of G is the ratio of the number of edges in G to the maximum possible number of edges (of the complete graph on V). In G', the complement graph of G, two distinct vertices are adjacent if and only if they are not adjacent in G [17]. In addition,"}, {"title": "B. Proposed Clustering Algorithm: Dynamic Clustering", "content": "For an FL system, we form a circular cluster with the diameter of $ d_{max} $, within which nodes are treated to have identically-distributed datasets by the definition of $ d_{max} $. The clustering problem, however, is not complete by simply forming a cluster, since node mobility should also be considered. For instance, a node that has been predominantly located outside the cluster could enter its coverage at the last moment before clustering is performed. It is also possible that a certain node located at the cluster edge disappears and reappears over time due to high mobility. In such cases, the identicalness within the cluster may not be ensured if we blindly let such nodes participate in FL. As a result, clustering must consider per-node location history, instead of just using a snapshot of node locations.\nIn this regard, we propose the 'Clustering Suitability' (CS) metric, with which only suitable nodes can be identified for guaranteeing intra-cluster identicalness. For sample time t\u2208 {1,...,T}, CS of device i, denoted by CS(i), is defined as:\n$ CS(i) = \\frac{1}{T}\\sum_{t=1}^{T} w_t \\cdot I_{cs}(i,t) $        (1)\nwhere the clustering indicator $ I_{cs}(i,t) $ is set to 1 when node i is within the cluster at time t, and 0 otherwise. In addition, $ w_t $ is a monotonic non-decreasing discount factor (i.e., $ w_1 \\leq w_2 < ... < w_T $) to assign larger weight to a more recent location, which satisfies $ \\sum_{t=1}^{T} w_t = 1 $. Then, over T sampling times, nodes are required to be within the cluster for more than $ \\xi_{cs} $ (weighted) fraction of time (e.g., 0.7), i.e., $ CS(i) \\geq \\xi_{cs} $, to be eligible for participating in the FL process. Then, we can obtain the suitable node set $ \\tilde{N} $ (CN) such as\n$ \\tilde{N} = \\{i \\in N \\vert CS(i) \\geq \\xi_{cs}\\} $            (2)\nAfter constructing $ \\tilde{N} $, the FL server should partition them into a set of groups while ensuring that in each group any inter-device distance is larger than $ d_{min} $ to achieve near-independence. Again, since the mobility of devices may incur variations in the inter-device distance, appropriate pairings between such nodes should be established based on their location histories, via another proposed metric called 'Pairing Suitability'."}, {"title": "C. Proposed Grouping Algorithm: Partial-Steady Grouping", "content": "We can now apply the grouping technique to the obtained G. In graph theory, two types of grouping techniques are most popular: clique partitioning and graph coloring. Clique partitioning tries to divide a graph into separate and independent cliques [18], while graph coloring aims to color the vertices of a graph such that no two adjacent vertices share the same color thereby forming distinct groups [17]. Then, it is well known that the partitioning of G into cliques is equivalent to the coloring of its complement graph G' [18]. Since the density of G' is usually sparser than that of G under the assumption of uniformly distributed nodes in the cluster, this work adopts the graph coloring method on G' in the sequel, to alleviate the computational complexity of grouping.\n1) Design considerations: We only consider 'proper' coloring, i.e., coloring with no clashes (a clash refers to same-colored adjacent nodes [17]). If improper coloring is performed on G', nearby nodes could be grouped together resulting in potential inter-node data correlation. We thus ruled out any known improper coloring techniques in our design.\nOur main coloring objectives are: 'even' grouping and minimizing the number of ungrouped nodes. Even grouping implies the group size (in terms of the number of nodes) should be as even as possible, to maintain a similar amount of per-group federated dataset leading to the consistent model performance across the groups. This is to ensure little fluctuation in the global model evolution throughout successive rounds, regardless of the order of groups. Next, it is also desirable to minimize the number of non-participating (i.e., ungrouped) nodes, not only to avoid free-riders in the FL training [19] but also to mitigate the training bias due to the excluded local data from the dropped devices [20]. Therefore, the major challenge lies in jointly pursuing evenness and minimal ungrouped nodes with a right balance, within the scope of proper coloring.\nMoreover, the reduction in the number of groups should also be considered because too many small-sized groups may incur a large latency in completing all the rounds. In such a vein, it has been a main issue in graph coloring to search for the minimum number of colors k to color a given graph, which is called the chromatic number. Since determining the chromatic number is NP-hard [21], numerous heuristics and metaheuristic approaches have been devised. Among them, we adopt a popular metaheuristic method called PartialCol [22] and revise it to best fit into the nature of our problem.\nOriginal PartialCol is designed to ascertain the chromatic number of a given graph, by iteratively reducing k from its initial value while maintaining the properness of coloring. The following is an overview of the algorithm.\n\u2022 At each iteration, the algorithm first constructs an initial solution $ \\{S_1, S_2, ...S_k, U\\} $ comprising k color sets and one uncolored set U, via greedy coloring [17] as follows. Let P be a permutation of the nodes in N. For each node in P, the node is assigned to $ S_i $ if the color set includes no adjacent nodes of the node. Otherwise, the node tries other color sets in the increasing order of the set index. If the node fails to find its color set, it is assigned to U.\n\u2022 If the set U is not an empty set, then the algorithm performs Tabu Search (TS) on the initial solution. To empty U, an arbitrary node i in U is moved to a randomly selected set $ S_i $. Then, the nodes in $ S_i $ adjacent to i are moved to U and marked as 'tabu', prohibiting their move back to $ S_i $ for a pre-defined period.\n\u2022 TS continues to try the next move either until |U| is successfully reduced to 0 or until the maximum move count is reached. Such rules facilitate the exploration of unscouted solution spaces, increasing the chance of discovering an optimal solution that minimizes |U|.\n\u2022 If TS successfully reduces |U| to 0, the solution is saved and the algorithm restarts with k decremented by one.\n2) Partial-Steady Grouping (PSG): Our PSG algorithm re-defines the cost function of PartialCol (which was the number of ungrouped nodes |U|) into a joint cost as follows:\n$ C = \\alpha \\cdot \\vert U \\vert + (1 - \\alpha) \\cdot \\upsilon, 0 < \\alpha < 1 $,             (4)\nby combining |U| with the variance v of per-group sizes, where v is a measure of evenness (smaller v means better evenness) and a is a weighting factor between the two metrics. Since PartialCol iteratively decreases k, the initial k should be carefully chosen. We propose to determine the initial k by utilizing DSatur, a popular method for generating an initial solution in metaheuristics thanks to its ability to rapidly determine k and build a proper solution [23]. Although DSatur is known to produce k close to the chromatic number, our algorithm can further decrease k by allowing some nodes to be possibly uncolored for the sake of decreasing k, unlike DSatur that tries to find minimal k with no dropout nodes.\nOnce k is given, we need to construct an initial coloring solution, for which we propose a modified greedy algorithm called Equitable Largest-degree First (ELF). When coloring a node, Equitable coloring [17] tries to select the color set with the minimum number of nodes among possible proper sets, thus pursuing evenness. In addition, Largest-degree First coloring [24] determines the sequence of node coloring by prioritizing nodes according to their 'adjacent node count' in the descending order. By handling the node with the highest degree first, it intuitively leads to a smaller number of colors necessary to color a graph completely (or equivalently, it leads to a reduced number of uncolored nodes when the number of colors is limited). Therefore, by combining the two, we can obtain a superior initialization for TS, so as to eventually derive a better final solution.\nELF determines its solution $ S_{ELF} = \\{S_1, S_2, ..., S_k, U\\} $ similarly to the original PartialCol as follows. First, $ S_1,..., S_k $ and U start as empty sets. Then, for given G', the set P of the node coloring sequence is constructed by the Largest-degree First rule. Then, each node in P is assigned to a color set according to the Equitable coloring rule. If no color set is available for the node, it is assigned to U. After all nodes are assigned to $ S_{ELF} $, the algorithm calculates the cost $ C_{ELF} $ of the solution $ S_{ELF} $ according to Eq. (4).\nMeanwhile, due to the updated cost function, the principle of TS should be modified such as: TS should continue its operation even if U = 0 has been reached, as long as C remains large. Specifically, the modified TS selects the color set with the maximum number of nodes (denoted by $ S_M $), and randomly move r nodes in $ S_M $ to U, where $ r = \\vert S_M \\vert - \\vert S_m \\vert $ and $ S_m $ is the color set with the minimum node count. Then, thus-moved nodes are prohibited to move back to their original color set. In the mean time, for the sake of evenness, the modified TS is designed to always reach the maximum iterations as it scouts for better solutions. Other than the aforementioned changes, the modified TS is performed the same as the original TS, but now with three inputs: $ S_{ELF} $, G', a. The modified TS finally yields its solution $ s_{ts} $ as the one with the minimum C throughout all the iterations.\nFinally, we introduce \u2018cost-enhancing ratio\u2019 tr, to reduce the number of groups as much as possible. Original PartialCol reiterates itself by decrementing k by 1 when reaching |U| = 0, since it is no more possible to enhance the cost with given k. In our case, however, C = 0 is achieved extremely rarely due to the variance in the joint cost. Hence, in contrast to the reiteration mechansim of original PartialCol, we propose the following procedure for reiteration. According to the modified TS, if $ C < C_p \\times t_r $ (where $ C_p $ is the cost in the previous iteration), the algorithm decrements k by 1 and reiterates the grouping mechanism. That is, we treat \u2018enhancing the previous cost $ C_p $ by a certain fraction (i.e., $ t_r $)\u2019 as an indication for justifying the necessity of further decrementing k. On the contrary, if $ C > C_p \\times t_r $, PSG stops its operations and returns the previous solution (denoted by $ s_p $) and its cost $ C_p $."}, {"title": "D. Overall Procedure of the Proposed Mechanism", "content": "Our proposed FL mechanism is summarized as follows. The FL server executes DC in Algorithm 1 based on the location history of N nodes for the last T time samples, obtaining the Grouping Suitability Graph G. Subsequently, the server applies PSG in Algorithm 2 to G', and k independent groups are formed upon completion of PSG, each of which will participate in FL during its own corresponding round."}, {"title": "V. PERFORMANCE EVALUATION", "content": "This section evaluates our proposed algorithms via simulations across various scenarios. Then, our grouping algorithm's performance is compared with three benchmark coloring algorithms, followed by an analysis on the effect of REVES."}, {"title": "A. Scenario Setup", "content": "Table II shows the three scenarios considered, densely-, moderately-, and sparsely-populated areas, with device population density (in /m\u00b2) of 10\u20131, 10-2, 10-3, respectively. The dense case's device density is set based on the data in a public square in a city [26], the sparse case's density is obtained from the data taken in a sub-urban area [25], and the moderate case's density is set as a mid-point between the two aforementioned cases. Then, we also consider \u2018FL-participating device density\u2019, which is defined as the device population density multiplied by the mobile application popularity participating in the FL process (e.g., Instagram's app popularity: 40% [27]). In addition, we assumed that each scenario's area is a square with 100, 200, and 1000 meters per side, respectively.\nMobile devices are assumed uniformly distributed at random locations following the Poisson point process, each moving to a random direction at a random speed comparable to the human walking speed. We also assume that an FL server's cluster is circular with the diameter of $ d_{max} $ which is inscribed in the square-shaped area of each scenario.7 Furthermore, we set $ d_{min} $ = 10m for the densely-populated scenario, while $ d_{min} $ in the other two scenarios is set as $ 10m \\times \\sqrt{10^{-1}/10^{-2}} \\approx $ 32m for the moderately-populated scenario and $ 10m \\times \\sqrt{10^{-1}/10^{-3}} = $"}, {"title": "B. Ablation Study on PSG Algorithm", "content": "We first conducted an ablation study to reveal the individual impact of each design component in the PSG algorithm. The study compares original PartialCol with one of its evolved versions, where the latter is obtained by selectively applying ELF or modified TS (with two tr values) to the original PartialCol. As performance metrics for comparison, we measured 'the number of groups' and 'joint cost', for the three test scenarios. For each scenario, 20 realizations of node deployment have been generated for the DC algorithm, and for each realization each version of grouping algorithm (i.e., PartialCol, PartialCol with ELF, PartialCol with modified TS) has been executed 20 times. The average of thus-obtained 400 results is recorded in Table III.\n1) Impact of ELF: As shown in Table III, applying ELF to PartialCol's initialization decreases the cost by at least 67.3% and up to 78.6%, without sacrificing the number of groups at all. Since both PartialCol and 'PartialCol + ELF' have no ungrouped nodes in their solutions, we set their cost as a half of the evenness (i.e., a = 0.5). As intended, enhancing the initial solution of PartialCol by adopting ELF produces a superior solution indeed.\n2) Impact of C, modified TS, and tr: In case the original PartialCol's cost is revised to the joint cost C, both TS and the strategy of reducing the number of groups should be adjusted as well (as discussed in Section IV-C2). Accordingly, we analyzed the effect of revising the cost function and TS, and that of introducing tr (with two different values, 0.7 and 0.1), while setting a = 0.5 due to the absence of ungrouped nodes in the original PartialCol. Table III presents that 'PartialCol + modified TS' with tr = 0.7 can mitigate C significantly by 90.3% on average (averaged over the three scenarios) while mildly increasing the number of groups only by 6.81% on average, compared to the original PartialCol. With smaller tr (i.e., tr = 0.1), the performance of 'PartialCol + modified TS' still outperforms the original PartialCol, with a slight degradation from the case of tr = 0.7."}, {"title": "C. Performance of the Proposed Mechanism", "content": "We set our algorithm parameters as follows. For the clustering algorithm, the thresholds $ \\xi_{cs} $ and $ \\xi_{ps} $ are both set to 0.7, and the discount factor in CS and PS follows a linear monotonic increasing function such as $ w_t = t/\\sum_{t=1}^{T}t $. For each scenario, we have generated 20 realizations of node deployment for the DC algorithm, and for each realization we have run the PSG algorithm 20 times. Then, we took the average of thus-obtained 400 results, for each of the following four metrics we measured: the number of groups, the joint cost, the number of ungrouped nodes, and evenness. In addition, for PSG, tr varied as 0.7, 0.4, 0.1.\nFig. 7 and the upper three rows of Table IV present the results of the PSG algorithm across the three scenarios, where Fig. 7 varies a from 0 to 1 at intervals of 0.2 while Table IV sets a = 0.5. Table IV shows that 'the number of groups' tends to gradually increase as the environment becomes sparser, whereas it is insensitive to tr. The 'joint cost' achieved by PSG, however, is affected by both device density and tr as shown in Table IV and Fig. 7a, where the cost tends to deteriorate as device density or tr decreases. Regarding 'the number of ungrouped nodes' and 'evenness', however, we found that the three scenarios share similar tendency, and hence we focused on the moderately-populated case in Fig. 7b to show the tendency of the two metrics with varying tr and a. As shown, with the increase of a, the number of ungrouped nodes decreases whereas the evenness deteriorates, since the joint cost C puts more weight on |U| as a approaches 1."}, {"title": "D. Performance Comparison with Baseline Algorithms", "content": "Our PSG algorithm has been compared with three state-of-the-art algorithms, PartialCol [22], TabuCol [28], and DSatur [23]. Similar to PartialCol, TabuCol is a metaheuristic utilizing Tabu Search to minimize the initial solution's cost with the goal of finding the minimum number of groups. In TabuCol, however, the cost is defined as the count of clashes within the current solution, considering TabuCol's strategy of assigning a node to a random color set when no appropriate color set is available for proper coloring. DSatur is a polynomial-time algorithm in metaheuristic coloring, which is popularly used thanks to its quick generation of feasible initial solutions. The algorithm selects the node with the largest saturation degree (i.e., the number of different colors used by its neighbors) to color with the minimal-indexed color. In doing so, if a node cannot be colored properly, k is increased by 1.\nSince the aforementioned algorithms always yield proper solutions (i.e., no ungrouped nodes in the solution), we measured the evenness of their solutions (by following our definition of evenness) and took its half, to compare with our algorithm's cost with a = 0.5. As shown in Table IV, our cost is significantly smaller than the benchmark algorithms in every scenario, with a slight sacrifice in the number of groups. Specifically, the greatest disparity in the number of groups between the benchmarks and ours is only 0.93 (in the moderately-populated scenario), while the achieved cost is at least 110 times smaller than that of the counterparts. That is, our proposed algorithm shows significant enhancement in evenness and the number of ungrouped nodes, with minimal compromise in the group count."}, {"title": "E. Impact of REVES in Grouping", "content": "Recall that REVES aims to minimize the number of iterations run by our modified TS. We examined whether PSG with REVES can notably reduce the number of iterations without severely increasing the cost and the number of groups. With tr = 0.7, PSG with REVES has been simulated for the three population scenarios while varying a from 0.1 to 0.9, and the nine results obtained by varying a are averaged. Then, we utilized thus-derived values (in terms of the number of groups, cost, and the total number of iterations) to calculate how much change 'PSG with REVES' incurs compared to \u2018PSG without REVES'. For REVES, we set $ w_s $ = 150 and $ p = 70 $.\nAs indicated in Table V, REVES reduces the total number of iterations in PSG by 72.85% on average. Concurrently, the number of groups remains nearly unchanged, and the average cost mildly increases (by 0.32% ~ 14.08%). Due to its ability to substantially decrease the iteration count without causing a detrimental cost escalation, REVES could find its utility in time-intensive scenarios, e.g., with many nodes involved."}, {"title": "VI. CONCLUSION", "content": "This paper proposed a geographical approach to alleviating the non-IID data problem in FL, based on our intuition on the inter-device distance driven IoT data independence and identicalness. Facilitated by graph coloring methods, we have developed node clustering and grouping algorithms to achieve an almost-IID dataset per FL group. Through extensive simulations, we demonstrated that our mechanism much outperforms the existing alternatives in varying IoT environments.\nIn the future, we will investigate the general impact of the proposed methods by considering more diversified IoT data types (e.g., vision, audio) and environments (e.g., outdoor). In addition, we will try to deploy our proposed mechanism with existing FL algorithms to show its efficacy in real applications."}]}