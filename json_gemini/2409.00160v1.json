{"title": "Learning-Based Finite Element Methods Modeling for Complex Mechanical Systems", "authors": ["Shi Jiasheng", "Lin Fu", "Rao Weixiong"], "abstract": "Complex mechanic systems simulation is important in many real-world applications. The de-facto numeric solver using Finite Element Method (FEM) suffers from computationally intensive overhead. Though with many progress on the reduction of computational time and acceptable accuracy, the recent CNN or GNN-based simulation models still struggle to effectively represent complex mechanic simulation caused by the long-range spatial dependency of distance mesh nodes and independently learning local and global representation. In this paper, we propose a novel two-level mesh graph network. The key of the network is to interweave the developed Graph Block and Attention Block to better learn mechanic interactions even for long-rang spatial dependency. Evaluation on three synthetic and one real datasets demonstrates the superiority of our work. For example, on the Beam dataset, our work leads to 54.3% lower prediction errors and 9.87% fewer learnable network parameters.", "sections": [{"title": "Introduction", "content": "Simulation of complex mechanic systems is crucial in many real world applications, e.g., Solid Mechanics (Zienkiewicz and Taylor 2000) and Fluid Mechanics (Reddy 2015). Partial Differential Equations (PDEs) have been widely used to model the underlying mechanics, and Finite Element Method (FEM) now becomes the de-facto numeric solver for PDEs. It is mainly because FEM simulations provide valuable resources to remove instances of creating and testing expensive product prototypes for high-fidelity situations.\nFig. 1 gives the FEM simulation result of an example steering wheel. The left sub-figure shows the mesh structure divided by an FEM mesh generator, and the right one plots the heatmap of effective stress on the mesh structure. If the effective stress exceeds a certain threshold, the wheel might twist or even fracture. By FEM simulation, mechanic engineers can easily identify product design defects and then optimize the design for better mechanic performance.\nWhen the number of divided meshes is high (e.g., tens of thousands or even more), solving the PDEs by FEM is computationally intensive and costly. Even with minor changes to a mechanic system, e.g., the force F or the geometry"}, {"title": "Problem Definition", "content": "To learn mechanic simulation, we first model the mechanic system as a mesh graph as shown in Fig. 2. To this end, we can exploit a mesh generator that is nowadays widely provided by FEM tools, and discretize the mechanic system within a d-dimensional physical domain \u03a9 C Rd with d = 2 or 3 into mesh structures. Depending on the simulation, these mesh structures consist of either surface elements or volume ones. Essentially, these discrete mesh structures, i.e., finite elements, approximate the geometrical shape or volume of the mechanic system.\nNext, we model discrete mesh structures by a mesh graph G = (V, E). Each node vi \u2208 V is with a d-dimensional co-ordinate xi. Denote vi and vj with vi \u2260 vj \u2208 V to be the endpoints of an edge, We then associate the edge with two"}, {"title": "Framework", "content": "In Fig. 2, our solution follows an Encoder-Processor-Decoder framework. The Encoder first learns graph node and edge embedding vectors. Next, the Processor exploits two developed blocks, Graph Block (GBK) and Attention Block (ABK), to aggregate node embedding vectors via graph message passing. Finally, the Decoder generates the output Y from the aggregated embedding vectors."}, {"title": "Encoder", "content": "We perform the encoding operator on the input {G, F, B} to generate node and edge embeddings, vi \u2208 V and ei,j, by the embedding function & and \u025be, respectively.\n\nvi = (fi, bi), ei,j = \u025b (\u010dij, ||\u0113ij||).  (1)\n\nIn the equation above, vi is the embedding vector of node vi regarding the external force f\u2081 and boundary conditions bi applied onto this node, and ei,j is the embedding vector of the edge from node i to node j regarding the displacement vector eij and its Euclidean distance ||ij||."}, {"title": "Processor", "content": "In Fig. 2, this stage involves a sequence of M layers, each of which consists of a Graph Block (GBK) followed by an Attention Block (ABK). The GBK works within the input fine mesh graph G to learn local neighbor interactions. Subsequently, the ABK performs on coarse graphs (the mesh graph coarsening algorithm will be given soon) to effectively capture the global structure of the mechanic system via Transformer. In this way, the GBK and ABK work together on the two-level fine and coarse mesh graphs to learn local and global representation. For the l-th layer with 1 < 1 < M - 1, we have\n\n\u00d1\u00b9, El+1 \u2190 GBK(V', E'), Vl+1 \u2190 ABK(\u00d1\u00b9, E\u00b2+1)  (2)\n\nFor the sake of readability, we do not specially mention the vectors on the l-th layer in the rest of the paper."}, {"title": "Decoder", "content": "This stage decodes node embeddings back into physical response output by a decoding function \u03b4\u03c5 (\u00b7).\n\nyi = du (vi).  (3)\n\nIn the Encoder and Decoder above, we implement their associated functions such as \u03b5\u03c5, \u03b5e and \u03b4\u03c5 by MLPs."}, {"title": "Graph Block", "content": "This Graph Block (GBK) updates the node and edge embeddings, vi and ei,j, via message passing within the fine graph G. Firstly, an edge embedding ei,j is updated by the embeddings of the two endpoint nodes vi and v; of the edge to effectively learn their interactions. Subsequently, a node embedding vi is updated by aggregating those updated edge embeddings ei,j that connect to node vi.\n\nei,j \u2190 ei,jfE (ei,j, Vi, Vj)\nVi \u2190 Vif (vi, j\u2208N; ei,j)  (4)\n\nIn the equation above, fE(\u00b7) and fV(\u00b7) denote the update functions regarding the edge and node embeddings, respectively, and Ni indicates the direct neighbors of node vi. Here, unlike traditional GNNs, we exploit residual networks (He et al. 2016) to learn the changes between the original embeddings and updated ones. After that, we have the updated embeddings by the addition operation on the original embeddings and changes."}, {"title": "Attention Block", "content": "Unlike the GBK above, we develop the Attention Block (ABK) to learn global representation by a Transformer model. Recall that the Transformer requires computing in-tensive dot-product operations. Given an input graph G with a large number of nodes, the Transformer on such a graph"}, {"title": "Node Embedding Aggregation", "content": "As shown in Fig. 3, the ABK aggregates the node embeddings from the fine graph G to the coarse graph G. Specifically, we assume that the coarse mesh graph Ge contains N\u00ba nodes v with 1 \u2264 i \u2264 N\u00ba. Denote G(v) to be the group of fine mesh nodes in G that are mapped to the coarse node v. Then, we define the following aggregation operation.\n\nv\n= \n\n1\nG(v)\n \u03a3\u03c5\u03ba\u2208G(v) Vk  (5)\n\nIn the equation above, we perform the aggregation operation by an average over the embeddings vk of fine nodes vk in the group G(v). Such an average greatly reduces the number of learnable network parameters for higher efficiency."}, {"title": "Laplacian Position Encoding", "content": "We exploit the Laplacian Position Encoding to better learn the geometry information of the coarse graph Go. That is, for those nodes that are closer regarding their positions in the graph, the encoding leads to more similar positional features, and vice versa. Note that we have already used the relative coordinate displacement vectors \u0113ij = xi \u2013 xj, instead of absolute node coordinates x\u2081. Such relative coordinate displacement vectors facilitate the encoding to meet the aforementioned goal. In addition, due to a smaller number of nodes in the coarse graph Ge than the fine graph G, the ABK can achieve more efficient Laplacian Position Encoding on Ge.\nThe ABK follows the following step to compute the encoding. For the coarse graph Go, we first need to compute a Laplacian matrix L\u00ba \u2208 RN\u00b0\u00d7N\u00b0 by L\u00b0 = D\u00ba \u2013 A\u00ba, where A is the adjacency matrix of Ge and De is the diagonal degree matrix regarding the node degree in Ge."}, {"title": "Coarse Nodes Transformers", "content": "Until now, each coarse node is associated with two vectors v and p\u00e5 given by Eqs. (5 and 6). Next, denote V\u00ba = {v} and P = {p} to be the entire vectors of all coarse mesh nodes in Ge. Then, the ABK employs Transformers on coarse mesh nodes to learn their spatial dependencies. As shown in Fig. 4(a), the ABK first concatenates the layer-normalized node features Ve with the position-encoded features PC:\n\nZ = Concat [LayerNorm (V\u00ba), P]  (7)\n\nAfter that, the vector Z is then passed through a shared linear transformation to generate the query Q, key K, and value V vectors required for the attention mechanism:\n\nQ, K, V = Linear(Z)  (8)\n\nAs shown in Fig. 4(b), the attention mechanism computes attention scores by the dot product between queries and"}, {"title": "Scaling", "content": "keys, scaled by the inverse square root of the key vectors' dimensionality dn to avoid overly large dot product values.\n\nVC/\n= Softmax\n\nOKT QK\n\u221adn\n\u03bd  (9)\n\nSubsequently, the ABK applies multi-head self-attention by replicating the attention mechanism by H times independently (see Fig. 4(b)). The outputs from these H heads are concatenated and summed to generate a single vector, which is fed into the following layers.\n\nVC\" = MLP (LayerNorm(V\u2032 \u2295 V\u00ba)) \u2295 (V\u2032 \u2295V\u00b0) (10)\n\nAfter the operations above are applied, the updated coarse node embedding Ve now involves the individual embedding and contextual relationships within the coarse graph. It enables the updated embedding to adaptively learn the complex patterns in the coarse mesh graph, providing a comprehensive representation of the global structure."}, {"title": "Dissemination", "content": "Finally, the ABK sends the updated embeddings of coarse nodes back to the original fine nodes.\n\nvi \u2190 {v;\" | i \u2208 G(v)} \u2295 vi  (11)\n\nIn this equation, if the fine mesh node vi belongs to the group G(v) of fine mesh nodes in G that are mapped to the coarse node vf, the ABK then performs an addition operation over the embedding v\" and its own embedding vi. Now, the updated embedding vi incorporates both the global and local representation."}, {"title": "Model Training", "content": "To train the network, we first normalize the simulation in-put and output data using mean and variance. We employ an L2 loss function, L\n= \n\n1\nN\nNi=1\nni\n \u03a3\nj=1\n(\u0423\u0456,\u0458 \u2013 \u04f0i,j)2, to measure the loss between prediction values \u0177i,j and ground truth yi,j, where N denotes the number of datasets. ni represents the number of nodes in sample i. We use the Adam optimizer to train the neural network."}, {"title": "Experimental Setting", "content": "Datasets We use one generated, one real and two open datasets for performance evaluation.\n\nBeam: We use a popular FEM solver, ABAQUS 1, to generate this dataset on a 2D rectangular Cantilever Beam structure of the size 100 x 15 mm\u00b2. A circle hole with a radius r = 2.5 mm is within the beam structure. By varying the center position (x, y) of the hole, we generate 111 = 3 * 37 Beam objects, i.e., (x,y) = (5+2.5 * i,5 + 2.5 * j) with i = 0, 1, 2 and j = 0, 1, ..., 36. We apply the external force F with 300 newtons with 5 various directions in the reverse direction of the y-axis at the end of the beam and boundary conditions at the other end. For an individual beam, we discretize its surface into 2D triangle grids by using the mesh generation tool provided by ABAQUS, and perform the FEM simulation to generate the mechanic response (as ground truth).\nSteering-Wheel: We use a real industrial data set provided by an automobile part supplier with 239 different steering wheel objects. Following the bending mechanic trial standard of automobile steering wheels, expert engineers in the automobile company apply an external force F with 700 newtons in the reverse direction of the z-axis at the center of the steering wheel rim and meanwhile fix the wheel on a bottom plane. An industry-level FEM solver LS-DYNA2 is used to simulate the torsion test and measure the resulting stress field in the steering wheel. Here, the generated mesh structure includes three mixed types of grids (hexa-, penta-, and tetra-dedral).\nElasticity (Li et al. 2024): This open dataset simulates the behavior of solid materials under various loading conditions. with input point clouds and output stress fields. We pre-process the dataset by the Delaunay triangulation method to ensure that the dataset can work for GNNs.\nCylinderFlow (Pfaff et al. 2021): This open 2D fluid mechanics dataset examines fluid dynamics around a cylindrical obstacle on vortex patterns, i.e., the Von Karman vortex street. Since this dataset contains 600-step time series data, we follow the work solver-in-the-loop (Um"}, {"title": "Evaluation Result", "content": "Baseline Study In Table 2, our work performs best on almost all datasets, for example, with 54.30% lower errors than MS-GNN on the Beam dataset. Compared to the flat network model MGN, hierarchical models such as MS-MGN, MS-GNN, Eagle, and ours lead to lower errors. However, Geo-FNO does not work well with rather high errors due to irregular mesh structure in these datasets for complex mechanic simulation. In addition, from the result of the CylinderFlow time series data, our work performs best for the mid- and long-term prediction (t = 250 and t = 600 time steps) but not the very short-term prediction (t = 1).\nBesides, in the rightmost column of Table 2, we list the number of learnable network parameters for all models on the Beam dataset. Due to the adopted average operator in Eq. (5), our work leads to 9.87% fewer network parameters compared to MS-GNN. Here, the key insight is that our model outperforms the five competitors by the lowest MSE errors meanwhile with the fewest network parameters.\nFig. 5 visualizes the ground truth of an example cantilever beam structure, prediction result of four models, including the predicted mechanic response (left), the difference between predicted response and ground truth (middle), and the overlaid gradient of each mesh node against the node at the lower left corner in the beam structure (right). Here, we follow the similar idea of Eagle (Janny et al. 2023) to compute the gradients. This figure clearly demonstrates that our result is the closest to the ground truth. In terms of the overlaid gradient, our work can precisely visualize the top area that is applied by the external force F. Such result is meaningful because the two works (MGN and MS-GNN) are inherently limited to very close neighborhood determined by the number of message passing, and the receptive field, which is represented as lower-left concentric square overlaid over the gradients. Yet, Eagle does not illustrate the top area applied by the external force. Instead, our model is not spatially limited and can pass messages across the entire scene (partic-"}, {"title": "Study of Network Block Structure", "content": "Fig. 6a gives four network block structures including ours and three alternatives. Here, in the first two structures (1-2), the GBK and ABK blocks (i.e., the residual networks) are both within each of the M network layers, differing from the order of such two blocks, and the two rest structures (3-4) have the independent M-layer ABK (and GBK) blocks.\nAs shown in Fig. 6b, our work, i.e., the (1) block structure, is with the lowest errors on the three datasets. Particularly, the first two structures (1-2) perform much better than the rest two (3-4). It is because the first two structures can ensure that each of the M layer can learn both local and global node embeddings, and the M layers work together to interweave such two embeddings for better representation. Moreover, the block structures with the GBK first order (1, 3) perform better than the ones with the ABK first (2, 4). The reason is that the first learned global representation by the ABK may otherwise obscure the local one by the GBK."}, {"title": "Study of Coarsening Algorithms", "content": "In Fig. 6c, on the Beam dataset, we compare the used Louvain algorithm to generate coarse mesh node graphs against the following competitors.\n\nGrid Sampling (Grid) (Lino et al. 2022): By partitioning a multi-dimensional space into a set of grids, we choose those mesh nodes within a grid as a cluster.\nk-Means Sampling: This method applies the original k-Means clustering algorithm, which partitions the data into k clusters by minimizing the variance within each cluster. Each data point is assigned to the nearest cluster center, and these centers are iteratively updated.\nSame-Size-k-Means Sampling (Same-Size) (Ganganath, Cheng, and Tse 2014): An adaptation of the k-Means clustering algorithm that, in addition to minimizing variance, also ensures each cluster has approximately the same number of elements.\nHeuristic Uniform Sampling (HEURISTIC): We first randomly pick a single seed node and choose its k-hop nearest nodes as a cluster. We repeat this step among the remaining nodes until all nodes are clustered.\nFarthest Point Sampling (FPS)(Qi et al. 2017): By randomly choosing an initial point, FPS iteratively selects the point that is farthest from the already selected points,\n\nuntil the desired number of points is reached. FPS ensures that the selected points are well-distributed to capture the essential characteristics of the dataset.\nFor fairness, we expect that all these algorithms can generate coarse graphs with the equal number of coarse mesh nodes. To this end, since the Louvain algorithm does not pre-specify the number of communities (or clusters), we first apply the Louvain algorithm to generate coarse mesh node graphs (with the node count 14 on average) on the input fine graphs (with 523 mesh nodes on average). Next, by using the count of such coarse nodes as input, the five rest algorithms then generate the associated coarse node graphs.\nFrom this figure, we find that the Louvain algorithm leads to the lowest error. This is because the Louvain algorithm divides input graph nodes into multiple groups mainly depending upon the graph topology connectivity. Other algorithms, such as k-means and its variant Same-Size-k-means, mainly exploit node coordinates to compute node distance, and thus may not capture the graph topology, despite their widespread use in point clouds."}, {"title": "Study of Position Encoding Schemes", "content": "To demonstrate the superiority of the Laplacian position encoding, we compare our Laplacian scheme against the Sinusoidal scheme (Vaswani et al. 2017), which is adopted by the recent work Eagle (Janny et al. 2023). To be consistent with the work Eagle, we use the Sinusoidal scheme to encode the absolute position coordinates of mesh nodes by applying sine and cosine functions with varying frequencies. To evaluate the performance, we purposely change the centers of coordinate systems in our testing data, by horizontally moving its original coordinate center by 20 mm. In this way, we can study how the position encoding scheme can adapt such a change.\nFig. 6d plots the difference between the prediction result and ground truth. For each encoding scheme, we have two bars: the left bar is the original result before the change of coordinate centers and the right one is the result after the change. As shown in this figure, when compared to the Sinusoidal scheme, in the two bars, the Laplacian scheme both leads to the lower errors. Such result indicates the adopted Laplacian scheme can best represent mesh nodes, no matter coordinate centers change or not."}, {"title": "Conclusion", "content": "In this paper, we propose a novel two-level mesh graph network to represent complex mechanic interaction between"}]}