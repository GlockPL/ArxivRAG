{"title": "Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization", "authors": ["Haemin Park", "Diego Klabjan"], "abstract": "Federated Learning (FL) faces significant challenges related to communication efficiency and heterogeneity. To address these issues, we explore the potential of using low-rank updates. Our theoretical analysis reveals that client's loss exhibits a higher rank structure (gradients span higher rank subspace of Hessian) compared to the server's loss. Based on this insight, we hypothesize that constraining client-side optimization to a low-rank subspace could provide an implicit regularization effect. Consequently, we propose FedLoRU, a general low-rank update framework for federated learning. Our framework enforces low-rank client-side updates and accumulates these updates to form a higher-rank model. Additionally, variants of FedLoRU can adapt to environments with statistical and model heterogeneity by employing multiple or hierarchical low-rank updates. Experimental results demonstrate that FedLoRU performs comparably to full-rank algorithms and exhibits robustness to heterogeneous and large numbers of clients.", "sections": [{"title": "1 Introduction", "content": "Federated learning (McMahan et al., 2017) is a collaborative learning framework designed to enhance privacy preservation in machine learning applications. This approach has gained importance due to rising concerns over data privacy, as it allows multiple participants to train a model collectively without sharing raw data.\nWhile federated learning offers privacy benefits, it trades off some performance compared to centralized learning. Two primary factors contributing to this trade-off are communication overhead and heterogeneity. Despite improvements in computation and memory capacities, communication speeds have only slightly improved, making communication overhead a major factor in slowing down federated learning (Zheng et al., 2020). Additionally, various forms of heterogeneity\u2014statistical, system, and device further complicate FL (Kairouz et al., 2021; Ye et al., 2023). These issues are especially pronounced with a large number of clients, where frequent, less impactful updates slow down training and reduce performance.\nAddressing these challenges is becoming increasingly critical for training large language models (LLMs) in a federated learning framework. By low rank herein we refer to gradients spanning a low rank subspace of Hessian at any weights or the weight matrix being of the form AB where the number of columns of A is low. Utilizing private datasets on edge devices for LLM training is promising due to the limited availability of public data (Ye et al., 2024). However, this approach presents significant issues, notably in terms of communication overhead, as edge devices possess heterogeneous resources and data. Additionally, the need for effective regularization across clients is required. Consequently, the development of algorithms to tackle these challenges is an essential problem to bridge the gap between practical and conceptual federated learning applications.\nThere has been substantial research focusing on the low-rank characteristics in centralized learning. Utilizing low-rank factorized update models such as LoRA (Hu et al., 2021), DyLoRA (Valipour et al., 2022), and QLORA (Dettmers et al., 2024) can significantly reduce the number of trainable parameters, which helps to conserve memory and computational resources. Further observations (Huh et al., 2021; Ji and Telgarsky, 2018) indicate that over-parameterized models tend to find low-rank solutions, which provide implicit regularization effects."}, {"title": "2 Related work", "content": "Communication-Efficient Federated Learning Extensive research has been conducted to address the communication challenges in federated learning (Shahid et al., 2021). FedPAQ (Reisizadeh et al., 2020) and AdaQuantFL (Jhunjhunwala et al., 2021) employ quantization techniques that decrease the precision of weights and activations in neural networks. Besides, Fed-Dropout (Caldas et al., 2018) and FedMP (Jiang et al., 2023) utilize pruning to eliminate less important neurons or connections within models. Since quantization and sparsification techniques do not alter the fundamental network architecture and can be universally applied across any models, they are perceived as additional steps aimed at reducing communication overhead.\nIn contrast to these methods, model compression techniques in federated learning alter the model structure itself by transforming the original model into a smaller model update before communication, then restoring it before local training or aggregation. FedDLR (Qiao et al., 2021) employs low-rank approximation to compress the model during both server-to-client and client-to-server communications, reverting to the full-rank model during local training. On the other hand, FedHM (Yao et al., 2021) compresses the model solely during server-to-client communication, where clients train factorized low-rank models without reverting them and the server restores the local models before aggregation. While both FedDLR and FedHM effectively reduce communication overheads, their server-side compression approaches can lead to performance degradation. To mitigate potential information loss during server-side compression, we focus on client-side factorization, avoiding compression processes.\nLow-rank nature of centralized and federated learning Despite the over-parameterization of current deep learning models, numerous studies (Gur-Ari et al., 2018; Li et al., 2018; Sagun et al., 2016) assert that the training process in deep learning inherently possesses a low-rank nature. Low-Rank Adaptation (LoRA, Hu et al. (2021)) is a representative algorithm that leverages this low-rank characteristic, particularly for fine-tuning tasks. However, effectively utilizing the low-rank structure during the pre-training phase remains challenging. This difficulty is elucidated by some papers (Yu and Wu, 2023; Zhao et al., 2024), which attribute the issue to the weights not exhibiting a low-rank.\nExisting research in federated learning has attempted to exploit the low-rank nature observed in centralized learning. LBGM (Azam et al., 2021) and FedLRGD (Jadbabaie et al., 2023) approximate gradients by utilizing old or sampled gradients under the assumption that gradients live in a low-rank subspace. Nonetheless, there is a noticeable gap in their assumption and analysis regarding the rank characteristics specific to federated learning. In the context of federated learning, there is a complex loss landscape involving multiple client-side and a single server-side optimization, and leveraging a low-rank structure needs to consider their respective rank structures. To our knowledge, no prior work has examined the rank structure in federated learning contexts without making very stringent assumptions. Our study is pioneering in addressing this gap, using analytical results and insights to develop a novel algorithm.\nLow-Rank Adaptation The fundamental concept of LoRA involves freezing the pre-trained weights and adapting them to new tasks by introducing and optimizing an update through a two-layer low-rank decomposition. This is represented mathematically as $W = W_0 + AB$ where $W \\in \\mathbb{R}^{m\\times n}$, $A \\in \\mathbb{R}^{m\\times r}$, $B\\in \\mathbb{R}^{r\\times n}$, $r < m,n$. By constraining the update matrix to a low-rank factorization, the number of parameters in the update matrix is reduced. However, this method demonstrates suboptimal performance during pre-training because the trained weight matrix does not exhibit a low-rank structure. To address this, ReLORA (Lialin et al., 2023) seeks to achieve a higher-rank model by accumulating multiple low-rank updates, expressed as $W = W_0 + \\sum_{i=1}^{M} A_iB_i$ where $A_i \\in \\mathbb{R}^{m\\times r}$, $B_i \\in \\mathbb{R}^{r\\times n}$.\nLow-Rank Adaptation in Federated Learning Recent studies have studied the application of LORA within federated learning frameworks. Notable algorithms, such as FedLORA (Wu et al., 2024; Yi et al., 2023), FFALORA (Sun et al., 2024), and Hyperflora (Lu et al., 2024), employ LoRA adapters to facilitate personalization. These methods apply low-rank adaptation to a pre-trained model during the local personalization training phase. On the other hand, other works such as Kuo et al. (2024) and Cho et al. (2023) apply LoRA for fine-tuning within federated learning environments.\nThese approaches use only one low-rank matrices that restricts the model to lie in low-rank subspace, on the other hand, our work utilizes multiple accumulated layers of low-rank matrices allowing the model to achieve higher rank. Specifically, we extends the concept of LoRA by incorporating client-side"}, {"title": "3 Low-rank updates in federated learning", "content": "In centralized learning, neural network losses exhibit a low-rank structure, indicating that the gradient lies within the subspace spanned by the k eigenvectors of the Hessian (Gur-Ari et al., 2018) during training. While efforts have been made to utilize this low-rank structure to enhance federated learning algorithms, there is a lack of studies analyzing the rank structure of federated learning. In federated learning, the clients and server have distinct losses, resulting in different rank structures. Understanding these differing rank structures of client and server losses is crucial for developing low-rank-inspired algorithms tailored for federated learning.\nIn this section, we conduct a theoretical analysis of the rank structure of federated learning, with a particular emphasis on comparing the rank of client and server Hessians. Drawing from this theoretical analysis, we propose a novel federated learning algorithm FedLoRU designed to address communication efficiency and performance degradation issues associated with a large number of clients. Additionally, we introduce an algorithm that modifies FedLoRU to address challenges related to model and statistical heterogeneity in federated learning."}, {"title": "3.1 Higher rank nature of clients in federated learning", "content": "Notation and problem setup Suppose (x, y) is a data generating distribution for an input-output pair (x, y) \u2208 Rdz \u00d7 Rdy. We consider the problem of finding a prediction function hR(\u00b7; \u00b7) : Rd\u00a3 \u00d7RR \u2192 Rdy parameterized by a R-dim weight vector WR \u2208 RR. Given a loss function l(\u00b7, \u00b7) : Rdy \u00d7 Rdy \u2192 R, the true or population risk is defined as the loss over the data-generating distribution (x, y)\n$L_{true} (h_R, w_R) = \\int l(h_R(x; w_R), y)d(x, y)$.\n(1)\nThe corresponding true Hessian is $H_{true}(h_R,W_R) = \\nabla^2L_{true}(h_R, w_R)$. If $D_N = \\{(X_1,Y_1),\\cdots, (X_N, Y_N)\\}$ is a dataset generated from the distribution 4, then the loss and Hessian corresponding to the dataset $D_N$ are given by:\n$f_N(h_R, w_R) = \\frac{1}{N}\\sum_{(x,y) \\in D_N}l(h_R(x; w_R), y), H_N(h_R, w_R) = \\frac{1}{N}\\sum_{(x,y) \\in D_N} \\nabla^2l(h_R(x; w_R), y)$.\n(2)\nWe consider random selection of M samples without replacement from DN to form a sub-dataset $D_M\\subseteq D_N$. Let $f_M(h_R,W_R)$ and $H_M(h_R,W_R)$ denote the loss and Hessian for the sub-dataset $D_M$. For simplicity, we omit the explicit dependency on hR and wR when contextually clear. In federated learning, fv can be considered as the loss that the server optimizes, while fm represents the loss of a local client assuming the homogeneous setting.\nFor non-zero real numbers $\\theta_1,\\cdots, \\theta_k$, define $\\Omega_R(\\theta_1,\\ldots, \\theta_k)$ as the family of pairs $(h_R, w_R)$, where hr is an R-dimensional prediction function and w\u20a8 is a weight vector, such that the true Hessian has eigenvalues $\\theta_1,\\cdots,\\theta_k$. Specifically, $\\Omega_R(\\theta_1,\\ldots, \\theta_k) = \\{(h_R,w_R) : H_{true}(h_R,w_R) \\text{ has eigenvalues }\\theta_1,\\cdots,\\theta_k\\}$. Let $\\Omega(\\theta_1,\\ldots, \\theta_k) = \\bigcup_R\\Omega_R(\\theta_1,\\ldots, \\theta_k)$, representing the union of $\\Omega_R(\\theta_1,\\cdots,\\theta_k)$ over all dimensions R. We aim to show that the difference in stable rank between the Hessians of a server and a client eventually becomes positive as dimension R approaches infinity within the space of $\\Omega(\\theta_1,\\ldots, \\theta_\\kappa)$.\nIn fact, the set of all possible pairs $(h_R, w_R)$ is represented by the union over all dimensions R, integers k < R, and non-zero real values $\\theta_1,\\ldots, \\theta_k$ as follows:\n$\\{(h_R, w_R) : \\text{dimension }R < \\infty\\} = \\bigcup_{R=1}^{\\infty} \\bigcup_{k=1}^{R} \\bigcup_{(\\theta_1,\\ldots,\\theta_k)\\in\\mathbb{R}^k} \\Omega_R (\\theta_1,..., \\theta_\\kappa)$.\nThus, for any given pair $(h_R,w_R)$, there exist $\\theta_1,\\ldots,\\theta_k$ such that $(h_R,w_R) \\in \\Omega_R(\\theta_1,\\ldots,\\theta_k)$. According to the following proposition, the proof of which is provided in Appendix A.1, either the set $\\Omega(\\theta_1,\\ldots, \\theta_\\kappa)$ is empty or there exist infinitely many values of R for which $\\Omega_R(\\theta_1,\\ldots, \\theta_k) \\neq \\emptyset$."}, {"title": "Proposition 3.1.", "content": "Let $\\theta_1,\\ldots,\\theta_k$ be fixed non-zero real numbers, and suppose there exists $\\hat{R} > k$ such that $\\Omega_{\\hat{R}}(\\theta_1,\\ldots, \\theta_k)$ is non-empty. Then there are infinitely many R such that $\\Omega_R(\\theta_1,\\ldots, \\theta_k)$ is non-empty. In particular, $\\Omega_R(\\theta_1,\\ldots, \\theta_k)$ is non-empty for all $R > \\hat{R}$.\nComparing the stable rank of the client and server Hessians Now, we will focus on comparing the stable rank of the client and server Hessians. For given $p, q \\in \\mathbb{N}$, let $\\theta_1 > \\ldots > \\theta_p > 0 > \\theta_{p+1} > \\ldots > 0 > \\theta_{p+q}$ be deterministic non-zero real numbers, and let $(h_R,w_R) \\in \\Omega(\\theta_1,\\ldots, \\theta_{p+q})$ for some R. To compare the stable rank of Hessians for two datasets $D_N$ and $D_N$, we consider the additive perturbed model of the true Hessian as described by Baskerville et al. (2022):\n$H_N(h_R, W_R) = H_{true}(h_R, W_R) + E_R(N)$ (3)\n$H_M(h_R,w_R) = H_{true}(h_R,W_R) + e_R(M)$. (4)\nHere, $E_R(N), E_R(M) \\in \\mathbb{R}^{R\\times R}$ are defined as random error matrices associated with each Hessian. These matrices are assumed to be scaled according to $E_R(N) = s(N)X_R$, where $X_R \\in \\mathbb{R}^{R\\times R}$ is a random real symmetric matrix and s : N\u2192 (0, 1) is a decreasing function.\nAnother study (Granziol et al., 2022) employs the model $H_M(h_R,W_R) = H_N(h_R,W_R) + e_R$, implying a dependency structure between $H_M$ and $H_N$. However, their analysis assumes independence between these matrices, which is problematic given the underlying model and practical considerations. In contrast, we address this issue by introducing two decoupled additive perturbed models. Additionally, while Granziol et al. (2022) investigates outlier eigenvalues, our focus is on the difference in the rank of the Hessians.\nWe seek to determine the limiting eigenvalues of the Hessians $H_N(h_R,W_R)$ and $H_M (h_R, W_R)$ in relation to the eigenvalues of $H_{true}(h_R, W_R)$. Since $(h_R,w_R) \\in \\Omega_R(\\theta_1,\\ldots, \\theta_{p+q})$, the eigenvalues of $H_{true}(h_R,W_R)$ are $\\theta_1,\\cdots,\\theta_{p+q}$. Next, we need to make some assumptions about the random error matrix $X_R$. Assume $X_R$ is a random real symmetric matrix with eigenvalues $\\lambda_1(X_R),\\ldots, \\lambda_R(X_R)$ and a limiting spectral density \u03bc, such that $\\frac{1}{R} \\sum_{i=1}^{R} \\delta(\\lambda - \\lambda_i(X_R)) \\rightarrow \\mu(\\lambda)$, with convergence in the weak almost sure sense. Examples of matrices exhibiting a well-defined limiting spectral density include Wigner matrices, Wishart matrices, and Gaussian ensembles. We assume \u03bc is a compactly supported probability measure on $[l_\\mu,\\gamma_\\mu]$ which admits a smooth density with respect to the Lebesque measure and the eigenvectors of $X_R$ obey quantum unique ergodicity (QUE). For more detail about the QUE condition, we refer to Baskerville et al. (2022). We can now find the limiting eigenvalues of Hy and HM."}, {"title": "Proposition 3.2", "content": "(Limiting eigenvalues of $H_N$ (modified from Baskerville et al. (2022))). Let R be any integer such that $R > \\hat{R}$ where $\\hat{R}$ is the smallest integer such that $\\Omega_R(\\theta_1,\\ldots,\\theta_{p+q})$ is non-empty. For any pair $(h_R, W_R) \\in \\Omega_R(\\theta_1,\\ldots, \\theta_{p+q})$, consider the Hessian additive error model given by $H_N(h_R,w_R) = H_{true}(h_R, W_R) + e_R(N)$. If $\\lambda_i(H_N(h_R,w_R))$ denotes the i-th eigenvalue of $H_n(h_R,w_R)$, then for i= 1,\u2026\u2026,p, the following holds:\n$\\lambda_i(H_N(h_R, W_R)) \\rightarrow \\begin{cases}g_N^{-1}(0_i) \\text{ if } g_N^{-1}(0_i) > U_N \\\\U_N \\text{ otherwise}\\end{cases}$ (5)\nas $R \\rightarrow \\infty$, while for each fixed $i > p$, $\\lambda_i(H_n(h_R,w_R))$ that converges to a limit in $\\mathbb{R}\\[0, U_N]$ converges to $U_n$, i.e., $\\lambda_i(H_N(h_R,W_R)) \\rightarrow U_N$. Similarly, for i = 0,\u2026\u2026, q \u2212 1, we have\n$\\lambda_{R-i}(H_N(h_R, W_R)) \\rightarrow \\begin{cases}g_N^{-1}(0_{p+q-i}) \\text{ if } g_N^{-1}(0_{p+q-i}) < L_N \\\\L_N \\text{ otherwise}\\end{cases}$ (6)\nwhile for each fixed $i \\geq q$, $\\lambda_{R-i}(H_N(h_R,W_R))$ that converges to a limit in $\\mathbb{R}\\[L_v,0]$ converges to $L_n$, i.e., $\\lambda_{R-i}(H_N(h_R, W_R)) \\rightarrow L_v$. Here,\n$g_N^{-1}(0) = 0 + s(N)R_\\mu(s(N)0^{-1})$ (7)\nand $U_N$ and $L_N$ are lower and upper bounds of the limiting distribution $\\mu_n$ of $e_R(N)$.\nConvergence in Proposition 3.2 is weak almost sure convergence and $R_\\mu(w)$, known as the R-transform, is defined by $R_\\mu(w) = S_\\mu^{-1}(w) - \\frac{1}{w}$ where $S_\\mu(w)$ is the Stieltjes transform. Compared to Baskerville et al. (2022), which focuses solely on outlier eigenvalues, we extend the analysis to bulk eigenvalues and adopt a"}, {"title": "Stable rank", "content": "To compare the rank properties of Hessians of a client and a server, we introduce the concept of stable rank, defined as the square of the ratio between the Frobenius norm and the spectral norm of a matrix A:\n$srank(A) = \\frac{||A||_F^2}{||A||^2} = \\frac{\\sum_{i=1}^n \\sigma_i^2(A)}{\\sigma_1^2(A)}$ (8)\nwhere n is the rank of the matrix A and $\u03c3_i(A)$ is the i-th singular value of the matrix A. Stable rank serves as a continuous proxy for rank(A) and is known for its robustness against small perturbations. In fact, stable rank which emphasizes eigenvalues near the top eigenvalue can be considered a more accurate surrogate of the rank structure of the Hessian considering the empirical evidences that gradients are highly influenced by the top Hessian eigenvector, i.e., the eigenvectors corresponding to the largest eigenvalues. Additionally, bounds on the stable rank of a weight provide control over the model's complexity (Georgiev et al., 2021).\nIn the following theorem, we demonstrate that smaller datasets result in a higher limiting stable rank. Furthermore, given that modern neural network models typically possess a very large number of parameters, this finding is likely applicable to contemporary models."}, {"title": "Theorem 3.3", "content": "(Higher rank nature of Hessian of smaller dataset). Let $N > M > R$ be any integers where R is the smallest integer such that $\\Omega_R(\\theta_1,\\ldots,\\theta_{p+q})$ is non-empty. For any pair $(h_R,W_R) \\in \\Omega_R(\\theta_1,\\ldots, \\theta_{p+q})$, let $H_N(h_R, w_R)$ and $H_M(h_R,W_R)$ be the Hessians as defined previously. The difference in stable rank between $H_N(h_R,W_R)$ and $H_M(h_R,W_R)$ converges weakly almost surely to positive value $S(\\theta_1,\\ldots, \\theta_{p+q}, \\mu) > 0$ as $R \\rightarrow \\infty$, i.e.\n$srank(H_M(h_R, W_R)) \u2013 srank(H_N(h_R,w_R)) \\rightarrow S(\\theta_1,\\ldots, \\theta_{p+q}, \\mu) > 0$. (9)\nHere, the value $S(\\theta_1,\\ldots,\\theta_{p+q}, \\mu)$ does not dependent on the sequence $(h_R,W_R)$.\nIn federated learning, Theorem 3.3 implies that individual clients, often working with smaller, heterogeneous datasets, inherently possess a higher rank structure in their local Hessians compared to the Hessian of the server loss. This higher rank structure can lead to larger discrepancies across clients, as the complexity"}, {"title": "3.2 Federated Low-Rank Update(FedLoRU) Algorithm", "content": "In this section, we introduce the Federated Low-Rank Updates (FedLoRU) algorithm, along with its variants designed to adapt to statistical and model heterogeneity.\nConsider a federated learning system with K clients, where each client k has its own loss function $f_k : \\mathbb{R}^{m\\times n} \\rightarrow \\mathbb{R}$. In conventional federated learning algorithms such as FedAvg (McMahan et al., 2017), the server aims to find a global model $W \\in \\mathbb{R}^{m\\times n}$ that minimizes the aggregated loss function $f(W) = \\sum_{K}^{k=1}p(k) f_k (W)$, where p(k) is the weight of client k. During each round t, the server selects a set of M clients KM to participate in local training. These clients receive the global model Wt-1 from the server and perform local training without altering the network architecture. The server then receives the locally updated models W(k) from the clients and aggregates them into a global model by averaging: $W_t = \\sum_{k\\in K_M}p(k) W_t^{(k)}$.\nFedeated low-rank update algorithm To enhance communication efficiency, FedLoRU constraints clients' updates to low-rank. Analogous to LoRA (Hu et al., 2021) approach, upon receiving the global model $W \\in \\mathbb{R}^{mxn}$, each client k freezes this model and performs local training through low-rank matrices $A^{(k)} \\in \\mathbb{R}^{mxr}$ and $B^{(k)} \\in \\mathbb{R}^{r\\times n}$ by solving:\n$A^{(k)}, B^{(k)} = arg \\min_{A, B} f_k (W^{(k)} + \\alpha AB)$ (10)\nwhere a is a fixed scaling hyperparameter. In FedLoRU, each client solves (10) for E epochs in each round. The server then collects A(k) and B(k) from the clients and aggregates them by averaging: $A = \\sum_{k\\in K_M}p(k) A^{(k)}$, $B = \\sum_{k\\in K_M} p(k) B^{(k)}$. After aggregation, the server broadcasts the averaged A and B to the clients, who continue local training using these matrices.\nOccasionally, the server accumulates local updates into the global model after aggregation to achieve a higher-rank global model. Clients subsequently update their local global models by $W^{(k)} \\leftarrow W^{(k)} +&AB$ and reset their low-rank matrices. When we accumulate low-rank updates every \u03c4 rounds from the initial global model Wo, the final global model at T is\n$W_T = W_0 + \\sum_{t=1}^{T} A_tB_t$ (11)\nWe employ two strategies for initializing the low-rank update matrices in FedLoRU. For random initialization, we initialize A with a random Gaussian distribution and set B to zero, ensuring that AB is zero at the start. Alternatively, for momentum initialization, we retain the existing weights of the matrices, continuing to use the previous low-rank update matrices. This approach leverages momentum effects as described in the ReLORA (Lialin et al., 2023). The scheduling of accumulations is also critical due to the varying nature of the training phases across different rounds; in this study, we employ periodic accumulation, though this area warrants further investigation.\nCommunication overhead FedLoRU reduces communication overhead from Kmn to Kmn when $r\\ll morn$. While we use a low-rank factorized model here, other options like LoKr or LoHa can be employed, differing only in the factorization scheme but maintaining identical fundamental principles. Additionally, without a compression process, there is no extra computation compared to conventional compression-based communication-efficient federated learning algorithms."}, {"title": "Algorithm 1 FedLoRU.", "content": "W is a model, A0, Bo are initial low-rank update matrices, a is a scaling factor,\nTis an accumulation cycle, T is the total training round\nRequire: W, \u0391\u03bf, \u0392\u03bf, \u03b1, \u03c4, \u0422\nInitialize: Server sends W to each client, where client k initializes it as W(k)\nfor t = 1,...,T do\nServer selects M clients KM and distributes At-1, Bt-1 to clients in KM\nfor each client k \u2208 KM do\nLocal training: Find A(k), B(k) by solving (10) starting from At-1, Bt-1\nSend A(k), B(k) to the server\nend for\nServer aggregation: At \u2190 \u03a3k\u2208Kmp(k) A(k), Bt \u2191 \u03a3k\u2208Kmp(k) B(k)\nif t mod 7 = 0 then\nServer distributes At, Bt to all clients\nEach client k updates its local model: W(k) \u2190 W(k) +&AtBt\nend if\nend for\nReturn: W + \u03a3\u03a4 - \u03b1\u2211t=1: t mod 7=0 At Bt\nLow-rank local training, higher-rank global training, and implicit regularization Moreover, FedLoRU facilitates the training of a higher-rank global model concurrently with low-rank local updates. With each accumulation of low-rank update matrices, the global model's rank is incrementally enhanced, enabling the initiation of new learning phases. This process is evident in the training curve, which exhibits noticeable pattern shifts following each accumulation. Moreover, constraining the rank of local training introduces a regularization effect, thereby diminishing the discrepancy between updated local models.\nFederated low-rank update for statistical heterogeneous setting We develop the personalized FedLoRU (pFedLoRU) algorithm to address statistical heterogeneity in federated learning using low-rank updates idea. In pFedLoRU, each client k maintains a global model W, global low-rank matrices A(k) and B(k), and personal low-rank matrices L(k) and U(k). The matrices A(k) and B(k) are shared between the server and clients to update common global model, while L(k) and U(k) are tailored to adapt to the local distribution. In each round t, client k optimizes the personal matrices for Eper epochs and the global matrices for Eglobal by solving:\n$L, U \\leftarrow arg \\min_{L, U} f_k (W^{(k)} + \\alpha_{global} A_t B_t + \\alpha_{per} L^{(k)}U^{(k)})$ (12)\n$A^{(k)}, B^{(k)} \\leftarrow arg \\min_{A, B} f_k (W^{(k)} + \\alpha_{global} AB + \\alpha_{per} L^{(k)}U^{(k)})$ (13)\nSubsequently, the server collects the global update matrices from the clients, performs aggregation, and updates the global model accordingly. A detailed description of the pFedLoRU algorithm can be found in Appendix B.\nFederated low-rank update for model heterogeneous setting When local clients possess varying hardware resources and communication speeds, it becomes impractical to use uniform low-rank matrices across all clients. To address this issue, we develop the model-heterogeneous FedLoRU (mFedLoRU) algorithm, which employs hierarchical low-rank updates. Specifically, to update the low-rank matrices A and B, we apply low-rank updates recursively, enabling model adaptation through nested low-rank updates:\n$AB \\leftarrow (A + aA_dA_u)(B+ B_dB_u)$ (14)\nHere, AB is the rank-r factorization of a matrix $W\\in \\mathbb{R}^{mxn}$, and $A_dA_u$ and $B_dB_u$ are rank-ra and rank-re factorizations of A and B, respectively. In mFedLoRU, each client k decides whether to use nested low-rank updates and, if so, determines the locally adapted rank r(k) based on its resources. If a client opts out of nested low-rank updates, it updates its low-rank modules like in FedLoRU. However, if it chooses nested low-rank updates, it generates and optimizes the nested low-rank matrices $A_dA_u$ and $B_dB_u$ by solving:"}, {"title": "Experiments", "content": "In this section, we extensively evaluate FedLoRU on pre-training and fine-tuning on different heterogeneous settings. We first provide the experiment setup such as baselines and heterogeneous settings, then move on to the performance evaluation.\n4.1 Experiment setup\nDatasets and Models To evaluate the performance of our proposed algorithms, we employ three commonly used datasets in machine learning: CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and Alpaca (Taori et al., 2023). For the image datasets, we employ ResNet-18 (He et al., 2016) as the model architecture, and for the language dataset, we use LLaMa2-3B (Touvron et al., 2023). In pre-training experiments on the image datasets, we vary the number of clients between 20 and 400 to evaluate the algorithms under diverse conditions and to investigate the effect of client numbers. At each round, 50% of the clients are randomly sampled to train models over 5 local epochs. For fine-tuning the language model, we configure 10 clients with a 50% participation rate and conduct training for 1 local epoch. The learning rate is selected through grid search, and various rank configurations are tested for FedHM and FedLoRU.\nBaseline Algorithms We conduct a comparative analysis of FedLoRU against several benchmarks: FedAvg, the standard federated learning algorithm that trains full-rank models; FedLoRA, which trains low-rank modules without accumulating low-rank updates; and FedHM, the state-of-the-art in communication-efficient federated learning. For evaluating pFedLoRU, we compare it with pFedLoRA, and for mFedLoRU, we use the system-heterogeneous version of FedHM as the comparison baseline.\nHeterogeneous Settings In the statistically heterogeneous setting, we generate disjoint Non-IID client data using a Dirichlet distribution, Dir(\u03b1), with a concentration parameter \u03b1 set to 0.5, as described in Hsu et al. (2019). For the system heterogeneous setting, we simulate virtual environments where each client is assigned a different level of computational complexity, thereby restricting them to use low-rank update matrices of varying ranks. The specific configurations for these settings are detailed in Appendix C."}, {"title": "4.2 Performance Evaluation", "content": "We evaluate the Top-1 accuracy of models with varying parameter sizes in both IID and Non-IID scenarios across different federated learning configurations.\nPerformance of Pre-training Table 1 shows the performance of FedLoRU and baseline algorithms. In our experimental evaluation, FedLoRU consistently achieves competitive or superior accuracy compared to other algorithms, demonstrating its effectiveness in federated learning environments. While FedLoRU exhibits slightly lower performance compared to FedAvg in most settings, the performance degradation is minimal when considering the significantly reduced number of parameters. This indicates that FedLoRU's low-rank update and accumulation strategy effectively reduces communication overhead without substantially sacrificing model performance. Moreover, FedLoRU surpasses other communication-efficient federated learning algorithms.\nThe client regularization effect of FedLoRU, as predicted by our theoretical analysis, suggests that using client-side low-rank updates is particularly beneficial in environments with a large number"}]}