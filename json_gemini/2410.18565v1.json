{"title": "Bielik 7B v0.1: A Polish Language Model \u2013 Development, Insights, and\nEvaluation", "authors": ["Krzysztof Ociepa", "\u0141ukasz Flis", "Krzysztof Wr\u00f3bel", "Adrian Gwo\u017adziej", "Remigiusz Kinas"], "abstract": "We introduce Bielik 7B v0.1, a 7-billion-\nparameter generative text model for Polish lan-\nguage processing. Trained on curated Polish\ncorpora, this model addresses key challenges\nin language model development through inno-\nvative techniques. These include Weighted In-\nstruction Cross-Entropy Loss, which balances\nthe learning of different instruction types, and\nAdaptive Learning Rate, which dynamically\nadjusts the learning rate based on training\nprogress. To evaluate performance, we cre-\nated the Open PL LLM Leaderboard and Pol-\nish MT-Bench, novel frameworks assessing\nvarious NLP tasks and conversational abil-\nities. Bielik 7B v0.1 demonstrates signifi-\ncant improvements, achieving a 9 percentage\npoint increase in average score compared to\nMistral-7B-v0.1 on the RAG Reader task. It\nalso excels in the Polish MT-Bench, particu-\nlarly in Reasoning (6.15/10) and Role-playing\n(7.83/10) categories. This model represents\na substantial advancement in Polish language\nAI, offering a powerful tool for diverse linguis-\ntic applications and setting new benchmarks in\nthe field.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement in natural language pro-\ncessing (NLP) has led to the development of in-\ncreasingly sophisticated language models that can\nunderstand and generate human-like text. These\nmodels have shown remarkable success in various\nlinguistic tasks across multiple languages. How-\never, the development of high-performing models\nfor less-resourced languages remains a significant\nchallenge due to the scarcity of large and diverse\ndatasets and computational resources.\nExisting Polish language models, such as\nTRURL 2 (Voicelab, 2023) and Qra (National In-\nformation Processing Institute and Gda\u0144sk Uni-\nversity of Technology, 2024), have made impor-\ntant strides in this domain. TRURL 2, a collection\nof fine-tuned Llama 2 models with 7 billion and 13\nbillion parameters was trained on approximately\n1 million conversational Polish and English sam-\nples, with a context size of 4,096 tokens. Another\nseries of models is Qra, which comprises continu-\nously pretrained models with 1, 7, and 13 billion\nparameters. The Qra models were trained on Pol-\nish data, totaling 90 billion tokens, and also em-\nploy a context size of 4,096 tokens. While numer-\nous other Polish-focused language models exist,\nthe majority of them are fine-tuned using signifi-\ncantly smaller datasets or fine-tuning approaches,\nwhich can limit their performance and versatility.\nThis paper introduces Bielik 7B v0.1, a state-of-\nthe-art Polish language model developed as a col-\nlaborative effort between the open-science project\nSpeakLeash and the High Performance Comput-\ning (HPC) center: ACK Cyfronet AGH. Bielik\n7B v0.1 is an evolution of the Mistral 7B v0.1\nmodel (Jiang et al., 2023), enhanced to understand\nand generate Polish text with high accuracy. This\nmodel leverages a massive corpus of Polish texts\nand advanced machine learning techniques, mak-\ning it a pioneering tool in the realm of Polish Natu-\nral Language Processing (NLP). The development\nof Bielik 7B v0.1 addresses several challenges, in-\ncluding the adaptation of a model trained primar-\nily on English data to the Polish language, which\ninvolves significant linguistic and semantic adjust-\nments.\nIn the following sections, we detail the architec-\nture of Bielik 7B v0.1, describe the dataset prepa-\nration, discuss the training process, and evaluate\nthe model's performance on various NLP tasks.\nOur results demonstrate that Bielik 7B v0.1 not\nonly advances the state of Polish language under-\nstanding but also serves as a valuable resource for\nfurther research and application in Polish NLP."}, {"title": "2 Model and Tokenizer", "content": "In this section, we introduce the model design and\ntokenizer, presenting architectural decisions and\nconfigurations."}, {"title": "2.1 Model Architecture", "content": "The Bielik 7B v0.1 model builds upon the Trans-\nformer architecture (Vaswani et al., 2017), with its\nkey parameters detailed in Table 1, and incorpo-\nrates a suite of advanced techniques to enhance its\nperformance.\nSelf-attention with causal masks (Vaswani et al.,\n2017) allows the model to weigh the importance\nof different parts of the input sequence. The\ncausal mask ensures that the model only attends to\nprevious tokens, which is crucial for maintaining\nthe autoregressive property in language modeling\ntasks.\nGrouped-query attention (GQA) (Ainslie et al.,\n2023) reduces computational complexity and\nmemory usage while maintaining model quality. It\nachieves this by using fewer key-value heads than\nquery heads, allowing for more efficient process-\ning of long sequences.\nSliding Window Attention (Child et al., 2019;\nBeltagy et al., 2020) limits the attention span to\na fixed window size, reducing computational com-\nplexity from quadratic to linear in sequence length.\nIt enables the model to process longer sequences\nmore efficiently while still capturing local context\neffectively.\nSwiGLU activation function (Dauphin et al.,\n2016; Shazeer, 2020) is a combination of the\nSwish activation function and Gated Linear Units\n(GLU), offering improved performance and train-\nability compared to traditional activation functions\nlike ReLU.\nRotary Positional Embeddings (RoPE) (Su\net al., 2024) allow the model to better capture the\nrelative positions of tokens in the input sequence,\noffering advantages over absolute positional em-\nbeddings. It excels in tasks requiring nuanced un-\nderstanding of token positions, providing better\nextrapolation to longer sequences and improving\noverall performance.\nRoot Mean Square Layer Normalization (RM-\nSNorm) (Jiang et al., 2024) is used for normaliz-\ning activations within the network. It offers im-\nproved training stability and slightly faster com-\nputation compared to traditional Layer Normaliza-\ntion, contributing to more efficient training and in-\nference.\nPre-normalization applies layer normalization\nbefore the self-attention and feed-forward layers,\nrather than after, resulting in improved model con-\nvergence and overall performance.\nThe Bielik 7B v0.1 model was adapted from the\nMistral 7B v0.1 model and further pretrained. The\ndecision to use an existing model instead of train-\ning our own from scratch was due to the lack of ac-\ncess to sufficient high-quality data. Additionally,\ntraining from scratch would have required signifi-\ncantly more resources, including GPU power and\ntime. We chose the Mistral 7B v0.1 model because\nof its strong performance in benchmarks and its\npermissive Apache 2.0 license."}, {"title": "2.2 Tokenizer", "content": "One measure of the effectiveness of the tokeniza-\ntion process is the count of tokens generated for\nthe input text. A lower number of tokens indi-\ncates faster and more efficient text generation by\nthe language model. The tokenizer from the Mis-\ntral 7B model was not specifically trained for the\nPolish language. Therefore, we conducted a se-\nries of experiments aimed at expanding the orig-\ninal tokenizer to include Polish tokens. Our ap-\nproach to expanding the tokenizer involved in-\ncorporating tokens from the Polish APT3 model\n(Ociepa and Azurro Team, 2024) by extending the\nmodel's edge layers (embeddings and language\nmodel head) and continuing the training process.\nWe chose the preamble of the Constitution of the\nRepublic of Poland as the benchmark text because\nit effectively captures the essence of Polish writing\nand includes official English versions for compar-"}, {"title": "3 Pre-training", "content": "The primary objective of the pre-training phase\nwas to enhance the model's Polish language ca-\npabilities, focusing on both accuracy and fluency.\nTo accomplish this, we employed a diverse selec-\ntion of high-quality Polish texts. These materi-\nals were subjected to rigorous cleaning procedures\nand meticulous quality evaluations, ensuring the\nhighest standard of training data."}, {"title": "3.1 Pre-training Data", "content": "The pre-training of the Bielik model involved con-\nstructing a novel, diverse, and high-quality dataset,\nprimarily made up of Polish language texts. We\nleveraged resources from the SpeakLeash project\n(SpeakLeash Team, 2024). Using metadata as-\nsigned to each document, which included infor-\nmation about its topic and various stylometric fea-\ntures, we selected 18 million documents from dif-\nferent datasets that offered high quality and topic\ndiversity. These selected texts underwent thor-\nough cleaning and quality assessment procedures,\ndetailed in sections 3.1.1 and 3.1.2. Additionally,\nwe removed documents where, although robots.txt\ndid not prohibit scraping, the terms and condi-\ntions explicitly forbade using them for training\nlanguage models. Only documents meeting our\nstringent quality criteria were retained for train-\ning and subsequently tokenized. This meticulous\nprocess yielded a training dataset comprising 22\nbillion tokens. To improve the model's adapta-\ntion to a new language and mitigate catastrophic\nforgetting (Li et al., 2022; Ostapenko et al., 2022;\nIbrahim et al., 2024), we supplemented our train-\ning dataset with English texts, sourced from the\nSlimPajama dataset (Soboleva et al., 2023), known\nfor its diverse and high-quality English content.\nUltimately, our final training dataset consisted of\n36 billion tokens."}, {"title": "3.1.1 Data Cleanup", "content": "To improve the quality of the documents, we im-\nplemented a series of heuristics aimed at removing\ndamaged and unwanted text fragments, anonymiz-\ning personal data (such as physical addresses,\nemail addresses, phone numbers, and URLs), and\nfixing encoding or formatting issues. As a result\nof this process, we obtained higher-quality texts,"}, {"title": "3.1.2 Quality Evaluation", "content": "To create the training dataset for text quality eval-\nuation, we manually selected and annotated 9,000\ndocuments, assigning each to one of three quality\nclasses: HIGH, MEDIUM, or LOW. The HIGH\nclass represented documents of superior quality,\nLOW denoted poor-quality texts, while MEDIUM\nwas reserved for documents where the quality was\nambiguous to the annotator, falling between high\nand low standards. This classification scheme al-\nlowed for a nuanced approach to quality assess-\nment, accounting for the inherent complexities in\ndetermining text quality.\nFor each document, we calculated 266 stylo-\nmetric features, including metrics such as the fre-\nquency of verbs, nouns, sentences, and punctua-\ntion marks. This comprehensive set of features\nwas derived based on the methodology outlined\nin the StyloMetrix tool (Okulska et al., 2023).\nThese linguistic and structural attributes provided\na multifaceted representation of each text's stylis-\ntic properties.\nUsing these stylometric features as input, we\ntrained an XGBoost classifier model. This ma-\nchine learning approach allowed us to leverage\nthe complex interactions between various textual\ncharacteristics to predict document quality effec-"}, {"title": "3.2 Training Hyperparameters", "content": "We utilized the AdamW optimizer (Loshchilov\nand Hutter, 2017) with hyperparameters \u03b2\u2081 = 0.9,\nB2 = 0.95, and weight decay = 0.1. The learn-\ning rate followed a cosine decay schedule, starting\nat 3e-05 and decreasing to 2e-05, with a warmup"}, {"title": "4 Post-training", "content": "After finishing the pre-training phase, we moved\non to the post-training phase, which focused on\nimproving the model's capabilities across various\nareas, such as coding, mathematics, logical rea-\nsoning, and following instructions."}, {"title": "4.1 Post-training Data", "content": "There was no sufficiently large and open dataset\nof instructions and dialogues for the Polish lan-\nguage, which is why we have begun creating our\nown dataset that is continuously expanded and re-\nfined by annotators. This dataset was being manu-\nally developed through the writing of instructions\nand dialogues by annotators, ensuring high-quality\nand relevant content tailored to the needs of the\nPolish language.\nTo supplement the manually annotated data, we\nalso generated instructions based on the data used\nin pre-training (see Section 3.1). For this purpose,\nwe selected a collection of 1 million articles from\nvarious categories, and then generated an instruc-\ntion and response based on each article. A por-\ntion of these instructions was manually verified\nand corrected before being used for training pur-\nposes.\nTo further increase the number and diversity\nof instructions, we utilized publicly accessible\ncollections of English instructions, such as the\nOpenHermes-2.5 (Teknium, 2023) and orca-math-\nword-problems-200k (Mitra et al., 2024) datasets,\nwhich accounted for half of the instructions used\nin training. As a result, we obtained a training\ndataset containing over 2.3 million instructions,\namounting to more than 700 million tokens."}, {"title": "4.2 Supervised Fine-Tuning", "content": "The varying quality of training instructions nega-\ntively impacts a model's benchmark performance,\nas demonstrated in previous studies, which found\nthat poor-quality instructions degrade model capa-\nbilities (Zhou et al., 2023). These studies showed\nthat smaller, higher-quality instruction datasets of-\nten yield better results than larger, noisier datasets.\nTo address this, we introduced several improve-\nments, summarized below, while still utilizing the\npreviously mentioned datasets."}, {"title": "4.2.1 Masked Tokens", "content": "We employed a masked token approach, selec-\ntively applying loss only to certain parts of the\noutput. Specifically, we masked the loss on user\ninstruction and control tokens (Shi et al., 2024).\nThis technique ensures that these tokens do not\ncontribute to the overall loss during training, al-\nlowing the model to focus on learning from the\ncontent tokens."}, {"title": "4.2.2 Adaptive Learning Rate", "content": "The lengths of instructions can vary significantly,\nleading to fluctuations in the number of tokens\nused in computing the loss function. To ensure\nconsistent influence from each instruction, we im-\nplemented an adaptive learning rate (ALR). This\napproach is based on prior research that links\nlearning rates to batch sizes (Granziol et al., 2020).\nIn particular, the learning rate (LR) is scaled ac-\ncording to the square root of the ratio between the\nnumber of tokens in the batch (T) and the baseline\nbatch size (BS):\nALR = LR $\\sqrt{\\frac{T}{BS}}$"}, {"title": "4.2.3 Weighted Instruction Cross-Entropy\nLoss", "content": "This strategy, inspired by weighted cross-entropy\nloss (Wu et al., 2024), offline reinforcement learn-\ning (Xu et al., 2022) and C-RLFT (Wang et al.,\n2024), enabled us to effectively utilize mixed-\nquality training data annotated with fine-grained\nweight labels.\nGiven the SFT conversation dataset D =\n(xi, Yi), where xi indicates the instruction, yi is its\ncorresponding response, we assign a weight wi E\n(0, 1] to each instruction-response pair (xi, Yi) that\nis representing the quality of the pair. This al-\nlows us to construct a weighted dataset, Dw, where\nthe highest quality pairs are assigned a weight of\n1.0, while lower quality instructions have smaller\nweights (wi < 1.0). We can express the relation-\nship between weights and quality as:\nWi = $\\begin{cases}\n1.0, & \\text{highest quality} \\\\\n\u03b1, & \\text{lower quality} (0 < a < 1)\n\\end{cases}$ \nThis weighting scheme guides the model to favor\nhigh-quality responses while still learning from a\ndiverse range of instruction-response pairs. We la-\nbeled our dataset, as described in Section 4.1, and\nassigned weights to the instruction-response pairs"}, {"title": "", "content": "based on predefined rules:\nWi = $\\begin{cases}\n1.0, & \\text{high quality} \\\\\n0.7, & \\text{medium quality} \\\\\n0.5, & \\text{low quality}\n\\end{cases}$ \nwhere:\nhigh quality - instructions and dialogues manu-\nally written by annotators, the OpenHermes-2.5\n(Teknium, 2023) and orca-math-word-problems-\n200k (Mitra et al., 2024) datasets.\nmedium quality - generated instructions based on\npre-training data, which have been manually veri-\nfied and corrected.\nlow quality - generated instructions based on pre-\ntraining data without manual verification.\nSupervised Fine-Tuning (SFT) methods are de-\nsigned to adapt a pre-trained language model \u03c0\u03bf\ninto a fine-tuned model \u03c0SFT using a high-quality\ninstruction dataset D and supervised learning. We\nuse \u03c0(yx) to represent the probability of gener-\nating response y given instruction x in the dataset\nD. The objective of SFT can be expressed as a\nmaximum likelihood estimate (MLE):\nJSFT = E(x,y)~D[log #SFT(y|x)]\nTo ensure optimal fine-tuning performance, SFT\nrequires the instruction dataset D to be of the high-\nest possible quality, as it treats all training data\nuniformly (Zhou et al., 2023; Chen et al., 2024).\nHowever, assembling a sufficiently large and high-\nquality dataset can be both time-consuming and fi-\nnancially expensive.\nIn practice, the quality of available instructions\noften varies. It is possible that valuable and infor-\nmative instructions may have lower quality than\ndesired. To leverage the potential of such mixed-\nquality data, we introduce the weighted instruc-\ntion cross-entropy loss, which guides the learning\nprocess to prioritize more preferred answers while\nstill allowing the model to learn valuable insights\nfrom lower-quality instructions.\nThe standard Weighted Cross-Entropy Loss\n(King and Zeng, 2001), originating from the\nweighted exogenous sampling maximum-\nlikelihood estimator, is frequently used in\nmulti-class classification problems (Wu et al.,\n2024). It is commonly employed, for instance, to\naddress imbalanced class distributions (Rezaei-\nDastjerdehei et al., 2020). We can formulate\nstandard Weighted Cross-Entropy Loss as"}, {"title": "", "content": "l(Oi, Yi) = $\\sum_{c=1}^{C}wc \\cdot Yi,c log Pi,c$\nwhere C is the number of classes, yi =\n(Yi,1,\u2026\u2026\u2026, Yi,C) \u2208 {0,1}C is the one-hot encod-\ning of the ground truth label for sample xi, and\nYi,c = 1 indicates that xi belongs to class c. Mean-\nwhile, pi = (Pi,1,..., Pi,c) \u2208 R represents the\npredicted probability vector for sample xi across\nC classes. In multi-class classification problems\nusing deep neural networks, pi corresponds to the\nsoftmax values of the logits for each class pro-\nduced by the last layer of the network. Specifi-\ncally, Pi,c = $\\frac{exp(oi,c)}{\\sum_{j=1}^{C} exp(oi,j)}$, where oi,c is the logit\nfor class c for sample xi.\nTo integrate fine-grained weights from the\ndataset Dw, we modify Eq. 5 as follows:\nl(Oi, Yi) = -Wi\u00b7 $\\sum_{c=1}^{C} Yi,clog Pi,c$\nwhere wi represents the weight assigned to the\ninstruction-response pair (xi, Yi). This learning\nobjective provides a flexible framework for fine-\ntuning language models, offering more granular\ncontrol over the importance of each instruction\nduring training. It can capture subtle differences in\ndata quality while maintaining computational effi-"}, {"title": "4.3 Training Hyperparameters", "content": "We applied the AdamW optimizer, using \u03b2\u2081 =\n0.9, \u03b22 = 0.95, and a weight decay of 0.05.\nThe adaptive learning rate followed a cosine de-\ncay, starting at 7e-6 and tapering down to 6e-7,\nwith 50 warmup iterations. The training process\nspanned a total of 55,440 iterations. Our setup\nused a global batch size of 128, made up of lo-\ncal batches with a size of 1. Gradient clipping was\nenforced with a threshold of 1.0, and the model\nwas trained in mixed precision using bfloat16. We\ntrained the model for 3 epochs with a maximum\ncontext length of 4,096, processing a total of 2.1\nbillion tokens. The training loss, accuracy, and\nadaptive learning rate over the training iterations\nfor the instruction model are presented in Figures\n4, 5, and 6."}, {"title": "4.4 Efficient Implementation", "content": "For our training needs, we utilized the ALLaMo\nframework (Ociepa, 2023), developed by a co-"}, {"title": "5 Evaluations", "content": "The authors of\nthis model introduced numerous improvements\nto accelerate training, including FlashAttention-2\n(Dao, 2023), fused LayerNorm, fused SwiGLU,\nfused Cross-Entropy Loss, and fused Rotary Po-\nsitional Embeddings. The experiment was carried\nout on A100 40GB GPUs in 8x and 16x A100 con-\nfigurations, using a model with parameters iden-\ntical to the TinyLlama 1.1B model. When us-\ning ALLaMo, it was possible to increase the lo-\ncal batch size from 8 to 9, which further enhanced\ntraining throughput. Table 4 illustrates the perfor-\nmance differences between the TinyLlama imple-\nmentation and the ALLaMo framework."}, {"title": "5.1 Open PL LLM Leaderboard", "content": "The Open PL LLM Leaderboard, based on the\nOpen LLM Leaderboard v1 (Beeching et al.,\n2023), evaluates models on various NLP tasks, in-\ncluding: sentiment analysis, categorization, and\ntext classification, but does not test their con-\nversational capabilities (Wr\u00f3bel et al., 2024).\nThe leaderboard utilizes the Im-evaluation-harness\nframework for model evaluation (Gao et al., 2024).\nTasks:\n\u2022 polemo2: Sentiment analysis of online\nconsumer reviews across four domains\n(medicine, hotels, products, university) with\nfour-class labeling (positive, negative, neu-\ntral, ambiguous) (Koco\u0144 et al., 2019); metric:\naccuracy.\n\u2022 klej-ner: Named entity recognition in sen-\ntences containing single-type entities, classi-\nfying into six categories (no entity, place, per-\nson, organization, time, geographical name)\n(Rybak et al., 2020); metric: accuracy.\n\u2022 8tags: Topic classification of social media\nheadlines into eight categories (film, history,"}, {"title": "", "content": "food, medicine, motorization, work, sport,\ntechnology) (Dadas et al., 2020); metric: ac-\ncuracy.\n\u2022 belebele: Machine reading comprehension\nfor question answering (Bandarkar et al.,\n2024); metric: accuracy.\n\u2022 dyk: Question answering based on human-\nannotated pairs from Wikipedia's \"Did You\nKnow\" section (Marcinczuk et al., 2013);\nmetric: binary F1.\n\u2022 ppc: Text similarity assessment using man-\nually labeled sentence pairs (exact para-\nphrases, close paraphrases, non-paraphrases)\n(Dadas, 2022); metric: accuracy.\n\u2022 psc: Summarization of news articles (Ogrod-\nniczuk and Kope\u0107, 2014); metric: binary F1.\n\u2022 cbd: Text classification for cyberbullying\nand hate-speech detection (Ptaszynski et al.,\n2023); metric: macro F1.\n\u2022 polqa: Open-domain question answering\nfrom the \"Jeden z dziesi\u0119ciu\" TV show, with\nand without context (abstractive QA/RAG)\n(Rybak et al., 2024); metric: accuracy, lev-\nenshtein.\n\u2022 poquad: Context-based extractive question\nanswering (QA/RAG) (Tuora et al., 2023);\nmetric: levenshtein.\nMost of the tasks are multiple-choice tests,\nwhich means that the model chooses the correct\nanswer from a set of options. They are imple-\nmented as two types of tests:\n\u2022 Loglikelihood: We choose the highest prob-\nability token from the given set, e.g., ABCD.\nThese tests are suitable for base models.\n\u2022 Generate: Model generates answer freely.\nAll tasks are evaluated in both 0-shot and 5-shot\nsettings.\nEvaluation scores:\n\u2022 All tasks: The average score across all tasks,\nnormalized by baseline scores.\n\u2022 Reranking: The score of reranking task,\ncommonly used in Retrieval-Augmented\nGeneration (RAG).\n\u2022 Reader (Generator): The score of open-\nbook question-answering task, also com-\nmonly used in RAG.\n\u2022 Perplexity: A bonus metric that does not cor-\nrelate with other scores and should not be\nused for direct model comparison (lower is\nbetter).\nAs of April 3, 2024, Table 5 presents the cur-\nrent scores of both pretrained and continuously\npretrained models, as evaluated on the Open PL\nLLM Leaderboard in a 5-shot setting.\nThe Bielik 7B v0.1 model achieved one of the\nhighest scores in the RAG Reader task, demon-\nstrating a notable improvement of nearly 9 per-\ncentage points in average score compared to\nMistral-7B-v0.1. In our subjective evaluations of\nconversational abilities, our models outperformed\nothers that had higher average scores. The results\npresented in Table 5 were obtained without em-\nploying instruction templates for the instructional\nmodels, treating them instead as base models. This\napproach may have skewed the results, as instruc-\ntional models are specifically optimized to follow\nparticular instructions."}, {"title": "5.2 Polish MT-Bench", "content": "MT-bench (Zheng et al., 2023) is a tool designed to\ntest the ability of language models (LLMs) to con-\nduct two-step conversations and follow instruc-\ntions. It covers typical use cases and focuses on\nchallenging questions to differentiate the capabil-\nities of various models. Eight main categories of\nuser queries were identified, which were used to\nconstruct MT-bench:"}, {"title": "", "content": "\u2022 writing\n\u2022 role-playing\n\u2022 information extraction\n\u2022 reasoning\n\u2022 mathematics\n\u2022 coding\n\u2022 knowledge / hard sciences / stem\n\u2022 knowledge / humanities / social sciences\nFor each category, two-step questions were\nmanually developed.\nThe evaluation of responses is performed by a\nmetamodel. In the case of MT-Bench, this is the\nGPT-4 model. By using a metamodel, we can\nverify responses from open-ended questions, e.g.,\nwrite an article about hybrid cars. The model eval-\nuates the content of the response, the quality of\nfacts used, creativity, etc.\nThe Polish MT-Bench (Kinas et al., 2024) has\nbeen completely polonized. Each task was first\nmachine-translated and then verified. Addition-\nally, we introduced Polish accents, e.g., instead of\ndescribing a vacation in Hawaii, we suggested the\nlocation - Masuria. In our language version, many\nchanges were introduced to transfer the test into\nPolish linguistic realities.\nTable 6 presents the results of the Polish MT-\nBench evaluation for various language models.\nThe table shows three key metrics: the Polish\nscore (pl_score), the proportion of responses in\nPolish (responses_pl), and the average score. The\nBielik 7B v0.1 model has a pl_score of 5.40,\ndemonstrating competitive performance among\nlarger models.\nTable 7 provides a more detailed breakdown\nof the Polish MT-Bench results, showing scores\nacross eight different categories for each model.\nThe Bielik 7B v0.1 model shows competitive per-\nformance in several categories, notably excelling\nin Reasoning (6.15) and Role-playing (7.83).\nThese results demonstrate the model's versatility\nacross various tasks, despite its smaller size com-\npared to some of the top-performing models."}, {"title": "5.3 Bias, Toxicity and Misinformation", "content": "Language models have been shown to reproduce\nand amplify biases present in the training data, and\ncan generate toxic or offensive content. Since our\ntraining dataset contains a large proportion of data\nfrom the web, Bielik-7B-v0.1 may produce factu-\nally incorrect output and should not be relied upon\nfor producing accurate information. Despite sig-\nnificant efforts to clean the training data, it is still\npossible for this model to generate lewd, false, bi-\nased, or otherwise offensive content."}, {"title": "6 Model Quantization", "content": "In our work on the Bielik 7B v0.1 model, our pri-\nmary objective was to create quantized versions\nthat could be accessible to users with limited com-\nputational resources. This effort was driven by a\nvision to democratize advanced language models\nand make them available to those who do not have\naccess to powerful computing infrastructure. By\noptimizing our model for low-resource environ-\nments, we aimed to facilitate deployment on var-\nious devices, including edge devices such as mo-\nbile phones and embedded systems.\nTo achieve this, we developed and delivered\nseveral quantized versions of Bielik 7B v0.1, in-\ncluding GGUF (GPT - Generated Unified For-\nmat)\u00b9, HQQ (Half-Quadratic Quantization) (Badri\nand Shaji, 2023), AWQ (Activation-aware Weight\nQuantization) (Lin et al., 2024), MLX (Apple\nMLX Framework) (Hannun et al., 2023), EXL2\n(ExLlamaV2)2, GPTQ (Accurate Post-Training\nQuantization for Generative Pre-trained Trans-\nformers) (Frantar et al., 2022), and IQ2_XXS\n(GGUF IQ)3. Each quantization technique offered\ndifferent trade-offs in terms of performance, mem-\nory usage, and computational requirements, allow-\ning for flexibility depending on the intended use\ncase and hardware capabilities. The IQ2_XXS\nversion, in particular, was specifically designed for\nedge devices, with a bit-per-weight quantization of\n2.06 bpw, providing an efficient solution for de-\nployment on resource-constrained platforms such\nas mobile phones."}, {"title": "6.1 Calibration and Evaluation of Quantized\nModels", "content": "In addition to the standard quantization process,\nwe created calibrated versions of the imatrix (Im-\nportance Matrix) GGUF model. Calibration plays\na crucial role in minimizing performance degra-\ndation, which is often a concern during quan-\ntization. To support this process, we devel-\noped a multilingual (Polish-English) calibration\ndataset with a specific emphasis on the Polish\nlanguage. This multilingual approach aimed to\nimprove the model's generalization capabilities\nacross languages while ensuring high fidelity in its\nPolish-language outputs.\nTo assess the impact of calibration, we con-\nducted a thorough comparison between the uncal-\nibrated and calibrated versions of the model for\nthe Polish language. Our evaluation metrics fo-\ncused on both the accuracy of language under-\nstanding and the quality of generated text. The re-\nsults showed that the calibration process improved\nthe model's performance, particularly in language-\nspecific contexts where nuances and subtleties are\ncrucial.\nAcross all quantization schemes examined\n(Q8_0, Q6_K, Q5_K_M, Q4_K_M, Q3_K_M,\nQ2_K) (see table 8), models quantized with\nimatrix consistently outperform their counter-\nparts without imatrix quantization. This is evi-\ndent through multiple evaluation metrics, indicat-\ning that imatrix quantization effectively preserves\nmodel quality even at lower bit-widths. The KLD\nvalues are consistently lower for imatrix-quantized\nmodels, indicating a closer alignment of the"}]}