{"title": "Training Medical Large Vision-Language Models\nwith Abnormal-Aware Feedback", "authors": ["Yucheng Zhou", "Lingran Song", "Jianbing Shen"], "abstract": "Existing Medical Large Vision-Language Mod-\nels (Med-LVLMs), which encapsulate exten-\nsive medical knowledge, demonstrate excel-\nlent capabilities in understanding medical im-\nages and responding to human queries based\non these images. However, there remain chal-\nlenges in visual localization in medical images,\nwhich is crucial for abnormality detection and\ninterpretation. To address these issues, we\npropose a novel UMed-LVLM designed with\nUnveiling Medical abnormalities. Specifically,\nwe collect a Medical Abnormalities Unveiling\n(MAU) dataset and propose a two-stage train-\ning method for UMed-LVLM training. To col-\nlect MAU dataset, we propose a prompt method\nutilizing the GPT-4V to generate diagnoses\nbased on identified abnormal areas in medi-\ncal images. Moreover, the two-stage training\nmethod includes Abnormal-Aware Instruction\nTuning and Abnormal-Aware Rewarding, com-\nprising Abnormal Localization Rewarding and\nVision Relevance Rewarding. Experimental\nresults demonstrate that our UMed-LVLM sur-\npasses existing Med-LVLMs in identifying and\nunderstanding medical abnormality. In addi-\ntion, this work shows that enhancing the abnor-\nmality detection capabilities of Med-LVLMs\nsignificantly improves their understanding of\nmedical images and generalization capability.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) demon-\nstrate remarkable capability in various vision-\nlanguage tasks (OpenAI, 2023a; Liu et al., 2023b;\nLi et al., 2023a). Medical image analysis poses a\nsignificant challenge for LVLMs due to their in-\ntricate patterns and structures, thereby demanding\nan in-depth grasp of nuanced variations to ensure\nprecise diagnoses (Wu et al., 2023). To enhance\nLVLMs for medical images, some works (Qin et al.,\n2023) encapsulate a substantial medical corpus into"}, {"title": "2 Related Work (summarized)", "content": "Recent advancements in LVLMs have significantly\nimproved visual comprehension and contextual lan-\nguage understanding (Alayrac et al., 2022; Chen\net al., 2023; Zhou et al., 2024a). Notable mod-\nels like CLIP (Radford et al., 2021) and BLIP-2\n(Li et al., 2023b) have achieved impressive results\nin vision-language tasks by leveraging pre-trained\nimage-text pairs. The introduction of GPT-4 (Ope-\nnAI, 2023a) has further propelled this field, with\nmodels like LLaVA (Liu et al., 2023b) and its\nimproved version (Liu et al., 2023a) demonstrat-\ning exceptional capabilities in multimodal tasks.\nIn the medical domain, models such as LLaVA-\nMed (Li et al., 2023a), Visual Med-Alpaca (Shu\net al., 2023), OphGLM (Gao et al., 2023), and\nXrayGPT (Thawakar et al., 2023) have specialized\nin interpreting medical images and providing com-\nprehensive assistance. Observations in (Wu et al.,\n2023) indicated that although GPT-4V performs\nwell in differentiating medical image modalities\nand anatomy, it still has difficulties in disease diag-\nnosis and generating comprehensive reports. Ad-\nditionally, Reinforcement Learning (RL) has been\napplied to LLMs to enhance their performance and\nflexibility (Sutton and Barto, 1998; Mnih et al.,\n2015; Schulman et al., 2015; Mnih et al., 2016;\nSchulman et al., 2017). Successful implementa-\ntions include WebGPT (Nakano et al., 2021) and\nInstructGPT (Stiennon et al., 2020; Christiano et al.,\n2017; Brown et al., 2020), which use RL to opti-\nmize behavior and follow instructions. The full\nversion can be found in Appendix A."}, {"title": "3 Methodology", "content": "In this study, we train UMed-LVLM with a two-\nstage training method, i.e., Abnormal-Aware In-\nstruction Tuning and Abnormal-Aware Rewarding.\nAbnormal-Aware Rewarding comprises a LLM rel-\nelevance reward model and two rewarding strategies,"}, {"title": "3.1 Abnormal-Aware Instruction Tuning", "content": "To improve task adaptability, accuracy, and effi-\nciency in LVLMs, Liu et al. (2023b) employ In-\nstruction Tuning (Wei et al., 2022). This approach\nenhances the model's ability to understand and\nfollow instructions effectively. To enhance our\nmodel's capability to understand medical abnormal\nregions and generate corresponding diagnoses with\nabnormal regions, we perform Abnormal-Aware\nInstruction Tuning on UMed-LVLM.\nSpecifically, given a medical image x and a user\nquery q, our model generates a response a con-\ntaining the abnormal region and diagnosis through\nautoregressive decoding, i.e.,\n$p(a|x, q; \\theta) = \\prod_{t=1}^{T}p(a_t|a_{<t}, x, q; \\theta),$ (1)\nwhere $a_t$ represents the first $t$ tokens of a, T is\nthe length of the response, and $\\theta$ denotes the model\nparameters. The model predicts the next token\nbased on the preceding t tokens until the complete\nresponse is generated."}, {"title": "3.2 Abnormal-Aware Rewarding", "content": "To address the challenge of effectively identifying\nand describing abnormalities within medical im-\nages, we propose an Abnormal-Aware Rewarding\n(AAR) strategy for UMed-LVLM training. This\nreinforcement learning (RL) training strategy com-\nprises a LLM relevance reward model and two\nrewarding strategies, i.e., Abnormal Localization\nRewarding and Vision Relevance Rewarding, de-\nsigned to optimize the Med-LVLMs based on ab-\nnormalities. In contrast to Ouyang et al. (2022)\noptimize LLMs following user instructions by the\nProximal Policy Optimization (PPO (Schulman\net al., 2017)), AAR optimizes the Med-LVLMs\nby a more directed learning towards the accurate\nmedical diagnosis with abnormality recognition."}, {"title": "3.2.1 LLM Relevance rewarding", "content": "The LLM relevance rewarding framework is fun-\ndamentally structured around three pivotal compo-\nnents: the policy network, the value network and\nthe LLM reward model. Both networks play a cru-\ncial role in guiding the training process, with the"}, {"title": "4 MAU Dataset", "content": "To enhance the abnormality unveiling capabilities\nof the Med-LVLM, we construct the MAU dataset\nfor UMed-LVLM training. Firstly, we collect\nmedical image datasets with abnormal annotations.\nThen, we design a Prompt Method to construct the\nMAU dataset, a medical diagnosis dataset with ab-\nnormal annotations, by GPT-4V (OpenAI, 2023b)."}, {"title": "4.1 Collecting Medical Image Datasets with\nAbnormal Areas", "content": "We collect a medical image dataset annotated\nwith abnormal areas for training the Med-LVLM.\nThis dataset encompasses five distinct sub-datasets,\nnamely DeepLesion (Yan et al., 2017), Kidney-\nStone (TEZ, 2023), NIH (Wang et al., 2017),\nTBX11K (Liu et al., 2020), and KVASIR\n(Pogorelov et al., 2017), each originating from di-\nverse sources. DeepLesion includes 32,120 ax-\nial CT slices with eight types of abnormalities.\nThe Kidney Stone dataset contains 1,300 renal CT\nscans of various kidney stones in different sizes,\nshapes, and locations. The NIH dataset has 112,120\nchest X-ray images across 14 pathological cate-\ngories. The TBX11K dataset consists of 11,200\nchest X-ray images for tuberculosis detection. The\nKVASIR dataset provides 8,000 endoscopic im-"}, {"title": "4.2 Medical Abnormal Unveiling Dataset\nConstruction", "content": "To construct the Medical Abnormal Unveiling\n(MAU) Dataset, we design a Prompt Method that\nutilizes the GPT-4V model to generate diagnosis\nannotations with medical abnormal areas. The\npipeline, as shown in Figure 2, comprises two\nstages: diagnosing abnormalities in medical images\nand reflecting on the previous diagnosis. Firstly,\nwe integrate collected medical images, correspond-\ning abnormality categories, and the locations of\nabnormal areas into our designed prompt. This\nprompt is then passed into GPT-4V to generate di-\nagnosis responses based on the given abnormality\ncategories and areas. To build a diagnosis with step-\nby-step thoughts, a reflection prompt is designed\nto reorganize these diagnosis responses, starting\nfrom abnormality detection, identifying bounding\nbox regions, and finally recognizing abnormality\ncategories. The processed responses, the medical\nimages, and queries form medical image diagnosis\nsamples. Through this Prompt Method, we harness\nthe GPT-4V model to generate the MAU dataset,\nand examples of the MAU dataset can be found in\nAppendix B. Our prompt method is plug-and-play,\ncharacterized by its ease of integration with other\nmedical datasets and independence from disease-\nspecific designs. To ensure the safety and reliability\nof the GPT-4V-generated diagnoses, all generated\ndata have been reviewed and filtered by three doc-\ntoral students specializing in medicine."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setups", "content": "In our experiments, we evaluated our model on\nthe test set of MAU data. Our dataset comprises\nfive sub-datasets: DeepLesion, KidneyStone, NIH,\nTBX11K, and KVASIR. We utilized MedVInt, a\nvision-language model pre-trained extensively on\nmedical knowledge, as the initialization for our\nmodel. To prevent knowledge catastrophic forget-"}, {"title": "5.2 Results and Discussion", "content": "We compared the performance of our method with\nother LVLMs on the MAU dataset. As shown\nin Table 2, our method outperforms other meth-\nods across all datasets. This substantial lead is at-\ntributed to our model's ability to integrate abnormal\nunveiling capabilities. Unlike other specific mod-\nels, UMed-LVLM appears to more effectively gen-\neralize from the training data to the test scenarios,\nindicating critically valuable robustness in medical\nimaging applications where precision is paramount."}, {"title": "5.3 Ablation Study", "content": "To verify the efficacy of components in AAR, we\nconducted experiments by progressively removing\neach component from UMed-LVLM to observe\nthe impact on performance. As shown in Table 3,\nwe compare the performance of UMed-LVLM and\nits three variants, i.e., \"w/o ALR\u201d, \u201cw/o VRR\",\nand \"w/o AAR\u201d. The results show a performance\ndecline as components are removed, demonstrat-\ning that each component plays a critical role in\nenhancing diagnosis accuracy for medical images.\nFrom the table, we can observe that both ALR and\nVRR significantly contribute to the model's perfor-\nmance. The VRR, by aligning abnormal identifica-\ntion and attention regions, ensures that the model\ndoes not overlook subtle but critical abnormalities\nin the images. On the other hand, the ALR compo-\nnent reinforces the abnormal localization, which is\nparamount in medical diagnosis. The performance\ndegradation observed when either component is\nremoved substantiates the hypothesis that a dual-\nreward system, which addresses abnormality recog-\nnition and localization, is beneficial in diagnosis.\""}, {"title": "5.4 Analysis", "content": "Impact of Abnormal Localization. To verify\nthe impact of localization ability during the reason-"}, {"title": "3.2.2 Abnormal Localization Reward", "content": "The Abnormal Localization Reward (ALR) is cal-\nculated by Intersection over Union (IoU) scores\nbetween the generated and ground truth bounding\nboxes. Specifically, the ALR ($r_{I o U}^{t}$) is derived from\nthe IoU score, which quantitatively measures the\naccuracy of the predicted bounding boxes against\nthe ground truth one:\n$r_{I o U}^{t}$ = IoUScore(PredBBox, GTBBox), (10)\nwhere PredBBox and GTBBox represent the pre-\ndicted and ground truth bounding boxes, respec-\ntively. The IoUScore is calculated as follows:\nIoUScore($x_1,x_2$) =$\\frac{Overlap(x_1, x_2)}{Union(x_1, x_2)}$ (11)\nthis function quantifies the overlap between the pre-\ndicted and ground truth bounding boxes, providing\na measure of localization accuracy."}, {"title": "3.2.3 Vision Relevance Reward", "content": "The Vision Relevance Reward (VRR) is computed\nby the assigned attention weights between abnor-\nmal category tokens assigned and abnormal areas\nin the image. Specifically, the VRR, $r_{a t t}^{t}$, is de-\ntermined by analyzing the attention mechanisms\nwithin a transformer framework. This reward is\ncalculated based on the attention scores allocated\nto tokens that correspond to the abnormal cate-\ngories, with a specific focus on the image patches\nindicative of abnormal regions. The computation\nis designed to enhance the model's ability to con-\ncentrate on crucial visual areas, thereby improving\nits diagnosis accuracy:\n$r_{a t t}^{t} = \\sum_{i \\in N} \\sum_{j \\in N} ATTN(i, j),$ (12)\nwhere N denotes the set of tokens that are directly\nassociated with identified abnormalities within the\ntext, and N refers to the patches of the image\nthat are characterized as abnormal regions. The\nATTN(i, j) represents the weight attributed to the\ni-th token for its relevance to the j-th abnormal re-\ngion patch. The detailed calculation of ATTN(i, j)\nis as follows:\n$ATTN(i, j) = \\frac{score_{i,j}}{\\sum_{k \\in \\tilde{N}} exp (score_{i,k})}$ ,\n$score_{i,j}$ =$\\frac{Q_iK_j^T}{\\sqrt{d_k}}$ (13)\n(14)\nwhere $Q_i$ is the query vector for token i, and $K_j$\nis the key vector for image patch j; $d_k$ and $ \\tilde{N}$ are\nthe dimensionality of the key vectors and the set of\nall abnormal image patches. This approach allows\nus to quantify the model's focus on critical areas\nby leveraging the transformer's inherent attention"}, {"title": "3.2.4 Reward Normalization and Aggregation", "content": "To achieve equilibrium between the ALR ($r_{I o U}^{t}$) and\nthe VRR ($r_{a t t}^{t}$), we individually normalize these\nrewards for responses generated by the same query.\nThis normalization ensures that each reward type\ncontributes equally to the final reward calculation.\nThe combined reward for each response is com-\nputed as follows:\n$r^{t}= r_{R}^{t,V,LLM}+\\frac{r_{IoU}^{t}}{max(r_{IoU}^{t})}+ \\frac{r_{att}^{t}}{max(r_{att}^{t})},$ (15)\nwhere max($r_{IoU}^{t}$) and max($r_{att}^{t}$) are the maximum\nvalues of the abnormal-aware localization and at-\ntention reward, respectively, for all responses to a\nparticular query."}, {"title": "3.2.5 Optimization Process", "content": "As an improved version of the PPO framework, our\npolicy network is refined by maximizing an objec-\ntive function that incorporates the combined reward\n$r_t$ into the PPO framework. Specifically, we aim to\nmaximize an objective function that combines the\nclipped surrogate objective with an entropy bonus\nto encourage exploration:\n$L^{CLIP+ENT}(\\theta) = \\hat{E}[C^{CLIP}(\\theta)+ C_1r_t - C_2L^{VF}(\\phi) + C_3S[\\pi(\\cdot | s_t)]],$ (16)"}, {"title": "C Implementation Details", "content": "In the experiments, we employ MedVInT (Zhang\net al., 2023b) to initialize our model. During the\ninstruction tuning phase, we employed an Adam\noptimizer (Kingma and Ba, 2015) with a learning\nrate of 1 \u00d7 10\u22125, weight decay of 0.01 and a batch\nsize of 128. We train the model on four epochs,\nutilizing a linear decay strategy for the learning rate.\nFor the AAR method, we set y = 0.99, C\u2081 = 0.5,\nC2\n= 0.5, and C3 = 0.01. We used an Adam\noptimizer with a learning rate of 1 \u00d7 10-6 and\na batch size of 16, training the model for one epoch.\nTo enhance the diversity of sampling responses, we\nset the number of sampling candidates to eight and\ndecoded the model with a temperature of 0.9 and a\nprobability threshold (p) of 0.9. Our training was\nconducted on an NVIDIA H800 GPU."}, {"title": "D Large Vision-Language Models", "content": "In this study, we evaluate a variety of large\nvision-language models (LVLMs) and medical\nLVLMS (Med-LVLMs), including general-purpose\nmodels such as MiniGPT-4 (Zhu et al., 2024),\nmPLUG-Owl (Ye et al., 2023), LLaVA (Liu\net al., 2023b), Qwen-VL (Bai et al., 2023), and\nspecialized models like Med-LVLMs, including\nXrayGPT (Thawakar et al., 2023), LLaVA-Med (Li\net al., 2023a), Med-Flamingo (Moor et al., 2023),\nand MedVInt (Zhang et al., 2023a). While\nMiniGPT-4, mPLUG-Owl, LLaVA, and Qwen-VL\nare designed for general visual tasks, LLaVA-Med,\nXrayGPT, Med-Flamingo, and MedVInt are specif-\nically optimized for the medical domain. These\nmodels generally follow a two-stage training pro-\nIn the first stage, they align features us-\ning large-scale vision-language datasets, while the\nsecond stage fine-tunes the model on instruction-\nfollowing datasets. In our experiments, we ex-\ntended this approach by further training our model\non the MAU dataset using Instruction Tuning and\nARRL methods, building on the MedVInt frame-\nwork. For comparison, we also evaluated GPT-\n4V (OpenAI, 2023b), which was used in inference\nmode without additional training due to its closed\ncess.\nsource."}, {"title": "E Case Study", "content": "As shown in Figure 5, we randomly sample exam-\nples to compare our method (i.e., UMed-LVLM)\nand GPT-4V. For instance, in the first medical im-\nage, a chest X-ray, the UMed-LVLM identified"}, {"title": "Generalization Capability for Un-trained Cat-\negory.", "content": "This setting aims to evaluate the general-\nization capability of the UMed-LVLM on medical\ncategories not present in the training data. Specif-\nically, we removed data in three categories (i.e.,\n\"Abdomen\u201d, \u201cLung\u201d, and \u201cPelvis\u201d on \u201cDeepLe-\nsion\u201d) on the training set. Then we evaluated the\ntrained model on these categories to observe its\nability to handle un-trained categories. The results,\nshown in Table 4, present the performance in each\ncategory. The performance demonstrates that our\nmethod exhibits a degree of generalization capa-\nbility on un-trained categories of medical images.\nAlthough the performance on un-trained categories\nis somewhat reduced compared to training on these\ncategories, these findings indicate that the model\ncan generalize knowledge learned during training\nto new categories not present in the training data.\nIn contrast, MedVInt shows a slight ability to gen-\neralize to these un-trained categories with scores\nclose to zero across all categories. This demon-\nstrates that abnormal-aware learning is effective for\nMed-LVLM training."}, {"title": "Generalization Capability of Abnormality Un-\nveiling.", "content": "To assess the robustness and flexibility\nof UMed-LVLM and MedVInt, we respectively\nremoved the TBX11K and DeepLesion datasets\nfrom the training data to verify the models' gen-\neralization capabilities. We evaluated the models\non the excluded datasets to determine their ability\nto generalize across different medical datasets. As\nshown in Table 5, UMed-LVLM demonstrated sig-\nnificantly better generalization than other methods.\nThe performance disparity underscores our model's\neffectiveness in adapting to varied medical scenar-\nios, indicating a robust learning framework is not\nonly overfitted to training data but instead potential\nto handle diverse medical imaging challenges."}, {"title": "Generalization Capability for Cross-Modality.", "content": "To verify the generalization capabilities of UMed-\nLVLM, we train it on single-modality medical im-\nages and evaluate it across different modalities. As\nshown in Table 6, our approach demonstrates better\nperformance in cross-modal generalization com-\npared to other methods. The performance of our\nmodel is attributed to our abnormal-aware train-\ning method that enhances the model's capability to\nadapt to various medical images. The Abnormal-\nAware Instruction Tuning and AAR boost the\nmodel's capability to localize abnormalities and di-\nagnose accuracy in medical images. This is partic-\nularly important in scenarios where the modalities\ndiffer substantially (e.g., CT vs. Gross Pathology)."}, {"title": "6 Conclusion", "content": "This study introduces UMed-LVLM, a novel Med-\nLVLM designed to enhance medical diagnosis by\nthe visual localization of abnormalities in medi-\ncal images. Through a specialized training pro-\ncess involving the collection of a Medical Abnor-\nmalities Unveiling dataset and the implementa-\ntion of Abnormal-Aware Instruction Tuning and\nAbnormal-Aware Rewarding. Results show UMed-\nLVLM surpasses existing Med-LVLMs in accu-"}]}