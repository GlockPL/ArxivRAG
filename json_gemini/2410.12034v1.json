{"title": "A Survey on Deep Tabular Learning", "authors": ["SHRIYANK SOMVANSHI", "SUBASISH DAS", "SYED AAQIB JAVED", "GIAN ANTARIKSA", "AHMED HOSSAIN"], "abstract": "Tabular data, widely used in industries like healthcare, finance, and transportation, presents unique challenges for deep learning due to its heterogeneous nature and lack of spatial structure. This survey reviews the evolution of deep learning models for tabular data, from early fully connected networks (FCNs) to advanced architectures like TabNet, SAINT, TabTranSELU, and MambaNet. These models incorporate attention mechanisms, feature embeddings, and hybrid architectures to address tabular data complexities. TabNet uses sequential attention for instance-wise feature selection, improving interpretability, while SAINT combines self-attention and intersample attention to capture complex interactions across features and data points, both advancing scalability and reducing computational overhead. Hybrid architectures such as TabTransformer and FT-Transformer integrate attention mechanisms with multi-layer perceptrons (MLPs) to handle categorical and numerical data, with FT-Transformer adapting transformers for tabular datasets. Research continues to balance performance and efficiency for large datasets. Graph-based models like GNN4TDL and GANDALF combine neural networks with decision trees or graph structures, enhancing feature representation and mitigating overfitting in small datasets through advanced regularization techniques. Diffusion-based models like the Tabular Denoising Diffusion Probabilistic Model (TabDDPM) generate synthetic data to address data scarcity, improving model robustness. Similarly, models like TabPFN and Ptab leverage pre-trained language models, incorporating transfer learning and self-supervised techniques into tabular tasks. This survey highlights key advancements and outlines future research directions on scalability, generalization, and interpretability in diverse tabular data applications.", "sections": [{"title": "1 Introduction", "content": "Tabular data, which consists of rows and columns representing structured information [1, 2], is the most commonly used data format in many industries, including healthcare, finance, and transportation. Unlike unstructured data such as images and text, tabular data directly represents real-world phenomena in a structured form, making it crucial for decision-making processes in areas like risk assessment, predictive analytics, and safety monitoring. For example, in the field of transportation engineering, tabular data plays a key role in recording crash incidents, vehicle attributes, environmental factors, and human behavior, enabling researchers to predict crash severity and improve safety measures using data-driven insights.\nDespite the success of deep learning in domains like computer vision and natural language processing (NLP), its application to tabular data has been less straightforward. Deep learning models often struggle with tabular data due to several challenges:\n(1) Small sample sizes: Many tabular datasets are relatively small, especially when compared to large image or text datasets, leading to overfitting in complex deep learning models.\n(2) High dimensionality: Tabular data often involves many features, which can be sparse or irrelevant, making it difficult for models to identify meaningful patterns.\n(3) Complex feature interactions: Unlike images or text, where local structures are prominent, the interactions between features in tabular data are non-local and complex, requiring more specialized architectures to capture these relationships effectively.\nThese factors make tree-based models like XGBoost and Random Forests more effective for many tabular data tasks, as they are better suited to handle sparse features and complex interactions.\nOver recent years, significant strides have been made in the development of deep learning models specifically for tabular data, addressing the unique challenges posed by this data type. While early models like fully connected networks (FCNs) showed promise, new architectures have emerged that have significantly advanced the field [3-6]. One of the leading models in this space is the FT-Transformer, which adapts transformer models, initially developed for sequential data, to effectively handle tabular data by encoding features through attention mechanisms [7, 8]. This model has shown impressive performance due to its ability to learn complex interactions between features, making it well-suited for high-dimensional data.\nAnother recent innovation is the Self-Attention and Intersample Transformer (SAINT), which improves upon the original transformer by introducing intersample attention mechanisms, allowing the model to better capture relationships between rows of tabular data [9]. SAINT has demonstrated superior performance in various benchmarks compared to traditional models like XGBoost and deep learning models such as Neural Oblivious Decision Ensembles (NODE). Additionally, models like TabTransformer leverage transformers specifically for categorical feature encoding, providing a more scalable solution for handling mixed data types in tabular datasets. This approach enables the model to capture meaningful representations from categorical variables, which are often challenging for traditional deep-learning architectures to handle effectively. These new models have introduced significant innovations in terms of feature encoding, complex interaction learning, and model interpretability, which are crucial for advancing the application of deep learning to tabular data in many research areas. The objective of this survey paper is to review these advancements in detail, exploring their historical evolution as seen in Figure 1, key techniques, datasets, and potential applications."}, {"title": "2 Challenges in Modeling Tabular Data", "content": ""}, {"title": "2.1 Heterogeneous Feature Types", "content": "Tabular data, a foundational structure in fields like healthcare, finance, and transportation, commonly contains heterogeneous data types such as numerical, categorical, ordinal, text, and even multimedia elements like images and emojis. Numerical features often represent continuous or discrete values (e.g., age, income), while categorical features classify entities into discrete groups (e.g., gender, city) [10, 11]. In more complex cases, text data, images, or emojis may be embedded within tables, providing rich context but complicating feature representation and model training. Understanding how to handle these varied data types is critical to improving the performance of deep learning models on tabular data.\nTabular data can also be represented in two different formats: 1D tabular data, and 2D tabular data as shown in Figure 2 below. In 1D tabular data, each row represents a sample and columns represent specific features, making it easy to process and analyze. This format is ideal for traditional machine learning tasks, as each column follows a specific data type and the structure is fixed. For example, in transportation safety datasets, each row could represent an individual crash event, and the columns might include features like vehicle speed, crash time, or road conditions. The simplicity of this structure makes it highly useful in various fields.\nIn contrast, 2D tabular data provides a more complex format where each sample can be represented by a table, with multiple rows and columns within each table. This format is often used for tasks that require deeper relational analysis, such as tracking patient health over time or analyzing transportation data across different regions and times. 2D tabular data is also more flexible, incorporating diverse data types, including timestamps or unstructured data, like text or images, within each table. This additional complexity makes it suitable for applications in areas such as healthcare and transportation, where temporal and multi-dimensional data are critical.\nUnderstanding how to handle these varied data types is critical to improving the performance of deep learning models on tabular data. Some of these data types are explained below:\n\u2022 Binary Data: Binary data, a type of categorical data with two possible values (such as, \"Yes/No\"), is often represented as 0 or 1 in deep learning models [12].\n\u2022 Numerical Data: Numerical data, representing continuous or discrete variables (e.g., age, vehicle speed), is common in predictive modeling, especially in transportation safety [13]. Deep learning models handle it directly, but preprocessing, like scaling or standardization, is critical for performance. Advanced techniques, such as numerical embeddings, help capture non-linear relationships and interactions in the data.\n\u2022 Timestamps: Timestamps provide essential temporal information in systems like traffic management. Preprocessing involves extracting features such as day, month, or hour to capture temporal patterns for deep learning models [14].\n\u2022 Text Data: Text data in tabular formats, such as crash descriptions, presents challenges for deep learning models. Methods like TF-IDF [15] and word embeddings (e.g., word to vector, global vectors for word representation) convert text into numerical vectors [16, 17]. Advanced models like transformers (e.g., BERT) capture context-aware embeddings [18].\n\u2022 Image Data: In multi-modal datasets, image data is sometimes embedded in tables, such as in autonomous driving, where road images are paired with tabular data. Convolutional Neural Networks (CNNs) process images, but integrating image features with tabular data requires feature fusion techniques. Hybrid models like TabTransformer use attention mechanisms to merge image and tabular data, enhancing predictive performance [19].\n\u2022 Hyperlinks: Hyperlinks, though uncommon in traditional tabular datasets, are increasingly used in web data applications or web documents [20]. When tables include URLs, advanced preprocessing is required to extract metadata or context from the linked pages, often using NLP models to incorporate this information into the feature set.\n\u2022 Video Data: Video data in tabular formats provides valuable temporal information for domains like autonomous driving and traffic management. Keyframes from videos are processed using 3D-CNNs or Recurrent Neural Networks (RNNs) to capture spatial and temporal features, which are then integrated with tabular data to improve model predictions, such as in crash prediction models where video features enhance the understanding of road conditions and driver behavior [21, 22].\n\u2022 Emoji: Emojis common in social media and messaging platforms, enhance communication by visually conveying emotions or objects [23] and pose challenges for encoding sentiment. Deep learning models use character-level or emoji embeddings to map them to sentiment vectors, enabling effective interpretation alongside other data types.\nTabular data, composed of rows and columns, lacks the spatial or sequential structure found in images and text, making it difficult to apply traditional deep learning models like CNNs, which rely on spatial coherence. Unlike structured data, reordering columns or rows in tabular data doesn't change feature relationships, and deep learning models struggle without the inductive biases that machine learning models like XGBoost and Random Forests possess. Machine learning models excel in handling heterogeneous feature types, non-local interactions, and small, high-dimensional datasets, where deep learning models often overfit and fail to generalize.\nTo address the limitations of traditional deep learning models when applied to tabular data, recent advancements have led to the development of specialized architectures such as TabNet, TabTransformer, and SAINT. These models introduce mechanisms like attention layers, feature embeddings, and hybrid architectures to dynamically focus on the most relevant features, improving their ability to handle the complexity of heterogeneous tabular data. For instance, TabNet [24] employs sequential attention mechanisms for instance-wise feature selection, while TabTransformer [19] uses self-attention layers to capture feature dependencies more effectively than CNNs. SAINT [25] enhances this approach by incorporating intersample attention, enabling the model to capture relationships between data rows. Moreover, models such as TabTranSELU [26] and GNN4TDL [27] are designed to efficiently manage both categorical and numerical features by employing hybrid structures and regularization techniques, which help mitigate overfitting and improve generalization. These innovations have enabled deep learning models to rival or surpass traditional machine learning methods in tasks involving tabular data, including fraud detection, and predictive analytics. Additionally, novel techniques such as transforming tabular data into image-like structures [2, 28], employing multi-view representation learning, and extracting schemas from tabular data [29] further contribute to overcoming the challenges posed by the absence of inherent spatial relationships in tabular datasets.\nBy leveraging these advancements, recent tabular deep learning models not only address the unique challenges of tabular data but also offer significant improvements in performance, interpretability, and scalability compared to both traditional deep learning and machine learning approaches. These innovations demonstrate the growing potential for deep learning in handling complex, non-spatial data across a wide range of real-world applications."}, {"title": "2.2 Non-spatial relationships", "content": "Traditional deep learning models, such as CNNs and RNNs, excel in capturing spatial and sequential relationships in structured data types like images and text, where spatial coherence or temporal dependencies play a crucial role. CNN, for instance, detects local patterns by processing spatially adjacent pixels, allowing them to capture meaningful features through convolutions [30]. Similarly, RNNs excel at learning from sequential data, where past information influences future predictions, making them well-suited for text and time-series data. However, tabular data lacks such inherent spatial or temporal structures. In tabular formats, features do not follow any specific spatial or temporal order, and their relative positions carry no meaningful information. Reordering columns or rows does not alter the relationships between features, making models like CNNs and RNNs unsuitable without significant adaptation [31, 32]. This absence of local correlations and temporal dependencies in tabular data makes it challenging for traditional deep-learning models to perform effectively, particularly when non-spatial relationships are critical.\nRecent research has sought to address these challenges by introducing novel architectures specifically designed to capture relational structures within tabular data. For example, models such as Dual-Route Structure-Adaptive Graph Networks (DRSA-Net) [33] and Homological CNNs [34] employ graph-based and topologically constrained approaches to model the dependencies between features. Other approaches, like GOGGLE [35], focus on learning generative models by exploiting underlying relational structures, while TabularNet [36] combines spatial and relational information using advanced techniques such as pooling and Graph Convolutional Networks (GCNs). These innovations represent significant progress in adapting deep learning architectures to the unique challenges posed by tabular data, paving the way for more effective modeling of complex, non-spatial relationships."}, {"title": "2.3 Overfitting in Small Tabular Datasets", "content": "Overfitting is a process where a model is unable to generalize and fits too closely to the training dataset. It can occur for a number of causes, including insufficient data samples and an inadequate training set to adequately represent all potential input data values. Overfitting is a significant challenge in deep learning, particularly when working with small datasets, as the model may end up memorizing the training data instead of learning generalizable patterns. To address this issue, various strategies have been proposed in the literature. One notable approach is transfer learning, where a model pre-trained on a large, related dataset is fine-tuned on a smaller target dataset. This helps to mitigate overfitting by leveraging the prior knowledge embedded in the pre-trained model, thus reducing the need for extensive data in the target domain [32, 41]. Additionally, the use of techniques like image generation from tabular data and the application of CNNs are explored to handle small datasets more effectively. In their work, Koppe et al. [42] highlight the importance of balancing the bias-variance trade-off in deep learning models trained on small datasets. They argue that overfitting occurs when models capture noise and peculiarities specific to the training data rather than generalizable patterns. To counter this, they recommend regularization techniques such as dropout and the incorporation of domain knowledge during model training. These methods help to constrain the model's flexibility and reduce the likelihood of overfitting. LeCun et al. [32] pointed out that while deep networks can learn complex representations, their flexibility can lead to memorization of the training data. To mitigate this, they suggest using unsupervised pre-training and data augmentation, which have proven effective in improving the generalization capabilities of deep learning models.\nWhile deep learning has made significant strides in domains like computer vision and NLP, its application to tabular data has proven more difficult. This disparity can be attributed to the fundamental differences in data volume and structure between these domains. In computer vision and NLP, large-scale datasets such as ImageNet (with millions of labeled images) and GPT-3's massive corpora help models learn complex representations without overfitting. By contrast, tabular datasets commonly found in fields like engineering, healthcare, and finance are often significantly smaller, comprising only hundreds or thousands of samples. This size limitation makes it challenging to train deep learning models effectively and increases the risk of overfitting due to insufficient data diversity. As explained in few studies [43, 44] outlined, deep learning models excel in image classification due to their ability to learn from vast amounts of data, but struggle when applied to small datasets, leading to overfitting. The high number of parameters in deep models, such as those found in CNNs, further exacerbates the problem when there are limited training examples. Similarly, Jain et al., [41] emphasize that deep learning models tend to perform poorly on tabular data due to the dataset's heterogeneous nature. Deep models, which rely on large-scale homogeneous data (as seen in computer vision and NLP), often fail to generalize well in domains where tabular data is used, causing overfitting after only a few epochs.\nTo overcome this, researchers have proposed several methods to address the challenges of training deep learning models on small tabular datasets. One prominent approach is transfer learning, which has been successful in mitigating overfitting in small datasets for computer vision and NLP. For example, Zhao [43] proposes using transfer learning combined with data augmentation to tackle overfitting in small datasets as shown in Figure 5. By pre-training a CNN on large datasets like ImageNet, and then fine-tuning it on smaller datasets, the model can leverage previously learned representations to improve performance on the target task. Jain et al., [41] extend this concept to tabular data by converting tabular datasets into image representations using techniques like IGTD and SuperTML. These methods allow deep learning models, particularly CNNs, to be applied to tabular data by transforming it into an image-like format that can take advantage of pre-trained models, thus reducing overfitting. Horenko [45] introduces the entropy-optimal scalable probabilistic approximations algorithm algorithm to breach the overfitting barrier at a fraction of the computational cost. Badger [46] demonstrates the effectiveness of small language models in processing tabular data without extensive preprocessing, achieving record classification accuracy. Another promising solution is the HyperTab method proposed by Wydma\u0144ski et al. [47], which uses a hypernetwork-based approach to build an ensemble of neural networks specialized for small tabular datasets. A general HyperTab structure is shown in Figure 6. By employing feature subsetting as a form of data augmentation, HyperTab virtually increases the number of training samples without altering the number of parameters. This approach allows the model to generalize better by preventing overfitting, particularly on small datasets."}, {"title": "3 Historical Evolution of Tabular Deep Learning", "content": ""}, {"title": "3.1 Classical Approaches", "content": "Before the emergence of deep learning, traditional machine learning models like Support Vector Machines (SVMs), linear and logistic regression, and tree-based methods have long been the preferred choice for tabular data analysis [48]. These classical approaches were well-suited for small-scale tabular datasets but limited to classification and regression tasks. These models were not only highly interpretable, allowing users to understand and explain predictions, but they also handled both numerical and categorical data well. They were ideally suited for small to medium-sized datasets, as they required less computational power and were quick to train. Despite the rise of deep learning, these traditional models are still preferred in certain cases today. Additionally, their faster training and deployment times make them ideal for applications where real-time decisions are necessary. These classical approaches were well-suited for small-scale tabular datasets but limited to classification and regression tasks. However, these traditional models are not without their limitations. For instance, Clark et al. [49] point out that logistic regression models can encounter problems such as complete and quasi-complete separation, where the model either perfectly or nearly perfectly separates the data. This can lead to extremely large or infinite coefficient estimates, making statistical inference unreliable. Moreover, logistic regression is particularly sensitive to small sample sizes, especially when dealing with low-frequency categorical variables, which can worsen separation issues. To counter this, covariates are often removed, or categories merged, but such actions can lead to oversimplification and a reduction in the model's predictive power. Similarly, Carreras et al. [50] highlight several drawbacks of SVMs, particularly in the soft margin variant. These include the risk of overfitting, increased computational complexity when feature selection is involved, and the non-convex nature of the optimization problem, which complicates finding optimal solutions. Additionally, achieving a complete Pareto front in multi-objective optimization is difficult due to new parameters leading to similar solutions despite weight variations.\nExpanding on the strengths and limitations of traditional models like SVMs and logistic regression, models like Decision Trees, Naive Bayes, and early Neural Networks also relied on manual feature engineering, requiring domain expertise to select relevant features [51]. This process, while labor-intensive, allowed these models to perform effectively on smaller datasets. Abrar and Samad [52] emphasize that while fully connected deep neural networks have become popular in recent years, traditional machine learning models like gradient boosting trees still outperform deep learning models in many cases, particularly when dealing with tabular data containing uncorrelated variables. This study highlights that traditional models like gradient boosting trees are superior when deep models fail, especially in datasets that lack the strong correlations often found in real-world data.\nAdditionally, these classical models do not face the overfitting challenges or high computational costs associated with deep learning. Unlike deep learning models, which tend to overly smooth the relationships in data, tree-based methods can accurately partition the feature space and learn locally constant functions, making them ideal for datasets with irregular target functions. These models are also more robust to uninformative features, which are common in tabular datasets, while neural networks, especially multi-layer perceptrons (MLPs), struggle with irrelevant or redundant features, negatively impacting their performance [53]. Additionally, tree-based models preserve the original orientation of the data, which is important in tabular datasets where features often carry individual meanings, such as age or income. Tree-based models excel at handling the complexities of tabular data by partitioning the feature space in a way that captures non-linear relationships, which deep learning models often struggle to do without overfitting. A study by Fayaz et al. [54] found that even when applied to large datasets, traditional models like XGBoost consistently outperformed state-of-the-art deep learning models, especially when the data lacked strong correlations that deep learning models rely on.\nWhile tree-based models offer numerous advantages for tabular data, they are not without their challenges. Tree-based models, such as decision trees, face several challenges when dealing with tabular data. One key issue is scalability, particularly with large datasets [55]. As the complexity of the dataset increases, decision trees tend to grow deep, which significantly raises runtime and computational costs. This scalability problem is especially pronounced in models designed to handle large datasets, as they struggle to balance depth and size without sacrificing accuracy. Another major drawback is overfitting, where deep trees tend to memorize the training data, including noise and irrelevant features, leading to poor generalization on unseen data [56]. Although techniques like pruning can help mitigate this, they may reduce model accuracy. Traditional decision trees also rely on univariate splits, which can oversimplify complex relationships between features in tabular data, often leading to unnecessarily large trees. While multivariate trees can capture more intricate patterns, they come with added complexity and reduced interpretability. Additionally, decision trees often struggle with imbalanced data, as they tend to be biased towards the majority class [57]. Techniques like SMOTE or cost-sensitive learning are required to address this, but these methods increase computational overhead. Furthermore, decision trees that use models in their leaves, known as model trees, face significant increases in training time and complexity, especially when evaluating a large number of candidate splits across a wide range of features. As tabular deep learning transitioned from classical methods, foundational models emerged that addressed many of the limitations of tree-based approaches."}, {"title": "3.2 Shallow Neural Networks", "content": "Recent research highlights the ongoing challenges of applying deep learning to tabular data. Despite deep learning's success in image and text domains, tree-based models like XGBoost and Random Forests continue to outperform neural networks on medium-sized tabular datasets [9]. This performance gap persists even after extensive hyperparameter tuning. Researchers have identified key challenges for developing tabular-specific neural networks, including robustness to uninformative features, preserving data orientation, and learning irregular functions [7, 70].\nKatzir et al. [60] introduced the Net-DNF architecture, embedding inductive biases similar to gradient-boosting decision trees (GBDTs), to address the shortcomings of FCNs in tabular data tasks. Their experiments demonstrated that Net-DNF outperformed traditional FCNs, particularly on large-scale tabular datasets, underscoring the limitations of conventional neural architectures for this domain. Similarly, Borisov et al. [7] critiqued deep neural networks for tabular data, noting that early attempts with shallow and FCN often failed to match the performance of tree-based models like decision trees and GBDTs. They emphasized that FCNs struggle with the unique challenges of tabular data, such as handling categorical variables, missing entries, and imbalanced datasets, and that feature engineering alone rarely closes the performance gap. In line with this, Abutbul et al. [71] proposed DNF-Net, a neural architecture incorporating Boolean logic and feature selection, which consistently outperformed FCNs on large-scale tabular classification tasks. Chauhan and Singh [72] and Abrar and Samad [73] similarly recognized the use of shallow networks, such as MLPs, for tabular data but highlighted their limitations, including overfitting and limited research focus compared to more advanced methods. While MLPs are effective, they are often surpassed by specialized or more sophisticated architectures in handling the complexities of tabular data.\nEarly applications of shallow neural networks, particularly FCNs, to tabular data often underperformed compared to specialized models like GBDTs. However, recent studies have shown that with proper tuning and architectural enhancements, neural networks can rival or surpass GBDTs. Chen et al. [74] emphasized the efficiency of shallow networks in handling unordered tabular data, while Erichson et al. [75] demonstrated their competitiveness in tasks like fluid dynamics, where fast training and regularization were key advantages. Rubachev et al. [76] further noted that with optimized tuning and techniques like unsupervised pre-training, shallow networks can close the performance gap with GBDTs, although this gain is context-dependent. Fiedler [77] introduced structural innovations such as Leaky Gates and Ghost Batch Norm, which significantly enhanced MLPs for tabular data, enabling them to outperform GBDTs in several cases. These advancements demonstrate that shallow networks when effectively optimized, can meet the unique challenges of tabular data and compete with traditional models.\nThese findings align with the broader consensus that the standard architecture of FCNs often lacks the inductive biases necessary for effectively modeling the complexities of tabular data, such as categorical variables, missing data, and imbalanced datasets. Specialized neural networks are frequently required to address these challenges. However, Grinsztajn et al. [9] offer a more optimistic view, demonstrating that shallow FCNs, like MLPs, can remain competitive when combined with regularization techniques to mitigate overfitting and generalization issues. They further suggest that even simple architectures, such as ResNet, can match the performance of more advanced models, indicating that, with proper modifications, shallow networks can still play a valuable role in handling tabular data."}, {"title": "3.3 Initial Breakthroughs", "content": "TabNet and NODE represent significant advancements in the application of deep learning to tabular data, addressing longstanding challenges in performance, interpretability, and efficiency. This research explores how these models tackle issues inherent to tabular data, such as handling heterogeneous features and preventing overfitting while introducing innovations that set them apart from classical machine learning methods and earlier neural network approaches."}, {"title": "3.3.1 TabNet", "content": "TabNet is a deep learning architecture specifically designed to address the challenges associated with applying neural networks to tabular data. Unlike image or text data, tabular data often consists of heterogeneous features, making it difficult for traditional deep learning models, such as MLPs, to efficiently capture the relationships between the features. Classical machine learning models have traditionally excelled in this domain due to their ability to handle the complex decision boundaries of tabular data. However, deep learning offers potential advantages, such as end-to-end learning and the ability to integrate with other data types, which TabNet leverages through its novel architecture. TabNet introduces several key innovations to overcome these challenges. One of the central features of TabNet is its sequential attention mechanism, which allows the model to select the most important features for each decision step dynamically [24]. This instance-wise feature selection sets TabNet apart from other models, as it can tailor the features used for each individual input rather than relying on a fixed set of features for all instances. This dynamic feature selection leads to more efficient learning by focusing the model's capacity on the most relevant features, which is particularly beneficial for tabular data that may have a mix of irrelevant or redundant information.\nTabNet offers significant advancements in interpretability compared to both classical machine learning models and traditional neural networks. By integrating sparse attention and feature masks, TabNet enhances performance while providing insights into which features influence predictions. This results in both local and global interpretability, making it possible to visualize individual feature importance and quantify overall contributions [79]. The newer InterpreTabNet builds on this by improving feature attribution methods, further enhancing interpretability, and making the model's decisions more transparent at both local and global levels [78].\nTabNet excels in working with raw tabular data without the need for extensive preprocessing or manual feature engineering, which is often required by traditional models like GBDTs. Its end-to-end learning capabilities allow TabNet to directly process raw data, simplifying workflows while maintaining high performance. Additionally, TabNet introduces self-supervised learning, a novel feature for tabular data, where it can pre-train on unlabeled data using masked feature prediction to improve performance on supervised tasks, particularly when labeled data is scarce. Evaluated on various datasets, TabNet has been shown to outperform or match state-of-the-art models, including GBDTs, in both classification and regression tasks. For example, in facies classification, it achieves superior accuracy compared to traditional tree-based models and other deep learning architectures like 1D-CNNs and MLPs [79]. Its flexible architecture, which incorporates sequential feature transformers and attention mechanisms, enhances generalization across different domains, while the use of sparse attention ensures interpretability, addressing a key limitation of traditional deep learning models.\nTabNet, despite its innovations like interpretability and sparse attention, is often outperformed by XGBoost across various datasets, requiring more hyperparameter tuning and showing less consistent results [80]. Additionally, TabNet's training time is significantly longer, making it less practical for quick iterations or real-time applications [81]. It is also prone to overfitting on smaller datasets due to its complex architecture, especially when not tuned correctly."}, {"title": "3.3.2 Neural Oblivious Decision Ensembles (NODE)", "content": "NODE have been proposed to address the specific challenges of applying deep learning to tabular data, which has traditionally been dominated by tree-based models like GBDT. Popov et al. (2019) identified the key limitations of deep learning models in handling tabular data, primarily due to their inability to outperform GBDT consistently. To bridge this gap, NODE was introduced as a deep learning architecture that generalizes ensembles of oblivious decision trees, offering end-to-end gradient-based optimization and multi-layer hierarchical representation learning. This design allows NODE to capture complex feature interactions within tabular data, a task where traditional deep learning models often fall short. One of NODE's key innovations is the use of differentiable oblivious decision trees, where splitting decisions are made through the entmax transformation, allowing for soft, gradient-based feature selection. This approach makes the decision-making process more flexible and differentiable, unlike conventional decision trees that rely on hard splits.\nAdditionally, NODE's multi-layer architecture is designed to capture both shallow and deep interactions within tabular data, effectively functioning as a deep, fully differentiable GBDT model trained end-to-end via backpropagation [62]. The architecture of NODE stacks multiple layers of differentiable oblivious decision trees, which enables NODE to outperform existing tree-based models in many tasks. Furthermore, NODE enhances computational efficiency by allowing pre-computation of feature selectors, significantly speeding up inference without sacrificing accuracy. Joseph [82] explored NODE within the PyTorch Tabular framework, which simplifies deep learning for tabular data by offering a unified API that integrates both NODE and TabNet. This framework addresses the complexity of training deep learning models compared to traditional machine learning libraries like Scikit-learn, making advanced models more accessible for practitioners and researchers. Fayaz et al. [80] compared NODE, TabNet, and XGBoost, noting that while NODE introduces key innovations such as handling mixed data types and data imbalance, it often requires more hyperparameter tuning than XGBoost. However, combining NODE with XGBoost enhances performance, showing NODE's strength in complementing traditional models for tabular data."}, {"title": "4 Recent Advances in Tabular Deep Learning", "content": "While a previous study [7] provides a structured overview of deep learning for tabular data, focusing on challenges like handling categorical variables, data transformation, and model comparison, this survey takes a different approach by emphasizing the historical evolution and algorithmic advancements in the field. We highlight the development of more recent models such as MambaNet, SwitchTab, and TP-BERTa, showing how these architectures have evolved to address the unique complexities of tabular data. By exploring advancements in attention mechanisms, hybrid architectures, and other recent breakthroughs, this survey underscores the transformation of deep learning models into more efficient, scalable, and interpretable solutions. Unlike previous work, this study does not focus on model comparison, as a comprehensive evaluation across models requires a separate analysis tailored to various types of tabular data.\nIn the rapidly evolving field of tabular deep learning, significant improvements have been made with each year bringing new architectures designed to address the increasing complexity of tabular data. Recent models, such as HyperTab and GANDALF, push the boundaries of scalability and interpretability, offering enhanced methods for handling heterogeneous features and high-dimensional data. These newer architectures build upon foundational work, leading to marked performance improvements over traditional approaches."}, {"title": "4.1 TabTransformer", "content": "The TabTransformer model introduces significant advancements in tabular deep learning by leveraging attention mechanisms and hybrid architectures to address the unique challenges posed by tabular data [19]. At its core, TabTransformer employs multi-head self-attention layers adapted from the Transformer architecture, traditionally used in NLP, to capture complex feature interactions and dependencies across the dataset as seen in Figure 10. This attention mechanism enables the model to effectively capture relationships between features, making it particularly useful for datasets with numerous categorical variables.\nThe TabTransformer architecture combines transformer layers with MLP components, forming a hybrid structure optimized for tabular data. Categorical features are embedded using a column embedding layer, which transforms each category into a dense, learnable representation. These embeddings are passed through Transformer layers, which aggregate contextual information from other features to capture interdependencies. The contextualized categorical features are then concatenated with continuous features and processed through the MLP for final prediction. This design leverages the strengths of both contextual learning for categorical data and traditional MLP benefits for continuous data. Additionally, TabTransformer incorporates masked language modeling and replaced token detection, enabling it to pre-train on large amounts of unlabeled data, thus improving performance in low-labeled data scenarios and making it effective for real-world applications.\nRecent advancements in TabTransformer models, such as the self-supervised TabTransformer introduced by Vyas [94], further refine this architecture by leveraging MLM in a pre-training phase to learn from unlabeled data. This self-supervised approach enhances the model's ability to generalize by capturing intricate feature dependencies through self-attention mechanisms. By combining Transformer layers with MLP for final prediction, the model handles mixed data types and smaller dataset sizes effectively. However, trade-offs exist while the model demonstrates strong performance gains, particularly in semi-supervised settings, the reliance on masked language modeling pre-training increases computational overhead, potentially limiting scalability. Interpretability remains moderate, with attention scores providing insights into feature importance, though the model is less interpretable than traditional models like GBDT.\nAnother significant advancement is the GatedTabTransformer, introduced by Cholakov and Kolev [95], which enhances the original TabTransformer by incorporating a gated multi-layer perceptron. This modification improves the model's ability to capture cross-token interactions using spatial gating units. The GatedTabTransformer boosts performance by approximately 1 percent in AUROC compared to the standard TabTransformer, especially in binary classification tasks. However, this comes at the cost of increased computational complexity due to the additional processing required for the spatial gating units. While the model shows improved performance, its scalability, and interpretability remain limited compared to simpler models like MLPs or GBDTs.\nTherefore, while TabTransformer models offer notable improvements in handling tabular data through attention mechanisms and hybrid architectures, they present trade-offs in terms of performance, scalability, and interpretability. Recent variations like the self-supervised TabTransformer and GatedTabTransformer demonstrate the potential of these models to outperform traditional approaches, though at the cost of higher computational demands."}, {"title": "4.2 FT-Transformer", "content": "The FT-Transformer model, as presented by Gorishniy et al. [96"}]}