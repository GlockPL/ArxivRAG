{"title": "Testing Neural Network Verifiers: A Soundness Benchmark with Hidden Counterexamples", "authors": ["Xingjian Zhou", "Hongji Xu", "Andy Xu", "Zhouxing Shi", "Cho-Jui Hsieh", "Huan Zhang"], "abstract": "In recent years, many neural network (NN) verifiers have been developed to formally verify certain properties of neural networks such as robustness. Although many benchmarks have been constructed to evaluate the performance of NN verifiers, they typically lack a ground-truth for hard instances where no current verifier can verify and no counterexample can be found, which makes it difficult to check the soundness of a new verifier if it claims to verify hard instances which no other verifier can do. We propose to develop a soundness benchmark for NN verification. Our benchmark contains instances with deliberately inserted counterexamples while we also try to hide the counterexamples from regular adversarial attacks which can be used for finding counterexamples. We design a training method to produce neural networks with such hidden counterexamples. Our benchmark aims to be used for testing the soundness of NN verifiers and identifying falsely claimed verifiability when it is known that hidden counterexamples exist. We systematically construct our benchmark and generate instances across diverse model architectures, activation functions, input sizes, and perturbation radii. We demonstrate that our benchmark successfully identifies bugs in state-of-the-art NN verifiers, as well as synthetic bugs, providing a crucial step toward enhancing the reliability of testing NN verifiers. Our code is available at https://github.com/MVP-Harry/SoundnessBench and our benchmark is available at https://huggingface.co/datasets/SoundnessBench/SoundnessBench.", "sections": [{"title": "1 Introduction", "content": "As neural networks (NNs) have demonstrated great capability in various applications, the problem of formal verification for NNs has attracted much attention, due to its importance in the trustworthy deployment of NNs with formal guarantees especially for safety-critical applications. Typically the goal of NN verification is to formally check if the output by an NN provably satisfies particular properties for any input within a certain range. In recent years, many softwares specialized for formally verifying NNs, a.k.a. NN verifiers, have been developed, such as \u03b1,\u03b2-CROWN (Zhang et al., 2018; Xu et al., 2020, 2021; Wang et al., 2021; Zhang et al., 2022; Shi et al., 2024), Marabou (Katz et al., 2019; Wu et al., 2024), NeuralSAT (Duong et al., 2024), VeriNet (Henriksen and Lomuscio, 2020), nnenum (Bak, 2021), NNV (Tran et al., 2020), MN-BaB (Ferrari et al., 2021), etc.\nTo measure the performance of various NN verifiers and incentivize new development, many benchmarks have been proposed, and there is a series of standardized competitions, International Verification of Neural Networks Competition (VNN-COMP) (Bak et al., 2021; M\u00fcller et al., 2023; Brix et al., 2023a,b), that compiled diverse benchmarks to evaluate NN verifiers. However, there remains a crucial limitation in the existing benchmarks: instances in the benchmarks lack a \u201cground truth\u201d, making it difficult to determine whether a verification claim is sound when no existing verifier can falsify the properties being checked. For example, in VNN-COMP 2023 (Brix et al., 2023a), a benchmark of Vision Transformers (ViT) (Dosovitskiy et al., 2021) proposed by a participating \u03b1,\u03b2-CROWN team (Shi et al., 2024) did not contain any instance falsifiable by the participants, while another participating Marabou (Wu et al., 2024) team showed 100% verification on this benchmark, which was later found to be unsound due to issues when leveraging an external solver (Brix et al., 2023a). Such a limitation of existing benchmarks also makes it difficult for developers of NN verifiers to promptly identify bugs due to the error-prone nature of NN verification."}, {"title": "2 Background and Related Work", "content": "The problem of NN verification. An NN can generally be viewed as a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$ which takes an d-dimensional input $x \\in \\mathbb{R}^d$ and outputs $h(x) \\in \\mathbb{R}^K$. For instance, if the NN is used in a K-way classification task, h(x) denotes the predicted logits score for each of the K classes. NN verification involves a specification on the input and the output. The input can be specified with an input region C. And the properties to verify for the output can be specified using a function $s : \\mathbb{R}^K \\rightarrow \\mathbb{R}$ which maps the output of the NN to a value, and the desired property can be denoted as $h(x) = s(f(x)) > 0$. Formally, the NN verification problem can be defined as verifying:\n$\\forall x \\in C, h(x)>0, \\text{ where } h(x) = s(f(x)).$ (1)\nIn the example of K-way classification with a verification problem for the robustness of prediction against small perturbations on the input, the input region is often defined as a small $l_\\infty$ ball with a perturbation radius of e around a clean data point xo, as\n$C = B(x_0, \\epsilon) := \\{ x | ||x - x_0 ||_\\infty \\leq \\epsilon \\},$ (2)\nwhere x xo is a small perturbation within the allowed range. The output specification can be defined as $s(z) := \\min\\{ z_y - z_i | 1 \\leq i \\leq K, i \\neq c\\}$ for $z = f(x)$, where y is the ground-truth label for the classification, i \u2260 y is any other incorrect class, and s(z) denotes the minimum margin between the ground-truth class and other incorrect classes. The goal of the verification is to verify that $h(x) = s(f(x)) > 0$ ($\\forall x \\in C$), which means that the classification is always correct and thus robust under any small perturbation with $||x - x_0||_\\infty \\leq \\epsilon$. It can also be directly written as:\n$\\forall x \\in B(x_0,\\epsilon), h(x) = \\min\\{ f_y(x) - f_i(x) | 1 \\leq i \\leq K, i \\neq y\\} > 0,$ (3)\nwhere $f_y(x)$ and $f_i(x)$ denote the logits scores for the correct class y and other classes i \u2260 y, respectively. This problem is commonly used for benchmarking NN verifiers, and we also focus on this setting.\nSoundness of NN verifiers. An NN verifier is sound if the verification process is rigorously provable, so that every time the verifier claims that Eq. (1) holds, there should exist no counterexample x \u2208 C which violates the condition h(x) > 0. However, even if an NN verifier is theoretically designed to be sound and rigorous, its implementation may contain bugs or errors which can compromise the soundness in practice. The soundness issue of an NN verifier can be identified by a falsifier if a counterexample x \u2208 C is found to have h(x) < 0 and thus violate the desired property h(x) > 0, while the NN verifier still claims a successful verification. Although methods such as adversarial attacks by projected gradient descent (PGD) (Madry et al., 2018) can often be used to find adversarial examples as"}, {"title": "3 Methodology", "content": "To address this limitation, we propose a novel way to test NN verifiers and examine their soundness. We propose to develop a benchmark containing instances with hidden counterexamples predefined counterexamples for the properties being verified, which can be kept away from NN verifier developers. Considering that NN verifiers often contain a preliminary check for counterexamples using adversarial attacks (Madry et al., 2018) before starting the more expensive verification process, we aim to eliminate any easily found counterexamples and ensure that the pre-defined counterexamples also remain hidden against adversarial attacks, so that the NN verifiers enter the actual verification process. We then check if the NN verifier falsely claims a successful verification when we know the existence of a hidden counterexample.\nWe focus on a classification task using NN and the properties being verified are about checking if the NN always makes robust predictions given small input perturbations within a pre-defined range, which is commonly used for benchmarking NN verifiers. To build a benchmark with hidden counterexamples, it is essentially about training NNs which 1) make a wrong prediction on the pre-defined counterexamples; 2) make correct and robust predictions on most other inputs so that counterexamples cannot be trivially found by adversarial attacks. To meet both objectives, we propose to train NNs with a two-part loss function. The first part aims to achieve misclassification on the pre-defined counterexamples, while the second part is based on adversarial training (Madry et al., 2018) to enforce robustness on other inputs. Since the inherent conflicts between these two objectives makes training particularly challenging, we strengthen the training by designing a margin objective in the first part and incorporating a perturbation sliding window in the second part.\nIn the end, our proposed training framework enables us to build a benchmark that incorporates 26 distinct NN models with diverse architectures (CNN, MLP and ViT), activation functions (ReLU, Sigmoid and Tanh), and perturbation radii, designed to comprehensively test different verifiers. In these models, we planted a total of 206 hidden counterexamples. Using the benchmark, we successfully identified real bugs in well-established and state-of-the-art NN verifiers including \u03b1,\u03b2-CROWN, Marabou, and NeuralSAT. Additionally, we tested our benchmark by introducing synthetic bugs into various NN verifiers, to further demonstrate its effectiveness in detecting errors in NN verifiers."}, {"title": "3.1 Problem Definition", "content": "Table 1 contains a list of terms used in this paper. Our problem is about building a benchmark for NN verification with hidden pre-defined counterexamples. Focusing on the setting of robustness verification for K-way classification described in Section 2, the benchmark contains a set of instances, where each instance (f, xo, y, \u20ac) defines a property of NN model f to verify as:\n$\\forall x \\in B(x_0, \\epsilon), f_y(x) - f_i(x) > 0.$ (5)"}, {"title": "3.2 Data Generation", "content": "This aims to verify the robustness of model f, such that the prediction by f for the classification task is always y as $f_y(x) - f_i(x) > 0$, for any input \u00e6 within an $l_\\infty$ ball with perturbation radius e around 20 define in Eq. (2).\nSince we are primarily concerned with testing the soundness of NN verifiers, our benchmark contains unverifiable instances with pre-defined counterexamples deliberately planted. Specifically, suppose (f, x0, y, e) is an unverifiable instance, we aim to plant a counterexample xcex with perturbation Scex which violates Eq. (5):\n$x_{\\text{cex}} = x_0 + \\delta_{\\text{cex}} \\text{ s.t. } ||\\delta_{\\text{cex}}||_\\infty \\leq \\epsilon, f_y(x_{\\text{cex}}) \u2013 f_i(x_{\\text{cex}}) \\leq 0.$ (6)\nSuch an instance allows us to detect bugs in an NN verifier if it claims Eq. (5) can be verified while we know the existence of xcex satisfying Eq. (6).\nMeanwhile, we also aim to make the planted counterexample \u201chidden\u201d to NN verifiers so that NN verifiers cannot easily falsify the unverifiable instances without even entering the actual verification procedure. In particular, we aim to make the counterexample hard to be found by adversarial attacks which are often used in NN verifiers as a preliminary check before using a more expensive verification procedure. For any perturbation 8 ($||\\delta||_\\infty < \\epsilon$) found by an adversarial attack such as PGD (Madry et al., 2018), we aim to achieve:\n$f_y(x_0 + \\delta) \u2013 f_i(x_0 + \\delta) > 0, \\quad \\text{where } \\delta \\approx \\underset{||\\delta|| \\leq \\epsilon}{\\text{arg min}} f_y(x_0 + \\delta) \u2013 f_i(x_0 + \\delta),$ (7)\nwhere the minimization in Eq. (7) is approximately solved by an adversarial attack (which may not reach the globally optimal solution), so that the example x0 +8 found is not a counterexample. If both Eq. (6) and Eq. (7) are achieved, we call xcex a hidden counterexample, and we have a ground-truth for the instance (f, x0, y, e), knowing that the instance cannot be verified.\nAdditionally, our benchmark also contains regular clean instances which do not have pre-defined counterexamples planted. These instances are added so that developers of NN verifiers cannot simply tell that all the instances are unverifiable. When we train our NNs for the benchmark, we try to eliminate counterexamples which can be found by adversarial attacks. Verifiers may be able to verify some of these instances, but it is still possible that counterexamples may exist in some of the instances. Unlike the unverifiable instances, we do not have a ground-truth for these instances which are not our major focus for testing the soundness of NN verifiers.\nSince the goal of our benchmark is to test the soundness of NN verifiers instead of their applicability and performance in real-world applications, we build our benchmark using a synthetic classification task. While our final benchmark contains many different NN models, without the loss of generality, we focus on one model with one perturbation radius \u20ac when we introduce our data generation and training. Thereby, for each instance (f, xo, y, \u20ac) defined in Section 3.1, we now use (x, y) to denote an instance for simplicity given a single model f and perturbation radius \u20ac.\nOur dataset D Dunv UDclean contains 2n instances, where Dunv and Dclean are a set of unverifiable instances and clean instances, respectively:\n$D = D_{\\text{unv}} \\cup D_{\\text{clean}} = \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(2n)}, y^{(2n)}) \\},$ (8)\n$D_{\\text{unv}} = \\{ (x^{(\\text{unv},1)},y^{(\\text{unv},1)}), (x^{(\\text{unv},2)},y^{(\\text{unv},2)}), \u2026\u2026\u2026, (x^{(\\text{unv},n)},y^{(\\text{unv},n)}) \\},$ (9)\n$D_{\\text{clean}} = \\{ (x^{(\\text{clean},1)},y^{(\\text{clean},1)}), (x^{(\\text{clean},2)}, y ^{(\\text{clean}, 2)}), ..., (x^{(\\text{clean},n)}, y^{(\\text{clean},n)}) \\}.$ (10)\nFor each instance (x(i), y(i)) \u2208 D (1 \u2264 i \u2264 2n), x(i) \u2208 [\u2212c, c]d is an input and y(i) (1 \u2264 y(i) < K) is the label, and they are both randomly generated following uniform distributions.\nFor each unverifiable instance $(x^{(\\text{unv},i)}, y^{(\\text{unv},i)}) \\in D_{\\text{unv}} (1 \\leq i \\leq n)$, we generate a pre-defined counterexample $(x_{\\text{cex}}^{(i)}, y_{\\text{cex}}^{(i)})$ where $x_{\\text{cex}}^{(i)} = x^{(\\text{unv},i)} + \\delta_{\\text{cex}}^{(i)}$ such that $\\delta_{\\text{cex}}^{(i)} \\in \\mathbb{R}^d$ is the perturbation, and $y_{\\text{cex}}^{(i)}$ is the target label. $\\delta_{\\text{cex}}^{(i)}$ is randomly generated from $([-\\epsilon, -r \\cdot \\epsilon] \\cup[r \\cdot \\epsilon, \\epsilon])^d (0 \\leq r < 1)$ satisfying $r\\epsilon \\leq ||\\delta_{\\text{cex}}^{(i)}||_\\infty < \\epsilon$, and $y_{\\text{cex}}^{(i)}$ is randomly generated from $\\{j | 1 \\leq j \\leq K,j \\neq y^{(\\text{unv},i)}\\}$ satisfying $y_{\\text{cex}}^{(i)} \\neq y^{(\\text{unv},i)}$. Since we aim to make $x^{(\\text{unv},i)}$ and $x_{\\text{cex}}^{(i)}$ have different labels, we try to make them separated, and thus we set $||\\delta_{\\text{cex}}^{(i)}||_\\infty > r\\epsilon$ with a hyperparameter r (0 \u2264 r < 1). We visualize the construction of the pre-defined counterexample in Figure 1. As mentioned before, clean instances in Dclean do not have pre-defined counterexamples."}, {"title": "3.3 Training", "content": "Our training contains two training objectives. The first part aims to make our pre-defined counterexamples become real counterexamples, to satisfy Eq. (6). The second part aims to eliminate counterexamples which can be found by adversarial attacks to satisfy Eq. (7) for both unverifiable instances and clean instances.\nObjective for the pre-defined counterexamples. In the first objective regarding Eq. (6), for each unverifiable instance $(x^{(\\text{unv},i)}, y^{(\\text{unv},i)}) (1 \\leq i \\leq n)$ with the pre-defined counterexample $(x_{\\text{cex}}^{(i)}, y_{\\text{cex}}^{(i)})$, we aim to make the class predicted from output logits $f(x_{\\text{cex}}^{(i)})$ match target label $y_{\\text{cex}}^{(i)}$, i.e.,\n$\\underset{1<j<K}{\\text{arg max}} f_j(x_{\\text{cex}}^{(i)}) = y_{\\text{cex}}^{(i)}$\nThis may be achieved by a standard cross-entropy (CE) loss which is commonly used for training NN classifiers:\n$L_{\\text{cex, CE}} = \\sum_{i=1}^n L_{\\text{CE}} (f(x_{\\text{cex}}^{(i)}), y_{\\text{cex}}^{(i)}),$ (11)\nwhere $L_{\\text{CE}}(z, y) = -log \\frac{\\text{exp}(z_y)}{\\sum_{j=1}^K \\text{exp}(z_j)}$ for output logits $z \\in \\mathbb{R}^K$ and label y is the cross-entropy loss. Training with this loss tends to maximize $f_{y_{\\text{cex}}}(x_{\\text{cex}}^{(i)})$ and minimize $f_{y^{(\\text{unv},i)}}(x^{(\\text{unv},i)})$ without a cap on the margin between them, to achieve the goal with $\\underset{1<j<K}{\\text{arg max}} f_j(x_{\\text{cex}}^{(i)}) = y_{\\text{cex}}^{(i)}$. However, we find that making $f_{y_{\\text{cex}}}(x_{\\text{cex}}^{(i)})$ substantially greater than $f_{y^{(\\text{unv},i)}}(x^{(\\text{unv},i)})$ tends to make the counterexamples more obvious to adversarial attacks and thus make our training with the second objective difficult. Therefore, instead of using the cross-entropy loss in Eq. (11), we design a margin objective in order to achieve $0 \\leq f_{y_{\\text{cex}}}(x_{\\text{cex}}^{(i)}) - f_{y^{(\\text{unv},i)}}(x^{(\\text{unv},i)}) \\leq \\lambda$ with a maximum desired margin \u03bb :\n$L_{\\text{cex}} = L_{\\text{cex, margin}} = \\frac{1}{n}\\sum_{i=1}^n \\text{max}\\{0, f_{y^{(\\text{unv},i)}}(x^{(\\text{unv},i)}) \u2013 f_{y_{\\text{cex}}}(x_{\\text{cex}}^{(i)}) + \\lambda\\}.$ (12)\nObjective with adversarial training. Our second objective is adversarial training for the entire dataset ID with both unverifiable instances and clean instances as defined in Eq. (8). As mentioned in Eq. (4), adversarial training solves a"}, {"title": "3.4 Benchmark Design", "content": "min-max problem. Our loss function for the outer minimization is basically (we will introduce an enhanced version in Eq. (16)):\n$L_{\\text{adv, naive}} = \\frac{1}{2n} \\sum_{1<i<2n} L_{\\text{CE}}(f(x^{(i)} + A(x^{(i)}, y^{(i)})), y^{(i)}),$ (13)\nwhere $A(x^{(i)}, y^{(i)})$ is from the inner maximization which is approximately solved by an adversarial attack:\n$A(x^{(i)}, y^{(i)}) \\approx \\underset{||\\delta||_\\infty \\leq \\epsilon}{\\text{arg max }} L_{\\text{CE}} (f(x^{(i)} + \\delta), y^{(i)}).$ (14)\nThis objective aims to make the classification correct on any worst-case example which can be found by adversarial attacks, thereby eliminating obvious counterexamples.\nHowever, since a new perturbation $A(x^{(i)}, y^{(i)})$ is generated at each training epoch by conducting an adversarial attack on the current model, we find that it is hard for the training to converge if we only use the new perturbation at each epoch. This is because training on a new perturbation can often break the model on other perturbations including those used in previous epochs. The inherent conflict between our two training objectives makes the training particularly challenging. To resolve this issue, we propose a perturbation sliding window, where we use multiple perturbations generated from the most recent epochs. For each instance (x(i), y(i)), we maintain a window W(i) of perturbations generated in the most recent w epochs:\n$W^{(i)} = \\{ \\delta^{(i)}_1, \\delta^{(i)}_2, ..., \\delta^{(i)}_w\\},$ (15)\nwhere w is the maximum length of the sliding window (there can be fewer than w elements in W(i) in the first w 1 epochs). In each epoch, a new perturbation $A(x^{(i)}, y^{(i)})$ is added to the end of window W(i), and if there are more than w perturbations in W(i), the oldest perturbation will be erased from the window. Using this window, we update the our objective originally defined in Eq. (13), to utilize all the perturbations in the sliding window for the training, as:\n$L_{\\text{adv}} = L_{\\text{adv, window}} = \\frac{1}{2n}\\sum_{1<i<2n} \\frac{1}{|W^{(i)}|} \\sum_{\\delta\\in W^{(i)}} L_{\\text{CE}} (f(x^{(i)} + \\delta),y^{(i)}).$ (16)\nOverall objective. Suppose model f is parameterized by parameters 0, denoted as fe, and our two loss functions both depend on 0, denoted as Lcex (0) and Ladv (0). Then our overall loss function combines the two objective as\n$L(\\theta) = L_{\\text{cex}} (\\theta) + L_{\\text{adv}}(\\theta),$ (17)\nand we optimize model parameters by solving arg min, L(0) using a gradient-based optimizer."}, {"title": "4 Experiments", "content": "In this section, we conduct a comprehensive set of experiments. We evaluate the effectiveness of our NN training method for building our benchmark. We also use our benchmark to test the soundness of several state-of-the-art NN verifiers. Additionally, we demonstrate the ability of our benchmark to uncover synthetic bugs which we deliberately introduce to two NN verifiers. Finally, we conduct an ablation study to justify the design choices for our training."}, {"title": "4.1 Training NNs and Building the Benchmark", "content": "Training. For every model in the benchmark, we apply our aforementioned training framework with the following hyperparameters: (1) Number of training epochs is set to 5000; (2) Adam optimizer (Kingma and Ba, 2015) with a cyclic learning rate schedule is used, where the learning rate gradually increases from 0 to a peak value during the first half of the training, and then it gradually decreases to 0 during the second half of the training, with the peak learning rate set to 0.001; (3) The threshold in the margin objective defined by Eq. (12) is set to x = 0.01; (4) The size of the perturbation sliding window used in Eq. (16) is set to w = 300; (5) We use the PGD attack (Madry et al., 2018) for the adversarial training with up to 150 restarts (number of different random starting points for attack handled in parallel) and 150 attack steps (number of PGD steps in the attack).\nEvaluation. For each unverifiable instance $(x^{(\\text{unv},i)},y^{(\\text{unv},i)}) (1 \\leq i \\leq n)$, we evaluate it in 3 steps: (1) We check whether the model predicts the correct class $y^{(\\text{unv},i)}$ for input $x^{(\\text{unv},i)}$; (2) We check whether the model predicts the expected class $y_{\\text{cex}} \\neq y^{(\\text{unv},i)}$ for the pre-defined counterexample $x_{\\text{cex}}^{(i)}$; (3) We check whether no counterexample can be found by adversarial attacks in the region $B(x^{(\\text{unv},i)}, \\epsilon)$, i.e. $x_{\\text{cex}}^{(i)}$ is a hidden counterexample. For the clean instances $(x^{(\\text{clean},i)},y^{(\\text{clean},i)}) (1 \\leq i < n)$, we evaluate it in 2 steps: (1) We check whether the model predicts the correct class $y^{(\\text{clean},i)}$ for input $x^{(\\text{clean},i)}$; (2) We check whether no counterexample can be found by adversarial attacks in the region $B(x^{(\\text{clean},i)}, \\epsilon)$.\nTo make sure that our pre-defined counterexamples are truly hidden against adversarial attacks which aim to find counterexamples, we evaluate them using strong adversarial attacks. Specifically, we apply PGD attack with strong attack"}, {"title": "4.2 Verification Settings & Results", "content": "We use our benchmark to test the soundness of the following 4 representative NN verifiers:\n\u2022 \u03b1,\u03b2-CROWN 1 (Zhang et al., 2018; Xu et al., 2020, 2021; Wang et al., 2021; Zhang et al., 2022; Shi et al., 2024) is a state-of-the-art NN verifier that combines linear relaxation-based perturbation analysis (LiRPA) implemented in auto_LiRPA2 with branch-and-bound to verify robustness and safety properties of NNs. It is open-source and accelerated with GPUs."}, {"title": "4.3 Synthetic Bugs", "content": "Findings. The results are presented in Tables 5 to 8, with the following key findings:\n(1) Our benchmark successfully identified bugs in three well-established verifiers, including \u03b1,\u03b2-CROWN, Neu- ralSAT, and Marabou 2023. Table 5 shows the number of \u201cverified\u201d instances claimed by the verifiers on unverifiable instances. A bug-free verifier should not claim a successful verification on any of these instances. However, both \u03b1,\u03b2- CROWN and NeuralSAT incorrectly claimed many unverifiable instances as verified for two \u201cCNN AvgPool\u201d models. The issue is likely caused by some bug in the implementation of verification strategy for AvgPool layers. \u03b1,\u03b2-CROWN and NeuralSAT both use auto_LiRPA as the bound computation engine, and thus it is reasonable that they share the same bug. For Marabou 2023, it claimed successful verification on many unverifiable instances for \u201cCNN Tanh\u201d and ViT models, which shows that Marabou 2023 has bugs when handling these models. We also notice that no bug has been detected for Marabou 2024 on \u201cCNN Tanh\" and it does not support ViTs. We have highlighted the proportion of unsound verification claims in bold in Table 5.\n(2) Hidden counterexamples in our benchmark are difficult for existing verifiers to find. One goal of our bench- mark is to ensure that most of our pre-defined counterexamples remain difficult for existing verifiers to find, as NN verifiers typically contain falsification mechanisms. Table 6 shows the number of unverifiable instances falsified by NN verifiers. Results show that most unverifiable instances are not falsified by NN verifiers, which means that most of\nTo further test our soundness benchmark, we simulate NN verifiers with bugs by introducing synthetic bugs into \u03b1,\u03b2- CROWN and NeuralSAT, with shrunk input & intermediate bounds, and randomly dropped domains, respectively:\n\u2022 Shrunk input & intermediate bounds: In this synthetic bug, we modify verifiers and deliberately shrink input and intermediate bounds in an unsound way, by a factor denoted as a (0 < a < 1). The input bounds basically correspond to the perturbation radius e, and we reduce \u0454 into (1 \u2212 \u03b1)\u00b7\u03b5. Intermediate bounds are output bounds of intermediate layers as required by the verifiers to perform linear relaxation in LiRPA. Suppose [hk, hk] denote the lower and upper bound, respectively, of an intermediate layer k, and hk (x0) is the output of layer k when an unperturbed input is given. We then shrink the intermediate bounds by pushing them towards hk (x0):\n$[h_k, h_k] \\rightarrow [h_k+a (h_k(x_0) \u2013 h_k), h_k + a (h_k(x_0) - h_k)] \\text{ for } 0 < a \\leq 1.$ \nShrinking the input bounds and intermediate bounds breaks the soundness of verification, which is thus used as a synthetic bug.\n\u2022 Randomly dropped domains: Branch-and-bound used in NN verifiers iteratively branches (or splits) the verification problem into smaller subproblems, where each subproblem involves a domain of input bounds or intermediate bounds, and tighter verified bounds can be computed for the smaller domains. In this synthetic bug, we randomly drop a proportion of new domains created in each branch-and-bound iteration, and thereby break the soundness of the verifier. We use \u1e9e (0 < \u03b2 < 1) to denote the ratio of randomly dropped domains."}, {"title": "4.4 Ablation Study", "content": "In Section 3.3, we introduced two training techniques, margin objective and perturbation sliding window, in addition to the main training framework that are crucial to producing hidden counterexamples. This section presents an ablation study to evaluate the individual contributions of these techniques to the model's ability to produce hidden adversarial instances.\nFor the study, we select a baseline architecture, CNN 1 Conv, and we conduct experiments across four configurations of perturbation radii (\u20ac = 0.2, 0.5) and input sizes (1\u00d75\u00d75 and 3\u00d75\u00d75). We maintain consistency in training parameters and train 5 models with different random seeds per configuration to consider randomness. For each experiment, the only variable we adjust is the presence of the ablation subject (i.e. whether margin objective is enabled and the maximum window size of the perturbation sliding window), so we can isolate and quantify their individual effects. Our primary metric is the average number of hidden counterexamples generated under each setting, allowing us to evaluate the effectiveness of each training technique."}, {"title": "5 Conclusion", "content": "In this paper, we present a benchmark with pre-defined hidden counterexamples that can be used to test the soundness of NN verifiers. Using a two-objective training framework along with two additional training techniques, margin objective and perturbation sliding window, we manage to incorporate a total of 26 models, 206 unverifiable instances, and 260 clean instances across 9 distinct NN architectures. Notably, our benchmark revealed internal bugs in three well-established NN verifiers: \u03b1,\u03b2-CROWN, NeuralSAT, and Marabou 2023. This highlights the critical need for a reliable benchmark with ground-truth (unverifiable instances in our benchmark are known to be unverifiable), which can effectively expose hidden bugs and prevent similar errors. Therefore, we believe that this benchmark will serve as a valuable resource for developers building and improving NN verifiers in the future."}]}