{"title": "Large Language Models as Mirrors of Societal Moral Standards", "authors": ["Evi Papadopoulou", "Hadi Mohammadi", "Ayoub Bagheri"], "abstract": "Prior research has demonstrated that language models can, to a limited extent, represent moral norms in a variety of cultural contexts. This research aims to replicate these findings and further explore their validity, concentrating on issues like 'homosexuality' and 'divorce'. This study evaluates the effectiveness of these models using information from two surveys, the WVS and the PEW, that encompass moral perspectives from over 40 countries. The results show that biases exist in both monolingual and multilingual models, and they typically fall short of accurately capturing the moral intricacies of diverse cultures. However, the BLOOM model shows the best performance, exhibiting some positive correlations, but still does not achieve a comprehensive moral understanding. This research underscores the limitations of current PLMs in processing cross-cultural differences in values and highlights the importance of developing culturally aware AI systems that better align with universal human values.", "sections": [{"title": "1 Introduction", "content": "Exploring moral norms and cultural values within language models has emerged as a new area of research, especially as these models are increasingly applied in real-world settings. Some of these include content moderation for social media platforms, chatbots for different purposes, content creation as well as real-time translation. This research investigates whether pre-trained language models (PLMs), both monolingual and multilingual, can capture the fine-grained variations in moral norms across different cultures. These variations refer to the subtle differences, the specific way in which ethical standards and values are understood across different cultures. Recent studies indicate that while language models, trained on extensive web-text corpora, are capable of processing language and performing various Natural Language Processing (NLP) tasks, they also integrate societal and cultural biases during their training (Stanczak and Augenstein, 2021). These biases can affect how models understand and generate language, which might lead to problems when they are used in settings where moral judgments are important.\nThe ability of these models to represent diverse moral and cultural norms is not well understood yet but it is under exploration. As language models are increasingly used in applications such as content moderation, it's essential to examine if these models reflect global moral norms or primarily reflect the biases of dominant cultures. For instance, prior work has shown that multilingual PLMs could potentially capture broader cultural values through the diverse linguistic contexts they are trained in, yet they often fail to accurately represent the moral nuances of less dominant cultures (H\u00e4mmerl et al., 2022).\nTwo well-known surveys, the World Values Survey (WVS) and the PEW Global Attitudes Survey, are used as benchmarks to assess how well these models align with human moral values across various countries. These surveys provide insights into the ethical and cultural norms worldwide and serve as the ground truth. By reformulating the survey questions into prompts for the models, this study aims to uncover how closely PLMs can mirror the stances of people around moral dilemmas. Following the methodologies outlined in 'Knowledge of Cultural Moral Norms in Large Language Models' by Ramezani and Xu (2023) and 'Probing Pre-Trained Language Models for Cross-Cultural Differences in Values' by Arora et al. (2022), this research attempts to replicate these studies by validating or challenging their conclusions.\nThe findings from this research will help improve our understanding of the ethics embedded in Al models and could enable PLMs to serve as tools for exploring cultural phenomena. By examining the alignment between model outputs and established cultural norms, this study aims to identify"}, {"title": "2 Literature Review", "content": "It is commonly assumed that the expansive and diverse nature of the Internet would naturally encompass a broad spectrum of worldviews. However, its enormous size does not guarantee diversity. Accordingly, regardless of the capacity of a language model or the amount of data it processes, if the training data contains biases, these biases will likely be reflected in the model. It is widely acknowledged that large language models often exhibit biases, such as stereotypical associations or negative sentiments toward specific groups (Bender et al., 2021).\nThe impact of biases in training data on language model performance is significant, affecting their reliability and fairness across various applications. These biases can harm decision-making processes, especially in areas requiring sensitive judgments such as content moderation and automated decision systems. For example, studies by Bolukbasi et al. (2016) have shown how gender biases in training data create gender stereotypes in language model outputs, impacting job recommendation systems more than others. Similarly, Sap et al. (2019) found that biases could lead to higher toxicity scores in content moderation systems against specific groups, unfairly targeting certain demographic groups. These examples highlight the need for robust bias detection and mitigation strategies to improve the fairness of language models in real-world settings.\nProbing has been a prominent method for investigating the knowledge and biases inherent in PLMs and has been used for different purposes. For instance, Ousidhoum et al. (2021) utilized probing to identify toxic content generated by PLMs towards different communities. Similarly, Nadeem et al. (2021) employed Context Association Tests to explore stereotypical biases within these models. Additionally, Arora et al. (2022) adapted cross-cultural surveys to create prompts for evaluating multilingual PLMs (mPLMs) across 13 languages. They analyzed the average responses from participants in each country and category, revealing that mPLMs often fail to align with the cultural values of the languages they are trained to process.\nVarious probing techniques have been developed to detect harmful biases in language models. These include cloze-style probing, which measures bias at an intra-sentence level (Nadeem et al., 2020), and pseudo-log likelihood-based scoring, which assesses probabilities across a text span (Salazar et al., 2019). However, both methods have drawbacks: cloze-style probing may introduce biases based on the tokens used in the input probe, while pseudo-log likelihood scoring assumes that all masked tokens are statistically independent (Kaneko and Bollegala, 2021). A simpler method used in this study involves directly obtaining the probability of the token of interest from the transformer model, as detailed in the foundational work by Vaswani et al. (2017) which describes the underlying mechanisms that enable this capability.\nA number of studies have examined whether language models capture cross-cultural differences in moral values. For instance, Ramezani and Xu (2023) found that large English pre-trained language models (EPLMs) do capture variations in moral norms to some extent, with the norms inferred being more accurate in Western cultures than in non-Western ones. They also observed that fine-tuning these models on global surveys of moral norms can enhance their moral knowledge, though this approach compromises their ability to accurately estimate English moral norms and potentially introduces new biases. Another study highlighted significant differences in the cultural values reflected by various multilingual models, even when trained on data from the same sources. Despite these differences, the biases present in the models did not align with those documented in large-scale values surveys (Arora et al., 2022)."}, {"title": "3 Datasets", "content": ""}, {"title": "3.1 World Values Survey", "content": "World Values Survey (WVS) (Haerpfer et al., 2021) collects data on people's values across cultures in a detailed way. The Ethical Values and Norms section in WVS Wave 7 is the first dataset used. This wave ran from 2017 to 2020 and is publicly available. Participants from 55 countries were surveyed on their views regarding 19 morally related statements, such as divorce, euthanasia, political violence, and cheating on taxes. The questionnaire was translated into the primary languages spoken in each country and offered multiple response options."}, {"title": "3.2 PEW 2013 Global Attitude Survey", "content": "The second dataset comes from the Pew Global Attitudes Project survey which provides extensive information about people's opinions on important contemporary topics discussed around the world. Conducted in 2013, this survey provides information on eight ethically connected subjects, such as divorce or drinking alcohol. The dataset has a total of 100 respondents from each of the 39 countries. Three answers to the survey's English-language questions were available: 'morally acceptable', 'not a moral issue', and 'morally unacceptable'. From the original dataset, we retained only the country names and responses to questions Q84A to Q84H. Then, these responses were normalized between -1 and 1. For each country-topic pair, the mean of all normalized responses was calculated."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Pre-Processing", "content": "Version 5 of the WVS data was preprocessed by first removing all the columns except those that corresponded to the moral questions Q177 to Q195 and the country code (B_COUNTRY). These questions cover a variety of moral issues, including tax cheating, accepting bribes, and attitudes toward homosexuality. After this initial filtering, each row was given a country name based on the B_COUNTRY codes using a predefined country mapping dataset. Responses that had values of -1, -2, -4, and -5 which represent 'Don't know', 'No answer', 'Not asked in survey', and 'Missing; Not available', respectively were replaced with zero. This adjustment was made to guarantee that calculations, like averaging, were not impacted by non-responses. Following that, the dataset was aggregated by country to determine the average response for every moral question for every country. This gave for every country a unique average score for each each ethical issue. The average scores were then normalized on a scale from 1 to -1, where 1 indicates that the behavior is always justified and -1 indicates that it is never justified, to allow comparisons across various countries and questions. In order to fit the new scale, the mean responses, which at first varied from 1 to 10, had to be adjusted during this normalization process. For cross-national comparisons, this step was required. Also, to improve clarity, normalized values were rounded to four decimal places.\nThe first step in preprocessing the Pew Research Global Attitudes Project data from Spring 2013 was to filter the dataset so that only the columns relevant to the country identifier (COUNTRY) and the moral questions Q84A to Q84H were kept. Survey questions that investigated society's perceptions in a range of moral issues, from gambling to contraception use were included. Following that, the available responses like 'Morally acceptable' and 'Morally unacceptable' were assigned numerical values. More specifically, 'Morally acceptable' was assigned a code of 1, 'Not a moral issue' was assigned a code of 0, and non-responses like 'Depends on situation (Volunteered)', 'Refused', and 'Don't know' were assigned a code of -1. This numeric transformation was needed to perform quantitative analysis and calculate the mean moral values.\nIn the following steps, the dataset was grouped by country, and the average score for each moral question was calculated for each country. These mean values represent the dominant opinion in each country regarding each issue. The column names were replaced with the titles of the moral issues they represent, such as 'using contraceptives' and 'getting a divorce'. Finally, the processed data was rounded to four decimal places as previously."}, {"title": "4.2 Method for Moral Score Calculation", "content": "For all models, the following two types of prompts are utilized:\n\u2022 In {country} {topic} is {moral_judgment}.\n\u2022 People in {country} believe {topic} is {moral_judgment}.\nHere, the moral_judgment is derived from pairs of opposing moral judgments, such as (always justifiable, never justifiable), (morally good, morally bad), (right, wrong), (ethical, unethical), and (ethically right, ethically wrong). Examples of these prompts include 'In China, getting a divorce is always justifiable' and 'People in Germany believe abortion is ethically wrong.'\nBy inputting these prompts into each model, we measure the model's perception of the morality of the described actions. More specifically, for each moral judgment (e.g., ethical, unjustifiable), the logit corresponding to the word appearing after the prompt is calculated and then converted into a log probability.\nThe way transformer-based auto-regressive models generate text is relatively simple as a concept and helps with understanding how the above-mentioned probabilities are calculated. The text given to the model, for example, a sentence, is separated into smaller units named tokens, which are usually words or parts of words. Then, each token is converted into a vector, an abstract numerical representation that captures the token's meaning. This is part of the embedding layer. In the next layers, a mechanism called the self-attention mechanism allows the model to focus on different parts of the input text, giving more weight to the relevant tokens. Then, the feed-forward neural network processes this information further. After passing through these layers, the model generates a set of raw scores called logits. Each logit corresponds to a token in the vocabulary.\nFollowing this, the logits are passed through a softmax function, which converts these raw scores into probabilities. The softmax function ensures that the probabilities of all possible tokens sum up to 1. The resulting probabilities indicate how likely each token is to be the next token in the sequence. The model picks the token with the highest probability as its prediction for the next token. The log probabilities are then calculated by taking the logarithm of these resulting probabilities.\nTo measure the model's bias, two types of log probabilities are calculated:\n\u2022 moral_logprob: The log probability associated with responses to the morally charged token.\n\u2022 nonmoral_logprob: The log probability associated with responses to the non-morally charged token.\nFinally, the above log probabilities are used to calculate the 'moral_score', a final value that reflects the model's overall stance on the topic. For example, given the input prompt 'In India, homosexuality is', the model will assign probabilities to all 10 morally charged tokens like 'ethical' and 'unethical'. The probability for the former token is the so-called moral_logprob and for the latter the nonmoral_logprob. Then, the score from the language model is determined as follows:\n$language\\_model\\_score$\n$moral\\_logprob \u2013 nonmoral\\_logprob$\nThis score represents the difference in log probabilities between pairs of moral and non-moral tokens. Finally, these differences are averaged across all pairs to compute a 'moral score,' which quantifies the model's bias towards moral topics."}, {"title": "4.3 Pre-trained LLMS", "content": "This study uses four NLP models to explore how moral values differ across cultures based on responses to a series of statements. While these models are all autoregressive and transformer-based, they have different implementations, training datasets, and design objectives. By using this diverse set of models, we aim for a comprehensive analysis and comparison of how different models perceive and generate responses related to moral norms. Despite their differences, they all produce probabilities for tokens and are well-suited for text generation, giving us a common basis for comparison.\nAdditionally, all the models used in this research come from Hugging Face 1, a well-known provider of cutting-edge NLP models. Hugging Face models are recognized for their robust performance and reliability, making them a suitable choice for our analysis of moral values across different cultural contexts. Importantly, none of the models were trained or fine-tuned for this study, as our goal is to understand the inherent perspectives these models hold regarding moral topics without the influence of training on similar datasets."}, {"title": "4.3.1 Monolingual Models", "content": "The first part of the study involves employing two monolingual models. The first one is the GPT-2 language model, which is primarily trained in English text. GPT-2 was chosen for its strong performance in generating coherent and contextually relevant text, as demonstrated in various studies as well as because it is computationally less expensive than the newest versions, making it more accessible. It has been fine-tuned to accurately predict the probability of a word based on its context within a sentence. Its architecture and training process enable it to generate human-like text, making it a suitable choice for tasks involving nuanced language understanding (Radford et al., 2019).\nIn particular, three versions of GPT-2 were utilized to assess the influence of model size on moral understanding. These include: 'gpt2' with 124 million parameters, 'gpt2-medium' with 355 million parameters, and 'gpt2-large' with 774 million parameters. The selection of multiple versions allowed for a comparative analysis of how increasing the number of parameters and computational complexity might increase the model's ability to process and interpret morally charged content. Larger models generally have a higher capacity for learning and can potentially gain a deeper understanding of complex concepts. This approach provides insights into whether increased computational resources reflect biases more accurately.\nThe OPT model (Zhang et al., 2022), part of the Open Pre-trained Transformer (OPT) series developed by Meta AI, is the second model included in this study. This series features open-sourced, large causal language models that perform comparably to GPT-3, with configurations varying in the number of parameters. Two such variants, the OPT-125M and the OPT-350M, are used in this analysis. OPT is a transformer-based language model designed to generate human-like text by predicting the next word in a sequence based on the provided context. Primarily trained in English text, OPT has been exposed to diverse datasets, enabling it to effectively handle a wide range of text generation tasks. This model was selected for its balance between computational efficiency and performance, providing a benchmark for comparing smaller, resource-efficient models against larger, more complex models."}, {"title": "4.3.2 Multilingual Models", "content": "The second part of the study involves employing multilingual models. Using multilingual models allows for an analysis of how these models, trained on diverse and extensive datasets, influence moral judgments across different countries compared to monolingual models.\nThe first multilingual model used is the Big-Science Large Open-science Open-access Multi-lingual Language Model, commonly known as BLOOM (Le Scao et al., 2023). BLOOM is a transformer-based, auto-regressive language model designed to support a wide range of languages and was developed as part of the BigScience project. It has been trained transparently on diverse datasets encompassing 46 natural and 13 programming languages, making it highly versatile and capable of generating text across various languages and contexts. BLOOM was chosen for its strong multilingual capabilities, its free open-access nature, and its ability to be instructed to perform text tasks it hasn't been explicitly trained for by casting them as text generation tasks.\nA variant of BLOOM, known as BLOOMZ-560M, which also has 560 million parameters and is provided by BigScience (bigscience/bloomz-560m), was chosen since it is fine-tuned for enhanced performance on zero-shot learning tasks, making it better at generalizing to new tasks without extensive training. Also, it has demonstrated robust cross-lingual generalization, effectively handling unseen tasks and languages. Although the original BLOOM model has 176 billion parameters, it was excluded from this study due to its substantial computational demands.\nThe Qwen2 model (Bai et al., 2023), developed by the Alibaba Cloud team, was also included in this study. Qwen2 is another multilingual transformer-based language model trained on data in 29 languages, including English and Chinese. Compared to other state-of-the-art open-source language models, including the previously released Qwen1.5, Qwen2 has generally surpassed most open-source models and demonstrated competitiveness against proprietary models across various benchmarks targeting language understanding, multilingual capability, coding, reasoning, and more. It is available in four parameter sizes: 0.5B, 1.5B, 7B, and 72B. The 0.5B version was chosen for this study due to computational considerations."}, {"title": "5 Results", "content": "To compare the values from each model, the first step involved normalizing the averaged logarithmic probabilities. Two normalization approaches were utilized: scaling to the range [-1, 1] as well as Z-score normalization. Despite the different methodologies, the results produced were very similar. Following normalization, the Pearson correlation coefficient (R-value) was calculated to assess the linear relationship between the model-generated moral scores and the survey-based moral scores."}, {"title": "5.1 Monolingual Models Results", "content": "The performance of the monolingual models did not meet expectations. For all three variants of the GPT-2 model-GPT2 base, GPT2-Medium, and GPT2-Large-the correlations are negligible and occasionally negative, despite most results being statistically significant. For the WVS dataset using the 'in' prompt type, the R-values are 0.04, -0.07*, and -0.03, respectively. Using the 'people' prompt type, they shift to -0.14***, 0.004, and -0.23***. Similarly, for the PEW dataset and the 'in' prompt type, the R-values are -0.4***, 0.12*, and -0.23***, respectively; for the 'people' prompt type, they are -0.29***, 0.01, and -0.25***.\nIn order to get a better and deeper understanding of the results, additional experiments were conducted using individual token pairs rather than averaging across the five pairs previously used. In the following tables, the outcomes of these experiments are displayed for each model, clearly showing the responses for each of the two types of prompts, applied to all five token pairs. The token pairs used are listed below:\n\u2022 pair1 = (always justifiable, never justifiable)\n\u2022 pair2 = (right, wrong)\n\u2022 pair3 = (morally good, morally bad)\n\u2022 pair4 = (ethically right, ethically wrong)\n\u2022 pair5 = (ethical, unethical)\nThe third column, labeled 'Mode', refers to the type of prompt: 'in', which corresponds to the format 'In {country} {topic} is {moral_judgment}', and 'people', which corresponds to 'People in {country} believe {topic} is {moral_judgment}'. The second-to-last column displays the R-values for these configurations, while the last column indicates the significance levels: \u201c*\u201d, \u201c**\u201d, and \u201c***\u201d for p-values < 0.05, 0.01, and 0.001, respectively."}, {"title": "5.2 Multilingual Models Results", "content": "The performance of the multilingual models is comparable to that of the monolingual models. Specifically, the Qwen2 model from Alibaba Cloud produced negative results. In the WVS dataset, the 'in' and 'people' prompt types gave R-values of 0.02 and -0.26***, respectively. In a similar manner, the PEW dataset results for these prompt types were -0.09 and -0.23***, correspondingly.\nThe results from the Qwen2-0.5B model, as detailed in Tables 7 and 8, are less favorable than those obtained with the OPT model, presenting weaker correlations between the model outputs and the survey scores. These results predominantly show statistically significant and largely negative correlations across the different token pairs, particularly within the WVS dataset. There appears to be a consistent pattern where the choice of moral token generally has a more substantial impact on the score than the prompt mode used. Notably, the highest moral scores recorded are 0.14 in the WVS dataset and 0.30 in the PEW dataset, both achieving a 99.9% significance level.\nThe BLOOMZ-560M model has produced the best results so far in terms of alignment between the model outputs and the survey scores. Using the WVS questions as prompts, the average moral scores are 0.25*** and 0.29***, both significant and the highest recorded thus far across the averaged token pairs scores. Similarly, when using the topics and countries from PEW, the scores are 0.16** and 0.11* for the two prompt types.\nAs illustrated in Table 9, the results for the WVS dataset showcase a prevalence of significant, strong positive correlations, surpassing the performance of previous models. Three token pairs achieve these notable results for both prompts, with the highest recorded at 0.36***. Although negative correlations are present, they are comparatively less pronounced.\nSimilarly, for the PEW dataset, the results are also encouraging as shown in Table 10. Significant positive correlations prevail, though they are not as high as those observed for the WVS dataset. The highest positive correlation recorded is 0.28***, while the negative correlations, that are present in the dataset, lack statistical significance."}, {"title": "5.3 Distribution of moral scores per topic", "content": "As depicted in Figure 2 in section 3, the spread of responses varies significantly across different moral topics. Topics such as 'for a man to beat his wife', 'stealing property', and 'violence against other people' show limited variation across countries and are mainly positioned on the left side, indicating negative moral scores. In contrast, topics like 'homosexuality', 'sex before marriage', and 'having casual sex' exhibit a wide range of responses, spanning both negative and positive moral scores."}]}