{"title": "Diffusion Models and Representation Learning: A Survey", "authors": ["Michael Fuest", "Pingchuan Ma", "Ming Gui", "Johannes S. Fischer", "Vincent Tao Hu", "Bj\u00f6rn Ommer"], "abstract": "Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion Models [68, 151, 154] have recently emerged as the state-of-the-art of generative modeling, demonstrating remarkable results in image synthesis [43, 67, 68, 141] and across other modalities including natural language [9, 70, 77, 101], computational chemistry [6, 71] and audio synthesis [80, 92, 109]. The remarkable generative capabilities of Diffusion Models suggest that Diffusion Models learn both low and high-level features of their input data, potentially making them well-suited for general representation learning. Unlike other generative models like Generative Adversarial Networks (GANs) [22, 53, 84] and Variational Autoencoders (VAEs) [88, 137], diffusion models do not contain fixed architectural components that capture data representations [124]. This makes diffusion model-based representation learning challenging. Nevertheless, approaches leveraging diffusion models for representation learning have seen increasing interest, simultaneously driven by advancements in training and sampling of Diffusion Models.\nCurrent state-of-the-art self-supervised representation learning approaches [8, 24, 33, 55] have demonstrated great scalability. It is thus likely that diffusion models exhibit similar scaling properties [159]. Controlled generation approaches like Classifier Guidance [43] and Classifier-free Guidance [67] used to obtain state-of-the-art generation results rely on annotated data, which represents a bottleneck for scaling up diffusion models. Guidance approaches that leverage representation learning and that are thus annotation-free offer a solution, potentially enabling diffusion models to train on much larger, annotation-free datasets.\nThis survey paper aims to elucidate the relationship and interplay between diffusion models and representation learning. We highlight two central perspectives: Using diffusion models themselves for representation learning and using representation learning for improving diffusion models. We introduce a taxonomy of current approaches and derive generalized frameworks that demonstrate commonalities among current approaches.\nInterest in exploring the representation learning capabilities of diffusion models has been growing since the original formulation of diffusion models by Ho et al. [68], Sohl-Dickstein et al. [151], Song et al. [154]. As demonstrated"}, {"title": "2 BACKGROUND", "content": "The following section outlines the required mathematical foundations of diffusion models. We also highlight current architecture backbones of diffusion models and provide a brief overview of sampling methods and conditional generation approaches."}, {"title": "2.1 Mathematical Foundations", "content": "Consider a set of training examples drawn from an underlying probability distribution p(x). The idea behind generative diffusion models is to learn a denoising process that maps samples of random noise to novel images sampled from p(x) [133]. To achieve this, images are corrupted by gradually adding different levels of Gaussian noise. Given an uncorrupted training sample x0 ~ p(x), where index 0 denotes the fact that the sample is not corrupted, the corrupted samples x1, x2..., xT are generated according to a Markovian process. One common choice for the transition kernel p(xt|xt\u22121) is the following:\np(xt|xt\u22121) = N(xt; \u221a1 \u2013 \u03b2t xt-1, \u03b2tI), \u2200t \u2208 {1, ..., T}, (1)\nwhere T denotes the number of diffusion timesteps, \u03b2t is a time-dependent variance schedule and I is an identity matrix with dimensionality equal to x0 [37]. Note that other parametrizations of the transition kernel p(xt|xt\u22121) are also applicable in the same manner [87, 188]. We proceed with the parametrization used in DDPMs [68] to simplify the discussion moving forward. A noisy image xt can be sampled directly from x0 with the help of a reparametrization trick [151] as follows:\np(xt|x0) = N(xt; \u221a\u03b1tx0; (1 \u2013 \u03b1t)1), (2)\nwhere \u03b1t := 1 \u2014 \u03b2t and \u0101t := \u03a0ti=1\u03b1i. Given the original input image x0, we can now obtain xt in one step by sampling Gaussian vector \u03b5t ~ N(0, I) and applying:\nxt = \u221a\u03b1tx0 + \u221a(1 \u2212 \u0101t)\u03b5t. (3)\nWe can generate novel samples from p(x0) starting from a pure noise image xT ~ \u03c0(xT) = N(0, 1) with dimensionality equivalent to the data and sequentially denoise it such that at every step, p\u03b8(xt-1|xt) = N (xt-1; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)). In practice, this requires training a neural network p\u03b8(xt-1|xt) that predicts the mean \u00b5\u03b8(x, t) and the covariance \u03a3\u03b8(xt, t) given a diffusion timestep t and the noisy input image xt [172]. Training this neural network with a maximum likelihood objective is intractable [37], so the objective is amended to minimize a Variational Lower-Bound of the Negative Log-Likelihood instead [68, 151]:\nLulb = \u2211log p\u03b8 (x0|x1) + DKL (p(xT|x0)||\u03c0(xT))\n+ \u2211DKL (p(xt-1|xt, x0)||p\u03b8(xt-1|xt)), (4)\nt>1\nwhere DKL is the Kullback-Leibler divergence. This objective ensures that the neural network is trained to minimize the distance between p\u03b8(xt-1|xt) and the true posterior of the forward process when conditioned on x0. The denoising network is generally applied to parametrize the reverse mean \u00b5\u03b8(x, t) of the distribution of the reverse transition p\u03b8(xt-1|xt) := N(xt-1; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)) [27]. The true value of the reverse mean is a function of x0, which is unknown in the reverse process and must therefore be estimated using input timestep t and the noisy data xt. Specifically, the reverse mean is formulated as the following:\n\u00b5(x, t) :=\n\u221a\u03b1t-1(1-\u03b1t-1)xt + \u221a\u03b1t-1(1 - \u03b1t)x0\n1- \u0101t\n, (5)\nwhere the original data x0 is unavailable in the reverse process and must therefore be estimated. We denote the denoising network's prediction of the original data as x\u03020. This prediction x\u03020 can then be used to obtain \u00b5\u03b8 (xt, t) using Equation 5. Parametrizing with x\u03020 directly is beneficial at the beginning of sampling, since predicting x\u03020 directly helps the denoising network to learn higher-level structural features [115]."}, {"title": "2.2 Backbone Architectures", "content": "We outline the mathematical foundations of diffusion models in Section 2.1. Since denoising prediction networks are generally parametrized by parameters \u03b8, we discuss the formulation of \u03b8 by several neural network architectures in the following section. All of these network architectures map from the same input space to the same output space.\nHo et al. [68] use a U-Net backbone similar to an unmasked PixelCNN++ [144] to approximate the score function. This U-Net architecture, originally used in semantic segmentation approaches [30, 31, 113, 140], is based on a Wide ResNet [182] and takes a noisy image and the diffusion timestep t as input, encodes the image to a lower-dimensional representation, and outputs the noise prediction for that image and noise level. The U-Net consists of an encoder and a decoder with residual connections between blocks that preserve gradient flow and help recover fine-grained details lost in the compressed representation. The encoder consists of a series of residual and self-attention blocks and downsamples the input image to a low-dimensional representation. The decoder mirrors this structure, gradually upsampling the low-dimensional representation to match the input dimensionality. The diffusion timestep t is specified by adding a sinusoidal positional embedding in each residual block [68] that scales and shifts the input features, enhancing the network's ability to capture temporal dependencies.\nDDPMs operate in the pixel space, making their training and inference computationally expensive. Rombach et al. [138] address this by proposing Latent Diffusion Models (LDMs), which operate in the latent space of a pre-trained variational autoencoder. The diffusion process is applied to the generated representation as opposed to the image directly, leading to computational benefits without sacrificing generation quality. While the authors introduce additional cross-attention mechanisms to allow for more flexible conditioned generation, the denoising network backbone remains very close to the DDPM U-Net architecture.\nRecent advances in the use of transformer architectures for vision tasks like ViT [45] have led to the adoption of transformer-based architectures for diffusion models. Peebles and Xie [132] propose Diffusion Transformers (DiT), a diffusion model backbone architecture that is largely inspired by ViTs, and demonstrates state-of-the-art generation performance on ImageNet when combined with the LDM framework. Following ViT, DiTs work by transforming input images into a sequence of patches, which are converted"}, {"title": "2.3 Diffusion Model Guidance", "content": "Recent improvements in image generation results have largely been driven by improved guidance approaches. The ability to control generation by passing user-defined conditions is an important property of generative models, and guidance describes the modulation of the strength of the conditioning signal within the model. Conditioning signals can have a wide range of modalities, ranging from class labels, to text embeddings to other images. A simple method to pass spatial conditioning signals to diffusion models is to simply concatenate the conditioning signal with the denoising targets and then pass the signal through the denoising network [12, 75]. Another effective approach uses cross-attention mechanisms, where a conditioning signal c is preprocessed by an encoder to an intermediate projection E(c), and then injected into the intermediate layer of the denoising network using cross-attention [76, 142]. These conditioning approaches alone do not leave the possibility"}, {"title": "3 METHODS", "content": "Having covered the main preliminaries for diffusion models, we outline a series of methods related to diffusion models and representation learning in the following section. In subsection 3.1 we describe and categorize current frameworks utilizing representations learned by pre-trained diffusion models for downstream recognition tasks. In subsection 3.2, we describe methods that leverage advances in representation learning to improve diffusion models themselves."}, {"title": "3.1 Diffusion Models for Representation Learning", "content": "Learning useful representations is one of the main motivations for designing architectures like VAEs [88, 89] and GANs [22, 84]. Contrastive learning approaches, where the goal is to learn a feature space in which representations of similar images are very close together, and vice versa for dissimilar images (e.g. SimCLR [34], MoCo [60]), have also led to significant advances in representation learning. These contrastive methods are not fully self-supervised however, since they require supervision in the form of augmentations that preserve the original content of the image.\nDiffusion models offer a promising alternative to these approaches. While diffusion models are primarily designed for generation tasks, the denoising process encourages the learning of semantic image representations [15], that can be used for downstream recognition tasks. The diffusion model learning process is similar to the learning process of Denoising Autoencoders (DAE) [18, 162], which are trained to reconstruct images corrupted by adding noise. The main difference is that diffusion models additionally take the diffusion timestep t as input, and can thus be viewed as multi-level DAEs with different noise scales [169]. Since DAEs learn meaningful representations in the compressed latent space, it is intuitive that diffusion models exhibit similar representation learning capabilities. We outline and discuss current approaches in the following section."}, {"title": "3.1.1 Leveraging intermediate activations", "content": "Baranchuk et al. [15] investigate the intermediate activations from the U-Net network that approximates the Markov step of the reverse diffusion process in DDPMs [42]. They show that for certain diffusion timesteps, these intermediate activations capture semantic information that can be used for downstream semantic segmentation. The authors take a noise-predictor network \u03f5\u03b8(xt, t) trained on the LSUN- Horse [177] and FFHQ-256 [84] datasets and extract feature maps produced by one of the network's 18 decoder blocks for label-efficient downstream segmentation tasks. Selecting the ideal diffusion timestep and decoder block activation to extract is non-trivial. To understand the efficacy of pixel-level representations of different decoder blocks, the authors train a multi-layer perceptron (MLP) to predict the semantic"}, {"title": "3.1.2 A general representation extraction framework", "content": "Many of the methods outlined in the previous section follow a similar procedure in leveraging learned representations of pre-trained diffusion models for downstream vision tasks. In this section, we aim to consolidate these approaches to a common three-step framework. We do this to provide clarity on the relationship between diffusion models and their use for downstream predictive tasks. To leverage intermediate activations for downstream tasks, a selection methodology that outputs the ideal diffusion timestep input as well as the intermediate layer number(s) whose activation maps have the highest predictive performance when upsampled and linearly probed must be applied. This can be a trainable model [116], a grid search procedure [169] or a learning agent [173]. The goal of this methodology is generally to select timestep t \u2208 T and a set of decoder block numbers B that maximize predictive performance on a downstream task. Given a set of possible timesteps T and a set of decoder blocks B, the goal is to find:\n(t\u2217, B\u2217) = arg min Ldiscr(t, B)\nt\u2208T, BCB (16)\nwhere Ldiscr(t, B) represents the discriminative loss at timestep t when the blocks in B are used for downstream prediction. Generally, discriminative tasks will require more"}, {"title": "3.1.3 Knowledge transfer", "content": "Aside from leveraging intermediate activations from pre-trained diffusion models directly as inputs to a recognition network, several recent approaches propose a more indirect method of reusing learned representations for downstream tasks. We summarize these under the term knowledge transfer methods. This reflects the common idea of distilling representations from pre-trained diffusion models and then transferring them to auxiliary networks in a way that is distinct from simply providing aggregated feature activation maps as input. Several of these approaches are discussed in the following section.\nYang and Wang [173] propose RepFusion, a knowledge distillation approach that dynamically extracts intermediate representations at different time steps using a reinforcement learning framework, and uses the extracted representations as auxiliary supervision for student networks. Given an input x with label y, the authors extract a pair of features, one from the diffusion probabilistic model (DPM) and one from the student model, where z(t) is the diffusion model representation and z is the student model representation. The distance between the two is minimized during training using a loss function Lkd. After the distillation, the student network is reapplied as a feature extractor and fine-tuned on the available task labels. Previous approaches for using diffusion model representations rely on grid-search to determine which diffusion timestep to use for feature extraction. Here, the authors formulate a reinforcement learning environment where the action space is the set of all possible timesteps t available for selection, and the reward function is the negative task loss \u2212Ltask(y, g(z(t); \u03b8g)). Given the input x, a policy network \u03c0\u03b8\u03c0(t|x) is trained to determine which timestep t to use for representation extraction. Once the timestep is selected, the authors use the feature representations in the mid-block of the DPM for the selected timestep t* to obtain z(t*). After the distillation phase, the student network is used as a feature extractor and subsequently fine-tuned on the task label y.\nLi et al. [96] introduce DreamTeacher, a knowledge distillation method using a feature regressor module that distills the learned representations of a generative model G into a target image recognition backbone f. Given a feature dataset D = {xi, f}Ni=1 consisting of images x and extracted features f, f is trained by distilling f into the intermediate features of f(xi). The features are extracted from G by running a forward diffusion process for T timesteps and conducting a single denoising step to extract f from the intermediate layers of the U-Net backbone. The extracted features are distilled using a feature regressor module with a top-down architecture containing lateral skip connections that aligns the image backbone features with the generative features. Intermediate CNN encoder features ff at layers l and regressor outputs f\u02c6l are used to compute an MSE feature regression loss inspired by FitNet [139]:\nLMSE = \u22111 \u2225W (f l)\nN 1=1 L \u2212 f\u02c6 \u2225 (17)\nwhere W is a non-learnable operator implemented as LayerNorm [11]. This loss is combined with the activation-based Attention Transfer (AT) objective [181], which distills a one dimensional \"attention map\" for each spatial feature. DreamTeacher is evaluated on a range of downstream recognition tasks by fine-tuning the pre-trained backbone with additional classification heads for each task. DreamTeacher"}, {"title": "3.1.4 Reconstructing diffusion models", "content": "Previous diffusion representation learning techniques do not propose making fundamental modifications to diffusion model architectures and training methodologies. While these techniques often show encouraging performance for downstream tasks, they fail to generate deep insights into the architectural components and techniques required to learn useful representations. It remains largely unclear for example whether the representation learning abilities of diffusion models are driven by the diffusion process, or by the model's denoising capabilities. It is also unclear what architectural and optimization choices can improve diffusion models' representation learning capabilities.\nChen et al. [35] investigate these questions by deconstructing a denoising diffusion model (DDM), modifying individual model components to turn a DDM into a Denoising Autoencoder. The deconstruction process consists of three stages. In the first stage, the DDM is reoriented for self-supervised learning. This entails the removal of class conditioning and a reconstruction of the VQGAN tokenizer [47] used in the DiT baseline. Both the perceptual and adversarial loss terms rely on annotated data and are thus removed. This essentially converts the VQGAN to a VAE. The second stage consists of simplifying the VAE tokenizer even further, replacing it with different autoencoder variants. Surprisingly, the authors find that using simpler autoencoder variants, like patch-wise PCA, does not degrade performance substantially. The authors conclude that the dimensionality per token of the latent space has a much larger impact on probing accuracy than the chosen autoencoder. The final deconstruction step includes converting the DDM to predict the denoised input instead of the added noise and removing input scaling, as well as changing the diffusion model to operate directly in the pixel space. This final stage results in what the authors call the latent Denoising Autoencoder (l-DAE). They conclude that representation learning abilities are largely driven by the denoising-driven process rather than the diffusion process.\nl-DAE is inspired by the observation that diffusion models resemble hierarchical autoencoders with varying"}, {"title": "3.1.5 Joint diffusion models", "content": "Many current diffusion-based representation learning methods focus on using the diffusion model's latent variables to benefit the training of a separate recognition network. These frameworks are conceptually equivalent to constructing hybrid models that solely concentrate on synthesis in the pre-training stage, and on downstream recognition in the post-training/fine-tuning phase. The recognition head and the diffusion denoising network do not share a parametrization, and the recognition head is often trained separately while keeping the weights of the denoising network frozen. A natural question that arises is whether this separation is necessary and whether approaches that optimize a generative and a discriminative objective simultaneously in a shared parametrization can improve representation learning.\nHybViT [174] is an approach that establishes a direct connection between diffusion models and vision transformers by training a single hybrid model for both image classification and image generation. This hybrid model uses a shared parametrization for image classification and reconstruction. The authors use a ViT backbone to train a model with a combined loss L consisting of a standard cross-entropy loss to train p(y|x) and the simple denoising loss to train p(x). HybViT provides stable training and outperforms previous hybrid models on both generative and discriminative tasks, but lags behind generative-only models in generation quality. HybViT also requires more training iterations to achieve high classification performance, and the sampling speed during inference is slow.\nJoint Diffusion Models (JDM) [40] is a related work that produces meaningful representations across generative and discriminative tasks. Using a U-Net backbone, JDM consists of an encoder ev, a decoder dy, and a classifier gw. The encoder maps an input xt to feature vectors Zt = ev(xt). The decoder reconstructs these into a denoised sample xt\u22121 = dy(Zt), and the classifier predicts the target class y\u02c6 = gw(Zt). The combined training objective includes cross-entropy loss Lclass and the noise prediction network's simplified objective Lt,diff(\u03bd, \u03c8), resulting in the following loss:\nL(\u03bd, \u03c8,w) = Lclass (\u03bd, w)\u2212L\u03c3(\u03bd, \u03c8)\u2212\u2211 TLt,diff(\u03bd, \u03c8)\u2212LT(\u03bd, \u03c8). (1)\nt=2\nJDM also enables a simplification of classifier guidance. By applying the classifier to noisy images xt, the classifier is effectively augmented to be robust to noise. To guide the generated sample towards a target label, representations Zt are optimized according to the classifier gradient, giving Z\u2217t = Zt \u2212 \u03b1Z\u2207Zt log gw(y|Zt). JDM achieves state-of-the-art performance for joint models on CIFAR and CelebA datasets, outperforming HybViT.\nTian et al. [158] propose the Alternating Denoising Diffusion Process (ADDP). ADDP alternately denoises pixels and VQ tokens. Given an image x0, a pre-trained VQ Encoder [26] maps time image to VQ tokens z0. The alternating diffusion process masks regions of z0 with a Markov chain according to diffusion timestep t, producing zt. Unreliable tokens a\u02c6t are generated by a token predictor and fed into a VQ Decoder to synthesize xt, replacing the masked regions of z0. A pixel-to-token generation network is then trained to approximate the distribution of a\u02c6t\u22121. During sampling, ADDP starts with a representation of pure unreliable tokens zT and iteratively denoises the token sequence by predicting zt\u22121. For recognition, the representations learned by the pixel-to-token generation network can be forwarded to different task-specific recognition heads. ADDP with the VQ-GAN tokenizer [47] MAGE-Large [99] token predictor and ViT-Large [45] pixel-to-token encoder, outperforms previous unified models in image classification, object detection, semantic segmentation, and unconditional generation."}, {"title": "3.1.6 Generative augmentation", "content": "A lot of state-of-the-art representation learning methods [33, 55, 60] rely on a fixed set of data augmentations to define positive labels for learning representations. This approach encourages encoders to learn to map the original and the augmented image to similar embedding space representations [10]. These augmentations should not alter the semantics of the image, and they should not render the image unrealistic in a real-world setting. A set of standard transformations might not adequately capture the distribution of real-world data, raising the question of how to design"}, {"title": "3.2 Representation Learning for Diffusion Model Guidance", "content": "Despite the remarkable performance of generative models, there exists a gap in quality between conditional and unconditional image generation approaches [25]. This is especially the case for GANs [53], which suffer from mode collapse when trained in a fully unsupervised setting [110]. Unconditional GANs often fail to accurately model multimodal distributions, e.g. not being able to generate all digits for MNIST [110]. Class-conditional GANs [22] [123] mitigate this issue, but require labeled data. Recent approaches like self-conditioned GANs [110] and instance-conditioned GANs [25] attempt to train conditional GANs without requiring labeled data, and are able to achieve competitive generation results.\nDiffusion models have since surpassed the image generation capabilities of GANs [42], but suffer from a similar performance discrepancy between conditional and fully self-supervised approaches. Current state-of-the-art diffusion models are conditional models that rely on guidance approaches that also require annotated data. Self-supervised guidance approaches can leverage much larger unlabeled datasets for pre-training, and thus have the potential to transcend current image generation approaches. One intuitive approach for leveraging representation learning to facilitate these guidance methods is to explore methods that assign labels to unlabeled data, e.g. through clustering and classification approaches. We introduce several approaches in the following section. Fig. 5 shows a proposed taxonomy of representation learning techniques for diffusion guidance."}, {"title": "3.2.1 Assignment-based guidance", "content": "Sheynin et al. [149] propose kNN-Diffusion, an efficient text-to-image diffusion model trained without large-scale image text pairings. To facilitate text-guided image generation without paired text-image data, a shared text-image encoder mapping text-image pairs into the same latent space is required. The authors use CLIP to achieve this, a pre-trained encoder trained using contrastive loss on a large-scale text-image pair dataset. kNN-Diffusion leverages k-Nearest-Neighbors search to generate k embeddings from a retrieval model. The retrieval model uses the input image representation during training, and the text prompt representation curing inference. This approach eliminates the need for annotated data but still requires a pre-trained encoder like CLIP, which in turn requires a large-scale dataset of text-image embeddings for pre-training.\nBlattmann et al. [20] propose retrieval-augmented diffusion models (RDM), which equip diffusion models with an image database for composing new scenes based on retrieved images. Inspired by advances in retrieval-augmented NLP [21, 168], RDM enhances performance with"}, {"title": "3.2.2 A generalized framework for assignment-based guidance", "content": "Assignment-based guidance approaches all rely on assigning annotation to inputs during training, which enables controlled generation during inference when conditioning on this annotation. We therefore propose to formulate a generalized framework that encapsulates all assignment-based guidance approaches discussed here. This framework consists of three main components. The first is a self-supervised image encoder E(x), that maps inputs to a low-dimensional feature representation z. Using a multi-modal feature extractor like CLIP has the advantage of enabling text-based as well as image-based conditioning, but other feature extractors can be used, provided they generate semantically meaningful image representations.\nThe second is a self-annotation function f(z), which uses the image representation to produce annotation c for input image x. In the simplest case, this self-annotation function is an external pre-trained image classifier that generates pseudo-class labels from image representations, similar to the approach employed in DPT [176], where the external classifier is subsequently re-trained on the conditionally generated images. In other cases, the self-annotation function is a retrieval model, which uses a distance function d to retrieve images similar to the training image, and uses representations of the retrieved images for generating the guidance signal c.\nThe final component is a denoising network D\u03b8(xt, c, t), which takes the noisy image xt, the diffusion timestep t and the guidance signal c as input, and denoises the image. During inference, controlled generation is enabled by passing an initial guidance signal k (which can be multi-modal as long as the embedding space of the encoder E is shared between modalities) through the encoder to generate representation z = E(k). The conditioning signal c is then generated by passing z to the self-annotation function f where c = f(z). Passing xt, c and t to the denoising network D\u03b8 now enables synthesis of novel images semantically similar to the initial guidance signal k.\nOne of the main motivations behind the design of assignment-based guidance methods is the reliance on existing methods on labeled data. While it could be argued that the aforementioned assignment-based guidance approaches are indirectly reliant on annotated data through the pre-trained image encoder, it is important to note that this encoder can be replaced with a fully self-supervised encoder as well. CLIP relies on the availability of a large-scale dataset of image-caption pairs and is thus not fully self-supervised, but other representation learning methods are also able to generate semantic representations. CLIP is used in many approaches to facilitate both text prompt-based and image conditioning during inference, which may no longer be possible when using primarily image-based feature extractors."}, {"title": "3.2.3 Representation-based guidance", "content": "Li et al. [100] present Representation-Conditioned Image Generation (RCG), a framework conditioning diffusion models on a self-supervised representation distribution mapped from the image distribution using a pre-trained encoder. The idea is to train a Representation Diffusion Model (RDM) on the representations generated by a pre-trained encoder to generate low-dimensional image representations.\nAfter this, a pixel generator conditioned on the representation is trained to map noise distributions to image distributions. RCG consists of three main components. The first is a pre-trained image encoder, which converts the original image distribution into a representation distribution. The authors propose using self-supervised contrastive learning methods (e.g. MoCo v3) for generating this representation distribution. The second is a representation generator in the form of an RDM, which learns to generate representations from Gaussian noise following the DDIM [152] sampling process. The final component is a pixel generator that crafts image pixels conditioned on image representations. RCG can easily incorporate classifier-free guidance for unconditional generation tasks, since the pixel generator is conditioned on self-supervised representations. RCG emerges as a highly promising method for bridging the gap between conditional and unconditional image generation, outperforming pre-existing unconditional generation approaches on ImageNet, and exhibiting competitive performance with current state-of-the-art class-conditional approaches.\nReadout Guidance (RG) [117] makes use of auxiliary readout heads trained on top of a frozen diffusion model to extract properties of the generated image that can be used for guidance. These properties can include human pose, depth maps, edges, and even higher-order properties like similarity to another image. During sampling, the properties extracted by the readout heads can be compared to user-defined control targets, and used in a methodology similar to classifier guidance [43] to guide generation.\nLin and Yang [105] identified a novel self-perceptual objective that enhances diffusion models, enabling them to generate more realistic samples. Contrary to the conventional approach of training or employing an image encoder, the authors demonstrate that a pre-trained diffusion model inherently functions as a perceptual network and can be used to generate perceptual representations. The perceptual loss facilitates the model's ability to generate more realistic images even with unconditional synthesis.\nAlso inspired by the downsides of classifier guidance and classifier-free guidance, Hong et al. [69] introduce Self-Attention Guidance (SAG). SAG adversarially blurs regions that contain salient information by leveraging intermediate self-attention activation maps, using the residual information as guidance. This increases the generation quality without requiring external information or additional training. The self-attention mechanism, contained in both U-Net and DiT diffusion backbones, allows the noise predictor to attend to the most informative features of the input. The self-attention maps A \u2208 RN\u00d7(HW)\u00d7(HW) are aggregated and reshaped to dimension RH\u00d7W using global average pooling and nearest-neighbor upsampling to match the resolution of xt. The difference between the blurred image x\u02c6t and xt is used as conditioning, thereby retaining the information masked in this process."}, {"title": "3.2.4 Objective-based guidance", "content": "Many of the previous outlined approaches focus on eliminating the need for pre-trained classifiers, encoders and dataset annotations for training conditional diffusion models. Other recent works [46, 86] have demonstrated that internal diffusion model representations can be used to improve generation control over the structural and semantic composition of generated images.\nOne such approach is Self-guidance for Controllable Image Generation [46] (which we denote SGCIG to distinguish it from [75]). SGCIG is a zero-shot method designed to increase user control over structural and semantic elements of objects in images generated by text-to-image diffusion models. Incorporating similar ideas as [65], the authors of SGCIG leverage representations from intermediate activations and attention maps to steer the generation process. SGCIG works by adding a series of guidance terms to the objective of the denoising network that each define a series of properties that can be used to perform image manipulations. Image edits can then be carried out by guiding properties to change in the pixel generation process. While the method is limited to the manipulation of objects explicitly stated in the conditioning text prompt, it represents a promising first step towards increased control over generated images. Diffusion Handles [131] extend this to 3D object editing, using manipulated diffusion model activations to produce plausible edits.\nDepth-aware guidance (DAG) [86] is a related method that uses semantic information from intermediate denoising network layers for improved depth-aware image synthesis. Kim et al. [86] propose training depth predictors with limited depth-labeled data using internal U-Net backbone representations, similar to DDPM-Seg [15]. The used depth predictors are pixel-wise shallow MLP regressors estimating depth values from intermediate U-Net features ft at timestep t. Features are concatenated across layers to form gt, with depth maps dt = MLP(gt,t) generated using an appended time-embedding block. This depth predictor is trained using a limited depth-labeled dataset. To now guide the diffusion process toward depth-aware generation, two guidance strategies are introduced: Depth consistency guidance uses pseudo-labels with a consistency loss Ldc between weak and strong depth predictors, guiding the generation process using the gradient of Ldc with respect to xt in a methodology similar to [42]. Depth prior guidance employs an additional small-resolution diffusion U-Net on the depth domain, adding noise to depth predictions and using a denoising objective Ldp. The gradient of Ldp is treated like an external classifier gradient and added to the image generation objective. Combining both methods during training results in enhanced depth semantics in generated images.\nPerturbed Attention Guidance (PAG) [3] is a sampling guidance method that improves generation quality for both conditional and unconditional settings. PAG does not require additional training or external pre-trained models. Instead, Ahn et al. [3] introduce an implicit discriminator D that differentiates between desirable and undesirable samples during the diffusion process, where y is a desirable and y\u02c6 is an undesirable sample. The diffusion sampling process is then redefined to incorporate the derivative of the discriminator loss LD. The score with undesirable label y\u02c6 cannot be approximated using the existing denoising network \u03f5\u03b8 (xt). Thus the score is estimated by perturbing the forward pass of a pre-trained denoising network, denoted by \u03f5\u02c6\u03b8. PAG works by perturbing the self-attention maps in the diffusion U-Net, replacing them with an identity matrix to guide the sampling process away from degraded samples. The final noise prediction is obtained by feeding xt into both \u03f5\u03b8(\u00b7) and \u03f5\u02c6\u03b8(\u00b7) to get the final noise prediction \u03f5\u03b8. PAG improves generation quality in both conditional and unconditional settings, and can be combined with existing guidance methods like classifier guidance."}, {"title": "4 CHALLENGES & FUTURE DIRECTIONS", "content": "Diffusion model-based representation learning is a novel research field with a lot of potential for[{\"title\":\"4.1 General Challenges\""}, {"content": "Diffusion model-based representation learning is a novel research field with a lot of potential for theoretical and prac-tical improvements. Improving synergies between represen-tation learning and generative models is akin to a chicken-and-egg problem, where better diffusion models simulta-neously lead to higher quality image representations, andbetter representation learning methods improve generativequality of diffusion models when applied to self-supervised"}]}