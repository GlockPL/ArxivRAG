{"title": "Diffusion Models and Representation Learning: A Survey", "authors": ["Michael Fuest", "Pingchuan Ma", "Ming Gui", "Johannes S. Fischer", "Vincent Tao Hu", "Bj\u00f6rn Ommer"], "abstract": "Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion Models [68, 151, 154] have recently emerged as the state-of-the-art of generative modeling, demonstrating remarkable results in image synthesis [43, 67, 68, 141] and across other modalities including natural language [9, 70, 77, 101], computational chemistry [6, 71] and audio synthesis [80, 92, 109]. The remarkable generative capabilities of Diffusion Models suggest that Diffusion Models learn both low and high-level features of their input data, potentially making them well-suited for general representation learning. Unlike other generative models like Generative Adversarial Networks (GANs) [22, 53, 84] and Variational Autoencoders (VAEs) [88, 137], diffusion models do not contain fixed architectural components that capture data representations [124]. This makes diffusion model-based representation learning challenging. Nevertheless, approaches leveraging diffusion models for representation learning have seen increasing interest, simultaneously driven by advancements in training and sampling of Diffusion Models.\nCurrent state-of-the-art self-supervised representation learning approaches [8, 24, 33, 55] have demonstrated great scalability. It is thus likely that diffusion models exhibit similar scaling properties [159]. Controlled generation approaches like Classifier Guidance [43] and Classifier-free Guidance [67] used to obtain state-of-the-art generation results rely on annotated data, which represents a bottleneck for scaling up diffusion models. Guidance approaches that leverage representation learning and that are thus annotation-free offer a solution, potentially enabling diffusion models to train on much larger, annotation-free datasets.\nThis survey paper aims to elucidate the relationship and interplay between diffusion models and representation learning. We highlight two central perspectives: Using diffusion models themselves for representation learning and using representation learning for improving diffusion models. We introduce a taxonomy of current approaches and derive generalized frameworks that demonstrate commonalities among current approaches.\nInterest in exploring the representation learning capabilities of diffusion models has been growing since the original formulation of diffusion models by Ho et al. [68], Sohl-Dickstein et al. [151], Song et al. [154]. As demonstrated in Fig. 1, we expect this trend to continue this year. The increased volume of published works on diffusion models and representation learning makes it more difficult for researchers to identify state-of-the-art approaches and stay on top of current developments. This can hinder progress in the space, which is why we feel a comprehensive overview and categorization is required.\nResearch on representation learning and diffusion models is in its infancy. Many of the current approaches rely on using diffusion models solely trained for generative synthesis for representation learning. We therefore hypothesize that there are significant opportunities for further progress in this area in the future and that diffusion models can increasingly challenge the current state-of-the-art in representation learning. Fig. 2 shows qualitative results from existing methods. We hope that this survey can contribute to advances in diffusion-based representation learning, by clarifying commonalities and differences among current approaches. In summary, the main contributions of this paper are the following:\n\u2022\tComprehensive Overview: Offers a thorough survey of the interplay between diffusion models and representation learning, providing clarity on how diffusion models can be used for representation learning and vice versa.\n\u2022\tTaxonomy of Approaches: We introduce a taxonomy of current approaches in diffusion-based representation learning, categorizing and highlighting commonalities and differences among them.\n\u2022\tGeneralized Frameworks: The paper derives generalized frameworks for both diffusion model feature extraction and assignment-based guidance, offering a structured view on a large number of works on diffusion models and representation learning.\n\u2022\tFuture Directions: We identify key opportunities for further progress in the field, encouraging the exploration of diffusion models and flow matching as a new state-of-the-art in representation learning."}, {"title": "2 BACKGROUND", "content": "The following section outlines the required mathematical foundations of diffusion models. We also highlight current architecture backbones of diffusion models and provide a brief overview of sampling methods and conditional generation approaches.", "\n Mathematical Foundations": "Consider a set of training examples drawn from an underlying probability distribution p(x). The idea behind generative diffusion models is to learn a denoising process that maps samples of random noise to novel images sampled from p(x) [133]. To achieve this, images are corrupted by gradually adding different levels of Gaussian noise. Given an uncorrupted training sample x_0 ~ p(x), where index 0 denotes the fact that the sample is not corrupted, the corrupted samples X1, X2..., XT are generated according to a Markovian process. One common choice for the transition kernel p(x_t|x_{t-1}) is the following:\np(x_t|x_{t-1}) = N(x_t; \u221a{1 \u2013 \u03b2_t}x_{t-1}, \u03b2_t I), \u2200t \u2208 {1, ..., T}, (1)\nwhere T denotes the number of diffusion timesteps, \u03b2t is a time-dependent variance schedule and I is an identity matrix with dimensionality equal to xo [37]. Note that other parametrizations of the transition kernel p(xt|xt-1) are also applicable in the same manner [87, 188]. We proceed with the parametrization used in DDPMs [68] to simplify the discussion moving forward. A noisy image x_t can be sampled directly from x_0 with the help of a reparametrization trick [151] as follows:\np(x_t|x_0) = N(x_t; \u221a{\u0101_t}x_0; (1 \u2013 \u0101t)1), (2)\nwhere \u03b1_t := 1 \u2014 \u03b2_t and \u0101t := \u03a0_{i=1}^t&i. Given the original input image x_0, we can now obtain x_t in one step by sampling Gaussian vector \u03f5t ~ N(0, I) and applying:\nx_t = \u221a{\u0101t}x_0 + \u221a(1 \u2212 \u0101t)\u03f5_t. (3)\nWe can generate novel samples from p(x_0) starting from a pure noise image x_T ~ \u03c0(x_T) = N(0, I) with dimensionality equivalent to the data and sequentially denoise it such that at every step, p\u03b8(xt-1|xt) = N(xt-1; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)). In practice, this requires training a neural network p\u03b8(xt-1|xt) that predicts the mean \u00b5\u03b8(x, t) and the covariance \u03a3\u03b8(xt, t) given a diffusion timestep t and the noisy input image xt [172]. Training this neural network with a maximum likelihood objective is intractable [37], so the objective is amended to minimize a Variational Lower-Bound of the Negative Log-Likelihood instead [68, 151]:\nLulb = -log p\u03b8(x_0|x_1) + DKL(p(x_T|x_0)||\u03c0(x_T))\n+ \u2211_{t>1} DKL (p(x_{t-1}|x_t, x_0)||p_\u03b8(x_{t-1}|x_t)), (4)\nwhere DKL is the Kullback-Leibler divergence. This objective ensures that the neural network is trained to minimize the distance between p\u03b8(xt-1|xt) and the true posterior of the forward process when conditioned on x_0. The denoising network is generally applied to parametrize the reverse mean \u00b5\u03b8(x, t) of the distribution of the reverse transition p\u03b8(xt-1|xt) := N(xt-1; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)) [27].\nThe true value of the reverse mean is a function of x_0, which is unknown in the reverse process and must therefore be estimated using input timestep t and the noisy data x_t. Specifically, the reverse mean is formulated as the following:\n\u00b5(x, t) := \u221a{\u0101t-1}(1-at)x_t + \u221a{\u221a{\u0101t-1}(1 - at)}x_0 / (1- \u0101t), (5)\nwhere the original data x_0 is unavailable in the reverse process and must therefore be estimated. We denote the denoising network's prediction of the original data as \u01760. This prediction \u01760 can then be used to obtain \u00b5\u03b8(xt, t) using Equation 5. Parametrizing with \u01760 directly is beneficial at the beginning of sampling, since predicting x_0 directly helps the denoising network to learn higher-level structural features [115]."}, {"title": "2.2 Backbone Architectures", "content": "We outline the mathematical foundations of diffusion models in Section 2.1. Since denoising prediction networks are generally parametrized by parameters \u03b8, we discuss the formulation of \u03b8 by several neural network architectures in the following section. All of these network architectures map from the same input space to the same output space.\nHo et al. [68] use a U-Net backbone similar to an unmasked PixelCNN++ [144] to approximate the score function. This U-Net architecture, originally used in semantic segmentation approaches [30, 31, 113, 140], is based on a Wide ResNet [182] and takes a noisy image and the diffusion timestep t as input, encodes the image to a lower-dimensional representation, and outputs the noise prediction for that image and noise level. The U-Net consists of an encoder and a decoder with residual connections between blocks that preserve gradient flow and help recover fine-grained details lost in the compressed representation. The encoder consists of a series of residual and self-attention blocks and downsamples the input image to a low-dimensional representation. The decoder mirrors this structure, gradually upsampling the low-dimensional representation to match the input dimensionality. The diffusion timestep t is specified by adding a sinusoidal positional embedding in each residual block [68] that scales and shifts the input features, enhancing the network's ability to capture temporal dependencies.\nDDPMs operate in the pixel space, making their training and inference computationally expensive. Rombach et al. [138] address this by proposing Latent Diffusion Models (LDMs), which operate in the latent space of a pre-trained variational autoencoder. The diffusion process is applied to the generated representation as opposed to the image directly, leading to computational benefits without sacrificing generation quality. While the authors introduce additional cross-attention mechanisms to allow for more flexible conditioned generation, the denoising network backbone remains very close to the DDPM U-Net architecture.\nRecent advances in the use of transformer architectures for vision tasks like ViT [45] have led to the adoption of transformer-based architectures for diffusion models. Peebles and Xie [132] propose Diffusion Transformers (DiT), a diffusion model backbone architecture that is largely inspired by ViTs, and demonstrates state-of-the-art generation performance on ImageNet when combined with the LDM framework. Following ViT, DiTs work by transforming input images into a sequence of patches, which are converted into a sequence of tokens using a \"patchify\" layer. After adding ViT-style positional embeddings to all input tokens, the tokens are fed through a series of transformer blocks. These blocks are equivalent to standard ViT blocks that take additional conditional information such as the diffusion timestep t and a conditioning signal c as inputs. A detailed overview of their structure can be seen in Fig 3.\nU-ViTs [12] combine the U-Net and ViT backbones into a unified backbone. U-ViTs follow the design methodology of transformers in tokenizing time, conditioning and image inputs, but additionally employ long skip connections between shallow and deep layers. These skip connections provide shortcuts for low-level features and therefore stabilize training of the denoising network [12]. Works utilizing U-ViT-based backbones [13, 72] achieve results on par with U-Net CNN-based architectures, demonstrating their potential as a viable alternative to other denoising network backbones."}, {"title": "2.3 Diffusion Model Guidance", "content": "Recent improvements in image generation results have largely been driven by improved guidance approaches. The ability to control generation by passing user-defined conditions is an important property of generative models, and guidance describes the modulation of the strength of the conditioning signal within the model. Conditioning signals can have a wide range of modalities, ranging from class labels, to text embeddings to other images. A simple method to pass spatial conditioning signals to diffusion models is to simply concatenate the conditioning signal with the denoising targets and then pass the signal through the denoising network [12, 75]. Another effective approach uses cross-attention mechanisms, where a conditioning signal c is preprocessed by an encoder to an intermediate projection E(c), and then injected into the intermediate layer of the denoising network using cross-attention [76, 142]. These conditioning approaches alone do not leave the possibility to regulate the strength of the conditioning signal within the model. Diffusion model guidance has recently emerged as an approach to more precisely trade-off generation quality and diversity.\nDhariwal and Nichol [42] use classifier guidance, a compute-efficient method leveraging a pre-trained noise-robust classifier to improve sample quality. Classifier guidance is based on the observation that a pre-trained diffusion model can be conditioned using the gradients of a classifier parametrized by \u03c6 outputting p\u03c6(c|xt, t). The gradients of the log-likelihood of this classifier \u2207x_t log p\u03c6(c|xt, t) can be used to guide the diffusion process towards generating an image belonging to class label y. The score estimator for p(x|c) can be written as\n\u2207_{xt} log (p\u03b8(xt)p\u03c6(c|xt)) = \u2207_{xt} log p\u03b8(xt)+\u2207_{xt} log p\u03c6(c|xt). (11)\nBy using Bayes' theorem, the noise prediction network can then be rewritten to estimate:\n\u03f5\u03b8(xt, c) = \u03f5\u03b8(Xt, c) - w\u2207_{xt} log p\u03c6(c|xt), (12)\nwhere the parameter w modulates the strength of the conditioning signal. Classifier guidance is a versatile approach that increases sample quality, but it is heavily reliant on the availability of a noise-robust pre-trained classifier, which in turn relies on the availability of annotated data, which is not available in many applications.\nTo address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier. CFG works by training an unconditional diffusion model parametrized by \u03f5\u03b8(xt,t,\u03c6) together with a conditional model parametrized by \u03f5\u03b8(xt, t, c). For the unconditional model, a null input token \u03c6 is used as a conditioning signal c. The network is trained by randomly dropping out the conditioning signal with probability Puncond. Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\u1f14\u03c1(xt, c) = (1 + w)eo(xt, c) \u2013 wee(xt, $). (13)\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network. Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73, 100]. These methods do not need annotated data, allowing the use of larger unlabelled datasets.\nTable 1 shows the requirements of current guidance methods. While classifier and classifier-free guidance improve generation results, they require annotated training data. Self-guidance and online guidance are fully self-supervised alternatives that achieve competitive performance without annotations.\nClassifier and classifier-free guidance are controlled generation methods that rely on conditional training. Training-free approaches modify the generation process of a pre-trained model by binding multiple diffusion processes [14] or using time-independent energy functions [179]. Other controlled generation methods take a variational perspective [54, 119, 146, 164], treating controlled generation as a source point optimization problem [17]. The goal is to find samples x that minimize a loss function L(x) and are likely under the model's distribution p. The optimization is formulated as minx, L(x), where xo is the source noise point. The loss function L(x) can be modified for conditional sampling to generate a sample belonging to a particular class y."}, {"title": "3 METHODS", "content": "Having covered the main preliminaries for diffusion models, we outline a series of methods related to diffusion models and representation learning in the following section. In subsection 3.1 we describe and categorize current frameworks utilizing representations learned by pre-trained diffusion models for downstream recognition tasks. In subsection 3.2, we describe methods that leverage advances in representation learning to improve diffusion models themselves."}, {"title": "3.1 Diffusion Models for Representation Learning", "content": "Learning useful representations is one of the main motivations for designing architectures like VAEs [88, 89] and GANs [22, 84]. Contrastive learning approaches, where the goal is to learn a feature space in which representations of similar images are very close together, and vice versa for dissimilar images (e.g. SimCLR [34], MoCo [60]), have also led to significant advances in representation learning. These contrastive methods are not fully self-supervised however, since they require supervision in the form of augmentations that preserve the original content of the image.\nDiffusion models offer a promising alternative to these approaches. While diffusion models are primarily designed for generation tasks, the denoising process encourages the learning of semantic image representations [15], that can be used for downstream recognition tasks. The diffusion model learning process is similar to the learning process of Denoising Autoencoders (DAE) [18, 162], which are trained to reconstruct images corrupted by adding noise. The main difference is that diffusion models additionally take the diffusion timestep t as input, and can thus be viewed as multi-level DAEs with different noise scales [169]. Since DAEs learn meaningful representations in the compressed latent space, it is intuitive that diffusion models exhibit similar representation learning capabilities. We outline and discuss current approaches in the following section."}, {"title": "3.1.1 Leveraging intermediate activations", "content": "Baranchuk et al. [15] investigate the intermediate activations from the U-Net network that approximates the Markov step of the reverse diffusion process in DDPMs [42]. They show that for certain diffusion timesteps, these intermediate activations capture semantic information that can be used for downstream semantic segmentation. The authors take a noise-predictor network \u03f5\u03b8(xt,t) trained on the LSUN-Horse [177] and FFHQ-256 [84] datasets and extract feature maps produced by one of the network's 18 decoder blocks for label-efficient downstream segmentation tasks. Selecting the ideal diffusion timestep and decoder block activation to extract is non-trivial. To understand the efficacy of pixel-level representations of different decoder blocks, the authors train a multi-layer perceptron (MLP) to predict the semantic label from features produced by different decoder blocks on a specific diffusion step t. The representations from a fixed set of blocks B of the pre-trained U-Net decoder and higher diffusion timesteps are upsampled to the image size using bilinear interpolation and concatenated. The obtained feature vectors are then used to train an ensemble of independent MLPs which predict a semantic label for each pixel. The final prediction is obtained by majority voting. This method, denoted DDPM-Seg, outperforms baselines that exploit alternative generative models and achieves segmentation results competitive with MAE [61], illustrating that intermediate denoising network activations contain semantic image features.\nXiang et al. [169] extend this approach to further architectures and image recognition on CIFAR-10 and Tiny-ImageNet. They investigate the discriminative efficacy of extracted features for different backbones (U-Net and DiT [132]) under different frameworks (DDPM and EDM [85]). The relationship between feature quality and layer-noise combinations is evaluated through grid search, where the quality of feature representations is determined using linear probing. The best-performing features lie in the middle of up-sampling using relatively small noising levels, which is in line with conclusions drawn in DDPM-Seg [15]. Benchmark comparisons against diffusion-based methods like HybViT [174] and SBGC [190] on CIFAR-10 and Tiny-ImageNet [41] show that EDM-based Denoising Diffusion Autoencoders (DDAEs) outperform previous supervised and unsupervised diffusion-based methods on both generation and recognition, especially after fine-tuning. Benchmarking against contrastive learning methods shows that the EDM-based DDAE is comparable with Sim-CLRs considering model sizes, and outperforms SimCLRs with comparable parameters on CIFAR-10 and Tiny-ImageNet.\nODISE [170] is a related approach that unites text-to-image diffusion models with discriminative models to perform panoptic segmentation [90, 91], a segmentation approach unifying instance and semantic segmentation into a common framework for comprehensive scene understanding. ODISE extracts the internal features of a pre-trained text-to-image diffusion model. These features are input to a mask generator trained on annotated masks. A mask classification module then categorizes each generated binary mask into an open vocabulary category by relating the predicted mask's diffusion features with text embeddings of object category names. The authors use the Stable Diffusion U-Net DDPM backbone and extract features by computing a single forward pass and extracting the intermediate ac-"}, {"title": "tivations", "content": "f = UNet(x_t, \u03c4(s), t) where \u03c4(s) is an encoded representation of the image caption s obtained leveraging a pre-trained text encoder \u03c4. Interestingly, the authors obtain the best results using t = 0, whereas previous methods obtain better results using higher diffusion timesteps. To overcome reliance on available image captions, Xu et al. [170] additionally train an MLP-based implicit captioner that computes an implicit text embedding from the image itself. ODISE establishes a new state-of-the-art in open-vocabulary segmentation and is a further example of the rich semantic representations learned by denoising diffusion models.\nMukhopadhyay et al. [125] also propose leveraging intermediate activations from the unconditional ADM U-Net architecture [42] for ImageNet classification. The methodology for layer and timestep selection is similar to previous approaches. Additionally, the impact of different sizes for feature map pooling is evaluated and several different lightweight architectures for classification (including linear, MLP, CNN, and attention-based classification heads) are used. Feature quality is found to be mostly insensitive to pooling size, and is mostly dependent on time steps and the selected block number. Their approach, which we term guided diffusion classification (GDC), achieves competitive performance against other unified models, namely BigBiGAN [44] and MAGE [99]. The attention-based classification heads perform best on ImageNet-50, but perform poorly on Fine-Grained Visual Classification datasets, indicating their reliance on a large amount of available data.\nIn a continuation of their previous work, Mukhopadhyay et al. [126] extend this approach by introducing two methods for more fine-grained block and denoising time step selection. The first is DifFormer [126], an attention mechanism replacing the fixed pooling and linear classification head from [125] with an attention-based feature fusion head. This fusion head is designed to replace the fixed flattening and pooling operation required to generate vector feature representations from the U-Net CNN used in the GDC approach with a learnable pooling mechanism. The second mechanism is DifFeed [126], a dynamic feedback mechanism that decouples the feature extraction process into two forward passes. In the first forward pass, only the selected decoder feature maps are stored. These are fed to an auxiliary feedback network that learns to map decoder features to a feature space suitable for adding them to the encoder blocks of corresponding blocks. In the second forward pass, the feedback features are added to the encoder features, and the DifFeed attention head is used on top of those second forward pass features. These additional improvements further increase the quality of learned representations and improve ImageNet and fine-grained visual classification performance.\nThe previously described diffusion representation learning methods focus on segmentation and classification, which are only a subset of downstream recognition tasks. Correspondence tasks are another subset that generally involves identifying and matching points or features between different images. The problem setting is as follows: Consider two images I1 and I2 and a pixel location p1 in I1. A correspondence task involves finding the corresponding pixel location p2 in I2. The relationship between p1 and p2 can be semantic (pixels that contain similar semantics), geometrical (pixels that contain different views of an object) or temporal (pixels that contain the same object deforming over time). DIFT (Diffusion Features) [157] is an approach leveraging pre-trained diffusion model representations for correspondence tasks. DIFT also relies on extracting diffusion model features. Similarly to previous approaches, diffusion timestep and network layer numbers used for extraction are an important consideration. The authors observe more semantically meaningful features for large diffusion timesteps and earlier network layer combinations, whereas lower-level features are captured in smaller diffusion timesteps and later denoising network layers. DIFT is shown to outperform other self-supervised and weakly-supervised methods across a range of correspondence tasks, showing on-par performance with state-of-the-art methods on semantic correspondence specifically.\nZhang et al. [183] evaluate how learned diffusion features relate across multiple images, instead of focusing on downstream tasks for single images. To investigate this, they employ Stable Diffusion features for semantic correspondence as well. The authors observe that Stable Diffusion features have a strong sense of spatial layout, but sometimes provide inaccurate semantic matches. DINOv2 [128], a method for self-supervised representation learning using knowledge distillation and vision transformers, produces more sparse features that provide more accurate matches. Zhang et al. [183] therefore propose to combine the two features and employ zero-shot evaluation of nearest neighbor search on the combined features to achieve state-of-the-art performance on several semantic correspondence datasets like SPair-71k and TSS.\nSD4Match [103] builds on this approach by using various prompt tuning and conditioning techniques. One method, SD4Match-Class, fine-tunes prompt embedding for each semantic class using a semantic matching loss [102]. Given images I and I, the Stable Diffusion U-Net f(\u00b7) extracts feature maps FA and F\u03b2 by Ft = f(It, t, \u0398). Correspondence points are predicted by normalizing feature maps and computing a correlation map, which is converted to a probability distribution using a softmax operation. Additionally, Li et al. [103] propose conditioning prompts on input images using a Conditional Prompting Module (CPM), which includes a DINOv2 feature extractor, linear layers, and an adaptive MaxPooling layer. The conditioning embedding Ocond is formed by concatenating feature representations and projecting them to the prompt embedding dimension. The final prompt OAB is obtained by appending Ocond to a global prompt Oglobal. This method sets new benchmark accuracies on SPair-71k [122], PF-Willow, and PF-Pascal [59], surpassing methods like DIFT [157] and SD+DINO [183]\nLuo et al. [116] introduce Diffusion Hyperfeatures, a framework designed to consolidate multiple intermediate activation maps across diffusion timesteps for downstream recognition. Activations are consolidated using an interpretable aggregation network, that takes the collection of intermediate feature maps as input and produces a single feature descriptive feature map as output. While other approaches manually select fixed diffusion timesteps and activations from a pre-determined number of intermediate network layers, Diffusion Hyperparameters cache all feature"}, {"title": "maps across all layers and timesteps in the diffusion process to generate a dense set of activations. This high dimensional set of activations is upsampled, passed through a bottleneck layer B and weighed with a unique learnable mixing weight wi,s for each layer and timestep combination. The final diffusion hyperfeatures take on the form", "content": "\u2211_{s=0}^{S_L}\u2211_{l=1}^{L} w_{l,s} B_l(\u03b9,\u03c2), (14)\nwhere L is the number of layers, S is a subsample of the number of diffusion timesteps and r is an activation feature map. Bottleneck layers and mixing weights are finetuned on the specific downstream task. Similar to previous approaches, Diffusion Hyperfeatures is used for semantic correspondence. The authors extract activations from Stable-Diffusion and tune the aggregation network on a subset of SPair-71k. Diffusion Hyperfeatures outperforms models that use self-supervised descriptors or supervised hypercolumns on the SPair-71k and CUB datasets.\nHedlin et al. [62] focus on optimizing the prompt embeddings by exploiting intermediate attention maps specifically. Given a certain input text prompt, these attention activation maps correspond to the semantics of the prompt. Instead of optimizing a global or a class-dependent prompt embedding using the semantic loss, Hedlin et al. [62] optimize the embedding to maximize the cross-attention at the location of interest. Locating corresponding points in a second image then comes down to conditioning on the optimized prompt, and selecting the point with the pixel attaining the maximum attention map value within the target image. Note that this approach does not utilize supervised training specific to semantic correspondence. However, they require test-time optimization which is costly. Text prompts are optimized using an off-the-shelf diffusion model without fine-tuning. Several further works building on aforementioned approaches [120, 184] exist, showing that exploiting pre-trained diffusion models for semantic correspondence remains a promising application of diffusion models.\nZhao et al. [187] propose Visual Perception with a pre-trained Diffusion Model (VDM), a framework closely related to USCSD [62] that employs a text feature refinement network as well as an additional recognition encoder for semantic segmentation and depth estimation. Here, the denoising network is fed with refined text representations as well as an input image, and the resulting feature maps as well as the cross-attention maps between the text and image features are used to provide guidance for a decoder. To achieve this, the prediction model is written as p\u03a8(y|x, S), where S represents the set of all category labels of the downstream task. The prediction model is implemented as the following:\np(y|x, S) = p3(y|F)p2(F|x,C)p1(C|S), (15)\nwhere F denotes the set of feature maps and C denotes the text features. Here, p\u03c61(C|S) denotes a text adapter consisting of a two-layer MLP that refines the text features obtained by applying the CLIP text encoder to a text template of \"a photo of a [CLS]\". p\u03c62 (F|x) extracts the feature maps from the denoising network given the input image x and the set of refined text features C. The authors use t = 0 when feeding the denoising network the latent representation of the input image generated by using the VQGAN encoder [47] to obtain feature maps F. Finally, p\u03c63(y|F) serves as a light-weight prediction head implemented as a semantic feature pyramid network [90] that is adapted to the downstream task. VDM is evaluated on semantic segmentation and depth estimation, and achieves highly competitive performance and fast convergence compared to methods with other pre-training paradigms.\nA more indirect application of text-to-image diffusion model representations is instructional image editing [23, 51, 98], where the desired image edit is described by a natural language instruction rather than a description of the desired new image [81]. Prompt-based image editing is challenging since small changes in the textual prompt can lead to vastly different generation outcomes. [65] propose a textual editing method for pre-trained text-conditioned diffusion models that leverages the semantic strength of the intermediate cross-attention layers in the denoising backbone. This approach is based on a key observation also employed in [62, 187]: Cross-attention maps contain rich information on the spatial layout and geometry of the generated image. Injecting the cross-attention layers obtained when generating an image I into the generation process of the edited image I* ensures that the edited image preserves the original spatial layout. Hertz et al. [65] use Imagen [141] to conduct experiments and demonstrate promising results on text-only localized editing, global editing, and real image editing. Following works like Plug-and-play Diffusion Features [160] further improve upon this by leveraging all intermediate activation maps to enable instructional image editing. Other techniques like TokenFlow [52] and work by Yatim et al. [175] have extended this idea to the video space, using diffusion features to enable prompt-based video editing and text-driven motion transfer."}, {"title": "3.1.2 A general representation extraction framework", "content": "Many of the methods outlined in the previous section follow a similar procedure in leveraging learned representations of pre-trained diffusion models for downstream vision tasks. In this section, we aim to consolidate these approaches to a common three-step framework. We do this to provide clarity on the relationship between diffusion models and their use for downstream predictive tasks. To leverage intermediate activations for downstream tasks, a selection methodology that outputs the ideal diffusion timestep input as well as the intermediate layer number(s) whose activation maps have the highest predictive performance when upsampled and linearly probed must be applied. This can be a trainable model [116], a grid search procedure [169] or a learning agent [173]. The goal of this methodology is generally to select timestep t \u2208 T and a set of decoder block numbers B that maximize predictive performance on a downstream task. Given a set of possible timesteps T and a set of decoder blocks B, the goal is to find:\n(t*, B*) = arg min Ldiscr(t, B), (16)\nt\u2208T, BCB\nwhere Ldiscr(t, B) represents the discriminative loss at timestep t when the blocks in B are used for downstream prediction. Generally, discriminative tasks will require more"}, {"title": "high-level features corresponding to structural elements and shapes, whereas generative tasks mapping random noise to images will require the computation of lower-level features. The ideal intermediate layer number as well as the optimal diffusion timestep will largely depend on the exact downstream prediction task, the dataset, and the architecture of the diffusion model used.", "content": "Once the ideal timestep and layer number are determined, an input image and the selected diffusion timestep are passed to the diffusion model, and the intermediate activations in the selected decoder blocks computed in the forward pass are extracted and generally concatenated and pre-processed depending on the downstream task (e.g. through upsampling, pooling, etc.). Finally, a classification head is trained on the annotated dataset, taking the preprocessed features extracted from the diffusion model as input. This classification head can be an MLP, a CNN, or an attention-based network depending on the availability of labeled data and predictive performance on the dataset. The diffusion model weights are usually frozen in this probing process, but additional fine-tuning regimes can increase discriminative performance for certain datasets and architectures (see e.g., Xiang et al. [169]). Fig. 4 shows an overview of the generalized framework."}, {"title": "3.1.3 Knowledge transfer", "content": "Aside from leveraging intermediate activations from pre-trained diffusion models directly as inputs to a recognition network, several recent approaches propose a more indirect method of reusing learned representations for downstream tasks. We summarize these under the term knowledge transfer methods. This reflects the common idea of distilling representations from pre-trained diffusion models and then transferring them to auxiliary networks in a way that is distinct from simply providing aggregated feature activation maps as input. Several of these approaches are discussed in the following section.\nYang and Wang [173"}]}