{"title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "abstract": "Many fields could benefit from the rapid development of the large language models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the typically fields facing new opportunities as the LLMs have supported more and more modalities. Here, by utilizing vision-language model (VLM), we proposed an e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided into four stages, which are perception, prediction, planning, and behavior. Each stage consists of several visual question answering (VQA) pairs and VQA pairs interconnect with each other constructing a graph called Graph VQA (GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our method could achieve e2e driving with language. In our method, vision transformers (ViT) models are employed to process nuScenes visual data, while VLM are utilized to interpret and reason about the information extracted from the visual inputs. In the perception stage, the system identifies and classifies objects from the driving environment. The prediction stage involves forecasting the potential movements of these objects. The planning stage utilizes the gathered information to develop a driving strategy, ensuring the safety and efficiency of the autonomous vehicle. Finally, the behavior stage translates the planned actions into executable commands for the vehicle. Our experiments demonstrate that SimpleLLM4AD achieves competitive performance in complex driving scenarios.", "sections": [{"title": "1. Introduction", "content": "Autonomous driving has garnered significant attention from both academia and industry over the past decade. [1-3] The promise of safer roads, reduced traffic congestion, and increased mobility for all populations has driven rapid advancements in this field. Traditional approaches to autonomous driving often rely on a modular pipeline consisting of perception, prediction, planning, and control. However, these methods can suffer from compounding errors across modules, leading to suboptimal performance in complex and dynamic driving environments.\nWith the advent of large language models (LLMs), [4,5] a new opportunity arises to redefine the approach to autonomous driving. LLMs, particularly when integrated with vision-language models (VLMs), [6, 7] have shown remarkable capabilities in understanding and generating human-like text based on visual in-"}, {"title": "2. Related work", "content": "Large Language Models\nLLMs have rapidly evolved in recent years, demonstrating unprecedented ca- pabilities in natural language understanding and generation. These models, exemplified by OpenAI's GPT-3 [8] and GPT-4 [9], and other open-sourced transformer-based architectures such as the LLaMA series, [4,5] Vicuna, [10] Baichuan, [11] Qwen, [12] Yuan 2.0, [13] Yuan 2.0-M32, [14] leverage vast amounts of data and sophisticated training techniques to produce human-like text and comprehend complex linguistic structures.\nOne of the significant breakthroughs with LLMs has been their ability to perform a wide range of tasks with minimal fine-tuning. This versatility arises from the models' pre-training on diverse corpora, enabling them to capture extensive world knowledge and linguistic patterns. The emergence of LLMs has led to remarkable advancements in various fields, including natural language processing (NLP), question answering, machine translation, and text summarization.\nVision-language Models\nVLMs represent a significant advancement in the field of artificial intelligence, aiming to bridge the gap between visual and textual data. The architecture of VLMs typically involves two main components: a visual encoder and a language decoder. The visual encoder processes the image to extract features, which are then combined with text representations generated by the language decoder. This integrated approach enables the model to perform complex reasoning tasks that require both visual and linguistic understanding. These models combine the strengths of vision models, such as convolutional neural networks (CNNs) [15,16] and ViT [17-20], with the capabilities of language models, allowing for more comprehensive understanding and interaction with multimodal inputs.\nEfforts in VLMs focused on tasks like image captioning and VQA. Image caption- ing involves generating descriptive textual annotations for images, while VQA requires the model to answer questions about the content of an image. Nowadays, more and more VLMs have demonstrated impressive capabilities in these areas by effectively integrating visual and textual information. Flamingo [21] utilizes visual and language inputs as prompts, demonstrating exceptional few-shot performance in visual question answering. GPT-4, [9] the LLaVA series [6,7] introduces visual instruction tuning to enhance the instruction-following capabil- ities of VLMs. Concurrently, models like KOSMOS-2 [22], and Qwen-VL [23] have advanced VLMs by incorporating visual grounding capabilities, facilitating tasks such as region description and localization. Furthermore, PaLM-E [24] and EmbodiedGPT [25] represent significant strides in adapting VLMs for embodied applications, vastly expanding their potential use cases. These advancements illustrate the rapid progress in the development and application of VLMs, show- casing their ability to handle increasingly complex multimodal tasks. As the integration of vision and language continues to evolve, VLMs are poised to"}, {"title": "Language-grounded Driving", "content": "Language-grounded driving is an emerging research area that combines the principles of NLP with AD technologies. This interdisciplinary approach leverages the understanding and generation of natural language to inform and enhance the decision-making processes of autonomous vehicles. By grounding driving actions in language, these systems can achieve greater interpretability, flexibility, and user interaction capabilities.\nRecent advancements in LLMs and VLMs have propelled the field of language- grounded driving. Models like GPTDriver, [26] LLM-Driver, [27] and LMDrive [28] offer new possibilities for integrating language grounding into autonomous driving. These models enable more nuanced and context-aware interpretations of driving environments and instructions.\nLanguage-grounded driving represents a promising frontier in autonomous vehicle technology. By integrating natural language understanding with driving sys- tems, researchers are developing more interactive, interpretable, and adaptable autonomous vehicles. Our SimpleLLM4AD method builds upon these advance- ments, utilizing language grounding to enhance each stage of the e2eAD pipeline, from perception and prediction to planning and behavior. This integration not only improves the vehicle's decision-making capabilities but also facilitates better human-vehicle interaction and scenario-based training."}, {"title": "3. Method", "content": "3.1. Overall architecture.\nThe pipeline of our method is outlined in Figure 1. The overall architecture comprises two main modules: a vision encoder that processes images, and an LLM decoder that handles questions.\nVision Encoder. We choose InternViT-6B as the vision encoder. InternViT-6B, a vision transformer with 6 billion parameters, was first introduced by Chen et al. [29] It was pre-trained using web-scale image-text data from various sources to align with large language models. The Query Model serves as a bridge between the vision encoder and the LLM decoder, aligning the vision and text modalities. This vision-text alignment component is initialized with a multilingual-enhanced LLaMA. [30]\nLLM Decoder. We employ Vicuna-13B as the LLM decoder. Vicuna-13B is an open-source LLM that was fine-tuned from LLaMA using user-shared conversations collected from ShareGPT.31 Although different questions share the same LLM decoder model, we have devised a GVQA strategy to enhance the language model's capabilities and crafted tailored prompts based on different problem types."}, {"title": "3.2. GVQA logical dependency", "content": "The SimpleLLM4AD method encompasses a sequence of four stages, each in- tricately linked by the logical dependencies of the QA pairs they contain. As depicted in Fig. 2, the logical dependency of GVQA is graphically represented,"}, {"title": "4. Experiments", "content": "4.1. Datasets and Metrics\nDataset\nThe datasets used for fine-tuning and evaluation is DriveLM-nuScenes. [31] DriveLM-nuScenes is a comprehensive dataset specifically designed for develop- ing and benchmarking autonomous driving models. It consists of a training set of 4072 frames and a validation set of 799 frames, providing a robust founda- tion for training and testing. DriveLM-nuScenes incorporates both scene-level descriptions and frame-level QA pairs, divided into three categories: perception, prediction, and planning. This structure ensures a comprehensive understanding of the driving scenarios by addressing various aspects of the driving process.\nPerception: This category involves questions related to the thorough examination of the entire frame. While some questions are manually annotated, prompts are designed to generate questions based on the observational facets of objects within the scene, leveraging ground truth from nuScenes and OpenLane-V2 datasets.\nPrediction: This category includes inquiries regarding the future states of key objects and the ego vehicle in the current frame, focusing on the reasoning process behind these predictions. Due to the complexity of these predictions, the answers are manually annotated.\nPlanning: This involves questions related to the subsequent actions of the ego vehicle within the current frame. Similar to prediction, planning questions are challenging and require manual annotation for the reasoning process.\nEach key object within the QA pairs is encoded as c tags in the format <c, CAM, x, y>, where c is the identifier, CAM indicates the camera, and x, y are the coordinates of the object's 2D bounding box in the respective camera's coordinate system. The dataset also includes a dictionary in each key frame, recording basic information about key objects such as bounding box size, category, moving state, and visual description.\nMetrics\nTo evaluate the performance of our models, we use a set of well-established metrics tailored to different tasks within the autonomous driving domain. The metrics are divided into three main categories: P1-3 VQA Metrics, Behavior Task Metrics, and Motion Task Metrics.\nP1-3 VQA Metrics: These metrics assess the performance of perception, pre- diction, and planning question-answering (QA) tasks. Common Visual Question Answering (VQA) metrics are employed, alongside a new GPT score for a more nuanced semantic evaluation."}, {"title": "4.2. Implementation Details", "content": "In our method, the SimpleLLM4AD model is finetuned with DriveLM-nuScenes dataset. We use the pretrained weights of InternViT-6B and keep it frozen. QLLAMA and 96 queries are trainable during finetuing. The pretrained Vicuna- 13B could be totally frozen or tuned by parameter-efficient fine-tuning (PEFT) [32] methods such as LoRA [33]. The image resolution is set to 224 \u00d7 224. We finetuned the model on NVIDIA GPU with a learning rate of 1e-4 and a global batch size of 16."}, {"title": "4.3. Test Results on DriveLM-nuScenes", "content": "All models in Table 1 are fine-tuned on the training split of the DriveLM- nuScenes dataset and tested on the test set of the same dataset. The DriveLM"}, {"title": "4.4. Ablation", "content": "During our exploration, we conducted training and inference of SimpleLLM4AD under various settings. The differences among these solutions primarily involve the treatment of prompts and the methods for detecting key objects. The baseline performance shown in Table 1 was obtained by using LLaMA-Adapter-V2 for"}, {"title": "Chain of Thought", "content": "Chain-of-Thought (CoT) is an approach for prompting, in which one includes the intermediate steps of reasoning within the prompt (intermediate \u201cthoughts\"), besidesthe task input/output. [36] CoT was shown to significantly improve the capability of LLMs to solve problems without resorting to any model updates.\nIn SimpleLLM4AD, we employ CoT by use the answer of Np as contextual information for Ns, and Ns would always be the next QA pair of Np in the dataset. Comparing to the DriveLM baseline, Version A that utilizing CoT show a remarkable improvement in acc. And language score.\nIn the DriveLM-nuScenes dataset, 'What are the important objects in the current scene?' question would always be the first question in each frame, which we call it No. In Version B, we apply No + Np as context for each Ns.\""}, {"title": "Graph of Thought", "content": "While CoT only support unidirectional single branch of thought when prompting LLMs step by step, graph of thought (GoT) enable current task use any previous QA pairs as context and would allow any subsequent questions refer to the results of current task. After trying different logical dependency graphs, the prominent one is shown in Fig. 2. By using GoT to arrange context like this, Version C get a prominent improvement."}, {"title": "Prompt Optimization", "content": "Refined prompt would contribute to the performance of LLMs. In the solution of DriveLM baseline, the context that pass to the subsequent question would be"}, {"title": "Detection of the Key Objects", "content": "Format_instruction is \"Input six images in turn. The first of six images is <CAM_FRONT>, which is in the front of the ego vehicle. The second of six images is <CAM_FRONT_LEFT>, which is in the front left of the ego vehicle. The third of six images is <CAM_FRONT_RIGHT>, which is in the front right of the ego vehicle. The fourth of six images is <CAM_BACK>, which is in the back of the ego vehicle. The fifth of six images is <CAM_BACK_LEFT>, which is in the back left of the ego vehicle. The sixth of six images is <CAM_BACK_RIGHT>, which is in the back right left of the ego vehicle. <number, number> is object box center coordinate in the image (1600*900).\"\nTo enhance the performance, we have intergrated Format_instruction to guide LLM. To accurately detect and classify objects, we leverage existing, well- established detection networks, such as dinov2, which provide us with robust target detection output. In addition, we have trained a specialized detection classification network that goes beyond mere object detection. This network"}, {"title": "5. Conclusion", "content": "In this paper, we present SimpleLLM4AD, a multi-modal language model that achieves competitive performance in complex driving scenarios. The integration of VLM allows for more nuanced and context-aware decision-making, significantly improving the robustness and reliability of the autonomous driving system. This approach also highlights the potential for LLMs to enhance multimodal AI applications, paving the way for further advancements in the field of e2eAD. In the future, we will continue to optimize box positioning as well as the alignment of multi-frame vision-language large models and other related directions."}]}