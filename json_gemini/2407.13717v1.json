{"title": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases", "authors": ["Usman Gohar", "Michael C. Hunter", "Robyn R. Lutz", "Myra B. Cohen"], "abstract": "Constructing assurance cases is a widely used, and sometimes required, process toward demonstrating that safety-critical systems will operate safely in their planned environment. To mitigate the risk of errors and missing edge cases, the concept of defeaters arguments or evidence that challenge claims in an assurance case has been introduced. Defeaters can provide timely detection of weaknesses in the arguments, prompting further investigation and timely mitigations. However, capturing defeaters relies on expert judgment, experience, and creativity and must be done iteratively due to evolving requirements and regulations. This new ideas paper proposes CoDefeater, an automated process to leverage large language models (LLMs) for finding defeaters. Initial results on two systems show that LLMs can efficiently find known and unforeseen feasible defeaters to support safety analysts in enhancing the completeness and confidence of assurance cases.", "sections": [{"title": "1 INTRODUCTION", "content": "Safety-critical systems have become deeply integrated into many societal domains, including healthcare, transportation, energy, and aviation [21, 32, 46]. Failures in these systems can lead to catastrophic consequences for human safety, including fatalities and environmental and property damage [59]. This has led to an increased focus on their dependence, reliability, and safety [38, 57]. Many systems must comply with regulations [9, 54], provide evidence of safety, and undergo rigorous certification processes [35, 53] for approval from regulatory bodies. Assurance cases (ACs) have emerged as a common practice for this purpose, facilitating the verification of system correctness and the validation of specific claims regarding safety, security, and trustworthiness, among others [5, 7, 45].\nAn assurance case is a structured hierarchy of claims and arguments supported by evidence that a system will function as intended in a specified environment [7, 60]. Several formal notations (e.g., Goal Structuring Notation (GSN) [36], Claims-Arguments-Evidence (CAE) [1], and Eliminative Argumentation (EA) [28]), along with tools [20, 45, 64], have been proposed. However, concerns arise over their completeness, uncertainty, and soundness for cyber-physical systems. [24, 30], leading to false confidence and catastrophic failures [63]. For example, failure of the minimum safe altitude warning system that led to a major aviation accident was attributed to incomplete and flawed reasoning in the safety case [31].\nTo enhance the robustness of assurance cases, it is critical to identify and mitigate their defeaters (also known as assurance weakeners). Defeaters highlight gaps in evidence or reasoning that undermine the validity of claims in the assurance case [33]. An example of a defeater, drawn from the assurance case for the safe operation of an sUAS (small Uncrewed Aircraft System) battery, challenges the assurance case's claim that \"The sUAS has enough battery charge to complete its mission.\" The defeater casting doubt on this claim is, \"Unless the battery monitor is not calibrated/inaccurate.\" The defeater serves to record, within the assurance case itself, the analyst's challenge to the validity of the claim.\nVarious approaches have been proposed to identify and mitigate defeaters [29, 37]. However, manually creating defeaters is a labor-intensive and time-consuming process [48, 51], relying heavily on safety analysts' judgment, experience, creativity, and understanding of the system. This can lead to confirmation bias [6, 64]. As assurance cases evolve with new standards and technological advances, ongoing efforts are focused on formal and semi-automated approaches for detecting and managing defeaters [22, 40]. Best practices build assurance cases incrementally, so automating all/or part of this process is important [22, 67].\nLarge Language Models (LLMs) are increasingly automating software engineering tasks like test generation and defect detection [2, 34]. In particular, these models have become valuable in tasks requiring complex understanding, such as vulnerability detection, requirements elicitation, and code generation [4, 16, 68]. Moreover, LLMs have shown promise in automating evaluation tasks and"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Assurance case arguments typically adopt an inductive approach, where sub-claims offer direct evidence to support the parent claim but do not ensure it with certainty [7, 12]. Focusing solely on proving a claim may introduce confirmation bias, as exemplified by the Nimrod aircraft crash [11]. Recent approaches have therefore embraced defeasible reasoning, which acknowledges that arguments about system properties in practice are inherently defeasible [27, 42, 56]. Defeaters are potential doubts or objections that challenge the validity of a claim, reflecting gaps in evidence and reasoning [24, 27, 43, 50]. Figure 2 shows a fragment of an assurance case for an sUAS battery, with examples of defeaters (red boxes).\nDefeaters are typically represented using EA notation [28] but have also been integrated into GSN and CAE notations [7, 33]. Hence, this study does not aim to evaluate LLMs' performance in identifying defeaters in any specific notation or semantic accuracy but rather to investigate the feasibility of this approach. Finally, the indefeasibility criterion requires a thorough search for defeaters in an assurance case [7], motivating our investigation into LLMs' potential to assist practitioners and safety analysts in identifying and generating novel defeaters.\nIn software engineering, LLMs are increasingly employed to assist developers and automate tasks such as discovering requirements, code generation, testing, and program synthesis [4, 34]. Diemert and Weber [23] have reported ChatGPT's effectiveness in hazard analysis for safety-critical systems, highlighting their potential to assist human analysts. In the context of software assurance cases, Sivakumar et al. [62] assessed LLM's (GPT-4) proficiency in understanding GSN representations and its performance in constructing safety cases. Viger et al. [65] proposed using LLMs to"}, {"title": "3 METHODOLOGY", "content": "We conducted a preliminary exploratory study toward answering the following two research questions:\n\u2022 RQ1: (Effectiveness). How effective are LLMs in identifying and analyzing defeaters in assurance cases?\n\u2022 RQ2: (Utility). Can LLMs support practitioners in generating novel and meaningful defeaters?\n3.1 Experimental Setup\nDatasets. We performed our experiments on two assurance cases. The first is for the CERN Large Hadron Collider (LHC) Machine Protection System (MPS), which provides assurance that the MPS will prevent damage to the LHC from unstable, high-energy particle beams [49, 58]. The assurance case for the CERN LHC uses EA notation, and we extracted claim nodes with their corresponding defeaters for our experiments. The second assurance case is a fragment of a larger one that our team has recently created for small Uncrewed Aircraft Systems (sUAS)\u00b9 using GSN. It addresses the claim \"the sUAS has enough charge in its battery to complete the mission\" (see claim 1.2 in Figure 2). We include both the assurance case fragment and corresponding defeaters in the supplementary material\u00b2. The real-world complexity of these systems and the availability of defeaters made them suitable for our preliminary experiments."}, {"title": "3.2 Prompt Design", "content": "An effective prompt design is crucial for achieving good performance, as the choice of prompts significantly impacts the quality, relevance, and accuracy of the LLM's response [14]. It involves crafting a system prompt to establish the context and prepare the LLM for the task, along with a user prompt that contains the specific task request [52]. We designed the system prompt following OpenAI's best practices [52] and relevant literature in software engineering [13, 71] and open-ended evaluation tasks [17]. Our process identified role-based system prompts [41] as the most effective approach. Figure 3 shows the system prompt used in our study.\nSeveral user prompting techniques have been proposed, e.g., zero-shot, one-shot, few-shot, and chain-of-thought [14]. Zero-shot learning involves providing the model with only the task description (system prompt), without examples of unseen tasks to learn from. In contrast, one-shot and few-shot learning conditions the model on one or more examples in the prompt, respectively [10]. For our preliminary study, we adopted the zero-shot learning setting. This approach both (1) facilitates the immediate, off-the-shelf application of LLMs, eliminating the need for computationally expensive fine-tuning procedures, and (2) is naturally suited to scenarios such as ours, with limited data availability for training or fine-tuning [39]. Each prompt was presented independently to the model to avoid influencing subsequent responses, allowing us to assess its standalone capabilities [15]."}, {"title": "3.3 Evaluation Criteria", "content": "Due to the complexity and open-ended nature of the task (e.g., varying response length vs. ground truth, and subjectivity), automatic evaluation metrics were not suitable [17]. Therefore, we relied on human evaluation for assessing LLM performance. For RQ1, we used a deductive coding approach [23, 44], where the first two authors independently reviewed responses and categorized them as complete match, partial match, or no match based on similarity to ground-truth defeaters. We used the defeaters in the LHC assurance case as the ground truth. For the battery assurance case, a set of ground-truth defeaters was provided by one of the authors (independently) familiar with the domain, following best practices [49]."}, {"title": "3.4 Threats to Validity", "content": "There is potential subjectivity in the qualitative evaluation of LLM performance on defeater identification. To address that, two authors independently coded the LLM responses, following best practices [8, 26], and held multiple discussions to avoid misinterpretations. We also computed Cohen's kappa [18], which indicated substantial inter-reviewer agreement. To avoid confirmation bias, disagreements were coded as partial or no match. The non-deterministic nature of LLMs and different versions might produce slightly different responses; however, we used a single version of ChatGPT. Finally, the preliminary results presented here lead us to propose that the use of LLMs to generate defeaters merits further work; however, generalizability awaits larger studies with improved LLMs."}, {"title": "4 RESULTS", "content": "In this section, we present the key findings of our experiments, grouped by our research questions."}, {"title": "4.1 (RQ1): Effectiveness in Identifying Defeaters", "content": "Finding 1: The LLM displayed promising zero-shot capabilities for defeater analysis in assurance cases."}, {"title": "4.2 (RQ2): Utility in Generating Novel Defeaters", "content": "To answer RQ2, we evaluated the LLM's performance on the sUAS battery's assurance case, where we have the necessary domain knowledge. Using the same prompting method, we iteratively requested additional defeaters to assess its capacity to generate novel"}, {"title": "5 DISCUSSION", "content": "Based on our findings, we highlight several challenges and opportunities for an LLM-based process to help find defeaters.\nDesigning better prompts. Prompt designing has been shown to significantly impact LLM performance [14, 41]. Many prompting methods have been proposed, and further investigation for suitability to defeater analysis is needed. Moreover, our study revealed that the LLM can generate creative, redundant, and far-fetched scenarios (e.g., defeaters due to budget constraints). Balancing LLM creativity with defeater relevance poses an important challenge.\nRationale behind defeaters. In our experiments, we found that the LLM responses not only identified defeaters but also provided helpful rationale and examples. For instance, if the ground truth defeater stated, \"Unless there are incorrect readings,\" the LLM suggested, \"The sensors may not be properly calibrated, leading to inaccurate readings.\" These explanations can assist analysts in understanding and analyzing both a defeater's feasibility and its potential mitigations. Investigating explainable prompting techniques such as Chain-Of-Thought [14] is an important next step.\nTowards incremental assurance using LLMs. Our study focused on single claims and associated defeaters. Future research should evaluate the performance of LLMs on a combination of claims. It will be interesting to investigate whether LLMs can identify the impact of defeaters on multiple claims and assess if the provided evidence adequately addresses them. This direction will require developing detailed data for evidence analysis and exploring prompts specifically designed for this purpose."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "We have presented CoDefeater, a process for automated defeater discovery in assurance cases using LLMs (GPT-3.5). Our evaluation on two real-world case studies demonstrated the LLM's zero-shot capabilities in identifying defeaters and its potential to support practitioners in an iterative human-in-the-loop process. We make available the portion of a new assurance case and its ground-truth defeaters used in our experiments for other researchers. Future work will expand beyond the zero-shot setting to explore one-shot"}]}