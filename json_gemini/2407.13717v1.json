{"title": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases", "authors": ["Usman Gohar", "Robyn R. Lutz", "Michael C. Hunter", "Myra B. Cohen"], "abstract": "Constructing assurance cases is a widely used, and sometimes required, process toward demonstrating that safety-critical systems will operate safely in their planned environment. To mitigate the risk of errors and missing edge cases, the concept of defeaters arguments or evidence that challenge claims in an assurance case has been introduced. Defeaters can provide timely detection of weaknesses in the arguments, prompting further investigation and timely mitigations. However, capturing defeaters relies on expert judgment, experience, and creativity and must be done iteratively due to evolving requirements and regulations. This new ideas paper proposes CoDefeater, an automated process to leverage large language models (LLMs) for finding defeaters. Initial results on two systems show that LLMs can efficiently find known and unforeseen feasible defeaters to support safety analysts in enhancing the completeness and confidence of assurance cases.", "sections": [{"title": "1 INTRODUCTION", "content": "Safety-critical systems have become deeply integrated into many societal domains, including healthcare, transportation, energy, and aviation [21, 32, 46]. Failures in these systems can lead to catastrophic consequences for human safety, including fatalities and environmental and property damage [59]. This has led to an increased focus on their dependence, reliability, and safety [38, 57]. Many systems must comply with regulations [9, 54], provide evidence of safety, and undergo rigorous certification processes [35, 53] for approval from regulatory bodies. Assurance cases (ACs) have emerged as a common practice for this purpose, facilitating the verification of system correctness and the validation of specific claims regarding safety, security, and trustworthiness, among others [5, 7, 45].\nAn assurance case is a structured hierarchy of claims and arguments supported by evidence that a system will function as intended in a specified environment [7, 60]. Several formal notations (e.g., Goal Structuring Notation (GSN) [36], Claims-Arguments-Evidence (CAE) [1], and Eliminative Argumentation (EA) [28]), along with tools [20, 45, 64], have been proposed. However, concerns arise over their completeness, uncertainty, and soundness for cyber-physical systems. [24, 30], leading to false confidence and catastrophic failures [63]. For example, failure of the minimum safe altitude warning system that led to a major aviation accident was attributed to incomplete and flawed reasoning in the safety case [31].\nTo enhance the robustness of assurance cases, it is critical to identify and mitigate their defeaters (also known as assurance weakeners). Defeaters highlight gaps in evidence or reasoning that undermine the validity of claims in the assurance case [33]. An example of a defeater, drawn from the assurance case for the safe operation of an sUAS (small Uncrewed Aircraft System) battery, challenges the assurance case's claim that \"The sUAS has enough battery charge to complete its mission.\" The defeater casting doubt on this claim is, \"Unless the battery monitor is not calibrated/inaccurate.\" The defeater serves to record, within the assurance case itself, the analyst's challenge to the validity of the claim.\nVarious approaches have been proposed to identify and mitigate defeaters [29, 37]. However, manually creating defeaters is a labor-intensive and time-consuming process [48, 51], relying heavily on safety analysts' judgment, experience, creativity, and understanding of the system. This can lead to confirmation bias [6, 64]. As assurance cases evolve with new standards and technological advances, ongoing efforts are focused on formal and semi-automated approaches for detecting and managing defeaters [22, 40]. Best practices build assurance cases incrementally, so automating all/or part of this process is important [22, 67].\nLarge Language Models (LLMs) are increasingly automating software engineering tasks like test generation and defect detection [2, 34]. In particular, these models have become valuable in tasks requiring complex understanding, such as vulnerability detection, requirements elicitation, and code generation [4, 16, 68]. Moreover, LLMs have shown promise in automating evaluation tasks and acting as surrogate evaluators [17, 69]. Consequently, we explore whether LLMs' capabilities can be harnessed to automate defeater analysis towards the completeness, soundness, and confidence of assurance cases. Despite calls for more research into LLMs' ability to identify defeaters [61, 65], no study has been conducted to evaluate and investigate their effectiveness.\nThis new ideas paper presents the first empirical investigation of the feasibility and utility of LLMs for identifying defeaters, using a process we call CoDefeater. It evaluates their potential to aid practitioners and safety analysts in the iterative human-in-the-loop process of finding defeaters, as shown in Figure 1. We evaluate the performance of an LLM (ChatGPT) in automated defeater analysis on two complex real-world case studies. Our experimental results suggest that CoDefeater is a promising approach for identifying and generating novel assurance case defeaters. Overall, this work makes three key contributions. 1) To the best of our knowledge, we provide the first empirical results from an investigation of the effectiveness and usefulness of an LLM (GPT 3.5) in identifying and creating defeaters for real-world assurance cases. 2) We provide a new assurance case fragment with defeaters that can be leveraged for further research on automated defeater identification techniques. 3). Based on our findings, we outline current challenges and directions for future work. All experimental artifacts are available here: https://gitlab.com/anonymousdot/codefeater."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Assurance case arguments typically adopt an inductive approach, where sub-claims offer direct evidence to support the parent claim but do not ensure it with certainty [7, 12]. Focusing solely on proving a claim may introduce confirmation bias, as exemplified by the Nimrod aircraft crash [11]. Recent approaches have therefore embraced defeasible reasoning, which acknowledges that arguments about system properties in practice are inherently defeasible [27, 42, 56]. Defeaters are potential doubts or objections that challenge the validity of a claim, reflecting gaps in evidence and reasoning [24, 27, 43, 50].  Defeaters are typically represented using EA notation [28] but have also been integrated into GSN and CAE notations [7, 33]. Hence, this study does not aim to evaluate LLMs' performance in identifying defeaters in any specific notation or semantic accuracy but rather to investigate the feasibility of this approach. Finally, the indefeasibility criterion requires a thorough search for defeaters in an assurance case [7], motivating our investigation into LLMs' potential to assist practitioners and safety analysts in identifying and generating novel defeaters.\nIn software engineering, LLMs are increasingly employed to assist developers and automate tasks such as discovering requirements, code generation, testing, and program synthesis [4, 34]. Diemert and Weber [23] have reported ChatGPT's effectiveness in hazard analysis for safety-critical systems, highlighting their potential to assist human analysts. In the context of software assurance cases, Sivakumar et al. [62] assessed LLM's (GPT-4) proficiency in understanding GSN representations and its performance in constructing safety cases. Viger et al. [65] proposed using LLMs to identify defeaters; however, they did not report an empirical evaluation of their capabilities. Shahandashti et al. [61] explored LLM's (GPT-4) understanding of EA notation and defeater concepts, with empirical validation left as future work. We aim to address this gap by examining LLM performance in identifying defeaters for assurance cases in two case studies.\nThe Machine Learning (ML) community also has explored the reasoning abilities of LLMs [19, 66, 70]. This line of inquiry investigates how LLMs fare in open-ended tasks and their effectiveness in assisting humans. Findings have indicated that LLMs can demonstrate consistent responses exhibiting similarities with human evaluations, suggesting their potential as automated tools [17]."}, {"title": "3 METHODOLOGY", "content": "We conducted a preliminary exploratory study toward answering the following two research questions:\n\u2022 RQ1: (Effectiveness). How effective are LLMs in identifying and analyzing defeaters in assurance cases?\n\u2022 RQ2: (Utility). Can LLMs support practitioners in generating novel and meaningful defeaters?"}, {"title": "3.1 Experimental Setup", "content": "Datasets. We performed our experiments on two assurance cases. The first is for the CERN Large Hadron Collider (LHC) Machine Protection System (MPS), which provides assurance that the MPS will prevent damage to the LHC from unstable, high-energy particle beams [49, 58]. The assurance case for the CERN LHC uses EA notation, and we extracted claim nodes with their corresponding defeaters for our experiments. The second assurance case is a fragment of a larger one that our team has recently created for small Uncrewed Aircraft Systems (sUAS)\u00b9 using GSN. It addresses the claim \"the sUAS has enough charge in its battery to complete the mission\" (see claim 1.2 in Figure 2). We include both the assurance case fragment and corresponding defeaters in the supplementary material\u00b2. The real-world complexity of these systems and the availability of defeaters made them suitable for our preliminary experiments."}, {"title": "3.2 Prompt Design", "content": "An effective prompt design is crucial for achieving good performance, as the choice of prompts significantly impacts the quality, relevance, and accuracy of the LLM's response [14]. It involves crafting a system prompt to establish the context and prepare the LLM for the task, along with a user prompt that contains the specific task request [52]. We designed the system prompt following OpenAI's best practices [52] and relevant literature in software engineering [13, 71] and open-ended evaluation tasks [17]. Our process identified role-based system prompts [41] as the most effective approach. \nSeveral user prompting techniques have been proposed, e.g., zero-shot, one-shot, few-shot, and chain-of-thought [14]. Zero-shot learning involves providing the model with only the task description (system prompt), without examples of unseen tasks to learn from. In contrast, one-shot and few-shot learning conditions the model on one or more examples in the prompt, respectively [10]. For our preliminary study, we adopted the zero-shot learning setting. This approach both (1) facilitates the immediate, off-the-shelf application of LLMs, eliminating the need for computationally expensive fine-tuning procedures, and (2) is naturally suited to scenarios such as ours, with limited data availability for training or fine-tuning [39]. Each prompt was presented independently to the model to avoid influencing subsequent responses, allowing us to assess its standalone capabilities [15]."}, {"title": "3.3 Evaluation Criteria", "content": "Due to the complexity and open-ended nature of the task (e.g., varying response length vs. ground truth, and subjectivity), automatic evaluation metrics were not suitable [17]. Therefore, we relied on human evaluation for assessing LLM performance. For RQ1, we used a deductive coding approach [23, 44], where the first two authors independently reviewed responses and categorized them as complete match, partial match, or no match based on similarity to ground-truth defeaters. We used the defeaters in the LHC assurance case as the ground truth. For the battery assurance case, a set of ground-truth defeaters was provided by one of the authors (independently) familiar with the domain, following best practices [49].\nFor RQ2, the responses were evaluated for being reasonable [13], i.e., the defeater could reasonably be in the ground truth but had been overlooked. This aimed to assess the LLM's capability to identify novel defeaters. \nNext, the reviewers met to discuss and finalize their assigned codes. In the case of post-discussion disagreement, if one reviewer labeled a response as a partial match and the other as a complete match, we categorized it as a partial match to avoid confirmation bias [25]. In the one instance where one reviewer indicated no match while the other identified a partial or complete match, it was discarded. Last, we calculated inter-rater agreement using Cohen's Kappa [18] to evaluate the consistency and reliability of the coding process."}, {"title": "3.4 Threats to Validity", "content": "There is potential subjectivity in the qualitative evaluation of LLM performance on defeater identification. To address that, two authors independently coded the LLM responses, following best practices [8, 26], and held multiple discussions to avoid misinterpretations. We also computed Cohen's kappa [18], which indicated substantial inter-reviewer agreement. To avoid confirmation bias, disagreements were coded as partial or no match. The non-deterministic nature of LLMs and different versions might produce slightly different responses; however, we used a single version of ChatGPT. Finally, the preliminary results presented here lead us to propose that the use of LLMs to generate defeaters merits further work; however, generalizability awaits larger studies with improved LLMs."}, {"title": "4 RESULTS", "content": "In this section, we present the key findings of our experiments, grouped by our research questions."}, {"title": "4.1 (RQ1): Effectiveness in Identifying Defeaters", "content": "Finding 1: The LLM displayed promising zero-shot capabilities for defeater analysis in assurance cases."}, {"title": "Finding 2: The LLM struggled with defeaters that challenged implicit assumptions.", "content": "We conducted a manual analysis of the unidentified defeaters (n = 17) to investigate whether there were any patterns behind the LLM's failure. Interestingly, we found that the model struggled to identify those defeaters that implicitly challenged the truth of an assumption. For example, for the claim \"The BICs will not produce a FALSE BEAM-PERMIT to trigger a beam dump, unless a loss of the high-frequency signal (10Mhz) in either Beam Permit Loop (A and B) is detected...\", a (ground-truth) defeater in the LHC assurance case questions the assumption that the 10Mhz signal is the right signal to monitor. In other words, the presence of another similar high-frequency signal might lead to a false indication of TRUE BEAM-PERMIT. Unlike the analysts, the LLM did not question the underlying assumption in the claim and thus did not identify the defeater. Consequently, future work should explore integrating external knowledge sources since, in similar tasks, it significantly enhances LLM's performance [55, 71]."}, {"title": "4.2 (RQ2): Utility in Generating Novel Defeaters", "content": "To answer RQ2, we evaluated the LLM's performance on the sUAS battery's assurance case, where we have the necessary domain knowledge. Using the same prompting method, we iteratively requested additional defeaters to assess its capacity to generate novel defeaters beyond the ones that had been identified in the building of the assurance case."}, {"title": "Finding 3: LLMs can support practitioners in providing useful and novel defeaters.", "content": "The LLM output was a useful source of five novel defeaters, each of which was feasible upon further investigation using our evaluation criteria. These were: (1) an unexpected power drain due to an onboard component failure; (2) an emergency external to the sUAS that forced the sUAS into a longer flight; (3) a missed waypoint to which the pilot had to return; (4) unexpected power drain arising from ongoing efforts to recover a lost GPS; and (5) external interference that the sUAS had to dodge repeatedly. The last one was interesting to us because the LLM gave as an example of interference that birds might attack the sUAS. This, in fact, happens quite often and is dangerous [3]. The response shows how an LLM can offer a creative perspective that catches missing edge cases."}, {"title": "5 DISCUSSION", "content": "Based on our findings, we highlight several challenges and opportunities for an LLM-based process to help find defeaters.\nDesigning better prompts. Prompt designing has been shown to significantly impact LLM performance [14, 41]. Many prompting methods have been proposed, and further investigation for suitability to defeater analysis is needed. Moreover, our study revealed that the LLM can generate creative, redundant, and far-fetched scenarios (e.g., defeaters due to budget constraints). Balancing LLM creativity with defeater relevance poses an important challenge.\nRationale behind defeaters. In our experiments, we found that the LLM responses not only identified defeaters but also provided helpful rationale and examples. For instance, if the ground truth defeater stated, \"Unless there are incorrect readings,\" the LLM suggested, \"The sensors may not be properly calibrated, leading to inaccurate readings.\" These explanations can assist analysts in understanding and analyzing both a defeater's feasibility and its potential mitigations. Investigating explainable prompting techniques such as Chain-Of-Thought [14] is an important next step.\nTowards incremental assurance using LLMs. Our study focused on single claims and associated defeaters. Future research should evaluate the performance of LLMs on a combination of claims. It will be interesting to investigate whether LLMs can identify the impact of defeaters on multiple claims and assess if the provided evidence adequately addresses them. This direction will require developing detailed data for evidence analysis and exploring prompts specifically designed for this purpose."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "We have presented CoDefeater, a process for automated defeater discovery in assurance cases using LLMs (GPT-3.5). Our evaluation on two real-world case studies demonstrated the LLM's zero-shot capabilities in identifying defeaters and its potential to support practitioners in an iterative human-in-the-loop process. We make available the portion of a new assurance case and its ground-truth defeaters used in our experiments for other researchers. Future work will expand beyond the zero-shot setting to explore one-shot and few-shot learning approaches for improved performance. Additionally, fine-tuning LLMs on assurance cases presents an avenue to improve their performance. Our study provides preliminary results as a starting point for future research to explore the role of LLMs as a tool to assist with the identification of defeaters toward the development of improved assurance cases."}]}