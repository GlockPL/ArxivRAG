{"title": "ACTIVATION-INFORMED MERGING OF LARGE LANGUAGE MODELS", "authors": ["Amin Heyrani Nobari", "Kaveh Alim", "Ali ArjomandBigdeli", "Akash Srivastava", "Faez Ahmed", "Navid Azizan"], "abstract": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40% increase in benchmark performance.", "sections": [{"title": "Introduction", "content": "Foundation models are rapidly becoming the dominant force for building Artificial Intelligence (AI) systems. In many cases, researchers build their machine learning models by starting from pre-trained foundation models and fine-tuning (FT) these pre-trained models for some desired target task Pareja et al. (2024). In such a paradigm, numerous fine-tuned models are developed for various tasks. However, an important opportunity is missed, as these fine-tuned task-specialized models typically operate in isolation without leveraging the rich features that each possesses Sudalairaj et al. (2024). This fact highlights the importance of a growing area of research focused on combining multiple task-specialized models fine-tuned from the same base foundation model.\nIn particular, as large language models (LLMs) continue to evolve, it becomes increasingly important to develop methods that can effectively fuse the specialized knowledge of various fine-tuned models derived from the same foundation model. Model merging has shown broad applications, including enhancing accuracy and robustness Wortsman et al. (2022), improving generalization Ram\u00e9 et al. (2023), multi-modal models Sung et al. (2023), and model alignment to human feedback Rame et al. (2024); Ram\u00e9 et al. (2024). Given these benefits, a substantial amount of attention has been devoted to developing more effective merging algorithms for LLMs.\nIn the vast majority of cases, merging LLMs is done using algorithms that explore the weight space of models and do not leverage the information in the activation space. Activation space information has been widely used to develop model pruning and compression methods, both in the context of general deep learning methods Frantar & Alistarh (2022), and more specifically for large language models Lin et al. (2024). However, this direction has remained under-explored"}, {"title": "Background & Related Work", "content": "LLM Merging. Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Often, fine-tuned models are readily available, but their training data is not due to data privacy or intellectual property concerns Dodge et al. (2020). With many fine-tuned checkpoints from the same pre-trained model, LLM merging has emerged as a complementary approach to fine-tuning, combining multiple task-specialized models into a unified architecture. This technique offers several advantages: it reduces storage and inference costs by consolidating expertise into a single model, facilitates compositional generalization by integrating specialized capabilities from different fine-tuned models, and supports decentralized model development by allowing independently trained models to be merged efficiently Yadav et al. (2024).\nModel soup, proposed by Wortsman et al. (2022), demonstrates even simple averaging of the weights of multiple fine-tuned models can enhance accuracy and robustness in image classification and natural language processing applications. Further, Ram\u00e9 et al. (2023) show that model merging can improve out-of-distribution generalization performance. Extending this approach beyond unimodal settings, Sung et al. (2023) empirically demonstrate that model merging"}, {"title": "Methodology: Activation-Informed Merging", "content": "As discussed in Section 2, most existing approaches for merging FT LLMs primarily focus on the weight space of the models being merged. However, it is well established that the activation space of these models contains crucial insights into the degree of importance of different parameters of LLMs. This was shown to be the case, for instance, in the work done by Lin et al. (2024) on Activation-aware Weight Quantization (AWQ), outperforming traditional quantization"}, {"title": "The Merging Problem and Connections to Continual Learning", "content": "Consider the merging of N models with parameters $\\theta_1, \\theta_2,\\cdots, \\theta_N$ fine-tuned on different tasks from a common pre-trained model with parameters $\\theta_{pre}$. For each fine-tuned LLM, we are essentially creating experts on specific tasks that move away from the generalist pre-trained model, hence usually degrading performance in some tasks while improving performance on the task for which the model is fine-tuned. In this sense, each FT model with parameters $\\theta_n$ can be seen as a model fitted to a new task $D_n = \\{X_n, Y_n \\}$ in a continual learning scenario with the potential for catastrophic forgetting on the generalist pre-trained model, which may not perform as well on the specific task but will have a more balanced performance across various tasks. As such, we hypothesize that when merging FT LLMs adapted from the same base model, emphasis on the base model can build better robustness to large performance degradation across numerous tasks while still allowing capturing each FT expert's capabilities. AIM seeks to achieve this by relaxing the changes to the salient weights of the base model in the final merged model. In this way, AIM is analogous to weight regularization in many continual learning approaches Wang et al. (2024b); Ritter et al. (2018); Kirkpatrick et al. (2017); Aljundi et al. (2018); Shi et al. (2024). Notably, the saliency of weights is determined by analyzing the activation space of the base model rather than just regularizing based on the weight space."}, {"title": "Activation Space Analysis", "content": "AIM determines the saliency of a model's weights by looking at the scale of activations by passing a calibration dataset to the model and recording the scale of activations in each channel. To better understand why, we will analyze how the perturbation of weights of a given model affects the model outputs. Let the original weights be $w \\in \\mathbb{R}^{N\\times M}$ and the perturbation be $\\delta\\omega \\in \\mathbb{R}^{N\\times M}$, such that the perturbed weights are:\n$w' = \\omega + \\delta\\omega$ (1)\nThe output of a linear layer with input $x \\in \\mathbb{R}^{N}$ and perturbed weights is:\n$y' = w'x = x(w + \\delta\\omega) = xw + x(\\delta\\omega)$ (2)\nThe error due to the perturbation is:\n$Error = y' - y = x(\\delta\\omega)$ (3)\nAnd the magnitude of the error scales with the magnitude of activation $x$:\n$|| Error||_p = ||x\\delta\\omega||_p \\leq ||x\\delta\\omega||_1 \\leq (\\sum_{i=1}^{N} |x_i|) (\\sum_{j=1}^{M} |\\delta\\omega_j|)$ (4)\nFor any specific input channel $x_i$, the error contribution from perturbation in the i-th row of w, $\\delta w_i$ is amplified by the magnitude of the same channel in the input. As such, one could selectively regularize the weights based on the importance of the input channels, i.e., their magnitudes. In this way, we use a calibration dataset to capture the average magnitude of the input channels for each layer of the base model and determine the saliency of weights in the base model from the activation space. We choose the calibration dataset to be a subset of the validation data from the pile dataset Gao et al. (2020) which is similar in distribution to most pre-training data and is the choice of calibration data for AWQ Lin et al. (2024) as well (based on the latest implementation on GitHub at the time of writing). This calibration dataset is considered to be fairly diverse and not task-specific, while also being similar to most pre-training data, which should allow for capturing a representative set of activation magnitudes, and hence salient weights. Using this data to capture the average magnitudes of the input activation, we come up with a complementary solution to apply relaxation to weights of merged models using any merging method and show that our adaptable relaxation scheme helps improve the performance of merged models significantly."}, {"title": "Adaptable Relaxation Scheme", "content": "As discussed, we introduce an adaptable relaxation scheme based on the activations of the base model, which we wish not to stray away from significantly. To make the scheme adaptable to any merging algorithm in the weight"}, {"title": "Experimental Setup and Evaluation Metrics", "content": "We conduct two separate experiments with AIM: 1) we apply AIM to 5 different merging methods including the two latest works on the topic with different numbers of experts being merged and report the performance of the models on 6 different benchmarks; 2) we conduct an ablation study on the w parameter in AIM and analyze how w affects each of the merging methods in a scenario where 3 different experts are being merged."}, {"title": "Selection of FT Expert LLMs", "content": "To understand how AIM reacts with different merging methods, we conduct experiments with merging different experts fine-tuned from the same base model. The set of experts we use is the same set of experts used by the two latest LLM merging algorithms in the literature, namely DARE Yu et al. (2024a) and WIDEN Yu et al. (2024b), which use the same three experts fine-tuned from Llama-2-13b Touvron et al. (2023). These experts include the WizardLM-13B Xu et al. (2024) model fine-tuned for instruction following, WizardMath-13B Luo et al. (2025) fine-tuned for superior mathematical reasoning, and llama-2-13b-code-alpaca which serves as the code expert Chaudhary (2023).\nA Note On Weights: The weights we use in our experiments may not be exactly identical to the weights used in the experiments by Yu et al. (2024a) and Yu et al. (2024b), since the referenced weights for WizardMath-13B are no longer available publicly, instead we use a publicly available copy of the model. See Appendix A for more details."}, {"title": "Merging Methods Implementations", "content": "In our experiments, we implement the latest merging methods in the literature for LLM merging. These include newly developed DARE and WIDEN Yu et al. (2024a,b) methods as well as some of the long-established approaches of TIES merging Yadav et al. (2023), and task arithmetic Ilharco et al. (2023). For all merging methods except WIDEN, we use the comprehensive MergeKit implementations developed by Goddard et al. (2024), and for WIDEN we use the publicly available implementation provided by the authors of the paper.\nWe note that in many of the merging algorithms, many hyper-parameters can be adjusted. In these cases, we use the author-recommended values where available and the default parameters recommended by Goddard et al. (2024). Note that it is possible to perform a grid search on these hyper-parameters to find optimal values for each benchmark, however, this would essentially be over-fitting on benchmarks and does not provide any value to our analysis of the proposed complementary relaxation scheme which applies adjustments to the merged models. For reproducibility, all of our checkpoints and code to reproduce the results will be made publicly available."}, {"title": "Benchmarks Used For Evaluations", "content": "Given that the expert models we use in our experiments involve fine-tuning on instruction following, mathematical reasoning, and code generation we use several common benchmarks for each of these tasks. Specifically, we measure model performance on language understanding with the MMLU Hendrycks et al. (2021a) benchmark, instruction following with IFEval Zhou et al. (2023) benchmark, code generation with HumanEval Chen et al. (2021) and MBPP Austin et al. (2021) benchmarks, and mathematical reasoning with the MATH Hendrycks et al. (2021b) and GSM8K Cobbe et al. (2021) benchmarks. For all benchmarks, we use the latest versions and up-to-date implementations developed by Gao et al. (2024) except for mathematical reasoning for which we use the chain of thought prompting used by Luo et al. (2025) to replicate the results of the original model as closely as possible. The code we use for these benchmark results will also be made publicly available for reproducibility.\nIn addition to the common benchmarks that we use to evaluate merged models, we also propose a new evaluation metric for LLM merging (or merging of different experts in general) which we believe helps better contextualize the value added by any given merging algorithm which we discuss in the following section."}, {"title": "Measuring Performance From an Optimization Perspective", "content": "We note that in most cases LLMs are not meant to operate as narrow expert models, unlike a large portion of deep learning applications where models are trained to perform very specific tasks such as classification or regression. LLMs in contrast, are generalist language models aiming to assist across a wide variety of tasks and applications. As such LLMs can be viewed from a multi-objective optimization perspective. In merging scenarios specifically, multiple expert models are brought together to create a merged model aiming to find the balance of performance across the different expertise of the fine-tuned models. In this sense, each expert can be thought of as optimized for a specific objective. This perspective lends itself rather well to a multi-objective optimization view of the problem. Given this, only looking at how each model performs on each of the benchmarks does not give us a full picture of the multi-objective goal of merging.\nTo obtain a more comprehensive view of merging performance, we propose a hypervolume-based metric that quantifies the contribution of the merged model to the multi-objective frontier of FT LLMs. Consider a performance space defined over N benchmarks, where each model's performance is represented as a point in an N-dimensional space. The performance on each benchmark is normalized to the range [0, 1], where 0 represents the worst-case (reference point), and 1 represents the best possible performance with 100% accuracy on the benchmark in question.\nLet r = (r1,2,...,rn) denote the reference point in this space, which we set to (0, 0, . . ., 0) to ensure hypervolume calculations are consistently defined. Given a set S of FT models and the pre-trained base model, let S* C S denote the subset of models that are Pareto-optimal, i.e., models that are not dominated by any other model in S. The hypervolume of this set, denoted as HV (S*), is defined as:\n$HV(S^*) = \\lambda \\Big(\\bigcup_{x \\in S^*} dom(x,r)\\Big)$ (7)\nwhere $\\lambda(\u00b7)$ denotes the Lebesgue measure (i.e., volume in $\\mathbb{R}^N$), and $dom(x, r)$ represents the hypervolume dominated by x with respect to the reference point r.\nWhen a merged model M is introduced, the new set becomes S' = S\u222a {M}, and the updated Pareto-optimal set is denoted as S'*. Given this set including the merged model we can measure the added value of the merged model from a multi-objective perspective as the normalized hypervolume gain (HV Gain) as a result of adding this merged model:\n$HV Gain = \\sqrt[d]{HV(S'^*) \u2013 HV(S^*)}$ (8)\nWhere d is the number of dimensions/benchmarks. Since hypervolume is computed only over Pareto-optimal models,\nwe have HV(S'*) > HV(S*), ensuring that HV Gain > 0. This metric provides an aggregated measure of merging effectiveness, capturing trade-offs across multiple benchmarks rather than focusing on isolated improvements thus providing a full picture of merging performance. In our experiments, we track HV Gain along with the 6 aforementioned benchmarks as described here."}, {"title": "AIM Applied to Various Merging Approaches", "content": "To demonstrate the effectiveness of AIM, we conduct experiments on 5 different merging methods under 4 different scenarios. As mentioned before we use 3 different FT LLM experts in our experiments. As such we merge models using each merging method for all 4 possible permutations of these expert LLMs. Then we apply AIM to all merged models and measure the performance of each model in all 6 benchmarks. We also report the HV gain for each merged model compared to the population of the base model and the models being merged (in cases with 2 models the population will only include the models used for merging). These results are presented in Table 1. For this experiment, we used w = 0.4, which we found to be the best balance of performance among the various merging methods we use. This choice was informed by our analysis in Section 5.2. In Table 1 we have highlighted the gain/loss of performance for each benchmark due to AIM and we can see that in the vast majority of cases, AIM causes a significant performance boost, with an Average Percentage-Point Change of 13% (ignoring the Inf value) and more than 40% HV Gain in 20% cases, further highlighted by the fact that the top performers on each benchmark, as well as the largest hypervolume gain, are all in models merged with AIM. We observe HumanEval (10 out of 20) and MBPP (17 out of 20) often see large boosts with AIM, especially when merging Instruction Tuned models with others. Some merges also reveal small drops in GSM8K or IFEval even when other benchmarks improve, reflecting the inherent trade-offs in merging specialized models. Overall, a clear majority (80%) of merges exhibit improved HV Gain under AIM, reinforcing that the method often enhances multi-task performance overall. We can further visualize this increase in hypervolume by looking at how AIM pushes the Pareto frontier. Figure 2 shows how applying AIM to existing merging methods extends the Pareto optimal frontier, which we also quantitatively measured using HV gain. These results showcase the efficacy of the proposed method across a variety of merging methods and reinforce the hypothesis that the activation space encompasses useful insight for merging."}, {"title": "Ablation study", "content": "To understand the effects of changing w in AIM, we conduct an ablation study on the case of merging all three expert LLMs. For this study, we apply AIM with w \u2208 {0.0, 0.2, 0.4, 0.6, 0.8} and run the benchmarks on each merged model with each value of w. For brevity we do not report all benchmark results for each value here, instead, we track the hypervolume gain (The full set of results are presented in Appendix B). Specifically, to better visualize the effect of w we measure the relative change in HV Gain compared to no AIM (i.e. w = 1.0). We present these results in Figure 3. In most merging methods, we see that decreasing w to even 0 still benefits the model performance while, in some cases, still increases the performance. However, in TIES merging particularly, we see that decreasing w beyond 0.4 seems to degrade performance, and setting w to the most extreme case of 0.0 does see some degradation in WIDEN as well. Given this, it seems that in these experiments, a value of 0.4 balances the performance gains in methods responding"}, {"title": "Conclusion and Outlook", "content": "In this work, we introduced Activation-Informed Merging (AIM) as a complementary algorithm to existing model merging techniques for large language models (LLMs). We hypothesized that the activation space of LLMs harbors useful information that is often overlooked in model merging, as most existing methods operate purely on the weight space. To explore this potential information in the activation space, we viewed the problem from a continual learning perspective and proposed leveraging the activation space information from a task-agnostic calibration set. This approach selectively preserves critical weights from the pre-trained model, mitigating catastrophic forgetting while incorporating knowledge from fine-tuned models, yielding overall higher-performing models.\nThrough extensive empirical evaluations across multiple merging methods and benchmark tasks, we demonstrated that AIM consistently improves performance, often yielding superior results in comparison to the original merging methods it was applied to. These results empirically confirm our hypothesis on the importance of the activation space. Notably, AIM boosted merged model performance by up to 40% in some cases, underscoring the crucial rule and the potential of activation information in merging methods. Our findings highlight the necessity of incorporating activation-informed strategies when merging multiple fine-tuned models.\nMoving forward, our findings open up several promising directions for future research. First, our results indicate that even aggressively preserving salient weights of the pre-trained model is effective across many merging scenarios. This highlights the promise for more advanced activation-informed strategies and non-linear relaxation methods to enhance performance potentially further. Beyond the pre-trained activations explored in this work, there is room to improve existing merging methods by leveraging the broader activation space of the models being merged. So far, AIM has"}]}