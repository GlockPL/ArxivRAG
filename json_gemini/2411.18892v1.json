{"title": "Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges", "authors": ["Majid Ghasemi", "Amir Hossein Moosavi", "Dariush Ebrahimi"], "abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) is a subfield of Artificial Intelligence (AI) in which an agent learns to make decisions by interacting with an environment, aiming to maximize cumulative reward over time [1]. RL has rapidly evolved since the 1950s, when Richard Bellman's work on Dynamic Programming (DP) established foundational concepts that underpin current approaches [2], [3]. The field gradually became more widespread by proposing more advanced approaches, such as Temporal-Difference (TD) Learning [4], [5] and suggesting solutions to exploration-exploitation dilemma [6], [7]. RL has further been evolving rapidly due to its integration with Deep Learning (DL), giving rise to Deep Reinforcement Learning (DRL). This advancement enables researchers to tackle more sophisticated and complex problems [8], [9]. It has proven to be highly effective in solving sequential decision-making problems in a variety of fields, such as game playing ([10], [11], [12], [13]), robotics ([14], [15], [16], [17]), and autonomous systems, particularly in Intelligent Transportation Systems (ITS) ([18], [19], [20], [21], [22]).\nThis survey examines the practical application of different RL approaches through various domains including but not limited to: robotics [14], [23], [24], optimization [25], [26], energy efficiency and power management [27], [28], [29], networks [30], [31], [32], dynamic and partially observable environments [33], [34], [35], [36], video games [37], real-time systems and hardware implementations [38], [39], financial portfolios [40], ITS [18], [41], [42], signal processing [43], benchmark tasks [44], data management and processing [45], [46], multi-agent and cloud-based systems [47], [48], [49], [50]. Moreover, our survey gives detailed explanations of RL algorithms, ranging from traditional tabular methods to state-of-the-art methods.\nRelated Surveys: Several notable survey papers have examined different aspects of RL or attempted to teach various concepts to readers. Foundational work, such as [6], offers an in-depth computer science perspective on RL, encompassing both its historical roots and modern developments. In the realm of DRL, [51], [52], [53], [54] offer comprehensive analyses of DRL's integration with DL, highlighting its applications and theoretical advancements. Model-based RL is identified as a promising area in RL, with authors in [55], [56], [57] emphasizing its potential to enhance sample efficiency by simulating trial-and-error learning in predictive models, reducing the need for costly real-world interactions.\nIn [58], authors provided a survey of Model-free RL specifically within the financial portfolio management domain, exploring its applications and effectiveness. Meanwhile, [59] analyzes the combination of Model-free RL and Imitation Learning (IL), a hybrid approach known as RL from Expert Demonstrations (RLED), which leverages expert data to enhance RL performance. These survey papers help to understand the various aspects of RL and its integration with other areas.\nTo our knowledge, no papers have analyzed the strengths and weaknesses of algorithms used in papers and provide a comprehensive analysis of an entire paper. The first motivation of this survey is to address this gap.\nRL involves various challenges in choosing appropriate algorithms due to diverse factors, such as problem characteristics and environmental dynamics. In general, it depends on numerous factors based on the characteristics of the problem, including whether the state and action spaces are large, whether their values are discrete or continuous, or if the dynamics of the environment are stochastic or deterministic. Data availability and sample efficiency are other factors to consider. It is also important to consider the degree to which direct implementation can be achieved and how easily it can be debugged. In these respects, simpler algorithms may tend to be more user-friendly but cannot be applied to complex problems. Convergence and stability are important considerations, as certain algorithms provide better guarantees in specific circumstances. In conclusion, the decision-making process is influenced by exploration style, domain-specific requirements, and past research results. To determine which algorithm to use based on comparing the problem they are solving with similar existing research, researchers should have a comprehensive paper examining several papers in different domains thoroughly and accurately. Besides saving time and resources, this will also prevent the excess cost of going through a trial-and-error process to determine which solution to choose, which is the second motivation behind conducting this survey.\nThe rest of the paper is organized as follows: In II, we provide a general overview of RL before diving into algorithms. Consequently, we undertake an examination of various algorithms within the domain of RL, inclusive of the associated papers, as well as an analysis of their respective merits and drawbacks. It is imperative to acknowledge that these algorithms fall into three overarching categories: Value-based Methods, Policy-based Methods, and Actor-Critic Methods. Section III initiates the discussion by focusing on Value-based Methods, delineated by its four core components: Dynamic Programming, Tabular Model-free, Approximation Model-free, and Tabular Model-based Methods. Section IV subsequently talks about the Policy-based Methods. Furthermore, section V offers detailed insights into Actor-Critic Methods. In section VI, we give a summary of the paper and discuss the scope of it. Finally, section VII provides a synthesis of the paper through a review of key points and an exposition of future research directions."}, {"title": "II. GENERAL OVERVIEW OF RL", "content": "In this section, we give a general overview of RL, assuming readers have a basic knowledge of RL. In the framework of RL, the learning process is defined by several key components. The fundamental concept of RL is to capture the crucial elements of a real problem faced by an agent that interacts with its environment to achieve a goal. It is evident that such an agent must be capable of sensing the state of the environment to some extent and must have the ability to take actions that influence the state [1]. In general, an action refers to any decision an agent will have to make, while a state refers to any factor that the agent will have to consider when making that decision.\nBeyond the agent and the environment, an RL system has four main sub-elements: a policy, a reward signal, a value function, and, optionally, a model of the environment. The reward signal determines the agent's behavior in the given environment. During each time step, the environment sends a single number, a reward, to the agent. Ultimately, the agent's sole objective is to maximize the total reward it receives. A policy defines how a learning agent should behave at a particular point in time. In simple terms, the agent's policy is the mapping from a possible state to a potential action. The value function corresponds to the agent's current mapping from the set of possible states to its estimates of the net long-term reward it can expect once it visits a state (or state-action pair) and continues to follow its current policy. As a final point, the model of the environment simulates the behavior of the environment or, more generally, can be used to infer how the environment will behave [1], [60], [61].\nThere are two broad categories of RL methodology: Model-free and Model-based. Model-free methods do not assume knowledge of the environment's dynamics and learn directly from interactions with the environment. On the other hand, Model-based methods involve building a model of the environment's dynamics and using this model to develop and improve policies [6], [3]. Each of the mentioned categories has its own advantages and disadvantages, that will be discussed later in the paper.\nFollowing this, we will briefly examine two of the most crucial components of RL. First, we will explore the Markov Decision Process (MDP), the foundational framework that structures the learning environment and guides the agent's decision-making process. Then, we will discuss the exploration-exploitation dilemma, one of the most imperative characteristics of RL, which balances the need to gather new information with the goal of maximizing rewards."}, {"title": "A. Markov Decision Process (MDP)", "content": "The MDP is a sequential decision-making process in which the costs and transition functions are directly related to the current state and actions of the system. MDPs aim to provide the decision-maker with an optimal policy \u03c0: S \u2192 A. The models have been applied to a wide range of subjects, including queueing, inventory control, and recommender systems [62], [63], [64].\nMDP is defined by a tuple (S, A, P, R, \u03b3), where:\n\u2022 S is a finite set of states,\n\u2022 A is a finite set of actions,\n\u2022 P:S\u00d7A\u00d7S \u2192 [0,1] is the state transition probability function, where P(s'|s, a) denotes the probability of transitioning to state s' from state s by taking action a,\n\u2022 R:S\u00d7A\u2192 R is the reward function, where R(s, a) denotes the immediate reward received after taking action a in state s,\n\u2022 \u03b3\u2208 [0, 1] is the discount factor that determines the importance of future rewards.\nThe agent's behavior is defined by a policy \u03c0 : S \u2192 A, which maps states to actions. The goal of the agent is to find an optimal policy \u03c0* that maximizes the expected cumulative reward, often termed the return (Gt), which is defined as:\n$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R(S_{t+k}, a_{t+k})$ (1)"}, {"title": "B. Exploration vs Exploitation", "content": "It is important to note that exploration and exploitation represent a fundamental trade-off in RL. The objective of exploration is to discover the effects of new actions, whereas the objective of exploitation is to select actions that have been shown to yield high rewards, to the best of the knowledge of the agent at that timestep.\nExploration techniques can be classified into two: undirected and directed exploration methods. Undirected exploration methods such as Semi-uniform (e-greedy) exploration and the Boltzmann exploration try to explore the whole state-action space by assigning positive probabilities to all possible actions [1], [71]. On the other hand, directed exploration methods like the E\u00b3 algorithm and exploration bonus use the statistics obtained through past experiences to execute efficient exploration [72], [73].\nBalancing exploration and exploitation is a critical aspect of RL, and various strategies have been developed to manage this trade-off effectively. For instance, in [74], authors introduced a Model-based RL method that dynamically balances exploitation and exploration, particularly in changing environments. By using Bayesian inference with a forgetting effect, it estimates state-transition probabilities and adjusts the balance parameter based on action-outcome variations and environmental changes. Furthermore, [75] proposed Decoupled RL (DeRL) which trains separate policies for exploration and exploitation and can be applied with on-policy and off-policy RL algorithms. Using decoupling policies, DeRL improves robustness and sample efficiency in sparse reward environments. More advanced methodologies have been used in [76], where the study investigated the trade-off between exploration and exploitation in continuous-time RL using an entropy-regularized reward function. As a result, the optimal exploration-exploitation balance was achieved through a Gaussian distribution for the control policy, where exploitation was captured by the mean and exploration by the variance. Moreover, various strategies such as e-c, Upper Confidence Bound (UCB), and Thompson Sampling are employed to balance this trade-off in simpler environments, like Bandits [7], [1], [8].\nIn the subsequent sections, we will undertake an examination of various algorithms within the domain of RL, inclusive of the associated papers, as well as an analysis of their respective merits and drawbacks. It is imperative to acknowledge that these algorithms fall into three overarching categories: Value-based Methods, Policy-based Methods, and Actor-Critic Methods. Section III will initiate the discussion by focusing on Value-based Methods, delineated by its three core components: Tabular Model-free, Tabular Model-based, and Approximation Model-free Methods. Section IV will subsequently address the sole method within the Policy-based Methods umbrella, namely Approximation Model-free. Furthermore, section V will offer detailed insights into Actor-Critic-based Methods. In section VI, we give a detailed explanation of how this survey is meant to be read to get the most out of it. Finally, section VII will provide a synthesis of the paper through a review of key points."}, {"title": "III. VALUE-BASED METHODS", "content": "Value-based Methods in RL are techniques that focus on estimating the value of states or state-action pairs to guide decision-making. An essential component of the methodology is the learning of a value function, which quantifies the expected long-term reward for a given state under a given policy. Value-based Methods are ones that iteratively update their value estimates based on the observed rewards and transitions. Examples of Value-based methods algorithms include Q-learning and State-Action-Reward-State-Action (SARSA) (will be discussed briefly later in this chapter). These methods aim to derive an optimal policy by maximizing the value function, enabling the agent to choose actions that lead to the highest cumulative rewards.\nValue-based Methods are divided into two broad categories: Tabular Model-based and Tabular Model-free methods. Model-based methods refer to a group of methods that rely on an explicit or learned model of the environment's dynamics. The model predicts how the environment will respond to an agent's actions (state transitions and rewards). On the other hand, Model-free methods do not rely on a model of the environment. Instead, they directly learn a policy or value function based on interactions with the environment.\nWe will begin by introducing the main part of Tabular Model-based methods, Dynamic Programming (DP). Next, we will explore both Tabular and Approximate Model-free algorithms. Finally, we will cover the advanced part of Tabular Model-based methods, Model-based Planning."}, {"title": "A. Tabular Model-based Algorithms", "content": "In this subsection, we examine the first part of the Tabular Model-based methods in RL, Dynamic Programming methods. It must be noted that the advanced Tabular Model-based algorithms will be analyzed in section III-D along with various studies.\nA Tabular Model-based algorithm is an example of an RL technique used for solving problems with a finite and discrete state and action space. These algorithms are explicitly based on the maintenance and updating of a table or matrix, which represents the dynamic nature of the environment and its reward. 'Model-based' algorithms are characterized by the fact that they involve the construction of an environmental model for use in decision-making. Key characteristics of Tabular Model-based techniques include Model Representation, Planning and Policy Evaluation, and Value Iteration [1].\nIn Model Representation, the model depicts the dynamics in a tabular form with transition probabilities represented as P(s'|s, a) and the reward function as R(s, a). These elements define the probability of transitioning to a new state s' and the expected reward when taking action a in state s. Planning and Policy Evaluation involves approximating the updating value function V(s) iteratively using the Bellman equation until convergence. To calculate the optimal policy, the algorithm examines all possible future states and their associated actions. Value Iteration also approximates the updating value function V(s) iteratively using the Bellman equation until convergence. Similar to Planning and Policy Evaluation, it analyzes all potential future states and their linked actions to determine the optimal policy.\nTabular Model-based algorithms can be divided into two general categories: DP and Model-based Planning, where this variant will be discussed in section III-D1.\n1) Dynamic Programming (DP): DP methods are fundamental techniques used to solve MDPs when a complete model of the environment is known. These methods are iterative and make use of the Bellman equations to compute the optimal policies. Two primary DP-based methods are Policy Iteration and Value Iteration. We first start by examining Policy Iteration, then, we discuss Value Iteration.\na) Policy Iteration: Policy Iteration is a method that iteratively improves the policy until it converges to the optimal policy. It consists of two main steps: policy evaluation and policy improvement [1]. Policy Iteration consists of two different steps, Policy Evaluation and Improvement. Policy Evaluation calculates the value function V\u03c0(s) for a given policy \u03c0. This involves solving the Bellman expectation equation for the current policy:\n$V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} P(s'|s,a)[R(s,a,s')+V_{\\pi}(s')]$ (5)\nThis step iteratively updates the value of each state under the current policy until the values converge [3]. Policy Improvement, on the other hand, improves the policy by making it greedy with respect to the current value function:\n$\\pi'(s) = arg \\max_a \\sum_{s'}P(s'|s, a) [R(s, a, s') + V_{\\pi}(s')]$ (6)\nThis step updates the policy by selecting actions that maximize the expected value based on the current value function [55]. Alg. 1 gives an overview of how policy iteration can be implemented.\nValue Iteration is another approach in DP, which will be discussed in the next subsection.\nb) Value Iteration: Value Iteration is another DP method that directly computes the optimal value function by iteratively updating the value of each state. It combines the steps of policy evaluation and policy improvement into a single step. Value Iteration consists of two different steps, Value Update and Policy Extraction. Value Update updates the value function for each state based on the Bellman optimality equation:\n$V(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s, a, s') + \\gamma V(s')]$(7)"}, {"title": "B. Tabular Model-free Algorithms", "content": "Tabular Model-free algorithms are techniques suitable for problems with discrete state and action spaces that are typically small enough to be tabulated. Unlike Model-based algorithms, which require a model of the environment's dynamics, Model-free algorithms learn directly from interactions with the environment without understanding its underlying mechanics. In this context, we will explore various methods and algorithms that are categorized as Tabular Model-free algorithms, starting with MC Methods.\n1) Monte Carlo Methods: In situations where complete knowledge of the environment is unavailable or undesirable, MC methods can be employed. MC methods rely on experience, using sample sequences of states, actions, and rewards obtained through real or simulated interactions with an environment [1]. Learning from real experience is notable because MC does not require prior knowledge of the environment's dynamics, yet it can still achieve optimal behavior. Similarly, learning from simulated experience is powerful. While a model is necessary, it only needs to generate sample transitions, not the complete probability distributions of all possible transitions as required in DP. To ensure well-defined returns, MC methods are used specifically for episodic tasks, where experiences are divided into episodes, and all episodes eventually terminate regardless of the selected actions. Changes to value estimates and policies occur only at the end of an episode (sample returns). As a result, MC methods show incremental behavior on an episode-by-episode basis instead of a step-by-step (online) fashion. While the term \"Monte Carlo\" is commonly used broadly for any estimation method involving a significant random component, here it specifically refers to methods based on averaging complete returns [1].\nIn the following, we will explain how MC methods are used to learn the state-value function for a given policy \u03c0,where there are several ways of doing so.\na) Monte Carlo Estimation of State Values: A straightforward method of estimating the value of a state based on experience is to average its observed returns after visits to the state. In terms of value, the state is defined as the anticipated cumulative discounted reward in the future. The average tends to converge to the expected value as more returns are observed (the law of large numbers), which is the basis for MC.\nThe value \u03c5\u03c0(s) of a state s under policy \u03c0 can be estimated by considering a set of episodes obtained by following and passing through s. Each occurrence of state s in an episode is known as a visit. When s is visited multiple times within an episode, the first occurrence is referred to as the first visit to s. According to the First-visit MC method, \u03c5\u03c0(s) is calculated as the average of first visits to the visited states, while under the Every-visit MC method, the average is calculated based on all visits to s.\nConsider a simple 3x3 grid world where an agent starts at the top-left corner (State So) and aims to reach the bottom-right corner (Goal State G). The agent can take one of four actions in each state: up, down, left, or right. Each action transitions the agent to an adjacent state unless it moves outside the grid boundaries, in which case the agent remains in its current state. The episode ends once the agent reaches the goal state, G.\nKey Rules:\n1) States can be revisited during an episode.\n2) The return at the goal state is defined as the sum of rewards from the starting state to the goal state.\n3) Two approaches are considered for state-value estimation:\n\u2022 First-visit MC: Only the first visit to each state in an episode is used for value estimation.\n\u2022 Every-visit MC: All visits to each state in an episode are considered."}, {"title": "C. Approximation Model-free Algorithms", "content": "In this section, we analyze Approximation Model-free algorithm variations and their applications of in various domains. It must be noted we assume that readers have knowledge in DL before reading this section, and readers are referred to [133], [134], [135], [136] to understand and learn DL. Approximation Model-free algorithms in RL comprise methods oriented to learning policies and value functions solely from interaction with an environment, though without an explicitly stated model of the environment's dynamics. These algorithms typically use function approximators, such as neural networks, and generalize from observed state-action pairs to unknown ones. Consequently, they are quite effective at handling large or continuous states and actions [59], [137], [138].\nKey features of the Approximation Model-free algorithms are:\n\u2022 These algorithms learn the policy directly by optimizing expected reward without an explicit construction of the model of the environment.\n\u2022 In value-function estimation, the value function is estimated using function approximators predicting expected rewards for states or state-action pairs.\n\u2022 The solutions are scalable, thus fitting for problems with large or continuous state and action spaces since they can generalize from limited data."}, {"title": "IV. POLICY-BASED METHODS", "content": "Policy-based methods are another fundamental RL method that more strongly emphasizes direct policy optimization in the process of choosing actions for an agent. In contrast to Value-based methods, which search for the value function implicit in the task, and then derive an optimal policy, Policy-based methods directly parameterize and optimize the policy. This approach offers several advantages, particularly better dealing with very challenging environments that have high-dimensional action spaces or where policies are inherently stochastic.\nPerhaps at the core, Policy-based methods conduct their operation based on the parameterization of policies, usually denoted as \u03c0(a|s; \u03b8). Here, \u03b8 is used to denote the parameters of the policy, while s denotes the state and a denotes the action. In other words, it finds the optimal parameters \u03b8* that maximize the expected cumulative reward. Needless to say, this is generally done by gradient ascent techniques and more specifically by Policy Gradient methods that explicitly compute the gradient of expected reward with respect to the policy parameters, modifying parameters in the direction of reward increase [1], [6], [15].\nThe gradient of the policy with respect to Q values is estimated in the following way:\n$V_\\theta J(\\theta) = E_{\\pi_{\\theta}}[\\nabla_{\\theta} log \\pi_{\\theta}(a|s)Q_{\\pi_{\\theta}}(s, a)]$(31)\nHere, $Q_{\\pi_{\\theta}}(s, a)$ represents the action-value function under the current policy. This gradient can be estimated using samples from the environment, allowing the application of gradient ascent to update the policy parameters iteratively. The advantages of Policy-based Methods can be simplified to:\n\u2022 Direct Optimization of the Policy: These methods thus optimize the policy directly, hence, in contrast to the Value-based methods, working more successfully with continuous and high-dimensional action spaces. They usually work very well for problems within robotics and control where actions are naturally continuous.\n\u2022 Stochastic Policies: Policy-based methods naturally accommodate stochastic policies, which may be important in settings where exploration is necessary or the optimal policy is inherently stochastic. Stochastic policies aid in shooting up the exploration-exploitation trade-off more efficiently as well.\n\u2022 Improved Convergence Properties: Sometimes, Policy-based methods may have much smoother convergence than Value-based methods, especially when the latter are prone to a number of instabilities and divergence related to the Church condition for value function approximations.\nNow, it is time to talk about the first Policy Gradient method, REINFORCE, in the next subsection."}, {"title": "V. ACTOR-CRITIC METHODS", "content": "Actor-critic methods combine Value-based and Policy-based approaches. Essentially, these methods consist of two components: the Actor, who selects actions based on a policy, and the Critic, who evaluates the actions based on their value function. By providing feedback on the quality of the actions taken, the critic guides the actor in updating the policy directly. As a result of this synergy, learning can be more stable and efficient, addressing some limitations of pure policy or Value-based approaches [229], [52].\nIn the next subsection, we first introduce two main versions of Actor-Critic methods, Asynchronous Advantage Actor-Critic (A3C) & Advantage Actor-Critic (A2C), and then, we will analyze various applications of each.\n$A(s, a) = Q(s, a) - V(s).$ (42)\nBy using the advantage function, A2C focuses on actions that yield higher returns than the average, which helps in making more informed updates to the policy [1]. Unlike A3C, where multiple agents update the global model asynchronously, A2C synchronizes these updates. Multiple agents run in parallel environments, collecting experiences and calculating gradients, which are then aggregated and used to update the global model synchronously. This synchronization reduces the complexity of implementation and avoids issues related to asynchronous updates, such as non-deterministic behavior and potential overwriting of gradients."}]}