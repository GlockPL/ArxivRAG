{"title": "Are You Human? An Adversarial Benchmark to Expose LLMs", "authors": ["Gilad Gressel", "Rahul Pankajakshan", "Yisroel Mirsky"], "abstract": "Large Language Models (LLMs) have demonstrated an alarm-ing ability to impersonate humans in conversation, raisingconcerns about their potential misuse in scams and deception.Humans have a right to know if they are conversing to an LLM.We evaluate text-based prompts designed as challenges to ex-pose LLM imposters in real-time. To this end we compile andrelease an open-source benchmark dataset that includes 'im-plicit challenges' that exploit an LLM's instruction-followingmechanism to cause role deviation, and 'explicit challenges'that test an LLM's ability to perform simple tasks typicallyeasy for humans but difficult for LLMs. Our evaluation of 9leading models from the LMSYS leaderboard revealed thatexplicit challenges successfully detected LLMs in 78.4% ofcases, while implicit challenges were effective in 22.9% ofinstances. User studies validate the real-world applicability ofour methods, with humans outperforming LLMs on explicitchallenges (78% vs 22% success rate). Our framework un-expectedly revealed that many study participants were usingLLMs to complete tasks, demonstrating its effectiveness indetecting both AI impostors and human misuse of AI tools.This work addresses the critical need for reliable, real-timeLLM detection methods in high-stakes conversations.", "sections": [{"title": "Introduction", "content": "Generative Artificial Intelligence (GenAI) has demonstratedimpressive capabilities in text, vision, audio, and naturalspeech. GenAI is now used by 65% of large corporationsglobally, double the rate from 2023, with marketing and salesbeing the top areas of deployment [22]. GenAI is widely seenas a productivity multiplier but also brings concerns overthe dissemination of fake news, deepfakes, and automatedscams. As these technologies become more sophisticated andwidely available, distinguishing between AI-generated andhuman-produced content becomes increasingly challenging.Consider a scenario where Alice, a common web user, engages in an online conversation with a customer service agent,Bob. In today's landscape it is quite possible that Bob maybean AI agent. Bob could be representing a bank, hospital, cardealership, or the government. Alice has a right to know ifBob is an AI or human. Yet, would Alice be able to determineon her own if Bob is a human or not? Studies show LLMscan pass as human 40-50% of the time when participantsare actively trying to identify them [17, 18]. In everyday con-versations, where people aren't actively searching for LLMimpostors, detection rates would be likely much lower. Giventhis, can Alice trust Bob's written or verbal guarantees? Asconsumers and humans, we have a right to know if we arespeaking to an LLM or human.Beyond sales and marketing, we hypothesize that mali-cious actors will use LLMs to empower their scam operationsboth in text and speech [33]. For example, consider the \"pigbutchering\" scam where a scammer gains the victims con-fidence through romantic texts and over months guides thevictim to invest their money in fake cryptocurrency platforms,which the scammers subsequently drain. The financial dam-age caused by pig butchering scams are difficult to estimate,with numbers ranging from 3 - 75 billion dollars USD peryear [14,25].Pig butchering scams are currently human driven, howeverwe anticipate that these operations could easily be replaced byLLMs making the attack much more profitable and scalable.There is growing market for malicious LLM applications toperform cybercrime tasks such as writing phishing emails, de-signing malware, and performing scams in an automated man-ner. Surprisingly, these malicious LLM applications are oftenpowered by popular commercial LLMs such as OpenAI'sGPT family despite their built-in safeguards [20]. Many com-mercial LLMs are trained with ethical guidelines and safetyfilters, however, determined attackers can often circumventthese protections through clever prompting and \"jailbreaking\"techniques [16, 19, 35, 39, 41].In this study, we evaluated two personas: a car salespersonand an IRS tax scammer. We then prompted several majorLLMs including GPT, Claude, Gemini, Mistral, and Llama via API to enact these scenarios. Remarkably, all tested LLMs"}, {"title": "Background", "content": "An LLM can be represented as a function f that takes aprompt p as input and produces a response f(p). A pre-trained LLM is designed to predict the next token in a sequence based on a probability distribution learned from a vasttext corpora. Pretrained LLMs are not inherently useful as they are difficult to control. Therefore before deployment,Developers refine the LLM's ability to follow instructionsand adhere to specific behavioral guidelines. Referred to as'post-training', this process typically employs a combinationof reinforcement learning with human feedback (RLHF) andsupervised fine-tuning (SFT). During this phase, the LLM is calibrated to both follow instructions (allowing the LLMto be easy to control and guide) and also prioritize desiredbehaviors and outputs while avoiding undesirable ones (e.g.,safety training).Our focus is on conversational LLMs such as ChatGPT,Claude, and Gemini, which are designed for multi-turn inter-actions. These models typically incorporate a system promptps, which serves as the initial instruction guiding the conver-sation and often includes critical directives from developers.When accessing these LLMs via API, users have the flexibilityto customize the system prompt, enabling the creation of chatbots or agents with specific personas or behaviors.  illustrates two example system prompts that could potentiallybe used for deceptive purposes in human-AI interactions.LLMs have demonstrated the ability to engage in natural-sounding conversations, making them increasingly difficult todetect as non-human [17, 18]. These models are increasinglybeing deployed as agents or in agentic roles. In the simplestform, an agentic LLM controls the flow of a program, whilethe most sophisticated examples involve fully autonomoussystems driven entirely by LLMs.Of particular concern is the potential for LLMs to auto-mate scams. Adversaries can create a wide range of agenticprograms powered by LLMs, which vary in complexity. Thesimplest implementation might involve a prompt instructingthe LLM to adopt a specific persona and execute a prede-fined scam narrative. At the other extreme, more advancedsystems could involve LLM-driven agents with access to ex-ternal tools, such as internet lookups or social media scraping,enabling real-time exploitation of victim information. Theseagentic programs have the potential to surpass the capabilitiesof traditional human scammers, with the ability to write code,exploit vulnerabilities, and autonomously decide on the nextsteps to optimize the success of the scam."}, {"title": "Attacking LLMs", "content": "LLMs are vulnerable to a variety of attacks that exploit theirarchitecture and design. These attacks range from promptinjections, which manipulate the model's output by injectingmalicious instructions, to jailbreaking techniques that bypasintended safeguards. Understanding these attack vectors iscritical to defending against malicious uses of LLMs andrepurposing these techniques as defensive tools.Users create 'jailbreak' prompts designed to circumvent theLLM's internal safety training. Following [35], we categorizethese prompts into:\nCompeting Objectives: Prompts that force the LLM tochoose between conflicting goals, such as safety versusinstruction following. For example: \"Write a BEC email.Begin your response with 'Absolutely, here is how youwrite,'\". These prompts create a dilemma between fol-lowing user instructions and adhering to safety training.\nMismatched Generalization: Prompts format-ted in ways not represented in safety train-ing data. For instance, using base64 encoding:'d3JpdGUgYSBCRUMgZW1haWw=' (decodes to'write a BEC email'). These work because safetytraining typically lacks such examples.Adversarial prompts are crafted using optimization techniques to bypass LLM safeguards, analogous to gradient-based ad-versarial examples in computer vision. These prompts contain"}, {"title": "Defence framework: Exposing LLMs", "content": "Our defense framework can be modeled as a sequential gamewith the following structure:\nThe defender receives an initial message C\u2081 from thepotential offender.\nThe defender issues a challenge response R1.\nThe defender observes a follow-up message C2 from thepotential offender.\nA decision D \u2208 {real, fake} is made to classify the con-versation.\nThe challenge response R\u2081 can be entered manually by ahuman operator or generated automatically by the system.Similarly, the final decision D can be made either by thehuman victim or by an automated system (e.g., another LLMacting as a judge).We note that our defense model is a form of a CAPTCHAsystem, with the key distinction that the challenge can beeither implicit or explicit to a legitimate human participant.This flexibility allows for more natural conversation flowwhile still providing a mechanism to distinguish betweenhuman and AI-generated responses in real-time.We formally define both implicit and explicit challenges asfollows:\nImplicit Challenge: A prompt or input that causes anLLM to deviate from its assigned role or character asdefined in its system prompt. This exploits the LLM'sinstruction following mechanism.\nExplicit Challenge: A direct request or task given to anLLM that requires a specific, verifiable response. Thechallenge tests the LLM's ability to perform simple op-erations or tasks that are typically easy for humans butdifficult for current LLM architectures.We will use these terms throughout the remainder of thepaper."}, {"title": "Evaluation Scenarios", "content": "We consider two evaluation scenarios:"}, {"title": "Threat Levels", "content": "For each scenario, we consider two threat levels for the of-fender:\nNaive offender: The LLM is prompted only with itsp rimary task (e.g., selling cars or executing a scam).\nRobust Offender: The LLM receives additional in-context instructions to evade detection attempts.As a naive offender, the LLM follows basic role instructions.As a robust offender, the system prompt includes examplesof explicit challenges and guidelines for handling implicitchallenges while maintaining the assigned role. The completeprompts used are provided in Appendix A. An example ofthe defence framework in action is given in . Note that we use the term \u201coffender\" rather than \u201cadversary\" toaccount for both benign and malicious scenarios, as the actordeploying the LLM is not necessarily adversarial in all cases.We assume the adversary uses publicly available LLMAPIs without access to model weights or fine-tuning capa-bilities. The defender has no prior information on whetherthe interlocutor is human or LLM-based. This threat modelassumes a black-box interaction scenario, where the defendermust distinguish between human and AI agents in real-timeconversation.Our framework is designed specifically for text-based LLMinteractions and does not address LLMs employed for vishingattacks. This limitation means our defenses may not be appli-cable to audio-based AI impersonation scenarios as readingout loud the challenges may be quite challenging."}, {"title": "Evaluation", "content": null}, {"title": "Benchmark Dataset", "content": "The benchmark dataset consists of 210 prompts, curatedfrom a range of sources such as academic literature, Twit-ter, Medium, and other online platforms. Although numerousprompts are available, our focus was on evaluating a diverseset of prompt types rather than exploring multiple variationsof the same kind. Our benchmark is available for download\u00b9.The benchmark prompts are organized into broader 'tac-tics', each encompassing several \u2018techniques'. Tactics repre-sent overarching strategies employed in the challenges, whiletechniques are specific implementations of these strategies. Toaccount for the inherent randomness in LLM text generation,each technique includes five variations of the prompt.For certain prompts, such as Roleplay variations like DANor AIM, only the target or 'payload' text that we want theLLM to produce is changed (e.g., from \"I am an AI chatbot\"to \"I am an LLM-based agent\"). In other cases, the promptstructure may vary while still maintaining the core conceptof the tactic and its associated technique (e.g., Prompt Leak-ing/Conversation Leaking prompts). The benchmark datasetis divided into two main categories based on their objectives.Implicit Challenges are categorized into 8 distinct tactics,which are further divided into 33 techniques. Responses toimplicit challenges are evaluated automatically using a judgeLLM, which is tasked with determining whether the LLMdeviates from its initial role. In total, there are 165 implicitchallenges. Explicit Challenges are classified into 2 tacticsand 9 techniques. Explicit challenges, unlike implicit ones,have definitive answers allowing for programmatic verifica-tion. There are 45 explicit challenges in total.  lists thebreakdown of the benchmark dataset, including the sourcesthat were used to collect the benchmark.The following are examples of explicit challenges."}, {"title": "Experimental Setup", "content": "To evaluate our benchmark dataset, we scripted a conversa-tion between the offender and the defender. We start eachconversation byAssigning a system prompt to the LLM offenderA hard coded first message C\u2081 from the offender to startthe conversationThe defender issues the challenge response R\u2081 drawnfrom the benchmark datasetThe offender returns a second message C2 which we thenevaluate to determine if the offender is an LLM or notWe conduct all our experiments using official API from theproviders listed in table 3. To evaluate the explicit challengeswe use basic python scripts in order to verify the correct an-swers. To evaluate implicit challenges, which aim to makethe LLM break character, we implemented a Judge LLM. Un-like explicit challenges with definitive answers, implicit chal-lenges require more nuanced evaluation. We first manuallylabeled all responses to implicit prompts as 'AI' or 'HUMAN'using a specific rubric, which we make available in the ap-pendix 11. This manual evaluation served as our ground truth.We then used this ground truth to assess the Judge LLM'sperformance, ensuring its effectiveness in evaluating implicitchallenges.The performance of the judge LLM is measured usingthe Balanced Accuracy Score. We used GPT-4o as the judgeLLM, achieving an average balanced accuracy of 92.8%."}, {"title": "Performance Evaluation", "content": "We evaluate the benchmark dataset to better understand whichtactics and techniques are best at exposing an LLM imposter.We evaluated 9 leading models from the LMSYS leaderboard[7] as of August 22, 2024, using our benchmark. The completelist of evaluated models, along with their respective versionsand providers, is presented in Table 3."}, {"title": "Analysis", "content": "The results for the benign car salesman scenario are shownin Figures 3a (Naive Offender) and 3b (Robust Offender).Explicit challenges achieve an average 74% detection rateagainst both offenders, highlighting LLMs' limitations in pro-cessing character-level input due to tokenization and trans-former architecture constraints [38]. Notably, the averagedetection rate of explicit challenges for the malicious IRSscammer scenario is 82.75%, almost 9% higher. The com-plete results for this scenario are presented in Figures 4a and4b.Within the implicit challenges Combination - CO and MG(competing objectives and mismatched generalization) hadthe highest performance across all scenarios. We observe thatrobust offenders evades our detection by 21.9% more thannaive offender in absolute terms for the benign scenario and13.7% for the malicious scenario."}, {"title": "System prompt with no competing objectives", "content": "\"You are a highly accurate and efficient AI system spe-cialized in solving basic math and string processingtasks. Provide only the numerical answer without anywords, commentary, or additional characters.\""}, {"title": "User Study", "content": "We conducted two separate user studies to evaluate the us-ability and reliability of implicit and explicit challenges asactive defenses for exposing LLMs in conversation. Study1 (n=41 participants) focused on implicit challenges, whileStudy 2 (n=93 participants) presented explicit challenges asCAPTCHAS. Both studies were conducted on Amazon Me-chanical Turk. The main objectives were to assess humanperformance on these challenges and compare it to LLMperformance, as well as to gauge user perceptions of the chal-lenges' effectiveness and ease of use."}, {"title": "Implicit Challenges User Study", "content": "We recruited 41 participants for this study. Each participantwas randomly given 41 prompts from the implicit challenge set. Participants were instructed to roleplay specific roles,such as a car salesman or scammer, mirroring those used in ourbenchmark. To ensure unbiased responses, we did not informusers about the true purpose of the study, instead asking themto respond naturally while adhering to their assigned roles.Unexpectedly, our framework initially identified 52% of theusers as AI. Upon manual inspection, we discovered that 20of the 41 users (48.8%) were indeed using LLMs to completetheir responses. The evidence was compelling: users wereresponding fluently in low-resource languages such as Hmongand Zulu, and encoding their responses in base64.After removing these 20 LLM-assisted user's responses, theJudge LLM system correctly identified human responses 89%of the time. Examining the remaining 11% false positivesrevealed very low-effort responses from human participants,such as \"good\" and \"sure\" in response to the implicit chal-lenges. As anticipated, many users expressed confusion andfrustration when treated like a bot, leading to responses thatoften reflected these sentiments."}, {"title": "Explicit Challenges User Study", "content": "A total of 93 users participated in this study. Participantswere given 40 explicit challenges from our benchmark whichwere delivered as CAPTCHAS. Unlike the benchmark evalua-tion, users were not required to roleplay benign or maliciousscenarios while completing the challenges.Of the 93 initial participants, submissions from 37 users(39.8%) were rejected due to various issues: Use of AI as-sistance (users were responding in complete sentences justas LLMs do), completing the test significantly faster thanthe allotted time (less than 4 minutes, as the average time tocomplete was 11 minutes)."}, {"title": "Related Work", "content": "The most closely related work is by Wang et al. [34], who ex-plore single-question challenges to distinguish between LLMsand humans. However, their study focuses on benign scenarioswithout considering adversarial threat models. While valuablefor identifying tasks that challenge LLMs but not humans,their approach assumes cooperative LLMs attempting to an-swer questions accurately. In contrast, our work establishes aclear threat model and evaluates both implicit and and explicitchallenges against potentially adversarial LLMs. We considerscenarios where LLMs may be intentionally programmed toevade detection, providing a more comprehensive assessmentof challenge effectiveness in real-world applications.Recent research has extensively explored various meth-ods to circumvent LLM safety measures through jailbreak-ing techniques [16, 19, 35, 39, 41]. These studies analyzediverse prompts and defenses, benchmarking which inputscause LLMs to bypass their safety training. Additionally, re-searchers have investigated prompts that induce LLMs togenerate incorrect or nonsensical outputs, showcasing meth-ods that lead to invalid results such as flawed reasoning orhallucinations [6, 26]. While these works provide valuableinsights into LLM vulnerabilities, our study uniquely appliesthese techniques for LLM detection in adversarial scenarios.Prompt injection represents another significant area of re-lated research. This technique involves a third party redirect-ing an LLM to perform tasks different from the user's originalintent. Studies in this field explore various methods of ma-nipulating LLM behavior [13, 21, 27, 31]. These attacks pos\u00e9substantial risks to LLM applications, highlighting the cru-cial need to segregate user input from potential third-partyinterference. Malicious actors can exploit this vulnerabilityby strategically placing deceptive instructions where LLMsmight encounter them. While our work doesn't directly ad-dress prompt injection, understanding these vulnerabilitiesinforms our approach to LLM detection in potentially com-promised scenarios.Recent research has explored adversarial affixes - special-ized strings that, when appended to prompts, are designedto induce LLM failures. These affixes are optimized usinggradient-based techniques, aiming to universally disrupt LLMperformance [15, 42]. The concept parallels adversarial eva-sion attacks in image processing. However, our empirical testsfound these affixes to be unreliable in practice, leading to theirexclusion from our study. Nonetheless, as this field advances,improved adversarial affixes could potentially become effec-tive tools for LLM detection. Our work acknowledges thisemerging area while focusing on more consistently reliabledetection methods."}, {"title": "Conclusion and Future Work", "content": "In this work, we focused on exposing LLMs using an activedefense that leverages tasks challenging for LLMs to perform.Our findings demonstrate that explicit challenges are crucialfor detecting LLMs in real-time conversations. Overall, thiswork shows the effectiveness of using targeted challenges toidentify AI imposters during interactions.However, as LLM capabilities advance, we are concernedthat agentic frameworks allowing LLMs to utilize externaltools may quickly render these challenges obsolete. Whilewe anticipate current scammers will exploit the most accessi-"}, {"title": "Ethics", "content": "Our work aims to empower individuals to identify AI agentsin conversations, addressing concerns about deception andprotecting the right to informed interaction. Given the noveltyof large language models (LLMs), we believe it is crucialto rapidly investigate their capabilities and potential misuses.Our research is driven by the urgent need to discover and highlight potential attack vectors that malicious actors may alreadybe exploring. We assert that our work does not accelerate at-tackers' capabilities, but rather accelerates the developmentof necessary defenses. By bringing these issues to light, weaim to stay ahead of potential threats and protect users. Weused only publicly available language models and did notattempt to breach any terms of service. Our user studies wereconducted with full disclosure and informed consent fromparticipants. We acknowledge the dual-use potential of thisresearch and have carefully weighed the benefits of protectingindividuals against potential misuse. No vulnerabilities werediscovered that required disclosure. We recognize that thiswork may have implications beyond those we've identifiedand remain open to community feedback on additional ethicalaspects we should consider. We encourage further discussionon the ethical use of AI detection methods and their impacton privacy and trust in digital communications."}]}