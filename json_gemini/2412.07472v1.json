{"title": "SmartAgent: Chain-of-User-Thought for Embodied Personalized Agent\nin Cyber World", "authors": ["Jiaqi Zhang", "Chen Gao", "Liyuan Zhang", "Yong Li", "Hongzhi Yin"], "abstract": "Recent advances in embodied agents with multimodal per-\nception and reasoning capabilities based on large vision-\nlanguage models (LVLMs), excel in autonomously interact-\ning either real or cyber worlds, helping people make in-\ntelligent decisions in complex environments. However, the\ncurrent works are normally optimized by golden action tra-\njectories or ideal task-oriented solutions toward a definitive\ngoal. This paradigm considers limited user-oriented fac-\ntors, which could be the reason for their performance reduc-\ntion in a wide range of personal assistant applications. To\naddress this, we propose Chain-of-User-Thought (COUT),\na novel embodied reasoning paradigm that takes a chain of\nthought from basic action thinking to explicit and implicit\npersonalized preference thought to incorporate personal-\nized factors into autonomous agent learning. The main\nchallenges of achieving COUT include: 1) the definition\nof embodied personalized tasks, 2) the embodied environ-\nment epitomizes personalized preference, and 3) the way to\nmodel embodied personalized actions. To target COUT, we\nintroduce SmartAgent, an agent framework perceiving cy-\nber environments and reasoning personalized requirements\nas: 1) interacting with GUI to access an item pool, 2) gen-\nerating users' explicit requirements implied by previous ac-\ntions, and 3) recommending items to fulfill users' implicit\nrequirements. To demonstrate SmartAgent's capabilities,\nwe also create a brand-new dataset SmartSpot that offers", "sections": [{"title": "1. Introduction", "content": "Embodied artificial intelligence [9] is considered as a cru-\ncial stride toward Artificial General Intelligence (AGI) [10].\nPowered by the recent advances in large multi-modal mod-\nels, embodied agents have been built upon to behave like\nreal humans, capable of perceiving, reasoning, and acting\nwith their surroundings in both real and cyber worlds. The\nenthusiasm for deploying such humanoid capabilities is ev-\nident in various tasks, including autonomic robotics [2, 7],\ngame AI [35, 44], smart device assistants [17, 30], and\nsmart city [12, 42]. Many of these scenarios require em-\nbodied agents to do more than follow instructions and ex-\necute actions like emotionless robots; otherwise, they are\nexpected to serve as personal assistants attuned to human\npreferences in the meantime. For instance, for smart device\nassistance, agents struggle to personalize responses to am-\nbiguous user queries such as providing music recommen-\ndations, though they fully understand the operation logic of\na music player. Generally speaking, a fully functional em-\nbodied agent necessitates personalized perceptual capabili-\nties, thereby enabling a comprehensive agent-environment-\nuser triadic perception of the world, similar to JARVIS\u00b9, a\nfictional artificial assistant created by Iron Man which can\nmake personalized intelligent decisions for him.\nHowever, this personalized consideration is absent\namong the current embodied agent works, where the op-\ntimization normally relies on golden action trajectories [6,\n31] or ideal task-oriented solutions [8, 22]. Although these\nfixed paths can effectively accomplish task goals, they can\nonly train embodied agents to be rigid task-oriented prob-\nlem solvers, overlooking the multiple valid approaches that\noften exist as user-oriented indicators. Furthermore, the\npractical environment may exhibit unpredictable behavior,\nsuch as when new functions are involved or unseen scenes\nare observed. In these cases, task-oriented agents are not\nflexible enough to even capture dynamic changes in basic\ntask goals [23, 34], let alone discern changes in user prefer-\nence. As a result, such training paradigms actually restrict\nthe learning of user-oriented perceptual capabilities, which"}, {"title": "2. Related Work", "content": "Embodied AI with Large Vision-Language Models. Em-\nbodied AI marks a significant transformation from tradi-\ntional artificial intelligence which relies on static datasets,\nto autonomous agents that learn through interactions with\ntheir environments. Thanks to the rapid development of\nLVLM, embodied agents can now process pure visual and\ntextual observation as input. The current effort for training\nsuch embodied agents can be categorized into main direc-\ntions, considering fixed and dynamic environments respec-\ntively [30]. Many existing approaches follow fixed envi-\nronments, by comparing agents' action trajectories to pre-\ncollected human demonstrations [20, 26, 28, 43, 50, 52].\nLEO [20] presents a generalist agent capable of interacting"}, {"title": "3. Chain-of-User-Thought (COUT)", "content": "Chain-of-User-Thought (COUT) is a reasoning paradigm\nwhere an agent controls smart devices based on both task\ngoals and user personalized preference as follows:"}, {"title": "3.1. Definition", "content": "Chain-of-User-Thought (COUT) is a reasoning paradigm\nwhere an agent controls smart devices based on both task\ngoals and user personalized preference as follows:\n$A_{COUT} = \\{Action_i | Task\\, goal, User\\, preference\\}$, (1)\nwhere $A_{COUT}$ is the action space, which differs from the\nexisting work that relies solely on task goal as:\n$A_{existing work} = \\{Action_i | Task\\, goal\\}$. (2)\nSpecifically, an agent is required to generate $Action_i$\nthrough a progressive reasoning chain from basic embod-\nied behavior level, gradually to deeper explicit personal-\nized reasoning level, and finally to high implicit personal-\nized reasoning level. This process requires embodied agents\nthe ability of a multi-faceted understanding of user person-\nalized preferences."}, {"title": "3.2. Components", "content": "As illustrated in Eq. (1), we formulate the COUT process\nin a common cyber world case, personal assistance of smart\ndevices, in terms of three stages of thoughts: a) Thought\n#1 GUI Navigation, b) Thought #2 Underlying Reasoning,\nand c) Thought #3 Personalized Recommendation. Thought\n#1 denotes the surface-level thought for basic embodied be-\nhavior on the device GUI, e.g., This action clicks a button\nnamed [Button_name]. Thought #2 denotes the deeper-level\nthought for explicit user preference, e.g., It seems the user\nneeds items with [Restriction1], [Restriction2], and [Re-\nstriction3]. Thought #3 denotes the high-level thought for"}, {"title": "4. The SmartSpot Benchmark", "content": "Given the scarcity of training data for embodied agents that\nexplicitly captures the personalized analysis highlighted\nin the COUT paradigm, we propose to construct a novel\nbenchmark named SmartSpot."}, {"title": "4.1. Dataset Summary", "content": "We collect SmartSpot data from Meituan, a well-known In-\nternet service platform in China that offers a variety of life\nservices, including food recommendations, hotel bookings,\nonline flight ticket sales, etc. To create more practical per-\nsonal scenes, we develop SmartSpot with two scenarios: the\nsingle-channel scenario which focuses on one type of ser-\nvice, and the multi-channel scenario which combines two\nsingle channels. We select five of the most daily used sin-\ngle channels: Food, Hotel, Flight, Movie, and Medicine.\nFor the multi-channel scenarios, we pair Flight and Food"}, {"title": "4.2. Dataset Collection & Analysis", "content": "We curate SmartSpot following the real-life usage of this\nplatform. The process begins with generating pairs of user\ninstructions along with the underlying requirements. To\nensure consistency, we establish intention seeds to gener-\nate them simultaneously. Specifically, we recruit annota-\ntors experienced on this platform to select 2-3 significant\nattributes as seeds for searching specific item pools for each\nchannel. For example, the \"[recently played]\" in Fig. 1\npresents one seed result, which could be other choices that\nusers can click on. By creating multiple permutations of\nintention seeds, we collect a diverse batch of instruction\npairs while ensuring there were no duplicates. Then, ac-\ncording to these instruction pairs, the annotators performed\nepisodic GUI operations, capturing screenshots and ground-"}, {"title": "5. SmartAgent Approach", "content": ""}, {"title": "5.1. Overview", "content": "Given a user instruction $i \\in I$ to complete, the agent will\nnavigate through an episode comprising three groups of\nsteps: searching for an item pool, finding the item pool, and\nmaking item recommendations. At time step t, the agent\nreceives a screenshot observation $o_t \\in O$. Then, the agent\nshould takes an action $a_t \\in A$ according to its current as-\nsets $\\{o_t, i, h_{t-1}\\}$, where $h_{t-1} = (o_1, A_1, ..., O_{t-1}, a_{t-1})$ is\nthe historical actions. The chosen action signifies either a\nGUI command, a signal that an item pool has been found,\nor a recommendation result for items within that pool. We\nwill details the action space A in Sec. 4\nThe primary design principles of SmartAgent are two-\nfold: i) It should process the multimodal input of high-\nresolution screenshot images and textual instructions, and\nthe output of embodied action, along with textual thoughts;\nii) It should deal with ambiguous instructions as reasoning\ngoals at all the stages of COUT. We therefore transform\nall data of different modalities into a token sequence, illus-\ntrated below:\nGenerate..\nsystem message\n$T_{step1}:....$\nimage .... image\nscreenshot image tokens previous actions\nAgent: $T^{(1)}_{res}, ...T^{(N)}_{res}$\nUser: I want...\ninstruction\nresponse\n(3)\nUsing this representation, we formulate the learning of\nSmartAgent as GPT-style auto-regressive approach, in line\nwith [20]. For instance, in Fig. 1, given a smartphone\nscreenshot and user's instruction \"Can you give me some\nmusic recommendations?\", we craft a query prompt as:"}, {"title": "5.2. Agent Training", "content": "To achieve COUT reasoning, we divide each episode into\nthe following embodied and personalized stages to train\nSmartAgent successively. Specifically, the SmartAgent\nbackbone LVLM functions in two roles: a Perceiver trained\nin our environment to predict actions or a Reasoner utilizes\nthe original LVLMs to generate thoughts.\nEmbodied Stage. This stage aims to complete ambigu-\nous instructions to find the item pool. The agent takes only\nGUI action and item pool screenshots as visual input. As\nstated in the Sec. 5.1, the multimodal assets first feed into\nthe Perceiver to predict the specific embodied actions, re-\nferred to as Thought #1 in COUT. Subsequently, the Rea-\nsoner infers step-wise action thoughts and summaries as an\nunderlying requirement as Thought #2. The Thought #2 in-\ndicates the user's intention explicitly reflected in this stage.\nFor instance, as illustrated in Fig. 1, for user inquiries about\nmusic, Thought #2 may include specific constraints not\npresent in the original instruction, such as \u201cJustin Bieber\u201d.\nAs a result, the underlying requirement serves as a key in-\ntermediate output, offering a clearer representation of user\nintentions for the next personalized stage.\nPersonalized Stage. With the fine-grained underlying\nrequirement in Thought #2, this stage focuses on mak-\ning personalized recommendations. The same Perceiver\nmodel takes item screenshots as visual input and deter-\nmines whether a recommendation is warranted, responding\nwith either textual \u201cYes\u201d or \u201cNo\u201d, which is designated as\nThought #3."}, {"title": "5.3. Implementation Details", "content": "We choose Qwen-VL [1] as our backbone LVLM, which\nencodes visual inputs with a high resolution of 448*448.\nThe training of SmartAgent starts from the continual pre-"}, {"title": "6. Evaluation", "content": "We demonstrate SmartAgent's capabilities by comprehen-\nsive evaluations of overall abilities in Sec. 6.1 and a full\nspectrum of sub-tasks encompassing GUI Grounding in\nSec. 6.2, Autonomous GUI Operation in Sec. 6.3, Explicit\n& Implicit Personalized Reasoning in Sec. 6.4, and Zero-\nshot Reasoning in Sec. 6.5. We also report more insights in\nSec. 6.6.\nMetrics. Following most of the setting in [3], we com-\npute the cross-entropy loss for Thought #1 and Thought #3\nwith their ground-truth actions, and semantic similarity for\nThought #2 with the underlying requirement. A bounding\nbox may be contained in ground-truth action to verify if a\nclick action is hit. We therefore evaluate SmartAgent us-\ning the below metrics, in terms of embodied action metrics\n(following Mind2web [6]) for Thought #1, and personalized\npreference metrics for Thought #2 and Thought #3:\n\u2022 Element Accuracy (Ele.Acc): The accuracy of predicted\ncoordinate with ground-truth for click and type action.\n\u2022 Step Successful Rate (SSR): The rate of steps that both\nthe action type and value are successfully predicted.\n\u2022 Explicit Preference Accuracy (Exp.Acc): The semantic\nsimilarity between the predicted underlying requirement\nand ground truth.\n\u2022 Implicit Preference Accuracy (Imp.Acc): The accuracy\nof predicted item recommendations with ground truth."}, {"title": "6.1. Embodied Personalized Reasoning", "content": "We first investigate the comprehensive capabilities of\nSmartAgent in handling embodied tasks and personalized\nreasoning, primarily focusing on SmartSpot for validation.\nSpecifically, we categorize the tasks based on the SmartSpot"}, {"title": "6.2. GUI Grounding", "content": "An important consideration is whether training with per-\nsonalized capabilities leads to catastrophic forgetting of\npre-trained embodied abilities. In this section, we evalu-\nate SmartAgent on a renowned GUI Grounding benchmark\nSmartSpot to assess its foundational perception of raw GUI\ndata. The comparative baselines are divided into two main\ncategories: GUI specialist models and general LLMs."}, {"title": "6.3. Autonomous GUI Operation", "content": "Automated execution of human instructions is a founda-\ntional capability of embodied agents. In this section, we\nvalidate SmartAgent's basic abilities to handle GUI action\nepisodes autonomously using the classic GUI agent bench-\nmark Mind2Web."}, {"title": "6.4. Explicit & Implicit Personalized Reasoning", "content": "In this section, we present a case study to demonstrate\nSmartAgent's performance in explicit user underlying rea-\nsoning and implicit item recommendation tasks. As shown\nin Fig. 5, SmartAgent correctly predicted the action of\nclicking on the economy class option in the visual obser-\nvation. This prediction is reflected in the summarization of\nthe user's underlying requirements. Leveraging this textual\nrepresentation of explicit needs, SmartAgent made recom-\nmendations for the last two flight options, effectively ad-\ndressing the user's implicit requirements."}, {"title": "6.5. Zero-shot Reasoning", "content": "Zero-shot perception is the ultimate goal for embodied\nagents, enabling them truly to learn from interactions\nwith their environment. In this section, we utilize the\nMEDICINE channel in SmartSpot to evaluate SmartAgent's\nzero-shot performance in unseen scenarios."}, {"title": "6.6. More Insights into COUT", "content": "In this section, we provide deeper insights into COUT rea-\nsoning, focusing on the two-stage training and end-to-end\ntraining settings (without underlying thought generation). We therefore\nargue that balancing embodied perceptions in response to\nchanging environments with reliable personal service is cru-\ncial for future COUT research."}, {"title": "7. Conclusion and Future Work", "content": "In this paper, we introduce a novel embodied reasoning\nparadigm, COUT, which for the first time defines an embod-\nied personalized task. We establish a clear definition and\ncomponents of the COUT paradigm and analyze its chal-\nlenges. To address these challenges, we propose SmartA-\ngent to instantiate COUT through a two-stage training from\nessential GUI reasoning to high-level user thought reason-\ning. For benchmarking this progress in SmartAgent, we cre-\nated SmartSpot, the first embodied AI benchmark featuring\nexplicit personalization evaluations. Results on SmartSpot\ndemonstrate the effectiveness and proficiency of SmartA-\ngent over full-stage embodied personalized reasoning tasks."}, {"title": "8. Action Space of SmartSpot", "content": "SmartSpot offers common GUI actions in daily usage. We\nfollow AiTW [31] to assign a action type_id to each\naction for model prediction. The full action space is shown\nas follows:\n\u2022 click(x,y):4. A click action at (x,y), where each\nvalue ranges from [0,1], indicates the corresponding po-\nsition ratio relative to the image's width and height.\n\u2022 type(\"text\"):3. An action that types text at (x,y).\n\u2022 scroll (direction): Scroll actions for the screen,\nscroll up/down/left/right are assigned with 1, 0, 8, and 9.\n\u2022 complete:10. An action of determining if has reached\nthe item pool.\n\u2022 recommendation(\"Yes/No\"):2. An action of rec-\nommending an item, as either \u201cYes\u201d or \u201cNo\u201d.\n\u2022 BACK:5. An action for returning to the previous step.\n\u2022 HOME: 6. An action for returning to the homepage."}, {"title": "9. More Training Details", "content": "Following the approach of [3], We intuitively present nu-\nmerical coordinates as natural languages without additional\ntokenization or pre-/post-processing. We train SmartAgent\n15 epochs for both the embodied and personalized stages.\nEmbodied baselines are trained for 15 rounds. Results on\nthe ScreenSpot and Mind2Web benchmarks are evaluated\nvia direct testing and 10 epochs of training, as in [3]."}, {"title": "10. More Results", "content": "This section shows SmartAgent's performance results on\nbasic embodied sub-tasks. The results of GUI grounding\non the benchmark ScreenSpot are presented in Tab. 5, while\nthe results of autonomous GUI operation on the benchmark\nMind2Web can be found in Tab. 6."}]}